2020.acl-main.414,D17-1042,0,0.024517,"iven question and candidate answers are covered by the retrieved justifications. Despite its simplicity, our approach outperforms all the previous methods (including supervised methods) on the evidence selection task on two datasets: MultiRC and QASC. When these evidence sentences are fed into a RoBERTa answer classification component, we achieve state-of-the-art QA performance on these two datasets. 1 Introduction Explainability in machine learning (ML) remains a critical unsolved challenge that slows the adoption of ML in real-world applications (Biran and Cotton, 2017; Gilpin et al., 2018; Alvarez-Melis and Jaakkola, 2017; Arras et al., 2017). Question answering (QA) is one of the challenging natural language processing (NLP) tasks that benefits from explainability. In particular, multihop QA requires the aggregation of multiple evidence facts in order to answer complex natural language questions (Yang et al., 2018). Several multi-hop QA datasets have been proposed recently (Yang et al., 2018; Khashabi et al., 2018a; Welbl et al., 2018; Dua et al., 2019; Chen and Durrett, 2019; Khot et al., 2019a; Sun et al., 2019b; Jansen and Ustalov, 2019; Rajpurkar et al., 2018). While several neural methods have achieved s"
2020.acl-main.414,D19-5310,0,0.25542,"of interpretability via evidence text selection. QA approaches that focus on interpretability can be broadly classified into three main categories: supervised, which require annotated justifications at training time, latent, which extract justification sentences through latent variable methods driven by answer quality, and, lastly, unsupervised ones, which use unsupervised algorithms for evidence extraction. In the first class of supervised approaches, a supervised classifier is normally trained to identify correct justification sentences driven by a query (Nie et al., 2019; Tu et al., 2019; Banerjee, 2019). Many systems tend to utilize a multi-task learning setting to learn both answer extraction and justification selection with the same network (Min et al., 2018; Gravina et al., 2018). Although these approaches have achieved impressive performance, they rely on annotated justification sentences, which may not be always available. Few approaches have used distant supervision methods (Lin et al., 2018; Wang et al., 2019b) to create noisy training data for evidence retrieval but these usually underperform due to noisy labels. In the latent approaches for selecting justifica4515 tions, reinforceme"
2020.acl-main.414,D18-1454,0,0.0625691,"Missing"
2020.acl-main.414,P17-1171,0,0.198061,"tems have relied on structured knowledge base (KB) QA. For example, several previous works have used ConceptNet (Speer et al., 2017) to keep the QA process interpretable (Khashabi et al., 2018b; Sydorova et al., 2019). However, the construction of such structured knowledge bases is expensive, and may need frequent updates. Instead, in this work we focus on justification selection from textual (or unstructured) KBs, which are inexpensive to build and can be applied in several domains. In the same category of unsupervised approaches, conventional information retrieval (IR) methods such as BM25 (Chen et al., 2017) have also been widely used to retrieve independent individual sentences. As shown by (Khot et al., 2019a; Qi et al., 2019), and our table 2, these techniques do not work well for complex multi-hop questions, which require knowledge aggregation from multiple related justifications. Some unsupervised methods extract groups of justification sentences (Chen et al., 2019; Yadav et al., 2019b) but these methods are exponentially expensive in the retrieval step. Contrary to all of these, AIR proposes a simpler and more efficient method for chaining justification sentences. As shown in fig. 2, the pr"
2020.acl-main.414,N19-1405,0,0.0543589,"cal unsolved challenge that slows the adoption of ML in real-world applications (Biran and Cotton, 2017; Gilpin et al., 2018; Alvarez-Melis and Jaakkola, 2017; Arras et al., 2017). Question answering (QA) is one of the challenging natural language processing (NLP) tasks that benefits from explainability. In particular, multihop QA requires the aggregation of multiple evidence facts in order to answer complex natural language questions (Yang et al., 2018). Several multi-hop QA datasets have been proposed recently (Yang et al., 2018; Khashabi et al., 2018a; Welbl et al., 2018; Dua et al., 2019; Chen and Durrett, 2019; Khot et al., 2019a; Sun et al., 2019b; Jansen and Ustalov, 2019; Rajpurkar et al., 2018). While several neural methods have achieved state-of-theart results on these datasets (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019), we argue that many of these directions lack a human-understandable explanation of their inference process, which is necessary to transition these approaches into realworld applications. This is especially critical for multi-hop, multiple choice QA (MCQA) where: (a) the answer text may not come from an actual knowledge base passage, and (b) reasoning is required"
2020.acl-main.414,P17-1020,0,0.0221913,"ti-task learning setting to learn both answer extraction and justification selection with the same network (Min et al., 2018; Gravina et al., 2018). Although these approaches have achieved impressive performance, they rely on annotated justification sentences, which may not be always available. Few approaches have used distant supervision methods (Lin et al., 2018; Wang et al., 2019b) to create noisy training data for evidence retrieval but these usually underperform due to noisy labels. In the latent approaches for selecting justifica4515 tions, reinforcement learning (Geva and Berant, 2018; Choi et al., 2017) and PageRank (Surdeanu et al., 2008) have been widely used to select justification sentences without explicit training data. While these directions do not require annotated justifications, they tend to need large amounts of question/correct answer pairs to facilitate the identification of latent justifications. strategy that relies on explicit terms from the query and the previously retrieved justification. Lastly, none of the previous iterative retrieval approaches address the problem of semantic drift, whereas AIR accounts for drift by controlling the query reformulation as explained in sec"
2020.acl-main.414,N19-1423,0,0.0311486,"(QA) is one of the challenging natural language processing (NLP) tasks that benefits from explainability. In particular, multihop QA requires the aggregation of multiple evidence facts in order to answer complex natural language questions (Yang et al., 2018). Several multi-hop QA datasets have been proposed recently (Yang et al., 2018; Khashabi et al., 2018a; Welbl et al., 2018; Dua et al., 2019; Chen and Durrett, 2019; Khot et al., 2019a; Sun et al., 2019b; Jansen and Ustalov, 2019; Rajpurkar et al., 2018). While several neural methods have achieved state-of-theart results on these datasets (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019), we argue that many of these directions lack a human-understandable explanation of their inference process, which is necessary to transition these approaches into realworld applications. This is especially critical for multi-hop, multiple choice QA (MCQA) where: (a) the answer text may not come from an actual knowledge base passage, and (b) reasoning is required to link the candidate answers to the given question (Yadav et al., 2019b). Figure 1 shows one such multi-hop example from a MCQA dataset. In this paper we introduce a simple alignmentbased iterati"
2020.acl-main.414,N19-1246,0,0.0428887,"Missing"
2020.acl-main.414,P19-1222,0,0.0917786,"poses a simpler and more efficient method for chaining justification sentences. As shown in fig. 2, the proposed QA approach consists of two components: (a) an unsupervised, iterative component that retrieves chains of justification sentences given a query; and (b) an answer classification component that classifies a candidate answer as correct or not, given the original question and the previously retrieved justifications. We detail these components in the next two sub-sections. Recently, many supervised iterative justification retrieval approaches for QA have been proposed (Qi et al., 2019; Feldman and El-Yaniv, 2019; Banerjee, 2019; Das et al., 2018). While these were shown to achieve good evidence selection performance for complex questions when compared to earlier approaches that relied on just the original query (Chen et al., 2017; Yang et al., 2018), they all require supervision. As opposed to all these iterative-retrieval methods and previously discussed directions, our proposed approach AIR is completely unsupervised, i.e., it does not require annotated justifications. Further, unlike many of the supervised iterative approaches (Feldman and El-Yaniv, 2019; Sun et al., 2019a) that perform query refo"
2020.acl-main.414,C18-1014,0,0.0155935,"s tend to utilize a multi-task learning setting to learn both answer extraction and justification selection with the same network (Min et al., 2018; Gravina et al., 2018). Although these approaches have achieved impressive performance, they rely on annotated justification sentences, which may not be always available. Few approaches have used distant supervision methods (Lin et al., 2018; Wang et al., 2019b) to create noisy training data for evidence retrieval but these usually underperform due to noisy labels. In the latent approaches for selecting justifica4515 tions, reinforcement learning (Geva and Berant, 2018; Choi et al., 2017) and PageRank (Surdeanu et al., 2008) have been widely used to select justification sentences without explicit training data. While these directions do not require annotated justifications, they tend to need large amounts of question/correct answer pairs to facilitate the identification of latent justifications. strategy that relies on explicit terms from the query and the previously retrieved justification. Lastly, none of the previous iterative retrieval approaches address the problem of semantic drift, whereas AIR accounts for drift by controlling the query reformulation"
2020.acl-main.414,N18-1023,0,0.135584,"uction Explainability in machine learning (ML) remains a critical unsolved challenge that slows the adoption of ML in real-world applications (Biran and Cotton, 2017; Gilpin et al., 2018; Alvarez-Melis and Jaakkola, 2017; Arras et al., 2017). Question answering (QA) is one of the challenging natural language processing (NLP) tasks that benefits from explainability. In particular, multihop QA requires the aggregation of multiple evidence facts in order to answer complex natural language questions (Yang et al., 2018). Several multi-hop QA datasets have been proposed recently (Yang et al., 2018; Khashabi et al., 2018a; Welbl et al., 2018; Dua et al., 2019; Chen and Durrett, 2019; Khot et al., 2019a; Sun et al., 2019b; Jansen and Ustalov, 2019; Rajpurkar et al., 2018). While several neural methods have achieved state-of-theart results on these datasets (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019), we argue that many of these directions lack a human-understandable explanation of their inference process, which is necessary to transition these approaches into realworld applications. This is especially critical for multi-hop, multiple choice QA (MCQA) where: (a) the answer text may not come from"
2020.acl-main.414,D19-1281,0,0.0649489,"hat slows the adoption of ML in real-world applications (Biran and Cotton, 2017; Gilpin et al., 2018; Alvarez-Melis and Jaakkola, 2017; Arras et al., 2017). Question answering (QA) is one of the challenging natural language processing (NLP) tasks that benefits from explainability. In particular, multihop QA requires the aggregation of multiple evidence facts in order to answer complex natural language questions (Yang et al., 2018). Several multi-hop QA datasets have been proposed recently (Yang et al., 2018; Khashabi et al., 2018a; Welbl et al., 2018; Dua et al., 2019; Chen and Durrett, 2019; Khot et al., 2019a; Sun et al., 2019b; Jansen and Ustalov, 2019; Rajpurkar et al., 2018). While several neural methods have achieved state-of-theart results on these datasets (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019), we argue that many of these directions lack a human-understandable explanation of their inference process, which is necessary to transition these approaches into realworld applications. This is especially critical for multi-hop, multiple choice QA (MCQA) where: (a) the answer text may not come from an actual knowledge base passage, and (b) reasoning is required to link the candid"
2020.acl-main.414,P18-1161,0,0.0211601,"extraction. In the first class of supervised approaches, a supervised classifier is normally trained to identify correct justification sentences driven by a query (Nie et al., 2019; Tu et al., 2019; Banerjee, 2019). Many systems tend to utilize a multi-task learning setting to learn both answer extraction and justification selection with the same network (Min et al., 2018; Gravina et al., 2018). Although these approaches have achieved impressive performance, they rely on annotated justification sentences, which may not be always available. Few approaches have used distant supervision methods (Lin et al., 2018; Wang et al., 2019b) to create noisy training data for evidence retrieval but these usually underperform due to noisy labels. In the latent approaches for selecting justifica4515 tions, reinforcement learning (Geva and Berant, 2018; Choi et al., 2017) and PageRank (Surdeanu et al., 2008) have been widely used to select justification sentences without explicit training data. While these directions do not require annotated justifications, they tend to need large amounts of question/correct answer pairs to facilitate the identification of latent justifications. strategy that relies on explicit t"
2020.acl-main.414,D19-1242,0,0.250002,"on of ML in real-world applications (Biran and Cotton, 2017; Gilpin et al., 2018; Alvarez-Melis and Jaakkola, 2017; Arras et al., 2017). Question answering (QA) is one of the challenging natural language processing (NLP) tasks that benefits from explainability. In particular, multihop QA requires the aggregation of multiple evidence facts in order to answer complex natural language questions (Yang et al., 2018). Several multi-hop QA datasets have been proposed recently (Yang et al., 2018; Khashabi et al., 2018a; Welbl et al., 2018; Dua et al., 2019; Chen and Durrett, 2019; Khot et al., 2019a; Sun et al., 2019b; Jansen and Ustalov, 2019; Rajpurkar et al., 2018). While several neural methods have achieved state-of-theart results on these datasets (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019), we argue that many of these directions lack a human-understandable explanation of their inference process, which is necessary to transition these approaches into realworld applications. This is especially critical for multi-hop, multiple choice QA (MCQA) where: (a) the answer text may not come from an actual knowledge base passage, and (b) reasoning is required to link the candidate answers to the"
2020.acl-main.414,2021.ccl-1.108,0,0.271141,"Missing"
2020.acl-main.414,Q19-1014,0,0.29854,"on of ML in real-world applications (Biran and Cotton, 2017; Gilpin et al., 2018; Alvarez-Melis and Jaakkola, 2017; Arras et al., 2017). Question answering (QA) is one of the challenging natural language processing (NLP) tasks that benefits from explainability. In particular, multihop QA requires the aggregation of multiple evidence facts in order to answer complex natural language questions (Yang et al., 2018). Several multi-hop QA datasets have been proposed recently (Yang et al., 2018; Khashabi et al., 2018a; Welbl et al., 2018; Dua et al., 2019; Chen and Durrett, 2019; Khot et al., 2019a; Sun et al., 2019b; Jansen and Ustalov, 2019; Rajpurkar et al., 2018). While several neural methods have achieved state-of-theart results on these datasets (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019), we argue that many of these directions lack a human-understandable explanation of their inference process, which is necessary to transition these approaches into realworld applications. This is especially critical for multi-hop, multiple choice QA (MCQA) where: (a) the answer text may not come from an actual knowledge base passage, and (b) reasoning is required to link the candidate answers to the"
2020.acl-main.414,P18-1160,0,0.0348701,", which require annotated justifications at training time, latent, which extract justification sentences through latent variable methods driven by answer quality, and, lastly, unsupervised ones, which use unsupervised algorithms for evidence extraction. In the first class of supervised approaches, a supervised classifier is normally trained to identify correct justification sentences driven by a query (Nie et al., 2019; Tu et al., 2019; Banerjee, 2019). Many systems tend to utilize a multi-task learning setting to learn both answer extraction and justification selection with the same network (Min et al., 2018; Gravina et al., 2018). Although these approaches have achieved impressive performance, they rely on annotated justification sentences, which may not be always available. Few approaches have used distant supervision methods (Lin et al., 2018; Wang et al., 2019b) to create noisy training data for evidence retrieval but these usually underperform due to noisy labels. In the latent approaches for selecting justifica4515 tions, reinforcement learning (Geva and Berant, 2018; Choi et al., 2017) and PageRank (Surdeanu et al., 2008) have been widely used to select justification sentences without expl"
2020.acl-main.414,N19-1270,0,0.0749582,"Missing"
2020.acl-main.414,D19-1258,0,0.165544,"Missing"
2020.acl-main.414,D14-1162,0,0.0835913,"the Association for Computational Linguistics, pages 4514–4525 c July 5 - 10, 2020. 2020 Association for Computational Linguistics AIR also conditionally expands the query using the justifications retrieved in the previous steps. In particular, our key contributions are: (1) We develop a simple, fast, and unsupervised iterative evidence retrieval method, which achieves state-of-the-art results on justification selection on two multi-hop QA datasets: MultiRC (Khashabi et al., 2018a) and QASC (Khot et al., 2019a). Notably, our simple unsupervised approach that relies solely on GloVe embeddings (Pennington et al., 2014) outperforms three transformer-based supervised state-of-the-art methods: BERT (Devlin et al., 2019), XLnet (Yang et al., 2019) and RoBERTa (Liu et al., 2019) on the justification selection task. Further, when the retrieved justifications are fed into a QA component based on RoBERTa (Liu et al., 2019), we obtain the best QA performance on the development sets of both MultiRC and QASC.2 (2) AIR can be trivially extended to capture parallel evidence chains by running multiple instances of AIR in parallel starting from different initial evidence sentences. We show that aggregating multiple parall"
2020.acl-main.414,N19-1403,0,0.0392251,"Missing"
2020.acl-main.414,D19-1261,0,0.159536,"evidence retrieval on both the datasets, which emphasizes the interpretability of AIR. (3) We demonstrate that AIR’s iterative process that focuses on missing information is more robust to semantic drift. We show that even the supervised RoBERTa-based retriever trained to retrieve evidences iteratively, suffers substantial drops in performance with retrieval from consecutive hops. 2 Related Work Our work falls under the revitalized direction that focuses on the interpretability of QA systems, where the machine’s inference process is explained to the end user in natural language evidence text (Qi et al., 2019; Yang et al., 2018; Wang et al., 2019b; Yadav et al., 2019b; Bauer et al., 2018). Several 2 In settings where external labeled resources are not used. Question: Exposure to oxygen and water can cause iron to (A) decrease strength (B) melt (C) uncontrollable burning (D) thermal expansion (E) turn orange on the surface (F) vibrate (G) extremes of temperature (H) levitate Gold justification sentences: 1. when a metal rusts , that metal becomes orange on the surface 2. Iron rusts in the presence of oxygen and water. Parallel evidence chain 1: 1. Dissolved oxygen in water usually causes the oxidat"
2020.acl-main.414,P19-1488,0,0.0303909,"swer pairs to facilitate the identification of latent justifications. strategy that relies on explicit terms from the query and the previously retrieved justification. Lastly, none of the previous iterative retrieval approaches address the problem of semantic drift, whereas AIR accounts for drift by controlling the query reformulation as explained in section 3.1. In unsupervised approaches, many QA systems have relied on structured knowledge base (KB) QA. For example, several previous works have used ConceptNet (Speer et al., 2017) to keep the QA process interpretable (Khashabi et al., 2018b; Sydorova et al., 2019). However, the construction of such structured knowledge bases is expensive, and may need frequent updates. Instead, in this work we focus on justification selection from textual (or unstructured) KBs, which are inexpensive to build and can be applied in several domains. In the same category of unsupervised approaches, conventional information retrieval (IR) methods such as BM25 (Chen et al., 2017) have also been widely used to retrieve independent individual sentences. As shown by (Khot et al., 2019a; Qi et al., 2019), and our table 2, these techniques do not work well for complex multi-hop q"
2020.acl-main.414,N19-1302,0,0.11548,"Missing"
2020.acl-main.414,K19-1065,0,0.384987,"ets, which emphasizes the interpretability of AIR. (3) We demonstrate that AIR’s iterative process that focuses on missing information is more robust to semantic drift. We show that even the supervised RoBERTa-based retriever trained to retrieve evidences iteratively, suffers substantial drops in performance with retrieval from consecutive hops. 2 Related Work Our work falls under the revitalized direction that focuses on the interpretability of QA systems, where the machine’s inference process is explained to the end user in natural language evidence text (Qi et al., 2019; Yang et al., 2018; Wang et al., 2019b; Yadav et al., 2019b; Bauer et al., 2018). Several 2 In settings where external labeled resources are not used. Question: Exposure to oxygen and water can cause iron to (A) decrease strength (B) melt (C) uncontrollable burning (D) thermal expansion (E) turn orange on the surface (F) vibrate (G) extremes of temperature (H) levitate Gold justification sentences: 1. when a metal rusts , that metal becomes orange on the surface 2. Iron rusts in the presence of oxygen and water. Parallel evidence chain 1: 1. Dissolved oxygen in water usually causes the oxidation of iron. 2. When iron combines wit"
2020.acl-main.414,Q18-1021,0,0.0396203,"machine learning (ML) remains a critical unsolved challenge that slows the adoption of ML in real-world applications (Biran and Cotton, 2017; Gilpin et al., 2018; Alvarez-Melis and Jaakkola, 2017; Arras et al., 2017). Question answering (QA) is one of the challenging natural language processing (NLP) tasks that benefits from explainability. In particular, multihop QA requires the aggregation of multiple evidence facts in order to answer complex natural language questions (Yang et al., 2018). Several multi-hop QA datasets have been proposed recently (Yang et al., 2018; Khashabi et al., 2018a; Welbl et al., 2018; Dua et al., 2019; Chen and Durrett, 2019; Khot et al., 2019a; Sun et al., 2019b; Jansen and Ustalov, 2019; Rajpurkar et al., 2018). While several neural methods have achieved state-of-theart results on these datasets (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019), we argue that many of these directions lack a human-understandable explanation of their inference process, which is necessary to transition these approaches into realworld applications. This is especially critical for multi-hop, multiple choice QA (MCQA) where: (a) the answer text may not come from an actual knowledge b"
2020.acl-main.414,N19-1274,1,0.552747,"talov, 2019; Rajpurkar et al., 2018). While several neural methods have achieved state-of-theart results on these datasets (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019), we argue that many of these directions lack a human-understandable explanation of their inference process, which is necessary to transition these approaches into realworld applications. This is especially critical for multi-hop, multiple choice QA (MCQA) where: (a) the answer text may not come from an actual knowledge base passage, and (b) reasoning is required to link the candidate answers to the given question (Yadav et al., 2019b). Figure 1 shows one such multi-hop example from a MCQA dataset. In this paper we introduce a simple alignmentbased iterative retriever (AIR)1 , which retrieves high-quality evidence sentences from unstructured knowledge bases. We demonstrate that these evidence sentences are useful not only to explain the required reasoning steps that answer a question, but they also considerably improve the performance of the QA system itself. Unlike several previous works that depend on supervised methods for the retrieval of justification sentences (deployed mostly in settings that rely on small sets of"
2020.acl-main.414,D19-1260,1,0.7612,"talov, 2019; Rajpurkar et al., 2018). While several neural methods have achieved state-of-theart results on these datasets (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019), we argue that many of these directions lack a human-understandable explanation of their inference process, which is necessary to transition these approaches into realworld applications. This is especially critical for multi-hop, multiple choice QA (MCQA) where: (a) the answer text may not come from an actual knowledge base passage, and (b) reasoning is required to link the candidate answers to the given question (Yadav et al., 2019b). Figure 1 shows one such multi-hop example from a MCQA dataset. In this paper we introduce a simple alignmentbased iterative retriever (AIR)1 , which retrieves high-quality evidence sentences from unstructured knowledge bases. We demonstrate that these evidence sentences are useful not only to explain the required reasoning steps that answer a question, but they also considerably improve the performance of the QA system itself. Unlike several previous works that depend on supervised methods for the retrieval of justification sentences (deployed mostly in settings that rely on small sets of"
2020.acl-main.414,D18-1259,0,0.511364,"assification component, we achieve state-of-the-art QA performance on these two datasets. 1 Introduction Explainability in machine learning (ML) remains a critical unsolved challenge that slows the adoption of ML in real-world applications (Biran and Cotton, 2017; Gilpin et al., 2018; Alvarez-Melis and Jaakkola, 2017; Arras et al., 2017). Question answering (QA) is one of the challenging natural language processing (NLP) tasks that benefits from explainability. In particular, multihop QA requires the aggregation of multiple evidence facts in order to answer complex natural language questions (Yang et al., 2018). Several multi-hop QA datasets have been proposed recently (Yang et al., 2018; Khashabi et al., 2018a; Welbl et al., 2018; Dua et al., 2019; Chen and Durrett, 2019; Khot et al., 2019a; Sun et al., 2019b; Jansen and Ustalov, 2019; Rajpurkar et al., 2018). While several neural methods have achieved state-of-theart results on these datasets (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019), we argue that many of these directions lack a human-understandable explanation of their inference process, which is necessary to transition these approaches into realworld applications. This is espec"
2020.acl-main.414,P08-1082,1,\N,Missing
2020.acl-main.414,D19-5309,0,\N,Missing
2020.acl-srw.23,W16-2920,1,0.891614,"Missing"
2020.acl-srw.23,D15-1166,0,0.0308271,"Access Subset.2 while e(pi ) and char(wi ) are initialized randomly. where W q , W k , W v are learned matrices of dimension 200 × 200, H E contains the biLSTM’s hidden states, and h z is the hidden state of the entity z from C i ) with H E . We concatenate each context vector (C HE the entity vector (H ) and feed the concatenated i vector to two feedforward layers with a softmax function, and use its output to predict if there is a trigger in this position. We calculate the classifier’s loss using the binary log loss function. 3.2 Task 2: Rule Decoder Inspired by neural machine translation (Luong et al., 2015), we use another LSTM with attention as the decoder. To center rule decoding around the trigger, which must be generated first, we first feed the trigger vector from the encoder’s context as the initial state in the decoder. Then, in each timestep t, we generate the attention context vector C D t by using the current hidden state of the decoder, h D t : A D s t (j) = C E j W ht 2 https://www.ncbi.nlm.nih.gov/pmc/ tools/openftlist/ 171 (7) a t = softmax(sst ) CD t = X (8) hE a t (j)h j (9) j where W A is a learned matrix of dimensions 100 × 200, and C E are the context vectors from the previous"
2020.acl-srw.23,N16-3020,0,0.0444955,"d approach on three biomedical events and show that the decoder generates interpretable rules that serve as accurate explanations for the event classifier’s decisions, and, importantly, that the joint training generally improves the performance of the event classifier. Lastly, we show that our approach can be used for semi-supervised learning, and that its performance improves when trained on automatically-labeled data generated by a rulebased system. 1 Introduction Interpretability is a key requirement for machine learning (ML) in many domains, e.g., legal, medical, finance. In the words of (Ribeiro et al., 2016), “if users do not trust the model or a prediction, they will not use it.” However, there is a tension between generalization and interpretability in deep learning, as interpretable models are often generated by “distilling” a model with good generalization, e.g., a deep learning one that relies on distributed representations, into models that are more interpretable but lose some generalization, e.g., linear models or decision trees (Craven and Shavlik, 1996; Ribeiro et al., 2016; Frosst and Hinton, 2017). Here, we argue that both generalization and interpretability are equally important. For"
2020.acl-srw.23,L16-1050,1,0.756229,"Missing"
2020.acl-srw.23,N18-1168,0,0.01772,"n, however, is that a decision tree’s interpretability tends to decay as the tree increases in size. Rather than converting a statistical model into an interpretable model such as a decision tree, other efforts have focused on jointly learning a statistical model with explanations for the model’s output. Our work falls in this camp as well. (Hendricks et al., 2016) proposed a system for image classification that generates a natural language (NL) explanation to accompany each decision. Similarly, (Blunsom et al., 2018) learned NL explanations for the natural language inference (NLI) task, and (Ye et al., 2018) applied this idea to crime case prediction. Inspired by such approaches, here we learn to generate declarative information extraction rules that serve to explain the predictions of an event classifier. (4) Our approach can be easily extended to a semisupervised setting, where we use the rules associated with the events of interest to extract additional training data with “silver” labels, i.e., where we use the rule predictions as training labels for the classifier. We show that despite the inherent noise in this process, the performance of our approach improves considerably in this semi-super"
2020.coling-main.297,Q17-1010,0,0.0465152,"ss is then the average of the losses of the forward and backward LSTMs for all MWEs identified in the training dataset. We investigated two approaches for modeling the context in which the MWE appears at testing time. The first approach applies the learned composition function over the MWE alone during testing. This is necessary as the MWEs in this dataset appear in isolation, without any context. The second approach associates the MWE with context extracted automatically from a large unstructured corpus (Section 3.4). Following Shwartz (2019), for our experiments we used FastText embeddings (Bojanowski et al., 2017), which, overall, performed the best on the Tratz dataset (Tratz, 2011). The embeddings were 3 trained on the English Wikipedia dump from January 2018 to facilitate comparison between this work and that of Shwartz (2019). 3.2 Multi-word Expression Boundaries Our approach can incorporate and use any set of MWE boundaries during training. One direction follows previous work, and uses the predefined MWEs in the provided Tratz dataset. The second trains with no supervised knowledge of the MWE boundaries, but rather for a fully unsupervised approach, we obtain silver MWE boundaries using a single p"
2020.coling-main.297,D09-1049,0,0.0437622,"nsupervised approach, we obtain silver MWE boundaries using a single pattern over part-of-speech tags: (JJ |NN) NN. 4 That is, we require a sequence of either two nouns, or an adjective followed by a noun. This method of obtaining silver MWEs for training allows our approach to work with other domains or languages, 5 provided that there a part-of-speech tagger is available, and that such an extraction pattern is reasonably straightforward to write (e.g., in Spanish an adjective often follows the noun rather than preceding it). Alternatively to the rule-based extraction, the system proposed in Boukobza and Rappoport (2009) for MWE identification may be used, provided that there is an initial set of MWEs available. 3.3 Training Variations While we found empirically that running the mapping function over the whole sentence to produce a context-aware embedding of the MWE performs better overall, we also experimented with a variant that uses only the multi-word as input to an RNN. Formally, using the same notation as above, where we have a sentence S consisting of n words, which contains a MWE of length k, we take the last hidden state of a word-level RNN as the embedding of the entire expression: (ek ) h = RN N (["
2020.coling-main.297,S10-1007,0,0.0725335,"Missing"
2020.coling-main.297,W16-1604,0,0.25753,"en outperforms it in some configurations. 1 Introduction Multi-word expressions (MWEs) are fundamental to language and, as such, having a robust semantic representation for MWEs is important for any natural language processing task that involves text understanding such as information extraction, or question answering (e.g., da Silva and Souza, 2012; Thurmair, 2018; Subramanian et al., 2018). While MWEs have received attention in recent years, leading to considerable progress in learning MWE representations (Mitchell and Lapata, 2010; Butnariu et al., 2010; Tratz, 2011; Hendrickx et al., 2013; Dima, 2016; Shwartz and Dagan, 2018; Shwartz, 2019), we argue that the proposed methods have limitations. First, some methods require concatenating words in specified MWEs, and treating the resulting MWE phrases as atomic units. Training a set of dedicated distributional embeddings for the new multi-word terms (Shwartz and Dagan, 2018; Dima, 2016) suffers from language sparsity. For example, the MWE “red flower” is two orders of magnitude less frequent in Google search results than the noun “flower,” which is likely to affect the quality of its learned MWE representation. These methods additionally have"
2020.coling-main.297,C92-2082,0,0.5871,"Missing"
2020.coling-main.297,S13-2025,0,0.0613323,"Missing"
2020.coling-main.297,P18-1031,0,0.0180903,"is likely to affect the quality of its learned MWE representation. These methods additionally have no straightforward way of handling MWEs that are out of vocabulary. Second, other approaches require supervision for MWE boundaries (Yu and Dredze, 2015), which hinders scalability and portability to different languages. In all situations, the reliance on having determined your entities of interest ahead of time threatens to dramatically reduce the real-world utility of these approaches. Here, we propose a method that addresses both limitations by combining recent advances in language modeling (Howard and Ruder, 2018; Merity et al., 2017) with the simplicity and proven capability of the Skip-Gram training objective (Mikolov et al., 2013a). Our approach is summarized in Figure 1. Intuitively, our method has two components. We use a bidirectional long short-term memory network (biLSTM) (Hochreiter and Schmidhuber, 1997) to encode each MWE. Then, we use this encoding to predict the words in the context of the MWE, similarly to the original Skip-Gram algorithm. Importantly, This work is licensed under a Creative Commons Attribution 4.0 International License. http://creativecommons.org/licenses/by/4.0/. Licens"
2020.coling-main.297,N15-1098,0,0.0581632,"Missing"
2020.coling-main.297,D14-1162,0,0.0837402,"Missing"
2020.coling-main.297,W19-5111,0,0.0634651,"ons. 1 Introduction Multi-word expressions (MWEs) are fundamental to language and, as such, having a robust semantic representation for MWEs is important for any natural language processing task that involves text understanding such as information extraction, or question answering (e.g., da Silva and Souza, 2012; Thurmair, 2018; Subramanian et al., 2018). While MWEs have received attention in recent years, leading to considerable progress in learning MWE representations (Mitchell and Lapata, 2010; Butnariu et al., 2010; Tratz, 2011; Hendrickx et al., 2013; Dima, 2016; Shwartz and Dagan, 2018; Shwartz, 2019), we argue that the proposed methods have limitations. First, some methods require concatenating words in specified MWEs, and treating the resulting MWE phrases as atomic units. Training a set of dedicated distributional embeddings for the new multi-word terms (Shwartz and Dagan, 2018; Dima, 2016) suffers from language sparsity. For example, the MWE “red flower” is two orders of magnitude less frequent in Google search results than the noun “flower,” which is likely to affect the quality of its learned MWE representation. These methods additionally have no straightforward way of handling MWEs"
2020.coling-main.297,P18-1111,0,0.0137812,"ms it in some configurations. 1 Introduction Multi-word expressions (MWEs) are fundamental to language and, as such, having a robust semantic representation for MWEs is important for any natural language processing task that involves text understanding such as information extraction, or question answering (e.g., da Silva and Souza, 2012; Thurmair, 2018; Subramanian et al., 2018). While MWEs have received attention in recent years, leading to considerable progress in learning MWE representations (Mitchell and Lapata, 2010; Butnariu et al., 2010; Tratz, 2011; Hendrickx et al., 2013; Dima, 2016; Shwartz and Dagan, 2018; Shwartz, 2019), we argue that the proposed methods have limitations. First, some methods require concatenating words in specified MWEs, and treating the resulting MWE phrases as atomic units. Training a set of dedicated distributional embeddings for the new multi-word terms (Shwartz and Dagan, 2018; Dima, 2016) suffers from language sparsity. For example, the MWE “red flower” is two orders of magnitude less frequent in Google search results than the noun “flower,” which is likely to affect the quality of its learned MWE representation. These methods additionally have no straightforward way o"
2020.coling-main.297,N18-2035,0,0.0144311,"56.86 y y n y y y y y 43.14 ±2.52 47.5 47.8 37.2 33.85 ±0.18 38.1 42.9 33.4 61.00 ±0.18 66.2 73.6 77.5 60.75 ±0.36 63.9 71.4 72.5 n n n n n n 41.42 ±2.09 43.51 ±2.28 44.38 ±0.46 37.18 ±0.65 36.53 ±0.74 37.66 ±0.78 60.91 ±0.50 59.73 ±0.79 59.57 ±0.78 59.70 ±0.89 57.97 ±0.58 57.85 ±0.49 n n n y y y 47.95 ±0.88 50.40 ±2.88 49.75 ±2.50 36.90 ±0.75 37.82 ±1.41 38.09 ±0.76 62.46 ±0.23 62.38 ±0.20 62.20 ±0.17 60.44 ±0.36 60.76 ±0.64 60.50 ±0.16 n n 53.69 40.89 67.11 64.48 Supervised Baselines FastText Max Pool FastText Average Supervised Minimize L2 (our implementation) Minimize L2 ♦ Shwartz (2019) Shwartz and Waterson (2018) Dima (2016) No supervision MWE-only RNN Context-aware RNN (train only) Context-aware RNN (train/test) Supervised boundaries MWE-only RNN Context-aware RNN (train only) Context-aware RNN (train/test) Transformers BERT base (frozen) Table 2: Weighted F1 scores (Shwartz, 2019) of our proposed approach, various baselines, and other supervised methods, including the state-of-the-art methods of Dima (2016) and Shwartz and Waterson (2018), on the Tratz dataset. We differentiate between methods that need supervision for both distributional embeddings and boundaries of MWEs (supervised), only MWE boun"
2020.coling-main.297,W18-2609,0,0.0303367,"Missing"
2020.coling-main.297,P10-1070,0,0.0885366,"Missing"
2020.coling-main.297,P18-1042,0,0.018498,"ant vector spaces are different. Our proposed approach does not suffer from this observer effect, as we learn our compositional function indirectly (through Skip-Gram), without relying on a distributionally learned embedding for MWEs for training. Shwartz (2019) also avoid this reliance on the gold embedding of the multi-word, learning the function indirectly. The compositional function is used to encode the multiword and its paraphrase and is then trained to maximize the cosine similarity between the encodings. The paraphrases are generated either using backtranslation (Wieting et al., 2017; Wieting and Gimpel, 2018), or by treating frequent joint co-occurrences as paraphrases. However, this approach is outperformed by much cheaper unsupervised approaches, such as the average of the constituents’ emebddings (Shwartz, 2019). Furthermore, the backtranslation approach depends on an external system, adding complexity to the model and restricting the languages and domains of application. Alternatively, a compositional function can be trained directly with a language modeling objective, leveraging the vast amounts of available unlabeled data. Our proposed approach falls into this category, and is similar in nat"
2020.coling-main.297,D17-1026,0,0.0239741,"requencies, the resultant vector spaces are different. Our proposed approach does not suffer from this observer effect, as we learn our compositional function indirectly (through Skip-Gram), without relying on a distributionally learned embedding for MWEs for training. Shwartz (2019) also avoid this reliance on the gold embedding of the multi-word, learning the function indirectly. The compositional function is used to encode the multiword and its paraphrase and is then trained to maximize the cosine similarity between the encodings. The paraphrases are generated either using backtranslation (Wieting et al., 2017; Wieting and Gimpel, 2018), or by treating frequent joint co-occurrences as paraphrases. However, this approach is outperformed by much cheaper unsupervised approaches, such as the average of the constituents’ emebddings (Shwartz, 2019). Furthermore, the backtranslation approach depends on an external system, adding complexity to the model and restricting the languages and domains of application. Alternatively, a compositional function can be trained directly with a language modeling objective, leveraging the vast amounts of available unlabeled data. Our proposed approach falls into this cate"
2020.coling-main.297,Q15-1017,0,0.182899,"ing words in specified MWEs, and treating the resulting MWE phrases as atomic units. Training a set of dedicated distributional embeddings for the new multi-word terms (Shwartz and Dagan, 2018; Dima, 2016) suffers from language sparsity. For example, the MWE “red flower” is two orders of magnitude less frequent in Google search results than the noun “flower,” which is likely to affect the quality of its learned MWE representation. These methods additionally have no straightforward way of handling MWEs that are out of vocabulary. Second, other approaches require supervision for MWE boundaries (Yu and Dredze, 2015), which hinders scalability and portability to different languages. In all situations, the reliance on having determined your entities of interest ahead of time threatens to dramatically reduce the real-world utility of these approaches. Here, we propose a method that addresses both limitations by combining recent advances in language modeling (Howard and Ruder, 2018; Merity et al., 2017) with the simplicity and proven capability of the Skip-Gram training objective (Mikolov et al., 2013a). Our approach is summarized in Figure 1. Intuitively, our method has two components. We use a bidirectiona"
2020.insights-1.10,W09-0106,0,0.0465092,"P, capsule networks are relatively new to the field. Due to their recency, it’s not always clear if or when they are better than other widely used sequence models. This paper investigates the CapsNet architecture in comparison with LSTMs and CNNs. For our analysis, we apply these three architectures to a part of speech (POS) tagging task, on two languages, and using both low- and high-resource scenarios. Much of the focus of NLP research is on resource-rich languages like English. However, the performance of different models can depend on the linguistic properties of the language under study (Bender, 2009) and the amount of training data available. To compare the performance of these architectures under different training conditions, we look at Spanish—another resource-rich language—and Scottish Gaelic—a low-resource language using different amounts of training data. This comparison is a step in the right direction, but it does have the limitations of comparing neural network architectures implemented in different frameworks and only comparing two languages. The main contribution of this paper is comparing the LSTM, CNN, and CapsNet architectures across different training conditions. Our analys"
2020.insights-1.10,L18-1550,0,0.0652373,"Missing"
2020.insights-1.10,N16-1030,0,0.109352,"Missing"
2020.insights-1.10,P19-1150,0,0.0273363,"hted sum of all predictions are considered for classification. Essentially, a CapsNet uses convolution to create first round predictions for objects—primary capsules—and then utilizes routing by agreement to predict the presence of higher level objects—secondary capsules. Many implementations of CapsNets are designed for image recognition (Hinton et al., 2011; Sabour et al., 2017; Hinton et al., 2018). However, the CapsNet architecture is being applied more and more to NLP tasks, including Chinese word segmentation (Li et al., 2018), and multi-label text classification and question answering (Zhao et al., 2019). This paper continues this path by investigating how CapsNets compare to other neural network architectures for the task of part of speech tagging. 3 Partition Sentences Tokens Avg. Sent. Length train100 train50 train10 train1 dev test 14,305 7,152 1,430 143 1,654 1,721 446,144 255,213 43,480 5,912 52,511 52,801 31.2 35.7 30.4 41.3 31.7 30.7 Table 1: Number of sentences, tokens, and average sentence length for each partition of Spanish. The n in the train partitions corresponds to the amount (percent) of the original data used for training. Partition Sentences Tokens Avg. Sent. Length train10"
2020.insights-1.12,P19-1615,0,0.015244,"ction through a thermal conductor” yields “heat travels through steel”. And, finally, “heat travels through steel” supports the correct explanation that “a steel spoon in a cafeteria would let the most heat travel through.” Generating such reasoning chains can be crucial for the adoption of natural language processing applications such as QA in critical domains such as medical or law. The emergence of large pretrained language models (LM) (Devlin et al., 2019; Liu et al., 2019) yielded significant progress in question answering (QA), including complex QA tasks that require multihop reasoning (Banerjee et al., 2019; Asai et al., 2019; Yadav et al., 2019). Most of these stateof-the-art (SOTA) approaches address multi-hop reasoning tasks in a discriminative manner: they take the question, the candidate answer, and all the context available as the input, and produce a single score indicating the likelihood of the answer as justified by the provided context (an example is shown in Figure 1). However, why that context actually justifies the answer remains unclear to the human end user of the QA system. In contrast, most of us are likely to answer the question in Figure 1 by building a reasoning chain from th"
2020.insights-1.12,N19-1423,0,0.0391701,"dicating the probability of the candidate answer being justified by all of the inputs. But why the facts explain the answer is normally not covered. Introduction through a thermal conductor” yields “heat travels through steel”. And, finally, “heat travels through steel” supports the correct explanation that “a steel spoon in a cafeteria would let the most heat travel through.” Generating such reasoning chains can be crucial for the adoption of natural language processing applications such as QA in critical domains such as medical or law. The emergence of large pretrained language models (LM) (Devlin et al., 2019; Liu et al., 2019) yielded significant progress in question answering (QA), including complex QA tasks that require multihop reasoning (Banerjee et al., 2019; Asai et al., 2019; Yadav et al., 2019). Most of these stateof-the-art (SOTA) approaches address multi-hop reasoning tasks in a discriminative manner: they take the question, the candidate answer, and all the context available as the input, and produce a single score indicating the likelihood of the answer as justified by the provided context (an example is shown in Figure 1). However, why that context actually justifies the answer remai"
2020.insights-1.12,2021.ccl-1.108,0,0.0755446,"Missing"
2020.insights-1.12,D18-1260,0,0.0280202,"ference hops necessary to explain a candidate answer explicitly. In this work, we investigate the capability of a state-of-the-art transformer LM to generate explicit inference hops, i.e., to infer a new statement necessary to answer a question given some premise input statements. Our analysis shows that such LMs can generate new statements for some simple inference types, but performance remains poor for complex, real-world inference types such as those that require monotonicity, composition, and commonsense knowledge. 1 Figure 1: An example of question and candidate answers from OpenbookQA (Mihaylov et al., 2018) (the correct answer is option B). The science fact and the commonsense knowledge facts are needed to explain the correct answer. Usually the large LMs solve this problem by taking the question, the science fact, the common knowledge facts and each candidate answer as the input and producing a single score indicating the probability of the candidate answer being justified by all of the inputs. But why the facts explain the answer is normally not covered. Introduction through a thermal conductor” yields “heat travels through steel”. And, finally, “heat travels through steel” supports the correc"
2020.insights-1.12,P17-1099,0,0.125739,"Missing"
2020.insights-1.12,D19-1458,0,0.0273768,"out other restrictions (Khot et al., 2020). 2 The task we investigate here is whether transformer-based LMs can infer the combined fact when provided with the two initial facts. Table 1: Statistics of the quality of the generated T5 statements on the dev set of QASC. The same randomly sampled 87 examples are manually evaluated for their quality, in both the “without hint” and “with hint” configurations. 2 Approach Related Work Recently several works have investigated whether deep learning (DL) language models (LM) are able to learn and use the explicit and implicit rules in natural language. (Sinha et al., 2019) build a synthetic dataset containing the relationships between people; their language model needs to predict the unstated relationships between people. The problem can be summarized as: given that “Mike is the child of Kate and Kate is the child of Tom”, the model needs to predict “Tom is the grandparent of Mike”, by learning the implicit rule: “If X is the child of Y and Y is the child of Z, then Z is the grandparent of X”. It has been shown that the transformer networks perform well on this task. Other works have analyzed whether DL language models are able to leverage explicit rules. (Clar"
2020.insights-1.12,D19-1260,1,0.822684,"“heat travels through steel”. And, finally, “heat travels through steel” supports the correct explanation that “a steel spoon in a cafeteria would let the most heat travel through.” Generating such reasoning chains can be crucial for the adoption of natural language processing applications such as QA in critical domains such as medical or law. The emergence of large pretrained language models (LM) (Devlin et al., 2019; Liu et al., 2019) yielded significant progress in question answering (QA), including complex QA tasks that require multihop reasoning (Banerjee et al., 2019; Asai et al., 2019; Yadav et al., 2019). Most of these stateof-the-art (SOTA) approaches address multi-hop reasoning tasks in a discriminative manner: they take the question, the candidate answer, and all the context available as the input, and produce a single score indicating the likelihood of the answer as justified by the provided context (an example is shown in Figure 1). However, why that context actually justifies the answer remains unclear to the human end user of the QA system. In contrast, most of us are likely to answer the question in Figure 1 by building a reasoning chain from the given facts. For example, such a possi"
2020.lrec-1.643,D15-1159,0,0.042181,"Missing"
2020.lrec-1.643,P16-1231,0,0.0365759,"Missing"
2020.lrec-1.643,D16-1211,0,0.0281735,"Missing"
2020.lrec-1.643,W06-2920,0,0.599148,"Missing"
2020.lrec-1.643,D14-1082,0,0.239643,"y for this task. 2. Related Work Early algorithms for automatic dependency parsing such as the dynamic programming approach proposed by Eisner (1996) had a complexity of O(n3 ). Currently, the two primary approaches, i.e., the graph-based maximum spanning tree approach proposed by McDonald et al. (2005) and the transition based approach formalized by Nivre (2003), have complexities of O(n2 ) and O(n), respectively. Many variants of these lower-complexity transition-based approaches have been proposed. Yamada and Matsumoto (2003) trained a support vector machine to direct a shiftreduce parser. Chen and Manning (2014) encode features as embeddings and feed them to a multi-layer perceptron. 5225 Kiperwasser and Goldberg (2016) use a BiLSTM to learn feature representations, and use these to encode the parser state. Xipeng (2009) proposed a simpler method, where the dependency parsing task is transformed into a sequence labeling problem using conditional random fields. More recently, Ma et al. (2018) proposed a stack-pointer network, which uses information from the sentence as a whole. Fern´andez-Gonz´alez and G´omez-Rodr´ıguez (2019) introduced a left-to-right parsing approach with a pointer network, reducin"
2020.lrec-1.643,D16-1238,0,0.0306694,"Missing"
2020.lrec-1.643,W08-1301,0,0.178275,"Missing"
2020.lrec-1.643,N19-1423,0,0.0347603,"Missing"
2020.lrec-1.643,P15-1033,0,0.0576879,"Missing"
2020.lrec-1.643,C96-1058,0,0.0456816,"es that the BERT contextualized embeddings and the word-level BiLSTM have considerable contributions to performance, confirming earlier work that indicated the importance of contextualized embeddings for many NLP tasks (Devlin et al., 2018), and that LSTMs capture grammatical structure (Linzen et al., 2016; Kuncoro et al., 2018). As removing either of these components impacts performance negatively, this suggests that we are approaching the limits of simplicity for this task. 2. Related Work Early algorithms for automatic dependency parsing such as the dynamic programming approach proposed by Eisner (1996) had a complexity of O(n3 ). Currently, the two primary approaches, i.e., the graph-based maximum spanning tree approach proposed by McDonald et al. (2005) and the transition based approach formalized by Nivre (2003), have complexities of O(n2 ) and O(n), respectively. Many variants of these lower-complexity transition-based approaches have been proposed. Yamada and Matsumoto (2003) trained a support vector machine to direct a shiftreduce parser. Chen and Manning (2014) encode features as embeddings and feed them to a multi-layer perceptron. 5225 Kiperwasser and Goldberg (2016) use a BiLSTM to"
2020.lrec-1.643,N18-2109,0,0.0234834,"Missing"
2020.lrec-1.643,N19-1076,0,0.057993,"Missing"
2020.lrec-1.643,N19-1419,0,0.0298379,"− 1 to n. Our approach continues this simplification trend with a method that reduces parsing to a simple sequence modeling task, similar with the approaches of (Strzyz et al., 2019) and (Li et al., 2018), albeit with a much simpler architecture and better overall performance. Li et al. (2018) propose an encoder-decoder architecture with an attention layer; Strzyz et al. (2019) propose an encoder-decoder architecture with a more complex encoding scheme. We propose a much simpler architecture, consisting of only a BiLSTM operating on top of contextualized embeddings. Similar to our direction, Hewitt and Manning (2019) showed that syntax trees are embedded in a linear transformation of the BERT and ELMo embeddings. However, they did not consider labels and the evaluation was done only on undirected unlabeled attachment score (UUAS) after generating a minimum spanning tree on the predicted distance graph. In the pool of exciting methods that simplify the parsing task, our approach is closer in spirit to the approach of Zhang et al. (2017) for dependency parsing, and Marcheggiani et al. (2017) for semantic role labeling. Both these works also encode the tokens in a sentence using an LSTM. However, they operat"
2020.lrec-1.643,Q16-1023,0,0.176957,"programming approach proposed by Eisner (1996) had a complexity of O(n3 ). Currently, the two primary approaches, i.e., the graph-based maximum spanning tree approach proposed by McDonald et al. (2005) and the transition based approach formalized by Nivre (2003), have complexities of O(n2 ) and O(n), respectively. Many variants of these lower-complexity transition-based approaches have been proposed. Yamada and Matsumoto (2003) trained a support vector machine to direct a shiftreduce parser. Chen and Manning (2014) encode features as embeddings and feed them to a multi-layer perceptron. 5225 Kiperwasser and Goldberg (2016) use a BiLSTM to learn feature representations, and use these to encode the parser state. Xipeng (2009) proposed a simpler method, where the dependency parsing task is transformed into a sequence labeling problem using conditional random fields. More recently, Ma et al. (2018) proposed a stack-pointer network, which uses information from the sentence as a whole. Fern´andez-Gonz´alez and G´omez-Rodr´ıguez (2019) introduced a left-to-right parsing approach with a pointer network, reducing the number of transitions required by Ma et al. (2018) from 2n − 1 to n. Our approach continues this simplif"
2020.lrec-1.643,P18-1132,0,0.0310074,"Missing"
2020.lrec-1.643,N16-1030,0,0.0150985,"set. We use the same range for all languages. 3.1. Token Representations Given an input sentence s, consisting of n tokens t1 , . . . , tn , we represent each token ti by a vector ei , which is the concatenation of (a) the pretrained BERT representation2 (Devlin et al., 2018) of the token; (b) the word embedding (we) for the token; (c) the character CNN encoding (ce) of the token; and (d) its part-of-speech (POS) embedding (pos): h i (bert) (we) (ce) (pos) ei = ti · ti · ti · ti (2) For the character-level encoding, we use a character CNN with max pooling, which has been shown to be useful by Lample et al. (2016). We learned the embeddings for part-of-speech tags and words, which were initialized using Xavier initialization (Glorot and Bengio, 2010). 3.2. Architecture Our proposed architecture, as shown in Figure 1, consists of a BiLSTM (Hochreiter and Schmidhuber, 1997) that operates over our token representations ei , producing the corresponding hidden state hi . For each token ti we use hi in two ways: to predict the relative position of the head, and to predict the label of the corresponding dependency relation. For the relative position, we pass hi into a multi-layer perceptron (MLP), followed by"
2020.lrec-1.643,C18-1271,0,0.243937,"pendency parsing task is transformed into a sequence labeling problem using conditional random fields. More recently, Ma et al. (2018) proposed a stack-pointer network, which uses information from the sentence as a whole. Fern´andez-Gonz´alez and G´omez-Rodr´ıguez (2019) introduced a left-to-right parsing approach with a pointer network, reducing the number of transitions required by Ma et al. (2018) from 2n − 1 to n. Our approach continues this simplification trend with a method that reduces parsing to a simple sequence modeling task, similar with the approaches of (Strzyz et al., 2019) and (Li et al., 2018), albeit with a much simpler architecture and better overall performance. Li et al. (2018) propose an encoder-decoder architecture with an attention layer; Strzyz et al. (2019) propose an encoder-decoder architecture with a more complex encoding scheme. We propose a much simpler architecture, consisting of only a BiLSTM operating on top of contextualized embeddings. Similar to our direction, Hewitt and Manning (2019) showed that syntax trees are embedded in a linear transformation of the BERT and ELMo embeddings. However, they did not consider labels and the evaluation was done only on undirec"
2020.lrec-1.643,Q16-1037,0,0.0510207,"Missing"
2020.lrec-1.643,P18-1130,0,0.113762,"and O(n), respectively. Many variants of these lower-complexity transition-based approaches have been proposed. Yamada and Matsumoto (2003) trained a support vector machine to direct a shiftreduce parser. Chen and Manning (2014) encode features as embeddings and feed them to a multi-layer perceptron. 5225 Kiperwasser and Goldberg (2016) use a BiLSTM to learn feature representations, and use these to encode the parser state. Xipeng (2009) proposed a simpler method, where the dependency parsing task is transformed into a sequence labeling problem using conditional random fields. More recently, Ma et al. (2018) proposed a stack-pointer network, which uses information from the sentence as a whole. Fern´andez-Gonz´alez and G´omez-Rodr´ıguez (2019) introduced a left-to-right parsing approach with a pointer network, reducing the number of transitions required by Ma et al. (2018) from 2n − 1 to n. Our approach continues this simplification trend with a method that reduces parsing to a simple sequence modeling task, similar with the approaches of (Strzyz et al., 2019) and (Li et al., 2018), albeit with a much simpler architecture and better overall performance. Li et al. (2018) propose an encoder-decoder"
2020.lrec-1.643,K17-1041,0,0.0182696,"ler architecture, consisting of only a BiLSTM operating on top of contextualized embeddings. Similar to our direction, Hewitt and Manning (2019) showed that syntax trees are embedded in a linear transformation of the BERT and ELMo embeddings. However, they did not consider labels and the evaluation was done only on undirected unlabeled attachment score (UUAS) after generating a minimum spanning tree on the predicted distance graph. In the pool of exciting methods that simplify the parsing task, our approach is closer in spirit to the approach of Zhang et al. (2017) for dependency parsing, and Marcheggiani et al. (2017) for semantic role labeling. Both these works also encode the tokens in a sentence using an LSTM. However, they operate over pairs of words to predict a syntactic dependency between a modifier and head (Zhang et al., 2017), or a token’s semantic role given a predicate (Marcheggiani et al., 2017). Further, (Marcheggiani et al., 2017) separately encode the sentence with an LSTM for each predicate. Our approach is simpler and faster, i.e., we encode the sentence just once with the LSTM, and we predict the relative position of head words rather than relying on all possible pairs of words. 3. PaT:"
2020.lrec-1.643,J93-2004,0,0.0771662,"cted, we concatenate the MLP output for the dependent and the predicted head, and pass it to another linear layer to predict the corresponding dependency label. probability which does not add a cycle, until all tokens are covered. (3) Optimal cycle removal, which finds a maximum spanning tree (MST) using the Chu-Liu-Edmonds algorithm (Edmonds, 1967). 4. Experiments and Results We used the same datasets as Fern´andez-Gonz´alez and G´omez-Rodr´ıguez (2019) and Ma et al. (2018). We tested our model on the Stanford Dependencies (de Marneffe and Manning, 2008) (SD) conversion of the Penn Treebank (Marcus et al., 1993) and on 15 languages from the Universal Dependencies (UD) Treebank. For SD we used the standard splits and the predicted part-of-speech tags. For UD we used the same set up as Fern´andez-Gonz´alez and G´omez-Rodr´ıguez (2019),3 with the addition of Arabic, Estonian, and Japanese. Table 1 lists the unlabeled (UAS) and labeled (LAS) accuracies of our model (averaged over 3 runs) for 15 languages from the UD Treebank, compared against the state-of-theart method of Fern´andez-Gonz´alez and G´omez-Rodr´ıguez (2019). The hyper parameters were tuned on English UD, 3 In particular, we also used versio"
2020.lrec-1.643,H05-1066,0,0.300483,"indicated the importance of contextualized embeddings for many NLP tasks (Devlin et al., 2018), and that LSTMs capture grammatical structure (Linzen et al., 2016; Kuncoro et al., 2018). As removing either of these components impacts performance negatively, this suggests that we are approaching the limits of simplicity for this task. 2. Related Work Early algorithms for automatic dependency parsing such as the dynamic programming approach proposed by Eisner (1996) had a complexity of O(n3 ). Currently, the two primary approaches, i.e., the graph-based maximum spanning tree approach proposed by McDonald et al. (2005) and the transition based approach formalized by Nivre (2003), have complexities of O(n2 ) and O(n), respectively. Many variants of these lower-complexity transition-based approaches have been proposed. Yamada and Matsumoto (2003) trained a support vector machine to direct a shiftreduce parser. Chen and Manning (2014) encode features as embeddings and feed them to a multi-layer perceptron. 5225 Kiperwasser and Goldberg (2016) use a BiLSTM to learn feature representations, and use these to encode the parser state. Xipeng (2009) proposed a simpler method, where the dependency parsing task is tra"
2020.lrec-1.643,W03-3017,0,0.376335,"s (Devlin et al., 2018), and that LSTMs capture grammatical structure (Linzen et al., 2016; Kuncoro et al., 2018). As removing either of these components impacts performance negatively, this suggests that we are approaching the limits of simplicity for this task. 2. Related Work Early algorithms for automatic dependency parsing such as the dynamic programming approach proposed by Eisner (1996) had a complexity of O(n3 ). Currently, the two primary approaches, i.e., the graph-based maximum spanning tree approach proposed by McDonald et al. (2005) and the transition based approach formalized by Nivre (2003), have complexities of O(n2 ) and O(n), respectively. Many variants of these lower-complexity transition-based approaches have been proposed. Yamada and Matsumoto (2003) trained a support vector machine to direct a shiftreduce parser. Chen and Manning (2014) encode features as embeddings and feed them to a multi-layer perceptron. 5225 Kiperwasser and Goldberg (2016) use a BiLSTM to learn feature representations, and use these to encode the parser state. Xipeng (2009) proposed a simpler method, where the dependency parsing task is transformed into a sequence labeling problem using conditional r"
2020.lrec-1.643,P17-2018,0,0.213183,"Missing"
2020.lrec-1.643,N19-1077,0,0.172651,"mpler method, where the dependency parsing task is transformed into a sequence labeling problem using conditional random fields. More recently, Ma et al. (2018) proposed a stack-pointer network, which uses information from the sentence as a whole. Fern´andez-Gonz´alez and G´omez-Rodr´ıguez (2019) introduced a left-to-right parsing approach with a pointer network, reducing the number of transitions required by Ma et al. (2018) from 2n − 1 to n. Our approach continues this simplification trend with a method that reduces parsing to a simple sequence modeling task, similar with the approaches of (Strzyz et al., 2019) and (Li et al., 2018), albeit with a much simpler architecture and better overall performance. Li et al. (2018) propose an encoder-decoder architecture with an attention layer; Strzyz et al. (2019) propose an encoder-decoder architecture with a more complex encoding scheme. We propose a much simpler architecture, consisting of only a BiLSTM operating on top of contextualized embeddings. Similar to our direction, Hewitt and Manning (2019) showed that syntax trees are embedded in a linear transformation of the BERT and ELMo embeddings. However, they did not consider labels and the evaluation wa"
2020.lrec-1.643,P16-1218,0,0.0267983,"Missing"
2020.lrec-1.643,P15-1032,0,0.0257199,"Missing"
2020.lrec-1.643,W03-3023,0,0.448358,"impacts performance negatively, this suggests that we are approaching the limits of simplicity for this task. 2. Related Work Early algorithms for automatic dependency parsing such as the dynamic programming approach proposed by Eisner (1996) had a complexity of O(n3 ). Currently, the two primary approaches, i.e., the graph-based maximum spanning tree approach proposed by McDonald et al. (2005) and the transition based approach formalized by Nivre (2003), have complexities of O(n2 ) and O(n), respectively. Many variants of these lower-complexity transition-based approaches have been proposed. Yamada and Matsumoto (2003) trained a support vector machine to direct a shiftreduce parser. Chen and Manning (2014) encode features as embeddings and feed them to a multi-layer perceptron. 5225 Kiperwasser and Goldberg (2016) use a BiLSTM to learn feature representations, and use these to encode the parser state. Xipeng (2009) proposed a simpler method, where the dependency parsing task is transformed into a sequence labeling problem using conditional random fields. More recently, Ma et al. (2018) proposed a stack-pointer network, which uses information from the sentence as a whole. Fern´andez-Gonz´alez and G´omez-Rodr"
2020.lrec-1.643,E17-1063,0,0.0473469,"complex encoding scheme. We propose a much simpler architecture, consisting of only a BiLSTM operating on top of contextualized embeddings. Similar to our direction, Hewitt and Manning (2019) showed that syntax trees are embedded in a linear transformation of the BERT and ELMo embeddings. However, they did not consider labels and the evaluation was done only on undirected unlabeled attachment score (UUAS) after generating a minimum spanning tree on the predicted distance graph. In the pool of exciting methods that simplify the parsing task, our approach is closer in spirit to the approach of Zhang et al. (2017) for dependency parsing, and Marcheggiani et al. (2017) for semantic role labeling. Both these works also encode the tokens in a sentence using an LSTM. However, they operate over pairs of words to predict a syntactic dependency between a modifier and head (Zhang et al., 2017), or a token’s semantic role given a predicate (Marcheggiani et al., 2017). Further, (Marcheggiani et al., 2017) separately encode the sentence with an LSTM for each predicate. Our approach is simpler and faster, i.e., we encode the sentence just once with the LSTM, and we predict the relative position of head words rathe"
2020.lrec-1.850,H05-1079,0,0.124776,"calized versions of three common datasets used in natural language inference. These datasets are delexicalized using two methods: one which replaces the lexical entities in an overlap-aware manner, and a second, which additionally incorporates semantic lifting of nouns and verbs to their WordNet hypernym synsets. 1. Introduction The task of natural language inference (NLI) is considered to be an integral part of natural language understanding (NLU). In this task, which can be seen as a particular instance of recognizing textual entailment (RTE) (Fyodorov et al., 2000; Condoravdi et al., 2003; Bos and Markert, 2005; MacCartney and Manning, 2009), a model is asked to classify if a given sentence (premise) entails, contradicts or is neutral given a second sentence (hypothesis). In order to advance any task in natural language processing (NLP), quality data sets are quintessential. Some such data sets which have enabled the advancement of NLI (and fact verification) are SNLI (Bowman et al., 2015) MNLI (Williams et al., 2017), FEVER (Thorne et al., 2018), and FNC (Pomerleau and Rao, 2017). However these datasets are not devoid of biases (subtle statistical patterns in a dataset, which could have been introd"
2020.lrec-1.850,D15-1075,0,0.0491912,"ed to be an integral part of natural language understanding (NLU). In this task, which can be seen as a particular instance of recognizing textual entailment (RTE) (Fyodorov et al., 2000; Condoravdi et al., 2003; Bos and Markert, 2005; MacCartney and Manning, 2009), a model is asked to classify if a given sentence (premise) entails, contradicts or is neutral given a second sentence (hypothesis). In order to advance any task in natural language processing (NLP), quality data sets are quintessential. Some such data sets which have enabled the advancement of NLI (and fact verification) are SNLI (Bowman et al., 2015) MNLI (Williams et al., 2017), FEVER (Thorne et al., 2018), and FNC (Pomerleau and Rao, 2017). However these datasets are not devoid of biases (subtle statistical patterns in a dataset, which could have been introduced either due to the methodology of data collection or due to an inherent social bias). For example, (Gururangan et al., 2018) and Poliak et al. (2018) show that biases were introduced into the MNLI dataset by certain language creation choices made by the crowd workers. Similarly, Schuster et al. (2019) show that in the FEVER, the REFUTES label (same as the contradicts label mentio"
2020.lrec-1.850,W03-1022,0,0.295859,"mmercial service. OA-NER+SS Tags With organization-c1, the artifact-c1 motion-c1 commercial act-c1 . Evidence The A380 made its first flight on 27 April 2005 and entered commercial service on 25 October 2007 with Singapore Airlines. The A380 made its ordinal-e1 flight on date-e1 and entered commercial service on date-e2 with organization-c1. The A380 stative its ordinal-e1 cognition-e1 on date-e1 and motion-c1 commercial act-c1 on date-e4 with organization-c1. Table 1: Example illustrating our masking techniques, compared to the original fully lexicalized data. synsets using Super Sense tags (Ciaramita and Johnson, 2003; Miller et al., 1990). (2) We analyze and examine the effect of such delexicalization techniques on several state of the art methods in NLI and confirm that these methods can still achieve comparative performance in domain. We also show that in an outof-domain set up, the model trained on delexicalized data outperforms that of the state of the art model trained on lexicalized data. This empirically supports our hypothesis that delexicalization is a necessary process for meaningful machine learning. 2. Masking Techniques In Suntwal et al. (2019) the authors explore multiple methods for delexic"
2020.lrec-1.850,W03-0906,0,0.264097,"Missing"
2020.lrec-1.850,C96-1079,0,0.0410595,"depend upon. In particular, the contributions of our work are: (1) To motivate further research in using delexicalized datasets, we present the delexicalized versions of several benchmark datasets used in NLI (e.g., FEVER, Fake News Challenge, SNLI, and MNLI), along with the corresponding software for the delexicalization. These datasets have been delexicalized using several strategies which are based on human intuition and underlying linguistic semantics. For example, in one of these techniques, lexical tokens are replaced or masked with indicators corresponding to their named entity class (Grishman and Sundheim, 1996). Our work differs from early works on delexicalization (Zeman and Resnik, 2008) in that we earmark overlapping entities (between the hypothesis and premise) with a unique id. Further we also explore semantic lifting to WordNet 6883 Config. Lexicalized Claim With Singapore Airlines, the Airbus A380 entered commercial service. OA-NER With organization-c1, the misc-c1 entered commercial service. OA-NER+SS Tags With organization-c1, the artifact-c1 motion-c1 commercial act-c1 . Evidence The A380 made its first flight on 27 April 2005 and entered commercial service on 25 October 2007 with Singapor"
2020.lrec-1.850,N18-2017,0,0.0518093,"tradicts or is neutral given a second sentence (hypothesis). In order to advance any task in natural language processing (NLP), quality data sets are quintessential. Some such data sets which have enabled the advancement of NLI (and fact verification) are SNLI (Bowman et al., 2015) MNLI (Williams et al., 2017), FEVER (Thorne et al., 2018), and FNC (Pomerleau and Rao, 2017). However these datasets are not devoid of biases (subtle statistical patterns in a dataset, which could have been introduced either due to the methodology of data collection or due to an inherent social bias). For example, (Gururangan et al., 2018) and Poliak et al. (2018) show that biases were introduced into the MNLI dataset by certain language creation choices made by the crowd workers. Similarly, Schuster et al. (2019) show that in the FEVER, the REFUTES label (same as the contradicts label mentioned above) highly correlates with the presence of negation phrases. These biases can be readily exploited by neural networks (NNs), and thus have influence on performance. As an example, Gururangan et al. (2018) demonstrate that many state of the art methods in NLI could still achieve reasonable accuracies when trained with the hypothesis a"
2020.lrec-1.850,W09-3714,0,0.0414309,"ee common datasets used in natural language inference. These datasets are delexicalized using two methods: one which replaces the lexical entities in an overlap-aware manner, and a second, which additionally incorporates semantic lifting of nouns and verbs to their WordNet hypernym synsets. 1. Introduction The task of natural language inference (NLI) is considered to be an integral part of natural language understanding (NLU). In this task, which can be seen as a particular instance of recognizing textual entailment (RTE) (Fyodorov et al., 2000; Condoravdi et al., 2003; Bos and Markert, 2005; MacCartney and Manning, 2009), a model is asked to classify if a given sentence (premise) entails, contradicts or is neutral given a second sentence (hypothesis). In order to advance any task in natural language processing (NLP), quality data sets are quintessential. Some such data sets which have enabled the advancement of NLI (and fact verification) are SNLI (Bowman et al., 2015) MNLI (Williams et al., 2017), FEVER (Thorne et al., 2018), and FNC (Pomerleau and Rao, 2017). However these datasets are not devoid of biases (subtle statistical patterns in a dataset, which could have been introduced either due to the methodol"
2020.lrec-1.850,P14-5010,1,0.0134387,"of the state of the art model trained on lexicalized data. This empirically supports our hypothesis that delexicalization is a necessary process for meaningful machine learning. 2. Masking Techniques In Suntwal et al. (2019) the authors explore multiple methods for delexicalization. In this work we choose the two of their highest performing masking methods. 2.1. Overlap-Aware Named Entity Recognition In our overlap-aware named entity recognition (OA-NER) technique, the tokens in a given dataset are first tagged as named or numeric entities (NEs) by the named entity recognizer (NER) of CoreNLP(Manning et al., 2014). Next, to capture the entity overlap between premise and hypothesis sentences, we uniquely enumerate the named entities. Specifically, in the claim (c) the first instance of an entity is tagged with c1. Subsequently wherever, in claim or evidence, this same entity is found next, it is replaced with this unique tag. In contrast, if an entity exists only in evidence, it is marked with an e tag. For example person-c1 denotes the first time the proper name is found in claim, while location-e3 indicates the third location found in evidence. An example of this is shown in Table 1. 2.2. OA-NER + Sup"
2020.lrec-1.850,D16-1244,0,0.132313,"Missing"
2020.lrec-1.850,P19-1101,0,0.014855,"lt to accurately judge the degree to which these methods actually extract reasonable representations, correlate with human intuition or understand the underlying semantics (Dagan et al., 2013). In this work we postulate that altering these datasets based on lexical importance is beneficial for organizing research and guiding future empirical work. While the technique of delexicalization (or masking) has been used before (Zeman and Resnik, 2008), we have expanded it by incorporating semantic information (the assumption that meaning arises from a set of independent and discrete semantic units) (Peyrard, 2019). Since these techniques are general and compatible with most existing semantic representations, we believe they can be further extended onto datasets used for other NLP tasks. Thus, by enabling integration of these techniques into the training pipeline, we hope to control lexicalization in the datasets which the NN methods possibly depend upon. In particular, the contributions of our work are: (1) To motivate further research in using delexicalized datasets, we present the delexicalized versions of several benchmark datasets used in NLI (e.g., FEVER, Fake News Challenge, SNLI, and MNLI), alon"
2020.lrec-1.850,S18-2023,0,0.0927427,"Missing"
2020.lrec-1.850,D18-1187,0,0.0180678,"ner as with OA-NER (see Table 1). This technique, specifically, will be used to explore the impact of semantic lifting (i.e., if a more coarse-grained type is possibly less domain dependent) in both the in-domain and out-ofdomain settings. 3. Datasets and Methods For analyzing the effects of various delexicalization operations we chose four popular datasets in NLI: Multi-genre NLI (MNLI) dataset (Williams et al., 2017), Fact Extraction and Verification (FEVER) dataset (Thorne et al., 2018), the Fake News Challenge (FNC ) dataset (Pomerleau and Rao, 2017), and the Medical NLI (MedNLI) dataset (Romanov and Shivade, 2018). In MNLI the trained models were tested on both of the validation partitions, the matched partition (which serves as the in-domain partition) and mismatched (the out-of-domain) partition. We ran these experiments using two high-performing NLI methods: the Decomposable Attention (DA) (Parikh et al., 2016) and the Enhanced Sequential Inference (ESIM) (Chen et al., 2016). All the methods were re-trained out of the box (without any parameter tuning) and tested on the corresponding evaluation partitions of the dataset. 4. Results and Discussion Table 2 shows the performance of each of these method"
2020.lrec-1.850,D19-1341,0,0.16356,"sets which have enabled the advancement of NLI (and fact verification) are SNLI (Bowman et al., 2015) MNLI (Williams et al., 2017), FEVER (Thorne et al., 2018), and FNC (Pomerleau and Rao, 2017). However these datasets are not devoid of biases (subtle statistical patterns in a dataset, which could have been introduced either due to the methodology of data collection or due to an inherent social bias). For example, (Gururangan et al., 2018) and Poliak et al. (2018) show that biases were introduced into the MNLI dataset by certain language creation choices made by the crowd workers. Similarly, Schuster et al. (2019) show that in the FEVER, the REFUTES label (same as the contradicts label mentioned above) highly correlates with the presence of negation phrases. These biases can be readily exploited by neural networks (NNs), and thus have influence on performance. As an example, Gururangan et al. (2018) demonstrate that many state of the art methods in NLI could still achieve reasonable accuracies when trained with the hypothesis alone. Similarly, Suntwal et al. (2019) show that some NN methods in NLI with very high performance accuracies are heavily dependent on lexical information. Further (Yadav et al.,"
2020.lrec-1.850,D19-1340,1,0.839866,"Missing"
2020.lrec-1.850,N18-1074,0,0.0874455,"g (NLU). In this task, which can be seen as a particular instance of recognizing textual entailment (RTE) (Fyodorov et al., 2000; Condoravdi et al., 2003; Bos and Markert, 2005; MacCartney and Manning, 2009), a model is asked to classify if a given sentence (premise) entails, contradicts or is neutral given a second sentence (hypothesis). In order to advance any task in natural language processing (NLP), quality data sets are quintessential. Some such data sets which have enabled the advancement of NLI (and fact verification) are SNLI (Bowman et al., 2015) MNLI (Williams et al., 2017), FEVER (Thorne et al., 2018), and FNC (Pomerleau and Rao, 2017). However these datasets are not devoid of biases (subtle statistical patterns in a dataset, which could have been introduced either due to the methodology of data collection or due to an inherent social bias). For example, (Gururangan et al., 2018) and Poliak et al. (2018) show that biases were introduced into the MNLI dataset by certain language creation choices made by the crowd workers. Similarly, Schuster et al. (2019) show that in the FEVER, the REFUTES label (same as the contradicts label mentioned above) highly correlates with the presence of negation"
2020.lrec-1.850,N19-1274,1,0.82913,"et al. (2019) show that in the FEVER, the REFUTES label (same as the contradicts label mentioned above) highly correlates with the presence of negation phrases. These biases can be readily exploited by neural networks (NNs), and thus have influence on performance. As an example, Gururangan et al. (2018) demonstrate that many state of the art methods in NLI could still achieve reasonable accuracies when trained with the hypothesis alone. Similarly, Suntwal et al. (2019) show that some NN methods in NLI with very high performance accuracies are heavily dependent on lexical information. Further (Yadav et al., 2019) show that this issue is relevant not just in NLI but in other NLP applications such as question answering (QA) also. This tendency of NNs to inadvertantly exploit such dataset artifacts is likely worsened by the fact that currently the success of NLP approaches is almost exclusively measured by empirical performance on benchmark datasets. While this emphasis on performance has facilitated the development of practical solutions, they may lack guidance as they are often not motivated by more general linguistic principles or human intuition. This makes it difficult to accurately judge the degree"
2020.lrec-1.850,I08-3008,0,0.0108417,"the development of practical solutions, they may lack guidance as they are often not motivated by more general linguistic principles or human intuition. This makes it difficult to accurately judge the degree to which these methods actually extract reasonable representations, correlate with human intuition or understand the underlying semantics (Dagan et al., 2013). In this work we postulate that altering these datasets based on lexical importance is beneficial for organizing research and guiding future empirical work. While the technique of delexicalization (or masking) has been used before (Zeman and Resnik, 2008), we have expanded it by incorporating semantic information (the assumption that meaning arises from a set of independent and discrete semantic units) (Peyrard, 2019). Since these techniques are general and compatible with most existing semantic representations, we believe they can be further extended onto datasets used for other NLP tasks. Thus, by enabling integration of these techniques into the training pipeline, we hope to control lexicalization in the datasets which the NN methods possibly depend upon. In particular, the contributions of our work are: (1) To motivate further research in"
2021.emnlp-main.558,N19-1423,0,0.0722521,"Missing"
2021.emnlp-main.558,N18-2017,0,0.104033,"utperforms state-of-theart classifiers that rely on the original data. 1 Introduction rejects the claim, or does not have enough information to reach a conclusion. Several fact verification datasets have been proposed recently, based on realworld news articles (Pomerleau and Rao, 2017), Wikipedia based knowledge bases (Thorne et al., 2018), fact verification websites (Wang, 2017), etc. Several transformer networks (Vaswani et al., 2017) based approaches (Liu et al., 2020) have achieved SOTA performance on these tasks. However, as shown in (Panenghat et al., 2020; Karimi Mahabadi et al., 2020; Gururangan et al., 2018), these models are also similarly affected by syntactic and lexical artifacts seen in other NLP tasks. Specifically, (Suntwal et al., 2019) shows the presence of such artifacts in the Fact Extraction and Verification (FEVER) dataset (Thorne et al., 2018), along with demonstrating that this limits the ability of the trained models to transfer knowledge to other similar datasets such as the Fake News Challenge (FNC) dataset (Pomerleau and Rao, 2017). To mitigate this dependency on such artifacts, they proposed a data distillation (or delexicalization) approach, which replaces some lexical artifa"
2021.emnlp-main.558,P84-1044,0,0.368539,"Missing"
2021.emnlp-main.558,2020.acl-main.769,0,0.0409526,"training dataset and outperforms state-of-theart classifiers that rely on the original data. 1 Introduction rejects the claim, or does not have enough information to reach a conclusion. Several fact verification datasets have been proposed recently, based on realworld news articles (Pomerleau and Rao, 2017), Wikipedia based knowledge bases (Thorne et al., 2018), fact verification websites (Wang, 2017), etc. Several transformer networks (Vaswani et al., 2017) based approaches (Liu et al., 2020) have achieved SOTA performance on these tasks. However, as shown in (Panenghat et al., 2020; Karimi Mahabadi et al., 2020; Gururangan et al., 2018), these models are also similarly affected by syntactic and lexical artifacts seen in other NLP tasks. Specifically, (Suntwal et al., 2019) shows the presence of such artifacts in the Fact Extraction and Verification (FEVER) dataset (Thorne et al., 2018), along with demonstrating that this limits the ability of the trained models to transfer knowledge to other similar datasets such as the Fake News Challenge (FNC) dataset (Pomerleau and Rao, 2017). To mitigate this dependency on such artifacts, they proposed a data distillation (or delexicalization) approach, which re"
2021.emnlp-main.558,P14-5010,1,0.0188402,"Missing"
2021.emnlp-main.558,2021.naacl-main.360,1,0.376332,"ith demonstrating that this limits the ability of the trained models to transfer knowledge to other similar datasets such as the Fake News Challenge (FNC) dataset (Pomerleau and Rao, 2017). To mitigate this dependency on such artifacts, they proposed a data distillation (or delexicalization) approach, which replaces some lexical artifacts such as named entities with their type and a unique id to indicate the occurrence of the same artifact in claim and evidence. A key unresolved issue in this direction is how much delexicalization to apply. As indicated in previous work (Suntwal et al., 2019; Mithun et al., 2021), delexicalization reduces overfitting. But too much delexicalization may discard critical information, e.g., replacing India with its NE label, say LOCATION, may remove contextual information about the country that is necessary for the correct classification of the claim-evidence pair. Our work proposes a solution for this problem, with the following contributions: Neural networks have achieved state-of-the-art (SOTA) performance across many natural language processing (NLP) tasks, usually in supervised settings. However, it has been shown that there are limitations to these methods caused in"
2021.emnlp-main.558,2020.lrec-1.850,1,0.829298,"lization strategy for the given training dataset and outperforms state-of-theart classifiers that rely on the original data. 1 Introduction rejects the claim, or does not have enough information to reach a conclusion. Several fact verification datasets have been proposed recently, based on realworld news articles (Pomerleau and Rao, 2017), Wikipedia based knowledge bases (Thorne et al., 2018), fact verification websites (Wang, 2017), etc. Several transformer networks (Vaswani et al., 2017) based approaches (Liu et al., 2020) have achieved SOTA performance on these tasks. However, as shown in (Panenghat et al., 2020; Karimi Mahabadi et al., 2020; Gururangan et al., 2018), these models are also similarly affected by syntactic and lexical artifacts seen in other NLP tasks. Specifically, (Suntwal et al., 2019) shows the presence of such artifacts in the Fact Extraction and Verification (FEVER) dataset (Thorne et al., 2018), along with demonstrating that this limits the ability of the trained models to transfer knowledge to other similar datasets such as the Fake News Challenge (FNC) dataset (Pomerleau and Rao, 2017). To mitigate this dependency on such artifacts, they proposed a data distillation (or delexi"
2021.emnlp-main.558,D16-1244,0,0.076961,"Missing"
2021.emnlp-main.558,D19-1340,1,0.905482,"o reach a conclusion. Several fact verification datasets have been proposed recently, based on realworld news articles (Pomerleau and Rao, 2017), Wikipedia based knowledge bases (Thorne et al., 2018), fact verification websites (Wang, 2017), etc. Several transformer networks (Vaswani et al., 2017) based approaches (Liu et al., 2020) have achieved SOTA performance on these tasks. However, as shown in (Panenghat et al., 2020; Karimi Mahabadi et al., 2020; Gururangan et al., 2018), these models are also similarly affected by syntactic and lexical artifacts seen in other NLP tasks. Specifically, (Suntwal et al., 2019) shows the presence of such artifacts in the Fact Extraction and Verification (FEVER) dataset (Thorne et al., 2018), along with demonstrating that this limits the ability of the trained models to transfer knowledge to other similar datasets such as the Fake News Challenge (FNC) dataset (Pomerleau and Rao, 2017). To mitigate this dependency on such artifacts, they proposed a data distillation (or delexicalization) approach, which replaces some lexical artifacts such as named entities with their type and a unique id to indicate the occurrence of the same artifact in claim and evidence. A key unr"
2021.emnlp-main.558,W18-5501,0,0.0634783,"ouraged to independently learn from each other through pair-wise consistency losses. In several cross-domain experiments between the FEVER and FNC fact verification datasets, we show that our approach learns the best delexicalization strategy for the given training dataset and outperforms state-of-theart classifiers that rely on the original data. 1 Introduction rejects the claim, or does not have enough information to reach a conclusion. Several fact verification datasets have been proposed recently, based on realworld news articles (Pomerleau and Rao, 2017), Wikipedia based knowledge bases (Thorne et al., 2018), fact verification websites (Wang, 2017), etc. Several transformer networks (Vaswani et al., 2017) based approaches (Liu et al., 2020) have achieved SOTA performance on these tasks. However, as shown in (Panenghat et al., 2020; Karimi Mahabadi et al., 2020; Gururangan et al., 2018), these models are also similarly affected by syntactic and lexical artifacts seen in other NLP tasks. Specifically, (Suntwal et al., 2019) shows the presence of such artifacts in the Fact Extraction and Verification (FEVER) dataset (Thorne et al., 2018), along with demonstrating that this limits the ability of the"
2021.findings-emnlp.343,C96-2183,0,0.397895,"Missing"
2021.findings-emnlp.343,2020.acl-main.709,0,0.0544348,"Missing"
2021.findings-emnlp.343,2020.tacl-1.5,0,0.0291171,"ength in this dataset is 22.3 words. Each data point contains a premise-hypothesis pair and one of the three labels: contradiction, entailment, and neutral. We selected MNLI as the second task to further understand the effects of TS on machine performance on tasks that rely on long text, which is a challenge for TS methods (Shardlow, 2014; Xu et al., 2015). Table 4: F1 on TACRED test set of the LSTM and SpanBERT approaches using NTS (Nisioi et al., 2017) as the TS method. This table follows the same format as Table 3. LSTMs1 (Hochreiter and Schmidhuber, 1997), and a second based on SpanBERT2 (Joshi et al., 2020). For MNLI, we trained a BERT-based classifier3 (Devlin et al., 2018). For reproducibility, we use the default settings and general hyper parameters recommended by the task and creators of the transformer networks (Zhang et al., 2017; Joshi et al., 2020; Devlin et al., 2018). Through this, we aim to separate potential improvements of our approaches from those coming from improved configurations. Text simplification methods: For TS, we use two out-of-the-box neural seq2seq TS approaches: ACCESS (Martin et al., 2019), and NTS (Nisioi et al., 2017). Tables 1 and 2 show the BLEU scores (Papineni e"
2021.findings-emnlp.343,O13-1007,0,0.0193657,"Missing"
2021.findings-emnlp.343,N18-2072,0,0.0212526,"± 0.23 0.81 ± 0.17 Dev matched the use of rule-based TS methods. In contrast, we investigate the potential use of domain-agnostic neural TS systems in simplifying inputs for downstream tasks. We show that, despite the complexity of the tasks investigated and the domain agnosticity of the TS approaches, TS improves both tasks when used for training data augmentation, but not when used to simplify evaluation texts. On data augmentation for natural language processing downstream tasks, previous work show significant benefits of introducing noisy data on the machine performance (Van et al., 2021; Kobayashi, 2018). Previous efforts used TS approaches, e.g. lexical substitution, to augment training data for downstream tasks such as text classification (Zhang et al., 2015; Wei and Zou, 2019). However, these methods focused on replacing words with thesaurus-based synonyms, and did not emphasize other important lexical and syntactic simplification. Here, we use two out-of-the-box neural TS systems that apply both lexical and syntactic sentence simplification for data augmentation, and show that our data augmentation consistently leads to better performances. Note that we do not use rulebased TS systems bec"
2021.findings-emnlp.343,N19-1317,0,0.0117911,"h disabilities (Rello et al., 2013). While learning approaches to solving machine learning this can be achieved in a variety of approaches problems (Ghosh et al., 2021; Blalock et al., 2020; (Sikka et al., 2020), most TS research has focused Yin et al., 2017). on two major approaches: rule-based and neural sequence-to-sequence (seq2seq). Since 2017, there With respect to input simplification, several is a significant increase of neural seq2seq TS meth- works have utilized TS as a pre-processing step ods (Zhang and Lapata, 2017; Zhao et al., 2018; for downstream NLP tasks such as information exKriz et al., 2019; Maddela et al., 2020; Jiang et al., traction (Miwa et al., 2010; Schmidek and Barbosa, 2020). 2014; Niklaus et al., 2017), parsing (Chandrasekar In this paper, we analyze another potential use et al., 1996), semantic role labeling (Vickrey and of the latter TS direction: assisting machines per- Koller, 2008), and machine translation (Štajner and forming natural language processing (NLP) tasks. Popovi´c, 2016). However, most of them focus on 4074 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 4074–4080 November 7–11, 2021. ©2021 Association for Computational Ling"
2021.findings-emnlp.343,2021.naacl-main.277,0,0.0595087,"Missing"
2021.findings-emnlp.343,C10-1089,0,0.0205821,"solving machine learning this can be achieved in a variety of approaches problems (Ghosh et al., 2021; Blalock et al., 2020; (Sikka et al., 2020), most TS research has focused Yin et al., 2017). on two major approaches: rule-based and neural sequence-to-sequence (seq2seq). Since 2017, there With respect to input simplification, several is a significant increase of neural seq2seq TS meth- works have utilized TS as a pre-processing step ods (Zhang and Lapata, 2017; Zhao et al., 2018; for downstream NLP tasks such as information exKriz et al., 2019; Maddela et al., 2020; Jiang et al., traction (Miwa et al., 2010; Schmidek and Barbosa, 2020). 2014; Niklaus et al., 2017), parsing (Chandrasekar In this paper, we analyze another potential use et al., 1996), semantic role labeling (Vickrey and of the latter TS direction: assisting machines per- Koller, 2008), and machine translation (Štajner and forming natural language processing (NLP) tasks. Popovi´c, 2016). However, most of them focus on 4074 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 4074–4080 November 7–11, 2021. ©2021 Association for Computational Linguistics ACCESS NTS 1 Training Data 0.67 ± 0.16 0.89 ± 0.22 2 Dev"
2021.findings-emnlp.343,P17-2014,0,0.0559736,"tream tasks such as text classification (Zhang et al., 2015; Wei and Zou, 2019). However, these methods focused on replacing words with thesaurus-based synonyms, and did not emphasize other important lexical and syntactic simplification. Here, we use two out-of-the-box neural TS systems that apply both lexical and syntactic sentence simplification for data augmentation, and show that our data augmentation consistently leads to better performances. Note that we do not use rulebased TS systems because they have been proven to perform worse than their neural counterparts (Zhang and Lapata, 2017; Nisioi et al., 2017). Further, rule-based TS systems are harder to build in a domain-independent way due to the many linguistic/syntactic variations across domains. 3 Approach We investigate the impact of text simplification on downstream NLP tasks in two ways: (a) simplifying input texts at prediction time, and (b) augmenting training data for the respective NLP tasks. We discuss the settings of these experiments next. 5 Premise 0.62 ± 0.30 0.75 ± 0.26 6 Hypothesis 0.60 ± 0.25 0.80 ± 0.17 Table 2: The empirical differences in BLEU scores (Papineni et al., 2002) between original and simplified text generated by t"
2021.findings-emnlp.343,P02-1040,0,0.116483,"we analyze another potential use et al., 1996), semantic role labeling (Vickrey and of the latter TS direction: assisting machines per- Koller, 2008), and machine translation (Štajner and forming natural language processing (NLP) tasks. Popovi´c, 2016). However, most of them focus on 4074 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 4074–4080 November 7–11, 2021. ©2021 Association for Computational Linguistics ACCESS NTS 1 Training Data 0.67 ± 0.16 0.89 ± 0.22 2 Dev Data 0.68 ± 0.15 0.92 ± 0.18 ACCESS NTS Train Table 1: The empirical differences in BLEU scores (Papineni et al., 2002) between original and simplified text generated by two TS systems, ACCESS and NTS, in TACRED training and dev datasets. 1 Premise 0.62 ± 0.24 0.76 ± 0.25 2 Hypothesis 0.62 ± 0.30 0.80 ± 0.17 Dev mismatched 3 Premise 0.62 ± 0.28 0.80 ± 0.22 4 Hypothesis 0.65 ± 0.23 0.81 ± 0.17 Dev matched the use of rule-based TS methods. In contrast, we investigate the potential use of domain-agnostic neural TS systems in simplifying inputs for downstream tasks. We show that, despite the complexity of the tasks investigated and the domain agnosticity of the TS approaches, TS improves both tasks when used for t"
2021.findings-emnlp.343,W14-1210,0,0.0607571,"Missing"
2021.findings-emnlp.343,schmidek-barbosa-2014-improving,0,0.0316643,"Missing"
2021.findings-emnlp.343,W16-3411,0,0.0498877,"Missing"
2021.findings-emnlp.343,2020.coling-main.122,1,0.839639,"Missing"
2021.findings-emnlp.343,P08-1040,0,0.0694732,"Missing"
2021.findings-emnlp.343,D19-1670,0,0.0467744,"Missing"
2021.findings-emnlp.343,Q15-1021,0,0.0599003,"Missing"
2021.findings-emnlp.343,D17-1062,0,0.100692,"(Pellow and Eskenazi, 2014), and application of neural networks and neural deep people with disabilities (Rello et al., 2013). While learning approaches to solving machine learning this can be achieved in a variety of approaches problems (Ghosh et al., 2021; Blalock et al., 2020; (Sikka et al., 2020), most TS research has focused Yin et al., 2017). on two major approaches: rule-based and neural sequence-to-sequence (seq2seq). Since 2017, there With respect to input simplification, several is a significant increase of neural seq2seq TS meth- works have utilized TS as a pre-processing step ods (Zhang and Lapata, 2017; Zhao et al., 2018; for downstream NLP tasks such as information exKriz et al., 2019; Maddela et al., 2020; Jiang et al., traction (Miwa et al., 2010; Schmidek and Barbosa, 2020). 2014; Niklaus et al., 2017), parsing (Chandrasekar In this paper, we analyze another potential use et al., 1996), semantic role labeling (Vickrey and of the latter TS direction: assisting machines per- Koller, 2008), and machine translation (Štajner and forming natural language processing (NLP) tasks. Popovi´c, 2016). However, most of them focus on 4074 Findings of the Association for Computational Linguistics: EMNL"
2021.findings-emnlp.343,D17-1004,0,0.025542,"n machine performance on tasks that rely on long text, which is a challenge for TS methods (Shardlow, 2014; Xu et al., 2015). Table 4: F1 on TACRED test set of the LSTM and SpanBERT approaches using NTS (Nisioi et al., 2017) as the TS method. This table follows the same format as Table 3. LSTMs1 (Hochreiter and Schmidhuber, 1997), and a second based on SpanBERT2 (Joshi et al., 2020). For MNLI, we trained a BERT-based classifier3 (Devlin et al., 2018). For reproducibility, we use the default settings and general hyper parameters recommended by the task and creators of the transformer networks (Zhang et al., 2017; Joshi et al., 2020; Devlin et al., 2018). Through this, we aim to separate potential improvements of our approaches from those coming from improved configurations. Text simplification methods: For TS, we use two out-of-the-box neural seq2seq TS approaches: ACCESS (Martin et al., 2019), and NTS (Nisioi et al., 2017). Tables 1 and 2 show the BLEU scores (Papineni et al., 2002) between original and simplified text generated by these two TS systems for the two tasks. The tables highlight that both systems change the input texts, with ACCESS being more aggressive. Evaluation measures: We directly"
2021.naacl-main.360,N19-1423,0,0.0277614,"2.3 Classifiers were divided into four classes, agree, disagree, disWe experiment with a state-of-the-art method for cuss and unrelated. These claim-evidence pairs fact verification, transformers (Vaswani et al., were created using the headlines and content sec2017), which has achieved state-of-the-art results tion of real news articles respectively. While the not only in the task of fact verification but in several training partition of the publicly available dataset other NLP tasks. Specifically, we use the PyTorch comprised 49,972 data points, the testing partition implementation of BERT (Devlin et al., 2019) from had 25,413 data points. We further divided the huggingface (Wolf et al., 2019). training partition into 40,904 data points for trainWe experimented with several pre-trained BERT- ing and 9,068 data points for development. base models and found that the one which gave the Cross-domain labels: In order to evaluate the highest performance was the BERT-cased model proposed methods in a cross-domain setting, we when used with a sequence length of 128. Further, modified the label space of the source domain to to distinguish the vocabulary of the delexicalized match that of the target domain. I"
2021.naacl-main.360,N18-2017,0,0.119436,"Missing"
2021.naacl-main.360,P84-1044,0,0.407657,"Missing"
2021.naacl-main.360,2020.acl-main.769,0,0.348764,"Missing"
2021.naacl-main.360,P14-5010,1,0.0112105,"experiments that more fine-grained NE types dicted label distributions between the student and yield better models. In particular, we utilize the the teacher. The consistency loss is implemented FIGER named entity recognizer (NER) (Ling and as a mean squared error between the label scores Weld, 2012) to detect and replace named entities predicted by the student and the teacher. Additionwith their most specific label returned by the NER. ally, both the student and the teacher components Further, we also process the text with the CoreNLP include a regular classification loss on their respecNER (Manning et al., 2014) to delexicalize addi- tive data, which is implemented using cross entropy. tional NER classes not covered by FIGER. We This encourages both the student and the teacher to 4547 3 3.1 Experiments Data We use two distinct fact verification datasets for our experiments, FEVER (Thorne et al., 2018) and FNC (Pomerleau and Rao, 2017). The Fact Extraction and Verification (FEVER) dataset: This dataset consists of 145,449 data points each having a claim and evidence pair. These claim-evidence pairs typically contain one or more sentences compiled from Wikipedia using an information retrieval (IR) modu"
2021.naacl-main.360,2020.fever-1.1,0,0.026477,"Missing"
2021.naacl-main.360,2020.lrec-1.850,1,0.832208,"Missing"
2021.naacl-main.360,S18-2023,0,0.0571661,"Missing"
2021.naacl-main.360,D19-1341,0,0.0342202,"Missing"
2021.naacl-main.360,D19-1340,1,0.523359,"distillation We propose a model distillation strategy to mitigate the risk of overly aggressive data distillation. In particular, we introduce a teacher-student architecture (shown in figure 1) (Hinton et al., 2015; Tarvainen and Valpola, 2017; Laine and Aila, 2016; Sajjadi et al., 2016), where the teacher is trained on the original, lexicalized data, and the student is trained on the data delexicalized with the approach 2 Methodology described in the previous sub-section. The intuition behind our model distillation ap2.1 Data distillation proach is that the proposed teacher model will “pull” Suntwal et al. (2019) demonstrated that named en- the student model towards the original underlytities are most prone to overfitting for fact verifi- ing semantics, which are partially obscured to the cation. Based on this observation, we also replace student due to the delexicalization of its training named entities with their type (and a unique id). data. More formally, this is captured through a conHowever, unlike their work, we have observed in sistency loss that minimizes the difference in preearly experiments that more fine-grained NE types dicted label distributions between the student and yield better mode"
2021.naacl-main.360,W18-5501,0,0.101675,"2012) to detect and replace named entities predicted by the student and the teacher. Additionwith their most specific label returned by the NER. ally, both the student and the teacher components Further, we also process the text with the CoreNLP include a regular classification loss on their respecNER (Manning et al., 2014) to delexicalize addi- tive data, which is implemented using cross entropy. tional NER classes not covered by FIGER. We This encourages both the student and the teacher to 4547 3 3.1 Experiments Data We use two distinct fact verification datasets for our experiments, FEVER (Thorne et al., 2018) and FNC (Pomerleau and Rao, 2017). The Fact Extraction and Verification (FEVER) dataset: This dataset consists of 145,449 data points each having a claim and evidence pair. These claim-evidence pairs typically contain one or more sentences compiled from Wikipedia using an information retrieval (IR) module and are classified into three classes: supports, refutes and not enough info. The evidence for data points that had the gold label of not enough info were retrieved (using a task-provided IR component) either by finding the nearest neighbor to the claim or randomly. Even though the training"
2021.naacl-main.363,D19-5310,0,0.0151878,"rall execution flow of our approaches. Second, we show that the candidate QA system in Figure 2. The four key components evidence chain from WAIR assist reranker method of the system are explained below. to learn compositional and aggregative reasoning. 1. Initial evidence sentence retrieval: In the first Other recent works have proposed supervised step, we retrieve candidate evidence sentences (or iterative and multi-task approaches for evidence retrieval (Feldman and El-Yaniv, 2019; Qi et al., justification) given a query. We propose a simple unsupervised approach, which, however, has 2019; Banerjee, 2019). But, these supervised chain retrieval approaches are expensive in their run- been designed to bridge the “lexical chasm” inherent between multi-hop questions and their antime and do not scale well on large KB based QA swers (Berger et al., 2000). We call our algorithm datasets. On the contrary, our retrieval approach does not require any labeling data and is faster be- weighted alignment-based information retrieval (WAIR). WAIR operates in two steps, by combincause of its unsupervised nature. Further, our joint ing ideas from embedding based-alignment (Yadav approach is much simpler, perform"
2021.naacl-main.363,C10-2007,0,0.0531478,"Missing"
2021.naacl-main.363,W19-4828,0,0.0511309,"Missing"
2021.naacl-main.363,N19-1423,0,0.0157628,"ticularly focused on a) evaluating attention scores on linking terms that approximate multi-hop compositionality and, b) complementary knowledge aggregation necessary for multi-hop QA. Importance of Evidence Retrieval for Question Answering Several neural QA methods have achieved high performance without relying on evidence texts. Many of these approaches utilize external labeled training data (Raffel et al., 2019; Pan et al., 2019), which limits their portability to other domains. Others rely on pretraining, which tends to be computationally expensive but can be used as starting checkpoints (Devlin et al., 2019; Liu et al., 2019). More importantly, many of these directions lack explanation of their selected answers to the end user. In contrast, QA methods that incorporate an evidence retrieval module can provide these evidence texts as human-readable explanations. Further, several works have demonstrated that retrieve and read approaches (similar to ours) tend to achieve higher performance than the former QA methods (Chen et al., 2017; Qi et al., 2019). Our work is inspired by these directions but mostly focuses on jointly retrieving+reranking clusters of evidence sentences that leads to substantial"
2021.naacl-main.363,D19-1006,0,0.081497,"retrieval approach does not require any labeling data and is faster be- weighted alignment-based information retrieval (WAIR). WAIR operates in two steps, by combincause of its unsupervised nature. Further, our joint ing ideas from embedding based-alignment (Yadav approach is much simpler, performs well and scales et al., 2019a) and pseudo-relevance feedback (Bernon large KB based QA such as QASC. In this work, we focus on analyzing the multi- hard, 2010) approaches. hop evidence reasoning via attention (Clark et al., In its first step, WAIR uses a query that con2019) and learned embeddings (Ethayarajh, 2019) sists of the non-stop words of the original ques4573 g Joi Sent 0 - RNA is a small molecule that can squeeze ..… membrane Sent 1 - RNA synthesis in eukaryotic cells …. of evidence sentences {Sent 0,Sent 1} {Sent 0,Sent 2} . .. {Sent 8,Sent 9} K=2 {Sent 0, Sent 1, Sent 2} {Sent 0, Sent 1, Sent 3} .... .... {Sent 7,Sent 8, Sent 9} K=3 RoBERTa reranker Answer Selector QASC (MCQA) MultiRC (Classification) Dataset WAIR BM25 Alignment QASC (top 2) MultiRC (top 3) 78.85 55.92 61.42 39.86 63.40 52.98 Gold Evidence 80.81 63.95 Table 1: The coverage of question (+candidate answer) terms in the sentence"
2021.naacl-main.363,P19-1222,0,0.0191434,"oposed Approach stantially improved resulting in state-of-the-art performance and thus outperforming all the previous We summarize the overall execution flow of our approaches. Second, we show that the candidate QA system in Figure 2. The four key components evidence chain from WAIR assist reranker method of the system are explained below. to learn compositional and aggregative reasoning. 1. Initial evidence sentence retrieval: In the first Other recent works have proposed supervised step, we retrieve candidate evidence sentences (or iterative and multi-task approaches for evidence retrieval (Feldman and El-Yaniv, 2019; Qi et al., justification) given a query. We propose a simple unsupervised approach, which, however, has 2019; Banerjee, 2019). But, these supervised chain retrieval approaches are expensive in their run- been designed to bridge the “lexical chasm” inherent between multi-hop questions and their antime and do not scale well on large KB based QA swers (Berger et al., 2000). We call our algorithm datasets. On the contrary, our retrieval approach does not require any labeling data and is faster be- weighted alignment-based information retrieval (WAIR). WAIR operates in two steps, by combincause o"
2021.naacl-main.363,D19-1107,0,0.0267738,"Missing"
2021.naacl-main.363,P17-1147,0,0.0315705,"sed information retrieval techniques, e.g., BM25 (Robertson et al., 2009), tf-idf (Ramos et al., 2003; Manning et al., 2008), or alignment-based methods (Kim et al., 2017), have (1) We introduce a simple, unsupervised and fast evidence retrieval approach -WAIR for multi-hop QA that generates complete and associated candidate evidence chains. To show the multi-hop rea1 soning approximated within WAIR candidate eviCodes - https://github.com/vikas95/WAIR_ dence chains, We present several attention weights interpretability 4572 been widely used to retrieve evidence texts for open-domain QA tasks (Joshi et al., 2017; Dunn et al., 2017). Although these approaches have been strong benchmarks for decades, they usually do not perform well on recent complex reasoning-based QA tasks (Yang et al., 2018; Khot et al., 2019a). More recently, supervised neural network (NN) based retrieval methods have achieved strong results on complex questions (Karpukhin et al., 2020; Nie et al., 2019; Tu et al., 2019). However, these approaches require annotated data for initial retrieval and suffer from the same disadvantages at the reranking stage as the other methods that retrieve+rerank individual evidence sentences, i.e., t"
2021.naacl-main.363,N18-1023,0,0.105211,"g et al., 2019; Yang et al., 2018). Our work is also focused on improv1 Introduction ing the explainability of QA methods by the means of evidence (or justification) sentence retrieval. Recent advances in question answering (QA) have achieved excellent performance on several benchEvidence retrieval for multi-hop QA is a chalmark datasets (Wang et al., 2019a), even when rely- lenging task as it requires compositional infering on partial (Gururangan et al., 2018), incorrect ence based aggregation of multiple evidence sen(Jia and Liang, 2017) or no supporting knowledge tences (Yang et al., 2018; Khashabi et al., 2018; (Raffel et al., 2019). Specifically, black-box neural Welbl et al., 2018; Khot et al., 2019a). For such QA methods have shown to rely on spurious signals compositional aggregation, we emphasize on the confirming unfaithful or non-explainable behavior importance of jointly handeling the set of evidence 4571 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4571–4581 June 6–11, 2021. ©2021 Association for Computational Linguistics facts within the QA pipeline. The motivation behind our work is s"
2021.naacl-main.363,D19-1281,0,0.102092,"ainability of QA methods by the means of evidence (or justification) sentence retrieval. Recent advances in question answering (QA) have achieved excellent performance on several benchEvidence retrieval for multi-hop QA is a chalmark datasets (Wang et al., 2019a), even when rely- lenging task as it requires compositional infering on partial (Gururangan et al., 2018), incorrect ence based aggregation of multiple evidence sen(Jia and Liang, 2017) or no supporting knowledge tences (Yang et al., 2018; Khashabi et al., 2018; (Raffel et al., 2019). Specifically, black-box neural Welbl et al., 2018; Khot et al., 2019a). For such QA methods have shown to rely on spurious signals compositional aggregation, we emphasize on the confirming unfaithful or non-explainable behavior importance of jointly handeling the set of evidence 4571 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4571–4581 June 6–11, 2021. ©2021 Association for Computational Linguistics facts within the QA pipeline. The motivation behind our work is simple: jointly handling evidence sentences gives access to the complete information together"
2021.naacl-main.363,2020.emnlp-main.574,0,0.031376,"Missing"
2021.naacl-main.363,N18-2017,0,0.058,"Missing"
2021.naacl-main.363,2021.ccl-1.108,0,0.04565,"Missing"
2021.naacl-main.363,W18-1703,0,0.0637151,"Missing"
2021.naacl-main.363,D17-1215,0,0.0141551,"important for faithfulness and explainability of neural QA methods (DeYoung et al., 2019; Yang et al., 2018). Our work is also focused on improv1 Introduction ing the explainability of QA methods by the means of evidence (or justification) sentence retrieval. Recent advances in question answering (QA) have achieved excellent performance on several benchEvidence retrieval for multi-hop QA is a chalmark datasets (Wang et al., 2019a), even when rely- lenging task as it requires compositional infering on partial (Gururangan et al., 2018), incorrect ence based aggregation of multiple evidence sen(Jia and Liang, 2017) or no supporting knowledge tences (Yang et al., 2018; Khashabi et al., 2018; (Raffel et al., 2019). Specifically, black-box neural Welbl et al., 2018; Khot et al., 2019a). For such QA methods have shown to rely on spurious signals compositional aggregation, we emphasize on the confirming unfaithful or non-explainable behavior importance of jointly handeling the set of evidence 4571 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4571–4581 June 6–11, 2021. ©2021 Association for Computational L"
2021.naacl-main.363,D19-1258,0,0.0523276,"Missing"
2021.naacl-main.363,D19-5804,0,0.0162984,"shown attention based analysis on pretrained transformer language models (Rogers et al., 2020) on various NLP tasks including QA (van Aken et al., 2019). Our novel analyses are particularly focused on a) evaluating attention scores on linking terms that approximate multi-hop compositionality and, b) complementary knowledge aggregation necessary for multi-hop QA. Importance of Evidence Retrieval for Question Answering Several neural QA methods have achieved high performance without relying on evidence texts. Many of these approaches utilize external labeled training data (Raffel et al., 2019; Pan et al., 2019), which limits their portability to other domains. Others rely on pretraining, which tends to be computationally expensive but can be used as starting checkpoints (Devlin et al., 2019; Liu et al., 2019). More importantly, many of these directions lack explanation of their selected answers to the end user. In contrast, QA methods that incorporate an evidence retrieval module can provide these evidence texts as human-readable explanations. Further, several works have demonstrated that retrieve and read approaches (similar to ours) tend to achieve higher performance than the former QA methods (Ch"
2021.naacl-main.363,D19-1261,0,0.0153823,"eir portability to other domains. Others rely on pretraining, which tends to be computationally expensive but can be used as starting checkpoints (Devlin et al., 2019; Liu et al., 2019). More importantly, many of these directions lack explanation of their selected answers to the end user. In contrast, QA methods that incorporate an evidence retrieval module can provide these evidence texts as human-readable explanations. Further, several works have demonstrated that retrieve and read approaches (similar to ours) tend to achieve higher performance than the former QA methods (Chen et al., 2017; Qi et al., 2019). Our work is inspired by these directions but mostly focuses on jointly retrieving+reranking clusters of evidence sentences that leads to substantial QA performance improvements. Jointly retrieving evidence sentences: Recently, several works have proposed retrieval of evidence chains that has led to stronger evidence retrieval performance (Yadav et al., 2019b; Khot et al., 2019a). Our WAIR approach aligns in the same direction and particularly utilizes coverage and associativity that leads to higher performance. Importantly, our work focuses on highlighting the benefits of feeding evidence ch"
2021.naacl-main.363,2020.tacl-1.54,0,0.0742247,"ese approaches require annotated data for initial retrieval and suffer from the same disadvantages at the reranking stage as the other methods that retrieve+rerank individual evidence sentences, i.e., the retrieval algorithm is not aware of what information has already been retrieved and what is missing, or how individual facts need to be combined for explaining the multi-hop reasoning (Khot et al., 2019b). Our proposed joint retrieval and reranking approach mitigates both these limitations. analyses. Several works have shown attention based analysis on pretrained transformer language models (Rogers et al., 2020) on various NLP tasks including QA (van Aken et al., 2019). Our novel analyses are particularly focused on a) evaluating attention scores on linking terms that approximate multi-hop compositionality and, b) complementary knowledge aggregation necessary for multi-hop QA. Importance of Evidence Retrieval for Question Answering Several neural QA methods have achieved high performance without relying on evidence texts. Many of these approaches utilize external labeled training data (Raffel et al., 2019; Pan et al., 2019), which limits their portability to other domains. Others rely on pretraining,"
2021.naacl-main.363,K19-1065,0,0.300058,"ch other. Both the gold evidence are also found in sentences from step-1 and step2. (Geva et al., 2019). Thus, justifying the underlying knowledge or evidence text has been deemed very important for faithfulness and explainability of neural QA methods (DeYoung et al., 2019; Yang et al., 2018). Our work is also focused on improv1 Introduction ing the explainability of QA methods by the means of evidence (or justification) sentence retrieval. Recent advances in question answering (QA) have achieved excellent performance on several benchEvidence retrieval for multi-hop QA is a chalmark datasets (Wang et al., 2019a), even when rely- lenging task as it requires compositional infering on partial (Gururangan et al., 2018), incorrect ence based aggregation of multiple evidence sen(Jia and Liang, 2017) or no supporting knowledge tences (Yang et al., 2018; Khashabi et al., 2018; (Raffel et al., 2019). Specifically, black-box neural Welbl et al., 2018; Khot et al., 2019a). For such QA methods have shown to rely on spurious signals compositional aggregation, we emphasize on the confirming unfaithful or non-explainable behavior importance of jointly handeling the set of evidence 4571 Proceedings of the 2021 Con"
2021.naacl-main.363,Q18-1021,0,0.0243297,"duction ing the explainability of QA methods by the means of evidence (or justification) sentence retrieval. Recent advances in question answering (QA) have achieved excellent performance on several benchEvidence retrieval for multi-hop QA is a chalmark datasets (Wang et al., 2019a), even when rely- lenging task as it requires compositional infering on partial (Gururangan et al., 2018), incorrect ence based aggregation of multiple evidence sen(Jia and Liang, 2017) or no supporting knowledge tences (Yang et al., 2018; Khashabi et al., 2018; (Raffel et al., 2019). Specifically, black-box neural Welbl et al., 2018; Khot et al., 2019a). For such QA methods have shown to rely on spurious signals compositional aggregation, we emphasize on the confirming unfaithful or non-explainable behavior importance of jointly handeling the set of evidence 4571 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4571–4581 June 6–11, 2021. ©2021 Association for Computational Linguistics facts within the QA pipeline. The motivation behind our work is simple: jointly handling evidence sentences gives access to the complete in"
2021.naacl-main.363,N19-1274,1,0.913614,"can provide these evidence texts as human-readable explanations. Further, several works have demonstrated that retrieve and read approaches (similar to ours) tend to achieve higher performance than the former QA methods (Chen et al., 2017; Qi et al., 2019). Our work is inspired by these directions but mostly focuses on jointly retrieving+reranking clusters of evidence sentences that leads to substantial QA performance improvements. Jointly retrieving evidence sentences: Recently, several works have proposed retrieval of evidence chains that has led to stronger evidence retrieval performance (Yadav et al., 2019b; Khot et al., 2019a). Our WAIR approach aligns in the same direction and particularly utilizes coverage and associativity that leads to higher performance. Importantly, our work focuses on highlighting the benefits of feeding evidence chains to transformer based reranking methods. First, the evidence retrieval performance of the same reranker is sub- 3 Proposed Approach stantially improved resulting in state-of-the-art performance and thus outperforming all the previous We summarize the overall execution flow of our approaches. Second, we show that the candidate QA system in Figure 2. The fo"
2021.naacl-main.363,N19-1270,0,0.0505889,"Missing"
2021.naacl-main.363,D19-1260,1,0.896253,"can provide these evidence texts as human-readable explanations. Further, several works have demonstrated that retrieve and read approaches (similar to ours) tend to achieve higher performance than the former QA methods (Chen et al., 2017; Qi et al., 2019). Our work is inspired by these directions but mostly focuses on jointly retrieving+reranking clusters of evidence sentences that leads to substantial QA performance improvements. Jointly retrieving evidence sentences: Recently, several works have proposed retrieval of evidence chains that has led to stronger evidence retrieval performance (Yadav et al., 2019b; Khot et al., 2019a). Our WAIR approach aligns in the same direction and particularly utilizes coverage and associativity that leads to higher performance. Importantly, our work focuses on highlighting the benefits of feeding evidence chains to transformer based reranking methods. First, the evidence retrieval performance of the same reranker is sub- 3 Proposed Approach stantially improved resulting in state-of-the-art performance and thus outperforming all the previous We summarize the overall execution flow of our approaches. Second, we show that the candidate QA system in Figure 2. The fo"
2021.naacl-main.363,N19-1302,0,0.0242364,"Missing"
2021.naacl-main.363,2020.acl-main.414,1,0.823917,"Missing"
2021.naacl-main.363,D18-1259,0,0.112588,"ear membrane are called eukaryotic 2. Eukaryotic cells have three different RNA polymerases. Figure 1: An example question from the QASC dataset with evidece sentences retrieved by BM25 and two steps of WAIR. The evidence retrieved in step-2 of WAIR contain information missed by sentences in step1 and are associated with each other. Both the gold evidence are also found in sentences from step-1 and step2. (Geva et al., 2019). Thus, justifying the underlying knowledge or evidence text has been deemed very important for faithfulness and explainability of neural QA methods (DeYoung et al., 2019; Yang et al., 2018). Our work is also focused on improv1 Introduction ing the explainability of QA methods by the means of evidence (or justification) sentence retrieval. Recent advances in question answering (QA) have achieved excellent performance on several benchEvidence retrieval for multi-hop QA is a chalmark datasets (Wang et al., 2019a), even when rely- lenging task as it requires compositional infering on partial (Gururangan et al., 2018), incorrect ence based aggregation of multiple evidence sen(Jia and Liang, 2017) or no supporting knowledge tences (Yang et al., 2018; Khashabi et al., 2018; (Raffel et"
2021.naacl-main.97,N19-1423,0,0.0124938,"ally suffer from the compositionality generalization problem, meaning that they tend to fail when the number of reasoning steps are much larger in the evaluation set than in the training set (Hupkes et al., 2020; Hahn, 2020; Clark et al., 2020). Newell (1994) categorized cognitive processes based on their time scales: unconscious activities take around 50 ms, whereas conscious actions can vary from 100 ms to hours. Importantly, Newell 1 Introduction (1994) argued that conscious actions are sequences of simple conscious/unconscious actions. ExtrapoLarge pretrained language models such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., lating from cognitive science to natural language processing (NLP), in this paper we ask the question: 2019) have been successfully used in multi-hop can we design an interpretable multi-hop reasoning reasoning problems (Banerjee et al., 2020; Asai et al., 2019; Yadav et al., 2019). Usually, these pre- system that sequentially applies neural networks trained on simpler tasks? Further, motivated by the trained language models solve multi-hop reasoning problems in a discriminative end-to-end manner: finding from cognitive science that people might use internal monologue"
2021.naacl-main.97,2020.acl-main.495,0,0.0198096,"015), Dynamic Memory Networks (DMN) (Kumar et al., 2016) and Compositional Attention Networks (MAC) (Hudson and Manning, 2018). Another direction are the neural modular networks, where what components to use are determined dynamically for each question (Gupta et al., 2019; Jiang and Bansal, 2019). However, it is hard to prove the components are actually fulfilling the designed functionality after training due to the distributed nature of the intermediate representations. In contrast, we explicitly evaluate the performance of each component of EVR after training, achieving better faithfulness (Subramanian et al., 2020). Formal Theorem Prover: Neural components have been used to augment formal theorem proving in several ways. Polu and Sutskever (2020) apply a Seq2Seq neural network for mathematical theorem proving by training the neural network to generate the proof at each step. Some works seek to use distributed representations to augment the rule-based backward chaining (Weber et al., 2019; Dong et al., 2018). However, these works still highly rely on the formal representations and they do not generate the natural language subgoals at each step. Problem Solver and Cognitive Architectures: Our work is also"
2021.naacl-main.97,P19-1618,0,0.0283695,"nality after training due to the distributed nature of the intermediate representations. In contrast, we explicitly evaluate the performance of each component of EVR after training, achieving better faithfulness (Subramanian et al., 2020). Formal Theorem Prover: Neural components have been used to augment formal theorem proving in several ways. Polu and Sutskever (2020) apply a Seq2Seq neural network for mathematical theorem proving by training the neural network to generate the proof at each step. Some works seek to use distributed representations to augment the rule-based backward chaining (Weber et al., 2019; Dong et al., 2018). However, these works still highly rely on the formal representations and they do not generate the natural language subgoals at each step. Problem Solver and Cognitive Architectures: Our work is also largely inspired by cognitive arTables 7 and 8 show that EVR1 yields stable perfor- chitectures such as ACT-R (Anderson et al., 1997) mance and proofs when trained with considerably and SOAR (Laird, 2012), which originate from less data (10k and 30k examples). In the lowest Newell’s GPS. These cognitive architectures emdata configuration (10k), EVR outperforms PR con- ploy sym"
2021.naacl-main.97,2020.tacl-1.13,0,0.0243725,"ernal monologue in problem solving/reasoning is mixed. Studies have been shown that internal monologue might not be crucial to visual reasoning (Phillips, 1999), whereas in verbal reasoning tasks, some subjects indeed rely more on the internal monologue than the visual imagery (Bacon et al., 2003). However, there is still not a wide concensus on the form/grammar of the internal monologue. Question Decomposition: Our work is also different from several existing works about question decomposition, where the strategy of decomposition is largely reflected by the question itself (Min et al., 2019; Wolfson et al., 2020). In contrast, the expressions of our questions are already simple, and don’t reflect the decomposition strategies. 6 6.1 Discussion and Future Work Discussion of the Current Method Does EVR solve the problems raised in Section 2.4? We believe our neural GPS at least partially solves the issues mentioned in Section 2.4. First, EVR decomposes a hard problem into several simple ones, thus resembling the human thinking process more. In addition, this modular strategy also enables EVR to suffer less from the compositionality generalization problem (as shown in Section 4). Second, during each step"
2021.naacl-main.97,D19-1260,1,0.834229,"me scales: unconscious activities take around 50 ms, whereas conscious actions can vary from 100 ms to hours. Importantly, Newell 1 Introduction (1994) argued that conscious actions are sequences of simple conscious/unconscious actions. ExtrapoLarge pretrained language models such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., lating from cognitive science to natural language processing (NLP), in this paper we ask the question: 2019) have been successfully used in multi-hop can we design an interpretable multi-hop reasoning reasoning problems (Banerjee et al., 2020; Asai et al., 2019; Yadav et al., 2019). Usually, these pre- system that sequentially applies neural networks trained on simpler tasks? Further, motivated by the trained language models solve multi-hop reasoning problems in a discriminative end-to-end manner: finding from cognitive science that people might use internal monologues to guide their reasoning, these models take the question and all the relevant we want to explore whether it is possible to use evidence as the input, and produce the final answer to the question. This raises two problems. First, natural language to guide this sequential process. In this paper, we propose"
2021.trustnlp-1.1,D12-1042,1,0.743888,"7 June 10, 2021. ©2021 Association for Computational Linguistics 3.1 Bunescu and Mooney, 2005) to neural methods. Neural approaches for RE range from methods that rely on simpler representations such as CNNs (Zeng et al., 2014) and RNNs (Zhang and Wang, 2015) to more complicated ones such as augmenting RNNs with different components (Xu et al., 2015; Zhou et al., 2016), combining RNNs and CNNs (Vu et al., 2016; Wang et al., 2016), and using mechanisms like attention (Zhang et al., 2017) or GCNs (Zhang et al., 2018). To solve the lack of annotated data, distant supervision (Mintz et al., 2009; Surdeanu et al., 2012) is commonly used to generate a training dataset from an existing knowledge base. Jat et al. (2018) address the inherent noise in distant supervision with an entity attention method. Rule-based methods in IE have also been extensively investigated. Riloff (1996) developed a system that learns extraction patterns using only a pre-classified corpus of relevant and irrelevant texts. Lin and Pantel (2001) proposed a unsupervised method for discovering inference rules from text based on the Harris distributional similarity hypothesis (Harris, 1954). Valenzuela-Escárcega et al. (2016) introduced a r"
2021.trustnlp-1.1,J13-4004,1,0.798465,"Missing"
2021.trustnlp-1.1,L16-1050,1,0.81221,"ision (Mintz et al., 2009; Surdeanu et al., 2012) is commonly used to generate a training dataset from an existing knowledge base. Jat et al. (2018) address the inherent noise in distant supervision with an entity attention method. Rule-based methods in IE have also been extensively investigated. Riloff (1996) developed a system that learns extraction patterns using only a pre-classified corpus of relevant and irrelevant texts. Lin and Pantel (2001) proposed a unsupervised method for discovering inference rules from text based on the Harris distributional similarity hypothesis (Harris, 1954). Valenzuela-Escárcega et al. (2016) introduced a rule language that covers both surface text and syntactic dependency graphs. Angeli et al. (2015) further show that converting rule-based models to statistical ones can capture some of the benefits of both, i.e., the precision of patterns and the generalizability of statistical models. Interpretability has gained more attention recently in the ML/NLP community. For example, some efforts convert neural models to more interpretable ones such as decision trees (Craven and Shavlik, 1996; Frosst and Hinton, 2017). Some others focus on producing a post-hoc explanation of individual mod"
2021.trustnlp-1.1,N16-1065,0,0.0151286,"atistical methods is well studied. Methods range from supervised, “traditional” approaches (Zelenko et al., 2003; 1 Proceedings of the First Workshop on Trustworthy Natural Language Processing, pages 1–7 June 10, 2021. ©2021 Association for Computational Linguistics 3.1 Bunescu and Mooney, 2005) to neural methods. Neural approaches for RE range from methods that rely on simpler representations such as CNNs (Zeng et al., 2014) and RNNs (Zhang and Wang, 2015) to more complicated ones such as augmenting RNNs with different components (Xu et al., 2015; Zhou et al., 2016), combining RNNs and CNNs (Vu et al., 2016; Wang et al., 2016), and using mechanisms like attention (Zhang et al., 2017) or GCNs (Zhang et al., 2018). To solve the lack of annotated data, distant supervision (Mintz et al., 2009; Surdeanu et al., 2012) is commonly used to generate a training dataset from an existing knowledge base. Jat et al. (2018) address the inherent noise in distant supervision with an entity attention method. Rule-based methods in IE have also been extensively investigated. Riloff (1996) developed a system that learns extraction patterns using only a pre-classified corpus of relevant and irrelevant texts. Lin and"
2021.trustnlp-1.1,W02-0109,0,0.187442,".6 BLEU 96.9 90.8 88.8 89.4 our word embeddings. We use the Adagrad optimizer (Duchi et al., 2011). We apply entity masking to subject and object entities in the sentence, which is replacing the original token with a special <NER&gt;–SUBJ or <NER&gt;–OBJ token where <NER&gt; is the corresponding name entity label provided by TACRED. We used micro precision, recall, and F1 scores to evaluate the RE classifier. We used the BLEU score to measure the quality of generated rules, i.e., how close they are to the corresponding gold rules that extracted the same output. We used the BLEU implementation in NLTK (Loper and Bird, 2002), which allows us to calculate multi-reference BLEU scores over 1 to 4 grams.4 We report BLEU scores only over the non ’no_relation’ extractions with the corresponding testing data points that are matched by one of the rules in (Zhang et al., 2017). Table 3: Learning curve of our approach based on amount of rules used, in the rule-only data configuration. These results are on TACRED development. our models from the patterns in the rule-based system of Angeli et al. (2015), which uses 4,528 surface patterns (in the Tokensregex language) and 169 patterns over syntactic dependencies (using Semgre"
2021.trustnlp-1.1,P14-5010,1,0.0157991,"Missing"
2021.trustnlp-1.1,P16-1123,0,0.0662666,"Missing"
2021.trustnlp-1.1,P09-1113,0,0.100288,"Processing, pages 1–7 June 10, 2021. ©2021 Association for Computational Linguistics 3.1 Bunescu and Mooney, 2005) to neural methods. Neural approaches for RE range from methods that rely on simpler representations such as CNNs (Zeng et al., 2014) and RNNs (Zhang and Wang, 2015) to more complicated ones such as augmenting RNNs with different components (Xu et al., 2015; Zhou et al., 2016), combining RNNs and CNNs (Vu et al., 2016; Wang et al., 2016), and using mechanisms like attention (Zhang et al., 2017) or GCNs (Zhang et al., 2018). To solve the lack of annotated data, distant supervision (Mintz et al., 2009; Surdeanu et al., 2012) is commonly used to generate a training dataset from an existing knowledge base. Jat et al. (2018) address the inherent noise in distant supervision with an entity attention method. Rule-based methods in IE have also been extensively investigated. Riloff (1996) developed a system that learns extraction patterns using only a pre-classified corpus of relevant and irrelevant texts. Lin and Pantel (2001) proposed a unsupervised method for discovering inference rules from text based on the Harris distributional similarity hypothesis (Harris, 1954). Valenzuela-Escárcega et a"
2021.trustnlp-1.1,D15-1206,0,0.069632,"Missing"
2021.trustnlp-1.1,D14-1162,0,0.0852618,"Missing"
2021.trustnlp-1.1,D18-1244,0,0.0657426,"al., 2003; 1 Proceedings of the First Workshop on Trustworthy Natural Language Processing, pages 1–7 June 10, 2021. ©2021 Association for Computational Linguistics 3.1 Bunescu and Mooney, 2005) to neural methods. Neural approaches for RE range from methods that rely on simpler representations such as CNNs (Zeng et al., 2014) and RNNs (Zhang and Wang, 2015) to more complicated ones such as augmenting RNNs with different components (Xu et al., 2015; Zhou et al., 2016), combining RNNs and CNNs (Vu et al., 2016; Wang et al., 2016), and using mechanisms like attention (Zhang et al., 2017) or GCNs (Zhang et al., 2018). To solve the lack of annotated data, distant supervision (Mintz et al., 2009; Surdeanu et al., 2012) is commonly used to generate a training dataset from an existing knowledge base. Jat et al. (2018) address the inherent noise in distant supervision with an entity attention method. Rule-based methods in IE have also been extensively investigated. Riloff (1996) developed a system that learns extraction patterns using only a pre-classified corpus of relevant and irrelevant texts. Lin and Pantel (2001) proposed a unsupervised method for discovering inference rules from text based on the Harris"
2021.trustnlp-1.1,D17-1004,0,0.0694139,"onal” approaches (Zelenko et al., 2003; 1 Proceedings of the First Workshop on Trustworthy Natural Language Processing, pages 1–7 June 10, 2021. ©2021 Association for Computational Linguistics 3.1 Bunescu and Mooney, 2005) to neural methods. Neural approaches for RE range from methods that rely on simpler representations such as CNNs (Zeng et al., 2014) and RNNs (Zhang and Wang, 2015) to more complicated ones such as augmenting RNNs with different components (Xu et al., 2015; Zhou et al., 2016), combining RNNs and CNNs (Vu et al., 2016; Wang et al., 2016), and using mechanisms like attention (Zhang et al., 2017) or GCNs (Zhang et al., 2018). To solve the lack of annotated data, distant supervision (Mintz et al., 2009; Surdeanu et al., 2012) is commonly used to generate a training dataset from an existing knowledge base. Jat et al. (2018) address the inherent noise in distant supervision with an entity attention method. Rule-based methods in IE have also been extensively investigated. Riloff (1996) developed a system that learns extraction patterns using only a pre-classified corpus of relevant and irrelevant texts. Lin and Pantel (2001) proposed a unsupervised method for discovering inference rules f"
2021.trustnlp-1.1,P16-2034,0,0.0178442,". 2 Related Work Relation extraction using statistical methods is well studied. Methods range from supervised, “traditional” approaches (Zelenko et al., 2003; 1 Proceedings of the First Workshop on Trustworthy Natural Language Processing, pages 1–7 June 10, 2021. ©2021 Association for Computational Linguistics 3.1 Bunescu and Mooney, 2005) to neural methods. Neural approaches for RE range from methods that rely on simpler representations such as CNNs (Zeng et al., 2014) and RNNs (Zhang and Wang, 2015) to more complicated ones such as augmenting RNNs with different components (Xu et al., 2015; Zhou et al., 2016), combining RNNs and CNNs (Vu et al., 2016; Wang et al., 2016), and using mechanisms like attention (Zhang et al., 2017) or GCNs (Zhang et al., 2018). To solve the lack of annotated data, distant supervision (Mintz et al., 2009; Surdeanu et al., 2012) is commonly used to generate a training dataset from an existing knowledge base. Jat et al. (2018) address the inherent noise in distant supervision with an entity attention method. Rule-based methods in IE have also been extensively investigated. Riloff (1996) developed a system that learns extraction patterns using only a pre-classified corpus"
C16-1278,W16-1303,1,0.778456,"ured: Study Guides: A collection of free text from six resources: study guides for two elementary science exams, a teacher’s manual, a set of flashcards, and two dictionary resources: a science dictionary for kids, and the open-domain Simple English Wiktionary6 . A total of 3,832 science-domain sentences and 17,473 open-domain definition sentences were included. Aristo TableStore: An open collection7 of approximately 100 semi-formal tables (approximately 10k rows, 30k cells) containing knowledge tailored to elementary science exams, constructed using a mixture of manual and automatic methods (Dalvi et al., 2016). The table knowledge spans across knowledge types, from properties and taxonomic knowledge to causality, processes, and domain models. Each table encodes an aspect of the science domain (e.g., animal adaptations, measuring instruments, energy conversions, etc.), where variations are typically enumerated (e.g. “a <grill&gt; converts <chemical energy&gt; to <heat energy&gt;”, “a <flashlight&gt; converts <electrical energy&gt; into <light energy&gt;”, etc.). 4.2 Solvers We characterize QA approaches from two families: a baseline that uses “learning to rank” (L2R) with information retrieval (IR) features, and more"
C16-1278,Q15-1015,1,0.491036,"Missing"
C16-1278,P14-1092,1,0.856906,"ere variations are typically enumerated (e.g. “a <grill&gt; converts <chemical energy&gt; to <heat energy&gt;”, “a <flashlight&gt; converts <electrical energy&gt; into <light energy&gt;”, etc.). 4.2 Solvers We characterize QA approaches from two families: a baseline that uses “learning to rank” (L2R) with information retrieval (IR) features, and more recent inference models. Retrieval Model: We use an L2R model which finds answers by scoring passage level evidence for each answer choice from the unstructured textual knowledge sources. Our implementation is based on the candidate ranking (CR) model described in Jansen et al. (2014). Short passages are scored based on how similar they are to the words in the question and the corresponding answer choice. The similarity scores are computed using cosine similarity of tf.idf representations of the question and passages, and used in a L2R framework to produce the final ranking of the answer choices. We created two versions of the solver: one that uses the study guide collection, and the other with a textual representation of the Aristo TableStore. Apache Lucene8 is used to index and retrieve passages. Inference Models: For inference, we use two models that operate over a stru"
C16-1278,N16-3020,0,0.00981887,"e a sub-categorization of these. Here we build upon these prior works and provide both a more fine-grained characterization of the knowledge types required to answer these questions, along with manually curated answer explanations. 2957 This allows us to compare the relative strengths and weaknesses of different QA systems from knowledge and inference requirements identified using both bottom-up (from explanations) and top-down (from questions) approaches. More broadly and with respect to explanations, there is a recent trend towards emphasizing interpretable models for machine learning (e.g. Ribeiro et al. (2016)) that are able to produce human-readable explanations for their reasoning, both to improve human trust in automated inference, as well as to verify that a given model is accurately capturing the aspects of complex reasoning required for a given task. We view this work as complementary, here characterizing the knowledge and inference requirements that an automated reasoning method for science exams must meet to assemble compelling human-readable explanations as part of the inference process. 3 Knowledge and Inference Analysis Estimating knowledge and inference requirements is challenging for m"
C16-1278,roberts-hickl-2008-scaling,0,0.0801933,"Missing"
C16-1278,D15-1080,1,\N,Missing
C18-1196,W99-0613,0,0.771162,"ch as self-training or bootstrapping (Riloff, 1996; Abney, 2007; Carlson et al., 2010a; Carlson et al., 2010b; McIntosh, 2010; Gupta and Manning, 2015, inter alia), co-training (Blum and Mitchell, 1998), or graph-based methods such as label propagation (Delalleau et al., 2005). Among these, perhaps the most-widely adopted approach in the NLP field is bootstrapping, which has been used in many applications, including information extraction (Carlson et al., 2010a; Gupta and Manning, 2014; Gupta and Manning, 2015), lexicon acquisition (Neelakantan and Collins, 2015), named entity classification (Collins and Singer, 1999) and sentiment analysis (Rao and Ravichandran, 2009). However, as mentioned, most of these approaches suffer from brittle statistics due to the sparsity of the patterns learned, and from semantic drift, due to their iterative nature. The methods we investigate in this paper address these two limitations through representation learning and one-shot learning. Auto-encoder frameworks have received considerable attention in the machine learning community recently. Such frameworks include recursive auto-encoders (Socher et al., 2011), denoising autoencoders (Vincent et al., 2008), and others. They"
C18-1196,N15-1184,0,0.0375421,"es. In the same space, teacher-student networks have shown to provide state-of-the-art semi-supervised learning results on several image processing tasks. The most prominent example of such a network is the Mean Teacher framework (Tarvainen and Valpola, 2017).To our knowledge, we are the first to apply the MT framework to an IE task (Section 6). Distributed representations of words serve as underlying representation for many NLP tasks. For example, Levy and Goldberg (2014a) generalized the word2vec algorithm (Mikolov et al., 2013) to use any arbitrary context instead of just individual words. Faruqui et al. (2015) demonstrated that embeddings learned without supervision can be retro-fitted to better conform to some semantic lexicon. Generating custom embeddings that encode more specific semantic properties has been shown to be useful for downstream tasks (Sharp et al., 2016). Riedel et al. (2013; Toutanova et al. (2015; Toutanova et al. (2016) used knowledge bases in conjunction with surface patterns to learn custom representations for relation extraction. Note, however, that most of these works that customize embeddings for a specific task rely on some form of supervision. In contrast, our approach th"
C18-1196,W14-1611,0,0.233747,"st by combining a minimal amount of labeled data with a large amount of unlabeled data. In information extraction (IE), SSL often takes the form of bootstrapping, which starts with a few seed examples, e.g., “Barack Obama” as an example of a person’s name, and continues with an iterative approach that alternates between learning extraction patterns such as word n-grams, e.g., the pattern “@ENTITY , former president” could be used to extract person names, and applying these patterns to extract the desired structures (entities, relations, etc.) (Riloff, 1996; Abney, 2007; Carlson et al., 2010b; Gupta and Manning, 2014; Gupta and Manning, 2015, inter alia). There are, however, two drawbacks to this direction. First, as learning in this iterative framework advances, the task often drifts semantically (Komachi et al., 2008) into a related but different space, e.g., from learning women names into learning flower names (McIntosh, 2010; Yangarber, 2003). Second, the statistics used to estimate the quality of the learned patterns are often brittle due to the sparsity of the extraction patterns. Recently, several semi-supervised representation learning methods have been proposed to mitigate these issues. One direc"
C18-1196,N15-1128,0,0.217689,"amount of labeled data with a large amount of unlabeled data. In information extraction (IE), SSL often takes the form of bootstrapping, which starts with a few seed examples, e.g., “Barack Obama” as an example of a person’s name, and continues with an iterative approach that alternates between learning extraction patterns such as word n-grams, e.g., the pattern “@ENTITY , former president” could be used to extract person names, and applying these patterns to extract the desired structures (entities, relations, etc.) (Riloff, 1996; Abney, 2007; Carlson et al., 2010b; Gupta and Manning, 2014; Gupta and Manning, 2015, inter alia). There are, however, two drawbacks to this direction. First, as learning in this iterative framework advances, the task often drifts semantically (Komachi et al., 2008) into a related but different space, e.g., from learning women names into learning flower names (McIntosh, 2010; Yangarber, 2003). Second, the statistics used to estimate the quality of the learned patterns are often brittle due to the sparsity of the extraction patterns. Recently, several semi-supervised representation learning methods have been proposed to mitigate these issues. One direction learns custom repres"
C18-1196,D08-1106,0,0.214198,"Barack Obama” as an example of a person’s name, and continues with an iterative approach that alternates between learning extraction patterns such as word n-grams, e.g., the pattern “@ENTITY , former president” could be used to extract person names, and applying these patterns to extract the desired structures (entities, relations, etc.) (Riloff, 1996; Abney, 2007; Carlson et al., 2010b; Gupta and Manning, 2014; Gupta and Manning, 2015, inter alia). There are, however, two drawbacks to this direction. First, as learning in this iterative framework advances, the task often drifts semantically (Komachi et al., 2008) into a related but different space, e.g., from learning women names into learning flower names (McIntosh, 2010; Yangarber, 2003). Second, the statistics used to estimate the quality of the learned patterns are often brittle due to the sparsity of the extraction patterns. Recently, several semi-supervised representation learning methods have been proposed to mitigate these issues. One direction learns custom representations for both patterns and the structures to be extracted to mitigate the sparsity mentioned above (see Section 4). A different direction, illustrated This work is licensed unde"
C18-1196,P14-2050,0,0.286754,"e Section 5). Further, we demonstrate that LNs perform very well on a complex IE task, considerably outperforming several state-of-the-art approaches. In the same space, teacher-student networks have shown to provide state-of-the-art semi-supervised learning results on several image processing tasks. The most prominent example of such a network is the Mean Teacher framework (Tarvainen and Valpola, 2017).To our knowledge, we are the first to apply the MT framework to an IE task (Section 6). Distributed representations of words serve as underlying representation for many NLP tasks. For example, Levy and Goldberg (2014a) generalized the word2vec algorithm (Mikolov et al., 2013) to use any arbitrary context instead of just individual words. Faruqui et al. (2015) demonstrated that embeddings learned without supervision can be retro-fitted to better conform to some semantic lexicon. Generating custom embeddings that encode more specific semantic properties has been shown to be useful for downstream tasks (Sharp et al., 2016). Riedel et al. (2013; Toutanova et al. (2015; Toutanova et al. (2016) used knowledge bases in conjunction with surface patterns to learn custom representations for relation extraction. Not"
C18-1196,U08-1013,0,0.022136,"4 of Mikolov et al. (2013) (skip-gram with negative sampling), but, crucially, adapted to operate over entities (rather than words), and a context consisting of the bag of all patterns that match each entity (rather than context words). The Attract term, encourage entities or patterns in the same pool to be close to each other (E.g. two person entities should be attracted to each other), whereas the Repel term encourages that the pools be mutually exclusive, which is a soft version of the counter training approach of Yangarber (2003) or the weighted mutual-exclusive bootstrapping algorithm of McIntosh and Curran (2008) (e.g., person names should be far from organization names in the semantic embedding space). They are depicted by the following two equations: Attract = X X &gt; log(σ(Vx1 Vx2 )) (2) P x1,x2∈P Repel = X P 1,P 2 if P 16=P 2 X X &gt; log(σ(−Vx1 Vx2 )) (3) x1∈P 1 x2∈P 2 where P is the entity/pattern pool for a category, x1, x2 are entities/patterns in said pool in Eq 2. and P 1, P 2 are different pools, and x1 and x2 are entities/patterns in P 1, and P 2, respectively. (2) Pattern promotion: We generate the patterns that match the entities in each pool entPoolc , rank those patterns using point-wise mu"
C18-1196,D10-1035,0,0.509762,"ing extraction patterns such as word n-grams, e.g., the pattern “@ENTITY , former president” could be used to extract person names, and applying these patterns to extract the desired structures (entities, relations, etc.) (Riloff, 1996; Abney, 2007; Carlson et al., 2010b; Gupta and Manning, 2014; Gupta and Manning, 2015, inter alia). There are, however, two drawbacks to this direction. First, as learning in this iterative framework advances, the task often drifts semantically (Komachi et al., 2008) into a related but different space, e.g., from learning women names into learning flower names (McIntosh, 2010; Yangarber, 2003). Second, the statistics used to estimate the quality of the learned patterns are often brittle due to the sparsity of the extraction patterns. Recently, several semi-supervised representation learning methods have been proposed to mitigate these issues. One direction learns custom representations for both patterns and the structures to be extracted to mitigate the sparsity mentioned above (see Section 4). A different direction, illustrated This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License detail"
C18-1196,N18-2057,1,0.620487,"θ, η)k2 (8) where this function is defined as the expected distance between the prediction of the teacher model (with 0 0 weights θ and noise η ) and the prediction of the student model (with weights θ and noise η). The student weights are updated via back propagation. The teacher weights are deterministically updated after each mini batch of gradient descent on the student, through an exponentially weighted moving average (EMA) from the previous iterations of the student, given by the following equation: 1 LNs for semi-supervised named entity classification was presented by our earlier work (Nagesh and Surdeanu, 2018). In this work, we compare LNs with MT and provide a more in-depth analysis of these frameworks under the umbrella of teacher-student architectures. 2317 0 0 θt = αθt−1 + (1 − α)θt (9) 0 where θt is defined at training step t as the EMA of successive weights θ (Tarvainen and Valpola, 2017). We explored two different architectures for the student model: Simple model (MT simple): In this model, we used bag-of-word averaging to obtain embeddings for entity mentions and the patterns matching them. In particular, we: (a) average word embeddings to obtain a single embedding for the entity mention an"
C18-1196,W13-3516,0,0.0959789,"hese representation learning methods compare against each other. In this work, we answer the above questions by adapting all these methods to the same IE task, and performing a rigorous comparative analysis. Specifically, the contributions of this paper are the following: • We are the first to adapt the mean teacher (MT) framework (Tarvainen and Valpola, 2017) to a semi-supervised IE task, in particular named entity classification (NEC). • We implement and evaluate the three semi-supervised representation learning approaches summarized above for the same NEC task, using two distinct datasets (Pradhan et al., 2013; Tjong Kim Sang and De Meulder, 2003), the same inputs, and the same evaluation measures to understand how they compare both against each other and against more traditional semi-supervised NLP approaches. • We perform a thorough comparative analysis of all these methods, which highlights several important observations. First, our analysis confirms that all these representation learning methods outperform state-of-the-art semi-supervised methods that do not rely on representation learning (Gupta and Manning, 2015; Gupta and Manning, 2014; Zhu and Ghahramani, 2002). Second, one-shot learning me"
C18-1196,E09-1077,0,0.0340818,"; Abney, 2007; Carlson et al., 2010a; Carlson et al., 2010b; McIntosh, 2010; Gupta and Manning, 2015, inter alia), co-training (Blum and Mitchell, 1998), or graph-based methods such as label propagation (Delalleau et al., 2005). Among these, perhaps the most-widely adopted approach in the NLP field is bootstrapping, which has been used in many applications, including information extraction (Carlson et al., 2010a; Gupta and Manning, 2014; Gupta and Manning, 2015), lexicon acquisition (Neelakantan and Collins, 2015), named entity classification (Collins and Singer, 1999) and sentiment analysis (Rao and Ravichandran, 2009). However, as mentioned, most of these approaches suffer from brittle statistics due to the sparsity of the patterns learned, and from semantic drift, due to their iterative nature. The methods we investigate in this paper address these two limitations through representation learning and one-shot learning. Auto-encoder frameworks have received considerable attention in the machine learning community recently. Such frameworks include recursive auto-encoders (Socher et al., 2011), denoising autoencoders (Vincent et al., 2008), and others. They are primarily used as a pre-training mechanism befor"
C18-1196,N13-1008,0,0.0188059,"first to apply the MT framework to an IE task (Section 6). Distributed representations of words serve as underlying representation for many NLP tasks. For example, Levy and Goldberg (2014a) generalized the word2vec algorithm (Mikolov et al., 2013) to use any arbitrary context instead of just individual words. Faruqui et al. (2015) demonstrated that embeddings learned without supervision can be retro-fitted to better conform to some semantic lexicon. Generating custom embeddings that encode more specific semantic properties has been shown to be useful for downstream tasks (Sharp et al., 2016). Riedel et al. (2013; Toutanova et al. (2015; Toutanova et al. (2016) used knowledge bases in conjunction with surface patterns to learn custom representations for relation extraction. Note, however, that most of these works that customize embeddings for a specific task rely on some form of supervision. In contrast, our approach that learns a custom representation for the NEC task is lightly supervised, with a only few seed examples per category (Section 4). 3 Task and Approaches Named entity classification We implement and evaluate the proposed semi-supervised learning approaches on the task of named entity clas"
C18-1196,D16-1014,1,0.850582,"nowledge, we are the first to apply the MT framework to an IE task (Section 6). Distributed representations of words serve as underlying representation for many NLP tasks. For example, Levy and Goldberg (2014a) generalized the word2vec algorithm (Mikolov et al., 2013) to use any arbitrary context instead of just individual words. Faruqui et al. (2015) demonstrated that embeddings learned without supervision can be retro-fitted to better conform to some semantic lexicon. Generating custom embeddings that encode more specific semantic properties has been shown to be useful for downstream tasks (Sharp et al., 2016). Riedel et al. (2013; Toutanova et al. (2015; Toutanova et al. (2016) used knowledge bases in conjunction with surface patterns to learn custom representations for relation extraction. Note, however, that most of these works that customize embeddings for a specific task rely on some form of supervision. In contrast, our approach that learns a custom representation for the NEC task is lightly supervised, with a only few seed examples per category (Section 4). 3 Task and Approaches Named entity classification We implement and evaluate the proposed semi-supervised learning approaches on the task"
C18-1196,D11-1014,0,0.0120753,"Neelakantan and Collins, 2015), named entity classification (Collins and Singer, 1999) and sentiment analysis (Rao and Ravichandran, 2009). However, as mentioned, most of these approaches suffer from brittle statistics due to the sparsity of the patterns learned, and from semantic drift, due to their iterative nature. The methods we investigate in this paper address these two limitations through representation learning and one-shot learning. Auto-encoder frameworks have received considerable attention in the machine learning community recently. Such frameworks include recursive auto-encoders (Socher et al., 2011), denoising autoencoders (Vincent et al., 2008), and others. They are primarily used as a pre-training mechanism before supervised training. Recently, such networks have also been used for semi-supervised learning as they are more amenable to combining supervised and unsupervised components of the objective functions (Zhai and Zhang, 2015). Ladder networks (LN) are stacked denoising auto-encoders with skip-connections in the intermediate layers (Rasmus et al., 2015; Valpola, 2014). LNs have been shown to produce state-of-the-art perfor2313 mance on both supervised and semi-supervised tasks on"
C18-1196,D15-1174,0,0.0511871,"Missing"
C18-1196,P16-1136,0,0.0210266,"(Section 6). Distributed representations of words serve as underlying representation for many NLP tasks. For example, Levy and Goldberg (2014a) generalized the word2vec algorithm (Mikolov et al., 2013) to use any arbitrary context instead of just individual words. Faruqui et al. (2015) demonstrated that embeddings learned without supervision can be retro-fitted to better conform to some semantic lexicon. Generating custom embeddings that encode more specific semantic properties has been shown to be useful for downstream tasks (Sharp et al., 2016). Riedel et al. (2013; Toutanova et al. (2015; Toutanova et al. (2016) used knowledge bases in conjunction with surface patterns to learn custom representations for relation extraction. Note, however, that most of these works that customize embeddings for a specific task rely on some form of supervision. In contrast, our approach that learns a custom representation for the NEC task is lightly supervised, with a only few seed examples per category (Section 4). 3 Task and Approaches Named entity classification We implement and evaluate the proposed semi-supervised learning approaches on the task of named entity classification (NEC), defined as identifying the corr"
C18-1196,P03-1044,0,0.702021,"atterns such as word n-grams, e.g., the pattern “@ENTITY , former president” could be used to extract person names, and applying these patterns to extract the desired structures (entities, relations, etc.) (Riloff, 1996; Abney, 2007; Carlson et al., 2010b; Gupta and Manning, 2014; Gupta and Manning, 2015, inter alia). There are, however, two drawbacks to this direction. First, as learning in this iterative framework advances, the task often drifts semantically (Komachi et al., 2008) into a related but different space, e.g., from learning women names into learning flower names (McIntosh, 2010; Yangarber, 2003). Second, the statistics used to estimate the quality of the learned patterns are often brittle due to the sparsity of the extraction patterns. Recently, several semi-supervised representation learning methods have been proposed to mitigate these issues. One direction learns custom representations for both patterns and the structures to be extracted to mitigate the sparsity mentioned above (see Section 4). A different direction, illustrated This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 2312 Pr"
D10-1048,D08-1031,0,0.846856,"s. 1 The second attack occurred after some rocket firings aimed, apparently, toward [the israelis], apparently in retaliation. [we]’re checking our facts on that one. ... the president, quoted by ari fleischer, his spokesman, is saying he’s concerned the strike will undermine efforts by palestinian authorities to bring an end to terrorist attacks and does not contribute to the security of [israel]. Introduction Recent work on coreference resolution has shown that a rich feature space that models lexical, syntactic, semantic, and discourse phenomena is crucial to successfully address the task (Bengston and Roth, 2008; Haghighi and Klein, 2009; Haghighi and Klein, 2010). When such a rich representation Most state-of-the-art models will incorrectly link we to the israelis because of their proximity and compatibility of attributes (both we and the israelis are plural). In contrast, a more cautious approach is to first cluster the israelis with israel because the demonymy relation is highly precise. This initial clustering step will assign the correct animacy attribute (inanimate) to the corresponding geo-political entity, which will prevent the incorrect merging with the mention we (animate) in later steps."
D10-1048,P06-1005,0,0.370589,"ttributes. These are crucial factors for pronominal coreference. Like previous work, we implement pronominal coreference resolution by enforcing agreement constraints between the coreferent mentions. We use the following attributes for these constraints: Number – we assign number attributes based on: (a) a static list for pronouns; (b) NER labels: mentions marked as a named entity are considered singular with the exception of organizations, which can be both singular or plural; (c) part of speech tags: NN*S tags are plural and all other NN* tags are singular; and (d) a static dictionary from (Bergsma and Lin, 2006). Gender – we assign gender attributes from static lexicons from (Bergsma and Lin, 2006; Ji and Lin, 2009). Person – we assign person attributes only to pronouns. However, we do not enforce this constraint when linking two pronouns if one appears within quotes. This is a simple heuristic for speaker detection, e.g., I and she point to the same person in “[I] voted my conscience,” [she] said. Animacy – we set animacy attributes using: (a) a static list for pronouns; (b) NER labels, e.g., PERSON is animate whereas LOCATION is not; and (c) a dictionary boostrapped from the web (Ji and Lin, 2009)."
D10-1048,J93-2003,0,0.0155393,"ls entity clusters explicitly using a mostly-unsupervised generative model. As previously mentioned, our work is not constrained by first-order or Bayesian formalisms in how it uses cluster information. Additionally, the deterministic models in our tiered model are significantly simpler, yet perform generally better than the complex inference models proposed in these works. From a high level perspective, this work falls under the theory of shaping, defined as a “method of successive approximations” for learning (Skinner, 1938). This theory is known by different names in many NLP applications: Brown et al. (1993) used simple models as “stepping stones” for more complex word alignment models; Collins (1999) used “cautious” decision list learning for named entity classification; Spitkovsky et al. (2010) used “baby steps” for unsupervised dependency parsing, etc. To the best of our knowledge, we are the first to apply this theory to coreference resolution. 3 Description of the Task Intra-document coreference resolution clusters together textual mentions within a single document based on the underlying referent entity. Mentions are usually noun phrases (NPs) headed by nominal or pronominal terminals. To f"
D10-1048,W99-0613,0,0.0599343,"Missing"
D10-1048,N07-1011,0,0.106871,"in both supervised and unsupervised learning setups (Bengston and Roth, 2008; Haghighi and Klein, 2009). Our work reinforces this observation, and extends it by proposing a novel architecture that: (a) allows easy deployment of such features, and (b) infuses global information that can be readily exploited by these features or constraints. Most coreference resolution approaches perform the task by aggregating local decisions about pairs of mentions (Bengston and Roth, 2008; Finkel and Manning, 2008; Haghighi and Klein, 2009; Stoyanov, 2010). Two recent works that diverge from this pattern are Culotta et al. (2007) and Poon and 493 Domingos (2008). They perform coreference resolution jointly for all mentions in a document, using first-order probabilistic models in either supervised or unsupervised settings. Haghighi and Klein (2010) propose a generative approach that models entity clusters explicitly using a mostly-unsupervised generative model. As previously mentioned, our work is not constrained by first-order or Bayesian formalisms in how it uses cluster information. Additionally, the deterministic models in our tiered model are significantly simpler, yet perform generally better than the complex inf"
D10-1048,P10-2007,0,0.0457833,"Missing"
D10-1048,P05-1045,1,0.02431,"onference (MUC-6) evaluation. It contains 30 documents and 2,068 mentions. We used the first corpus (ACE2004-ROTH-DEV) for development. The other corpora are reserved for testing. We parse all documents using the Stanford parser (Klein and Manning, 2003). The syntactic information is used to identify the mention head words and to define the ordering of mentions in a given sentence (detailed in the next section). For a fair comparison with previous work, we do not use gold named entity labels or mention types but, instead, take the labels provided by the Stanford named entity recognizer (NER) (Finkel et al., 2005). 3.2 Evaluation Metrics We use three evaluation metrics widely used in the literature: (a) pairwise F1 (Ghosh, 2003) – computed over mention pairs in the same entity cluster; (b) MUC (Vilain et al., 1995) – which measures how many predicted clusters need to be merged to cover the gold clusters; and (c) B3 (Amit and Baldwin, 1998) – which uses the intersection between predicted and gold clusters for a given mention to mark correct mentions and the sizes of the the predicted and gold clusters as denominators for precision and recall, respectively. We refer the interested reader to (X. Luo, 2005"
D10-1048,P08-2012,1,0.850737,"rk This work builds upon the recent observation that strong features outweigh complex models for coreference resolution, in both supervised and unsupervised learning setups (Bengston and Roth, 2008; Haghighi and Klein, 2009). Our work reinforces this observation, and extends it by proposing a novel architecture that: (a) allows easy deployment of such features, and (b) infuses global information that can be readily exploited by these features or constraints. Most coreference resolution approaches perform the task by aggregating local decisions about pairs of mentions (Bengston and Roth, 2008; Finkel and Manning, 2008; Haghighi and Klein, 2009; Stoyanov, 2010). Two recent works that diverge from this pattern are Culotta et al. (2007) and Poon and 493 Domingos (2008). They perform coreference resolution jointly for all mentions in a document, using first-order probabilistic models in either supervised or unsupervised settings. Haghighi and Klein (2010) propose a generative approach that models entity clusters explicitly using a mostly-unsupervised generative model. As previously mentioned, our work is not constrained by first-order or Bayesian formalisms in how it uses cluster information. Additionally, the"
D10-1048,D09-1120,0,0.142423,"curred after some rocket firings aimed, apparently, toward [the israelis], apparently in retaliation. [we]’re checking our facts on that one. ... the president, quoted by ari fleischer, his spokesman, is saying he’s concerned the strike will undermine efforts by palestinian authorities to bring an end to terrorist attacks and does not contribute to the security of [israel]. Introduction Recent work on coreference resolution has shown that a rich feature space that models lexical, syntactic, semantic, and discourse phenomena is crucial to successfully address the task (Bengston and Roth, 2008; Haghighi and Klein, 2009; Haghighi and Klein, 2010). When such a rich representation Most state-of-the-art models will incorrectly link we to the israelis because of their proximity and compatibility of attributes (both we and the israelis are plural). In contrast, a more cautious approach is to first cluster the israelis with israel because the demonymy relation is highly precise. This initial clustering step will assign the correct animacy attribute (inanimate) to the corresponding geo-political entity, which will prevent the incorrect merging with the mention we (animate) in later steps. We propose an unsupervised"
D10-1048,N10-1061,0,0.468981,"Missing"
D10-1048,Y09-1024,0,0.0248102,"reference resolution by enforcing agreement constraints between the coreferent mentions. We use the following attributes for these constraints: Number – we assign number attributes based on: (a) a static list for pronouns; (b) NER labels: mentions marked as a named entity are considered singular with the exception of organizations, which can be both singular or plural; (c) part of speech tags: NN*S tags are plural and all other NN* tags are singular; and (d) a static dictionary from (Bergsma and Lin, 2006). Gender – we assign gender attributes from static lexicons from (Bergsma and Lin, 2006; Ji and Lin, 2009). Person – we assign person attributes only to pronouns. However, we do not enforce this constraint when linking two pronouns if one appears within quotes. This is a simple heuristic for speaker detection, e.g., I and she point to the same person in “[I] voted my conscience,” [she] said. Animacy – we set animacy attributes using: (a) a static list for pronouns; (b) NER labels, e.g., PERSON is animate whereas LOCATION is not; and (c) a dictionary boostrapped from the web (Ji and Lin, 2009). NER label – from the Stanford NER. If we cannot detect a value, we set attributes to unknown and treat th"
D10-1048,P03-1054,1,0.0100249,"a et al., 2007; Bengston and Roth, 2008; Haghighi and Klein, 2009). It consists of 107 documents and 5,469 mentions. • ACE2004-NWIRE – the newswire subset of the ACE 2004 corpus, utilized by Poon and Domingos (2008) and Haghighi and Klein (2009) for testing. It contains 128 documents and 11,413 mentions. • MUC6-TEST – test corpus from the sixth Message Understanding Conference (MUC-6) evaluation. It contains 30 documents and 2,068 mentions. We used the first corpus (ACE2004-ROTH-DEV) for development. The other corpora are reserved for testing. We parse all documents using the Stanford parser (Klein and Manning, 2003). The syntactic information is used to identify the mention head words and to define the ordering of mentions in a given sentence (detailed in the next section). For a fair comparison with previous work, we do not use gold named entity labels or mention types but, instead, take the labels provided by the Stanford named entity recognizer (NER) (Finkel et al., 2005). 3.2 Evaluation Metrics We use three evaluation metrics widely used in the literature: (a) pairwise F1 (Ghosh, 2003) – computed over mention pairs in the same entity cluster; (b) MUC (Vilain et al., 1995) – which measures how many pr"
D10-1048,H05-1004,0,0.787257,"l., 2005). 3.2 Evaluation Metrics We use three evaluation metrics widely used in the literature: (a) pairwise F1 (Ghosh, 2003) – computed over mention pairs in the same entity cluster; (b) MUC (Vilain et al., 1995) – which measures how many predicted clusters need to be merged to cover the gold clusters; and (c) B3 (Amit and Baldwin, 1998) – which uses the intersection between predicted and gold clusters for a given mention to mark correct mentions and the sizes of the the predicted and gold clusters as denominators for precision and recall, respectively. We refer the interested reader to (X. Luo, 2005; Finkel and Manning, 2008) for an analysis of these metrics. 4 Description of the Multi-Pass Sieve Our sieve framework is implemented as a succession of independent coreference models. We first describe how each model selects candidate mentions, and then describe the models themselves. 494 4.1 Mention Processing Given a mention mi , each model may either decline to propose a solution (in the hope that one of the subsequent models will solve it) or deterministically select a single best antecedent from a list of previous mentions m1 , . . . , mi−1 . We sort candidate antecedents using syntacti"
D10-1048,D08-1068,0,0.227799,"• ACE2004-ROTH-DEV2 – development split of Bengston and Roth (2008), from the corpus used in the 2004 Automatic Content Extraction (ACE) evaluation. It contains 68 documents and 4,536 mentions. 2 We use the same corpus names as (Haghighi and Klein, 2009) to facilitate comparison with previous work. • ACE2004-CULOTTA-TEST – partition of ACE 2004 corpus reserved for testing by several previous works (Culotta et al., 2007; Bengston and Roth, 2008; Haghighi and Klein, 2009). It consists of 107 documents and 5,469 mentions. • ACE2004-NWIRE – the newswire subset of the ACE 2004 corpus, utilized by Poon and Domingos (2008) and Haghighi and Klein (2009) for testing. It contains 128 documents and 11,413 mentions. • MUC6-TEST – test corpus from the sixth Message Understanding Conference (MUC-6) evaluation. It contains 30 documents and 2,068 mentions. We used the first corpus (ACE2004-ROTH-DEV) for development. The other corpora are reserved for testing. We parse all documents using the Stanford parser (Klein and Manning, 2003). The syntactic information is used to identify the mention head words and to define the ordering of mentions in a given sentence (detailed in the next section). For a fair comparison with pr"
D10-1048,N10-1116,0,0.0131313,"er information. Additionally, the deterministic models in our tiered model are significantly simpler, yet perform generally better than the complex inference models proposed in these works. From a high level perspective, this work falls under the theory of shaping, defined as a “method of successive approximations” for learning (Skinner, 1938). This theory is known by different names in many NLP applications: Brown et al. (1993) used simple models as “stepping stones” for more complex word alignment models; Collins (1999) used “cautious” decision list learning for named entity classification; Spitkovsky et al. (2010) used “baby steps” for unsupervised dependency parsing, etc. To the best of our knowledge, we are the first to apply this theory to coreference resolution. 3 Description of the Task Intra-document coreference resolution clusters together textual mentions within a single document based on the underlying referent entity. Mentions are usually noun phrases (NPs) headed by nominal or pronominal terminals. To facilitate comparison with most of the recent previous work, we report results using gold mention boundaries. However, our approach does not make any assumptions about the underlying mentions,"
D10-1048,M95-1005,0,0.961704,"ing the Stanford parser (Klein and Manning, 2003). The syntactic information is used to identify the mention head words and to define the ordering of mentions in a given sentence (detailed in the next section). For a fair comparison with previous work, we do not use gold named entity labels or mention types but, instead, take the labels provided by the Stanford named entity recognizer (NER) (Finkel et al., 2005). 3.2 Evaluation Metrics We use three evaluation metrics widely used in the literature: (a) pairwise F1 (Ghosh, 2003) – computed over mention pairs in the same entity cluster; (b) MUC (Vilain et al., 1995) – which measures how many predicted clusters need to be merged to cover the gold clusters; and (c) B3 (Amit and Baldwin, 1998) – which uses the intersection between predicted and gold clusters for a given mention to mark correct mentions and the sizes of the the predicted and gold clusters as denominators for precision and recall, respectively. We refer the interested reader to (X. Luo, 2005; Finkel and Manning, 2008) for an analysis of these metrics. 4 Description of the Multi-Pass Sieve Our sieve framework is implemented as a succession of independent coreference models. We first describe h"
D10-1048,P09-1074,0,\N,Missing
D12-1042,P07-1073,0,0.721989,"ormation (e.g., events, binary relations, etc.) from free text, has received renewed interest in the “big data” era, when petabytes of natural-language text containing thousands of different structure types are readily available. However, traditional supervised methods are unlikely to scale in this context, as training data is either limited or nonexistent for most of these structures. One of the most promising approaches to IE that addresses this limitation is distant supervision, which generates training data automatically by aligning a database of facts with text (Craven and Kumlien, 1999; Bunescu and Mooney, 2007). In this paper we focus on distant supervision for relation extraction (RE), a subproblem of IE that addresses the extraction of labeled relations between two named entities. Figure 1 shows a simple example for a RE domain with two labels. Distant supervision introduces two modeling challenges, which we highlight in the table. The first challenge is that some training examples obtained through this heuristic are not valid, e.g., the last sentence in Figure 1 is not a correct example for any of the known labels for the tuple. The percentage of such false positives can be quite high. For exampl"
D12-1042,P05-1045,1,0.0789188,"Missing"
D12-1042,P11-1055,0,0.705827,"ns that certain labels tend to be generated jointly while others cannot be jointly assigned to the same tuple. (b) We show that MIML-RE performs competitively on two difficult domains. 2 Related Work Distant supervision for IE was introduced by Craven and Kumlien (1999), who focused on the extraction of binary relations between proteins and cells/tissues/diseases/drugs using the Yeast Protein Database as a source of distant supervision. Since then, the approach grew in popularity (Bunescu and Mooney, 2007; Bellare and McCallum, 2007; Wu and Weld, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Nguyen and Moschitti, 2011; Sun et al., 2011; Surdeanu et al., 2011a). However, most of these approaches make one or more approximations in learning. For example, most proposals heuristically transform distant supervision to traditional supervised learning (i.e., singleinstance single-label) (Bellare and McCallum, 2007; Wu and Weld, 2007; Mintz et al., 2009; Nguyen and Moschitti, 2011; Sun et al., 2011; Surdeanu 456 et al., 2011a). Bunescu and Mooney (2007) and Riedel et al. (2010) model distant supervision for relation extraction as a multi-instance single-label problem, which allows multip"
D12-1042,P09-1113,0,0.84884,"ween labels). For example, our model learns that certain labels tend to be generated jointly while others cannot be jointly assigned to the same tuple. (b) We show that MIML-RE performs competitively on two difficult domains. 2 Related Work Distant supervision for IE was introduced by Craven and Kumlien (1999), who focused on the extraction of binary relations between proteins and cells/tissues/diseases/drugs using the Yeast Protein Database as a source of distant supervision. Since then, the approach grew in popularity (Bunescu and Mooney, 2007; Bellare and McCallum, 2007; Wu and Weld, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Nguyen and Moschitti, 2011; Sun et al., 2011; Surdeanu et al., 2011a). However, most of these approaches make one or more approximations in learning. For example, most proposals heuristically transform distant supervision to traditional supervised learning (i.e., singleinstance single-label) (Bellare and McCallum, 2007; Wu and Weld, 2007; Mintz et al., 2009; Nguyen and Moschitti, 2011; Sun et al., 2011; Surdeanu 456 et al., 2011a). Bunescu and Mooney (2007) and Riedel et al. (2010) model distant supervision for relation extraction as a multi-instan"
D12-1042,P11-2048,0,0.0116906,"tend to be generated jointly while others cannot be jointly assigned to the same tuple. (b) We show that MIML-RE performs competitively on two difficult domains. 2 Related Work Distant supervision for IE was introduced by Craven and Kumlien (1999), who focused on the extraction of binary relations between proteins and cells/tissues/diseases/drugs using the Yeast Protein Database as a source of distant supervision. Since then, the approach grew in popularity (Bunescu and Mooney, 2007; Bellare and McCallum, 2007; Wu and Weld, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Nguyen and Moschitti, 2011; Sun et al., 2011; Surdeanu et al., 2011a). However, most of these approaches make one or more approximations in learning. For example, most proposals heuristically transform distant supervision to traditional supervised learning (i.e., singleinstance single-label) (Bellare and McCallum, 2007; Wu and Weld, 2007; Mintz et al., 2009; Nguyen and Moschitti, 2011; Sun et al., 2011; Surdeanu 456 et al., 2011a). Bunescu and Mooney (2007) and Riedel et al. (2010) model distant supervision for relation extraction as a multi-instance single-label problem, which allows multiple mentions for the same tup"
D12-1042,W11-0902,1,0.586201,"ot be jointly assigned to the same tuple. (b) We show that MIML-RE performs competitively on two difficult domains. 2 Related Work Distant supervision for IE was introduced by Craven and Kumlien (1999), who focused on the extraction of binary relations between proteins and cells/tissues/diseases/drugs using the Yeast Protein Database as a source of distant supervision. Since then, the approach grew in popularity (Bunescu and Mooney, 2007; Bellare and McCallum, 2007; Wu and Weld, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Nguyen and Moschitti, 2011; Sun et al., 2011; Surdeanu et al., 2011a). However, most of these approaches make one or more approximations in learning. For example, most proposals heuristically transform distant supervision to traditional supervised learning (i.e., singleinstance single-label) (Bellare and McCallum, 2007; Wu and Weld, 2007; Mintz et al., 2009; Nguyen and Moschitti, 2011; Sun et al., 2011; Surdeanu 456 et al., 2011a). Bunescu and Mooney (2007) and Riedel et al. (2010) model distant supervision for relation extraction as a multi-instance single-label problem, which allows multiple mentions for the same tuple but disallows more than one label per"
D12-1045,W99-0201,0,0.522129,"ooking at which semantic role the entity mentions can have and the verb pairs of their predicates. We confirm 490 that such features are useful but also show that the complementary features for verbal mentions lead to even better performance, especially when event and entity clusters are jointly modeled. Compared to the extensive work on entity coreference, the related problem of event coreference remains relatively under-explored, with minimal work on how entity and event coreference can be considered jointly on an open domain. Early work on event coreference for MUC (Humphreys et al., 1997; Bagga and Baldwin, 1999) focused on scenariospecific events. More recently, there have been approaches that looked at event coreference for wider domains. Chen and Ji (2009) proposed using spectral graph clustering to cluster events. Bejan and Harabagiu (2010) proposed a nonparametric Bayesian model for open-domain event resolution. However, most of this prior work focused only on event coreference, whereas we address both entities and events with a single model. Humphreys et al. (1997) considered entities as well as events, but due to the lack of a corpus annotated with event coreference, their approach was only eva"
D12-1045,P10-1143,0,0.762167,"ormance, especially when event and entity clusters are jointly modeled. Compared to the extensive work on entity coreference, the related problem of event coreference remains relatively under-explored, with minimal work on how entity and event coreference can be considered jointly on an open domain. Early work on event coreference for MUC (Humphreys et al., 1997; Bagga and Baldwin, 1999) focused on scenariospecific events. More recently, there have been approaches that looked at event coreference for wider domains. Chen and Ji (2009) proposed using spectral graph clustering to cluster events. Bejan and Harabagiu (2010) proposed a nonparametric Bayesian model for open-domain event resolution. However, most of this prior work focused only on event coreference, whereas we address both entities and events with a single model. Humphreys et al. (1997) considered entities as well as events, but due to the lack of a corpus annotated with event coreference, their approach was only evaluated implicitly in the MUC-6 template filling task. To our knowledge, the only previous work that considered entity and event coreference resolution jointly is He (2007), but limited to the medical domain and focused on just five sema"
D12-1045,J95-4004,0,0.0241778,"omic unit. On the other hand, our approach generates more training data than online learning, which trains using only the actual decisions taken during inference in each iteration (i.e., 2 We skip the pronoun sieve here because it does not affect the decisions taken during the iterative resolution steps. 493 the pair (e1 , e2 ) in step 13). After each epoch we have a new training corpus Γ, which we use to train the new linear regression model Θ’ (step 15), which is then interpolated with the old one (step 16). Our training procedure is similar in spirit to transformation based learning (TBL) (Brill, 1995). Similarly to TBL, our approach repeatedly applies the model over the training data and attempts to minimize the error rate of the current model. However, while TBL learns rules that directly minimize the current error rate, our approach achieves this indirectly, by incorporating the reduction in error rate in the score of the generated datums. This allows us to fit a linear regression to this task, which, as discussed before, is a better model for this task. Just like any hill-climbing algorithm, our approach has the risk of converging to a local maximum. To mitigate this risk, we do not ini"
D12-1045,W09-3208,0,0.403524,"ow that the complementary features for verbal mentions lead to even better performance, especially when event and entity clusters are jointly modeled. Compared to the extensive work on entity coreference, the related problem of event coreference remains relatively under-explored, with minimal work on how entity and event coreference can be considered jointly on an open domain. Early work on event coreference for MUC (Humphreys et al., 1997; Bagga and Baldwin, 1999) focused on scenariospecific events. More recently, there have been approaches that looked at event coreference for wider domains. Chen and Ji (2009) proposed using spectral graph clustering to cluster events. Bejan and Harabagiu (2010) proposed a nonparametric Bayesian model for open-domain event resolution. However, most of this prior work focused only on event coreference, whereas we address both entities and events with a single model. Humphreys et al. (1997) considered entities as well as events, but due to the lack of a corpus annotated with event coreference, their approach was only evaluated implicitly in the MUC-6 template filling task. To our knowledge, the only previous work that considered entity and event coreference resolutio"
D12-1045,N07-1030,0,0.00724113,"well studied problem with many successful techniques for identifying mention clusters (Ponzetto and Strube, 2006; Haghighi and Klein, 2009; Stoyanov et al., 2009; Haghighi and Klein, 2010; Raghunathan et al., 2010; Rahman and Ng, 2011, inter alia). Most of these techniques focus on matching compatible noun pairs using various syntactic and semantic features, with efforts targeted toward improving features and clustering models. Prior work showed that models that jointly resolve mentions across multiple entities result in better performance than simply resolving mentions in a pairwise fashion (Denis and Baldridge, 2007; Poon and Domingos, 2008; Wick et al., 2008; Lee et al., 2011, inter alia). A natural extension is to perform coreference jointly across both entities and events. Yet there has been little attempt in this direction. We know of only limited work that incorporates event-related information in entity coreference, typically by incorporating the verbs in context as features. For instance, Haghighi and Klein (2010) include the governor of the head of nominal mentions as features in their model. Rahman and Ng (2011) also used event-related information by looking at which semantic role the entity men"
D12-1045,D09-1120,0,0.0481785,"cies. • We annotate and release a new corpus with coreference relations between both entities and events across documents. The relations annotated are both intra and inter-document, which more accurately models real-world scenarios. • We evaluate our cross-document coreference resolution system on this corpus and show that our joint approach significantly outperforms two strong baselines that resolve entities and events separately. 2 Related Work Entity coreference resolution is a well studied problem with many successful techniques for identifying mention clusters (Ponzetto and Strube, 2006; Haghighi and Klein, 2009; Stoyanov et al., 2009; Haghighi and Klein, 2010; Raghunathan et al., 2010; Rahman and Ng, 2011, inter alia). Most of these techniques focus on matching compatible noun pairs using various syntactic and semantic features, with efforts targeted toward improving features and clustering models. Prior work showed that models that jointly resolve mentions across multiple entities result in better performance than simply resolving mentions in a pairwise fashion (Denis and Baldridge, 2007; Poon and Domingos, 2008; Wick et al., 2008; Lee et al., 2011, inter alia). A natural extension is to perform co"
D12-1045,N10-1061,0,0.0614507,"coreference relations between both entities and events across documents. The relations annotated are both intra and inter-document, which more accurately models real-world scenarios. • We evaluate our cross-document coreference resolution system on this corpus and show that our joint approach significantly outperforms two strong baselines that resolve entities and events separately. 2 Related Work Entity coreference resolution is a well studied problem with many successful techniques for identifying mention clusters (Ponzetto and Strube, 2006; Haghighi and Klein, 2009; Stoyanov et al., 2009; Haghighi and Klein, 2010; Raghunathan et al., 2010; Rahman and Ng, 2011, inter alia). Most of these techniques focus on matching compatible noun pairs using various syntactic and semantic features, with efforts targeted toward improving features and clustering models. Prior work showed that models that jointly resolve mentions across multiple entities result in better performance than simply resolving mentions in a pairwise fashion (Denis and Baldridge, 2007; Poon and Domingos, 2008; Wick et al., 2008; Lee et al., 2011, inter alia). A natural extension is to perform coreference jointly across both entities and events"
D12-1045,W97-1311,0,0.841147,"related information by looking at which semantic role the entity mentions can have and the verb pairs of their predicates. We confirm 490 that such features are useful but also show that the complementary features for verbal mentions lead to even better performance, especially when event and entity clusters are jointly modeled. Compared to the extensive work on entity coreference, the related problem of event coreference remains relatively under-explored, with minimal work on how entity and event coreference can be considered jointly on an open domain. Early work on event coreference for MUC (Humphreys et al., 1997; Bagga and Baldwin, 1999) focused on scenariospecific events. More recently, there have been approaches that looked at event coreference for wider domains. Chen and Ji (2009) proposed using spectral graph clustering to cluster events. Bejan and Harabagiu (2010) proposed a nonparametric Bayesian model for open-domain event resolution. However, most of this prior work focused only on event coreference, whereas we address both entities and events with a single model. Humphreys et al. (1997) considered entities as well as events, but due to the lack of a corpus annotated with event coreference, t"
D12-1045,W11-1902,1,0.774662,"Missing"
D12-1045,P98-2127,0,0.0174313,"and {acquired} in the above example is 1. E Indicator feature set to 1 if the two clusters have at least one coreferent predicate for a given role. For example, for the clusters {the man} and {the person}, extracted from the sentences helped [the man]Arg1 and helped [the person]Arg1 , the value of this feature is 1 if the two helped verbs were previously clustered together. E Cosine similarity of vectors containing words that are distributionally similar to words in the cluster mentions. We built these vectors by extracting the top-ten most-similar words in Dekang Lin’s similarity thesaurus (Lin, 1998) for all the nouns/adjectives/verbs in a cluster. For example, for the singleton cluster {a new home}, we construct this vector by expanding new and home to: {new:1, original:1, old:1, existing:1, current:1, unique:1, modern:1, different:1, special:1, major:1, small:1, home:1, house:1, apartment:1, building:1, hotel:1, residence:1, office:1, mansion:1, school:1, restaurant:1, hospital:1 }. E Cosine similarity of number, gender, animacy, and NE label vectors. For example, the number and gender vectors for the two-mention cluster {systems, a pen} are Number = {singular:1, plural:1}, Gender = {ne"
D12-1045,H05-1004,0,0.873901,"82.2 71.5 71.7 72.3 54.2 54.8 55.9 Table 4: Performance of the two baselines and our model. We report scores for entity clusters, event clusters and the complete task using five metrics. 6.2 Evaluation We use five coreference evaluation metrics widely used in the literature: MUC (Vilain et al., 1995) Link-based metric which measures how many predicted and gold clusters need to be merged to cover the gold and predicted clusters, respectively. B3 (Bagga and Baldwin, 1998) Mention-based metric which measures the proportion of overlap between predicted and gold clusters for a given mention. CEAF (Luo, 2005) Entity-based metric that, unlike B 3 , enforces a one-to-one alignment between gold and predicted clusters. We employ the entity-based version of CEAF. BLANC (Recasens and Hovy, 2011) Metric based on the Rand index (Rand, 1971) that considers both coreference and non-coreference links to address the imbalance between singleton and coreferent mentions. CoNLL F1 Average of MUC, B 3 , and CEAF-φ4 . This was the official metric in the CoNLL-2011 shared task (Pradhan et al., 2011). We followed the CoNLL-2011 evaluation methodology, that is, we removed all singleton clusters, and apposition/copular"
D12-1045,W04-2705,0,0.0538477,"al mention we consider the merge an operation between event (V) clusters; otherwise it is a merge between entity (E) clusters. We append to all entity features the suffix Proper or Common based on the type of the head word of the first mention in each of the two clusters. We use the suffix Proper only if both head words are proper nouns. paper we used a single heuristic: the possessor of a nominal event’s predicate is marked as its Arg0, e.g., Logan is the Arg0 to run in Logan’s run.4 4 A principled solution to this problem is to use an SRL system for nominal predicates trained using NomBank (Meyers et al., 2004). We will address this in future work. 494 We extracted named entity labels using the named entity recognizer from the Stanford CoreNLP suite. 6 Evaluation 6.1 Corpus The training and test data sets were derived from the EventCorefBank (ECB) corpus5 created by Bejan and Harabagiu (2010) to study event coreference since standard corpora such as OntoNotes (Pradhan et al., 2007) contain a small number of annotated event clusters. The ECB corpus consists of 482 documents from Google News clustered into 43 topics, where a topic is described as a seminal event. The reason for including comparable do"
D12-1045,passonneau-2004-computing,0,0.00676247,"ENTi hENTITY COREFID=“28”i rehab h/ENTITYi. Figure 1: Annotation example. Light verbs Verbs such as give and make followed by a noun (e.g., make an offer) were not annotated, but the noun was. Phrasal verbs We annotated the verb together with the preposition or adverb (e.g., check in). Idioms They were annotated with all their elements (e.g., booze it up). The first topic was annotated by all four annotators as burn-in. Afterwards, annotation disagreements were resolved between all annotators and the next three topics were annotated again by all four annotators to measure agreement. Following Passonneau (2004), we computed an inter-annotator agreement of α = 0.55 (Krippendorff, 2004) on these three topics, indicating moderate agreement among the annotators. Given the complexity of the task, we consider this to be a good score. For example, the average of the CoNLL F1 between any two annotators is 73.58, which is much higher than the system scores reported in the literature. After annotating the four topics, disagreements were resolved again and all the documents in the four topics were corrected to match the consensus. The rest of the corpus was split between the four annotators, and each document"
D12-1045,N06-1025,0,0.0653392,"Missing"
D12-1045,D08-1068,0,0.0185834,"any successful techniques for identifying mention clusters (Ponzetto and Strube, 2006; Haghighi and Klein, 2009; Stoyanov et al., 2009; Haghighi and Klein, 2010; Raghunathan et al., 2010; Rahman and Ng, 2011, inter alia). Most of these techniques focus on matching compatible noun pairs using various syntactic and semantic features, with efforts targeted toward improving features and clustering models. Prior work showed that models that jointly resolve mentions across multiple entities result in better performance than simply resolving mentions in a pairwise fashion (Denis and Baldridge, 2007; Poon and Domingos, 2008; Wick et al., 2008; Lee et al., 2011, inter alia). A natural extension is to perform coreference jointly across both entities and events. Yet there has been little attempt in this direction. We know of only limited work that incorporates event-related information in entity coreference, typically by incorporating the verbs in context as features. For instance, Haghighi and Klein (2010) include the governor of the head of nominal mentions as features in their model. Rahman and Ng (2011) also used event-related information by looking at which semantic role the entity mentions can have and the ve"
D12-1045,W11-1901,0,0.0618863,"998) Mention-based metric which measures the proportion of overlap between predicted and gold clusters for a given mention. CEAF (Luo, 2005) Entity-based metric that, unlike B 3 , enforces a one-to-one alignment between gold and predicted clusters. We employ the entity-based version of CEAF. BLANC (Recasens and Hovy, 2011) Metric based on the Rand index (Rand, 1971) that considers both coreference and non-coreference links to address the imbalance between singleton and coreferent mentions. CoNLL F1 Average of MUC, B 3 , and CEAF-φ4 . This was the official metric in the CoNLL-2011 shared task (Pradhan et al., 2011). We followed the CoNLL-2011 evaluation methodology, that is, we removed all singleton clusters, and apposition/copular relations before scoring. We evaluated the systems on three different settings: only on entity clusters, only on event clusters, and on the complete task, i.e., both entities and events. Note that the gold corpus separates clusters into entity and event clusters (see Table 3), but our 496 system does not make this distinction at runtime. In order to compute the entity-only and event-only scores in Table 4, we implemented the following procedure: (a) when scoring entity cluste"
D12-1045,D10-1048,1,0.937981,"ween both entities and events across documents. The relations annotated are both intra and inter-document, which more accurately models real-world scenarios. • We evaluate our cross-document coreference resolution system on this corpus and show that our joint approach significantly outperforms two strong baselines that resolve entities and events separately. 2 Related Work Entity coreference resolution is a well studied problem with many successful techniques for identifying mention clusters (Ponzetto and Strube, 2006; Haghighi and Klein, 2009; Stoyanov et al., 2009; Haghighi and Klein, 2010; Raghunathan et al., 2010; Rahman and Ng, 2011, inter alia). Most of these techniques focus on matching compatible noun pairs using various syntactic and semantic features, with efforts targeted toward improving features and clustering models. Prior work showed that models that jointly resolve mentions across multiple entities result in better performance than simply resolving mentions in a pairwise fashion (Denis and Baldridge, 2007; Poon and Domingos, 2008; Wick et al., 2008; Lee et al., 2011, inter alia). A natural extension is to perform coreference jointly across both entities and events. Yet there has been littl"
D12-1045,P11-1082,0,0.0281979,"nts across documents. The relations annotated are both intra and inter-document, which more accurately models real-world scenarios. • We evaluate our cross-document coreference resolution system on this corpus and show that our joint approach significantly outperforms two strong baselines that resolve entities and events separately. 2 Related Work Entity coreference resolution is a well studied problem with many successful techniques for identifying mention clusters (Ponzetto and Strube, 2006; Haghighi and Klein, 2009; Stoyanov et al., 2009; Haghighi and Klein, 2010; Raghunathan et al., 2010; Rahman and Ng, 2011, inter alia). Most of these techniques focus on matching compatible noun pairs using various syntactic and semantic features, with efforts targeted toward improving features and clustering models. Prior work showed that models that jointly resolve mentions across multiple entities result in better performance than simply resolving mentions in a pairwise fashion (Denis and Baldridge, 2007; Poon and Domingos, 2008; Wick et al., 2008; Lee et al., 2011, inter alia). A natural extension is to perform coreference jointly across both entities and events. Yet there has been little attempt in this dir"
D12-1045,P09-1074,0,0.0236089,"lease a new corpus with coreference relations between both entities and events across documents. The relations annotated are both intra and inter-document, which more accurately models real-world scenarios. • We evaluate our cross-document coreference resolution system on this corpus and show that our joint approach significantly outperforms two strong baselines that resolve entities and events separately. 2 Related Work Entity coreference resolution is a well studied problem with many successful techniques for identifying mention clusters (Ponzetto and Strube, 2006; Haghighi and Klein, 2009; Stoyanov et al., 2009; Haghighi and Klein, 2010; Raghunathan et al., 2010; Rahman and Ng, 2011, inter alia). Most of these techniques focus on matching compatible noun pairs using various syntactic and semantic features, with efforts targeted toward improving features and clustering models. Prior work showed that models that jointly resolve mentions across multiple entities result in better performance than simply resolving mentions in a pairwise fashion (Denis and Baldridge, 2007; Poon and Domingos, 2008; Wick et al., 2008; Lee et al., 2011, inter alia). A natural extension is to perform coreference jointly acros"
D12-1045,M95-1005,0,0.941575,"70.8 82.6 60.7 66.3 61.3 41.8 31.5 46.3 24.1 33.2 25.5 30.6 32.3 32.9 63.4 65.4 63.9 78.4 68.0 81.1 68.2 66.6 69.2 50.8 52.2 52.6 This paper Entity Event Both 60.7 62.7 61.2 70.6 62.8 75.9 65.2 62.7 67.8 55.5 62.5 53.9 74.9 73.9 79.0 63.7 67.7 64.1 39.3 34.0 45.2 29.5 33.9 30.0 33.7 33.9 35.8 66.9 67.6 67.1 79.6 78.5 82.2 71.5 71.7 72.3 54.2 54.8 55.9 Table 4: Performance of the two baselines and our model. We report scores for entity clusters, event clusters and the complete task using five metrics. 6.2 Evaluation We use five coreference evaluation metrics widely used in the literature: MUC (Vilain et al., 1995) Link-based metric which measures how many predicted and gold clusters need to be merged to cover the gold and predicted clusters, respectively. B3 (Bagga and Baldwin, 1998) Mention-based metric which measures the proportion of overlap between predicted and gold clusters for a given mention. CEAF (Luo, 2005) Entity-based metric that, unlike B 3 , enforces a one-to-one alignment between gold and predicted clusters. We employ the entity-based version of CEAF. BLANC (Recasens and Hovy, 2011) Metric based on the Rand index (Rand, 1971) that considers both coreference and non-coreference links to a"
D12-1045,P88-1014,0,0.616127,"(NPs). Focusing on NPs is a way to restrict the challenging problem of coreference resolution, but misses coreference relations like the one between hanged and his suicide in (1), and between placed and put in (2). 1. (a) One of the key suspected Mafia bosses arrested yesterday has hanged himself. (b) Police said Lo Presti had hanged himself. (c) His suicide appeared to be related to clan feuds. 2. (a) The New Orleans Saints placed Reggie Bush on the injured list on Wednesday. (b) Saints put Bush on I.R. As (1c) shows, NPs can also refer to events, and so corefer with phrases other than NPs (Webber, 1988). By being anchored in spatio-temporal dimensions, events represent the most frequent referent of verbal elements. In addition to time and location, events are characterized by their participants or arguments, which often correspond with discourse entities. This two-way feedback between events and their arguments (or entities) is the core of our approach. Since arguments play a key role in describing an event, knowing that two arguments corefer is useful for finding coreference relations between events, and knowing that two events corefer is useful for finding coreference relations between ent"
D12-1045,P95-1026,0,0.175973,", e.g., High-precision sieves Discourse processing sieve Exact string match sieve Relaxed string match sieve Precise constructs sieve (e.g., appositives) Strict head match sieves Proper head noun match sieve Relaxed head matching sieve Table 1: Deterministic sieves in step 6 of Algorithm 1. one sieve clusters together two entity mentions only when they have the same head word. Note that all these heuristics were designed for within-document coreference. They work well in our context because we apply them in individual document clusters, where the one-sense-per-discourse principle still holds (Yarowsky, 1995). Importantly, these sieves do not address verbal mentions. That is, all verbal mentions are still in singleton clusters after this step. Furthermore, none of these sieves use features that facilitate the joint resolution of nominal and verbal mentions (e.g., features from semantic role frames). All these limitations are addressed next. 3.4 Iterative Entity/Event Resolution In this stage (steps 7 – 9 in Algorithm 1), we construct entity and event clusters using a cautious or “baby steps” approach. We use a single linear regressor (Θ) to model cluster merge operations between both verbal and no"
D12-1045,C98-2122,0,\N,Missing
D16-1014,D14-1067,0,0.060924,"Missing"
D16-1014,J93-2003,0,0.0544067,"Missing"
D16-1014,D14-1082,0,0.017627,"Missing"
D16-1014,D15-1112,0,0.0413018,"Missing"
D16-1014,Q15-1015,1,0.838997,"Missing"
D16-1014,C92-2082,0,0.290538,"Missing"
D16-1014,W09-2415,0,0.0627835,"Missing"
D16-1014,P15-1162,0,0.0490213,"Missing"
D16-1014,P14-1092,1,0.907147,"Missing"
D16-1014,D15-1242,0,0.0608293,"Missing"
D16-1014,P14-2050,0,0.106352,"Missing"
D16-1014,N15-1098,0,0.0556435,"Missing"
D16-1014,P14-5010,1,0.0560812,"Missing"
D16-1014,W12-3018,0,0.0430808,"Missing"
D16-1014,J03-1002,0,0.0131617,"Missing"
D16-1014,P13-1170,0,0.22825,"Missing"
D16-1014,N13-1008,0,0.0471301,"Missing"
D16-1014,P07-1059,0,0.0875959,"Missing"
D16-1014,J11-2003,1,0.90094,"Missing"
D16-1014,L16-1050,1,0.798163,"Missing"
D16-1014,D15-1295,0,0.0532995,"Missing"
D16-1014,D14-1071,0,0.0491014,"Missing"
D16-1014,D13-1056,1,0.904776,"Missing"
D16-1014,P13-1171,0,0.0333684,"Missing"
D16-1014,P03-1003,0,\N,Missing
D16-1014,W16-2506,0,\N,Missing
D17-1313,D16-1245,0,0.0284451,"al., 2012, 2009, inter alia) for a starting point. However, most of this work focuses on how to read, not on what to read given a goal. To our knowledge, we are the first to focus on the latter task. Reinforcement learning has been used to achieve state of the art performance in several natural language processing (NLP) and information retrieval (IR) tasks. For example, RL has been used to guide IR and filter irrelevant web content (Seo and Zhang, 2000; Zhang and Seo, 2001). More recently, RL has been combined with deep learning with great success, e.g., for improving coreference resolution (Clark and Manning, 2016). Finally, RL has been used to improve the efficiency of IE by learning how to incrementally reconcile new information and help choose what to look for next (Narasimhan et al., 2016), a task close to ours. This serves as an inspiration for the work we present here, but with a critical difference: Narasimhan et al. (2016) focus on slot filling using a pre-existing template. This makes both the information integration and stopping criteria welldefined. On the other hand, in our focused reading mTOR + (Positive) Cellular Apoptosis Figure 1: Example of a graph edge encoding the relation extracted"
D17-1313,W09-1401,0,0.0166322,"Missing"
D17-1313,D16-1261,0,0.0194472,"n the latter task. Reinforcement learning has been used to achieve state of the art performance in several natural language processing (NLP) and information retrieval (IR) tasks. For example, RL has been used to guide IR and filter irrelevant web content (Seo and Zhang, 2000; Zhang and Seo, 2001). More recently, RL has been combined with deep learning with great success, e.g., for improving coreference resolution (Clark and Manning, 2016). Finally, RL has been used to improve the efficiency of IE by learning how to incrementally reconcile new information and help choose what to look for next (Narasimhan et al., 2016), a task close to ours. This serves as an inspiration for the work we present here, but with a critical difference: Narasimhan et al. (2016) focus on slot filling using a pre-existing template. This makes both the information integration and stopping criteria welldefined. On the other hand, in our focused reading mTOR + (Positive) Cellular Apoptosis Figure 1: Example of a graph edge encoding the relation extracted from the text: mTOR triggers cellular apoptosis. domain, we do not know ahead of time which new pieces of information are necessarily relevant and must be taken in context. 3 Focused"
D17-1313,W13-2001,0,0.0307521,"Missing"
D17-1313,P15-4022,1,0.890103,"Missing"
D18-1229,D14-1164,0,0.0959896,"et al., 2009), or provides a light amount of supervision up front, as in the case of bootstrapping (Angeli et al., 2015). Common techniques for bootstrapping are to use rules for incrementally classifying entities (Collins and Singer, 1999) or to use syntactic (He and Grishman, 2015) and semantic (Gupta and Manning, 2014, 2015) contextual features. However, such approaches suffer from semantic drift, as previously discussed. Considering a human-in-the-loop for IE has the potential to mitigate drift and greatly benefit performance, yet the challenge lies in minimizing human effort. The work of Angeli et al. (2014) show how to use active learning to improve distantly supervised relation extraction techniques (Surdeanu et al., 2012) through humans labeling informative relations. Werling et al. (2015) use Bayesian decision theory to minimize human cost and maximize accuracy for named entity recognition. For certain IE tasks, however, human supervision can be very noisy and thus counterproductive, especially from crowds, thus previous work has shown the importance of how to pose tasks for humans in providing labels (Liu et al., 2016), as well as automatically distinguishing simple labeling tasks from exper"
D18-1229,W99-0613,0,0.898122,"ntity, and labeling consensus across multiple users. Our results suggest that supervision acquired from the scatterplot interface, despite being noisier, yields improvements in classification performance compared with the list interface, due to a larger quantity of supervision acquired. 1 Ajay Nagesh University of Arizona ajaynagesh@email.arizona.edu Introduction One strategy for mitigating the cost of supervised learning in information extraction (IE) is to bootstrap extractors with light supervision from a few provided examples (or seeds). Most typical bootstrapping methods (Yarowsky, 1995; Collins and Singer, 1999; Abney, 2007; Carlson et al., 2010; Gupta and Manning, 2014, 2015, inter alia) are iterative in nature, and suffer from semantic drift: as the learning advances, the task often drifts semantically into a related but different space, e.g., from learning women names into learning flower names (Komachi et al., 2008; Yangarber, 2003). In such cases, a human-in-the-loop to help guide bootstrapping through active learning (AL) (Settles, 2012) can be highly beneficial. In this work, we challenge the common assumption made for AL methods in the context of IE: a visual interface that shows a list of s"
D18-1229,W16-1303,0,0.0143936,"rift: as the learning advances, the task often drifts semantically into a related but different space, e.g., from learning women names into learning flower names (Komachi et al., 2008; Yangarber, 2003). In such cases, a human-in-the-loop to help guide bootstrapping through active learning (AL) (Settles, 2012) can be highly beneficial. In this work, we challenge the common assumption made for AL methods in the context of IE: a visual interface that shows a list of samples ranked by their informativeness to the classifier is effective for building classifiers that minimize human annotator time (Dalvi et al., 2016; He and Grishman, 2015). We argue that this is an inefficient form of acquiring supervision from humans. Instead, we propose a two-dimensional (2D) scatterplot interface (rather than the one-dimensional (1D) list), where the examples to be annotated are selected by their capacity to cluster together (rather than by their informativeness to the classifier). We demonstrate that our approach leads to more data being annotated, and better overall performance for the model being learned. In particular, we focus on the task of bootstrapped named entity classification (NEC), where a classifier is tr"
D18-1229,I13-1081,0,0.0310866,"s updated based on the provided set of human labels. We study the effectiveness of two different interfaces (right): a list, and a 2D scatterplot of entities. teractive visual labeling. However, their method is focused on acquiring labels solely from humans, whereas our method studies the interplay between self-labeling and human labels in a visualization context. 3 Human-Machine Workflow for Bootstrapping We first describe the general workflow on which our study is based. We consider bootstrapping where both the human and the machine label entities in tandem, following the setup described in Fu and Grishman (2013) . More specifically, we consider the following iterative process (c.f. Fig. 1): • (S1) The model automatically classifies entities, adds them as labeled data, and updates its parameters, as detailed in §4. • (S2) The human interacts with a visual interface, driven by the model and the current set of labeled and unlabeled entities. The result of this step is a set of entities labeled by the human. We define this step a round of labeling, or just round. • (S3) The model then updates its parameters given the human-labeled entities. Yet, in this process we would like to minimize human effort whil"
D18-1229,W14-1611,0,0.294665,"lts suggest that supervision acquired from the scatterplot interface, despite being noisier, yields improvements in classification performance compared with the list interface, due to a larger quantity of supervision acquired. 1 Ajay Nagesh University of Arizona ajaynagesh@email.arizona.edu Introduction One strategy for mitigating the cost of supervised learning in information extraction (IE) is to bootstrap extractors with light supervision from a few provided examples (or seeds). Most typical bootstrapping methods (Yarowsky, 1995; Collins and Singer, 1999; Abney, 2007; Carlson et al., 2010; Gupta and Manning, 2014, 2015, inter alia) are iterative in nature, and suffer from semantic drift: as the learning advances, the task often drifts semantically into a related but different space, e.g., from learning women names into learning flower names (Komachi et al., 2008; Yangarber, 2003). In such cases, a human-in-the-loop to help guide bootstrapping through active learning (AL) (Settles, 2012) can be highly beneficial. In this work, we challenge the common assumption made for AL methods in the context of IE: a visual interface that shows a list of samples ranked by their informativeness to the classifier is"
D18-1229,N15-1128,0,0.294494,"Missing"
D18-1229,N15-3007,0,0.157779,"g advances, the task often drifts semantically into a related but different space, e.g., from learning women names into learning flower names (Komachi et al., 2008; Yangarber, 2003). In such cases, a human-in-the-loop to help guide bootstrapping through active learning (AL) (Settles, 2012) can be highly beneficial. In this work, we challenge the common assumption made for AL methods in the context of IE: a visual interface that shows a list of samples ranked by their informativeness to the classifier is effective for building classifiers that minimize human annotator time (Dalvi et al., 2016; He and Grishman, 2015). We argue that this is an inefficient form of acquiring supervision from humans. Instead, we propose a two-dimensional (2D) scatterplot interface (rather than the one-dimensional (1D) list), where the examples to be annotated are selected by their capacity to cluster together (rather than by their informativeness to the classifier). We demonstrate that our approach leads to more data being annotated, and better overall performance for the model being learned. In particular, we focus on the task of bootstrapped named entity classification (NEC), where a classifier is trained to label named ent"
D18-1229,D08-1106,0,0.0277204,"izona ajaynagesh@email.arizona.edu Introduction One strategy for mitigating the cost of supervised learning in information extraction (IE) is to bootstrap extractors with light supervision from a few provided examples (or seeds). Most typical bootstrapping methods (Yarowsky, 1995; Collins and Singer, 1999; Abney, 2007; Carlson et al., 2010; Gupta and Manning, 2014, 2015, inter alia) are iterative in nature, and suffer from semantic drift: as the learning advances, the task often drifts semantically into a related but different space, e.g., from learning women names into learning flower names (Komachi et al., 2008; Yangarber, 2003). In such cases, a human-in-the-loop to help guide bootstrapping through active learning (AL) (Settles, 2012) can be highly beneficial. In this work, we challenge the common assumption made for AL methods in the context of IE: a visual interface that shows a list of samples ranked by their informativeness to the classifier is effective for building classifiers that minimize human annotator time (Dalvi et al., 2016; He and Grishman, 2015). We argue that this is an inefficient form of acquiring supervision from humans. Instead, we propose a two-dimensional (2D) scatterplot inte"
D18-1229,N16-1104,0,0.0172167,"rmance, yet the challenge lies in minimizing human effort. The work of Angeli et al. (2014) show how to use active learning to improve distantly supervised relation extraction techniques (Surdeanu et al., 2012) through humans labeling informative relations. Werling et al. (2015) use Bayesian decision theory to minimize human cost and maximize accuracy for named entity recognition. For certain IE tasks, however, human supervision can be very noisy and thus counterproductive, especially from crowds, thus previous work has shown the importance of how to pose tasks for humans in providing labels (Liu et al., 2016), as well as automatically distinguishing simple labeling tasks from expert tasks in crowd-based task assignments (Wang et al., 2017). Our work shares a similar view of human supervision for IE, yet we instead study the impact of the annotation interface on the overall performance. The role of the human-in-the-loop for topic modeling has also been extensively explored. For instance Smith et al. (2018) consider the types of modifications that one can provide to a built topic model (Hu et al., 2014) to make the topics more meaningful, while also studying the downstream human factor implications."
D18-1229,P09-1113,0,0.160365,"eserve a certain quantity of annotated data. A consensus model for the list interface that conservatively estimates labels reduces performance despite highly accurate labels due to the small amount of annotations. In contrast, the same model for the scatterplot interface yields higher label noise, but more annotations within the same budget of time and gives the best performance. 2 Related Work Information extraction (IE) techniques commonly assume that the human supervision comes in the form of knowledge bases of facts disconnected from supporting text, as in the case of distant supervision (Mintz et al., 2009), or provides a light amount of supervision up front, as in the case of bootstrapping (Angeli et al., 2015). Common techniques for bootstrapping are to use rules for incrementally classifying entities (Collins and Singer, 1999) or to use syntactic (He and Grishman, 2015) and semantic (Gupta and Manning, 2014, 2015) contextual features. However, such approaches suffer from semantic drift, as previously discussed. Considering a human-in-the-loop for IE has the potential to mitigate drift and greatly benefit performance, yet the challenge lies in minimizing human effort. The work of Angeli et al."
D18-1229,P16-1110,0,0.0245452,"e the entropy over this distribution as a measure of unconfidence, and promote entities with the lowest entropy. 5 Supervision Interfaces We now turn to the visual interface through which humans provide supervision (step S2). We distinguish the interfaces by how entities are sampled, presented to the user, and each interface’s set of interactions for labeling. 5.1 List Interface Sampling. We sample entities through sorting them by confidence, as defined in §4.1, and sample the most unconfident entities. This form of uncertainty sampling is common in list-based interfaces (Angeli et al., 2014; Poursabzi-Sangdeh et al., 2016), as a measure of informativeness for updating the model (Settles, 2012). Presentation. We next show the 15 most uncertain entities in a 1D list-based visual interface (Fig. 2a (I)). Interactions. The user labels entities by first selecting their desired category (Fig. 2a (II)), followed by clicking on the entity in the main display. We also allow the user to select multiple entities at once, for a given category. As the user labels entities, we repopulate the display with the next set of most unconfident entities. 5.2 Scatterplot Interface Sampling. For the scatterplot interface, we aim to sa"
D18-1229,P95-1026,0,0.34651,"uracy, label quantity, and labeling consensus across multiple users. Our results suggest that supervision acquired from the scatterplot interface, despite being noisier, yields improvements in classification performance compared with the list interface, due to a larger quantity of supervision acquired. 1 Ajay Nagesh University of Arizona ajaynagesh@email.arizona.edu Introduction One strategy for mitigating the cost of supervised learning in information extraction (IE) is to bootstrap extractors with light supervision from a few provided examples (or seeds). Most typical bootstrapping methods (Yarowsky, 1995; Collins and Singer, 1999; Abney, 2007; Carlson et al., 2010; Gupta and Manning, 2014, 2015, inter alia) are iterative in nature, and suffer from semantic drift: as the learning advances, the task often drifts semantically into a related but different space, e.g., from learning women names into learning flower names (Komachi et al., 2008; Yangarber, 2003). In such cases, a human-in-the-loop to help guide bootstrapping through active learning (AL) (Settles, 2012) can be highly beneficial. In this work, we challenge the common assumption made for AL methods in the context of IE: a visual interf"
D18-1229,Q17-1001,0,0.0303909,"shares a similar view of human supervision for IE, yet we instead study the impact of the annotation interface on the overall performance. The role of the human-in-the-loop for topic modeling has also been extensively explored. For instance Smith et al. (2018) consider the types of modifications that one can provide to a built topic model (Hu et al., 2014) to make the topics more meaningful, while also studying the downstream human factor implications. Furthermore, prior work has also considered how different visual representations of topics impact a human’s understanding of topic semantics (Smith et al., 2017). Closely related is the technique of PoursabziSangdeh et al. (2016), where they highlight how different visual representations can have an impact on the effectiveness and efficiency of human labeling for document classification, comparing standard list interfaces with topic-grouped lists. Our method instead considers how 2D scatterplot interfaces, via embedding-based techniques, capture semantics for the purposes of providing labels in an IE task. The visual analytics community has also investigated the role of visual interfaces and interaction tools for annotating data in supervised learning"
D18-1229,D12-1042,1,0.790486,"15). Common techniques for bootstrapping are to use rules for incrementally classifying entities (Collins and Singer, 1999) or to use syntactic (He and Grishman, 2015) and semantic (Gupta and Manning, 2014, 2015) contextual features. However, such approaches suffer from semantic drift, as previously discussed. Considering a human-in-the-loop for IE has the potential to mitigate drift and greatly benefit performance, yet the challenge lies in minimizing human effort. The work of Angeli et al. (2014) show how to use active learning to improve distantly supervised relation extraction techniques (Surdeanu et al., 2012) through humans labeling informative relations. Werling et al. (2015) use Bayesian decision theory to minimize human cost and maximize accuracy for named entity recognition. For certain IE tasks, however, human supervision can be very noisy and thus counterproductive, especially from crowds, thus previous work has shown the importance of how to pose tasks for humans in providing labels (Liu et al., 2016), as well as automatically distinguishing simple labeling tasks from expert tasks in crowd-based task assignments (Wang et al., 2017). Our work shares a similar view of human supervision for IE"
D18-1229,D17-1205,0,0.0313025,"ve distantly supervised relation extraction techniques (Surdeanu et al., 2012) through humans labeling informative relations. Werling et al. (2015) use Bayesian decision theory to minimize human cost and maximize accuracy for named entity recognition. For certain IE tasks, however, human supervision can be very noisy and thus counterproductive, especially from crowds, thus previous work has shown the importance of how to pose tasks for humans in providing labels (Liu et al., 2016), as well as automatically distinguishing simple labeling tasks from expert tasks in crowd-based task assignments (Wang et al., 2017). Our work shares a similar view of human supervision for IE, yet we instead study the impact of the annotation interface on the overall performance. The role of the human-in-the-loop for topic modeling has also been extensively explored. For instance Smith et al. (2018) consider the types of modifications that one can provide to a built topic model (Hu et al., 2014) to make the topics more meaningful, while also studying the downstream human factor implications. Furthermore, prior work has also considered how different visual representations of topics impact a human’s understanding of topic s"
D18-1229,P03-1044,0,0.0810859,".arizona.edu Introduction One strategy for mitigating the cost of supervised learning in information extraction (IE) is to bootstrap extractors with light supervision from a few provided examples (or seeds). Most typical bootstrapping methods (Yarowsky, 1995; Collins and Singer, 1999; Abney, 2007; Carlson et al., 2010; Gupta and Manning, 2014, 2015, inter alia) are iterative in nature, and suffer from semantic drift: as the learning advances, the task often drifts semantically into a related but different space, e.g., from learning women names into learning flower names (Komachi et al., 2008; Yangarber, 2003). In such cases, a human-in-the-loop to help guide bootstrapping through active learning (AL) (Settles, 2012) can be highly beneficial. In this work, we challenge the common assumption made for AL methods in the context of IE: a visual interface that shows a list of samples ranked by their informativeness to the classifier is effective for building classifiers that minimize human annotator time (Dalvi et al., 2016; He and Grishman, 2015). We argue that this is an inefficient form of acquiring supervision from humans. Instead, we propose a two-dimensional (2D) scatterplot interface (rather than"
D19-1260,D18-1237,0,0.0165894,"l., 2018; Welbl et al., 2018; Mihaylov et al., 2018; Bauer et al., 2018; Dunn et al., 2017; Dhingra et al., 2017; Lai et al., 2017; Rajpurkar et al., 2018; Sun et al., 2019). The task of selecting justification sentences is complex for multi-hop QA, because of the additional knowledge aggregation requirement (examples of such questions and answers are shown in Figures 1 and 2). Although various neural QA methods have achieved high performance on some of these datasets (Sun et al., 2018; Trivedi et al., 2019; Tymoshenko et al., 2017; Seo et al., 2016; Wang and Jiang, 2016; De Cao et al., 2018; Back et al., 2018), we argue that more effort must be dedicated to explaining their inference process. 2578 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 2578–2589, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics In this work we propose an unsupervised algorithm for the selection of multi-hop justifications from unstructured knowledge bases (KB). Unlike other supervised selection methods (Dehghani et al., 2019; Bao et al., 2016; Lin et al., 2018; Wang e"
D19-1260,C16-1236,0,0.164837,"6; De Cao et al., 2018; Back et al., 2018), we argue that more effort must be dedicated to explaining their inference process. 2578 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 2578–2589, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics In this work we propose an unsupervised algorithm for the selection of multi-hop justifications from unstructured knowledge bases (KB). Unlike other supervised selection methods (Dehghani et al., 2019; Bao et al., 2016; Lin et al., 2018; Wang et al., 2018b,a; Tran and Niedere´ee, 2018; Trivedi et al., 2019), our approach does not require any training data for justification selection. Unlike approaches that rely on structured KBs, which are expensive to create, (Khashabi et al., 2016; Khot et al., 2017; Zhang et al., 2018; Khashabi et al., 2018b; Cui et al., 2017; Bao et al., 2016), our method operates over KBs of only unstructured texts. We demonstrate that our approach has a bigger impact on downstream QA approaches that use these justification sentences as additional signal than a strong baseline that rel"
D19-1260,D18-1454,0,0.0734574,"Missing"
D19-1260,N19-1240,0,0.0377168,"Missing"
D19-1260,D15-1075,0,0.0336108,"aining data to learn how to select justification sentences (i.e., questions and answers coupled with correct justifications); (b) methods that treat justifications as latent variables and learn jointly how to answer questions and how to select justifications from questions and answers alone; (c) approaches that rely on information retrieval to select justification sentences; and, lastly, (d) methods that do not use justification sentences at all. In the first category, previous works (e.g., (Trivedi et al., 2019)) have used entailment resources including labeled trained datasets such as SNLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2017) to train components for selecting justification sentences for QA. Other works have explicitly focused on training sentence selection components for QA models (Min et al., 2018; Lin et al., 2018; Wang et al., 2019). In datasets where gold justification sentences are not provided, researchers have trained such components by retrieving justifications from structured KBs (Cui et al., 2017; Bao et al., 2016; Zhang et al., 2016; Hao et al., 2017) such as ConceptNet (Speer et al., 2017), or from IR systems coupled 2579 with denoising components (Wang et al., 2019"
D19-1260,C18-1014,0,0.0405596,"tion sentences are not provided, researchers have trained such components by retrieving justifications from structured KBs (Cui et al., 2017; Bao et al., 2016; Zhang et al., 2016; Hao et al., 2017) such as ConceptNet (Speer et al., 2017), or from IR systems coupled 2579 with denoising components (Wang et al., 2019). While these works offer exciting directions, they all rely on training data for justifications, which is expensive to generate and may not be available in real-world use cases. The second group of methods tend to rely on reinforcement learning (Choi et al., 2017; Lai et al., 2018; Geva and Berant, 2018) or PageRank (Surdeanu et al., 2008) to learn how to select justification sentences without explicit training data. Other works have used end-to-end (mostly RNNs with attention mechanisms) QA architectures for learning to pay more attention on better justification sentences (Min et al., 2018; Seo et al., 2016; Yu et al., 2014; Gravina et al., 2018). While these approaches do not require annotated justifications, they need large amounts of question/answer pairs during training so they can discover the latent justifications. In contrast to these two directions, our approach requires no training"
D19-1260,P17-1171,0,0.0410073,"nces and the performance of the downstream QA system. The last group of QA approaches learn how to classify answers without any justification sentences (Mihaylov et al., 2018; Sun et al., 2018; Devlin et al., 2018). While this has been shown to obtain good performance for answer classification, we do not focus on it in this work because these methods cannot easily explain their inference. Note that some of the works discussed here transfer knowledge from external datasets into the QA task they address (Chung et al., 2017; Sun et al., 2018; Pan et al., 2019; Min et al., 2017; Qiu et al., 2018; Chen et al., 2017). In this work, we focus solely on the resources provided in the task itself because such compatible external resources may not be available in real-world applications of QA. 3 Approach ROCC, coupled with a QA system, operates in the following steps (illustrated in Figure 2): KBs (e.g., ARC), we retrieve the top n sentences1 from this KB using an IR query that concatenates the question and the candidate answer, similar to Clark et al. (2018); Yadav et al. (2019). We implemented this using the BM25 IR model with the default parameters in Lucene2 . For reading comprehension datasets where the qu"
D19-1260,N19-1405,0,0.0958436,"Missing"
D19-1260,P17-1020,0,0.0344093,"19). In datasets where gold justification sentences are not provided, researchers have trained such components by retrieving justifications from structured KBs (Cui et al., 2017; Bao et al., 2016; Zhang et al., 2016; Hao et al., 2017) such as ConceptNet (Speer et al., 2017), or from IR systems coupled 2579 with denoising components (Wang et al., 2019). While these works offer exciting directions, they all rely on training data for justifications, which is expensive to generate and may not be available in real-world use cases. The second group of methods tend to rely on reinforcement learning (Choi et al., 2017; Lai et al., 2018; Geva and Berant, 2018) or PageRank (Surdeanu et al., 2008) to learn how to select justification sentences without explicit training data. Other works have used end-to-end (mostly RNNs with attention mechanisms) QA architectures for learning to pay more attention on better justification sentences (Min et al., 2018; Seo et al., 2016; Yu et al., 2014; Gravina et al., 2018). While these approaches do not require annotated justifications, they need large amounts of question/answer pairs during training so they can discover the latent justifications. In contrast to these two dire"
D19-1260,N18-1143,0,0.05971,"Missing"
D19-1260,P17-1021,0,0.0307899,"rks (e.g., (Trivedi et al., 2019)) have used entailment resources including labeled trained datasets such as SNLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2017) to train components for selecting justification sentences for QA. Other works have explicitly focused on training sentence selection components for QA models (Min et al., 2018; Lin et al., 2018; Wang et al., 2019). In datasets where gold justification sentences are not provided, researchers have trained such components by retrieving justifications from structured KBs (Cui et al., 2017; Bao et al., 2016; Zhang et al., 2016; Hao et al., 2017) such as ConceptNet (Speer et al., 2017), or from IR systems coupled 2579 with denoising components (Wang et al., 2019). While these works offer exciting directions, they all rely on training data for justifications, which is expensive to generate and may not be available in real-world use cases. The second group of methods tend to rely on reinforcement learning (Choi et al., 2017; Lai et al., 2018; Geva and Berant, 2018) or PageRank (Surdeanu et al., 2008) to learn how to select justification sentences without explicit training data. Other works have used end-to-end (mostly RNNs with attentio"
D19-1260,P18-1031,0,0.0137097,"n. In particular, we employed BERT as a binary classifier operating over two texts. The first text consists of the concatenated question and answer, and the second text consists of the justification text. The classifier operates over the hidden states of the two texts, i.e., the state corresponding to the [CLS] token (Devlin et al., 2018).5 We observed empirically that pre-training the BERT classifier on all n sentences retrieved by BM25, and then fine tuning on the ROCC justifications improves performance on all datasets we experimented with. This resembles the transfer learning discussed by Howard and Ruder (2018), where the source domain would be the BM25 sentences, and the target domain the ROCC justifications. However, one important distinction is that, in our case, all this knowledge comes solely from the resources provided within each dataset, and is retrieved using unsupervised method (BM25). We conjecture that this helped mainly because the pretraining step exposed BERT to more data which, even if imperfect, is topically related to the corresponding question and answer. scored by annotator with a precision of 12 because the first justification sentence is not relevant, and a coverage of 12 becau"
D19-1260,N18-1023,0,0.315856,"de adoption of ML solutions in many fields such as healthcare, finance, and law (Samek et al., 2017; Alvarez-Melis and Jaakkola, 2017; Arras et al., 2017; Gilpin et al., 2018; Biran and Cotton, 2017) For complex natural language processing (NLP) such as question answering (QA), human readable explanations of the inference process have been proposed as a way to interpret QA models (Zhou et al., 2018). Recently, multiple datasets have been proposed for multi-hop QA, in which questions can only be answered when considering information from multiple sentences and/or documents (Clark et al., 2018; Khashabi et al., 2018a; Yang et al., 2018; Welbl et al., 2018; Mihaylov et al., 2018; Bauer et al., 2018; Dunn et al., 2017; Dhingra et al., 2017; Lai et al., 2017; Rajpurkar et al., 2018; Sun et al., 2019). The task of selecting justification sentences is complex for multi-hop QA, because of the additional knowledge aggregation requirement (examples of such questions and answers are shown in Figures 1 and 2). Although various neural QA methods have achieved high performance on some of these datasets (Sun et al., 2018; Trivedi et al., 2019; Tymoshenko et al., 2017; Seo et al., 2016; Wang and Jiang, 2016; De Cao et"
D19-1260,P17-2049,0,0.156226,"Missing"
D19-1260,Q19-1014,0,0.0420159,"ton, 2017) For complex natural language processing (NLP) such as question answering (QA), human readable explanations of the inference process have been proposed as a way to interpret QA models (Zhou et al., 2018). Recently, multiple datasets have been proposed for multi-hop QA, in which questions can only be answered when considering information from multiple sentences and/or documents (Clark et al., 2018; Khashabi et al., 2018a; Yang et al., 2018; Welbl et al., 2018; Mihaylov et al., 2018; Bauer et al., 2018; Dunn et al., 2017; Dhingra et al., 2017; Lai et al., 2017; Rajpurkar et al., 2018; Sun et al., 2019). The task of selecting justification sentences is complex for multi-hop QA, because of the additional knowledge aggregation requirement (examples of such questions and answers are shown in Figures 1 and 2). Although various neural QA methods have achieved high performance on some of these datasets (Sun et al., 2018; Trivedi et al., 2019; Tymoshenko et al., 2017; Seo et al., 2016; Wang and Jiang, 2016; De Cao et al., 2018; Back et al., 2018), we argue that more effort must be dedicated to explaining their inference process. 2578 Proceedings of the 2019 Conference on Empirical Methods in Natura"
D19-1260,C18-1181,0,0.0203987,"ere gold justification sentences are not provided, researchers have trained such components by retrieving justifications from structured KBs (Cui et al., 2017; Bao et al., 2016; Zhang et al., 2016; Hao et al., 2017) such as ConceptNet (Speer et al., 2017), or from IR systems coupled 2579 with denoising components (Wang et al., 2019). While these works offer exciting directions, they all rely on training data for justifications, which is expensive to generate and may not be available in real-world use cases. The second group of methods tend to rely on reinforcement learning (Choi et al., 2017; Lai et al., 2018; Geva and Berant, 2018) or PageRank (Surdeanu et al., 2008) to learn how to select justification sentences without explicit training data. Other works have used end-to-end (mostly RNNs with attention mechanisms) QA architectures for learning to pay more attention on better justification sentences (Min et al., 2018; Seo et al., 2016; Yu et al., 2014; Gravina et al., 2018). While these approaches do not require annotated justifications, they need large amounts of question/answer pairs during training so they can discover the latent justifications. In contrast to these two directions, our approa"
D19-1260,P18-1161,0,0.124236,"2018; Back et al., 2018), we argue that more effort must be dedicated to explaining their inference process. 2578 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 2578–2589, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics In this work we propose an unsupervised algorithm for the selection of multi-hop justifications from unstructured knowledge bases (KB). Unlike other supervised selection methods (Dehghani et al., 2019; Bao et al., 2016; Lin et al., 2018; Wang et al., 2018b,a; Tran and Niedere´ee, 2018; Trivedi et al., 2019), our approach does not require any training data for justification selection. Unlike approaches that rely on structured KBs, which are expensive to create, (Khashabi et al., 2016; Khot et al., 2017; Zhang et al., 2018; Khashabi et al., 2018b; Cui et al., 2017; Bao et al., 2016), our method operates over KBs of only unstructured texts. We demonstrate that our approach has a bigger impact on downstream QA approaches that use these justification sentences as additional signal than a strong baseline that relies on information"
D19-1260,D18-1260,0,0.473433,"finance, and law (Samek et al., 2017; Alvarez-Melis and Jaakkola, 2017; Arras et al., 2017; Gilpin et al., 2018; Biran and Cotton, 2017) For complex natural language processing (NLP) such as question answering (QA), human readable explanations of the inference process have been proposed as a way to interpret QA models (Zhou et al., 2018). Recently, multiple datasets have been proposed for multi-hop QA, in which questions can only be answered when considering information from multiple sentences and/or documents (Clark et al., 2018; Khashabi et al., 2018a; Yang et al., 2018; Welbl et al., 2018; Mihaylov et al., 2018; Bauer et al., 2018; Dunn et al., 2017; Dhingra et al., 2017; Lai et al., 2017; Rajpurkar et al., 2018; Sun et al., 2019). The task of selecting justification sentences is complex for multi-hop QA, because of the additional knowledge aggregation requirement (examples of such questions and answers are shown in Figures 1 and 2). Although various neural QA methods have achieved high performance on some of these datasets (Sun et al., 2018; Trivedi et al., 2019; Tymoshenko et al., 2017; Seo et al., 2016; Wang and Jiang, 2016; De Cao et al., 2018; Back et al., 2018), we argue that more effort must"
D19-1260,P17-2081,0,0.0314461,"e quality of the justification sentences and the performance of the downstream QA system. The last group of QA approaches learn how to classify answers without any justification sentences (Mihaylov et al., 2018; Sun et al., 2018; Devlin et al., 2018). While this has been shown to obtain good performance for answer classification, we do not focus on it in this work because these methods cannot easily explain their inference. Note that some of the works discussed here transfer knowledge from external datasets into the QA task they address (Chung et al., 2017; Sun et al., 2018; Pan et al., 2019; Min et al., 2017; Qiu et al., 2018; Chen et al., 2017). In this work, we focus solely on the resources provided in the task itself because such compatible external resources may not be available in real-world applications of QA. 3 Approach ROCC, coupled with a QA system, operates in the following steps (illustrated in Figure 2): KBs (e.g., ARC), we retrieve the top n sentences1 from this KB using an IR query that concatenates the question and the candidate answer, similar to Clark et al. (2018); Yadav et al. (2019). We implemented this using the BM25 IR model with the default parameters in Lucene2 . For readi"
D19-1260,P18-1160,0,0.0435189,"uestions and how to select justifications from questions and answers alone; (c) approaches that rely on information retrieval to select justification sentences; and, lastly, (d) methods that do not use justification sentences at all. In the first category, previous works (e.g., (Trivedi et al., 2019)) have used entailment resources including labeled trained datasets such as SNLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2017) to train components for selecting justification sentences for QA. Other works have explicitly focused on training sentence selection components for QA models (Min et al., 2018; Lin et al., 2018; Wang et al., 2019). In datasets where gold justification sentences are not provided, researchers have trained such components by retrieving justifications from structured KBs (Cui et al., 2017; Bao et al., 2016; Zhang et al., 2016; Hao et al., 2017) such as ConceptNet (Speer et al., 2017), or from IR systems coupled 2579 with denoising components (Wang et al., 2019). While these works offer exciting directions, they all rely on training data for justifications, which is expensive to generate and may not be available in real-world use cases. The second group of methods tend"
D19-1260,D19-5804,0,0.0334415,"ortant for both the quality of the justification sentences and the performance of the downstream QA system. The last group of QA approaches learn how to classify answers without any justification sentences (Mihaylov et al., 2018; Sun et al., 2018; Devlin et al., 2018). While this has been shown to obtain good performance for answer classification, we do not focus on it in this work because these methods cannot easily explain their inference. Note that some of the works discussed here transfer knowledge from external datasets into the QA task they address (Chung et al., 2017; Sun et al., 2018; Pan et al., 2019; Min et al., 2017; Qiu et al., 2018; Chen et al., 2017). In this work, we focus solely on the resources provided in the task itself because such compatible external resources may not be available in real-world applications of QA. 3 Approach ROCC, coupled with a QA system, operates in the following steps (illustrated in Figure 2): KBs (e.g., ARC), we retrieve the top n sentences1 from this KB using an IR query that concatenates the question and the candidate answer, similar to Clark et al. (2018); Yadav et al. (2019). We implemented this using the BM25 IR model with the default parameters in L"
D19-1260,P18-2034,0,0.0225965,"ustification sentences and the performance of the downstream QA system. The last group of QA approaches learn how to classify answers without any justification sentences (Mihaylov et al., 2018; Sun et al., 2018; Devlin et al., 2018). While this has been shown to obtain good performance for answer classification, we do not focus on it in this work because these methods cannot easily explain their inference. Note that some of the works discussed here transfer knowledge from external datasets into the QA task they address (Chung et al., 2017; Sun et al., 2018; Pan et al., 2019; Min et al., 2017; Qiu et al., 2018; Chen et al., 2017). In this work, we focus solely on the resources provided in the task itself because such compatible external resources may not be available in real-world applications of QA. 3 Approach ROCC, coupled with a QA system, operates in the following steps (illustrated in Figure 2): KBs (e.g., ARC), we retrieve the top n sentences1 from this KB using an IR query that concatenates the question and the candidate answer, similar to Clark et al. (2018); Yadav et al. (2019). We implemented this using the BM25 IR model with the default parameters in Lucene2 . For reading comprehension d"
D19-1260,P18-2124,0,0.0844555,"Missing"
D19-1260,P08-1082,1,0.737225,"earchers have trained such components by retrieving justifications from structured KBs (Cui et al., 2017; Bao et al., 2016; Zhang et al., 2016; Hao et al., 2017) such as ConceptNet (Speer et al., 2017), or from IR systems coupled 2579 with denoising components (Wang et al., 2019). While these works offer exciting directions, they all rely on training data for justifications, which is expensive to generate and may not be available in real-world use cases. The second group of methods tend to rely on reinforcement learning (Choi et al., 2017; Lai et al., 2018; Geva and Berant, 2018) or PageRank (Surdeanu et al., 2008) to learn how to select justification sentences without explicit training data. Other works have used end-to-end (mostly RNNs with attention mechanisms) QA architectures for learning to pay more attention on better justification sentences (Min et al., 2018; Seo et al., 2016; Yu et al., 2014; Gravina et al., 2018). While these approaches do not require annotated justifications, they need large amounts of question/answer pairs during training so they can discover the latent justifications. In contrast to these two directions, our approach requires no training data at all for the justification se"
D19-1260,N19-1302,0,0.213355,"Missing"
D19-1260,D17-1093,0,0.0166111,"iple sentences and/or documents (Clark et al., 2018; Khashabi et al., 2018a; Yang et al., 2018; Welbl et al., 2018; Mihaylov et al., 2018; Bauer et al., 2018; Dunn et al., 2017; Dhingra et al., 2017; Lai et al., 2017; Rajpurkar et al., 2018; Sun et al., 2019). The task of selecting justification sentences is complex for multi-hop QA, because of the additional knowledge aggregation requirement (examples of such questions and answers are shown in Figures 1 and 2). Although various neural QA methods have achieved high performance on some of these datasets (Sun et al., 2018; Trivedi et al., 2019; Tymoshenko et al., 2017; Seo et al., 2016; Wang and Jiang, 2016; De Cao et al., 2018; Back et al., 2018), we argue that more effort must be dedicated to explaining their inference process. 2578 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 2578–2589, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics In this work we propose an unsupervised algorithm for the selection of multi-hop justifications from unstructured knowledge bases (KB). Unlike other supervised sel"
D19-1260,K19-1065,0,0.405005,"tions from questions and answers alone; (c) approaches that rely on information retrieval to select justification sentences; and, lastly, (d) methods that do not use justification sentences at all. In the first category, previous works (e.g., (Trivedi et al., 2019)) have used entailment resources including labeled trained datasets such as SNLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2017) to train components for selecting justification sentences for QA. Other works have explicitly focused on training sentence selection components for QA models (Min et al., 2018; Lin et al., 2018; Wang et al., 2019). In datasets where gold justification sentences are not provided, researchers have trained such components by retrieving justifications from structured KBs (Cui et al., 2017; Bao et al., 2016; Zhang et al., 2016; Hao et al., 2017) such as ConceptNet (Speer et al., 2017), or from IR systems coupled 2579 with denoising components (Wang et al., 2019). While these works offer exciting directions, they all rely on training data for justifications, which is expensive to generate and may not be available in real-world use cases. The second group of methods tend to rely on reinforcement learning (Cho"
D19-1260,P18-1178,0,0.0511533,"Missing"
D19-1260,Q18-1021,0,0.0574361,"such as healthcare, finance, and law (Samek et al., 2017; Alvarez-Melis and Jaakkola, 2017; Arras et al., 2017; Gilpin et al., 2018; Biran and Cotton, 2017) For complex natural language processing (NLP) such as question answering (QA), human readable explanations of the inference process have been proposed as a way to interpret QA models (Zhou et al., 2018). Recently, multiple datasets have been proposed for multi-hop QA, in which questions can only be answered when considering information from multiple sentences and/or documents (Clark et al., 2018; Khashabi et al., 2018a; Yang et al., 2018; Welbl et al., 2018; Mihaylov et al., 2018; Bauer et al., 2018; Dunn et al., 2017; Dhingra et al., 2017; Lai et al., 2017; Rajpurkar et al., 2018; Sun et al., 2019). The task of selecting justification sentences is complex for multi-hop QA, because of the additional knowledge aggregation requirement (examples of such questions and answers are shown in Figures 1 and 2). Although various neural QA methods have achieved high performance on some of these datasets (Sun et al., 2018; Trivedi et al., 2019; Tymoshenko et al., 2017; Seo et al., 2016; Wang and Jiang, 2016; De Cao et al., 2018; Back et al., 2018), we argue"
D19-1260,N19-1274,1,0.753069,"RNNs with attention mechanisms) QA architectures for learning to pay more attention on better justification sentences (Min et al., 2018; Seo et al., 2016; Yu et al., 2014; Gravina et al., 2018). While these approaches do not require annotated justifications, they need large amounts of question/answer pairs during training so they can discover the latent justifications. In contrast to these two directions, our approach requires no training data at all for the justification selection process. The third category of methods utilize IR techniques to retrieve justifications from both unstructured (Yadav et al., 2019) and structured (Khashabi et al., 2016) KBs. Our approach is closer in spirit to this direction, but it is adjusted to account for more intentional knowledge aggregation. As we show in Section 4, this is important for both the quality of the justification sentences and the performance of the downstream QA system. The last group of QA approaches learn how to classify answers without any justification sentences (Mihaylov et al., 2018; Sun et al., 2018; Devlin et al., 2018). While this has been shown to obtain good performance for answer classification, we do not focus on it in this work because"
D19-1260,D18-1259,0,0.0369764,"ons in many fields such as healthcare, finance, and law (Samek et al., 2017; Alvarez-Melis and Jaakkola, 2017; Arras et al., 2017; Gilpin et al., 2018; Biran and Cotton, 2017) For complex natural language processing (NLP) such as question answering (QA), human readable explanations of the inference process have been proposed as a way to interpret QA models (Zhou et al., 2018). Recently, multiple datasets have been proposed for multi-hop QA, in which questions can only be answered when considering information from multiple sentences and/or documents (Clark et al., 2018; Khashabi et al., 2018a; Yang et al., 2018; Welbl et al., 2018; Mihaylov et al., 2018; Bauer et al., 2018; Dunn et al., 2017; Dhingra et al., 2017; Lai et al., 2017; Rajpurkar et al., 2018; Sun et al., 2019). The task of selecting justification sentences is complex for multi-hop QA, because of the additional knowledge aggregation requirement (examples of such questions and answers are shown in Figures 1 and 2). Although various neural QA methods have achieved high performance on some of these datasets (Sun et al., 2018; Trivedi et al., 2019; Tymoshenko et al., 2017; Seo et al., 2016; Wang and Jiang, 2016; De Cao et al., 2018; Back et"
D19-1260,C18-1171,0,0.0410304,"Missing"
D19-1340,P18-1246,0,0.0312662,"Missing"
D19-1340,W03-1022,0,0.0925695,"date 27 April 2005 occurs only in the evidence, and hence it is replaced with date-e1. Importantly, we create pseudo-pretrained embeddings for these new OA-NER-based tokens by adding a small amount of random Gaussian noise (mean 0 and variance of 0.1) to pre-trained embeddings (Pennington et al., 2014) of the root word corresponding to the category (e.g., person). Thus the embeddings of all the sub-tags, while being unique, are close to that of the root word. OA-NER + Super Sense (SS) Tags: Super-sense tagging is a sequence modeling approach that annotates phrases with coarse WordNet senses (Ciaramita and Johnson, 2003; Miller et al., 1990). In this masking method, we not only replace named entities with their OA-NER tags, but also replace other lexical items with their corresponding super sense tags, if found. As with the OA-NER approach, the lexical overlap is also explicitly marked for all these tags with unique ids (see Table 2). 5 Results In Table 3 we provide the fact verification results of the state-of-the-art DA model trained with each of our masking approaches, as well as with the original fully lexicalized input. First, we note that indeed the fully lexicalized model, which performs well in-domai"
D19-1340,I08-3008,0,0.104768,"Missing"
D19-1340,P14-5010,1,0.025145,"nfirms our observation that these models anchor themselves on lexical information that is more likely to be domain dependent. (3) To mitigate this dependence on lexicalized information, we experiment with several strategies for delexicalization, i.e., where lexical tokens are replaced (or masked) with indicators of their class. While the technique of delexicalization/masking has been used before (Zeman and Resnik, 2008, e.g.,), here we expand it by incorporating semantic information. In particular, we first replace named entities with their corresponding semantic tags from Stanford’s CoreNLP (Manning et al., 2014). To keep track of which entities are referenced between claim and evidence sentences, we extend these tags with unique identifiers. Second, we similarly replace other word classes in the sentence (common nouns, verbs, adjectives, and adverbs) with their super sense tags (Ciaramita and Johnson, 2003). (4) The evaluation of the proposed masking strategy on the two fact verification datasets indicates that, while the in-domain performance remains on par with that of the model trained on the original, lexicalized data, it improves considerably when tested in the out-of-domain dataset. For example"
D19-1340,D16-1244,0,0.188665,"Missing"
D19-1340,D14-1162,0,0.0820776,"e). For example, in the claim-evidence pair shown in Table 2, when the named entity Singapore Airlines appears in the claim it is replaced with organization-c1, since it is the first organization to appear in claim. The same id is used wherever the same entity is seen again, e.g., in the evidence sentence. However, the date 27 April 2005 occurs only in the evidence, and hence it is replaced with date-e1. Importantly, we create pseudo-pretrained embeddings for these new OA-NER-based tokens by adding a small amount of random Gaussian noise (mean 0 and variance of 0.1) to pre-trained embeddings (Pennington et al., 2014) of the root word corresponding to the category (e.g., person). Thus the embeddings of all the sub-tags, while being unique, are close to that of the root word. OA-NER + Super Sense (SS) Tags: Super-sense tagging is a sequence modeling approach that annotates phrases with coarse WordNet senses (Ciaramita and Johnson, 2003; Miller et al., 1990). In this masking method, we not only replace named entities with their OA-NER tags, but also replace other lexical items with their corresponding super sense tags, if found. As with the OA-NER approach, the lexical overlap is also explicitly marked for a"
D19-1340,N18-1074,0,0.0673256,"Missing"
D19-6212,L16-1472,1,0.926258,"rends over time. Specifically, using pointwise mutual information (PMI) and a custom-built collection of healthy/unhealthy food words, we investigate the strength of healthy/unhealthy food references on Twitter, and observe a downward trend for healthy food references and an upward trend for unhealthy food words in the US. 4. Lastly, we provide a visualization tool to help understand and visualize semantic relations between words and various categories such as how different genders refer to vegetarian vs. lowcarb diets.8 Our tool is based on semantic axes plots (Heimerl and Gleicher, 2018). 2 Bell et al. (2016) proposed a strategy that uses a game-like quiz with data and questions acquired semi-automatically from Twitter to acquire relevant training data necessary to detect individual T2DM risk. In following work, Bell et al. (2018) predicted individual T2DM risk using a neural approach, which incorporates tweet texts with gender information and information about the recency of posts. Sadeque et al. (2018) discussed several approaches for predicting depression status from a user’s social media posts. They proposed a new latency-based F1 metric to measure the quality and speed of the model. Further,"
D19-6212,W18-5601,1,0.853262,"downward trend for healthy food references and an upward trend for unhealthy food words in the US. 4. Lastly, we provide a visualization tool to help understand and visualize semantic relations between words and various categories such as how different genders refer to vegetarian vs. lowcarb diets.8 Our tool is based on semantic axes plots (Heimerl and Gleicher, 2018). 2 Bell et al. (2016) proposed a strategy that uses a game-like quiz with data and questions acquired semi-automatically from Twitter to acquire relevant training data necessary to detect individual T2DM risk. In following work, Bell et al. (2018) predicted individual T2DM risk using a neural approach, which incorporates tweet texts with gender information and information about the recency of posts. Sadeque et al. (2018) discussed several approaches for predicting depression status from a user’s social media posts. They proposed a new latency-based F1 metric to measure the quality and speed of the model. Further, they re-implemented some of the common approaches for this task, and analyzed their results using their proposed metric. Lastly, they introduced a window-based technique that trades off between latency and precision in predict"
D19-6212,P15-1162,0,0.060567,"Missing"
E09-1029,W06-1670,1,0.843047,"Missing"
E09-1029,E06-1021,0,0.0510284,"Missing"
E09-1029,H05-1115,0,0.326597,"o a value of 5.0 on the development set). (1) 4 Relatedness to Query where c is the business summary of a company, tfw,c is the frequency of w in c, N is the total number of business summaries we have, cfw is the number of summaries that contain w. This formula penalizes words occurring in most summaries (e.g., company, produce, offer, operate, found, headquarter, management). At the moment of running the experiments, N was about 3,000, slightly less than the total number of symOnce the expanded query is generated, it can be used for sentence ranking. We chose the system of Otterbacher et al. (2005) as a a starting point for our approach and also as a competitive baseline because it has been successfully tested in a similar setting–it has been applied to multi-document query-focused summarization of news documents. Given a graph G = (S, E), where S is the set of all sentences from all input documents, and E is the set of edges representing normalized sentence similarities, Otterbacher et al. (2005) rank all sen6 http://finance.yahoo.com/q/pr?s=AAPL where the trading symbol of any company can be used instead of AAPL. 248 AAPL apple music mac software ipod computer peripheral movie player"
E09-1029,N06-1025,0,0.0331696,"Missing"
E09-1029,W00-0403,0,0.0405635,"roduced a forecasting system for prediction markets that combines news analysis with a price trend analysis model. This approach was shown to be successful for the forecasting of public opinion about political candidates in such prediction markets. Our approach can be seen as a complement to both these approaches, necessary especially for financial markets where the news typically cover many events, only some related to the company of interest. Unsupervized summarization systems extract sentences whose relevance can be inferred from the inter-sentence relations in the document collection. In (Radev et al., 2000), the centroid of the collection, i.e., the words with the highest TF*IDF, is considered and the sentences which contain more words from the centroid are extracted. Mihalcea & Tarau (2004) explore several methods developed for ranking documents in information retrieval for the single-document summarization task. Similarly, Erkan & Radev (2004) apply in-degree and PageRank to build a summary from a collection of related documents. They show that their method, called LexRank, achieves good results. In (Otterbacher et al., 2005; Erkan, 2006) the ranking function of LexRank is extended to become a"
E09-1029,C08-1060,0,0.0424509,"Missing"
E09-1029,P07-1059,0,0.0220542,"o experiment with positional features which have proven useful for generic summarization. We also plan to test the system extrinsically. For example, it would be of interest to see if a classifier may predict the move of stock prices based on a set of features extracted from company-oriented summaries. Query expansion has been used for improving information retrieval (IR) or question answering (QA) systems with mixed results. One of the problems is that the queries are expanded word by word, ignoring the context and as a result the extensions often become inadequate7 . However, Riezler et al. (2007) take the entire query into account when adding new words by utilizing techniques used in statistical machine translation. Query expansion for summarization has not yet been explored as extensively as in IR or QA. Nastase (2008) uses Wikipedia and WordNet for query expansion and proposes that a concept can be expanded by adding the text of all hyperlinks from the first paragraph of the Wikipedia article about this concept. The automatic evaluation demonstrates that extracting relevant concepts from Wikipedia leads to better performance compared with WordNet: both expansion systems outperform t"
E09-1029,N03-1020,0,0.0914463,"ences based on their relatedness to the expanded query and their overall importance (Section 4). Finally, the most relevant sentences are re-ranked based on the degree of novelty they carry (Section 5). The paper makes the following contributions. First, we present a new query expansion technique which is useful in the context of companydependent news summarization as it helps identify sentences important to the company. Second, we introduce a simple and efficient method for sentence ranking which foregrounds novel information of interest. Our system performs well in terms of the ROUGE score (Lin & Hovy, 2003) compared with a competitive baseline (Section 6). 2 3 Query Expansion In company-oriented summarization query expansion is crucial because, by default, our query contains only the symbol, that is the abbreviation of the name of the company. Unfortunately, existing query expansion techniques which utilize such knowledge sources as WordNet or Wikipedia are not useful for symbol expansion. WordNet does not include organizations in any systematic way. Wikipedia covers many companies but it is unclear how it can be used for expansion. 3 http://finance.yahoo.com http://biz.yahoo.com, http://www. se"
E09-1029,D08-1080,0,0.0836614,"t al. (1998) were among the first who introduced an automatic stock indices prediction system which relies on textual information only. The system generates weighted rules each of which returns the probability of the stock going up, down or remaining steady. The only information used in the rules is the presence or absence of certain keyphrases provided by a human expert who “judged them to be influential factors potentially moving stock markets”. In this approach, training data is required to measure the usefulness of the keyphrases for each of the three classes. More recently, Lerman et al. (2008) introduced a forecasting system for prediction markets that combines news analysis with a price trend analysis model. This approach was shown to be successful for the forecasting of public opinion about political candidates in such prediction markets. Our approach can be seen as a complement to both these approaches, necessary especially for financial markets where the news typically cover many events, only some related to the company of interest. Unsupervized summarization systems extract sentences whose relevance can be inferred from the inter-sentence relations in the document collection."
E09-1029,W04-3252,0,\N,Missing
E09-1029,atserias-etal-2008-semantically,1,\N,Missing
H05-1081,W04-2412,1,0.881998,"Missing"
H05-1081,W05-0620,1,0.859115,"Missing"
H05-1081,W04-2415,1,0.848646,"with clause nor chunk boundaries, discard ARG0-5 arguments not present in PropBank frames for a certain verb, etc. 1 Features extracted from partial parsing and Named Entities are common to Model 1 and 2, while features coming from full parse trees only apply to Model 2. Relative position, distance in words and chunks, and level of embedding (in #clause-levels) with respect to the constituent. Constituent path as described in (Gildea and Jurafsky, 2002) and all 3/4/5-grams of path constituents beginning at the verb predicate or ending at the constituent. Partial parsing path as described in (Carreras et al., 2004) and all 3/4/5-grams of path elements beginning at the verb predicate or ending at the constituent. Syntactic frame as described by Xue and Palmer (2004) Table 3: Predicate–constituent features: Models 1/2 The syntactic label of the candidate constituent. The constituent head word, suffixes of length 2, 3, and 4, lemma, and POS tag. The constituent content word, suffixes of length 2, 3, and 4, lemma, POS tag, and NE label. Content words, which add informative lexicalized information different from the head word, were detected using the heuristics of (Surdeanu et al., 2003). The first and last"
H05-1081,J02-3001,0,0.704534,"cantly boost results of individual systems. This combination scheme is also very flexible since the individual systems are not required to provide any information other than their solution. Extensive experimental evaluation in the CoNLL2005 shared task framework supports our previous claims. The proposed architecture outperforms the best results reported in that evaluation exercise. 1 Introduction The task of Semantic Role Labeling (SRL), i.e. the process of detecting basic event structures such as who did what to whom, when and where, has received considerable interest in the past few years (Gildea and Jurafsky, 2002; Surdeanu et al., 2003; Xue and Palmer, 2004; Pradhan et al., 2005a; Carreras and M`arquez, 2005). It was shown that the identification of such event frames has a significant contribution for many Natural Language Processing (NLP) applications such as Information Extraction (Surdeanu et al., 2003) and Question Answering (Narayanan and Harabagiu, 2004). Most current SRL approaches can be classified in one of two classes: approaches that take adOn the other hand, when only automaticallygenerated syntax is available, the quality of the information provided through full syntax decreases because t"
H05-1081,W05-0623,0,0.094927,"didates is performed in an elegant global inference procedure as constraint satisfaction, which, formulated as Integer Linear Programming, can be solved efficiently. Interestingly, the generalized inference layer allows to include in the objective function, jointly with the candidate argument scores, a number of linguistically-motivated constraints to obtain a coherent solution. Differing from the strategy presented in this paper, their inference layer does not include learning. Also, they require confidence values from individual classifiers. This is the best performing system at CoNLL-2005. Haghighi et al. (2005) implemented a double reranking model on top of the base SRL models to select the most probable solution among a set of candidates. The re-ranking is performed, first, on a set of n-best solutions obtained by the base system run on a single parse tree, and, then, on the set of bestcandidates coming from the n-best parse trees. The re-ranking approach allows to define global complex features applying to complete candidate solutions to train the rankers. The main drawback, compared to our approach, is that re-ranking does not permit to combine different solutions since it is forced to select a c"
H05-1081,W05-0625,0,0.249114,"ses only global attributes extracted from the solutions provided by the individual systems, e.g., the sequence of role labels generated by each system for the current predicate. We do not use any attributes specific to the individual models, not even the confidence assigned by the individual classifiers. Besides simplicity, the consequence of this decision is that our approach does not impose any restrictions on the individual SRL strategies, as long as one solution is provided for each predicate. On the other hand, probabilistic inference processes, which have been successfully used for SRL (Koomen et al., 2005), mandate that each individual candidate argument be associated with its raw activation, or confidence, in the given model. However, this information is not directly available in two out of three of our individual models, which classify argument chunks and not entire arguments. Despite its simplicity, our approach obtains encouraging results: the combined system outperforms any of the individual systems and, using exactly the same data, it is also competitive with the best SRL systems that participated in the latest CoNLL shared task evaluation (Carreras and M`arquez, 2005). 2 Semantic Corpora"
H05-1081,W05-0628,1,0.868909,"Missing"
H05-1081,C04-1100,0,0.0556393,"eported in that evaluation exercise. 1 Introduction The task of Semantic Role Labeling (SRL), i.e. the process of detecting basic event structures such as who did what to whom, when and where, has received considerable interest in the past few years (Gildea and Jurafsky, 2002; Surdeanu et al., 2003; Xue and Palmer, 2004; Pradhan et al., 2005a; Carreras and M`arquez, 2005). It was shown that the identification of such event frames has a significant contribution for many Natural Language Processing (NLP) applications such as Information Extraction (Surdeanu et al., 2003) and Question Answering (Narayanan and Harabagiu, 2004). Most current SRL approaches can be classified in one of two classes: approaches that take adOn the other hand, when only automaticallygenerated syntax is available, the quality of the information provided through full syntax decreases because the state-of-the-art of full parsing is less robust and performs worse than the tools used for partial syntactic analysis. Under such real-world conditions, the difference between the two SRL approaches (with full or partial syntax) is not that high. More interestingly, the two SRL strategies perform better for different semantic roles. For example, mod"
H05-1081,W05-0634,0,0.105345,"lso very flexible since the individual systems are not required to provide any information other than their solution. Extensive experimental evaluation in the CoNLL2005 shared task framework supports our previous claims. The proposed architecture outperforms the best results reported in that evaluation exercise. 1 Introduction The task of Semantic Role Labeling (SRL), i.e. the process of detecting basic event structures such as who did what to whom, when and where, has received considerable interest in the past few years (Gildea and Jurafsky, 2002; Surdeanu et al., 2003; Xue and Palmer, 2004; Pradhan et al., 2005a; Carreras and M`arquez, 2005). It was shown that the identification of such event frames has a significant contribution for many Natural Language Processing (NLP) applications such as Information Extraction (Surdeanu et al., 2003) and Question Answering (Narayanan and Harabagiu, 2004). Most current SRL approaches can be classified in one of two classes: approaches that take adOn the other hand, when only automaticallygenerated syntax is available, the quality of the information provided through full syntax decreases because the state-of-the-art of full parsing is less robust and performs wor"
H05-1081,W05-0635,1,0.778897,"Missing"
H05-1081,P03-1002,1,0.917855,"Missing"
H05-1081,W04-3212,0,0.373931,"ombination scheme is also very flexible since the individual systems are not required to provide any information other than their solution. Extensive experimental evaluation in the CoNLL2005 shared task framework supports our previous claims. The proposed architecture outperforms the best results reported in that evaluation exercise. 1 Introduction The task of Semantic Role Labeling (SRL), i.e. the process of detecting basic event structures such as who did what to whom, when and where, has received considerable interest in the past few years (Gildea and Jurafsky, 2002; Surdeanu et al., 2003; Xue and Palmer, 2004; Pradhan et al., 2005a; Carreras and M`arquez, 2005). It was shown that the identification of such event frames has a significant contribution for many Natural Language Processing (NLP) applications such as Information Extraction (Surdeanu et al., 2003) and Question Answering (Narayanan and Harabagiu, 2004). Most current SRL approaches can be classified in one of two classes: approaches that take adOn the other hand, when only automaticallygenerated syntax is available, the quality of the information provided through full syntax decreases because the state-of-the-art of full parsing is less r"
J11-2003,D07-1119,1,0.845397,"Missing"
J11-2003,J93-2003,0,0.036581,"Missing"
J11-2003,A00-2018,0,0.0312239,"Missing"
J11-2003,W06-1670,1,0.115553,"quality of the content using both the textual and non-textual information available in the database (Jeon et al. 2006; Agichtein et al. 2008). We plan to further investigate these issues, which are not the main object of this work. The data was processed as follows. The text was split at the sentence level, tokenized and POS tagged, in the style of the Wall Street Journal Penn TreeBank (Marcus, Santorini, and Marcinkiewicz 1993). Each word was morphologically simpliﬁed using the morphological functions of the WordNet library. Sentences were annotated with WNSS categories, using the tagger of Ciaramita and Altun (2006), which annotates text with a 46-label tagset.10 These tags, deﬁned by WordNet lexicographers, provide a broad semantic categorization for nouns and verbs and include labels for nouns such as food, animal, body, and feeling, and for verbs labels such as communication, contact, and possession. We chose to annotate the data with this tagset because it is less biased towards a speciﬁc domain or set of semantic categories than, for example, a namedentity tagger. Using the same tagger as before we also annotated the text with a namedentity tagger trained on the BBN Wall Street Journal (WSJ) Entity"
J11-2003,W08-2138,1,0.847958,"Missing"
J11-2003,W03-1022,1,0.692981,"mple is Arg2 represented as gets −−→ from-rotors. In all representations we remove structures where either one of the elements is a stop word and convert the remaining words to their WordNet lemmas.4 The structures we propose are highly conﬁgurable. In this research, we investigate this issue along three dimensions: Degree of lexicalization: We reduce the sparsity of the proposed structures by replacing the lexical elements with semantic tags which might provide better generalization. In this article we use two sets of tags, the ﬁrst consisting of coarse WordNet senses, or supersenses (WNSS) (Ciaramita and Johnson 2003), and the second of named-entity labels extracted from the Wall Street Journal corpus. We present in detail the tag sets and the processors used to extract them in Section 3. For an overview, we show a sample annotated sentence in the bottom part of Figure 2. Labels of relations: Both dependency and predicate–argument relations can be labeled Arg0 or unlabeled (e.g., gets −−→ helicopter versus gets → helicopter). We make this distinction in our experiments for two reasons: (a) removing relation labels reduces the model sparsity because fewer elements are created, and (b) performing relation re"
J11-2003,P03-1003,0,0.495097,"ameters.5 To understand the contribution of our syntactic and semantic processors we compute the similarity features for different representations of the question and answer content, ranging from bag of words to semantic roles. We detail these representations in Section 2.1. FG2: Translation Features. Berger et al. (2000) showed that similarity-based models are doomed to perform poorly for QA because they fail to “bridge the lexical chasm” between questions and answers. One way to address this problem is to learn questionto-answer transformations using a translation model (Berger et al. 2000; Echihabi and Marcu 2003; Soricut and Brill 2006; Riezler et al. 2007). In our model, we incorporate this approach by adding the probability that the question Q is a translation of the answer A, P(Q|A), as a feature. This probability is computed using IBM’s Model 1 (Brown et al. 1993):  P(q|A) (2) P(Q|A) = q∈Q P(q|A) = (1 − λ )Pml (q|A) + λPml (q|C) Pml (q|A) =  (T(q|a)Pml (a|A)) (3) (4) a∈A where the probability that the question term q is generated from answer A, P(q|A), is smoothed using the prior probability that the term q is generated from the entire collection of answers C, Pml (q|C). λ is the smoothing para"
J11-2003,W03-1210,0,0.0314156,"stion type. In the same space, Riezler et al. (2007) develop SMT-based query expansion methods and use them for retrieval from FAQ pages. In our work we did not address the issue of query expansion and re-writing directly: While our re-ranking approach is limited to the recall of the retrieval model, these methods of query transformation could be used in a complementary manner to improve the recall. Even more interesting would be to couple the two approaches in an efﬁcient manner; this remains as future work. There has also been some work in the problem of selection for non-factoid questions. Girju (2003) extracts non-factoid answers by searching for certain semantic structures (e.g., causation relations as answers to causation questions). We generalized this methodology (in the form of semantic roles) and evaluated it systematically. Soricut and Brill (2006) develop a statistical model by extracting (in an unsupervised manner) QA pairs from one million FAQs obtained from the Web. They show how different statistical models may be used for the problems of ranking, selection, and extraction of non-factoid QAs on the Web; due to the scale of their problem they only consider lexical n-grams and co"
J11-2003,I08-1055,0,0.620748,"ower from rotors or blades. So as the rotors turn, air ﬂows more quickly over the tops of the blades than it does below. This creates enough lift for ﬂight. Low Quality Q: How to extract html tags from an html documents with c++? A: very carefully The situation is different once one moves beyond the task of factoid QA. Comparatively little research has focused on QA models for non-factoid questions such as causation, manner, or reason questions. Because virtually no training data is available for this problem, most automated systems train either on small hand-annotated corpora built in-house (Higashinaka and Isozaki 2008) or on question–answer pairs harvested from Frequently Asked Questions (FAQ) lists or similar resources (Soricut and Brill 2006; Riezler et al. 2007; Agichtein et al. 2008). None of these situations is ideal: The cost of building the training corpus in the former setup is high; in the latter scenario the data tend to be domain-speciﬁc, hence unsuitable for the learning of opendomain models, and for drawing general conclusions about the underlying scientiﬁc problems. On the other hand, recent years have seen an explosion of user-generated content (or social media). Of particular interest in our"
J11-2003,P07-1099,0,0.0144932,"other answers retrieved in the top N by the retrieval component. 4. Experiments We used several measures to evaluate our models. Recall that we are using an initial retrieval engine to select a pool of N answer candidates (Figure 1), which are then reranked. This couples the performance of the initial retrieval engine and the re-rankers. We tried to de-couple them in our performance measures, as follows. We note that if the initial retrieval engine does not rank the correct answer in the pool of top N results, it is impossible for any re-ranker to do well. We therefore follow the approach of Ko et al. (2007) and deﬁne performance measures only with respect to the subset of pools which contain the correct answer for a given N. This complicates slightly the typical notions of recall and precision. Let us call Q the set of all queries in the collection and QN the subset of queries for which the retrieved answer pool of size N contains the correct answer. We will then use the following performance measure deﬁnitions: |QN | Retrieval Recall@N: The usual recall deﬁnition: |Q |. This is equal for all re-rankers. Re-ranking Precision@1: Average Precision@1 over the QN set, where the Precision@1 of a quer"
J11-2003,J93-2004,0,0.0359995,"Missing"
J11-2003,H05-1086,0,0.0240931,"ctions using the maximum likelihood estimator. To mitigate sparsity, we set Pml (q|C) to a small value for out-of-vocabulary words.6 Pml (q|A) is computed as the sum of the probabilities that the question term q is a translation of an answer term a, T(q|a), weighted by the probability that a is generated from A. The translation table for T(q|a) is computed using the EM algorithm implemented in the GIZA++ toolkit.7 Translation models have one important limitation when used for retrieval tasks: They do not guarantee that the probability of translating a word to itself, that is, T(w|w), is high (Murdock and Croft 2005). This is a problem for QA, where word overlap between question and answer is a good indicator of relevance (Moldovan et al. 1999). We address this limitation with a simple algorithm: we set T(w|w) = 0.5 and re-scale for all other words w in the vocabulary to sum to 0.5, to the other T(w |w)  probabilities  guarantee that w T(w |w) = 1. This has the desired effect that T(w|w) becomes larger than any other T(w |w). Our initial experiments proved empirically that this is essential for good performance. As prior work indicates, tuning the smoothing parameter λ is also crucial for the perfor"
J11-2003,J05-1004,0,0.0395144,"Missing"
J11-2003,P07-1059,0,0.538626,"ow Quality Q: How to extract html tags from an html documents with c++? A: very carefully The situation is different once one moves beyond the task of factoid QA. Comparatively little research has focused on QA models for non-factoid questions such as causation, manner, or reason questions. Because virtually no training data is available for this problem, most automated systems train either on small hand-annotated corpora built in-house (Higashinaka and Isozaki 2008) or on question–answer pairs harvested from Frequently Asked Questions (FAQ) lists or similar resources (Soricut and Brill 2006; Riezler et al. 2007; Agichtein et al. 2008). None of these situations is ideal: The cost of building the training corpus in the former setup is high; in the latter scenario the data tend to be domain-speciﬁc, hence unsuitable for the learning of opendomain models, and for drawing general conclusions about the underlying scientiﬁc problems. On the other hand, recent years have seen an explosion of user-generated content (or social media). Of particular interest in our context are community-driven questionanswering sites, such as Yahoo! Answers, where users answer questions posed by other users and best answers ar"
J11-2003,P06-1112,0,0.0233813,"when coupled with appropriate distances. Shen and Joshi (2005) extend this idea with a supervised learning approach, training dependency tree kernels to compute the similarity. In our work we also used this type of feature, although we show that, in our context, features based on dependency tree kernels are subsumed by simpler features that measure the overlap of binary dependencies. Another alternative is proposed by Cui et al. (2005), where signiﬁcant words are aligned and similarity measures (based on mutual information of correlations) are then computed on the resulting dependency paths. Shen and Klakow (2006) extend this using a dynamic time warping algorithm to improve the alignment for approximate question phrase mapping, and learn a Maximum Entropy model to combine the obtained scores for re-ranking. Wang, Smith, and Mitamura (2007) propose to use a probabilistic quasi-synchronous grammar to learn the syntactic transformations between questions and answers. We extend the work of Cui et al. by considering paths within and across different representations beyond dependency trees, although we do not investigate the issue of alignment speciﬁcally—instead we use standard statistical translation mode"
J11-2003,D07-1003,0,0.025754,"Missing"
J11-2003,W08-2121,1,\N,Missing
J11-2003,J13-3006,1,\N,Missing
J11-2003,P07-1098,0,\N,Missing
J11-2003,J10-2003,0,\N,Missing
J13-3006,P08-1037,1,0.925043,"Missing"
J13-3006,P11-2123,1,0.853466,"hen test item Texas in Example (6) is to be labeled, the higher similarity to Dallas and New York, in contrast to the lower similarity to November and winter, would be used to label the argument with the Location role. The automatic acquisition of selectional preferences is a well-studied topic in NLP. Many methods using semantic classes and selectional preferences have been proposed and applied to a variety of syntactic–semantic ambiguity problems, including syntactic parsing (Hindle 1990; Resnik 1993b; Pantel and Lin 2000; Agirre, Baldwin, and Martinez 2008; Koo, Carreras, and Collins 2008; Agirre et al. 2011), word sense disambiguation (Resnik 1993a; Agirre and Martinez 2001; McCarthy and Carroll 2003), pronoun resolution (Bergsma, Lin, and Goebel 2008) and named-entity recognition (Ratinov and Roth 2009). In addition, selectional preferences have been shown to be effective to improve the quality of inference and information extraction rules (Pantel et al. 2007; Ritter, Mausam, and Etzioni 2010). In some cases, the aforementioned papers do not mention selectional preferences, but all of them use some notion of preferring certain semantic types over others in order to accomplish their respective ta"
J13-3006,W01-0703,1,0.790514,"Missing"
J13-3006,C96-1005,1,0.358301,"are usually more useful for characterizing selectional preferences, as in the <tool> class for the instrument role of break. The priority of using specific synsets over more general ones is, thus, justified in the sense that they may better represent the most relevant semantic characteristics of the selectional preferences. The alternative method (SPwn ) is based on the depth of the concepts in the WordNet hierarchy and the frequency of the nouns. The use of the depth in hierarchies to model the specificity of concepts (the deeper the more specific) is not new (Rada et al. 1989; Sussna 1993; Agirre and Rigau 1996). Our method tries to be conservative with respect to generalization: When we check which SP is a better fit for a given target head, we always prefer the SP that contains the most specific generalization for the target head (the lowest synset which is a hypernym of the target word). 641 Computational Linguistics Volume 39, Number 3 Table 4 Excerpt from the selectional preferences for write-Arg0 according to SPwn , showing from deeper to shallower the synsets in WordNet which are connected to head words in Table 1. Depth lists the depth of synsets in WordNet. Description includes the words and"
J13-3006,J10-4006,0,0.243113,"on of Resnik (1993b) in order to be coherent with the formulae presented in this paper. 636 Zapirain et al. Selectional Preferences for Semantic Role Classification The models return weights for (verb, syntactic function, noun) triples, and correlation with human plausibility judgment is used for evaluation. Resnik’s selectional preference scored best among WordNet-based methods (Li and Abe 1998; Clark and Weir 2002). Despite its earlier publication, Resnik’s method is still the most popular ´ Pado, ´ and Erk 2007; Erk, Pado, ´ representative among WordNet-based methods (Pado, and Pado´ 2010; Baroni and Lenci 2010). We also chose to use Resnik’s model in this paper. One of the disadvantages of the WordNet-based models, compared with the distributional similarity models, is that they require that the heads are present in WordNet. This limitation can negatively influence the coverage of the model, and also its generalization ability. 2.2 Distributional Similarity Models Distributional similarity models assume that a word is characterized by the words it co-occurs with. In the simplest model, co-occurring words are taken from a fixed-size context window. Each word w would be represented by the set of words"
J13-3006,D08-1007,0,0.0621817,"Missing"
J13-3006,boas-2002-bilingual,0,0.013958,"e predicates. For instance, consider the following sentence, in which the arguments of the predicate to send have been annotated with their respective semantic roles.1 (1) [Mr. Smith]Agent sent [the report]Object [to me]Recipient [this morning]Temporal . Recognizing these event structures has been shown to be important for a broad spectrum of NLP applications. Information extraction, summarization, question answering, machine translation, among others, can benefit from this shallow semantic analysis at sentence level, which opens the door for exploiting the semantic relations among arguments (Boas 2002; Surdeanu et al. 2003; Narayanan and Harabagiu 2004; Melli et al. 2005; Moschitti et al. 2007; Higashinaka and Isozaki 2008; Surdeanu, Ciaramita, and Zaragoza 2011). In M`arquez et al. (2008) the reader can find a broad introduction to SRL, covering several historical and definitional aspects of the problem, including also references to the main resources and systems. State-of-the-art systems leverage existing hand-tagged corpora (Fillmore, Ruppenhofer, and Baker 2004; Palmer, Gildea, and Kingsbury 2005) to learn supervised machine learning systems, and typically perform SRL in two sequential"
J13-3006,E03-1034,0,0.0636171,"P(c0 ) R(p, r) (3) The numerator formalizes the goodness of fit for the best semantic class c0 that contains w0 . The hypernym (i.e., superclass) of w0 yielding the maximum value is chosen. The denominator models how restrictive the selectional preference is for p and r, as modeled in Equation (1). Variations of Resnik’s idea to find a suitable level of generalization have been explored in later years. Li and Abe (1998) applied the minimum-description length principle. Alternatively, Clark and Weir (2002) devised a procedure to decide when a class should be preferred rather than its children. Brockmann and Lapata (2003) compared several class-based models (including Resnik’s selectional preferences) on a syntactic plausibility judgment task for German. 3 We slightly modified the notation of Resnik (1993b) in order to be coherent with the formulae presented in this paper. 636 Zapirain et al. Selectional Preferences for Semantic Role Classification The models return weights for (verb, syntactic function, noun) triples, and correlation with human plausibility judgment is used for evaluation. Resnik’s selectional preference scored best among WordNet-based methods (Li and Abe 1998; Clark and Weir 2002). Despite i"
J13-3006,W04-2412,1,0.876656,"Missing"
J13-3006,W05-0620,1,0.896889,"Missing"
J13-3006,N10-1058,1,0.848114,"esnik’s model tends to always predict the most frequent roles whereas our model covers a wider role selection. Resnik’s tendency to overgeneralize makes more frequent roles cover all the vocabulary, and the weighting system penalizes roles with fewer occurrences. 12 We verified that the lexical model shows higher classification accuracy than all the more elaborate SP models on the subset of cases covered by both the lexical and the SP models. In this situation, if we aimed at constructing the best role classifier with SPs alone we could devise a back-off strategy, in the style of Chambers and Jurafsky (2010), which uses the predictions of the lexical model when present and one of the SP models if not. As presented in Section 5, however, our main goal is to integrate these SP models in a real end-to-end SRL system, so we keep their analysis as independent predictors for the moment. 650 Zapirain et al. Selectional Preferences for Semantic Role Classification The results for distributional models indicate that the SPs using Lin’s ready-made pre thesaurus (simLin ) outperforms Pado´ and Lapata’s distributional similarity model (Pado´ and Lapata 2007) calculated over the BNC (simLin ) in both Tables 1"
J13-3006,A00-2018,0,0.119058,"state-of-the-art semantic role labeling system (Surdeanu et al. 2007). SwiRL ranked second among the systems that did not implement model combination at the CoNLL-2005 shared task and fifth overall (Carreras and M`arquez 2005). Because the focus of this section is on role classification, we modified the SRC component of SwiRL to use gold argument boundaries, that is, we assume that semantic role identification works perfectly. Nevertheless, for a realistic evaluation, all the features in the role classification model are generated using actual syntactic trees generated by the Charniak parser (Charniak 2000). The key idea behind our approach is model combination: We generate a battery of base models using all resources available and we combine their outputs using multiple strategies. Our pool of base models contains 13 different models: The first is the 13 The data sets used for the experiments reported in this section are exactly the ones described in Section 4.1. 651 Computational Linguistics Volume 39, Number 3 unmodified SwiRL SRC, the next six are the selected SP models from the previous section, and the last six are variants of SwiRL SRC. In each variant, the feature set of the unmodified S"
J13-3006,J02-2003,0,0.0199537,"s SPRes (p, r, w0 ), is formulated as follows:3 SPRes (p, r, w0 ) = max c0 ∈hyp(w0 ) P(c0 |p, r)log P(c0 |p,r) P(c0 ) R(p, r) (3) The numerator formalizes the goodness of fit for the best semantic class c0 that contains w0 . The hypernym (i.e., superclass) of w0 yielding the maximum value is chosen. The denominator models how restrictive the selectional preference is for p and r, as modeled in Equation (1). Variations of Resnik’s idea to find a suitable level of generalization have been explored in later years. Li and Abe (1998) applied the minimum-description length principle. Alternatively, Clark and Weir (2002) devised a procedure to decide when a class should be preferred rather than its children. Brockmann and Lapata (2003) compared several class-based models (including Resnik’s selectional preferences) on a syntactic plausibility judgment task for German. 3 We slightly modified the notation of Resnik (1993b) in order to be coherent with the formulae presented in this paper. 636 Zapirain et al. Selectional Preferences for Semantic Role Classification The models return weights for (verb, syntactic function, noun) triples, and correlation with human plausibility judgment is used for evaluation. Resn"
J13-3006,P97-1067,0,0.0336605,"ntains John loves Mary, then the pair (ncsubj, love) would be in the set T for John. The measure uses information-theoretic principles, and I(w, d, v) represents the information content of the triple (Lin 1998). Although the use of co-occurrence vectors for words to compute similarity has been ¨ standard practice, some authors have argued for more complex uses. Schutze (1998) builds vectors for each context of occurrence of a word, combining the co-occurrence vectors for each word in the context. The vectors for contexts were used to induce senses and to improve information retrieval results. Edmonds (1997) built a lexical cooccurrence network, and applied it to a lexical choice task. Chakraborti et al. (2007) used transitivity over co-occurrence relations, with good results on several classification tasks. Note that all these works use second order and higher order to refer to their method. In this paper, we will also use second order to refer to a new method which goes beyond the usual co-occurrence vectors (cf. Section 3.3). A full review of distributional models is out of the scope of this paper, as we are interested in showing that some of those models can be used successfully to improve SR"
J13-3006,P07-1028,0,0.76274,"tion could be more fine-grained, as defined by the WordNet hierarchy (Resnik 1993b; Agirre and Martinez 2001; McCarthy and Carroll 2003), and other lexical resources could be used as well. Other authors have used automatically induced hierarchical word classes, clustered according to occurrence information from corpora (Koo, Carreras, and Collins 2008; Ratinov and Roth 2009). On the other extreme, each word would be its own semantic class, as in the lexical model, but one could also model selectional preference using distributional similarity (Grefenstette 1992; Lin 1998; Pantel and Lin 2000; Erk 2007; Bergsma, Lin, and Goebel 2008). In this paper we will focus on WordNet-based models that use the whole hierarchy and on distributional similarity models, and we will use the lexical model as baseline. 2.1 WordNet-Based Models Resnik (1993b) proposed the modeling of selectional preferences using semantic classes from WordNet and applied the model to tackle some ambiguity issues in syntax, such as noun-compounds, coordination, and prepositional phrase attachment. Given two alternative structures, Resnik used selectional preferences to choose the attachment maximizing the fitness of the head to"
J13-3006,J10-4007,0,0.0471056,"Missing"
J13-3006,J02-3001,0,0.690118,"yntactic ambiguity. For instance, Pantel and Lin (2000) obtained very good results on PP-attachment using the distributional similarity measure defined by Lin (1998). Distributional similarity was used to overcome sparsity problems: Alongside the counts in the training data of the target words, the counts of words similar to the target ones were also used. Although not made explicit, Lin was actually using a distributional similarity model of selectional preferences. The application of distributional selectional preferences to semantic roles (as opposed to syntactic functions) is more recent. Gildea and Jurafsky (2002) are the only ones applying selectional preferences in a real SRL task. They used distributional clustering and WordNet-based techniques on a SRL task on FrameNet roles. They report a very small improvement of the overall performance when using distributional clustering techniques. In this paper we present complementary experiments, with a different role set and annotated corpus (PropBank), a wider range of selectional preference models, and the analysis of out-of-domain results. Other papers applying semantic preferences in the context of semantic roles rely on the evaluation of artificial ta"
J13-3006,P92-1052,0,0.05076,"oth training examples for Temporal (i.e., November and winter), and <geographical area> covers the examples for Location. When test words Texas and December occur in Examples (6) and (7), the semantic classes to which they belong can be used to tag the first as Location and the second as Temporal. As an alternative to the use of WordNet, one can also apply automatically acquired distributional similarity thesauri. Distributional similarity methods analyze the cooccurrence patterns of words and are able to capture, for instance, that December is more closely related to November than to Dallas (Grefenstette 1992). Distributional similarity is typically used on-line (i.e., given a pair of words, their similarity is computed on the go), 634 Zapirain et al. Selectional Preferences for Semantic Role Classification but, in order to speed up its use, it has also been used to produce off-line a full thesauri, storing, for every word, the weighted list of all outstanding similar words (Lin 1998). In the Distributional similarity model, when test item Texas in Example (6) is to be labeled, the higher similarity to Dallas and New York, in contrast to the lower similarity to November and winter, would be used to"
J13-3006,I08-1055,0,0.0292023,"end have been annotated with their respective semantic roles.1 (1) [Mr. Smith]Agent sent [the report]Object [to me]Recipient [this morning]Temporal . Recognizing these event structures has been shown to be important for a broad spectrum of NLP applications. Information extraction, summarization, question answering, machine translation, among others, can benefit from this shallow semantic analysis at sentence level, which opens the door for exploiting the semantic relations among arguments (Boas 2002; Surdeanu et al. 2003; Narayanan and Harabagiu 2004; Melli et al. 2005; Moschitti et al. 2007; Higashinaka and Isozaki 2008; Surdeanu, Ciaramita, and Zaragoza 2011). In M`arquez et al. (2008) the reader can find a broad introduction to SRL, covering several historical and definitional aspects of the problem, including also references to the main resources and systems. State-of-the-art systems leverage existing hand-tagged corpora (Fillmore, Ruppenhofer, and Baker 2004; Palmer, Gildea, and Kingsbury 2005) to learn supervised machine learning systems, and typically perform SRL in two sequential steps: argument identification and argument classification. Whereas the former is mostly a syntactic recognition task, the"
J13-3006,P90-1034,0,0.513268,"every word, the weighted list of all outstanding similar words (Lin 1998). In the Distributional similarity model, when test item Texas in Example (6) is to be labeled, the higher similarity to Dallas and New York, in contrast to the lower similarity to November and winter, would be used to label the argument with the Location role. The automatic acquisition of selectional preferences is a well-studied topic in NLP. Many methods using semantic classes and selectional preferences have been proposed and applied to a variety of syntactic–semantic ambiguity problems, including syntactic parsing (Hindle 1990; Resnik 1993b; Pantel and Lin 2000; Agirre, Baldwin, and Martinez 2008; Koo, Carreras, and Collins 2008; Agirre et al. 2011), word sense disambiguation (Resnik 1993a; Agirre and Martinez 2001; McCarthy and Carroll 2003), pronoun resolution (Bergsma, Lin, and Goebel 2008) and named-entity recognition (Ratinov and Roth 2009). In addition, selectional preferences have been shown to be effective to improve the quality of inference and information extraction rules (Pantel et al. 2007; Ritter, Mausam, and Etzioni 2010). In some cases, the aforementioned papers do not mention selectional preferences"
J13-3006,P08-1068,0,0.105046,"Missing"
J13-3006,P99-1004,0,0.246709,"at co-occur with it, T(w). In a more elaborate model, each word w would be represented as a vector   i (w) corresponds to the weight of the ith word in with weights, where T of words T(w) the vector. The weights can be calculated following a simple frequency of co-occurrence, or using some other formula. Then, given two words w and w0 , their similarity can be computed using any similarity measure between their co-occurrence sets or vectors. For instance, early work by Grefenstette (1992) used the Jaccard similarity coefficient of the two sets T(w) and T(w0 ) (cf. Equation (4) in Figure 1). Lee (1999) reviews a wide range of similarity functions,   0 ) (cf. Equation (5) and T(w including Jaccard and the cosine between two vectors T(w) in Figure 1). In the context of lexical semantics, the similarity measure defined by Lin (1998) has been very successful. This measure (cf. Equation (6) in Figure 1) takes into account syntactic dependencies (d) in its co-occurrence model. In this case, the set T(w) of cooccurrences of w contains pairs (d,v) of dependencies and words, representing the fact simJac (w, w0 ) = simcos (w, w0 ) =   simLin (w, w0 ) =  |T(w) ∩ T(w0 )| |T(w) ∪ T(w0 )| n"
J13-3006,J98-2002,0,0.0537239,"erence of a predicate p and role r for a head w0 of any potential argument, noted as SPRes (p, r, w0 ), is formulated as follows:3 SPRes (p, r, w0 ) = max c0 ∈hyp(w0 ) P(c0 |p, r)log P(c0 |p,r) P(c0 ) R(p, r) (3) The numerator formalizes the goodness of fit for the best semantic class c0 that contains w0 . The hypernym (i.e., superclass) of w0 yielding the maximum value is chosen. The denominator models how restrictive the selectional preference is for p and r, as modeled in Equation (1). Variations of Resnik’s idea to find a suitable level of generalization have been explored in later years. Li and Abe (1998) applied the minimum-description length principle. Alternatively, Clark and Weir (2002) devised a procedure to decide when a class should be preferred rather than its children. Brockmann and Lapata (2003) compared several class-based models (including Resnik’s selectional preferences) on a syntactic plausibility judgment task for German. 3 We slightly modified the notation of Resnik (1993b) in order to be coherent with the formulae presented in this paper. 636 Zapirain et al. Selectional Preferences for Semantic Role Classification The models return weights for (verb, syntactic function, noun)"
J13-3006,P98-2127,0,0.198549,"ional similarity thesauri. Distributional similarity methods analyze the cooccurrence patterns of words and are able to capture, for instance, that December is more closely related to November than to Dallas (Grefenstette 1992). Distributional similarity is typically used on-line (i.e., given a pair of words, their similarity is computed on the go), 634 Zapirain et al. Selectional Preferences for Semantic Role Classification but, in order to speed up its use, it has also been used to produce off-line a full thesauri, storing, for every word, the weighted list of all outstanding similar words (Lin 1998). In the Distributional similarity model, when test item Texas in Example (6) is to be labeled, the higher similarity to Dallas and New York, in contrast to the lower similarity to November and winter, would be used to label the argument with the Location role. The automatic acquisition of selectional preferences is a well-studied topic in NLP. Many methods using semantic classes and selectional preferences have been proposed and applied to a variety of syntactic–semantic ambiguity problems, including syntactic parsing (Hindle 1990; Resnik 1993b; Pantel and Lin 2000; Agirre, Baldwin, and Marti"
J13-3006,S07-1005,0,0.346404,"Missing"
J13-3006,W06-2106,0,0.177585,"Missing"
J13-3006,H94-1020,0,0.407013,"Missing"
J13-3006,J08-2001,1,0.891986,"Missing"
J13-3006,J03-4004,0,0.257012,"New York, in contrast to the lower similarity to November and winter, would be used to label the argument with the Location role. The automatic acquisition of selectional preferences is a well-studied topic in NLP. Many methods using semantic classes and selectional preferences have been proposed and applied to a variety of syntactic–semantic ambiguity problems, including syntactic parsing (Hindle 1990; Resnik 1993b; Pantel and Lin 2000; Agirre, Baldwin, and Martinez 2008; Koo, Carreras, and Collins 2008; Agirre et al. 2011), word sense disambiguation (Resnik 1993a; Agirre and Martinez 2001; McCarthy and Carroll 2003), pronoun resolution (Bergsma, Lin, and Goebel 2008) and named-entity recognition (Ratinov and Roth 2009). In addition, selectional preferences have been shown to be effective to improve the quality of inference and information extraction rules (Pantel et al. 2007; Ritter, Mausam, and Etzioni 2010). In some cases, the aforementioned papers do not mention selectional preferences, but all of them use some notion of preferring certain semantic types over others in order to accomplish their respective task. In fact, one could use different notions of semantic types. In one extreme, we would have a"
J13-3006,P07-1098,0,0.0545287,"s of the predicate to send have been annotated with their respective semantic roles.1 (1) [Mr. Smith]Agent sent [the report]Object [to me]Recipient [this morning]Temporal . Recognizing these event structures has been shown to be important for a broad spectrum of NLP applications. Information extraction, summarization, question answering, machine translation, among others, can benefit from this shallow semantic analysis at sentence level, which opens the door for exploiting the semantic relations among arguments (Boas 2002; Surdeanu et al. 2003; Narayanan and Harabagiu 2004; Melli et al. 2005; Moschitti et al. 2007; Higashinaka and Isozaki 2008; Surdeanu, Ciaramita, and Zaragoza 2011). In M`arquez et al. (2008) the reader can find a broad introduction to SRL, covering several historical and definitional aspects of the problem, including also references to the main resources and systems. State-of-the-art systems leverage existing hand-tagged corpora (Fillmore, Ruppenhofer, and Baker 2004; Palmer, Gildea, and Kingsbury 2005) to learn supervised machine learning systems, and typically perform SRL in two sequential steps: argument identification and argument classification. Whereas the former is mostly a sy"
J13-3006,C04-1100,0,0.0643825,"der the following sentence, in which the arguments of the predicate to send have been annotated with their respective semantic roles.1 (1) [Mr. Smith]Agent sent [the report]Object [to me]Recipient [this morning]Temporal . Recognizing these event structures has been shown to be important for a broad spectrum of NLP applications. Information extraction, summarization, question answering, machine translation, among others, can benefit from this shallow semantic analysis at sentence level, which opens the door for exploiting the semantic relations among arguments (Boas 2002; Surdeanu et al. 2003; Narayanan and Harabagiu 2004; Melli et al. 2005; Moschitti et al. 2007; Higashinaka and Isozaki 2008; Surdeanu, Ciaramita, and Zaragoza 2011). In M`arquez et al. (2008) the reader can find a broad introduction to SRL, covering several historical and definitional aspects of the problem, including also references to the main resources and systems. State-of-the-art systems leverage existing hand-tagged corpora (Fillmore, Ruppenhofer, and Baker 2004; Palmer, Gildea, and Kingsbury 2005) to learn supervised machine learning systems, and typically perform SRL in two sequential steps: argument identification and argument classif"
J13-3006,P10-1045,0,0.0304033,"Missing"
J13-3006,J07-2002,0,0.165313,"Missing"
J13-3006,D07-1042,0,0.235467,"Missing"
J13-3006,J05-1004,0,0.62839,"Missing"
J13-3006,N07-1071,0,0.014092,"erences have been proposed and applied to a variety of syntactic–semantic ambiguity problems, including syntactic parsing (Hindle 1990; Resnik 1993b; Pantel and Lin 2000; Agirre, Baldwin, and Martinez 2008; Koo, Carreras, and Collins 2008; Agirre et al. 2011), word sense disambiguation (Resnik 1993a; Agirre and Martinez 2001; McCarthy and Carroll 2003), pronoun resolution (Bergsma, Lin, and Goebel 2008) and named-entity recognition (Ratinov and Roth 2009). In addition, selectional preferences have been shown to be effective to improve the quality of inference and information extraction rules (Pantel et al. 2007; Ritter, Mausam, and Etzioni 2010). In some cases, the aforementioned papers do not mention selectional preferences, but all of them use some notion of preferring certain semantic types over others in order to accomplish their respective task. In fact, one could use different notions of semantic types. In one extreme, we would have a small set of coarse semantic classes. For instance, some authors have used the 26 so-called “semantic fields” used to classify all nouns in WordNet (Agirre, Baldwin, and Martinez 2008; Agirre et al. 2011). The classification could be more fine-grained, as defined"
J13-3006,P00-1014,0,0.249246,"sparseness as one of the main reasons. In this work, we will focus on Semantic Role Classification (SRC), and we will show that selectional preferences (SP) are useful for generalizing lexical features, helping fight sparseness and domain shifts, and improving SRC results. Selectional preferences try to model the kind of words that can fill a specific argument of a predicate, and have been widely used in computational linguistics since the early days (Wilks 1975). Both semantic classes from existing lexical resources like WordNet (Resnik 1993b) and distributional similarity based on corpora (Pantel and Lin 2000) have been successfully used for acquiring selectional preferences, and in this work we have used several of those models. The contributions of this work to the field of SRL are the following: 1. We formalize and implement a method that applies several selectional preference models to Semantic Role Classification, introducing for the first time the use of selectional preferences for prepositions, in addition to selectional preferences for verbs. 2. We show that the selectional preference models are able to generalize lexical features and improve role classification performance in a controlled"
J13-3006,J08-2006,0,0.0842412,"Missing"
J13-3006,W09-1119,0,0.00868279,"th the Location role. The automatic acquisition of selectional preferences is a well-studied topic in NLP. Many methods using semantic classes and selectional preferences have been proposed and applied to a variety of syntactic–semantic ambiguity problems, including syntactic parsing (Hindle 1990; Resnik 1993b; Pantel and Lin 2000; Agirre, Baldwin, and Martinez 2008; Koo, Carreras, and Collins 2008; Agirre et al. 2011), word sense disambiguation (Resnik 1993a; Agirre and Martinez 2001; McCarthy and Carroll 2003), pronoun resolution (Bergsma, Lin, and Goebel 2008) and named-entity recognition (Ratinov and Roth 2009). In addition, selectional preferences have been shown to be effective to improve the quality of inference and information extraction rules (Pantel et al. 2007; Ritter, Mausam, and Etzioni 2010). In some cases, the aforementioned papers do not mention selectional preferences, but all of them use some notion of preferring certain semantic types over others in order to accomplish their respective task. In fact, one could use different notions of semantic types. In one extreme, we would have a small set of coarse semantic classes. For instance, some authors have used the 26 so-called “semantic fi"
J13-3006,H93-1054,0,0.155446,"rgument classification subtask, and suggested the lexical data sparseness as one of the main reasons. In this work, we will focus on Semantic Role Classification (SRC), and we will show that selectional preferences (SP) are useful for generalizing lexical features, helping fight sparseness and domain shifts, and improving SRC results. Selectional preferences try to model the kind of words that can fill a specific argument of a predicate, and have been widely used in computational linguistics since the early days (Wilks 1975). Both semantic classes from existing lexical resources like WordNet (Resnik 1993b) and distributional similarity based on corpora (Pantel and Lin 2000) have been successfully used for acquiring selectional preferences, and in this work we have used several of those models. The contributions of this work to the field of SRL are the following: 1. We formalize and implement a method that applies several selectional preference models to Semantic Role Classification, introducing for the first time the use of selectional preferences for prepositions, in addition to selectional preferences for verbs. 2. We show that the selectional preference models are able to generalize lexica"
J13-3006,P10-1044,0,0.130344,"Missing"
J13-3006,J98-1004,0,0.108058,"s co-occurring with w, and I(w, d, v) is the mutual information between w and d, v. 637 Computational Linguistics Volume 39, Number 3 that the corpus contains an occurrence of w having dependency d with v. For instance, if the corpus contains John loves Mary, then the pair (ncsubj, love) would be in the set T for John. The measure uses information-theoretic principles, and I(w, d, v) represents the information content of the triple (Lin 1998). Although the use of co-occurrence vectors for words to compute similarity has been ¨ standard practice, some authors have argued for more complex uses. Schutze (1998) builds vectors for each context of occurrence of a word, combining the co-occurrence vectors for each word in the context. The vectors for contexts were used to induce senses and to improve information retrieval results. Edmonds (1997) built a lexical cooccurrence network, and applied it to a lexical choice task. Chakraborti et al. (2007) used transitivity over co-occurrence relations, with good results on several classification tasks. Note that all these works use second order and higher order to refer to their method. In this paper, we will also use second order to refer to a new method whi"
J13-3006,D11-1012,0,0.155516,"Missing"
J13-3006,P03-1002,1,0.820408,"s. For instance, consider the following sentence, in which the arguments of the predicate to send have been annotated with their respective semantic roles.1 (1) [Mr. Smith]Agent sent [the report]Object [to me]Recipient [this morning]Temporal . Recognizing these event structures has been shown to be important for a broad spectrum of NLP applications. Information extraction, summarization, question answering, machine translation, among others, can benefit from this shallow semantic analysis at sentence level, which opens the door for exploiting the semantic relations among arguments (Boas 2002; Surdeanu et al. 2003; Narayanan and Harabagiu 2004; Melli et al. 2005; Moschitti et al. 2007; Higashinaka and Isozaki 2008; Surdeanu, Ciaramita, and Zaragoza 2011). In M`arquez et al. (2008) the reader can find a broad introduction to SRL, covering several historical and definitional aspects of the problem, including also references to the main resources and systems. State-of-the-art systems leverage existing hand-tagged corpora (Fillmore, Ruppenhofer, and Baker 2004; Palmer, Gildea, and Kingsbury 2005) to learn supervised machine learning systems, and typically perform SRL in two sequential steps: argument ident"
J13-3006,J11-2003,1,0.89747,"Missing"
J13-3006,P09-2019,1,0.908931,"Missing"
J13-3006,N07-1070,0,\N,Missing
J13-3006,P10-1046,0,\N,Missing
J13-3006,P92-1053,0,\N,Missing
J13-3006,C98-2122,0,\N,Missing
J13-4004,W97-1306,0,0.430065,"le-based systems relied on hand-tuned weights and were not capable of global inference, two factors that led to poor performance and replacement by machine learning. We propose a new approach that brings together the insights of these modern supervised and unsupervised models with the advantages of deterministic, rule-based systems. We introduce a model that performs entity-centric coreference, where all mentions that point to the same real-world entity are jointly modeled, in a rich feature space using solely simple, deterministic rules. Our work is inspired both by the seminal early work of Baldwin (1997), who first proposed that a series of high-precision rules could be used to build a high-precision, low-recall system for anaphora resolution, and by more recent work that has suggested that deterministic rules can outperform machine learning models for coreference (Zhou and Su 2004; Haghighi and Klein 2009) and for named entity recognition (Chiticariu et al. 2010). Figure 1 illustrates the two main stages of our new deterministic model: mention detection and coreference resolution, as well as a smaller post-processing step. In the mention detection stage, nominal and pronominal mentions are i"
J13-4004,P12-1041,0,0.0506069,"Missing"
J13-4004,D08-1031,0,0.592969,"Missing"
J13-4004,P06-1005,0,0.313052,"rs for pronominal coreference. We implement pronominal coreference resolution using an approach standard for many decades: enforcing agreement constraints between the coreferent mentions. We use the following attributes for these constraints: r r r r r r Number – we assign number attributes based on: (a) a static list for pronouns; (b) NER labels: mentions marked as a named entity are considered singular with the exception of organizations, which can be both singular and plural; (c) part of speech tags: NN*S tags are plural and all other NN* tags are singular; and (d) a static dictionary from Bergsma and Lin (2006). Gender – we assign gender attributes from static lexicons from Bergsma and Lin (2006), and Ji and Lin (2009). Person – we assign person attributes only to pronouns. We do not enforce this constraint when linking two pronouns, however, if one appears within quotes. This is a simple heuristic for speaker detection (e.g., I and she point to the same person in “[I] voted my conscience,” [she] said). Animacy – we set animacy attributes using: (a) a static list for pronouns; (b) NER labels (e.g., PERSON is animate whereas LOCATION is not); and (c) a dictionary bootstrapped from the Web (Ji and Lin"
J13-4004,P08-1002,0,0.0325892,"Missing"
J13-4004,C82-1006,0,0.137371,"coreference work mentioned earlier and discussed in Section 6, we draw on classic ideas that have proved to be important again and again in the history of natural language processing. The idea of beginning with the most accurate models or starting with smaller subproblems that allow for high-precision solutions combines the intuitions of “shaping” or “successive approximations” first proposed for learning by Skinner (1938), and widely used in NLP (e.g., the successively trained IBM MT models of Brown et al. [1993]) and the “islands of reliability” approaches to parsing and speech recognition [Borghesi and Favareto 1982; Corazza et al. 1991]). The idea of beginning with a high-recall list of candidates that are followed by a series of high-precision filters dates back to one of the earliest architectures in natural language processing, the part of speech tagging algorithm of the Computational Grammar Coder (Klein and Simmons 887 Computational Linguistics Volume 39, Number 4 1963) and the TAGGIT tagger (Greene and Rubin 1971), which begin with a high-recall list of all possible tags for words, and then used high-precision rules to filter likely tags based on context. In the next section we walk through an exa"
J13-4004,W05-0406,0,0.105348,"Missing"
J13-4004,P87-1022,0,0.348698,"Missing"
J13-4004,J93-2003,0,0.0334508,"Missing"
J13-4004,W11-1907,0,0.0406925,"Missing"
J13-4004,W99-0611,0,0.559667,"Missing"
J13-4004,W11-1904,0,0.0068184,"tter we used gold mentions. The only reason for this distinction is to facilitate comparison with previous work (all systems listed in Table 5 used gold mention boundaries). The two tables show that, regardless of evaluation corpus and methodology, our system generally outperforms the previous state of the art. In the CoNLL shared task, 900 Lee et al. Deterministic Coreference Resolution Based on Entity-Centric, Precision-Ranked Rules our system scores 1.8 CoNLL F1 points higher than the next system in the closed track and 2.6 points higher than the second-ranked system in the open track. The Chang et al. (2011) system has marginally higher B3 and BLANC F1 scores, but does not outperform our model on the other two metrics and the average F1 score. Table 5 shows that our model has higher B3 F1 scores than all the other models in the two ACE corpora. The model of Haghighi and Klein (2009) minimally outperforms ours by 0.6 B3 F1 points in the MUC corpus. All in all, these results prove that our approach compares favorably with a wide range of models, which include most aspects deemed important for coreference resolution, among other things, supervised learning using ´ and Turmo 2011; Chang et al. 2011),"
J13-4004,W12-4504,0,0.0360779,"Missing"
J13-4004,D10-1098,0,0.062478,"Missing"
J13-4004,W99-0613,0,0.44242,"Missing"
J13-4004,1991.iwpt-1.24,0,0.0410389,"Missing"
J13-4004,N07-1011,0,0.284716,"or example, even if we start with a perfect set of gold mentions, if we miss all coreference relations in a text, every mention will remain as a singleton and will be removed by the OntoNotes post Table 5 Comparison of our system with the other reported results on the ACE and MUC corpora. All these systems use gold mention boundaries. System B3 MUC R P F1 R P F1 74.5 78.5 73.2 74.5 88.7 79.6 86.7 88.3 81.0 79.0 79.3 80.8 74.1 74.5 – 65.2 87.3 79.4 – 86.8 80.2 76.9 – 74.5 63.1 67.3 – 49.7 90.6 84.7 – 90.9 74.4 75.0 – 64.3 ACE2004-Culotta-Test This paper Haghighi and Klein (2009) Culotta et al. (2007) Bengston and Roth (2008) 70.2 77.7 – 69.9 82.7 74.8 – 82.7 75.9 79.6 – 75.8 ACE2004-nwire This paper Haghighi and Klein (2009) Poon and Domingos (2008) Finkel and Manning (2008) 75.1 75.9 70.5 58.5 84.6 77.0 71.3 78.7 79.6 76.5 70.9 67.1 MUC6-Test This paper Haghighi and Klein (2009) Poon and Domingos (2008) Finkel and Manning (2008) 69.1 77.3 75.8 55.1 90.6 87.2 83.0 89.7 78.4 81.9 79.2 68.3 901 Computational Linguistics Volume 39, Number 4 processing, resulting in zero mentions in the final output. Therefore, we included the score using gold mention boundaries in the last part of Table 4 (“"
J13-4004,H05-1013,0,0.0270679,"Missing"
J13-4004,W08-1301,0,0.0240857,"Missing"
J13-4004,N07-1030,0,0.190945,"Missing"
J13-4004,doddington-etal-2004-automatic,0,0.0156715,"ible that . . . , It seems that . . . , It turns out . . . ). The complete set of patterns, using the tregex2 notation, is shown in Appendix B. 5. We discard adjectival forms of nations or nationality acronyms (e.g., American, U.S., U.K.), following the OntoNotes annotation guidelines. 6. We remove stop words from the following list determined by error analysis on mention detection: there, ltd., etc, ’s, hmm. Note that some rules change depending on the corpus we use for evaluation. In particular, adjectival forms of nations are valid mentions in the Automated Content Extraction (ACE) corpus (Doddington et al. 2004), thus they would not be removed when processing this corpus. 3.2 Resolution Architecture Traditionally, coreference resolution is implemented as a quadratic problem, where potential coreference links between any two mentions in a document are considered. This is not ideal, however, as it increases both the likelihood of errors and the processing time. In this article, we argue that it is better to cautiously construct high-quality mention clusters,3 and use an entity-centric model that allows the sharing of information across these incrementally constructed clusters. We achieve these goals by"
J13-4004,P10-2007,0,0.0175971,"Notes corpus, this sieve does not enhance recall significantly, mainly because appositions and predicate nominatives are not annotated in this corpus (they are annotated in ACE). Regardless of annotation standard, however, this sieve is important because it grows entities with high quality elements, which has a significant impact on the entity’s features (as discussed in Section 3.2.3). 3.3.5 Pass 5 – Strict Head Match. Linking a mention to an antecedent based on the naive matching of their head words generates many spurious links because it completely ignores possibly incompatible modifiers (Elsner and Charniak 2010). For example, Yale University and Harvard University have similar head words, but they are obviously different entities. To address this issue, this pass implements several constraints that must all be matched in order to yield a link: r Entity head match – the mention head word matches any head word of mentions in the antecedent entity. Note that this feature is actually more relaxed than naive head matching in a pair of mentions because here it is satisfied when the mention’s head matches the head of any mention in the candidate entity. We constrain this feature by enforcing a conjunction w"
J13-4004,W12-4502,0,0.381614,"Missing"
J13-4004,P05-1045,0,0.191986,"Missing"
J13-4004,P08-2012,0,0.146569,"Missing"
J13-4004,P07-2027,1,0.571167,"Missing"
J13-4004,P07-1107,0,0.299269,"Missing"
J13-4004,D09-1120,0,0.188162,"ng knowledge in rule-based systems that has implications throughout computational linguistics. 1. Introduction Coreference resolution, the task of finding all expressions that refer to the same entity in a discourse, is important for natural language understanding tasks like summarization, question answering, and information extraction. The long history of coreference resolution has shown that the use of highly precise lexical and syntactic features is crucial to high quality resolution (Ng and Cardie 2002b; Lappin and Leass 1994; Poesio et al. 2004a; Zhou and Su 2004; Bengtson and Roth 2008; Haghighi and Klein 2009). Recent work has also shown the importance of global inference—performing coreference resolution jointly for several or all mentions in a document—rather than greedily disambiguating individual pairs of mentions (Morton 2000; Luo et al. 2004; Yang et al. 2004; Culotta et al. 2007; Yang et al. 2008; Poon and Domingos 2008; Denis and Baldridge 2009; Rahman and Ng 2009; Haghighi and Klein 2010; Cai, Mujdricza-Maydt, and Strube 2011). Modern systems have met this need for carefully designed features and global or entity-centric inference with machine learning approaches to coreference resolution."
J13-4004,N10-1061,0,0.481142,"Missing"
J13-4004,Y09-1024,0,0.646111,"ecades: enforcing agreement constraints between the coreferent mentions. We use the following attributes for these constraints: r r r r r r Number – we assign number attributes based on: (a) a static list for pronouns; (b) NER labels: mentions marked as a named entity are considered singular with the exception of organizations, which can be both singular and plural; (c) part of speech tags: NN*S tags are plural and all other NN* tags are singular; and (d) a static dictionary from Bergsma and Lin (2006). Gender – we assign gender attributes from static lexicons from Bergsma and Lin (2006), and Ji and Lin (2009). Person – we assign person attributes only to pronouns. We do not enforce this constraint when linking two pronouns, however, if one appears within quotes. This is a simple heuristic for speaker detection (e.g., I and she point to the same person in “[I] voted my conscience,” [she] said). Animacy – we set animacy attributes using: (a) a static list for pronouns; (b) NER labels (e.g., PERSON is animate whereas LOCATION is not); and (c) a dictionary bootstrapped from the Web (Ji and Lin 2009). NER label – from the Stanford NER. Pronoun distance - sentence distance between a pronoun and its ante"
J13-4004,W97-0319,0,0.282039,"Missing"
J13-4004,P03-1054,0,0.0128291,"ition of the ACE 2004 corpus reserved for testing by several previous studies (Culotta et al. 2007; Bengtson and Roth 2008; Haghighi and Klein 2009). ACE2004-nwire – newswire subset of the ACE 2004 corpus, utilized by Poon and Domingos (2008) and Haghighi and Klein (2009) for testing. MUC6-Test – test corpus from the sixth Message Understanding Conference (MUC-6) evaluation. The corpora statistics are shown in Table 3. We used the first corpus (OntoNotes-Dev) for development and all others for the formal evaluation. We parsed all documents in the ACE and MUC corpora using the Stanford parser (Klein and Manning 2003) and the Stanford NER (Finkel, Grenager, and Manning 2005). We used the provided parse Table 3 Corpora statistics. Corpora OntoNotes-Dev OntoNotes-Test ACE2004-Culotta-Test ACE2004-nwire MUC6-Test 898 # Documents # Sentences # Words # Entities # Mentions 303 322 107 128 30 6,894 8,262 1,993 3,594 576 136K 142K 33K 74K 13K 3,752 3,926 2,576 4,762 496 14,291 16,291 5,455 11,398 2,136 Lee et al. Deterministic Coreference Resolution Based on Entity-Centric, Precision-Ranked Rules trees and named entity labels (not gold) in the OntoNotes corpora to facilitate the comparison with other systems. 4.2"
J13-4004,P11-1079,0,0.0279236,"Missing"
J13-4004,J94-4002,0,0.268022,"guistics Volume 39, Number 4 Arabic. Our system thus offers a new paradigm for combining knowledge in rule-based systems that has implications throughout computational linguistics. 1. Introduction Coreference resolution, the task of finding all expressions that refer to the same entity in a discourse, is important for natural language understanding tasks like summarization, question answering, and information extraction. The long history of coreference resolution has shown that the use of highly precise lexical and syntactic features is crucial to high quality resolution (Ng and Cardie 2002b; Lappin and Leass 1994; Poesio et al. 2004a; Zhou and Su 2004; Bengtson and Roth 2008; Haghighi and Klein 2009). Recent work has also shown the importance of global inference—performing coreference resolution jointly for several or all mentions in a document—rather than greedily disambiguating individual pairs of mentions (Morton 2000; Luo et al. 2004; Yang et al. 2004; Culotta et al. 2007; Yang et al. 2008; Poon and Domingos 2008; Denis and Baldridge 2009; Rahman and Ng 2009; Haghighi and Klein 2010; Cai, Mujdricza-Maydt, and Strube 2011). Modern systems have met this need for carefully designed features and globa"
J13-4004,W11-1902,1,0.676764,"Missing"
J13-4004,H05-1004,0,0.725061,"Missing"
J13-4004,P04-1018,0,0.0538826,"understanding tasks like summarization, question answering, and information extraction. The long history of coreference resolution has shown that the use of highly precise lexical and syntactic features is crucial to high quality resolution (Ng and Cardie 2002b; Lappin and Leass 1994; Poesio et al. 2004a; Zhou and Su 2004; Bengtson and Roth 2008; Haghighi and Klein 2009). Recent work has also shown the importance of global inference—performing coreference resolution jointly for several or all mentions in a document—rather than greedily disambiguating individual pairs of mentions (Morton 2000; Luo et al. 2004; Yang et al. 2004; Culotta et al. 2007; Yang et al. 2008; Poon and Domingos 2008; Denis and Baldridge 2009; Rahman and Ng 2009; Haghighi and Klein 2010; Cai, Mujdricza-Maydt, and Strube 2011). Modern systems have met this need for carefully designed features and global or entity-centric inference with machine learning approaches to coreference resolution. But machine learning, although powerful, has limitations. Supervised machine learning systems rely on expensive hand-labeled data sets and generalize poorly to new words or domains. Unsupervised systems are increasingly more complex, making"
J13-4004,P00-1023,0,0.102077,"ral language understanding tasks like summarization, question answering, and information extraction. The long history of coreference resolution has shown that the use of highly precise lexical and syntactic features is crucial to high quality resolution (Ng and Cardie 2002b; Lappin and Leass 1994; Poesio et al. 2004a; Zhou and Su 2004; Bengtson and Roth 2008; Haghighi and Klein 2009). Recent work has also shown the importance of global inference—performing coreference resolution jointly for several or all mentions in a document—rather than greedily disambiguating individual pairs of mentions (Morton 2000; Luo et al. 2004; Yang et al. 2004; Culotta et al. 2007; Yang et al. 2008; Poon and Domingos 2008; Denis and Baldridge 2009; Rahman and Ng 2009; Haghighi and Klein 2010; Cai, Mujdricza-Maydt, and Strube 2011). Modern systems have met this need for carefully designed features and global or entity-centric inference with machine learning approaches to coreference resolution. But machine learning, although powerful, has limitations. Supervised machine learning systems rely on expensive hand-labeled data sets and generalize poorly to new words or domains. Unsupervised systems are increasingly more"
J13-4004,D08-1067,0,0.32593,"Missing"
J13-4004,N09-1065,0,0.134029,"shown that the use of highly precise lexical and syntactic features is crucial to high quality resolution (Ng and Cardie 2002b; Lappin and Leass 1994; Poesio et al. 2004a; Zhou and Su 2004; Bengtson and Roth 2008; Haghighi and Klein 2009). Recent work has also shown the importance of global inference—performing coreference resolution jointly for several or all mentions in a document—rather than greedily disambiguating individual pairs of mentions (Morton 2000; Luo et al. 2004; Yang et al. 2004; Culotta et al. 2007; Yang et al. 2008; Poon and Domingos 2008; Denis and Baldridge 2009; Rahman and Ng 2009; Haghighi and Klein 2010; Cai, Mujdricza-Maydt, and Strube 2011). Modern systems have met this need for carefully designed features and global or entity-centric inference with machine learning approaches to coreference resolution. But machine learning, although powerful, has limitations. Supervised machine learning systems rely on expensive hand-labeled data sets and generalize poorly to new words or domains. Unsupervised systems are increasingly more complex, making them hard to tune and difficult to apply to new problems and genres as well. Rule-based models like Lappin and Leass (1994) wer"
J13-4004,P10-1142,0,0.165739,"Missing"
J13-4004,C02-1139,0,0.652854,"ics Computational Linguistics Volume 39, Number 4 Arabic. Our system thus offers a new paradigm for combining knowledge in rule-based systems that has implications throughout computational linguistics. 1. Introduction Coreference resolution, the task of finding all expressions that refer to the same entity in a discourse, is important for natural language understanding tasks like summarization, question answering, and information extraction. The long history of coreference resolution has shown that the use of highly precise lexical and syntactic features is crucial to high quality resolution (Ng and Cardie 2002b; Lappin and Leass 1994; Poesio et al. 2004a; Zhou and Su 2004; Bengtson and Roth 2008; Haghighi and Klein 2009). Recent work has also shown the importance of global inference—performing coreference resolution jointly for several or all mentions in a document—rather than greedily disambiguating individual pairs of mentions (Morton 2000; Luo et al. 2004; Yang et al. 2004; Culotta et al. 2007; Yang et al. 2008; Poon and Domingos 2008; Denis and Baldridge 2009; Rahman and Ng 2009; Haghighi and Klein 2010; Cai, Mujdricza-Maydt, and Strube 2011). Modern systems have met this need for carefully des"
J13-4004,P02-1014,0,0.574409,"Missing"
J13-4004,P04-1019,0,0.0115911,"ber 4 Arabic. Our system thus offers a new paradigm for combining knowledge in rule-based systems that has implications throughout computational linguistics. 1. Introduction Coreference resolution, the task of finding all expressions that refer to the same entity in a discourse, is important for natural language understanding tasks like summarization, question answering, and information extraction. The long history of coreference resolution has shown that the use of highly precise lexical and syntactic features is crucial to high quality resolution (Ng and Cardie 2002b; Lappin and Leass 1994; Poesio et al. 2004a; Zhou and Su 2004; Bengtson and Roth 2008; Haghighi and Klein 2009). Recent work has also shown the importance of global inference—performing coreference resolution jointly for several or all mentions in a document—rather than greedily disambiguating individual pairs of mentions (Morton 2000; Luo et al. 2004; Yang et al. 2004; Culotta et al. 2007; Yang et al. 2008; Poon and Domingos 2008; Denis and Baldridge 2009; Rahman and Ng 2009; Haghighi and Klein 2010; Cai, Mujdricza-Maydt, and Strube 2011). Modern systems have met this need for carefully designed features and global or entity-centric"
J13-4004,W04-0707,0,0.046836,"ber 4 Arabic. Our system thus offers a new paradigm for combining knowledge in rule-based systems that has implications throughout computational linguistics. 1. Introduction Coreference resolution, the task of finding all expressions that refer to the same entity in a discourse, is important for natural language understanding tasks like summarization, question answering, and information extraction. The long history of coreference resolution has shown that the use of highly precise lexical and syntactic features is crucial to high quality resolution (Ng and Cardie 2002b; Lappin and Leass 1994; Poesio et al. 2004a; Zhou and Su 2004; Bengtson and Roth 2008; Haghighi and Klein 2009). Recent work has also shown the importance of global inference—performing coreference resolution jointly for several or all mentions in a document—rather than greedily disambiguating individual pairs of mentions (Morton 2000; Luo et al. 2004; Yang et al. 2004; Culotta et al. 2007; Yang et al. 2008; Poon and Domingos 2008; Denis and Baldridge 2009; Rahman and Ng 2009; Haghighi and Klein 2010; Cai, Mujdricza-Maydt, and Strube 2011). Modern systems have met this need for carefully designed features and global or entity-centric"
J13-4004,D08-1068,0,0.551661,"n extraction. The long history of coreference resolution has shown that the use of highly precise lexical and syntactic features is crucial to high quality resolution (Ng and Cardie 2002b; Lappin and Leass 1994; Poesio et al. 2004a; Zhou and Su 2004; Bengtson and Roth 2008; Haghighi and Klein 2009). Recent work has also shown the importance of global inference—performing coreference resolution jointly for several or all mentions in a document—rather than greedily disambiguating individual pairs of mentions (Morton 2000; Luo et al. 2004; Yang et al. 2004; Culotta et al. 2007; Yang et al. 2008; Poon and Domingos 2008; Denis and Baldridge 2009; Rahman and Ng 2009; Haghighi and Klein 2010; Cai, Mujdricza-Maydt, and Strube 2011). Modern systems have met this need for carefully designed features and global or entity-centric inference with machine learning approaches to coreference resolution. But machine learning, although powerful, has limitations. Supervised machine learning systems rely on expensive hand-labeled data sets and generalize poorly to new words or domains. Unsupervised systems are increasingly more complex, making them hard to tune and difficult to apply to new problems and genres as well. Rule"
J13-4004,W12-4501,0,0.631873,"Missing"
J13-4004,W11-1901,0,0.834001,"ang Zhou and a consultant. . . are removed in this stage. 4. Experimental Results We start this section with overall results on three corpora widely used for the evaluation of coreference resolution systems. We continue with a series of ablative experiments that analyze the contribution of each aspect of our approach and conclude with error analysis, which highlights cases currently not solved by our approach. 4.1 Corpora We used the following corpora for development and formal evaluation: r r r r r OntoNotes-Dev – development partition of OntoNotes v4.0 provided in the CoNLL2011 shared task (Pradhan et al. 2011). OntoNotes-Test – test partition of OntoNotes v4.0 provided in the CoNLL-2011 shared task. ACE2004-Culotta-Test – partition of the ACE 2004 corpus reserved for testing by several previous studies (Culotta et al. 2007; Bengtson and Roth 2008; Haghighi and Klein 2009). ACE2004-nwire – newswire subset of the ACE 2004 corpus, utilized by Poon and Domingos (2008) and Haghighi and Klein (2009) for testing. MUC6-Test – test corpus from the sixth Message Understanding Conference (MUC-6) evaluation. The corpora statistics are shown in Table 3. We used the first corpus (OntoNotes-Dev) for development a"
J13-4004,D10-1048,1,0.78311,"s of i and j but also any information (head word, named entity type, gender, or number) about the other mentions already linked to i and j in previous steps. Finally, the architecture is highly modular, which means that additional coreference resolution models can be easily integrated. The two stage architecture offers a powerful way to balance both high recall and precision in the system and make use of entity-level information with rule-based architecture. The mention detection stage heavily favors recall, and the following sieves favor precision. Our results here and in our earlier papers (Raghunathan et al. 2010; Lee et al. 2011) show that this design leads to state-of-the-art performance despite the simplicity of the individual components, and that the lack of language-specific lexical features makes the system easy to port to other languages. The intuition is not new; in addition to the prior coreference work mentioned earlier and discussed in Section 6, we draw on classic ideas that have proved to be important again and again in the history of natural language processing. The idea of beginning with the most accurate models or starting with smaller subproblems that allow for high-precision solution"
J13-4004,D09-1101,0,0.0519169,"Missing"
J13-4004,P10-1144,0,0.0676267,"Missing"
J13-4004,N13-1110,0,0.0170762,"Missing"
J13-4004,W11-1903,0,0.0811866,"Missing"
J13-4004,W12-4514,0,0.0215518,"Missing"
J13-4004,J01-4004,0,0.898996,"Missing"
J13-4004,N10-1116,0,0.0385068,"Missing"
J13-4004,C12-1154,0,0.107033,"Missing"
J13-4004,J00-4003,0,0.143805,"Labor Party wants credit controls. • Parser or NER error: Um alright uh Mister Zalisko do you know anything from your personal experience of having been on the cruise as to what happened? – Mister Zalisko is not recognized as a PERSON • Enumerations: This year, the economies of the five large special economic zones, namely, Shenzhen, Zhuhai, Shantou, Xiamen and Hainan, have maintained strong growth momentum. . . . A three dimensional traffic frame in Zhuhai has preliminarily taken shape and the investment environment improves daily. add more sophisticated anaphoricity detection to our system (Vieira and Poesio 2000; Ng and Cardie 2002a; Poesio et al. 2004b; Boyd, Gegg-Harrison, and Byron 2005; Gupta, Purver, and Jurafsky 2007; Bergsma, Lin, and Goebel 2008; Ng 2009). Event mentions. Our system was tailored for the resolution of entity coreference and does not have any event-specific features, such as, for example, matching event participants. Furthermore, our model considers only noun phrases as antecedent candidates, thus missing all mentions that are verbal phrases. Therefore, our system misses most coreference links between event mentions. For example, in Table 12 the pronoun That 907 Computational L"
J13-4004,M95-1005,0,0.945762,"entions 303 322 107 128 30 6,894 8,262 1,993 3,594 576 136K 142K 33K 74K 13K 3,752 3,926 2,576 4,762 496 14,291 16,291 5,455 11,398 2,136 Lee et al. Deterministic Coreference Resolution Based on Entity-Centric, Precision-Ranked Rules trees and named entity labels (not gold) in the OntoNotes corpora to facilitate the comparison with other systems. 4.2 Evaluation Metrics We use five evaluation metrics widely used in the literature. B3 and CEAF have implementation variations in how to take system mentions into account. We followed the same implementation as used in CoNLL-2011 shared task. r MUC (Vilain et al. 1995) – link-based metric which measures how many predicted and gold mention clusters need to be merged to cover the gold and predicted clusters, respectively.  (|Gi |−|p(Gi ) |)  (Gi : a gold mention cluster, p(Gi ): partitions of Gi ) R= (|G |−1)  r r r r P= F1 = i (|Si |−|p(Si ) |)  (|Si |−1) 2PR P+ R (Si : a system mention cluster, p(Si ): partitions of Si ) B3 (Bagga and Baldwin 1998) – mention-based metric which measures the proportion of overlap between predicted and gold mention clusters for a given mention. When Gmi is the gold cluster of mention mi and Smi is the system cluster of men"
J13-4004,W12-3204,1,0.511811,"62.1 60.1 65.5 61.4 56.8 55.0 58.8 68.4 37.2 63.9 62.1 59.8 59.5 59.6 61.5 51.6 55.5 35.2 69.5 68.3 62.2 64.5 73.2 77.1 53.9 54.4 55.5 70.6 65.2 76.7 70.3 62.2 52.5 73.4 70.2 68.2 70.0 66.7 68.7 67.3 67.3 62.5 62.2 61.3 61.2 resources. For the closed track, the organizers provided dictionaries for gender and number information, in addition to parse trees and named entity labels (Pradhan et al. 2011). For the open track, we used the following additional resources: (a) a hand-built list of genders of first names that we created, incorporating frequent names from census lists and other sources (Vogel and Jurafsky 2012) (b) an animacy list (Ji and Lin 2009), (c) a country and state gazetteer, and (d) a demonym list. These resources were also used for the results reported in Table 5. A significant difference between Tables 4 and 5 is that in the former (other than its last block) we used predicted mentions (detected with the algorithm described in Section 3.1), whereas in the latter we used gold mentions. The only reason for this distinction is to facilitate comparison with previous work (all systems listed in Table 5 used gold mention boundaries). The two tables show that, regardless of evaluation corpus and"
J13-4004,W12-4506,0,0.0271675,"Missing"
J13-4004,P07-1067,0,0.0314736,"Missing"
J13-4004,P08-1096,0,0.147619,"ng, and information extraction. The long history of coreference resolution has shown that the use of highly precise lexical and syntactic features is crucial to high quality resolution (Ng and Cardie 2002b; Lappin and Leass 1994; Poesio et al. 2004a; Zhou and Su 2004; Bengtson and Roth 2008; Haghighi and Klein 2009). Recent work has also shown the importance of global inference—performing coreference resolution jointly for several or all mentions in a document—rather than greedily disambiguating individual pairs of mentions (Morton 2000; Luo et al. 2004; Yang et al. 2004; Culotta et al. 2007; Yang et al. 2008; Poon and Domingos 2008; Denis and Baldridge 2009; Rahman and Ng 2009; Haghighi and Klein 2010; Cai, Mujdricza-Maydt, and Strube 2011). Modern systems have met this need for carefully designed features and global or entity-centric inference with machine learning approaches to coreference resolution. But machine learning, although powerful, has limitations. Supervised machine learning systems rely on expensive hand-labeled data sets and generalize poorly to new words or domains. Unsupervised systems are increasingly more complex, making them hard to tune and difficult to apply to new problems"
J13-4004,C04-1033,0,0.0520591,"ks like summarization, question answering, and information extraction. The long history of coreference resolution has shown that the use of highly precise lexical and syntactic features is crucial to high quality resolution (Ng and Cardie 2002b; Lappin and Leass 1994; Poesio et al. 2004a; Zhou and Su 2004; Bengtson and Roth 2008; Haghighi and Klein 2009). Recent work has also shown the importance of global inference—performing coreference resolution jointly for several or all mentions in a document—rather than greedily disambiguating individual pairs of mentions (Morton 2000; Luo et al. 2004; Yang et al. 2004; Culotta et al. 2007; Yang et al. 2008; Poon and Domingos 2008; Denis and Baldridge 2009; Rahman and Ng 2009; Haghighi and Klein 2010; Cai, Mujdricza-Maydt, and Strube 2011). Modern systems have met this need for carefully designed features and global or entity-centric inference with machine learning approaches to coreference resolution. But machine learning, although powerful, has limitations. Supervised machine learning systems rely on expensive hand-labeled data sets and generalize poorly to new words or domains. Unsupervised systems are increasingly more complex, making them hard to tune"
J13-4004,W12-4507,0,0.0515344,"Missing"
J13-4004,W12-4510,0,0.0210587,"Missing"
J13-4004,C04-1075,0,0.276472,"tem thus offers a new paradigm for combining knowledge in rule-based systems that has implications throughout computational linguistics. 1. Introduction Coreference resolution, the task of finding all expressions that refer to the same entity in a discourse, is important for natural language understanding tasks like summarization, question answering, and information extraction. The long history of coreference resolution has shown that the use of highly precise lexical and syntactic features is crucial to high quality resolution (Ng and Cardie 2002b; Lappin and Leass 1994; Poesio et al. 2004a; Zhou and Su 2004; Bengtson and Roth 2008; Haghighi and Klein 2009). Recent work has also shown the importance of global inference—performing coreference resolution jointly for several or all mentions in a document—rather than greedily disambiguating individual pairs of mentions (Morton 2000; Luo et al. 2004; Yang et al. 2004; Culotta et al. 2007; Yang et al. 2008; Poon and Domingos 2008; Denis and Baldridge 2009; Rahman and Ng 2009; Haghighi and Klein 2010; Cai, Mujdricza-Maydt, and Strube 2011). Modern systems have met this need for carefully designed features and global or entity-centric inference with mach"
J13-4004,D12-1045,1,\N,Missing
J13-4004,J14-4004,0,\N,Missing
J17-2005,W13-2322,0,0.00982113,"n. Closer to the other end of the formality continuum, several approaches were proposed to not only select a correct answer, but also provide a formally valid justification for that answer. For example, some QA systems have sought to answer questions by creating formal proofs driven by logic reasoning (Moldovan et al. 2003a, 2007; Balduccini, Baral, and Lierler 2008; MacCartney 2009; Lewis and Steedman 2013; Liang, Jordan, and Klein 2013) answer-set programming (Tari and Baral 2006; Baral, Liang, and Nguyen 2011; Baral and Liang 2012; Baral, Vo, and Liang 2012), or connecting semantic graphs (Banarescu et al. 2013; Sharma et al. 2015). However, the formal representations used in these systems (e.g., logic forms) are both expensive to generate and tend to be brittle because they rely extensively on imperfect tools such as complete syntactic analysis and word sense disambiguation. We offer the lightly structured sentence representation generated by our approach (see Section 5) as a shallower and consequently more robust approximation of those logical forms, and show that they are well-suited for the complexity of our questions. Our approach allows us to robustly aggregate information from a variety of kn"
J17-2005,J08-1001,0,0.061091,"Missing"
J17-2005,J05-3002,0,0.0367337,"ur learning models to linear classifiers that learn efficiently at this scale. However, as discussed, tree kernels offer distinct advantages over linear models. We leave the adaptation of tree kernels to the problem discussed here as future work. Information aggregation (or fusion) is broadly defined as the assembly of knowledge from different sources, and has been used in several NLP applications, including summarization and QA. In the context of summarization, information aggregation has been used to assemble summaries from non-contiguous text fragments (Barzilay, McKeown, and Elhadad 1999; Barzilay and McKeown 2005, inter alia), whereas in QA, aggregation has been used to assemble answers to both factoid questions (Pradhan et al. 2002) and definitional questions (Blair-Goldensohn, McKeown, and Schlaikjer 2003). Critical to the current work, in an in-depth open-domain QA error analysis, Moldovan et al. (2003b) identified a subset of questions for which information from a single source is not sufficient, and designated a separate class within their taxonomy of QA systems for those systems that were capable of performing answer fusion. Combining multiple sources, however, creates the need for context disam"
J17-2005,P99-1071,0,0.151159,"Missing"
J17-2005,P14-1005,0,0.0147402,"ons are assembled, our method selects the answer that corresponds to the best (i.e., highest-scoring) justification. We learn which justifications are indicative of a correct answer by extending ranking perceptrons (Shen and Joshi 2005) that have been previously used in QA (Surdeanu, Ciaramita, and Zaragoza 2011) to include a latent layer that models the correctness of the justifications. Latent-variable perceptrons have been proposed for several other NLP tasks (Liang et al. 2006; Zettlemoyer and Collins 2007; Sun et al. 2009; Hoffmann et al. 2011; Fernandes, ¨ Dos Santos, and Milidiu´ 2012; Bjorkelund and Kuhn 2014), but to our knowledge, we are the first to adapt them to reranking scenarios. Finally, we round out our discussion of question answering systems with a comparison to the famous Watson QA system, which achieved performance on par with the 411 Computational Linguistics Volume 43, Number 2 human champions in the Jeopardy! game (Ferrucci 2012). Several of the ideas proposed in our work are reminiscent of Watson. For example, our component that generates text aggregation graphs (Section 5) shares functionality with the Prismatic engine used in Watson. Similar to Watson, we extract evidence from mu"
J17-2005,D14-1067,0,0.0127029,"Missing"
J17-2005,P16-1223,0,0.0349813,"Missing"
J17-2005,D14-1082,0,0.0212775,"vector), and continue to use the implementation of P that uses only the TAG with highest score. In this way, for each pair of candidate answers we have two justifications (one for the correct answer and one for the incorrect), and so perform a model update (i.e., backpropagation) for each. b) The combination of embeddings with explicit features that come from the CR system and the TAG features. This strategy of mixing latent and 432 Jansen et al. Framing QA as Building and Ranking Intersentence Answer Justifications explicit features has been demonstrated to be successful in other NLP tasks (Chen and Manning 2014; Suggu et al. 2016). Despite their advantages, neural networks have many hyperparameters that need to be tuned. Continuing the inspiration of Iyyer et al. (2015), we lightly turned the network on development, both in terms of network shape as well as additional parameters. Doing so, we arrived at a network that has a single dense layer with length 100 followed by the output layer of length one.14 For our word embeddings, we used a recurrent neural network language model (Mikolov et al. 2010, 2013) trained over a concatenation of all of our in-domain elementary science resources (i.e., the stu"
J17-2005,W02-1001,0,0.0379275,"cussed there, this performed far worse. Inference: During inference, the algorithm ranks all candidate answers for a given question in descending order of their score (as given by F, but using the averaged parameter vector, Θ avg ) and returns the top answer. Practical concerns: Two practical issues were omitted in Algorithm 1 for clarity, but improved performance in practice. First, we used the averaged percetron at inference 9 In fact, we found that performance consistently dropped as the number of justifications chosen by P increased. 423 Computational Linguistics Volume 43, Number 2 time (Collins 2002). That is, instead of using the latest Θ after training, we averaged all parameter vectors produced during training, and used the averaged vector, Θ avg , for inference. Second, we used a large-margin perceptron, similar to Surdeanu, Ciaramita, and Zaragoza (2011). In particular, we update the model not only when the predicted answer is incorrect (line 5), but also when the current model is not confident enough—that is, when the predicted answer is correct, but the difference in F scores between this answer and the second predicted answer is smaller than a small positive hyper parameter τ. 8."
J17-2005,P07-1033,0,0.0351743,"Missing"
J17-2005,P15-1026,0,0.0235445,"in a TAG. Eliminating this structure (i.e., considering each sentence as a bag of words, which is equivalent to a graphlet with a single nugget) substantially reduces the performance of the 1G + 2G model from 38.69 to 28.90 P@1 and the performance of the 1GCT + 2GCT model from 42.88 to 28.08 P@1. 8.5 From Latent Perceptron to Latent Neural Networks Neural networks (NNs) have recently received much attention in many NLP tasks and have been shown to be quite effective in certain question answering tasks (Bordes, Chopra, and Weston 2014; Iyyer et al. 2014; Bordes et al. 2015; Iyyer et al. 2015; Dong et al. 2015; Wang and Nyberg 2015; Yih et al. 2015; He and Lin 2016; Suggu et al. 2016). To determine if a deep learning approach would benefit our system, we explored providing our text aggregation graph features and the candidate retrieval scores to a neural ranking architecture. Under this framework we were also able to straightforwardly experiment with including vectorized representations (i.e., embeddings) of the question, candidate answer, and corresponding justification, in hopes of accommodating some of the lexical variation in natural language. Neural network systems have varied dramatically in"
J17-2005,P03-1003,0,0.0184502,"uss the results, and analyze the error classes observed, respectively. We conclude in Section 11. 409 Computational Linguistics Volume 43, Number 2 2. Related Work In one sense, QA systems can be described in terms of their position along a formality continuum ranging from shallow models that rely on information retrieval, lexical semantics, or alignment to highly structured models based on first-order logic. On the shallower end of the spectrum, QA models can be constructed either from structured text, such as question–answer pairs, or unstructured text. Alignment models (Berger et al. 2000; Echihabi and Marcu 2003; Soricut and Brill 2006; Riezler et al. 2007; Surdeanu, Ciaramita, and Zaragoza 2011; Yao et al. 2013) require aligned question– answer pairs for training, a burden that often limits their practical usage (though Sharp et al. [2015] recently proposed a method for using the discourse structure of free text as a surrogate for this alignment structure). Lexical semantic models such as neural-network language models (Yih et al. 2013; Jansen, Surdeanu, and Clark 2014; Sultan, Bethard, and Sumner 2014), on the other hand, have the advantage of being readily constructed from free text. Fried et al."
J17-2005,W12-4502,0,0.0178075,"Missing"
J17-2005,P10-1074,0,0.0158212,"dent, and 14 that are affixed by each of the connection types. For example, for a given feature such as numFocusQ, we create 15 copies: numFocusQ, numFocusQ1 ... numFocusQ14 , where the subscript (when present) indicates a specific connection type. For a TAG of connection type i, only values for the type-specific feature numFocusQi as well as the general feature numFocusQ are populated—all other numFocusQj features are set to a value of zero. This allows the learning algorithm in the next section to learn whether each feature generalizes across connection types, or not. As shown in Finkel and Manning (2010), this approach is equivalent to a joint model with a hierarchical prior. The features introduced in Table 4 apply to justifications containing any number of sentences, but characterizing justifications that are longer than two sentences is not straightforward, as the number of connection types (Figure 4) would become prohibitively large. We handle three-sentence justifications by treating each as a set of three two-sentence TAGs, and characterize each two-sentence connection individually. In the case where two groups of sentences have the same connection type, we take the highest scoring vers"
J17-2005,Q15-1015,1,0.523338,"nd Marcu 2003; Soricut and Brill 2006; Riezler et al. 2007; Surdeanu, Ciaramita, and Zaragoza 2011; Yao et al. 2013) require aligned question– answer pairs for training, a burden that often limits their practical usage (though Sharp et al. [2015] recently proposed a method for using the discourse structure of free text as a surrogate for this alignment structure). Lexical semantic models such as neural-network language models (Yih et al. 2013; Jansen, Surdeanu, and Clark 2014; Sultan, Bethard, and Sumner 2014), on the other hand, have the advantage of being readily constructed from free text. Fried et al. (2015) called these approaches first-order models because associations are explicitly learned, and introduced a higher-order lexical semantics QA model where indirect associations are detected through traversals of the association graph. Other recent efforts have applied deep learning architectures to QA to learn nonlinear answer scoring functions that model lexical semantics (Iyyer et al. 2014; Hermann et al. 2015). However, although lexical semantic approaches to QA have shown robust performance across a variety of tasks, a disadvantage of these methods is that, even when a correct answer is selec"
J17-2005,N16-1108,0,0.0200563,"each sentence as a bag of words, which is equivalent to a graphlet with a single nugget) substantially reduces the performance of the 1G + 2G model from 38.69 to 28.90 P@1 and the performance of the 1GCT + 2GCT model from 42.88 to 28.08 P@1. 8.5 From Latent Perceptron to Latent Neural Networks Neural networks (NNs) have recently received much attention in many NLP tasks and have been shown to be quite effective in certain question answering tasks (Bordes, Chopra, and Weston 2014; Iyyer et al. 2014; Bordes et al. 2015; Iyyer et al. 2015; Dong et al. 2015; Wang and Nyberg 2015; Yih et al. 2015; He and Lin 2016; Suggu et al. 2016). To determine if a deep learning approach would benefit our system, we explored providing our text aggregation graph features and the candidate retrieval scores to a neural ranking architecture. Under this framework we were also able to straightforwardly experiment with including vectorized representations (i.e., embeddings) of the question, candidate answer, and corresponding justification, in hopes of accommodating some of the lexical variation in natural language. Neural network systems have varied dramatically in terms of their primary architecture (feed-forward, convo"
J17-2005,P11-1055,0,0.0371042,"Missing"
J17-2005,D14-1070,0,0.0448051,"Missing"
J17-2005,P15-1162,0,0.0146041,"Missing"
J17-2005,J13-4004,1,0.725101,"hat question. We believe this intuition may be general and applicable to other domains. 414 Jansen et al. Framing QA as Building and Ranking Intersentence Answer Justifications Table 3 Syntactic patterns used to detect answer type words. Square brackets represent optional elements. Pattern Example (SBARQ (WHNP (WHNP (WDT) (NN)) [(PP)]... (SBARQ (WHNP (WP)) (SQ (VBZ is) (NP)... (S (NP (NP (DT A) (NN)) (SBAR (WHNP (WDT that)) ... (S (NP (NP) (PP)) (VP (VBZ is) ... What kind of energy ... What is one method that ... A tool that .. The main function of ... is to ... decreasing order of precision (Lee et al. 2013). Each of the five sieves attempts to assign question words into one of the following categories:4 1. Lists and sequences: Lists in questions generally contain highly important terms. We identify comma delimited lists of the form X, Y, ..., <and/or> Z (e.g., sleet, rain, and hail). Given the prevalence of questions that involve causal or process knowledge, we also identify from/to sequences (e.g., from a solid to a liquid) using paired prep from and prep to Stanford dependencies (De Marneffe and Manning 2008). 2. Focus words: Content lemmas (nouns, verbs, adjectives, and adverbs) with concrete"
J17-2005,P06-1096,0,0.144756,"Missing"
J17-2005,J13-2005,0,0.0223269,"Missing"
J17-2005,P14-5010,1,0.0129513,"Missing"
J17-2005,N03-1022,0,0.0955541,"Missing"
J17-2005,P01-1052,0,0.0742007,"Missing"
J17-2005,P04-1043,0,0.0376045,"a shallower and consequently more robust approximation of those logical forms, and show that they are well-suited for the complexity of our questions. Our approach allows us to robustly aggregate information from a variety of knowledge sources to create human-readable answer justifications. It is these justifications that are then ranked in order to choose the correct answer, using a reranking perceptron with a latent layer that models the correctness of those justifications. Covering the middle ground between shallow and formal representations, learning to rank methods based on tree-kernels (Moschitti 2004) perform well for various QA tasks, including passage reranking, answer sentence selection, or answer extraction (Moschitti et al. 2007; Moschitti and Quarteroni 2011; Severyn and Moschitti 2012, 2013; Severyn, Nicosia, and Moschitti 2013; Tymoshenko and Moschitti 2015, inter alia). The key to tree kernels’ success is their ability to automate feature engineering rather than having to rely on hand-crafted features, which allows them to explore a larger representation space. Further, tree kernels operate over structures that encode syntax and/or shallow semantics such as semantic role labeling"
J17-2005,P07-1098,0,0.0526244,"ity of our questions. Our approach allows us to robustly aggregate information from a variety of knowledge sources to create human-readable answer justifications. It is these justifications that are then ranked in order to choose the correct answer, using a reranking perceptron with a latent layer that models the correctness of those justifications. Covering the middle ground between shallow and formal representations, learning to rank methods based on tree-kernels (Moschitti 2004) perform well for various QA tasks, including passage reranking, answer sentence selection, or answer extraction (Moschitti et al. 2007; Moschitti and Quarteroni 2011; Severyn and Moschitti 2012, 2013; Severyn, Nicosia, and Moschitti 2013; Tymoshenko and Moschitti 2015, inter alia). The key to tree kernels’ success is their ability to automate feature engineering rather than having to rely on hand-crafted features, which allows them to explore a larger representation space. Further, tree kernels operate over structures that encode syntax and/or shallow semantics such as semantic role labeling (Severyn and Moschitti 410 Jansen et al. Framing QA as Building and Ranking Intersentence Answer Justifications 2012), knowledge from s"
J17-2005,P07-1059,0,0.0134058,"served, respectively. We conclude in Section 11. 409 Computational Linguistics Volume 43, Number 2 2. Related Work In one sense, QA systems can be described in terms of their position along a formality continuum ranging from shallow models that rely on information retrieval, lexical semantics, or alignment to highly structured models based on first-order logic. On the shallower end of the spectrum, QA models can be constructed either from structured text, such as question–answer pairs, or unstructured text. Alignment models (Berger et al. 2000; Echihabi and Marcu 2003; Soricut and Brill 2006; Riezler et al. 2007; Surdeanu, Ciaramita, and Zaragoza 2011; Yao et al. 2013) require aligned question– answer pairs for training, a burden that often limits their practical usage (though Sharp et al. [2015] recently proposed a method for using the discourse structure of free text as a surrogate for this alignment structure). Lexical semantic models such as neural-network language models (Yih et al. 2013; Jansen, Surdeanu, and Clark 2014; Sultan, Bethard, and Sumner 2014), on the other hand, have the advantage of being readily constructed from free text. Fried et al. (2015) called these approaches first-order mo"
J17-2005,D13-1044,0,0.0364372,"Missing"
J17-2005,W13-3509,0,0.0729068,"Missing"
J17-2005,N15-1025,1,0.912841,"Missing"
J17-2005,C16-1135,0,0.0588818,"Missing"
J17-2005,Q14-1018,0,0.0274658,"Missing"
J17-2005,J11-2003,1,0.51277,"Missing"
J17-2005,N03-2037,0,0.0353155,"ce-level context. Our approach works in two steps, both of which are performed offline (i.e., before the actual QA process). First, we decompose sentences that are likely to justify science exam answers (e.g., from knowledge bases such as study guides) into smaller units based on clausal and prepositional phrase boundaries. Intuitively, this allows us to maintain sufficient context to control semantic drift, while mitigating the sparsity of complete sentences. Following previous QA terminology, we call these smaller units, which represent clauses or prepositional phrases, information nuggets (Voorhees 2003). We connect two information nuggets within the same sentence if there are any syntactic dependencies that connect any words in these nuggets. We call the resulting graph of these information nuggets, which represents an entire sentence, a graphlet. Figure 2 shows a number of example graphlets. Formally, we transform sentences into graphlets using the following algorithm: 1. Parse knowledge base sentences: All knowledge base sentences are syntactically parsed using the Stanford CoreNLP toolkit (Manning et al. 2014). 2. Decompose sentences into information nuggets: For each sentence, beginning"
J17-2005,P15-2116,0,0.0745859,"Missing"
J17-2005,D13-1056,1,0.891877,"Missing"
J17-2005,P15-1128,0,0.0197819,".e., considering each sentence as a bag of words, which is equivalent to a graphlet with a single nugget) substantially reduces the performance of the 1G + 2G model from 38.69 to 28.90 P@1 and the performance of the 1GCT + 2GCT model from 42.88 to 28.08 P@1. 8.5 From Latent Perceptron to Latent Neural Networks Neural networks (NNs) have recently received much attention in many NLP tasks and have been shown to be quite effective in certain question answering tasks (Bordes, Chopra, and Weston 2014; Iyyer et al. 2014; Bordes et al. 2015; Iyyer et al. 2015; Dong et al. 2015; Wang and Nyberg 2015; Yih et al. 2015; He and Lin 2016; Suggu et al. 2016). To determine if a deep learning approach would benefit our system, we explored providing our text aggregation graph features and the candidate retrieval scores to a neural ranking architecture. Under this framework we were also able to straightforwardly experiment with including vectorized representations (i.e., embeddings) of the question, candidate answer, and corresponding justification, in hopes of accommodating some of the lexical variation in natural language. Neural network systems have varied dramatically in terms of their primary architecture (fe"
J17-2005,P13-1171,0,0.154955,"spectrum, QA models can be constructed either from structured text, such as question–answer pairs, or unstructured text. Alignment models (Berger et al. 2000; Echihabi and Marcu 2003; Soricut and Brill 2006; Riezler et al. 2007; Surdeanu, Ciaramita, and Zaragoza 2011; Yao et al. 2013) require aligned question– answer pairs for training, a burden that often limits their practical usage (though Sharp et al. [2015] recently proposed a method for using the discourse structure of free text as a surrogate for this alignment structure). Lexical semantic models such as neural-network language models (Yih et al. 2013; Jansen, Surdeanu, and Clark 2014; Sultan, Bethard, and Sumner 2014), on the other hand, have the advantage of being readily constructed from free text. Fried et al. (2015) called these approaches first-order models because associations are explicitly learned, and introduced a higher-order lexical semantics QA model where indirect associations are detected through traversals of the association graph. Other recent efforts have applied deep learning architectures to QA to learn nonlinear answer scoring functions that model lexical semantics (Iyyer et al. 2014; Hermann et al. 2015). However, alt"
J17-2005,D07-1071,0,0.0171921,"ntext, and as we demonstrate in Section 8, this boosts overall performance. Once the candidate answer justifications are assembled, our method selects the answer that corresponds to the best (i.e., highest-scoring) justification. We learn which justifications are indicative of a correct answer by extending ranking perceptrons (Shen and Joshi 2005) that have been previously used in QA (Surdeanu, Ciaramita, and Zaragoza 2011) to include a latent layer that models the correctness of the justifications. Latent-variable perceptrons have been proposed for several other NLP tasks (Liang et al. 2006; Zettlemoyer and Collins 2007; Sun et al. 2009; Hoffmann et al. 2011; Fernandes, ¨ Dos Santos, and Milidiu´ 2012; Bjorkelund and Kuhn 2014), but to our knowledge, we are the first to adapt them to reranking scenarios. Finally, we round out our discussion of question answering systems with a comparison to the famous Watson QA system, which achieved performance on par with the 411 Computational Linguistics Volume 43, Number 2 human champions in the Jeopardy! game (Ferrucci 2012). Several of the ideas proposed in our work are reminiscent of Watson. For example, our component that generates text aggregation graphs (Section 5)"
J17-2005,Q13-1015,0,\N,Missing
J17-2005,P11-1060,0,\N,Missing
J17-2005,P14-1092,1,\N,Missing
K17-1009,P15-1162,0,0.0715512,"Missing"
K17-1009,C16-1278,1,0.732006,"Missing"
K17-1009,P16-1223,0,0.04917,"Missing"
K17-1009,J17-2005,1,0.833369,"tionally lose all justification improvements from our system (see Section 6.2), demonstrating that learning this reranking is key to our approach. Additionally, we tracked the number of times a new justification was chosen by the model as it trained. We found that our system converges to a stable set of justifications during training, shown in Figure 3. the top-scoring justifications as re-ranked by our model. Each of these justifications was composed of a single sentence from our corpus, though a future version could use multi-sentence passages, or aggregate several sentences together, as in Jansen et al. (2017). Following the methodology of Jansen et al. (2017), each justification received a rating of either Good (if the connection between the question and correct answer was fully covered), Half (if there was a missing link), Topical (if the justification was simply of the right topic), or Off-Topic (if the justification was completely unrelated to the question). Examples of each rating are provided in Table 4. Results of this analysis are shown using three evaluation metrics in Table 5. The first two columns show the percentage of questions which had a Good justification at position 1 (Good@1), and"
K17-1009,D14-1082,0,0.333881,"n (Wang and Manning, 2010; Severyn and Moschitti, 2012, 2013; Severyn et al., 2013; Severyn and Moschitti, 2015; Wang and Nyberg, 2015, inter alia). Answer sentence selection is typically framed as a fully or semi-supervised task for factoid questions, where Related work In many ways, deep learning has become the canonical example of the ”black box” of machine learning and many of the approaches to explaining it can be loosely categorized into two types: approaches that try to interpret the parameters themselves (e.g., with visualizations and heat maps 70 Additionally, similar to the strategy Chen and Manning (2014) applied to parsing, we combine representation-based features with explicit features that capture additional information that is difficult to model through embeddings, especially with limited training data. The architecture of our approach is summarized in Figure 1. Given a question and a candidate answer, we first query an textual knowledge base (KB) to retrieve a pool of potential justifications for that answer candidate. For each justification, we extract a set of features designed to model the relations between questions, answers, and answer justifications based on word embeddings, lexical"
K17-1009,P14-1092,1,0.809068,"tification words that do not appear in either the question or the answer; and the length of the justification in words.4 Feature Extraction For each retrieved candidate justification, we extract a set of features based on (a) distributed representations of the question, candidate answer, and justification terms; (b) strict lexical overlap; (c) discourse relations present in the justification; and (d) the IR scores for the justification. Semi-Lexicalized Discourse features (lexDisc): These features use the discourse structure of the justification text, which has been shown to be useful for QA (Jansen et al., 2014; Sharp et al., 2015; Sachan et al., 2016). We use the discourse parser of Surdeanu et al. (2015) to fragment the text into elementary discourse units (EDUs) and then recursively connect neighboring EDUs with binary discourse relations. For each of the 18 possible relation labels, we create a set of semi-lexicalized discourse features that indicate the presence of a given discourse relation as well as whether or not the head Representation-based features (Emb): To model the similarity between the text of each question (Q), candidate answer (A), and candidate justification (J), we include a set"
K17-1009,P17-2049,1,0.847548,"Missing"
K17-1009,D16-1011,0,0.0482648,"Missing"
K17-1009,P14-2050,0,0.0145418,"beddings, we found performance was improved when using pre-trained embeddings, and in this low-data domain, fixing these embeddings to not update during training substantially reduced the amount of model over-fitting. In order to pretrain domain-relevant embeddings for our vocabulary, we used the documents from the StudyStack and Quizlet corpora, supplemented by the newly released Aristo MINI corpus (December 2016 release)9 , which contains 1.2M science-related sentences from various web sources. The training was done using the word2vec algorithm (Mikolov et al., 2010, 2013) as implemented by Levy and Goldberg (2014), such that the context for each word in a sentence is composed of all the other words in the same sentence. We used embeddings of size 50 as we did not see a performance improvement with higher dimensionality. 5.4 8 9 Model Random IR Baseline IR++ Iyyer et al. (2015) Khot et al. (2017) Our approach w/o IR P@1 Val 25 47.2 50.7∗∗ – – 50.54∗ P@1 Test 25 47 36.35 32.52 46.17 48.66 7 Our approach 54.0∗∗†† 53.3∗∗† Table 2: Performance on the AI2 Kaggle questions, measured by precision-at-one (P@1). ∗ s indicate that the difference between the corresponding model and the IR baseline is statistically"
K17-1009,P15-1026,0,0.073672,"Missing"
K17-1009,N16-1082,0,0.0398038,"Missing"
K17-1009,D16-1166,0,0.0664065,"Missing"
K17-1009,N15-1025,1,0.858117,"do not appear in either the question or the answer; and the length of the justification in words.4 Feature Extraction For each retrieved candidate justification, we extract a set of features based on (a) distributed representations of the question, candidate answer, and justification terms; (b) strict lexical overlap; (c) discourse relations present in the justification; and (d) the IR scores for the justification. Semi-Lexicalized Discourse features (lexDisc): These features use the discourse structure of the justification text, which has been shown to be useful for QA (Jansen et al., 2014; Sharp et al., 2015; Sachan et al., 2016). We use the discourse parser of Surdeanu et al. (2015) to fragment the text into elementary discourse units (EDUs) and then recursively connect neighboring EDUs with binary discourse relations. For each of the 18 possible relation labels, we create a set of semi-lexicalized discourse features that indicate the presence of a given discourse relation as well as whether or not the head Representation-based features (Emb): To model the similarity between the text of each question (Q), candidate answer (A), and candidate justification (J), we include a set of features that ut"
K17-1009,P14-5010,1,0.00874152,"esting dataset is not publicly available. 5.3 Corpora For our pool of candidate justifications (as well as the scores for our IR baselines) we used the corpora that were cited as being most helpful to the top-performing systems of the Kaggle challenge. These consisted of short, flash-card style texts gathered from two online resources: about 700K sentences from StudyStack7 and 25K sentences from Quizlet8 . From these corpora, we use the top 50 sentences retrieved by the IR model as our set of candidate justifications. All of our corpora were annotated using using the Stanford CoreNLP toolkit (Manning et al., 2014), the dependency parser of Chen and Manning (2014), and the discourse parser of Surdeanu et al. (2015). While our model is able to learn a set of embeddings, we found performance was improved when using pre-trained embeddings, and in this low-data domain, fixing these embeddings to not update during training substantially reduced the amount of model over-fitting. In order to pretrain domain-relevant embeddings for our vocabulary, we used the documents from the StudyStack and Quizlet corpora, supplemented by the newly released Aristo MINI corpus (December 2016 release)9 , which contains 1.2M sc"
K17-1009,N15-3001,1,0.874346,"Missing"
K17-1009,P16-1044,0,0.0439936,"Missing"
K17-1009,D16-1244,0,0.092624,"Missing"
K17-1009,N16-3020,0,0.206761,"Missing"
K17-1009,P15-2116,0,0.0612714,"Missing"
K17-1009,P16-2076,0,0.0260312,"ther the question or the answer; and the length of the justification in words.4 Feature Extraction For each retrieved candidate justification, we extract a set of features based on (a) distributed representations of the question, candidate answer, and justification terms; (b) strict lexical overlap; (c) discourse relations present in the justification; and (d) the IR scores for the justification. Semi-Lexicalized Discourse features (lexDisc): These features use the discourse structure of the justification text, which has been shown to be useful for QA (Jansen et al., 2014; Sharp et al., 2015; Sachan et al., 2016). We use the discourse parser of Surdeanu et al. (2015) to fragment the text into elementary discourse units (EDUs) and then recursively connect neighboring EDUs with binary discourse relations. For each of the 18 possible relation labels, we create a set of semi-lexicalized discourse features that indicate the presence of a given discourse relation as well as whether or not the head Representation-based features (Emb): To model the similarity between the text of each question (Q), candidate answer (A), and candidate justification (J), we include a set of features that utilize distributed repr"
K17-1009,C10-1131,0,0.0917657,"Missing"
K17-1009,D13-1044,0,0.0592355,"Missing"
K17-1009,W13-3509,0,0.0707555,"Missing"
L16-1027,W11-1902,1,0.795792,"ous study concerned the class II paired box gene Pax8, and its interaction with Smad3. Finally, domain general systems are able to make assumptions that do not hold in this domain. For example, in the general domain, a mention of Barack or Obama is likely to corefer with the more complete mention President Barack Obama. However, in the biomedical domain, entity names overlap to a great extent, and “glycogen synthase kinase 3 beta” is a different entity than “glycogen”. The contributions of this work are twofold. First, we adapt a sieve-based coreference resolution algorithm (Lee et al., 2013; Lee et al., 2011) to the biomedical domain, capitalizing on domain-specific knowledge, extending the biomedical information extraction system of Valenzuela-Esc´arcega 1 Bold face text denotes an anaphor, and italicized text denotes the antecedent chosen by the approach in discussion. Importantly, for entity resolution, we focus only on entities that participate in biochemical events; an underline denotes the anchor phrase of the corresponding event. The asterisk indicates an incorrect resolution. 177 et al. (2015). Importantly, our extensions address both entity and event (i.e., interaction) coreference resolu"
L16-1027,J13-4004,1,0.931203,"r effective reading and assembly, as it is crucial for deciding which spans of text refer to the same entity in the real world, which can have such disparate labels as “the protein”, “both”, or “ASPP1”. However, with the notable exception of the BioNLP shared task (Kim et al., 2013), coreference has rarely been a focus of reading-based work. Domain-general coreference systems perform poorly in this domain, because they fail to capitalize on domain-specific constraints on possible coreference relations. This is illustrated in example 1a, in which a domain-general coreference resolution system (Lee et al., 2013) would link its to GSK3β because of a generally trustworthy heuristic that the earliest named entity in the sentence is likely to be the antecedent of a pronoun if they match grammatically (Hobbs, 1978). The domain-specific knowledge that a protein binding to itself would not be referred to this way allows us to rule this link out and instead correctly choose Axin GBD, as in 1b. (1) a. b. * . . . we incubated GSK3β with excess Axin GBD protein to saturate its binding to GSK3β . . . 1 . . . we incubated GSK3β with excess Axin GBD protein to saturate its binding to GSK3β . . . Similarly, domain-"
L16-1027,P15-4022,1,0.863908,"Missing"
L16-1027,W13-2002,0,\N,Missing
L16-1050,X98-1004,0,0.115859,"al (Valenzuela-Esc´arcega et al., 2015a), which has been made available as an arXiv document1 . 1 2. Related Work Since the advent of FASTUS (Appelt et al., 1993), most rule-based IE frameworks implement architectures relying on a cascade of finite state automata (FSA). This approach has proven capable of producing fast and robust parsers for unstructured text (Abney, 1996). The success of FSA cascades continues even today with systems such as GATE (Cunningham et al., 2002). FASTUS introduced the Common Pattern Specification Language (CPSL) as a formalism for specifying cascaded FSA grammars (Appelt and Onyshkevych, 1998). A grammar in CPSL is specified by defining a cascade of finite state transducers that work by matching regular expressions over the lexical features of the input symbols. Other languages that follow CPSL’s approach of matching regular expressions over the lexical features of the input are GATE’s Java Annotation Patterns Engine (JAPE) (Thakker et al., 2009), Stanford’s TokensRegex (Chang and Manning, 2014), and the Allen Institute for Artificial Intelligence taggers2 . Odin follows in this lineage; however, unlike these approaches, Odin allows the mixing of both surface- and syntax-based rule"
L16-1050,W07-1427,0,0.0157554,"Missing"
L16-1050,levy-andrew-2006-tregex,0,0.0460516,"(De Marneffe and Manning, 2008a), the learning curve for Odin’s Runes is short. SProUT’s XTDL (Piskorski et al., 2004) extends CPSL’s approach using unification-based grammars to give the language more expressivity. However, this introduces additional complexity in the language. In our opinion, this is not always necessary in domain-specific scenarios, where lexical information fully disambiguates the context. Furthermore, similar to most previous work, XTDL does not support syntactic patterns. From the languages that support syntax, Stanford’s Tregex matches patterns over constituency trees (Levy and Andrew, 2006). For Odin’s Runes we chose to use dependency-based syntax for two reasons: simplicity of representation, and availability of linear-time parsers (Chen http://arxiv.org/abs/1509.07513v1 2 322 https://github.com/allenai/taggers WTX inhibits the ubiquitination of NRF2. • The ner rule converts the IOB output of an external NER tool into Odin entity mentions labeled Protein. In general, Odin mentions are data structures that store the output of a matched rule. For example, in this instance, the mention created by this rule captures the fact that the span of tokens from 1 to 2 (exclusive) and from"
L16-1050,P11-4019,0,0.110984,"e:Event • The negreg-surf rule matches a negative regulation event using a similar pattern. First, a protein mention is captured as the event theme, followed by a token with the following attributes: Example 1: Rules that capture the events shown in Figure 1. All the rules use surface patterns. and Manning, 2014). Semgrex is a language that modifies Tregex to operate over dependency graphs (Chambers et al., 2007)3 . However, neither of these languages support cascaded FSA. In a departure from CPSL, IBM’s SystemT is a rule-based IE system that uses the AQL language, which is inspired from SQL (Li et al., 2011). AQL is a powerful language that implements an IE algebra (Reiss et al., 2008). However, in our opinion, this loses some of the simplicity that Odin’s Runes enjoys. 3. Walkthrough Example In this section we show two Odin grammars in the biomedical domain as a gentle introduction to the language. Both grammars match over the sentence shown in Figure 1. All Odin grammars are encoded using YAML, which is a human-readable data serialization language (Ben-Kiki et al., 2005). YAML’s readability and support for comments were the main motivations for choosing it as the format for Odin’s Runes. Exampl"
L16-1050,P14-5010,1,0.00645987,"mation extraction (IE) framework (Valenzuela-Esc´arcega et al., 2015b). At the core of this framework is Odin’s Runes, our rule grammar language. The core feature of this language is supporting different types of rules, e.g., operating over surface or syntactic structures, which can interact in the same grammar. At a high level, the design of this rule language follows the simplicity principles promoted by other natural language processing (NLP) toolkits, such as Stanford’s CoreNLP, which aim to “avoid over-design”, “do one thing well”, and have a user “up and running in ten minutes or less” (Manning et al., 2014). In particular, we aimed for the following desirable characteristics: Simplicity: The language extends familiar concepts from regular expressions and context free grammars. Expressivity: The rules capture complex constructs when necessary, such as: (a) nested structures, and (b) complex regular expressions over syntactic patterns for event arguments. Robustness: To recover from unavoidable syntactic errors, syntactic patterns can be used alongside token-based surface patterns that incorporate shallow linguistic features. Extensibility: The language is designed to be modular, i.e., new types o"
L16-1050,E12-2021,0,0.0239742,"Missing"
L16-1050,W08-2121,1,0.289404,"Missing"
L16-1050,P15-4022,1,0.190178,"Missing"
L16-1050,W08-1301,0,\N,Missing
L16-1050,D14-1082,0,\N,Missing
L16-1472,N15-1019,0,0.0303163,"viduals can be labeled based on the known label of their home county, e.g., all individuals in an overweight county are overweight, which is less than ideal. In contrast, our work collects actual individual information through the survey derived from community information. Even though performing classification at state or county granularity tends to be robust and accurate (Fried et al., 2014), characteristics that are specific to individuals are more meaningful and practical. A wave of computational work on the automatic identification of latent attributes of individuals has recently emerged. Ardehaly and Culotta (2015) utilize label regularization, a lightly supervised learning method, to infer latent attributes of individuals, such as age and ethnicity. Other efforts have focused on inferring the gender of people on Twitter (Bamman et al., 2014; Burger et al., 2011) or their location on the basis of the text in their tweets (Cheng et al., 2010; Eisenstein et al., 2010). These are exciting approaches, but it is unlikely they will perform as well as a fully supervised model, which is the ultimate goal of our work. 3. Method Fried et al. (2014) showed that states and large cities generate a considerable numbe"
L16-1472,D11-1120,0,0.0284367,"information. Even though performing classification at state or county granularity tends to be robust and accurate (Fried et al., 2014), characteristics that are specific to individuals are more meaningful and practical. A wave of computational work on the automatic identification of latent attributes of individuals has recently emerged. Ardehaly and Culotta (2015) utilize label regularization, a lightly supervised learning method, to infer latent attributes of individuals, such as age and ethnicity. Other efforts have focused on inferring the gender of people on Twitter (Bamman et al., 2014; Burger et al., 2011) or their location on the basis of the text in their tweets (Cheng et al., 2010; Eisenstein et al., 2010). These are exciting approaches, but it is unlikely they will perform as well as a fully supervised model, which is the ultimate goal of our work. 3. Method Fried et al. (2014) showed that states and large cities generate a considerable number of food-related tweets, which can be used to infer important information about the respective community, such as overweight status or diabetes risk. In an initial experiment, we tested this classifier on the identification of overweight individuals. T"
L16-1472,D10-1124,0,0.0696759,"Missing"
L16-1472,N10-1021,0,0.0239455,"Missing"
L18-1098,W99-0613,0,0.12721,"m Learning Lastly, we add new terms to the pool of known terms (E) using the patterns previously learned. Terms are ranked by the following formula: 4 WNN(ck ) = ( X n2 n3 + ) × log( nl ) |Syn(ck )| |Ant(ck )| (3) l=1 ( where n1 , n2 , n3 , and n4 are the number of terms in the set intersections shown in Figure 1. The log component follows the intuition of (Riloff, 1996) to promote terms that are frequent; but here we adapt it to use the size of the overlap between WordNet synsets and the pools of known terms. We use the score in Eq. 1 in multiple ways. First, we implement two “cautiousness” (Collins and Singer, 1999) constraints, i.e., we accept only candidate terms that: (a) have all the following three conditions true: EDP≥ EDN, WNP≥ WNN, EMP≥ EMN (i.e., their association with the positive class is stronger than the one with the negative class under all resources); and (b) have at least one of the constraints satisfied: EDP−EDN ≡ 1.03 , WNP − WNN ≥ λ2 , EMP − EMN≥ λ3 . Second, we use the score to estimate the quality of extraction patterns, as detailed next. 3.2. Pattern Learning In this work, we define patterns similarly to McIntosh and Curran (2008), as five- or four-grams over surface tokens that inc"
L18-1098,W14-1611,0,0.349859,"n the desired category to be acquired. Yangarber (2003) proposed “counter training”, which introduces competition between the multiple categories (e.g., lexicon or event types) that are learned simultaneously (i.e., they are not allowed to overlap). This idea was generalized by the NELL system (Carlson et al., 2010). McIntosh and Curran (2010) extended counter training with negative categories that are discovered on the fly. Our approach is closest to counter training, with the extension that we propose multiple “soft” exclusion criteria. With respect to the better handling of unlabeled data, Gupta and Manning (2014) improved the scoring of extraction patterns by predicting the labels of unlabeled terms, and using this information to better estimate the precision of the candidate patterns. Gupta and Manning (2015) extended this idea with a k nearest neighbors (kNN) formulation that expands the labeled training data with unlabeled entities that are close (according to kNN) to seed examples. Popescu and Etzioni (2005) applied a similar idea to the extraction of opinion words, where the unlabeled terms are labeled using a combination of syntactic, WordNet constraints, and relaxation labeling. Our work builds"
L18-1098,N15-1128,0,0.0151731,"neously (i.e., they are not allowed to overlap). This idea was generalized by the NELL system (Carlson et al., 2010). McIntosh and Curran (2010) extended counter training with negative categories that are discovered on the fly. Our approach is closest to counter training, with the extension that we propose multiple “soft” exclusion criteria. With respect to the better handling of unlabeled data, Gupta and Manning (2014) improved the scoring of extraction patterns by predicting the labels of unlabeled terms, and using this information to better estimate the precision of the candidate patterns. Gupta and Manning (2015) extended this idea with a k nearest neighbors (kNN) formulation that expands the labeled training data with unlabeled entities that are close (according to kNN) to seed examples. Popescu and Etzioni (2005) applied a similar idea to the extraction of opinion words, where the unlabeled terms are labeled using a combination of syntactic, WordNet constraints, and relaxation labeling. Our work builds on these ideas with a simpler approach (no classifier is used). We also investigate more resources to measure the distance 613 Algorithm 1: Bootstrapping for emotion dimensions 1 2 3 4 5 6 7 8 9 10 11"
L18-1098,D10-1108,0,0.0400423,"ces (WordNet, word embeddings that project words in a continuous vector space that capture distributional similarity, and edit distance similarity) all help for the above two contributions, and are complementary to each other. 4. We empirically demonstrate that our approach outperforms several strong baselines for the acquisition of emotion dimensions lexicons from informal texts such as product reviews on the web. 2. Related Work In the vast bootstrapping literature, a few works attempted to address the two limitations mentioned in the introduction. With respect to mitigating semantic drift, Kozareva and Hovy (2010) used stronger constraints for their lexicon extraction patterns, encouraging them to stay within the desired category to be acquired. Yangarber (2003) proposed “counter training”, which introduces competition between the multiple categories (e.g., lexicon or event types) that are learned simultaneously (i.e., they are not allowed to overlap). This idea was generalized by the NELL system (Carlson et al., 2010). McIntosh and Curran (2010) extended counter training with negative categories that are discovered on the fly. Our approach is closest to counter training, with the extension that we pro"
L18-1098,P14-5010,1,0.0122286,"Missing"
L18-1098,U08-1013,0,0.25814,") // keep most relevant terms: E(k) = E(k) + getTopWithPolarityChecking(T (k)) output: E from known examples, ranging from WordNet (Miller et al., 1990) to word2vec (Mikolov et al., 2013), and show that they provide complementary information. 3. Approach Algorithm 1 lists our proposed algorithm that extracts lexicons corresponding to emotion dimension values (or categories). Our algorithm builds on the traditional bootstrapping approach, which starts with a small set of seed examples, and alternates between learning extraction patterns and using them to discover new information (Riloff, 1996; McIntosh and Curran, 2008). There are two fundamental differences between our approach and previous work. First, by using external information such as word embedding similarity, we expand the current set of acquired terms for each category, which are then softly labeled with category information (lines 4 – 5). The expanded term set is then used for the discovery of new extraction patterns (lines 6 – 8). Second, unlike McIntosh and Curran (2008)’s approach, which defined mutually exclusive categories, we remove the hard mutual-exclusivity constraint between categories. Instead, by taking advantage of the inherent polari"
L18-1098,D10-1035,0,0.0707425,"Missing"
L18-1098,H05-1043,0,0.177129,"discovered on the fly. Our approach is closest to counter training, with the extension that we propose multiple “soft” exclusion criteria. With respect to the better handling of unlabeled data, Gupta and Manning (2014) improved the scoring of extraction patterns by predicting the labels of unlabeled terms, and using this information to better estimate the precision of the candidate patterns. Gupta and Manning (2015) extended this idea with a k nearest neighbors (kNN) formulation that expands the labeled training data with unlabeled entities that are close (according to kNN) to seed examples. Popescu and Etzioni (2005) applied a similar idea to the extraction of opinion words, where the unlabeled terms are labeled using a combination of syntactic, WordNet constraints, and relaxation labeling. Our work builds on these ideas with a simpler approach (no classifier is used). We also investigate more resources to measure the distance 613 Algorithm 1: Bootstrapping for emotion dimensions 1 2 3 4 5 6 7 8 9 10 11 input : A set of documents D; seed words S for k emotion dimension values, k ∈ {praiseworthy, blameworthy, desirable, undesirable} Z = hi// stores extraction patterns for each k E = S// stores terms for ea"
L18-1098,P03-1044,0,0.0956869,"r the above two contributions, and are complementary to each other. 4. We empirically demonstrate that our approach outperforms several strong baselines for the acquisition of emotion dimensions lexicons from informal texts such as product reviews on the web. 2. Related Work In the vast bootstrapping literature, a few works attempted to address the two limitations mentioned in the introduction. With respect to mitigating semantic drift, Kozareva and Hovy (2010) used stronger constraints for their lexicon extraction patterns, encouraging them to stay within the desired category to be acquired. Yangarber (2003) proposed “counter training”, which introduces competition between the multiple categories (e.g., lexicon or event types) that are learned simultaneously (i.e., they are not allowed to overlap). This idea was generalized by the NELL system (Carlson et al., 2010). McIntosh and Curran (2010) extended counter training with negative categories that are discovered on the fly. Our approach is closest to counter training, with the extension that we propose multiple “soft” exclusion criteria. With respect to the better handling of unlabeled data, Gupta and Manning (2014) improved the scoring of extrac"
L18-1098,D12-1111,0,0.0282501,"the pool of category k as expansion candidates for k, and their antonyms as candidates for −k. It is important to note that these three resources have complementary strengths and weaknesses. For example, edit distance naturally captures misspellings, but it also introduces false positives, e.g., “goods” as a candidate for the dimension containing “good”. However, using the similarity of word embedding vectors mitigates this problem because the distributional similarity of the two words is low. On the other hand, antonyms (e.g., “good” vs. “bad”) tend to have a high distributional similarity (Yih et al., 2012). WordNet addresses the latter problem, but its coverage is far more limited than that of word embedding models, which limits its applicability to domain-specific texts. In order to allow these different resources to help each other, they have to interact. To do this, we assign to each candidate term produced above a score that combines the three resources: cscore(ck ) = EDP − EDN + EM P − EM N + W N P − W N N (1) where ck is a candidate term for category k. EDP and EDN use the discretized edit distance from the current positive/negative entities (i.e., terms belonging to category k vs. −k) of"
L18-1169,W06-2920,0,0.130019,"ment the more traditional reading of a parse tree. Additionally, it can be used to highlight the results of an event extraction process, and used as input for a structured search across other annotated documents. A third panel can be displayed on demand that provides the user with a range of options to change the visual appearance of text, to filter out particular annotation types, and to toggle on or off syntactic or semantic annotations. 3.1. Loading Data TAG is meant to be agnostic to data schema and currently supports the following formats: BRAT standoff (Stenetorp et al., 2012), CoNLL-X (Buchholz and Marsi, 2006), and bio-C (Comeau et al., 2013). Additional formats can be added as desired by following the import templates for the supported formats. The software looks for files in a special ‘data’ folder, and populates a drop down list to provide the option for the user to load in any of those data files. By choosing an item in this drop down list, which is located in the top left of the page, the data file is loaded into the TAG application, and populates the annotation panel. A taxonomy file can also be associated with a data file, which enables TAG to color annotations based on their taxonomic type,"
L18-1169,W16-4011,0,0.0314368,"Missing"
L18-1169,D17-2009,0,0.0123109,"s, a feature necessary to completely describe complex events. Complex relations are previously explored by the authors in a range of different visualization projects that represent hierarchically-nested and/or clustered data derived from the machine reading of scientific texts describing biochemical events (Dang et al., 2015; Dang et al., 2016; Dang and Forbes, 2017; Dang et al., 2017; Forbes et al., 2018; Murray et al., 2017; Paduano and Forbes, 2015; Paduano et al., 2016). In addition to BRAT, a range of newer projects investigate visual encodings for specific annotation tasks. For example, Sarnat et al. (2017) introduce a web interface for exploring a parse tree. By entering in a sentence, an interactive visualization is created that includes expand/collapse functionality, positional and color cues, explicit visual support for sequential structure, and dynamic highlighting to convey node-to-text correspondence. The tool includes an unusual representation of sequential structure as a container of linked nodes. While this may help a user to distinguish relevant structural elements, it demands a large portion of the screen, which can make it difficult to view relationships over longer sequences of tex"
L18-1169,E12-2021,0,0.091362,"Missing"
L18-1169,P15-4022,1,0.873754,"Missing"
L18-1169,P14-5016,0,0.048037,"Missing"
L18-1529,W15-0103,0,0.0226317,"to link gradable adjectives with numerical quantities that co-occur in the context of the gradable adjective mention (Shivade et al., 2016; Narisawa et al., 2013). However, this dependence on corpus resources to find evidence for gradability requires complex information extraction and suffers greatly from sparsity, especially when attempting to ground adjectives in a new domain. Additionally, it results in a solution that is highly domain-specific (Shivade et al., 2016). Kim and de Marneffe repurposed neural network language models by using word embeddings to rank gradable adjectives (2013). Bakhshandeh and Allen (2015) use bootstrapping to discover properties of adjectives, including what they can modify. However, rather than simply ranking adjectives or extracting their attributes, here we focus on determining a concrete, numerical grounding for each. There have also been recent works that use crowdsourcing to determine gradability of adjectives. For example, Qing and Franke (2014) use crowdsourcing to gather intuitions about the interpretation of gradable adjectives, but they are testing whether or not adjective usage corresponds to optimal language use, whereas our resource grounds gradable adjectives. A"
L18-1529,D13-1169,0,0.256969,"Missing"
L18-1529,W16-1607,0,0.267832,"Missing"
L18-1529,P13-1038,0,0.0298908,"in context, while removing the need to test each adjective along with each possible noun it could modify. Whitman et al. (2003) propose a model for finding perceptual groundings for adjectives. That is, they ground their adjectives using audio features and are then able to predict a lexical description of unheard music. This is quite similar to what we do in spirit, however our groundings are numerical rather than perceptual. Several works have attempted to link gradable adjectives with numerical quantities that co-occur in the context of the gradable adjective mention (Shivade et al., 2016; Narisawa et al., 2013). However, this dependence on corpus resources to find evidence for gradability requires complex information extraction and suffers greatly from sparsity, especially when attempting to ground adjectives in a new domain. Additionally, it results in a solution that is highly domain-specific (Shivade et al., 2016). Kim and de Marneffe repurposed neural network language models by using word embeddings to rank gradable adjectives (2013). Bakhshandeh and Allen (2015) use bootstrapping to discover properties of adjectives, including what they can modify. However, rather than simply ranking adjectives"
L18-1529,D14-1162,0,0.080278,"Missing"
L18-1529,W16-2903,0,0.0471359,"Missing"
L18-1529,W03-0613,0,0.048881,"Work Gradable adjectives have been experimentally demonstrated to be interpreted in part based on the semantics of the nouns they modify (i.e., a small mouse versus a small building) (Bonini et al., 1999; Alxatib and Pelletier, 2011; Bylinina, 2014). For this reason, we test our adjectives using legal non-words (e.g. mards), for which we provide a typical size distribution, and we include the provided distribution in the linear model. This allows us to model the semantics of the adjective in context, while removing the need to test each adjective along with each possible noun it could modify. Whitman et al. (2003) propose a model for finding perceptual groundings for adjectives. That is, they ground their adjectives using audio features and are then able to predict a lexical description of unheard music. This is quite similar to what we do in spirit, however our groundings are numerical rather than perceptual. Several works have attempted to link gradable adjectives with numerical quantities that co-occur in the context of the gradable adjective mention (Shivade et al., 2016; Narisawa et al., 2013). However, this dependence on corpus resources to find evidence for gradability requires complex informati"
L18-1529,L16-1424,0,0.0176212,"nterpretation of gradable adjectives, but they are testing whether or not adjective usage corresponds to optimal language use, whereas our resource grounds gradable adjectives. Accordingly, they test only four adjectives using visual cues, while we test 98 adjectives using numerical Figure 1: Example prompt given to Amazon Mechanical Turk workers to elicit the impact of gradable adjectives. Workers were given a specific distribution (of an imaginary item) and asked for the increase they perceived from the given adjective. The full set of prompts is included with our release. cues. The work by Wilkinson and Tim (2016) is closest to our work. They use crowdsourcing to create ranked lists of gradable adjectives that correspond to a variety of different scales (e.g., temperature, dimension, speed, and so on). Unlike Wilkinson and Tim, we use only one scale (magnitude) and again, we are interested in creating a concrete grounding for adjectives rather than a ranking. 3. Approach While humans often use high-level language to describe events, models of the interactions between these events require specific quantitative information. To bridge the gap between gradable adjectives and this quantified representation,"
lee-etal-2014-importance,P13-1086,0,\N,Missing
lee-etal-2014-importance,P12-2018,0,\N,Missing
lee-etal-2014-importance,W02-1011,0,\N,Missing
lee-etal-2014-importance,D11-1121,0,\N,Missing
lee-etal-2014-importance,N09-1031,0,\N,Missing
N10-1058,W05-0620,1,0.899021,"Missing"
N10-1058,A00-2018,0,0.0292806,"Missing"
N10-1058,P07-1028,0,0.14412,"t parsers and SRL systems use just lexical features, which can be seen as the most simple form of SP, 1 http://www.surdeanu.name/mihai/swirl/ 373 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 373–376, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics where the headword needs to be seen in the training data, and otherwise the SP is not satisfied. Gildea and Jurafsky (2002) showed barely significant improvements in semantic role classification of NPs for FrameNet roles using distributional clusters. In (Erk, 2007) a number of SP models are tested in a pseudo-task related to SRL. More recently, we showed (Zapirain et al., 2009) that several methods to automatically generate SPs generalize well and outperform lexical match in a large dataset for semantic role classification, but the impact on a full system was not explored. In this work we apply a subset of the SP methods proposed in (Zapirain et al., 2009). These methods can be split in two main families, depending on the resource used to compute similarity: WordNetbased methods and distributional methods. Both families define a similarity score between"
N10-1058,J02-3001,0,0.663127,"Classification SPs have been widely believed to be an important knowledge source when parsing and performing SRL, especially role classification. Still, present parsers and SRL systems use just lexical features, which can be seen as the most simple form of SP, 1 http://www.surdeanu.name/mihai/swirl/ 373 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 373–376, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics where the headword needs to be seen in the training data, and otherwise the SP is not satisfied. Gildea and Jurafsky (2002) showed barely significant improvements in semantic role classification of NPs for FrameNet roles using distributional clusters. In (Erk, 2007) a number of SP models are tested in a pseudo-task related to SRL. More recently, we showed (Zapirain et al., 2009) that several methods to automatically generate SPs generalize well and outperform lexical match in a large dataset for semantic role classification, but the impact on a full system was not explored. In this work we apply a subset of the SP methods proposed in (Zapirain et al., 2009). These methods can be split in two main families, dependi"
N10-1058,P98-2127,0,0.0232366,"two words was computed using the cosine (or Jaccard measure) of the co-occurrence vectors of the two words. Co-occurrence vectors where constructed using freely available software (Pad´o and Lapata, 2007) run over the British National Corpus. We used the optimal parameters (Pad´o and Lapata, 2007, p. 179). We will refer to these similarities as simcos and simJac , respectively. In contrast, second order similarity uses vectors of similar words, i.e., the similarity of two words was computed using the cosine (or Jaccard measure) between the thesaurus entries of those words in Lin’s thesaurus (Lin, 1998). We refer to these as sim2cos and sim2Jac . Given a target sentence with a verb and its arguments, the task of SR classification is to assign the correct role to each of the arguments. When using SPs alone, we only use the headwords of the ar374 guments, and each argument is classified independently of the rest. For each headword, we select the role (r) of the verb (c) which fits best the head word (w), where the goodness of fit (SPsim (v, r, w)) is modeled using one of the similarity models above, between the headword w and the headwords seen in training data for role r of verb v. This selec"
N10-1058,J07-2002,0,0.0496725,"Missing"
N10-1058,N07-1070,0,0.0906038,"that all systems showed a significant performance degradation (∼10 F1 points) when applied to test data from a different genre of that of the training Mihai Surdeanu Stanford NLP Group Stanford Univ. mihais@stanford.edu set. Pradhan et al. (2008) showed that this performance degradation is essentially caused by the argument classification subtask, and suggested the lexical data sparseness as one of the main reasons. The same authors studied the contribution of the different feature types in SRL and concluded that the lexical features were the most salient features in argument classification (Pradhan et al., 2007). In recent work, we showed (Zapirain et al., 2009) how automatically generated selectional preferences (SP) for verbs were able to perform better than pure lexical features in a role classification experiment, disconnected from a full-fledged SRL system. SPs introduce semantic generalizations on the type of arguments preferred by the predicates and, thus, they are expected to improve results on infrequent and unknown words. The positive effect was especially relevant for out-of-domain data. In this paper we advance (Zapirain et al., 2009) in two directions: (1) We learn separate SPs for prepo"
N10-1058,J08-2006,0,0.183777,"count. Semantic information is usually captured through lexicalized features on the predicate and the head–word of the argument to be classified. Since lexical features tend to be sparse, SRL systems are prone to overfit the training data and generalize poorly to new corpora. Indeed, the SRL evaluation exercises at CoNLL2004 and 2005 (Carreras and M`arquez, 2005) observed that all systems showed a significant performance degradation (∼10 F1 points) when applied to test data from a different genre of that of the training Mihai Surdeanu Stanford NLP Group Stanford Univ. mihais@stanford.edu set. Pradhan et al. (2008) showed that this performance degradation is essentially caused by the argument classification subtask, and suggested the lexical data sparseness as one of the main reasons. The same authors studied the contribution of the different feature types in SRL and concluded that the lexical features were the most salient features in argument classification (Pradhan et al., 2007). In recent work, we showed (Zapirain et al., 2009) how automatically generated selectional preferences (SP) for verbs were able to perform better than pure lexical features in a role classification experiment, disconnected fr"
N10-1058,H93-1054,0,0.563865,"Missing"
N10-1058,P09-2019,1,0.279698,"Missing"
N10-1058,C98-2122,0,\N,Missing
N10-1091,C96-1058,0,0.66391,"Missing"
N10-1091,D07-1097,0,0.436482,"29 83.02 76.18 82.02 Table 1: Labeled attachment scores (LAS) and unlabeled attachment scores (UAS) for the base models. The parsers are listed in descending order of LAS in the development partition. Introduction Several ensemble models have been proposed for the parsing of syntactic dependencies. These approaches can generally be classified in two categories: models that integrate base parsers at learning time, e.g., using stacking (Nivre and McDonald, 2008; Attardi and Dell’Orletta, 2009), and approaches that combine independently-trained models only at parsing time (Sagae and Lavie, 2006; Hall et al., 2007; Attardi and Dell’Orletta, 2009). In the latter case, the correctness of the final dependency tree is ensured by: (a) selecting entire trees proposed by the base parsers (Henderson and Brill, 1999); or (b) re-parsing the pool of dependencies proposed by the base models (Sagae and Lavie, 2006). The latter approach was shown to perform better for constituent parsing (Henderson and Brill, 1999). While all these models achieved good performance, the previous work has left several questions unanswered. Here we answer the following questions, in the context of English dependency parsing: 1. When co"
N10-1091,W99-0623,0,0.128418,"partition. Introduction Several ensemble models have been proposed for the parsing of syntactic dependencies. These approaches can generally be classified in two categories: models that integrate base parsers at learning time, e.g., using stacking (Nivre and McDonald, 2008; Attardi and Dell’Orletta, 2009), and approaches that combine independently-trained models only at parsing time (Sagae and Lavie, 2006; Hall et al., 2007; Attardi and Dell’Orletta, 2009). In the latter case, the correctness of the final dependency tree is ensured by: (a) selecting entire trees proposed by the base parsers (Henderson and Brill, 1999); or (b) re-parsing the pool of dependencies proposed by the base models (Sagae and Lavie, 2006). The latter approach was shown to perform better for constituent parsing (Henderson and Brill, 1999). While all these models achieved good performance, the previous work has left several questions unanswered. Here we answer the following questions, in the context of English dependency parsing: 1. When combining models at parsing time, what is the best scoring model for candidate dependencies during re-parsing? Can a meta classifier improve over unsupervised voting? 2. Are (potentially-expensive) re"
N10-1091,W08-2123,0,0.0225964,"Missing"
N10-1091,P08-1108,0,0.669281,".95 85.96 88.64 85.61 88.14 85.36 88.06 83.90 86.70 83.53 86.17 82.51 85.35 Out of domain LAS UAS 80.48 86.08 78.74 84.18 78.55 83.68 77.23 82.39 76.69 82.57 77.29 83.02 76.18 82.02 Table 1: Labeled attachment scores (LAS) and unlabeled attachment scores (UAS) for the base models. The parsers are listed in descending order of LAS in the development partition. Introduction Several ensemble models have been proposed for the parsing of syntactic dependencies. These approaches can generally be classified in two categories: models that integrate base parsers at learning time, e.g., using stacking (Nivre and McDonald, 2008; Attardi and Dell’Orletta, 2009), and approaches that combine independently-trained models only at parsing time (Sagae and Lavie, 2006; Hall et al., 2007; Attardi and Dell’Orletta, 2009). In the latter case, the correctness of the final dependency tree is ensured by: (a) selecting entire trees proposed by the base parsers (Henderson and Brill, 1999); or (b) re-parsing the pool of dependencies proposed by the base models (Sagae and Lavie, 2006). The latter approach was shown to perform better for constituent parsing (Henderson and Brill, 1999). While all these models achieved good performance,"
N10-1091,N06-2033,0,0.372368,"3 82.39 76.69 82.57 77.29 83.02 76.18 82.02 Table 1: Labeled attachment scores (LAS) and unlabeled attachment scores (UAS) for the base models. The parsers are listed in descending order of LAS in the development partition. Introduction Several ensemble models have been proposed for the parsing of syntactic dependencies. These approaches can generally be classified in two categories: models that integrate base parsers at learning time, e.g., using stacking (Nivre and McDonald, 2008; Attardi and Dell’Orletta, 2009), and approaches that combine independently-trained models only at parsing time (Sagae and Lavie, 2006; Hall et al., 2007; Attardi and Dell’Orletta, 2009). In the latter case, the correctness of the final dependency tree is ensured by: (a) selecting entire trees proposed by the base parsers (Henderson and Brill, 1999); or (b) re-parsing the pool of dependencies proposed by the base models (Sagae and Lavie, 2006). The latter approach was shown to perform better for constituent parsing (Henderson and Brill, 1999). While all these models achieved good performance, the previous work has left several questions unanswered. Here we answer the following questions, in the context of English dependency"
N10-1091,W08-2121,1,0.202862,"When combining models at parsing time, what is the best scoring model for candidate dependencies during re-parsing? Can a meta classifier improve over unsupervised voting? 2. Are (potentially-expensive) re-parsing strategies justified for English? What percentage of trees are not well-formed if one switches to a light word-by-word voting scheme? 3. How important is the integration of base parsers at learning time? 4. How do ensemble models compare against state-of-the-art supervised parsers? 2 Setup In our experiments we used the syntactic dependencies from the CoNLL 2008 shared task corpus (Surdeanu et al., 2008). We used seven base parsing models in this paper: six are variants of the Malt parser1 and the seventh is the projective version of MSTParser that uses only first-order features2 (or MST for short). The six Malt 1 http://maltparser.org/ http://sourceforge.net/projects/ mstparser/ 2 649 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 649–652, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics Unweighted # of parsers 3 4 5 6 7 LAS 86.03 86.79 86.98 87.14 86.81 UAS 89.44 90.14 90.33 90.51 90.21 Weighted by"
N10-1091,N09-2066,0,\N,Missing
N15-1025,J93-2003,0,0.0745619,"done using a shallow candidate retrieval (CR) component.6 Then, these answers are reranked using a more expressive model that incorporates alignment features alongside the CR score. As a learning framework we use SVMrank , a Support Vector Machine tailored for ranking.7 We compare this alignment-based reranking model against one that uses a state-of-the-art recurrent neural network language model (RNNLM) (Mikolov et al., 2010; Mikolov et al., 2013), which has been successfully applied to QA previously (Yih et al., 2013). Alignment Model: The alignment matrices were generated with IBM Model 1 (Brown et al., 1993) using GIZA++ (Och and Ney, 2003), and the corresponding models were implemented as per Surdeanu et al. (2011) with a global alignment probability. We extend this alignment model with features from Fried et al. (In press) that treat each (source) word’s probability distribution (over destination words) in the alignment matrix as a distributed semantic representation, and make use the Jensen-Shannon distance (JSD)8 between these conditional distributions. A summary of all these features is shown in Table 1. RNNLM: We learned word embeddings using the word2vec RNNLM of Mikolov et al. (2013), and"
N15-1025,P03-1003,0,0.119724,"al model, but the sequential model is an acceptable approximation when a discourse parser is not available. 3. We evaluate the proposed methods on two corpora, including a low-resource domain where training data is expensive (biology). 4. We experimentally demonstrate that monolingual alignment models trained using our method considerably outperform state-of-theart neural network language models in low resource domains. 2 Related Work Lexical semantic models have shown promise in bridging Berger et al.’s (2000) ”lexical chasm.” In general, these models can be classified into alignment models (Echihabi and Marcu, 2003; Soricut and Brill, 2006; Riezler et al., 2007; Surdeanu et al., 2011; Yao et al., 2013) which require structured training data, and language models (Jansen et al., 2014; Sultan et al., 2014; Yih et al., 2013), which operate over free text. Here, we close this gap in resource availability by developing a method to train an alignment model over free text by making use of discourse structures. Discourse has been previously applied to QA to help identify answer candidates that contain explanatory text (e.g. Verberne et al. (2007)). Jansen et al. (2014) proposed a reranking model that used both s"
N15-1025,P12-1007,0,0.13135,"pensive training data requirements, requiring a large set of aligned indomain question-answer pairs for training. For lowresource languages or specialized domains like science or biology, often the only option is to enlist a domain expert to generate gold QA pairs – a process that is both expensive and time consuming. All of this means that only in rare cases are we accorded the luxury of having enough high-quality QA pairs to properly train an alignment model, and so these models are often underutilized or left struggling for resources. Making use of recent advancements in discourse parsing (Feng and Hirst, 2012), here we address this issue, and investigate whether alignment models for QA can be trained from artificial question-answer pairs generated from discourse structures imposed on free text. We evaluate our methods on two corpora, generating alignment models for an opendomain community QA task using Gigaword2 , and for a biology-domain QA task using a biology textbook. 1 In practice, alignment for QA is often done from answer to question, as answers tend to be longer and provide more opportunity for association (Surdeanu et al., 2011). 2 LDC catalog number LDC2012T21 231 Human Language Technolog"
N15-1025,P14-1092,1,0.922176,"-resource domain where training data is expensive (biology). 4. We experimentally demonstrate that monolingual alignment models trained using our method considerably outperform state-of-theart neural network language models in low resource domains. 2 Related Work Lexical semantic models have shown promise in bridging Berger et al.’s (2000) ”lexical chasm.” In general, these models can be classified into alignment models (Echihabi and Marcu, 2003; Soricut and Brill, 2006; Riezler et al., 2007; Surdeanu et al., 2011; Yao et al., 2013) which require structured training data, and language models (Jansen et al., 2014; Sultan et al., 2014; Yih et al., 2013), which operate over free text. Here, we close this gap in resource availability by developing a method to train an alignment model over free text by making use of discourse structures. Discourse has been previously applied to QA to help identify answer candidates that contain explanatory text (e.g. Verberne et al. (2007)). Jansen et al. (2014) proposed a reranking model that used both shallow and deep discourse features to identify answer structures in large answer collections across different tasks and genres. Here we use discourse to impose structure"
N15-1025,W12-3018,0,0.0204879,"Missing"
N15-1025,J03-1002,0,0.0135375,"rieval (CR) component.6 Then, these answers are reranked using a more expressive model that incorporates alignment features alongside the CR score. As a learning framework we use SVMrank , a Support Vector Machine tailored for ranking.7 We compare this alignment-based reranking model against one that uses a state-of-the-art recurrent neural network language model (RNNLM) (Mikolov et al., 2010; Mikolov et al., 2013), which has been successfully applied to QA previously (Yih et al., 2013). Alignment Model: The alignment matrices were generated with IBM Model 1 (Brown et al., 1993) using GIZA++ (Och and Ney, 2003), and the corresponding models were implemented as per Surdeanu et al. (2011) with a global alignment probability. We extend this alignment model with features from Fried et al. (In press) that treat each (source) word’s probability distribution (over destination words) in the alignment matrix as a distributed semantic representation, and make use the Jensen-Shannon distance (JSD)8 between these conditional distributions. A summary of all these features is shown in Table 1. RNNLM: We learned word embeddings using the word2vec RNNLM of Mikolov et al. (2013), and include the cosine similarity-ba"
N15-1025,P07-1059,0,0.0766621,"approximation when a discourse parser is not available. 3. We evaluate the proposed methods on two corpora, including a low-resource domain where training data is expensive (biology). 4. We experimentally demonstrate that monolingual alignment models trained using our method considerably outperform state-of-theart neural network language models in low resource domains. 2 Related Work Lexical semantic models have shown promise in bridging Berger et al.’s (2000) ”lexical chasm.” In general, these models can be classified into alignment models (Echihabi and Marcu, 2003; Soricut and Brill, 2006; Riezler et al., 2007; Surdeanu et al., 2011; Yao et al., 2013) which require structured training data, and language models (Jansen et al., 2014; Sultan et al., 2014; Yih et al., 2013), which operate over free text. Here, we close this gap in resource availability by developing a method to train an alignment model over free text by making use of discourse structures. Discourse has been previously applied to QA to help identify answer candidates that contain explanatory text (e.g. Verberne et al. (2007)). Jansen et al. (2014) proposed a reranking model that used both shallow and deep discourse features to identify"
N15-1025,N03-1030,0,0.102502,"in-house parser (Surdeanu et al., 2015), which follows the architecture introduced by Hernault et al. (2010) and Feng and Hirst (2012). The parser first segments text into elementary discourse units (EDUs), which may be at sub-sentence granularity, then recursively connects neighboring units with binary discourse relations, such as Elaboration or Contrast.4 Our parser differs from previous work with respect to feature generation in that we implement all features that rely on syntax using solely dependency syntax. For example, a crucial feature used by the parser is the dominance relations of Soricut and Marcu (2003), which capture syntactic dominance between discourse units located in the same sentence. While originally these dominance relations were implemented using constituent syntax, we provide an equivalent implementation that relies on dependency syntax. The main advantage to this approach is speed: the resulting parser performs at least an order of magnitude faster than the parser of Feng and Hirst (2012). Importantly, we generate artificial alignment pairs from this imposed structure by aligning the governing text (nucleus) with its dependent text (satellite).5 Turning again to the example in Fig"
N15-1025,Q14-1018,0,0.0310099,"e training data is expensive (biology). 4. We experimentally demonstrate that monolingual alignment models trained using our method considerably outperform state-of-theart neural network language models in low resource domains. 2 Related Work Lexical semantic models have shown promise in bridging Berger et al.’s (2000) ”lexical chasm.” In general, these models can be classified into alignment models (Echihabi and Marcu, 2003; Soricut and Brill, 2006; Riezler et al., 2007; Surdeanu et al., 2011; Yao et al., 2013) which require structured training data, and language models (Jansen et al., 2014; Sultan et al., 2014; Yih et al., 2013), which operate over free text. Here, we close this gap in resource availability by developing a method to train an alignment model over free text by making use of discourse structures. Discourse has been previously applied to QA to help identify answer candidates that contain explanatory text (e.g. Verberne et al. (2007)). Jansen et al. (2014) proposed a reranking model that used both shallow and deep discourse features to identify answer structures in large answer collections across different tasks and genres. Here we use discourse to impose structure on free text to creat"
N15-1025,J11-2003,1,0.965745,"urces. Making use of recent advancements in discourse parsing (Feng and Hirst, 2012), here we address this issue, and investigate whether alignment models for QA can be trained from artificial question-answer pairs generated from discourse structures imposed on free text. We evaluate our methods on two corpora, generating alignment models for an opendomain community QA task using Gigaword2 , and for a biology-domain QA task using a biology textbook. 1 In practice, alignment for QA is often done from answer to question, as answers tend to be longer and provide more opportunity for association (Surdeanu et al., 2011). 2 LDC catalog number LDC2012T21 231 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 231–237, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics The contributions of this work are: 1. We demonstrate that by exploiting the discourse structure of free text, monolingual alignment models can be trained to surpass the performance of models built from expensive indomain question-answer pairs. 2. We compare two methods of discourse parsing: a simple sequential model, and a deep model based on Rhetorical St"
N15-1025,N15-3001,1,0.879427,"Missing"
N15-1025,D13-1056,1,0.788159,"Missing"
N15-1025,P13-1171,0,0.242378,"pensive (biology). 4. We experimentally demonstrate that monolingual alignment models trained using our method considerably outperform state-of-theart neural network language models in low resource domains. 2 Related Work Lexical semantic models have shown promise in bridging Berger et al.’s (2000) ”lexical chasm.” In general, these models can be classified into alignment models (Echihabi and Marcu, 2003; Soricut and Brill, 2006; Riezler et al., 2007; Surdeanu et al., 2011; Yao et al., 2013) which require structured training data, and language models (Jansen et al., 2014; Sultan et al., 2014; Yih et al., 2013), which operate over free text. Here, we close this gap in resource availability by developing a method to train an alignment model over free text by making use of discourse structures. Discourse has been previously applied to QA to help identify answer candidates that contain explanatory text (e.g. Verberne et al. (2007)). Jansen et al. (2014) proposed a reranking model that used both shallow and deep discourse features to identify answer structures in large answer collections across different tasks and genres. Here we use discourse to impose structure on free text to create inexpensive knowl"
N15-1025,Q15-1015,1,\N,Missing
N15-1066,P11-1040,0,0.395988,"ct magnitude is 7.3 (cf. Table 2). The second row shows a first tweet with ambiguous information, which leads our baseline model to extract the incorrect country (in bold; correct country is Peru). The following two tweets help disambiguate the context. The last row shows a tweet containing information (in bold) that is missing in the knowledge base. ing data by aligning a knowledge base of known event instances with tweets (Mintz et al., 2009; Hoffmann et al., 2011), which is then used to train a supervised extraction model (sequence tagger in our case). In seminal work on event extraction, (Benson et al., 2011) applied DS to both detect tweets about local events and then extracted values about two arguments (artist and venue). In our work, we work on automatically selected tweets, and scale the task to complex events with a large number of arguments. We focus on the domain of earthquakes, where each event has up to 20 arguments. Table 2 summarizes this task. The contributions of this work are the following: 1. To our knowledge, this is one of the first works that analyzes the problem of distantly supervised extraction of complex events with many arguments from microblogs. 2. Our analysis shows (Sect"
N15-1066,D12-1091,0,0.0457082,"Missing"
N15-1066,doddington-etal-2004-automatic,0,0.110275,"Missing"
N15-1066,C96-1079,0,0.731575,"Missing"
N15-1066,P11-1055,0,0.0609118,"Table 1: Challenges and opportunities for event extraction from Twitter. The first row shows a tweet with approximate information (in bold); the correct magnitude is 7.3 (cf. Table 2). The second row shows a first tweet with ambiguous information, which leads our baseline model to extract the incorrect country (in bold; correct country is Peru). The following two tweets help disambiguate the context. The last row shows a tweet containing information (in bold) that is missing in the knowledge base. ing data by aligning a knowledge base of known event instances with tweets (Mintz et al., 2009; Hoffmann et al., 2011), which is then used to train a supervised extraction model (sequence tagger in our case). In seminal work on event extraction, (Benson et al., 2011) applied DS to both detect tweets about local events and then extracted values about two arguments (artist and venue). In our work, we work on automatically selected tweets, and scale the task to complex events with a large number of arguments. We focus on the domain of earthquakes, where each event has up to 20 arguments. Table 2 summarizes this task. The contributions of this work are the following: 1. To our knowledge, this is one of the first"
N15-1066,P09-1113,0,0.859192,"tsunami alert issued Table 1: Challenges and opportunities for event extraction from Twitter. The first row shows a tweet with approximate information (in bold); the correct magnitude is 7.3 (cf. Table 2). The second row shows a first tweet with ambiguous information, which leads our baseline model to extract the incorrect country (in bold; correct country is Peru). The following two tweets help disambiguate the context. The last row shows a tweet containing information (in bold) that is missing in the knowledge base. ing data by aligning a knowledge base of known event instances with tweets (Mintz et al., 2009; Hoffmann et al., 2011), which is then used to train a supervised extraction model (sequence tagger in our case). In seminal work on event extraction, (Benson et al., 2011) applied DS to both detect tweets about local events and then extracted values about two arguments (artist and venue). In our work, we work on automatically selected tweets, and scale the task to complex events with a large number of arguments. We focus on the domain of earthquakes, where each event has up to 20 arguments. Table 2 summarizes this task. The contributions of this work are the following: 1. To our knowledge, t"
N15-1066,N10-1021,0,0.117399,"Missing"
N15-1066,reschke-etal-2014-event,1,0.817525,"considerable attention recently. Nevertheless, most of these works focus on the extraction of binary relations from well-formed documents (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). We use the much noisier Twitter as the underlying text, and extract complex events instead of binary relations. We note, however, that the idea of feature aggregation is inspired by these works (Mintz et al., 2009; Riedel et al., 2010), but, to our knowledge, we are the first to apply it to event extraction and sequence tagging. In the DS space, our work is closest to (Reschke et al., 2014), which use it to extract complex events (airplane crashes) from newswire text. Because they focus on newswire, they do not need to address the potential for inaccurate or ambiguous information, which is the main focus of our work. 8 Discussion: An alternate evaluation measure Designing relevant measures for lenient evaluations, such as the one discussed here, is an open research issue. For example, the method proposed in Section 4 gives partial credit to all reported (positive) numeric values in the interval [0, 2g], where g is the correct value for the corresponding slot (see the equation in"
N15-1066,D12-1042,1,0.880611,"notate two argument mentions in the first tweet from Table 1, country and affected-country, as follows: Earthquake in <country>Honduras</country>. So strong it was felt in <affectedcountry>Guatemala</affected-country> as well. 7.1 offshore atlantic. Note that the magnitude in the tweet is different from the one reported in the KB and it will thus be left unmarked (we revisit this issue in Section 5). Using this automatically-generated data, we trained a sequential tagger based on Conditional Random Fields (CRF)9 . Based on the output of the CRF, we inferred the arguments values using noisyor (Surdeanu et al., 2012), which selects the value with the largest probability for each single-valued argument by aggregating the individual mention probabilities produced by the CRF.10 In the case of multi-valued arguments (affected-country and affected-region) we choose all values that had been annotated by the sequential tagger. 3 Initial results and analysis The left block in Table 3 reports the results on development (5-fold cross-validation) of the initial event 8 We identified two types of arguments: those that have binary (yes/no) values (tsunami and landslides) and those having other values. For the first ty"
N15-3001,D14-1082,0,0.0797032,"2 http://nlp.stanford.edu/software/ corenlp.shtml 2 discourse units under consideration. The intuition behind this feature is that the more coreferential relations exist between two discourse units, the more likely they are to be directly connected. Using the above framework, we implemented two discourse parsers. The first uses CoreNLP for syntactic parsing and coreference resolution. This parser uses both constituent- and dependency-based features generated using the parser of Manning and Klein (2003). The second discourse parser uses either Malt3 or the recent neural-network-based parser of Chen and Manning (2014) for dependency parsing. The second discourse parser does not use constituent- nor coreference-based features. For all syntactic parsers, we used the “basic” Stanford dependency representation (de Marneffe et al., 2006). Empirically, we found that this representation yields better discourse parsing performance than any of the “collapsed” representations. 3 Analysis We analyze the performance of the two discourse parsers in Table 1. For conciseness, we identify the parser that uses both constituent- and dependencybased syntax and coreference resolution (all produced using CoreNLP) as C, and the"
N15-3001,de-marneffe-etal-2006-generating,0,0.111096,"Missing"
N15-3001,P12-1007,0,0.121273,"visualization tool that runs the two parsers in parallel, and displays the two generated discourse structures side by side. This allows users to directly compare the runtimes and outputs of the two parsers. This visualization tool will be the centerpiece of the proposed demo session. We summarize this tool in Section 5. Introduction This paper describes the design and development of two practical parsers for Rhetorical Structure Theory (RST) discourse (Mann and Thompson, 1988). This work contributes to the already vast body of research on RST parsing (see, inter alia, Soricut and Marcu, 2003; Feng and Hirst, 2012; Joty et al., 2013, Joty et al., 2014) with the following: 1. We propose two parsers that use constituentbased and dependency-based syntax, respectively. The underlying framework, other than the syntax-based features, is identical between the parsers, which permits a rigorous analysis of the impact of constituent and dependency syntax to RST parsing. We describe 2 The Two Parsers The proposed parsing approach follows the architecture introduced by Hernault et al. (2010), and Feng and Hirst (2012). The parser first segments the text into elementary discourse units (EDUs) using an i.i.d. classi"
N15-3001,P14-1092,1,0.531258,"o parsers, one built on top of constituent-based syntax, and the other that uses dependency-based syntax. Both parsers obtain state-of-the-art performance, are fast, and are easy to use through a simple API. In future work, we will aim at improving the performance of the parsers using joint parsing models. Nevertheless, it is important to note that RST parsers have already demonstrated their potential to improve natural language processing applications. For example, in our previous work we used features extracted from RST discourse relations to enhance a non-factoid question answering system (Jansen et al., 2014). In recent work, we showed how to use discourse relations to generate artificial training data for mono-lingual alignment models for question answering (Sharp et al., 2015). 7 8 https://grails.org 4 DiscourseTree ( Label of this tree, if non-terminal */ relationLabel:String, Direction of the relation, if non-terminal. It can be: LeftToRight, RightToLeft, or None. */ relationDir:RelationDirection.Value, Children of this non-terminal node */ children:Array[DiscourseTree], Raw text attached to this node */ rawText:String, Position of the first token in the text covered by this discourse tree */"
N15-3001,D14-1219,0,0.0605139,"proach that interleaves two classifiers: the first de1 Proceedings of NAACL-HLT 2015, pages 1–5, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics tects which two adjacent discourse units are most likely to be connected given the current sequence of units; and the second labels the corresponding relation. The resulting discourse unit produced by the new relation replaces its two children. The process repeats until there is a single discourse unit spanning the text.1 We chose this algorithm rather than other recent proposed approaches (Joty et al., 2013; Joty and Moschitti, 2014) because: (a) it promotes a simple, modular architecture; (b) it is fast, and (c) as we show later, it performs well. For classification, we experimented with Support Vector Machines (SVM), Perceptron, and Logistic Regression (LR). The results reported here use Perceptron for EDU segmentation and relation detection, and LR for relation labeling, thus offering a good balance between performance and quick training. With respect to features, our approach builds on previous work (Hernault et al., 2010; Feng and Hirst, 2012; Joty et al., 2013) and extends it in two ways. First, we implement all syn"
N15-3001,P13-1048,0,0.178098,"t runs the two parsers in parallel, and displays the two generated discourse structures side by side. This allows users to directly compare the runtimes and outputs of the two parsers. This visualization tool will be the centerpiece of the proposed demo session. We summarize this tool in Section 5. Introduction This paper describes the design and development of two practical parsers for Rhetorical Structure Theory (RST) discourse (Mann and Thompson, 1988). This work contributes to the already vast body of research on RST parsing (see, inter alia, Soricut and Marcu, 2003; Feng and Hirst, 2012; Joty et al., 2013, Joty et al., 2014) with the following: 1. We propose two parsers that use constituentbased and dependency-based syntax, respectively. The underlying framework, other than the syntax-based features, is identical between the parsers, which permits a rigorous analysis of the impact of constituent and dependency syntax to RST parsing. We describe 2 The Two Parsers The proposed parsing approach follows the architecture introduced by Hernault et al. (2010), and Feng and Hirst (2012). The parser first segments the text into elementary discourse units (EDUs) using an i.i.d. classifier that identifie"
N15-3001,P03-1054,0,0.0388932,"Missing"
N15-3001,P14-5010,1,0.0110175,"ormation has a small contribution to the overall performance (0.3 F1 points when gold EDUs are used). Nevertheless, we find this result exciting, considering that this is a first attempt at using coreference information for discourse parsing. 4 Usage With respect to usage, we adhere to the simplicity principles promoted by Stanford’s CoreNLP, which introduced a simple, concrete Java API rather than relying on heavier frameworks, such as UIMA (Ferrucci and Lally, 2004). This guarantees that a user is “up and running in ten minutes or less”, by “doing one thing well” and “avoiding over-design” (Manning et al., 2014). Following this idea, our API contains two Processor objects, one for each discourse parser, and a single method call, annotate(), which implements the complete analysis of a document (represented as a String), from tokenization to discourse parsing.5 Figure 1 shows sample API usage. The annotate() method produces a Document object, which stores all NLP annotations: tokens, part-of-speech tags, constituent trees, dependency graphs, coreference relations, and discourse trees. 5 Additional methods are provided for pre-existing tokenization and/or sentence segmentation. import edu.arizona.sista."
N15-3001,N15-1025,1,0.821397,"easy to use through a simple API. In future work, we will aim at improving the performance of the parsers using joint parsing models. Nevertheless, it is important to note that RST parsers have already demonstrated their potential to improve natural language processing applications. For example, in our previous work we used features extracted from RST discourse relations to enhance a non-factoid question answering system (Jansen et al., 2014). In recent work, we showed how to use discourse relations to generate artificial training data for mono-lingual alignment models for question answering (Sharp et al., 2015). 7 8 https://grails.org 4 DiscourseTree ( Label of this tree, if non-terminal */ relationLabel:String, Direction of the relation, if non-terminal. It can be: LeftToRight, RightToLeft, or None. */ relationDir:RelationDirection.Value, Children of this non-terminal node */ children:Array[DiscourseTree], Raw text attached to this node */ rawText:String, Position of the first token in the text covered by this discourse tree */ firstToken: TokenOffset, Position of the last token in the text covered by this discourse tree; this is inclusive! */ lastToken: TokenOffset https://github.com/cpettitt/dagr"
N15-3001,N03-1030,0,0.231517,". 3. We also introduce a visualization tool that runs the two parsers in parallel, and displays the two generated discourse structures side by side. This allows users to directly compare the runtimes and outputs of the two parsers. This visualization tool will be the centerpiece of the proposed demo session. We summarize this tool in Section 5. Introduction This paper describes the design and development of two practical parsers for Rhetorical Structure Theory (RST) discourse (Mann and Thompson, 1988). This work contributes to the already vast body of research on RST parsing (see, inter alia, Soricut and Marcu, 2003; Feng and Hirst, 2012; Joty et al., 2013, Joty et al., 2014) with the following: 1. We propose two parsers that use constituentbased and dependency-based syntax, respectively. The underlying framework, other than the syntax-based features, is identical between the parsers, which permits a rigorous analysis of the impact of constituent and dependency syntax to RST parsing. We describe 2 The Two Parsers The proposed parsing approach follows the architecture introduced by Hernault et al. (2010), and Feng and Hirst (2012). The parser first segments the text into elementary discourse units (EDUs)"
N15-3001,W01-1605,0,\N,Missing
N18-2057,W14-1611,0,0.0855329,"the particular entity mention. Related Work There is a long line of work in semi-supervised learning for NLP (Zhu, 2005; Abney, 2007). This encompasses many different types of techniques such as self-training or bootstrapping (Carlson et al., 2010a,b; McIntosh, 2010; Gupta and Manning, 2015, inter alia), co-training (Blum and Mitchell, 1998), or graph-based methods such as label propagation (Delalleau et al., 2005). Perhaps the most popular approach among them is selftraining, or bootstrapping, which has been used in many applications, including information extraction (Carlson et al., 2010a; Gupta and Manning, 2014, 2015), lexicon acquisition (Neelakantan and Collins, 2015), named entity classification (Collins and Singer, 1999) and sentiment analysis (Rao and Ravichandran, 2009). However, most of these approaches are iterative, and suffer from semantic drift (Komachi et al., 2008). Auto-encoder frameworks have been getting a lot of attention in the machine learning community recently. Such framewoks include recursive autoencoders (Socher et al., 2011), denoising autoencoders (Vincent et al., 2008), etc. They are primarily used as a pre-training mechanism before supervised training. Recently, such netwo"
N18-2057,N15-1128,0,0.438616,"re 1 for an example sentence snippet, an entity mention (in boldface) and its context. Using these as input, the classifier must infer that the mention’s correct label is person.2 For the NEC task, the embedding of a mention and its context is concatenated to produce X which is input to the ladder network to predict a label y for the particular entity mention. Related Work There is a long line of work in semi-supervised learning for NLP (Zhu, 2005; Abney, 2007). This encompasses many different types of techniques such as self-training or bootstrapping (Carlson et al., 2010a,b; McIntosh, 2010; Gupta and Manning, 2015, inter alia), co-training (Blum and Mitchell, 1998), or graph-based methods such as label propagation (Delalleau et al., 2005). Perhaps the most popular approach among them is selftraining, or bootstrapping, which has been used in many applications, including information extraction (Carlson et al., 2010a; Gupta and Manning, 2014, 2015), lexicon acquisition (Neelakantan and Collins, 2015), named entity classification (Collins and Singer, 1999) and sentiment analysis (Rao and Ravichandran, 2009). However, most of these approaches are iterative, and suffer from semantic drift (Komachi et al., 20"
N18-2057,D08-1106,0,0.351892,"and Manning, 2015, inter alia), co-training (Blum and Mitchell, 1998), or graph-based methods such as label propagation (Delalleau et al., 2005). Perhaps the most popular approach among them is selftraining, or bootstrapping, which has been used in many applications, including information extraction (Carlson et al., 2010a; Gupta and Manning, 2014, 2015), lexicon acquisition (Neelakantan and Collins, 2015), named entity classification (Collins and Singer, 1999) and sentiment analysis (Rao and Ravichandran, 2009). However, most of these approaches are iterative, and suffer from semantic drift (Komachi et al., 2008). Auto-encoder frameworks have been getting a lot of attention in the machine learning community recently. Such framewoks include recursive autoencoders (Socher et al., 2011), denoising autoencoders (Vincent et al., 2008), etc. They are primarily used as a pre-training mechanism before supervised training. Recently, such networks have also been used for semi-supervised learning as they are more amenable to combining supervised and unsupervised components of the objective functions (Zhai and Zhang, 2015). Ladder networks (LN) are stacked denoising auto-encoders with skip-connections in the inte"
N18-2057,P14-2050,0,0.0421317,"ilable in the scikit-learn package of the LP algorithm (Zhu and Ghahramani, 2002).5 In each bootstrapping epoch, we run LP, select the entities with the lowest entropy, and add them to their top category. Each entity is represented by a feature vector that contains the cooccurrence counts of the entity and each of the patterns that matches it in text.6 Settings: For each entity mention, we consider a n-gram window of size 4 on either side as a pattern. We initialized the mention and contexts embeddings input to the ladder network as well as the baseline system with pre-trained embeddings from Levy and Goldberg (2014) (size 300d) as this 4 We used pre-trained word representations, averaged for multi-word entities, to compute cosine similarities between pairs of entities. 5 http://scikit-learn.org/stable/modules/generated/ sklearn.semi_supervised.LabelPropagation.html 6 7 We also ran a cautious approach of promoting 10 entities per category per iteration and noticed that the former had better performance. We experimented with other feature values, e.g., pattern PMI scores, but all performed worse than raw counts. 355 Purity Purity 0.4 0.4 0.2 0.2 0.2 0.0 0 5 10 15 20 25 30 35 Epochs or Confidence score (in"
N18-2057,W99-0613,0,0.731618,", 2005; Abney, 2007). This encompasses many different types of techniques such as self-training or bootstrapping (Carlson et al., 2010a,b; McIntosh, 2010; Gupta and Manning, 2015, inter alia), co-training (Blum and Mitchell, 1998), or graph-based methods such as label propagation (Delalleau et al., 2005). Perhaps the most popular approach among them is selftraining, or bootstrapping, which has been used in many applications, including information extraction (Carlson et al., 2010a; Gupta and Manning, 2014, 2015), lexicon acquisition (Neelakantan and Collins, 2015), named entity classification (Collins and Singer, 1999) and sentiment analysis (Rao and Ravichandran, 2009). However, most of these approaches are iterative, and suffer from semantic drift (Komachi et al., 2008). Auto-encoder frameworks have been getting a lot of attention in the machine learning community recently. Such framewoks include recursive autoencoders (Socher et al., 2011), denoising autoencoders (Vincent et al., 2008), etc. They are primarily used as a pre-training mechanism before supervised training. Recently, such networks have also been used for semi-supervised learning as they are more amenable to combining supervised and unsupervi"
N18-2057,D10-1035,0,0.77154,"generating it requires costly human supervision. Semi-supervised learning addresses this challenge by combining limited supervision with a large, unannotated dataset, thereby mitigating the supervision cost. For NLP, bootstrapping is a popular approach to semi-supervised learning due its relative simplicity coupled with reasonable performance (Abney, 2007). However, a crucial limitation of bootstrapping, which is typically iterative, is that, as learning advances, the task often drifts semantically into a related but different space, e.g., from learning women names into learning flower names (McIntosh, 2010; Yangarber, 2003). In this paper, we propose an effective technique for semi-supervised learning for information extraction (IE), which obviates the need for an iterative approach, thereby mitigating the problem of semantic drift. Our technique is based on the (1) We provide a novel application of LNs to an IE task, in particular semi-supervised named entity classification (NEC). Our approach is simple: we concatenate embeddings of entity mentions with that of its context1 and feed the resulting vectors into the LN’s denoising auto-encoder. (2) We empirically demonstrate, for the task of semi"
N18-2057,E14-1048,0,0.0516437,"Missing"
N18-2057,W13-3516,0,0.0397815,"ts context embedding vectors generated as mentioned previously, and y represents one of the predicted mention labels (e.g. person). We introduce noise in this architecture by perturbing the embeddings with a standard Gaussian noise with a fixed standard deviation. The objective function is a combination of a supervised training cost and unsupervised reconstruction costs at each layer (including the hidden layers): N Cost = − X n=1 4 Datasets: We used two datasets, the CoNLL2003 shared task dataset (Tjong Kim Sang and De Meulder, 2003), which contains 4 entity types, and the OntoNotes dataset (Pradhan et al., 2013), which contains 113 , both of which are benchmark datasets for supervised named entity recognition (NER). These datasets contain marked entity boundaries with labels for each marked entity. Here we only use the entity boundaries but not the labels of these entities during the training of our bootstrapping systems. To simulate learning from large texts, we tuned hyper parameters on development, but ran the actual experiments on the train partitions. logP (y˜n = yn∗ |Xn )+ M L X X λl ReconstCost(Zn(l) , Zˆn(l) ) Experiments (4) Baselines: We compared against 2 baselines: n=N +1 l=1 Explicit Pat"
N18-2057,E09-1077,0,0.159775,"ent types of techniques such as self-training or bootstrapping (Carlson et al., 2010a,b; McIntosh, 2010; Gupta and Manning, 2015, inter alia), co-training (Blum and Mitchell, 1998), or graph-based methods such as label propagation (Delalleau et al., 2005). Perhaps the most popular approach among them is selftraining, or bootstrapping, which has been used in many applications, including information extraction (Carlson et al., 2010a; Gupta and Manning, 2014, 2015), lexicon acquisition (Neelakantan and Collins, 2015), named entity classification (Collins and Singer, 1999) and sentiment analysis (Rao and Ravichandran, 2009). However, most of these approaches are iterative, and suffer from semantic drift (Komachi et al., 2008). Auto-encoder frameworks have been getting a lot of attention in the machine learning community recently. Such framewoks include recursive autoencoders (Socher et al., 2011), denoising autoencoders (Vincent et al., 2008), etc. They are primarily used as a pre-training mechanism before supervised training. Recently, such networks have also been used for semi-supervised learning as they are more amenable to combining supervised and unsupervised components of the objective functions (Zhai and"
N18-2057,D11-1014,0,0.215657,"Missing"
N18-2057,P03-1044,0,0.651941,"quires costly human supervision. Semi-supervised learning addresses this challenge by combining limited supervision with a large, unannotated dataset, thereby mitigating the supervision cost. For NLP, bootstrapping is a popular approach to semi-supervised learning due its relative simplicity coupled with reasonable performance (Abney, 2007). However, a crucial limitation of bootstrapping, which is typically iterative, is that, as learning advances, the task often drifts semantically into a related but different space, e.g., from learning women names into learning flower names (McIntosh, 2010; Yangarber, 2003). In this paper, we propose an effective technique for semi-supervised learning for information extraction (IE), which obviates the need for an iterative approach, thereby mitigating the problem of semantic drift. Our technique is based on the (1) We provide a novel application of LNs to an IE task, in particular semi-supervised named entity classification (NEC). Our approach is simple: we concatenate embeddings of entity mentions with that of its context1 and feed the resulting vectors into the LN’s denoising auto-encoder. (2) We empirically demonstrate, for the task of semi-supervised NEC on"
N19-1274,C18-1139,0,0.160093,"porting paragraphs from a knowledge base (KB) given a question and candidate answer. Then AHE aligns each word in the question and candidate answer with the most similar word in the retrieved supporting paragraph, and weighs each alignment score with the inverse document frequency (IDF) of the corresponding question/answer term. AHE’s overall alignment score is the sum of the IDF weighted scores of each of the question/answer term. Importantly, AHE’s alignment function operates over contextualized embeddings that model the underlying text at different levels of abstraction: character (FLAIR) (Akbik et al., 2018), word (BERT) (Devlin et al., 2018), and sentence (InferSent) (Conneau et al., 2017), where the latter is the only supervised component in the proposed approach. The different representations are combined through an ensemble approach that by default is unsupervised (using a variant of the NoisyOr formula), but can be replaced with a supervised meta-classifier. The contributions of our work are the following: 1. To our knowledge, this is the first unsupervised alignment approach for QA that: (a) operates over contextualized embeddings, and (b) captures text at multiple levels of abstraction, in"
N19-1274,P06-4018,0,0.0432538,". We use as query the question concatenated with the corresponding answer candidate, and BM25 (Robertson et al., 2009) as the ranking function3 . For each query, we keep the top C Lucene documents, where each document consists of a sentence retrieved from the ARC corpus. Similar to our previous work (Yadav et al., 2018), we boost candidate answer terms by a factor of 3 while keeping question terms as it is in the BM25 ranking function. All texts were preprocessed by discarding the case of the tokens, removing the stop words from Lucene’s list, and lemmatizing the remaining tokens using NLTK (Bird, 2006). For all experiments reported on the ARC dataset we used C = 20. Here we also calculate the IDF of each query term qi (required later during alignment): N − docfreq(qi ) + 0.5 idf (qi ) = log (1) docfreq(qi ) + 0.5 where N is the number of documents (e.g., 14.3M for the ARC KB) and docf req(qi ) is the number of documents that contain qi . 3.2 Alignment Algorithm For representations that produce word embeddings (e.g., FLAIR, BERT, GloVe), we use the alignment algorithm in Figure 3. Our method computes the alignment score of each query token with every token in the given KB paragraph, using th"
N19-1274,J93-2003,0,0.0953459,"017), etc. Other works have utilized word alignments as features in supervised models (Surdeanu et al., 2011; Wang and Ittycheriah, 2015). For example, Wang and Ittycheriah (2015) utilized the alignment of words between two questions as a feature in a feedforward neural network that matches similar FAQ questions. Recently, Yadav et al. (2018) showed that alignment methods remain competitive for non-factoid QA. However, the majority of alignment models that rely on representation learning utilize uncontextualized word embeddings such as GloVe, coupled with other BoW models such as IBM Model 1 (Brown et al., 1993) for alignment (Kenter and De Rijke, 2015; Kim et al., 2017; Yadav et al., 2018). To our knowledge, we are the first to adapt these ideas to contextualized embeddings, which mitigates the BoW limitations of previous efforts (as shown in Figure 1). While contextualized representations have been shown to be extremely useful for multiple NLP tasks (Devlin et al., 2018; Peters et al., 2018; Howard and Ruder, 2018), our work is the first to apply them to an unsupervised alignment approach. Further, we show that different contextualized representations of text (character, word, sentence) capture com"
N19-1274,P16-1223,0,0.0346033,"nces containing the correct answers (Yang et al., 2015). Alignment models have also been proposed for other types of QA, such as reading comprehension (RC) QA (Chakravarti et al., 2017). We believe AHE can be similarly extended to RC, but, in this work, we have limited our experiments to answer selection and multiple-choice QA tasks. Most QA approaches today use neural, supervised methods. Most use stacked architectures usually coupled with attention mechanisms (He and Lin, 2016; Yin et al., 2015; Seo et al., 2016; Xiong et al., 2016b; Kumar et al., 2016; Tan et al., 2015; Wang et al., 2017a; Chen et al., 2016; Cheng et al., 2016; Golub and He, 2016). Some of these works also rely on structured knowledge bases (Zhong et al., 2018a; Ni et al., 2018) such as ConceptNet (Speer et al., 2017). Some approaches use query expansion methods in addition to the above methods (Musa et al., 2018; Nogueira and Cho, 2017; Ni et al., 2018). For example, Musa et al. (2018) used a sequence to sequence model (Sutskever et al., 2014) to generate an enhanced query for ARC which retrieves better supporting passages. However, in general, all these approaches rely 2682 on annotated training data, and, some, on structured"
N19-1274,D16-1053,0,0.0603275,"Missing"
N19-1274,D17-1070,0,0.458125,"r. Then AHE aligns each word in the question and candidate answer with the most similar word in the retrieved supporting paragraph, and weighs each alignment score with the inverse document frequency (IDF) of the corresponding question/answer term. AHE’s overall alignment score is the sum of the IDF weighted scores of each of the question/answer term. Importantly, AHE’s alignment function operates over contextualized embeddings that model the underlying text at different levels of abstraction: character (FLAIR) (Akbik et al., 2018), word (BERT) (Devlin et al., 2018), and sentence (InferSent) (Conneau et al., 2017), where the latter is the only supervised component in the proposed approach. The different representations are combined through an ensemble approach that by default is unsupervised (using a variant of the NoisyOr formula), but can be replaced with a supervised meta-classifier. The contributions of our work are the following: 1. To our knowledge, this is the first unsupervised alignment approach for QA that: (a) operates over contextualized embeddings, and (b) captures text at multiple levels of abstraction, including character, word, and sentence. 2. We obtain (near) state-of-the-art results"
N19-1274,N16-1108,0,0.396075,"(Clark et al., 2018); or answer sentence selection, where candidate answer sentences are provided and the task is to select the sentences containing the correct answers (Yang et al., 2015). Alignment models have also been proposed for other types of QA, such as reading comprehension (RC) QA (Chakravarti et al., 2017). We believe AHE can be similarly extended to RC, but, in this work, we have limited our experiments to answer selection and multiple-choice QA tasks. Most QA approaches today use neural, supervised methods. Most use stacked architectures usually coupled with attention mechanisms (He and Lin, 2016; Yin et al., 2015; Seo et al., 2016; Xiong et al., 2016b; Kumar et al., 2016; Tan et al., 2015; Wang et al., 2017a; Chen et al., 2016; Cheng et al., 2016; Golub and He, 2016). Some of these works also rely on structured knowledge bases (Zhong et al., 2018a; Ni et al., 2018) such as ConceptNet (Speer et al., 2017). Some approaches use query expansion methods in addition to the above methods (Musa et al., 2018; Nogueira and Cho, 2017; Ni et al., 2018). For example, Musa et al. (2018) used a sequence to sequence model (Sutskever et al., 2014) to generate an enhanced query for ARC which retrieves"
N19-1274,P18-1031,0,0.0159015,"QA. However, the majority of alignment models that rely on representation learning utilize uncontextualized word embeddings such as GloVe, coupled with other BoW models such as IBM Model 1 (Brown et al., 1993) for alignment (Kenter and De Rijke, 2015; Kim et al., 2017; Yadav et al., 2018). To our knowledge, we are the first to adapt these ideas to contextualized embeddings, which mitigates the BoW limitations of previous efforts (as shown in Figure 1). While contextualized representations have been shown to be extremely useful for multiple NLP tasks (Devlin et al., 2018; Peters et al., 2018; Howard and Ruder, 2018), our work is the first to apply them to an unsupervised alignment approach. Further, we show that different contextualized representations of text (character, word, sentence) capture complementary information, and combining them improves performance further. 3 Approach The core component of our approach computes the score of a candidate answer by aligning two texts. For multiple-choice questions, the first text consists of the question concatenated with the candidate answer, and the second is a supporting paragraph such as the one shown in Figure 1, which consists of one or more sentences ret"
N19-1274,P16-1045,0,0.0213277,"2016). Some of these works also rely on structured knowledge bases (Zhong et al., 2018a; Ni et al., 2018) such as ConceptNet (Speer et al., 2017). Some approaches use query expansion methods in addition to the above methods (Musa et al., 2018; Nogueira and Cho, 2017; Ni et al., 2018). For example, Musa et al. (2018) used a sequence to sequence model (Sutskever et al., 2014) to generate an enhanced query for ARC which retrieves better supporting passages. However, in general, all these approaches rely 2682 on annotated training data, and, some, on structured KBs, which are expensive to create (Jauhar et al., 2016). Further, as we demonstrate in Section 5, these methods tend to be tailored to a specific dataset and do not port well to other domains or even within different splits of the same dataset. In contrast, our method is mostly unsupervised and does not require training. Even then, our approach performs well on three distinct QA datasets, with top three performance in all. Our work is inspired by previous efforts on using alignment methods for NLP (Echihabi and Marcu, 2003). Unsupervised alignment models have been proposed for several NLP tasks such as short text similarity (Kenter and De Rijke, 2"
N19-1274,P03-1003,0,0.461272,"b.com/vikas95/AHE electrical → light → chemical electrical →chemical → light chemical → light → electrical chemical → electrical → light Tymoshenko et al., 2017; Xiong et al., 2016a; Wang et al., 2018; Radford et al., 2018; Li et al., 2018, inter alia). However, an undesired effect of this focus on neural approaches was that other methods have fallen out of focus, including strong unsupervised benchmarks that are necessary to highlight the true gains of supervised approaches. For instance, alignment approaches have received considerably less interest recently, despite their initial successes (Echihabi and Marcu, 2003; Surdeanu et al., 2011, inter alia). While a few recent efforts have adapted these alignment methods to operate over word representations (Kenter and De Rijke, 2015; Kim et al., 2017; Yadav et al., 2018), they generally underperfom supervised neural methods due to their underlying bag-of-word (BoW) assumptions and reliance on uncontextualized word representations such as GloVe (Pennington et al., 2014). In this work we argue that alignment approaches are more meaningful today after the advent of contextualized word representations, which mitigate the above BoW limitations. For example, Figure"
N19-1274,D18-1260,0,0.212973,"ed text text, structured – text, structured text, structured text, structured 18 19 20 21 No No Minimal Minimal text text text text 22 23 Yes Yes text text 24 – text Model Baselines Random baseline AI2 IR Solver (Clark et al., 2018) AI2 IR Solver (our implementation) Sanity Check (Yadav et al., 2018) AHE over GloVe AHE over FLAIR AHE over BERT AHE over InferSent (trained on NLI) AHE over InferSent (trained on ARC) Previous work Tuple-Inf (Clark et al., 2018) Decomp-att (Clark et al., 2018) DGEM (Clark et al., 2018) BiDAF (for ARC) (Clark et al., 2018) KG2 (Zhang et al., 2018) Bi-LSTM max-out (Mihaylov et al., 2018) NCRF++/match-LSTM (Musa et al., 2018) TriAN+f(dir)(cs)+f(ind)(cs) (Zhong et al., 2018b) ET-RR (Ni et al., 2018) Unsupervised AHE AHE (FLAIR+BERT) AHE (FLAIR+BERT+GloVe) AHE (FLAIR+BERT+InferSent) AHE (FLAIR+BERT+GloVe+InferSent) Supervised AHE AHE (FLAIR+BERT+GloVe+InferSent) AHE (FLAIR+BERT+GloVe+InferSent) with grade Oracle Oracle ensemble (FLAIR+BERT+GloVe+InferSent) Easy P@1 Challenge P@1 25.02 59.99 60.31 58.36 60.71 62.29 62.73 32.13 54.01 25.02 23.98 23.74 26.56 28.75 31.05 32.76 25.36 31.66 60.71 52.95 58.97 51.05 34.26 52.22 - 23.83 24.40 27.11 26.54 31.70 33.87 33.20 33.39 36.56 63."
N19-1274,D16-1166,0,0.0313402,"ng et al., 2015). Alignment models have also been proposed for other types of QA, such as reading comprehension (RC) QA (Chakravarti et al., 2017). We believe AHE can be similarly extended to RC, but, in this work, we have limited our experiments to answer selection and multiple-choice QA tasks. Most QA approaches today use neural, supervised methods. Most use stacked architectures usually coupled with attention mechanisms (He and Lin, 2016; Yin et al., 2015; Seo et al., 2016; Xiong et al., 2016b; Kumar et al., 2016; Tan et al., 2015; Wang et al., 2017a; Chen et al., 2016; Cheng et al., 2016; Golub and He, 2016). Some of these works also rely on structured knowledge bases (Zhong et al., 2018a; Ni et al., 2018) such as ConceptNet (Speer et al., 2017). Some approaches use query expansion methods in addition to the above methods (Musa et al., 2018; Nogueira and Cho, 2017; Ni et al., 2018). For example, Musa et al. (2018) used a sequence to sequence model (Sutskever et al., 2014) to generate an enhanced query for ARC which retrieves better supporting passages. However, in general, all these approaches rely 2682 on annotated training data, and, some, on structured KBs, which are expensive to create (Jauha"
N19-1274,D16-1147,0,0.175913,"7 No Yes No No No No Yes 8 9 10 11 12 13 14 15 16 17 18 19 20 Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes 21 22 23 24 No No Minimal Minimal 25 Yes 26 – Model Baselines Wgt Word Cnt (Yang et al., 2015) LCLR (Yang et al., 2015) Sanity Check (Yadav et al., 2018) AHE over GloVe AHE over FLAIR AHE over BERT AHE over InferSent Previous work CNN+Cnt (Yang et al., 2015) RNN-1way (Jurczyk et al., 2016) RNN-Attention pool (Jurczyk et al., 2016) CNN (avg + emb) (Jurczyk et al., 2016) AP-CNN (dos Santos et al., 2016) LSTM-att (Miao et al., 2016) ABCNN (Yin et al., 2015) Key-value memory network (Miller et al., 2016) CubeCNN (He and Lin, 2016) BiMPM (Wang et al., 2017b) (Tymoshenko et al., 2017) Compare-Aggregate (Wang and Jiang, 2016) (Li et al., 2018) Unsupervised AHE AHE (FLAIR+BERT) AHE (FLAIR+BERT+Glove) AHE (FLAIR+BERT+InferSent) AHE (FLAIR+BERT+Glove+InferSent) Supervised AHE AHE (FLAIR+BERT+Glove+InferSent) Oracle Oracle ensemble (FLAIR+BERT+Glove+InferSent) MAP MRR 50.99 59.93 64.02 63.40 64.91 65.13 66.93 51.32 60.86 65.39 66.51 66.40 68.70 65.20 66.64 67.47 68.78 68.86 68.86 69.21 70.69 70.90 71.80 72.19 74.33 75.41 66.52 68.70 68.92 70.82 69.57 70.69 71.08 72.65 72.34 73.10 74.08 75.45 76.59 6"
N19-1274,D17-1061,0,0.0296577,"election and multiple-choice QA tasks. Most QA approaches today use neural, supervised methods. Most use stacked architectures usually coupled with attention mechanisms (He and Lin, 2016; Yin et al., 2015; Seo et al., 2016; Xiong et al., 2016b; Kumar et al., 2016; Tan et al., 2015; Wang et al., 2017a; Chen et al., 2016; Cheng et al., 2016; Golub and He, 2016). Some of these works also rely on structured knowledge bases (Zhong et al., 2018a; Ni et al., 2018) such as ConceptNet (Speer et al., 2017). Some approaches use query expansion methods in addition to the above methods (Musa et al., 2018; Nogueira and Cho, 2017; Ni et al., 2018). For example, Musa et al. (2018) used a sequence to sequence model (Sutskever et al., 2014) to generate an enhanced query for ARC which retrieves better supporting passages. However, in general, all these approaches rely 2682 on annotated training data, and, some, on structured KBs, which are expensive to create (Jauhar et al., 2016). Further, as we demonstrate in Section 5, these methods tend to be tailored to a specific dataset and do not port well to other domains or even within different splits of the same dataset. In contrast, our method is mostly unsupervised and does"
N19-1274,D14-1162,0,0.0982023,"s that are necessary to highlight the true gains of supervised approaches. For instance, alignment approaches have received considerably less interest recently, despite their initial successes (Echihabi and Marcu, 2003; Surdeanu et al., 2011, inter alia). While a few recent efforts have adapted these alignment methods to operate over word representations (Kenter and De Rijke, 2015; Kim et al., 2017; Yadav et al., 2018), they generally underperfom supervised neural methods due to their underlying bag-of-word (BoW) assumptions and reliance on uncontextualized word representations such as GloVe (Pennington et al., 2014). In this work we argue that alignment approaches are more meaningful today after the advent of contextualized word representations, which mitigate the above BoW limitations. For example, Figure 1 shows an example of a question from AI2’s Reasoning Challenge (ARC) dataset (Clark et al., 2018), 2681 Proceedings of NAACL-HLT 2019, pages 2681–2691 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics which is not answered correctly by a state-of-theart BoW alignment method (Yadav et al., 2018), but is correctly answered by our alignment approach when oper"
N19-1274,N18-1202,0,0.0242007,"itive for non-factoid QA. However, the majority of alignment models that rely on representation learning utilize uncontextualized word embeddings such as GloVe, coupled with other BoW models such as IBM Model 1 (Brown et al., 1993) for alignment (Kenter and De Rijke, 2015; Kim et al., 2017; Yadav et al., 2018). To our knowledge, we are the first to adapt these ideas to contextualized embeddings, which mitigates the BoW limitations of previous efforts (as shown in Figure 1). While contextualized representations have been shown to be extremely useful for multiple NLP tasks (Devlin et al., 2018; Peters et al., 2018; Howard and Ruder, 2018), our work is the first to apply them to an unsupervised alignment approach. Further, we show that different contextualized representations of text (character, word, sentence) capture complementary information, and combining them improves performance further. 3 Approach The core component of our approach computes the score of a candidate answer by aligning two texts. For multiple-choice questions, the first text consists of the question concatenated with the candidate answer, and the second is a supporting paragraph such as the one shown in Figure 1, which consists of"
N19-1274,J11-2003,1,0.876306,"al → light → chemical electrical →chemical → light chemical → light → electrical chemical → electrical → light Tymoshenko et al., 2017; Xiong et al., 2016a; Wang et al., 2018; Radford et al., 2018; Li et al., 2018, inter alia). However, an undesired effect of this focus on neural approaches was that other methods have fallen out of focus, including strong unsupervised benchmarks that are necessary to highlight the true gains of supervised approaches. For instance, alignment approaches have received considerably less interest recently, despite their initial successes (Echihabi and Marcu, 2003; Surdeanu et al., 2011, inter alia). While a few recent efforts have adapted these alignment methods to operate over word representations (Kenter and De Rijke, 2015; Kim et al., 2017; Yadav et al., 2018), they generally underperfom supervised neural methods due to their underlying bag-of-word (BoW) assumptions and reliance on uncontextualized word representations such as GloVe (Pennington et al., 2014). In this work we argue that alignment approaches are more meaningful today after the advent of contextualized word representations, which mitigate the above BoW limitations. For example, Figure 1 shows an example of"
N19-1274,D17-1093,0,0.299413,"rect sequence, and cannot be answered correctly when relying on uncontextualized embeddings. Introduction The “deep learning tsunami”(Manning, 2015) has had a major impact on important natural language processing (NLP) applications such as question answering (QA). Many neural approaches for QA have been proposed in the past few years, with impressive results on several QA tasks (Seo et al., 2016; Wang and Jiang, 2016; Wang et al., 2017b; 1 Code: https://github.com/vikas95/AHE electrical → light → chemical electrical →chemical → light chemical → light → electrical chemical → electrical → light Tymoshenko et al., 2017; Xiong et al., 2016a; Wang et al., 2018; Radford et al., 2018; Li et al., 2018, inter alia). However, an undesired effect of this focus on neural approaches was that other methods have fallen out of focus, including strong unsupervised benchmarks that are necessary to highlight the true gains of supervised approaches. For instance, alignment approaches have received considerably less interest recently, despite their initial successes (Echihabi and Marcu, 2003; Surdeanu et al., 2011, inter alia). While a few recent efforts have adapted these alignment methods to operate over word representatio"
N19-1274,P18-1158,0,0.0204218,"y when relying on uncontextualized embeddings. Introduction The “deep learning tsunami”(Manning, 2015) has had a major impact on important natural language processing (NLP) applications such as question answering (QA). Many neural approaches for QA have been proposed in the past few years, with impressive results on several QA tasks (Seo et al., 2016; Wang and Jiang, 2016; Wang et al., 2017b; 1 Code: https://github.com/vikas95/AHE electrical → light → chemical electrical →chemical → light chemical → light → electrical chemical → electrical → light Tymoshenko et al., 2017; Xiong et al., 2016a; Wang et al., 2018; Radford et al., 2018; Li et al., 2018, inter alia). However, an undesired effect of this focus on neural approaches was that other methods have fallen out of focus, including strong unsupervised benchmarks that are necessary to highlight the true gains of supervised approaches. For instance, alignment approaches have received considerably less interest recently, despite their initial successes (Echihabi and Marcu, 2003; Surdeanu et al., 2011, inter alia). While a few recent efforts have adapted these alignment methods to operate over word representations (Kenter and De Rijke, 2015; Kim et al"
N19-1274,P17-1018,0,0.3508,"e ARC dataset with the correct answer in bold font. This question is answered correctly by our alignment method that relies on contextualized word embeddings that capture the correct sequence, and cannot be answered correctly when relying on uncontextualized embeddings. Introduction The “deep learning tsunami”(Manning, 2015) has had a major impact on important natural language processing (NLP) applications such as question answering (QA). Many neural approaches for QA have been proposed in the past few years, with impressive results on several QA tasks (Seo et al., 2016; Wang and Jiang, 2016; Wang et al., 2017b; 1 Code: https://github.com/vikas95/AHE electrical → light → chemical electrical →chemical → light chemical → light → electrical chemical → electrical → light Tymoshenko et al., 2017; Xiong et al., 2016a; Wang et al., 2018; Radford et al., 2018; Li et al., 2018, inter alia). However, an undesired effect of this focus on neural approaches was that other methods have fallen out of focus, including strong unsupervised benchmarks that are necessary to highlight the true gains of supervised approaches. For instance, alignment approaches have received considerably less interest recently, despite t"
N19-1274,D15-1237,0,0.0930922,"Missing"
N19-4003,N18-1067,0,0.0368197,"Missing"
N19-4003,N19-4008,1,0.880311,"Missing"
N19-4003,L18-1169,1,0.683696,"Missing"
N19-4003,P17-4018,1,0.907963,"Missing"
N19-4003,N10-1091,1,0.731572,"Missing"
N19-4003,K18-1021,0,0.0393544,"Missing"
N19-4003,L16-1050,1,0.901642,"Missing"
N19-4008,P98-1013,0,0.252612,"Missing"
N19-4008,L18-1529,1,0.864912,"Missing"
N19-4008,E12-2021,0,0.0648887,"Missing"
N19-4008,L16-1599,1,0.888366,"Missing"
N19-4008,L16-1050,1,0.530295,"Missing"
N19-4008,C18-1182,1,0.857504,"Missing"
N19-4008,P17-4018,1,0.857295,"Missing"
N19-4008,S19-2232,1,0.846579,"Missing"
N19-4008,Q18-1025,1,0.846998,"Missing"
P01-1037,A00-1023,0,\N,Missing
P01-1037,C00-1043,1,\N,Missing
P01-1037,H94-1052,0,\N,Missing
P01-1037,P00-1071,1,\N,Missing
P01-1037,A00-1025,0,\N,Missing
P01-1037,P96-1025,0,\N,Missing
P01-1037,A00-1021,0,\N,Missing
P01-1037,P95-1037,0,\N,Missing
P01-1037,A00-1041,0,\N,Missing
P02-1005,A00-1023,0,\N,Missing
P02-1005,C00-1043,1,\N,Missing
P02-1005,N01-1005,0,\N,Missing
P02-1005,A00-1025,0,\N,Missing
P02-1005,W01-1201,0,\N,Missing
P02-1005,H01-1069,0,\N,Missing
P02-1005,A00-1041,0,\N,Missing
P03-1002,J95-4004,0,0.248294,"Missing"
P03-1002,P97-1003,0,0.0654598,"led for. Additionally, the argument may include functional tags from Treebank, e.g. ArgM-DIR indicates a directional, ArgM-LOC indicates a locative, and ArgM-TMP stands for a temporal. 2.2 The Model In previous work using the PropBank corpus, (Gildea and Palmer, 2002) proposed a model predicting argument roles using the same statistical method as the one employed by (Gildea and Jurafsky, 2002) for predicting semantic roles based on the FrameNet corpus (Baker et al., 1998). This statistical technique of labeling predicate argument operates on the output of the probabilistic parser reported in (Collins, 1997). It consists of two tasks: (1) identifying the parse tree constituents corresponding to arguments of each predicate encoded in PropBank; and (2) recognizing the role corresponding to each argument. Each task can be cast a separate classifier. For example, the result of the first classifier on the sentence illustrated in Figure 2 is the identification of the two NPs as arguments. The second classifier assigns the specific roles ARG1 and ARG0 given the predicate “assailed”. − PHRASE TYPE (pt): This feature indicates the syntactic type of the phrase labeled as a predicate argument, e.g. NP for A"
P03-1002,J02-3001,0,0.822141,"ecific. For example, when retrieving the argument structure for the verb-predicate assail with the sense ”to tear attack” from www.cis.upenn.edu/ cotton/cgibin/pblex fmt.cgi, we find Arg0:agent, Arg1:entity assailed and Arg2:assailed for. Additionally, the argument may include functional tags from Treebank, e.g. ArgM-DIR indicates a directional, ArgM-LOC indicates a locative, and ArgM-TMP stands for a temporal. 2.2 The Model In previous work using the PropBank corpus, (Gildea and Palmer, 2002) proposed a model predicting argument roles using the same statistical method as the one employed by (Gildea and Jurafsky, 2002) for predicting semantic roles based on the FrameNet corpus (Baker et al., 1998). This statistical technique of labeling predicate argument operates on the output of the probabilistic parser reported in (Collins, 1997). It consists of two tasks: (1) identifying the parse tree constituents corresponding to arguments of each predicate encoded in PropBank; and (2) recognizing the role corresponding to each argument. Each task can be cast a separate classifier. For example, the result of the first classifier on the sentence illustrated in Figure 2 is the identification of the two NPs as arguments."
P03-1002,P02-1031,0,0.815742,"or direct object or theme whereas Arg2 represents indirect object, benefactive or instrument, but mnemonics tend to be verb specific. For example, when retrieving the argument structure for the verb-predicate assail with the sense ”to tear attack” from www.cis.upenn.edu/ cotton/cgibin/pblex fmt.cgi, we find Arg0:agent, Arg1:entity assailed and Arg2:assailed for. Additionally, the argument may include functional tags from Treebank, e.g. ArgM-DIR indicates a directional, ArgM-LOC indicates a locative, and ArgM-TMP stands for a temporal. 2.2 The Model In previous work using the PropBank corpus, (Gildea and Palmer, 2002) proposed a model predicting argument roles using the same statistical method as the one employed by (Gildea and Jurafsky, 2002) for predicting semantic roles based on the FrameNet corpus (Baker et al., 1998). This statistical technique of labeling predicate argument operates on the output of the probabilistic parser reported in (Collins, 1997). It consists of two tasks: (1) identifying the parse tree constituents corresponding to arguments of each predicate encoded in PropBank; and (2) recognizing the role corresponding to each argument. Each task can be cast a separate classifier. For exampl"
P03-1002,C00-2136,0,\N,Missing
P03-1002,N01-1006,0,\N,Missing
P03-1002,P98-1013,0,\N,Missing
P03-1002,C98-1013,0,\N,Missing
P08-1082,D07-1119,1,0.444893,"Missing"
P08-1082,J93-2003,0,0.0101924,"mas. FG2: Translation Features Berger et al. (2000) showed that similarity-based models are doomed to perform poorly for QA because they fail to “bridge the lexical chasm” between questions and answers. One way to address this problem is to learn question-to-answer transformations using a translation model (Berger et al., 2000; Echihabi and Marcu, 2003; Soricut and Brill, 2006; Riezler et al., 2007). In our model, we incorporate this approach by adding the probability that the question Q is a translation of the answer A, P (Q|A), as a feature. This probability is computed using IBM’s Model 1 (Brown et al., 1993): P (Q|A) = Y P (q|A) (3) P (q|A) = (1 − λ)Pml (q|A) + λPml (q|C) (4) q∈Q Pml (q|A) = X (T (q|a)Pml (a|A)) (5) a∈A where the probability that the question term q is generated from answer A, P (q|A), is smoothed using the prior probability that the term q is generated from the entire collection of answers C, Pml (q|C). λ is the smoothing parameter. Pml (q|C) is computed using the maximum likelihood estimator. Pml (q|A) is computed as the sum of the probabilities that the question term q is a translation of an answer term a, T (q|a), weighted by the probability that a is generated from A. The tr"
P08-1082,W06-1670,1,0.487558,"larity, we group the features into four sets: features that model 721 the similarity between questions and answers (FG1), features that encode question-to-answer transformations using a translation model (FG2), features that measure keyword density and frequency (FG3), and features that measure the correlation between question-answer pairs and other collections (FG4). Wherever applicable, we explore different syntactic and semantic representations of the textual content, e.g., extracting the dependency-based representation of the text or generalizing words to their WordNet supersenses (WNSS) (Ciaramita and Altun, 2006). We detail each of these feature groups next. FG1: Similarity Features We measure the similarity between a question Q and an answer A using the length-normalized BM 25 formula (Robertson and Walker, 1997). We chose this similarity formula because, out of all the IR models we tried, it provided the best ranking at the output of the answer retrieval component. For completeness we also include in the feature set the value of the tf ·idf similarity measure. For both formulas we use the implementations available in the Terrier IR platform4 with the default parameters. To understand the contributio"
P08-1082,D07-1112,0,0.0131507,"Missing"
P08-1082,P03-1003,0,0.0255573,"view for a fair analysis of the above syntactic views. Generalized bigrams (Bg ) - same as above, but the words are generalized to their WNSS. 4 http://ir.dcs.gla.ac.uk/terrier In all these representations we skip stop words and normalize all words to their WordNet lemmas. FG2: Translation Features Berger et al. (2000) showed that similarity-based models are doomed to perform poorly for QA because they fail to “bridge the lexical chasm” between questions and answers. One way to address this problem is to learn question-to-answer transformations using a translation model (Berger et al., 2000; Echihabi and Marcu, 2003; Soricut and Brill, 2006; Riezler et al., 2007). In our model, we incorporate this approach by adding the probability that the question Q is a translation of the answer A, P (Q|A), as a feature. This probability is computed using IBM’s Model 1 (Brown et al., 1993): P (Q|A) = Y P (q|A) (3) P (q|A) = (1 − λ)Pml (q|A) + λPml (q|C) (4) q∈Q Pml (q|A) = X (T (q|a)Pml (a|A)) (5) a∈A where the probability that the question term q is generated from answer A, P (q|A), is smoothed using the prior probability that the term q is generated from the entire collection of answers C, Pml (q|C). λ is the smooth"
P08-1082,W03-1210,0,0.0208745,"that an optimal retrieval engine from social media should combine all these three methodologies. Moreover, our approach might have applications outside of social media (e.g., for opendomain web-based QA), because the ranking model built is based only on open-domain knowledge and the analysis of textual content. In the QA literature, answer ranking for nonfactoid questions has typically been performed by learning question-to-answer transformations, either using translation models (Berger et al., 2000; Soricut and Brill, 2006) or by exploiting the redundancy of the Web (Agichtein et al., 2001). Girju (2003) extracts non-factoid answers by searching for certain semantic structures, e.g., causation relations as answers to causation questions. In this paper we combine several methodologies, including the above, into a single model. This approach allowed us to perform a systematic feature analysis on a large-scale real-world corpus and a comprehensive feature set. Recent work has showed that structured retrieval improves answer ranking for factoid questions: Bilotti et al. (2007) showed that matching predicateargument frames constructed from the question and the expected answer types improves answer"
P08-1082,I08-1055,0,0.0412224,"eason questions. Because virtually no training data is available for this problem, most automated systems train either 1 2 Q: How do you quiet a squeaky door? A: Spray WD-40 directly onto the hinges of the door. Open and close the door several times. Remove hinges if the door still squeaks. Remove any rust, dirt or loose paint. Apply WD-40 to removed hinges. Put the hinges back, open and close door several times again. Q: How to extract html tags from an html documents with c++? A: very carefully http://trec.nist.gov http://www.clef-campaign.org on small hand-annotated corpora built in house (Higashinaka and Isozaki, 2008) or on question-answer pairs harvested from Frequently Asked Questions (FAQ) lists or similar resources (Soricut and Brill, 2006). None of these situations is ideal: the cost of building the training corpus in the former setup is high; in the latter scenario the data tends to be domain-specific, hence unsuitable for the learning of open-domain models. On the other hand, recent years have seen an explosion of user-generated content (or social media). Of particular interest in our context are communitydriven question-answering sites, such as Yahoo! Answers3 , where users answer questions posed b"
P08-1082,P07-1099,0,0.0132825,"structed from the question and the expected answer types improves answer ranking. Cui et al. (2005) learned transformations of dependency paths from questions to answers to improve passage ranking. However, both approaches use similarity models at their core because they require the matching of the lexical elements in the search structures. On the other hand, our approach allows the learning of full transformations from question structures to answer structures using translation models applied to different text representations. Our answer ranking framework is closest in spirit to the system of Ko et al. (2007) or Higashinaka et al. (2008). However, the former was applied only to factoid QA and both are limited to similarity, re726 dundancy and gazetteer-based features. Our model uses a larger feature set that includes correlation and transformation-based features and five different content representations. Our evaluation is also carried out on a larger scale. Our work is also related to that of Riezler et al. (2007) where SMT-based query expansion methods are used on data from FAQ pages. 6 Conclusions In this work we described an answer ranking engine for non-factoid questions built using a large c"
P08-1082,J93-2004,0,0.0344608,"Missing"
P08-1082,P07-1098,0,0.12847,"s the number of nonstop question words that are recognized in the same order in the answer. Answer span - the largest distance (in words) between two non-stop question words in the answer. Same sentence match - number of non-stop question terms matched in a single sentence in the answer. Overall match - number of non-stop question terms matched in the complete answer. These last two features are computed also for the other four text representations previously introduced (B, Bg , D, and Dg ). Counting the number of matched dependencies is essentially a simplified tree kernel for QA (e.g., see (Moschitti et al., 2007)) matching only trees of depth 2. Experiments with full dependency tree kernels based on several variants of the convolution kernels of Collins and Duffy (2001) did not yield improvements. We conjecture that the mistakes of the syntactic parser may be amplified in tree kernels, which consider an exponential number of sub-trees. Informativeness - we model the amount of information contained in the answer by counting the number of non-stop nouns, verbs, and adjectives in the answer text that do not appear in the question. FG4: Web Correlation Features Previous work has shown that the redundancy"
P08-1082,P07-1059,0,0.344663,"ews. Generalized bigrams (Bg ) - same as above, but the words are generalized to their WNSS. 4 http://ir.dcs.gla.ac.uk/terrier In all these representations we skip stop words and normalize all words to their WordNet lemmas. FG2: Translation Features Berger et al. (2000) showed that similarity-based models are doomed to perform poorly for QA because they fail to “bridge the lexical chasm” between questions and answers. One way to address this problem is to learn question-to-answer transformations using a translation model (Berger et al., 2000; Echihabi and Marcu, 2003; Soricut and Brill, 2006; Riezler et al., 2007). In our model, we incorporate this approach by adding the probability that the question Q is a translation of the answer A, P (Q|A), as a feature. This probability is computed using IBM’s Model 1 (Brown et al., 1993): P (Q|A) = Y P (q|A) (3) P (q|A) = (1 − λ)Pml (q|A) + λPml (q|C) (4) q∈Q Pml (q|A) = X (T (q|a)Pml (a|A)) (5) a∈A where the probability that the question term q is generated from answer A, P (q|A), is smoothed using the prior probability that the term q is generated from the entire collection of answers C, Pml (q|C). λ is the smoothing parameter. Pml (q|C) is computed using the m"
P11-1163,W09-1402,0,0.0948592,"Missing"
P11-1163,P09-1068,0,0.0290898,"either protein entity mentions (PROT) or other events as arguments. The latter is what allows for nested event structures. Existing dependency parsing models can be adapted to produce these semantic structures instead of syntactic dependencies. We built a global reranking parser model using multiple decoders from MSTParser (McDonald et al., 2005; McDonald et al., 2005b). The main contributions of this paper are the following: Event structures in open domain texts are frequently highly complex and nested: a “crime” event can cause an “investigation” event, which can lead to an “arrest” event (Chambers and Jurafsky, 2009). The same observation holds in specific domains. For example, the BioNLP’09 shared task (Kim et al., 2009) focuses on the extraction of nested biomolecular 1. We demonstrate that parsing is an attractive apevents, where, e.g., a REGULATION event causes a proach for extracting events, both nested and TRANSCRIPTION event (see Figure 1a for a detailed otherwise. 1 example). Despite this observation, many stateWhile our approach only works on trees, we show how we of-the-art supervised event extraction models still can handle directed acyclic graphs in Section 5. 1626 Proceedings of the 49th Annu"
P11-1163,P05-1022,0,0.556932,"se in the BioNLP’09 domain used for evaluation entities (PROTEINs) are given (but including entity recognition is an obvious extension of our model). Our parsers are several instances of MSTParser2 (McDonald et al., 2005; McDonald et al., 2005b) configured with different decoders. However, our approach is agnostic to the actual parsing models used and could easily be adapted to other dependency parsers. The output from the reranking parser is 2 http://sourceforge.net/projects/mstparser/ 1628 converted back to the original event representation and passed to a reranker component (Collins, 2000; Charniak and Johnson, 2005), tailored to optimize the task-specific evaluation metric. Note that although we use the biomedical event domain from the BioNLP’09 shared task to illustrate our work, the core of our approach is almost domain independent. Our only constraints are that each event mention be activated by a phrase that serves as an event anchor, and that the event-argument structures be mapped to a dependency tree. The conversion between event and dependency structures and the reranker metric are the only domain dependent components in our approach. 3.1 Converting between Event Structures and Dependencies As in"
P11-1163,N09-1037,1,0.67237,"d show that our approach obtains competitive results. 2 Related Work The pioneering work of Miller et al. (1997) was the first, to our knowledge, to propose parsing as a framework for information extraction. They extended the syntactic annotations of the Penn Treebank corpus (Marcus et al., 1993) with entity and relation mentions specific to the MUC-7 evaluation (Chinchor et al., 1997) — e.g., EMPLOYEE OF relations that hold between person and organization named entities — and then trained a generative parsing model over this combined syntactic and semantic representation. In the same spirit, Finkel and Manning (2009) merged the syntactic annotations and the named entity annotations of the OntoNotes corpus (Hovy et al., 2006) and trained a discriminative parsing model for the joint problem of syntactic parsing and named entity recognition. However, both these works require a unified annotation of syntactic and semantic elements, which is not always feasible, and focused only on named entities and binary relations. On the other hand, our approach focuses on event structures that are nested and have an arbitrary number of arguments. We do not need 1627 a unified syntactic and semantic representation (but we"
P11-1163,D09-1015,1,0.789522,"d show that our approach obtains competitive results. 2 Related Work The pioneering work of Miller et al. (1997) was the first, to our knowledge, to propose parsing as a framework for information extraction. They extended the syntactic annotations of the Penn Treebank corpus (Marcus et al., 1993) with entity and relation mentions specific to the MUC-7 evaluation (Chinchor et al., 1997) — e.g., EMPLOYEE OF relations that hold between person and organization named entities — and then trained a generative parsing model over this combined syntactic and semantic representation. In the same spirit, Finkel and Manning (2009) merged the syntactic annotations and the named entity annotations of the OntoNotes corpus (Hovy et al., 2006) and trained a discriminative parsing model for the joint problem of syntactic parsing and named entity recognition. However, both these works require a unified annotation of syntactic and semantic elements, which is not always feasible, and focused only on named entities and binary relations. On the other hand, our approach focuses on event structures that are nested and have an arbitrary number of arguments. We do not need 1627 a unified syntactic and semantic representation (but we"
P11-1163,N06-2015,0,0.00709008,"the first, to our knowledge, to propose parsing as a framework for information extraction. They extended the syntactic annotations of the Penn Treebank corpus (Marcus et al., 1993) with entity and relation mentions specific to the MUC-7 evaluation (Chinchor et al., 1997) — e.g., EMPLOYEE OF relations that hold between person and organization named entities — and then trained a generative parsing model over this combined syntactic and semantic representation. In the same spirit, Finkel and Manning (2009) merged the syntactic annotations and the named entity annotations of the OntoNotes corpus (Hovy et al., 2006) and trained a discriminative parsing model for the joint problem of syntactic parsing and named entity recognition. However, both these works require a unified annotation of syntactic and semantic elements, which is not always feasible, and focused only on named entities and binary relations. On the other hand, our approach focuses on event structures that are nested and have an arbitrary number of arguments. We do not need 1627 a unified syntactic and semantic representation (but we can and do extract features from the underlying syntactic structure of the text). Finkel and Manning (2009b) a"
P11-1163,P08-1067,0,0.014766,"ctures When decoding, the parser finds the highest scoring tree which incorporates global properties of the sentence. However, its features are edge-factored and thus unable to take into account larger contexts. To incorporate arbitrary global features, we employ a two-step reranking parser. For the first step, we extend our parser to output its n-best parses instead of just its top scoring parse. In the second step, a discriminative reranker rescores each parse and reorders the n-best list. Rerankers have been successfully used in syntactic parsing (Collins, 2000; Charniak and Johnson, 2005; Huang, 2008) and semantic role labeling (Toutanova et al., 2008). Rerankers provide additional advantages in our case due to the mismatch between the dependency structures that the parser operates on and their corresponding event structures. We convert the output from the parser to event structures (Section 3.1) before including them in the reranker. This allows the reranker to capture features over the actual event structures rather than their original dependency trees which may contain extraneous portions.8 Furthermore, this lets the reranker optimize the actual BioNLP F1 score. The parser, on the other"
P11-1163,N10-1095,0,0.0140743,"r detection and reranking. Decoder names include the features order (1 or 2) followed by the projectivity (P = projective, N = non-projective). decoder; number of different decoders producing the parse (when using multiple decoders). 4 Experimental Results To improve performance and robustness, features are pruned as in Charniak and Johnson (2005): selected features must distinguish a parse with the highest F1 score in a n-best list, from a parse with a suboptimal F1 score at least five times. Rerankers can also be used to perform model combination (Toutanova et al., 2008; Zhang et al., 2009; Johnson and Ural, 2010). While we use a single parsing model, it has multiple decoders.10 When combining multiple decoders, we concatenate their n-best lists and extract the unique parses. Our experiments use the BioNLP’09 shared task corpus (Kim et al., 2009) which includes 800 biomedical abstracts (7,449 sentences, 8,597 events) for training and 150 abstracts (1,450 sentences, 1,809 events) for development. The test set includes 260 abstracts, 2,447 sentences, and 3,182 events. Throughout our experiments, we report BioNLP F1 scores with approximate span and recursive event matching (as described in the shared task"
P11-1163,J93-2004,0,0.0404709,"ose a wide range of features for event extraction. Our analysis indicates that features which model the global event structure yield considerable performance improvements, which proves that modeling event structure jointly is beneficial. 3. We evaluate on the biomolecular event corpus from the the BioNLP’09 shared task and show that our approach obtains competitive results. 2 Related Work The pioneering work of Miller et al. (1997) was the first, to our knowledge, to propose parsing as a framework for information extraction. They extended the syntactic annotations of the Penn Treebank corpus (Marcus et al., 1993) with entity and relation mentions specific to the MUC-7 evaluation (Chinchor et al., 1997) — e.g., EMPLOYEE OF relations that hold between person and organization named entities — and then trained a generative parsing model over this combined syntactic and semantic representation. In the same spirit, Finkel and Manning (2009) merged the syntactic annotations and the named entity annotations of the OntoNotes corpus (Hovy et al., 2006) and trained a discriminative parsing model for the joint problem of syntactic parsing and named entity recognition. However, both these works require a unified a"
P11-1163,N10-1004,1,0.636022,"and extract the unique parses. Our experiments use the BioNLP’09 shared task corpus (Kim et al., 2009) which includes 800 biomedical abstracts (7,449 sentences, 8,597 events) for training and 150 abstracts (1,450 sentences, 1,809 events) for development. The test set includes 260 abstracts, 2,447 sentences, and 3,182 events. Throughout our experiments, we report BioNLP F1 scores with approximate span and recursive event matching (as described in the shared task definition). For preprocessing, we parsed all documents using the self-trained biomedical McClosky-CharniakJohnson reranking parser (McClosky, 2010). We bias the anchor detector to favor recall, allowing the parser and reranker to determine which event anchors will ultimately be used. When performing nbest parsing, n = 50. For parser feature pruning, α = 0.001. Table 1a shows the performance of each of the decoders when using gold event anchors. In both cases where n-best decoding is available, the reranker improves performance over the 1-best parsers. We also present the results from a reranker trained from multiple decoders which is our highest scoring model.11 In Table 1b, we present the output for the predicted anchor scenario. In the"
P11-1163,W11-1806,1,0.744235,"Missing"
P11-1163,P05-1012,0,0.160804,"main. Figure 1 shows a sentence and its converted form from the biomedical domain with four events: two POSITIVE REGULATION events, anchored by the phrase “acts as a costimulatory signal,” and two TRANSCRIPTION events, both anchored on “gene transcription.” All events take either protein entity mentions (PROT) or other events as arguments. The latter is what allows for nested event structures. Existing dependency parsing models can be adapted to produce these semantic structures instead of syntactic dependencies. We built a global reranking parser model using multiple decoders from MSTParser (McDonald et al., 2005; McDonald et al., 2005b). The main contributions of this paper are the following: Event structures in open domain texts are frequently highly complex and nested: a “crime” event can cause an “investigation” event, which can lead to an “arrest” event (Chambers and Jurafsky, 2009). The same observation holds in specific domains. For example, the BioNLP’09 shared task (Kim et al., 2009) focuses on the extraction of nested biomolecular 1. We demonstrate that parsing is an attractive apevents, where, e.g., a REGULATION event causes a proach for extracting events, both nested and TRANSCRIPTION even"
P11-1163,H05-1066,0,0.137371,"Missing"
P11-1163,E06-1011,0,0.0666491,"Missing"
P11-1163,W10-1905,0,0.0151987,"Missing"
P11-1163,N10-1123,0,0.0865664,"c and semantic representation (but we can and do extract features from the underlying syntactic structure of the text). Finkel and Manning (2009b) also proposed a parsing model for the extraction of nested named entity mentions, which, like this work, parses just the corresponding semantic annotations. In this work, we focus on more complex structures (events instead of named entities) and we explore more global features through our reranking layer. In the biomedical domain, two recent papers proposed joint models for event extraction based on Markov logic networks (MLN) (Riedel et al., 2009; Poon and Vanderwende, 2010). Both works propose elegant frameworks where event anchors and arguments are jointly predicted for all events in the same sentence. One disadvantage of MLN models is the requirement that a human expert develop domainspecific predicates and formulas, which can be a cumbersome process because it requires thorough domain understanding. On the other hand, our approach maintains the joint modeling advantage, but our model is built over simple, domain-independent features. We also propose and analyze a richer feature space that captures more information on the global event structure in a sentence."
P11-1163,W09-1406,0,0.100515,"27 a unified syntactic and semantic representation (but we can and do extract features from the underlying syntactic structure of the text). Finkel and Manning (2009b) also proposed a parsing model for the extraction of nested named entity mentions, which, like this work, parses just the corresponding semantic annotations. In this work, we focus on more complex structures (events instead of named entities) and we explore more global features through our reranking layer. In the biomedical domain, two recent papers proposed joint models for event extraction based on Markov logic networks (MLN) (Riedel et al., 2009; Poon and Vanderwende, 2010). Both works propose elegant frameworks where event anchors and arguments are jointly predicted for all events in the same sentence. One disadvantage of MLN models is the requirement that a human expert develop domainspecific predicates and formulas, which can be a cumbersome process because it requires thorough domain understanding. On the other hand, our approach maintains the joint modeling advantage, but our model is built over simple, domain-independent features. We also propose and analyze a richer feature space that captures more information on the global ev"
P11-1163,D10-1001,0,0.0332233,"Missing"
P11-1163,J08-2002,1,0.586574,"rt in text. This result is consistent with observations in previous work (Bj¨orne et al., 2009). • Token-level: The form, lemma, and whether the token is present in a gazetteer of known anchor words.4 • Surface context: The above token features extracted from a context of two words around the current token. Additionally, we build token bigrams in this context window, and model them with similar features. • Syntactic context: We model all syntactic dependency paths up to depth two starting from the token to be classified. These paths are built from Stanford syntactic dependencies (Marneffe and Manning, 2008). We extract token features from the first and last token in these paths. We also generate combination features by concatenating: (a) the last token in each path with the sequence of dependency labels along the corresponding path; and (b) the word to be classified, the last token in each path, and the sequence of dependency labels in that path. • Bag-of-word and entity count: Extracted from (a) the entire sentence, and (b) a window of five words around the token to be classified. 3.3 Parsing Event Structures Given the entities and event anchors from the previous stages in the pipeline, the par"
P11-1163,D09-1161,0,0.00955178,"mpact of event anchor detection and reranking. Decoder names include the features order (1 or 2) followed by the projectivity (P = projective, N = non-projective). decoder; number of different decoders producing the parse (when using multiple decoders). 4 Experimental Results To improve performance and robustness, features are pruned as in Charniak and Johnson (2005): selected features must distinguish a parse with the highest F1 score in a n-best list, from a parse with a suboptimal F1 score at least five times. Rerankers can also be used to perform model combination (Toutanova et al., 2008; Zhang et al., 2009; Johnson and Ural, 2010). While we use a single parsing model, it has multiple decoders.10 When combining multiple decoders, we concatenate their n-best lists and extract the unique parses. Our experiments use the BioNLP’09 shared task corpus (Kim et al., 2009) which includes 800 biomedical abstracts (7,449 sentences, 8,597 events) for training and 150 abstracts (1,450 sentences, 1,809 events) for development. The test set includes 260 abstracts, 2,447 sentences, and 3,182 events. Throughout our experiments, we report BioNLP F1 scores with approximate span and recursive event matching (as des"
P11-1163,W08-2121,1,\N,Missing
P11-1163,W09-1401,0,\N,Missing
P11-1163,W08-1301,1,\N,Missing
P11-1163,C08-1095,0,\N,Missing
P11-1163,W09-1201,1,\N,Missing
P11-1163,W12-2400,0,\N,Missing
P11-1163,M98-1009,0,\N,Missing
P14-1092,P12-1007,0,0.0709132,"ng model for Japanense why QA. In terms of discourse parsing, Verberne et al. (2007) conducted an initial evaluation of the utility of RST structures to why QA by evaluating Figure 1: Architecture of the reranking framework for QA. performance on a small sample of seven WSJ articles drawn from the RST Treebank (Carlson et al., 2003). They later concluded that while discourse parsing appears to be useful for QA, automated discourse parsing tools are required before this approach can be tested at scale (Verberne et al., 2010). Inspired by this previous work and recent work in discourse parsing (Feng and Hirst, 2012), our work is the first to systematically explore structured discourse features driven by several discourse representations, combine discourse with lexical semantic models, and evaluate these representations on thousands of questions using both in-domain and cross-domain experiments. 3 Approach The proposed answer reranking component is embedded in the QA framework illustrated in Figure 1. This framework functions in two distinct scenarios, which use the same AR model, but differ in the way candidate answers are retrieved: CQA: In this scenario, the task is defined as reranking all the user-po"
P14-1092,P07-1059,0,0.0858365,"S, Yih et al. (2013) recently addressed the problem of answer sentence selection and demonstrated that LS models, including recurrent neural network language models (RNNLM), have a higher contribution to overall performance than exploiting syntactic analysis. We extend this work by showing that discourse models coupled with LS achieve the best performance for NF AR. The related work on NF QA is considerably more scarce, but several trends are clear. First, most NF QA approaches tend to use multiple similarity models (information retrieval or alignment) as features in discriminative rerankers (Riezler et al., 2007; Higashinaka and Isozaki, 2008; Verberne et al., 2010; Surdeanu et al., 2011). Second, and more relevant to this work, all these approaches focus either on bag-of-word representations or linguistic structures that are restricted to single sentences (e.g., syntactic dependencies, semantic roles, or standalone discourse cue phrases). Answering how questions using a single discourse marker, by, was previously explored by Prager et al. (2000), who searched for by followed by a present participle (e.g. by *ing) to elevate answer candidates in a ranking framework. Verberne et al. (2011) extracted 4"
P14-1092,I08-1055,0,0.258695,"ecently addressed the problem of answer sentence selection and demonstrated that LS models, including recurrent neural network language models (RNNLM), have a higher contribution to overall performance than exploiting syntactic analysis. We extend this work by showing that discourse models coupled with LS achieve the best performance for NF AR. The related work on NF QA is considerably more scarce, but several trends are clear. First, most NF QA approaches tend to use multiple similarity models (information retrieval or alignment) as features in discriminative rerankers (Riezler et al., 2007; Higashinaka and Isozaki, 2008; Verberne et al., 2010; Surdeanu et al., 2011). Second, and more relevant to this work, all these approaches focus either on bag-of-word representations or linguistic structures that are restricted to single sentences (e.g., syntactic dependencies, semantic roles, or standalone discourse cue phrases). Answering how questions using a single discourse marker, by, was previously explored by Prager et al. (2000), who searched for by followed by a present participle (e.g. by *ing) to elevate answer candidates in a ranking framework. Verberne et al. (2011) extracted 47 cue phrases such as because f"
P14-1092,J11-2003,1,0.622072,"tion and demonstrated that LS models, including recurrent neural network language models (RNNLM), have a higher contribution to overall performance than exploiting syntactic analysis. We extend this work by showing that discourse models coupled with LS achieve the best performance for NF AR. The related work on NF QA is considerably more scarce, but several trends are clear. First, most NF QA approaches tend to use multiple similarity models (information retrieval or alignment) as features in discriminative rerankers (Riezler et al., 2007; Higashinaka and Isozaki, 2008; Verberne et al., 2010; Surdeanu et al., 2011). Second, and more relevant to this work, all these approaches focus either on bag-of-word representations or linguistic structures that are restricted to single sentences (e.g., syntactic dependencies, semantic roles, or standalone discourse cue phrases). Answering how questions using a single discourse marker, by, was previously explored by Prager et al. (2000), who searched for by followed by a present participle (e.g. by *ing) to elevate answer candidates in a ranking framework. Verberne et al. (2011) extracted 47 cue phrases such as because from a small collection of web documents, and us"
P14-1092,P13-1171,0,0.205167,"he 52nd Annual Meeting of the Association for Computational Linguistics, pages 977–986, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics perform single-sentence models when answers span multiple sentences. 4. We demonstrate good domain transfer performance between these corpora, suggesting that answer discourse structures are largely independent of domain, and thus broadly applicable to NF QA. 2 Related Work The body of work on factoid QA is too broad to be discussed here (see, e.g., the TREC workshops for an overview). However, in the context of LS, Yih et al. (2013) recently addressed the problem of answer sentence selection and demonstrated that LS models, including recurrent neural network language models (RNNLM), have a higher contribution to overall performance than exploiting syntactic analysis. We extend this work by showing that discourse models coupled with LS achieve the best performance for NF AR. The related work on NF QA is considerably more scarce, but several trends are clear. First, most NF QA approaches tend to use multiple similarity models (information retrieval or alignment) as features in discriminative rerankers (Riezler et al., 2007"
P14-1092,P13-1170,0,0.0651051,"Missing"
P14-1092,W01-1605,0,\N,Missing
P14-1092,J10-2003,0,\N,Missing
P14-5010,P05-1045,1,0.148222,"ides a PTBstyle tokenizer, extended to reasonably handle noisy and web text. The corresponding components for Chinese and Arabic provide word and clitic segmentation. The tokenizer saves the character offsets of each token in the input text. cleanxml Removes most or all XML tags from the document. ssplit Splits a sequence of tokens into sentences. truecase Determines the likely true case of tokens in text (that is, their likely case in well-edited text), where this information was lost, e.g., for all upper case text. This is implemented with a discriminative model using a CRF sequence tagger (Finkel et al., 2005). pos Labels tokens with their part-of-speech (POS) tag, using a maximum entropy POS tagger (Toutanova et al., 2003). lemma Generates the lemmas (base forms) for all tokens in the annotation. gender Adds likely gender information to names. ner Recognizes named (PERSON, LOCATION, ORGANIZATION, MISC) and numerical (MONEY, NUMBER, DATE, TIME, DURATION, SET) entities. With the default Figure 4: A simple, complete example program. annotators in a pipeline is controlled by standard Java properties in a Properties object. The most basic property to specify is what annotators to run, in what order, as"
P14-5010,J13-4004,1,0.147631,"(IDEOLOGY), nationalities (NATIONALITY), religions (RELIGION), and titles (TITLE). parse Provides full syntactic analysis, including both constituent and dependency representation, based on a probabilistic parser (Klein and Manning, 2003; de Marneffe et al., 2006). sentiment Sentiment analysis with a compositional model over trees using deep learning (Socher et al., 2013). Nodes of a binarized tree of each sentence, including, in particular, the root node of each sentence, are given a sentiment score. dcoref Implements mention detection and both pronominal and nominal coreference resolution (Lee et al., 2013). The entire coreference graph of a text (with head words of mentions as nodes) is provided in the Annotation. Figure 5: An example of a simple custom annotator. The annotator marks the words of possibly multi-word locations that are in a gazetteer. props.setProperty(""tokenize.whitespace"", ""true""); props.setProperty(""ssplit.eolonly"", ""true""); or via command-line flags: -tokenize.whitespace -ssplit.eolonly We do not attempt to describe all the properties understood by each annotator here; they are available in the documentation for Stanford CoreNLP. However, we note that they follow the pattern"
P14-5010,D13-1170,1,0.163373,"Annotator is to provide a simple framework to allow a user to incorporate NE labels that are not annotated in traditional NL corpora. For example, a default list of regular expressions that we distribute in the models file recognizes ideologies (IDEOLOGY), nationalities (NATIONALITY), religions (RELIGION), and titles (TITLE). parse Provides full syntactic analysis, including both constituent and dependency representation, based on a probabilistic parser (Klein and Manning, 2003; de Marneffe et al., 2006). sentiment Sentiment analysis with a compositional model over trees using deep learning (Socher et al., 2013). Nodes of a binarized tree of each sentence, including, in particular, the root node of each sentence, are given a sentiment score. dcoref Implements mention detection and both pronominal and nominal coreference resolution (Lee et al., 2013). The entire coreference graph of a text (with head words of mentions as nodes) is provided in the Annotation. Figure 5: An example of a simple custom annotator. The annotator marks the words of possibly multi-word locations that are in a gazetteer. props.setProperty(""tokenize.whitespace"", ""true""); props.setProperty(""ssplit.eolonly"", ""true""); or via comman"
P14-5010,bethard-etal-2014-cleartk,1,0.157881,"Missing"
P14-5010,N03-1033,1,0.145833,"nese and Arabic provide word and clitic segmentation. The tokenizer saves the character offsets of each token in the input text. cleanxml Removes most or all XML tags from the document. ssplit Splits a sequence of tokens into sentences. truecase Determines the likely true case of tokens in text (that is, their likely case in well-edited text), where this information was lost, e.g., for all upper case text. This is implemented with a discriminative model using a CRF sequence tagger (Finkel et al., 2005). pos Labels tokens with their part-of-speech (POS) tag, using a maximum entropy POS tagger (Toutanova et al., 2003). lemma Generates the lemmas (base forms) for all tokens in the annotation. gender Adds likely gender information to names. ner Recognizes named (PERSON, LOCATION, ORGANIZATION, MISC) and numerical (MONEY, NUMBER, DATE, TIME, DURATION, SET) entities. With the default Figure 4: A simple, complete example program. annotators in a pipeline is controlled by standard Java properties in a Properties object. The most basic property to specify is what annotators to run, in what order, as shown here. But as discussed below, most annotators have their own properties to allow further customization of the"
P14-5010,de-marneffe-etal-2006-generating,1,\N,Missing
P14-5010,chang-manning-2012-sutime,1,\N,Missing
P14-5010,clarke-etal-2012-nlp,0,\N,Missing
P14-5010,P02-1022,0,\N,Missing
P15-4022,X98-1004,0,0.542919,"information extraction (IE) has long enjoyed wide adoption throughout industry, though it has remained largely ignored in academia, in favor of machine learning (ML) methods (Chiticariu et al., 2013). However, rule-based systems have several advantages over pure ML systems, including: (a) the rules are interpretable and thus suitable for rapid development and domain transfer; and (b) humans and machines can contribute to the same model. Why then have such systems failed to hold the attention of the academic community? One argument raised by Chiticariu et al. is that, despite notable efforts (Appelt and Onyshkevych, 1998; Levy and Andrew, 2006; Hunter et al., 2008; Cunningham et al., 2011; Chang and Manning, 2014), there is not a standard language for this task, or a “standard way to express rules”, which raises the entry cost Robust: To recover from unavoidable syntactic errors, SD patterns (such as the ones in Examples 1 and 2) can be can be freely mixed with surface, token-based patterns, using a language inspired by the Allen Insti1 Hereafter abbreviated as SD. Events that take other events as arguments (see Figure 1 and the corresponding Example (2) for such an event in the biochemical domain. The Positi"
P15-4022,W13-2014,0,0.0150214,"Missing"
P15-4022,D13-1079,0,0.0104361,"ex regular expressions over syntactic patterns for event arguments. Inspired by Stanford’s Semgrex3 , we have extended a standard regular expression language to describe patterns over directed graphs4 , e.g., we introduce new &lt; and &gt; operators to specify the direction of edge traversal in the dependency graph. Finally, we allow for (c) optional arguments5 and multiple arguments with the same name. Introduction Rule-based information extraction (IE) has long enjoyed wide adoption throughout industry, though it has remained largely ignored in academia, in favor of machine learning (ML) methods (Chiticariu et al., 2013). However, rule-based systems have several advantages over pure ML systems, including: (a) the rules are interpretable and thus suitable for rapid development and domain transfer; and (b) humans and machines can contribute to the same model. Why then have such systems failed to hold the attention of the academic community? One argument raised by Chiticariu et al. is that, despite notable efforts (Appelt and Onyshkevych, 1998; Levy and Andrew, 2006; Hunter et al., 2008; Cunningham et al., 2011; Chang and Manning, 2014), there is not a standard language for this task, or a “standard way to expre"
P15-4022,W08-1301,0,0.0182333,"Missing"
P15-4022,W04-1204,0,0.299922,"Missing"
P15-4022,levy-andrew-2006-tregex,0,0.475656,"as long enjoyed wide adoption throughout industry, though it has remained largely ignored in academia, in favor of machine learning (ML) methods (Chiticariu et al., 2013). However, rule-based systems have several advantages over pure ML systems, including: (a) the rules are interpretable and thus suitable for rapid development and domain transfer; and (b) humans and machines can contribute to the same model. Why then have such systems failed to hold the attention of the academic community? One argument raised by Chiticariu et al. is that, despite notable efforts (Appelt and Onyshkevych, 1998; Levy and Andrew, 2006; Hunter et al., 2008; Cunningham et al., 2011; Chang and Manning, 2014), there is not a standard language for this task, or a “standard way to express rules”, which raises the entry cost Robust: To recover from unavoidable syntactic errors, SD patterns (such as the ones in Examples 1 and 2) can be can be freely mixed with surface, token-based patterns, using a language inspired by the Allen Insti1 Hereafter abbreviated as SD. Events that take other events as arguments (see Figure 1 and the corresponding Example (2) for such an event in the biochemical domain. The Positive Regulation takes a P"
P15-4022,P14-5010,1,0.00639518,"ed Framework for Event Extraction Marco A. Valenzuela-Esc´arcega Gus Hahn-Powell Thomas Hicks Mihai Surdeanu University of Arizona, Tucson, AZ, USA {marcov,hahnpowell,msurdeanu,hickst}@email.arizona.edu Abstract for new rule-based systems. Here we aim to address this issue with a novel event extraction (EE) language and framework called ODIN (Open Domain INformer). We follow the simplicity principles promoted by other natural language processing toolkits, such as Stanford’s CoreNLP, which aim to “avoid over-design”, “do one thing well”, and have a user “up and running in ten minutes or less” (Manning et al., 2014). In particular, our approach is: We describe the design, development, and API of ODIN (Open Domain INformer), a domainindependent, rule-based event extraction (EE) framework. The proposed EE approach is: simple (most events are captured with simple lexico-syntactic patterns), powerful (the language can capture complex constructs, such as events taking other events as arguments, and regular expressions over syntactic graphs), robust (to recover from syntactic parsing errors, syntactic patterns can be freely mixed with surface, token-based patterns), and fast (the runtime environment processes"
P15-4022,W13-2009,0,\N,Missing
P15-4022,N15-3007,0,\N,Missing
P15-4022,D10-1098,0,\N,Missing
P15-4022,E12-2021,0,\N,Missing
P17-4018,J13-4004,1,0.746933,"lowing Banko et al. (2007), we instead consider expanded noun phrases as a coarse approximation of the concepts we wish to link. Starting at each noun, we traverse amod, advmod, ccmod, dobj, nn, vmod, and prep_* Stanford collapsed dependency relations and promote only the longest span to our pool of candidate concepts. For example, starting at infections in “viral infections among infants are [. . . ]”, the expansion procedure produces viral infections among infants as a candidate entity. Coreference Resolution and Negation The coreference resolution component in REACH adapts the algorithm of Lee et al. (2013) to the biomedical domain, where it operates both over entity mentions (e.g., by resolving pronominal and nominal mentions such as “it” or “this protein” to the corresponding entity) and event mentions (e.g., “this interaction” is resolved to an actual event) (Bell et al., 2016). The negation detection module identifies explicit statements that a reaction does not occur (e.g., “ZAP70 does not induce TRIM phosphorylation”) in a particular experimental context. REACH also handles more subtle linguistic phenomena such as reversing the polarity of events. For example, the naive interpretation of t"
P17-4018,L16-1027,1,0.838762,"st span to our pool of candidate concepts. For example, starting at infections in “viral infections among infants are [. . . ]”, the expansion procedure produces viral infections among infants as a candidate entity. Coreference Resolution and Negation The coreference resolution component in REACH adapts the algorithm of Lee et al. (2013) to the biomedical domain, where it operates both over entity mentions (e.g., by resolving pronominal and nominal mentions such as “it” or “this protein” to the corresponding entity) and event mentions (e.g., “this interaction” is resolved to an actual event) (Bell et al., 2016). The negation detection module identifies explicit statements that a reaction does not occur (e.g., “ZAP70 does not induce TRIM phosphorylation”) in a particular experimental context. REACH also handles more subtle linguistic phenomena such as reversing the polarity of events. For example, the naive interpretation of the text “decreased PTPN13 expression enhances EphrinB1 phosphorylation” yields a positive regulation (due to “enhances”). The polarity correction module changes this to a negative regulation due to the presence of the “decreased” modifier. Event Extraction and Resolution For eve"
P17-4018,W09-1402,0,0.0335526,"alenzuela-Escarcega et al., 2016) and captures 17 kinds of events, including nested events (i.e., events involving other events). The event grammars are applied in cascades composed of rules that describe patterns over both syntactic dependencies and token sequences, using constraints over a token’s attributes (part-of-speech tag, lemma, etc.). The system architecture is summarized in Figure 1, and described below. There is a substantial body of work addressing open-domain machine reading (Banko et al., 2007; Carlson et al., 2010; Zhang, 2015), as well as systems that target specific domains (Björne et al., 2009; Nédellec et al., 2013; Peters et al., 2014). All of these systems read individual facts, which makes Swanson’s observation even more valid today. Attempts have been made in the biomedical domain to assemble relations extracted by machine reading into coherent models (Hahn-Powell et al., 2016). This domain is known for the complexity of its models (Lander, 2010), which has spurred research on improved visualizations for actionable insights (Dang et al., 2015). Our work builds on these previous efforts by assembling complex influence graphs from the extractions of a machine reading system, and"
P17-4018,L16-1050,1,0.464542,"Missing"
P17-4018,W16-2920,1,0.832322,"r a token’s attributes (part-of-speech tag, lemma, etc.). The system architecture is summarized in Figure 1, and described below. There is a substantial body of work addressing open-domain machine reading (Banko et al., 2007; Carlson et al., 2010; Zhang, 2015), as well as systems that target specific domains (Björne et al., 2009; Nédellec et al., 2013; Peters et al., 2014). All of these systems read individual facts, which makes Swanson’s observation even more valid today. Attempts have been made in the biomedical domain to assemble relations extracted by machine reading into coherent models (Hahn-Powell et al., 2016). This domain is known for the complexity of its models (Lander, 2010), which has spurred research on improved visualizations for actionable insights (Dang et al., 2015). Our work builds on these previous efforts by assembling complex influence graphs from the extractions of a machine reading system, and through a novel search engine that efficiently searches this graph and visualizes the results in a simple and intuitive interface. 3 Preprocessing Extraction of Entities Grounding via KBs Extraction of Simple Events Extraction of Nested Events Negation and Polarity Detection Approach Coreferen"
Q15-1015,E09-1005,0,0.0194689,"Missing"
Q15-1015,P14-1023,0,0.0289107,"nalysis already performed by the CR model. Mikolov et al. (2013) to this QA task. In particular, we use their skip-gram model with hierarchical sampling. This model predicts the context of a word given the word itself, and through this process embeds words into a latent conceptual space with a fixed number of dimensions. Consequently, related words tend to have vector representations that are close to each other in this space. This type of predictive algorithm has been found to perform considerably better than count-based approaches to distributional similarity on a variety of semantic tasks (Baroni et al., 2014). We derive four LS measures from these vectors, which are then are included as features in the reranker. The first is a measure of the overall similarity of the question and answer candidate, which is computed as the cosine similarity between two composite vectors. These composite vectors are assembled by summing the vectors for individual question (or answer candidate) words, and re-normalizing this composite vector to unit length.5 In addition to this overall similarity score, we compute the pairwise similarities between each word in the question and answer candidates, and include as featur"
Q15-1015,S13-1002,0,0.0187129,"-order NNLM: In the NNLM setting, we use the cosine similarity of vectors as interpolation weights and to choose the nearest neighbors: scos (w(i), w(j)) = w(i) · w(j) ||w(i) |w(j)|| (9) We found that applying the softmax function to each term’s vector of k-highest scos similarities, to ensure all interpolation weights are positive and have a consistent range across terms, improved performance. As such, all higher-order NNLM models use this softmax normalization. 203 The resulting interpolation can be conceptualized in several ways. Viewing cosine similarity as a representation of entailment (Beltagy et al., 2013), the higher-order NNLM model reflects multiple-hop inference on top of the corresponding association graph, similar to the higher-order alignment model. The interpolation could also be viewed as smoothing term representations in vector space, averaging each term’s vector with its nearest neighbors according to their cosine similarity. Higher-order Hybrid Model: We also implement a hybrid model, which interpolates the alignment distribution vectors, but using the pairwise cosine similarities from the NNLM setting: X w ˆ A (i) = scos (wN (i), wN (j)) wA (j) j∈Nk (wN (i)) where wN (i) and wA (i)"
Q15-1015,J93-2003,0,0.0585777,"individual question (or answer candidate) words, and re-normalizing this composite vector to unit length.5 In addition to this overall similarity score, we compute the pairwise similarities between each word in the question and answer candidates, and include as features the average, minimum, and maximum pairwise similarities. 4.2 Alignment Models Berger et al. (2000) showed that learning questionto-answer transformations using a statistical machine translation (SMT) model begins to “bridge the lexical chasm” between questions and answers. We build upon this observation, using the IBM Model 1 (Brown et al., 1993) variant of Surdeanu et al. (2011) to determine the probability that a question Q is a translation of an answer A, P (Q|A): Y P (Q|A) = P (q|A) (1) q∈Q P (q|A) = (1 − λ)Pml (q|A) + λPml (q|C) X Pml (q|A) = (T (q|a)Pml (a|A)) the probabilities that the question term q is a translation of any answer term a, T (q|a), weighted by the probability that a is generated from A. The translation table for T (q|a) is computed using GIZA++ (Och and Ney, 2003). Similar to Surdeanu et al. (2011) we: (a) set Pml (q|C) to a small value for out-of-vocabulary words; (b) modify the T (.|.) distributions to guaran"
Q15-1015,de-marneffe-etal-2006-generating,0,0.0294551,"Missing"
Q15-1015,P03-1003,0,0.29025,"ent models capture complementary information, and can be combined to improve the performance of the CQA system for manner questions. 2 Related Work We focus on statistical LS methods for opendomain QA, in particular CQA tasks, such as the ones driven by Yahoo! Answers datasets (Wang et al., 2009; Jansen et al., 2014). Berger et al. (2000) were the first to propose a statistical LS model to “bridge the lexical chasm” between questions and answers for a QA task. Building off this work, a number of LS models using either words or other syntactic and semantic structures have been proposed for QA (Echihabi and Marcu, 2003; Soricut and Brill, 2006; Riezler et al., 2007; Surdeanu et al., 2011; Yao et al., 2013; Jansen et al., 2014), or related tasks, such as semantic textual similarity (Sultan et al., 2014). However, all of these models fall into the class of first-order models. To our knowledge, this work is the first to investigate higher-order LS models for QA, which can take advantage of indirect evidence, i.e., the “neighbors of neighbors” in the association graph. Second-order lexical models have recently been brought to bear on a variety of other NLP tasks. Zapirain et al. (2013) use a second-order model"
Q15-1015,P14-1092,1,0.661709,"els, over both words and syntactic structures, can be adapted to the proposed higher-order formalism. In this latter respect we introduce a novel syntax-based variant of the neural network language model (NNLM) of Mikolov et al. (2013) that models syntactic dependencies rather than words, which allows it to capture knowledge that is complementary to that of word-based NNLMs. 3. The training process for alignment models requires a large corpus of QA pairs. Due to these resource requirements, we evaluate our higher-order LS models on a community question answering (CQA) task (Wang et al., 2009; Jansen et al., 2014) across thousands of how questions, and show that most higher-order models perform significantly better than their first-order variants. 4. We demonstrate that language models and alignment models capture complementary information, and can be combined to improve the performance of the CQA system for manner questions. 2 Related Work We focus on statistical LS methods for opendomain QA, in particular CQA tasks, such as the ones driven by Yahoo! Answers datasets (Wang et al., 2009; Jansen et al., 2014). Berger et al. (2000) were the first to propose a statistical LS model to “bridge the lexical c"
Q15-1015,D11-1049,0,0.0276153,"model based on distributional similarity (Lin, 1998) to improve a selectional preference model, and Lee et al. (2012) use a similar approach for coreference resolution. Our work expands on these ideas with models of arbitrary order, an approach to control semantic drift, and an application to QA. This work falls under the larger umbrella of algorithms for graph-based inference, which have been successfully applied to other NLP and information retrieval problems, such as relation extraction (Chakrabarti and Agarwal, 2006; Chakrabarti, 2007; Lao and Cohen, 2010), inference over knowledge bases (Lao et al., 2011), name disambiguation (Minkov et al., 2006), and search (Bhalotia et al., 2002; Tong et al., 2006). While many of these approaches use random walk algorithms, in pilot experiments we observed that random walks, such as PageRank (PR) (Page et al., 1999), tend to accumulate semantic drift from the originating node because they consider all possible paths in the graph. This semantic drift reduces the quality of the higher-order associations, which impacts QA performance. Here we implement a conservative graph traversal algorithm, similar in spirit to the “cautious bootstrapping” algorithm of Yaro"
Q15-1015,D12-1045,1,0.49906,"en et al., 2014), or related tasks, such as semantic textual similarity (Sultan et al., 2014). However, all of these models fall into the class of first-order models. To our knowledge, this work is the first to investigate higher-order LS models for QA, which can take advantage of indirect evidence, i.e., the “neighbors of neighbors” in the association graph. Second-order lexical models have recently been brought to bear on a variety of other NLP tasks. Zapirain et al. (2013) use a second-order model based on distributional similarity (Lin, 1998) to improve a selectional preference model, and Lee et al. (2012) use a similar approach for coreference resolution. Our work expands on these ideas with models of arbitrary order, an approach to control semantic drift, and an application to QA. This work falls under the larger umbrella of algorithms for graph-based inference, which have been successfully applied to other NLP and information retrieval problems, such as relation extraction (Chakrabarti and Agarwal, 2006; Chakrabarti, 2007; Lao and Cohen, 2010), inference over knowledge bases (Lao et al., 2011), name disambiguation (Minkov et al., 2006), and search (Bhalotia et al., 2002; Tong et al., 2006)."
Q15-1015,P14-2050,0,0.0139523,"ergence (JSD) between their conditional distributions.6 Let m(i, j) be the element-wise average of the vectors w(i) and w(j), that is (w(i) + w(j))/2, and let K(w, v) be the Kullback-Leibler divergence between distributions (represented as vectors) w and v: (2) (3) K(w, v) = i=0 a∈A where the probability that the question term q is generated from answer A, P (q|A), is smoothed using the prior probability that the term q is generated from the entire collection of answers C, Pml (q|C). Pml (q|A) is computed as the sum of 5 We also tried the multiplicative strategy for word-vector combination of Levy and Goldberg (2014b), but it did not improve our results. 200 |V | X wi ln wi vi (4) where |V |is the vocabulary size (and the dimensionality of w). Then the distance between words i and j is J(w(i), w(j)) = r K(w(i), m(i, j)) + K(w(j), m(i, j)) 2 (5) 6 We use the square root of the Jensen-Shannon divergence, derived from Kullback-Leibler divergence, since it is a distance metric (in particular it is finite and symmetric). We derive four additional LS features from these alignment vector representations, which parallel the features derived from the NNLM vector representations (§4.1). The first is the JSD betwee"
Q15-1015,W14-1618,0,0.0604173,"ergence (JSD) between their conditional distributions.6 Let m(i, j) be the element-wise average of the vectors w(i) and w(j), that is (w(i) + w(j))/2, and let K(w, v) be the Kullback-Leibler divergence between distributions (represented as vectors) w and v: (2) (3) K(w, v) = i=0 a∈A where the probability that the question term q is generated from answer A, P (q|A), is smoothed using the prior probability that the term q is generated from the entire collection of answers C, Pml (q|C). Pml (q|A) is computed as the sum of 5 We also tried the multiplicative strategy for word-vector combination of Levy and Goldberg (2014b), but it did not improve our results. 200 |V | X wi ln wi vi (4) where |V |is the vocabulary size (and the dimensionality of w). Then the distance between words i and j is J(w(i), w(j)) = r K(w(i), m(i, j)) + K(w(j), m(i, j)) 2 (5) 6 We use the square root of the Jensen-Shannon divergence, derived from Kullback-Leibler divergence, since it is a distance metric (in particular it is finite and symmetric). We derive four additional LS features from these alignment vector representations, which parallel the features derived from the NNLM vector representations (§4.1). The first is the JSD betwee"
Q15-1015,P98-2127,0,0.0467549,"t al., 2007; Surdeanu et al., 2011; Yao et al., 2013; Jansen et al., 2014), or related tasks, such as semantic textual similarity (Sultan et al., 2014). However, all of these models fall into the class of first-order models. To our knowledge, this work is the first to investigate higher-order LS models for QA, which can take advantage of indirect evidence, i.e., the “neighbors of neighbors” in the association graph. Second-order lexical models have recently been brought to bear on a variety of other NLP tasks. Zapirain et al. (2013) use a second-order model based on distributional similarity (Lin, 1998) to improve a selectional preference model, and Lee et al. (2012) use a similar approach for coreference resolution. Our work expands on these ideas with models of arbitrary order, an approach to control semantic drift, and an application to QA. This work falls under the larger umbrella of algorithms for graph-based inference, which have been successfully applied to other NLP and information retrieval problems, such as relation extraction (Chakrabarti and Agarwal, 2006; Chakrabarti, 2007; Lao and Cohen, 2010), inference over knowledge bases (Lao et al., 2011), name disambiguation (Minkov et al"
Q15-1015,P14-5010,1,0.0355837,"compared the effects of using either words or lemmas as the base lexical unit for the LS models, and found that words achieved higher P@1 scores in both the alignment and NNLM models on the development dataset. As such, all results reported here use words for the syntax-independent models, and tuples of words for the syntax-driven models. Content Filtering: We investigated using partof-speech (POS) tags to filter the content consid12 ered by the lexical similarity models, by excluding certain non-informative classes of words such as determiners. Using POS tags generated by Stanford’s CoreNLP (Manning et al., 2014), we filtered content to only include nouns, adjectives, verbs, and adverbs for the word-based models, and tuples where both words have one of these four POS tags for the syntax-based models. We found that this increased P@1 scores for all wordbased alignment and NNLM models (including the Levy-Goldberg (L-G) model13 ), but did not improve performance for models that used dependency representations.14 Results reported in the remainder of this paper use this POS filtering for all word-based alignment and NNLM models (including L-G’s) as well as the dependency alignment model, but not for our de"
Q15-1015,H05-1086,0,0.0431779,"a translation of an answer A, P (Q|A): Y P (Q|A) = P (q|A) (1) q∈Q P (q|A) = (1 − λ)Pml (q|A) + λPml (q|C) X Pml (q|A) = (T (q|a)Pml (a|A)) the probabilities that the question term q is a translation of any answer term a, T (q|a), weighted by the probability that a is generated from A. The translation table for T (q|a) is computed using GIZA++ (Och and Ney, 2003). Similar to Surdeanu et al. (2011) we: (a) set Pml (q|C) to a small value for out-of-vocabulary words; (b) modify the T (.|.) distributions to guarantee that the probability of translating a word to itself, i.e., T (w|w), is highest (Murdock and Croft, 2005); and (c) tune the smoothing parameter λ on a development corpus. QA systems generally use the above model to determine the global alignment probability between a given question and answer candidate, P (Q|A). A novel contribution of our work is that we also use the alignment model’s probability distributions (from a source word to destination words) as distributed representations for source words, based on the observation that words with similar alignment distributions are likely to have a similar meaning. Formally, we denote the alignment vector representation for the ith word in the vocabula"
Q15-1015,W12-3018,0,0.0275503,"Missing"
Q15-1015,J03-1002,0,0.0307592,"e translation (SMT) model begins to “bridge the lexical chasm” between questions and answers. We build upon this observation, using the IBM Model 1 (Brown et al., 1993) variant of Surdeanu et al. (2011) to determine the probability that a question Q is a translation of an answer A, P (Q|A): Y P (Q|A) = P (q|A) (1) q∈Q P (q|A) = (1 − λ)Pml (q|A) + λPml (q|C) X Pml (q|A) = (T (q|a)Pml (a|A)) the probabilities that the question term q is a translation of any answer term a, T (q|a), weighted by the probability that a is generated from A. The translation table for T (q|a) is computed using GIZA++ (Och and Ney, 2003). Similar to Surdeanu et al. (2011) we: (a) set Pml (q|C) to a small value for out-of-vocabulary words; (b) modify the T (.|.) distributions to guarantee that the probability of translating a word to itself, i.e., T (w|w), is highest (Murdock and Croft, 2005); and (c) tune the smoothing parameter λ on a development corpus. QA systems generally use the above model to determine the global alignment probability between a given question and answer candidate, P (Q|A). A novel contribution of our work is that we also use the alignment model’s probability distributions (from a source word to destinat"
Q15-1015,P07-1059,0,0.0600946,"an be combined to improve the performance of the CQA system for manner questions. 2 Related Work We focus on statistical LS methods for opendomain QA, in particular CQA tasks, such as the ones driven by Yahoo! Answers datasets (Wang et al., 2009; Jansen et al., 2014). Berger et al. (2000) were the first to propose a statistical LS model to “bridge the lexical chasm” between questions and answers for a QA task. Building off this work, a number of LS models using either words or other syntactic and semantic structures have been proposed for QA (Echihabi and Marcu, 2003; Soricut and Brill, 2006; Riezler et al., 2007; Surdeanu et al., 2011; Yao et al., 2013; Jansen et al., 2014), or related tasks, such as semantic textual similarity (Sultan et al., 2014). However, all of these models fall into the class of first-order models. To our knowledge, this work is the first to investigate higher-order LS models for QA, which can take advantage of indirect evidence, i.e., the “neighbors of neighbors” in the association graph. Second-order lexical models have recently been brought to bear on a variety of other NLP tasks. Zapirain et al. (2013) use a second-order model based on distributional similarity (Lin, 1998)"
Q15-1015,Q14-1018,0,0.0177403,"domain QA, in particular CQA tasks, such as the ones driven by Yahoo! Answers datasets (Wang et al., 2009; Jansen et al., 2014). Berger et al. (2000) were the first to propose a statistical LS model to “bridge the lexical chasm” between questions and answers for a QA task. Building off this work, a number of LS models using either words or other syntactic and semantic structures have been proposed for QA (Echihabi and Marcu, 2003; Soricut and Brill, 2006; Riezler et al., 2007; Surdeanu et al., 2011; Yao et al., 2013; Jansen et al., 2014), or related tasks, such as semantic textual similarity (Sultan et al., 2014). However, all of these models fall into the class of first-order models. To our knowledge, this work is the first to investigate higher-order LS models for QA, which can take advantage of indirect evidence, i.e., the “neighbors of neighbors” in the association graph. Second-order lexical models have recently been brought to bear on a variety of other NLP tasks. Zapirain et al. (2013) use a second-order model based on distributional similarity (Lin, 1998) to improve a selectional preference model, and Lee et al. (2012) use a similar approach for coreference resolution. Our work expands on thes"
Q15-1015,J11-2003,1,0.749423,"search, Berger (2000) observed that lexical matching methods are generally insufficient for QA, where questions and answers often have little to no lexical overlap (as in the case of Where should we go for breakfast? and Zoe’s Diner has great pancakes). Previous work has shown that lexical semantics (LS) models are well suited to bridging this “lexical chasm”, and at least two flavors of lexical semantics have been successfully applied to QA. The first treats QA as a monolingual alignment problem, learning associations between words (or other structures) that appear in question-answer pairs (Surdeanu et al., 2011; Yao et al., 2013). The second computes the semantic similarity between question and answer using language models acquired from relevant texts (Yih et al., 2013; Jansen et al., 2014). Here we argue that while these models begin to bridge the “lexical chasm”, many still suffer from sparsity and only capitalize on direct evidence. Returning to our example question, if we also train on the QA pair What goes well with pancakes? and hashbrowns and toast, we can use the 197 Transactions of the Association for Computational Linguistics, vol. 3, pp. 197–210, 2015. Action Editor: Sharon Goldwater. Sub"
Q15-1015,P12-1065,0,0.0121701,"disambiguation (Minkov et al., 2006), and search (Bhalotia et al., 2002; Tong et al., 2006). While many of these approaches use random walk algorithms, in pilot experiments we observed that random walks, such as PageRank (PR) (Page et al., 1999), tend to accumulate semantic drift from the originating node because they consider all possible paths in the graph. This semantic drift reduces the quality of the higher-order associations, which impacts QA performance. Here we implement a conservative graph traversal algorithm, similar in spirit to the “cautious bootstrapping” algorithm of Yarowsky (Whitney and Sarkar, 2012; Yarowsky, 1995). By constraining the traversal paths, our algorithm runs two orders of magnitude faster than PR, while controlling for semantic drift. 3 Approach The architecture of our proposed QA framework is illustrated in Figure 2. Here we evaluate both firstorder and higher-order LS models in the context of community question answering (CQA), using a large dataset of QA pairs from Yahoo! Answers1 . We use a standard CQA evaluation task (Jansen et al., 2014), where one must rank a set of usergenerated answers to a given question, such that the community-selected best answer appears in th"
Q15-1015,D13-1056,1,0.846443,"Missing"
Q15-1015,P95-1026,0,0.461793,"al., 2006), and search (Bhalotia et al., 2002; Tong et al., 2006). While many of these approaches use random walk algorithms, in pilot experiments we observed that random walks, such as PageRank (PR) (Page et al., 1999), tend to accumulate semantic drift from the originating node because they consider all possible paths in the graph. This semantic drift reduces the quality of the higher-order associations, which impacts QA performance. Here we implement a conservative graph traversal algorithm, similar in spirit to the “cautious bootstrapping” algorithm of Yarowsky (Whitney and Sarkar, 2012; Yarowsky, 1995). By constraining the traversal paths, our algorithm runs two orders of magnitude faster than PR, while controlling for semantic drift. 3 Approach The architecture of our proposed QA framework is illustrated in Figure 2. Here we evaluate both firstorder and higher-order LS models in the context of community question answering (CQA), using a large dataset of QA pairs from Yahoo! Answers1 . We use a standard CQA evaluation task (Jansen et al., 2014), where one must rank a set of usergenerated answers to a given question, such that the community-selected best answer appears in the top position. M"
Q15-1015,P13-1171,0,0.536462,"as in the case of Where should we go for breakfast? and Zoe’s Diner has great pancakes). Previous work has shown that lexical semantics (LS) models are well suited to bridging this “lexical chasm”, and at least two flavors of lexical semantics have been successfully applied to QA. The first treats QA as a monolingual alignment problem, learning associations between words (or other structures) that appear in question-answer pairs (Surdeanu et al., 2011; Yao et al., 2013). The second computes the semantic similarity between question and answer using language models acquired from relevant texts (Yih et al., 2013; Jansen et al., 2014). Here we argue that while these models begin to bridge the “lexical chasm”, many still suffer from sparsity and only capitalize on direct evidence. Returning to our example question, if we also train on the QA pair What goes well with pancakes? and hashbrowns and toast, we can use the 197 Transactions of the Association for Computational Linguistics, vol. 3, pp. 197–210, 2015. Action Editor: Sharon Goldwater. Submission batch: 12/2014; Revision batch 3/2015; Published 4/2015. c 2015 Association for Computational Linguistics. Distributed under a CC-BY-NC-SA 4.0 license. t"
Q15-1015,J13-3006,1,\N,Missing
Q15-1015,C98-2122,0,\N,Missing
reschke-etal-2014-event,D12-1042,1,\N,Missing
reschke-etal-2014-event,P09-1113,1,\N,Missing
reschke-etal-2014-event,W11-0307,0,\N,Missing
S07-1095,atserias-etal-2006-freeling,0,0.0251929,"Missing"
S07-1095,W03-0421,1,0.890417,"Missing"
S07-1095,P02-1034,0,0.0541836,"Missing"
S07-1095,H05-1081,1,0.897279,"Missing"
S07-1095,S07-1008,1,0.816864,"Missing"
S07-1095,P05-1073,0,0.0449545,"to the features used, we focus only on global features that can be extracted independently of the local models. We show in Section 6 that this approach performs better on the small SemEval corpora than approaches that include features from the local models. We group the features into two sets: (a) features that extract information from the whole candidate set, and (b) features that model the structure of each candidate frame: Features from the whole candidate set: (1) Position of the current candidate in the whole set. Frame candidates are generated using the dynamic programming algorithm of Toutanova et al. (2005), and then sorted in descending order of the log probability of the whole frame (i.e., the sum of all argument log probabilities as reported by the local model). Hence, smaller positions indicate candidates that the local model considers better. (2) For each argument in the current frame, we store its number of repetitions in the whole candidate set. The intuition is that an argument that appears in many candidate frames is most likely correct. Features from each candidate frame: (3) The complete sequence of argument labels, extended with the predicate lemma and voice, similar to Toutanova et"
S14-1016,A00-1031,0,0.035504,"verbs, such as to be and feel, and extract mental state labels from these structures if the corresponding subject is one of the mentions detected above. Second, we search for mental states along adjectival modifier relations, where the head is an actor mention. For all patterns, we make sure to filter for only mental state complements belonging to the initial seed list. The same POS restriction as in the other models also applies. We increment the joint frequency f for the n-gram once for As λ1 + λ2 + λ3 = 1, P represents a probability distribution. We use the deleted interpolation algorithm (Brants, 2000) to estimate one set of lambda values for the model, based on all trigrams. For each query tuple generated in a video, 160 different trigrams are computed, one for each mental state label in the seed set, resulting in 160 conditional probability scores. We normalize these scores into a single distribution – the mental state distribution for that query tuple. We then combine 5 http://nlp.stanford.edu/software/ corenlp.shtml. 125 G R1 R2 R3 each neighborhood that properly contain all search terms from the n-gram in the correct POS. The event model addresses both limitations of the sent model: it"
S14-1016,H05-1004,0,0.0156444,"scared) = 1, with σ of any two identical strings being 1, and σ of all other pairs are 0. The F1 score penalizes the responses in Table 3 that include semantically similar labels to those in G, and fails to reflect the weights of the labels in G and R. 5.2 p 0 1 Similarity-Aligned F1 Score Although the standard F1 does not immediately fit our needs, it is a good starting point. We can incorporate the semantic similarity of distribution elements by generalizing the formulas for precision and recall as follows: 1 X precision = max σ(r, g) , g∈G |R| r∈R (3) 1 X recall = max σ(r, g) , r∈R |G| by (Luo, 2005) for coreference resolution. CEAF computes an optimal one-to-one mapping between subsets of reference and system entities before it computes recall, precision and F. Similarly, SA-F1 finds optimal mappings between the labels of the two sets based on σ (this is what the max terms in Eq 3 do). Table 3 shows that SA-F1 correctly rewards the use of synonyms. The high scores given to R2 , however, indicate that it does not measure the similarity between distribution shapes. where σ ∈ [0, 1] is a function that yields the similarity between two elements. The standard F1 has:  1, if r = g , σ(r, g) ="
S14-1016,P10-1018,0,0.0651864,"Missing"
S14-1016,P97-1003,0,0.0412198,"res for each context tuple. The distributions are then integrated into a single distribution representative of the complete activity as follows: (a) the distributions at each context back-off level are averaged to generate a single distribution per level – for the second level (which includes activity and actor types), it means distributions for all (activity, actor) tuples are averaged, whereas the first level only has a single distribution from the singleton activity tuple (chase); and (b) distributions for the different levels are linearly interpolated, similar to the back-off strategy of (Collins, 1997). Let e1 and e2 represent the weights of some mental state label m from the average distribution at the first and second level, respectively. Then the interpolated distribution score e for m is: also included frequently used labels gathered from synsets found in WordNet (see Table 1 for examples). Note that the gold standard annotations produced by MTurk workers (Sec. 3) was not a source for this set, nor was it restricted to these terms. 4.1 Back-off Interpolation in Vector Space Our first model uses the recurrent neural network language model (RNNLM) of Mikolov et al. (2013) to project both"
S14-1016,P11-2050,0,0.0441345,"Missing"
S14-1016,N04-3012,0,0.132289,"Missing"
S19-1020,D18-1217,0,0.0629242,"stance, adding a small amount random Gaussian jitter can be considered as noisy input. So are other image transformations such as translation, rotation, removing color and so on. However, a discrete domain such as language is not easily amenable to noise augmentation. While one can certainly add random Gaussian noise to embeddings of words (continuous vector representation such as word2vec rather than one-hot encoding), the intuition behind such perturbation is not apparent. Algorithms which require explicit modeling of noise require careful thinking in the language domain and is challenging (Clark et al., 2018; Nagesh and Surdeanu, 2018a). To the best of our knowledge, previous work in the area of modeling noise in natural language processing (NLP) applications has been limited. Clark et al. (2018) acknowledge the difficulty of modeling noise for language and incorporate a simple word dropout in their experiments. So does the work by Nagesh and Surdeanu (2018a). Nagesh and Surdeanu (2018b) add a standard Gaussian perturbation with a fixed variance to the pretrained word vectors to simulate noise. Belinkov and Bisk (2017) is perhaps one of the most comprehensive works that explore various noise stra"
S19-1020,N15-1128,0,0.0299465,"epochs. Further, the cost function is a linear combination of supervision cost (from the limited number amount of supervision) and consistency cost (agreement between the representation from the teacher and student models measured as the L2 norm difference between them). The motivation of using consistency in the cost function and averaging the parameters in the teacher is to reduce confirmation bias in the teacher when its own predictions In our previous work (Nagesh and Surdeanu, 2018a), we evaluated three different semi-supervised learning paradigms, namely, bootstrapping-based approaches (Gupta and Manning, 2015), ladder networks (Rasmus et al., 2015) and mean-teacher (Tarvainen and Valpola, 2017) for the semi-supervised named entity classification (NEC) task. The mean-teacher (MT) approach produced the best performance. However, our exploration of noise was limited in the previous study and hence is the focus of the current paper. Semi-supervised Deep Learning Semi-supervised learning (SSL) is one of the cornerstones in machine learning (ML) (Zhu, 2005). This is especially true in the case of natural language processing (NLP), as obtaining labeled training data is a costly and tedious process for mos"
S19-1020,P14-2050,0,0.0577174,"Missing"
S19-1020,C18-1196,1,0.929345,"ll amount random Gaussian jitter can be considered as noisy input. So are other image transformations such as translation, rotation, removing color and so on. However, a discrete domain such as language is not easily amenable to noise augmentation. While one can certainly add random Gaussian noise to embeddings of words (continuous vector representation such as word2vec rather than one-hot encoding), the intuition behind such perturbation is not apparent. Algorithms which require explicit modeling of noise require careful thinking in the language domain and is challenging (Clark et al., 2018; Nagesh and Surdeanu, 2018a). To the best of our knowledge, previous work in the area of modeling noise in natural language processing (NLP) applications has been limited. Clark et al. (2018) acknowledge the difficulty of modeling noise for language and incorporate a simple word dropout in their experiments. So does the work by Nagesh and Surdeanu (2018a). Nagesh and Surdeanu (2018b) add a standard Gaussian perturbation with a fixed variance to the pretrained word vectors to simulate noise. Belinkov and Bisk (2017) is perhaps one of the most comprehensive works that explore various noise strategies with a different end"
S19-1020,N18-2057,1,0.929207,"ll amount random Gaussian jitter can be considered as noisy input. So are other image transformations such as translation, rotation, removing color and so on. However, a discrete domain such as language is not easily amenable to noise augmentation. While one can certainly add random Gaussian noise to embeddings of words (continuous vector representation such as word2vec rather than one-hot encoding), the intuition behind such perturbation is not apparent. Algorithms which require explicit modeling of noise require careful thinking in the language domain and is challenging (Clark et al., 2018; Nagesh and Surdeanu, 2018a). To the best of our knowledge, previous work in the area of modeling noise in natural language processing (NLP) applications has been limited. Clark et al. (2018) acknowledge the difficulty of modeling noise for language and incorporate a simple word dropout in their experiments. So does the work by Nagesh and Surdeanu (2018a). Nagesh and Surdeanu (2018b) add a standard Gaussian perturbation with a fixed variance to the pretrained word vectors to simulate noise. Belinkov and Bisk (2017) is perhaps one of the most comprehensive works that explore various noise strategies with a different end"
S19-2232,C18-1139,0,0.0470975,"Missing"
S19-2232,P18-1119,0,0.335716,"red Task 12, in which we focus mainly on toponym detection. For this sub-task, we propose a recurrent neural network that combines word, character and affix information. By making use of the baseline provided by the organizers for toponym disambiguation, we also obtain results for the end-to-end sub-task. 2 Introduction Geoparsing is the task of detecting geolocation phrases in unstructured text and normalizing them to a unique identifier, e.g. GeoNames1 IDs. Although many automatic resolvers have been released in the past years, their performance fluctuates when applied to different domains (Gritta et al., 2018b). Most have also not been applied to and evaluated on scientific publications. The SemEval 2019 Shared Task 12: Toponym Resolution in Scientific Papers (Weissenbacher et al., 2019) aims to boost the research on geoparsing for the scientific domain by focusing on epidemiology journal articles. The task includes three sub-tasks: toponym detection, toponym disambiguation, and end-to-end toponym resolution. The first one requires participants to detect the text boundaries of all toponym mentions in articles. In toponym disambiguation, the toponym mentions are known, and the resolver has to align"
S19-2232,E12-2021,0,0.0868602,"Missing"
S19-2232,W03-0419,0,0.348497,"Missing"
S19-2232,S19-2155,0,0.128207,"Missing"
S19-2232,C18-1182,1,0.916829,"t (Stenetorp et al., 2012). The organizers also released a strong baseline that combines the model by Magge et al. (2018) for toponomy detection and the P opulation heuristic described in (Weissenbacher et al., 2015) for disambiguation.2 4 Approach 4.1 Preprocessing We used the tokenizer included in the baseline provided by the organizers as we observed it provided the best final results among other options (see Section 5.3). Again using baseline system preprocessing codes, we converted the data into CoNLL 2003 format (Tjong Kim Sang and De Meulder, 2003) for task 1. Following our prior work (Yadav and Bethard, 2018), we have used a BIO encoding instead of the IO encoding provided by the baseline system. We used the model proposed by Yadav et al. (2018) for Named Entity Recognition (NER), shown in figure 1, which uses character, word and affix information. In this architecture, a word is represented by concatenating its word embedding, an LSTM representation over the characters of the word, and learned embeddings for prefixes and suffixes of the word3 . Then another LSTM is used at the sentence level to give a contextual representation of each word. These representations of words in the sentence are given"
S19-2232,S18-2021,1,0.919374,"nce than both the individual heuristics. Gritta et al. (2018a) used a feedforward neural network approach for the disambiguation of geolocations. 1319 Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 1319–1323 Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics B-LOC Label I-LOC O Word CRF Word LSTM-B Word LSTM-F Word Representation Yor ork York Word Features Cit ∅ ity City ∅ ’s Char LSTM-B Char LSTM-F Char Embedding Characters Y o r k C i t y ’ s Figure 1: Word+character+affix neural network architecture from Yadav et al. (2018). 3 Data and Baseline 4.2 The corpus of the task is composed of 150 journal articles downloaded from PubMed Central. After removing the author names, acknowledgments and references, titles and body text were fully annotated. The annotators identified and labelled toponyms with their corresponding coordinates according to GeoNames. For cases not found in GeoNames, they used Google Maps and Wikipedia. If the coordinates of a toponym were not available in any of these resources the special value N/A was used. The data is provided in Brat format (Stenetorp et al., 2012). The organizers also releas"
W05-0635,W04-2412,0,0.0442031,"Missing"
W05-0635,W04-2415,0,0.0585849,"Missing"
W05-0635,J02-3001,0,0.117413,"apping is generally caused by incorrect syntax. Such cases are very hard to be learned due to the irregularities of the parser errors. Table 1 shows the distribution of semantic arguments into one of the above classes, using the syntactic trees provided by the Charniak parser. For the results reported in this paper, we model only oneto-one mappings between semantic arguments and syntactic constituents. A subset of the one-to-many mappings are addressed with a simple heuristic, described in Section 2.4. 2.2 Features The features incorporated in the proposed model are inspired from the work of (Gildea and Jurafsky, 2002; Surdeanu et al., 2003; Pradhan et al., 2005; Collins, 1999) and can be classified into five classes: (a) features that capture the internal structure of the candidate argument, (b) features extracted 222 The syntactic label of the candidate constituent. The constituent head word, suffixes of length 2, 3, and 4, lemma, and POS tag. The constituent content word, suffixes of length 2, 3, and 4, lemma, POS tag, and NE label. Content words, which add informative lexicalized information different from the head word, were detected using the heuristics of (Surdeanu et al., 2003). The first and last"
W05-0635,W04-2416,0,0.0180832,"Missing"
W05-0635,P03-1002,1,0.499442,"Missing"
W05-0635,J03-4003,0,\N,Missing
W05-0635,W05-0620,0,\N,Missing
W06-2207,J04-3004,0,0.133551,"osts, some of the latter approaches use lightly supervised bootstrapping algorithms that require as input only a small set of documents annotated with their corresponding category label. The focus of this paper is to improve such lightly supervised pattern acquisition methods. Moreover, we focus on robust bootstrapping algorithms that can handle real-world document collections, which contain many domains. Although a rich literature covers bootstrapping methods applied to natural language problems (Yarowsky, 1995; Riloff, 1996; Collins and Singer, 1999; Yangarber et al., 2000; Yangarber, 2003; Abney, 2004) several questions remain unanswered when these methods are applied to syntactic or semantic pattern acquisition. In this paper we answer two of these questions: (1) Can pattern acquisition be improved with text categorization techniques? Bootstrapping-based pattern acquisition algorithms can also be regarded as incremental text categorization (TC), since in each iteration documents containing certain patterns are assigned the corresponding category label. Although TC is obviously not the main goal of pattern acquisition methodologies, it is nevertheless an integral part of the learning algori"
W06-2207,W99-0613,0,0.758839,"ber, 2003; Stevenson and Greenwood, 2005). To minimize annotation costs, some of the latter approaches use lightly supervised bootstrapping algorithms that require as input only a small set of documents annotated with their corresponding category label. The focus of this paper is to improve such lightly supervised pattern acquisition methods. Moreover, we focus on robust bootstrapping algorithms that can handle real-world document collections, which contain many domains. Although a rich literature covers bootstrapping methods applied to natural language problems (Yarowsky, 1995; Riloff, 1996; Collins and Singer, 1999; Yangarber et al., 2000; Yangarber, 2003; Abney, 2004) several questions remain unanswered when these methods are applied to syntactic or semantic pattern acquisition. In this paper we answer two of these questions: (1) Can pattern acquisition be improved with text categorization techniques? Bootstrapping-based pattern acquisition algorithms can also be regarded as incremental text categorization (TC), since in each iteration documents containing certain patterns are assigned the corresponding category label. Although TC is obviously not the main goal of pattern acquisition methodologies, it"
W06-2207,P05-1047,0,0.557477,"dexing in news stories, feeding link discovery applications, etcetera. By and large the identification and selective extraction of relevant information is built around a set of domain-specific linguistic patterns. For example, for a “financial market change” domain one relevant pattern is &lt;NOUN fall MONEY to MONEY>. When this pattern is matched on the text “London gold fell $4.70 to $308.35”, a change of $4.70 is detected for the financial instrument “London gold”. Domain-specific patterns are either handcrafted or acquired automatically (Riloff, 1996; Yangarber et al., 2000; Yangarber, 2003; Stevenson and Greenwood, 2005). To minimize annotation costs, some of the latter approaches use lightly supervised bootstrapping algorithms that require as input only a small set of documents annotated with their corresponding category label. The focus of this paper is to improve such lightly supervised pattern acquisition methods. Moreover, we focus on robust bootstrapping algorithms that can handle real-world document collections, which contain many domains. Although a rich literature covers bootstrapping methods applied to natural language problems (Yarowsky, 1995; Riloff, 1996; Collins and Singer, 1999; Yangarber et al"
W06-2207,C00-2136,0,0.441999,"tional databases, providing eventlevel indexing in news stories, feeding link discovery applications, etcetera. By and large the identification and selective extraction of relevant information is built around a set of domain-specific linguistic patterns. For example, for a “financial market change” domain one relevant pattern is &lt;NOUN fall MONEY to MONEY>. When this pattern is matched on the text “London gold fell $4.70 to $308.35”, a change of $4.70 is detected for the financial instrument “London gold”. Domain-specific patterns are either handcrafted or acquired automatically (Riloff, 1996; Yangarber et al., 2000; Yangarber, 2003; Stevenson and Greenwood, 2005). To minimize annotation costs, some of the latter approaches use lightly supervised bootstrapping algorithms that require as input only a small set of documents annotated with their corresponding category label. The focus of this paper is to improve such lightly supervised pattern acquisition methods. Moreover, we focus on robust bootstrapping algorithms that can handle real-world document collections, which contain many domains. Although a rich literature covers bootstrapping methods applied to natural language problems (Yarowsky, 1995; Riloff"
W06-2207,P03-1044,0,0.454313,"ing eventlevel indexing in news stories, feeding link discovery applications, etcetera. By and large the identification and selective extraction of relevant information is built around a set of domain-specific linguistic patterns. For example, for a “financial market change” domain one relevant pattern is &lt;NOUN fall MONEY to MONEY>. When this pattern is matched on the text “London gold fell $4.70 to $308.35”, a change of $4.70 is detected for the financial instrument “London gold”. Domain-specific patterns are either handcrafted or acquired automatically (Riloff, 1996; Yangarber et al., 2000; Yangarber, 2003; Stevenson and Greenwood, 2005). To minimize annotation costs, some of the latter approaches use lightly supervised bootstrapping algorithms that require as input only a small set of documents annotated with their corresponding category label. The focus of this paper is to improve such lightly supervised pattern acquisition methods. Moreover, we focus on robust bootstrapping algorithms that can handle real-world document collections, which contain many domains. Although a rich literature covers bootstrapping methods applied to natural language problems (Yarowsky, 1995; Riloff, 1996; Collins a"
W06-2207,P95-1026,0,0.845671,"Yangarber et al., 2000; Yangarber, 2003; Stevenson and Greenwood, 2005). To minimize annotation costs, some of the latter approaches use lightly supervised bootstrapping algorithms that require as input only a small set of documents annotated with their corresponding category label. The focus of this paper is to improve such lightly supervised pattern acquisition methods. Moreover, we focus on robust bootstrapping algorithms that can handle real-world document collections, which contain many domains. Although a rich literature covers bootstrapping methods applied to natural language problems (Yarowsky, 1995; Riloff, 1996; Collins and Singer, 1999; Yangarber et al., 2000; Yangarber, 2003; Abney, 2004) several questions remain unanswered when these methods are applied to syntactic or semantic pattern acquisition. In this paper we answer two of these questions: (1) Can pattern acquisition be improved with text categorization techniques? Bootstrapping-based pattern acquisition algorithms can also be regarded as incremental text categorization (TC), since in each iteration documents containing certain patterns are assigned the corresponding category label. Although TC is obviously not the main goal o"
W06-2925,W06-2920,0,0.0741801,"Eisner (2000). We experiment with a large feature set that models: the tokens involved in dependencies and their immediate context, the surfacetext distance between tokens, and the syntactic context dominated by each dependency. In experiments, the treatment of multilingual information was totally blind. 1 2 Parsing and Learning Algorithms This section describes the three main components of the dependency parsing: the parsing model, the parsing algorithm, and the learning algorithm. 2.1 Introduction We describe a learning system for the CoNLL-X Shared Task on multilingual dependency parsing (Buchholz et al., 2006), for 13 different languages. Our system is a bottom-up projective dependency parser, based on the cubic-time algorithm by Eisner (1996; 2000). The parser uses a learning function that scores all possible labeled dependencies. This function is trained globally with online Perceptron, by parsing training sentences and correcting its parameters based on the parsing mistakes. The features used to score, while based on the previous work in dependency parsing (McDonald et al., 2005), introduce some novel concepts such as better codification of context and surface distances, and runtime information"
W06-2925,W02-1001,0,0.184514,", the scoring function is used to select the dependency label that maximizes the score. We take advantage of this two-step processing to introduce features for the scoring function that represent some of the internal dependencies of the span (see Section 3 for details). It has to be noted that the parsing algorithm we use does not score dependencies on top of every possible internal structure. Thus, by conditioning on features extracted from y we are making the search approximative. 2.3 Perceptron Learning As learning algorithm, we use Perceptron tailored for structured scenarios, proposed by Collins (2002). In recent years, Perceptron has been used in a number of Natural Language Learning works, such as in 182 partial parsing (Carreras et al., 2005) or even dependency parsing (McDonald et al., 2005). Perceptron is an online learning algorithm that learns by correcting mistakes made by the parser when visiting training sentences. The algorithm is extremely simple, and its cost in time and memory is independent from the size of the training corpora. In terms of efficiency, though, the parsing algorithm must be run at every training sentence. Our system uses the regular Perceptron working in prima"
W06-2925,C96-1058,0,0.052651,"ext distance between tokens, and the syntactic context dominated by each dependency. In experiments, the treatment of multilingual information was totally blind. 1 2 Parsing and Learning Algorithms This section describes the three main components of the dependency parsing: the parsing model, the parsing algorithm, and the learning algorithm. 2.1 Introduction We describe a learning system for the CoNLL-X Shared Task on multilingual dependency parsing (Buchholz et al., 2006), for 13 different languages. Our system is a bottom-up projective dependency parser, based on the cubic-time algorithm by Eisner (1996; 2000). The parser uses a learning function that scores all possible labeled dependencies. This function is trained globally with online Perceptron, by parsing training sentences and correcting its parameters based on the parsing mistakes. The features used to score, while based on the previous work in dependency parsing (McDonald et al., 2005), introduce some novel concepts such as better codification of context and surface distances, and runtime information from dependencies previously parsed. Regarding experimentation, the treatment of multilingual data has been totally blind, with no spec"
W06-2925,P05-1012,0,0.831221,"rithm. 2.1 Introduction We describe a learning system for the CoNLL-X Shared Task on multilingual dependency parsing (Buchholz et al., 2006), for 13 different languages. Our system is a bottom-up projective dependency parser, based on the cubic-time algorithm by Eisner (1996; 2000). The parser uses a learning function that scores all possible labeled dependencies. This function is trained globally with online Perceptron, by parsing training sentences and correcting its parameters based on the parsing mistakes. The features used to score, while based on the previous work in dependency parsing (McDonald et al., 2005), introduce some novel concepts such as better codification of context and surface distances, and runtime information from dependencies previously parsed. Regarding experimentation, the treatment of multilingual data has been totally blind, with no special processing or features that depend on the language. Considering its simplicity, our system Model Let 1, . . . , L be the dependency labels, defined beforehand. Let x be a sentence of n words, x1 . . . xn . Finally, let Y(x) be the space of well-formed dependency trees for x. A dependency tree y ∈ Y(x) is a set of n dependencies of the form ["
W06-2925,W03-2403,0,\N,Missing
W06-2925,J03-4003,0,\N,Missing
W06-2925,dzeroski-etal-2006-towards,0,\N,Missing
W06-2925,W03-2405,0,\N,Missing
W06-2925,afonso-etal-2002-floresta,0,\N,Missing
W08-2121,D07-1101,0,0.331336,"sults, but the improvement is not large. These initial efforts indicate at least that the joint modeling of this problem is not a trivial task. The D Arch. and D Inference columns summarize the parsing architectures and the corresponding inference strategies. Similar to last year’s shared task (Nivre et al., 2007), the vast majority of parsing models fall in two classes: transition-based (“trans” in the table) or graph-based (“graph”) models. By and large, transition-based models use a greedy inference strategy, whereas graph-based models used different Maximum Spanning Tree (MST) algorithms: Carreras (2007) – MSTC , Eisner (2000) – MSTE , or Chu-Liu/Edmonds (McDonald et al., 2005; Chu and Liu, 1965; Edmonds, 1967) – MSTCL/E . More interestingly, most of the best systems used some strategy to mitigate parsing errors. In the top three systems in the closed challenge, two (che and ciaramita) used parser combination through voting and/or stacking of different models (see the D Comb. column). Samuelsson et al. (2008) perform a MST inference with the bag of all dependencies output by the individual MALT parser variants. Johansson and Nugues (2008) use a single parsing model, but this model is extended"
W08-2121,W08-2134,0,0.108513,"movements, split clauses, and split noun phrases. 6.3 Normalized SRL Performance Table 6.3 lists the scores for the semantic subtask measured as the ratio of the labeled F1 score and LAS. As previously mentioned, this score estimates the performance of the SRL component independent of the performance of the syntactic parser. This analysis is not a substitute for the actual experiment where the SRL components are evaluated using correct syntactic information but, nevertheless, it indicates several interesting facts. First, the ranking of the top three systems in Table 10 changes: the system of Che et al. (2008) is now ranked first, and the system of Johansson and Nugues (2008) is second. This shows that Che et al. have a relatively stronger SRL component, whereas Johansson and Nugues developed a better parser. Second, several other systems improved their ranking compared to Table 10: e.g., chen from position thirteenth to ninth and choi from sixteenth to eighth. This indicates that these systems were penalized in the official ranking mainly due to the relative poor performance of their parsers. Note that this experiment is relevant only for systems that implemented pipeline architectures, where the"
W08-2121,W08-2139,0,0.0101528,"MaltParser with labels enriched with semantic information; Llu´ıs and M`arquez, who used a modified version of the Eisner algorithm to jointly predict syntactic and semantic dependencies; and finally, Sun et al. (2008), who integrated dependency label classification and argument identification using a maximum-entropy Markov model. Additionally, Johansson and Nugues (2008), who had the highest ranked system in the closed challenge, integrate syntactic and semantic analysis in a final reranking step, which maximizes the joint syntactic-semantic score in the top k solutions. In the same spirit, Chen et al. (2008) search in the top k solutions for the one that maximizes a global measure, in this case the joint probability of the complete problem. These joint learning strategies are summarized in the Joint Learning/Opt. column in the table. The system of Riedel and MezaRuiz (2008) deserves a special mention: even though Riedel and Meza-Ruiz did not implement a syntactic parser, they are the only group that performed the complete SRL subtask – i.e., predicate identification and classification, argument identification and classification – jointly, simultaneously for all the predicates in a sentence. They"
W08-2121,M98-1028,0,0.0358658,"and other phenomena, based on a theoretical perspective similar to that of Government and Binding Theory (Chomsky, 1981). 3.1.2 BBN Pronoun Coreference and Entity Type Corpus BBN’s NE annotation of the Wall Street Journal corpus (Weischedel and Brunstein, 2005) takes the form of SGML inline markup of text, tokenized to be completely compatible with the Penn Treebank annotation, e.g., fearlast and cannot are split in the same ways. Named entity categories include: Person, Organization, Location, GPE, Facility, Money, Percent, Time and Date, based on the definitions of these categories in MUC (Chinchor and Robinson, 1998) and ACE7 tasks. Subcategories are included as well. Note however that from this corpus we only use NE boundaries to derive NAME dependencies between NE tokens, e.g., we create a NAME dependency from Mary to Smith given the NE mention Mary Smith. 3.1.3 Proposition Bank I (PropBank) The PropBank annotation (Palmer et al., 2005) classifies the arguments of all the main verbs in the Penn Treebank corpus, other than be. Arguments are numbered (ARG0, ARG1, . . .) based on lexical entries or frame files. Different sets of arguments are assumed for different rolesets. Dependent constituents that fall"
W08-2121,W08-2138,1,0.51476,"Missing"
W08-2121,W06-1670,0,0.0107243,"haracters (“ ”) are used in columns 2–5 to ensure the same number of rows for all columns corresponding to one sentence. All syntactic and semantic dependencies are annotated relative to the split word forms (columns 6–8). Table 2 shows the columns available to the systems participating in the open challenge: namedentity labels as in the CoNLL-2003 Shared Task (Tjong Kim San and De Meulder, 2003) and from the BBN Wall Street Journal Entity Corpus,2 WordNet supersense tags, and the output of an offthe-shelf dependency parser (Nivre et al., 2007b). Columns 1–3 were predicted using the tagger of Ciaramita and Altun (2006). Because the BBN corpus shares lexical content with the Penn Treebank, we generated the BBN tags using a 2-fold cross-validation procedure. 2.2 Evaluation Measures We separate the evaluation measures into two groups: (i) official measures, which were used for the ranking of participating systems, and (ii) additional unofficial measures, which provide further insight into the performance of the participating systems. 2.2.1 Official Evaluation Measures The official evaluation measures consist of three different scores: (i) syntactic dependencies are scored using the labeled attachment score (LA"
W08-2121,W05-0620,1,0.81831,"Missing"
W08-2121,gimenez-marquez-2004-svmtool,1,0.278194,"Missing"
W08-2121,C04-1186,0,0.0984296,"formalism, but also extends them significantly: this year’s syntactic dependencies include more information such as named-entity boundaries; the semantic dependencies model roles of both verbal and nominal predicates. In this paper, we define the shared task and describe how the data sets were created. Furthermore, we report and analyze the results and describe the approaches of the participating systems. 1 • SRL is performed and evaluated using a dependency-based representation for both syntactic and semantic dependencies. While SRL on top of a dependency treebank has been addressed before (Hacioglu, 2004), our approach has several novelties: (i) our constituent-to-dependency conversion strategy transforms all annotated semantic arguments in PropBank and NomBank not just a subset; (ii) we address propositions centered around both verbal (PropBank) and nominal (NomBank) predicates. Introduction In 2004 and 2005 the shared tasks of the Conference on Computational Natural Language Learning (CoNLL) were dedicated to semantic role labeling (SRL), in a monolingual setting (English). In 2006 and 2007 the shared tasks were devoted to the parsing of syntactic dependencies, using corpora from up to 13 la"
W08-2121,W08-2122,0,0.366479,"Missing"
W08-2121,W08-2136,0,0.0382094,"Missing"
W08-2121,W08-2123,1,0.339084,"-TMP AM-MNR AM-LOC A3 AM-MOD AM-ADV AM-DIS R-A0 AM-NEG A4 C-A1 R-A1 AM-PNC AM-EXT AM-CAU AM-DIR R-AM-TMP R-A2 R-AM-LOC R-AM-MNR A5 AM-PRD C-A0 C-A2 R-AM-CAU C-A3 R-A3 C-AM-MNR C-AM-ADV AM-REC AA R-AM-PNC C-AM-EXT C-AM-TMP C-A4 Frequency &lt; 10 Frequency 161409 109437 51197 25913 13080 11409 10269 9986 9496 5369 4432 4097 3281 3118 2565 2445 1428 1346 1318 797 307 246 155 91 78 70 65 50 37 29 24 20 16 14 12 11 11 11 70 watanabe). Remarkably, the top-scoring system (johansson) is in a class of its own, with scores 2–3 points higher than the next system. This is most likely caused by the fact that Johansson and Nugues (2008) implemented a thorough system that addressed all facets of the task with state-ofthe-art methods: second-order parsing model, argument identification/classification models separately tuned for PropBank and NomBank, reranking inference for the SRL task, and, finally, joint optimization of the complete task using metalearning (more details in Section 5). Table 11 lists the official results in the open challenge. The results in this challenge are lower than in the closed challenge, but this was somewhat to be expected considering that there were fewer participants in this challenge and none of t"
W08-2121,W08-2135,0,0.0101844,"sks: Henderson et al. (2008), who implemented a generative history-based model (Incremental Sigmoid Belief Networks with vectors of latent variables) where syntactic and semantic structures are separately 170 generated but using a synchronized derivation (sequence of actions); Samuelsson et al. (2008), who, within an ensemble-based architecture, implemented a joint syntactic-semantic model using MaltParser with labels enriched with semantic information; Llu´ıs and M`arquez, who used a modified version of the Eisner algorithm to jointly predict syntactic and semantic dependencies; and finally, Sun et al. (2008), who integrated dependency label classification and argument identification using a maximum-entropy Markov model. Additionally, Johansson and Nugues (2008), who had the highest ranked system in the closed challenge, integrate syntactic and semantic analysis in a final reranking step, which maximizes the joint syntactic-semantic score in the top k solutions. In the same spirit, Chen et al. (2008) search in the top k solutions for the one that maximizes a global measure, in this case the joint probability of the complete problem. These joint learning strategies are summarized in the Joint Learn"
W08-2121,W07-2416,1,0.71052,"oNLL) were dedicated to semantic role labeling (SRL), in a monolingual setting (English). In 2006 and 2007 the shared tasks were devoted to the parsing of syntactic dependencies, using corpora from up to 13 languages. The CoNLL-2008 shared task1 proposes a unified dependency-based c 2008. ° Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 1 http://www.yr-bcn.es/conll2008 159 • Based on the observation that a richer set of syntactic dependencies improves semantic processing (Johansson and Nugues, 2007), the syntactic dependencies modeled are more complex than the ones used in the previous CoNLL shared tasks. For example, we now include apposition links, dependencies derived from named entity (NE) structures, and better modeling of long-distance grammatical relations. • A practical framework is provided for the joint learning of syntactic and semantic dependencies. CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 159–177 Manchester, August 2008 Given the complexity of this shared task, we limited the evaluation to a monolingual, Englishonly set"
W08-2121,W03-0419,0,0.0172595,"Missing"
W08-2121,W08-2124,1,0.554724,"Missing"
W08-2121,W08-2140,0,0.0210024,"a single parsing model, but this model is extended with second-order features. The PA Arch. and PA Inference columns summarize the architectures and inference strategies used for the identification and classification of predicates and arguments. The columns indicate that most systems modeled the SRL problem as a token-by-token classification problem (“class” in the table) with a corresponding greedy inference strategy. Some systems (e.g., yuret, samuelsson, henderson, lluis) incorporate SRL within parsing, in which case we report the corresponding parsing architecture and inference approach. Vickrey and Koller (2008) simplify the sentences to be labeled using a set of hand-crafted rules before deploying a classification model on top of a constituent-based representation. Unlike in the case of parsing, few systems (yuret, samuelssson, and morante) combine several PA models and the combination is limited to simple voting strategies (see the PA Comb. column). Finally, the ML Methods column lists the Machine Learning (ML) methods used. The column indicates that maximum entropy (ME) was the most popular method (12 distinct systems relied on it). Support Vector Machines (SVM) (eight systems) and the Perceptron"
W08-2121,J93-2004,0,0.0476215,"of the constituent-to-dependency conversion process. The section concludes with an overview of the shared task corpora. 3.1 Input Corpora Input to our merging procedures includes the Penn Treebank, BBN’s named entity corpus, PropBank and NomBank. In this section, we will provide brief descriptions of these annotations in terms of both form and content. All annotations are currently being distributed by the Linguistic Data Consortium, with the exception of NomBank, which is freely downloadable.6 6 http://nlp.cs.nyu.edu/meyers/NomBank. html 162 3.1.1 Penn Treebank 3 The Penn Treebank 3 corpus (Marcus et al., 1993) consists of hand-coded parses of the Wall Street Journal (test, development and training) and a small subset of the Brown corpus (W. N. Francis and H. Kuˆcera, 1964) (test only). These hand parses are notated in-line and sometimes involve changing the strings of the input data. For example, in file wsj 0309, the token fearlast in the text corresponds to the two tokens fear and last in the annotated data. In a similar way, cannot is regularly split to can and not. It is significant that the other annotations assume the tokenization of the Penn Treebank, as this makes it easier for us to merge"
W08-2121,W03-3023,0,0.13388,"s(T ) return create-dependency-tree(T ) 3.2 Conversion to Dependencies 3.2.1 Syntactic Dependencies There exists no large-scale dependency treebank for English, and we thus had to construct a dependency-annotated corpus automatically from the Penn Treebank (Marcus et al., 1993). Since dependency syntax represents grammatical structure by means of labeled binary head–dependent relations rather than phrases, the task of the conversion procedure is to identify and label the head–dependent pairs. The idea underpinning constituent-to-dependency conversion algorithms (Magerman, 1994; Collins, 1999; Yamada and Matsumoto, 2003) is that head–dependent pairs are created from constituents by selecting one word in each phrase as the head and setting all other as its dependents. The dependency labels are then inferred from the phrase–subphrase or phrase–word relations. Our conversion procedure (Johansson and Nugues, 2007) differs from this basic approach by exploiting the rich structure of the constituent format used in Penn Treebank 3: procedure import-glarf(T ) Import a GLARF surface dependency graph G for each multi-word name N in G for each token d in N Set the function tag of d to NAME for each dependency link h →L"
W08-2121,H05-1066,0,0.305206,"ate at least that the joint modeling of this problem is not a trivial task. The D Arch. and D Inference columns summarize the parsing architectures and the corresponding inference strategies. Similar to last year’s shared task (Nivre et al., 2007), the vast majority of parsing models fall in two classes: transition-based (“trans” in the table) or graph-based (“graph”) models. By and large, transition-based models use a greedy inference strategy, whereas graph-based models used different Maximum Spanning Tree (MST) algorithms: Carreras (2007) – MSTC , Eisner (2000) – MSTE , or Chu-Liu/Edmonds (McDonald et al., 2005; Chu and Liu, 1965; Edmonds, 1967) – MSTCL/E . More interestingly, most of the best systems used some strategy to mitigate parsing errors. In the top three systems in the closed challenge, two (che and ciaramita) used parser combination through voting and/or stacking of different models (see the D Comb. column). Samuelsson et al. (2008) perform a MST inference with the bag of all dependencies output by the individual MALT parser variants. Johansson and Nugues (2008) use a single parsing model, but this model is extended with second-order features. The PA Arch. and PA Inference columns summari"
W08-2121,W08-2126,0,0.0266466,"mBank, reranking inference for the SRL task, and, finally, joint optimization of the complete task using metalearning (more details in Section 5). Table 11 lists the official results in the open challenge. The results in this challenge are lower than in the closed challenge, but this was somewhat to be expected considering that there were fewer participants in this challenge and none of the top five groups in the closed challenge submitted results in the open challenge. Only one of the systems that participated in both challenges (zhang) improved the results submitted in the closed challenge. Zhang et al. (2008) achieved this by extracting features for their semantic subtask models both from the parser used in the closed challenge and a secondary parser that was trained on a different corpus. The improvements measured were relatively small for the in-domain WSJ corpus (0.2 labeled macro F1 points) but larger for the out-of-domain Brown corpus (approximately 1 labeled macro F1 point). Table 9: Statistics for semantic roles. 4 Submissions and Results Nineteen groups submitted test runs in the closed challenge and five groups participated in the open challenge. Three of the latter groups participated on"
W08-2121,W01-1511,0,0.0379316,"Missing"
W08-2121,W04-2705,1,0.723818,"Missing"
W08-2121,W06-2933,1,0.512001,"Missing"
W08-2121,J05-1004,0,0.584805,"compatible with the Penn Treebank annotation, e.g., fearlast and cannot are split in the same ways. Named entity categories include: Person, Organization, Location, GPE, Facility, Money, Percent, Time and Date, based on the definitions of these categories in MUC (Chinchor and Robinson, 1998) and ACE7 tasks. Subcategories are included as well. Note however that from this corpus we only use NE boundaries to derive NAME dependencies between NE tokens, e.g., we create a NAME dependency from Mary to Smith given the NE mention Mary Smith. 3.1.3 Proposition Bank I (PropBank) The PropBank annotation (Palmer et al., 2005) classifies the arguments of all the main verbs in the Penn Treebank corpus, other than be. Arguments are numbered (ARG0, ARG1, . . .) based on lexical entries or frame files. Different sets of arguments are assumed for different rolesets. Dependent constituents that fall into categories independent of the lexical entries are classified as various types 7 http://projects.ldc.upenn.edu/ace/ of ARGM (TMP, ADV, etc.).8 Rather than using PropBank directly, we used the version created for the CoNLL-2005 shared task (Carreras and M`arquez, 2005). PropBank’s pointers to subtrees are converted into th"
W08-2121,W08-2125,0,0.116607,"g paper in the proceedings. Results are sorted in descending order of the labeled F1 score for semantic dependencies on the WSJ+Brown corpus. The number in parentheses next to the WSJ+Brown scores indicates the system rank in the corresponding task. 5 Approaches Table 5 summarizes the properties of the systems that participated in the closed the open challenges. The second column of the table highlights the overall architectures. We used + to indicate that the components are sequentially connected. The lack of a + sign indicates that the corresponding tasks are performed jointly. For example, Riedel and Meza-Ruiz (2008) perform predicate and argument identification and classification jointly, whereas Ciaramita et al. (2008) implemented a pipeline architecture of three components. We use the ||to indicate that several different architectures that span multiple subtasks were deployed in parallel. This summary of system architectures indicates that it is common that systems combine several components in the semantic or syntactic subtasks – e.g., nine systems jointly performed predicate/argument identification and classification – but only four systems combined components between the syntactic and semantic subta"
W08-2121,J03-4003,0,\N,Missing
W08-2121,D07-1096,1,\N,Missing
W08-2121,W04-2412,1,\N,Missing
W08-2138,W06-2922,1,0.69496,"ddress each of these subtasks with separate components without backward feedback between sub-tasks. However, the use of multiple parsers at the beginning of the process, and re-ranking at the end, contribute beneficial stochastic aspects to the system. Figure 1 summarizes the system architecture. We detail the parsing All authors contributed equally to this work. c 2008. ° Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. ∗ DeSRlef t−to−right This model is a version of DeSR (Attardi, 2006), a deterministic classifier-based Shif t/Reduce parser. The parser processes input tokens advancing on the input from left to right with Shif t actions and accumulates processed tokens on a stack with Reduce actions. The parser has been adapted for this year’s shared task and extended with additional classifiers, e.g., Multi Layer Perceptron and multiple SVMs.2 The parser uses the following features: System description ∗ Parsing 1. SPLIT LEMMA: from tokens −1, 0, 1, prev(0), lef tChild(0), rightChild(0) 2. PPOSS: from −2, −1, 0, 1, 2, 3, prev(0), next(−1), lef tChild(−1), lef tChild(0), right"
W08-2138,N07-1049,1,0.890278,"Missing"
W08-2138,W05-1505,0,0.107878,"Missing"
W08-2138,N06-2033,0,0.0610556,"Missing"
W08-2138,P05-1073,0,0.0657299,"or split lemmas. 4. Length of the dependency path. 5. Distance in tokens between p and a. 6. Position of a relative to p: before or after. We implemented two inference strategies: greedy and reranking. The greedy strategy sorts all arguments in a frame Fin in descending order of their scores and iteratively adds each argument to the output frame Fout only if it respects the domain constraints with the other arguments already selected. The only domain constraint we use is that core arguments cannot repeat. 3.3 Reranking of Argument Frames The reranking inference strategy adapts the approach of Toutanova et al. (2005) to the dependency representation with notable changes in candidate selection, feature set, and learning model. For candidate selection we modify Algorithm 1: instead of storing only yˆ for each argument in Fin we store the top k best labels. Then, from the arguments in Fin , we generate the top k frames with the highest score, where the score of a frame is the product of all its argument probabilities, computed as the sof tmax function on the output of the Perceptron. In this set of candidate frames we mark the frame with the highest F1 score as the positive example and all others as negative"
W08-2138,W08-2121,1,\N,Missing
W08-2138,S07-1095,1,\N,Missing
W09-1201,burchardt-etal-2006-salsa,1,0.483589,"Missing"
W09-1201,D07-1101,0,0.391748,"Missing"
W09-1201,W09-1202,0,0.0278745,"order syntactic parsing and a particular setting for Catalan 16 and Spanish. (Gesmundo et al., 2009) use an incremental parsing model with synchronous syntactic and semantic derivations and a joint probability model for syntactic and semantic dependency structures. The system uses a single input queue but two separate stacks and synchronizes syntactic and semantic derivations at every word. The synchronous derivations are modeled with an Incremental Sigmoid Belief Network that has latent variables for both syntactic and semantic states and connections from syntax to semantics and vice versa. (Dai et al., 2009) designed an iterative system to exploit the inter-connections between the different subtasks of the CoNLL shared task. The idea is to decompose the joint learning problem into four subtasks – syntactic dependency identification, syntactic dependency labeling, semantic dependency identification and semantic dependency labeling. The initial step is to use a pipeline approach to use the input of one subtask as input to the next, in the order specified. The iterative steps then use additional features that are not available in the initial step to improve the accuracy of the overall system. For ex"
W09-1201,W09-1205,0,0.222475,"al token; and (c), the existence of an edge between each pair of tokens. Subsequently, they combine the (possibly conflicting) output of the three classifiers by a ranking approach to determine the most likely structure that meets all well-formedness constraints. (Llu´ıs et al., 2009) present a joint approach based on an extension of Eisner’s parser to accommodate also semantic dependency labels. This architecture is similar to the one presented by the same authors in the past edition, with the extension to a second-order syntactic parsing and a particular setting for Catalan 16 and Spanish. (Gesmundo et al., 2009) use an incremental parsing model with synchronous syntactic and semantic derivations and a joint probability model for syntactic and semantic dependency structures. The system uses a single input queue but two separate stacks and synchronizes syntactic and semantic derivations at every word. The synchronous derivations are modeled with an Incremental Sigmoid Belief Network that has latent variables for both syntactic and semantic states and connections from syntax to semantics and vice versa. (Dai et al., 2009) designed an iterative system to exploit the inter-connections between the differen"
W09-1201,W09-1212,1,0.83271,"Missing"
W09-1201,S07-1008,1,0.697359,"Missing"
W09-1201,H05-1066,1,0.168004,"Missing"
W09-1201,W04-2705,1,0.527791,"Missing"
W09-1201,W09-1219,0,0.0294123,"Missing"
W09-1201,H05-1108,1,0.506508,"y, adding further manual labels where necessary. Then, we used frequency and grammatical realization information to map the remaining roles onto higher-numbered Arg roles. We considerably simplified the annotations provided by SALSA, which use a rather complex annotation scheme. In particular, we removed annotation for multi-word expressions (which may be non-contiguous), annotations involving multiple frames for the same predicate (metaphors, underspecification), and inter-sentence roles. The out-of-domain dataset was taken from a study on the multi-lingual projection of FrameNet annotation (Pado and Lapata, 2005). It is sampled from the EUROPARL corpus and was chosen to maximize the lexical coverage, i.e., it contains of a large number of infrequent predicates. Both syntactic and semantic structure were annotated manually, in the TIGER and SALSA format, respectively. Since it uses a simplified annotation schemes, we did not have to discard any annotation. For both datasets, we converted the syntactic TIGER (Brants et al., 2002) representations into dependencies with a similar set of head-finding rules used for the preparation of the CoNLL-X shared task German dataset. Minor modifications (for the con1"
W09-1201,C08-1085,1,0.175934,"Missing"
W09-1201,J05-1004,0,0.213522,"nnotation of the Wall Street Journal corpus (Weischedel and Brunstein, 2005) takes the form of SGML inline markup of text, tokenized to be completely compatible with the Penn Treebank annotation. For the CoNLL-2008 shared task evaluation, this corpus was extended by the task organizers to cover the subset of the Brown corpus used as a secondary testing dataset. From this corpus we only used NE boundaries to derive NAME dependencies between NE tokens, e.g., we create a NAME dependency from Mary to Smith given the NE mention Mary Smith. • Proposition Bank I (PropBank) – The PropBank annotation (Palmer et al., 2005) classifies the arguments of all the main verbs in the Penn Treebank corpus, other than be. Arguments are numbered (Arg0, Arg1, . . .) based on lexical entries or frame files. Different sets of arguments are assumed for different rolesets. Dependent constituents that fall into categories independent of the lexical entries are classified as various types of adjuncts (ArgM-TMP, -ADV, etc.). • NomBank – NomBank annotation (Meyers et al., 2004) uses essentially the same framework as PropBank to annotate arguments of nouns. Differences between PropBank and NomBank stem from differences between noun"
W09-1201,E09-1087,1,0.646818,"Missing"
W09-1201,W08-2121,1,0.597132,"Missing"
W09-1201,taule-etal-2008-ancora,0,0.543017,"Missing"
W09-1201,cmejrek-etal-2004-prague,1,0.63993,"Missing"
W09-1201,kawahara-etal-2002-construction,1,\N,Missing
W09-1201,J93-2004,0,\N,Missing
W09-1201,D07-1096,1,\N,Missing
W09-2207,W99-0613,0,0.0746193,"ainst a fraction of the unlabelled corpus for time efficiency. There remains an issue with whether multisets (counting each repeated instance several times) or normal sets (counting them only once) should be used for the instance sets Ip . Our experiments indicate that the best results are obtained by employing multisets for the frequency-based score and normal sets for the precision score. Given the two partial scores above, we have tried three different strategies for combining them: • Multiplicative combination: λ1 log(1 + freq sc(p)) + λ2 log(2 + prec sc(p)) • The strategy suggested in (Collins and Singer, 1999): Patterns are first filtered by imposing a threshold on their precision score. Only for those patterns that pass this first filter, their final score is considered to be their frequency-based score. • The  strategy suggested in (Riloff, 1996): prec sc(p) · log(freq sc(p)) 0 4.1.1 if prec sc(p) ≥ thr otherwise Analysis of subsumptions Intertwined with the selection step, an analysis of subsumptions is performed among the selected patterns, and the patterns found to be subsumed by others in the set are discarded. This is repeated until either a maximum of m patterns with no subsumptions 53 amo"
W09-2207,W01-1313,0,0.0674939,"Missing"
W09-2207,P08-1119,0,0.0166356,"s of the NAACL HLT Workshop on Semi-supervised Learning for Natural Language Processing, pages 49–57, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics Bootstrapping techniques have been used for such diverse NLP problems as: word sense disambiguation (Yarowsky, 1995), named entity classification (Collins and Singer, 1999), IE pattern acquisition (Riloff, 1996; Yangarber et al., 2000; Yangarber, 2003; Stevenson and Greenwood, 2005), document classification (Surdeanu et al., 2006), fact extraction from the web (Pas¸ca et al., 2006) and hyponymy relation extraction (Kozareva et al., 2008). (Yarowsky, 1995) used bootstrapping to train decision list classifiers to disambiguate between two senses of a word, achieving impressive classification accuracy. (Collins and Singer, 1999) applied bootstrapping to extract rules for named entity (NE) classification, seeding the sytem with a few handcrafted rules. Their main innovation was to split training in two alternate stages: during one step, only contextual rules are sought; during the second step, the new contextual rules are used to tag further NEs and these are used to produce new spelling rules. Bootstrapping approaches are employe"
W09-2207,P98-2127,0,0.170046,"s with a given POS. 4. Lemma PEs: Match tokens with a given lemma. 5. Syntactic chunk PEs: Match a sequence of tokens that is a syntactic chunk of a given type (e.g. NP) and whose headword has the same lemma as indicated. 6. Generalized syntactic PEs: Same as the previous, but the lemma of the headword may be any in a given semantic similarity class. The semantic similarity class of a word is defined as the word itself plus a group of other semantically similar words. For computing these, we employ Lin’s corpus of pairwise distributional similarities among words (nouns, verbs and adjectives) (Lin, 1998), filtered to include only those words whose similarity value is above both an absolute (highest n) and relative (to the highest similarity value in the class) threshold. Even after filtering, Lin’s similarities can be “noisy”, since the corpus has been constructed relying on purely statistical means. Therefore, we are employing in addition a set of manually defined semantic classes (hardcoded lists) sensitive to our domain of temporal expressions, such that these lists “override” the Lin’s similarity corpus whenever the semantic class of a word present 52 adjectives (e.g. three-hour, @@-minut"
W09-2207,P00-1010,0,0.367137,"Missing"
W09-2207,P06-1102,0,0.0364807,"Missing"
W09-2207,P05-1047,0,0.0425551,"Missing"
W09-2207,W06-2207,1,0.891383,"Missing"
W09-2207,C00-2136,0,0.108001,"Missing"
W09-2207,P03-1044,0,0.0430123,"Missing"
W09-2207,P95-1026,0,0.102274,"Missing"
W09-2207,C98-2122,0,\N,Missing
W11-0902,de-marneffe-etal-2006-generating,1,0.182751,"Missing"
W11-0902,P05-1045,1,0.109487,"system. 3 Description of the Generic IE System We illustrate our proposed ideas using a simple IE system that implements a pipeline architecture: entity mention extraction followed by relation mention extraction. Note however that the domain customization discussion in Section 5 is independent of the system architecture or classifiers used for EMD and RMD, and we expect the proposed ideas to apply to other IE approaches as well. We performed all pre-processing (tokenization, part-of-speech (POS) tagging) with the Stanford CoreNLP toolkit.2 For EMD we used the Stanford named entity recognizer (Finkel et al., 2005). In all our experiments we used a generic set of features (“macro”) and the IO notation3 for entity mention labels (e.g., the labels for the tokens “over the Seattle Seahawks on Sunday” (from Figure 1) are encoded as “O O NFLT EAM NFLT EAM O DATE”). 2 http://nlp.stanford.edu/software/ corenlp.shtml 3 The IO notation facilitates faster inference than the IOB or IOB2 notations with minimal impact on performance, when there are fewer adjacent mentions with the same type. Argument Features Syntactic Features Surface Features – Head words of the two arguments and their combination – Entity mention"
W11-0902,D10-1033,0,0.0934157,"Missing"
W11-0902,J02-3001,0,0.206465,"es faster inference than the IOB or IOB2 notations with minimal impact on performance, when there are fewer adjacent mentions with the same type. Argument Features Syntactic Features Surface Features – Head words of the two arguments and their combination – Entity mention labels of the two arguments and their combination – Sequence of dependency labels in the dependency path linking the heads of the two arguments – Lemmas of all words in the dependency path – Syntactic path in the constituent parse tree between the largest constituents headed by the same words as the two arguments (similar to Gildea and Jurafsky (2002)) – Concatenation of POS tags between arguments – Binary indicators set to true if there is an entity mention with a given type between the two arguments Words 110 70,119 Entity Mentions 2,188 Relation Mentions 1,629 Table 2: Summary statistics of the NFL corpus, after our conversion to binary relations. Table 1: Feature set used for RMD. The RMD model was built from scratch as a multi-class classifier that extracts binary relations between entity mentions in the same sentence. During training, known relation mentions become positive examples for the corresponding label and all other possible"
W11-0902,D09-1120,0,0.0229873,"is heuristic is flawed: since most parsers are heavily context dependent, they are likely to not parse correctly arbitrarily short text fragments. For example, the Stanford parser generates the incorrect parse tree: Improving Head Identification for Entity Mentions Table 1 indicates that most RMD features (e.g., lexical information on arguments, dependency paths between arguments) depend on the syntactic heads of entity mentions. This observation applies to other natural language processing (NLP) tasks as well, e.g., semantic role labeling or coreference resolution (Gildea and Jurafsky, 2002; Haghighi and Klein, 2009). It is thus crucial that syntactic heads of mentions be correctly identified. Originally we employed a common heuristic: we first try to find a constituent with the exact same span as the given entity mention in the parse tree of the entire sentence, and extract its head. If no such constituent exists, we parse only the text corresponding to the mention and return the head of the generated tree (Haghighi 7 The syntactic head is “5” for the mention “a 5-yard scoring pass” instead of “pass.”10 This problem is exacerbated out of domain, where the parse tree of the entire sentence is likely to be"
W11-0902,W09-1401,0,0.0604544,"Missing"
W11-0902,P03-1054,1,0.0153456,"e positive examples for the corresponding label and all other possible combinations between entity mentions in the same sentence become negative examples. We used a multiclass logistic regression classifier with L2 regularization. Our feature set is taken from (Yao et al., 2010; Mintz et al., 2009; Roth and Yih, 2007; Surdeanu and Ciaramita, 2007) and models the relation arguments, the surface distance between the relation arguments, and the syntactic path between the two arguments, using both constituency and dependency representations. For syntactic information, we used the Stanford parser (Klein and Manning, 2003) and the Stanford dependency representation (de Marneffe et al., 2006). For RMD, we implemented an additive feature selection algorithm similar to the one in (Surdeanu et al., 2008), which iteratively adds the feature with the highest improvement in F1 score to the current feature set, until no improvement is seen. The algorithm was configured to select features that yielded the best combined performance on the dataset from Roth and Yih (2007) and the training partition of ACE 2007.4 We used ten-fold cross val4 Documents idation on both datasets. We decided to use a standard F1 score to evalua"
W11-0902,E99-1001,0,0.0585842,"(and potentially tune model parameters). In this paper we argue that, even when considerable training data is available, this is not sufficient to maximize performance. We apply several simple ideas that yield a significant performance boost, and can be implemented with minimal effort. In particular: • We show that a combination of a conditional random field model (Lafferty et al., 2001) with a rule-based approach that is recall oriented yields better performance for EMD and for the downstream RMD component. The rulebased approach includes gazetteers, which have been shown to be important by Mikheev et al. (1999), among others. • We improve the unification of the predicted semantic annotations with the syntactic analysis of the corresponding text, i.e., finding the syntactic head of a given semantic constituent. Since many features in an IE system depend on syntactic analysis, this leads to more consistent features and better extraction models. • We add a simple inference engine that generates additional relation mentions based solely on the relation mentions extracted by the RMD model. This engine mitigates some of the limitations of a text-based RMD model, which cannot extract relations not explicit"
W11-0902,N06-2024,0,0.0656665,"Missing"
W11-0902,P09-1113,0,0.1178,"s 1,629 Table 2: Summary statistics of the NFL corpus, after our conversion to binary relations. Table 1: Feature set used for RMD. The RMD model was built from scratch as a multi-class classifier that extracts binary relations between entity mentions in the same sentence. During training, known relation mentions become positive examples for the corresponding label and all other possible combinations between entity mentions in the same sentence become negative examples. We used a multiclass logistic regression classifier with L2 regularization. Our feature set is taken from (Yao et al., 2010; Mintz et al., 2009; Roth and Yih, 2007; Surdeanu and Ciaramita, 2007) and models the relation arguments, the surface distance between the relation arguments, and the syntactic path between the two arguments, using both constituency and dependency representations. For syntactic information, we used the Stanford parser (Klein and Manning, 2003) and the Stanford dependency representation (de Marneffe et al., 2006). For RMD, we implemented an additive feature selection algorithm similar to the one in (Surdeanu et al., 2008), which iteratively adds the feature with the highest improvement in F1 score to the current"
W11-0902,P01-1052,0,0.0542362,"Missing"
W11-0902,N10-1123,0,0.0159955,"ing features built on external knowledge, are more important than the learning model itself. These works are conceptually similar to our paper, but we propose several additional directions to improve robustness, and we investigate their impact in a complete IE system instead of just EMD. Several of our lessons are drawn from the BioCreative challenge1 and the BioNLP shared task (Kim 1 http://biocreative.sourceforge.net/ 3 et al., 2009). These tasks have shown the importance of high quality syntactic annotations and using heuristic fixes to correct systematic errors (Schuman and Bergler, 2006; Poon and Vanderwende, 2010, among others). Systems in the latter task have also shown the importance of high recall in the earlier stages of pipeline system. 3 Description of the Generic IE System We illustrate our proposed ideas using a simple IE system that implements a pipeline architecture: entity mention extraction followed by relation mention extraction. Note however that the domain customization discussion in Section 5 is independent of the system architecture or classifiers used for EMD and RMD, and we expect the proposed ideas to apply to other IE approaches as well. We performed all pre-processing (tokenizati"
W11-0902,W09-1119,0,0.0606409,"ction 2 surveys related work. Section 3 describes the IE system used. We cover the target domain that serves as use case in this paper in Section 4. Section 5 introduces our ideas and evaluates their impact in the target domain. Finally, Section 6 concludes the paper. 2 Related Work Other recent works have analyzed the robustness of information extraction systems. For example, Florian et al. (2010) observed that EMD systems perform badly on noisy inputs, e.g., automatic speech transcripts, and propose system combination (similar to our first proposal) to increase robustness in such scenarios. Ratinov and Roth (2009) also investigate design challenges for named entity recognition, and showed that other design choices, such as the representation of output labels and using features built on external knowledge, are more important than the learning model itself. These works are conceptually similar to our paper, but we propose several additional directions to improve robustness, and we investigate their impact in a complete IE system instead of just EMD. Several of our lessons are drawn from the BioCreative challenge1 and the BioNLP shared task (Kim 1 http://biocreative.sourceforge.net/ 3 et al., 2009). These"
W11-0902,W06-3312,0,0.0253567,"ion of output labels and using features built on external knowledge, are more important than the learning model itself. These works are conceptually similar to our paper, but we propose several additional directions to improve robustness, and we investigate their impact in a complete IE system instead of just EMD. Several of our lessons are drawn from the BioCreative challenge1 and the BioNLP shared task (Kim 1 http://biocreative.sourceforge.net/ 3 et al., 2009). These tasks have shown the importance of high quality syntactic annotations and using heuristic fixes to correct systematic errors (Schuman and Bergler, 2006; Poon and Vanderwende, 2010, among others). Systems in the latter task have also shown the importance of high recall in the earlier stages of pipeline system. 3 Description of the Generic IE System We illustrate our proposed ideas using a simple IE system that implements a pipeline architecture: entity mention extraction followed by relation mention extraction. Note however that the domain customization discussion in Section 5 is independent of the system architecture or classifiers used for EMD and RMD, and we expect the proposed ideas to apply to other IE approaches as well. We performed al"
W11-0902,P08-1082,1,0.718591,"c regression classifier with L2 regularization. Our feature set is taken from (Yao et al., 2010; Mintz et al., 2009; Roth and Yih, 2007; Surdeanu and Ciaramita, 2007) and models the relation arguments, the surface distance between the relation arguments, and the syntactic path between the two arguments, using both constituency and dependency representations. For syntactic information, we used the Stanford parser (Klein and Manning, 2003) and the Stanford dependency representation (de Marneffe et al., 2006). For RMD, we implemented an additive feature selection algorithm similar to the one in (Surdeanu et al., 2008), which iteratively adds the feature with the highest improvement in F1 score to the current feature set, until no improvement is seen. The algorithm was configured to select features that yielded the best combined performance on the dataset from Roth and Yih (2007) and the training partition of ACE 2007.4 We used ten-fold cross val4 Documents idation on both datasets. We decided to use a standard F1 score to evaluate RMD performance rather than the more complex ACE score because we believe that the former is more interpretable. We used gold entity mentions for the feature selection process. T"
W11-0902,D10-1099,0,0.0186421,"8 Relation Mentions 1,629 Table 2: Summary statistics of the NFL corpus, after our conversion to binary relations. Table 1: Feature set used for RMD. The RMD model was built from scratch as a multi-class classifier that extracts binary relations between entity mentions in the same sentence. During training, known relation mentions become positive examples for the corresponding label and all other possible combinations between entity mentions in the same sentence become negative examples. We used a multiclass logistic regression classifier with L2 regularization. Our feature set is taken from (Yao et al., 2010; Mintz et al., 2009; Roth and Yih, 2007; Surdeanu and Ciaramita, 2007) and models the relation arguments, the surface distance between the relation arguments, and the syntactic path between the two arguments, using both constituency and dependency representations. For syntactic information, we used the Stanford parser (Klein and Manning, 2003) and the Stanford dependency representation (de Marneffe et al., 2006). For RMD, we implemented an additive feature selection algorithm similar to the one in (Surdeanu et al., 2008), which iteratively adds the feature with the highest improvement in F1 s"
W11-1806,W09-1402,0,0.297186,"Missing"
W11-1806,E03-1009,0,0.0142277,"(GE) dataset (Kim et al., 2011b) and the new Epigenetics and Post-translational Modifications (EPI) and Infectious Diseases (ID) datasets (Ohta et al., 2011; Pyysalo et al., 2011, respectively). Other changes are relatively minor but documented here as implementation notes. Several improvements were made to anchor detection, improving its accuracy on all three domains. The first is the use of distributional similarity features. Using a large corpus of abstracts from PubMed (30,963,886 word tokens of 335,811 word types), we cluster words by their syntactic contexts and morphological contents (Clark, 2003). We used the Ney-Essen clustering model with morphology to produce 45 clusters. Using these clusters, we extended the feature set for anchor detection from McClosky et al. (2011) as follows: for each lexicalized feature we create an equivalent feature where the corresponding word is replaced by its cluster ID. This yielded consistent improvements of at least 1 percentage point in both anchor detection and event 5 http://github.com/BLLIP/bllip-parser extraction in the development partition of the GE dataset. Additionally, we improved the head percolation rules for selecting the head of each mu"
W11-1806,P07-1033,0,0.025258,"Missing"
W11-1806,N10-1095,0,0.0161104,"our system, receives an n-best list of event structures from each decoder in the event parsing step. The reranker can use any global features of an event structure to rescore it and outputs the highest scoring structure. This is based on parse reranking (Ratnaparkhi, 1999; Collins, 2000) but uses features on event structures instead of syntactic constituency structures. We used Mark Johnson’s cvlm estimator5 (Charniak and Johnson, 2005) when learning weights for the reranking model. Since the reranker can incorporate the outputs from multiple decoders, we use it as an ensemble technique as in Johnson and Ural (2010). 3 Extensions for BioNLP 2011 This section outlines the changes between our BioNLP 2011 shared task submission and the system described in McClosky et al. (2011). The main differences are that all dataset-specific portions of the model have been factored out to handle the expanded Genia (GE) dataset (Kim et al., 2011b) and the new Epigenetics and Post-translational Modifications (EPI) and Infectious Diseases (ID) datasets (Ohta et al., 2011; Pyysalo et al., 2011, respectively). Other changes are relatively minor but documented here as implementation notes. Several improvements were made to an"
W11-1806,W11-1801,0,0.576114,"l event anchors, (2) event parsing to form candidate event structures by linking entities and event anchors, and (3) event reranking to select the best candidate event structure. As the full details on our approach are described in McClosky et al. (2011), we will only provide an outline of our methods here along with additional implementation notes. Before running our system, we perform basic preprocessing on the corpora. Sentences need to be segmented, tokenized, and parsed syntactically. We use custom versions of these (except for Infectious Diseases where we use those from Stenetorp et al. (2011)). To ease event parsing, our 2 http://sourceforge.net/projects/mstparser/ 41 Proceedings of BioNLP Shared Task 2011 Workshop, pages 41–45, c Portland, Oregon, USA, 24 June, 2011. 2011 Association for Computational Linguistics tokenizations are designed to split off suffixes which are often event anchors. For example, we split the token RelA-induced into the two tokens RelA and induced3 since RelA is a protein and induced an event anchor. If this was a single token, our event parser would be unable to link them since it cannot predict self-loops in the dependency graph. For syntactic parsing,"
W11-1806,W11-1802,0,0.202989,"instead of syntactic constituency structures. We used Mark Johnson’s cvlm estimator5 (Charniak and Johnson, 2005) when learning weights for the reranking model. Since the reranker can incorporate the outputs from multiple decoders, we use it as an ensemble technique as in Johnson and Ural (2010). 3 Extensions for BioNLP 2011 This section outlines the changes between our BioNLP 2011 shared task submission and the system described in McClosky et al. (2011). The main differences are that all dataset-specific portions of the model have been factored out to handle the expanded Genia (GE) dataset (Kim et al., 2011b) and the new Epigenetics and Post-translational Modifications (EPI) and Infectious Diseases (ID) datasets (Ohta et al., 2011; Pyysalo et al., 2011, respectively). Other changes are relatively minor but documented here as implementation notes. Several improvements were made to anchor detection, improving its accuracy on all three domains. The first is the use of distributional similarity features. Using a large corpus of abstracts from PubMed (30,963,886 word tokens of 335,811 word types), we cluster words by their syntactic contexts and morphological contents (Clark, 2003). We used the Ney-E"
W11-1806,P11-1163,1,0.92818,"is paper, we show that with minimal domain-specific tuning, we are able to achieve competitive performance across the three event extraction domains in the BioNLP 2011 shared task. At the heart of our system1 is an off-the-shelf 1 nlp.stanford.edu/software/eventparser.shtml Event Parsing Our system includes three components: (1) anchor detection to identify and label event anchors, (2) event parsing to form candidate event structures by linking entities and event anchors, and (3) event reranking to select the best candidate event structure. As the full details on our approach are described in McClosky et al. (2011), we will only provide an outline of our methods here along with additional implementation notes. Before running our system, we perform basic preprocessing on the corpora. Sentences need to be segmented, tokenized, and parsed syntactically. We use custom versions of these (except for Infectious Diseases where we use those from Stenetorp et al. (2011)). To ease event parsing, our 2 http://sourceforge.net/projects/mstparser/ 41 Proceedings of BioNLP Shared Task 2011 Workshop, pages 41–45, c Portland, Oregon, USA, 24 June, 2011. 2011 Association for Computational Linguistics tokenizations are des"
W11-1806,N10-1004,1,0.20041,"/projects/mstparser/ 41 Proceedings of BioNLP Shared Task 2011 Workshop, pages 41–45, c Portland, Oregon, USA, 24 June, 2011. 2011 Association for Computational Linguistics tokenizations are designed to split off suffixes which are often event anchors. For example, we split the token RelA-induced into the two tokens RelA and induced3 since RelA is a protein and induced an event anchor. If this was a single token, our event parser would be unable to link them since it cannot predict self-loops in the dependency graph. For syntactic parsing, we use the self-trained biomedical parsing model from McClosky (2010) with the Charniak and Johnson (2005) reranking parser. We use its actual constituency tree, the dependency graph created by applying head percolation rules, and the Stanford Dependencies (de Marneffe and Manning, 2008) extracted from the tree (collapsed and uncollapsed). Anchor detection uses techniques inspired from named entity recognition to label each token with an event type or none. The features for this stage are primarily drawn from Bj¨orne et al. (2009). We reduce multiword event anchors to their syntactic head.4 We classify each token independently using a logistic regression classi"
W11-1806,E06-1011,0,0.0351838,"Missing"
W11-1806,H05-1066,0,0.0429686,"Missing"
W11-1806,W11-1803,0,0.57546,"l event anchors, (2) event parsing to form candidate event structures by linking entities and event anchors, and (3) event reranking to select the best candidate event structure. As the full details on our approach are described in McClosky et al. (2011), we will only provide an outline of our methods here along with additional implementation notes. Before running our system, we perform basic preprocessing on the corpora. Sentences need to be segmented, tokenized, and parsed syntactically. We use custom versions of these (except for Infectious Diseases where we use those from Stenetorp et al. (2011)). To ease event parsing, our 2 http://sourceforge.net/projects/mstparser/ 41 Proceedings of BioNLP Shared Task 2011 Workshop, pages 41–45, c Portland, Oregon, USA, 24 June, 2011. 2011 Association for Computational Linguistics tokenizations are designed to split off suffixes which are often event anchors. For example, we split the token RelA-induced into the two tokens RelA and induced3 since RelA is a protein and induced an event anchor. If this was a single token, our event parser would be unable to link them since it cannot predict self-loops in the dependency graph. For syntactic parsing,"
W11-1806,W11-1804,0,0.559547,"l event anchors, (2) event parsing to form candidate event structures by linking entities and event anchors, and (3) event reranking to select the best candidate event structure. As the full details on our approach are described in McClosky et al. (2011), we will only provide an outline of our methods here along with additional implementation notes. Before running our system, we perform basic preprocessing on the corpora. Sentences need to be segmented, tokenized, and parsed syntactically. We use custom versions of these (except for Infectious Diseases where we use those from Stenetorp et al. (2011)). To ease event parsing, our 2 http://sourceforge.net/projects/mstparser/ 41 Proceedings of BioNLP Shared Task 2011 Workshop, pages 41–45, c Portland, Oregon, USA, 24 June, 2011. 2011 Association for Computational Linguistics tokenizations are designed to split off suffixes which are often event anchors. For example, we split the token RelA-induced into the two tokens RelA and induced3 since RelA is a protein and induced an event anchor. If this was a single token, our event parser would be unable to link them since it cannot predict self-loops in the dependency graph. For syntactic parsing,"
W11-1806,W11-1808,1,0.884176,"l event anchors, (2) event parsing to form candidate event structures by linking entities and event anchors, and (3) event reranking to select the best candidate event structure. As the full details on our approach are described in McClosky et al. (2011), we will only provide an outline of our methods here along with additional implementation notes. Before running our system, we perform basic preprocessing on the corpora. Sentences need to be segmented, tokenized, and parsed syntactically. We use custom versions of these (except for Infectious Diseases where we use those from Stenetorp et al. (2011)). To ease event parsing, our 2 http://sourceforge.net/projects/mstparser/ 41 Proceedings of BioNLP Shared Task 2011 Workshop, pages 41–45, c Portland, Oregon, USA, 24 June, 2011. 2011 Association for Computational Linguistics tokenizations are designed to split off suffixes which are often event anchors. For example, we split the token RelA-induced into the two tokens RelA and induced3 since RelA is a protein and induced an event anchor. If this was a single token, our event parser would be unable to link them since it cannot predict self-loops in the dependency graph. For syntactic parsing,"
W11-1806,W11-1816,0,0.330404,"Missing"
W11-1806,W08-1301,1,\N,Missing
W11-1806,P05-1022,0,\N,Missing
W11-1808,W09-1401,0,0.534253,"Missing"
W11-1808,W11-1801,0,0.421255,"ion of type t0 that we want to assign, and type tS that S predicts. Inference in this model amounts to maximizing s (e, a, b) over Y. Our approach to solving this problem is dual decomposition (Komodakis et al., 2007; Rush et al., 2010). We divide the problem into three subproblems: (1) finding the best trigger label and set of outgoing edges for each candidate trigger; (2) finding the best trigger label and set of incoming edges for each candidate trigger; (3) finding the best pairs of entities to appear in the same binding. Due to space limitations we refer the reader to Riedel and McCallum (2011) for further details. 2.2 Stacked Model For the stacked model, we use a system based on an event parsing framework (McClosky et al., 2011a) referred to as the Stanford model in this paper. This model converts event structures to dependency trees which are parsed using MSTParser (McDonald et al., 2005).1 Once parsed, the resulting dependency tree is converted back to event structures. Using the Stanford model as the stacked model is helpful since it captures tree structure which is not the focus in the UMass model. Of course, this is also a limitation since actual BioNLP event graphs are DAGs,"
W11-1808,W11-1802,0,0.36214,"ctions on test and development sets we used models learned from the the complete training set. Predictions over training data were produced using crossvalidation. This helps to avoid a scenario where the stacking model learns to rely on high accuracy at training time that cannot be matched at test time. Note that, unlike Stanford’s individual submission in this shared task, the stacked models in this paper do not include the Stanford reranker. This is because it would have required making a reranker model for each crossvalidation fold. We made 19 crossvalidation training folds for Genia (GE) (Kim et al., 2011b), 12 for Epigenetics (EPI), and 17 for Infectious Diseases (ID) (Kim et al., 2011b; Ohta et al., 2011; Pyysalo et al., 2011, respectively). Note that while ID is the smallest and would seem like it would have the fewest folds, we combined the training data of ID with the training and development data from GE. To produce predictions over the test data, we combined the training folds with 6 development folds for GE, 4 for EPI, and 1 for ID. 3 Experiments Table 1 gives an overview of our results on the test sets for all four tasks we submitted to. Note that for the EPI and ID tasks we show the"
W11-1808,P11-1163,1,0.535908,"b) over Y. Our approach to solving this problem is dual decomposition (Komodakis et al., 2007; Rush et al., 2010). We divide the problem into three subproblems: (1) finding the best trigger label and set of outgoing edges for each candidate trigger; (2) finding the best trigger label and set of incoming edges for each candidate trigger; (3) finding the best pairs of entities to appear in the same binding. Due to space limitations we refer the reader to Riedel and McCallum (2011) for further details. 2.2 Stacked Model For the stacked model, we use a system based on an event parsing framework (McClosky et al., 2011a) referred to as the Stanford model in this paper. This model converts event structures to dependency trees which are parsed using MSTParser (McDonald et al., 2005).1 Once parsed, the resulting dependency tree is converted back to event structures. Using the Stanford model as the stacked model is helpful since it captures tree structure which is not the focus in the UMass model. Of course, this is also a limitation since actual BioNLP event graphs are DAGs, but the model does well considering these restrictions. Additionally, this constraint encourages the Stanford model to provide different"
W11-1808,W11-1806,1,0.588093,"ion of type t0 that we want to assign, and type tS that S predicts. Inference in this model amounts to maximizing s (e, a, b) over Y. Our approach to solving this problem is dual decomposition (Komodakis et al., 2007; Rush et al., 2010). We divide the problem into three subproblems: (1) finding the best trigger label and set of outgoing edges for each candidate trigger; (2) finding the best trigger label and set of incoming edges for each candidate trigger; (3) finding the best pairs of entities to appear in the same binding. Due to space limitations we refer the reader to Riedel and McCallum (2011) for further details. 2.2 Stacked Model For the stacked model, we use a system based on an event parsing framework (McClosky et al., 2011a) referred to as the Stanford model in this paper. This model converts event structures to dependency trees which are parsed using MSTParser (McDonald et al., 2005).1 Once parsed, the resulting dependency tree is converted back to event structures. Using the Stanford model as the stacked model is helpful since it captures tree structure which is not the focus in the UMass model. Of course, this is also a limitation since actual BioNLP event graphs are DAGs,"
W11-1808,H05-1066,0,0.0337346,") finding the best trigger label and set of outgoing edges for each candidate trigger; (2) finding the best trigger label and set of incoming edges for each candidate trigger; (3) finding the best pairs of entities to appear in the same binding. Due to space limitations we refer the reader to Riedel and McCallum (2011) for further details. 2.2 Stacked Model For the stacked model, we use a system based on an event parsing framework (McClosky et al., 2011a) referred to as the Stanford model in this paper. This model converts event structures to dependency trees which are parsed using MSTParser (McDonald et al., 2005).1 Once parsed, the resulting dependency tree is converted back to event structures. Using the Stanford model as the stacked model is helpful since it captures tree structure which is not the focus in the UMass model. Of course, this is also a limitation since actual BioNLP event graphs are DAGs, but the model does well considering these restrictions. Additionally, this constraint encourages the Stanford model to provide different (and thus more useful for stacking) results. Of particular interest to this paper are the four possible decoders in MSTParser. These four decoders come from combinat"
W11-1808,P08-1108,0,0.0519048,"Missing"
W11-1808,W11-1803,0,0.278435,"ion of type t0 that we want to assign, and type tS that S predicts. Inference in this model amounts to maximizing s (e, a, b) over Y. Our approach to solving this problem is dual decomposition (Komodakis et al., 2007; Rush et al., 2010). We divide the problem into three subproblems: (1) finding the best trigger label and set of outgoing edges for each candidate trigger; (2) finding the best trigger label and set of incoming edges for each candidate trigger; (3) finding the best pairs of entities to appear in the same binding. Due to space limitations we refer the reader to Riedel and McCallum (2011) for further details. 2.2 Stacked Model For the stacked model, we use a system based on an event parsing framework (McClosky et al., 2011a) referred to as the Stanford model in this paper. This model converts event structures to dependency trees which are parsed using MSTParser (McDonald et al., 2005).1 Once parsed, the resulting dependency tree is converted back to event structures. Using the Stanford model as the stacked model is helpful since it captures tree structure which is not the focus in the UMass model. Of course, this is also a limitation since actual BioNLP event graphs are DAGs,"
W11-1808,W11-1804,0,0.36653,"ion of type t0 that we want to assign, and type tS that S predicts. Inference in this model amounts to maximizing s (e, a, b) over Y. Our approach to solving this problem is dual decomposition (Komodakis et al., 2007; Rush et al., 2010). We divide the problem into three subproblems: (1) finding the best trigger label and set of outgoing edges for each candidate trigger; (2) finding the best trigger label and set of incoming edges for each candidate trigger; (3) finding the best pairs of entities to appear in the same binding. Due to space limitations we refer the reader to Riedel and McCallum (2011) for further details. 2.2 Stacked Model For the stacked model, we use a system based on an event parsing framework (McClosky et al., 2011a) referred to as the Stanford model in this paper. This model converts event structures to dependency trees which are parsed using MSTParser (McDonald et al., 2005).1 Once parsed, the resulting dependency tree is converted back to event structures. Using the Stanford model as the stacked model is helpful since it captures tree structure which is not the focus in the UMass model. Of course, this is also a limitation since actual BioNLP event graphs are DAGs,"
W11-1808,W11-1807,1,0.818921,"edictions from the Stanford system into the UMass system (e.g., as in Nivre and McDonald (2008)). This has the advantage that one model (Umass) determines how to integrate the outputs of the other model (Stanford) into its own structure, whereas in reranking, for example, the combined model is required to output a complete structure produced by only one of the input models. 2 Approach In the following we briefly present both the stacking and the stacked model and some possible ways of integrating the stacked information. 2.1 Stacking Model As our stacking model, we employ the UMass extractor (Riedel and McCallum, 2011). It is based on a discriminatively trained model that jointly predicts trigger labels, event arguments and protein pairs in 51 Proceedings of BioNLP Shared Task 2011 Workshop, pages 51–55, c Portland, Oregon, USA, 24 June, 2011. 2011 Association for Computational Linguistics binding. We will briefly describe this model but first introduce three types of binary variables that will represent events in a given sentence. Variables ei,t are active if and only if the token at position i has the label t. Variables ai,j,r are active if and only if there is an event with trigger i that has an argument"
W11-1808,D10-1001,0,0.0101169,"(p, q) to include predictions from the systems to be stacked. For example, for every system S to be stacked and every pair of event types (t0 , tS ) we add the features ( 1 hS (i) = tS ∧ t0 = t fS,t0 ,tS (i, t) = 0 otherwise 52 to fT (i, t). Here hS (i) is the event label given to token i according to S. These features allow different weights to be given to each possible combination of type t0 that we want to assign, and type tS that S predicts. Inference in this model amounts to maximizing s (e, a, b) over Y. Our approach to solving this problem is dual decomposition (Komodakis et al., 2007; Rush et al., 2010). We divide the problem into three subproblems: (1) finding the best trigger label and set of outgoing edges for each candidate trigger; (2) finding the best trigger label and set of incoming edges for each candidate trigger; (3) finding the best pairs of entities to appear in the same binding. Due to space limitations we refer the reader to Riedel and McCallum (2011) for further details. 2.2 Stacked Model For the stacked model, we use a system based on an event parsing framework (McClosky et al., 2011a) referred to as the Stanford model in this paper. This model converts event structures to d"
W11-1808,N10-1091,1,0.0608368,"Missing"
W11-1902,W97-1306,0,0.302132,"rmance despite the simplicity of the individual components. This strategy has been successfully used before for information extraction, e.g., in the BioNLP 2009 event extraction shared task (Kim et al., 2009), several of the top systems had a first high-recall component to identify event anchors, followed by high-precision classifiers, which identified event arguments and removed unlikely event candidates (Bj¨orne et al., 2009). In the coreference resolution space, several works have shown that applying a list of rules from highest to lowest precision is beneficial for coreference resolution (Baldwin, 1997; Raghunathan el al., 2010). However, we believe we are the first to show that this high-recall/high-precision strategy yields competitive results for the complete task of coreference resolution, i.e., including mention detection and both nominal and pronominal coreference. 2.1 Mention Detection Sieve In our particular setup, the recall of the mention detection component is more important than its precision, because any missed mentions are guaranteed to affect the final score, but spurious mentions may not impact the overall score if they are left as singletons, which are discarded by our post"
W11-1902,D08-1031,0,0.585246,"ld-out corpus (see Table 1 for the complete list of sieves deployed in our system). Since these two sieves use 3 We initialize the clusters as singletons and grow them progressively in each sieve. 30 Bare plurals - bare plurals are generic and cannot have a coreferent antecedent. 2.3.2 Semantic-Similarity Sieves We first extend the above system with two new sieves that exploit semantics from WordNet, Wikipedia infoboxes, and Freebase records, drawing on previous coreference work using these databases (Ng & Cardie, 2002; Daum´e & Marcu, 2005; Ponzetto & Strube, 2006; Ng, 2007; Yang & Su, 2007; Bengston & Roth, 2008; Huang et al., 2009; inter alia). Since the input to a sieve is a collection of mention clusters built by the previous (more precise) sieves, we need to link mention clusters (rather than individual mentions) to records in these three knowledge bases. The following steps generate a query for these resources from a mention cluster. First, we select the most representative mention in a cluster by preferring mentions headed by proper nouns to mentions headed by common nouns, and nominal mentions to pronominal ones. In case of ties, we select the longer string. For example, the mention selected f"
W11-1902,W09-1402,0,0.0144679,"Missing"
W11-1902,H05-1013,0,0.0128588,"Missing"
W11-1902,N10-1061,0,0.158402,"68.3 68.9 70.0 70.8 R 43.4 43.3 46.3 46.3 CEAFE P 47.8 46.8 50.5 49.6 F1 45.5 45.0 48.3 47.9 R 70.6 71.9 72.0 73.4 BLANC P 76.2 76.6 78.6 79.0 F1 73.0 74.0 74.8 75.8 avg F1 57.8 58.3 60.7 61.4 Table 4: Results on the official test set. as well, whereas in development (lines 6 and 7 in Table 3), gold mentions included only mentions part of an actual coreference chain. This explains the large difference between, say, line 6 in Table 3 and line 4 in Table 4. Our scores are comparable to previously reported state-of-the-art results for coreference resolution with predicted mentions. For example, Haghighi and Klein (2010) compare four state-of-the-art systems on three different corpora and report B3 scores between 63 and 77 points. While the corpora used in (Haghighi and Klein, 2010) are different from the one in this shared task, our result of 68 B3 suggests that our system’s performance is competitive. In this task, our submissions in both the open and the closed track obtained the highest scores. 4 Conclusion In this work we showed how a competitive end-toend coreference resolution system can be built using only deterministic models (or sieves). Our approach starts with a high-recall mention detection compo"
W11-1902,N06-2015,0,0.132313,"i Surdeanu, Dan Jurafsky Stanford NLP Group Stanford University, Stanford, CA 94305 {heeyoung,peirsman,angelx,natec,mihais,jurafsky}@stanford.edu Abstract standing, e.g., linking speakers to compatible pronouns. Second, we incorporated a mention detection sieve at the beginning of the processing flow. This sieve filters our syntactic constituents unlikely to be mentions using a simple set of rules on top of the syntactic analysis of text. And lastly, we added a post-processing step, which guarantees that the output of our system is compatible with the shared task and OntoNotes specifications (Hovy et al., 2006; Pradhan et al., 2007). This paper details the coreference resolution system submitted by Stanford at the CoNLL2011 shared task. Our system is a collection of deterministic coreference resolution models that incorporate lexical, syntactic, semantic, and discourse information. All these models use global document-level information by sharing mention attributes, such as gender and number, across mentions in the same cluster. We participated in both the open and closed tracks and submitted results using both predicted and gold mentions. Our system was ranked first in both tracks, with a score of"
W11-1902,D09-1128,0,0.0144125,"e 1 for the complete list of sieves deployed in our system). Since these two sieves use 3 We initialize the clusters as singletons and grow them progressively in each sieve. 30 Bare plurals - bare plurals are generic and cannot have a coreferent antecedent. 2.3.2 Semantic-Similarity Sieves We first extend the above system with two new sieves that exploit semantics from WordNet, Wikipedia infoboxes, and Freebase records, drawing on previous coreference work using these databases (Ng & Cardie, 2002; Daum´e & Marcu, 2005; Ponzetto & Strube, 2006; Ng, 2007; Yang & Su, 2007; Bengston & Roth, 2008; Huang et al., 2009; inter alia). Since the input to a sieve is a collection of mention clusters built by the previous (more precise) sieves, we need to link mention clusters (rather than individual mentions) to records in these three knowledge bases. The following steps generate a query for these resources from a mention cluster. First, we select the most representative mention in a cluster by preferring mentions headed by proper nouns to mentions headed by common nouns, and nominal mentions to pronominal ones. In case of ties, we select the longer string. For example, the mention selected from the cluster {Pre"
W11-1902,Y09-1024,0,0.0223833,"for the gold mention the working meeting of the 32 ”863 Program”. Due to this boundary mismatch, all mentions found to be coreferent with this predicted mention are counted as precision errors, and all mentions in the same coreference cluster with the gold mention are counted as recall errors. Table 3 lists the results of our end-to-end system on the development partition. “External Resources”, which were used only in the open track, includes: (a) a hand-built list of genders of first names that we created, incorporating frequent names from census lists and other sources, (b) an animacy list (Ji and Lin, 2009), (c) a country and state gazetteer, and (d) a demonym list. “Discourse” stands for the sieve introduced in Section 2.3.3. “Semantics” stands for the sieves presented in Section 2.3.2. The table shows that the discourse sieve yields an improvement of almost 2 points to the overall score (row 1 versus 3), and external resources contribute 0.5 points. On the other hand, the semantic sieves do not help (row 3 versus 4). The latter result contradicts our initial experiments, where we measured a minor improvement when these sieves were enabled and gold mentions were used. Our hypothesis is that, wh"
W11-1902,P07-1068,0,0.0061436,"on their precision in a held-out corpus (see Table 1 for the complete list of sieves deployed in our system). Since these two sieves use 3 We initialize the clusters as singletons and grow them progressively in each sieve. 30 Bare plurals - bare plurals are generic and cannot have a coreferent antecedent. 2.3.2 Semantic-Similarity Sieves We first extend the above system with two new sieves that exploit semantics from WordNet, Wikipedia infoboxes, and Freebase records, drawing on previous coreference work using these databases (Ng & Cardie, 2002; Daum´e & Marcu, 2005; Ponzetto & Strube, 2006; Ng, 2007; Yang & Su, 2007; Bengston & Roth, 2008; Huang et al., 2009; inter alia). Since the input to a sieve is a collection of mention clusters built by the previous (more precise) sieves, we need to link mention clusters (rather than individual mentions) to records in these three knowledge bases. The following steps generate a query for these resources from a mention cluster. First, we select the most representative mention in a cluster by preferring mentions headed by proper nouns to mentions headed by common nouns, and nominal mentions to pronominal ones. In case of ties, we select the longer str"
W11-1902,P02-1014,0,0.574048,"wo new sieves that address nominal mentions and are inserted based on their precision in a held-out corpus (see Table 1 for the complete list of sieves deployed in our system). Since these two sieves use 3 We initialize the clusters as singletons and grow them progressively in each sieve. 30 Bare plurals - bare plurals are generic and cannot have a coreferent antecedent. 2.3.2 Semantic-Similarity Sieves We first extend the above system with two new sieves that exploit semantics from WordNet, Wikipedia infoboxes, and Freebase records, drawing on previous coreference work using these databases (Ng & Cardie, 2002; Daum´e & Marcu, 2005; Ponzetto & Strube, 2006; Ng, 2007; Yang & Su, 2007; Bengston & Roth, 2008; Huang et al., 2009; inter alia). Since the input to a sieve is a collection of mention clusters built by the previous (more precise) sieves, we need to link mention clusters (rather than individual mentions) to records in these three knowledge bases. The following steps generate a query for these resources from a mention cluster. First, we select the most representative mention in a cluster by preferring mentions headed by proper nouns to mentions headed by common nouns, and nominal mentions to p"
W11-1902,W11-1901,0,0.492284,"c coreference resolution models that incorporate lexical, syntactic, semantic, and discourse information. All these models use global document-level information by sharing mention attributes, such as gender and number, across mentions in the same cluster. We participated in both the open and closed tracks and submitted results using both predicted and gold mentions. Our system was ranked first in both tracks, with a score of 57.8 in the closed track and 58.3 in the open track. 1 Introduction This paper describes the coreference resolution system used by Stanford at the CoNLL-2011 shared task (Pradhan et al., 2011). Our system extends the multi-pass sieve system of Raghunathan et al. (2010), which applies tiers of deterministic coreference models one at a time from highest to lowest precision. Each tier builds on the entity clusters constructed by previous models in the sieve, guaranteeing that stronger features are given precedence over weaker ones. Furthermore, this model propagates global information by sharing attributes (e.g., gender and number) across mentions in the same cluster. We made three considerable extensions to the Raghunathan et al. (2010) model. First, we added five additional sieves,"
W11-1902,D10-1048,1,0.816894,"ntic, and discourse information. All these models use global document-level information by sharing mention attributes, such as gender and number, across mentions in the same cluster. We participated in both the open and closed tracks and submitted results using both predicted and gold mentions. Our system was ranked first in both tracks, with a score of 57.8 in the closed track and 58.3 in the open track. 1 Introduction This paper describes the coreference resolution system used by Stanford at the CoNLL-2011 shared task (Pradhan et al., 2011). Our system extends the multi-pass sieve system of Raghunathan et al. (2010), which applies tiers of deterministic coreference models one at a time from highest to lowest precision. Each tier builds on the entity clusters constructed by previous models in the sieve, guaranteeing that stronger features are given precedence over weaker ones. Furthermore, this model propagates global information by sharing attributes (e.g., gender and number) across mentions in the same cluster. We made three considerable extensions to the Raghunathan et al. (2010) model. First, we added five additional sieves, the majority of which address the semantic similarity between mentions, e.g.,"
W11-1902,P07-1067,0,0.0116669,"precision in a held-out corpus (see Table 1 for the complete list of sieves deployed in our system). Since these two sieves use 3 We initialize the clusters as singletons and grow them progressively in each sieve. 30 Bare plurals - bare plurals are generic and cannot have a coreferent antecedent. 2.3.2 Semantic-Similarity Sieves We first extend the above system with two new sieves that exploit semantics from WordNet, Wikipedia infoboxes, and Freebase records, drawing on previous coreference work using these databases (Ng & Cardie, 2002; Daum´e & Marcu, 2005; Ponzetto & Strube, 2006; Ng, 2007; Yang & Su, 2007; Bengston & Roth, 2008; Huang et al., 2009; inter alia). Since the input to a sieve is a collection of mention clusters built by the previous (more precise) sieves, we need to link mention clusters (rather than individual mentions) to records in these three knowledge bases. The following steps generate a query for these resources from a mention cluster. First, we select the most representative mention in a cluster by preferring mentions headed by proper nouns to mentions headed by common nouns, and nominal mentions to pronominal ones. In case of ties, we select the longer string. For example,"
W11-1902,W09-1401,0,\N,Missing
W11-1902,N06-1025,0,\N,Missing
W16-2907,N16-1180,0,0.0330694,"luate our approach on the BioNLP 2009 event extraction task. Our results show that there is a small performance penalty when converting the statistical model to rules, but the gain in interpretability compensates for that: with minimal effort, human experts improve this model to have similar performance to the statistical model that served as starting point. 1 Introduction Due to the deluge of unstructured data, information extraction (IE) systems, which aim to translate this data to structured information, have become ubiquitous. For example, applications of IE range from parsing literature (Iyyer et al., 2016) to converting thousands of cancer research publications into complex proteins signaling pathways (Cohen, 2015). By and large, in academia most of these approaches are implemented using machine learning (ML). This choice is warranted: generally, ML approaches, where the machine learns directly from (1) We introduce a simple strategy that converts statistical models for IE to rule-based models. We call the proposed algorithm SnapToGrid. Our approach works in three steps. First, we train a statistical model for the task at hand. Here we experiment with logistic regression, but the proposed metho"
W16-2907,W09-1402,0,0.0968273,"Missing"
W16-2907,P11-4019,0,0.0323516,". Do not use word constraints. Only use lemma and tag fea- This modification prefers lexical constraints on lemmas, tures in trigger rules for simple events (other than transcription because they generalize better than constraints on actual and binding). words. Remove redundant constraints. For example, in patterns like [incoming=nsubj & tag=/ˆN/] the POS tag is redundant because it is implicitly defined through the incoming dependency (nominal subject). Table 5: Representative examples of the rule changes suggested by linguistic experts. 1993; Cunningham et al., 2002; Piskorski et al., 2004; Li et al., 2011; Chang and Manning, 2014) are preferable when the corresponding systems have to be deployed for long periods of time, during which they have to be maintained and improved. This has been recognized in industry (Chiticariu et al., 2013). the behavior of its statistical counterpart, with a small penalty of 1-2 F1 points throughout. As discussed before, this performance loss can be mitigated through interventions by domain experts. 4 Related Work Most of the biomedical IE systems in academia rely on supervised machine learning. This includes the top performing system at the BioNLP 2009 shared tas"
W16-2907,P14-5010,1,0.0109908,"rt the statistical model into an interpretable, rule-based model (Section 2.2): (a) First, we convert the features to rules in the Odin language. (b) Then, we assign to each rules “votes” for a given class, by “snapping to grid”, i.e., converting to discrete values, the weights computed by the above statistical model. Syntactic features: These features capture the syntactic dependencies (both incoming and outgoing) directly connected to the token. All syntactic information was represented using Stanford dependencies (De Marneffe and Manning, 2008), and was generated using the CoreNLP toolkit (Manning et al., 2014). For each of these paths, we generate two different versions: one containing just the label and direction of the syntactic dependencies, and another including also the destination words. 3. Domain experts edit the produced rule-based model directly, aiming to improve its quality with respect to both coverage and precision (Section 2.3). 57 Input CD2 signaling induces phosphoryla4on of CREB in primary lymphocytes. Protein Protein Classiﬁer #1: detects event triggers Phosphoryla4on Output +Reg Classiﬁer #2: detects rela4ons between triggers and en44es or other events Theme Cause Theme Figure 1:"
W16-2907,D13-1079,0,0.223589,"ints on actual and binding). words. Remove redundant constraints. For example, in patterns like [incoming=nsubj & tag=/ˆN/] the POS tag is redundant because it is implicitly defined through the incoming dependency (nominal subject). Table 5: Representative examples of the rule changes suggested by linguistic experts. 1993; Cunningham et al., 2002; Piskorski et al., 2004; Li et al., 2011; Chang and Manning, 2014) are preferable when the corresponding systems have to be deployed for long periods of time, during which they have to be maintained and improved. This has been recognized in industry (Chiticariu et al., 2013). the behavior of its statistical counterpart, with a small penalty of 1-2 F1 points throughout. As discussed before, this performance loss can be mitigated through interventions by domain experts. 4 Related Work Most of the biomedical IE systems in academia rely on supervised machine learning. This includes the top performing system at the BioNLP 2009 shared task (Bj¨orne et al., 2009), as well as several following approaches that improve upon its performance (Miwa et al., 2010; McClosky et al., 2012; Miwa et al., 2012; Bui and Sloot, 2012; Venugopal et al., 2014). However, rule-based approac"
W16-2907,J00-1002,0,0.0342175,"ention. 40 F1 score 30 20 10 0 200 400 600 800 Number of abstracts Figure 4: Learning curve showing the change in F1 performance as a function of the amount of training data. We compare the performance of the L1 -regularized logistic regression (shown using circles) with the rule-based model prior to the expert intervention (shown using triangles). Another planned extension of this work focuses on reducing the number of generated rules by merging/collapsing similar paths into a single pattern. This can be achieved by constructing a minimal deterministic acyclic finite-state automaton (DAFSA) (Daciuk et al., 2000) with the paths that are similar, and then converting the DAFSA into a single pattern (Neumann, 2005). For example, such approaches would collapse the two patterns: dobj and dobj nn, into a single one: dobj nn?. This is fundamental for the long-term maintainability of the rule-based model, because the human experts would have to maintain considerably fewer rules. presents the “human in the loop” with new examples to annotate (Thompson et al., 1999). Although active learning may require less domain expertise than our proposal, it generally does not guarantee that the examples provided are actua"
W16-2907,D12-1042,1,0.777005,"e to their potential extensions to distant supervision (DS), where training data is generated automatically by aligning a knowledge base (KB) of known examples (e.g., known drug-gene interactions) with text (e.g., scientific publications). Distant supervision has obvious applications to bioinformatics (Craven et al., 1999), but it generally suffers from noise in the automaticallygenerated annotations (Riedel et al., 2010). In future work, we plan to combine our work with distant supervision by adapting our proposal to logistic regression variants that are robust to the noise introduced in DS (Surdeanu et al., 2012). This extension would make it possible to generate rules even when no annotated examples are available, as long as a suitable KB of known examples exists. Table 6: Performance of the rule-based model, after expert intervention. 40 F1 score 30 20 10 0 200 400 600 800 Number of abstracts Figure 4: Learning curve showing the change in F1 performance as a function of the amount of training data. We compare the performance of the L1 -regularized logistic regression (shown using circles) with the rule-based model prior to the expert intervention (shown using triangles). Another planned extension of"
W16-2907,L16-1050,1,0.760332,"stical model. Further, our strategy can operate over multiple classifiers that are part of the same IE system (e.g., one classifier to identify event triggers, and another to identify event arguments). Second, we convert features 56 Proceedings of the 15th Workshop on Biomedical Natural Language Processing, pages 56–65, c Berlin, Germany, August 12, 2016. 2016 Association for Computational Linguistics We detail this process in the rest of this section, focusing on the BioNLP core event extraction task as the domain of interest. to rules implemented in Odin, a modern declarative rule language (Valenzuela-Escarcega et al., 2016; Valenzuela-Escarcega et al., 2015). We also discard most of the statistical information acquired previously, by converting feature weights to discrete votes, which guarantees interpretability (hence the SnapToGrid name). Third, human domain experts inspect and manually improve the generated model, under certain time constraints. 2.1 Our statistical model is inspired by the top performing approach at the 2009 evaluation (Bj¨orne et al., 2009). The approach is summarized in Figure 1. Similar to (Bj¨orne et al., 2009), our approach consists of two classifiers: the first classifier detects and l"
W16-2907,D14-1090,0,0.148424,"ated 1,190,029 features with non-zero weights. The table shows that this model 3 (Kim et al., 2009) report results on the official test partition, which are not directly comparable with our results. However, in the authors’ experience, the difference in scores between the development and test partitions in this dataset tend to be small. Since the 2009 evaluation, several works have improved upon these results, with performance reaching 58 F1 points, but using more complex methods, including joint inference, coreference resolution, and domain adaptation (Miwa et al., 2012; Bui and Sloot, 2012; Venugopal et al., 2014). 2 The online scoring website, which would have allowed us to also obtain scores on the official test partition, was down due to updates during the development of this work. 60 Event Class Gene expression Transcription Protein catabolism Phosphorylation Localization Binding Event Total Regulation Positive regulation Negative regulation Regulation Total All Total Recall 58.71 37.80 61.90 46.81 56.60 16.13 42.75 8.88 13.13 8.16 11.41 25.54 Precision 78.28 55.36 86.67 84.62 88.24 33.33 66.60 65.22 40.50 55.17 44.44 59.35 Event Class Gene expression Transcription Protein catabolism Phosphorylatio"
W16-2920,S15-2136,0,0.0719434,"Missing"
W16-2920,S16-1165,0,0.0497139,"Missing"
W16-2920,Q14-1022,0,0.24282,"contributions in aid of automatically extracting causal ordering in biomedical text. First, we provide and describe a dataset of real text examples, manually annotated for causal precedence. Second, we analyze the efficacy of a battery of different models in automatically determining precedence, built on top of the Reach automatic reading system (Valenzuela-Esc´arcega et al., 2015a; Valenzuela-Esc´arcega et al., 2015c) and measured against this novel corpus. In particular, we investigate three classes of models: (a) deterministic rule-based models inspired by the precedence sieves proposed by Chambers et al. (2014), (b) feature-based models, and (c) models that rely on latent representations such as long short-term memory (LSTM) networks (Hochreiter and Schmidhuber, 1997). Our analysis indicates that while independently the top-performing model achieves a micro F1 of 43, these models are largely complementary with a combined recall of 58 points. Lastly, we conduct an error analysis of these models to motivate and inform future research. 2 Relation Example E1 precedes E2 A is phosphorylated by B. Following its phosphorylation, A binds with C. E2 precedes E1 A is phosphorylated by B. Prior to its phosphor"
W16-2920,W13-0107,0,0.0287921,"the annotation corpus, using general linguistic expertise and domain knowledge. (3) [A is phosphorylated by B]before . As a downstream effect, [C is . . . ]after (4) [A is phosphorylated by B]before . [C is then . . . ]after Other phrases captured include: “Later”, “In response”, “For this”, and “Ultimately”. Verbal tense- and aspect-based (Reichenbach) ordering Following Chambers et al. (2014), we use deterministic rules to establish precedence between events that have certain verbal tense and aspect. These rules are derived from linguistic analysis of tense and aspect by (Reichenbach, 1947; Derczynski and Gaizauskas, 2013). Example (5) illustrates a case in which we can accurately infer order just from this information. Because has been phosphorylated has past tense and perfective Intra-sentence ordering Within sentences, syntactic regularities can be exploited to cover a large variety of grammatical constructions indicating precedence relations. Rules defined over dependency parses (De Marneffe and Manning, 2008) capture precedence in sentences like those in (1) and (2) as well as many others. 149 aspect, this model concludes that it precedes share (present tense, simple aspect) and thus the binding of histone"
W16-2920,P15-4022,1,0.890908,"Missing"
W16-2920,W09-1401,0,0.032145,"ng the best performers on the simpler temporalonly precedence tasks. Feature-based and latent representation models each outperform the rule-based models, but their performance is complementary to one another. We apply a sieve-based architecture to capitalize on this lack of overlap, achieving a micro F1 score of 46 points. 1 Introduction In the biomedical domain, an enormous amount of information about protein, gene, and drug interactions appears in the form of natural language across millions of academic papers. There is a tremendous ongoing effort (N´edellec et al., 2013; Kim et al., 2012; Kim et al., 2009) to extract individual chemical interactions from these texts, but these interactions are only isolated fragments of larger causal mechanisms such as protein signaling pathways. Nowhere, however, including any A precedes B if and only if the output of A is necessary for the successful execution of B.2 1 2 Personal communication. See the “precedes” examples in Table 1. 146 Proceedings of the 15th Workshop on Biomedical Natural Language Processing, pages 146–155, c Berlin, Germany, August 12, 2016. 2016 Association for Computational Linguistics Very little annotated data exists for causal preced"
W16-2920,W13-2001,0,0.0310235,"Missing"
W18-1701,P17-4018,1,0.783994,"Missing"
W18-5528,P17-1171,0,0.0274418,"o the pro∩ e| ∩ e| portion of words in e: |c |c| and |c |e| . We approach the task of distinguishing between fake and real claims in a series of steps. Specifically, given a claim, we: 1. Information retrieval (IR): We use an IR component to gather claim-relevant texts from a large corpus of evidence (i.e., Wikipedia articles). For retrieving the Wikipedia articles which contain sentences relevant to the given claim, we reused the competition-provided information retrieval system (Thorne et al., 2018) since we are focusing here on the RTE portion of the task. To be specific, we used the DrQA (Chen et al., 2017) for document retrieval. For sentence selection the modified DrQA with binning with comparison to unigram TF-IDF implementation using NLTK (Bird and Loper, 2004) was used. The k and l parameters values, for document retrieval and sentence selection, respectively, was also left as is to be 5. Any claim which had the label of NOT ENOUGH INFO was removed from the Oracle setting. For all these features, we used the lemma form of the words and first removed stop words (see Appendix A for the list of stop words that were removed). In all, there were 5 features in this feature set, two each for noun"
W18-5528,D16-1244,0,0.0883142,"Missing"
W18-5528,N18-1074,0,0.0264782,"Missing"
W18-5601,P07-1033,0,0.0997978,"Missing"
W18-5601,D11-1145,0,0.0780052,"Missing"
W18-5601,W16-1601,0,0.0426478,"Missing"
W18-5601,W17-5221,0,0.0355021,"Missing"
W18-5601,P15-1162,0,0.0969044,"Missing"
W18-5601,K15-1011,0,0.0247191,"Missing"
W18-5601,W16-4320,0,0.0671701,"Missing"
W18-5601,C14-1184,0,0.0615072,"Missing"
W18-5601,N13-1039,0,0.0645994,"Missing"
W18-5601,D13-1187,0,0.0698925,"Missing"
W18-5601,P12-2018,0,0.0967061,"Missing"
W19-1504,U08-1013,0,0.258332,"ken from Gupta and Manning (2015). We use these vectors to compute the cosine similarity score of a given candidate entity e to the entities in entPoolc , and add the average and maximum similarities as features. The top 10 entities classified with the highest confidence P x1,x2∈P where P is the entity/pattern pool for a category, and x1, x2 are entities/patterns in said pool. 20 Lastly, the third term, Repel, encourages that the pools be mutually exclusive, which is a soft version of the counter training approach of Yangarber (2003) or the weighted mutual-exclusive bootstrapping algorithm of McIntosh and Curran (2008). For example, person names should be far from organization names in the semantic embedding space: Repel = X X X Score(e, c) = 1− Y (1 − probc (pc )) {pc ∈patP oolc |matches(pc ,e)} (5) Each entity is then assigned to the category with the highest overall score. 4 > Vx2 )) log(σ(−Vx1 Experiments We evaluate the above algorithms on the task of named entity classification from free text. P 1,P 2 if P 16=P 2 x1∈P 1 x2∈P 2 (4) Datasets: We used two datasets, the CoNLL2003 shared task dataset (Tjong Kim Sang and De Meulder, 2003), which contains 4 entity types, and the OntoNotes dataset (Pradhan et"
W19-1504,D15-1056,0,0.149556,"In this work we use surface patterns, but the proposed algorithm is agnostic to the types of patterns learned. 18 Proceedings of 3rd Workshop on Structured Prediction for NLP, pages 18–28 c Minneapolis, Minnesota, June 7, 2019. 2019 Association for Computational Linguistics and antonymy relations as supervision to fine tune word vector spaces, using an Attract/Repel method similar to our idea. However, most of these works that customize embeddings for a specific task rely on some form of supervision. In contrast, our approach is lightly supervised, with a only few seed examples per category. Batista et al. (2015) perform bootstrapping for relation extraction using pre-trained word embeddings. They do not learn custom pattern embeddings that apply to multi-word entities and patterns. We show that customizing embeddings for the learned patterns is important for interpretability. ing approach is suitable for semi-supervised NEC. We compare our approach against several stateof-the-art semi-supervised approaches on two datasets: CoNLL-2003 (Tjong Kim Sang and De Meulder, 2003) and OntoNotes (Pradhan et al., 2013). We show that, despite its simplicity, our method outperforms all other approaches. (3) Our ap"
W19-1504,Q17-1022,0,0.0282133,"Missing"
W19-1504,D13-1079,0,0.012212,"ies getting a bigger boost. This improvement shows that, for non-ambiguous categories that are well-defined by the local contexts captured by our patterns, these patterns truly are interpretable to end users. 2 Lastly, our work addresses the interpretability aspect of information extraction methods. Interpretable models mitigate the technical debt of machine learning (Sculley et al., 2014). For example, it allows domain experts to make manual, gradual improvements to the models. This is why rulebased approaches are commonly used in industry applications, where software maintenance is crucial (Chiticariu et al., 2013). Furthermore, the need for interpretability also arises in critical systems, e.g., recommending treatment to patients, where these systems are deployed to aid human decision makers (Lakkaraju and Rudin, 2016). The benefits of interpretability have encouraged efforts to either extract interpretable models from opaque ones (Craven and Shavlik, 1996), or to explain their decisions (Ribeiro et al., 2016b). Related Work Bootstrapping is an iterative process that alternates between learning representative patterns, and acquiring new entities (or relations) belonging to a given category (Riloff, 199"
W19-1504,W13-3516,0,0.0392927,". In contrast, our approach is lightly supervised, with a only few seed examples per category. Batista et al. (2015) perform bootstrapping for relation extraction using pre-trained word embeddings. They do not learn custom pattern embeddings that apply to multi-word entities and patterns. We show that customizing embeddings for the learned patterns is important for interpretability. ing approach is suitable for semi-supervised NEC. We compare our approach against several stateof-the-art semi-supervised approaches on two datasets: CoNLL-2003 (Tjong Kim Sang and De Meulder, 2003) and OntoNotes (Pradhan et al., 2013). We show that, despite its simplicity, our method outperforms all other approaches. (3) Our approach also outputs an interpretation of the learned model, consisting of a decision list of patterns, where each pattern gets a score per class based on the proximity of its embedding to the average entity embedding in the given class. This interpretation is global, i.e., it explains the entire model rather than local predictions. We show that this decision-list model performs comparably to the complete model on the two datasets. Recent work has focused on explanations of machine learning models tha"
W19-1504,W99-0613,0,0.705498,".g., predicting words based on their context from large amounts of data, mitigating the brittle statistics affecting traditional bootstrapping approaches. However, the resulting realvalued embedding vectors are hard to interpret. Here we argue that these two directions are complementary, and should be combined. We propose such a bootstrapping approach for information extraction (IE), which blends the advantages of both directions. As a use case, we instantiate our idea for named entity classification (NEC), i.e., classifying a given set of unknown entities into a predefined set of categories (Collins and Singer, 1999). The contributions of this work are: (1) We propose an approach for bootstrapping NEC that iteratively learns custom embeddings for both the multi-word entities to be extracted and the patterns that match them from a few example entities per category. Our approach changes the objective function of a neural network language models (NNLM) to include a semi-supervised component that models the known examples, i.e., by attracting entities and patterns in the same category to each other and repelling them from elements in different categories, and it adds an external iterative process that “cautio"
W19-1504,N16-3020,0,0.265327,"it allows domain experts to make manual, gradual improvements to the models. This is why rulebased approaches are commonly used in industry applications, where software maintenance is crucial (Chiticariu et al., 2013). Furthermore, the need for interpretability also arises in critical systems, e.g., recommending treatment to patients, where these systems are deployed to aid human decision makers (Lakkaraju and Rudin, 2016). The benefits of interpretability have encouraged efforts to either extract interpretable models from opaque ones (Craven and Shavlik, 1996), or to explain their decisions (Ribeiro et al., 2016b). Related Work Bootstrapping is an iterative process that alternates between learning representative patterns, and acquiring new entities (or relations) belonging to a given category (Riloff, 1996; McIntosh, 2010). Patterns and extractions are ranked using either formulas that measure their frequency and association with a category, or classifiers, which increases robustness due to regularization (Carlson et al., 2010; Gupta and Manning, 2015). While semi-supervised learning is not novel (Yarowsky, 1995; Gupta and Manning, 2014), our approach performs better than some modern implementations"
W19-1504,W14-1611,0,0.4939,"by human experts to mitigate some of that loss and in some cases outperform the original model. 1 Introduction One strategy for mitigating the cost of supervised learning in information extraction (IE) is to bootstrap extractors with light supervision from a few provided examples (or seeds). Traditionally, bootstrapping approaches iterate between learning extraction patterns such as word ngrams, e.g., the pattern “@ENTITY , former president” could be used to extract person names,1 and applying these patterns to extract the desired structures (entities, relations, etc.) (Carlson et al., 2010; Gupta and Manning, 2014, 2015, 1 In this work we use surface patterns, but the proposed algorithm is agnostic to the types of patterns learned. 18 Proceedings of 3rd Workshop on Structured Prediction for NLP, pages 18–28 c Minneapolis, Minnesota, June 7, 2019. 2019 Association for Computational Linguistics and antonymy relations as supervision to fine tune word vector spaces, using an Attract/Repel method similar to our idea. However, most of these works that customize embeddings for a specific task rely on some form of supervision. In contrast, our approach is lightly supervised, with a only few seed examples per c"
W19-1504,N15-1128,0,0.668315,"nterpretability have encouraged efforts to either extract interpretable models from opaque ones (Craven and Shavlik, 1996), or to explain their decisions (Ribeiro et al., 2016b). Related Work Bootstrapping is an iterative process that alternates between learning representative patterns, and acquiring new entities (or relations) belonging to a given category (Riloff, 1996; McIntosh, 2010). Patterns and extractions are ranked using either formulas that measure their frequency and association with a category, or classifiers, which increases robustness due to regularization (Carlson et al., 2010; Gupta and Manning, 2015). While semi-supervised learning is not novel (Yarowsky, 1995; Gupta and Manning, 2014), our approach performs better than some modern implementations of these methids such as Gupta and Manning (2014). Distributed representations of words (Deerwester et al., 1990; Mikolov et al., 2013; Levy and Goldberg, 2014) serve as underlying representation for many NLP tasks such as information extraction and question answering (Riedel et al., 2013; Toutanova et al., 2015, 2016; Sharp et al., 2016). Mrkˇsi´c et al. (2017) build on traditional distributional models by incorporating synonymy As machine lear"
W19-1504,N13-1008,0,0.106658,"rvised Representation Learning with Global Interpretability Andrew Zupon, Maria Alexeeva, Marco A. Valenzuela-Esc´arcega, Ajay Nagesh, and Mihai Surdeanu University of Arizona, Tucson, AZ, USA {zupon, alexeeva, marcov, ajaynagesh, msurdeanu} @email.arizona.edu Abstract inter alia). One advantage of this direction is that these patterns are interpretable, which mitigates the maintenance cost associated with machine learning systems (Sculley et al., 2014). On the other hand, representation learning has proven to be useful for natural language processing (NLP) applications (Mikolov et al., 2013; Riedel et al., 2013; Toutanova et al., 2015, 2016, inter alia). Representation learning approaches often include a component that is trained in an unsupervised manner, e.g., predicting words based on their context from large amounts of data, mitigating the brittle statistics affecting traditional bootstrapping approaches. However, the resulting realvalued embedding vectors are hard to interpret. Here we argue that these two directions are complementary, and should be combined. We propose such a bootstrapping approach for information extraction (IE), which blends the advantages of both directions. As a use case,"
W19-1504,P14-2050,0,0.026928,"ies (or relations) belonging to a given category (Riloff, 1996; McIntosh, 2010). Patterns and extractions are ranked using either formulas that measure their frequency and association with a category, or classifiers, which increases robustness due to regularization (Carlson et al., 2010; Gupta and Manning, 2015). While semi-supervised learning is not novel (Yarowsky, 1995; Gupta and Manning, 2014), our approach performs better than some modern implementations of these methids such as Gupta and Manning (2014). Distributed representations of words (Deerwester et al., 1990; Mikolov et al., 2013; Levy and Goldberg, 2014) serve as underlying representation for many NLP tasks such as information extraction and question answering (Riedel et al., 2013; Toutanova et al., 2015, 2016; Sharp et al., 2016). Mrkˇsi´c et al. (2017) build on traditional distributional models by incorporating synonymy As machine learning models are becoming more complex, the focus on interpretability has become more important, with new funding programs focused on this topic.2 Our approach for exporting an interpretable model (§3) is similar to Valenzuela-Esc´arcega et al. (2016), but we start from distributed representations, whereas they"
W19-1504,D10-1035,0,0.0341335,"urthermore, the need for interpretability also arises in critical systems, e.g., recommending treatment to patients, where these systems are deployed to aid human decision makers (Lakkaraju and Rudin, 2016). The benefits of interpretability have encouraged efforts to either extract interpretable models from opaque ones (Craven and Shavlik, 1996), or to explain their decisions (Ribeiro et al., 2016b). Related Work Bootstrapping is an iterative process that alternates between learning representative patterns, and acquiring new entities (or relations) belonging to a given category (Riloff, 1996; McIntosh, 2010). Patterns and extractions are ranked using either formulas that measure their frequency and association with a category, or classifiers, which increases robustness due to regularization (Carlson et al., 2010; Gupta and Manning, 2015). While semi-supervised learning is not novel (Yarowsky, 1995; Gupta and Manning, 2014), our approach performs better than some modern implementations of these methids such as Gupta and Manning (2014). Distributed representations of words (Deerwester et al., 1990; Mikolov et al., 2013; Levy and Goldberg, 2014) serve as underlying representation for many NLP tasks"
W19-1504,D16-1014,1,0.862324,"ith a category, or classifiers, which increases robustness due to regularization (Carlson et al., 2010; Gupta and Manning, 2015). While semi-supervised learning is not novel (Yarowsky, 1995; Gupta and Manning, 2014), our approach performs better than some modern implementations of these methids such as Gupta and Manning (2014). Distributed representations of words (Deerwester et al., 1990; Mikolov et al., 2013; Levy and Goldberg, 2014) serve as underlying representation for many NLP tasks such as information extraction and question answering (Riedel et al., 2013; Toutanova et al., 2015, 2016; Sharp et al., 2016). Mrkˇsi´c et al. (2017) build on traditional distributional models by incorporating synonymy As machine learning models are becoming more complex, the focus on interpretability has become more important, with new funding programs focused on this topic.2 Our approach for exporting an interpretable model (§3) is similar to Valenzuela-Esc´arcega et al. (2016), but we start from distributed representations, whereas they started from a logistic regression model with explicit features. 2 DARPA’s Explainable AI program: http://www.darpa. mil/program/explainable-artificial-intelligence. 19 3 Approach"
W19-1504,D15-1174,0,0.207243,"Learning with Global Interpretability Andrew Zupon, Maria Alexeeva, Marco A. Valenzuela-Esc´arcega, Ajay Nagesh, and Mihai Surdeanu University of Arizona, Tucson, AZ, USA {zupon, alexeeva, marcov, ajaynagesh, msurdeanu} @email.arizona.edu Abstract inter alia). One advantage of this direction is that these patterns are interpretable, which mitigates the maintenance cost associated with machine learning systems (Sculley et al., 2014). On the other hand, representation learning has proven to be useful for natural language processing (NLP) applications (Mikolov et al., 2013; Riedel et al., 2013; Toutanova et al., 2015, 2016, inter alia). Representation learning approaches often include a component that is trained in an unsupervised manner, e.g., predicting words based on their context from large amounts of data, mitigating the brittle statistics affecting traditional bootstrapping approaches. However, the resulting realvalued embedding vectors are hard to interpret. Here we argue that these two directions are complementary, and should be combined. We propose such a bootstrapping approach for information extraction (IE), which blends the advantages of both directions. As a use case, we instantiate our idea"
W19-1504,P16-1136,0,0.0537199,"Missing"
W19-1504,W16-2907,1,0.897343,"Missing"
W19-1504,P03-1044,0,0.0481158,"e use the set of embedding vectors learned in step (1). These features are taken from Gupta and Manning (2015). We use these vectors to compute the cosine similarity score of a given candidate entity e to the entities in entPoolc , and add the average and maximum similarities as features. The top 10 entities classified with the highest confidence P x1,x2∈P where P is the entity/pattern pool for a category, and x1, x2 are entities/patterns in said pool. 20 Lastly, the third term, Repel, encourages that the pools be mutually exclusive, which is a soft version of the counter training approach of Yangarber (2003) or the weighted mutual-exclusive bootstrapping algorithm of McIntosh and Curran (2008). For example, person names should be far from organization names in the semantic embedding space: Repel = X X X Score(e, c) = 1− Y (1 − probc (pc )) {pc ∈patP oolc |matches(pc ,e)} (5) Each entity is then assigned to the category with the highest overall score. 4 > Vx2 )) log(σ(−Vx1 Experiments We evaluate the above algorithms on the task of named entity classification from free text. P 1,P 2 if P 16=P 2 x1∈P 1 x2∈P 2 (4) Datasets: We used two datasets, the CoNLL2003 shared task dataset (Tjong Kim Sang and"
W19-1504,P95-1026,0,0.103447,"models from opaque ones (Craven and Shavlik, 1996), or to explain their decisions (Ribeiro et al., 2016b). Related Work Bootstrapping is an iterative process that alternates between learning representative patterns, and acquiring new entities (or relations) belonging to a given category (Riloff, 1996; McIntosh, 2010). Patterns and extractions are ranked using either formulas that measure their frequency and association with a category, or classifiers, which increases robustness due to regularization (Carlson et al., 2010; Gupta and Manning, 2015). While semi-supervised learning is not novel (Yarowsky, 1995; Gupta and Manning, 2014), our approach performs better than some modern implementations of these methids such as Gupta and Manning (2014). Distributed representations of words (Deerwester et al., 1990; Mikolov et al., 2013; Levy and Goldberg, 2014) serve as underlying representation for many NLP tasks such as information extraction and question answering (Riedel et al., 2013; Toutanova et al., 2015, 2016; Sharp et al., 2016). Mrkˇsi´c et al. (2017) build on traditional distributional models by incorporating synonymy As machine learning models are becoming more complex, the focus on interpret"
W19-1505,W99-0613,0,0.304161,"Missing"
W19-1505,N15-1128,0,0.0413518,"Missing"
W19-1505,P15-1162,0,0.0285472,"Missing"
W19-1505,D15-1206,0,0.0741948,"nd named entity recognition, where the teacher is derived from a manually specified set of rule-templates that regularizes a neural student, thereby allowing one to combine neural and symbolic systems. Our MT system is different in that the teacher is a simple running average of the students across different epochs of training, which removes the need of human supervision through rules. More recently, Nagesh and Surdeanu (2018) applied the MT architecture for the task of semisupervised Named Entity Classification, which is a simpler task compared to our RE task. Recent works (Liu et al., 2015; Xu et al., 2015; Su et al., 2018) use neural networks to learn syntactic features for relation extraction via traversing the shortest dependency path. Following this trend, we adapt such syntax-based neural models to both of our student and teacher classifiers in the MT architecture. 3 Mean Teacher Framework Our approach repurposes the Mean Teacher framework for relation extraction. This section an overview of the general MT framework. The next section discusses the adaptation of this framework for RE. The Mean Teacher (MT) framework, like other teacher-student algorithms, learns from a limited set of labele"
W19-1505,Q16-1037,0,0.0748953,"Missing"
W19-1505,P03-1044,0,0.148246,"Missing"
W19-1505,P95-1026,0,0.660123,"Missing"
W19-1505,P15-2047,0,0.18384,"ty mentions that participate in the given relation. When exposed to unlabeled data, the MT framework learns by maximizing the consistency between a student classifier and a teacher that is an average of 29 Proceedings of 3rd Workshop on Structured Prediction for NLP, pages 29–37 c Minneapolis, Minnesota, June 7, 2019. 2019 Association for Computational Linguistics past students, where both classifiers are exposed to different noise. For RE, we introduce a strategy to generate noise through word dropout (Iyyer et al., 2015). We provide all code and resources needed to reproduce results1 . Both Liu et al. (2015) and Su et al. (2018) use neural networks to encode the words and dependencies along the shortest path between the two entities, and Liu et al. additionally encode the dependency subtrees of the words for additional context. We include this representation (words and dependencies) in our experiments. While the inclusion of the subtrees gives Liu et al. a slight performance boost, here we opt to focus only on the varying representations of the dependency path between the entities, without the additional context. Su et al. (2018) use an LSTM to model the shortest path between the entities, but ke"
W19-1505,P16-1005,0,0.102223,"tion of the two models to be the similar, and it is applied to both labeled and unlabeled inputs (as these distributions can be constrained even if the true label is unknown) to ensure that the learned representations (distributions) are not sensitive to the applied noise. When the model is given a labeled input, it additionally assigns a classification cost using the prediction of the student model. This guides the learned representation to be useful to the task at hand. can often be distilled into the task of identifying and classifying triggers, i.e., words which signal specific relations (Yu and Ji, 2016). For example, the verb died is highly indicative of the placeOfDeath relation. There are several ways of finding a candidate trigger such as the PageRank-inspired approach of Yu and Ji (2016). Here, for simplicity, we use the governing head of the two entities within their token interval (i.e., the node in the dependency graph that dominates the two entities) as a proxy for the trigger. We then create the shortest dependency paths from it to each of the two entities, and concatenate these two sequences to form the representation of the corresponding relation mention. This is shown as the head"
W19-1505,D10-1035,0,0.0236951,"Missing"
W19-1505,D15-1203,0,0.0796239,",772 are NA), the development partition contains 1,864 instances (447 NA), and the test partition contains 5663 instances (1360 NA). The dataset is divided such that there is no overlap among entity pairs 5.3 Baselines Previous work: We compare our model against the previous state-of-the-art work in Jat et al. (2017). They propose two fully-supervised models: a bidirectional gated recurrent unit neural network with word attention (their BGWA model) and a piecewise convolutional neural network (PCNN) with an entity attention layer (their EA model), which is itself based on the PCNN approach of Zeng et al. (2015). Since we use single models only, we omit Jat et al.’s supervised ensemble for a more direct comparison. Note these two approaches are state-of-the-art methods for RE that rely on more complex attentionbased models. On the other hand, in this work we use only vanilla LSTMs for both of our student and teacher models. However, since the MT framework is agnostic to the student model, in fu3 In this work we use k = 1. In initial experiments, other values of k did not change results significantly. 4 https://research. googleblog.com/2013/04/ 50000-lessons-on-how-to-read-relation. html 33 6 ture wor"
W19-1505,C18-1196,1,0.91107,"t al., 2015). However, they have not yet been well-studied in the context of natural language processing. Hu et al. (2016) propose a teacherstudent model for the task of sentiment classification and named entity recognition, where the teacher is derived from a manually specified set of rule-templates that regularizes a neural student, thereby allowing one to combine neural and symbolic systems. Our MT system is different in that the teacher is a simple running average of the students across different epochs of training, which removes the need of human supervision through rules. More recently, Nagesh and Surdeanu (2018) applied the MT architecture for the task of semisupervised Named Entity Classification, which is a simpler task compared to our RE task. Recent works (Liu et al., 2015; Xu et al., 2015; Su et al., 2018) use neural networks to learn syntactic features for relation extraction via traversing the shortest dependency path. Following this trend, we adapt such syntax-based neural models to both of our student and teacher classifiers in the MT architecture. 3 Mean Teacher Framework Our approach repurposes the Mean Teacher framework for relation extraction. This section an overview of the general MT f"
W19-1505,D14-1162,0,0.0849507,"function). 5.4 Results Model Tuning We lightly tuned the approach and hyperparameters on the development partitions. We built our architecture in PyTorch5 , a popular deep learning library, extending the framework of Tarvainen and Valpola (2017)6 . For our model, we used a bidirectional LSTM with a hidden size of 50. The subsequent fully connected network has one hidden layer of 100 dimensions and ReLU activations. The training was done with the Adam optimizer (Kingma and Ba, 2015) with the default learning rate (0.1). We initialized our word embeddings with GloVe 100-dimensional embeddings (Pennington et al., 2014)7 . The dependency embeddings were also of size 100 and were randomly initialized. Both of these embedding were allowed to update during training. Any word or token which occurred fewer than 5 times in a dataset was treated as unknown. For our word dropout, we removed one word from each input at random. MT has some additional parameters: the consistency cost weight (the weight given to the consistency component of the cost function) and the EMA decay rate (↵ in Eq. 2). We set these to be 1.0 and 0.99 respectively. Akin to a burn-in at the beginning of training, we had an initial consistency co"
W19-1505,N18-1202,0,0.0158462,"n the other hand, the gradients are not backpropagated through the teacher weights, rather they are deterministically updated after each mini-batch of gradient descent in the student. The update uses an exponentially-weighted moving average (EMA) that combines the previous teacher with the latest Surface LSTM (surfaceLSTM): The next learned representation also uses the surface form of the input, i.e., the sequence of words between the two entities, but replaces the word embedding average with a single-layer bidirectional LSTM, which have been demonstrated to encode some syntactic information (Peters et al., 2018). The representation from the LSTM is passed to the feed-forward network as above. Head LSTM (headLSTM): Under the intuition that the trigger is the most important lexical item in the context of a relation, relation extraction 31 Figure 1: The Mean Teacher (MT) framework for relation extraction which makes use of a large, unlabeled corpus and a small number of labeled data points. Intuitively, the larger, unlabeled corpus allows the model to learn a robust representation of the input (i.e., one which is not sensitive to noise), and the smaller set of labeled data constrains this learned repres"
W19-1505,P15-1150,0,0.0104004,"hey have other channels for additional information such as part of speech (POS)). Rather than maintaining distinct channels for the different representations, here we elect to keep both surface and syntactic forms in the same sequence and instead experiment with different degrees of syntactic representation. We also do not include other types of information (e.g., POS) here, as it is beyond the scope of the current work. There are several more structured (and more complex) models proposed for relation extraction, e.g., tree-based recurrent neural networks (Socher et al., 2010) and tree LSTMs (Tai et al., 2015). Our semi-supervised framework is an orthogonal improvement, and it is flexible enough to potentially incorporate any of these more complex models. (2) We represent sentences with several types of syntactic abstractions, and employ a simple neural sequence model to embed these representations. We demonstrate in our experiments that the representation that explicitly models syntactic information helps SSL to make the most of limited training data in the RE task. (3) We evaluate the proposed approach on the Google-IISc Distant Supervision (GDS) dataset introduced in Jat et al. (2017). Our empir"
W19-2603,P15-1034,0,0.019678,"an the more complicated ones, (b) all DL approaches outperform the standalone linguistically-informed method, and (c) the difference between the two strategies grows larger with the complexity of the text. 2 Related work The rate of scientific publishing has grown substantially each year, reaching a level that exceeds the human capacity to read and process. For example, PubMed, a search engine of biomedical publications2 now indexes over 25 million papers, 17 million of which were published between 1990 and the present. Domain-agnostic approaches, such as open information extraction (OpenIE) (Angeli et al., 2015) can begin to mitigate this by extracting information in the form of relation triples. However the widely varied language used by authors means that extractions can be difficult to aggregate and utilize. On the other hand, there have been significant efforts to develop domain-specific information extraction approaches that are tailored to scientific publications. These approaches range from rulebased to machine learning-based, and hybrid approaches (Valenzuela-Esc´arcega et al., 2018; Peng et al., 2017; Quirk and Poon, 2016; Kim et al., 2013; Bj¨orne and Salakoski, 2013; Hakala et al., 2013; B"
W19-2603,P09-1113,0,0.543281,"ent may seriously hinder the quality of downstream results. In particular, biomedical research literature is prone to noise caused by the mischaracterization of the polarity (e.g., promotion vs. inhibition) of biochemical interactions. This is the focus of this work. The identification of polarity in the biomedical domain is complicated by the fact that the language used is often hedged through multiple nega(1) We introduce a novel dataset that annotates the polarity of biomedical interactions. The dataset comes in multiple variants. A first variant was derived using distant supervision (DS) (Mintz et al., 2009) by aligning a knowledge base (KB) of protein interactions (Perfetto et al., 2015) with the outputs of a machine reader (ValenzuelaEsc´arcega et al., 2018). This dataset contains 52,779 promotion and 35,177 inhibition interactions. To account for the noise introduced through the DS process, we provide a second variant of this dataset consisting of a sample of the full dataset 1 This representation is better but not perfect. The correct representation should be: (decrease of Bad) causes (decrease of p38 MAPK). However, the promotes/inhibits representation is widely used both in IE datasets and"
W19-2603,W13-2003,0,0.0250642,"Missing"
W19-2603,W13-2014,0,0.0718377,"Missing"
W19-2603,Q17-1008,0,0.0794252,"bution accuracy. We use this dataset to train and evaluate several deep learning models for polarity identification, and compare these to a linguistically-informed model. The best performing deep learning architecture achieves 0.968 average F1 performance in a five-fold cross-validation study, a considerable improvement over the linguistically informed model average F1 of 0.862. 1 Introduction Recent advances in information extraction (IE) have resulted in high-precision, high-throughput systems tailored to the reading of biomedical scientific publications (Valenzuela-Esc´arcega et al., 2018; Peng et al., 2017; Quirk and Poon, 2016; Kim et al., 2013; Bj¨orne and Salakoski, 2013; Hakala et al., 2013; Bui et al., 2013, inter alia). This, in turn, has resulted in the use of machine reading systems as the foundation of more complex, higher-level inference applications in specific domains such as cancer research (ValenzuelaEsc´arcega et al., 2018). However, the presence of noise in pipelined systems that use IE as an initial component may seriously hinder the quality of downstream results. In particular, biomedical research literature is prone to noise caused by the mischaracterization of the polarity ("
W19-2603,W13-2004,0,0.0242663,"Missing"
W19-2603,H05-1044,0,0.0636777,"d partitions contain 62 and 67 data points, respectively. one factor promotes or inhibits another factor. Biological models have been assembled from these interactions and used for domain-specific applications (Gyori et al., 2017). Here we propose an approach for automatically detecting this polarity. Polarity detection has been explored in several other natural language processing tasks, perhaps most notably in sentiment analysis (e.g., Pang et al., 2008; Liu, 2012; Liu and Zhang, 2012), where the polarity of a text is measured on a spectrum from negative to positive sentiment. Similarly, in Wilson et al. (2005), the authors frame the problem of extracting opinion polarity explicitly as a sentiment analysis task. Our work is similar in spirit, but it focuses on the polarity of scientific statements. In (Lauscher et al., 2017), the authors investigate the polarity polarity of citations within the context of bibliometric analysis. In contrast, our work addresses the polarity of content, i.e., events extracted from the biomedical literature. To summarize, our approach is inspired by this previous work, but it differs in two ways: first, we focus on statements in the biomedical domain, and, second, we ex"
W19-2603,D11-1135,0,0.0195719,"m seeds and the numbers reported are the corresponding averages from all the trials. Table 2 reports these average scores as well as the standard deviations for all the approaches analyzed. Tables 3 and 4 contain the results on the manually-curated Easy and Challenge partitions, when the corresponding models were trained on the entire DS dataset. The code and data used to generate these results are available at this URL: https://github. com/clulab/releases/tree/master/ naacl-essp2019-polarity. Table 1: Label distribution on the DS dataset. The distant supervision process is potentially noisy (Yao et al., 2011). To control for this noise, we also created two smaller hand-curated datasets, as follows: 1. We randomly sampled 100 sentences from the sentences where Reach agreed with SIGNOR, and 100 from the sentences where Reach disagreed with SIGNOR. Based on the intuition mentioned in the previous paragraph, we call these partitions Easy and Challenge. 7 Discussion 7.1 Discussion of the main results Table 2 shows that the linguistically-informed approach performs reasonably well overall, with a F1 score of 0.862. This is encouraging, but also somewhat misleading. The DS dataset consists of mostly Easy"
