2015.mtsummit-papers.27,borin-etal-2014-bring,1,0.854875,"Missing"
2015.mtsummit-papers.27,P15-2001,0,0.0786639,"Missing"
2015.mtsummit-papers.27,de-melo-2014-etymological,1,0.857672,"Missing"
2015.mtsummit-papers.27,2007.mtsummit-papers.24,0,0.0125991,"tors. We investigate to what extent external large-scale resources can be used to create much more multilingual word representation data. In particular, we rely on Wiktionary, a sister project of Wikipedia that for many years now has been creating a large, collaboratively edited online dictionary. Due to its rich multilingual data, now with over 4 million entries in over 1,000 languages, Wiktionary has been used extensively in natural language processing, e.g. for part-of-speech tagging (Li et al., 2012) and named entity recognition (Richman and Schone, 2008), for cross-language image search (Etzioni et al., 2007) and text classification (Nastase and Strapparava, 2013), and for producing language registries (de Melo, 2015) and etymological databases (de Melo, 2014). Wiktionary has also made it possible to translate lexical knowledge bases such as WordNet to hundreds of languages (de Melo and Weikum, 2009) and to translate thesauri (Borin et al., 2014). Finally, it has been used as an extra ingredient in regular machine translation systems (G¨ohring, 2014). Relying on Wiktionary instead of on other training data has playfully been called Wikily supervision (Li et al., 2012). Our work constitutes a form"
2015.mtsummit-papers.27,N15-1184,0,0.0231578,"e known well beyond the core natural language processing community. Subsequently, a number of improvements to the learning algorithms have been proposed. Our objective function is related to those of other models that aim to exploit similarities between words. Chen et al. (2015) extend the word2vec CBOW objective function in order to pay special attention to contexts that reveal more explicit semantic relationships, rather than treating all contexts as equal. The semantic relationships are obtained using information extraction methods, e.g. from lists and definitions. Yu and Dredze (2014) and Faruqui et al. (2015) propose to optimize monolingual word embeddings so as to match information from pre-existing lexical resources. Hill et al. (2015) used dictionary glosses from several resources (including Wiktionary) in order to train neural networks to produce vectors for multi-word phrases. Proceedings of MT Summit XV, vol.1: MT Researchers&apos; Track Miami, Oct 30 - Nov 3, 2015 |p. 355 While most word representation learning research has been monolingual aiming at English, recently there has been some interest in multilingual aspects of it. Some works take pre-existing vectors for different languages and conn"
2015.mtsummit-papers.27,E14-1049,0,0.0371155,"Wiktionary) in order to train neural networks to produce vectors for multi-word phrases. Proceedings of MT Summit XV, vol.1: MT Researchers&apos; Track Miami, Oct 30 - Nov 3, 2015 |p. 355 While most word representation learning research has been monolingual aiming at English, recently there has been some interest in multilingual aspects of it. Some works take pre-existing vectors for different languages and connect them. Mikolov et al. (2013b) develop a method to learn projections between two monolingual word embedding spaces. Lazaridou et al. (2015) investigate means to improve such projections. Faruqui and Dyer (2014) propose using canonical correlation analysis (CCA), while Lu et al. (2015) suggest using Deep CCA instead. Our method, in contrast, does not assume that we have already created non-English word vectors. We only rely on English word vectors, which are readily available from numerous sources. A number of studies have focused on using multilingual corpora, often parallel corpora, to produce bilingual word vector spaces (Kalchbrenner and Blunsom, 2013; Koˇcisk´y et al., 2014). Utt and Pad´o (2014) investigate using syntax for bilingual vector space models. Hermann and Blunsom (2014) create biling"
2015.mtsummit-papers.27,W14-1006,0,0.0609251,"Missing"
2015.mtsummit-papers.27,D09-1124,0,0.0548014,"e provide evaluation results on German-language datasets, while Tables 5 and 6 provide similar results on Spanish and French datasets. For comparison, we list all published results known to us that are also based on vectors as well as results on all other non-English word vectors we could obtain and evaluate directly. In all cases, we see that our vectors fare significantly better than the competitors. 4.4 Cross-Lingual Semantic Relatedness Semantic relatedness can also be evaluated across languages. We adopt the same methodology as earlier but rely on the Spanish-English evaluation data from Hassan and Mihalcea (2009), which we can use to compare our vectors with those of Chandar A P et al. (2014). Further, we consider the new cross-lingual semantic relatedness evaluation data released by CamachoCollados et al. (2015), which is based on the RG65 dataset. Our results on these cross-lingual datasets are listed in Table 7. Again, our Wiktionary-based representations compare favorably with other available results. 4.5 Word Choice Problems Word choice problems consist of a target word and a selection of possible words or phrases describing it. Consider the following three examples. gourmet a) enjoys cooking b)"
2015.mtsummit-papers.27,D13-1176,0,0.0246258,"evelop a method to learn projections between two monolingual word embedding spaces. Lazaridou et al. (2015) investigate means to improve such projections. Faruqui and Dyer (2014) propose using canonical correlation analysis (CCA), while Lu et al. (2015) suggest using Deep CCA instead. Our method, in contrast, does not assume that we have already created non-English word vectors. We only rely on English word vectors, which are readily available from numerous sources. A number of studies have focused on using multilingual corpora, often parallel corpora, to produce bilingual word vector spaces (Kalchbrenner and Blunsom, 2013; Koˇcisk´y et al., 2014). Utt and Pad´o (2014) investigate using syntax for bilingual vector space models. Hermann and Blunsom (2014) create bilingual word representations without word alignment. Hill et al. (2014) showed that word embeddings obtained from translations better reflect the ontological status of words than regular neural embeddings. One advantage of corpus-based approaches is the potential to have a substantial coverage, given sufficiently large corpora. However, as the amount of available parallel text is somewhat limited, in practice, this advantage may only apply to methods t"
2015.mtsummit-papers.27,P15-1027,0,0.0191243,"et al. (2015) used dictionary glosses from several resources (including Wiktionary) in order to train neural networks to produce vectors for multi-word phrases. Proceedings of MT Summit XV, vol.1: MT Researchers&apos; Track Miami, Oct 30 - Nov 3, 2015 |p. 355 While most word representation learning research has been monolingual aiming at English, recently there has been some interest in multilingual aspects of it. Some works take pre-existing vectors for different languages and connect them. Mikolov et al. (2013b) develop a method to learn projections between two monolingual word embedding spaces. Lazaridou et al. (2015) investigate means to improve such projections. Faruqui and Dyer (2014) propose using canonical correlation analysis (CCA), while Lu et al. (2015) suggest using Deep CCA instead. Our method, in contrast, does not assume that we have already created non-English word vectors. We only rely on English word vectors, which are readily available from numerous sources. A number of studies have focused on using multilingual corpora, often parallel corpora, to produce bilingual word vector spaces (Kalchbrenner and Blunsom, 2013; Koˇcisk´y et al., 2014). Utt and Pad´o (2014) investigate using syntax for"
2015.mtsummit-papers.27,D12-1127,0,0.0423234,"Missing"
2015.mtsummit-papers.27,N15-1028,0,0.0225876,"hrases. Proceedings of MT Summit XV, vol.1: MT Researchers&apos; Track Miami, Oct 30 - Nov 3, 2015 |p. 355 While most word representation learning research has been monolingual aiming at English, recently there has been some interest in multilingual aspects of it. Some works take pre-existing vectors for different languages and connect them. Mikolov et al. (2013b) develop a method to learn projections between two monolingual word embedding spaces. Lazaridou et al. (2015) investigate means to improve such projections. Faruqui and Dyer (2014) propose using canonical correlation analysis (CCA), while Lu et al. (2015) suggest using Deep CCA instead. Our method, in contrast, does not assume that we have already created non-English word vectors. We only rely on English word vectors, which are readily available from numerous sources. A number of studies have focused on using multilingual corpora, often parallel corpora, to produce bilingual word vector spaces (Kalchbrenner and Blunsom, 2013; Koˇcisk´y et al., 2014). Utt and Pad´o (2014) investigate using syntax for bilingual vector space models. Hermann and Blunsom (2014) create bilingual word representations without word alignment. Hill et al. (2014) showed"
2015.mtsummit-papers.27,P15-1002,0,0.0164129,"of the original word vector representations, yielding a large multilingual dictionary of word embeddings. We believe that this resource can enable numerous monolingual and cross-lingual applications, as evidenced in a series of monolingual and cross-lingual semantic experiments that we have conducted. 1 Introduction Vectorial representations of words have grown to play an important role in natural language processing and machine translation. Especially for the latter, deep learning and representation learning-based approaches have recently proven remarkably effective (Sutskever et al., 2014; Luong et al., 2015). Vector-based encodings of meaning are a central ingredient in many of these recent neural machine translation systems, although they can also be beneficial in ordinary phrase-based machine translation (Mikolov et al., 2013b). In this work, we focus on the task of creating vector representations of multilingual words (as well as lexicalized phrases). Previous work in this area has relied on multilingual corpora to train bilingual word vectors. We investigate to what extent external large-scale resources can be used to create much more multilingual word representation data. In particular, we r"
2015.mtsummit-papers.27,N13-1090,0,0.455299,"es of monolingual and cross-lingual semantic experiments that we have conducted. 1 Introduction Vectorial representations of words have grown to play an important role in natural language processing and machine translation. Especially for the latter, deep learning and representation learning-based approaches have recently proven remarkably effective (Sutskever et al., 2014; Luong et al., 2015). Vector-based encodings of meaning are a central ingredient in many of these recent neural machine translation systems, although they can also be beneficial in ordinary phrase-based machine translation (Mikolov et al., 2013b). In this work, we focus on the task of creating vector representations of multilingual words (as well as lexicalized phrases). Previous work in this area has relied on multilingual corpora to train bilingual word vectors. We investigate to what extent external large-scale resources can be used to create much more multilingual word representation data. In particular, we rely on Wiktionary, a sister project of Wikipedia that for many years now has been creating a large, collaboratively edited online dictionary. Due to its rich multilingual data, now with over 4 million entries in over 1,000 l"
2015.mtsummit-papers.27,D07-1060,0,0.0329018,"tionary-based representations compare favorably with other available results. 4.5 Word Choice Problems Word choice problems consist of a target word and a selection of possible words or phrases describing it. Consider the following three examples. gourmet a) enjoys cooking b) has indigestion c) has an expert appreciation of food d) is hungry dale a) plain b) retreat c) shelter d) valley brace a) to scream b) prepare for danger c) hold your breath d) close your eyes Here, the correct answers are c) for gourmet, d) for dale, and b) for brace. For English, we rely on a well-known dataset used by Mohammad et al. (2007). We also use a large Germanlanguage collection of similar quiz questions3 . The latter consists of 984 problem instances collected from 2001 to 2005 editions of the German version of Reader’s Digest Magazine, where they appear as “Word Power” problems. We compute cosine similarities between the target word and the candidate answers. Some answers are individual words or expressions already covered in our data, in which case this is simple. If a candidate answer, however, consists of multiple words that are not covered in our data as a multi-word expression, we simply use the maximum cosine sim"
2015.mtsummit-papers.27,P13-1064,0,0.015937,"rge-scale resources can be used to create much more multilingual word representation data. In particular, we rely on Wiktionary, a sister project of Wikipedia that for many years now has been creating a large, collaboratively edited online dictionary. Due to its rich multilingual data, now with over 4 million entries in over 1,000 languages, Wiktionary has been used extensively in natural language processing, e.g. for part-of-speech tagging (Li et al., 2012) and named entity recognition (Richman and Schone, 2008), for cross-language image search (Etzioni et al., 2007) and text classification (Nastase and Strapparava, 2013), and for producing language registries (de Melo, 2015) and etymological databases (de Melo, 2014). Wiktionary has also made it possible to translate lexical knowledge bases such as WordNet to hundreds of languages (de Melo and Weikum, 2009) and to translate thesauri (Borin et al., 2014). Finally, it has been used as an extra ingredient in regular machine translation systems (G¨ohring, 2014). Relying on Wiktionary instead of on other training data has playfully been called Wikily supervision (Li et al., 2012). Our work constitutes a form of Wiktionary-based supervision This research was suppor"
2015.mtsummit-papers.27,D14-1162,0,0.0985355,"y instead of on other training data has playfully been called Wikily supervision (Li et al., 2012). Our work constitutes a form of Wiktionary-based supervision This research was supported by China 973 Program Grants 2011CBA00300, 2011CBA00301, and NSFC Grants 61033001, 61361136003, 61450110088. Proceedings of MT Summit XV, vol.1: MT Researchers&apos; Track Miami, Oct 30 - Nov 3, 2015 |p. 346 for multilingual word representation learning. More specifically, our method starts with existing word representations such as the widely available ones trained on large English corpora (Mikolov et al., 2013a; Pennington et al., 2014). It then uses Wiktionary to decide how to place new words into the same vector space. 2 Method For obtaining the new word representations, we adopt the following framework. We assume, we are given vectors uw for words w ∈ V0 , where V0 is some initial vocabulary of words. Such vectors may come from any of the popular methods for training word vectors. We later use the well-known vectors from the word2vec (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014) projects. Our goal is to create new vectors vw for all words w in a substantially larger vocabulary V , which typically will contai"
2015.mtsummit-papers.27,P08-1001,0,0.0180603,"s relied on multilingual corpora to train bilingual word vectors. We investigate to what extent external large-scale resources can be used to create much more multilingual word representation data. In particular, we rely on Wiktionary, a sister project of Wikipedia that for many years now has been creating a large, collaboratively edited online dictionary. Due to its rich multilingual data, now with over 4 million entries in over 1,000 languages, Wiktionary has been used extensively in natural language processing, e.g. for part-of-speech tagging (Li et al., 2012) and named entity recognition (Richman and Schone, 2008), for cross-language image search (Etzioni et al., 2007) and text classification (Nastase and Strapparava, 2013), and for producing language registries (de Melo, 2015) and etymological databases (de Melo, 2014). Wiktionary has also made it possible to translate lexical knowledge bases such as WordNet to hundreds of languages (de Melo and Weikum, 2009) and to translate thesauri (Borin et al., 2014). Finally, it has been used as an extra ingredient in regular machine translation systems (G¨ohring, 2014). Relying on Wiktionary instead of on other training data has playfully been called Wikily sup"
2015.mtsummit-papers.27,P10-1040,0,0.0482472,"ibution of likely word co-occurrences. Since many distributional approaches use numerical vectors to represent the contexts, this sparsity often is manifested in the form of sparse vectors with many zeros. Smoothing techniques and algorithms such as Latent Semantic Analysis (Deerwester et al., 1990) were proposed to alleviate some of these problems. More recently, distributed and distributional methods have grown together in the form of neural network-inspired architectures that learn distributed representations from large corpora by accounting for word co-occurrences (Collobert et al., 2011; Turian et al., 2010). The resulting representations are still vectorial and based on corpus co-occurrences, but much lowerdimensional than in traditional distributional approaches and thus significantly less sparse. The massive attention on deep learning in recent years, paired with fast training methods as in the word2vec method by Mikolov et al. (2013a), which actually forgoes deep learning, has propelled these methods to the forefront of NLP, to the point that they are known well beyond the core natural language processing community. Subsequently, a number of improvements to the learning algorithms have been p"
2015.mtsummit-papers.27,Q14-1020,0,0.0287731,"Missing"
2015.mtsummit-papers.27,P14-2089,0,0.0234382,"to the point that they are known well beyond the core natural language processing community. Subsequently, a number of improvements to the learning algorithms have been proposed. Our objective function is related to those of other models that aim to exploit similarities between words. Chen et al. (2015) extend the word2vec CBOW objective function in order to pay special attention to contexts that reveal more explicit semantic relationships, rather than treating all contexts as equal. The semantic relationships are obtained using information extraction methods, e.g. from lists and definitions. Yu and Dredze (2014) and Faruqui et al. (2015) propose to optimize monolingual word embeddings so as to match information from pre-existing lexical resources. Hill et al. (2015) used dictionary glosses from several resources (including Wiktionary) in order to train neural networks to produce vectors for multi-word phrases. Proceedings of MT Summit XV, vol.1: MT Researchers&apos; Track Miami, Oct 30 - Nov 3, 2015 |p. 355 While most word representation learning research has been monolingual aiming at English, recently there has been some interest in multilingual aspects of it. Some works take pre-existing vectors for di"
2020.coling-industry.4,P17-1161,0,0.0194385,"Commercial web search engines are known to incorporate thousands of carefully designed features, and the feature engineering process is time-consuming, incomplete, and over-specified. In recent years, neural models have attracted attention in light of their ability to obviate the need for handcrafted features. Well-known neural ranking models include DSSM (Huang et al., 2013), DRMM (Guo et al., 2016), DUET (Mitra et al., 2017), PACRR (Hui et al., 2017), and Co-PACRR (Hui et al., 2018). 2.2 Pre-trained Lanuage Models Recently, neural models pre-trained on language modeling tasks such as ELMo (Peters et al., 2017), Open-AI GPT (Radford et al., 2019), and BERT (Devlin et al., 2018) have achieved impressive results across wide swaths of the NLP landscape. Among these model variants, BERT, the pre-trained deep 34 bidirectional Transformer, has shown strong performance on search-related tasks, including retrieval-based question answering (Yang et al., 2019a), and numerous experiments confirm that BERT-based methods can outperform state-of-the-art ad-hoc ranking baselines (MacAvaney et al., 2019; Nogueira and Cho, 2019; Nogueira et al., 2019; Qiao et al., 2019). 2.3 Knowledge Distilling To address the compu"
2020.coling-industry.4,N19-4013,0,0.0219028,"clude DSSM (Huang et al., 2013), DRMM (Guo et al., 2016), DUET (Mitra et al., 2017), PACRR (Hui et al., 2017), and Co-PACRR (Hui et al., 2018). 2.2 Pre-trained Lanuage Models Recently, neural models pre-trained on language modeling tasks such as ELMo (Peters et al., 2017), Open-AI GPT (Radford et al., 2019), and BERT (Devlin et al., 2018) have achieved impressive results across wide swaths of the NLP landscape. Among these model variants, BERT, the pre-trained deep 34 bidirectional Transformer, has shown strong performance on search-related tasks, including retrieval-based question answering (Yang et al., 2019a), and numerous experiments confirm that BERT-based methods can outperform state-of-the-art ad-hoc ranking baselines (MacAvaney et al., 2019; Nogueira and Cho, 2019; Nogueira et al., 2019; Qiao et al., 2019). 2.3 Knowledge Distilling To address the computational overhead of large models, techniques such as the Knowledge Distillation (KD) framework have been proposed (Ba and Caruana, 2014; Hinton et al., 2015). These have shown remarkable potential in accelerating the inference time and improving the performance. Well-trained wide and deep networks are recruited as teachers, and the target stu"
2020.coling-industry.8,N03-1007,0,0.279824,"Missing"
2020.coling-industry.8,N19-1423,0,0.0177749,"upon data is selected. To construct corpora at a relatively low cost, the annotation task is simplified so as to merely elicit a “yes” or “no” response. The whole annotation process is divided to two stages. At the first stage, we collect ambiguous questions by annotating online query logs. If a query lacks a predicate or the object of the predicate, it is annotated as ambiguous. At the second stage, we annotate potential intents for each ambiguous question. As Table 1 shows, for each ambiguous question (“How to apply”), the top 50 most relevant intent candidates are collected using the BERT (Devlin et al., 2019) semantic similarity model applied to the intent inventory. The human annotators are asked to decide whether an intent can possibly address a user’s question. 4 Reinforcement Learning for Label Recommendation Figure 4: Label recommendation policy model architecture. Label Recommendation as an RL problem. In order to train a model able to recommend labels one by one, we have two options: 1) Deduce a path reversely for supervised learning. 2) Create an environment for the model to explore. We believe that creating an environment for the model to explore different label sequences may lead to bett"
2020.coling-industry.8,D19-1605,0,0.0257251,"e the clarification ability of phrase collections and an end-to-end sequential phrase recommendation model trained with reinforcement learning. 3. Both offline and online experiments confirm that our method outperforms pertinent baselines significantly. 2 Related Work Query Refinement. Several works explore the use of clarification questions for query refinement (Kotov and Zhai, 2010; Sajjad et al., 2012; Zheng et al., 2011; Ma et al., 2010; Sadikov et al., 2010). For instance, Kotov and Zhai (2010) and Sajjad et al. (2012) use question templates to generate a list of clarification questions. Elgohary et al. (2019) rewrite questions using the dialogue context. Zhang et al. (2019) invoke graph edit distance for query refinement. Other studies rely on reinforcement learning to refine user queries (Nogueira and Cho, 2017; Buck et al., 2018; Liu et al., 2019), but consider queries that are unambiguous (though possibly ill-formed or non-standard). Accordingly, they seek to increase the recall, while in our setting, we consider ambiguous user queries, and our model primarily seeks to address the task of question clarification. Dialogue. Boni and Manandhar (2003) developed an algorithm to recognize clarificati"
2020.coling-industry.8,D17-1061,0,0.0208241,"outperforms pertinent baselines significantly. 2 Related Work Query Refinement. Several works explore the use of clarification questions for query refinement (Kotov and Zhai, 2010; Sajjad et al., 2012; Zheng et al., 2011; Ma et al., 2010; Sadikov et al., 2010). For instance, Kotov and Zhai (2010) and Sajjad et al. (2012) use question templates to generate a list of clarification questions. Elgohary et al. (2019) rewrite questions using the dialogue context. Zhang et al. (2019) invoke graph edit distance for query refinement. Other studies rely on reinforcement learning to refine user queries (Nogueira and Cho, 2017; Buck et al., 2018; Liu et al., 2019), but consider queries that are unambiguous (though possibly ill-formed or non-standard). Accordingly, they seek to increase the recall, while in our setting, we consider ambiguous user queries, and our model primarily seeks to address the task of question clarification. Dialogue. Boni and Manandhar (2003) developed an algorithm to recognize clarification dialogue, rather than for asking clarification questions. Varges et al. (2010) found that the use of clarification has a positive effect on concept precision in task-oriented dialogue. Li et al. (2017) fo"
2020.coling-industry.8,P18-1255,0,0.0448652,"Missing"
2020.coling-industry.8,N19-1013,0,0.0611136,"Missing"
2020.coling-industry.8,C12-1143,0,0.0082112,"unts to obtaining a suitable set of labels. The main contributions of our work are: 1. We formulate interactive clarification as a collection partitioning problem. 2. We propose a novel reward function to evaluate the clarification ability of phrase collections and an end-to-end sequential phrase recommendation model trained with reinforcement learning. 3. Both offline and online experiments confirm that our method outperforms pertinent baselines significantly. 2 Related Work Query Refinement. Several works explore the use of clarification questions for query refinement (Kotov and Zhai, 2010; Sajjad et al., 2012; Zheng et al., 2011; Ma et al., 2010; Sadikov et al., 2010). For instance, Kotov and Zhai (2010) and Sajjad et al. (2012) use question templates to generate a list of clarification questions. Elgohary et al. (2019) rewrite questions using the dialogue context. Zhang et al. (2019) invoke graph edit distance for query refinement. Other studies rely on reinforcement learning to refine user queries (Nogueira and Cho, 2017; Buck et al., 2018; Liu et al., 2019), but consider queries that are unambiguous (though possibly ill-formed or non-standard). Accordingly, they seek to increase the recall, whi"
2020.coling-industry.8,W10-4337,0,0.0364782,"2019) invoke graph edit distance for query refinement. Other studies rely on reinforcement learning to refine user queries (Nogueira and Cho, 2017; Buck et al., 2018; Liu et al., 2019), but consider queries that are unambiguous (though possibly ill-formed or non-standard). Accordingly, they seek to increase the recall, while in our setting, we consider ambiguous user queries, and our model primarily seeks to address the task of question clarification. Dialogue. Boni and Manandhar (2003) developed an algorithm to recognize clarification dialogue, rather than for asking clarification questions. Varges et al. (2010) found that the use of clarification has a positive effect on concept precision in task-oriented dialogue. Li et al. (2017) focus on clarification in the specific circumstance of a bot not understanding a teacher because of spelling mistakes, which is a sub-problem of 79 our setting. Zhang et al. (2018) generate clarification questions using language patterns with predicted aspect. They do not use reinforcement learning to optimize the order of the questions. Wang et al. (2018) devised soft and hard-typed decoders to generate good questions by capturing different roles of different word types."
2020.coling-industry.8,P18-1204,0,0.0230919,"(2003) developed an algorithm to recognize clarification dialogue, rather than for asking clarification questions. Varges et al. (2010) found that the use of clarification has a positive effect on concept precision in task-oriented dialogue. Li et al. (2017) focus on clarification in the specific circumstance of a bot not understanding a teacher because of spelling mistakes, which is a sub-problem of 79 our setting. Zhang et al. (2018) generate clarification questions using language patterns with predicted aspect. They do not use reinforcement learning to optimize the order of the questions. Wang et al. (2018) devised soft and hard-typed decoders to generate good questions by capturing different roles of different word types. Aliannejadi et al. (2019) designed a two-stage retrieval and ranking model to rank clarification question candidates generated by human annotators, different from our end-to-end reinforcement learning approach. Korpusik and Glass (2019) construct clarification questions from a food attribute list (brand, fat, etc.). They rely on a hybrid reinforcement learning approach to select the order of clarification questions to ask, while we present an end-to-end reinforcement learning"
2020.coling-industry.8,I11-1106,0,0.0332382,"uitable set of labels. The main contributions of our work are: 1. We formulate interactive clarification as a collection partitioning problem. 2. We propose a novel reward function to evaluate the clarification ability of phrase collections and an end-to-end sequential phrase recommendation model trained with reinforcement learning. 3. Both offline and online experiments confirm that our method outperforms pertinent baselines significantly. 2 Related Work Query Refinement. Several works explore the use of clarification questions for query refinement (Kotov and Zhai, 2010; Sajjad et al., 2012; Zheng et al., 2011; Ma et al., 2010; Sadikov et al., 2010). For instance, Kotov and Zhai (2010) and Sajjad et al. (2012) use question templates to generate a list of clarification questions. Elgohary et al. (2019) rewrite questions using the dialogue context. Zhang et al. (2019) invoke graph edit distance for query refinement. Other studies rely on reinforcement learning to refine user queries (Nogueira and Cho, 2017; Buck et al., 2018; Liu et al., 2019), but consider queries that are unambiguous (though possibly ill-formed or non-standard). Accordingly, they seek to increase the recall, while in our setting, w"
2020.coling-main.300,D19-1380,0,0.0193828,"Methods In order to move from word vector representations towards representations for entire sentences, a simple baseline is to simply average the word embeddings of all words in a sentence. Although this method neglects the order of words, it performs surprisingly well in many downstream tasks. Pagliardini et al. (2017) proposed a method to learn word and n-gram embeddings such that the average of all words and n-grams in a sentence can serve as a high-quality sentence vector. Rücklé et al. (2018) improved the average pooling method by concatenating different power means of word embeddings. Almarwani et al. (2019) proposed the use of a Discrete Cosine Transform (DCT) to compress word vectors into sentence embeddings, while retaining word order information. Several methods have been proposed to directly learn representations of sentences. The Skip-Thought Vector approach (Kiros et al., 2015), inspired by the skip-gram word2vec approach (Mikolov et al., 2013a), attempts to learn representations that enable the prediction of neighbouring sentences. It relies on an encoder–decoder structure based on Gated Recurrent Units. The Quick-Thought Vector approach (Logeswaran and Lee, 2018) improves both the effici"
2020.coling-main.300,W18-5440,0,0.0203436,"shed sufficient light on the topological properties of the representation space. The most well-known way to inspect the capabilities of sentence embeddings has been via what has been dubbed probing, i.e., supervised training of models that predict specific linguistic phenomena given embeddings as input. Kiros et al. (2015) evaluated the quality of their embeddings by using them for supervised downstream tasks such as sentiment polarity and question type classification. Adi et al. (2016) attempted to gain more specific insights by predicting word occurrences, word order, and sentence lengths. Bacon and Regier (2018) considered this approach to predict verb tense. Ettinger et al. (2018) trained classifiers for semantic roles and negation detection. Dasgupta et al. (2018) studied the argument sensitivity of the InferSent model by probing with respect to an NLI classification (contradiction vs. entailment). Kann et al. (2019) investigated verb alternation acceptability classifications. Conneau et al. (2018) predicted a wide range of mostly syntactic phenomena such as major syntactic constituents, the depth of the syntactic tree, grammatical number of the subject, and grammatical number of the object. For ea"
2020.coling-main.300,D15-1075,0,0.014193,"lace the plural noun with its singular form and replace the numeral word with “one&quot;, “a&quot;, or “an&quot;. • Verb Conjugation. We first identify sentences containing an auxiliary verb followed by a verb and then remove the auxiliary verb and replace the verb in the sentence with its inflected form. 3.2 Analogy based on Relationships Between Sentences In addition to our sentence analogy data derived from word analogies, we also create new diagnostic sentence analogy data based on specific forms of relationships between sentences. We start off with sentences extracted from NLI datasets, including SNLI (Bowman et al., 2015), Multi-NLI (Williams et al., 2018b), and SICK (Marelli et al., 2014). 3.2.1 Relationships Entailment. Given two sentence pairs SA , SB and SC , SD , an entailment analogy holds between these two sentence pairs if the respective relationships between SA and SB and between SC and SD are both entailment, as annotated in the NLI data. 2 We rely on SpaCy’s English models for these two tasks. 3392 Table 1: Examples of Lexical Analogy Common Capital Cities All Capital Cities Currencies City in State Man – Woman Comparative Nationality Adjective Opposites Plurals Verb Conjunction SA They traveled to"
2020.coling-main.300,D17-1070,0,0.0333638,"been proposed to directly learn representations of sentences. The Skip-Thought Vector approach (Kiros et al., 2015), inspired by the skip-gram word2vec approach (Mikolov et al., 2013a), attempts to learn representations that enable the prediction of neighbouring sentences. It relies on an encoder–decoder structure based on Gated Recurrent Units. The Quick-Thought Vector approach (Logeswaran and Lee, 2018) improves both the efficiency and performance of Skip-Thought Vectors by replacing the decoder with a simple classifier that selects the correct sentence among a set of candidates. InferSent (Conneau et al., 2017) learns sentence representations by auxiliary supervised learning on Natural Language Inference (NLI) data, outperforming prior methods on tasks that require detailed semantic understanding (Zhu et al., 2018). Subramanian et al. (2018) proposed methods to learn general purpose sentence representations via Multi-Task Learning. In recent years, contextualized word embeddings have drawn considerable attention in light of the formidable gains that they achieve across a wide range of NLP and IR tasks. The pioneering work on ELMo (Peters et al., 2018) showed that significant gains can be achieved ac"
2020.coling-main.300,P18-1198,0,0.0223903,"ownstream tasks such as sentiment polarity and question type classification. Adi et al. (2016) attempted to gain more specific insights by predicting word occurrences, word order, and sentence lengths. Bacon and Regier (2018) considered this approach to predict verb tense. Ettinger et al. (2018) trained classifiers for semantic roles and negation detection. Dasgupta et al. (2018) studied the argument sensitivity of the InferSent model by probing with respect to an NLI classification (contradiction vs. entailment). Kann et al. (2019) investigated verb alternation acceptability classifications. Conneau et al. (2018) predicted a wide range of mostly syntactic phenomena such as major syntactic constituents, the depth of the syntactic tree, grammatical number of the subject, and grammatical number of the object. For each probing task, they provide 100,000 training instances. Probing provides important insights about whether sufficient signals needed for a given downstream task are available if one has sufficient supervision. However, training on 100,000 instances does not reveal whether these signals are genuinely present in the sentence representations, as opposed to just being learnable from the training"
2020.coling-main.300,P19-1285,0,0.0197342,"(Peters et al., 2018) showed that significant gains can be achieved across a range of NLP tasks by considering the intermediate layers of a deep BiLSTM-based language model. Instead of standard bidirectional language modeling as in ELMo, the BERT approach (Devlin et al., 2019) developed at Google uses a training regimen considering Cloze-style masked language modeling, in which both sides of the context are simultaneously used to reconstruct an artificially masked word, along with an additional neighbour sentence prediction task. XLNet (Yang et al., 2019) is an auto-regressive Transformer-XL (Dai et al., 2019) based model using a permutation language model as the training task. XLNet outperforms BERT on various downstream tasks when they share the same number of model parameters and training corpus size. RoBERTa (Liu et al., 2019) improves the pre-training task of the original BERT model by removing the Next Sentence Prediction task and randomly generating different masks for words in a sentence. It also improves the performance of BERT by adding more training data. Reimers and Gurevych (2019) proposed Sentence-BERT, which utilizes Siamese and Triplet Networks to fine-tune BERT on NLI and Semantic"
2020.coling-main.300,N19-1423,0,0.136571,"nt models differ substantially in their ability to reflect such regularities. 1 Introduction Sentence embeddings are dense vectors that reflect salient semantic properties of a sentence. Similar to how commonly used word embedding methods such as word2vec (Mikolov et al., 2013a) capture semantic relationships between words, sentence embeddings are expected to encode semantic relationships between sentences. A number of different sentence embedding methods have been proposed (cf. Section 2.1 for an overview). In recent years, pretrained language models such as ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), XLNet (Yang et al., 2019), and RoBERTa (Liu et al., 2019) have become the method of choice when encoding text. Thus, such models are also often invoked to represent sentences by means of individual embeddings.1 While important properties of word vector representations have been studied extensively, far less is known about the properties of sentence vector representations. A particularly prominent aspect of word vector representations induced by methods such as word2vec is that the vector space exhibits certain kinds of regularities. Many of these are of the sort considered in word analogies."
2020.coling-main.300,K19-1085,0,0.0205513,"arn to recognize salient kinds of spam emails with an accuracy significantly above the level of chance. However, this does not license the conclusion that the bag-of-words representation inherently captures some notion of spamicity, as it were. Hence, an important complementary endeavour is to study the topology of the representation space. Zhu et al. (2018) proposed assessing sentence embeddings from a relational perspective in terms of proximity. In this paper, we specifically examine to what extent analogical relationships are reflected in terms of regularities in the representation space. Diallo et al. (2019) explored analogical embeddings for the relationship between questions and answers in question answering. Zhang and Baldwin (2019) investigated to what extent analogical reasoning can be used for relationships between documents. 3 Methodology Our goal is to explore to what extent different sentence embedding spaces reflect analogical regularities of the form A is to B as C is to D. In the remainder of this paper, we shall invoke the notation A : B :: C : D to refer to this sort of relationship. We will assess such relationships using the same methods as considered for word vectors. A typical c"
2020.coling-main.300,C18-1152,0,0.0175007,"on space. The most well-known way to inspect the capabilities of sentence embeddings has been via what has been dubbed probing, i.e., supervised training of models that predict specific linguistic phenomena given embeddings as input. Kiros et al. (2015) evaluated the quality of their embeddings by using them for supervised downstream tasks such as sentiment polarity and question type classification. Adi et al. (2016) attempted to gain more specific insights by predicting word occurrences, word order, and sentence lengths. Bacon and Regier (2018) considered this approach to predict verb tense. Ettinger et al. (2018) trained classifiers for semantic roles and negation detection. Dasgupta et al. (2018) studied the argument sensitivity of the InferSent model by probing with respect to an NLI classification (contradiction vs. entailment). Kann et al. (2019) investigated verb alternation acceptability classifications. Conneau et al. (2018) predicted a wide range of mostly syntactic phenomena such as major syntactic constituents, the depth of the syntactic tree, grammatical number of the subject, and grammatical number of the object. For each probing task, they provide 100,000 training instances. Probing provi"
2020.coling-main.300,W19-0129,0,0.0305915,"(2015) evaluated the quality of their embeddings by using them for supervised downstream tasks such as sentiment polarity and question type classification. Adi et al. (2016) attempted to gain more specific insights by predicting word occurrences, word order, and sentence lengths. Bacon and Regier (2018) considered this approach to predict verb tense. Ettinger et al. (2018) trained classifiers for semantic roles and negation detection. Dasgupta et al. (2018) studied the argument sensitivity of the InferSent model by probing with respect to an NLI classification (contradiction vs. entailment). Kann et al. (2019) investigated verb alternation acceptability classifications. Conneau et al. (2018) predicted a wide range of mostly syntactic phenomena such as major syntactic constituents, the depth of the syntactic tree, grammatical number of the subject, and grammatical number of the object. For each probing task, they provide 100,000 training instances. Probing provides important insights about whether sufficient signals needed for a given downstream task are available if one has sufficient supervision. However, training on 100,000 instances does not reveal whether these signals are genuinely present in"
2020.coling-main.300,W14-1618,0,0.0361083,"0,128 80,601 11,175 7,875 Comparative Opposite Nationality Adjective Plural Verb Conjugation 466 513 205 512 451 108,345 131,328 20,910 130,816 101,475 Entailment Negation Passivization Objective Clause Adjective Clause 673 511 256 563 550 226,128 130,305 32,640 158,203 150,975 Evaluation Metric In word analogy tasks, the offset between word vectors is often used to determine relations between words. For example, in order to solve man is to woman as king is to W , we find a word W for which the corresponding vector is the closest to ~vman − ~vwoman + ~vking . This amounts to optimizing Eq. 1. Levy and Goldberg (2014) studied this in more detail, referring to the aforementioned method as 3CosAdd, while introducing a multiplicative variant called 3CosMul, which often yields better empirical results. Linzen (2016), Schluter (2018), and Nissim et al. (2019) highlighted the significance of excluding the other analogy words in Eq. 1. Given a word analogy problem of the form A : B :: C : D, the standard procedure is to disregard any D that is equal to A, B, or C. This constraint drastically improves the performance of word embedding models on word analogy datasets such as the Google dataset (Mikolov et al., 2013"
2020.coling-main.300,W16-2503,0,0.0217504,"ive Clause 673 511 256 563 550 226,128 130,305 32,640 158,203 150,975 Evaluation Metric In word analogy tasks, the offset between word vectors is often used to determine relations between words. For example, in order to solve man is to woman as king is to W , we find a word W for which the corresponding vector is the closest to ~vman − ~vwoman + ~vking . This amounts to optimizing Eq. 1. Levy and Goldberg (2014) studied this in more detail, referring to the aforementioned method as 3CosAdd, while introducing a multiplicative variant called 3CosMul, which often yields better empirical results. Linzen (2016), Schluter (2018), and Nissim et al. (2019) highlighted the significance of excluding the other analogy words in Eq. 1. Given a word analogy problem of the form A : B :: C : D, the standard procedure is to disregard any D that is equal to A, B, or C. This constraint drastically improves the performance of word embedding models on word analogy datasets such as the Google dataset (Mikolov et al., 2013a), but may also lead to biased results. In our experiments, we consider both 3CosAdd and 3CosMul, and evaluate these both with the additional constraint (3CosAdd, 3CosMul) and without it (3CosAdd-U"
2020.coling-main.300,S14-2001,0,0.0385665,"word with “one&quot;, “a&quot;, or “an&quot;. • Verb Conjugation. We first identify sentences containing an auxiliary verb followed by a verb and then remove the auxiliary verb and replace the verb in the sentence with its inflected form. 3.2 Analogy based on Relationships Between Sentences In addition to our sentence analogy data derived from word analogies, we also create new diagnostic sentence analogy data based on specific forms of relationships between sentences. We start off with sentences extracted from NLI datasets, including SNLI (Bowman et al., 2015), Multi-NLI (Williams et al., 2018b), and SICK (Marelli et al., 2014). 3.2.1 Relationships Entailment. Given two sentence pairs SA , SB and SC , SD , an entailment analogy holds between these two sentence pairs if the respective relationships between SA and SB and between SC and SD are both entailment, as annotated in the NLI data. 2 We rely on SpaCy’s English models for these two tasks. 3392 Table 1: Examples of Lexical Analogy Common Capital Cities All Capital Cities Currencies City in State Man – Woman Comparative Nationality Adjective Opposites Plurals Verb Conjunction SA They traveled to Havana. I’ve never been to Amman. The economy in Japan was great. The"
2020.coling-main.300,N13-1090,0,0.647951,"spaces as well reflect certain kinds of regularities. We propose a number of schemes to induce evaluation data, based on lexical analogy data as well as semantic relationships between sentences. Our experiments consider a wide range of sentence embedding methods, including ones based on BERT-style contextual embeddings. We find that different models differ substantially in their ability to reflect such regularities. 1 Introduction Sentence embeddings are dense vectors that reflect salient semantic properties of a sentence. Similar to how commonly used word embedding methods such as word2vec (Mikolov et al., 2013a) capture semantic relationships between words, sentence embeddings are expected to encode semantic relationships between sentences. A number of different sentence embedding methods have been proposed (cf. Section 2.1 for an overview). In recent years, pretrained language models such as ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), XLNet (Yang et al., 2019), and RoBERTa (Liu et al., 2019) have become the method of choice when encoding text. Thus, such models are also often invoked to represent sentences by means of individual embeddings.1 While important properties of word vector re"
2020.coling-main.300,N18-1202,0,0.296653,"ddings. We find that different models differ substantially in their ability to reflect such regularities. 1 Introduction Sentence embeddings are dense vectors that reflect salient semantic properties of a sentence. Similar to how commonly used word embedding methods such as word2vec (Mikolov et al., 2013a) capture semantic relationships between words, sentence embeddings are expected to encode semantic relationships between sentences. A number of different sentence embedding methods have been proposed (cf. Section 2.1 for an overview). In recent years, pretrained language models such as ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), XLNet (Yang et al., 2019), and RoBERTa (Liu et al., 2019) have become the method of choice when encoding text. Thus, such models are also often invoked to represent sentences by means of individual embeddings.1 While important properties of word vector representations have been studied extensively, far less is known about the properties of sentence vector representations. A particularly prominent aspect of word vector representations induced by methods such as word2vec is that the vector space exhibits certain kinds of regularities. Many of these are of the sort c"
2020.coling-main.300,D19-1250,0,0.0175791,"antic analogies considered in our study is more challenging than capturing the syntactic analogies. Most of the sentence embedding models we tested (except XLNet) excelled at solving syntactic question pairs using the 3CosAdd metric, while few of them perform well on semantic analogy pairs. In particular, contextual embedding-based models appear capable of reflecting syntactic phenomena, but do not appear to yield semantic knowledge at the same level as word embedding models such as GloVe, although they are known to be able to emit world knowledge when evaluated in language modeling settings (Petroni et al., 2019). Table 4: Experimental results over all lexical analogy-based data Representation GloVe DCT (k=0) SkipThought QuickThought InferSentV1 InferSentV2 GenSen USE-DAN USE-Transformer BERT-Base-AVG BERT-Large-AVG XLNet-Base-AVG XLNet-Large-AVG RoBERTa-Base-CLS RoBERTa-Large-AVG SBERT-Base-AVG SBERT-Large-AVG SRoBERTa-Base-AVG SRoBERTa-Large-AVG 3CosAdd-U 0.4092 0.5193 0.1805 0.1337 0.2787 0.3405 0.3756 0.0316 0.0518 0.1537 0.2375 0.0234 0.0105 0.0645 0.0793 0.0977 0.1513 0.0881 0.1073 3CosAdd 0.8189 0.8865 0.6251 0.6318 0.7118 0.8323 0.5366 0.4995 0.5714 0.6471 0.6643 0.4223 0.2397 0.6117 0.6229 0."
2020.coling-main.300,D19-1410,0,0.273151,"prisingly well. The widely used word analogy task that they proposed takes the following form. Given embeddings ~vA , ~vB , ~vC , ~vD for words A, B, C, D for an analogy of the above form, the task consists in identifying the correct word D given A, B, and C. Most commonly, this is This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. ∗ The work was done when Xunjie was a master’s student at Rutgers University 1 Consider e.g. https://github.com/hanxiao/bert-as-service and the work of Reimers et al. (Reimers and Gurevych, 2019). 3389 Proceedings of the 28th International Conference on Computational Linguistics, pages 3389–3400 Barcelona, Spain (Online), December 8-13, 2020 achieved by optimizing argmax sim(~vD , ~vB − ~vA + ~vC ) (1) D∈V s.t. D 6∈ {A, B, C}, where sim(~v1 , ~v2 ) typically denotes cosine similarity between two vectors. This sort of analogy task is one of the most commonly invoked means of assessing the quality of word vector induction techniques. However, little is known about the topology of vector representation spaces for entire sentences. In this paper we fill this gap, considering models with a"
2020.coling-main.300,2020.tacl-1.54,0,0.0225482,"semantically meaningful sentence embeddings that can be compared using cosine similarity. 3390 2.2 Analysing Linguistic Representations Whereas in the field of computer vision, there has been prominent work on understanding what is happening inside popular kinds of models (Zeiler and Fergus, 2014), the latent representations of recent NLP models have long remained impervious and opaque, in the sense that it is not well-understood how they represent the relevant properties of language. While recently there has been substantial research on assessing the capabilities of BERT-like architectures (Rogers et al., 2020), this research for the most part does not shed sufficient light on the topological properties of the representation space. The most well-known way to inspect the capabilities of sentence embeddings has been via what has been dubbed probing, i.e., supervised training of models that predict specific linguistic phenomena given embeddings as input. Kiros et al. (2015) evaluated the quality of their embeddings by using them for supervised downstream tasks such as sentiment polarity and question type classification. Adi et al. (2016) attempted to gain more specific insights by predicting word occur"
2020.coling-main.300,N18-2039,0,0.0185029,"511 256 563 550 226,128 130,305 32,640 158,203 150,975 Evaluation Metric In word analogy tasks, the offset between word vectors is often used to determine relations between words. For example, in order to solve man is to woman as king is to W , we find a word W for which the corresponding vector is the closest to ~vman − ~vwoman + ~vking . This amounts to optimizing Eq. 1. Levy and Goldberg (2014) studied this in more detail, referring to the aforementioned method as 3CosAdd, while introducing a multiplicative variant called 3CosMul, which often yields better empirical results. Linzen (2016), Schluter (2018), and Nissim et al. (2019) highlighted the significance of excluding the other analogy words in Eq. 1. Given a word analogy problem of the form A : B :: C : D, the standard procedure is to disregard any D that is equal to A, B, or C. This constraint drastically improves the performance of word embedding models on word analogy datasets such as the Google dataset (Mikolov et al., 2013a), but may also lead to biased results. In our experiments, we consider both 3CosAdd and 3CosMul, and evaluate these both with the additional constraint (3CosAdd, 3CosMul) and without it (3CosAdd-U, 3CosMul-U), whe"
2020.coling-main.300,N18-1101,0,0.156264,"iven a template “My grandpa makes wooden crafts and arts.&quot;, we can replace the word “grandpa&quot; with any word describing family members such as “grandma”, “father”, and “mother”. However, when the candidate word is a word that describes an occupation, we replace the word “My&quot; in the original template with “The&quot;. When the candidate word is a pronoun such as “he&quot; or “she&quot;, we omit the word “My&quot; from the template. 3.1.2 Syntactic Instances For (morpho-)syntactic questions, we first perform part-of-speech tagging and dependency parsing2 to analyze the structure of the sentences in the MNLI dataset (Williams et al., 2018a) and extract sentences that correspond to a certain structure. Subsequently, we invoke a set of rules to generate new sentences from the original ones. The specific sentence generation schemes invoked to generate the evaluation data for the syntactic categories are as follows. • Comparative. We first find a sentence containing a comparative adjective followed by “than”, and then replace the comparative adjective with its original form and remove the noun or clause after “than” to obtain comparative sentence pairs, as again exemplified in Table 1. • Nationality Adjectives. We create templates"
2020.coling-main.300,P18-2100,1,0.85649,"Missing"
2020.coling-main.300,N18-1049,0,\N,Missing
2020.coling-main.300,W18-5400,0,\N,Missing
2020.coling-main.479,P13-1158,0,0.0316957,"15). Unlike images and speech, text cannot naturally be regarded as a continuous signal that can be perturbed arbitrarily, given its discrete units of semantic meaning. To faithfully retain these features, paraphrases are an intuitive and in some sense ideal way to expand a dataset by incorporating alternative expressions for the existing sentences. However, human rephrasing is too expensive and unrealistic, whereas machine paraphrasing currently has its limitations, e.g., it only works on specific tasks (Wang and Yang, 2015; Hou et al., 2018) and a specific paraphrase corpus may be required (Fader et al., 2013; Qiu et al., 2020). To cope with the generalization issue of data augmentation, a number of universal approaches have been proposed, and we thoroughly evaluate several such universal data augmentation approaches, which can be classified into three types: simple re-sampling, wordlevel transformations, and neural text generation. In the following, we explicitly introduce each method, and for VAE-based neural generation modeling, we additionally introduce some new adaptions to facilitate the task of learning a classification. 2.1 Simple Resampling Simple resampling is the most effortless and con"
2020.coling-main.479,C18-1105,0,0.0770993,"y applied at the audio signal level (Cui et al., 2015; Ko et al., 2015). Unlike images and speech, text cannot naturally be regarded as a continuous signal that can be perturbed arbitrarily, given its discrete units of semantic meaning. To faithfully retain these features, paraphrases are an intuitive and in some sense ideal way to expand a dataset by incorporating alternative expressions for the existing sentences. However, human rephrasing is too expensive and unrealistic, whereas machine paraphrasing currently has its limitations, e.g., it only works on specific tasks (Wang and Yang, 2015; Hou et al., 2018) and a specific paraphrase corpus may be required (Fader et al., 2013; Qiu et al., 2020). To cope with the generalization issue of data augmentation, a number of universal approaches have been proposed, and we thoroughly evaluate several such universal data augmentation approaches, which can be classified into three types: simple re-sampling, wordlevel transformations, and neural text generation. In the following, we explicitly introduce each method, and for VAE-based neural generation modeling, we additionally introduce some new adaptions to facilitate the task of learning a classification. 2"
2020.coling-main.479,D14-1181,0,0.0130314,"Missing"
2020.coling-main.479,P17-4012,0,0.0175989,"adford et al., 2015; Denton et al., 2015) rather than in language tasks. Although a number of attempts regarding text generation have been made (Yu et al., 2017; Su et al., 2018), the training process is known to be extremely unstable and the model requires very careful tuning to find a sweet pot between diversity and quality. In this work, we propose to exploit standard Seq2Seq neural generation as well as VAEbased models that inject additional variation with stochastical latent variables for data augmentation. Specifically, we consider the following models. Seq2Seq text generation: OpenNMT (Klein et al., 2017) is a neural machine translation system that can also be used to generate text (Hou et al., 2018). MASS (Song et al., 2019) is another Seq2Seq neural generative model, including pre-training procedure for a language model within a masked encoder–decoder framework and further fine-tuning process for other downstream tasks, such as text summarization and conversational response generation. Since Seq2Seq models require both source and target texts, this option is usually applied for conversational text inputs and may not be suitable for arbitrary ordinary text classification tasks. VAE models: In"
2020.coling-main.479,N18-2072,0,0.0233437,"1) We refer to the resampling procedure as Undersampling when data samples for each of the majority classes are randomly dropped, thus undersampling such classes, such that the amount of data in all classes becomes the same as the smallest one. (2) Oversampling instead refers to the opposite scenario, i.e., minority classes are oversampled to increase their size. 2.2 Word-level Transformations Word-level transformations can be leveraged to produce new sentences while preserving the semantic features of the original texts to a certain extent. The most intuitive way is synonym replacement (SR) (Kobayashi, 2018), which entails replacing a random word in a data sample with one of its synonyms to construct a new sentence. This can be a promising way of obtaining likely paraphrases of the original sentences, especially for classification tasks (Kobayashi, 2018). Easy Data Augmentation (EDA) is another universal data augmentation technique for NLP (Wei and Zou, 2019), in which one of a set of possible operations, including synonym replacement, random insertion, random swapping, and random deletion, are randomly chosen 5495 and applied to a given sentence. Although the authors show a promising performance"
2020.coling-main.479,Q19-1016,0,0.0606376,"Missing"
2020.coling-main.479,D18-1463,1,0.847019,"train a single conditional VAE (Eq. 2) by taking the class information as an additional input. As for the sampling strategy of the latent code, we also have two options: sampling from the prior distribution or from the posterior distribution for each training data point. The posterior distribution has a lower variance, and thus it usually corresponds to text semantically similar to the training data. On the contrary, samples from the prior distribution exhibit a greater diversity and can often synthesize novel text different from the training corpus (Bowman et al., 2015; Serban et al., 2017; Shen et al., 2018). By combining these variants, we arrive at the following three kinds of models for data augmentation: 1) SentenceVAE: Unconditional VAE + prior sampling; 2) CVAE: conditional VAE + prior sampling; 3) CVAE-posterior (CVAE-p): conditional VAE + posterior sampling 2.4 Sweet Spot Optimization ´ As suggested by Lopez et al. (2013), a blend of oversampling and undersampling might mitigate the imbalance issues in binary classification tasks. Here, we propose that in multi-class classification tasks, there exists a sweet spot with regard to the balance of majority and minority classes. That is to say"
2020.coling-main.479,D19-1054,1,0.811692,"rior latent code space. To maintain the training efficiency, we define p(z) as a standard Normal distribution and parametrize qφ (z |x) as a Gaussian distribution with a diagonal covariance matrix. θ and φ are simultaneously trained to minimize L(θ, φ) by gradient descent. The reparametrization trick (Kingma and Welling, 2013; Rezende et al., 2014) is used to backpropagate gradients through sampled stochastic latent variables. Eq. 1 can also be extended to a label-dependent form. This essentially turns it into a conditional variational autoencoder (CVAE) (Sohn et al., 2015; Zhao et al., 2017; Shen et al., 2019a). 5496 Dataset Category Sizes CoQA Wikipedia 21,127 RACE 21,615 Gutenberg 21,488 CNN 21,819∗ MCTest 7,255† - ICS Class A 62,739 Class B 59,693 Class C 49,084† Class D 77,680∗ Class E 76,716 - NEWS Politics 23,728∗ Wellness 14,292 Entertainment 10,756 Style&Beauty 7,672 Travel 7,482 Parenting 6,945† Average Size # Minority Classes 18,661 1 65,183 3 11,813 4 Table 1: Class distribution of training data in CoQA, ICS, and NEWS datasets, where cates the class with the largest number of data samples and † denotes the smallest. ∗ indiThe objective function is changed accordingly to condition on an"
2020.coling-main.479,D19-1390,1,0.799017,"rior latent code space. To maintain the training efficiency, we define p(z) as a standard Normal distribution and parametrize qφ (z |x) as a Gaussian distribution with a diagonal covariance matrix. θ and φ are simultaneously trained to minimize L(θ, φ) by gradient descent. The reparametrization trick (Kingma and Welling, 2013; Rezende et al., 2014) is used to backpropagate gradients through sampled stochastic latent variables. Eq. 1 can also be extended to a label-dependent form. This essentially turns it into a conditional variational autoencoder (CVAE) (Sohn et al., 2015; Zhao et al., 2017; Shen et al., 2019a). 5496 Dataset Category Sizes CoQA Wikipedia 21,127 RACE 21,615 Gutenberg 21,488 CNN 21,819∗ MCTest 7,255† - ICS Class A 62,739 Class B 59,693 Class C 49,084† Class D 77,680∗ Class E 76,716 - NEWS Politics 23,728∗ Wellness 14,292 Entertainment 10,756 Style&Beauty 7,672 Travel 7,482 Parenting 6,945† Average Size # Minority Classes 18,661 1 65,183 3 11,813 4 Table 1: Class distribution of training data in CoQA, ICS, and NEWS datasets, where cates the class with the largest number of data samples and † denotes the smallest. ∗ indiThe objective function is changed accordingly to condition on an"
2020.coling-main.479,2020.acl-main.641,1,0.816113,"ch one of a set of possible operations, including synonym replacement, random insertion, random swapping, and random deletion, are randomly chosen 5495 and applied to a given sentence. Although the authors show a promising performance gain of EDA over SR, EDA’s effectiveness has only been demonstrated on small datasets. 2.3 Neural Text Generation Text generation is a widely explored yet still very challenging task in NLP. The application of neural networks to text generation has achieved great success in a sizeable number of works (Bowman et al., 2015; Shen et al., 2017; Radford et al., 2019; Shen et al., 2020). This raises the question of whether neural text generation can also serve as a data augmentation technique. The first neural language model was proposed by Bengio et al. (2003), and the superiority of applying neural network models to text generation tasks has been validated in subsequent work, such as recurrent neural network language models (RNNLM) (Mikolov et al., 2010) and long short-term memory (Hochreiter and Schmidhuber, 1997). Compared with conventional language models, generative adversarial nets (GANs) (Goodfellow et al., 2014) and variational autoencoding (VAE) (Kingma and Welling"
2020.coling-main.479,2020.acl-main.634,1,0.83153,"data augmentation technique. The first neural language model was proposed by Bengio et al. (2003), and the superiority of applying neural network models to text generation tasks has been validated in subsequent work, such as recurrent neural network language models (RNNLM) (Mikolov et al., 2010) and long short-term memory (Hochreiter and Schmidhuber, 1997). Compared with conventional language models, generative adversarial nets (GANs) (Goodfellow et al., 2014) and variational autoencoding (VAE) (Kingma and Welling, 2013) along with its variants, are capable of producing more diverse results (Su et al., 2020). Currently, GAN-based models have excelled primarily in image generation (Radford et al., 2015; Denton et al., 2015) rather than in language tasks. Although a number of attempts regarding text generation have been made (Yu et al., 2017; Su et al., 2018), the training process is known to be extremely unstable and the model requires very careful tuning to find a sweet pot between diversity and quality. In this work, we propose to exploit standard Seq2Seq neural generation as well as VAEbased models that inject additional variation with stochastical latent variables for data augmentation. Specif"
2020.coling-main.479,2020.acl-main.500,0,0.0728731,"Missing"
2020.coling-main.479,D15-1306,0,0.0299071,"models, and is mainly applied at the audio signal level (Cui et al., 2015; Ko et al., 2015). Unlike images and speech, text cannot naturally be regarded as a continuous signal that can be perturbed arbitrarily, given its discrete units of semantic meaning. To faithfully retain these features, paraphrases are an intuitive and in some sense ideal way to expand a dataset by incorporating alternative expressions for the existing sentences. However, human rephrasing is too expensive and unrealistic, whereas machine paraphrasing currently has its limitations, e.g., it only works on specific tasks (Wang and Yang, 2015; Hou et al., 2018) and a specific paraphrase corpus may be required (Fader et al., 2013; Qiu et al., 2020). To cope with the generalization issue of data augmentation, a number of universal approaches have been proposed, and we thoroughly evaluate several such universal data augmentation approaches, which can be classified into three types: simple re-sampling, wordlevel transformations, and neural text generation. In the following, we explicitly introduce each method, and for VAE-based neural generation modeling, we additionally introduce some new adaptions to facilitate the task of learning"
2020.coling-main.479,D19-1670,0,0.0168172,"eir size. 2.2 Word-level Transformations Word-level transformations can be leveraged to produce new sentences while preserving the semantic features of the original texts to a certain extent. The most intuitive way is synonym replacement (SR) (Kobayashi, 2018), which entails replacing a random word in a data sample with one of its synonyms to construct a new sentence. This can be a promising way of obtaining likely paraphrases of the original sentences, especially for classification tasks (Kobayashi, 2018). Easy Data Augmentation (EDA) is another universal data augmentation technique for NLP (Wei and Zou, 2019), in which one of a set of possible operations, including synonym replacement, random insertion, random swapping, and random deletion, are randomly chosen 5495 and applied to a given sentence. Although the authors show a promising performance gain of EDA over SR, EDA’s effectiveness has only been demonstrated on small datasets. 2.3 Neural Text Generation Text generation is a widely explored yet still very challenging task in NLP. The application of neural networks to text generation has achieved great success in a sizeable number of works (Bowman et al., 2015; Shen et al., 2017; Radford et al."
2020.coling-main.479,P17-1061,0,0.0268934,"text into its posterior latent code space. To maintain the training efficiency, we define p(z) as a standard Normal distribution and parametrize qφ (z |x) as a Gaussian distribution with a diagonal covariance matrix. θ and φ are simultaneously trained to minimize L(θ, φ) by gradient descent. The reparametrization trick (Kingma and Welling, 2013; Rezende et al., 2014) is used to backpropagate gradients through sampled stochastic latent variables. Eq. 1 can also be extended to a label-dependent form. This essentially turns it into a conditional variational autoencoder (CVAE) (Sohn et al., 2015; Zhao et al., 2017; Shen et al., 2019a). 5496 Dataset Category Sizes CoQA Wikipedia 21,127 RACE 21,615 Gutenberg 21,488 CNN 21,819∗ MCTest 7,255† - ICS Class A 62,739 Class B 59,693 Class C 49,084† Class D 77,680∗ Class E 76,716 - NEWS Politics 23,728∗ Wellness 14,292 Entertainment 10,756 Style&Beauty 7,672 Travel 7,482 Parenting 6,945† Average Size # Minority Classes 18,661 1 65,183 3 11,813 4 Table 1: Class distribution of training data in CoQA, ICS, and NEWS datasets, where cates the class with the largest number of data samples and † denotes the smallest. ∗ indiThe objective function is changed accordingly"
2020.coling-main.517,I17-1051,0,0.0262679,"emotionally tagged text crawled from specific Web sites. Raji and de Melo (2020) revealed that unsupervised distributional semantics can outperform such supervised techniques. 2.2 Cross-Lingual Emotion Lexicon Induction Leveau et al. (2012) showed that word translations across languages are strongly correlated in emotion. As machine translation gradually increased in accuracy, inducing affect-related resources cross-lingually become more feasible (Mihalcea et al., 2007). Lexicons for sentiment polarity have been induced crosslingually using various forms of supervision (Chen and Skiena, 2014; Abdalla and Hirst, 2017; Barnes et al., 2018; Dong and de Melo, 2018b; Dong and de Melo, 2018a). In terms of emotion, Buechel et al. (2020) induced fine-grained emotion lexicons for the 91 languages for which Google Translate was available. However, machine translation tools are limited by the amount of available training data. In recent years, induction has thus often been achieved by means of cross-lingual word embeddings. While numerous approaches for bilingual embedding training (Gouws and Søgaard, 2015) have been explored, it can be more convenient to draw on potentially larger amounts of monolingual data for e"
2020.coling-main.517,P18-1073,0,0.24384,"enient to draw on potentially larger amounts of monolingual data for embedding training and then achieve a post-hoc alignment of the embedding spaces. Mikolov et al. (2013) showed that word vectors in different languages can often be aligned with reasonably high accuracy using simple linear transformations. Xing et al. (2015) showed that enforcing orthogonality on the linear transformation matrix may result in better translation accuracy. There are now also several unsupervised alignment algorithms seeking to identify orthogonal transformations of embedding vector spaces (Lample et al., 2018; Artetxe et al., 2018; Grave et al., 2019). In this paper, we investigate such approaches for cross-lingual emotion lexicon induction. Work so far has been limited in at least one of the following ways: 1) polarity lexicon induction as opposed to fine-grained emotion lexicons, 2) induction dependent on supervised data, or 3) unsupervised induction but with languages for which resources like Google Translate or pre-trained fastText embeddings are available. In the following sections, we present a method of emotion lexicon induction that works with resource-poor languages for which such tooling is unavailable. 3 Pro"
2020.coling-main.517,P18-1231,0,0.0183278,"rawled from specific Web sites. Raji and de Melo (2020) revealed that unsupervised distributional semantics can outperform such supervised techniques. 2.2 Cross-Lingual Emotion Lexicon Induction Leveau et al. (2012) showed that word translations across languages are strongly correlated in emotion. As machine translation gradually increased in accuracy, inducing affect-related resources cross-lingually become more feasible (Mihalcea et al., 2007). Lexicons for sentiment polarity have been induced crosslingually using various forms of supervision (Chen and Skiena, 2014; Abdalla and Hirst, 2017; Barnes et al., 2018; Dong and de Melo, 2018b; Dong and de Melo, 2018a). In terms of emotion, Buechel et al. (2020) induced fine-grained emotion lexicons for the 91 languages for which Google Translate was available. However, machine translation tools are limited by the amount of available training data. In recent years, induction has thus often been achieved by means of cross-lingual word embeddings. While numerous approaches for bilingual embedding training (Gouws and Søgaard, 2015) have been explored, it can be more convenient to draw on potentially larger amounts of monolingual data for embedding training and"
2020.coling-main.517,2020.acl-main.112,0,0.0430959,"Missing"
2020.coling-main.517,P14-2063,0,0.0221114,"ical measures based on emotionally tagged text crawled from specific Web sites. Raji and de Melo (2020) revealed that unsupervised distributional semantics can outperform such supervised techniques. 2.2 Cross-Lingual Emotion Lexicon Induction Leveau et al. (2012) showed that word translations across languages are strongly correlated in emotion. As machine translation gradually increased in accuracy, inducing affect-related resources cross-lingually become more feasible (Mihalcea et al., 2007). Lexicons for sentiment polarity have been induced crosslingually using various forms of supervision (Chen and Skiena, 2014; Abdalla and Hirst, 2017; Barnes et al., 2018; Dong and de Melo, 2018b; Dong and de Melo, 2018a). In terms of emotion, Buechel et al. (2020) induced fine-grained emotion lexicons for the 91 languages for which Google Translate was available. However, machine translation tools are limited by the amount of available training data. In recent years, induction has thus often been achieved by means of cross-lingual word embeddings. While numerous approaches for bilingual embedding training (Gouws and Søgaard, 2015) have been explored, it can be more convenient to draw on potentially larger amounts"
2020.coling-main.517,P18-1235,1,0.882874,"Missing"
2020.coling-main.517,N15-1157,0,0.0175712,"sentiment polarity have been induced crosslingually using various forms of supervision (Chen and Skiena, 2014; Abdalla and Hirst, 2017; Barnes et al., 2018; Dong and de Melo, 2018b; Dong and de Melo, 2018a). In terms of emotion, Buechel et al. (2020) induced fine-grained emotion lexicons for the 91 languages for which Google Translate was available. However, machine translation tools are limited by the amount of available training data. In recent years, induction has thus often been achieved by means of cross-lingual word embeddings. While numerous approaches for bilingual embedding training (Gouws and Søgaard, 2015) have been explored, it can be more convenient to draw on potentially larger amounts of monolingual data for embedding training and then achieve a post-hoc alignment of the embedding spaces. Mikolov et al. (2013) showed that word vectors in different languages can often be aligned with reasonably high accuracy using simple linear transformations. Xing et al. (2015) showed that enforcing orthogonality on the linear transformation matrix may result in better translation accuracy. There are now also several unsupervised alignment algorithms seeking to identify orthogonal transformations of embedd"
2020.coling-main.517,N16-1095,0,0.0269146,"-truth emotion lexicons are typically constructed by manually annotating words with associated emotions. Bradley et al. (1999) aggregated results of a questionnaire to create an emotion lexicon with ratings for the PAD model, Warriner et al. (2013) compiled a similar dataset with larger coverage, and Shoeb and de Melo (2020) solicited emotion ratings for emojis. Crowd-sourcing platforms such as Amazon’s Mechanical Turk can be used to expedite the annotation process (Mohammad and Turney, 2013), with techniques such as best-worst scaling to better account for the variance between crowd workers (Kiritchenko and Mohammad, 2016). Apart from manual compilation, different strategies can be invoked to construct monolingual emotion lexicons automatically. For instance, the DepecheMood lexicon (Staiano and Guerini, 2014) was derived using statistical measures based on emotionally tagged text crawled from specific Web sites. Raji and de Melo (2020) revealed that unsupervised distributional semantics can outperform such supervised techniques. 2.2 Cross-Lingual Emotion Lexicon Induction Leveau et al. (2012) showed that word translations across languages are strongly correlated in emotion. As machine translation gradually inc"
2020.coling-main.517,P15-1027,0,0.0777547,"Missing"
2020.coling-main.517,P07-1123,0,0.0598746,"l emotion lexicons automatically. For instance, the DepecheMood lexicon (Staiano and Guerini, 2014) was derived using statistical measures based on emotionally tagged text crawled from specific Web sites. Raji and de Melo (2020) revealed that unsupervised distributional semantics can outperform such supervised techniques. 2.2 Cross-Lingual Emotion Lexicon Induction Leveau et al. (2012) showed that word translations across languages are strongly correlated in emotion. As machine translation gradually increased in accuracy, inducing affect-related resources cross-lingually become more feasible (Mihalcea et al., 2007). Lexicons for sentiment polarity have been induced crosslingually using various forms of supervision (Chen and Skiena, 2014; Abdalla and Hirst, 2017; Barnes et al., 2018; Dong and de Melo, 2018b; Dong and de Melo, 2018a). In terms of emotion, Buechel et al. (2020) induced fine-grained emotion lexicons for the 91 languages for which Google Translate was available. However, machine translation tools are limited by the amount of available training data. In recent years, induction has thus often been achieved by means of cross-lingual word embeddings. While numerous approaches for bilingual embed"
2020.coling-main.517,L18-1027,0,0.23564,"prise, anticipation, trust, and joy. There have been efforts to create emotion lexicons, where each word is assigned either scores or discrete classes reflecting the associated emotions. Such lexicons are useful in emotional analyses of product reviews, literary texts, or posts on social media, inter alia. Bradley et al. (1999) solicited human affective norm ratings to create such a dataset for English based on the PAD model. Mohammad and Turney (2013) relied on crowdsourcing to annotate words with Plutchik’s 8 basic emotions, providing binary labels. The recent NRC Emotion Intensity Lexicon (Mohammad, 2018) reconciles the notion of discrete emotions, corresponding to commonly invoked emotion names, with the benefits of continuous scoring in accounting for degrees of emotion intensity. Again relying on crowdsourcing, the lexicon provides intensity scores for Plutchik’s eight basic emotions. Affective norm ratings have as well been procured for certain other languages. An alternative route is to draw on automated techniques such as machine translation, as has been done for the NRC Emotion Intensity lexicon, where the English words are translated to other languages using Google Translate while reta"
2020.coling-main.517,2020.emnlp-main.720,1,0.537199,"Missing"
2020.coling-main.517,P14-2070,0,0.182396,"th ratings for the PAD model, Warriner et al. (2013) compiled a similar dataset with larger coverage, and Shoeb and de Melo (2020) solicited emotion ratings for emojis. Crowd-sourcing platforms such as Amazon’s Mechanical Turk can be used to expedite the annotation process (Mohammad and Turney, 2013), with techniques such as best-worst scaling to better account for the variance between crowd workers (Kiritchenko and Mohammad, 2016). Apart from manual compilation, different strategies can be invoked to construct monolingual emotion lexicons automatically. For instance, the DepecheMood lexicon (Staiano and Guerini, 2014) was derived using statistical measures based on emotionally tagged text crawled from specific Web sites. Raji and de Melo (2020) revealed that unsupervised distributional semantics can outperform such supervised techniques. 2.2 Cross-Lingual Emotion Lexicon Induction Leveau et al. (2012) showed that word translations across languages are strongly correlated in emotion. As machine translation gradually increased in accuracy, inducing affect-related resources cross-lingually become more feasible (Mihalcea et al., 2007). Lexicons for sentiment polarity have been induced crosslingually using vari"
2020.coling-main.517,P19-1300,0,0.0413478,"mbeddings (see Section 6.2 for details). The text in each monolingual corpus is preprocessed to eliminate all Unicode punctuation and converted to lower case. We obtain two embedding matrices XS and XT for the source and target languages, respectively, with corresponding vocabularies VS and VT . Our goal is to induce a single cross-lingual embedding matrix XC that covers both VS and VT in a single space. For this, we explore three algorithms to align XS and XT : Wasserstein-Procrustes (Grave et al., 2019), Unsupervised Orthogonal Refinement (Artetxe et al., 2018), and a neural language model (Wada et al., 2019). We also consider modifications of the latter two algorithms and evaluate these modified variants alongside the original ones. Note that the neural language model does not require word embeddings to have already been trained on monolingual corpora, as it jointly trains on two corpora to produce embeddings that already reside in a common space. Thus, only the preprocessing steps are performed for it. In the following, we describe each of these techniques in more detail. 4.2 Wasserstein-Procrustes Given two matrices XS and XT containing word embeddings in two different languages, the Wasserstei"
2020.coling-main.517,N15-1104,0,0.0228252,"slation tools are limited by the amount of available training data. In recent years, induction has thus often been achieved by means of cross-lingual word embeddings. While numerous approaches for bilingual embedding training (Gouws and Søgaard, 2015) have been explored, it can be more convenient to draw on potentially larger amounts of monolingual data for embedding training and then achieve a post-hoc alignment of the embedding spaces. Mikolov et al. (2013) showed that word vectors in different languages can often be aligned with reasonably high accuracy using simple linear transformations. Xing et al. (2015) showed that enforcing orthogonality on the linear transformation matrix may result in better translation accuracy. There are now also several unsupervised alignment algorithms seeking to identify orthogonal transformations of embedding vector spaces (Lample et al., 2018; Artetxe et al., 2018; Grave et al., 2019). In this paper, we investigate such approaches for cross-lingual emotion lexicon induction. Work so far has been limited in at least one of the following ways: 1) polarity lexicon induction as opposed to fine-grained emotion lexicons, 2) induction dependent on supervised data, or 3) u"
2020.coling-main.578,baccianella-etal-2010-sentiwordnet,0,0.426826,"edia analytics (Rosenthal et al., 2017; Wang et al., 2019; Shoeb et al., 2019), marketing and customer support (Gamon, 2004), as well as recommendation (Yang et al., 2013). Apart from machine learning-driven systems (Pang et al., 2002; Socher et al., 2013; Kalchbrenner et al., 2014, inter alia), which require supervision using labeled training data, there are also lexical resource-driven systems that exploit sentiment lexicons and can be run out-of-the-box without the need for any labeled training data. Well-known sentiment lexicons include the Hu and Liu (2004) Opinion Lexicon, SentiWordNet (Baccianella et al., 2010), LIWC (Pennebaker et al., 2001), and VADER (Hutto and Gilbert, 2014). There are numerous techniques for lexicon-driven sentiment analysis (Taboada et al., 2011), SentiStrength (Thelwall et al., 2010) being an example of a more modern lexicon-driven sentiment analysis system. Sentiment lexicons can also be used to bootstrap domain-specific supervised sentiment analysis models (Mudinas et al., 2018). A sentiment lexicon is a resource that, for a given word (form) w, provides an annotation label lw describing its overall sentiment polarity. Some lexicons merely provide labels in {positive, negat"
2020.coling-main.578,Q17-1010,0,0.0492487,"prediction scores in the corpus as a binary threshold. Figure 5 plots the results of this evaluation. We observe that in almost all the domains, the domainspecific lexicons (plotted as bars) outperformed the domain-independent lexicons (horizontal lines). As expected, the results are particularly strong for the domains that are closest to the movie domain. 3.3.2 Cross-Lingual Word-Level Sentiment Prediction Finally, we evaluated the performance of predicted domain-specific lexicons on cross-lingual word-level sentiment score prediction. For this, cross-lingually aligned fastText word vectors (Bojanowski et al., 2017; Joulin et al., 2018) for four languages (English, Spanish, French, and Polish) were used as word embeddings. As the ground truth, we considered the mean sentiment scores of 7,504 English tokens from VADER, as well as the mean human ratings of valence for 875 Spanish words (Hinojosa et al., 2015), 1,031 French words (Monnier and Syssau, 2013), and 1,586 Polish words (Imbir, 2014). Any words from the ground truth data that are missing in the aligned fastText word vectors are eliminated. The sentiment prediction model was trained on 24 different domains separately, as described in Section 3.2.2"
2020.coling-main.578,L16-1007,0,0.0271922,"gorithms that aim to increase the coverage of an individual sentiment lexicon. Often, these start from seeds and then rely on graph-based algorithms to gather additional data, as for instance explored by Kim and Hovy (2004) and in the approach used to induce SentiWordNet (Baccianella et al., 2010). The extension can also be based on vector representations of words, as proposed in the Densifier approach (Rothe et al., 2016). Such work has shown that dense word vectors trained on large amounts of data harbour signals that are useful for sentiment analysis. Instead of a regular supervised setup, Castellucci et al. (2016) used distant supervision based on emoticons to obtain sentiment labels for entire sentences. They then trained a sentiment model on sentence vector representations sharing a common representation space with word vectors, which allowed them to apply the trained model to predict word-level scores. However, techniques such as the above mostly have not targeted domain-specific sentiment lexicons. The SocialSent project (Hamilton et al., 2016) induced Reddit community-specific sentiment lexicons without labeled corpora. Their SentProp approach constructs a graph of words and then considers random"
2020.coling-main.578,Q13-1023,1,0.866092,"Missing"
2020.coling-main.578,P18-1235,1,0.872241,"Missing"
2020.coling-main.578,C04-1121,0,0.215003,"to overcome the scarcity of the original seed data. Our analysis shows substantial differences between domains, which make domain-specific sentiment lexicons a promising form of lexical resource in downstream tasks, and the predicted lexicons indeed perform effectively on tasks such as review classification and cross-lingual word sentiment prediction. 1 Introduction Sentiment analysis is among the most prominent forms of natural language processing, with applications such as social media analytics (Rosenthal et al., 2017; Wang et al., 2019; Shoeb et al., 2019), marketing and customer support (Gamon, 2004), as well as recommendation (Yang et al., 2013). Apart from machine learning-driven systems (Pang et al., 2002; Socher et al., 2013; Kalchbrenner et al., 2014, inter alia), which require supervision using labeled training data, there are also lexical resource-driven systems that exploit sentiment lexicons and can be run out-of-the-box without the need for any labeled training data. Well-known sentiment lexicons include the Hu and Liu (2004) Opinion Lexicon, SentiWordNet (Baccianella et al., 2010), LIWC (Pennebaker et al., 2001), and VADER (Hutto and Gilbert, 2014). There are numerous technique"
2020.coling-main.578,D16-1057,0,0.0182539,"n that dense word vectors trained on large amounts of data harbour signals that are useful for sentiment analysis. Instead of a regular supervised setup, Castellucci et al. (2016) used distant supervision based on emoticons to obtain sentiment labels for entire sentences. They then trained a sentiment model on sentence vector representations sharing a common representation space with word vectors, which allowed them to apply the trained model to predict word-level scores. However, techniques such as the above mostly have not targeted domain-specific sentiment lexicons. The SocialSent project (Hamilton et al., 2016) induced Reddit community-specific sentiment lexicons without labeled corpora. Their SentProp approach constructs a graph of words and then considers random walks emanating from a small set of seed words with known sentiment polarity. The polarity scores are based on the frequency of random walk visits and the polarity of the seed word from which those random walks started. While Reddit communities provide substantial diversity, the language used in Reddit posts differs quite substantially from the kinds of language one encounters in reviews. Kreutz and Daelemans (2018) adopted SentProp to cus"
2020.coling-main.578,D18-1330,0,0.0233454,"Missing"
2020.coling-main.578,P14-1062,0,0.0388981,"ent lexicons a promising form of lexical resource in downstream tasks, and the predicted lexicons indeed perform effectively on tasks such as review classification and cross-lingual word sentiment prediction. 1 Introduction Sentiment analysis is among the most prominent forms of natural language processing, with applications such as social media analytics (Rosenthal et al., 2017; Wang et al., 2019; Shoeb et al., 2019), marketing and customer support (Gamon, 2004), as well as recommendation (Yang et al., 2013). Apart from machine learning-driven systems (Pang et al., 2002; Socher et al., 2013; Kalchbrenner et al., 2014, inter alia), which require supervision using labeled training data, there are also lexical resource-driven systems that exploit sentiment lexicons and can be run out-of-the-box without the need for any labeled training data. Well-known sentiment lexicons include the Hu and Liu (2004) Opinion Lexicon, SentiWordNet (Baccianella et al., 2010), LIWC (Pennebaker et al., 2001), and VADER (Hutto and Gilbert, 2014). There are numerous techniques for lexicon-driven sentiment analysis (Taboada et al., 2011), SentiStrength (Thelwall et al., 2010) being an example of a more modern lexicon-driven sentime"
2020.coling-main.578,C04-1200,0,0.319585,"fication. The solid and dotted horizontal lines represent the baseline from GloVe-based induction from VADER and fast-based induction from VADER respectively. 4 Related Work The traditional way of obtaining sentiment lexicons has been to build them manually, relying either on experts or invoking crowd-sourcing. A prominent example is the Hu and Liu (2004) Opinion Lexicon. There are numerous algorithms that aim to increase the coverage of an individual sentiment lexicon. Often, these start from seeds and then rely on graph-based algorithms to gather additional data, as for instance explored by Kim and Hovy (2004) and in the approach used to induce SentiWordNet (Baccianella et al., 2010). The extension can also be based on vector representations of words, as proposed in the Densifier approach (Rothe et al., 2016). Such work has shown that dense word vectors trained on large amounts of data harbour signals that are useful for sentiment analysis. Instead of a regular supervised setup, Castellucci et al. (2016) used distant supervision based on emoticons to obtain sentiment labels for entire sentences. They then trained a sentiment model on sentence vector representations sharing a common representation s"
2020.coling-main.578,C18-1090,0,0.0151363,"ons. The SocialSent project (Hamilton et al., 2016) induced Reddit community-specific sentiment lexicons without labeled corpora. Their SentProp approach constructs a graph of words and then considers random walks emanating from a small set of seed words with known sentiment polarity. The polarity scores are based on the frequency of random walk visits and the polarity of the seed word from which those random walks started. While Reddit communities provide substantial diversity, the language used in Reddit posts differs quite substantially from the kinds of language one encounters in reviews. Kreutz and Daelemans (2018) adopted SentProp to customize an existing general-purpose sentiment lexicon for use in one specific domain. We instead focus on inducing a number of domain-specific lexicons to obtain a lexical resource that is more suitable for typical sentiment analysis use cases. The approach by Labille et al. (2017) also starts from labeled data for consumer products. It infers word polarity scores directly based on posterior probabilities and inverse document frequencies. However, such scores are limited to words that occur in the labeled training data. Instead, in our work, we draw on word vectors to gr"
2020.coling-main.578,P11-1015,0,0.0925124,"e results, along with the high correlation of the predictions in Section 3.2.2, corroborate that our domain-specific lexicons capture human-like sentiment toward different domains. 3.3 Applications of Induced Lexicons Finally, we assessed the performance of the induced domain-specific sentiment lexicons on downstream tasks such as review sentiment classification and cross-lingual word-level sentiment prediction. 3.3.1 Unsupervised Review Sentiment Classification Here, we used our predicted domain-specific lexicon to perform sentiment classification on the IMDB movie review dataset compiled by Maas et al. (2011). The test portion of movie review data set has 25,000 reviews in total, among which 12,500 are positive and 12,500 are negative. As for the word embeddings, in this evaluation, along with GloVe, we also used fastText (Mikolov et al., 2018) to obtain a second set of domain-specific lexicons for comparison. As baselines, along with the raw VADER lexicon, two further domain-independent lexicons were derived by using the VADER lexicon as seed data and invoking GloVe and fastText to expand their coverage using our neural expansion approach. 6582 Figure 4: (a) Heat-map of cross-correlation values f"
2020.coling-main.578,L18-1008,0,0.0213125,"d the performance of the induced domain-specific sentiment lexicons on downstream tasks such as review sentiment classification and cross-lingual word-level sentiment prediction. 3.3.1 Unsupervised Review Sentiment Classification Here, we used our predicted domain-specific lexicon to perform sentiment classification on the IMDB movie review dataset compiled by Maas et al. (2011). The test portion of movie review data set has 25,000 reviews in total, among which 12,500 are positive and 12,500 are negative. As for the word embeddings, in this evaluation, along with GloVe, we also used fastText (Mikolov et al., 2018) to obtain a second set of domain-specific lexicons for comparison. As baselines, along with the raw VADER lexicon, two further domain-independent lexicons were derived by using the VADER lexicon as seed data and invoking GloVe and fastText to expand their coverage using our neural expansion approach. 6582 Figure 4: (a) Heat-map of cross-correlation values for predicted sentiment among all domains, (b) 2D representation of 24 domains according to the predicted domain-specific sentiment lexicons. For unsupervised prediction given a document x in the test set, we simply compute a prediction scor"
2020.coling-main.578,Q18-1020,0,0.0941304,"xploit sentiment lexicons and can be run out-of-the-box without the need for any labeled training data. Well-known sentiment lexicons include the Hu and Liu (2004) Opinion Lexicon, SentiWordNet (Baccianella et al., 2010), LIWC (Pennebaker et al., 2001), and VADER (Hutto and Gilbert, 2014). There are numerous techniques for lexicon-driven sentiment analysis (Taboada et al., 2011), SentiStrength (Thelwall et al., 2010) being an example of a more modern lexicon-driven sentiment analysis system. Sentiment lexicons can also be used to bootstrap domain-specific supervised sentiment analysis models (Mudinas et al., 2018). A sentiment lexicon is a resource that, for a given word (form) w, provides an annotation label lw describing its overall sentiment polarity. Some lexicons merely provide labels in {positive, negative} or {positive, neutral, negative}. Others offer more informative intensity scores to account for the fact that some words are more negative or positive than others. For example, an emphatic word such as spectacular is generally considered stronger than a simple good (de Melo and Bansal, 2013). Such scores could be in the range [−1, 1], with −1 denoting the most negative sentiment polarity, wher"
2020.coling-main.578,W02-1011,0,0.0397767,"mains, which make domain-specific sentiment lexicons a promising form of lexical resource in downstream tasks, and the predicted lexicons indeed perform effectively on tasks such as review classification and cross-lingual word sentiment prediction. 1 Introduction Sentiment analysis is among the most prominent forms of natural language processing, with applications such as social media analytics (Rosenthal et al., 2017; Wang et al., 2019; Shoeb et al., 2019), marketing and customer support (Gamon, 2004), as well as recommendation (Yang et al., 2013). Apart from machine learning-driven systems (Pang et al., 2002; Socher et al., 2013; Kalchbrenner et al., 2014, inter alia), which require supervision using labeled training data, there are also lexical resource-driven systems that exploit sentiment lexicons and can be run out-of-the-box without the need for any labeled training data. Well-known sentiment lexicons include the Hu and Liu (2004) Opinion Lexicon, SentiWordNet (Baccianella et al., 2010), LIWC (Pennebaker et al., 2001), and VADER (Hutto and Gilbert, 2014). There are numerous techniques for lexicon-driven sentiment analysis (Taboada et al., 2011), SentiStrength (Thelwall et al., 2010) being an"
2020.coling-main.578,D14-1162,0,0.0943287,"reliability of induced seed scores may be low if a word was infrequent in the respective domainspecific labeled corpus Di . Machine learning based on large-scale distributional semantics as reflected in word vector representations can allow us to overcome the above shortcomings and enable the sentiment scoring of millions of words. Specifically, for each domain i, we train a model φi (vw ) ∈ R to predict a real-valued domainspecific sentiment polarity score for a word w based on its generic vector representation vw as input. Word vectors trained on large amounts of data (Mikolov et al., 2013; Pennington et al., 2014) capture important aspects of lexical semantics. Although they are typically trained based on distributional word co-occurrence information, they have also been found to reveal sentiment signals (Rothe et al., 2016). As the machine learning component, we consider deep neural regression networks as our prediction models φi (vw ). The architecture is described in Table 1. In particular, we incorporate several hidden layers, but add batch normalization and dropout for regularization. Additionally, we found that initializing the output layer of our model to scale the softmax scores to the sentimen"
2020.coling-main.578,S17-2088,0,0.0286739,"tial word intensity scores, and then train new deep models based on word vector representations to overcome the scarcity of the original seed data. Our analysis shows substantial differences between domains, which make domain-specific sentiment lexicons a promising form of lexical resource in downstream tasks, and the predicted lexicons indeed perform effectively on tasks such as review classification and cross-lingual word sentiment prediction. 1 Introduction Sentiment analysis is among the most prominent forms of natural language processing, with applications such as social media analytics (Rosenthal et al., 2017; Wang et al., 2019; Shoeb et al., 2019), marketing and customer support (Gamon, 2004), as well as recommendation (Yang et al., 2013). Apart from machine learning-driven systems (Pang et al., 2002; Socher et al., 2013; Kalchbrenner et al., 2014, inter alia), which require supervision using labeled training data, there are also lexical resource-driven systems that exploit sentiment lexicons and can be run out-of-the-box without the need for any labeled training data. Well-known sentiment lexicons include the Hu and Liu (2004) Opinion Lexicon, SentiWordNet (Baccianella et al., 2010), LIWC (Penne"
2020.coling-main.578,N16-1091,0,0.0409672,"Missing"
2020.coling-main.578,R19-1126,1,0.892681,"Missing"
2020.coling-main.578,D13-1170,0,0.0101613,"omain-specific sentiment lexicons a promising form of lexical resource in downstream tasks, and the predicted lexicons indeed perform effectively on tasks such as review classification and cross-lingual word sentiment prediction. 1 Introduction Sentiment analysis is among the most prominent forms of natural language processing, with applications such as social media analytics (Rosenthal et al., 2017; Wang et al., 2019; Shoeb et al., 2019), marketing and customer support (Gamon, 2004), as well as recommendation (Yang et al., 2013). Apart from machine learning-driven systems (Pang et al., 2002; Socher et al., 2013; Kalchbrenner et al., 2014, inter alia), which require supervision using labeled training data, there are also lexical resource-driven systems that exploit sentiment lexicons and can be run out-of-the-box without the need for any labeled training data. Well-known sentiment lexicons include the Hu and Liu (2004) Opinion Lexicon, SentiWordNet (Baccianella et al., 2010), LIWC (Pennebaker et al., 2001), and VADER (Hutto and Gilbert, 2014). There are numerous techniques for lexicon-driven sentiment analysis (Taboada et al., 2011), SentiStrength (Thelwall et al., 2010) being an example of a more mo"
2020.coling-main.578,J11-2001,0,0.10707,"2013). Apart from machine learning-driven systems (Pang et al., 2002; Socher et al., 2013; Kalchbrenner et al., 2014, inter alia), which require supervision using labeled training data, there are also lexical resource-driven systems that exploit sentiment lexicons and can be run out-of-the-box without the need for any labeled training data. Well-known sentiment lexicons include the Hu and Liu (2004) Opinion Lexicon, SentiWordNet (Baccianella et al., 2010), LIWC (Pennebaker et al., 2001), and VADER (Hutto and Gilbert, 2014). There are numerous techniques for lexicon-driven sentiment analysis (Taboada et al., 2011), SentiStrength (Thelwall et al., 2010) being an example of a more modern lexicon-driven sentiment analysis system. Sentiment lexicons can also be used to bootstrap domain-specific supervised sentiment analysis models (Mudinas et al., 2018). A sentiment lexicon is a resource that, for a given word (form) w, provides an annotation label lw describing its overall sentiment polarity. Some lexicons merely provide labels in {positive, negative} or {positive, neutral, negative}. Others offer more informative intensity scores to account for the fact that some words are more negative or positive than"
2020.emnlp-main.720,E17-2017,0,0.0136484,"Derks et al., 2008), as also reflected in their name, a portmanteau of the words emotion and icon. This mandates additional analysis of the nexus between emojis and emotion. Past work has compiled a list of sentiment polarity scores for a set of emojis (Novak et al., 2015). Rakhmetullina et al. (2018) categorized a set of 15 emojis into 4 different emotion classes, while Li et al. (2019) used a lexicon-based heuristic to compare connections between emojis and emotions in social media data. Several studies have explored the linguistic connection between words and emojis (Cappallo et al., 2019; Barbieri et al., 2017; Na’aman et al., 2017; Shoeb et al., 2019). However, previous work has not assessed to what extent humans associate particular emotions with different emojis. In this work, we present EmoTag1200, a dataset of human ratings of association for a set of 150 popular emojis with regard to 8 different emotions. Each of the resulting 1,200 pairs of emojis and emotions has been annotated by 9 human raters on a 5-point scale. The purpose of this endeavor is to measure the degree of emotion that people associate with the use of a given emoji in written expression. As the set of emotions, we consider th"
2020.emnlp-main.720,L16-1626,0,0.116622,"es declared the Face with Tears of Joy emoji its Word of the Year 2015. Emojis may also be useful as a more instantaneously and widely recognized form of communicating degrees of satisfaction. Kaye et al. (2017) go as far as suggesting them for consideration as possible alternatives to regular Likert scales. Emoji Semantics. The MIT DeepMoji project (Felbo et al., 2017) developed a model that recommends emojis given a natural language sentence as input. A deep neural architecture was trained on a collection of 1.2B tweets to learn the sentiment, emotions, and the use of sarcasm in short text. Barbieri et al. (2016b) proposed a method to learn vector space embeddings of emojis using the standard word2vec skip-gram approach, applied to a large collection of tweets. In contrast, Eisner et al. (2016) attempted to learn vector embeddings of emojis based on their short descriptions in the Unicode standard. EmojiNet (Wijeratne et al., 2017) provides a sense inventory to distinguish different senses of an emoji, drawing on Web-crawled emoji definitions and connecting them to word senses from a lexical resource, along with vector representations of context words. The first paper to thoroughly investigate the se"
2020.emnlp-main.720,C10-2028,0,0.0481424,"ments and ornaments (Voronova and Sterligov, 1997). User studies have shown that images (Lang et al., 1999), color (Bartram et al., 2017; Kulahcioglu and de Melo, 2019), and typography (Kulahcioglu and de Melo, 2018, 2020) contribute to conveying affect. Emoticons. Emoticons such as “:-)” and Japanese 顔文字 (kaomoji) such as “(ˆ ˆ)”, both composed from regular symbols, have been in use for several decades. Early studies focused on the use of emoticons in social media. Go et al. (2009) proposed a form of distant supervision by using emoticons as noisy labels for Twitter sentiment classification. Davidov et al. (2010) adopted a similar approach by handpicking smileys and hashtags as tweet labels to train a supervised model to classify the sentiment of tweets. Emojis. Emoji characters are pictorial, similar to earlier dingbat characters, but also colorful. Despite the lexicographic similarity between the two words emoji and emotion, etymologically, the former stems from the Japanese words 絵 (e, picture) and 文字 (moji, character). Emojis originated in Japan in the 1990s and have only recently spread globally. Historically, the spread of emojis has been driven in large part by their adoption in popular messagi"
2020.emnlp-main.720,D16-1235,0,0.0291455,"Missing"
2020.emnlp-main.720,P18-1235,1,0.887802,"Missing"
2020.emnlp-main.720,W16-6208,0,0.0400591,"Missing"
2020.emnlp-main.720,J15-4004,0,0.0697283,"edness resources that seek to quantify contextindependent lexical associations (Finkelstein et al., 2001) or word–emotion associations (Mohammad and Turney, 2013; Mohammad, 2018). The degree of association was specified numerically as a score ranging from 0 (no association with the emotion) to 4 (representing the highest degree of association with the emotion). While we are cognizant of the challenges of directly eliciting scalar ratings from the annotators, we opted to follow prominent previous work on collecting association ratings (Rubenstein and Goodenough, 1965; Finkelstein et al., 2001; Hill et al., 2015; Gerz et al., 2016) in order to make our data comparable to such efforts. 3.2 EmoTag1200 Data Collection Interface. We developed a web interface to collect ratings. We randomly split the target set of 150 8959 emojis into a total of 6 subsets, each consisting of 25 emojis. When a rater selects a set from the main page, the corresponding 25 emojis are presented to the user alongside their official names, each to be annotated with respect to our set of 8 different emotions. Within each set, we randomize the order of displayed emojis upon each page load, such that different raters do not observe"
2020.emnlp-main.720,L18-1010,1,0.892474,"Missing"
2020.emnlp-main.720,D17-1169,0,0.255446,"s may aid the interlocutor in disambiguating utterances that would otherwise remain ambiguous. One of their principal uses has been to convey emotion, particularly via facial expression emojis, as explained in Section 1. In 2015, Oxford Dictionaries declared the Face with Tears of Joy emoji its Word of the Year 2015. Emojis may also be useful as a more instantaneously and widely recognized form of communicating degrees of satisfaction. Kaye et al. (2017) go as far as suggesting them for consideration as possible alternatives to regular Likert scales. Emoji Semantics. The MIT DeepMoji project (Felbo et al., 2017) developed a model that recommends emojis given a natural language sentence as input. A deep neural architecture was trained on a collection of 1.2B tweets to learn the sentiment, emotions, and the use of sarcasm in short text. Barbieri et al. (2016b) proposed a method to learn vector space embeddings of emojis using the standard word2vec skip-gram approach, applied to a large collection of tweets. In contrast, Eisner et al. (2016) attempted to learn vector embeddings of emojis based on their short descriptions in the Unicode standard. EmojiNet (Wijeratne et al., 2017) provides a sense invento"
2020.emnlp-main.720,P14-2050,0,0.0320591,"ated in terms of Pearson correlation coefficients. The pretrained GloVe embeddings exhibit very low correlations, as both models have a limited coverage of just 26 out of the 150 emojis in the ground truth data. Our emoji-centric corpus yields stronger results. Among the two variants, word vectors trained with a larger context window size of 25 perform better, because emojis are often placed at the end of tweets. This result also accords with previous studies that show that larger context windows tend to capture generic relatedness, while shorter ones emphasize functional similarity of words (Levy and Goldberg, 2014). Using EmoLex with our binary emotion label scores, we observe varied results, including strong correlation for disgust, but low or even negative for several others. This is because the EmoLex lexicon merely signals whether or not it considers a word as being associated with an emotion. Such binary emotion labels do not appear to convey sufficient information for a more accurate prediction. With the NRC Emotion Intensity lexicon (Mohammad, 2018), we are able to obtain substantially higher correlations for a range of different settings of top-k words, both with our emoji corpus vector similari"
2020.emnlp-main.720,L18-1027,0,0.651831,"ectly conveying joy. Note also that this notion of association reflects a general, abstract form of connection, much like a prior. Clearly, embedded in a specific utterance, the specific emotions that are evoked may differ quite substantially, due to the complex ways in which different words along with embedded emojis interact to give rise to an overall interpretation. In this regard, our ratings are similar to widely used word relatedness resources that seek to quantify contextindependent lexical associations (Finkelstein et al., 2001) or word–emotion associations (Mohammad and Turney, 2013; Mohammad, 2018). The degree of association was specified numerically as a score ranging from 0 (no association with the emotion) to 4 (representing the highest degree of association with the emotion). While we are cognizant of the challenges of directly eliciting scalar ratings from the annotators, we opted to follow prominent previous work on collecting association ratings (Rubenstein and Goodenough, 1965; Finkelstein et al., 2001; Hill et al., 2015; Gerz et al., 2016) in order to make our data comparable to such efforts. 3.2 EmoTag1200 Data Collection Interface. We developed a web interface to collect rati"
2020.emnlp-main.720,P17-3022,0,0.0259353,"Missing"
2020.emnlp-main.720,D14-1162,0,0.0838996,"Missing"
2020.emnlp-main.720,R19-1126,1,0.845262,"Missing"
2020.emnlp-main.720,P18-1104,0,\N,Missing
2020.lrec-1.382,D18-1526,0,0.0278559,"Missing"
2020.lrec-1.382,W17-6901,0,0.0852428,"med entities are distinguished (e.g., people, geopolitical entities, artifacts, etc.), as in named entity recognition, including temporal categories (e.g., there are separate tags for decades, day of week, etc.). Moreover, there are dedicated tags for different tenses of events (such as past tense ate), tense & aspect markers (e.g., was in was reading), roles (victim), implications (if, unless), greetings (hi, bye), and many others. At the same time, more syntactically motivated distinctions, such as between adjectives and adverbs, are disregarded in this annotation scheme. In follow-up work, Abzianidze and Bos (2017) presented an improved tag set and showed that the tags exhibit the potential to apply cross-lingually. Motivation. In terms of available data, the Parallel Meaning Bank project (Abzianidze et al., 2017) provides a parallel corpus covering four languages (English, Dutch, German, and Italian) with rich annotations based on Discourse Representation Theory. At the lexical level, it includes semantic tag annotations, which we rely on in our work. However, due to the novelty of this task, the available annotations are limited in quantity and consist of a mix of gold and silver standard data. Just u"
2020.lrec-1.382,E17-2039,0,0.086705,"stinguished (e.g., people, geopolitical entities, artifacts, etc.), as in named entity recognition, including temporal categories (e.g., there are separate tags for decades, day of week, etc.). Moreover, there are dedicated tags for different tenses of events (such as past tense ate), tense & aspect markers (e.g., was in was reading), roles (victim), implications (if, unless), greetings (hi, bye), and many others. At the same time, more syntactically motivated distinctions, such as between adjectives and adverbs, are disregarded in this annotation scheme. In follow-up work, Abzianidze and Bos (2017) presented an improved tag set and showed that the tags exhibit the potential to apply cross-lingually. Motivation. In terms of available data, the Parallel Meaning Bank project (Abzianidze et al., 2017) provides a parallel corpus covering four languages (English, Dutch, German, and Italian) with rich annotations based on Discourse Representation Theory. At the lexical level, it includes semantic tag annotations, which we rely on in our work. However, due to the novelty of this task, the available annotations are limited in quantity and consist of a mix of gold and silver standard data. Just u"
2020.lrec-1.382,C16-1333,0,0.0612132,"rge-scale word representation data to derive a large new Semantic Tag lexicon. Our experiments show that we can infer semantic tags for words with high accuracy both monolingually and cross-lingually. Keywords: semantic tagging, word vectors, multilingual 1. Introduction Lexical category distinctions have been studied since the beginnings of linguistics. Traditional part-of-speech tagging has focused on distinctions based on the grammatical function of words, i.e., the syntactic role that words play within a sentence. Semantic Tags. In contrast, the recently proposed task of Semantic Tagging (Bjerva et al., 2016) considers a set of tags that are informed by semantic distinctions conjectured to be pertinent for semantic parsing and other semanticsdriven tasks. The annotation scheme distinguishes, for instance, privative attributes (PRI) such as former, fake from intersective ones (IST) such as vegetarian, and subsective ones (SST) such as skillful, making it easier for a system to discern that a fake detective is not a detective. Different kinds of named entities are distinguished (e.g., people, geopolitical entities, artifacts, etc.), as in named entity recognition, including temporal categories (e.g."
2020.lrec-1.382,A00-1031,0,0.521262,"ntation. Overview. We predict such interpretable representations by drawing on the annotated seed corpus in conjunction with information about word relatedness from larger-scale word vector representation data. Our experiments show that our method infers semantic tags for unseen words with high accuracy for four languages. This can finally be used to induce a large new Semantic Tag lexicon, providing semantic tag vectors for millions of words and names. Additionally, we can induce vectors cross-lingually for numerous additional languages. 2. Related Work Interpretable Lexical Representations. Brants (2000) highlighted the importance of handling unknown words in part-of-speech tagging. Our work has similar goals to the research by Cucerzan and Yarowsky (2000) on estimating part-of-speech probabilities for unseen words based on probabilities for known words. Faruqui and Dyer (2015) consulted a range of lexical resources to create non-distributional vectors that capture numerous different properties of words. These vectors are thus fairly high-dimensional. Recent work has considered lower-dimensional interpretable vectors focusing on particular aspects of words. For instance, Dong and de Melo (201"
2020.lrec-1.382,P00-1035,0,0.243512,"ut word relatedness from larger-scale word vector representation data. Our experiments show that our method infers semantic tags for unseen words with high accuracy for four languages. This can finally be used to induce a large new Semantic Tag lexicon, providing semantic tag vectors for millions of words and names. Additionally, we can induce vectors cross-lingually for numerous additional languages. 2. Related Work Interpretable Lexical Representations. Brants (2000) highlighted the importance of handling unknown words in part-of-speech tagging. Our work has similar goals to the research by Cucerzan and Yarowsky (2000) on estimating part-of-speech probabilities for unseen words based on probabilities for known words. Faruqui and Dyer (2015) consulted a range of lexical resources to create non-distributional vectors that capture numerous different properties of words. These vectors are thus fairly high-dimensional. Recent work has considered lower-dimensional interpretable vectors focusing on particular aspects of words. For instance, Dong and de Melo (2018b) induced vectors that capture the sentiment polarity of words in different domains, ?) developed interpretable vectors reflecting emotional associations"
2020.lrec-1.382,de-melo-2014-etymological,1,0.883751,"Missing"
2020.lrec-1.382,2015.mtsummit-papers.27,1,0.852284,"Missing"
2020.lrec-1.382,I17-5002,1,0.903464,"Missing"
2020.lrec-1.382,P18-1235,1,0.900953,"Missing"
2020.lrec-1.382,P15-2076,0,0.0143757,"for unseen words with high accuracy for four languages. This can finally be used to induce a large new Semantic Tag lexicon, providing semantic tag vectors for millions of words and names. Additionally, we can induce vectors cross-lingually for numerous additional languages. 2. Related Work Interpretable Lexical Representations. Brants (2000) highlighted the importance of handling unknown words in part-of-speech tagging. Our work has similar goals to the research by Cucerzan and Yarowsky (2000) on estimating part-of-speech probabilities for unseen words based on probabilities for known words. Faruqui and Dyer (2015) consulted a range of lexical resources to create non-distributional vectors that capture numerous different properties of words. These vectors are thus fairly high-dimensional. Recent work has considered lower-dimensional interpretable vectors focusing on particular aspects of words. For instance, Dong and de Melo (2018b) induced vectors that capture the sentiment polarity of words in different domains, ?) developed interpretable vectors reflecting emotional associations of words, and Shoeb et al. (2019) proposed vectors reflecting associations with emojis. In this paper, we seek to induce in"
2020.lrec-1.382,D14-1162,0,0.0854375,"Missing"
2020.lrec-1.382,R19-1126,1,0.891925,"Missing"
2020.lrec-1.856,P12-2073,0,0.0246986,"n (2017) created a synthetic dataset of incorrect key strokes by sampling from a Gaussian at each key location on a virtual keyboard. They also created another dataset by replacing correct words with their misspellings, as given by an annotated typo corpus (Aramaki, 2010). Their paper noted the lack of datasets for this problem, which led them to create their own to train and test their model. Our work differs from their approach in that we induce a noise model from text that can then be used to introduce a wide range of errors, instead of replacing a small set of words by their misspellings. Baba and Suzuki (2012) studied different error categories arising during typing. Methods for context-aware spelling correction have mostly focused on small sets of words typically confused in human writing: Many of the most well-known approaches to this task rely on predefined confusion sets to solve it (Carlson and Fette, 2007; Golding and Roth, 1999; Carlson et al., 2001; Banko and Brill, 2001). However, with the prominence of autocorrect systems on mobile devices, this approach is ineffective. Hence, we explore generating realistic spelling errors requiring context-aware correction in this paper. Psubstitution ("
2020.lrec-1.856,P01-1005,0,0.262071,"d test their model. Our work differs from their approach in that we induce a noise model from text that can then be used to introduce a wide range of errors, instead of replacing a small set of words by their misspellings. Baba and Suzuki (2012) studied different error categories arising during typing. Methods for context-aware spelling correction have mostly focused on small sets of words typically confused in human writing: Many of the most well-known approaches to this task rely on predefined confusion sets to solve it (Carlson and Fette, 2007; Golding and Roth, 1999; Carlson et al., 2001; Banko and Brill, 2001). However, with the prominence of autocorrect systems on mobile devices, this approach is ineffective. Hence, we explore generating realistic spelling errors requiring context-aware correction in this paper. Psubstitution (c0 |c). fsubstitution (c) f(c) P (substitution |c) = fsubstitution (c0 , c) Psubstitution (c0 |c) = X fsubstitution (c, c) c∈C where fsubstitution (x) denotes the frequency of character x being replaced, while fsubstitution (y, x) denotes the frequency of character y replacing character x. Insertion Errors. Insertion errors occur when an additional character is inserted by m"
2020.lrec-1.856,W11-2838,0,0.0298028,"hile autocorrection typically considers just the current and possibly the last few words, deep models that can account for both sides of a larger context window have the potential to more accurately determine whether a word fits in context. Unfortunately, deep learning generally requires large annotated corpora for effective training. Such large annotated datasets are difficult to procure, as the labeling process tends to be time-consuming and expensive (Qiu et al., 2020). There have been significant efforts in the last decade to overcome this gap for the task of grammatical error correction (Dale and Kilgarriff, 2011; Ng et al., 2013; Ng et al., 2014). However, no sufficiently large datasets exist for typographical error correction. In this paper, we aim to generate realistic typographical errors based on statistical distributions collected from a relatively small annotated seed dataset. Our method captures the error patterns from the seed data and generates similar error distributions on error-free target corpora of arbitrary size. An important special case, particularly when typing on hand-held devices, is the abundance of real-word errors, also known as atomic typos. These occur when misspelled words h"
2020.lrec-1.856,E14-3013,0,0.0113659,"but used in the wrong context, requiring context-aware spelling correction. We examine inducing such errors automatically via a dictionary-based spelling corrector. Finally, we explore the effectiveness of deep neural networks to detect and correct such errors. While the experiments in this paper are limited to English, our method is applicable to any language with a restricted (true) alphabet. 2. Related Work Due to the high cost and difficulty of labeling large text datasets for error correction, several studies explored generating artificial grammatical mistakes (Foster and Andersen, 2009; Felice and Yuan, 2014; Ng et al., 2013; Rei et al., 2017; Kasewa et al., 2018). For example, Foster and Andersen (2009) achieved this by moving, substituting, inserting, and removing words in a sentence, and investigated partof-speech tags to induce more realistic errors. Felice and Yuan (2014) used probability word-level statistics computed on the CoNLL 2013 shared task data (Ng et al., 2013) to introduce artificial grammatical errors at the word level. Rei et al. (2017) used a statistical machine translation system to translate correct text to ungrammatical text. For training, they relied on the annotated datase"
2020.lrec-1.856,W09-2112,0,0.0115103,"d words in the dictionary, but used in the wrong context, requiring context-aware spelling correction. We examine inducing such errors automatically via a dictionary-based spelling corrector. Finally, we explore the effectiveness of deep neural networks to detect and correct such errors. While the experiments in this paper are limited to English, our method is applicable to any language with a restricted (true) alphabet. 2. Related Work Due to the high cost and difficulty of labeling large text datasets for error correction, several studies explored generating artificial grammatical mistakes (Foster and Andersen, 2009; Felice and Yuan, 2014; Ng et al., 2013; Rei et al., 2017; Kasewa et al., 2018). For example, Foster and Andersen (2009) achieved this by moving, substituting, inserting, and removing words in a sentence, and investigated partof-speech tags to induce more realistic errors. Felice and Yuan (2014) used probability word-level statistics computed on the CoNLL 2013 shared task data (Ng et al., 2013) to introduce artificial grammatical errors at the word level. Rei et al. (2017) used a statistical machine translation system to translate correct text to ungrammatical text. For training, they relied"
2020.lrec-1.856,D18-1541,0,0.0184694,"lling correction. We examine inducing such errors automatically via a dictionary-based spelling corrector. Finally, we explore the effectiveness of deep neural networks to detect and correct such errors. While the experiments in this paper are limited to English, our method is applicable to any language with a restricted (true) alphabet. 2. Related Work Due to the high cost and difficulty of labeling large text datasets for error correction, several studies explored generating artificial grammatical mistakes (Foster and Andersen, 2009; Felice and Yuan, 2014; Ng et al., 2013; Rei et al., 2017; Kasewa et al., 2018). For example, Foster and Andersen (2009) achieved this by moving, substituting, inserting, and removing words in a sentence, and investigated partof-speech tags to induce more realistic errors. Felice and Yuan (2014) used probability word-level statistics computed on the CoNLL 2013 shared task data (Ng et al., 2013) to introduce artificial grammatical errors at the word level. Rei et al. (2017) used a statistical machine translation system to translate correct text to ungrammatical text. For training, they relied on the annotated dataset from CoNLL 2014 (Ng et al., 2014) but considered in inv"
2020.lrec-1.856,P11-1015,0,0.0387643,"These probabilities are independent of the error rate and are only applied once a substitution error is determined. Figure 3: Frequency of Deletion Errors by Characters 5. Experiments Input Data. We rely on the statistics derived from the Twitter Typo Corpus (Aramaki, 2010), described in Section 4 to introduce errors into two corpora, a food review corpus, and a large movie review one. For food reviews, we consider the Amazon fine food review dataset, which consists of 568,454 food reviews collected from Amazon, along with metadata (McAuley and Leskovec, 2013). The large movie review dataset (Maas et al., 2011) contains 50,000 labeled and 50,000 unlabeled movie reviews collected from IMDB. We only use the text from both of these datasets. Figure 4: Frequency of Substitution Errors by Characters to be missed. Figure 4 depicts the frequencies of different characters being replaced by another character, i.e., substitution errors. This type of error also exhibits correlation with the occurrence frequencies of characters. This statistic is used to determine the probability of a substitution error at a given character during error induction. Figure 5 shows the probabilities of particular characters being"
2020.lrec-1.856,W13-3601,0,0.200857,"ly considers just the current and possibly the last few words, deep models that can account for both sides of a larger context window have the potential to more accurately determine whether a word fits in context. Unfortunately, deep learning generally requires large annotated corpora for effective training. Such large annotated datasets are difficult to procure, as the labeling process tends to be time-consuming and expensive (Qiu et al., 2020). There have been significant efforts in the last decade to overcome this gap for the task of grammatical error correction (Dale and Kilgarriff, 2011; Ng et al., 2013; Ng et al., 2014). However, no sufficiently large datasets exist for typographical error correction. In this paper, we aim to generate realistic typographical errors based on statistical distributions collected from a relatively small annotated seed dataset. Our method captures the error patterns from the seed data and generates similar error distributions on error-free target corpora of arbitrary size. An important special case, particularly when typing on hand-held devices, is the abundance of real-word errors, also known as atomic typos. These occur when misspelled words happen to also be"
2020.lrec-1.856,P02-1040,0,0.105992,"Missing"
2020.lrec-1.856,D14-1162,0,0.083328,"r also exhibits correlation with the occurrence frequencies of characters. This statistic is used to determine the probability of a substitution error at a given character during error induction. Figure 5 shows the probabilities of particular characters being replaced by particular other characters, given that a substitution error is Error Corpus Induction. First of all, to remove preexisting typographical errors from these corpora, we construct a dictionary V from the Enchant spell checker, enhanced by the vocabulary of GloVe embeddings trained on 6 billion words from Wikipedia and Gigaword (Pennington et al., 2014).2 On the fine food review dataset, we also remove reviews that are outliers in terms of their length.3 This leads to 254,638 samples in the fine food review dataset, from 2 We do not rely on embeddings trained on CommonCrawl, as Web data contains substantially more misspelling forms. 3 Specifically, those with a character length three standard deviations above or below mean. Hence, we filter out reviews longer than 1,715 characters, but no reviews with shorter length, as three standard deviations below mean is less than 0. 6933 which we sample 160,000 for training, 40,000 as a held-out valida"
2020.lrec-1.856,W17-5032,0,0.0192536,"context-aware spelling correction. We examine inducing such errors automatically via a dictionary-based spelling corrector. Finally, we explore the effectiveness of deep neural networks to detect and correct such errors. While the experiments in this paper are limited to English, our method is applicable to any language with a restricted (true) alphabet. 2. Related Work Due to the high cost and difficulty of labeling large text datasets for error correction, several studies explored generating artificial grammatical mistakes (Foster and Andersen, 2009; Felice and Yuan, 2014; Ng et al., 2013; Rei et al., 2017; Kasewa et al., 2018). For example, Foster and Andersen (2009) achieved this by moving, substituting, inserting, and removing words in a sentence, and investigated partof-speech tags to induce more realistic errors. Felice and Yuan (2014) used probability word-level statistics computed on the CoNLL 2013 shared task data (Ng et al., 2013) to introduce artificial grammatical errors at the word level. Rei et al. (2017) used a statistical machine translation system to translate correct text to ungrammatical text. For training, they relied on the annotated dataset from CoNLL 2014 (Ng et al., 2014)"
2020.lrec-1.856,D09-1093,0,0.0524774,"as dyslexia, they may also be caused by 6930 simple key entry mistakes such as those referred to as fat finger errors. In the past, such errors were often corrected by means of a dictionary-based spelling corrector, but with the proliferation of hand-held devices, this no longer seems sufficient. Indeed, many such devices invoke autocorrection tools, which may lead to entirely new errors that are very hard to detect. Bigert et al. (2003) developed a tool called ‘Missplel’, which could introduce character-level errors, among others. However, those errors were not modeled upon real-world data. Whitelaw et al. (2009) repurposed Web data as a noisy corpus for spelling correction and used a limited corpus with artificially inserted misspellings to tune classifiers. Ghosh and Kristensson (2017) created a synthetic dataset of incorrect key strokes by sampling from a Gaussian at each key location on a virtual keyboard. They also created another dataset by replacing correct words with their misspellings, as given by an annotated typo corpus (Aramaki, 2010). Their paper noted the lack of datasets for this problem, which led them to create their own to train and test their model. Our work differs from their appro"
2020.lrec-1.856,W14-1701,0,\N,Missing
2021.acl-long.110,D17-1169,0,0.033882,"ave not been studies assessing them with regard to their ability to cope with modern emoji-laden text. Since emojis are becoming increasingly ubiquitous, it is crucial for developers and institutions deploying such software to know whether it can properly handle the kinds of text that nowadays may quite likely arrive as input data. In many real-world settings, applications and services are expected to operate on text containing emojis, and thus it is important to investigate these capabilities. Many academic studies present new models for particular NLP tasks relating to emojis. For instance, Felbo et al. (2017) developed an emoji prediction model for tweets. Weerasooriya et al. (2016) discussed how to extract essential keywords from a tweet using NLP tools. Cohn et al. (2019) attempted to understand the use of emojis from a grammatical perspective, seeking to determine the parts-of-speech of emoji occurrences in a sentence or tweet. Owoputi et al. (2013) proposed an improved part-of-speech tagging model for online conversational text based on word clusters. Proisl (2018) developed a part-of-speech tagger for German social media and Kong et al. (2014) developed a dependency parser for English tweets."
2021.acl-long.110,W18-2501,0,0.0521944,"Missing"
2021.acl-long.110,S19-2029,0,0.0460096,"Missing"
2021.acl-long.110,D14-1108,0,0.0125352,"ular NLP tasks relating to emojis. For instance, Felbo et al. (2017) developed an emoji prediction model for tweets. Weerasooriya et al. (2016) discussed how to extract essential keywords from a tweet using NLP tools. Cohn et al. (2019) attempted to understand the use of emojis from a grammatical perspective, seeking to determine the parts-of-speech of emoji occurrences in a sentence or tweet. Owoputi et al. (2013) proposed an improved part-of-speech tagging model for online conversational text based on word clusters. Proisl (2018) developed a part-of-speech tagger for German social media and Kong et al. (2014) developed a dependency parser for English tweets. However, such work mostly targets just one specific task and is typically not well-integrated with common open source toolkits, which we focus on in our study. 3 Experimental Data As we wish to assess the support of emojis provided by different text processing tools, we first consider some of the different cases of emoji use that one may encounter, in order to compile relevant data. 3.1 Emoji Use in Text Emojis can appear in a sentence or tweet in different circumstances. They may show up at the beginning or at the end of a tweet. Likewise, th"
2021.acl-long.110,N13-1039,0,0.0591173,"Missing"
2021.acl-long.110,L18-1106,0,0.0134029,"gate these capabilities. Many academic studies present new models for particular NLP tasks relating to emojis. For instance, Felbo et al. (2017) developed an emoji prediction model for tweets. Weerasooriya et al. (2016) discussed how to extract essential keywords from a tweet using NLP tools. Cohn et al. (2019) attempted to understand the use of emojis from a grammatical perspective, seeking to determine the parts-of-speech of emoji occurrences in a sentence or tweet. Owoputi et al. (2013) proposed an improved part-of-speech tagging model for online conversational text based on word clusters. Proisl (2018) developed a part-of-speech tagger for German social media and Kong et al. (2014) developed a dependency parser for English tweets. However, such work mostly targets just one specific task and is typically not well-integrated with common open source toolkits, which we focus on in our study. 3 Experimental Data As we wish to assess the support of emojis provided by different text processing tools, we first consider some of the different cases of emoji use that one may encounter, in order to compile relevant data. 3.1 Emoji Use in Text Emojis can appear in a sentence or tweet in different circum"
2021.acl-long.110,2020.acl-demos.14,0,0.132146,"ally considered some of the more advanced technical possibilities that the Unicode standard affords, such as zero width joiners to express more complex concepts. For instance, with regard to emoji skin tone modifiers, Robertson et al. (2020) study in depth how the use of such modifiers varies on social media, including cases of users modulating their skintone, i.e., using a different tone than the one they usually pick. Given the widespread use of emojis in everyday communication, there is an increasing need for NLP tools that can handle them. Prominent NLP toolkits such as Stanford’s Stanza (Qi et al., 2020) and NLTK (Bird et al., 2009) power a wide range of user-facing applications. A number of reports compare the pros and cons of popular NLP libraries (Wolff, 2020; Kozaczko, 2018; Choudhury, 2019; Bilyk, 2020), but these primarily consider the features and popularity of the tools, as well as their performance. There have not been studies assessing them with regard to their ability to cope with modern emoji-laden text. Since emojis are becoming increasingly ubiquitous, it is crucial for developers and institutions deploying such software to know whether it can properly handle the kinds of text t"
2021.acl-long.110,2020.emnlp-main.720,1,0.873971,"Missing"
2021.acl-long.110,R19-1126,1,0.86949,"Missing"
2021.acl-long.379,N19-1423,0,0.19262,"vising a structural model of language capable of learning both representations and meaningful syntactic structure without any humanannotated trees has been a long-standing but challenging goal. Across a diverse range of linguistic theories, human language is assumed to possess a recursive hierarchical structure (Chomsky, 1956, 2014; de Marneffe et al., 2006) such that lowerlevel meaning is combined to infer higher-level semantics. Humans possess notions of characters, words, phrases, and sentences, which children naturally learn to segment and combine. Pretrained language models such as BERT (Devlin et al., 2019) have achieved substantial gains ∗ Equal contribution. The code is available at: https://github.com/ alipay/StructuredLM_RTDT 1 In this paper, we revisit these ideas, and propose a model applying recursive Transformers along differentiable trees (R2D2). To obtain differentiability, we adopt Gumbel-Softmax estimation (Jang et al., 2017) as an elegant solution. Our encoder parser operates in a bottom-up fashion akin to CKY parsing, yet runs in linear time with regard to the number of composition steps, thanks to a novel pruned tree induction algorithm. As a training objective, the model seeks to"
2021.acl-long.379,2020.emnlp-main.392,0,0.0509019,"Missing"
2021.acl-long.379,N19-1116,0,0.0111656,"shows the training time of our R2D2 with and without pruning. The last row is proportionally estimated by running the small setting (12×12×1). It is clear that it is not feasible to run our R2D2 without pruning. 3.2 Unsupervised Constituency Parsing We next assess to what extent the trees that naturally arise in our model bear similarities with human-specified parse trees. 3.2.1 Setup Baselines and Evaluation. For comparison, we further include four recent strong models for unsupervised parsing with open source code: B ERT masking (Wu et al., 2020), Ordered Neurons (Shen et al., 2019), DIORA (Drozdov et al., 2019) and C-PCFG (Kim et al., 2019a). Following Htut et al. (2018), we train all systems on a training set consisting of raw text, and evaluate and report the results on an annotated test set. As an evaluation metric, we adopt sentence-level unlabeled F1 computed using the script from Kim et al. (2019a). We compare against the non-binarized gold trees per convention. The best checkpoint for each system is picked based on scores on the validation set. As our model is a pretrained model based on word-pieces, for a fair comparison, we test all models with two types of input: word level (W) and word-pi"
2021.acl-long.379,N16-1024,0,0.0711279,"Missing"
2021.acl-long.379,W02-1039,0,0.256459,"Missing"
2021.acl-long.379,W18-5452,0,0.0255333,"e last row is proportionally estimated by running the small setting (12×12×1). It is clear that it is not feasible to run our R2D2 without pruning. 3.2 Unsupervised Constituency Parsing We next assess to what extent the trees that naturally arise in our model bear similarities with human-specified parse trees. 3.2.1 Setup Baselines and Evaluation. For comparison, we further include four recent strong models for unsupervised parsing with open source code: B ERT masking (Wu et al., 2020), Ordered Neurons (Shen et al., 2019), DIORA (Drozdov et al., 2019) and C-PCFG (Kim et al., 2019a). Following Htut et al. (2018), we train all systems on a training set consisting of raw text, and evaluate and report the results on an annotated test set. As an evaluation metric, we adopt sentence-level unlabeled F1 computed using the script from Kim et al. (2019a). We compare against the non-binarized gold trees per convention. The best checkpoint for each system is picked based on scores on the validation set. As our model is a pretrained model based on word-pieces, for a fair comparison, we test all models with two types of input: word level (W) and word-piece level (WP)3 . To support word-piece level evaluation, we"
2021.acl-long.379,P19-1228,0,0.0279278,"Missing"
2021.acl-long.379,N19-1114,0,0.223604,"2 with and without pruning. The last row is proportionally estimated by running the small setting (12×12×1). It is clear that it is not feasible to run our R2D2 without pruning. 3.2 Unsupervised Constituency Parsing We next assess to what extent the trees that naturally arise in our model bear similarities with human-specified parse trees. 3.2.1 Setup Baselines and Evaluation. For comparison, we further include four recent strong models for unsupervised parsing with open source code: B ERT masking (Wu et al., 2020), Ordered Neurons (Shen et al., 2019), DIORA (Drozdov et al., 2019) and C-PCFG (Kim et al., 2019a). Following Htut et al. (2018), we train all systems on a training set consisting of raw text, and evaluate and report the results on an annotated test set. As an evaluation metric, we adopt sentence-level unlabeled F1 computed using the script from Kim et al. (2019a). We compare against the non-binarized gold trees per convention. The best checkpoint for each system is picked based on scores on the validation set. As our model is a pretrained model based on word-pieces, for a fair comparison, we test all models with two types of input: word level (W) and word-piece level (WP)3 . To support"
2021.acl-long.379,D15-1137,0,0.0262106,"erarchical process. This paper proposes a recursive Transformer model based on differentiable CKY style binary trees to emulate the composition process. We extend the bidirectional language model pre-training objective to this architecture, attempting to predict each word given its left and right abstraction nodes. To scale up our approach, we also introduce an efficient pruned tree induction algorithm to enable encoding in just a linear number of composition steps. Experimental results on language modeling and unsupervised parsing show the effectiveness of our approach.1 1 Inspired by Le and Zuidema (2015), Maillard et al. (2017) proposed a fully differentiable CKY parser to model the hierarchical process explicitly. To make their parser differentiable, they primarily introduce an energy function to combine all possible derivations when constructing each cell representation. However, their model is based on Tree-LSTMs (Tai et al., 2015; Zhu et al., 2015) and requires O(n3 ) time complexity. Hence, it is hard to scale up to large training data. Introduction The idea of devising a structural model of language capable of learning both representations and meaningful syntactic structure without any"
2021.acl-long.379,P14-5010,0,0.00677007,"Missing"
2021.acl-long.379,J93-2004,0,0.0765019,"eces, we force it to not prune or select spans that conflict with word spans during prediction, and then merge word-pieces into words in the final output. However, note that this constraint is only used for word-level prediction. For training, we use the same hyperparameters as in Section 3.1.1. Our model pretrained on WikiText-2 is finetuned on the training set with the same unsupervised loss objective. For Chinese, we use a subset of Chinese Wikipedia for pretraining, specifically the first 100,000 sentences shorter than 150 characters. Data. We test our approach on the Penn Treebank (PTB) (Marcus et al., 1993) with the standard splits (2-21 for training, 22 for validation, 23 for test) and the same preprocessing as in recent work (Kim et al., 2019a), where we discard punctuation and lower-case all tokens. To explore the universality of the model across languages, we also run experiments on Chinese Penn Treebank (CTB) 8 (Xue et al., 2005), on which we also remove punctuation. Note that in all settings, the training is conducted entirely on raw unannotated text. 3.2.2 Results and Discussion Table 3 provides the unlabeled F1 scores of all systems on the WSJ and CTB test sets. It is clear that all syst"
2021.acl-long.379,de-marneffe-etal-2006-generating,0,0.0815312,"Missing"
2021.acl-long.379,N18-1202,0,0.0427421,"nd compatible with a tree structure. To further understand the strengths and weaknesses of each baseline, we analyzed the compatibility of different sentence length ranges. Interestingly, we find that our approach performs better on long sentences compared with C-PCFG at the word-piece level. This shows that a bidirectional language modeling objective can learn to induce accurate structures even on very long sentences, on which custom-tailored methods may not work as well. 4904 4 Related Work Pre-trained models. Pre-trained models have achieved significant success across numerous tasks. ELMo (Peters et al., 2018), pretrained on bidirectional language modeling based on bi-LSTMs, was the first model to show significant improvements across many downstream tasks. GPT (Radford et al., 2018) replaces bi-LSTMs with a Transformer (Vaswani et al., 2017). As the global attention mechanism may reveal contextual information, it uses a left-to-right Transformer to predict the next word given the previous context. B ERT (Devlin et al., 2019) proposes masked language modeling (MLM) to enable bidirectional modeling while avoiding contextual information leakage by directly masking part of input tokens. As masking inpu"
2021.acl-long.379,2020.acl-main.240,0,0.0174738,". 3 Experiments As our approach (R2D2) is able to learn both representations and intermediate structure, we evaluate its representation learning ability on bidirectional language modeling and evaluate the intermediate structures on unsupervised parsing. 3.1 Bidirectional Language Modeling 3.1.1 trained from scratch on WikiText-2 with different settings (number of Transformer layers and training epochs). m is the pruning threshold. Setup Baselines and Evaluation. As the objective of our model is to predict each word with its left and right context, we use the pseudo-perplexity (PPPL) metric of Salazar et al. (2020) to evaluate bidirectional language modeling. n 1X logP (si |s1:i−1 , si+1:n , θ) L(S) = n i=1   N X 1 PPPL(S) = exp − L(Sj ) N j=1 PPPL is a bidirectional version of perplexity, establishing a macroscopic assessment of the model’s ability to deal with diverse linguistic phenomena. We compared our approach with SOTA autoencoding and autoregressive language models capable of capturing bidirectional contexts, including B ERT, XLNet (Yang et al., 2019), and ALBERT (Lan et al., 2020). For a fair apples to apples comparison, all models use the same vocabulary and are trained from scratch on a l"
2021.acl-long.379,D11-1014,0,0.204692,"Missing"
2021.acl-long.379,2020.acl-main.383,0,0.0139466,"del may perform even better with further deep layers. Table 2 shows the training time of our R2D2 with and without pruning. The last row is proportionally estimated by running the small setting (12×12×1). It is clear that it is not feasible to run our R2D2 without pruning. 3.2 Unsupervised Constituency Parsing We next assess to what extent the trees that naturally arise in our model bear similarities with human-specified parse trees. 3.2.1 Setup Baselines and Evaluation. For comparison, we further include four recent strong models for unsupervised parsing with open source code: B ERT masking (Wu et al., 2020), Ordered Neurons (Shen et al., 2019), DIORA (Drozdov et al., 2019) and C-PCFG (Kim et al., 2019a). Following Htut et al. (2018), we train all systems on a training set consisting of raw text, and evaluate and report the results on an annotated test set. As an evaluation metric, we adopt sentence-level unlabeled F1 computed using the script from Kim et al. (2019a). We compare against the non-binarized gold trees per convention. The best checkpoint for each system is picked based on scores on the validation set. As our model is a pretrained model based on word-pieces, for a fair comparison, we"
2021.acl-long.379,P15-1150,0,0.118091,"Missing"
2021.emnlp-main.312,P17-1152,0,0.171738,"., 2019). Yang et al. (2019a) show that contextualizing the self-attention network may improve the original representations, but they do not consider the scenario of sentence pairs with crossattention. Semantic text matching is among the most fundamental tasks in natural language processing. Given two sentences, the goal is to predict their semantic relationship. In this work, we focus in particular on question matching (QM) benchmarks. Recently, the availability of large-scale annotated datasets has led to a proliferation of deep neural architectures for text matching (Williams et al., 2018; Chen et al., 2017; Wang et al., 2017). Most existing neural models fall into two categories, namely the sentence encoding and the sentence interaction approaches (Lan and Xu, 2018). The former encodes sentences as fixed-length vector representations, which are then consulted to make the final prediction. The latter considers interactions between two sequences to identify their In this work, we aim to generalize the notion semantic connections, which tends to yield better of cross-sentence attention by enabling it to inresults. corporate rich contextual signals. We propose a 3846 Proceedings of the 2021 Confere"
2021.emnlp-main.312,N19-1423,0,0.0359221,"Missing"
2021.emnlp-main.312,2020.wnut-1.6,1,0.685266,"Missing"
2021.emnlp-main.312,C18-1328,0,0.0187989,"of sentence pairs with crossattention. Semantic text matching is among the most fundamental tasks in natural language processing. Given two sentences, the goal is to predict their semantic relationship. In this work, we focus in particular on question matching (QM) benchmarks. Recently, the availability of large-scale annotated datasets has led to a proliferation of deep neural architectures for text matching (Williams et al., 2018; Chen et al., 2017; Wang et al., 2017). Most existing neural models fall into two categories, namely the sentence encoding and the sentence interaction approaches (Lan and Xu, 2018). The former encodes sentences as fixed-length vector representations, which are then consulted to make the final prediction. The latter considers interactions between two sequences to identify their In this work, we aim to generalize the notion semantic connections, which tends to yield better of cross-sentence attention by enabling it to inresults. corporate rich contextual signals. We propose a 3846 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3846–3853 c November 7–11, 2021. 2021 Association for Computational Linguistics COntext-aware Intera"
2021.emnlp-main.312,P19-1314,0,0.0208336,"uestions Pairs corpus (Quora) contains over 400k English question pairs selected from Quora.com, for which we use the same data split as Wang et al. (2017). 2) LCQMC (Liu et al., 2018) is a large-scale open-domain Chinese question matching corpus constructed from Baidu Knows. We follow the data splits in the original papers, and apply a hard cut-off of the sentence length on both datasets by cropping or padding. The length is set to 32 for Quora and 50 for LCQMC. Training Details and Parameters. For Quora, we use 300 dimensional GloVe embeddings (Pennington et al., 2014). For LCQMC, following Li et al. (2019), we avoid word segmentation and instead use a randomly initialized character embedding matrix. The kernel size is 3 for convolutional layers with padding. We tune the dimensionality of the feed-forward layers from 150 to 300. The batch size is tuned from 32 to 128. Adam optimization is used with an initial learning rate of 0.001 and exponential decay. We use ReLU (Glorot et al., 2011) as the activation function in all feed-forward networks. To prevent over-fitting, dropout with a retention probability of 0.8 is applied. We apply 3 context-aware interaction blocks for Quora and 2 interaction b"
2021.emnlp-main.312,D19-1267,0,0.03616,"Missing"
2021.emnlp-main.312,C18-1166,0,0.0466231,"Missing"
2021.emnlp-main.312,P19-1500,0,0.0283428,"ned information by controlling the gate, especially when multiple interactions are applied. 2.3 Aggregation Layer To obtain high-level semantic representations for each sentence, we apply another convolutional neural network on top of the interaction blocks to obtain the aggregated sentence representations Va , Vb , serving as the inputs for the prediction layer. 2.4 Pooling and Prediction Layer We compute a weighted summation of the hidden states to get sentence vectors. To allow the model to represent each sequence in different representation subspaces, we adopt multi-head pooling following Liu and Lapata (2019). For each head z, we first transform the sequence into attention scores Sz and e z: values V Sz = softmax(Waz Va ) e z = W z Va V v (13) (14) where Waz ∈ R1×d and Wuz ∈ Rdh ×d are trainable parameters, with dh = d/nh as the dimensionality of each head and nh as the number of heads. The pooling vector of head z is computed as bz = V n X eiz , szi v (15) i=1 (7) eiz denote the calculated attention where szi and v scores and values. The pooling vectors of all heads are concatenated to form the final vector representations of each sequence Va0 and Vb0 . We combine Va0 and Vb0 to produce the overa"
2021.emnlp-main.312,D16-1244,0,0.0944564,"Missing"
2021.emnlp-main.312,D14-1162,0,0.0851957,"uct experiments on two datasets: 1) The Quora Questions Pairs corpus (Quora) contains over 400k English question pairs selected from Quora.com, for which we use the same data split as Wang et al. (2017). 2) LCQMC (Liu et al., 2018) is a large-scale open-domain Chinese question matching corpus constructed from Baidu Knows. We follow the data splits in the original papers, and apply a hard cut-off of the sentence length on both datasets by cropping or padding. The length is set to 32 for Quora and 50 for LCQMC. Training Details and Parameters. For Quora, we use 300 dimensional GloVe embeddings (Pennington et al., 2014). For LCQMC, following Li et al. (2019), we avoid word segmentation and instead use a randomly initialized character embedding matrix. The kernel size is 3 for convolutional layers with padding. We tune the dimensionality of the feed-forward layers from 150 to 300. The batch size is tuned from 32 to 128. Adam optimization is used with an initial learning rate of 0.001 and exponential decay. We use ReLU (Glorot et al., 2011) as the activation function in all feed-forward networks. To prevent over-fitting, dropout with a retention probability of 0.8 is applied. We apply 3 context-aware interacti"
2021.emnlp-main.312,D19-1410,0,0.0430984,"Missing"
2021.emnlp-main.312,D18-1185,0,0.0349908,"Missing"
2021.emnlp-main.312,N16-1170,0,0.0331262,"Missing"
2021.emnlp-main.312,P18-1158,0,0.01766,"opt of the feed-forward layer with respect to only the a multi-layer convolutional encoder on top of the individual token pairs, so the layer does not take embedding layer. In addition, we concatenate the advantage of valuable contextual signals. contextual representations with the original embed2.2.2 Context-Aware Cross-Attention Layer dings to produce better alignments in the following interaction blocks. This serves a similar purpose We propose a novel context-aware cross-attention as skip connections to represent words at different layer by incorporating contextual representations levels (Wang et al., 2018). into the cross-attention. The goal is to enable the 3847 model to identify salient contextual features for each token, and consider these features when computing the cross-attention matrix E. Given Ca = (ca1 , ..., cam ), Cb = (cb1 , ..., cbn ) as contextual representations for the two sentences, we modify the attention mechanism from Eq. 1 to be able to draw on these as additional inputs when computing the word-by-word attention matrix: Ecij = Attcontext (hai , hbj , cai , cbj ) = F1 (hai + cai )T F1 (hbj + cbj ) (4) By incorporating the contextual vectors, the model is able to take advanta"
2021.emnlp-main.312,N18-1101,0,0.0205704,"tructions (Storks et al., 2019). Yang et al. (2019a) show that contextualizing the self-attention network may improve the original representations, but they do not consider the scenario of sentence pairs with crossattention. Semantic text matching is among the most fundamental tasks in natural language processing. Given two sentences, the goal is to predict their semantic relationship. In this work, we focus in particular on question matching (QM) benchmarks. Recently, the availability of large-scale annotated datasets has led to a proliferation of deep neural architectures for text matching (Williams et al., 2018; Chen et al., 2017; Wang et al., 2017). Most existing neural models fall into two categories, namely the sentence encoding and the sentence interaction approaches (Lan and Xu, 2018). The former encodes sentences as fixed-length vector representations, which are then consulted to make the final prediction. The latter considers interactions between two sequences to identify their In this work, we aim to generalize the notion semantic connections, which tends to yield better of cross-sentence attention by enabling it to inresults. corporate rich contextual signals. We propose a 3846 Proceedings"
2021.emnlp-main.312,P19-1465,0,0.0155085,"sentence attention, existing models mostly focus on word-level local matching and fail to fully account for the overall semantics: each value of the attention matrix is based on just two individual tokens from the sequences without full consideration of the context. As shown in Figure 1, in the original attention mechanism, each token individually attends to the other tokens without accounting for important contextual information. However, accurate matching may require a deeper understanding of the two sentences along with pertinent linguistic patterns and constructions (Storks et al., 2019). Yang et al. (2019a) show that contextualizing the self-attention network may improve the original representations, but they do not consider the scenario of sentence pairs with crossattention. Semantic text matching is among the most fundamental tasks in natural language processing. Given two sentences, the goal is to predict their semantic relationship. In this work, we focus in particular on question matching (QM) benchmarks. Recently, the availability of large-scale annotated datasets has led to a proliferation of deep neural architectures for text matching (Williams et al., 2018; Chen et al., 2017; Wang et"
2021.emnlp-main.781,C18-1081,0,0.0160822,"o a target emotion results in less accurate emotion association scores. Thus, we disregarded any labels not in the ground truth. tained by taking the PARAGRAM-SL999 vectors by Wieting et al. (2015) and optimizing them using synonymy and antonymy constraints. The best results are obtained using AffectVec (Raji and de Melo, 2020), which post-processes the same vectors using not only synonymy and antonymy constraints, but also sentiment polarity ones. Sentiment polarity evidently helps to better distinguish different emotional associations. In contrast, the emotion-enriched word vectors (EWE) by Agrawal et al. (2018) do not perform well for word intensity prediction. Supervised method. For supervised methods, we report the mean correlation over 20 runs of the learning algorithm. The supervised models were chosen based on the proposed methods in the WASSA-2017 challenge (Mohammad and BravoMarquez, 2017), with minor simplifications to avoid overfitting, given the small size of our training set. As expected, the models succeed in learning emotional associations from word vectors with a higher correlation than unsupervised prediction. As this technique does not rely on cosine similarities, post-processing pro"
2021.emnlp-main.781,E17-2068,0,0.0162963,"applied to arbitrary emotions. To create the training labels for the selfsupervised model, we select the top 100 (k = 100) Unsupervised method. We next evaluate var- highest and lowest intensity predictions from our ious pretrained word vector models, including unsupervised model. This results in 200 automatiword2vec trained on Google News (Mikolov et al., cally induced training instances per emotion. For 2013), Glove 840B CommonCrawl (Pennington BERT, the vocabulary of fastText serves as the canet al., 2014), and fastText trained on Wikipedia didate set of words to calculate the top k predic(Joulin et al., 2017). We find that these outperform tions. the emotion lexicons by a substantial margin. They As expected, without access to gold standard also outperform BERT, for which we use the pre- training data, self-supervision is unable to comtrained BERT-base uncased model (Devlin et al., pete with the supervised approach. However, for 2019) and consider mean-pooled word-piece final a given set of word vectors, especially for BERT, layer output embeddings as word-level vectors. self-supervised learning mostly surpasses the unsuEven higher correlation can be attained with pervised approach despite drawing"
2021.emnlp-main.781,W10-0208,0,0.0360088,"emotions associated with words? Word vectors (Mikolov et al., Contribution. Overall, our intriguing finding in 2013; Pennington et al., 2014) have often been eval- this paper is that pretrained linguistic models allow uated on standard word relatedness benchmarks. In us to predict much more accurate emotion associthis paper, we instead explore to what extent they ations than state-of-the-art emotion lexicons. We encode emotional associations. show that this holds even without any supervision, Earlier methods (Strapparava and Mihalcea, while different kinds of supervised setups yield 2008; Mac Kim et al., 2010) used corpus statis- even better results. tics in tandem with dimensionality reduction techniques for emotion prediction. Shoeb et al. (2019) 2 Emotion Intensities as Associations considered word embeddings trained on emojis for emotion analysis. Rothe et al. (2016) proposed In our study, we seek to predict the emotions aspredicting sentiment polarity ratings from word sociated with individual words. Such predictions vectors, while other studies (Buechel and Hahn, can also be useful for sentence-level predictions, 9911 Proceedings of the 2021 Conference on Empirical Methods in Natural Language"
2021.emnlp-main.781,S18-2009,0,0.0197656,"along with our unsupervised, self-supervised, and regular supervised methods proposed in Section 3. Baselines. We first evaluate existing state-of-theart emotion lexicons. These provide either realvalued intensity scores or binary association scores (treated as 0.0 or 1.0). We found that that the scores that the lexicons provide exhibit very low correlation with the ground truth. In the case of EmoLex (Mohammad and Turney, 2013), this is because it merely provides binary labels, not intensities. For DepecheMood (Staiano and Guerini, 2014), DepecheMood++ (Araque et al., 2018), and EmoWordNet (Badaro et al., 2018), we conjecture that the data-driven automated techniques used to create them based on coarse-grained documentlevel labels do not result in word-level scores of the same sort as those solicited from human raters. The additional labels in DepecheMood and DepecheMood++ (e.g., Amused) may carry some information on some of the labels in our ground truth data (e.g., Joy). However, we find that mapping these emotions to a target emotion results in less accurate emotion association scores. Thus, we disregarded any labels not in the ground truth. tained by taking the PARAGRAM-SL999 vectors by Wieting"
2021.emnlp-main.781,C18-1245,0,0.0194507,"lf-supervised method of extracting emotional associations from pretrained vectors and models. Overall, we find that linguistic models carry substantial potential for inducing fine-grained emotion intensity scores, showing a far higher correlation with human ground truth ratings than state-of-the-art emotion lexicons based on labeled data. 1 Introduction 2018b; Buechel et al., 2018) predicted valence, arousal, and dominance using supervised deep neural networks. Khosla et al. (2018) proposed incorporating valence, arousal, and dominance ratings as additional signals into dense word embeddings. Buechel and Hahn (2018a) showed that emotion intensity scores can be predicted based on a lexicon providing valence–arousal–dominance ratings. The task of emotional association of words has been studied on other languages as well. Sidorov et al. (2012) presents a dataset of Spanish words labelled with Ekman’s six emotions, while others explored cross-lingual propagation from one language to another (Abdaoui et al., 2017) or to hundreds of other languages (Ramachandran and de Melo, 2020). We show that we can make use of linguistic models to obtain high correlations with emotion intensities without any need for manua"
2021.emnlp-main.781,W17-5205,0,0.0664379,"Missing"
2021.emnlp-main.781,N18-1173,0,0.018255,"lf-supervised method of extracting emotional associations from pretrained vectors and models. Overall, we find that linguistic models carry substantial potential for inducing fine-grained emotion intensity scores, showing a far higher correlation with human ground truth ratings than state-of-the-art emotion lexicons based on labeled data. 1 Introduction 2018b; Buechel et al., 2018) predicted valence, arousal, and dominance using supervised deep neural networks. Khosla et al. (2018) proposed incorporating valence, arousal, and dominance ratings as additional signals into dense word embeddings. Buechel and Hahn (2018a) showed that emotion intensity scores can be predicted based on a lexicon providing valence–arousal–dominance ratings. The task of emotional association of words has been studied on other languages as well. Sidorov et al. (2012) presents a dataset of Spanish words labelled with Ekman’s six emotions, while others explored cross-lingual propagation from one language to another (Abdaoui et al., 2017) or to hundreds of other languages (Ramachandran and de Melo, 2020). We show that we can make use of linguistic models to obtain high correlations with emotion intensities without any need for manua"
2021.emnlp-main.781,2020.acl-main.372,0,0.121599,"a given set of word vectors, especially for BERT, layer output embeddings as word-level vectors. self-supervised learning mostly surpasses the unsuEven higher correlation can be attained with pervised approach despite drawing on it for trainadditional post-processing. One example are the ing, owing to its ability to selectively pick out the counter-fitted vectors (Mrksic et al., 2016), ob- most pertinent cues from the vectors. 9914 4.2 Unsupervised Sentence Classification Experimental Setup. We additionally explore unsupervised sentence-level emotion classification on the GoEmotions dataset (Demszky et al., 2020) with its fine-grained inventory 28 different emotion labels. Given input document D, we choose the label X arg max γw,D σu (e, w), (6) e∈Σ w∈D where Σ is the set of labels, γw,D denotes the TF-IDF score of w in D, and σu (w, e) are word– emotion scores. For the latter, we use either our unsupervised word scoring (Eq. 1) or a baseline lexicon, where any emotion not covered by a resource is assumed to have intensity scores of 0.0. An exception is made for the neutral label, which we choose if the average prediction score across labels in Σ is ≤ 0.03, based on the reasoning that, similar to the"
2021.emnlp-main.781,N19-1423,0,0.0602449,"Missing"
2021.emnlp-main.781,J15-4004,0,0.0831131,"Missing"
2021.emnlp-main.781,L18-1027,0,0.196231,"ers explored cross-lingual propagation from one language to another (Abdaoui et al., 2017) or to hundreds of other languages (Ramachandran and de Melo, 2020). We show that we can make use of linguistic models to obtain high correlations with emotion intensities without any need for manual ratings. While this work focuses on English, the methodology can be applied to data in any language. There has been substantial research on methods to label words with associated emotions. Crowdsourcing approaches have been used to compile databases to study the nexus between them (Mohammad and Turney, 2013; Mohammad, 2018; Shoeb and de Melo, 2020). Another strategy, Overview. We introduce the task and experimenadopted by the well-known DepecheMood (Staiano tal setup in Section 2. Then, section 3 presents our and Guerini, 2014) and DepecheMood++ (Araque techniques to address this task based on linguistic et al., 2018) lexicons is to apply simple statistical models. We provide detailed empirical evaluation methods to emotionally tagged data crawled from results in Section 4. Finally, Section 5 concludes specific online sources. the paper by discussing the relevance of our experiIn this paper, we consider the que"
2021.emnlp-main.781,N16-1018,0,0.0211557,"tperform BERT, for which we use the pre- training data, self-supervision is unable to comtrained BERT-base uncased model (Devlin et al., pete with the supervised approach. However, for 2019) and consider mean-pooled word-piece final a given set of word vectors, especially for BERT, layer output embeddings as word-level vectors. self-supervised learning mostly surpasses the unsuEven higher correlation can be attained with pervised approach despite drawing on it for trainadditional post-processing. One example are the ing, owing to its ability to selectively pick out the counter-fitted vectors (Mrksic et al., 2016), ob- most pertinent cues from the vectors. 9914 4.2 Unsupervised Sentence Classification Experimental Setup. We additionally explore unsupervised sentence-level emotion classification on the GoEmotions dataset (Demszky et al., 2020) with its fine-grained inventory 28 different emotion labels. Given input document D, we choose the label X arg max γw,D σu (e, w), (6) e∈Σ w∈D where Σ is the set of labels, γw,D denotes the TF-IDF score of w in D, and σu (w, e) are word– emotion scores. For the latter, we use either our unsupervised word scoring (Eq. 1) or a baseline lexicon, where any emotion not"
2021.emnlp-main.781,D14-1162,0,0.101129,"Missing"
2021.emnlp-main.781,2020.coling-main.517,1,0.799415,"Missing"
2021.emnlp-main.781,N16-1091,0,0.0158217,"In us to predict much more accurate emotion associthis paper, we instead explore to what extent they ations than state-of-the-art emotion lexicons. We encode emotional associations. show that this holds even without any supervision, Earlier methods (Strapparava and Mihalcea, while different kinds of supervised setups yield 2008; Mac Kim et al., 2010) used corpus statis- even better results. tics in tandem with dimensionality reduction techniques for emotion prediction. Shoeb et al. (2019) 2 Emotion Intensities as Associations considered word embeddings trained on emojis for emotion analysis. Rothe et al. (2016) proposed In our study, we seek to predict the emotions aspredicting sentiment polarity ratings from word sociated with individual words. Such predictions vectors, while other studies (Buechel and Hahn, can also be useful for sentence-level predictions, 9911 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 9911–9917 c November 7–11, 2021. 2021 Association for Computational Linguistics as considered in Section 4.2, but that is not the primary focus of this paper. While many emotion lexicons only provide binary emotion labels, it is clear that words m"
2021.emnlp-main.781,2020.emnlp-main.720,1,0.889959,"Missing"
2021.emnlp-main.781,R19-1126,1,0.884359,"Missing"
2021.emnlp-main.781,P14-2070,0,0.0326435,"l lexicon baselines at significance level 0.001. We compare a series of baselines along with our unsupervised, self-supervised, and regular supervised methods proposed in Section 3. Baselines. We first evaluate existing state-of-theart emotion lexicons. These provide either realvalued intensity scores or binary association scores (treated as 0.0 or 1.0). We found that that the scores that the lexicons provide exhibit very low correlation with the ground truth. In the case of EmoLex (Mohammad and Turney, 2013), this is because it merely provides binary labels, not intensities. For DepecheMood (Staiano and Guerini, 2014), DepecheMood++ (Araque et al., 2018), and EmoWordNet (Badaro et al., 2018), we conjecture that the data-driven automated techniques used to create them based on coarse-grained documentlevel labels do not result in word-level scores of the same sort as those solicited from human raters. The additional labels in DepecheMood and DepecheMood++ (e.g., Amused) may carry some information on some of the labels in our ground truth data (e.g., Joy). However, we find that mapping these emotions to a target emotion results in less accurate emotion association scores. Thus, we disregarded any labels not i"
2021.emnlp-main.781,Q15-1025,0,0.0272601,"., 2018), we conjecture that the data-driven automated techniques used to create them based on coarse-grained documentlevel labels do not result in word-level scores of the same sort as those solicited from human raters. The additional labels in DepecheMood and DepecheMood++ (e.g., Amused) may carry some information on some of the labels in our ground truth data (e.g., Joy). However, we find that mapping these emotions to a target emotion results in less accurate emotion association scores. Thus, we disregarded any labels not in the ground truth. tained by taking the PARAGRAM-SL999 vectors by Wieting et al. (2015) and optimizing them using synonymy and antonymy constraints. The best results are obtained using AffectVec (Raji and de Melo, 2020), which post-processes the same vectors using not only synonymy and antonymy constraints, but also sentiment polarity ones. Sentiment polarity evidently helps to better distinguish different emotional associations. In contrast, the emotion-enriched word vectors (EWE) by Agrawal et al. (2018) do not perform well for word intensity prediction. Supervised method. For supervised methods, we report the mean correlation over 20 runs of the learning algorithm. The superv"
2021.lantern-1.3,N18-2121,0,0.025467,"ommon word (ignoring stop words) between caption and comments. These variants are suffixed overlap – e.g. +NE-overlap. We report experimental results on all of these variants, adopting a 30,000/8,000/8,000 train/val/test split for each of them. Related Work Image Captioning. Prior work on captioning conditioned only on images (Farhadi et al., 2010; Vinyals et al., 2015; Karpathy and Li, 2015; Krause et al., 2017) has been successful for descriptive captions with explicit grounding to image objects. Recently, captions with sentimental and abstract concepts have been explored (Gan et al., 2017; Chandrasekaran et al., 2018; Park et al., 2017; Liu et al., 2018; Shuster et al., 2019). Although external knowledge bases like DBpedia (factual knowledge) (Wu et al., 2018) and ConceptNet (commonsense knowledge) (Zhou et al., 2019) have been leveraged, all prior work ignores the knowledge present in the text surrounding images in social media and other domains. Contextual Image Captioning leverages the latter kind of knowledge. Multimodal Summarization. Research on multimodal embeddings (Laina et al., 2019; Xia et al., 2020; Scialom et al., 2020) has facilitated studying image–text data. Summarization of multimodal doc"
2021.louhi-1.4,K18-1050,0,0.104962,"sing any semantic type information, our model significantly out-performs two recent biomedical entity linking models – MedType (Vashishth et al., 2020) and SciSpacy (Neumann et al., 2019) – on two benchmark datasets. 2 2.1 Biomedical Entity Linking 2.3 End-to-End Entity Linking End-to-end entity linking refers to the task of predicting mention spans and the corresponding target entities jointly using a single model. Traditionally, span detection and entity disambiguation tasks were done in a pipelined approach, making these approaches susceptible to error propagation. To alleviate this issue, Kolitsas et al. (2018) proposed a neural end-to-end model that performs the dual tasks of mention span detection and entity disambiguation. However, for span detection and disambiguation, their method relies on an empirical probabilistic entity mapping p(e|m) to select a candidate set C(m) for each mention m. Such mention–entity prior p(e|m) is not available in every domain, especially in the biomedical domain that we consider in this paper. In contrast, our method does not rely on any extrinsic sources of information. Recently, Furrer et al. (2020) proposed a parallel sequence tagging model that treats both span d"
2021.louhi-1.4,W03-0428,0,0.25291,"Missing"
2021.louhi-1.4,N19-1423,0,0.0390981,"task of identifying mentions of named entities (or other terms) in a text document and disambiguating them by mapping them to canonical entities (or concepts) listed in a reference knowledge graph (Hogan et al., 2020). This is an essential step in information extraction, and therefore has been studied extensively both in domainspecific and domain-agnostic settings. Recent stateof-the-art models (Logeswaran et al., 2019; Wu et al., 2019) attempt to learn better representations of mentions and candidates using the rich contextual information encoded in pre-trained language models such as BERT (Devlin et al., 2019). These models follow a retrieve and rerank paradigm, which consists of two separate steps: First, the can28 Proceedings of the 12th International Workshop on Health Text Mining and Information Analysis, pages 28–37 April 19, 2021. ©2021 Association for Computational Linguistics 2.2 (1) Biomedical text typically contains substantial domain-specific jargon and abbreviations. For example, CT could stand for Computed tomography or Copper Toxicosis. (2) The target concepts in the knowledge base often have very similar surface forms, making the disambiguation task difficult. For example, Pseudomona"
2021.louhi-1.4,2020.emnlp-main.522,0,0.0329518,"an be pre-computed and cached. The inference task is thus reduced to finding the maximum dot product between each mention representation and all entity representations. (3) e∈E 3.4 + we |hj q=i Training and Inference |e tˆk = arg max{(um k ) v } σ(ws |hi End-to-End Entity Linking Many of the state-of-the-art entity disambiguation models assume that gold mention spans are available during test time and thus have limited applicability in real-world entity linking tasks, where such gold mentions are typically not available. To avoid this, recent works (Kolitsas et al., 2018; F´evry et al., 2020; Li et al., 2020) have investigated end-to-end entity linking, where a model needs to perform both mention span detection and entity disambiguation. 4 Mention Span Detection We experiment with two different methods for mention span detection with different computational complexity. In our first method, following F´evry et al. (2020), we use a simple BIO tagging scheme to identify the mention spans. Every token in the input text is annotated with one of these three tags. Under this tagging scheme, any contiguous segment of tokens starting with a B tag and followed by I tags is treated as a mention. Although thi"
2021.louhi-1.4,K19-1049,0,0.19272,"r the candidate entities. A schematic diagram of the model is presented in Figure 1. Following the BERT model, the input sequences to these encoders start and end with the special tokens [CLS] and [SEP], respectively. 3.2 Candidate Selection Candidate Retrieval Since the entity disambiguation task is formulated as a learning to rank problem, we need to retrieve negative candidate entities for ranking during training. To this end, we randomly sample a set of negative candidates from the pool of all entities in the knowledge base. Additionally, we adopt the hard negative mining strategy used by Gillick et al. (2019) to retrieve negative candidates by performing nearest neighbor search using the dense representations of mentions and Mention Encoder Given an input text document [xd1 , . . . , xdT ] of T tokens with M mentions, the output of the final layer of the encoder, denoted by [h1 , . . . , hT ], is a contextualized representation of the input tokens. For each mention span (i, j), we concatenate the first and the last tokens of the span and pass it through a linear layer to obtain the 30 candidates described above. The hard negative candidates are the entities that are more similar to the mention tha"
2021.louhi-1.4,P19-1335,0,0.451836,"entity disambiguation and out-performs two recently proposed models. 1 Gerard de Melo Hasso Plattner Institute University of Potsdam Potsdam, Germany gdm@demelo.org Introduction Entity linking is the task of identifying mentions of named entities (or other terms) in a text document and disambiguating them by mapping them to canonical entities (or concepts) listed in a reference knowledge graph (Hogan et al., 2020). This is an essential step in information extraction, and therefore has been studied extensively both in domainspecific and domain-agnostic settings. Recent stateof-the-art models (Logeswaran et al., 2019; Wu et al., 2019) attempt to learn better representations of mentions and candidates using the rich contextual information encoded in pre-trained language models such as BERT (Devlin et al., 2019). These models follow a retrieve and rerank paradigm, which consists of two separate steps: First, the can28 Proceedings of the 12th International Workshop on Health Text Mining and Information Analysis, pages 28–37 April 19, 2021. ©2021 Association for Computational Linguistics 2.2 (1) Biomedical text typically contains substantial domain-specific jargon and abbreviations. For example, CT could stan"
2021.louhi-1.4,W19-5034,0,0.0875379,"rs that perform per-mention entity disambiguation. At inference time, our model is 3-25x faster than other comparable models. • At the same time, our model obtains favorable results on two biomedical datasets compared to state-of-the-art entity linking models. • Our model can also perform end-to-end entity linking when trained with the multi-task objective of mention span detection and entity disambiguation. We show that without using any semantic type information, our model significantly out-performs two recent biomedical entity linking models – MedType (Vashishth et al., 2020) and SciSpacy (Neumann et al., 2019) – on two benchmark datasets. 2 2.1 Biomedical Entity Linking 2.3 End-to-End Entity Linking End-to-end entity linking refers to the task of predicting mention spans and the corresponding target entities jointly using a single model. Traditionally, span detection and entity disambiguation tasks were done in a pipelined approach, making these approaches susceptible to error propagation. To alleviate this issue, Kolitsas et al. (2018) proposed a neural end-to-end model that performs the dual tasks of mention span detection and entity disambiguation. However, for span detection and disambiguation,"
2021.louhi-1.4,2020.acl-main.748,0,0.239295,"the density of mentions per document make the biomedical entity linking very challenging. In the biomedical domain, there are many existing tools, such as TaggerOne (Leaman and Lu, 2016), MetaMap (Aronson, 2006), cTAKES (Savova et al., 2010), QuickUMLS (Soldaini and Goharian, 2016), among others, for normalizing mentions of biomedical concepts to a biomedical thesaurus. Most of these methods rely on feature-based approaches. Recently, Zhu et al. (2019) proposed a model that utilizes the latent semantic information of mentions and entities to perform entity linking. Other recent models such as Xu et al. (2020) and Vashishth et al. (2020) also leverage semantic type information for improved entity disambiguation. Our work is different from these approaches, as our model does not use semantic type information, since such information may not always be available. Recent studies such as Xu et al. (2020) and Ji et al. (2020) deploy a BERT-based retrieve and re-rank model. In contrast, our model does not rely on a separate re-ranker model, which significantly improves its efficiency. Contributions The key contributions of our work are as follows. • Training our collective entity disambiguation model is 3x"
2021.naacl-main.245,P19-1081,0,0.0527026,"Missing"
borin-etal-2014-bring,J91-1002,0,\N,Missing
borin-etal-2014-bring,P08-1048,0,\N,Missing
borin-etal-2014-bring,P98-1100,0,\N,Missing
borin-etal-2014-bring,C98-1097,0,\N,Missing
borin-etal-2014-bring,de-melo-weikum-2008-mapping,1,\N,Missing
borin-etal-2014-bring,borin-etal-2012-open,1,\N,Missing
borin-etal-2014-bring,O95-1004,0,\N,Missing
C12-3055,I11-1077,0,0.0252464,"439–446, COLING 2012, Mumbai, December 2012. 439 1 Introduction For a search query like “what is both edible and poisonous?”, most search engines retrieve pages with the keywords edible and poisonous, but users increasingly expect direct answers like pufferfish. If a user wants hot drinks, a mobile assistant like Siri should search for cafés, not sleazy bars. Commonsense knowledge (CSK) has a wide range of applications, from high-level applications like mobile assistants and commonsense-aware search engines (Hsu et al., 2006) to NLP tasks like textual entailment and word sense disambiguation (Chen and Liu, 2011). State-of-the-art sources like Cyc and ConceptNet have limited coverage, while automated information extraction (IE) typically suffers from low accuracy (Tandon et al., 2011), as IE patterns can be noisy and ambiguous. Large-scale high precision IE remains very challenging, and facts extracted by existing systems thus have rarely been put to practical use. Previously, Li et al. (Li et al., 2011) filtered facts extracted from a large corpus by propagating scores from human seed facts to related facts and contexts. However, their method does not handle the very ambiguous patterns typical of CSK"
C12-3055,C10-1057,0,0.0193261,"he-art sources like Cyc and ConceptNet have limited coverage, while automated information extraction (IE) typically suffers from low accuracy (Tandon et al., 2011), as IE patterns can be noisy and ambiguous. Large-scale high precision IE remains very challenging, and facts extracted by existing systems thus have rarely been put to practical use. Previously, Li et al. (Li et al., 2011) filtered facts extracted from a large corpus by propagating scores from human seed facts to related facts and contexts. However, their method does not handle the very ambiguous patterns typical of CSK. FactRank (Jain and Pantel, 2010) uses a simple graph of facts to find mistyped or out-of-domain arguments. However, they do not exploit the large number of seeds provided by databases like ConceptNet for robust pattern filtering. In contrast, our work proposes a joint model of candidate facts, seed facts, patterns and relations geared towards Web-scale CSK extractions. Standard IE deals with fact patterns like &lt;X&gt; is married to &lt;Y&gt; that are fairly reliable, so the same tuple is rarely encountered for multiple relations simultaneously and the resulting graphs are sparse. Our approach instead deals with CSK IE. Owing to the mu"
C12-3055,P06-1015,0,0.0859571,"with a graph-based Markov chain model that leverages rich Web-scale statistics from one or more large-scale extraction sources in order to achieve high accuracy. We develop an online system that can lookup commonsense property knowledge in real time. We make use of anchored patterns for fast lookup. The system provides a ranked property tag cloud. These scores are obtained using our graph-based Markov chain model. 2 Approach Tuple Graph Representation. We follow the standard bootstrapped IE methodology, where seed facts lead to patterns, which in turn induce newly discovered candidate facts (Pantel and Pennacchiotti, 2006). We apply it, however, to Web-scale N-Grams data. For a given candidate fact T , a directed tuple graph GT = (V, E) is created. The node set includes nodes for the candidate tuple T , for the pattern set P that extracted T , for the seed set S that induced P, as well as all for the relations R that s ∈ S belong to, plus an additional artificial relation node NO_RELATION to account for noise. A weighted edge from the tuple node vt to one or more patterns vp corresponds (after normalization) to a tuple probability P r(p|t), which is estimated using the pattern frequency in an extraction source"
C12-3055,N10-2012,0,0.0143083,"e developed an index lookup service over Google n-grams such that given a SIP, all relevant Google 5-grams are fetched. 5-grams provide the largest context and are therefore preferred over 4-grams. • Bing N-grams Web service caller: Microsoft’s Web N-gram Corpus is based on the complete collection of documents indexed for the English US version of the Bing search engine. The dataset is not distributed as such but made accessible by means of a Web service described using the WSDL standard. The service provides smoothed n-gram language model based probability scores rather than raw frequencies (Wang et al., 2010). We enable SIP lookup over this service. The results from these resources are then merged to generate tuple statistics of the form: x,y,[pattern:score]. These statistics form the input to our Markov chain method which provides a scored list of facts which are then displayed. Due to the locality of our approach, the system operates online. Figure 2 shows the flow of the system. Figure 3,4 provides screenshot for the output of property lookup for flower and fish at pattern support 2. 4 Experiments 4.1 Experimental Setup Using ConceptNet, we obtain patterns of the form X_NN is very Y_JJ, where t"
D17-1110,P82-1020,0,0.845286,"Missing"
D17-1110,P14-1062,0,0.0119275,"ent length; while each similarity matrix contains the same number of document term scores, longer documents have more opportunity to contain terms that are similar to query terms. To capture the strongest ns similarity signals for each query term, we first perform max pooling over the filter dimension nf to keep only the strongest signal from the nf different filters, assuming that there only exists one particular true matching pattern in a given n × n window, which serves different purposes compared with other tasks, such as the sub-sampling in computer vision. We then perform k-max pooling (Kalchbrenner et al., 2014) over the query dimension lq to keep the strongest ns similarity signals for each query term. Both pooling steps are performed on each of the lg − 1 matrices C i from the convolutional layer and on the original similarity matrix, which captures unigram matching, to produce the 3-dimensional tensor Plq ×lg ×ns . This tensor contains the ns strongest signals for each query term and for each n-gram size across all nf filters. Recurrent layer for global relevance. Finally, our model transforms the query term similarity signals in Plq ×lg ×ns into a single document relevance score rel (q, d). It ac"
D19-1658,P15-1162,0,0.0918153,"Missing"
D19-1658,P18-1073,0,0.067409,"Missing"
D19-1658,N06-1020,0,0.101244,"m a rapid decline during self-learning with throttling. This is because selecting from all samples leads to an imbalance between different classes and due to repeated error amplification this means that samples are increasingly likely to be assigned to the majority class in each self-learning iteration. 4 Related Work Semi-supervised Learning. There is a long history of research on semi-supervised Learning to exploit unlabeled data. Self-learning (also known as self-training) was successfully applied to NLP tasks in early work such as on word sense disambiguation (Yarowsky, 1995) and parsing (McClosky et al., 2006). In recent work, Artetxe et al. (2018) show that self-learning can iteratively improve unsupervised cross-lingual word embeddings. Clark et al. (2018) presents Cross-View Training, a new self-training algorithm that works well for neural sequence modeling. Other semisupervised methods, such as co-training (Blum and Mitchell, 1998) and tri-training (Zhou and Li, 2005), have as well been used for sentiment analysis. Ruder and Plank (2018) propose a novel multitask tri-training method that reduces the time and space complexity of classic tri-training for sentiment analysis. For cross-lingual sen"
D19-1658,N18-1202,0,0.322049,"ns on unlabeled nonEnglish data in order to obtain additional information that can be used during further finetuning. Compared with original multilingual models and other cross-lingual classification models, we observe significant gains in effectiveness on document and sentiment classification for a range of diverse languages. 1 Introduction Owing to notable advances in deep learning and representation learning, important progress has been achieved on text classification, reading comprehension, and other NLP tasks. Recently, pretrained language representations with self-supervised objectives (Peters et al., 2018; Devlin et al., 2018; Radford et al., 2018) have further pushed forward the state-of-the-art on many English tasks. While these sorts of deep models can be trained on different languages, deep models typically require substantial amounts of labeled data for the specific domain of data. Unfortunately, the cost of acquiring new custom-built resources for each combination of language and domain is very high, as it typically requires human annotation. Available resources for domain-specific tasks are often imbalanced between different languages. The scarcity of nonEnglish annotated corpora may pr"
D19-1658,P18-1096,0,0.0193232,"-learning (also known as self-training) was successfully applied to NLP tasks in early work such as on word sense disambiguation (Yarowsky, 1995) and parsing (McClosky et al., 2006). In recent work, Artetxe et al. (2018) show that self-learning can iteratively improve unsupervised cross-lingual word embeddings. Clark et al. (2018) presents Cross-View Training, a new self-training algorithm that works well for neural sequence modeling. Other semisupervised methods, such as co-training (Blum and Mitchell, 1998) and tri-training (Zhou and Li, 2005), have as well been used for sentiment analysis. Ruder and Plank (2018) propose a novel multitask tri-training method that reduces the time and space complexity of classic tri-training for sentiment analysis. For cross-lingual sentiment analysis, Wan (2009) uses machine translation to directly convert English training data to Chinese, which provides two views for co-training. Xu and Yang (2017) propose to use soft probabilistic predictions for the documents in a label-rich language as the (induced) supervisory labels in a parallel corpus of documents, while there is no need to use parallel corpora in our work. Chen et al. (2018) propose an Adversarial Deep Averag"
D19-1658,L18-1560,0,0.175098,"simultaneously pretrained on 104 languages, and has shown to perform well for cross-lingual transfer of natural language tasks (Wu and Dredze, 2019). Our model begins by learning just from available English samples, but then makes predictions on unlabeled non-English samples and a part of those samples with high confidence prediction scores are repurposed to serve as labeled examples for a next iteration of fine-tuning until the model converges. Based on this multilingual self-learning technique, we demonstrate the superiority of our framework on Multilingual Document Classification (MLDoc) (Schwenk and Li, 2018) in comparison with several strong baselines. Our study then proceeds to show that our method is better on Chinese sentiment classification than other cross-lingual methods that also consider unla6306 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 6306–6310, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics beled non-English data. This shows that our method is more effective at cross-lingual transfer for domain-specific tasks, using a mix"
D19-1658,P09-1027,0,0.0438886,"Artetxe et al. (2018) show that self-learning can iteratively improve unsupervised cross-lingual word embeddings. Clark et al. (2018) presents Cross-View Training, a new self-training algorithm that works well for neural sequence modeling. Other semisupervised methods, such as co-training (Blum and Mitchell, 1998) and tri-training (Zhou and Li, 2005), have as well been used for sentiment analysis. Ruder and Plank (2018) propose a novel multitask tri-training method that reduces the time and space complexity of classic tri-training for sentiment analysis. For cross-lingual sentiment analysis, Wan (2009) uses machine translation to directly convert English training data to Chinese, which provides two views for co-training. Xu and Yang (2017) propose to use soft probabilistic predictions for the documents in a label-rich language as the (induced) supervisory labels in a parallel corpus of documents, while there is no need to use parallel corpora in our work. Chen et al. (2018) propose an Adversarial Deep Averaging Network to learn invariance across languages, which is another baseline considered in our experiments. Cross-lingual Representation Learning. With models such as ELMo (Peters et al.,"
D19-1658,D19-1077,0,0.0337273,"r, 2007), alleviating the training bottleneck issues for low-resource languages. This is facilitated by recent advances in learning joint multilingual representations (Lample and Conneau, 2019; Artetxe and Schwenk, 2018; Devlin et al., 2018). In our work, we propose a self-learning framework to incorporate the predictions of the multilingual BERT model (Devlin et al., 2018) on nonEnglish data into an English training procedure. The initial multilingual BERT model was simultaneously pretrained on 104 languages, and has shown to perform well for cross-lingual transfer of natural language tasks (Wu and Dredze, 2019). Our model begins by learning just from available English samples, but then makes predictions on unlabeled non-English samples and a part of those samples with high confidence prediction scores are repurposed to serve as labeled examples for a next iteration of fine-tuning until the model converges. Based on this multilingual self-learning technique, we demonstrate the superiority of our framework on Multilingual Document Classification (MLDoc) (Schwenk and Li, 2018) in comparison with several strong baselines. Our study then proceeds to show that our method is better on Chinese sentiment cla"
D19-1658,P17-1130,0,0.260834,"ingual sentiment classification, we consider several diverse baselines as listed in Table 2. mSDA (Chen et al., 2012) is a very effective method for cross-domain sentiment classification on Amazon reviews, which can also be used in cross-lingual tasks, but it has the worst performance. Deep Averaging Networks (DANs) by Iyyer et al. (2015) consider an arithmetic mean of word vectors as a sentence representation and pass it to a classification module, while Chen et al. (2018) translate the Chinese test text into English as a machine translation baseline. The third category of baselines includes Xu and Yang (2017), who propose a crosslingual distillation (CLD) method that makes use of soft source predictions on a parallel corpus to train a target model (CLD-KCNN). They further propose an improved variant (CLDFA-KCNN) that utilizes adversarial training for domain adaptation within a single language. Adversarial DAN (ADAN) by Chen et al. (2018) is another state of the art baseline that improves cross-lingual generalization by means of adversarial training. We also run experiments on multilingual BERT and observe that it does not outperform CLD-based CLTC and ADAN, while our approach achieves the new stat"
D19-1658,P95-1026,0,0.661268,"w that the results suffer from a rapid decline during self-learning with throttling. This is because selecting from all samples leads to an imbalance between different classes and due to repeated error amplification this means that samples are increasingly likely to be assigned to the majority class in each self-learning iteration. 4 Related Work Semi-supervised Learning. There is a long history of research on semi-supervised Learning to exploit unlabeled data. Self-learning (also known as self-training) was successfully applied to NLP tasks in early work such as on word sense disambiguation (Yarowsky, 1995) and parsing (McClosky et al., 2006). In recent work, Artetxe et al. (2018) show that self-learning can iteratively improve unsupervised cross-lingual word embeddings. Clark et al. (2018) presents Cross-View Training, a new self-training algorithm that works well for neural sequence modeling. Other semisupervised methods, such as co-training (Blum and Mitchell, 1998) and tri-training (Zhou and Li, 2005), have as well been used for sentiment analysis. Ruder and Plank (2018) propose a novel multitask tri-training method that reduces the time and space complexity of classic tri-training for senti"
D19-1658,Q18-1039,0,0.0324001,"n used for sentiment analysis. Ruder and Plank (2018) propose a novel multitask tri-training method that reduces the time and space complexity of classic tri-training for sentiment analysis. For cross-lingual sentiment analysis, Wan (2009) uses machine translation to directly convert English training data to Chinese, which provides two views for co-training. Xu and Yang (2017) propose to use soft probabilistic predictions for the documents in a label-rich language as the (induced) supervisory labels in a parallel corpus of documents, while there is no need to use parallel corpora in our work. Chen et al. (2018) propose an Adversarial Deep Averaging Network to learn invariance across languages, which is another baseline considered in our experiments. Cross-lingual Representation Learning. With models such as ELMo (Peters et al., 2018), GPT2 (Radford et al., 2018), and BERT (Devlin et al., 2018), important progress has been made in learning improved sentence representations with context-specific encodings of words via a language modeling objective. The latter two approaches both rely on Transformer encoders, but BERT is trained using masked language modeling instead of right-to-left or left-to-right l"
D19-1658,D18-1217,0,0.0161622,"to repeated error amplification this means that samples are increasingly likely to be assigned to the majority class in each self-learning iteration. 4 Related Work Semi-supervised Learning. There is a long history of research on semi-supervised Learning to exploit unlabeled data. Self-learning (also known as self-training) was successfully applied to NLP tasks in early work such as on word sense disambiguation (Yarowsky, 1995) and parsing (McClosky et al., 2006). In recent work, Artetxe et al. (2018) show that self-learning can iteratively improve unsupervised cross-lingual word embeddings. Clark et al. (2018) presents Cross-View Training, a new self-training algorithm that works well for neural sequence modeling. Other semisupervised methods, such as co-training (Blum and Mitchell, 1998) and tri-training (Zhou and Li, 2005), have as well been used for sentiment analysis. Ruder and Plank (2018) propose a novel multitask tri-training method that reduces the time and space complexity of classic tri-training for sentiment analysis. For cross-lingual sentiment analysis, Wan (2009) uses machine translation to directly convert English training data to Chinese, which provides two views for co-training. Xu"
de-melo-2014-etymological,P12-3026,1,\N,Missing
de-melo-2014-etymological,P13-1064,0,\N,Missing
de-melo-etal-2012-empirical,passonneau-etal-2010-word,1,\N,Missing
de-melo-etal-2012-empirical,W09-1127,0,\N,Missing
de-melo-etal-2012-empirical,W09-3021,1,\N,Missing
de-melo-weikum-2008-mapping,C92-2082,0,\N,Missing
de-melo-weikum-2008-mapping,H94-1025,0,\N,Missing
de-melo-weikum-2010-providing,W09-4407,1,\N,Missing
de-melo-weikum-2010-providing,buscaldi-rosso-2008-geo,0,\N,Missing
de-melo-weikum-2010-providing,P06-2037,0,\N,Missing
de-melo-weikum-2010-providing,bond-etal-2008-boot,0,\N,Missing
de-paiva-etal-2014-nomlex,W04-2705,0,\N,Missing
de-paiva-etal-2014-nomlex,P98-1013,0,\N,Missing
de-paiva-etal-2014-nomlex,C98-1013,0,\N,Missing
de-paiva-etal-2014-nomlex,W14-0153,1,\N,Missing
de-paiva-etal-2014-nomlex,santos-bick-2000-providing,0,\N,Missing
de-paiva-etal-2014-nomlex,padro-stanilovsky-2012-freeling,0,\N,Missing
de-paiva-etal-2014-nomlex,C12-3044,1,\N,Missing
I17-5002,D17-1070,0,0.0398894,"(2014) train a siamese-style network architecture on a parallel corpus such that it learns to compose sentence representations into document representations. Finally, when documents are to be compared against short queries, it is important to consider the peculiarities of relevance modeling (Vuli´c and Moens, 2015b; Hui et al., 2017), which differs from semantic similarity modeling. External Supervision. Wieting et al. (2015) explored using supervision from paraphrase information to obtain custom-tailored word vectors that give rise to high-quality sentence embeddings. The InferSent approach (Conneau et al., 2017) relies on supervision from the Stanford Natural Language Inference data as an auxiliary task to obtain sentence representations. In terms of cross-lingual methods, neural machine translation based on sequence-tosequence learning can give rise to vector encodings of multilingual input sentences (Luong et al., 2015). These have been shown to be semantically meaningful (Schwenk and Douze, 2017). 4 5 Conclusion In summary, vector representations have made it easier to target multilingual and cross-lingual semantics. This is possible both at the level of individual words as well as at the level of"
I17-5002,2015.mtsummit-papers.27,1,0.871662,"Missing"
I17-5002,E14-1049,0,0.0863391,"Missing"
I17-5002,N16-1083,0,0.0117843,"y average the word vectors of words in a given sentence. Despite its simplicity, this method often works surprisingly well (Wieting et al., 2015; Arora et al., 2017). An early attempt to incorporate sentences more explicitly into the objective function was proposed with the Paragraph Vectors approach (Le and Mikolov, 2014). This method is also occasionally referred to as doc2vec, as it straightforwardly extends word2vec to additionally create representations of sentences or other longer units. Several authors have devised bilingual variants of the Paragraph Vector approach (Pham et al., 2015; Mogadala and Rettinger, 2016). The Skip-Thought Vector approach (Kiros et al., 2015), while also inspired by the word2vec skipgram method, instead draws on recurrent units to encode and decode sentence representations such that the resulting representations are optimized for predicting neighbouring sentences. may be more costly in terms of the resources used, and may neglect language-specific subtleties. Still, in practice, it does appear to be a strong baseline (de Melo and Siersdorfer, 2007). Modeling Document Semantics. While many methods treat sentences and documents as interchangeable, there are significant differenc"
I17-5002,W15-1512,0,0.0184647,"trategy is to simply average the word vectors of words in a given sentence. Despite its simplicity, this method often works surprisingly well (Wieting et al., 2015; Arora et al., 2017). An early attempt to incorporate sentences more explicitly into the objective function was proposed with the Paragraph Vectors approach (Le and Mikolov, 2014). This method is also occasionally referred to as doc2vec, as it straightforwardly extends word2vec to additionally create representations of sentences or other longer units. Several authors have devised bilingual variants of the Paragraph Vector approach (Pham et al., 2015; Mogadala and Rettinger, 2016). The Skip-Thought Vector approach (Kiros et al., 2015), while also inspired by the word2vec skipgram method, instead draws on recurrent units to encode and decode sentence representations such that the resulting representations are optimized for predicting neighbouring sentences. may be more costly in terms of the resources used, and may neglect language-specific subtleties. Still, in practice, it does appear to be a strong baseline (de Melo and Siersdorfer, 2007). Modeling Document Semantics. While many methods treat sentences and documents as interchangeable,"
I17-5002,N15-1157,0,0.0125954,"now ubiquitous in all subfields of natural language processing and text mining. While methods such as word2vec and GloVe are wellknown, multilingual and cross-lingual vector representations have also become important. In particular, such representations can not only describe words, but also of entire sentences and documents as well. 1 Parallel Corpora Approaches. The second strategy is to rely on parallel corpora and directly optimize a cross-lingual objective that considers sentence translations. Examples include the methods proposed by Klementiev et al. (2012), Koˇcisk´y et al. (2014), and Gouws and Søgaard (2015). Some of these simply use aligned sentences, while others require word alignments. Vuli´c and Moens (2015a) showed that comparable documents may suffice to learn bilingual embeddings. Introduction Vector representations are ubiquitous in all subfields of natural language processing and text mining. Well-known neural methods such as word2vec and GloVe enable us to obtain distributed vector representations of words, overcoming some of the sparsity issues faced by traditional distributional semantics methods. Such representations are learnt from co-occurrence information drawn from large monolin"
I17-5002,W17-2619,0,0.0139212,"al Supervision. Wieting et al. (2015) explored using supervision from paraphrase information to obtain custom-tailored word vectors that give rise to high-quality sentence embeddings. The InferSent approach (Conneau et al., 2017) relies on supervision from the Stanford Natural Language Inference data as an auxiliary task to obtain sentence representations. In terms of cross-lingual methods, neural machine translation based on sequence-tosequence learning can give rise to vector encodings of multilingual input sentences (Luong et al., 2015). These have been shown to be semantically meaningful (Schwenk and Douze, 2017). 4 5 Conclusion In summary, vector representations have made it easier to target multilingual and cross-lingual semantics. This is possible both at the level of individual words as well as at the level of sentences or even entire documents. Biography Multilingual Document Vectors Gerard de Melo is an Assistant Professor of Computer Science at Rutgers University, heading a team of researchers working on NLP, Big Data analytics, and web mining. He has published over 80 papers on these topics, with Best Paper/Demo awards at WWW 2011, CIKM 2010, ICGL 2008, the NAACL 2015 Workshop on Vector Space"
I17-5002,P14-1006,0,0.0195036,"es and documents as interchangeable, there are significant differences between the two. Methods that focus specifically on properties of documents have the potential to yield higher-quality document-level embeddings. Bagof-words vectors can be rendered cross-lingual by translating individual words (Song et al., 2016), or by moving from original words to bag-of-concept representations (de Melo and Siersdorfer, 2007), optionally drawing on distributed vectors for concepts (de Melo, 2017). Representations may also account for the salience of different parts of the text (Yang et al., 2016, 2017). Hermann and Blunsom (2014) train a siamese-style network architecture on a parallel corpus such that it learns to compose sentence representations into document representations. Finally, when documents are to be compared against short queries, it is important to consider the peculiarities of relevance modeling (Vuli´c and Moens, 2015b; Hui et al., 2017), which differs from semantic similarity modeling. External Supervision. Wieting et al. (2015) explored using supervision from paraphrase information to obtain custom-tailored word vectors that give rise to high-quality sentence embeddings. The InferSent approach (Connea"
I17-5002,P15-2118,0,0.0383795,"Missing"
I17-5002,C12-1089,0,0.168616,"Gouws et al. (2015). Neural vector representations are now ubiquitous in all subfields of natural language processing and text mining. While methods such as word2vec and GloVe are wellknown, multilingual and cross-lingual vector representations have also become important. In particular, such representations can not only describe words, but also of entire sentences and documents as well. 1 Parallel Corpora Approaches. The second strategy is to rely on parallel corpora and directly optimize a cross-lingual objective that considers sentence translations. Examples include the methods proposed by Klementiev et al. (2012), Koˇcisk´y et al. (2014), and Gouws and Søgaard (2015). Some of these simply use aligned sentences, while others require word alignments. Vuli´c and Moens (2015a) showed that comparable documents may suffice to learn bilingual embeddings. Introduction Vector representations are ubiquitous in all subfields of natural language processing and text mining. Well-known neural methods such as word2vec and GloVe enable us to obtain distributed vector representations of words, overcoming some of the sparsity issues faced by traditional distributional semantics methods. Such representations are learnt"
I17-5002,P14-2037,0,0.0288968,"Missing"
I17-5002,L16-1733,1,0.889859,"Missing"
L16-1386,2016.gwc-1.9,1,0.821562,"gy, since they are used as domain specific ontologies. Additionally, other supporting ontologies have been added, such as GeoNames for the named entities; PROTON as an upper ontology; SKOS as a mapper between ontologies and terminological lexicons; Dublin Core as a metadata ontology. Also, for the purposes of search, Web Interface Querying EUCases Linking Platform was designed. For its Web Interface, the EUCases Linking Platform relies on a customized version of the GraphDB Workbench27 , developed by Ontotext AD. 5.2. Wordnet Interlingual Index (ILI) A recent development (Vossen et al., 2016; Bond et al., 2016) has been the adoption of LLOD technology by the wordnet community, with a new plan that uses LLOD as the basic mechanism for the creation of links between wordnets in different languages. This Collaborative InterLingual Index enables wordnets to share and link their resources for concepts lexicalized in any of the group’s languages. This was supported directly by a workshop at the 2016 Global WordNet Conference and will lead to the adoption of LLOD technology by a new community. In addition, the open multilingual wordnet (Bond et al., 2014) provides all open wordnets for download using the le"
L16-1386,calzolari-etal-2012-lre,0,0.0712075,"Missing"
L16-1386,ehrmann-etal-2014-representing,1,0.804428,"lable by attempting to download it and discarding all resources that are no longer available. We have attempted to notify the authors of resources that no longer meet the criteria for inclusion in the cloud. However, our experience has been that this did not motivate many authors to update their resources. 2.5. Vocabularies The Linguistic Linked Open Data Cloud has grown significantly in the last few years and most notably, unlike the non-linguistic LOD Cloud, is not centered around one nucleus but instead has used many different vocabularies and datasets to link to. Among these are BabelNet (Ehrmann et al., 2014), LexInfo (Cimiano et al., 2011), and Lexvo (de Melo, 2015). In addition, a number of new vocabularies have emerged including the OntoLex model,13 the NLP Interchange format NIF (Hellmann et al., 2013), the WordNet Interlingual Index (Sect. 5.2.), and the FrameBase schema (Rouces et al., 2015a) (Sect. 5.3.). These vocabularies have increased the power of linked data to represent the complete spectrum of language resources and show that new resources can be created that use the power of linked data to link across different types of languages resources, such as terminologies and dictionaries (Si"
L16-1386,federmann-etal-2012-meta,0,0.0601026,"Missing"
L16-1386,W15-4205,1,0.920405,"not necessarily created for this purpose, e.g., large collections of texts such as news articles, terminological or encyclopedic and general-purpose knowledge bases such as DBpedia (Bizer et al., 2009), or metadata collections. 2.2. Infrastructure and Metadata The OWLG provides guidelines to data publishers on how to include their resources in the LLOD cloud.6 The cloud diagram is currently generated from metadata maintained at DataHub7 and hence contains only resources described in DataHub. An alternative metadata repository specialized for linguistic resources is under development: Linghub (McCrae et al., 2015a).8 It aims to provide a search engine and index for linguistic resources and attempts to harmonize metadata from a number of different sources, including Metashare (Federmann et al., 2012), CLARIN VLO (Van Uytvanck et al., 2012), DataHub and LRE Map (Calzolari et al., 2012). It will soon replace DataHub in the generation of the cloud diagram. LingHub, 4 http://lod2.eu/ http://qtleap.eu/ 6 http://wiki.okfn.org/Working_Groups/ Linguistics/How_to_contribute 7 http://datahub.io 8 http://linghub.org 5 Datasets Links 28 53 103 126 128 41 78 167 203 209 February 2012 September 2013 November 2014 Ma"
L16-1386,W15-4201,0,0.02663,"Missing"
L16-1386,W15-4207,1,0.813122,"4), LexInfo (Cimiano et al., 2011), and Lexvo (de Melo, 2015). In addition, a number of new vocabularies have emerged including the OntoLex model,13 the NLP Interchange format NIF (Hellmann et al., 2013), the WordNet Interlingual Index (Sect. 5.2.), and the FrameBase schema (Rouces et al., 2015a) (Sect. 5.3.). These vocabularies have increased the power of linked data to represent the complete spectrum of language resources and show that new resources can be created that use the power of linked data to link across different types of languages resources, such as terminologies and dictionaries (Siemoneit et al., 2015) and corpora and dictionaries (McGovern et al., 2015). 3. OWLG members have been very active in promoting the development and adoption of linguistic linked data, which had an effect not only in the growth of the LLOD cloud but in the development of representation models, guidelines, and best practices. These activities have been developed in the context of a number of W3C groups and projects, as it is detailed in the rest of this section. 13 12 http://lodvader.aksw.org/ Other Community Group Efforts http://cimiano.github.io/ontolex/ specification.html 2437 3.1. OntoLex 8. LLOD aware services 1"
L16-1386,van-uytvanck-etal-2012-semantic,0,0.0695217,"Missing"
L16-1733,W15-1523,1,0.814111,"Missing"
L16-1733,2015.mtsummit-papers.27,1,0.851694,"Missing"
L16-1733,O97-1002,0,0.0520418,"nsive research on more sophisticated methods that exploited graph connectivity and gloss comparisons. In the medical domain, similar lexical resources and ontologies have been created. Major examples are the Medical Subject Headings (MeSH)1 , the Unified Medical Language System (UMLS), and the SNOMED clinical terms (SNOMED CT). Similarity measures such as the one proposed by Wu and Palmer (1994) or Nguyen and Al-Mubaid2 consider the depth in the hierarchical structures of the used ontologies or path lengths between two concepts in order to compute a similarity metric. Resnik (1995) as well as Jiang and Conrath (1997) propose to additionally exploit the occurrence probabilities of the concepts, as computed on large corpora. Less ontology-dependent measures such as the Lesk measure and the Vector approach from Liu et al. (2012) rely on textual descriptions of the concepts and context expansions to compute the relatedness between terms. More details on such measures, especially on the relatedness measures, are given by Liu et al. (2012). Another line of work proposed more data-driven methods without the need for a knowledge base. The most well1 https://www.nlm.nih.gov/mesh/ http://atlas.ahc.umn.edu/umls_ sim"
L16-1733,D14-1162,0,0.111614,"t Dirichlet Allocation or LDA (Blei et al., 2003). These methods have been very influential in natural language processing and information retrieval. Still, many of them suffer from limited scalability and normally need to be re-applied to new document collections. Distributional semantic methods rely on term co-occurrence matrices rather than term-document matrices (Sch¨utze, 1993), delivering quite meaningful results. However, experimental results suggest that newer neural network-based models produce better word representations than both LSA and traditional distributional semantic methods (Pennington et al., 2014). In recent years, low-dimensional embeddings have been proposed as a particularly simple way to feed such knowledge of similarities into machine learning algorithms (Collobert et al., 2011; Turian et al., 2010). The fast algorithms by Mikolov et al. (2013) and their freely available word2vec implementation3 as well as the publicly available pretrained data has made such word embeddings very convenient to use. In recent years, numerous extensions have been proposed, e.g. better exploiting information extraction pattern occurrences (Chen and de Melo, 2015) or multilingual structured data (de Me"
L16-1733,P10-1040,0,0.0158761,"need to be re-applied to new document collections. Distributional semantic methods rely on term co-occurrence matrices rather than term-document matrices (Sch¨utze, 1993), delivering quite meaningful results. However, experimental results suggest that newer neural network-based models produce better word representations than both LSA and traditional distributional semantic methods (Pennington et al., 2014). In recent years, low-dimensional embeddings have been proposed as a particularly simple way to feed such knowledge of similarities into machine learning algorithms (Collobert et al., 2011; Turian et al., 2010). The fast algorithms by Mikolov et al. (2013) and their freely available word2vec implementation3 as well as the publicly available pretrained data has made such word embeddings very convenient to use. In recent years, numerous extensions have been proposed, e.g. better exploiting information extraction pattern occurrences (Chen and de Melo, 2015) or multilingual structured data (de Melo, 2015). All-in-text is based on the scalable Paragraph Vector algorithm (Le and Mikolov, 2014), which extends the word2vec ideas to jointly create representations of words and word sequences such as sentences"
L16-1733,P94-1019,0,0.0617346,"with the semantic similarity of words often meant relying on custom lexical resources. In the simplest case, this could be a simple list of synonyms or aliases. Lexical networks such as WordNet (Fellbaum, 1998) led to extensive research on more sophisticated methods that exploited graph connectivity and gloss comparisons. In the medical domain, similar lexical resources and ontologies have been created. Major examples are the Medical Subject Headings (MeSH)1 , the Unified Medical Language System (UMLS), and the SNOMED clinical terms (SNOMED CT). Similarity measures such as the one proposed by Wu and Palmer (1994) or Nguyen and Al-Mubaid2 consider the depth in the hierarchical structures of the used ontologies or path lengths between two concepts in order to compute a similarity metric. Resnik (1995) as well as Jiang and Conrath (1997) propose to additionally exploit the occurrence probabilities of the concepts, as computed on large corpora. Less ontology-dependent measures such as the Lesk measure and the Vector approach from Liu et al. (2012) rely on textual descriptions of the concepts and context expansions to compute the relatedness between terms. More details on such measures, especially on the r"
L18-1695,P98-1013,0,0.810789,"ches of this sort excel at finding novel poetic metaphors, e.g. hope is like a lightbulb, as discussed by Terai and Nakagawa (2010). The system discussed in this paper, in contrast, is biased towards finding novel variations of more fundamental conceptual metaphors that shape human thinking. The model achieves this by constructing a graph to capture relationships between metaphors, words, as well as mental schemas. For this, it draws on the MetaNet respository (Dodge et al., 2015), a manually created database of metaphor. The approach additionally relies on lexical resources such as FrameNet (Baker et al., 1998; Ruppenhofer et al., 2006) and WordNet (Fellbaum, 1998) and automated interlinking techniques. These resources are connected and their overall graph structure allows us to propose potential metaphoric means of referring to the given input words, possibly constrained with additional metaphoric seed words, which may be provided as supplementary inputs. 2. Metaphor and Cognition Metaphor is often regarded as a process that allows us to think of one thing in terms of another (Lakoff and Johnson, 1980). The following sentences provide examples of linguistic metaphors. (1) Their spirits were high."
L18-1695,2015.mtsummit-papers.27,1,0.84884,"Missing"
L18-1695,W15-1405,0,0.0983375,"lternative model for this, based on semantic similarity, which considers both adjectives and verbs as relevant noun properties. Approaches of this sort excel at finding novel poetic metaphors, e.g. hope is like a lightbulb, as discussed by Terai and Nakagawa (2010). The system discussed in this paper, in contrast, is biased towards finding novel variations of more fundamental conceptual metaphors that shape human thinking. The model achieves this by constructing a graph to capture relationships between metaphors, words, as well as mental schemas. For this, it draws on the MetaNet respository (Dodge et al., 2015), a manually created database of metaphor. The approach additionally relies on lexical resources such as FrameNet (Baker et al., 1998; Ruppenhofer et al., 2006) and WordNet (Fellbaum, 1998) and automated interlinking techniques. These resources are connected and their overall graph structure allows us to propose potential metaphoric means of referring to the given input words, possibly constrained with additional metaphoric seed words, which may be provided as supplementary inputs. 2. Metaphor and Cognition Metaphor is often regarded as a process that allows us to think of one thing in terms o"
L18-1695,J91-1003,0,0.669084,"nnection between anger and heat derives from human biology. Empirically, Tsvetkov et al. (2014) found that they were able to apply models trained to detect English linguistic metaphors also to the task of detecting linguistic metaphors in other languages, some of which are not phylogenetically close (specifically, they considered Spanish, Farsi, and Russian). 6. 6.1. Related Work Metaphor Detection Numerous papers have studied the task of automatically identifying metaphoric expressions in text. Many systems aim at achieving this by detecting violations of selectional preference restrictions (Fass, 1991; Shutova et al., 2010). For instance, the verb to kill usually applies to living beings, so when it is found in contexts such as my process got killed, it is quite likely that the word is being used metaphorically. Some approaches have additionally relied on lexical resources such as FrameNet (Gedigian et al., 2006) and HowNet (Tang et al., 2010) to increase the qual4394 Original Repository + Direct FrameNet Links + VerbNet/SemLink + WordNet + WordNet mappings Schemas Target Schemas Source Schemas 803 5,349 5,368 6,302 6,789 258 2,283 2,290 2,626 3,304 513 2,776 2,783 2,849 3,497 Table 2: Wor"
L18-1695,W06-3506,0,0.0482834,"ifically, they considered Spanish, Farsi, and Russian). 6. 6.1. Related Work Metaphor Detection Numerous papers have studied the task of automatically identifying metaphoric expressions in text. Many systems aim at achieving this by detecting violations of selectional preference restrictions (Fass, 1991; Shutova et al., 2010). For instance, the verb to kill usually applies to living beings, so when it is found in contexts such as my process got killed, it is quite likely that the word is being used metaphorically. Some approaches have additionally relied on lexical resources such as FrameNet (Gedigian et al., 2006) and HowNet (Tang et al., 2010) to increase the qual4394 Original Repository + Direct FrameNet Links + VerbNet/SemLink + WordNet + WordNet mappings Schemas Target Schemas Source Schemas 803 5,349 5,368 6,302 6,789 258 2,283 2,290 2,626 3,304 513 2,776 2,783 2,849 3,497 Table 2: Word–Schema Connections, where subsequent rows show the counts as additional resources are added (including all previously mentioned ones). Depth Original Repository Final Graph 1 2 3 4 5 6 7 8 742 742 742 1,202 742 9,024 742 29,246 742 62,411 742 94,065 742 117,372 742 125,038 Table 3: Words with Indirect Schema Connec"
L18-1695,Q16-1004,1,0.875514,"Missing"
L18-1695,W03-1403,0,0.185613,"Missing"
L18-1695,J04-1002,0,0.0848744,"ges and videos with associated tags to draw inferences about which predicate-argument pairs are more likely to be concrete. The underlying assumption is that such multimodal data is more closely grounded in the real world and hence linguistic descriptions are more likely to be concrete and literal (e.g., cutting hair) as opposed to more metaphorical (e.g., cutting costs), which is often predominant in newswire text. Only few systems have attempted to go beyond identifying individual linguistic metaphors towards recognizing more general conventionalized conceptual metaphors. The CorMet system (Mason, 2004) attempts to automatically infer metaphor mappings from a corpus by studying systematic differences in verb selectional preferences between domains. Such techniques could be used to extend the number of metaphors in our graph. 6.2. Metaphor Analysis The Metaphor Magnet system (Veale and Li, 2012) addresses the complementary task of metaphor interpretation, providing a list of attributes that explain what qualities a given source domain shares with a target domain that lend the metaphor its strength. The system takes a target and source domain as input (e.g., L OVE IS A D RUG), and then uses We"
L18-1695,W14-3004,0,0.0228648,"h manually specified edge type-specific weights. In order to increase the coverage of the graph, schemas without lexical units are automatically linked to WordNet synsets, using the first sense heuristic for disambiguation. Previous work has found that WordNet-like resources include certain types of commonsense knowledge that is highly relevant for capturing entailments in the target and source domains, although the coverage still tends to be limited (L¨onneker, 2003). From VerbNet (Schuler, 2005), we can include verb entries and their links to WordNet senses. The associated SemLink resource (Palmer et al., 2014) provides mappings between verb entries and FrameNet frames and lexical units, which are included as well. Overall, this process yields a rich graph with numerous connections between words, schemas, and other entities. For example, the graph connects schemas from the MetaNet repository with corresponding FrameNet frames in the following ways: 1) by means of explicitly provided links from the repository, 2) by means of indirect connections through other resources (WordNet, VerbNet), 3) by means of indirect connections via shared terms, and 4) various hybrid forms of indirect connections, often"
L18-1695,shutova-teufel-2010-metaphor,0,0.0295491,"ich may be provided as supplementary inputs. The experiments show that this algorithm greatly expands the potential of the original repository for this task by enabling new connections to be drawn. Keywords: Metaphor, lexical resources, graph structure 1. Introduction Whenever one says that issues become clear, stock markets go up, or time is spent, language is arguably being used in a non-literal, metaphorical manner, at least with respect to the original senses of the words. Corpus studies have found that metaphorical phenomena are very pervasive even in formal language (Steen et al., 2010; Shutova and Teufel, 2010). Not only is such metaphorical use of language one of the primary means for creative linguistic expression. It has been widely stipulated that our reliance on metaphor is a natural consequence of the way our brains reflect on and reason about the world. This paper presents a model that can be used to suggest both well-entrenched and novel metaphoric means of referring to a given input word or set of related input words. For example, given a word such as government, the method may propose expressions such as father, nanny, corresponding to ways of thinking about the government. While metaphor"
L18-1695,C10-1113,0,0.0291888,"ween anger and heat derives from human biology. Empirically, Tsvetkov et al. (2014) found that they were able to apply models trained to detect English linguistic metaphors also to the task of detecting linguistic metaphors in other languages, some of which are not phylogenetically close (specifically, they considered Spanish, Farsi, and Russian). 6. 6.1. Related Work Metaphor Detection Numerous papers have studied the task of automatically identifying metaphoric expressions in text. Many systems aim at achieving this by detecting violations of selectional preference restrictions (Fass, 1991; Shutova et al., 2010). For instance, the verb to kill usually applies to living beings, so when it is found in contexts such as my process got killed, it is quite likely that the word is being used metaphorically. Some approaches have additionally relied on lexical resources such as FrameNet (Gedigian et al., 2006) and HowNet (Tang et al., 2010) to increase the qual4394 Original Repository + Direct FrameNet Links + VerbNet/SemLink + WordNet + WordNet mappings Schemas Target Schemas Source Schemas 803 5,349 5,368 6,302 6,789 258 2,283 2,290 2,626 3,304 513 2,776 2,783 2,849 3,497 Table 2: Word–Schema Connections, w"
L18-1695,C12-2109,0,0.049877,"Missing"
L18-1695,P15-1092,1,0.905042,"Missing"
L18-1695,P14-1024,0,0.0164593,"ut candidates are likely to be unsuitable. However, in many cases, they turn out to be appropriate. For one, this may stem from similarities in metaphorical language use across related languages. For instance, in many Western languages, the word transparency, which in its original sense refers to the property of allowing the transmission of light through an object, is also used to refer to public evaluability and accountability. Additionally, this may also stem from a broadly shared experiential basis. For example, the connection between anger and heat derives from human biology. Empirically, Tsvetkov et al. (2014) found that they were able to apply models trained to detect English linguistic metaphors also to the task of detecting linguistic metaphors in other languages, some of which are not phylogenetically close (specifically, they considered Spanish, Farsi, and Russian). 6. 6.1. Related Work Metaphor Detection Numerous papers have studied the task of automatically identifying metaphoric expressions in text. Many systems aim at achieving this by detecting violations of selectional preference restrictions (Fass, 1991; Shutova et al., 2010). For instance, the verb to kill usually applies to living bei"
N19-1056,W15-0123,0,0.0702774,"Missing"
N19-1056,C18-1301,1,0.926057,"ds are far more effective than a picture” – Feiner and McKeown (1991). Modeling how visual and linguistic information can jointly contribute to coherent and effective communication is a longstanding open problem with implications across cognitive science. As Feiner and McKeown (1991) already observe, it is particularly important for automating the understanding and generation of text–image presentations. Theoretical models have suggested that images and text fit together into integrated presentations via coherence relations that are analogous to those that connect text spans in discourse; see Alikhani and Stone (2018a) and Section 2. This paper follows up this theoretical perspective through systematic corpus investigation. We are inspired by research on text discourse, which has led to large-scale corpora with information about discourse structure and discourse semantics. The Penn Discourse Treebank (PDTB) is one of the most well-known examples (Miltsakaki et al., 2004; Prasad et al., 2008). However, although multimodal corpora increasingly include 2 Discourse Coherence and Text–Image Presentations We begin with an example to motivate our approach and clarify its relationship to previous work. Figure 1 s"
N19-1056,miltsakaki-etal-2004-penn,0,0.159466,"ng and generation of text–image presentations. Theoretical models have suggested that images and text fit together into integrated presentations via coherence relations that are analogous to those that connect text spans in discourse; see Alikhani and Stone (2018a) and Section 2. This paper follows up this theoretical perspective through systematic corpus investigation. We are inspired by research on text discourse, which has led to large-scale corpora with information about discourse structure and discourse semantics. The Penn Discourse Treebank (PDTB) is one of the most well-known examples (Miltsakaki et al., 2004; Prasad et al., 2008). However, although multimodal corpora increasingly include 2 Discourse Coherence and Text–Image Presentations We begin with an example to motivate our approach and clarify its relationship to previous work. Figure 1 shows two steps in an online recipe for a ravioli casserole from the RecipeQA data set (Yagcioglu et al., 2018). The image of Figure 1a shows a moment towards the end of carrying out the “covering” action of the accompanying text; that of Figure 1b shows one instance of the result of the “spooning” actions of the text. Cognitive scientists have argued that su"
N19-1056,W19-1806,1,0.65032,"f the remaining segments of the text. Such correlations set a direction for designing or learning strategies to select when to include imagery. 5 1 2 3 4 5 contribution of this study is that it presents a discourse annotation scheme for cross-modal data, and establishes that annotations for this scheme can be procured from non-expert contributors via crowd-sourcing. Our paper sets the agenda for a range of future research. One obvious example is to extend the approach to other genres of communication with other coherence relations, such as the distinctive coherence of images and caption text (Alikhani and Stone, 2019). Another is to link coherence relations to the structure of multimodal discourse. For example, our methods have not yet addressed whether image–text relations have the same kinds of subordinating or coordinating roles that comparable relations have in structuring text discourse (Asher and Lascarides, 2003). Ultimately, of course, we hope to leverage such corpora to build and apply better models of multimodal communication. Acknowledgments The research presented here is supported by NSF Award IIS-1526723 and through a fellowship from the Rutgers Discovery Informatics Institute. Thanks to Gabri"
N19-1056,W15-2614,0,0.0242572,"etain only recipes with 70 or fewer words per step, for a final count of 516 documents (2,047 image–text pairs). The interface is designed such that if the answer to Question 8 is T RUE, the subject will be prompted with Question 9 and 10. Otherwise, Question 8 is the last question in the list. Agreement. To assess the inter-rater agreement, we determine Cohen’s κ and Fleiss’s κ values. For Cohen’s κ, we randomly selected 150 image–text pairs and assigned each to two participants, obtaining a Cohen’s κ of 0.844, which indicates almost perfect agreement. For Fleiss’s κ (Fleiss and Cohen, 1973; Cocos et al., 2015; Banerjee et al., 1999), we randomly selected 50 text–image pairs, assigned them to five subjects, and computed the average κ. We obtain a score of 0.736, which indicates substantial agreement (Viera et al., 2005). Protocol. We recruit participants through Amazon Mechanical Turk. All subjects were US citizens, agreed to a consent form approved by Rutgers’s institutional review board, and were compensated at an estimated rate of USD 15 an hour. 4 Experiment Interface. Given an image and the corresponding textual instruction from the dataset, participants were requested to answer the following"
N19-1056,P12-2018,0,0.0568739,"icts action in progress). It shows the features most correlated with the classification decision and their log probability estimates. For Q4, not surprisingly, numbers and units are positive instances. More interestingly, verbs of movement and combination are negative instances, perhaps because such steps normally involve material that has already been measured. For Q8, a range of physical action verbs are associated with actions in progress; negative features correlate with steps involved in actions that don’t require ongoing attention (e.g., baking). Table 4 reports top SVM with NB (NBSVM) (Wang and Manning, 2012) features for Q1 that asks subjects to highlight the part of the text that is most related to the image. Action verbs are part of highlighted text, whereas adverbs and quantitative information that cannot be easily depicted in images are part of the remaining segments of the text. Such correlations set a direction for designing or learning strategies to select when to include imagery. 5 1 2 3 4 5 contribution of this study is that it presents a discourse annotation scheme for cross-modal data, and establishes that annotations for this scheme can be procured from non-expert contributors via cro"
N19-1056,P09-1076,0,0.0415355,"in the step but not mentioned in the text. 6. The image shows how to prepare before carrying out the step. 7. The image shows the results of the action that is described in the text. 8. The image depicts an action in progress that is described in the text. 9. The text describes several different actions but the image only depicts one. 10. One would have to repeat the action shown in the image many times in order to complete this step. Work on text has found that text genre heavily influences both the kinds of discourse relations one finds in a corpus and the way those relations are signalled (Webber, 2009). Since our focus is on developing methodology for consistent annotation, we therefore choose to work within a single genre. We selected instructional text because of its concrete, practical subject matter and because of its step-by-step organization, which makes it possible to automatically group together short segments of related text and imagery. Text–Image Pairs. We base our data collection on an existing instructional dataset, RecipeQA (Yagcioglu et al., 2018). This is the only publicly available large-scale dataset of multimodal instructions. It consists of multimodal recipes—textual ins"
N19-1056,L18-1303,0,0.0253104,"cts to the broader discourse. In particular, inferences analogous to those used to interpret text seem to be necessary with such images to recognize their spatio-temporal perspective (Cumming et al., 2017), the objects they depict (Abusch, 2013), and their place in the arc of narrative progression (McCloud, 1993; Cohn, 2013). In fact, such inferences seem to be a general feature of multimodal communication, applying also in the coherent relationships of utterance to co-speech gesture (Lascarides and Stone, 2009) or the coherent relationships of elements in diagrams (Alikhani and Stone, 2018b; Hiippala and Orekhova, 2018). In empirical analyses of text corpora, researchers in projects such as the Penn Discourse Treebank (Miltsakaki et al., 2004; Prasad et al., 2008) have been successful at documenting such effects by annotating discourse structure and discourse semantics via coherence relations. We would like to apply a similar strategy to text– image documents like that shown in Figure 1. However, existing discourse annotation guidelines depend on the distinctive ways that coherence is signaled in text. In text, we find syntactic devices such as structural parallelism, semantic devices such as negation, and p"
P10-1087,W09-1604,0,0.103146,"c extractors, and YAGO (Suchanek et al., 2007), which adds an ontological structure on top of Wikipedia’s entities. Wikipedia has been used cross-lingually for cross-lingual IR (Nguyen et al., 2009), question answering (Ferr´andez et al., 2007) as well as for learning transliterations (Pasternack and Roth, 2009), among other things. Mihalcea and Csomai (2007) have studied predicting new links within a single edition of Wikipedia. Sorg and Cimiano (2008) considered the problem of suggesting new cross-lingual links, which could be used as additional inputs in our problem. Adar et al. (2009) and Bouma et al. (2009) show how cross-lingual links can be used to propagate information from one Wikipedia’s infoboxes to another edition. Our aggregation consistency algorithm uses theoretical ideas put forward by researchers studying graph cuts (Leighton and Rao, 1999; Garg et al., 1996; Avidor and Langberg, 2007). Our problem setting is related to that of correlation clustering (Bansal et al., 2004), where a graph consistDatabase of Named Entities The partitioning heuristics allowed us to process all entries in the complete set of Wikipedia dumps and produce a clean output set of connected components where each"
P10-1087,wentland-etal-2008-building,0,0.0108153,"ny smaller, more densely connected subgraphs. We thus investigated graph partitioning heuristics to decompose larger graphs into smaller parts that can more easily be handled with our algorithm. The METIS algorithms (Karypis and Kumar, 1998) can decompose graphs with hundreds of thousands of nodes almost instantly, but favour equally sized clusters over lower cut costs. We obtained partitionings with costs orders of magnitude lower using the heuristic by Dhillon et al. (2007). 4.5 Related Work A number of projects have used Wikipedia as a database of named entities (Ponzetto and Strube, 2007; Silberer et al., 2008). The most wellknown are probably DBpedia (Auer et al., 2007), which serves as a hub in the Linked Data Web, Freebase1 , which combines human input and automatic extractors, and YAGO (Suchanek et al., 2007), which adds an ontological structure on top of Wikipedia’s entities. Wikipedia has been used cross-lingually for cross-lingual IR (Nguyen et al., 2009), question answering (Ferr´andez et al., 2007) as well as for learning transliterations (Pasternack and Roth, 2009), among other things. Mihalcea and Csomai (2007) have studied predicting new links within a single edition of Wikipedia. Sorg a"
P12-3026,P98-1013,0,0.00908371,"ile frame semantics instead emphasizes the cognitive relatedness of words like ‘happy’, ‘unhappy’, ‘astonished’, and ‘amusement’, and explains that typical participants include an experiencer who experiences the emotions and external stimuli that evoke them. There have been individual systems that made use of both forms of knowledge (Shi and Mihalcea, 2005; Coppola and others, 2009), but due to their very different nature, there is currently no simple way to accomplish this feat. Our system addresses this by seamlessly integrating frame semantic knowledge into the system. We draw on FrameNet (Baker et al., 1998), the most well-known computational instantiation of frame semantics. While the FrameNet project is generally well-known, its use in practical applications has been limited due to the lack of easy-to-use APIs and because FrameNet alone does not cover as many words as WordNet. Our API simultaneously provides access to both sources. Language information For a given language, this extension provides information such as relevant writing systems, geographical regions, identification codes, and names in many different languages. These are all integrated into WordNet’s hypernym hierarchy, i.e. from l"
P12-3026,P10-1087,1,0.891386,"Missing"
P12-3026,2007.mtsummit-papers.24,0,0.0349635,"08). Further uses of lexical knowledge include data cleaning (Kedad and Métais, 2002), visual object recognition (Marszałek and Schmid, 2007), and biomedical data analysis (Rubin and others, 2006). Many of these applications have used Englishlanguage resources like WordNet (Fellbaum, 1998). Gerhard Weikum Max Planck Institute for Informatics weikum@mpi-inf.mpg.de However, a more multilingual resource equipped with an easy-to-use API would not only enable us to perform all of the aforementioned tasks in additional languages, but also to explore cross-lingual applications like cross-lingual IR (Etzioni et al., 2007) and machine translation (Chatterjee et al., 2005). This paper describes a new API that makes lexical knowledge about millions of items in over 200 languages available to applications, and a corresponding online user interface for users to explore the data. We first describe link prediction techniques used to create the multilingual core of the knowledge base with word sense information (Section 2). We then outline techniques used to incorporate named entities and specialized concepts (Section 3) and other types of knowledge (Section 4). Finally, we describe how the information is made accessi"
P12-3026,E12-1059,0,0.0281895,"Missing"
P12-3026,P10-1023,0,0.0593318,"Missing"
P12-3026,C98-1013,0,\N,Missing
P12-3026,I11-2001,0,\N,Missing
P14-1098,P11-1070,1,0.486021,"Missing"
P14-1098,P06-1038,0,0.0161687,"mmetric (in the sense that Sijk is redundant to Sikj ) and hence the patterns are undirected. Therefore, for each term pair, we first symmetrize the collected Web n-grams and Wikipedia patterns by accumulating the counts of symmetric patterns like rats or squirrels and squirrels or rats.8 4 Related Work In our work, we assume a known term set and do not address the problem of extracting related terms from text. However, a great deal of past work has considered automating this process, typically taking one of two major approaches. The clustering-based approach (Lin, 1998; Lin and Pantel, 2002; Davidov and Rappoport, 2006; Yamada et al., 2009) discovers relations based on the assumption that similar concepts appear in sim7 One can also add features on the full triple (xi , xj , xk ) but most such features will be sparse. 8 All the patterns and counts for our Web and Wikipedia edge and sibling features described above are extracted after stemming the words in the terms, the n-grams, and the abstracts (using the Porter stemmer). Also, we threshold the features (to prune away the sparse ones) by considering only those that fire for at least t trees in the training data (t = 4 in our experiments). Note that one co"
P14-1098,N12-1051,0,0.0313556,"nal probability of a set of relational evidence given a taxonomy. Yang and Callan (2009) incrementally cluster terms based on a pairwise semantic distance. Lao et al. (2012) extend a knowledge base using a random walk model to learn binary relational inference rules. However, the task of inducing full taxonomies without assuming a substantial initial partial taxonomy is relatively less well studied. There is some prior work on the related task of hierarchical clustering, or grouping together of semantically related words (Cimiano et al., 2005; Cimiano and Staab, 2005; Poon and Domingos, 2010; Fountain and Lapata, 2012). The task we focus on, though, is the discovery of direct taxonomic relationships (e.g., hypernymy) between words. We know of two closely-related previous systems, Kozareva and Hovy (2010) and Navigli et al. (2011), that build full taxonomies from scratch. Both of these systems use a process that starts by finding basic level terms (leaves of the final taxonomy tree, typically) and then using relational patterns (hand-selected ones in the case of Kozareva and Hovy (2010), and ones learned separately by a pairwise classifier on manually annotated co-occurrence patterns for Navigli and Velardi"
P14-1098,N03-1011,0,0.0146092,"ring only those that fire for at least t trees in the training data (t = 4 in our experiments). Note that one could also add various complementary types of useful features presented by previous work, e.g., bootstrapping using syntactic heuristics (Phillips and Riloff, 2002), dependency patterns (Snow et al., 2006), doubly anchored patterns (Kozareva et al., 2008; Hovy et al., 2009), and Web definition classifiers (Navigli et al., 2011). ilar contexts (Harris, 1954). The pattern-based approach uses special lexico-syntactic patterns to extract pairwise relation lists (Phillips and Riloff, 2002; Girju et al., 2003; Pantel and Pennacchiotti, 2006; Suchanek et al., 2007; Ritter et al., 2009; Hovy et al., 2009; Baroni et al., 2010; Ponzetto and Strube, 2011) and semantic classes or classinstance pairs (Riloff and Shepherd, 1997; Katz and Lin, 2003; Pas¸ca, 2004; Etzioni et al., 2005; Talukdar et al., 2008). We focus on the second step of taxonomy induction, namely the structured organization of terms into a complete and coherent tree-like hierarchy.9 Early work on this task assumes a starting partial taxonomy and inserts missing terms into it. Widdows (2003) place unknown words into a region with the most"
P14-1098,P96-1024,0,0.045358,"o et al., 2007), because multi-root spanning ‘forests’ are not applicable to our task. Also, note that we currently assume one node per term. We are following the task description from previous work where the goal is to create a taxonomy for a specific domain (e.g., animals). Within a specific domain, terms typically just have a single sense. However, our algorithms could certainly be adapted to the case of multiple term senses (by treating the different senses as unique nodes in the tree) in future work. 5 The MST that is found using these edge scores is actually the minimum Bayes risk tree (Goodman, 1996) for an edge accuracy loss function (Smith and Eisner, 2008). 1044 ties or syntactic word classes, which are primary drivers for dependency parsing, are mostly uninformative for taxonomy induction. Instead, inducing taxonomies requires world knowledge to capture the semantic relations between various unseen terms. For this, we use semantic cues to hypernymy and siblinghood via features on simple surface patterns and statistics in large text corpora. We fire features on both the edge and the sibling factors. We first describe all the edge features in detail (Section 3.1 and Section 3.2), and th"
P14-1098,C92-2082,0,0.253921,"gest common substring of xi and xj , and create indicator features for rounded-off and binned values of |LCS|/((|xi |+ |xj |)/2). Length difference: We compute the signed length difference between xj and xi , and create indicator features for rounded-off and binned values of (|xj |− |xi |)/((|xi |+ |xj |)/2). Yang and Callan (2009) use a similar feature. 3.2 3.2.1 Semantic Features Web n-gram Features Patterns and counts: Hypernymy for a term pair (P=xi , C=xj ) is often signaled by the presence of surface patterns like C is a P, P such as C in large text corpora, an observation going back to Hearst (1992). For each potential parent-child edge (P=xi , C=xj ), we mine the top k strings (based on count) in which both xi and xj occur (we use k=200). We collect patterns in both directions, which allows us to judge the correct direction of an edge (e.g., C is a P is a positive signal for hypernymy whereas P is a C is a negative signal).6 Next, for each pattern in this top-k list, we compute its normalized pattern count c, and fire an indicator feature on the tuple (pattern, t), for all thresholds t (in a fixed set) s.t. c ≥ t. Our supervised model then automatically learns which patterns are good in"
P14-1098,D09-1099,0,0.0559525,"et’s vertebrates taxonomy. Introduction Many tasks in natural language understanding, such as question answering, information extraction, and textual entailment, benefit from lexical semantic information in the form of types and hypernyms. A recent example is IBM’s Jeopardy! system Watson (Ferrucci et al., 2010), which used type information to restrict the set of answer candidates. Information of this sort is present in term taxonomies (e.g., Figure 1), ontologies, and thesauri. However, currently available taxonomies such as WordNet are incomplete in coverage (Pennacchiotti and Pantel, 2006; Hovy et al., 2009), unavailable in many domains and languages, and time-intensive to create or extend manually. There has thus been considerable interest in building lexical taxonomies automatically. In this work, we focus on the task of taking collections of terms as input and predicting a complete taxonomy structure over them as output. Our model takes a loglinear form and is represented using a factor graph that includes both 1st-order scoring factors on directed hypernymy edges (a parent and child in the taxonomy) and 2nd-order scoring factors on sibling edge pairs (pairs of hypernym edges with a shared par"
P14-1098,D07-1015,0,0.0118216,". Hence, at decoding time, we instead start out by once more using belief propagation to find marginal beliefs, and then set the score of each edge to be its belief odds ratio: 3 bYij (ON) 5 bYij (OFF) . Features While spanning trees are familiar from nonprojective dependency parsing, features based on the linear order of the words or on lexical identi4 See Georgiadis (2003) for a detailed algorithmic proof, and McDonald et al. (2005) for an illustrative example. Also, we constrain the Chu-Liu-Edmonds MST algorithm to output only single-root MSTs, where the (dummy) root has exactly one child (Koo et al., 2007), because multi-root spanning ‘forests’ are not applicable to our task. Also, note that we currently assume one node per term. We are following the task description from previous work where the goal is to create a taxonomy for a specific domain (e.g., animals). Within a specific domain, terms typically just have a single sense. However, our algorithms could certainly be adapted to the case of multiple term senses (by treating the different senses as unique nodes in the tree) in future work. 5 The MST that is found using these edge scores is actually the minimum Bayes risk tree (Goodman, 1996)"
P14-1098,D10-1108,0,0.852751,"signal of the link rodent → rat. Moreover, sibling or coordination cues like either rats or squirrels suggest that rat is a sibling of squirrel and adds evidence for the links rodent → rat and rodent → squirrel. Our supervised model captures exactly these types of intuitions by automatically discovering such heterogeneous relational patterns as features (and learning their weights) on edges and on sibling edge pairs, respectively. There have been several previous studies on taxonomy induction. e.g., the incremental taxonomy induction system of Snow et al. (2006), the longest path approach of Kozareva and Hovy (2010), and the maximum spanning tree (MST) approach of Navigli et al. (2011) (see Section 4 for a more detailed overview). The main contribution of this work is that we present the first discriminatively trained, structured probabilistic model over the full space of taxonomy trees, using a structured inference procedure through both the learning and decoding phases. Our model is also the first to directly learn relational patterns as part of the process of training an end-to-end taxonomic induction system, rather than using patterns that were hand-selected or learned via pairwise classifiers on man"
P14-1098,P08-1119,0,0.0317587,"ts for our Web and Wikipedia edge and sibling features described above are extracted after stemming the words in the terms, the n-grams, and the abstracts (using the Porter stemmer). Also, we threshold the features (to prune away the sparse ones) by considering only those that fire for at least t trees in the training data (t = 4 in our experiments). Note that one could also add various complementary types of useful features presented by previous work, e.g., bootstrapping using syntactic heuristics (Phillips and Riloff, 2002), dependency patterns (Snow et al., 2006), doubly anchored patterns (Kozareva et al., 2008; Hovy et al., 2009), and Web definition classifiers (Navigli et al., 2011). ilar contexts (Harris, 1954). The pattern-based approach uses special lexico-syntactic patterns to extract pairwise relation lists (Phillips and Riloff, 2002; Girju et al., 2003; Pantel and Pennacchiotti, 2006; Suchanek et al., 2007; Ritter et al., 2009; Hovy et al., 2009; Baroni et al., 2010; Ponzetto and Strube, 2011) and semantic classes or classinstance pairs (Riloff and Shepherd, 1997; Katz and Lin, 2003; Pas¸ca, 2004; Etzioni et al., 2005; Talukdar et al., 2008). We focus on the second step of taxonomy induction"
P14-1098,D12-1093,0,0.0195349,"et al., 2005; Talukdar et al., 2008). We focus on the second step of taxonomy induction, namely the structured organization of terms into a complete and coherent tree-like hierarchy.9 Early work on this task assumes a starting partial taxonomy and inserts missing terms into it. Widdows (2003) place unknown words into a region with the most semantically-similar neighbors. Snow et al. (2006) add novel terms by greedily maximizing the conditional probability of a set of relational evidence given a taxonomy. Yang and Callan (2009) incrementally cluster terms based on a pairwise semantic distance. Lao et al. (2012) extend a knowledge base using a random walk model to learn binary relational inference rules. However, the task of inducing full taxonomies without assuming a substantial initial partial taxonomy is relatively less well studied. There is some prior work on the related task of hierarchical clustering, or grouping together of semantically related words (Cimiano et al., 2005; Cimiano and Staab, 2005; Poon and Domingos, 2010; Fountain and Lapata, 2012). The task we focus on, though, is the discovery of direct taxonomic relationships (e.g., hypernymy) between words. We know of two closely-related"
P14-1098,C02-1144,0,0.0345728,"sibling factors are symmetric (in the sense that Sijk is redundant to Sikj ) and hence the patterns are undirected. Therefore, for each term pair, we first symmetrize the collected Web n-grams and Wikipedia patterns by accumulating the counts of symmetric patterns like rats or squirrels and squirrels or rats.8 4 Related Work In our work, we assume a known term set and do not address the problem of extracting related terms from text. However, a great deal of past work has considered automating this process, typically taking one of two major approaches. The clustering-based approach (Lin, 1998; Lin and Pantel, 2002; Davidov and Rappoport, 2006; Yamada et al., 2009) discovers relations based on the assumption that similar concepts appear in sim7 One can also add features on the full triple (xi , xj , xk ) but most such features will be sparse. 8 All the patterns and counts for our Web and Wikipedia edge and sibling features described above are extracted after stemming the words in the terms, the n-grams, and the abstracts (using the Porter stemmer). Also, we threshold the features (to prune away the sparse ones) by considering only those that fire for at least t trees in the training data (t = 4 in our e"
P14-1098,P98-2127,0,0.0361892,"s that the sibling factors are symmetric (in the sense that Sijk is redundant to Sikj ) and hence the patterns are undirected. Therefore, for each term pair, we first symmetrize the collected Web n-grams and Wikipedia patterns by accumulating the counts of symmetric patterns like rats or squirrels and squirrels or rats.8 4 Related Work In our work, we assume a known term set and do not address the problem of extracting related terms from text. However, a great deal of past work has considered automating this process, typically taking one of two major approaches. The clustering-based approach (Lin, 1998; Lin and Pantel, 2002; Davidov and Rappoport, 2006; Yamada et al., 2009) discovers relations based on the assumption that similar concepts appear in sim7 One can also add features on the full triple (xi , xj , xk ) but most such features will be sparse. 8 All the patterns and counts for our Web and Wikipedia edge and sibling features described above are extracted after stemming the words in the terms, the n-grams, and the abstracts (using the Porter stemmer). Also, we threshold the features (to prune away the sparse ones) by considering only those that fire for at least t trees in the trainin"
P14-1098,H05-1066,0,0.0473065,"Missing"
P14-1098,P10-1134,0,0.0105973,"in and Lapata, 2012). The task we focus on, though, is the discovery of direct taxonomic relationships (e.g., hypernymy) between words. We know of two closely-related previous systems, Kozareva and Hovy (2010) and Navigli et al. (2011), that build full taxonomies from scratch. Both of these systems use a process that starts by finding basic level terms (leaves of the final taxonomy tree, typically) and then using relational patterns (hand-selected ones in the case of Kozareva and Hovy (2010), and ones learned separately by a pairwise classifier on manually annotated co-occurrence patterns for Navigli and Velardi (2010), Navigli et al. (2011)) to find intermediate terms and all the attested hypernymy links between them.10 To prune down the resulting tax9 Determining the set of input terms is orthogonal to our work, and our method can be used in conjunction with various term extraction approaches described above. 10 Unlike our system, which assumes a complete set of terms and only attempts to induce the taxonomic structure, 1046 onomy graph, Kozareva and Hovy (2010) use a procedure that iteratively retains the longest paths between root and leaf terms, removing conflicting graph edges as they go. The end resu"
P14-1098,P06-1015,0,0.01721,"fire for at least t trees in the training data (t = 4 in our experiments). Note that one could also add various complementary types of useful features presented by previous work, e.g., bootstrapping using syntactic heuristics (Phillips and Riloff, 2002), dependency patterns (Snow et al., 2006), doubly anchored patterns (Kozareva et al., 2008; Hovy et al., 2009), and Web definition classifiers (Navigli et al., 2011). ilar contexts (Harris, 1954). The pattern-based approach uses special lexico-syntactic patterns to extract pairwise relation lists (Phillips and Riloff, 2002; Girju et al., 2003; Pantel and Pennacchiotti, 2006; Suchanek et al., 2007; Ritter et al., 2009; Hovy et al., 2009; Baroni et al., 2010; Ponzetto and Strube, 2011) and semantic classes or classinstance pairs (Riloff and Shepherd, 1997; Katz and Lin, 2003; Pas¸ca, 2004; Etzioni et al., 2005; Talukdar et al., 2008). We focus on the second step of taxonomy induction, namely the structured organization of terms into a complete and coherent tree-like hierarchy.9 Early work on this task assumes a starting partial taxonomy and inserts missing terms into it. Widdows (2003) place unknown words into a region with the most semantically-similar neighbors."
P14-1098,P06-1100,0,0.0219366,"1 Figure 1: An excerpt of WordNet’s vertebrates taxonomy. Introduction Many tasks in natural language understanding, such as question answering, information extraction, and textual entailment, benefit from lexical semantic information in the form of types and hypernyms. A recent example is IBM’s Jeopardy! system Watson (Ferrucci et al., 2010), which used type information to restrict the set of answer candidates. Information of this sort is present in term taxonomies (e.g., Figure 1), ontologies, and thesauri. However, currently available taxonomies such as WordNet are incomplete in coverage (Pennacchiotti and Pantel, 2006; Hovy et al., 2009), unavailable in many domains and languages, and time-intensive to create or extend manually. There has thus been considerable interest in building lexical taxonomies automatically. In this work, we focus on the task of taking collections of terms as input and predicting a complete taxonomy structure over them as output. Our model takes a loglinear form and is represented using a factor graph that includes both 1st-order scoring factors on directed hypernymy edges (a parent and child in the taxonomy) and 2nd-order scoring factors on sibling edge pairs (pairs of hypernym edg"
P14-1098,W02-1017,0,0.0512144,"full triple (xi , xj , xk ) but most such features will be sparse. 8 All the patterns and counts for our Web and Wikipedia edge and sibling features described above are extracted after stemming the words in the terms, the n-grams, and the abstracts (using the Porter stemmer). Also, we threshold the features (to prune away the sparse ones) by considering only those that fire for at least t trees in the training data (t = 4 in our experiments). Note that one could also add various complementary types of useful features presented by previous work, e.g., bootstrapping using syntactic heuristics (Phillips and Riloff, 2002), dependency patterns (Snow et al., 2006), doubly anchored patterns (Kozareva et al., 2008; Hovy et al., 2009), and Web definition classifiers (Navigli et al., 2011). ilar contexts (Harris, 1954). The pattern-based approach uses special lexico-syntactic patterns to extract pairwise relation lists (Phillips and Riloff, 2002; Girju et al., 2003; Pantel and Pennacchiotti, 2006; Suchanek et al., 2007; Ritter et al., 2009; Hovy et al., 2009; Baroni et al., 2010; Ponzetto and Strube, 2011) and semantic classes or classinstance pairs (Riloff and Shepherd, 1997; Katz and Lin, 2003; Pas¸ca, 2004; Etzio"
P14-1098,P10-1031,0,0.0146464,"y maximizing the conditional probability of a set of relational evidence given a taxonomy. Yang and Callan (2009) incrementally cluster terms based on a pairwise semantic distance. Lao et al. (2012) extend a knowledge base using a random walk model to learn binary relational inference rules. However, the task of inducing full taxonomies without assuming a substantial initial partial taxonomy is relatively less well studied. There is some prior work on the related task of hierarchical clustering, or grouping together of semantically related words (Cimiano et al., 2005; Cimiano and Staab, 2005; Poon and Domingos, 2010; Fountain and Lapata, 2012). The task we focus on, though, is the discovery of direct taxonomic relationships (e.g., hypernymy) between words. We know of two closely-related previous systems, Kozareva and Hovy (2010) and Navigli et al. (2011), that build full taxonomies from scratch. Both of these systems use a process that starts by finding basic level terms (leaves of the final taxonomy tree, typically) and then using relational patterns (hand-selected ones in the case of Kozareva and Hovy (2010), and ones learned separately by a pairwise classifier on manually annotated co-occurrence patte"
P14-1098,W97-0313,0,0.0581972,"otstrapping using syntactic heuristics (Phillips and Riloff, 2002), dependency patterns (Snow et al., 2006), doubly anchored patterns (Kozareva et al., 2008; Hovy et al., 2009), and Web definition classifiers (Navigli et al., 2011). ilar contexts (Harris, 1954). The pattern-based approach uses special lexico-syntactic patterns to extract pairwise relation lists (Phillips and Riloff, 2002; Girju et al., 2003; Pantel and Pennacchiotti, 2006; Suchanek et al., 2007; Ritter et al., 2009; Hovy et al., 2009; Baroni et al., 2010; Ponzetto and Strube, 2011) and semantic classes or classinstance pairs (Riloff and Shepherd, 1997; Katz and Lin, 2003; Pas¸ca, 2004; Etzioni et al., 2005; Talukdar et al., 2008). We focus on the second step of taxonomy induction, namely the structured organization of terms into a complete and coherent tree-like hierarchy.9 Early work on this task assumes a starting partial taxonomy and inserts missing terms into it. Widdows (2003) place unknown words into a region with the most semantically-similar neighbors. Snow et al. (2006) add novel terms by greedily maximizing the conditional probability of a set of relational evidence given a taxonomy. Yang and Callan (2009) incrementally cluster t"
P14-1098,D08-1016,0,0.0246291,"we wish to accomplish: computing expected feature counts and selecting a particular taxonomy tree for a given set of input terms (decoding). As an initial step to each of these procedures, we wish to compute the marginal probabilities of particular edges (and pairs of edges) being on. In a factor graph, the natural inference procedure for computing marginals is belief propagation. Note that finding taxonomy trees is a structurally identical problem to directed spanning trees (and thereby non-projective dependency parsing), for which belief propagation has previously been worked out in depth (Smith and Eisner, 2008). Therefore, we will only briefly sketch the procedure here. Belief propagation is a general-purpose inference method that computes marginals via directed messages passed from variables to adjacent factors (and vice versa) in the factor graph. These messages take the form of (possibly unnormalized) distributions over values of the variable. The two types of messages (variable to factor or factor to variable) have mutually recursive definitions. The message from a factor F to an adjacent variable V involves a sum over all possible values of every other variable that F touches. While the E DGE a"
P14-1098,P06-1101,0,0.389767,"like rat is a rodent in large corpora is a strong signal of the link rodent → rat. Moreover, sibling or coordination cues like either rats or squirrels suggest that rat is a sibling of squirrel and adds evidence for the links rodent → rat and rodent → squirrel. Our supervised model captures exactly these types of intuitions by automatically discovering such heterogeneous relational patterns as features (and learning their weights) on edges and on sibling edge pairs, respectively. There have been several previous studies on taxonomy induction. e.g., the incremental taxonomy induction system of Snow et al. (2006), the longest path approach of Kozareva and Hovy (2010), and the maximum spanning tree (MST) approach of Navigli et al. (2011) (see Section 4 for a more detailed overview). The main contribution of this work is that we present the first discriminatively trained, structured probabilistic model over the full space of taxonomy trees, using a structured inference procedure through both the learning and decoding phases. Our model is also the first to directly learn relational patterns as part of the process of training an end-to-end taxonomic induction system, rather than using patterns that were h"
P14-1098,D08-1061,0,0.0139227,"Missing"
P14-1098,N03-1036,0,0.0418323,"relation lists (Phillips and Riloff, 2002; Girju et al., 2003; Pantel and Pennacchiotti, 2006; Suchanek et al., 2007; Ritter et al., 2009; Hovy et al., 2009; Baroni et al., 2010; Ponzetto and Strube, 2011) and semantic classes or classinstance pairs (Riloff and Shepherd, 1997; Katz and Lin, 2003; Pas¸ca, 2004; Etzioni et al., 2005; Talukdar et al., 2008). We focus on the second step of taxonomy induction, namely the structured organization of terms into a complete and coherent tree-like hierarchy.9 Early work on this task assumes a starting partial taxonomy and inserts missing terms into it. Widdows (2003) place unknown words into a region with the most semantically-similar neighbors. Snow et al. (2006) add novel terms by greedily maximizing the conditional probability of a set of relational evidence given a taxonomy. Yang and Callan (2009) incrementally cluster terms based on a pairwise semantic distance. Lao et al. (2012) extend a knowledge base using a random walk model to learn binary relational inference rules. However, the task of inducing full taxonomies without assuming a substantial initial partial taxonomy is relatively less well studied. There is some prior work on the related task o"
P14-1098,D09-1097,0,0.0196102,"Missing"
P14-1098,P09-1031,0,0.555984,"or not. This captures pairs such as (fish, bony fish) in our data. Contains: Checks if xj contains xi , or not. This captures pairs such as (bird, bird of prey). Suffix match: Checks whether the k-length suffixes of xi and xj match, or not, for k = 1, 2, . . . , 7. LCS: We compute the longest common substring of xi and xj , and create indicator features for rounded-off and binned values of |LCS|/((|xi |+ |xj |)/2). Length difference: We compute the signed length difference between xj and xi , and create indicator features for rounded-off and binned values of (|xj |− |xi |)/((|xi |+ |xj |)/2). Yang and Callan (2009) use a similar feature. 3.2 3.2.1 Semantic Features Web n-gram Features Patterns and counts: Hypernymy for a term pair (P=xi , C=xj ) is often signaled by the presence of surface patterns like C is a P, P such as C in large text corpora, an observation going back to Hearst (1992). For each potential parent-child edge (P=xi , C=xj ), we mine the top k strings (based on count) in which both xi and xj occur (we use k=200). We collect patterns in both directions, which allows us to judge the correct direction of an edge (e.g., C is a P is a positive signal for hypernymy whereas P is a C is a negat"
P14-1098,C98-2122,0,\N,Missing
P15-1060,D12-1123,1,0.85578,"dependent and identically distributed. We define the following novel log-likelihood function ln LS , with four forms of regularization corresponding to the four kinds of priors: Figure 3: Prior Feature Extraction values and assign them an aspect prior probability pA,vk , indicating their general probability of being an aspect word. This TF-IDF approach is motivated by the following intuitions: the most frequently mentioned candidates in reviews have the highest probability of being an opinion target and false target words are non-domain specific and frequently appear in a general text corpus (Liu et al., 2012; Liu et al., 2013). For all adjective words, if the words are also included in the online sentiment resource SentiWordNet2 , we assign prior probability ps,vk to suggest that these words are generally recognized as sentiment words. Apart from these general priors, we obtain a small amount of fine-grained information as another type of prior knowledge. This fine-grained prior knowledge serves to indicate the probability of a known aspect word belonging to a specific aspect, denoted as pAj ,vk and an identified sentiment word bearing positive or negative sentiment, denoted as pSj ,vk . For inst"
P15-1060,N10-1122,0,0.55953,"roll, 2008; Mukherjee and Liu, 2012) were introduced to resolve certain aspect identification problems. However, supervised training requires handlabeled training data and has trouble coping with domain adaptation scenarios. Hence, unsupervised methods are often adopted to avoid this sort of dependency on labeled data. Latent Dirichlet Allocation, or LDA for short, (Blei et al., 2003) performs well in automatically extracting aspects and grouping corresponding representative words into categories. Thus, a number of LDA-based aspect identification approaches have been proposed in recent years (Brody and Elhadad, 2010; Titov and McDonald, 2008; Zhao et al., 2010). Still, these methods have several important drawbacks. First, inaccurate approximations of the distribution over topics may reduce the computational accuracy. Second, mixture models are unable to exploit the co-occurrence of topics to yield high probability predictions for words that are sharper than the distributions predicted by inAspect extraction and sentiment analysis of reviews are both important tasks in opinion mining. We propose a novel sentiment and aspect extraction model based on Restricted Boltzmann Machines to jointly address these"
P15-1060,P10-2050,0,0.0288242,"ation consists in judging whether an opinionated review expresses an overall positive or negative opinion. Regarding aspect identification, previous methods can be divided into three main categories: rule-based, supervised, and topic model-based methods. For instance, association rule-based methods (Hu and Liu, 2004; Liu et al., 1998) tend to focus on extracting product feature words and opinion words but neglect connecting product features at the aspect level. Existing rule-based methods typically are not able to group the extracted aspect terms into categories. Supervised (Jin et al., 2009; Choi and Cardie, 2010) and semisupervised learning methods (Zagibalov and Carroll, 2008; Mukherjee and Liu, 2012) were introduced to resolve certain aspect identification problems. However, supervised training requires handlabeled training data and has trouble coping with domain adaptation scenarios. Hence, unsupervised methods are often adopted to avoid this sort of dependency on labeled data. Latent Dirichlet Allocation, or LDA for short, (Blei et al., 2003) performs well in automatically extracting aspects and grouping corresponding representative words into categories. Thus, a number of LDA-based aspect identif"
P15-1060,P12-1036,0,0.626488,"negative opinion. Regarding aspect identification, previous methods can be divided into three main categories: rule-based, supervised, and topic model-based methods. For instance, association rule-based methods (Hu and Liu, 2004; Liu et al., 1998) tend to focus on extracting product feature words and opinion words but neglect connecting product features at the aspect level. Existing rule-based methods typically are not able to group the extracted aspect terms into categories. Supervised (Jin et al., 2009; Choi and Cardie, 2010) and semisupervised learning methods (Zagibalov and Carroll, 2008; Mukherjee and Liu, 2012) were introduced to resolve certain aspect identification problems. However, supervised training requires handlabeled training data and has trouble coping with domain adaptation scenarios. Hence, unsupervised methods are often adopted to avoid this sort of dependency on labeled data. Latent Dirichlet Allocation, or LDA for short, (Blei et al., 2003) performs well in automatically extracting aspects and grouping corresponding representative words into categories. Thus, a number of LDA-based aspect identification approaches have been proposed in recent years (Brody and Elhadad, 2010; Titov and M"
P15-1060,W02-1011,0,0.0243323,"riors for words in reviews and incorporate this information into the learning process of our Sentiment-Aspect Extraction model. We regularize our model based on these priors to constrain the aspect modeling and improve its accuracy. Figure 3 provides an example of how such priors can be applied to a sentence, with φi representing the prior knowledge. Research has found that most aspect words are nouns (or noun phrases), and sentiment is often expressed with adjectives. This additional information has been utilized in previous work on aspect extraction (Hu and Liu, 2004; Benamara et al., 2007; Pang et al., 2002). Inspired by this, we first rely on Part of Speech (POS) Tagging to identify nouns and adjectives. For all noun words, we first calculate their term frequency (TF) in the review corpus, and then compute their inverse document frequency (IDF) from an external Google n-gram corpus1 . Finally, we rank their TF∗IDF Wjk hj vbk j=1 k=1 K X F X (6) Under this architecture, this equation can be explained as the conditional probability from visible unit k to hidden unit j (softmax of words to aspect or sentiment). According to Eq. 6, the conditional probability for the k-th word feature towards the j-"
P15-1060,D10-1101,0,0.0209744,"in online reviews. Methods following their approach exploit frequent noun words and dependency relations to extract product features without supervision (Zhuang et al., 2006; Liu et al., 2005; Somasundaran and Wiebe, 2009). These methods work well when the aspect is strongly associated with a single noun, but obtain less satisfactory results when the aspect emerges from a combination of low frequency items. Additionally, rule-based methods have a common shortcoming in failing to group extracted aspect terms into categories. Supervised learning methods (Jin et al., 2009; Choi and Cardie, 2010; Jakob and Gurevych, 2010; Kobayashi et al., 2007) such as Hidden Markov Models, one-class SVMs, and Conditional Random Fields have been widely used in aspect information extraction. These supervised approaches for aspect identification are generally based on standard sequence labeling techniques. The downside of supervised learning is its requirement of large amounts of hand-labeled training data to provide enough information for aspect and opinion identification. Subsequent studies have proposed unsupervised learning methods, especially LDA-based topic modeling, to classify aspects of comments. Specific variants inc"
P15-1060,H05-1043,0,0.0238611,"products or services, on the Internet, especially in the course of e-commerce activities. Analyzing online reviews not only helps customers obtain useful product information, but also provide companies with feedback to enhance their products or service quality. Aspect-based opinion mining enables people to consider much more finegrained analyses of vast quantities of online reviews, perhaps from numerous different merchant sites. Thus, automatic identification of aspects of entities and relevant sentiment polarities in Big Data is a significant and urgent task (Liu, 2012; Pang and Lee, 2008; Popescu and Etzioni, 2005). Identifying aspect and analyzing sentiment words from reviews has the ultimate goal of discerning people’s opinions, attitudes, emotions, etc. towards entities such as products, services, organizations, individuals, events, etc. In this context, aspect-based opinion mining, also known as feature-based opinion mining, aims at extracting and summarizing particular salient aspects of entities and determining relevant sentiment polarities ∗ Corresponding Author: Kang Liu (kliu@nlpr.ia.ac.cn) 616 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th I"
P15-1060,P09-1026,0,0.0245404,"els presented previously. Overall, our main contributions are as follows: 4. Last but not the least, this RBM model is capable of jointly modeling aspect and sentiment information together. 2 Related Work We summarize prior state-of-the-art models for aspect extraction. In their seminal work, Hu and Liu (2004) propose the idea of applying classical information extraction to distinguish different aspects in online reviews. Methods following their approach exploit frequent noun words and dependency relations to extract product features without supervision (Zhuang et al., 2006; Liu et al., 2005; Somasundaran and Wiebe, 2009). These methods work well when the aspect is strongly associated with a single noun, but obtain less satisfactory results when the aspect emerges from a combination of low frequency items. Additionally, rule-based methods have a common shortcoming in failing to group extracted aspect terms into categories. Supervised learning methods (Jin et al., 2009; Choi and Cardie, 2010; Jakob and Gurevych, 2010; Kobayashi et al., 2007) such as Hidden Markov Models, one-class SVMs, and Conditional Random Fields have been widely used in aspect information extraction. These supervised approaches for aspect i"
P15-1060,D07-1114,0,0.0138766,"following their approach exploit frequent noun words and dependency relations to extract product features without supervision (Zhuang et al., 2006; Liu et al., 2005; Somasundaran and Wiebe, 2009). These methods work well when the aspect is strongly associated with a single noun, but obtain less satisfactory results when the aspect emerges from a combination of low frequency items. Additionally, rule-based methods have a common shortcoming in failing to group extracted aspect terms into categories. Supervised learning methods (Jin et al., 2009; Choi and Cardie, 2010; Jakob and Gurevych, 2010; Kobayashi et al., 2007) such as Hidden Markov Models, one-class SVMs, and Conditional Random Fields have been widely used in aspect information extraction. These supervised approaches for aspect identification are generally based on standard sequence labeling techniques. The downside of supervised learning is its requirement of large amounts of hand-labeled training data to provide enough information for aspect and opinion identification. Subsequent studies have proposed unsupervised learning methods, especially LDA-based topic modeling, to classify aspects of comments. Specific variants include the Multi-Grain LDA"
P15-1060,C08-1135,0,0.00873552,"esses an overall positive or negative opinion. Regarding aspect identification, previous methods can be divided into three main categories: rule-based, supervised, and topic model-based methods. For instance, association rule-based methods (Hu and Liu, 2004; Liu et al., 1998) tend to focus on extracting product feature words and opinion words but neglect connecting product features at the aspect level. Existing rule-based methods typically are not able to group the extracted aspect terms into categories. Supervised (Jin et al., 2009; Choi and Cardie, 2010) and semisupervised learning methods (Zagibalov and Carroll, 2008; Mukherjee and Liu, 2012) were introduced to resolve certain aspect identification problems. However, supervised training requires handlabeled training data and has trouble coping with domain adaptation scenarios. Hence, unsupervised methods are often adopted to avoid this sort of dependency on labeled data. Latent Dirichlet Allocation, or LDA for short, (Blei et al., 2003) performs well in automatically extracting aspects and grouping corresponding representative words into categories. Thus, a number of LDA-based aspect identification approaches have been proposed in recent years (Brody and"
P15-1060,D10-1006,0,0.802407,"d to resolve certain aspect identification problems. However, supervised training requires handlabeled training data and has trouble coping with domain adaptation scenarios. Hence, unsupervised methods are often adopted to avoid this sort of dependency on labeled data. Latent Dirichlet Allocation, or LDA for short, (Blei et al., 2003) performs well in automatically extracting aspects and grouping corresponding representative words into categories. Thus, a number of LDA-based aspect identification approaches have been proposed in recent years (Brody and Elhadad, 2010; Titov and McDonald, 2008; Zhao et al., 2010). Still, these methods have several important drawbacks. First, inaccurate approximations of the distribution over topics may reduce the computational accuracy. Second, mixture models are unable to exploit the co-occurrence of topics to yield high probability predictions for words that are sharper than the distributions predicted by inAspect extraction and sentiment analysis of reviews are both important tasks in opinion mining. We propose a novel sentiment and aspect extraction model based on Restricted Boltzmann Machines to jointly address these two tasks in an unsupervised setting. This mod"
P15-1060,H05-2017,0,\N,Missing
P15-1092,W99-0901,0,0.0618645,"ratory Max Planck Institute IIIS University of Cambridge, UK for Informatics, Germany Tsinghua University, China es407@cam.ac.uk ntandon@mpi-inf.mpg.de gdm@demelo.org Abstract (Fass, 1991; Mason, 2004; Shutova et al., 2013; Li et al., 2013). Automatic acquisition of SPs from linguistic data has thus become an active area of research. The community has investigated a range of techniques to tackle data sparsity and to perform generalisation from observed arguments to their underlying types, including the use of WordNet synsets as SP classes (Resnik, 1993; Li and Abe, 1998; Clark and Weir, 1999; Abney and Light, 1999; Ciaramita and Johnson, 2000), word clustering (Rooth et al., 1999; Bergsma et al., 2008; Sun and Korhonen, 2009), distributional similarity metrics (Erk, 2007; Peirsman and Pad´o, 2010), latent ´ S´eaghdha, 2010; Ritter et al., variable models (O 2010), and neural networks (Van de Cruys, 2014). Selectional preferences (SPs) are widely used in NLP as a rich source of semantic information. While SPs have been traditionally induced from textual data, human lexical acquisition is known to rely on both linguistic and perceptual experience. We present the first SP learning method that simultaneous"
P15-1092,R11-1055,0,0.066643,"predicate p is frequently observed in the data with the arguments a0 similar to a. The systems compute similarities between distributional representations of arguments in a vector space. Bergsma et al. (2008) trained an SVM classifier to discriminate between felicitous and infelicitous verb-argument pairs. Their training data consisted of observed verb-argument pairs (positive examples) with unobserved, randomly-generated ones (negative examples). They classified nominal arguments of verbs, using their verb co-occurrence probabilities and information about their semantic classes as features. Bergsma and Goebel (2011) extended this method by incorporating image-driven noun features. They extract color and SIFT keypoint features from images found for a particular noun via Google image searches and add them to the feature vectors to classify nouns as felicitous or infelicitous arguments of a given verb. This method is the closest in spirit to ours and the only one so far to investigate the relevance of visual fea951 tures to lexical preference learning. However, our work casts the problem in a different framework: rather than relying on low-level visual properties of nouns in isolation, we explicitly model i"
P15-1092,D08-1007,0,0.0603052,"Missing"
P15-1092,W02-1016,0,0.133523,"Missing"
P15-1092,P06-4020,0,0.0196478,"al., 2013), video descriptions (Rohrbach et al., 2013), or tags (Srivastava et al., 2014). Other applications of multimodal data include language modeling (Kiros et al., 2014) and knowledge mining from images (Chen et al., 2013; Divvala et al., 2014). Young et al. (2014) apply simplification rules to image captions, showing that the resulting hierarchy of mappings between natural language expressions and images can be used for entailment tasks. 3 Experimental data Textual data. We extract linguistic features for our model from the BNC. In particular, we parse the corpus using the RASP parser (Briscoe et al., 2006) and extract subject–verb and verb–object relations from its dependency output. These relations are then used as features for clustering to obtain SP classes, as well as to quantify the strength of association between a particular verb and a particular argument class. Visual data. For the visual features of our model, we mine the Yahoo! Webscope Flickr-100M dataset (Shamma, 2014). Flickr-100M contains 99.3 million images and 0.7 million videos with language tags annotated by users, enabling us to generalise SPs at a large scale. The tags reflect how humans describe objects and actions from a v"
P15-1092,P12-1015,0,0.0266966,"cted of positive (observed) and negative (randomly-generated) examples for training. The network weights were optimized by requiring the model to assign a higher score to an observed pair than to the unobserved one by a given margin. 2.2 Multi-modal methods in semantics Previous work has used multimodal data to determine distributional similarity or to learn multimodal embeddings that project multiple modalities into the same vector space. Some studies rely on extensions of LDA to obtain correlations between words and visual features (Feng and Lapata, 2010; Roller and Schulte im Walde, 2013). Bruni et al. (2012) integrated visual features into distributional similarity models using simple vector concatenation. Instead of generic visual features, Silberer et al. (2013) relied on supervised learning to train 412 higher-level visual attribute classifiers. Applications of multimodal embeddings include zero-shot object detection, i.e. recognizing objects in images without training data for the object class (Socher et al., 2013; Frome et al., 2013; Lazaridou et al., 2014), and automatic generation of image captions (Kulkarni et al., 2013), video descriptions (Rohrbach et al., 2013), or tags (Srivastava et"
P15-1092,C00-1028,0,0.099225,"tute IIIS University of Cambridge, UK for Informatics, Germany Tsinghua University, China es407@cam.ac.uk ntandon@mpi-inf.mpg.de gdm@demelo.org Abstract (Fass, 1991; Mason, 2004; Shutova et al., 2013; Li et al., 2013). Automatic acquisition of SPs from linguistic data has thus become an active area of research. The community has investigated a range of techniques to tackle data sparsity and to perform generalisation from observed arguments to their underlying types, including the use of WordNet synsets as SP classes (Resnik, 1993; Li and Abe, 1998; Clark and Weir, 1999; Abney and Light, 1999; Ciaramita and Johnson, 2000), word clustering (Rooth et al., 1999; Bergsma et al., 2008; Sun and Korhonen, 2009), distributional similarity metrics (Erk, 2007; Peirsman and Pad´o, 2010), latent ´ S´eaghdha, 2010; Ritter et al., variable models (O 2010), and neural networks (Van de Cruys, 2014). Selectional preferences (SPs) are widely used in NLP as a rich source of semantic information. While SPs have been traditionally induced from textual data, human lexical acquisition is known to rely on both linguistic and perceptual experience. We present the first SP learning method that simultaneously draws knowledge from text,"
P15-1092,W99-0631,0,0.0149694,"de Melo Computer Laboratory Max Planck Institute IIIS University of Cambridge, UK for Informatics, Germany Tsinghua University, China es407@cam.ac.uk ntandon@mpi-inf.mpg.de gdm@demelo.org Abstract (Fass, 1991; Mason, 2004; Shutova et al., 2013; Li et al., 2013). Automatic acquisition of SPs from linguistic data has thus become an active area of research. The community has investigated a range of techniques to tackle data sparsity and to perform generalisation from observed arguments to their underlying types, including the use of WordNet synsets as SP classes (Resnik, 1993; Li and Abe, 1998; Clark and Weir, 1999; Abney and Light, 1999; Ciaramita and Johnson, 2000), word clustering (Rooth et al., 1999; Bergsma et al., 2008; Sun and Korhonen, 2009), distributional similarity metrics (Erk, 2007; Peirsman and Pad´o, 2010), latent ´ S´eaghdha, 2010; Ritter et al., variable models (O 2010), and neural networks (Van de Cruys, 2014). Selectional preferences (SPs) are widely used in NLP as a rich source of semantic information. While SPs have been traditionally induced from textual data, human lexical acquisition is known to rely on both linguistic and perceptual experience. We present the first SP learning m"
P15-1092,P07-1028,0,0.149228,"act (Fass, 1991; Mason, 2004; Shutova et al., 2013; Li et al., 2013). Automatic acquisition of SPs from linguistic data has thus become an active area of research. The community has investigated a range of techniques to tackle data sparsity and to perform generalisation from observed arguments to their underlying types, including the use of WordNet synsets as SP classes (Resnik, 1993; Li and Abe, 1998; Clark and Weir, 1999; Abney and Light, 1999; Ciaramita and Johnson, 2000), word clustering (Rooth et al., 1999; Bergsma et al., 2008; Sun and Korhonen, 2009), distributional similarity metrics (Erk, 2007; Peirsman and Pad´o, 2010), latent ´ S´eaghdha, 2010; Ritter et al., variable models (O 2010), and neural networks (Van de Cruys, 2014). Selectional preferences (SPs) are widely used in NLP as a rich source of semantic information. While SPs have been traditionally induced from textual data, human lexical acquisition is known to rely on both linguistic and perceptual experience. We present the first SP learning method that simultaneously draws knowledge from text, images and videos, using image and video descriptions to obtain visual features. Our results show that it outperforms linguistic a"
P15-1092,J91-1003,0,0.600803,"Missing"
P15-1092,N10-1011,0,0.0190818,"elicitous and infelicitous arguments using the data constructed of positive (observed) and negative (randomly-generated) examples for training. The network weights were optimized by requiring the model to assign a higher score to an observed pair than to the unobserved one by a given margin. 2.2 Multi-modal methods in semantics Previous work has used multimodal data to determine distributional similarity or to learn multimodal embeddings that project multiple modalities into the same vector space. Some studies rely on extensions of LDA to obtain correlations between words and visual features (Feng and Lapata, 2010; Roller and Schulte im Walde, 2013). Bruni et al. (2012) integrated visual features into distributional similarity models using simple vector concatenation. Instead of generic visual features, Silberer et al. (2013) relied on supervised learning to train 412 higher-level visual attribute classifiers. Applications of multimodal embeddings include zero-shot object detection, i.e. recognizing objects in images without training data for the object class (Socher et al., 2013; Frome et al., 2013; Lazaridou et al., 2014), and automatic generation of image captions (Kulkarni et al., 2013), video desc"
P15-1092,J04-1002,0,0.0746674,"Missing"
P15-1092,J03-4004,0,0.033834,"ties are more likely to fill the predicate’s argument slot than others. For instance, while the sentences “The authors wrote a new paper.” and “The cat is eating your sausage!” sound natural and describe plausible real-life situations, the sentences “The carrot ate the keys.” and “The law sang a driveway.” appear implausible and difficult to interpret, as the arguments do not satisfy the verbs’ common preferences. SPs provide generalisations about word meaning and use and find a wide range of applications in natural language processing (NLP), including word sense disambiguation (Resnik, 1997; McCarthy and Carroll, 2003; Wagner et al., 2009), resolving ambiguous syntactic attachments (Hindle and Rooth, 1993), semantic role labelling (Gildea and Jurafsky, 2002; Zapirain et al., 2010), natural language inference (Zanzotto et al., 2006; Pantel et al., 2007), and figurative language processing Little research, however, has been concerned with the sources of knowledge that underlie the learning of SPs. There is ample evidence in cognitive and neurolinguistics that our concept learning and semantic representation are grounded in perception and action (Barsalou, 1999; Glenberg and Kaschak, 2002; Barsalou, 2008; Azi"
P15-1092,J02-3001,0,0.0450548,"“The cat is eating your sausage!” sound natural and describe plausible real-life situations, the sentences “The carrot ate the keys.” and “The law sang a driveway.” appear implausible and difficult to interpret, as the arguments do not satisfy the verbs’ common preferences. SPs provide generalisations about word meaning and use and find a wide range of applications in natural language processing (NLP), including word sense disambiguation (Resnik, 1997; McCarthy and Carroll, 2003; Wagner et al., 2009), resolving ambiguous syntactic attachments (Hindle and Rooth, 1993), semantic role labelling (Gildea and Jurafsky, 2002; Zapirain et al., 2010), natural language inference (Zanzotto et al., 2006; Pantel et al., 2007), and figurative language processing Little research, however, has been concerned with the sources of knowledge that underlie the learning of SPs. There is ample evidence in cognitive and neurolinguistics that our concept learning and semantic representation are grounded in perception and action (Barsalou, 1999; Glenberg and Kaschak, 2002; Barsalou, 2008; Aziz-Zadeh and Damasio, 2008). This suggests that word meaning and relational knowledge are acquired not only from linguistic input but also from"
P15-1092,P10-1045,0,0.264068,"Missing"
P15-1092,D07-1042,0,0.0379414,"Missing"
P15-1092,J93-1005,0,0.281033,"he sentences “The authors wrote a new paper.” and “The cat is eating your sausage!” sound natural and describe plausible real-life situations, the sentences “The carrot ate the keys.” and “The law sang a driveway.” appear implausible and difficult to interpret, as the arguments do not satisfy the verbs’ common preferences. SPs provide generalisations about word meaning and use and find a wide range of applications in natural language processing (NLP), including word sense disambiguation (Resnik, 1997; McCarthy and Carroll, 2003; Wagner et al., 2009), resolving ambiguous syntactic attachments (Hindle and Rooth, 1993), semantic role labelling (Gildea and Jurafsky, 2002; Zapirain et al., 2010), natural language inference (Zanzotto et al., 2006; Pantel et al., 2007), and figurative language processing Little research, however, has been concerned with the sources of knowledge that underlie the learning of SPs. There is ample evidence in cognitive and neurolinguistics that our concept learning and semantic representation are grounded in perception and action (Barsalou, 1999; Glenberg and Kaschak, 2002; Barsalou, 2008; Aziz-Zadeh and Damasio, 2008). This suggests that word meaning and relational knowledge are a"
P15-1092,N07-1071,0,0.386902,"nt variable, which represents a cluster of verb-argument interactions. The latent variable distribution and the probabilities that a latent variable generates the verb and the argument are learned from the data using Expectation Maximization (EM). The latent variables enable the model to recognise previously ´ S´eaghdha (2010) unseen verb-argument pairs. O and Ritter et al. (2010) similarly model SPs within a latent variable framework, but use Latent Dirichlet Allocation (LDA) to learn the probability distributions, for single-argument and multi-argument preferences respectively. Pad´o et al. (2007) and Erk (2007) used similarity metrics to approximate selectional preference classes. Their underlying hypothesis is that a predicate-argument combination (p, a) is felicitous if the predicate p is frequently observed in the data with the arguments a0 similar to a. The systems compute similarities between distributional representations of arguments in a vector space. Bergsma et al. (2008) trained an SVM classifier to discriminate between felicitous and infelicitous verb-argument pairs. Their training data consisted of observed verb-argument pairs (positive examples) with unobserved, randomly-"
P15-1092,J03-3005,0,0.78165,"ata. For instance, while concrete verbs, such as run, push or throw, are more likely to be prominent in visual data, abstract verbs, such as understand or speculate, are best 954 represented in text. Relative linguistic and visual frequencies of a verb provide a way to estimate the relevance of linguistic and visual features to its SP learning. 6 VSP ISP: ISP: ISP: ISP: ISP: ISP: ISP: ISP: ISP: ISP: LSP Direct evaluation and data analysis We evaluate the predicate-argument scores assigned by our models against a dataset of human plausibility judgements of verb-direct object pairs collected by Keller and Lapata (2003). Their dataset is balanced with respect to the frequency of verb-argument relations, as well as their plausibility and implausibility, thus creating a realistic SP evaluation task. Keller and Lapata selected 30 predicates and matched each of them to three arguments from different co-occurrence frequency bands according to their BNC counts, e.g. divert attention (high frequency), divert water (medium) and divert fruit (low). This constituted their dataset of Seen verb-noun pairs, 90 in total. Each of the predicates was then also paired with three randomly selected arguments with which it did n"
P15-1092,P14-2135,0,0.0328611,"ere is ample evidence in cognitive and neurolinguistics that our concept learning and semantic representation are grounded in perception and action (Barsalou, 1999; Glenberg and Kaschak, 2002; Barsalou, 2008; Aziz-Zadeh and Damasio, 2008). This suggests that word meaning and relational knowledge are acquired not only from linguistic input but also from our experiences in the physical world. Multi-modal models of word meaning have thus enjoyed a growing interest in semantics (Bruni et al., 2014), outperforming purely text-based models in tasks such as similarity estimation (Bruni et al., 2014; Kiela et al., 2014), predicting compositionality (Roller and Schulte im Walde, 2013), and concept categorization (Silberer and Lapata, 2014). However, to date these approaches relied on low-level image features such as color histograms or SIFT keypoints to represent the meaning of isolated words. To the best of our knowledge, there has not yet been a multimodal semantic approach performing extraction of 950 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 950–960, c Beijing, China, July 26-31,"
P15-1092,N10-1135,0,0.04861,"Missing"
P15-1092,P14-1132,0,0.0240024,"ly on extensions of LDA to obtain correlations between words and visual features (Feng and Lapata, 2010; Roller and Schulte im Walde, 2013). Bruni et al. (2012) integrated visual features into distributional similarity models using simple vector concatenation. Instead of generic visual features, Silberer et al. (2013) relied on supervised learning to train 412 higher-level visual attribute classifiers. Applications of multimodal embeddings include zero-shot object detection, i.e. recognizing objects in images without training data for the object class (Socher et al., 2013; Frome et al., 2013; Lazaridou et al., 2014), and automatic generation of image captions (Kulkarni et al., 2013), video descriptions (Rohrbach et al., 2013), or tags (Srivastava et al., 2014). Other applications of multimodal data include language modeling (Kiros et al., 2014) and knowledge mining from images (Chen et al., 2013; Divvala et al., 2014). Young et al. (2014) apply simplification rules to image captions, showing that the resulting hierarchy of mappings between natural language expressions and images can be used for entailment tasks. 3 Experimental data Textual data. We extract linguistic features for our model from the BNC."
P15-1092,J98-2002,0,0.0244617,"iket Tandon Gerard de Melo Computer Laboratory Max Planck Institute IIIS University of Cambridge, UK for Informatics, Germany Tsinghua University, China es407@cam.ac.uk ntandon@mpi-inf.mpg.de gdm@demelo.org Abstract (Fass, 1991; Mason, 2004; Shutova et al., 2013; Li et al., 2013). Automatic acquisition of SPs from linguistic data has thus become an active area of research. The community has investigated a range of techniques to tackle data sparsity and to perform generalisation from observed arguments to their underlying types, including the use of WordNet synsets as SP classes (Resnik, 1993; Li and Abe, 1998; Clark and Weir, 1999; Abney and Light, 1999; Ciaramita and Johnson, 2000), word clustering (Rooth et al., 1999; Bergsma et al., 2008; Sun and Korhonen, 2009), distributional similarity metrics (Erk, 2007; Peirsman and Pad´o, 2010), latent ´ S´eaghdha, 2010; Ritter et al., variable models (O 2010), and neural networks (Van de Cruys, 2014). Selectional preferences (SPs) are widely used in NLP as a rich source of semantic information. While SPs have been traditionally induced from textual data, human lexical acquisition is known to rely on both linguistic and perceptual experience. We present t"
P15-1092,Q13-1031,0,0.0358004,"Missing"
P15-1092,W97-0209,0,0.351304,"lasses of entities are more likely to fill the predicate’s argument slot than others. For instance, while the sentences “The authors wrote a new paper.” and “The cat is eating your sausage!” sound natural and describe plausible real-life situations, the sentences “The carrot ate the keys.” and “The law sang a driveway.” appear implausible and difficult to interpret, as the arguments do not satisfy the verbs’ common preferences. SPs provide generalisations about word meaning and use and find a wide range of applications in natural language processing (NLP), including word sense disambiguation (Resnik, 1997; McCarthy and Carroll, 2003; Wagner et al., 2009), resolving ambiguous syntactic attachments (Hindle and Rooth, 1993), semantic role labelling (Gildea and Jurafsky, 2002; Zapirain et al., 2010), natural language inference (Zanzotto et al., 2006; Pantel et al., 2007), and figurative language processing Little research, however, has been concerned with the sources of knowledge that underlie the learning of SPs. There is ample evidence in cognitive and neurolinguistics that our concept learning and semantic representation are grounded in perception and action (Barsalou, 1999; Glenberg and Kascha"
P15-1092,P10-1044,0,0.0969056,"Missing"
P15-1092,D13-1115,0,0.0807902,"Missing"
P15-1092,P99-1014,0,0.434155,"matics, Germany Tsinghua University, China es407@cam.ac.uk ntandon@mpi-inf.mpg.de gdm@demelo.org Abstract (Fass, 1991; Mason, 2004; Shutova et al., 2013; Li et al., 2013). Automatic acquisition of SPs from linguistic data has thus become an active area of research. The community has investigated a range of techniques to tackle data sparsity and to perform generalisation from observed arguments to their underlying types, including the use of WordNet synsets as SP classes (Resnik, 1993; Li and Abe, 1998; Clark and Weir, 1999; Abney and Light, 1999; Ciaramita and Johnson, 2000), word clustering (Rooth et al., 1999; Bergsma et al., 2008; Sun and Korhonen, 2009), distributional similarity metrics (Erk, 2007; Peirsman and Pad´o, 2010), latent ´ S´eaghdha, 2010; Ritter et al., variable models (O 2010), and neural networks (Van de Cruys, 2014). Selectional preferences (SPs) are widely used in NLP as a rich source of semantic information. While SPs have been traditionally induced from textual data, human lexical acquisition is known to rely on both linguistic and perceptual experience. We present the first SP learning method that simultaneously draws knowledge from text, images and videos, using image and vi"
P15-1092,N10-1147,1,0.930668,"o improve SP prediction across frequency bands. 7 Task-based evaluation In order to investigate the applicability of perceptually grounded SPs in wider NLP, we evaluate them in the context of an external semantic task – that of metaphor interpretation. Since metaphor is based on transferring imagery and knowledge across domains – typically from more familiar domains of physical experiences to the sphere of vague and elusive abstract thought – metaphor interpretation provides an ideal framework for testing perceptually grounded SPs. Our experiments rely on the metaphor interpretation method of Shutova (2010), in which text-derived SPs are a central component of the system. We replace the SP component with our LSP and ISP (λLM = 0.8) models and compare their performance in the context of metaphor interpretation. Shutova (2010) defined metaphor interpretation as a paraphrasing task, where literal paraphrases for metaphorical expressions are derived from corpus data using a set of statistical measures. For instance, their system interprets the metaphor “a carelessly leaked report” as “a carelessly disclosed report”. Focusing on metaphorical verbs in subject and direct object constructions, Shutova f"
P15-1092,P14-1068,0,0.055673,"grounded in perception and action (Barsalou, 1999; Glenberg and Kaschak, 2002; Barsalou, 2008; Aziz-Zadeh and Damasio, 2008). This suggests that word meaning and relational knowledge are acquired not only from linguistic input but also from our experiences in the physical world. Multi-modal models of word meaning have thus enjoyed a growing interest in semantics (Bruni et al., 2014), outperforming purely text-based models in tasks such as similarity estimation (Bruni et al., 2014; Kiela et al., 2014), predicting compositionality (Roller and Schulte im Walde, 2013), and concept categorization (Silberer and Lapata, 2014). However, to date these approaches relied on low-level image features such as color histograms or SIFT keypoints to represent the meaning of isolated words. To the best of our knowledge, there has not yet been a multimodal semantic approach performing extraction of 950 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 950–960, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics predicate-argument relations from visual data. In this paper, we pro"
P15-1092,P13-1056,0,0.0243041,"igher score to an observed pair than to the unobserved one by a given margin. 2.2 Multi-modal methods in semantics Previous work has used multimodal data to determine distributional similarity or to learn multimodal embeddings that project multiple modalities into the same vector space. Some studies rely on extensions of LDA to obtain correlations between words and visual features (Feng and Lapata, 2010; Roller and Schulte im Walde, 2013). Bruni et al. (2012) integrated visual features into distributional similarity models using simple vector concatenation. Instead of generic visual features, Silberer et al. (2013) relied on supervised learning to train 412 higher-level visual attribute classifiers. Applications of multimodal embeddings include zero-shot object detection, i.e. recognizing objects in images without training data for the object class (Socher et al., 2013; Frome et al., 2013; Lazaridou et al., 2014), and automatic generation of image captions (Kulkarni et al., 2013), video descriptions (Rohrbach et al., 2013), or tags (Srivastava et al., 2014). Other applications of multimodal data include language modeling (Kiros et al., 2014) and knowledge mining from images (Chen et al., 2013; Divvala e"
P15-1092,D09-1067,0,0.612588,"es407@cam.ac.uk ntandon@mpi-inf.mpg.de gdm@demelo.org Abstract (Fass, 1991; Mason, 2004; Shutova et al., 2013; Li et al., 2013). Automatic acquisition of SPs from linguistic data has thus become an active area of research. The community has investigated a range of techniques to tackle data sparsity and to perform generalisation from observed arguments to their underlying types, including the use of WordNet synsets as SP classes (Resnik, 1993; Li and Abe, 1998; Clark and Weir, 1999; Abney and Light, 1999; Ciaramita and Johnson, 2000), word clustering (Rooth et al., 1999; Bergsma et al., 2008; Sun and Korhonen, 2009), distributional similarity metrics (Erk, 2007; Peirsman and Pad´o, 2010), latent ´ S´eaghdha, 2010; Ritter et al., variable models (O 2010), and neural networks (Van de Cruys, 2014). Selectional preferences (SPs) are widely used in NLP as a rich source of semantic information. While SPs have been traditionally induced from textual data, human lexical acquisition is known to rely on both linguistic and perceptual experience. We present the first SP learning method that simultaneously draws knowledge from text, images and videos, using image and video descriptions to obtain visual features. Our"
P15-1092,D14-1004,0,0.261526,"Missing"
P15-1092,P06-1107,0,0.0193566,"e situations, the sentences “The carrot ate the keys.” and “The law sang a driveway.” appear implausible and difficult to interpret, as the arguments do not satisfy the verbs’ common preferences. SPs provide generalisations about word meaning and use and find a wide range of applications in natural language processing (NLP), including word sense disambiguation (Resnik, 1997; McCarthy and Carroll, 2003; Wagner et al., 2009), resolving ambiguous syntactic attachments (Hindle and Rooth, 1993), semantic role labelling (Gildea and Jurafsky, 2002; Zapirain et al., 2010), natural language inference (Zanzotto et al., 2006; Pantel et al., 2007), and figurative language processing Little research, however, has been concerned with the sources of knowledge that underlie the learning of SPs. There is ample evidence in cognitive and neurolinguistics that our concept learning and semantic representation are grounded in perception and action (Barsalou, 1999; Glenberg and Kaschak, 2002; Barsalou, 2008; Aziz-Zadeh and Damasio, 2008). This suggests that word meaning and relational knowledge are acquired not only from linguistic input but also from our experiences in the physical world. Multi-modal models of word meaning"
P15-1092,N10-1058,0,0.0588702,"Missing"
P15-1092,J13-2003,1,\N,Missing
P15-1092,Q14-1006,0,\N,Missing
P16-1123,H05-1091,0,0.81175,"c and syntactic features not only for the meanings of the target entities at the lexical level, but also for their immediate context and for the overall sentence structure. Thus, it is not surprising that numerous featureand kernel-based approaches have been proposed, many of which rely on a full-fledged NLP stack, including POS tagging, morphological analysis, dependency parsing, and occasionally semantic analysis, as well as on knowledge resources to capture lexical and semantic features (Kambhatla, 2004; Zhou et al., 2005; Suchanek et al., 2006; Qian et al., 2008; Mooney and Bunescu, 2005; Bunescu and Mooney, 2005). In recent years, we have seen a move towards deep architectures that are capable of learning relevant representations and features without extensive manual feature engineering or use of external resources. A number of convolutional neural network (CNN), recurrent neural network (RNN), and other neural architectures have been proposed for relation classification (Zeng et al., 2014; dos Santos et al., 2015; Xu et al., 2015b). Still, these models often fail to identify critical cues, and many of them still require an external dependency parser. We propose a novel CNN architecture that addresses"
P16-1123,D13-1137,0,0.0230714,"), who proposed the Ranking CNN (CR-CNN) model with a class embedding matrix, Miwa and Bansal (2016) similarly observed that LSTM-based RNNs are outperformed by models using CNNs, due to limited linguistic structure captured in the network architecture. Some more elaborate variants have been proposed to address this, including bidirectional LSTMs (Zhang et al., 2015), deep recurrent neural networks (Xu et al., 2016), and bidirectional treestructured LSTM-RNNs (Miwa and Bansal, 2016). Several recent works also reintroduce a dependency tree-based design, e.g., RNNs operating on syntactic trees (Hashimoto et al., 2013), shortest dependency path-based CNNs (Xu et al., 2015a), and the SDP-LSTM model (Xu et al., 2015b). Finally, Nguyen and Grishman (2015) train both CNNs and RNNs and variously aggregate their outputs using voting, stacking, or log-linear modeling (Nguyen and Grishman, 2015). Although these recent models achieve solid results, ideally, we would want a simple yet effective architecture that does not require dependency parsing or training multiple models. Our experiments in Section 4 demonstrate that we can indeed achieve this, while also obtaining substantial improvements in terms of the obtaine"
P16-1123,S10-1006,0,0.385997,"Missing"
P16-1123,P15-2047,0,0.371751,"Missing"
P16-1123,C10-1018,0,0.27468,"Missing"
P16-1123,P15-1003,0,0.0118049,"gonal Input att. matrix Input att. matrix (S,drinks) (S,diabetes) Input Attention Mechanism While position-based encodings are useful, we conjecture that they do not suffice to fully capture the relationships of specific words with the target entities and the influence that they may bear on the target relations of interest. We design our model so as to automatically identify the parts of the input sentence that are relevant for relation classification. Attention mechanisms have successfully been applied to sequence-to-sequence learning tasks such as machine translation (Bahdanau et al., 2015; Meng et al., 2015) and abstractive sentence summarization (Rush et al., 2015), as well as to tasks such as modeling sentence pairs (Yin et al., 2015) and question answering (Santos et al., 2016). To date, these mechanisms have generally been used to allow for an alignment of the input and output sequence, e.g. the source and target sentence in machine translation, or for an alignment between two input sentences as in sentence similarity scoring and question answering. Figure 2: Input and Primary Attention In our work, we apply the idea of modeling attention to a rather different kind of scenario involving heter"
P16-1123,I05-2045,0,0.0703878,"Missing"
P16-1123,N13-1090,0,0.0916889,"label embedding W L and the ground truth label y and δθ (S, yˆ− ) refers to the distance between wO and a selected incorrect relation label yˆ− . The latter is chosen as the one with the highest score among all incorrect classes (Weston et al., 2011; dos Santos et al., 2015), i.e. yˆ− = argmax δ(S, y 0 ). y 0 ∈Y,y 0 6=y (3) This margin-based objective has the advantage of a strong interpretability and effectiveness compared with empirical loss functions such as the ranking loss function in the CR-CNN approach by dos Santos et al. (2015). Based on a distance function motived by word analogies (Mikolov et al., 2013b), we minimize the gap between predicted outputs and ground-truth labels, while maximizing the distance with the selected incorrect class. By minimizing this pairwise loss function iteratively (see Section 3.5), δθ (S, y) are encouraged to decrease, while δθ (S, yˆ− ) increase. 3.2 Input Representation Given a sentence S = (w1 , w2 , ..., wn ) with marked entity mentions e1 (=wp ) and e2 (=wt ), (p, t ∈ [1, n], p 6= t), we first transform every word into a real-valued vector to provide lexicalsemantic features. Given a word embedding matrix WV of dimensionality dw × |V |, where V is the input"
P16-1123,P15-1061,0,0.543388,"sis, as well as on knowledge resources to capture lexical and semantic features (Kambhatla, 2004; Zhou et al., 2005; Suchanek et al., 2006; Qian et al., 2008; Mooney and Bunescu, 2005; Bunescu and Mooney, 2005). In recent years, we have seen a move towards deep architectures that are capable of learning relevant representations and features without extensive manual feature engineering or use of external resources. A number of convolutional neural network (CNN), recurrent neural network (RNN), and other neural architectures have been proposed for relation classification (Zeng et al., 2014; dos Santos et al., 2015; Xu et al., 2015b). Still, these models often fail to identify critical cues, and many of them still require an external dependency parser. We propose a novel CNN architecture that addresses some of the shortcomings of previous approaches. Our key contributions are as follows: 1. Our CNN architecture relies on a novel multi1298 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1298–1307, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics level attention mechanism to capture both entity-specific attention (primary a"
P16-1123,P04-1053,0,0.043296,"Missing"
P16-1123,P16-1105,0,0.38535,"ure the compositional aspects of the sentence semantics by exploiting syntactic trees. Zeng et al. (2014) proposed a deep convolutional neural network with softmax classification, extracting lexical and sentence level features. However, these approaches still depend on additional features from lexical resources and NLP toolkits. Yu et al. (2014) proposed the Factor-based Compositional Embedding Model, which uses syntactic dependency trees together with sentence-level embeddings. In addition to dos Santos et al. (2015), who proposed the Ranking CNN (CR-CNN) model with a class embedding matrix, Miwa and Bansal (2016) similarly observed that LSTM-based RNNs are outperformed by models using CNNs, due to limited linguistic structure captured in the network architecture. Some more elaborate variants have been proposed to address this, including bidirectional LSTMs (Zhang et al., 2015), deep recurrent neural networks (Xu et al., 2016), and bidirectional treestructured LSTM-RNNs (Miwa and Bansal, 2016). Several recent works also reintroduce a dependency tree-based design, e.g., RNNs operating on syntactic trees (Hashimoto et al., 2013), shortest dependency path-based CNNs (Xu et al., 2015a), and the SDP-LSTM mo"
P16-1123,P14-2012,0,0.189735,"Missing"
P16-1123,C08-1088,0,0.388895,"eeds to be able to account for useful semantic and syntactic features not only for the meanings of the target entities at the lexical level, but also for their immediate context and for the overall sentence structure. Thus, it is not surprising that numerous featureand kernel-based approaches have been proposed, many of which rely on a full-fledged NLP stack, including POS tagging, morphological analysis, dependency parsing, and occasionally semantic analysis, as well as on knowledge resources to capture lexical and semantic features (Kambhatla, 2004; Zhou et al., 2005; Suchanek et al., 2006; Qian et al., 2008; Mooney and Bunescu, 2005; Bunescu and Mooney, 2005). In recent years, we have seen a move towards deep architectures that are capable of learning relevant representations and features without extensive manual feature engineering or use of external resources. A number of convolutional neural network (CNN), recurrent neural network (RNN), and other neural architectures have been proposed for relation classification (Zeng et al., 2014; dos Santos et al., 2015; Xu et al., 2015b). Still, these models often fail to identify critical cues, and many of them still require an external dependency parse"
P16-1123,D09-1149,0,0.0188072,"tated target entity mentions e1 = “drinks” and e2 = “diabetes”, the goal would be to automatically recognize that this sentence expresses a causeeffect relationship between e1 and e2 , for which we use the notation Cause-Effect(e1 ,e2 ). Accurate relation classification facilitates precise sentence interpretations, discourse processing, and higherlevel NLP tasks (Hendrickx et al., 2010). Thus, ∗ † Equal contribution. Corresponding author. Email: liuzy@tsinghua.edu.cn relation classification has attracted considerable attention from researchers over the course of the past decades (Zhang, 2004; Qian et al., 2009; Rink and Harabagiu, 2010). In the example given above, the verb corresponds quite closely to the desired target relation. However, in the wild, we encounter a multitude of different ways of expressing the same kind of relationship. This challenging variability can be lexical, syntactic, or even pragmatic in nature. An effective solution needs to be able to account for useful semantic and syntactic features not only for the meanings of the target entities at the lexical level, but also for their immediate context and for the overall sentence structure. Thus, it is not surprising that numerous"
P16-1123,S10-1057,0,0.78958,"mentions e1 = “drinks” and e2 = “diabetes”, the goal would be to automatically recognize that this sentence expresses a causeeffect relationship between e1 and e2 , for which we use the notation Cause-Effect(e1 ,e2 ). Accurate relation classification facilitates precise sentence interpretations, discourse processing, and higherlevel NLP tasks (Hendrickx et al., 2010). Thus, ∗ † Equal contribution. Corresponding author. Email: liuzy@tsinghua.edu.cn relation classification has attracted considerable attention from researchers over the course of the past decades (Zhang, 2004; Qian et al., 2009; Rink and Harabagiu, 2010). In the example given above, the verb corresponds quite closely to the desired target relation. However, in the wild, we encounter a multitude of different ways of expressing the same kind of relationship. This challenging variability can be lexical, syntactic, or even pragmatic in nature. An effective solution needs to be able to account for useful semantic and syntactic features not only for the meanings of the target entities at the lexical level, but also for their immediate context and for the overall sentence structure. Thus, it is not surprising that numerous featureand kernel-based ap"
P16-1123,D15-1044,0,0.0430919,"betes) Input Attention Mechanism While position-based encodings are useful, we conjecture that they do not suffice to fully capture the relationships of specific words with the target entities and the influence that they may bear on the target relations of interest. We design our model so as to automatically identify the parts of the input sentence that are relevant for relation classification. Attention mechanisms have successfully been applied to sequence-to-sequence learning tasks such as machine translation (Bahdanau et al., 2015; Meng et al., 2015) and abstractive sentence summarization (Rush et al., 2015), as well as to tasks such as modeling sentence pairs (Yin et al., 2015) and question answering (Santos et al., 2016). To date, these mechanisms have generally been used to allow for an alignment of the input and output sequence, e.g. the source and target sentence in machine translation, or for an alignment between two input sentences as in sentence similarity scoring and question answering. Figure 2: Input and Primary Attention In our work, we apply the idea of modeling attention to a rather different kind of scenario involving heterogeneous objects, namely a sentence and two entities. With"
P16-1123,D12-1110,0,0.0321798,"are often derived from other pre-trained NLP tools or lexical and semantic resources. Although such approaches can benefit from the external NLP tools to discover the discrete structure of a sentence, syntactic parsing is error-prone and relying on its success may also impede performance (Bach and Badaskar, 2007). Further downsides include their limited lexical generalization abilities for unseen words and their lack of robustness when applied to new domains, genres, or languages. In recent years, deep neural networks have shown promising results. The Recursive MatrixVector Model (MV-RNN) by Socher et al. (2012) sought to capture the compositional aspects of the sentence semantics by exploiting syntactic trees. Zeng et al. (2014) proposed a deep convolutional neural network with softmax classification, extracting lexical and sentence level features. However, these approaches still depend on additional features from lexical resources and NLP toolkits. Yu et al. (2014) proposed the Factor-based Compositional Embedding Model, which uses syntactic dependency trees together with sentence-level embeddings. In addition to dos Santos et al. (2015), who proposed the Ranking CNN (CR-CNN) model with a class emb"
P16-1123,D15-1206,0,0.849633,"owledge resources to capture lexical and semantic features (Kambhatla, 2004; Zhou et al., 2005; Suchanek et al., 2006; Qian et al., 2008; Mooney and Bunescu, 2005; Bunescu and Mooney, 2005). In recent years, we have seen a move towards deep architectures that are capable of learning relevant representations and features without extensive manual feature engineering or use of external resources. A number of convolutional neural network (CNN), recurrent neural network (RNN), and other neural architectures have been proposed for relation classification (Zeng et al., 2014; dos Santos et al., 2015; Xu et al., 2015b). Still, these models often fail to identify critical cues, and many of them still require an external dependency parser. We propose a novel CNN architecture that addresses some of the shortcomings of previous approaches. Our key contributions are as follows: 1. Our CNN architecture relies on a novel multi1298 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1298–1307, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics level attention mechanism to capture both entity-specific attention (primary attention at the i"
P16-1123,C16-1138,0,0.363235,"kits. Yu et al. (2014) proposed the Factor-based Compositional Embedding Model, which uses syntactic dependency trees together with sentence-level embeddings. In addition to dos Santos et al. (2015), who proposed the Ranking CNN (CR-CNN) model with a class embedding matrix, Miwa and Bansal (2016) similarly observed that LSTM-based RNNs are outperformed by models using CNNs, due to limited linguistic structure captured in the network architecture. Some more elaborate variants have been proposed to address this, including bidirectional LSTMs (Zhang et al., 2015), deep recurrent neural networks (Xu et al., 2016), and bidirectional treestructured LSTM-RNNs (Miwa and Bansal, 2016). Several recent works also reintroduce a dependency tree-based design, e.g., RNNs operating on syntactic trees (Hashimoto et al., 2013), shortest dependency path-based CNNs (Xu et al., 2015a), and the SDP-LSTM model (Xu et al., 2015b). Finally, Nguyen and Grishman (2015) train both CNNs and RNNs and variously aggregate their outputs using voting, stacking, or log-linear modeling (Nguyen and Grishman, 2015). Although these recent models achieve solid results, ideally, we would want a simple yet effective architecture that does"
P16-1123,C14-1220,0,0.761797,"sionally semantic analysis, as well as on knowledge resources to capture lexical and semantic features (Kambhatla, 2004; Zhou et al., 2005; Suchanek et al., 2006; Qian et al., 2008; Mooney and Bunescu, 2005; Bunescu and Mooney, 2005). In recent years, we have seen a move towards deep architectures that are capable of learning relevant representations and features without extensive manual feature engineering or use of external resources. A number of convolutional neural network (CNN), recurrent neural network (RNN), and other neural architectures have been proposed for relation classification (Zeng et al., 2014; dos Santos et al., 2015; Xu et al., 2015b). Still, these models often fail to identify critical cues, and many of them still require an external dependency parser. We propose a novel CNN architecture that addresses some of the shortcomings of previous approaches. Our key contributions are as follows: 1. Our CNN architecture relies on a novel multi1298 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1298–1307, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics level attention mechanism to capture both entity-spec"
P16-1123,Y15-1009,0,0.185236,"ditional features from lexical resources and NLP toolkits. Yu et al. (2014) proposed the Factor-based Compositional Embedding Model, which uses syntactic dependency trees together with sentence-level embeddings. In addition to dos Santos et al. (2015), who proposed the Ranking CNN (CR-CNN) model with a class embedding matrix, Miwa and Bansal (2016) similarly observed that LSTM-based RNNs are outperformed by models using CNNs, due to limited linguistic structure captured in the network architecture. Some more elaborate variants have been proposed to address this, including bidirectional LSTMs (Zhang et al., 2015), deep recurrent neural networks (Xu et al., 2016), and bidirectional treestructured LSTM-RNNs (Miwa and Bansal, 2016). Several recent works also reintroduce a dependency tree-based design, e.g., RNNs operating on syntactic trees (Hashimoto et al., 2013), shortest dependency path-based CNNs (Xu et al., 2015a), and the SDP-LSTM model (Xu et al., 2015b). Finally, Nguyen and Grishman (2015) train both CNNs and RNNs and variously aggregate their outputs using voting, stacking, or log-linear modeling (Nguyen and Grishman, 2015). Although these recent models achieve solid results, ideally, we would"
P16-1123,P05-1053,0,0.614057,"agmatic in nature. An effective solution needs to be able to account for useful semantic and syntactic features not only for the meanings of the target entities at the lexical level, but also for their immediate context and for the overall sentence structure. Thus, it is not surprising that numerous featureand kernel-based approaches have been proposed, many of which rely on a full-fledged NLP stack, including POS tagging, morphological analysis, dependency parsing, and occasionally semantic analysis, as well as on knowledge resources to capture lexical and semantic features (Kambhatla, 2004; Zhou et al., 2005; Suchanek et al., 2006; Qian et al., 2008; Mooney and Bunescu, 2005; Bunescu and Mooney, 2005). In recent years, we have seen a move towards deep architectures that are capable of learning relevant representations and features without extensive manual feature engineering or use of external resources. A number of convolutional neural network (CNN), recurrent neural network (RNN), and other neural architectures have been proposed for relation classification (Zeng et al., 2014; dos Santos et al., 2015; Xu et al., 2015b). Still, these models often fail to identify critical cues, and many of them"
P16-1123,D15-1062,0,0.788036,"owledge resources to capture lexical and semantic features (Kambhatla, 2004; Zhou et al., 2005; Suchanek et al., 2006; Qian et al., 2008; Mooney and Bunescu, 2005; Bunescu and Mooney, 2005). In recent years, we have seen a move towards deep architectures that are capable of learning relevant representations and features without extensive manual feature engineering or use of external resources. A number of convolutional neural network (CNN), recurrent neural network (RNN), and other neural architectures have been proposed for relation classification (Zeng et al., 2014; dos Santos et al., 2015; Xu et al., 2015b). Still, these models often fail to identify critical cues, and many of them still require an external dependency parser. We propose a novel CNN architecture that addresses some of the shortcomings of previous approaches. Our key contributions are as follows: 1. Our CNN architecture relies on a novel multi1298 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1298–1307, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics level attention mechanism to capture both entity-specific attention (primary attention at the i"
P16-4005,D11-1072,0,0.131571,"Missing"
P18-1081,P16-1154,0,0.0572921,"reveals five distinct patterns. The first case is that of exact matches with the reference descriptions. The second involves examples on which there is a high overlap of words between the ground truth and generated descriptions, but the latter as a whole is incorrect because of semantic drift or other challenges. In some cases, the model may have never seen a word or named entity during training (e.g., Hypocrisy), or their frequency is very limited in the training set. While it has been shown that GRUs with an attention mechanism are capable of learning to copy random strings from the input (Gu et al., 2016), we conjecture that a dedicated copy mechanism may help to mitigate this problem, which we will explore in future research. In other cases, the model conflates semantically related concepts, as is evident from examples such as a film being described as a filmmaker and a polo player as a water polo player. Next, the third group involves generated descriptions that are more specific than the ground truth, but correct, while, in the fourth group, the generated outputs generalize the descriptions to a certain extent. For example, American musician and pianist is generalized as American musician,"
P18-1081,P14-1098,1,0.930516,"Missing"
P18-1081,C92-2082,0,0.145888,"t) chosen by type prediction methods. Related Work Type Prediction. There has been extensive work on predicting the ontological types of entities in large knowledge graphs (Neelakantan and Chang, 2015; Miao et al., 2016; Kejriwal and Szekely, 2017; Shimaoka et al., 2017), in semistructured resources such as Wikipedia (Ponzetto and Strube, 2007; de Melo and Weikum, 2010), as well as in text (Del Corro et al., 2015; Yaghoobzadeh and Sch¨utze, 2015; Ren et al., A separate, long-running series of work has obtained open vocabulary type predictions for named entities and concepts mentioned in text (Hearst, 1992; Snow et al., 2006), possibly also induc883 erating game commentary. Liang et al. (2009) learned alignments between formal descriptions such as rainChance(time=26-30,mode=Def) and natural language weather reports. Mei et al. (2016) used LSTMs for these sorts of generation tasks, via a custom coarse-to-fine architecture that first determines which input parts to focus on. Much of the aforementioned work essentially involves aligning small snippets in the input to the relevant parts in the training output and then learning to expand such input snippets into full sentences. In contrast, in our t"
P18-1081,I17-1031,0,0.150714,"ducing type descriptions from structured data. of which are more abstract in nature (e.g., human or artifact). In this work, we consider the task of generating more detailed open vocabulary descriptions (e.g., Swiss tennis player) that can readily be presented to end users, generated from facts in the knowledge graph. Apart from type descriptions, certain knowledge graphs, such as Freebase and DBpedia, also provide a paragraph-length textual abstract for every entity. In the latter case, these are sourced from Wikipedia. There has also been research on generating such abstracts automatically (Biran and McKeown, 2017). While abstracts of this sort provide considerably more detail than ontological types, they are not sufficiently concise to be grasped at a single glance, and thus the onus is put on the reader to comprehend and summarize them. Typically, a short description of an entity will hence need to be synthesized just by drawing on certain most relevant facts about it. While in many circumstances, humans tend to categorize entities at a level of abstraction commonly referred to as basic level categories (Rosch et al., 1976), in an information seeking setting, however, such as in Fig. 1, humans natural"
P18-1081,D16-1128,0,0.0364148,"equires synthesizing a short description by carefully selecting the most relevant and distinctive facts from the set of all available facts about the entity. Due to these differences, the WebNLG dataset was not suitable for the research question considered by our paper. ing taxonomies from them (Poon and Domingos, 2010; Velardi et al., 2013; Bansal et al., 2014). However, these methods typically just need to select existing spans of text from the input as the output description. Text Generation from Structured Data. Research on methods to generate descriptions for entities has remained scant. Lebret et al. (2016) take Wikipedia infobox data as input and train a custom form of neural language model that, conditioned on occurrences of words in the input table, generates biographical sentences as output. However, their system is limited to a single kind of description (biographical sentences) that tend to share a common structure. Wang et al. (2016) focus on the problem of temporal ordering of extracted facts. Biran and McKeown (2017) introduced a template-based description generation framework for creating hybrid concept-to-text and text-to-text generation systems that produce descriptions of RDF entiti"
P18-1081,D15-1103,0,0.0215804,"hat often indicate the type of entity, but typically are more natural-sounding and descriptive (e.g. French Impressionist artist) than the oftentimes abstract ontological types (such as human or artifact) chosen by type prediction methods. Related Work Type Prediction. There has been extensive work on predicting the ontological types of entities in large knowledge graphs (Neelakantan and Chang, 2015; Miao et al., 2016; Kejriwal and Szekely, 2017; Shimaoka et al., 2017), in semistructured resources such as Wikipedia (Ponzetto and Strube, 2007; de Melo and Weikum, 2010), as well as in text (Del Corro et al., 2015; Yaghoobzadeh and Sch¨utze, 2015; Ren et al., A separate, long-running series of work has obtained open vocabulary type predictions for named entities and concepts mentioned in text (Hearst, 1992; Snow et al., 2006), possibly also induc883 erating game commentary. Liang et al. (2009) learned alignments between formal descriptions such as rainChance(time=26-30,mode=Def) and natural language weather reports. Mei et al. (2016) used LSTMs for these sorts of generation tasks, via a custom coarse-to-fine architecture that first determines which input parts to focus on. Much of the aforementioned wo"
P18-1081,P09-1011,0,0.0152884,"extensive work on predicting the ontological types of entities in large knowledge graphs (Neelakantan and Chang, 2015; Miao et al., 2016; Kejriwal and Szekely, 2017; Shimaoka et al., 2017), in semistructured resources such as Wikipedia (Ponzetto and Strube, 2007; de Melo and Weikum, 2010), as well as in text (Del Corro et al., 2015; Yaghoobzadeh and Sch¨utze, 2015; Ren et al., A separate, long-running series of work has obtained open vocabulary type predictions for named entities and concepts mentioned in text (Hearst, 1992; Snow et al., 2006), possibly also induc883 erating game commentary. Liang et al. (2009) learned alignments between formal descriptions such as rainChance(time=26-30,mode=Def) and natural language weather reports. Mei et al. (2016) used LSTMs for these sorts of generation tasks, via a custom coarse-to-fine architecture that first determines which input parts to focus on. Much of the aforementioned work essentially involves aligning small snippets in the input to the relevant parts in the training output and then learning to expand such input snippets into full sentences. In contrast, in our task, alignments between parts of the input and the output do not suffice. Instead, descri"
P18-1081,Q18-1013,1,0.880119,"Missing"
P18-1081,Q13-1000,0,0.337256,"Missing"
P18-1081,W17-3518,0,0.0602659,"in the training output and then learning to expand such input snippets into full sentences. In contrast, in our task, alignments between parts of the input and the output do not suffice. Instead, describing an entity often also involves considering all available evidence about that entity to infer information about it that is often not immediately given. Rather than verbalizing facts, our method needs a complex attention mechanism to predict an object’s general type and consider the information that is most likely to appear salient to humans from across the entire input. The WebNLG Challenge (Gardent et al., 2017) is another task for generating text from structured data. However, this task requires a textual verbalization of every triple. On the contrary, the task we consider in this work is quite complementary in that a verbalization of all facts one-by-one is not the sought result. Rather, our task requires synthesizing a short description by carefully selecting the most relevant and distinctive facts from the set of all available facts about the entity. Due to these differences, the WebNLG dataset was not suitable for the research question considered by our paper. ing taxonomies from them (Poon and"
P18-1081,N16-1086,0,0.0192988,"al and Szekely, 2017; Shimaoka et al., 2017), in semistructured resources such as Wikipedia (Ponzetto and Strube, 2007; de Melo and Weikum, 2010), as well as in text (Del Corro et al., 2015; Yaghoobzadeh and Sch¨utze, 2015; Ren et al., A separate, long-running series of work has obtained open vocabulary type predictions for named entities and concepts mentioned in text (Hearst, 1992; Snow et al., 2006), possibly also induc883 erating game commentary. Liang et al. (2009) learned alignments between formal descriptions such as rainChance(time=26-30,mode=Def) and natural language weather reports. Mei et al. (2016) used LSTMs for these sorts of generation tasks, via a custom coarse-to-fine architecture that first determines which input parts to focus on. Much of the aforementioned work essentially involves aligning small snippets in the input to the relevant parts in the training output and then learning to expand such input snippets into full sentences. In contrast, in our task, alignments between parts of the input and the output do not suffice. Instead, describing an entity often also involves considering all available evidence about that entity to infer information about it that is often not immedia"
P18-1081,P15-2048,0,0.0728016,"Missing"
P18-1081,P06-1101,0,0.140488,"ditionally, descriptions of this sort can also be useful to determine the ontological type of an entity – another challenging task that often needs to be addressed in cross-domain knowledge graphs. Many knowledge graphs already provide ontological type information, and there has been substantial previous research on how to predict such types automatically for entities in knowledge graphs (Neelakantan and Chang, 2015; Miao et al., 2016; Kejriwal and Szekely, 2017), in semistructured resources such as Wikipedia (Ponzetto and Strube, 2007; de Melo and Weikum, 2010), or even in unstructured text (Snow et al., 2006; Bansal et al., 2014; Tandon et al., 2015). However, most such work has targeted a fixed inventory of types from a given target ontology, many Broad-coverage knowledge graphs such as Freebase, Wikidata, and NELL are increasingly being used in many NLP and AI tasks. For instance, DBpedia and YAGO were vital for IBM’s Watson! Jeopardy system (Welty et al., 2012). Google’s Knowledge Graph is tightly integrated into its search engine, yielding improved responses for entity queries as well as for question answering. In a similar effort, Apple Inc. is building an inhouse knowledge graph to power Si"
P18-1081,N15-1054,0,0.174768,"xtual descriptions. Introduction is Roger Federer?), named entity disambiguation (e.g., Philadelphia as a city vs. the film or even the brand of cream cheese), and information retrieval, to name but a few. Additionally, descriptions of this sort can also be useful to determine the ontological type of an entity – another challenging task that often needs to be addressed in cross-domain knowledge graphs. Many knowledge graphs already provide ontological type information, and there has been substantial previous research on how to predict such types automatically for entities in knowledge graphs (Neelakantan and Chang, 2015; Miao et al., 2016; Kejriwal and Szekely, 2017), in semistructured resources such as Wikipedia (Ponzetto and Strube, 2007; de Melo and Weikum, 2010), or even in unstructured text (Snow et al., 2006; Bansal et al., 2014; Tandon et al., 2015). However, most such work has targeted a fixed inventory of types from a given target ontology, many Broad-coverage knowledge graphs such as Freebase, Wikidata, and NELL are increasingly being used in many NLP and AI tasks. For instance, DBpedia and YAGO were vital for IBM’s Watson! Jeopardy system (Welty et al., 2012). Google’s Knowledge Graph is tightly i"
P18-1081,P10-1031,0,0.0961116,"l., 2017) is another task for generating text from structured data. However, this task requires a textual verbalization of every triple. On the contrary, the task we consider in this work is quite complementary in that a verbalization of all facts one-by-one is not the sought result. Rather, our task requires synthesizing a short description by carefully selecting the most relevant and distinctive facts from the set of all available facts about the entity. Due to these differences, the WebNLG dataset was not suitable for the research question considered by our paper. ing taxonomies from them (Poon and Domingos, 2010; Velardi et al., 2013; Bansal et al., 2014). However, these methods typically just need to select existing spans of text from the input as the output description. Text Generation from Structured Data. Research on methods to generate descriptions for entities has remained scant. Lebret et al. (2016) take Wikipedia infobox data as input and train a custom form of neural language model that, conditioned on occurrences of words in the input table, generates biographical sentences as output. However, their system is limited to a single kind of description (biographical sentences) that tend to shar"
P18-1081,D16-1144,0,0.110188,"Missing"
P18-1081,D15-1044,0,0.0209848,"was extractive in nature, i.e. it selects the most salient sentences from a given input text and concatenates them to form a shorter summary or presents them differently to the user (Yang et al., 2017). Abstractive summarization goes beyond this in generating new text not necessarily encountered in the input, as is typically necessary in our setting. The surge of sequence-to-sequence modeling of text via LSTMs naturally extends to the task of abstractive summarization by training a model to accept a longer sequence as input and learning to generate a shorter compressed sequence as a summary. Rush et al. (2015) employed this idea to generate a short headline from the first sentence of a text. Subsequent work investigated the use of 884 rely on them for reasoning purposes. To this end, DMN+ needs an additional layer of GRUs, which is used to capture sequential correlations among sentences. Our model does not need any such layer, as facts in a knowledge graph do not necessarily possess any sequential interconnections. Additionally, DMNs assume a predefined number of memory episodes, with the final memory state being passed to the answer module. Unlike DMNs, our model uses the dynamic context of the ou"
P18-1081,J13-3007,0,0.0651007,"for generating text from structured data. However, this task requires a textual verbalization of every triple. On the contrary, the task we consider in this work is quite complementary in that a verbalization of all facts one-by-one is not the sought result. Rather, our task requires synthesizing a short description by carefully selecting the most relevant and distinctive facts from the set of all available facts about the entity. Due to these differences, the WebNLG dataset was not suitable for the research question considered by our paper. ing taxonomies from them (Poon and Domingos, 2010; Velardi et al., 2013; Bansal et al., 2014). However, these methods typically just need to select existing spans of text from the input as the output description. Text Generation from Structured Data. Research on methods to generate descriptions for entities has remained scant. Lebret et al. (2016) take Wikipedia infobox data as input and train a custom form of neural language model that, conditioned on occurrences of words in the input table, generates biographical sentences as output. However, their system is limited to a single kind of description (biographical sentences) that tend to share a common structure."
P18-1081,P17-1099,0,0.0181777,"memory state being passed to the answer module. Unlike DMNs, our model uses the dynamic context of the output sequence to update the memory state. The number of memory updates in our model flexibly depends on the length of the generated sequence. DMNs also have an additional question module as input, which guides the memory updates and also the output, while our model does not leverage any such guiding factor. Finally, in DMNs, the output is typically a unigram, whereas our model emits a sequence of words. architectures such as pointer-generator networks to better cope with long input texts (See et al., 2017). Recently, Liu et al. (2018) presented a model that generates an entire Wikipedia article via a neural decoder component that performs abstractive summarization of multiple source documents. Our work differs from such previous work in that we do not consider a text sequence as input. Rather, our input are a series of entity relationships or properties, as reflected by our facts-to-sequence baselines in the experiments. Note that our task is in certain respects also more difficult than text summarization. While regular neural summarizers are often able to identify salient spans of text that ca"
P18-1081,D15-1083,0,0.053345,"Missing"
P18-1081,Y16-3009,0,\N,Missing
P18-1235,baccianella-etal-2010-sentiwordnet,0,0.010055,"s). Hence, the figure confirms that our DM-MCNN approach is able to exploit and customize the provided sentiment weights for the target domain. However, unlike the VADER data, our transfer learning approach results in multi-dimensional sentiment embeddings that can more easily capture multiple domains right from the start, thus making it possible to use them even without further fine-tuning. 4 Related Work Sentiment Mining and Embeddings. There is a long history of work on collecting word polarity scores manually (Hu and Liu, 2004) or via graphbased propagation from seeds (Kim and Hovy, 2004; Baccianella et al., 2010). Maas et al. (2011) present a probabilistic topic model that exploits sentiment supervision during training, leading to rep2531 Weight of Word 0.4 0.2 0.0 0.2 suwort s h po tlain we ike pe larful rfe ck c sotlly id nofun n p in la e t ste ys prroagead ma bl dy hilnagem ar e de kiouss fin ind coitely wme unantss le lagcensr s ple kinge asuno rr paes f r s a y p ma ilty carovirdter ptu es d re moamns ass ne urey seteread it de g wnag pre ho r ssi st ng lauwo mdev gh deistaokid miserve ss e intjokineg co eloress nv se t i ap fani ceds pe lur staulinge pid 0.4 Figure 3: Top 50 weight changes of w"
P18-1235,D14-1080,0,0.0893617,"Missing"
P18-1235,P14-1062,0,0.138844,"Missing"
P18-1235,P07-1056,0,0.128858,"ages. en es de ru α 0.0004 0.0008 0.003 0.003 cs it ja α 0.003 0.003 0.003 Common Crawl data1 , while for other languages, we rely on the Facebook fastText Wikipedia embeddings (Bojanowski et al., 2016) as input representations. All of these are 300-dimensional. The vectors are either fed to the CNN, or to the convolutional module of the DM-MCNN during initialization, while unknown words are initialized with zeros. All words, including the unknown ones, are fine-tuned during the training process. For our transfer learning approach, our experiments rely on the multi-domain sentiment dataset by Blitzer et al. (2007), collected from Amazon customers reviews. This dataset includes 25 categories of products and is used to generate our sentiment embeddings using linear models. Specifically, we train linear SVMs using scikit-learn to extract word coefficients in each domain and also for the union of all domains together, yielding a 26-dimensional sentiment embedding. For comparison and analysis, we also consider several alternative forms of infusing external cues. Firstly, lexicon-driven methods have often been used for domain-independent sentiment analysis. We consider a recent sentiment lexicon called VADER"
P18-1235,C04-1200,0,0.341814,"iptions of war movies). Hence, the figure confirms that our DM-MCNN approach is able to exploit and customize the provided sentiment weights for the target domain. However, unlike the VADER data, our transfer learning approach results in multi-dimensional sentiment embeddings that can more easily capture multiple domains right from the start, thus making it possible to use them even without further fine-tuning. 4 Related Work Sentiment Mining and Embeddings. There is a long history of work on collecting word polarity scores manually (Hu and Liu, 2004) or via graphbased propagation from seeds (Kim and Hovy, 2004; Baccianella et al., 2010). Maas et al. (2011) present a probabilistic topic model that exploits sentiment supervision during training, leading to rep2531 Weight of Word 0.4 0.2 0.0 0.2 suwort s h po tlain we ike pe larful rfe ck c sotlly id nofun n p in la e t ste ys prroagead ma bl dy hilnagem ar e de kiouss fin ind coitely wme unantss le lagcensr s ple kinge asuno rr paes f r s a y p ma ilty carovirdter ptu es d re moamns ass ne urey seteread it de g wnag pre ho r ssi st ng lauwo mdev gh deistaokid miserve ss e intjokineg co eloress nv se t i ap fani ceds pe lur staulinge pid 0.4 Figure 3:"
P18-1235,D14-1181,0,0.0102195,"Missing"
P18-1235,D16-1171,0,0.0149368,"er shallow models, our study examines how they are best be incorporated into deep neural models, as the baseline of na¨ıvely feeding them into the model does not work sufficiently well. Neural Architectures. In terms of architectures, deep recursive neural networks (Socher et al., 2013) were soon outperformed by deep convolutional and recurrent neural networks (˙Irsoy and Cardie, 2014; Kim, 2014). Recent work has investigated more involved models, with ingredients such as Tree-LSTMs (Tai et al., 2015; Looks et al., 2017), hierarchical attention (Yang et al., 2016), user and product attention (Chen et al., 2016), aspectspecific modeling (Wang et al., 2015), and part of speech-specific transition functions (Huang et al., 2017). Large ensemble models also tend to outperform individually trained sentiment analysis models (Looks et al., 2017). The goal of our study is not necessarily to devise the most sophisticated stateof-the-art neural architecture, but to demonstrate how external sentiment cues can be incorporated such architectures. Our initial explorations relied on a simple dual-channel convolutional neural network (Dong and de Melo, 2018). The present work proposes a more sophisticated approach,"
P18-1235,2015.mtsummit-papers.27,1,0.911127,"Missing"
P18-1235,D16-1057,0,0.0841494,"Missing"
P18-1235,P11-1015,0,0.108009,"rms that our DM-MCNN approach is able to exploit and customize the provided sentiment weights for the target domain. However, unlike the VADER data, our transfer learning approach results in multi-dimensional sentiment embeddings that can more easily capture multiple domains right from the start, thus making it possible to use them even without further fine-tuning. 4 Related Work Sentiment Mining and Embeddings. There is a long history of work on collecting word polarity scores manually (Hu and Liu, 2004) or via graphbased propagation from seeds (Kim and Hovy, 2004; Baccianella et al., 2010). Maas et al. (2011) present a probabilistic topic model that exploits sentiment supervision during training, leading to rep2531 Weight of Word 0.4 0.2 0.0 0.2 suwort s h po tlain we ike pe larful rfe ck c sotlly id nofun n p in la e t ste ys prroagead ma bl dy hilnagem ar e de kiouss fin ind coitely wme unantss le lagcensr s ple kinge asuno rr paes f r s a y p ma ilty carovirdter ptu es d re moamns ass ne urey seteread it de g wnag pre ho r ssi st ng lauwo mdev gh deistaokid miserve ss e intjokineg co eloress nv se t i ap fani ceds pe lur staulinge pid 0.4 Figure 3: Top 50 weight changes of words fine-tuned by t"
P18-1235,D14-1162,0,0.080071,"Missing"
P18-1235,N18-1202,0,0.0499671,"set of domains and languages if a suitable neural network architecture is used. 2 2.1 Approach Sentiment Embedding Computation Our goal is to incorporate external cues into a deep neural network such that the network is able to generalize better even when training data is scarce. While in computer vision, weights pre-trained on ImageNet are often used for transfer learning, the most popular way to incorporate external information into deep neural networks for text is to draw on word embeddings trained on vast amounts of word context information (Mikolov et al., 2013; Pennington et al., 2014; Peters et al., 2018). Indeed, the semantic relatedness signals provided by such representations often lead to slightly improved results in polarity classification tasks (Socher et al., 2013; Kim, 2014; dos Santos and de C. Gatti, 2014). However, the co-occurrence-based objectives of word2vec and GloVe do not consider sentiment 2524 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2524–2534 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics specifically. We thus seek to examine how complementary sentiment-specific i"
P18-1235,C14-1008,0,0.112676,"Missing"
P18-1235,D13-1170,0,0.364803,"ral networks tend to be particularly data-hungry. This is a problem in many real-world settings, where large amounts of training examples may be too costly to obtain for every target domain. A model trained on movie reviews, for instance, will fare very poorly on the task of assessing restaurant or hotel reviews, let alone tweets about politicians. In this paper, we investigate how extrinsic signals can be incorporated into deep neural networks for sentiment analysis. Numerous papers have found the use of regular pre-trained word vector representations to be beneficial for sentiment analysis (Socher et al., 2013; Kim, 2014; dos Santos and de C. Gatti, 2014). In our paper, we instead consider word embeddings specifically specialized for the task of sentiment analysis, studying how they can lead to stronger and more consistent gains, despite the fact that the embeddings were obtained using out-of-domain data. An intuitive solution would be to concatenate regular embeddings, which provide semantic relatedness cues, with sentiment polarity cues that are captured in additional dimensions. We instead propose a bespoke convolutional neural network architecture with a separate memory module dedicated to the"
P18-1235,P15-1150,0,0.0430715,"Missing"
P18-1235,P14-1146,0,0.0349634,"e sentiment memory module of the DM-MCNN, using the one-dimensional VADER embeddings, but considering only words with non-zero values in the original VADER data. Here, the dark shade (blue) refers to the original value of word vectors, while the light shade (red) refers to their fine-tuned values after training. The medium intensity (purple) corresponds to the overlap between the original and fine-tuned word vectors. resentations that include sentiment signals. However, in their experiments, the semantic-only models mostly outperform the corresponding full models with extra sentiment signals. Tang et al. (2014) showed that one can acquire sentiment information by learning from millions of training examples via distant supervision. While prior work used such signals for rule-based sentiment analysis or for feature engineering in SVMs and other shallow models, our study examines how they are best be incorporated into deep neural models, as the baseline of na¨ıvely feeding them into the model does not work sufficiently well. Neural Architectures. In terms of architectures, deep recursive neural networks (Socher et al., 2013) were soon outperformed by deep convolutional and recurrent neural networks (˙I"
P18-1235,P15-1060,1,0.88386,"Missing"
P18-1235,N16-1174,0,0.0302469,"lysis or for feature engineering in SVMs and other shallow models, our study examines how they are best be incorporated into deep neural models, as the baseline of na¨ıvely feeding them into the model does not work sufficiently well. Neural Architectures. In terms of architectures, deep recursive neural networks (Socher et al., 2013) were soon outperformed by deep convolutional and recurrent neural networks (˙Irsoy and Cardie, 2014; Kim, 2014). Recent work has investigated more involved models, with ingredients such as Tree-LSTMs (Tai et al., 2015; Looks et al., 2017), hierarchical attention (Yang et al., 2016), user and product attention (Chen et al., 2016), aspectspecific modeling (Wang et al., 2015), and part of speech-specific transition functions (Huang et al., 2017). Large ensemble models also tend to outperform individually trained sentiment analysis models (Looks et al., 2017). The goal of our study is not necessarily to devise the most sophisticated stateof-the-art neural architecture, but to demonstrate how external sentiment cues can be incorporated such architectures. Our initial explorations relied on a simple dual-channel convolutional neural network (Dong and de Melo, 2018). The prese"
P18-2100,marelli-etal-2014-sick,0,0.0503404,"Missing"
P18-2100,N13-1090,0,0.1247,"prominent sentence embedding methods exhibit select semantic properties. We propose a framework that generate triplets of sentences to explore how changes in the syntactic structure or semantics of a given sentence affect the similarities obtained between their sentence embeddings. 1 Gerard de Melo Rutgers University Piscataway, NJ, USA gdm@demelo.org Introduction Neural vector representations have become ubiquitous in all subfields of natural language processing. For the case of word vectors, important properties of the representations have been studied, including their linear substructures (Mikolov et al., 2013; Levy and Goldberg, 2014), the linear superposition of word senses (Arora et al., 2016b), and the nexus to pointwise mutual information scores between co-occurring words (Arora et al., 2016a). However, thus far, only little is known about the properties of sentence embeddings. Sentence embedding methods attempt to encode a variablelength input sentence into a fixed length vector. A number of such sentence embedding methods have been proposed in recent years (Le and Mikolov, 2014; Kiros et al., 2015; Wieting et al., 2015; Conneau et al., 2017; Arora et al., 2017). Sentence embeddings have main"
P18-2100,Q16-1028,0,0.0156863,"mework that generate triplets of sentences to explore how changes in the syntactic structure or semantics of a given sentence affect the similarities obtained between their sentence embeddings. 1 Gerard de Melo Rutgers University Piscataway, NJ, USA gdm@demelo.org Introduction Neural vector representations have become ubiquitous in all subfields of natural language processing. For the case of word vectors, important properties of the representations have been studied, including their linear substructures (Mikolov et al., 2013; Levy and Goldberg, 2014), the linear superposition of word senses (Arora et al., 2016b), and the nexus to pointwise mutual information scores between co-occurring words (Arora et al., 2016a). However, thus far, only little is known about the properties of sentence embeddings. Sentence embedding methods attempt to encode a variablelength input sentence into a fixed length vector. A number of such sentence embedding methods have been proposed in recent years (Le and Mikolov, 2014; Kiros et al., 2015; Wieting et al., 2015; Conneau et al., 2017; Arora et al., 2017). Sentence embeddings have mainly been evaluated in terms of how well their cosine similarities mirror human judgments"
P18-2100,N18-1049,0,0.112649,"Missing"
P18-2100,D14-1162,0,0.0965231,"Missing"
P18-2100,P17-1080,0,0.0294936,"benchmark the effectiveness of different models across a broad range of challenging lexical, syntactic, and semantic phenomena, in terms of both similarities and the ability to be predictive of entailment. However, even on SICK, oftentimes very shallow methods prove effective at obtaining fairly competitive results (Wieting et al., 2015). Adi et al. investigated to what extent different embedding methods are predictive of i) the occurrence of words in the original sentence, ii) the order of words in the original sentence, and iii) the length of the original sentence (Adi et al., 2016, 2017). Belinkov et al. (2017) inspected neural machine translation systems with regard to their ability to acquire morphology, while Shi et al. (2016) investigated to what extent they learn source side syntax. Wang et al. (2016) argue that the latent representations of advanced neural reading comprehension architectures encode information about predication. Finally, sentence embeddings have also often been investigated in classification tasks such as sentiment polarity or question type classification (Kiros et al., 2015). Concurrently with our research, Conneau et al. (2018) investigated to what extent one can learn to cl"
P18-2100,D16-1159,0,0.0317555,"a, in terms of both similarities and the ability to be predictive of entailment. However, even on SICK, oftentimes very shallow methods prove effective at obtaining fairly competitive results (Wieting et al., 2015). Adi et al. investigated to what extent different embedding methods are predictive of i) the occurrence of words in the original sentence, ii) the order of words in the original sentence, and iii) the length of the original sentence (Adi et al., 2016, 2017). Belinkov et al. (2017) inspected neural machine translation systems with regard to their ability to acquire morphology, while Shi et al. (2016) investigated to what extent they learn source side syntax. Wang et al. (2016) argue that the latent representations of advanced neural reading comprehension architectures encode information about predication. Finally, sentence embeddings have also often been investigated in classification tasks such as sentiment polarity or question type classification (Kiros et al., 2015). Concurrently with our research, Conneau et al. (2018) investigated to what extent one can learn to classify specific syntactic and semantic properties of sentences using large amounts of training data (100,000 instances) f"
P18-2100,P18-1198,0,0.0863357,"he original sentence (Adi et al., 2016, 2017). Belinkov et al. (2017) inspected neural machine translation systems with regard to their ability to acquire morphology, while Shi et al. (2016) investigated to what extent they learn source side syntax. Wang et al. (2016) argue that the latent representations of advanced neural reading comprehension architectures encode information about predication. Finally, sentence embeddings have also often been investigated in classification tasks such as sentiment polarity or question type classification (Kiros et al., 2015). Concurrently with our research, Conneau et al. (2018) investigated to what extent one can learn to classify specific syntactic and semantic properties of sentences using large amounts of training data (100,000 instances) for each property. Overall, still, remarkably little is known about what specific semantic properties are directly reflected by such embeddings. In this paper, we specifically focus on a few select aspects of sentence semantics and inspect to what extent prominent sentence embedding methods are able to capture them. Our framework generates triplets of sentences to explore how changes in the syntactic structure or semantics of a"
P18-2100,D17-1070,0,0.0648027,"been studied, including their linear substructures (Mikolov et al., 2013; Levy and Goldberg, 2014), the linear superposition of word senses (Arora et al., 2016b), and the nexus to pointwise mutual information scores between co-occurring words (Arora et al., 2016a). However, thus far, only little is known about the properties of sentence embeddings. Sentence embedding methods attempt to encode a variablelength input sentence into a fixed length vector. A number of such sentence embedding methods have been proposed in recent years (Le and Mikolov, 2014; Kiros et al., 2015; Wieting et al., 2015; Conneau et al., 2017; Arora et al., 2017). Sentence embeddings have mainly been evaluated in terms of how well their cosine similarities mirror human judgments of semantic relatedness, typically with respect to the SemEval Semantic Textual Similarity competitions. The SICK 632 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 632–637 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics 2 Analysis A: The young boy is climbing the wall made of rock. B: The young boy isn’t climbing the wall made of rock. To conduct our"
P18-2100,W14-1618,0,0.0850659,"edding methods exhibit select semantic properties. We propose a framework that generate triplets of sentences to explore how changes in the syntactic structure or semantics of a given sentence affect the similarities obtained between their sentence embeddings. 1 Gerard de Melo Rutgers University Piscataway, NJ, USA gdm@demelo.org Introduction Neural vector representations have become ubiquitous in all subfields of natural language processing. For the case of word vectors, important properties of the representations have been studied, including their linear substructures (Mikolov et al., 2013; Levy and Goldberg, 2014), the linear superposition of word senses (Arora et al., 2016b), and the nexus to pointwise mutual information scores between co-occurring words (Arora et al., 2016a). However, thus far, only little is known about the properties of sentence embeddings. Sentence embedding methods attempt to encode a variablelength input sentence into a fixed length vector. A number of such sentence embedding methods have been proposed in recent years (Le and Mikolov, 2014; Kiros et al., 2015; Wieting et al., 2015; Conneau et al., 2017; Arora et al., 2017). Sentence embeddings have mainly been evaluated in terms"
P19-1192,N18-2011,0,0.355763,"Missing"
P19-1192,D16-1126,0,0.14845,"eration in news systems. At the same time, poetry generation is of growing interest and has attained high levels of quality for classical Chinese poetry. Previously, Chinese poem composing research mainly focused on traditional Chinese poems. In light of the mostly short sentences and the metrical constraints of traditional Chinese poems, the majority of research attention focused on term selection to improve the thematic consistency (Wang et al., 2016). In contrast, modern Chinese poetry is more flexible and rich in rhetoric. Unlike sentimentcontrolled or topic-based text generation methods (Ghazvininejad et al., 2016), which have been widely used in poetry generation, existing research has largely disregarded the importance of rhetoric in poetry generation. Yet, to emulate humanwritten modern Chinese poems, it appears necessary to consider not only the topics but also the form of expression, especially with regard to rhetoric. In this paper, we propose a novel rhetorically controlled encoder-decoder framework inspired by the above sentiment-controlled and topic-based text generation methods, which can effectively generate poetry with metaphor and personification. Overall, the contributions of the paper are"
P19-1192,P17-4008,0,0.0599481,"ion, which play an essential role in enhancing the aesthetics of poetry. • We conduct extensive experiments showing that our model outperforms the state-of-theart both in automated and human evaluations. 2 2.1 Related Work Poetry Generation Poetry generation is a challenging task in NLP. Traditional methods (Gerv´as, 2001; Manurung, 2004; Greene et al., 2010; He et al., 2012) relied on grammar templates and custom semantic diagrams. In recent years, deep learning-driven methods have shown significant success in poetry generation, and topic-based poetry generation systems have been introduced (Ghazvininejad et al., 2017, 2018; Yi et al., 2018b). In particular, Zhang and Lapata (2014) propose to generate Chinese quatrains with Recurrent Neural Networks (RNNs), while Wang et al. (2016) obtain improved results by relying on a planning model for Chinese poetry generation. Recently, Memory Networks (Sukhbaatar et al., 2015) and Neural Turing Machines (Graves et al., 2014) have proven successful at certain tasks. The most relevant work for poetry generation is that of Zhang et al. (2017), which stores hundreds of human-authored poems in a static external memory to improve the generated quatrains and achieve a styl"
P19-1192,D10-1051,0,0.0828681,"methods, which can effectively generate poetry with metaphor and personification. Overall, the contributions of the paper are as follows: • We present the first work to generate modern Chinese poetry while controlling for the use of metaphor and personification, which play an essential role in enhancing the aesthetics of poetry. • We conduct extensive experiments showing that our model outperforms the state-of-theart both in automated and human evaluations. 2 2.1 Related Work Poetry Generation Poetry generation is a challenging task in NLP. Traditional methods (Gerv´as, 2001; Manurung, 2004; Greene et al., 2010; He et al., 2012) relied on grammar templates and custom semantic diagrams. In recent years, deep learning-driven methods have shown significant success in poetry generation, and topic-based poetry generation systems have been introduced (Ghazvininejad et al., 2017, 2018; Yi et al., 2018b). In particular, Zhang and Lapata (2014) propose to generate Chinese quatrains with Recurrent Neural Networks (RNNs), while Wang et al. (2016) obtain improved results by relying on a planning model for Chinese poetry generation. Recently, Memory Networks (Sukhbaatar et al., 2015) and Neural Turing Machines ("
P19-1192,P18-1139,0,0.0205024,"e a style transfer. The above models rely on an external memory to hold training data (i.e., external poems and articles). In contrast, Yi et al. (2018a) dynamically invoke a memory component by saving the writing history into memory. 2.2 Stylistic Language Generation The ability to produce diverse sentences in different styles under the same topics is an important characteristic of human writing. Some works have explored style control mechanisms for text generation tasks. For example, Zhou and Wang (2018) use naturally labeled emojis for large-scale emotional response generation in dialogue. Ke et al. (2018) and Wang et al. (2018) propose a sentence controlling function to generate interrogative, imperative, or declarative responses in dialogue. For the task of poetry generation, Yang et al. (2018) introduce an unsupervised style labeling to generate stylistic poetry, based on mutual information. Inspired by the above works, we regard rhetoric in 1993 poetry as a specific style and adopt a Conditional Variational Autoencoder (CVAE) model to generate rhetoric-aware poems. CVAEs (Sohn et al., 2015; Larsen et al., 2016) extend the traditional VAE model (Kingma and Welling, 2014) with an additional c"
P19-1192,N16-1014,0,0.060762,"results of human evaluation. F means Fluency. C stands for Coherence. M represents Meaningfulness while RA represents Rhetorical Aesthetics. how well the models fit the data. The RhetoricF1 score is used to measure the rhetorically controlled accuracy of the generated poem sentences. Specifically, if the rhetoric label of the generated sentence is consistent with the ground truth, the generated result is right, and wrong otherwise. The rhetoric label of each poem sentence is predicted by our rhetoric classifier mentioned above (see 4.1 for details about this classifier). Distinct1/Distinct-2 (Li et al., 2016) is used to evaluate the diversity of the generated poems. Human Evaluation. Following previous work (Yi et al., 2018b), we consider four criteria for human evaluation: • Fluency: Whether the generated poem is grammatically correct and fluent. • Coherence: Whether the generated poem is coherent with the topics and contexts. • Meaningfulness: Whether the generated poem contains meaningful information. • Rhetorical Aesthetics: Whether the generated rhetorical poem has some poetic and artistic beauty. Each criterion is scored on a 5-point scale ranging from 1 to 5. To build a test set for human e"
P19-1192,D18-1423,0,0.0467974,"Missing"
P19-1192,P18-1204,0,0.0279615,"he above models rely on an external memory to hold training data (i.e., external poems and articles). In contrast, Yi et al. (2018a) dynamically invoke a memory component by saving the writing history into memory. 2.2 Stylistic Language Generation The ability to produce diverse sentences in different styles under the same topics is an important characteristic of human writing. Some works have explored style control mechanisms for text generation tasks. For example, Zhou and Wang (2018) use naturally labeled emojis for large-scale emotional response generation in dialogue. Ke et al. (2018) and Wang et al. (2018) propose a sentence controlling function to generate interrogative, imperative, or declarative responses in dialogue. For the task of poetry generation, Yang et al. (2018) introduce an unsupervised style labeling to generate stylistic poetry, based on mutual information. Inspired by the above works, we regard rhetoric in 1993 poetry as a specific style and adopt a Conditional Variational Autoencoder (CVAE) model to generate rhetoric-aware poems. CVAEs (Sohn et al., 2015; Larsen et al., 2016) extend the traditional VAE model (Kingma and Welling, 2014) with an additional conditioned label to gui"
P19-1192,C16-1100,0,0.0828726,"ral language processing (NLP), e.g., for response generation in dialogue (Le et al., 2018), answer or question generation in question answering, and headline generation in news systems. At the same time, poetry generation is of growing interest and has attained high levels of quality for classical Chinese poetry. Previously, Chinese poem composing research mainly focused on traditional Chinese poems. In light of the mostly short sentences and the metrical constraints of traditional Chinese poems, the majority of research attention focused on term selection to improve the thematic consistency (Wang et al., 2016). In contrast, modern Chinese poetry is more flexible and rich in rhetoric. Unlike sentimentcontrolled or topic-based text generation methods (Ghazvininejad et al., 2016), which have been widely used in poetry generation, existing research has largely disregarded the importance of rhetoric in poetry generation. Yet, to emulate humanwritten modern Chinese poems, it appears necessary to consider not only the topics but also the form of expression, especially with regard to rhetoric. In this paper, we propose a novel rhetorically controlled encoder-decoder framework inspired by the above sentimen"
P19-1192,D18-1430,0,0.0819122,"by saving the writing history into memory. 2.2 Stylistic Language Generation The ability to produce diverse sentences in different styles under the same topics is an important characteristic of human writing. Some works have explored style control mechanisms for text generation tasks. For example, Zhou and Wang (2018) use naturally labeled emojis for large-scale emotional response generation in dialogue. Ke et al. (2018) and Wang et al. (2018) propose a sentence controlling function to generate interrogative, imperative, or declarative responses in dialogue. For the task of poetry generation, Yang et al. (2018) introduce an unsupervised style labeling to generate stylistic poetry, based on mutual information. Inspired by the above works, we regard rhetoric in 1993 poetry as a specific style and adopt a Conditional Variational Autoencoder (CVAE) model to generate rhetoric-aware poems. CVAEs (Sohn et al., 2015; Larsen et al., 2016) extend the traditional VAE model (Kingma and Welling, 2014) with an additional conditioned label to guide the generation process. Whereas VAEs essentially directly store latent attributes as probability distributions, CVAEs model latent variables conditioned on random varia"
P19-1192,N16-1174,0,0.0364431,"song lyrics from a small set of online music websites. The sentence rhetoric label is required for our model training. To this end, we built a classifier to predict the rhetoric label automatically. We sampled about 15,000 sentences from the original poetry dataset and annotated the data manually with three categories, i.e., metaphor, personification, and other. This dataset was divided into a training set, validation set, and test set. Three classifiers, including LSTM, Bi-LSTM, and Bi-LSTM with a self-attention model, were trained on this dataset. The Bi-LSTM with self-attention classifier (Yang et al., 2016) outperforms the other models and achieves the best accuracy of 0.83 on the 1 2 https://github.com/Lucien-qiang/Rhetoric-Generator http://www.shigeku.com/ Models for Comparisons • HRED: A hierarchical encoder-decoder model for text generation (Serban et al., 2016), which employs a hierarchical RNN to model the sentences at both the sentence level and the context level. • WM: A recent Working Memory model for poetry generation (Yi et al., 2018b). • CVAE: A standard CVAE model without the specific decoder. We adopt the same architecture as that introduced in Zhao et al. (2017). 4.3 Evaluation De"
P19-1192,K18-1024,0,0.198924,"in enhancing the aesthetics of poetry. • We conduct extensive experiments showing that our model outperforms the state-of-theart both in automated and human evaluations. 2 2.1 Related Work Poetry Generation Poetry generation is a challenging task in NLP. Traditional methods (Gerv´as, 2001; Manurung, 2004; Greene et al., 2010; He et al., 2012) relied on grammar templates and custom semantic diagrams. In recent years, deep learning-driven methods have shown significant success in poetry generation, and topic-based poetry generation systems have been introduced (Ghazvininejad et al., 2017, 2018; Yi et al., 2018b). In particular, Zhang and Lapata (2014) propose to generate Chinese quatrains with Recurrent Neural Networks (RNNs), while Wang et al. (2016) obtain improved results by relying on a planning model for Chinese poetry generation. Recently, Memory Networks (Sukhbaatar et al., 2015) and Neural Turing Machines (Graves et al., 2014) have proven successful at certain tasks. The most relevant work for poetry generation is that of Zhang et al. (2017), which stores hundreds of human-authored poems in a static external memory to improve the generated quatrains and achieve a style transfer. The above m"
P19-1192,P17-1125,0,0.014952,"hods have shown significant success in poetry generation, and topic-based poetry generation systems have been introduced (Ghazvininejad et al., 2017, 2018; Yi et al., 2018b). In particular, Zhang and Lapata (2014) propose to generate Chinese quatrains with Recurrent Neural Networks (RNNs), while Wang et al. (2016) obtain improved results by relying on a planning model for Chinese poetry generation. Recently, Memory Networks (Sukhbaatar et al., 2015) and Neural Turing Machines (Graves et al., 2014) have proven successful at certain tasks. The most relevant work for poetry generation is that of Zhang et al. (2017), which stores hundreds of human-authored poems in a static external memory to improve the generated quatrains and achieve a style transfer. The above models rely on an external memory to hold training data (i.e., external poems and articles). In contrast, Yi et al. (2018a) dynamically invoke a memory component by saving the writing history into memory. 2.2 Stylistic Language Generation The ability to produce diverse sentences in different styles under the same topics is an important characteristic of human writing. Some works have explored style control mechanisms for text generation tasks. F"
P19-1192,D14-1074,0,0.165013,"try. • We conduct extensive experiments showing that our model outperforms the state-of-theart both in automated and human evaluations. 2 2.1 Related Work Poetry Generation Poetry generation is a challenging task in NLP. Traditional methods (Gerv´as, 2001; Manurung, 2004; Greene et al., 2010; He et al., 2012) relied on grammar templates and custom semantic diagrams. In recent years, deep learning-driven methods have shown significant success in poetry generation, and topic-based poetry generation systems have been introduced (Ghazvininejad et al., 2017, 2018; Yi et al., 2018b). In particular, Zhang and Lapata (2014) propose to generate Chinese quatrains with Recurrent Neural Networks (RNNs), while Wang et al. (2016) obtain improved results by relying on a planning model for Chinese poetry generation. Recently, Memory Networks (Sukhbaatar et al., 2015) and Neural Turing Machines (Graves et al., 2014) have proven successful at certain tasks. The most relevant work for poetry generation is that of Zhang et al. (2017), which stores hundreds of human-authored poems in a static external memory to improve the generated quatrains and achieve a style transfer. The above models rely on an external memory to hold t"
P19-1192,P17-1061,0,0.0451678,"ion network. Usually, we assume that both the prior and the recognition networks are multivariate Gaussian distributions, and their mean and log variance are estimated through multilayer perceptrons (MLP) as follows:   µ, σ 2 = MLPposterior (LSTM(Y ), c) h 0 0 i (6) µ , σ 2 = MLPprior (c) A single layer of the LSTM is used to encode the current lines, and obtain the hX component of c. The same LSTM structure is also used to encode the next line Y in the training stage. By using Eq. (6), we calculate the KL divergence between these distributions to optimize Eq. (5). Following the practice in Zhao et al. (2017), a reparameterization technique is used when sampling from the recognition and the prior network during training and testing. 3.3.2 Automatic Control(AC) CVAE In the ACCVAE model, we first predict the rhetorical mode of the next sentence using an MLP that is designed as follows: p(r|hX ) = softmax(MLPpredictor (hX )) r = arg max p(r |hX ) (7) In this case, the conditional variable c is also [hX ; e(r)], where hX is taken as the last hidden state of the encoder LSTM. The loss function is then defined as: 1995 L = LKL + LdecoderCE + LpredictorCE (8) In this paper, a two-layer MLP is used for Eq"
P19-1192,P18-1104,0,0.0247288,"es hundreds of human-authored poems in a static external memory to improve the generated quatrains and achieve a style transfer. The above models rely on an external memory to hold training data (i.e., external poems and articles). In contrast, Yi et al. (2018a) dynamically invoke a memory component by saving the writing history into memory. 2.2 Stylistic Language Generation The ability to produce diverse sentences in different styles under the same topics is an important characteristic of human writing. Some works have explored style control mechanisms for text generation tasks. For example, Zhou and Wang (2018) use naturally labeled emojis for large-scale emotional response generation in dialogue. Ke et al. (2018) and Wang et al. (2018) propose a sentence controlling function to generate interrogative, imperative, or declarative responses in dialogue. For the task of poetry generation, Yang et al. (2018) introduce an unsupervised style labeling to generate stylistic poetry, based on mutual information. Inspired by the above works, we regard rhetoric in 1993 poetry as a specific style and adopt a Conditional Variational Autoencoder (CVAE) model to generate rhetoric-aware poems. CVAEs (Sohn et al., 20"
Q13-1023,P11-1070,1,0.835243,"ams corpus (Brants and Franz, 2006), which contains English n-grams (n = 1 to 5) and their observed frequency counts, generated from nearly 1 trillion word tokens and 95 billion sentences. We consider each pair of words (a1 , a2 ) in the input set in turn. For each pattern p in the two pattern sets (weak-strong Pws and strong-weak Psw ), we insert the word pair into the pattern as p(a1 , a2 ) to get a phrasal query like “big but not huge”. This is done by replacing the two wildcards in the pattern by the two words in order. Finally, we scan the Web ngrams corpus in a batch approach similar to Bansal and Klein (2011) and collect frequencies of all our phrase queries. Table 2 depicts some examples of useful intensity-based phrase queries and their frequencies in the Web-scale corpus. We also collect frequencies for the input word unigrams and the patterns for normalization purposes. Given a word pair (a1 , a2 ) and a corpus count function cnt, we define W1 = 1 X cnt(p1 (a1 , a2 )) P1 p1 ∈Pws 1 X S1 = cnt(p2 (a1 , a2 )) P2 p2 ∈Psw W2 = 1 X cnt(p1 (a2 , a1 )) P1 p1 ∈Pws 1 X cnt(p2 (a2 , a1 )) S2 = P2 (1) p2 ∈Psw with P1 = X cnt(p1 ) p1 ∈Pws P2 = X cnt(p2 ), (2) p2 ∈Psw 281 such that the final overall weak-st"
Q13-1023,W04-3205,0,0.0732836,"pattern collection). Many of these patterns also apply to other parts of speech (e.g. ‘drizzle but not rain’, ‘running or even sprinting’), with significant discrimination on the Web in the right direction. 2.1.3 Pairwise Scores Given an input set of words to be placed on a scale, we first collect evidence of their intensity order by using the above-mentioned intensity patterns and a large, Web-scale text corpus. Previous work on information extraction from limited-sized raw text corpora revealed that coverage is often limited (Hearst, 1992; Hatzivassiloglou and McKeown, 1993). Some studies (Chklovski and Pantel, 2004; Sheinman and Tokunaga, 2009) used hit counts from an online search engine, but this is unstable and irreproducible (Kilgarriff, 2007). To avoid these issues, we use the largest available (good, great) good , but not great → 24492.0 good , if not great → 1912.0 good , though not great → 504.0 good , or even great → 338.0 not just good but great → 181.0 good , almost great → 156.0 (great, good) not great , just good → 248.0 great or very good → 89.0 not great but still good → 47.0 (small, minute) small , almost minute → 97.0 small , even minute → 41.0 Table 2: Some examples from the Web-scale"
Q13-1023,P08-1079,0,0.0125031,". In our experiments, we reimplemented this approach and show that our MILP method improves over it by allowing individual pairwise decisions to benefit more from global information. Schulam and Fellbaum (2010) apply the approach of Sheinman and Tokunaga (2009) to German adjectives. Our method extends easily to various foreign languages as described in Section 5. Another related task is the extraction of lexicosyntactic and lexico-semantic intensity-order patterns from large text corpora (Hearst, 1992; Chklovski and Pantel, 2004; Tandon and de Melo, 2010). Sheinman and Tokunaga (2009) follows Davidov and Rappoport (2008) to automatically bootstrap adjective scaling patterns using seed adjectives and Web hits. These methods thus can be used to provide the input patterns for our algorithm. VerbOcean by Chklovski and Pantel (2004) extracts various fine-grained semantic relations (including the stronger-than relation) between pairs of verbs, using lexico-syntactic patterns over the Web. 4 Experiments 4.1 Data Input Clusters In order to obtain input clusters for evaluation, we started out with the satellite cluster or ‘dumbbell’ structure of adjectives in WordNet 3.0, which consists of two direct antonyms as the p"
Q13-1023,P10-1018,0,0.204588,"Missing"
Q13-1023,P93-1023,0,0.835131,"ection 4.1 for implementation details regarding our pattern collection). Many of these patterns also apply to other parts of speech (e.g. ‘drizzle but not rain’, ‘running or even sprinting’), with significant discrimination on the Web in the right direction. 2.1.3 Pairwise Scores Given an input set of words to be placed on a scale, we first collect evidence of their intensity order by using the above-mentioned intensity patterns and a large, Web-scale text corpus. Previous work on information extraction from limited-sized raw text corpora revealed that coverage is often limited (Hearst, 1992; Hatzivassiloglou and McKeown, 1993). Some studies (Chklovski and Pantel, 2004; Sheinman and Tokunaga, 2009) used hit counts from an online search engine, but this is unstable and irreproducible (Kilgarriff, 2007). To avoid these issues, we use the largest available (good, great) good , but not great → 24492.0 good , if not great → 1912.0 good , though not great → 504.0 good , or even great → 338.0 not just good but great → 181.0 good , almost great → 156.0 (great, good) not great , just good → 248.0 great or very good → 89.0 not great but still good → 47.0 (small, minute) small , almost minute → 97.0 small , even minute → 41.0"
Q13-1023,P97-1023,0,0.752309,"et of words using pairwise evidence is also applicable to the VerbOcean pairs, and should help address similar sparsity issues of local pairwise decisions. Such scales will again be quite useful for language learners and language understanding tools. de Marneffe et al. (2010) infer yes-or-no answers to questions with responses involving scalar adjectives in a dialogue corpus. They correlate adjectives with ratings in a movie review corpus to find that good appears in lower-rated reviews than excellent. Finally, there has been a lot of work on measuring the general sentiment polarity of words (Hatzivassiloglou and McKeown, 1997; Hatzivassiloglou and Wiebe, 2000; Turney and Littman, 2003; Liu and Seneff, 2009; Taboada et al., 2011; Yessenalina and Cardie, 2011; Pang and Lee, 2008). Our work instead aims at producing a large, unrestricted number of individual intensity scales for different qualities and hence can help in fine-grained sentiment analysis with respect to very particular content aspects. 40 41 27 30 20 12 10 3 3 2 7 8 0 3 4 5 6 Length of chain Figure 4: The histogram of cluster sizes in the test set. rected edge between each pair of adjectives that has a non-zero intensity score (based on the Web-scale sc"
Q13-1023,C00-1044,0,0.111279,"Missing"
Q13-1023,C92-2082,0,0.0511667,"ork (and see Section 4.1 for implementation details regarding our pattern collection). Many of these patterns also apply to other parts of speech (e.g. ‘drizzle but not rain’, ‘running or even sprinting’), with significant discrimination on the Web in the right direction. 2.1.3 Pairwise Scores Given an input set of words to be placed on a scale, we first collect evidence of their intensity order by using the above-mentioned intensity patterns and a large, Web-scale text corpus. Previous work on information extraction from limited-sized raw text corpora revealed that coverage is often limited (Hearst, 1992; Hatzivassiloglou and McKeown, 1993). Some studies (Chklovski and Pantel, 2004; Sheinman and Tokunaga, 2009) used hit counts from an online search engine, but this is unstable and irreproducible (Kilgarriff, 2007). To avoid these issues, we use the largest available (good, great) good , but not great → 24492.0 good , if not great → 1912.0 good , though not great → 504.0 good , or even great → 338.0 not just good but great → 181.0 good , almost great → 156.0 (great, good) not great , just good → 248.0 great or very good → 89.0 not great but still good → 47.0 (small, minute) small , almost minu"
Q13-1023,J06-2003,0,0.012551,"se two words are attested together on the Web with respect to the intensity patterns more than with other candidate words. Therefore, we try to respect the order of such word pairs more in the final ranking when we are breaking constraint-violating cycles. 3 Related Work Hatzivassiloglou and McKeown (1993) presented the first step towards automatic identification of adjective scales, thoroughly discussing the background of adjective semantics and a means of discovering clusters of adjectives that belong on the same scale, thus providing one way of creating the input for our ranking algorithm. Inkpen and Hirst (2006) study near-synonyms and nuances of meaning differentiation (such as stylistic, 283 attitudinal, etc.). They attempt to automatically acquire a knowledge base of near-synonym differences via an unsupervised decision-list algorithm. However, their method depends on a special dictionary of synonym differences to learn the extraction patterns, while we use only a raw Web-scale corpus. Mohammad et al. (2013) proposed a method of identifying whether two adjectives are antonymous. This problem is related but distinct, because the degree of antonymy does not necessarily determine their position on an"
Q13-1023,D09-1017,0,0.0273154,"ddress similar sparsity issues of local pairwise decisions. Such scales will again be quite useful for language learners and language understanding tools. de Marneffe et al. (2010) infer yes-or-no answers to questions with responses involving scalar adjectives in a dialogue corpus. They correlate adjectives with ratings in a movie review corpus to find that good appears in lower-rated reviews than excellent. Finally, there has been a lot of work on measuring the general sentiment polarity of words (Hatzivassiloglou and McKeown, 1997; Hatzivassiloglou and Wiebe, 2000; Turney and Littman, 2003; Liu and Seneff, 2009; Taboada et al., 2011; Yessenalina and Cardie, 2011; Pang and Lee, 2008). Our work instead aims at producing a large, unrestricted number of individual intensity scales for different qualities and hence can help in fine-grained sentiment analysis with respect to very particular content aspects. 40 41 27 30 20 12 10 3 3 2 7 8 0 3 4 5 6 Length of chain Figure 4: The histogram of cluster sizes in the test set. rected edge between each pair of adjectives that has a non-zero intensity score (based on the Web-scale scoring procedure described in Section 2.1.3). The resulting graph is then partition"
Q13-1023,P06-1101,0,0.02426,"attested together on the Web with respect to the intensity patterns more than with other candidate words. Therefore, we try to respect the order of such word pairs more in the final ranking when we are breaking constraint-violating cycles. 3 Related Work Hatzivassiloglou and McKeown (1993) presented the first step towards automatic identification of adjective scales, thoroughly discussing the background of adjective semantics and a means of discovering clusters of adjectives that belong on the same scale, thus providing one way of creating the input for our ranking algorithm. Inkpen and Hirst (2006) study near-synonyms and nuances of meaning differentiation (such as stylistic, 283 attitudinal, etc.). They attempt to automatically acquire a knowledge base of near-synonym differences via an unsupervised decision-list algorithm. However, their method depends on a special dictionary of synonym differences to learn the extraction patterns, while we use only a raw Web-scale corpus. Mohammad et al. (2013) proposed a method of identifying whether two adjectives are antonymous. This problem is related but distinct, because the degree of antonymy does not necessarily determine their position on an"
Q13-1023,J11-2001,0,0.0785274,"y issues of local pairwise decisions. Such scales will again be quite useful for language learners and language understanding tools. de Marneffe et al. (2010) infer yes-or-no answers to questions with responses involving scalar adjectives in a dialogue corpus. They correlate adjectives with ratings in a movie review corpus to find that good appears in lower-rated reviews than excellent. Finally, there has been a lot of work on measuring the general sentiment polarity of words (Hatzivassiloglou and McKeown, 1997; Hatzivassiloglou and Wiebe, 2000; Turney and Littman, 2003; Liu and Seneff, 2009; Taboada et al., 2011; Yessenalina and Cardie, 2011; Pang and Lee, 2008). Our work instead aims at producing a large, unrestricted number of individual intensity scales for different qualities and hence can help in fine-grained sentiment analysis with respect to very particular content aspects. 40 41 27 30 20 12 10 3 3 2 7 8 0 3 4 5 6 Length of chain Figure 4: The histogram of cluster sizes in the test set. rected edge between each pair of adjectives that has a non-zero intensity score (based on the Web-scale scoring procedure described in Section 2.1.3). The resulting graph is then partitioned into connected comp"
Q13-1023,C08-1114,0,0.0522616,"Missing"
Q13-1023,P07-1067,0,0.0763955,"Missing"
Q13-1023,D11-1016,0,0.0252742,"wise decisions. Such scales will again be quite useful for language learners and language understanding tools. de Marneffe et al. (2010) infer yes-or-no answers to questions with responses involving scalar adjectives in a dialogue corpus. They correlate adjectives with ratings in a movie review corpus to find that good appears in lower-rated reviews than excellent. Finally, there has been a lot of work on measuring the general sentiment polarity of words (Hatzivassiloglou and McKeown, 1997; Hatzivassiloglou and Wiebe, 2000; Turney and Littman, 2003; Liu and Seneff, 2009; Taboada et al., 2011; Yessenalina and Cardie, 2011; Pang and Lee, 2008). Our work instead aims at producing a large, unrestricted number of individual intensity scales for different qualities and hence can help in fine-grained sentiment analysis with respect to very particular content aspects. 40 41 27 30 20 12 10 3 3 2 7 8 0 3 4 5 6 Length of chain Figure 4: The histogram of cluster sizes in the test set. rected edge between each pair of adjectives that has a non-zero intensity score (based on the Web-scale scoring procedure described in Section 2.1.3). The resulting graph is then partitioned into connected components such that any adjective"
Q13-1023,J13-3004,0,\N,Missing
Q16-1004,P12-1042,0,0.0104454,"ive-words; and (4) automatically detects differences in perspectiveword distributions in the two languages. We perform a behavioural evaluation of a subset of the differences identified by the model and demonstrate their psychological validity. Our data and dictionaries are available from the first author upon request. 2 Related work View detection. Identifying different viewpoints is related to the well-studied area of subjectivity detection, which aims at exposing opinion, evaluation, and speculation in text (Wiebe et al., 2004) and attributing it to specific people (Awadallah et al., 2011; Abu-Jbara et al., 2012). In our work, we are less interested in explicit local forms of subjectivity, instead aiming at detecting more general contrasts 48 across linguistic communities. Another line of research has focused on inferring author attributes such as gender, age (Garera and Yarowsky, 2009), location (Jones et al., 2007), or political affiliation (Pennacchiotti and Popescu, 2011). Such studies make use of syntactic style, discourse characteristics, as well as lexical choice. The models used for this are typically binary classifiers trained in a fully supervised fashion. In contrast, in our task, we automa"
Q16-1004,D10-1111,0,0.0236667,"detection. LDA also assumes that the distribution of each topic is fixed across all documents in a corpus. Therefore, a topic associated with, e.g., war will have the same distribution over the lexicon regardless of whether the document was taken from a pro-war editorial or an anti-war speech. However, in reality we may expect a single topic to exhibit systematic and predictable variations in its distribution based on authorship. The cross-collection LDA model by Paul and Girju (2009) addresses this by specifically aiming to expose viewpoint differences across different document collections. Ahmed and Xing (2010) proposed a similar model for detecting ideological differences. Fang et al. (2012)’s Cross-Perspective Topic (CPT) model breaks up the terms in the vocabulary into topic terms and perspective terms with different generative processes, and differentiates between different collections of documents within the corpus. The topic terms are assumed to be generated as in LDA. However, the distribution of perspective terms in a document is taken to be dependent on both the topic mixture of the document as well as the collection from which the document is drawn. Recent works proposed models for specifi"
Q16-1004,D10-1005,0,0.00900521,"tion (Jones et al., 2007), or political affiliation (Pennacchiotti and Popescu, 2011). Such studies make use of syntactic style, discourse characteristics, as well as lexical choice. The models used for this are typically binary classifiers trained in a fully supervised fashion. In contrast, in our task, we automatically infer the topic distributions and find topic-specific contrasts. Probabilistic topic models. Probabilistic topic models have proven useful for a variety of semantic tasks, such as selectional-preference induction ´ S´eaghdha, 2010; Ritter et al., 2010), sentiment (O analysis (Boyd-Graber and Resnik, 2010) and studying the evolution of concepts and ideas (Hall et al., 2008). The goal of a topic model is to characterize observed data in terms of a much smaller set of unobserved, semantically coherent topics. A particularly popular probabilistic topic model is Latent Dirichlet Allocation (LDA) (Blei et al., 2003). Under its assumptions, each document has a unique mix of topics, and each topic is a distribution over terms in the vocabulary. A topic is chosen for every word token according to the topic mix of the document to which it belongs, and then the word’s identity is drawn from the correspon"
Q16-1004,D07-1069,0,0.142629,"ing methods to date focus on the analysis of monolingual texts. In contrast, we present a statistical model that simultaneously learns a set of common topics from multilingual, non-parallel data and automatically discovers the differences in perspectives on these topics across linguistic communities. We perform a behavioural evaluation of a subset of the differences identified by our model in English and Spanish to investigate their psychological validity. 1 Introduction Recent years have seen a growing interest in textmining applications aimed at uncovering public opinions and social trends (Fader et al., 2007; Monroe et al., 2008; Gerrish and Blei, 2011; Pennacchiotti and Popescu, 2011). They rest on the assumption that the language we use is indicative of our underlying worldviews. Research in cognitive and sociolinguistics suggests that linguistic variation across communities systematically reflects differences in their cultural and moral models and goes beyond lexicon and grammar (K¨ovecses, 2004; Lakoff and Wehling, 2012). Cross-cultural differences manifest themselves in text in a multitude of ways, most prominently through the use of explicit opinion vocabulary with respect to a certain topi"
Q16-1004,P09-1080,0,0.0190815,"vailable from the first author upon request. 2 Related work View detection. Identifying different viewpoints is related to the well-studied area of subjectivity detection, which aims at exposing opinion, evaluation, and speculation in text (Wiebe et al., 2004) and attributing it to specific people (Awadallah et al., 2011; Abu-Jbara et al., 2012). In our work, we are less interested in explicit local forms of subjectivity, instead aiming at detecting more general contrasts 48 across linguistic communities. Another line of research has focused on inferring author attributes such as gender, age (Garera and Yarowsky, 2009), location (Jones et al., 2007), or political affiliation (Pennacchiotti and Popescu, 2011). Such studies make use of syntactic style, discourse characteristics, as well as lexical choice. The models used for this are typically binary classifiers trained in a fully supervised fashion. In contrast, in our task, we automatically infer the topic distributions and find topic-specific contrasts. Probabilistic topic models. Probabilistic topic models have proven useful for a variety of semantic tasks, such as selectional-preference induction ´ S´eaghdha, 2010; Ritter et al., 2010), sentiment (O anal"
Q16-1004,D13-1191,0,0.0141878,"pic (CPT) model breaks up the terms in the vocabulary into topic terms and perspective terms with different generative processes, and differentiates between different collections of documents within the corpus. The topic terms are assumed to be generated as in LDA. However, the distribution of perspective terms in a document is taken to be dependent on both the topic mixture of the document as well as the collection from which the document is drawn. Recent works proposed models for specific types of data. Qiu and Jiang (2013) use user identities and interactions in threaded discussions, while Gottipati et al. (2013) developed a topic model for Debatepedia, a semi-structured resource in which arguments are explicitly enumerated. However, all of these models perform their analyses on monolingual datasets. Thus, they are useful for comparing different ideologies expressed in the same language, but not for cross-linguistic comparisons. 3 Method The goal of our model is to analyse large, nonparallel, multilingual corpora and present crosslingually valid topics and the associated perspectives, automatically inferring the differences in conceptualization of these topics across cultures. Following Boyd-Graber an"
Q16-1004,P08-1088,0,0.099971,"signed for monolingual text and thus it lacks the structure necessary to model cross-lingually valid topics. While topic models can be trained individually on two languages and then the acquired topics can be matched, the correspondences between the topics for the two terms will be highly unstable. To address this, Boyd-Graber and Blei (2009) (M U T O) and Jagarlamudi and Daum´e III (2010) (J OINT LDA) introduced the notion of crosslingually valid concepts associated with different terms in different languages, using bilingual dictionaries to model topics across languages. Based on a model by Haghighi et al. (2008), M U T O is capable of learning translations–i.e., matching between terms in the different languages being compared. The Polylingual Topic Model of Mimno et al. (2009) is another approach to finding topics in multilingual corpora, but it requires tuples composed of comparable documents in each language of the corpus. Topic models for view detection. LDA also assumes that the distribution of each topic is fixed across all documents in a corpus. Therefore, a topic associated with, e.g., war will have the same distribution over the lexicon regardless of whether the document was taken from a pro-"
Q16-1004,D08-1038,0,0.0125207,"2011). Such studies make use of syntactic style, discourse characteristics, as well as lexical choice. The models used for this are typically binary classifiers trained in a fully supervised fashion. In contrast, in our task, we automatically infer the topic distributions and find topic-specific contrasts. Probabilistic topic models. Probabilistic topic models have proven useful for a variety of semantic tasks, such as selectional-preference induction ´ S´eaghdha, 2010; Ritter et al., 2010), sentiment (O analysis (Boyd-Graber and Resnik, 2010) and studying the evolution of concepts and ideas (Hall et al., 2008). The goal of a topic model is to characterize observed data in terms of a much smaller set of unobserved, semantically coherent topics. A particularly popular probabilistic topic model is Latent Dirichlet Allocation (LDA) (Blei et al., 2003). Under its assumptions, each document has a unique mix of topics, and each topic is a distribution over terms in the vocabulary. A topic is chosen for every word token according to the topic mix of the document to which it belongs, and then the word’s identity is drawn from the corresponding topic’s distribution. Handling multilingual corpora. LDA is desi"
Q16-1004,D09-1092,0,0.0387022,"Missing"
Q16-1004,D11-1024,0,0.399322,"rplexity for topic words wi and perspective-words oi separately. For perplexity is defined P topic words, the w as exp(− wi ∈H logp(wi )/N ). As for standard LDA, exact inference of p(wi ) is intractable under this model. Therefore we adapted the estimator developed by Murray and Salakhutdinov (2009) to our models. Coherence is a measure inspired by pointwise mutual information (Newman et al., 2010). Let D(v) be the the number of documents with at least one token of type v and let D(v, w) be the number of documents containing at least one token of type v and at least one token of type w. Then Mimno et al. (2011) define the coherence of topic k as 1 M 2  M m−1 X X (k) log m=2 `=1 (k) (k) D(vm , v` ) +  (k) D(v` ) , (k) where V (k) = (v1 , ..., vM ) is a list of the M most probable words in topic k and  is a small smoothing constant used to avoid taking the logarithm of zero. Mimno et al. (2011) find that coherence correlates better with human judgments than do likelihoodbased measures. Coherence is topic-specific measure, so for each model variant we trained, we computed the median topic coherence across all the topics learned by the model. We set  = 0.1. Model performance and analysis. Fig. 2 sho"
Q16-1004,N10-1012,0,0.0403447,"anguage pairs. Perplexity is a measure of how well a model trained on a training set predicts the co-occurrence of words on an unseen test set H. Lower perplexity indicates better model fit. We evaluate the held-out perplexity for topic words wi and perspective-words oi separately. For perplexity is defined P topic words, the w as exp(− wi ∈H logp(wi )/N ). As for standard LDA, exact inference of p(wi ) is intractable under this model. Therefore we adapted the estimator developed by Murray and Salakhutdinov (2009) to our models. Coherence is a measure inspired by pointwise mutual information (Newman et al., 2010). Let D(v) be the the number of documents with at least one token of type v and let D(v, w) be the number of documents containing at least one token of type v and at least one token of type w. Then Mimno et al. (2011) define the coherence of topic k as 1 M 2  M m−1 X X (k) log m=2 `=1 (k) (k) D(vm , v` ) +  (k) D(v` ) , (k) where V (k) = (v1 , ..., vM ) is a list of the M most probable words in topic k and  is a small smoothing constant used to avoid taking the logarithm of zero. Mimno et al. (2011) find that coherence correlates better with human judgments than do likelihoodbased measures."
Q16-1004,P10-1045,0,0.0152701,"Missing"
Q16-1004,D09-1146,0,0.0263618,"multilingual corpora, but it requires tuples composed of comparable documents in each language of the corpus. Topic models for view detection. LDA also assumes that the distribution of each topic is fixed across all documents in a corpus. Therefore, a topic associated with, e.g., war will have the same distribution over the lexicon regardless of whether the document was taken from a pro-war editorial or an anti-war speech. However, in reality we may expect a single topic to exhibit systematic and predictable variations in its distribution based on authorship. The cross-collection LDA model by Paul and Girju (2009) addresses this by specifically aiming to expose viewpoint differences across different document collections. Ahmed and Xing (2010) proposed a similar model for detecting ideological differences. Fang et al. (2012)’s Cross-Perspective Topic (CPT) model breaks up the terms in the vocabulary into topic terms and perspective terms with different generative processes, and differentiates between different collections of documents within the corpus. The topic terms are assumed to be generated as in LDA. However, the distribution of perspective terms in a document is taken to be dependent on both the"
Q16-1004,W10-0510,0,0.0188758,"Missing"
Q16-1004,N13-1123,0,0.0120277,"ar model for detecting ideological differences. Fang et al. (2012)’s Cross-Perspective Topic (CPT) model breaks up the terms in the vocabulary into topic terms and perspective terms with different generative processes, and differentiates between different collections of documents within the corpus. The topic terms are assumed to be generated as in LDA. However, the distribution of perspective terms in a document is taken to be dependent on both the topic mixture of the document as well as the collection from which the document is drawn. Recent works proposed models for specific types of data. Qiu and Jiang (2013) use user identities and interactions in threaded discussions, while Gottipati et al. (2013) developed a topic model for Debatepedia, a semi-structured resource in which arguments are explicitly enumerated. However, all of these models perform their analyses on monolingual datasets. Thus, they are useful for comparing different ideologies expressed in the same language, but not for cross-linguistic comparisons. 3 Method The goal of our model is to analyse large, nonparallel, multilingual corpora and present crosslingually valid topics and the associated perspectives, automatically inferring th"
Q16-1004,P10-1044,0,0.024912,"Missing"
Q16-1004,D12-1087,0,0.0108175,"different model variants for different numbers of iterations at K=175. median topic coherence. Once again, this general pattern held true for the English-Russian pair and T WITTER corpora. Overall, the results show that M ULTIPLE S TATIC I NCLUDE provides superior performance across measures, corpora, topic numbers, and languages. We therefore used this variant in further data analysis and evaluation. Incidentally, the observed decrease in topic coherence as K increases is expected, because as K increases, lowerlikelihood topics tend to be more incoherent (Mimno et al., 2011). Experiments by Stevens et al. (2012) show that this effect is observed for LDA-, NMF-, and SVD-based topic models. Cross-linguistic matchings. The matchings inferred by the S INGLE I NFER I NCLUDE variant were of mixed quality. Some of the matchings corrected low-quality translations in the original dictionary. For instance, our prior dictionary matched passage in English to pasaje in Spanish. Though technically correct, the dominant meaning of pasaje is [travel] ticket. The T WITTER model correctly matched passage to ruta instead. Many of the matchings learned by the model did not provide technically correct translations, yet w"
Q16-1004,J04-3002,0,0.0288438,"finds cross-lingual topics specified by distributions over topic-words and perspective-words; and (4) automatically detects differences in perspectiveword distributions in the two languages. We perform a behavioural evaluation of a subset of the differences identified by the model and demonstrate their psychological validity. Our data and dictionaries are available from the first author upon request. 2 Related work View detection. Identifying different viewpoints is related to the well-studied area of subjectivity detection, which aims at exposing opinion, evaluation, and speculation in text (Wiebe et al., 2004) and attributing it to specific people (Awadallah et al., 2011; Abu-Jbara et al., 2012). In our work, we are less interested in explicit local forms of subjectivity, instead aiming at detecting more general contrasts 48 across linguistic communities. Another line of research has focused on inferring author attributes such as gender, age (Garera and Yarowsky, 2009), location (Jones et al., 2007), or political affiliation (Pennacchiotti and Popescu, 2011). Such studies make use of syntactic style, discourse characteristics, as well as lexical choice. The models used for this are typically binary"
Q18-1013,W05-0909,0,0.0884064,"et al., 2017), which aims to generate captions in a target language based on both source language captions and images. Our model can produce captions in the target language either with or without source language cues. For the MSVD dataset, a small number of captions in other languages are available. We consider German (DE) and Chinese (CN). The latter is tokenized using the Stanford Word Segmenter (Chang et al., 2008). We consider the words in these other languages as external semantic attributes. 4.3 Evaluation Metrics We rely on three standard metrics, BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and CIDEr (Vedantam et al., 2015) to evaluate our methods. These are commonly used in image and video captioning tasks, and allow us to compare our results against previous work. We use the Microsoft COCO evaluation toolkit (Chen et al., 2015), which is widely used in previous work, to compute the met180 ric scores. Across all three metrics, higher scores indicate that the generated captions are assessed as being closer to captions authored by humans. 4.4 Experimental Settings Based on our hyperparameter optimization on the validation set, the number of hidden units in the first multi-facted"
Q18-1013,C18-1250,0,0.041149,"Missing"
Q18-1013,W08-0336,0,0.0323383,"Missing"
Q18-1013,P11-1020,0,0.0402685,"er the parameters are learned, during the testing phase, we also have temporal and motion features extracted from the video as well as semantic attributes, which were either already given or are min − predicted using a model trained on the training set. Given a previous word, we can calculate the probability distribution of the next word pt+1 using the model described above. Thus, we can generate captions starting from the special symbol hBOSi with Beam Search. 4 Experimental Results 4.1 Datasets MSVD: We evaluate our video captioning models on the Microsoft Research Video Description Corpus (Chen and Dolan, 2011). MSVD consists of 1,970 video clips downloaded from YouTube that typically depict a single activity. Each video clip is annotated with multiple human-written descriptions in several languages. We only use the English descriptions, about 41 descriptions per video. In total, the dataset consists of 80,839 video-description pairs. Each description on average contains about 8 words. We use 1,200 videos for training, 100 videos for validation, and 670 videos for testing, as provided by previous work (Guadarrama et al., 2013). MSR-VTT: We also evaluate on the MSR Videoto-Text (MSR-VTT) dataset (Xu"
Q18-1013,W14-4012,0,0.138605,"Missing"
Q18-1013,D14-1179,0,0.0335745,"Missing"
Q18-1013,W09-4407,1,0.644653,"Missing"
Q18-1013,W17-4718,0,0.0214191,"ores. where the summation is over both images and videos. We train this attribute classifier on both MSCOCO (Chen et al., 2015) and the training set of the video captioning dataset, predicting attributes on the validation and test set of the video captioning dataset. We refer to these attributes as predicted semantic attributes (SP). External Semantic Attributes: We also consider using external information such as cross-lingual cues (de Melo and Weikum, 2009) to generate video captions. This is related to multimodal machine translation and cross-lingual image description (Specia et al., 2016; Elliott et al., 2017), which aims to generate captions in a target language based on both source language captions and images. Our model can produce captions in the target language either with or without source language cues. For the MSVD dataset, a small number of captions in other languages are available. We consider German (DE) and Chinese (CN). The latter is tokenized using the Stanford Word Segmenter (Chang et al., 2008). We consider the words in these other languages as external semantic attributes. 4.3 Evaluation Metrics We rely on three standard metrics, BLEU (Papineni et al., 2002), METEOR (Banerjee and L"
Q18-1013,P02-1040,0,0.100568,"on (Specia et al., 2016; Elliott et al., 2017), which aims to generate captions in a target language based on both source language captions and images. Our model can produce captions in the target language either with or without source language cues. For the MSVD dataset, a small number of captions in other languages are available. We consider German (DE) and Chinese (CN). The latter is tokenized using the Stanford Word Segmenter (Chang et al., 2008). We consider the words in these other languages as external semantic attributes. 4.3 Evaluation Metrics We rely on three standard metrics, BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and CIDEr (Vedantam et al., 2015) to evaluate our methods. These are commonly used in image and video captioning tasks, and allow us to compare our results against previous work. We use the Microsoft COCO evaluation toolkit (Chen et al., 2015), which is widely used in previous work, to compute the met180 ric scores. Across all three metrics, higher scores indicate that the generated captions are assessed as being closer to captions authored by humans. 4.4 Experimental Settings Based on our hyperparameter optimization on the validation set, the number of hid"
Q18-1013,D14-1162,0,0.0887218,"Missing"
Q18-1013,P15-1092,1,0.896161,"Missing"
Q18-1013,W16-2346,0,0.0551375,"Missing"
Q18-1013,N15-1173,0,0.743607,"ing that incorporate semantic concepts has achieved strong results. You et al. (2016) proposed a semantic attention approach to selectively attend to semantic concept proposals and fuse them into hidden states and outputs of RNNs, but their model is difficult to extend to deal with multiple sets of features and attributes together. Overall, none of these methods for image captioning need to account for temporal and motion aspects. Video captioning. For video captioning, many works utilize a recurrent neural architecture to generate video descriptions, conditioned on either an average-pooling (Venugopalan et al., 2015b) or recurrent encoding (Xu et al., 2015a; Donahue et al., 2015; Venugopalan et al., 2015a; Venugopalan et al., 2016) of frame-level features, or on a dynamic linear combination of annotation vectors obtained via temporal attention (Yao et al., 2015). Hierarchical recurrent neural encoders (HRNE) with attention mechanisms have been proposed to better encode videos (Pan et al., 2016a). Yu et al. (2016) exploit several forms of visual attention and rely on a multimodal layer to combine them. In our work, we present an effective multi-faceted attention, which can achieve more stable improvements"
Q18-1013,D16-1204,0,0.014827,"pproach to selectively attend to semantic concept proposals and fuse them into hidden states and outputs of RNNs, but their model is difficult to extend to deal with multiple sets of features and attributes together. Overall, none of these methods for image captioning need to account for temporal and motion aspects. Video captioning. For video captioning, many works utilize a recurrent neural architecture to generate video descriptions, conditioned on either an average-pooling (Venugopalan et al., 2015b) or recurrent encoding (Xu et al., 2015a; Donahue et al., 2015; Venugopalan et al., 2015a; Venugopalan et al., 2016) of frame-level features, or on a dynamic linear combination of annotation vectors obtained via temporal attention (Yao et al., 2015). Hierarchical recurrent neural encoders (HRNE) with attention mechanisms have been proposed to better encode videos (Pan et al., 2016a). Yu et al. (2016) exploit several forms of visual attention and rely on a multimodal layer to combine them. In our work, we present an effective multi-faceted attention, which can achieve more stable improvements in comparison to simple multimodal layers, to jointly model multiple heterogeneous signals, and we experimentally sho"
R19-1126,L16-1626,0,0.0614008,"of distant supervision by using emoticons as noisy labels for Twitter sentiment classification. Davidov et al. (2010) adopted a fairly similar approach by handpicking smileys and hashtags as tweet labels and relying on a supervised method for sentiment analysis of tweets. Emoji Semantics. A prominent work on emojis is the DeepMoji project (Campero et al., 2017) from MIT. It provided a model that recommends emojis given a natural language sentence as input. The deep learning model was trained on a collection of 1.2B tweets to learn the sentiment, emotions, and the use of sarcasm in short text. Barbieri et al. (2016) proposed a method to learn vector space embeddings of emojis using the 1095 standard word2vec skip-gram approach, applied to a large collection of tweets. In contrast, Eisner et al. (2016) attempted to learn vector embeddings of emojis based on their short descriptions in the Unicode standard. Emoji Associations. The first paper that thoroughly investigated the sentiment of emojis (Novak et al., 2015) proposed a sentiment ranking of 715 emojis on a corpus of 70,000 tweets. This work provides a basis for future research on the logographic usage of emojis in social media. Zhou and Wang (2017) t"
R19-1126,D17-1169,0,0.265381,"ous studies investigating the connection between words or sentences and the affects they convey. However, emojis are a particularly prominent feature of modern online interaction. Thus, this paper introduces a new basis for studying this new modality with regard to conveyed affective associations. Emojis have become widespread in social media, and are variously used to carry emotional and contextual information pertaining to the content of social media posts. There have been studies exploring the relationship between hashtags and tweets (Ferragina et al., 2015), and between emojis and tweets (Campero et al., 2017). Additional research has aimed at conducting sentiment analysis based on emojis and hashtags (Novak et al., 2015). A number of other works study the connection between words and emotions, resulting in datasets such as EmoLex (Mohammad and Turney, 2013). Most of these studies relied upon a crowdsourcing approach to compile the data and lexicons and to capture relationships among linguistic and paralinguistic elements (Kulahcioglu and de Melo, 2019). However, previous work has neglected to focus on the emotional aspects of emojis. For instance, we may ultimately be interested in devising a syst"
R19-1126,D14-1181,0,0.00349292,"ween 0 and 1. The Affective Tweets (AT) package was provided to all participants as a baseline for the competition (Mohammad and Bravo-Marquez, 2017b), providing a rich set of features constructed based on several emotion and sentiment lexicons such as NRC-EmoLex, NRC10E, etc. (Mohammad and Bravo-Marquez, 2017a). Model. We rely on a deep neural network to predict the emotion intensity for each tweet, adopting a similar CNN-LSTM architecture as that of IMS (K¨oper et al., 2017), the 2nd-ranked system among all participants in the competition, with the CNN architecture based on that proposed by Kim (2014). In training, each tweet is represented by a matrix of size m x d, where d is the dimensionality of the pre-trained word vectors and m = 50 is the maximal token sequence length considered for 1098 Method A Interpretable AT 0.65 EmoTag 0.70 Non-Interpretable Random Init. 0.68 Google News 0.70 GloVe 0.70 GloVe–Twitter 0.72 IMS 0.71 F J S Avg d 0.66 0.73 0.60 0.69 0.69 0.75 0.65 0.72 n/a 620 0.72 0.72 0.73 0.74 0.74 0.66 0.67 0.68 0.68 0.71 0.73 0.75 0.76 0.76 0.71 0.70 0.71 0.72 0.73 0.72 300 300 300 200 300 Unicode Table 3: Comparing with other methods, with regard to anger (A), fear (F), joy"
R19-1126,C10-2028,0,0.107873,"et al., 2017). Historically, the spread of emojis has been driven in large part by their adoption in popular messaging and social media platforms, which led, among things, to their inclusion in Shift JIS, and, subsequently, the Unicode standard. Nowadays, they are ubiquitous in social media and chat applications, but increasingly also in emails and other digital correspondence. 3 Related Work Emoticons. Early studies focused on the use of emoticons in social media. Go et al. (2009) proposed a form of distant supervision by using emoticons as noisy labels for Twitter sentiment classification. Davidov et al. (2010) adopted a fairly similar approach by handpicking smileys and hashtags as tweet labels and relying on a supervised method for sentiment analysis of tweets. Emoji Semantics. A prominent work on emojis is the DeepMoji project (Campero et al., 2017) from MIT. It provided a model that recommends emojis given a natural language sentence as input. The deep learning model was trained on a collection of 1.2B tweets to learn the sentiment, emotions, and the use of sarcasm in short text. Barbieri et al. (2016) proposed a method to learn vector space embeddings of emojis using the 1095 standard word2vec"
R19-1126,I17-5002,1,0.451376,"Missing"
R19-1126,P18-1235,1,0.514469,"Missing"
R19-1126,W16-6208,0,0.0956019,"Missing"
R19-1126,W17-5206,0,0.0375245,"Missing"
R19-1126,L18-1010,1,0.730374,"Missing"
R19-1126,L18-1027,0,0.272962,"oji, as obtained in our interpretable word vectors, where a higher similarity score entails a higher rank. For the top K words, we compute a weighted average of emotion labels. The emotion labels are taken from EmoLex, while the similarity scores are used as weights. This weighted average then serves as the final emotion score of the emoji. The same process is followed for all emojis. Results. We evaluate our induced emoji emotion scores indirectly by using them to reproduce emotion intensity scores for words, for which we have ground truth intensity scores in the Affect Intensity lexicon by (Mohammad, 2018). This lexicon comes with 6K tokens, where tokes are grouped by the four emotions anger, fear, joy, and sadness. It provides crowdsourced emotion intensity scores, which range between 0 and 1, with 1 meaning that the word exhibits the highest degree of association with a particular emotion and 0 referring to the lowest degree. Note that this ground truth resource is distinct from the NRC Emotion Lexicon used in inducing our scores. The latter merely provides Boolean labels for word–emotion pairs, and thus it is non-trivial to derive affect intensity scores from it, particularly via emojis. To"
R19-1126,W17-5205,0,0.0715799,"Missing"
R19-1126,S17-1007,0,0.0244349,"y, we develop the first resource providing emotion scores for emojis. We evaluate these by showing how they can be used to automatically induce emotion scores for words. 5.1 Emotion Intensity Prediction with Interpretable Emoji-Based Word Vectors We begin by evaluating the interpretable emojibased word vectors, assessing to what extent they are able to keep up with regular word vectors in a downstream task relating to emotions. Benchmark. In particular, we consider the EmoInt Shared Task from WASSA (Workshop on Computational Approaches to Subjectivity, Sentiment & Social Media Analysis) 2017 (Mohammad and Bravo-Marquez, 2017a), which involves determining the intensity or degree of emotion felt by a speaker when a tweet and a target emotion are given. Tweets were provided for four different emotion categories (anger, fear, joy, and sadness), and the ground truth intensity values range between 0 and 1. The Affective Tweets (AT) package was provided to all participants as a baseline for the competition (Mohammad and Bravo-Marquez, 2017b), providing a rich set of features constructed based on several emotion and sentiment lexicons such as NRC-EmoLex, NRC10E, etc. (Mohammad and Bravo-Marquez, 2017a). Model. We rely on"
W09-4407,J06-1003,0,0.0175676,"esult is then computed as # $ T v(s) v(t) wsd(t, s) = σ(t, s) α + (2) ||v(s) |v(t)|| Unlike standard word sense disambiguation setups, we prefer obtaining a weighted set of multiple possibly relevant senses rather than just the sense with the highest confidence score. We use α as a smoothing parameter: For higher values of α, the function tends towards a uniform distribution of scores among the relevant senses, i.e. among those with σ(t, s) = 1. Semantic Similarity For the semantic similarity measure, we do not rely on generic measures of semantic relatedness often described in the literature [1]. The purpose of this measure here is to identify only word senses that are identical or nearly identical (e.g. For each of these n-gram sets A1m , A3m , A2p , etc., we also consider the corresponding counter function a1m , a3m , a2p , etc., that counts how often the n-gram occurs in the example sentence in the respective relative  1 s1 = s2 position. Usually, this will either be 0 or 1, though     1 s1 , s2 in near-synonymy relationship an example sentence may also contain multiple occurrences of the word being described, so higher values sim(s1 , s2 ) = 1 s1 , s2 in hypernymy relations"
W09-4407,W02-0817,0,0.0300648,"ly obtain the respective output for any k "" &lt; k simply by pruning the ranked list generated for k. This can be very useful for interactive user interfaces. 4 Related Work Several means of generating example sentences for word senses have been proposed. Shinnou et al. [19] extract example sentences for a word from a corpus and attempt to distinguish senses by passing humanlabelled sentences as input to a clustering algorithm. This method requires significant human involvement and unlike our approach does not disambiguate senses with respect to a specific sense inventory. Chklovski and Mihalcea [2] presented a Web interface that asks Web users to tag sentences with the correct word sense and relies on active learning methods to select sentences that are hard to tag automatically. A different approach suggested by Mihalcea [10] finds example sentences by using a set of seed expressions to create appropriate queries to Web search engines. For example, for the fibre optic channel sense of word ‘channel ’, appropriate queries would be ‘optical fiber channel ’, ‘channel telephone ’, ‘transmission channel ’. This method works well when such multi-word constructions can be constructed and coul"
W09-4407,P91-1017,0,0.236739,"nd could be used to complement our approach. Another more recent approach [11] clusters words based on a dependency parse of a monolingual corpus. This means that for each word a set of similar words is available. One then tries to match example sentences from the corpus with example sentences already given in WordNet, taking into account the word similarities. Our approach uses a different strategy by relying on parallel corpora. The intuition that lexical ambiguities in parallel corpora can be resolved more easily has been used by a number of works on word sense disambiguation. Dagan et al. [3] provided an initial linguistic analysis of this hypothesis. Several studies [9, 5, etc.] then implemented this idea in word sense disambiguation algorithms. These approaches are similar to our work. They use simple heuristics on parallel corpora to arrive at sense-labelled data that can then be used for word sense disambiguation, while our approach relies on a word sense heuristic to create example sentences from a parallel corpus. With regards to the challenge of selecting the most valuable examples, Fujii et al. [8] proposed a method for choosing example sentences for word sense disambiguat"
W09-4407,W00-0801,0,0.0656467,"Missing"
W09-4407,mihalcea-2002-bootstrapping,0,0.0348802,"have been proposed. Shinnou et al. [19] extract example sentences for a word from a corpus and attempt to distinguish senses by passing humanlabelled sentences as input to a clustering algorithm. This method requires significant human involvement and unlike our approach does not disambiguate senses with respect to a specific sense inventory. Chklovski and Mihalcea [2] presented a Web interface that asks Web users to tag sentences with the correct word sense and relies on active learning methods to select sentences that are hard to tag automatically. A different approach suggested by Mihalcea [10] finds example sentences by using a set of seed expressions to create appropriate queries to Web search engines. For example, for the fibre optic channel sense of word ‘channel ’, appropriate queries would be ‘optical fiber channel ’, ‘channel telephone ’, ‘transmission channel ’. This method works well when such multi-word constructions can be constructed and could be used to complement our approach. Another more recent approach [11] clusters words based on a dependency parse of a monolingual corpus. This means that for each word a set of similar words is available. One then tries to match ex"
W09-4407,J03-1002,0,0.00278992,"curacy of sense-disambiguated example sentences such as word lists, sentence/word length, keyword position, etc. Since the system does not take into account diversity when generating a selection, it would be interesting to combine our algorithm with the scores from their classifiers as additional assets. 5 Results We conducted preliminary experiments on multiple corpora to evaluate the usefulness of our approach. 5.1 Resources In terms of parallel corpora, we relied on parts of the OPUS collection [21], in particular the OpenSubtitles [22] and the OpenOffice.org corpora. We made use of GIZA++ [12] and Uplug [20] to produce the word alignments for these corpora. Additionally, we evaluated example sentence selection for undisambiguated sentences using a subset of the Reuters RCV1 corpus [16], consisting of 39,351 documents. The following lexical knowledge bases were used to build up the sense inventory: • The original Princeton WordNet 3.0 [7] for the English language. • The Spanish WordNet jointly developed by three research groups in Spain [6]. Since it was created in alignment with WordNet 1.6, we applied sense mappings [4] to obtain sense identifiers aligned with the version 3.0 of W"
W09-4407,S07-1016,0,0.0961,"Extraction In the example extraction step, we connect sentences from a corpus to word senses in a given sense inventory whenever we are sufficiently confident that the sentence is an example of the corresponding word being used in the respective sense. Conventional word sense disambiguation heuristics could be used to determine word senses for a monolingual text, and then the sentences in that text could be linked to the respective senses. Unfortunately, even the most sophisticated all-words disambiguation techniques are currently not reliable enough when a finegrained sense inventory is used [14]. The intuition behind our method is that, given a parallel text that has been word aligned, we can jointly look at both versions of the text and determine the most likely senses of certain words with significantly greater accuracy than for any single version of the text. After word alignment, we independently apply word sense disambiguation heuristics for each of the languages to obtain ranked lists of senses for each word. One then analyses to what degree the ranked lists for aligned words overlap. In many cases, this makes it possible to infer the sense of a word much more reliably than wit"
W09-4407,J03-3002,0,0.0431729,"d. One then analyses to what degree the ranked lists for aligned words overlap. In many cases, this makes it possible to infer the sense of a word much more reliably than with conventional disambiguation heuristics. In such a case, we can use the respective sentence in which it occurs as an example sentence for that sense. Lexical Alignment In the past, parallel corpora had been rather difficult to obtain. This has changed with the increasing multilinguality of the Web as well as the greater demand for such resources resulting from the rise of statistical machine translation. Resnik and Smith [15] showed that the Web can be mined to obtain parallel corpora, while Tiedemann [21] built such corpora from sources such as movie subtitles and manuals of open source software. To compare the senses of words in both versions of a text, such parallel corpora first need to be wordaligned. This means that occurrences of terms (individual words or possibly lexicalized multi-word expressions) in one language need to be connected to the corresponding occurrences of semantically equivalent terms in the document for the other language. This is usually accomplished by first aligning sentences, and then"
W09-4407,W98-0709,0,0.0481438,"ora, we relied on parts of the OPUS collection [21], in particular the OpenSubtitles [22] and the OpenOffice.org corpora. We made use of GIZA++ [12] and Uplug [20] to produce the word alignments for these corpora. Additionally, we evaluated example sentence selection for undisambiguated sentences using a subset of the Reuters RCV1 corpus [16], consisting of 39,351 documents. The following lexical knowledge bases were used to build up the sense inventory: • The original Princeton WordNet 3.0 [7] for the English language. • The Spanish WordNet jointly developed by three research groups in Spain [6]. Since it was created in alignment with WordNet 1.6, we applied sense mappings [4] to obtain sense identifiers aligned with the version 3.0 of WordNet. When linking words in the corpus to this inventory, the TreeTagger [18] was used for morphological analysis. 5.2 Experiments We generated sense-disambiguated example sentences for several setups, and evaluated random samples by assessing whether or not the word was indeed used in the sense determined by our method. The results were generalized using Wilson score intervals, and are presented in Table 1. The smoothing parameter α from Section 2"
W09-4407,shinnou-sasaki-2008-division,0,0.102864,"in the case of highly frequent words such as conjunctions), we may 43 also choose to let the algorithm run on a smaller random sample X "" ⊂ X of sentences as input. A useful feature of this greedy algorithm is that it allows emitting a ranked list of entities. Having run the algorithm for a large k, perhaps even k = ∞, we can easily obtain the respective output for any k "" &lt; k simply by pruning the ranked list generated for k. This can be very useful for interactive user interfaces. 4 Related Work Several means of generating example sentences for word senses have been proposed. Shinnou et al. [19] extract example sentences for a word from a corpus and attempt to distinguish senses by passing humanlabelled sentences as input to a clustering algorithm. This method requires significant human involvement and unlike our approach does not disambiguate senses with respect to a specific sense inventory. Chklovski and Mihalcea [2] presented a Web interface that asks Web users to tag sentences with the correct word sense and relies on active learning methods to select sentences that are hard to tag automatically. A different approach suggested by Mihalcea [10] finds example sentences by using a"
W09-4407,E03-1026,0,0.01357,"-disambiguated example sentences such as word lists, sentence/word length, keyword position, etc. Since the system does not take into account diversity when generating a selection, it would be interesting to combine our algorithm with the scores from their classifiers as additional assets. 5 Results We conducted preliminary experiments on multiple corpora to evaluate the usefulness of our approach. 5.1 Resources In terms of parallel corpora, we relied on parts of the OPUS collection [21], in particular the OpenSubtitles [22] and the OpenOffice.org corpora. We made use of GIZA++ [12] and Uplug [20] to produce the word alignments for these corpora. Additionally, we evaluated example sentence selection for undisambiguated sentences using a subset of the Reuters RCV1 corpus [16], consisting of 39,351 documents. The following lexical knowledge bases were used to build up the sense inventory: • The original Princeton WordNet 3.0 [7] for the English language. • The Spanish WordNet jointly developed by three research groups in Spain [6]. Since it was created in alignment with WordNet 1.6, we applied sense mappings [4] to obtain sense identifiers aligned with the version 3.0 of WordNet. When li"
W09-4407,J98-4002,0,0.0523455,"s been used by a number of works on word sense disambiguation. Dagan et al. [3] provided an initial linguistic analysis of this hypothesis. Several studies [9, 5, etc.] then implemented this idea in word sense disambiguation algorithms. These approaches are similar to our work. They use simple heuristics on parallel corpora to arrive at sense-labelled data that can then be used for word sense disambiguation, while our approach relies on a word sense heuristic to create example sentences from a parallel corpus. With regards to the challenge of selecting the most valuable examples, Fujii et al. [8] proposed a method for choosing example sentences for word sense disambiguation systems. Unlike our approach, which aims at representative examples for end users, their approach aims at examples likely to be useful for training a disambiguation system. Their proposal selects example sentences that are hard to classify automatically due to the associated uncertainty, so particularly clear examples of a word’s use are in fact less likely to get elected. Rychly et al. [17] presented a semi-supervised selection system that learns scores based on combinations of weak classifiers. These classifiers"
W09-4407,tiedemann-nygaard-2004-opus,0,0.163714,"many cases, this makes it possible to infer the sense of a word much more reliably than with conventional disambiguation heuristics. In such a case, we can use the respective sentence in which it occurs as an example sentence for that sense. Lexical Alignment In the past, parallel corpora had been rather difficult to obtain. This has changed with the increasing multilinguality of the Web as well as the greater demand for such resources resulting from the rise of statistical machine translation. Resnik and Smith [15] showed that the Web can be mined to obtain parallel corpora, while Tiedemann [21] built such corpora from sources such as movie subtitles and manuals of open source software. To compare the senses of words in both versions of a text, such parallel corpora first need to be wordaligned. This means that occurrences of terms (individual words or possibly lexicalized multi-word expressions) in one language need to be connected to the corresponding occurrences of semantically equivalent terms in the document for the other language. This is usually accomplished by first aligning sentences, and then using global cooccurrence-based statistics to connect words of two corresponding s"
W09-4407,1992.tmi-1.9,0,0.352786,"Missing"
W14-0152,meyers-etal-2004-cross,0,0.0361499,"and Retor´e, 2013). With regard to lexical studies, deverbal nouns are particularly well-studied in English, with the NOMLEX project (Macleod et al., 1998) providing a well-established, open access baseline for corresponding results in other languages. Our work on NomLex-BR builds up from previous work on nominalizations in English (Gurevich et al., 2006). This previous work extended the coverage of NOMLEX’s English norminalizations, via the use of Xerox PARC’s state-of-the-art NLP system XLE (Maxwell and Kaplan, 1996) and some simple, but effective heuristics, and compared it to NOMLEX-PLUS (Meyers et al., 2004), the stateof-the-art in 2004. Our work here is an attempt at building the basic blocks underlying that work on nominalizations, for Portuguese. Our assumption was that the work done for English can be suitably adapted and re-used for Portuguese, if we keep the language comparisons in place. Additionally, we hoped to learn and adapt from the French experience with nominalizations, described in the Nomage project (Balvet et al., 2011). The original version of NOMLEX is a small resource of only around a thousand nominalizations, which seemed ideal to be used as a basis for our project. The origi"
W14-0152,W14-0153,1,0.842766,"Missing"
W14-0152,C12-3044,1,0.783933,"Missing"
W14-0152,kipper-etal-2006-extending,0,0.0392824,"na, for example, the phenomenon of diminutivization of nominals. 5 Preliminary Conclusions This lexicon of deverbals is just a first step for our lexical resources. It would be useful to include nominalizations of adjectives and of other nouns, which also need a common concept mapping for knowledge representation. Examples here would be the nominals “selvageria” or “bruxaria” Figure 1: Entry promover/promoc¸a˜ o from the adjective “selvagem” (wild) and the noun “bruxa” (witch). Another future plan is to produce capture verb semantics and in particular verb alternations, as covered by VerbNet (Kipper et al., 2006) for English. Again, there is hope that some of the original Levin classes used for the construction of VerbNet are also valid for Portuguese. In summary, we believe that the creation of linguistic resources requires openness of programs and of code. The only way to keep alive any resource is to make sure that people can modify it for their own purposes. If one wants the enterprise of automatic language understanding to flourish, especially in languages with fewer resources, one must make sure that the lexical resources we develop are freely available, freely modifiable and easy to use. Making"
W14-0153,baccianella-etal-2010-sentiwordnet,0,0.076615,"s data as quickly as possi3 See http://twitter.com ble. First, the system must be reliable: no information should be lost. This means that a highly available system is called for, with redundancy and active fault tolerance mechanisms. Second, it must have a high throughput, which leads us to an infrastructure that allows parallelism. Thirdly, sentiment classifiers should be able to work with limited resources in both time and space. The training phase should handle an unbalanced distribution of sentiments and in real time, it should be adaptive. OpenWN-PT, Princeton WordNet, and SentiWordNet (Baccianella et al., 2010) were used with the goal of assessing a Machine Learning-based sentiment analysis component integrated into the IBM InfoSphere Streams (ISS) platform. ISS was used to address the problem of handling large streaming Twitter data with availability and scalability in real-time. One main advantage of using OpenWN-PT and SentiWordNet during the development of the Machine Learning-based classifier was that we could start experimenting without training data. The experiment was possible because OpenWN-PT synsets are linked to Princeton WordNet synsets which, in turn, have their sentiment scores in Sen"
W14-0153,W12-3704,0,0.0312011,"ogging online social network is Twitter3 . As of 2013, there are more than 550 million active registered users and 58 million tweets are posted per day on average. These tweets are short messages that people send to provide updates on their activities, observations, or other interesting content, directly or indirectly to others. In sports, for instance, a lot of sentiment is expressed during a game match. Recently there have been several approaches that tackle the problem of classifying tweet sentiments using supervised or semi-supervised machine learning approaches (Celikyilmaz et al., 2010; Bakliwal et al., 2012) or lexicon-based methods, which are mostly unsupervised approaches (Li et al., 2011; Hogenboom et al., 2013). As people react to events and generate a large Twitter stream of data, it is impossible to manually process and analyze all these data during the event’s lifespan. There are several challenges related to analyzing all this data as quickly as possi3 See http://twitter.com ble. First, the system must be reliable: no information should be lost. This means that a highly available system is called for, with redundancy and active fault tolerance mechanisms. Second, it must have a high throu"
W14-0153,P13-1133,0,0.124147,"Missing"
W14-0153,W14-0152,1,0.842332,"Missing"
W14-0153,magnini-cavaglia-2000-integrating,0,0.120765,"ted to these words as well in that they all share a common more general hypernym. OpenWordnet-PT (or OpenWN-PT for short) is a lexical-semantic resource describing (Brazilian) Portuguese words and their relationships. It is modelled after and fully interoperable with the original Princeton WordNet for English (Fellbaum, 1998), relying on the same identifiers as WordNet 3.0. This means that one can easily find Portuguese equivalents for specific English word senses and vice versa. This also means that OpenWN-PT is part of a large ecosystem of compatible resources, including domain identifiers (Magnini and Cavaglia, 2000) and mappings to Wikipedia (de Melo and Weikum, 2010). Maira Gatti IBM Research Brazil Rio de Janeiro, Brazil mairacg@br.ibm.com In this paper, we specify the RDF-based representation chosen for OpenWN-PT (Section 2) and describe our recent efforts to extend this resource (Sections 3 to 5), most notably with nominalization relations connecting nouns and verbs (Section 4). We also highlight several important applications of OpenWN-PT (Section 6). 2 RDF Representation Wordnets have been distributed in a wide range of different incompatible data formats. An increasingly popular way of addressing"
W14-0153,padro-stanilovsky-2012-freeling,0,0.123986,"Missing"
W14-0153,C12-3044,1,0.663533,"Missing"
W15-1523,J92-4003,0,0.115255,"ce for every content word in the vocabulary. Such representations suffer from two problems. First, vectors for two distinct word forms have distinct vectors without any overlap, which means that the vector similarities for any two distinct individual word forms will fail to reflect any possible syntactic or semantic similarities between them. Second, the vector space dimensionality is proportional to the vocabulary size, which can be very large. For instance, the Google 1T corpus has 13M distinct words. To address these two problems, other representations have been proposed. Brown clustering (Brown et al., 1992) organizes words into a binary tree based on the contexts in which they occur. Latent Semantic Analysis and Indexing (LSA/LSI) use singular value decomposition (SVD) to identify the relationships between words in a corpus. Latent Dirichlet Analysis (LDA) (Blei et al., 2003), a generative graphical model, views each document as a collection of topics and assigns each word to these topics. Recently, neural networks have been applied to learn word embeddings in dense real-valued vector spaces. In training, such an approach may combine vector space semantics with predictions from probabilistic mod"
W15-1523,D14-1165,0,0.0316599,"t word. However, it simplifies the hidden layer to be just the average of surrounding words’ embeddings. The Skip-gram model tries to do the opposite. It uses the current word to predict the surrounding words. Both architectures can be trained in just a few hours, while obtaining state-of-the-art embeddings. Distributed word representations now have been applied to numerous natural language processing tasks. For instance, they have been used for sentiment analysis (Socher et al., 2013), paraphrase detection (Socher et al., 2011), machine translation (Devlin et al., 2014), relation extraction (Chang et al., 2014), and parsing, just to name a few. Some of these works use neural network models, e.g. recursive neural networks, auto-encoders, or convolutional neural networks. Others use word embeddings directly as features for clustering or classification with alternative machine learning algorithms. There have been other proposals to adapt the word2vec model. Similar to previous work on semantic spaces based on dependency parse relations (Pad´o and Lapata, 2007), Levy and Goldberg (2014) rely on dependency parsing to create word embeddings. These are able to capture contextual relationships between words"
W15-1523,P14-1129,0,0.082156,"Missing"
W15-1523,N15-1184,0,0.0369024,"n with alternative machine learning algorithms. There have been other proposals to adapt the word2vec model. Similar to previous work on semantic spaces based on dependency parse relations (Pad´o and Lapata, 2007), Levy and Goldberg (2014) rely on dependency parsing to create word embeddings. These are able to capture contextual relationships between words that are further apart in the sentence while simultaneously filtering out some words that are not directly related to the target word. Further analysis revealed that their word embeddings capture more functional but less topical similarity. Faruqui et al. (2015) apply post-processing steps to existing word embeddings in order to bring them more in accordance with semantic lexicons such as PPDB and FrameNet. Wang et al. (2014) train embeddings jointly on text and on Freebase, a wellknown large knowledge base. Their embeddings are trained to preserve relations between entities in the knowledge graph. Rather than using structured knowledge sources, our work focuses on improving word embeddings using textual data by relying on information extraction to expose particularly valuable contexts in a text corpus. 1 https://code.google.com/p/word2vec/ 3 Joint M"
W15-1523,C92-2082,0,0.11547,"China 973 Program Grants 2011CBA00300, 2011CBA00301, and NSFC Grants 61033001, 61361136003, 20141330245. In this example sentence, the token “parts” does not seem to bear any particularly close relationship with the meaning of some of the other tokens, e.g. “Roman” and “culture”. In contrast, the occurrence of an expression such as “Greek and Roman mythology” in a corpus appears to indicate that the two tokens “Roman” and “Greek” likely share certain commonalities. There is a large body of work on information extraction techniques to discover text patterns that reflect semantic relationships (Hearst, 1992; Tandon and de Melo, 2010). In this paper, we propose injecting semantic information into word embeddings by training them not just on general contexts but paying special attention to stronger semantic connections that can be discovered in specific contexts on the Web or in corpora. In particular, we investigate mining information of this sort from enumerations and lists, as well as from definitions. Our training procedure can exploit any source of knowledge about pairs of words being strongly coupled to improve over word embeddings trained just on generic corpus contexts. 2 Background and Re"
W15-1523,P14-2050,0,0.029553,"et al., 2013), paraphrase detection (Socher et al., 2011), machine translation (Devlin et al., 2014), relation extraction (Chang et al., 2014), and parsing, just to name a few. Some of these works use neural network models, e.g. recursive neural networks, auto-encoders, or convolutional neural networks. Others use word embeddings directly as features for clustering or classification with alternative machine learning algorithms. There have been other proposals to adapt the word2vec model. Similar to previous work on semantic spaces based on dependency parse relations (Pad´o and Lapata, 2007), Levy and Goldberg (2014) rely on dependency parsing to create word embeddings. These are able to capture contextual relationships between words that are further apart in the sentence while simultaneously filtering out some words that are not directly related to the target word. Further analysis revealed that their word embeddings capture more functional but less topical similarity. Faruqui et al. (2015) apply post-processing steps to existing word embeddings in order to bring them more in accordance with semantic lexicons such as PPDB and FrameNet. Wang et al. (2014) train embeddings jointly on text and on Freebase,"
W15-1523,J07-2002,0,0.113894,"Missing"
W15-1523,D13-1170,0,0.00447719,"the Skip-gram models. CBOW also relies on a window approach, attempting to use the surrounding words to predict the current target word. However, it simplifies the hidden layer to be just the average of surrounding words’ embeddings. The Skip-gram model tries to do the opposite. It uses the current word to predict the surrounding words. Both architectures can be trained in just a few hours, while obtaining state-of-the-art embeddings. Distributed word representations now have been applied to numerous natural language processing tasks. For instance, they have been used for sentiment analysis (Socher et al., 2013), paraphrase detection (Socher et al., 2011), machine translation (Devlin et al., 2014), relation extraction (Chang et al., 2014), and parsing, just to name a few. Some of these works use neural network models, e.g. recursive neural networks, auto-encoders, or convolutional neural networks. Others use word embeddings directly as features for clustering or classification with alternative machine learning algorithms. There have been other proposals to adapt the word2vec model. Similar to previous work on semantic spaces based on dependency parse relations (Pad´o and Lapata, 2007), Levy and Goldb"
W15-1523,D14-1167,0,0.0609224,"cy parse relations (Pad´o and Lapata, 2007), Levy and Goldberg (2014) rely on dependency parsing to create word embeddings. These are able to capture contextual relationships between words that are further apart in the sentence while simultaneously filtering out some words that are not directly related to the target word. Further analysis revealed that their word embeddings capture more functional but less topical similarity. Faruqui et al. (2015) apply post-processing steps to existing word embeddings in order to bring them more in accordance with semantic lexicons such as PPDB and FrameNet. Wang et al. (2014) train embeddings jointly on text and on Freebase, a wellknown large knowledge base. Their embeddings are trained to preserve relations between entities in the knowledge graph. Rather than using structured knowledge sources, our work focuses on improving word embeddings using textual data by relying on information extraction to expose particularly valuable contexts in a text corpus. 1 https://code.google.com/p/word2vec/ 3 Joint Model Our model simultaneously trains the word embeddings on generic contexts from the corpus on the one hand and semantically significant contexts, obtained using extr"
W19-0421,Q18-1034,0,0.0247086,"mic words the same vector representation, i.e., words that share the same spelling but have different meanings obtain the same representation. For example, the word “kiwi” can signify either a green fruit, a bird or, in informal contexts, the New Zealand dollar, which are three semantically distinct concepts. If only a single vector representation is used, then this representation is likely to primarily reflect the word’s most prominent sense, while neglecting other meanings (see Figure 1). More generally, a word vector may be a linear superposition of features of multiple unrelated meanings (Arora et al., 2018), resulting in incoherent vector spaces. In recent years, several ideas have been proposed to overcome this problem. They have in common that they obtain different vector representations for the different meanings of polysemes or homonyms. Most prior work only evaluates these multi-sense vectors on single word benchmarks, however, and there is comparably little evidence for the benefits of using these embeddings in other applications. One multi-word task that suffers from the presence of polysemy and homonymy is the building of a reverse dictionary that can take definitions of words as input a"
W19-0421,P17-1151,0,0.0233439,"ing prepositional phrase attachments. Pilehvar et al. (2017) use the same DeConf multi-sense embedding for integrating them in a downstream application. In contrast to our work, they require, however, a semantic network to do the disambiguation. In Sense2Vec (Trask et al., 2015), the authors create embeddings that distinguish between different meanings given the corresponding part-of-speech or named entity tag. They obtain an embedding that distinguishes e.g. between the location Washington and the person with the same name. The method requires the input data to be tagged with POS or NE tags. Athiwaratkun and Wilson (2017) represent multiple meanings as a mixture of Gaussian distributions. The number of senses per word is fixed globally to the number of Gaussian components. Raganato et al. (2017) and Pesaranghader et al. (2018) use bidirectional LSTMs to learn a mapping between words and multiple senses (not sense vectors) as a supervised sequence prediction task requiring sense-tagged text. An extensive survey on further ideas and work regarding vector representations of meaning is given by Camacho-Collados and Pilehvar (2018). Tang et al. (2018) analyzed different attention mechanisms in the specific context"
W19-0421,Q17-1010,0,0.0482508,"different embeddings, the reverse dictionary task and the corresponding architecture. We also motivate the use of multi-sense embeddings for the target and input vectors with qualitative examples and a quantitative analysis. 2.1 Single- and Multi-Sense Word Embeddings A single-sense word embedding es maps a word or token to an l-dimensional vector representation, i.e. es (wi ) = di ∈ Rl for a word wi . They are often pre-trained on large amounts of unlabeled text and serve as a fundamental building block in many neural NLP models. Popular word embeddings include word2vec, GloVe and fastText (Bojanowski et al., 2017). If a word has several meanings, these are still mapped to just a single vector representation. Multi-sense word embeddings em overcome this limitation by mapping each word wi to a list of sense vectors em (wi ) = (di1 , ..., dik ), where k is the number of senses that one considers wi to have. The vector dij then represents one sense of the given word. This difference is visualized in Figure 2. Often, these embeddings can also be pre-trained on unlabeled text. A discussion of different multi-sense word embeddings is given in Section 5. 2.2 Reverse Dictionaries A reverse dictionary is a tool"
W19-0421,D18-1181,0,0.0234915,"Related Work Hill et al. (2016) proposed to map dictionary definitions to vectors both for the practical application of reverse dictionaries as well as to study representations of phrases and sequences. In this setting, Bastos (2018) experimented with recursive neural networks and additional part-of-speech information. Independently of Hill et al., Scheepers et al. (2018) also used dictionary definitions to evaluate ways to compose sequences of words. They studied different single-sense word embeddings and composition methods such as vector addition and recurrent neural networks. The work by Bosc and Vincent (2018) improves word embeddings with an auto-encoder structure that goes from the target word embedding back to the definition. We consider these three works complementary to ours, as they study different single-sense architectures. In recent years, several approaches to creating multi-sense vector embeddings have been proposed. Rothe and Sch¨utze (2015), Pilehvar and Collier (2016) and Dasigi et al. (2017) use an existing single-sense word embedding and a lexical resource to induce vectors representing different senses of a word. The latter also employ an attention-based approach for creating vecto"
W19-0421,P17-1191,0,0.0213981,"s to evaluate ways to compose sequences of words. They studied different single-sense word embeddings and composition methods such as vector addition and recurrent neural networks. The work by Bosc and Vincent (2018) improves word embeddings with an auto-encoder structure that goes from the target word embedding back to the definition. We consider these three works complementary to ours, as they study different single-sense architectures. In recent years, several approaches to creating multi-sense vector embeddings have been proposed. Rothe and Sch¨utze (2015), Pilehvar and Collier (2016) and Dasigi et al. (2017) use an existing single-sense word embedding and a lexical resource to induce vectors representing different senses of a word. The latter also employ an attention-based approach for creating vectors based on the context for predicting prepositional phrase attachments. Pilehvar et al. (2017) use the same DeConf multi-sense embedding for integrating them in a downstream application. In contrast to our work, they require, however, a semantic network to do the disambiguation. In Sense2Vec (Trask et al., 2015), the authors create embeddings that distinguish between different meanings given the corr"
W19-0421,Q16-1002,0,0.294253,"everse Dictionaries A reverse dictionary is a tool for authors and writers seeking a word that is on the tip of their tongue. Given a user-provided definition or description, a reverse dictionary attempts to return the corresponding word (Zock and Bilac, 2004). We create a dataset for this task using the WordNet resource (Miller, 1995). For each word sense in this lexical database, we consider the provided gloss description as the input, and the word as the target. the size of something as given by the distance around it → circumference More details about the dataset are given in Section 4.1. Hill et al. (2016) presented a neural network approach for this task and also set it in the wider context of sequence embeddings. Each instance consists of a description, i.e. a sequence of words (w1 , ..., wn ), and a target vector t. Each word of the input sequence wi is mapped with a single-sense word embedding function es (e.g. word2vec) to a vector representation es (wi ). This sequence of vectors is then transformed into a single vector ˆ t = f (es (w1 ), ..., es (wn )). (1) For f , the authors use—among others—a combination of an LSTM (Hochreiter and Schmidhuber, 1997) and a dense layer. The network is t"
W19-0421,D18-1221,0,0.144282,"• For the single-sense baseline, we use the reverse dictionary architecture proposed by Hill et al. (2016), which also serves as the foundation of all the multi-sense models. • In first multi-sense, we experiment with using the first multi-sense vector for every word as a singlesense vector, i.e. vi = di1 . This is motivated by the fact that the WordNet-based multi-sense vectors tend to be roughly ordered by frequency of occurrence (see analysis in Section 4.7). • Random multi-sense evaluates using a randomly selected multi-sense vector. • The model not-pretrained is based on the approach of Kartsaklis et al. (2018). They recently proposed a method to obtain single-sense and multi-sense vector embeddings during training (in contrast to our use of pre-trained embeddings for both). While one of their experiments also evaluates on a reversedictionary setting, their results are unfortunately not directly comparable, as their targets are WordNet synsets and not words. We, therefore, integrate their proposed technique into our architecture in two ways: For the model not pre-trained, we use their equivalent version of vi . This means that we use their code for the training of the single and multi-sense embeddin"
W19-0421,D14-1162,0,0.086862,". In this work, we study the effect of multi-sense embeddings on the task of reverse dictionaries. We propose a technique to easily integrate them into an existing neural network architecture using an attention mechanism. Our experiments demonstrate that large improvements can be obtained when employing multi-sense embeddings both in the input sequence as well as for the target representation. An analysis of the sense distributions and of the learned attention is provided as well. 1 Introduction One problem with popular word embedding methods such as word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014) is that they assign polysemic or homonymic words the same vector representation, i.e., words that share the same spelling but have different meanings obtain the same representation. For example, the word “kiwi” can signify either a green fruit, a bird or, in informal contexts, the New Zealand dollar, which are three semantically distinct concepts. If only a single vector representation is used, then this representation is likely to primarily reflect the word’s most prominent sense, while neglecting other meanings (see Figure 1). More generally, a word vector may be a linear superposition of f"
W19-0421,N18-1202,0,0.0473007,", therefore, integrate their proposed technique into our architecture in two ways: For the model not pre-trained, we use their equivalent version of vi . This means that we use their code for the training of the single and multi-sense embeddings as well as for the creation of vi based on the context and the multi-sense embedding. The model only es pre-trained differs from this in that we use the pre-trained single-sense embedding instead of training it from scratch. • The BERT model belongs to the class of contextual word embeddings. This approach has been rapidly become popular with works by Peters et al. (2018), Radford et al. (2018), Peters et al. (2018b) and Devlin et al. (2018). Instead of using a direct mapping of words to vector representations, these approaches pre-train a neural language model on a large amount of text. The language model’s internal state for each input word is then used as a corresponding word vector representation for a different task. They can be viewed as inducing word vector representations that are specific to the surrounding context. We compare against the current state-of-the-art model BERT (Devlin et al., 2018). For this, the output of BERT’s last Transformer layer i"
W19-0421,D18-1179,0,0.0464118,"Missing"
W19-0421,P17-1170,0,0.0224723,"he target word embedding back to the definition. We consider these three works complementary to ours, as they study different single-sense architectures. In recent years, several approaches to creating multi-sense vector embeddings have been proposed. Rothe and Sch¨utze (2015), Pilehvar and Collier (2016) and Dasigi et al. (2017) use an existing single-sense word embedding and a lexical resource to induce vectors representing different senses of a word. The latter also employ an attention-based approach for creating vectors based on the context for predicting prepositional phrase attachments. Pilehvar et al. (2017) use the same DeConf multi-sense embedding for integrating them in a downstream application. In contrast to our work, they require, however, a semantic network to do the disambiguation. In Sense2Vec (Trask et al., 2015), the authors create embeddings that distinguish between different meanings given the corresponding part-of-speech or named entity tag. They obtain an embedding that distinguishes e.g. between the location Washington and the person with the same name. The method requires the input data to be tagged with POS or NE tags. Athiwaratkun and Wilson (2017) represent multiple meanings a"
W19-0421,D16-1174,0,0.0197688,"also used dictionary definitions to evaluate ways to compose sequences of words. They studied different single-sense word embeddings and composition methods such as vector addition and recurrent neural networks. The work by Bosc and Vincent (2018) improves word embeddings with an auto-encoder structure that goes from the target word embedding back to the definition. We consider these three works complementary to ours, as they study different single-sense architectures. In recent years, several approaches to creating multi-sense vector embeddings have been proposed. Rothe and Sch¨utze (2015), Pilehvar and Collier (2016) and Dasigi et al. (2017) use an existing single-sense word embedding and a lexical resource to induce vectors representing different senses of a word. The latter also employ an attention-based approach for creating vectors based on the context for predicting prepositional phrase attachments. Pilehvar et al. (2017) use the same DeConf multi-sense embedding for integrating them in a downstream application. In contrast to our work, they require, however, a semantic network to do the disambiguation. In Sense2Vec (Trask et al., 2015), the authors create embeddings that distinguish between differen"
W19-0421,N18-2084,0,0.0318724,"ponent of this architecture. The model that uses the embedding training and multi-sense vector selection of Kartsaklis et al. seems to struggle with building good embeddings in this setting with the 300-dimensional embeddings performing somewhat better but still not well. Providing pre-trained single-sense embeddings improves the performance considerably. Although they are not trained task-specifically, the pre-training of the single-sense embeddings on large amounts of unlabeled data seems to result in a very useful embedding space. This is consistent with other works in the literature, e.g. Qi et al. (2018). Our attention based multi-sense vector approach using pre-trained single- and multi-sense embeddings obtains the best results with respect to all four metrics, with the dot product similarity function performing somewhat better than cosine similarity. This shows that using pre-trained multi-sense vectors and selecting the right sense vectors can be beneficial in sequence embedding tasks. 4.7 Study of Senses and Attention In this section, we present a small study to gain more insight into the different senses occurring in the input sequences as well as into the learned attention. This is also"
W19-0421,D17-1120,0,0.029281,"quire, however, a semantic network to do the disambiguation. In Sense2Vec (Trask et al., 2015), the authors create embeddings that distinguish between different meanings given the corresponding part-of-speech or named entity tag. They obtain an embedding that distinguishes e.g. between the location Washington and the person with the same name. The method requires the input data to be tagged with POS or NE tags. Athiwaratkun and Wilson (2017) represent multiple meanings as a mixture of Gaussian distributions. The number of senses per word is fixed globally to the number of Gaussian components. Raganato et al. (2017) and Pesaranghader et al. (2018) use bidirectional LSTMs to learn a mapping between words and multiple senses (not sense vectors) as a supervised sequence prediction task requiring sense-tagged text. An extensive survey on further ideas and work regarding vector representations of meaning is given by Camacho-Collados and Pilehvar (2018). Tang et al. (2018) analyzed different attention mechanisms in the specific context of ambiguous words in machine translation. They limit their approach, however, to single-sense vectors and the established method of using attention over other parts of the sent"
W19-0421,P15-1173,0,0.0441489,"Missing"
W19-0421,D18-1458,0,0.0298887,"Missing"
W19-0421,W04-2105,0,0.227157,"f sense vectors em (wi ) = (di1 , ..., dik ), where k is the number of senses that one considers wi to have. The vector dij then represents one sense of the given word. This difference is visualized in Figure 2. Often, these embeddings can also be pre-trained on unlabeled text. A discussion of different multi-sense word embeddings is given in Section 5. 2.2 Reverse Dictionaries A reverse dictionary is a tool for authors and writers seeking a word that is on the tip of their tongue. Given a user-provided definition or description, a reverse dictionary attempts to return the corresponding word (Zock and Bilac, 2004). We create a dataset for this task using the WordNet resource (Miller, 1995). For each word sense in this lexical database, we consider the provided gloss description as the input, and the word as the target. the size of something as given by the distance around it → circumference More details about the dataset are given in Section 4.1. Hill et al. (2016) presented a neural network approach for this task and also set it in the wider context of sequence embeddings. Each instance consists of a description, i.e. a sequence of words (w1 , ..., wn ), and a target vector t. Each word of the input s"
