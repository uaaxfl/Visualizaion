2020.acl-main.687,P19-1122,1,0.852672,"asets We experiment with four language pairs, English↔{German, Romanian, French, Japanese} to show the consistency of our proposed attention variants. For the En-De pair, we use both the small IWSLT 20169 and the larger WMT 2014 datasets. For all datasets except WMT14 En→De and WMT14 En→Fr,10 we run experiments in both directions. For English-Japanese, we train and evaluate on IWSLT 2017 En↔Ja TED talk dataset. More dataset statistics are shown in Table 1. 4.2 Architectures Our BASE model is the original Transformer from Vaswani et al. (2017), reimplemented in PyTorch (Paszke et al., 2019) by Akoury et al. (2019).11 To implement hard-coded attention, we only modify the attention functions in this codebase and keep everything else the same. For the two small IWSLT datasets, we follow prior work 8 Code and scripts to reproduce our experimental results to be released after blind review. 9 We report BLEU on the IWSLT16 En-De dev set following previous work (Gu et al., 2018; Lee et al., 2018; Akoury et al., 2019). For other datasets, we report test BLEU. 10 As the full WMT14 En→Fr is too large for us to feasibly train on, we instead follow Akoury et al. (2019) and train on just the Europarl / Common Crawl"
2020.acl-main.687,D19-1223,0,0.119955,"Missing"
2020.acl-main.687,D19-5622,0,0.192008,"ost-hoc. There have been many efforts to modify MHA in Transformers. One such direction is to inject linguistic knowledge through auxiliary supervised tasks (Garg et al., 2019; Pham et al., 2019). Other work focuses on improving inference speed: Yang et al. (2018) replace decoder self-attention with a simple average attention network, assigning equal weights to target-side previous tokens.20 Wu et al. (2019) also speed up decoding by replacing selfattention with convolutions that have time-step dependent kernels; we further simplify this work with our fixed convolutional kernels in Section 6. Cui et al. (2019) also explore fixed attention while retaining some learned parameters, and Vashishth et al. (2019) show that using uniform or random attention deteriorates performances on paired sentences tasks including machine translation. Other work has also explored modeling locality (Shaw et al., 2018; Yang et al., 2018). 8 Conclusion In this paper, we present “hard-coded” Gaussian attention, which while lacking any learned parameters can rival multi-headed attention for neural machine translation. Our experiments suggest that encoder and decoder self-attention is not crucial for translation quality comp"
2020.acl-main.687,D19-1453,0,0.0211132,"ntly impact BLEU scores. Similarly, Voita et al. (2019) prune many encoder self-attention heads without degrading BLEU, while Tang et al. (2019) further 19 We used the smaller IWSLT En-De architecture for this experiment. simplify the Transformer by removing the entire encoder for a drop of three BLEU points. In contrast to existing literature on model pruning, we train our models without learned attention heads instead of removing them post-hoc. There have been many efforts to modify MHA in Transformers. One such direction is to inject linguistic knowledge through auxiliary supervised tasks (Garg et al., 2019; Pham et al., 2019). Other work focuses on improving inference speed: Yang et al. (2018) replace decoder self-attention with a simple average attention network, assigning equal weights to target-side previous tokens.20 Wu et al. (2019) also speed up decoding by replacing selfattention with convolutions that have time-step dependent kernels; we further simplify this work with our fixed convolutional kernels in Section 6. Cui et al. (2019) also explore fixed attention while retaining some learned parameters, and Vashishth et al. (2019) show that using uniform or random attention deteriorates pe"
2020.acl-main.687,N19-1357,0,0.0228089,"encoder’s final layer token representations through a linear projection. To summarize, MHA is used in three different components of the Transformer: encoder self-attention, decoder self-attention, and cross attention. 7690 2.2 Learned heads mostly focus on local windows The intuition behind MHA is that each head can focus on a different type of information (e.g., syntactic or semantic patterns). While some heads have been shown to possess interpretable patterns (Voita et al., 2019; Correia et al., 2019), other work has cautioned against using attention patterns to explain a model’s behavior (Jain and Wallace, 2019). In our analysis, we specifically examine the behavior of a head with respect to the current query token’s position in the sequence. We train a baseline Transformer model (five layers, two heads per layer) on the IWSLT 2016 En→De dataset, and compute aggregated statistics on its learned heads. Figure 2 shows that outside of a few layers, most of the model’s heads focus their attention (i.e., the argmax of the attention distribution) on a local neighborhood around the current sequence position. For example, both self-attention heads in the first layer of the encoder tend to focus on just a one"
2020.acl-main.687,D13-1176,0,0.0529737,"to distribute them across multiple layers 7696 WMT2016 En-Ro 30.5 Multiple heads same layer Single head across layers BLEU 29.5 28.5 27.5 1 2 3 Number of learned heads 4 Figure 7: Adding more cross attention heads in the same layer helps less than adding individual heads across different layers. of the decoder? Experiments on the WMT16 EnRo dataset19 (Figure 7) indicate that distributing learned heads over multiple layers leads to significantly better BLEU than adding all of them to the same layer. 7 Related Work Attention mechanisms were first introduced to augment vanilla recurrent models (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Chorowski et al., 2015; Wu et al., 2016; Miceli Barone et al., 2017) but have become the featured component of the state-of-the-art Transformer architecture (Vaswani et al., 2017) for NMT. We review recent research that focuses on analysing and improving multiheaded attention, and draw connections to our work. The intuitive advantage of MHA is that different heads can focus on different types of information, all of which will eventually be helpful for translation. Voita et al. (2019) find that some heads focus on adjacent tok"
2020.acl-main.687,D18-1149,0,0.0535693,"Missing"
2020.acl-main.687,D15-1166,0,0.0855297,"ds same layer Single head across layers BLEU 29.5 28.5 27.5 1 2 3 Number of learned heads 4 Figure 7: Adding more cross attention heads in the same layer helps less than adding individual heads across different layers. of the decoder? Experiments on the WMT16 EnRo dataset19 (Figure 7) indicate that distributing learned heads over multiple layers leads to significantly better BLEU than adding all of them to the same layer. 7 Related Work Attention mechanisms were first introduced to augment vanilla recurrent models (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Chorowski et al., 2015; Wu et al., 2016; Miceli Barone et al., 2017) but have become the featured component of the state-of-the-art Transformer architecture (Vaswani et al., 2017) for NMT. We review recent research that focuses on analysing and improving multiheaded attention, and draw connections to our work. The intuitive advantage of MHA is that different heads can focus on different types of information, all of which will eventually be helpful for translation. Voita et al. (2019) find that some heads focus on adjacent tokens to the query (mirroring our analysis in Section 2), while other"
2020.acl-main.687,W17-4710,0,0.0237767,"mber of learned heads 4 Figure 7: Adding more cross attention heads in the same layer helps less than adding individual heads across different layers. of the decoder? Experiments on the WMT16 EnRo dataset19 (Figure 7) indicate that distributing learned heads over multiple layers leads to significantly better BLEU than adding all of them to the same layer. 7 Related Work Attention mechanisms were first introduced to augment vanilla recurrent models (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Chorowski et al., 2015; Wu et al., 2016; Miceli Barone et al., 2017) but have become the featured component of the state-of-the-art Transformer architecture (Vaswani et al., 2017) for NMT. We review recent research that focuses on analysing and improving multiheaded attention, and draw connections to our work. The intuitive advantage of MHA is that different heads can focus on different types of information, all of which will eventually be helpful for translation. Voita et al. (2019) find that some heads focus on adjacent tokens to the query (mirroring our analysis in Section 2), while others focus on specific dependency relations or rare tokens. Correia et al"
2020.acl-main.687,W18-6319,0,0.0435433,"bottom). Hard-coded self-attention (HC - SA) achieves almost identical BLEU scores to BASE across all datasets, while a model with only one cross attention head (SH - X) performs slightly worse. by using a small Transformer architecture with embedding size 288, hidden size 507, four heads,12 five layers, and a learning rate 3e-4 with a linear scheduler. For the larger datasets, we use the standard Tranformer base model, with embedding size 512, hidden size 2048, eight heads, six layers, and a warmup scheduler with 4,000 warmup steps. For all experiments, we report BLEU scores using SacreBLEU (Post, 2018) to be able to compare with other work.13 4.3 Summary of results Broadly, the trends we observed on IWSLT16 EnDe in the previous section are consistent for all of the datasets and language pairs. Our findings are summarized as follows: • A Transformer with hard-coded self-attention in the encoder and decoder and learned cross attention (HC - SA) achieves almost equal BLEU scores to the BASE Transformer. • Hard-coding both cross attention and selfattention (HC - ALL) considerably drops BLEU compared to BASE, suggesting cross attention is more important for translation quality. • A configuration"
2020.acl-main.687,E17-2060,0,0.0344459,"performance declines as a function of sentence length. To measure this, we categorize the WMT14 En-De test set into five bins by reference length and plot the decrease in BLEU between BASE and our hard-coded configurations for each bin. Somewhat surprisingly, Figure 4 shows that the BLEU gap between BASE and HC - SA seems to be roughly constant across all bins.16 However, the fully hard-coded HC - ALL model clearly deteriorates as reference length increases. Does hard-coding attention produce any systematic linguistic errors? For a more fine-grained analysis, we run experiments on LingEval97 (Sennrich, 2017), an English→German dataset consisting of contrastive 16 We note that gradients will flow across long distances if the number of layers is large enough, since the effective window size increases with multiple layers (van den Oord et al., 2016; Kalchbrenner et al., 2016). Error type BASE HC - SA HC - ALL np-agreement subj-verb-agreement subj-adequacy polarity-particle-nicht-del polarity-particle-kein-del polarity-affix-del polarity-particle-nicht-ins polarity-particle-kein-ins polarity-affix-ins auxiliary verb-particle compound transliteration 54.2 87.5 87.3 94.0 91.4 91.6 92.6 94.8 91.9 89.1 7"
2020.acl-main.687,W16-2323,0,0.0286366,"nd selfattention (HC - ALL) considerably drops BLEU compared to BASE, suggesting cross attention is more important for translation quality. • A configuration with hard-coded self12 For hard-coded configurations, we duplicate heads to fit this architecture (e.g., we have two heads per layer in the encoder with means of i + 1 and i − 1). 13 SacreBLEU signature: BLEU+case.mixed+lang.LANG +numrefs.1+smooth.exp+test.TEST+tok.intl+version.1.2.11, with LANG ∈ {en-de, de-en, en-fr} and TEST ∈ {wmt14/full, iwslt2017/tst2013}. For WMT16 EnRo and IWSLT17 En-Ja, we follow previous work for preprocessing (Sennrich et al., 2016), encoding the latter with a 32K sentencepiece vocabulary (https://github.com/google/sentencepiece) and measuring the de-tokenized BLEU with SacreBLEU. attention and a single learned cross attention head in the final decoder layer (SH - X) consistently performs 1-3 BLEU worse than BASE . These results motivate a number of interesting analysis experiments (e.g., what kinds of phenomena is MHA better at handling than hard-coded attention), which we describe in Section 6. The strong performance of our highly-simplified models also suggests that we may be able to obtain memory or decoding speed im"
2020.acl-main.687,N18-2074,0,0.0533955,"tion with a simple average attention network, assigning equal weights to target-side previous tokens.20 Wu et al. (2019) also speed up decoding by replacing selfattention with convolutions that have time-step dependent kernels; we further simplify this work with our fixed convolutional kernels in Section 6. Cui et al. (2019) also explore fixed attention while retaining some learned parameters, and Vashishth et al. (2019) show that using uniform or random attention deteriorates performances on paired sentences tasks including machine translation. Other work has also explored modeling locality (Shaw et al., 2018; Yang et al., 2018). 8 Conclusion In this paper, we present “hard-coded” Gaussian attention, which while lacking any learned parameters can rival multi-headed attention for neural machine translation. Our experiments suggest that encoder and decoder self-attention is not crucial for translation quality compared to cross attention. We further find that a model with hard-coded selfattention and just a single cross attention head performs slightly worse than a baseline Transformer. Our work provides a foundation for future work into simpler and more computationally efficient neural machine trans"
2020.acl-main.687,P19-1580,0,0.365381,"d a different token within a local window. Introduction The Transformer (Vaswani et al., 2017) has become the architecture of choice for neural machine translation. Instead of using recurrence to contextualize source and target token representations, Transformers rely on multi-headed attention mechanisms (MHA), which speed up training by enabling parallelization across timesteps. Recent work has called into question how much MHA contributes to translation quality: for example, a significant fraction of attention heads in a pretrained Transformer can be pruned without appreciable loss in BLEU (Voita et al., 2019; Michel et al., 2019), and self-attention can be replaced by less expensive modules such as convolutions (Yang et al., 2018; Wu et al., 2019). In this paper, we take this direction to an extreme by developing a variant of MHA without * Authors contributed equally. went any learned parameters (Section 3). Concretely, we replace each attention head with a “hard-coded” version, which is simply a standard normal distribution centered around a particular position in the sequence (Figure 1).1 When we replace all encoder and decoder self-attention mechanisms with our hard-coded variant, we achieve a"
2020.acl-main.687,D18-1475,0,0.4587,"choice for neural machine translation. Instead of using recurrence to contextualize source and target token representations, Transformers rely on multi-headed attention mechanisms (MHA), which speed up training by enabling parallelization across timesteps. Recent work has called into question how much MHA contributes to translation quality: for example, a significant fraction of attention heads in a pretrained Transformer can be pruned without appreciable loss in BLEU (Voita et al., 2019; Michel et al., 2019), and self-attention can be replaced by less expensive modules such as convolutions (Yang et al., 2018; Wu et al., 2019). In this paper, we take this direction to an extreme by developing a variant of MHA without * Authors contributed equally. went any learned parameters (Section 3). Concretely, we replace each attention head with a “hard-coded” version, which is simply a standard normal distribution centered around a particular position in the sequence (Figure 1).1 When we replace all encoder and decoder self-attention mechanisms with our hard-coded variant, we achieve almost identical BLEU scores to the baseline Transformer for four different language pairs (Section 4).2 These experiments ma"
2020.acl-main.687,D18-1458,0,0.169461,"Missing"
2020.acl-main.687,R19-1136,0,0.24949,"Missing"
2020.emnlp-main.392,D14-1139,0,0.0400462,"Missing"
2020.emnlp-main.392,P05-1022,0,0.23399,"aches are closely related to fast and accurate parsing. More accurate models tend to use richer features that are more expensive to compute, influencing researchers to find efficient techniques to offset the loss in speed (Vieira and Eisner, 2017). In this paper, we use the most simple approach to learn to parse with the capability to recover from local errors by maintaining a beam of size at each cell in the chart. S-DIORA is often faster and discovers better trees than DIORA, but there are other methods for extracting lists of best or plausible parses (Resnik, 1992; Roark and Johnson, 1999; Charniak and Johnson, 2005; Huang and Chiang, 2005; Bouchard-cˆot´e et al., 2009) that might further improve performance. Sparse structured inference. Various work has explored sparse alternatives to soft-weighting. Sparsemax (Martins and Astudillo, 2016) is a deterministic sparse alternative to the softmax, and Gumbel-Softmax (Jang et al., 2017) uses the categorical reparameterization trick to sample a discrete value during training. Both have attractive properties but alone would not be sufficient for overcoming local errors in S-DIORA. Nonetheless, these options would be worth exploring for unsupervised parsing when"
2020.emnlp-main.392,P06-1111,0,0.152222,"Missing"
2020.emnlp-main.392,P19-1551,0,0.0198779,"to the softmax, and Gumbel-Softmax (Jang et al., 2017) uses the categorical reparameterization trick to sample a discrete value during training. Both have attractive properties but alone would not be sufficient for overcoming local errors in S-DIORA. Nonetheless, these options would be worth exploring for unsupervised parsing when training with more data or when the ground truth parse trees are very different than the ones in S-DIORA’s output frontier after initialization. Other work has explored methods for differentiable structured inference (Niculae et al., 2018; Mensch and Blondel, 2018; Corro and Titov, 2019a,b), which may also be suitable. It’s worth noting that PCFGs are not graphical models (Liang et al., 2009), and marginal inference is often not tractable,11 which is why these approximate methods may be helpful. Grammar induction. There is a rich research history in grammar induction and unsupervised parsing (Fu and Booth, 1975; Angluin, 1980; Carroll and Charniak, 1992). We cover notable work not already mentioned in Appendix A.2. 7 Conclusion We introduce S-DIORA, an extension to DIORA that enables for easy recovery from local errors and is not subject to wash out from vector averaging. Ou"
2020.emnlp-main.392,D16-1001,0,0.0209041,"trategy to follow From the outset the tobacco industry has been uncertain as to what strategy to follow From the outset the tobacco industry has been uncertain as to what strategy to follow From the outset the tobacco industry has been uncertain as to what strategy to follow Figure 5: As the beam-size increases, S-DIORA’s output tends to match the ground truth more closely. The displayed output, top to bottom, are from PTB, then S-DIORAP T B with = 1, 3, 5 respectively. vised parsers do not need error recovery because they use models with rich features and model each span score independently (Cross and Huang, 2016; Stern et al., 2017; Kitaev and Klein, 2018; Mrini et al., 2019). Previous research has attempted to achieve the “best of both worlds” by distilling a strong model for supervised parsing via an unsupervised model’s output (Le and Zuidema, 2015). These approaches are closely related to fast and accurate parsing. More accurate models tend to use richer features that are more expensive to compute, influencing researchers to find efficient techniques to offset the loss in speed (Vieira and Eisner, 2017). In this paper, we use the most simple approach to learn to parse with the capability to recov"
2020.emnlp-main.392,N19-1423,0,0.0210802,"notation (Klein and Manning, 2002; Bod, 2006; Ponvert et al., 2011; Shen et al., 2019; Kim et al., 2019, inter alia). Of these models, we focus on the deep insideoutside recursive autoencoder (DIORA; Drozdov et al. 2019a). DIORA encodes sentences in a procedure resembling the inside-outside algorithm (Baker, 1979), which allows it to induce syntactic tree structures for input sentences without access to labeled training data, and achieves near stateof-the-art results on unsupervised constituency parsing. DIORA resembles pre-trained language models, such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), in that it is trained with a self-supervised blank-filling objective on large amounts of unlabeled data. DIORA is a strong unsupervised parser in spite of its locally greedy nature. DIORA works by encoding all subtrees covering a particular span as separate vectors, and then computing a weighted average of these vectors — DIORA uses this averaged vector later in the dynamic program to represent the entire forest of trees covering a span. DIORA computes a score for each subtree; intuitively, a subtree’s score affects how strongly it is 4832 Proceedings of the 2020 Conference on Empirical Meth"
2020.emnlp-main.392,P18-2058,0,0.0187415,"data used for fine-tuning. 1 DIORA DIORA … … S-DIORA S-DIORA 1 1 2 2 Figure 1: DIORA (top row) is sensitive to locally nonoptimal decisions. By assigning a low weight to a potentially important subtree when recursively computing the vector for a target tree, it is difficult or impossible to recover and the important subtree is washed out (represented in light gray). Our method, S-DIORA (bottom row) can recover from errors, and the desired tree ends up at the top of the beam in the right-most column. Introduction Syntactic parse trees are valuable intermediate features for many NLP pipelines (He et al., 2018; Strubell et al., 2018), as a soft constraint (Rush and Collins, 2012), a hard constraint (Lee et al., 2019b), or in multi-task learning with syntactic scaffolds (Swayamdipta et al., 2018). Syntactic inductive bias can also improve generalization of deep learning models (Kuncoro et al., 2020). These results have motivated researchers to pursue unsupervised parsing, with the hope of training syntax-dependent models on large amounts of data without annotation (Klein and Manning, 2002; Bod, 2006; Ponvert et al., 2011; Shen et al., 2019; Kim et al., 2019, inter alia). Of these models, we focus on"
2020.emnlp-main.392,W05-1506,0,0.107565,"fast and accurate parsing. More accurate models tend to use richer features that are more expensive to compute, influencing researchers to find efficient techniques to offset the loss in speed (Vieira and Eisner, 2017). In this paper, we use the most simple approach to learn to parse with the capability to recover from local errors by maintaining a beam of size at each cell in the chart. S-DIORA is often faster and discovers better trees than DIORA, but there are other methods for extracting lists of best or plausible parses (Resnik, 1992; Roark and Johnson, 1999; Charniak and Johnson, 2005; Huang and Chiang, 2005; Bouchard-cˆot´e et al., 2009) that might further improve performance. Sparse structured inference. Various work has explored sparse alternatives to soft-weighting. Sparsemax (Martins and Astudillo, 2016) is a deterministic sparse alternative to the softmax, and Gumbel-Softmax (Jang et al., 2017) uses the categorical reparameterization trick to sample a discrete value during training. Both have attractive properties but alone would not be sufficient for overcoming local errors in S-DIORA. Nonetheless, these options would be worth exploring for unsupervised parsing when training with more data"
2020.emnlp-main.392,P19-1228,0,0.175854,"termediate features for many NLP pipelines (He et al., 2018; Strubell et al., 2018), as a soft constraint (Rush and Collins, 2012), a hard constraint (Lee et al., 2019b), or in multi-task learning with syntactic scaffolds (Swayamdipta et al., 2018). Syntactic inductive bias can also improve generalization of deep learning models (Kuncoro et al., 2020). These results have motivated researchers to pursue unsupervised parsing, with the hope of training syntax-dependent models on large amounts of data without annotation (Klein and Manning, 2002; Bod, 2006; Ponvert et al., 2011; Shen et al., 2019; Kim et al., 2019, inter alia). Of these models, we focus on the deep insideoutside recursive autoencoder (DIORA; Drozdov et al. 2019a). DIORA encodes sentences in a procedure resembling the inside-outside algorithm (Baker, 1979), which allows it to induce syntactic tree structures for input sentences without access to labeled training data, and achieves near stateof-the-art results on unsupervised constituency parsing. DIORA resembles pre-trained language models, such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), in that it is trained with a self-supervised blank-filling objective on large amo"
2020.emnlp-main.392,P18-1249,0,0.276204,"train DIORA for supervised parsing using a binarized version of the ‘ground truth’ parse trees from the English Penn Treebank (Marcus et al., 1993). The training procedure is done by optimizing the structured SVM loss: sup Jtree = max(0, S(ˆ y) S(y) + 1), where S(ˆ y ) is the score of the maximal tree and S(y) is the score of the ‘ground truth’. 1 Since the outside vector is used for word prediction, tricks associated with the inside-outside algorithm using only backpropagation of the inside-pass (Eisner, 2016) are not obviously applicable, if at all. 4834 We use the off-the-shelf parser from Kitaev and Klein (2018) as a baseline and the results are shown in Table 1. Although DIORA is strong in unsupervised parsing, the supervised parsing results are not as competitive with the baseline as we had expected, and lead us to consider deeply why this might be the case. We posit the low performance in supervised parsing is due to DIORA’s inability to effectively recover from local errors. Predicting trees in DIORA is exact — you are guaranteed to find the highest scoring tree given the scalar values associated with each span, but there is a weakness when assigning the scalar values. Specifically, the scalar va"
2020.emnlp-main.392,P14-5010,0,0.00331042,"iminaries: Constituency Parsing We measure the performance of our changes via unsupervised and supervised parsing on the test set of the WSJ Penn Treebank (Marcus et al., 1993).4 All models (S-DIORA and baselines) output unlabeled binary trees5 and are evaluated via sentence level F1 (S-F1). For supervised constituency parsing we use the off-the-shelf parser from Kitaev and Klein (2018) as a baseline to compare against DIORA and SDIORA. For training we use the parse trees from training split of PTB and evaluate using the validation data. We binarize the ground truth using the Stanford parser (Manning et al., 2014) and train for 10 epochs. Results against the binary trees and original n-ary (ingoring labels in both cases) is shown in Table 1. Both DIORA and S-DIORA are trained from random initialization using the structured SVM loss from Kitaev and Klein (2018). We see that DIORA is not competitive with the Kitaev and Klein (2018) parser, and attribute this to wash out and its inability to recover from errors. For S-DIORA we train and evaluate with 2 {1, 2, 3, 4} and results are shown in Table 2. We see, unsurprisingly, that regardless of the beamsize at training, when = 1 at test time the performance i"
2020.emnlp-main.392,D19-1587,0,0.0247266,"nd VP segment recall, but also has the fewest VP-A errors. This suggests that errors related to segment recall are likely folded into a different category such as PP attachment. The right-skewed model XLNet =1.5 substantially improves over XLNet =0 in SBAR recall and is comparable in this category with S-DIORA. Interestingly, although increasing the size of 8 To compute phrase embeddings, we follow the procedure from (Kitaev and Klein, 2018) which concatenates the forward and backward LSTM vectors at the beginning and end of each phrase. To compute vector similarity we follow the procedure in Kobayashi et al. (2019) which uses ELMo sentence embeddings for RST parsing — rather than document level parsing, our work pertains to sentence level parsing. in S-DIORA results in a near monotonic improvement in all categories (with some minor exceptions), S-DIORA shows a very different error profile when compared to pre-trained LMs, despite having a better S-F1. For instance, the pre-trained LMs make fewer coordinations errors, and perform better with adverbial phrases (ADVP), than any version of S-DIORA. In future work, it may be useful to understand why parser performance does not increase monotonically. Perhaps"
2020.emnlp-main.392,P03-1040,0,0.11365,"Missing"
2020.emnlp-main.392,D12-1096,0,0.10557,"or unsupervised parsing. In addition, we present a new baseline demonstrating that pretrained language models are better at unsupervised parsing than previously known. 5.1 Linguistic Error Analysis Parsing F1 is useful to quickly compare performance between parsers, and previous work in unsupervised parsing often also report segment recall to give a sense of which phrases are most often captured in the output. To provide an even more thorough treatment of linguistic errors we add labels to the parse trees using the parser from Kitaev and Klein (2018) and then run the Berkeley parser analyzer (Kummerfeld et al., 2012). This latter tool classifies mistakes for each predicted tree by the type of phrases (or patterns like coordination) involved in the error, allowing analysis of the types of errors being made by a model. In Table 4 we show the parsing F1, segment recall, and error counts as determined by the analyzer. By segment recall, we see that C-PCFG outperforms DIORA in segment recall for NP and PP, explaining its high S-F1. The linguistic analysis tells a slightly different story — C-PCFG makes less errors associated with NP internal structure and clause attachment, but substantially more errors associ"
2020.emnlp-main.392,2020.tacl-1.50,0,0.0181253,"e to recover and the important subtree is washed out (represented in light gray). Our method, S-DIORA (bottom row) can recover from errors, and the desired tree ends up at the top of the beam in the right-most column. Introduction Syntactic parse trees are valuable intermediate features for many NLP pipelines (He et al., 2018; Strubell et al., 2018), as a soft constraint (Rush and Collins, 2012), a hard constraint (Lee et al., 2019b), or in multi-task learning with syntactic scaffolds (Swayamdipta et al., 2018). Syntactic inductive bias can also improve generalization of deep learning models (Kuncoro et al., 2020). These results have motivated researchers to pursue unsupervised parsing, with the hope of training syntax-dependent models on large amounts of data without annotation (Klein and Manning, 2002; Bod, 2006; Ponvert et al., 2011; Shen et al., 2019; Kim et al., 2019, inter alia). Of these models, we focus on the deep insideoutside recursive autoencoder (DIORA; Drozdov et al. 2019a). DIORA encodes sentences in a procedure resembling the inside-outside algorithm (Baker, 1979), which allows it to induce syntactic tree structures for input sentences without access to labeled training data, and achiev"
2020.emnlp-main.392,N15-1067,0,0.0192985,"what strategy to follow Figure 5: As the beam-size increases, S-DIORA’s output tends to match the ground truth more closely. The displayed output, top to bottom, are from PTB, then S-DIORAP T B with = 1, 3, 5 respectively. vised parsers do not need error recovery because they use models with rich features and model each span score independently (Cross and Huang, 2016; Stern et al., 2017; Kitaev and Klein, 2018; Mrini et al., 2019). Previous research has attempted to achieve the “best of both worlds” by distilling a strong model for supervised parsing via an unsupervised model’s output (Le and Zuidema, 2015). These approaches are closely related to fast and accurate parsing. More accurate models tend to use richer features that are more expensive to compute, influencing researchers to find efficient techniques to offset the loss in speed (Vieira and Eisner, 2017). In this paper, we use the most simple approach to learn to parse with the capability to recover from local errors by maintaining a beam of size at each cell in the chart. S-DIORA is often faster and discovers better trees than DIORA, but there are other methods for extracting lists of best or plausible parses (Resnik, 1992; Roark and Jo"
2020.emnlp-main.392,J93-2004,0,0.0873489,"h it is still possible to make an error by ignoring a potentially important subtree. Fortunately, this can be alleviated by adding a beam to each cell of the chart, allowing multiple subtrees over any span to be considered. The key benefit of our modification is that error recovery is easily possible, where previously the vector serves as a bottleneck that makes error recovery difficult or impossible. We initialize an instance of S-DIORA using the previously released DIORA model, then finetune before evaluating on the target domain, constituency parse trees from the English WSJ Treebank (PTB, Marcus et al. 1993). In one experimental setting, we assume no access to the evaluation domain and use a subset of DIORA’s training data, a concatenation of the SNLI (Bowman et al., 2015) and Multi-NLI (Williams et al., 2018b) corpora (hereinafter NLI). In the other setting, we assume access to raw text in the target domain, parse tree labels excluded. In both cases, we see S-DIORA improves on the original DIORA performance by at least 4 F1, and training on the PTB raw text leads to more than 3 F1 over the previous state of the art in constituency parsing. In summary, the main contributions in this paper are: (a"
2020.emnlp-main.392,2020.iwpt-1.11,0,0.0731426,"Missing"
2020.emnlp-main.392,D10-1120,0,0.10064,"Missing"
2020.emnlp-main.392,P92-1017,0,0.723093,"Missing"
2020.emnlp-main.392,N18-1202,1,0.752702,"arge amounts of data without annotation (Klein and Manning, 2002; Bod, 2006; Ponvert et al., 2011; Shen et al., 2019; Kim et al., 2019, inter alia). Of these models, we focus on the deep insideoutside recursive autoencoder (DIORA; Drozdov et al. 2019a). DIORA encodes sentences in a procedure resembling the inside-outside algorithm (Baker, 1979), which allows it to induce syntactic tree structures for input sentences without access to labeled training data, and achieves near stateof-the-art results on unsupervised constituency parsing. DIORA resembles pre-trained language models, such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), in that it is trained with a self-supervised blank-filling objective on large amounts of unlabeled data. DIORA is a strong unsupervised parser in spite of its locally greedy nature. DIORA works by encoding all subtrees covering a particular span as separate vectors, and then computing a weighted average of these vectors — DIORA uses this averaged vector later in the dynamic program to represent the entire forest of trees covering a span. DIORA computes a score for each subtree; intuitively, a subtree’s score affects how strongly it is 4832 Proceedings of the 20"
2020.emnlp-main.392,P11-1108,0,0.12071,"ion Syntactic parse trees are valuable intermediate features for many NLP pipelines (He et al., 2018; Strubell et al., 2018), as a soft constraint (Rush and Collins, 2012), a hard constraint (Lee et al., 2019b), or in multi-task learning with syntactic scaffolds (Swayamdipta et al., 2018). Syntactic inductive bias can also improve generalization of deep learning models (Kuncoro et al., 2020). These results have motivated researchers to pursue unsupervised parsing, with the hope of training syntax-dependent models on large amounts of data without annotation (Klein and Manning, 2002; Bod, 2006; Ponvert et al., 2011; Shen et al., 2019; Kim et al., 2019, inter alia). Of these models, we focus on the deep insideoutside recursive autoencoder (DIORA; Drozdov et al. 2019a). DIORA encodes sentences in a procedure resembling the inside-outside algorithm (Baker, 1979), which allows it to induce syntactic tree structures for input sentences without access to labeled training data, and achieves near stateof-the-art results on unsupervised constituency parsing. DIORA resembles pre-trained language models, such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), in that it is trained with a self-supervised"
2020.emnlp-main.392,C92-1032,0,0.0618314,"put (Le and Zuidema, 2015). These approaches are closely related to fast and accurate parsing. More accurate models tend to use richer features that are more expensive to compute, influencing researchers to find efficient techniques to offset the loss in speed (Vieira and Eisner, 2017). In this paper, we use the most simple approach to learn to parse with the capability to recover from local errors by maintaining a beam of size at each cell in the chart. S-DIORA is often faster and discovers better trees than DIORA, but there are other methods for extracting lists of best or plausible parses (Resnik, 1992; Roark and Johnson, 1999; Charniak and Johnson, 2005; Huang and Chiang, 2005; Bouchard-cˆot´e et al., 2009) that might further improve performance. Sparse structured inference. Various work has explored sparse alternatives to soft-weighting. Sparsemax (Martins and Astudillo, 2016) is a deterministic sparse alternative to the softmax, and Gumbel-Softmax (Jang et al., 2017) uses the categorical reparameterization trick to sample a discrete value during training. Both have attractive properties but alone would not be sufficient for overcoming local errors in S-DIORA. Nonetheless, these options w"
2020.emnlp-main.392,P99-1054,0,0.393799,"idema, 2015). These approaches are closely related to fast and accurate parsing. More accurate models tend to use richer features that are more expensive to compute, influencing researchers to find efficient techniques to offset the loss in speed (Vieira and Eisner, 2017). In this paper, we use the most simple approach to learn to parse with the capability to recover from local errors by maintaining a beam of size at each cell in the chart. S-DIORA is often faster and discovers better trees than DIORA, but there are other methods for extracting lists of best or plausible parses (Resnik, 1992; Roark and Johnson, 1999; Charniak and Johnson, 2005; Huang and Chiang, 2005; Bouchard-cˆot´e et al., 2009) that might further improve performance. Sparse structured inference. Various work has explored sparse alternatives to soft-weighting. Sparsemax (Martins and Astudillo, 2016) is a deterministic sparse alternative to the softmax, and Gumbel-Softmax (Jang et al., 2017) uses the categorical reparameterization trick to sample a discrete value during training. Both have attractive properties but alone would not be sufficient for overcoming local errors in S-DIORA. Nonetheless, these options would be worth exploring f"
2020.emnlp-main.392,D18-1412,0,0.0193458,"ially important subtree when recursively computing the vector for a target tree, it is difficult or impossible to recover and the important subtree is washed out (represented in light gray). Our method, S-DIORA (bottom row) can recover from errors, and the desired tree ends up at the top of the beam in the right-most column. Introduction Syntactic parse trees are valuable intermediate features for many NLP pipelines (He et al., 2018; Strubell et al., 2018), as a soft constraint (Rush and Collins, 2012), a hard constraint (Lee et al., 2019b), or in multi-task learning with syntactic scaffolds (Swayamdipta et al., 2018). Syntactic inductive bias can also improve generalization of deep learning models (Kuncoro et al., 2020). These results have motivated researchers to pursue unsupervised parsing, with the hope of training syntax-dependent models on large amounts of data without annotation (Klein and Manning, 2002; Bod, 2006; Ponvert et al., 2011; Shen et al., 2019; Kim et al., 2019, inter alia). Of these models, we focus on the deep insideoutside recursive autoencoder (DIORA; Drozdov et al. 2019a). DIORA encodes sentences in a procedure resembling the inside-outside algorithm (Baker, 1979), which allows it to"
2020.emnlp-main.392,J11-4006,0,0.0318519,"and PTB with no improvements in unsupervised parsing). Most benefit is achieved using = 3, although in some cases it helps to increase it further (see Figure 5). Increasing the beam also helps with different classes of errors. In Figure 4 we see the benefit in sentences with tricky coordination. 5.4 Labeled Parsing We evaluate the labeled trees from §5.1, and the best performing S-DIORA model achieves 80.7 9 The NP-I category covers missed gold phrases within large noun phrases. In general, much of NP structure in PTB is not annotated, and in future work it is worth using the data provided by Vadas and Curran (2011) to investigate NP structure, as determined by unsupervised parsers, more thoroughly. He was punched and kicked by one player and the other broke his jaw He was punched and kicked by one player and the other broke his jaw Figure 4: Two sentences where beam-search helps with ambiguous coordination structures, correctly nesting noun phrases (top) and getting better coordination of verb phrases (bottom). The displayed parse tree output, top to bottom, are from PTB, then S-DIORAP T B with = 1, 3 respectively. labeled parsing F1 on the validation data (72.3 recall, 91.2 precision, and 11.7 complete"
2020.emnlp-main.392,Q17-1019,0,0.0246175,"rror recovery because they use models with rich features and model each span score independently (Cross and Huang, 2016; Stern et al., 2017; Kitaev and Klein, 2018; Mrini et al., 2019). Previous research has attempted to achieve the “best of both worlds” by distilling a strong model for supervised parsing via an unsupervised model’s output (Le and Zuidema, 2015). These approaches are closely related to fast and accurate parsing. More accurate models tend to use richer features that are more expensive to compute, influencing researchers to find efficient techniques to offset the loss in speed (Vieira and Eisner, 2017). In this paper, we use the most simple approach to learn to parse with the capability to recover from local errors by maintaining a beam of size at each cell in the chart. S-DIORA is often faster and discovers better trees than DIORA, but there are other methods for extracting lists of best or plausible parses (Resnik, 1992; Roark and Johnson, 1999; Charniak and Johnson, 2005; Huang and Chiang, 2005; Bouchard-cˆot´e et al., 2009) that might further improve performance. Sparse structured inference. Various work has explored sparse alternatives to soft-weighting. Sparsemax (Martins and Astudill"
2020.emnlp-main.392,Q18-1019,1,0.887078,"Missing"
2020.emnlp-main.392,N18-1101,0,0.0578444,"Missing"
2020.emnlp-main.392,P19-1180,0,0.0753977,"Missing"
2020.emnlp-main.392,P05-1044,0,0.245577,"Missing"
2020.emnlp-main.392,P09-1009,0,0.11279,"Missing"
2020.emnlp-main.392,P17-1076,0,0.0229894,"the outset the tobacco industry has been uncertain as to what strategy to follow From the outset the tobacco industry has been uncertain as to what strategy to follow From the outset the tobacco industry has been uncertain as to what strategy to follow Figure 5: As the beam-size increases, S-DIORA’s output tends to match the ground truth more closely. The displayed output, top to bottom, are from PTB, then S-DIORAP T B with = 1, 3, 5 respectively. vised parsers do not need error recovery because they use models with rich features and model each span score independently (Cross and Huang, 2016; Stern et al., 2017; Kitaev and Klein, 2018; Mrini et al., 2019). Previous research has attempted to achieve the “best of both worlds” by distilling a strong model for supervised parsing via an unsupervised model’s output (Le and Zuidema, 2015). These approaches are closely related to fast and accurate parsing. More accurate models tend to use richer features that are more expensive to compute, influencing researchers to find efficient techniques to offset the loss in speed (Vieira and Eisner, 2017). In this paper, we use the most simple approach to learn to parse with the capability to recover from local errors"
2020.emnlp-main.525,P18-1082,0,0.44985,"writing (Roemmele and Gordon, 2015). To spur research in this area, we partner with STO RIUM ,1 an online collaborative storytelling platform, to introduce a new dataset and evaluation methodology for story generation. The open-endedness of story writing does not just pose a barrier to humans—it also presents a challenge for building and evaluating computational models. Prior work relies on datasets that are either too artificial to generalize to longform stories, such as the crowdsourced ROCStories (Mostafazadeh et al., 2016) corpus, or too unconstrained, as in the r/writingprompts dataset (Fan et al., 2018), which pairs mediumlength stories with short prompts. Furthermore, lack of standardized evaluation makes measuring progress difficult: most prior work evaluates outputs using a combination of simple automatic metrics not designed for long-form creative text generation (e.g., BLEU and ROUGE against a single reference) and crowdsourced ratings (McIntyre and Lapata, 2009; Yao et al., 2019; Fan et al., 2019) that preclude evaluating long-form narratives. We address these limitations by (1) collecting a dataset of stories (Section 2) containing finegrained structural annotations written in natural"
2020.emnlp-main.525,P19-1254,0,0.371856,"are either too artificial to generalize to longform stories, such as the crowdsourced ROCStories (Mostafazadeh et al., 2016) corpus, or too unconstrained, as in the r/writingprompts dataset (Fan et al., 2018), which pairs mediumlength stories with short prompts. Furthermore, lack of standardized evaluation makes measuring progress difficult: most prior work evaluates outputs using a combination of simple automatic metrics not designed for long-form creative text generation (e.g., BLEU and ROUGE against a single reference) and crowdsourced ratings (McIntyre and Lapata, 2009; Yao et al., 2019; Fan et al., 2019) that preclude evaluating long-form narratives. We address these limitations by (1) collecting a dataset of stories (Section 2) containing finegrained structural annotations written in natural language, and (2) providing a platform for evaluating models in a machine-in-the-loop setting by allowing real STORIUM authors to interact with the generated stories (Section 4). Our dataset contains nearly 6K longform stories (125M tokens) written by STORIUM authors, each of which is broken into discourse-level scene entries annotated with narrative elements, such as character goals or abilities. Condit"
2020.emnlp-main.525,2020.emnlp-main.5,0,0.0447323,"Missing"
2020.emnlp-main.525,N16-1098,0,0.121259,"Missing"
2020.emnlp-main.525,W18-1505,1,0.840763,"AI-guided narratives are prevalent enough that we manually exclude these games from our experiments as they 6477 6 Related Work Our work builds on prior research in computational modeling for story generation. Early narrative prose generation systems (Meehan, 1977; Callaway and Lester, 2001; Riedl and Young, 2004) relied on graph-based planning formalisms and custom rules to structure their narratives, while story graphs have been used for interactive storytelling (Riedl and Bulitko, 2013). More recent work uses deep learning to generate stories by training neural models with limited context (Peng et al., 2018; Fan et al., 2018; Goldfarb-Tarrant et al., 2019) and structured knowledge, either external (Mao et al., 2019; Guan et al., 2020; Goldfarb-Tarrant et al., 2020) or derived (Yao et al., 2019; Fan et al., 2019). Compared to the datasets studied in those works, our STORIUM dataset contains much longer stories with built-in structural annotations written in natural language in the form of cards (Table 2). Our work connects more closely to existing machine-in-the-loop storytelling work (Roemmele and Gordon, 2015; Samuel et al., 2016; Clark et al., 2018), in which systems work in concert with users"
2020.emnlp-main.525,D19-1509,0,0.0699079,"Missing"
2020.emnlp-main.525,S18-2024,0,0.0955944,"2.7 2.8 3.2 L 3.8 3.9 3.6 4.1 R 2.3 2.3 2.4 2.7 Ppl 25.1 22.4 22.9 21.0 Jdg 90 77 62 85 Table 4: Exploratory experiments indicate optimally packing tokens using Cassowary (Cas), and including more history (His) is key to achieving low perplexity (Ppl), along with high fluency (F), coherence (C), likability (L), and relevance (R) based on a number of user judgments (Jdg). 4 A Machine-in-the-Loop Evaluation Platform The inadequacies of existing human and automatic evaluation methods are a major roadblock for story generation research. Automatic evaluations correlate weakly with human judgments (Sagarkar et al., 2018), and these judgments are obtained from crowd workers who are not invested in the narratives they are assessing. These concerns are magnified with STORIUM, as the story contexts are far too long for crowd workers to reliably evaluate (Section 5). In this section, we propose an improved evaluation methodology by directly integrating our models onto the STORIUM platform. This allows story authors to query a machine (Clark et al., 2018) for suggestions during the process of writing their own stories. We develop a new evaluation metric, User Story Edit Ratings (USER), computed on top of the edits"
2020.emnlp-main.525,K19-1079,0,0.184525,"lu 0.28 0.40 0.28 0.38 — — — — — — Coh 0.55 0.57 0.35 0.55 0.54 0.61 — — — — USER 0.51 0.39 0.34 0.35 0.13† 0.23 0.25 0.36 — — Rating 2.55 2.47 3.32 3.21 3.96 3.76 3.41 2.96 15.63 9.86 Table 5: Despite its low rating, relevance is clearly important as indicated by the moderately strong Pearson’s r correlations (first four columns) with USER and the remaining human judgments. All correlations are significant (p &lt; 0.01), except those indicated by † (p &gt; 0.05). p = 0.9.11 The sampling parameters, such as the k in top-k sampling, can significantly affect output quality of story generation models (See et al., 2019), so we choose values that worked well in prior work (Qin et al., 2019).12 Interestingly, while Holtzman et al. (2020) show that nucleus sampling improves over top-k sampling on measures like repetition, STORIUM users clearly prefer the top-k variant across all categories (last column of Table 5). We collect roughly 200 feedback ratings and 175 edits for each model over a span of three months beginning in late February 2020. We discover that both configurations score best on fluency and worst on relevance. This is unsurprising as (1) GPT-2 is known to produce fluent text and (2) the complex an"
2020.emnlp-main.525,2020.acl-main.704,0,0.0571905,"Missing"
2020.emnlp-main.55,S16-1081,0,0.338705,"bles denoising autoencoders (Vincent et al., 2008; Lample et al., 2018, DAE): fpara acts as a i semantic preserving noise function; finv reconstructs the input. 5 i We fine-tune a separate GPT-2 model finv per style. Section 5 shows that this outperforms a single inverse paraphraser shared across all styles with style input. Current state of style transfer evaluation We conduct a survey of 23 previously-published style transfer papers (more details in Appendix A.9), which reveals three common 739 6 This model achieves strong performance on semantic textual similarity (STS) SemEval benchmarks (Agirre et al., 2016). We remove all pairs with a score lower than 0.7. properties on which style transfer systems are evaluated. Here, we discuss how prior work implements evaluations for each of these properties and propose improved implementations to address some of their downsides. Transfer accuracy (ACC): Given an output sentence ¯ sj and a target style j, a common way of measuring transfer success is to train a classifier to identify the style of a transferred sentence and report its accuracy ACC on generated sentences (i.e., whether ¯ sj has a predicted style of j). 14 of 23 surveyed papers implement this s"
2020.emnlp-main.55,P19-1122,1,0.870742,"Missing"
2020.emnlp-main.55,E06-1032,0,0.0439219,"ly more reliable ACC evaluation.7 Semantic similarity (SIM): A style transfer system can achieve high ACC scores without maintaining the semantics of the input sentence, which motivates also measuring how much a transferred sentence deviates in meaning from the input. 15 / 23 surveyed papers use n-gram metrics like BLEU (Papineni et al., 2002) against reference sentences, often along with self-BLEU with the input, to evaluate semantic similarity. Using BLEU in this way has many problems, including (1) unreliable correlations between n-gram overlap and human evaluations of semantic similarity (Callison-Burch et al., 2006), (2) discouraging output diversity (Wieting et al., 2019), and (3) not upweighting important semantic words over other words (Wieting et al., 2019; Wang et al., 2020). These issues motivate us to measure semantic similarity using the subword embedding-based SIM model of Wieting et al. (2019), which performs well on semantic textual similarity (STS) benchmarks in SemEval workshops (Agirre et al., 2016).8 Fluency (FL): A system that produces ungrammatical outputs can still achieve high scores on both ACC and SIM , motivating a separate measure for fluency. Only 10 out of 23 surveyed papers did"
2020.emnlp-main.55,N19-1263,0,0.0611724,"Missing"
2020.emnlp-main.55,P18-1080,0,0.0309922,"h of its components contribute most to its improvements over baselines. Overall, these ablations validate the importance of both paraphrasing and pretraining for style transfer. STRAP 12 We use the implementations of both UNMT and DLSM made publicly available by He et al. (2020), and we verify that their UNMT model performs on par with reported sentiment transfer numbers in Subramanian et al. (2019). The original code of Subramanian et al. (2019) has not been open-sourced. 13 Results with other metrics such as BLEU, as well as comparisons against several other baselines like Li et al. (2018); Prabhumoye et al. (2018); Luo et al. (2019); Dai et al. (2019); Sudhakar et al. (2019) are provided in Appendix A.5. STRAP significantly outperforms all prior work. STRAP – Inf. PP – Mult. PP – Div. PP – GPT2 GPT2-md GPT2-sm Paraphrase diversity improves ACC: How critical is diversity in the paraphrase generation step? While our implementation of fpara is trained on data that is heavily-filtered to promote diversity, we also build a non-diverse paraphrase model by removing this diversity filtering of PARA NMT50M but keeping all other experimental settings identical. In Table 3, the –Div. PP rows show a drop in ACC ac"
2020.emnlp-main.55,N18-1012,0,0.0559566,"contain parallel data, we only use it to automatically evaluate our model outputs; for training, we follow prior work by using the non-parallel trainvalidation-test splits from He et al. (2020). The Shakespeare author imitation dataset (Xu et al., 2012) contains 37k training sentences from two styles — William Shakespeare’s original plays, and their modernized versions. Shakespeare’s plays are written in Early Modern English, which has a significantly different lexical (e.g., thou instead of you) and syntactic distribution compared to modern English. Our second dataset is Formality transfer (Rao and Tetreault, 2018), which contains 105k sentences, also from two styles. Sentences are written either in formal or informal modern English. Unlike formal sentences, informal sentences tend to have more misspellings, short forms (u instead of you), and non-standard usage of punctuation. 4.2 Comparisons against prior work We compare STRAP on the Shakespeare / Formality datasets against the following baselines: X ACC(x) · SIM(x) · FL(x) |X| x∈X where x is a sentence from a test corpus X. We treat ACC and FL at a sentence level as a binary judgement, ensuring incorrectly classified or disfluent sentences are automa"
2020.emnlp-main.55,P18-2031,0,0.0594569,"Missing"
2020.emnlp-main.55,2020.acl-main.704,0,0.0263789,"ed real-world applications: Pang (2019) argue that se1 We use the quasi-paraphrase definition of semantic equivalence from Bhagat and Hovy (2013) throughout this paper. We loosely define style as patterns in lexical and syntactic choice within the space of quasi-paraphrases. mantic preservation is critical for author obfuscation (Shetty et al., 2018), data augmentation (Xie et al., 2019; Kaushik et al., 2020), text simplification (Xu et al., 2015), writing assistance (Heidorn, 2000). Moreover, semantic preservation (via paraphrases) has several applications like better translation evaluation (Sellam et al., 2020; Freitag et al., 2020) and adversarial defenses (Iyyer et al., 2018). We propose to improve semantic preservation in style transfer by modeling the task as a controlled paraphrase generation problem. Our unsupervised method (Style Transfer via Paraphrasing, or STRAP) requires no parallel data between different styles and proceeds in three simple stages: 1. Create pseudo-parallel data by feeding sentences from different styles through a diverse paraphrase model (Figure 1, left). 2. Train style-specific inverse paraphrase models that convert these paraphrased sentences back into the original st"
2020.emnlp-main.55,P16-1009,0,0.0828612,"Missing"
2020.emnlp-main.55,Q15-1021,0,0.0214002,"that carry sentiment) warp both stylistic and semantic properties of a sentence (Preotiuc-Pietro et al., 2016). Attribute transfer has been criticized for its limited real-world applications: Pang (2019) argue that se1 We use the quasi-paraphrase definition of semantic equivalence from Bhagat and Hovy (2013) throughout this paper. We loosely define style as patterns in lexical and syntactic choice within the space of quasi-paraphrases. mantic preservation is critical for author obfuscation (Shetty et al., 2018), data augmentation (Xie et al., 2019; Kaushik et al., 2020), text simplification (Xu et al., 2015), writing assistance (Heidorn, 2000). Moreover, semantic preservation (via paraphrases) has several applications like better translation evaluation (Sellam et al., 2020; Freitag et al., 2020) and adversarial defenses (Iyyer et al., 2018). We propose to improve semantic preservation in style transfer by modeling the task as a controlled paraphrase generation problem. Our unsupervised method (Style Transfer via Paraphrasing, or STRAP) requires no parallel data between different styles and proceeds in three simple stages: 1. Create pseudo-parallel data by feeding sentences from different styles t"
2020.emnlp-main.55,C12-1177,0,0.0270577,".1 Datasets We focus exclusively on semantics-preserving style transfer tasks, which means that we do not evaluate on attribute transfer datasets such as sentiment, gender, and political transfer. Specifically, we use two standard benchmark datasets for Shakespeare author imitation and formality transfer to compare STRAP against prior work. While both datasets contain parallel data, we only use it to automatically evaluate our model outputs; for training, we follow prior work by using the non-parallel trainvalidation-test splits from He et al. (2020). The Shakespeare author imitation dataset (Xu et al., 2012) contains 37k training sentences from two styles — William Shakespeare’s original plays, and their modernized versions. Shakespeare’s plays are written in Early Modern English, which has a significantly different lexical (e.g., thou instead of you) and syntactic distribution compared to modern English. Our second dataset is Formality transfer (Rao and Tetreault, 2018), which contains 105k sentences, also from two styles. Sentences are written either in formal or informal modern English. Unlike formal sentences, informal sentences tend to have more misspellings, short forms (u instead of you),"
2020.emnlp-main.635,N18-1202,1,0.703049,"Missing"
2020.emnlp-main.635,silveira-etal-2014-gold,0,0.0307482,"Missing"
2020.emnlp-main.635,W17-2623,1,0.849412,"k embeddings for the 33 NLP tasks we study, along with a codebase that computes task embeddings for new tasks and identifies source tasks that will likely yield positive transferability.2 2 Task text classification/regression (CR) SNLI (Bowman et al., 2015) MNLI (Williams et al., 2018) QQP (Iyer et al., 2017) QNLI (Wang et al., 2019b) SST-2 (Socher et al., 2013) SciTail (Khot et al., 2018) CoLA (Warstadt et al., 2019) STS-B (Cer et al., 2017) MRPC (Dolan and Brockett, 2005) RTE (Dagan et al., 2005, et seq.) WNLI (Levesque, 2011) question answering (QA) SQuAD-2 (Rajpurkar et al., 2018) NewsQA (Trischler et al., 2017) HotpotQA (Yang et al., 2018) SQuAD-1 (Rajpurkar et al., 2016) DuoRC-p (Saha et al., 2018) DuoRC-s (Saha et al., 2018) DROP (Dua et al., 2019) WikiHop (Welbl et al., 2018) BoolQ (Clark et al., 2019) ComQA (Abujabal et al., 2019) CQ (Bao et al., 2016) sequence labeling (SL) ST (Bjerva et al., 2016) CCG (Hockenmaier and Steedman, 2007) Parent (Liu et al., 2019a) GParent (Liu et al., 2019a) GGParent (Liu et al., 2019a) POS-PTB (Marcus et al., 1993) GED (Yannakoudakis et al., 2011) NER (Tjong Kim Sang and De Meulder, 2003) POS-EWT (Silveira et al., 2014) Conj (Ficler and Goldberg, 2016) Chunk (Tjo"
2020.emnlp-main.635,P19-1439,0,0.133652,"Missing"
2020.emnlp-main.635,N18-1101,0,0.0343265,"embeddings for selecting source tasks (via simple cosine similarity) that effectively transfer to a given target task. To the best of our knowledge, this is the first work that builds explicit representations of NLP tasks to investigate transferability. We publicly release our task library, which consists of pretrained models and task embeddings for the 33 NLP tasks we study, along with a codebase that computes task embeddings for new tasks and identifies source tasks that will likely yield positive transferability.2 2 Task text classification/regression (CR) SNLI (Bowman et al., 2015) MNLI (Williams et al., 2018) QQP (Iyer et al., 2017) QNLI (Wang et al., 2019b) SST-2 (Socher et al., 2013) SciTail (Khot et al., 2018) CoLA (Warstadt et al., 2019) STS-B (Cer et al., 2017) MRPC (Dolan and Brockett, 2005) RTE (Dagan et al., 2005, et seq.) WNLI (Levesque, 2011) question answering (QA) SQuAD-2 (Rajpurkar et al., 2018) NewsQA (Trischler et al., 2017) HotpotQA (Yang et al., 2018) SQuAD-1 (Rajpurkar et al., 2016) DuoRC-p (Saha et al., 2018) DuoRC-s (Saha et al., 2018) DROP (Dua et al., 2019) WikiHop (Welbl et al., 2018) BoolQ (Clark et al., 2019) ComQA (Abujabal et al., 2019) CQ (Bao et al., 2016) sequence lab"
2020.emnlp-main.635,D18-1259,0,0.028202,"we study, along with a codebase that computes task embeddings for new tasks and identifies source tasks that will likely yield positive transferability.2 2 Task text classification/regression (CR) SNLI (Bowman et al., 2015) MNLI (Williams et al., 2018) QQP (Iyer et al., 2017) QNLI (Wang et al., 2019b) SST-2 (Socher et al., 2013) SciTail (Khot et al., 2018) CoLA (Warstadt et al., 2019) STS-B (Cer et al., 2017) MRPC (Dolan and Brockett, 2005) RTE (Dagan et al., 2005, et seq.) WNLI (Levesque, 2011) question answering (QA) SQuAD-2 (Rajpurkar et al., 2018) NewsQA (Trischler et al., 2017) HotpotQA (Yang et al., 2018) SQuAD-1 (Rajpurkar et al., 2016) DuoRC-p (Saha et al., 2018) DuoRC-s (Saha et al., 2018) DROP (Dua et al., 2019) WikiHop (Welbl et al., 2018) BoolQ (Clark et al., 2019) ComQA (Abujabal et al., 2019) CQ (Bao et al., 2016) sequence labeling (SL) ST (Bjerva et al., 2016) CCG (Hockenmaier and Steedman, 2007) Parent (Liu et al., 2019a) GParent (Liu et al., 2019a) GGParent (Liu et al., 2019a) POS-PTB (Marcus et al., 1993) GED (Yannakoudakis et al., 2011) NER (Tjong Kim Sang and De Meulder, 2003) POS-EWT (Silveira et al., 2014) Conj (Ficler and Goldberg, 2016) Chunk (Tjong Kim Sang and Buchholz, 200"
2020.emnlp-main.635,P11-1019,0,0.032339,"RTE (Dagan et al., 2005, et seq.) WNLI (Levesque, 2011) question answering (QA) SQuAD-2 (Rajpurkar et al., 2018) NewsQA (Trischler et al., 2017) HotpotQA (Yang et al., 2018) SQuAD-1 (Rajpurkar et al., 2016) DuoRC-p (Saha et al., 2018) DuoRC-s (Saha et al., 2018) DROP (Dua et al., 2019) WikiHop (Welbl et al., 2018) BoolQ (Clark et al., 2019) ComQA (Abujabal et al., 2019) CQ (Bao et al., 2016) sequence labeling (SL) ST (Bjerva et al., 2016) CCG (Hockenmaier and Steedman, 2007) Parent (Liu et al., 2019a) GParent (Liu et al., 2019a) GGParent (Liu et al., 2019a) POS-PTB (Marcus et al., 1993) GED (Yannakoudakis et al., 2011) NER (Tjong Kim Sang and De Meulder, 2003) POS-EWT (Silveira et al., 2014) Conj (Ficler and Goldberg, 2016) Chunk (Tjong Kim Sang and Buchholz, 2000) Exploring task transferability To shed light on the transferability between different NLP tasks,3 we perform an empirical study with 33 tasks across three broad classes of problems: text classification/regression (CR), question answering (QA), and sequence labeling (SL).4 In each experiment, we follow the STILTs pipeline of Phang et al. (2018) by taking a pretrained BERT model,5 fine-tuning it on an intermediate source task, and then fine-tuning"
2020.emnlp-main.635,Q19-1040,0,0.0461605,"our knowledge, this is the first work that builds explicit representations of NLP tasks to investigate transferability. We publicly release our task library, which consists of pretrained models and task embeddings for the 33 NLP tasks we study, along with a codebase that computes task embeddings for new tasks and identifies source tasks that will likely yield positive transferability.2 2 Task text classification/regression (CR) SNLI (Bowman et al., 2015) MNLI (Williams et al., 2018) QQP (Iyer et al., 2017) QNLI (Wang et al., 2019b) SST-2 (Socher et al., 2013) SciTail (Khot et al., 2018) CoLA (Warstadt et al., 2019) STS-B (Cer et al., 2017) MRPC (Dolan and Brockett, 2005) RTE (Dagan et al., 2005, et seq.) WNLI (Levesque, 2011) question answering (QA) SQuAD-2 (Rajpurkar et al., 2018) NewsQA (Trischler et al., 2017) HotpotQA (Yang et al., 2018) SQuAD-1 (Rajpurkar et al., 2016) DuoRC-p (Saha et al., 2018) DuoRC-s (Saha et al., 2018) DROP (Dua et al., 2019) WikiHop (Welbl et al., 2018) BoolQ (Clark et al., 2019) ComQA (Abujabal et al., 2019) CQ (Bao et al., 2016) sequence labeling (SL) ST (Bjerva et al., 2016) CCG (Hockenmaier and Steedman, 2007) Parent (Liu et al., 2019a) GParent (Liu et al., 2019a) GGParen"
2020.emnlp-main.635,I05-5002,0,\N,Missing
2020.emnlp-main.635,J93-2004,0,\N,Missing
2020.emnlp-main.635,D15-1075,0,\N,Missing
2020.emnlp-main.635,W00-0726,0,\N,Missing
2020.emnlp-main.635,W03-0419,0,\N,Missing
2020.emnlp-main.635,J07-3004,0,\N,Missing
2020.emnlp-main.635,D13-1170,0,\N,Missing
2020.emnlp-main.635,P16-1112,0,\N,Missing
2020.emnlp-main.635,C16-1333,0,\N,Missing
2020.emnlp-main.635,C16-1236,0,\N,Missing
2020.emnlp-main.635,E17-1005,0,\N,Missing
2020.emnlp-main.635,E17-2039,0,\N,Missing
2020.emnlp-main.635,S17-2001,0,\N,Missing
2020.emnlp-main.635,C18-1251,0,\N,Missing
2020.emnlp-main.635,N19-1027,0,\N,Missing
2020.emnlp-main.635,W18-5401,0,\N,Missing
2020.emnlp-main.635,N19-1246,0,\N,Missing
2020.emnlp-main.635,N19-1300,0,\N,Missing
2020.emnlp-main.635,N19-1423,0,\N,Missing
2020.emnlp-main.635,P19-1441,0,\N,Missing
2020.emnlp-main.635,Q18-1021,0,\N,Missing
2020.emnlp-main.635,E17-2026,0,\N,Missing
2020.lrec-1.214,P18-1001,0,0.0902292,"8.41 74.06 81.80 81.11 43.56 67.22 64.00 48.46 31.56 58.31 65.07 70.81 74.38 48.28 64.54 55.00 45.03 40.14 68.49 73.13 82.47 77.19 49.82 67.37 66.65 47.22 41.68 69.36 72.32 85.27 79.77 56.34 66.13 66.70 47.69 40.19 68.6 77.40 74.63 79.75 59.39 69.66 68.91 45.69 Table 6: Spearman’s correlation on non-contextual word similarity (MaxSim). GASI-β has higher correlation on three datasets and is competitive on the others. PFT- GM is trained with two components/senses while other models learn three. A full version including MSSG is in appendix. word similarity datasets.6 Like Lee and Chen (2017) and Athiwaratkun et al. (2018), we compute the word similarity based on senses by MaxSim (Reisinger and Mooney, 2010), which maximizes the cosine similarity over the combination of all sense pairs and does not require local contexts, MaxSim(w1 , w2 ) = max 0≤i≤K,0≤j≤K cos(s1i , s2j ). (13) GASI -β has better correlation on three datasets, is competitive on the rest (Table 6), and remains competitive without scaling. GASI is better than MUSE, the other hard-attention multi-prototype model, on six datasets and worse on three. Our model can reproduce word similarities as well or better than existing models through our sense s"
2020.lrec-1.214,E09-1013,0,0.0536033,"also does well in the human evaluations (Table 2). Both GASI without scaling and MUSE fail to learn distinguishable senses and cannot disambiguate senses. High word similarities do not necessarily indicate “good” sense embeddings quality; our human evaluation—contextual word sense selection—is complementary. 6. Related Work: Representation, Evaluation Schütze (1998) introduces context-group discrimination for senses and uses the centroid of context vectors as a sense representation. Other work induces senses by context clustering (Purandare and Pedersen, 2004) or probabilistic mixture models (Brody and Lapata, 2009). Reisinger and Mooney (2010) first introduce multiple sense-specific vectors for each word, inspiring other 6 RG-65 (Rubenstein and Goodenough, 1965); SimLex999 (Hill et al., 2015); WS-353 (Finkelstein et al., 2002); MEN3k (Bruni et al., 2014); MC-30 (Miller and Charles, 1991); YP130 (Yang and Powers, 2006); MTurk-287 (Radinsky et al., 2011); MTurk-771 (Halawi et al., 2012); RW-2k (Luong et al., 2013) 7 Given how good PDF - GM is, it could do better on contextual word similarity even though it ignores senses. Average and MaxSim are equivalent for this model; it ties GASI-β. multi-prototype se"
2020.lrec-1.214,D14-1110,0,0.0268296,"al., 2014); MC-30 (Miller and Charles, 1991); YP130 (Yang and Powers, 2006); MTurk-287 (Radinsky et al., 2011); MTurk-771 (Halawi et al., 2012); RW-2k (Luong et al., 2013) 7 Given how good PDF - GM is, it could do better on contextual word similarity even though it ignores senses. Average and MaxSim are equivalent for this model; it ties GASI-β. multi-prototype sense embedding models. Generally, to address polysemy in word embeddings, previous work trains on annotated sense corpora (Iacobacci et al., 2015, Gómez-Pérez and Denaux, 2019) or external sense inventories (Labutov and Lipson, 2013, Chen et al., 2014, Jauhar et al., 2015, Chen et al., 2015, Wu and Giles, 2015, Pilehvar and Collier, 2016, Mancini et al., 2017); Rothe and Schütze (2017) extend word embeddings to lexical resources without training; others induce senses via multilingual parallel corpora (Guo et al., 2014, Šuster et al., 2016, Ettinger et al., 2016). We contrast our GASI to unsupervised monolingual multiprototype models along two dimensions: sense induction methodology and differentiability. Our focus is unsupervised induction because for interpretability to be useful, we assume that sense inventories and disambiguations are e"
2020.lrec-1.214,P15-2003,0,0.0188272,"1991); YP130 (Yang and Powers, 2006); MTurk-287 (Radinsky et al., 2011); MTurk-771 (Halawi et al., 2012); RW-2k (Luong et al., 2013) 7 Given how good PDF - GM is, it could do better on contextual word similarity even though it ignores senses. Average and MaxSim are equivalent for this model; it ties GASI-β. multi-prototype sense embedding models. Generally, to address polysemy in word embeddings, previous work trains on annotated sense corpora (Iacobacci et al., 2015, Gómez-Pérez and Denaux, 2019) or external sense inventories (Labutov and Lipson, 2013, Chen et al., 2014, Jauhar et al., 2015, Chen et al., 2015, Wu and Giles, 2015, Pilehvar and Collier, 2016, Mancini et al., 2017); Rothe and Schütze (2017) extend word embeddings to lexical resources without training; others induce senses via multilingual parallel corpora (Guo et al., 2014, Šuster et al., 2016, Ettinger et al., 2016). We contrast our GASI to unsupervised monolingual multiprototype models along two dimensions: sense induction methodology and differentiability. Our focus is unsupervised induction because for interpretability to be useful, we assume that sense inventories and disambiguations are either unavailable or imperfect. On the d"
2020.lrec-1.214,N19-1423,0,0.0234796,"ion Before we explore human interpretability of sense induction, we first describe our simple models to disentangle word senses. Our two models are built on Word2Vec (Mikolov et al., 2013a, Mikolov et al., 2013b), which we review in Section 2.1. Both models use a straightforward attention mechanism to select which sense is used in a token’s context, which we contrast to alternatives for Gumbel softmax M Marginalization P (sik |wi , c˜i ) sense attention c¯i P (cij |wi ) predict contexts M ... G ci1 si1 chemical si2 007 cim ... Computers need to represent the meaning of words in context. BERT (Devlin et al., 2019) and ELM o (Peters et al., 2018) have dramatically changed how natural language processing represents text. Rather than one-size-fits-all word vectors that ignore the nuance of how words are used in context, these new representations have topped the leaderboards for question answering, inference, and classification. Contextual representations have supplanted multisense embeddings (Camacho-Collados and Pilehvar, 2018). While these methods learn a vector for each sense, they do not encode meanings in downstream tasks as well as contextual representations (Peters et al., 2018). However, computers"
2020.lrec-1.214,P09-1002,0,0.0498459,"ling one_hot(arg maxk (log αk + gk )) by yk = softmax((log αk + gk )/τ ). Sense Attention in Objective Function Assuming a center word wi has senses {si1 , si2 , . . . , siK }, the original SkipGram likelihood becomes a marginal distribution over all senses of wi with sense induction probability P (sik |wi ); we focus on the disambiguation given local context c˜i and estimate P (sik |wi ) ≈ P (sik |wi , c˜i ); and thus, P (cij |wi ) ≈ ∑ ∑ wi ∈V cij ∈˜ ci 2.3. Why Attention? Musing on Alternatives For fine-grained sense inventories, it makes sense to have graded assignment of tokens to senses (Erk et al., 2009, Jurgens and Klapaftis, 2015). However, for coarse senses— except for humor (Miller et al., 2017)—words typically are associated with a single sense, often a single sense per discourse (Gale et al., 1992). A good model should respect this. Previous models either use non-differentiable objectives or—in the case of the current state of the art, MUSE (Lee and Chen, 2017)—reinforcement learning to select word senses. By using Gumbel softmax, our model both approximates discrete sense selection and is differentiable. As we argue in the next section, applications with a human in the loop are best a"
2020.lrec-1.214,J13-3003,0,0.150267,"ning, matter, regarding, debated, legality Table 4: A case where MSSG has low overlap but confuses raters (agreement 0.33); the model chooses s1. mans’; its selections have the highest accuracy and assigns the largest probability assigned to the human choices (Table 2). Thus, GASI-β produces sense embeddings that are both more interpretable and distinguishable. GASI without a scaling factor, however, has low consistency and flat sense distribution. Model Confidence However, some contexts are more ambiguous than others. For fine-grained senses, best practice is to use graded sense assignments (Erk et al., 2013). Thus, we also show the model’s probability of the top human choice; 1 distributions close to K (0.33) suggest the model learns a distribution that cannot disambiguate senses. We consider granularity of senses further in Section 6. 1730 Inter-rater Agreement We use the confidence score computed by Figure-Eight to estimate the raters’ agreement for this task. GASI-β has the highest human-model agreement, while both M USE and GASI without scaling have the lowest. Error Analysis Next, we explore why crowdworkers disagree with the model even though the senses are interpretable (Table 1). Is it th"
2020.lrec-1.214,N16-1163,0,0.0221027,"lent for this model; it ties GASI-β. multi-prototype sense embedding models. Generally, to address polysemy in word embeddings, previous work trains on annotated sense corpora (Iacobacci et al., 2015, Gómez-Pérez and Denaux, 2019) or external sense inventories (Labutov and Lipson, 2013, Chen et al., 2014, Jauhar et al., 2015, Chen et al., 2015, Wu and Giles, 2015, Pilehvar and Collier, 2016, Mancini et al., 2017); Rothe and Schütze (2017) extend word embeddings to lexical resources without training; others induce senses via multilingual parallel corpora (Guo et al., 2014, Šuster et al., 2016, Ettinger et al., 2016). We contrast our GASI to unsupervised monolingual multiprototype models along two dimensions: sense induction methodology and differentiability. Our focus is unsupervised induction because for interpretability to be useful, we assume that sense inventories and disambiguations are either unavailable or imperfect. On the dimension of sense induction methodology, Huang et al. (2012) and Neelakantan et al. (2014) induce senses by context clustering; Tian et al. (2014) model a corpuslevel sense distribution; Li and Jurafsky (2015) model the sense assignment as a Chinese Restaurant Process; Qiu et"
2020.lrec-1.214,K17-1012,0,0.113263,"urther compare GASI-β with previous SOTA, MSSG -30 K and MUSE, on the Word in Context dataset (Pilehvar and Camacho-Collados, 2019, W i C ) which requires the model to identify whether a word has the same sense in two contexts. To reduce the variance in training and to focus on evaluating the sense selection module, we use an evaluation suited for unsupervised models: if the model selects different sense vectors given contexts, we mark that the word has different senses.5 For MUSE, MSSG and GASI-β, we use each model’s sense selection module; for DeConf (Pilehvar and Collier, 2016) and SW 2 V (Mancini et al., 2017), we follow Pilehvar and Camacho-Collados (2019) and Pelevina et al. (2016) by selecting the closest sense vectors to the context vector. DeConf results are comparable to supervised results (59.4± 0.7). GASI-β has the best result (55.3) apart from DeConf itself (58.55)(full results in Table 8 in appendix), which uses the same sense inventory (Miller and Fellbaum, 1998, WordNet) as WiC. Non-Contextual Word Similarity While contextual word similarity is best suited for our model and goals, other datasets without contexts (i.e., only word pairs and a rating) are both larger and ubiquitous for wor"
2020.lrec-1.214,D14-1113,0,0.53467,"2, and Table 5). Our final objective function for GASI-β is J(S, C) ∝ K ∑ ∑ ∑ wi ∈V wc ∈ci k=1 Model Sense Accuracy Judgment Accuracy Agreement 67.33 69.33 71.33 62.89 66.67 67.33 0.73 0.76 0.77 MUSE MSSG -30 K GASI -β γki log P (wc |sik ). (12) 3. Data and Training For fair comparisons, we try to remain consistent with previous work (Huang et al., 2012, Neelakantan et al., 2014, Lee and Chen, 2017) in all aspects of training. In particular, we train GASI on the same April 2010 Wikipedia snapshot (Shaoul and Westbury, 2010) with 1B tokens and the same vocabulary released by Neelakantan et al. (2014); set the number of senses K = 3 and dimension d = 300 for each word unless otherwise specified. More details are in the Appendix. Following Maddison et al. (2017), we fix the temperature τ = 0.5, and tune the scaling factor β = 0.4 using grid search within {0.1 . . . 0.9} on AvgSimC for contextual word similarity (Section 5); this tuning preceded all interpretability experiments. If not reprinted, numbers for competing models are either computed with pre-trained embeddings released by authors or trained on released code. 4. Evaluating Interpretability We turn to traditional evaluations of sen"
2020.lrec-1.214,S17-1015,0,0.01873,"vised induction because for interpretability to be useful, we assume that sense inventories and disambiguations are either unavailable or imperfect. On the dimension of sense induction methodology, Huang et al. (2012) and Neelakantan et al. (2014) induce senses by context clustering; Tian et al. (2014) model a corpuslevel sense distribution; Li and Jurafsky (2015) model the sense assignment as a Chinese Restaurant Process; Qiu et al. (2016) induce senses by minimizing an energy function on a context-depend network; Bartunov et al. (2016) model the sense assignment as a steak-breaking process; Nguyen et al. (2017) model the sense embeddings as a weighted combination of topic vectors with pre-computed weights by topic models; Athiwaratkun et al. (2018) model word representations as Gaussian Mixture embeddings where each Gaussian component captures different senses; Lee and Chen (2017) compute sense distribution by a separate set of sense induction vectors. The proposed GASI marginalizes the likelihood of contexts over senses and induces senses by local context vectors; the most similar sense selection module is a bilingual model (Šuster et al., 2016) except that it does not introduce lower bound for neg"
2020.nlpcss-1.23,N19-1303,0,0.036708,"Missing"
2020.nlpcss-1.23,P13-1035,1,0.913808,"Missing"
2020.nlpcss-1.23,W19-3802,0,0.0149994,"rpus (rTVTROPES =0.32). If di is high, trope i contains a larger-than-usual proportion of female words. We finally calculate the the genderedness score gi as di ’s normalized z-score.6 This results in scores from −1.84 (male-dominated) to 4.02 (female-dominated). For our analyses, we consider tropes with genderedness scores outside of [−1, 1] (one standard deviation) to be highly gendered (see Table 2 for examples). While similar to methods used in prior work (Garc´ıa et al., 2014), our genderedness score is limited by its lexicon and susceptible to gender generalization and explicit marking (Hitti et al., 2019). We leave exploration of more nuanced methods of capturing trope genderedness (Ananya et al., 2019) to future work. 4 Analyzing gender bias in TVTROPES Having collected TVTROPES and linked each trope with metadata and genderedness scores, we now turn to characterizing how gender bias manifests itself in the data. We explore (1) the effects of genre on genderedness, (2) what kinds of topics are used in highly-gendered tropes, (3) what tropes contribute most to IMDb ratings, and (4) what types of tropes are used more commonly by authors of one gender than another. 4.1 Genderedness across genre"
2020.nlpcss-1.23,W19-3638,0,0.146208,"earch analyzing gender bias. Methods to measure gender bias include using contextual cues to develop probabilistic estimates (Ananya et al., 2019), and using gender directions in word embedding spaces (Bolukbasi et al., 2016). Other work engages directly with tvtropes.org: Kiesel and Grimnes (2010) build a wrapper for the website, but perform no analysis of its content. Garc´ıa-Ortega et al. (2018) create PicTropes: a limited dataset of 5,925 films from the website. Bamman et al. (2013) collect a set of 72 character-based tropes, which they then use to evaluate induced character personas, and Lee et al. (2019) use data crawled from the website to explore different sexism dimensions within TV and film. Analyzing bias through tropes is a popular area of research within social science. Hansen (2018) focus in on the titular princess character in the video game The Legend of Zelda as an example of the Damsel in Distress trope. Lacroix (2011) study the development and representation in popular culture of the Casino Indian and Ignoble Savage tropes. The usage of biased tropes is often attributed to the lack of equal representation both on and off the screen. Geena Davis Inclusion Quotient (Google, 2017) q"
2020.nlpcss-1.23,W02-0109,0,0.0604112,"te the effectiveness of the lexicon in capturing genderedness by annotating 150 random examples of trope occurrences as male (86), female (23), or N/A (41). N/A represents examples that do not capture any aspect of gender. We then use the lexicon to classify each example as male (precision = 0.85, recall = 0.86, and F1 score = 0.86) or female (precision = 0.72, recall = 0.78, and F1 score = 0.75). To measure genderedness, for each trope i, we concatenate the trope’s description with all of the trope’s examples to form a document Xi . Next, we tokenize, preprocess, and lemmatize Xi using NLTK (Loper and Bird, 2002). We then compute the number of tokens in Xi that match the male lexicon, m(Xi ), and the female lexicon, f (Xi ). We also compute m(TVTROPES) and f (TVTROPES), the total number of matches for each gender across 4 We note that some demographics may be more inclined to report age and gender information than others. 5 The gender-balanced lexicon is obtained from Zhao et al. (2018) and comprises 222 male-female word pairs. 2 http:/github.com/dhruvilgala/tvtropes 3 We match by both the work’s title and its year of release to avoid duplicates. 213 Motivated by Fear Robot War Cure for Cancer Evil Ge"
2020.nlpcss-1.23,N18-2002,0,0.126935,"Missing"
2021.acl-long.349,D19-5611,0,0.0182042,"ejad et al., 2019) for running test-time inference. Guo et al. (2020) report a significant improvement over prior non-autoregressive models and superior performance comparing to autoregressive methods on IWSLT’14 German-English task. Their finding is consistent with our improvement using the pretained BERT model. However, our Joint-EBM model is a different way of using BERT for translation, which does not require separate BERT models for source and target language. Please see Section 4.9 for a detailed comparison. Finally, other works also discuss using BERT to improve the performance of NMT. Clinchant et al. (2019) describe initializing the embedding or the whole encoder with BERT’s parameters. Zhu et al. (2020) use an attention model to incorporate the output of BERT into encoder and decoder of NMT. In our approach, we use BERT as an external energy-based ranker. 4 4.1 Experiments Datasets We use German-English (De→En), RomanianEnglish (Ro→En) and Italian-English (It→En) from IWSLT’14 datasets and French-English (Fr→En) from IWSLT’17 translation tasks. We also use IWSLT’14 English-German (En→De) to show that the proposed method can be expanded to translation tasks with a different target language. All"
2021.acl-long.349,P19-1285,0,0.0532535,"Missing"
2021.acl-long.349,D19-1633,0,0.0950372,"to our work, Guo et al. (2020) proposes using two different BERT models as an encoder of the source language (X-BERT) and a decoder of the target language (Y-BERT). Guo et al. (2020) add an extra trainable encoder-decoder adaption module followed by a feed-forward module to each layer of the decoder and a feed-forward module to each layer of the encoder. (Please see Guo et al. (2020) for more detail on the architecture.) For fine-tuning XY-BERT for translation tasks, Guo et al. (2020) keep all XY-BERT’s parameters fixed except the parameters of the new modules, and use mask-predict decoding (Ghazvininejad et al., 2019) for running test-time inference. Guo et al. (2020) report a significant improvement over prior non-autoregressive models and superior performance comparing to autoregressive methods on IWSLT’14 German-English task. Their finding is consistent with our improvement using the pretained BERT model. However, our Joint-EBM model is a different way of using BERT for translation, which does not require separate BERT models for source and target language. Please see Section 4.9 for a detailed comparison. Finally, other works also discuss using BERT to improve the performance of NMT. Clinchant et al. ("
2021.acl-long.349,D19-1632,0,0.0409334,"Missing"
2021.acl-long.349,P17-4012,0,0.0354536,"wani et al., 2017) as our BaseNMT. Our Transformer architecture includes six encoder and six decoder layers, and the number of attention heads, embedding dimension and inner-layer dimension are 8, 512 and 4096, respectively. We use dropout, weight decay, label smoothing to regularize our models. We use layer normalization and early stopping. Models are optimized using Adam (Kingma and Ba, 2015) with parameters β1 = 0.9, β2 = 0.98, and  = 1e−8 and we use the same learning rate scheduler as Ott et al. (2019). We trained our models on 1 Nvidia TITANX GPU. 2 We use the implementation in Opennmt (Klein et al., 2017) and Fairseq (Ott et al., 2019) toolkits. 4531 Table 1: BLEU score comparison for IWSLT, FLoRes, and WMT (indicated using *) tasks. De− →En Fr− →En It− →En Ro− →En Si− →En Ne− →En En− →De De→En* En→De* BaseNMT + Beam BaseNMT + Sample BaseNMT + LM BaseNMT + MLM NCE-EBR Marginal-EBR Shared-EBR Conditional-EBM 33.87 33.98 34.25 34.42 34.47 35.68 35.75 37.58 31.50 31.59 31.56 32.13 32.00 33.77 33.80 35.02 32.08 32.22 32.52 33.68 32.89 34.00 34.14 36.05 33.21 33.64 33.01 33.85 32.23 34.48 34.65 37.19 7.10 7.19 7.11 7.70 7.98 8.62 10.29 10.47 6.07 6.44 6.02 7.21 7.36 7.26 9.25 9.82 28.83 28.85 28.91"
2021.acl-long.349,N16-1133,0,0.0163489,"r models: ωθ (y) = exp(−Eθ (y)). They train the EBM using noise contrastive estimation (Gutmann and Hyv¨arinen, 2010). We find this less suitable for re-ranking in the translation tasks (see Section 4). Discriminative re-ranking was first introduced by Shen et al. (2004) for improving the performance of machine translation (MT). They have trained a linear separator using the perceptron learning algorithm to distinguish the top r translations from the rest of the translations in the n-best possible outputs. The features for the discriminator are extracted from both source and target sentences. Mizumoto and Matsumoto (2016) combine the score of MT and the linear model using more complex syntactical features to re-rank the target sentences. Here, we rely on the features learned by BERT, and given the high capacity of the energy model, we train the energy model to respect the ranking of every pair of samples. Gulcehre et al. (2017) describe using language model (LM) to improve the performance of NMT using shallow and deep fusion. Shallow models combine the marginal probability of predicting each word in NMT and LM: log PNMT (yi |y<i ) + λ log PLM (yi |y<i ), while deep fusion concatenates the hidden states of two"
2021.acl-long.349,W19-5333,0,0.0129941,"n NMT and LM: log PNMT (yi |y<i ) + λ log PLM (yi |y<i ), while deep fusion concatenates the hidden states of two models before predicting each word and uses parallel data to fine-tune the weights. Similar to deep fusion, Domhan and Hieber (2017) feed the unnormalized output of LM to the decoder of NMT. Domhan and Hieber (2017) jointly train the LM and NMT using monolingual target-side data and parallel data, respectively. Sennrich et al. (2016a) augment the parallel training data with monolingual data with the target language and back-translation. Re-ranking with LM has also been explored by Ng et al. (2019), where they decode the output 4530 based on log p(y|x) + λ1 log p(x|y) + λ2 log p(y), where p(y|x) is the direct model provided by NMT, p(x|y) is computed via back-translation and p(y) is an LM. Our approach differs from the previous methods that use LMs for re-ranking as we train our energy-based model to be consistent with the task measure instead of using pre-trained LMs. In our experiments, we only explore the effect of using the direct model plus LM, nevertheless, backtranslation can also be added into our model for further improvement. Recently, Salazar et al. (2020) use masked language"
2021.acl-long.349,N19-4009,0,0.019104,"l. (2019) we use the devtest dataset for all our evaluations. 4.2 Base Model We use the Transformer2 (Vaswani et al., 2017) as our BaseNMT. Our Transformer architecture includes six encoder and six decoder layers, and the number of attention heads, embedding dimension and inner-layer dimension are 8, 512 and 4096, respectively. We use dropout, weight decay, label smoothing to regularize our models. We use layer normalization and early stopping. Models are optimized using Adam (Kingma and Ba, 2015) with parameters β1 = 0.9, β2 = 0.98, and  = 1e−8 and we use the same learning rate scheduler as Ott et al. (2019). We trained our models on 1 Nvidia TITANX GPU. 2 We use the implementation in Opennmt (Klein et al., 2017) and Fairseq (Ott et al., 2019) toolkits. 4531 Table 1: BLEU score comparison for IWSLT, FLoRes, and WMT (indicated using *) tasks. De− →En Fr− →En It− →En Ro− →En Si− →En Ne− →En En− →De De→En* En→De* BaseNMT + Beam BaseNMT + Sample BaseNMT + LM BaseNMT + MLM NCE-EBR Marginal-EBR Shared-EBR Conditional-EBM 33.87 33.98 34.25 34.42 34.47 35.68 35.75 37.58 31.50 31.59 31.56 32.13 32.00 33.77 33.80 35.02 32.08 32.22 32.52 33.68 32.89 34.00 34.14 36.05 33.21 33.64 33.01 33.85 32.23 34.48 34.6"
2021.acl-long.349,K19-1084,0,0.0629332,"Missing"
2021.acl-long.349,W18-6319,0,0.0199208,"t, 4532 we have Shared-EBR that trains single MarginalEBM for the tasks with the same target language. Shared-EBR is only trained on IWSLT and FLoRes tasks with English target. For this method, we first sample a translation task and then sample a batch from that task and follow Algorithm 1 for the training of the Marginal-EBM. Finally, as an upper bound for the best achievable result, we also extract the translations from the sample that are closest to the gold data (based on BLEU score). 4.6 Table 3: The effect of using gold data in the ranking objective for Marginal-EBR. 3 We use SacreBLEU (Post, 2018) as a consistent BLEU implementation for all of our experiments. 0.0 0.25 0.75 1.0 De→En Fr→En 35.68 33.77 35.00 33.15 34.20 31.65 33.75 30.82 Table 4: Effect of Entropy Regularization on IWSLT’14 DE-EN Results Table 1 shows the performance of the described methods for IWSLT, FLoRes, and WMT translation tasks.3 BaseNMT+Sample achieves a better score than beam decoding suggesting that our multinomial sampling supports the modes of the distribution defined by the BaseNMT. Similarly, oracle values are high, indicating that the samples also support the desired distribution. This satisfies the nece"
2021.acl-long.349,2020.acl-main.240,0,0.0222475,"as also been explored by Ng et al. (2019), where they decode the output 4530 based on log p(y|x) + λ1 log p(x|y) + λ2 log p(y), where p(y|x) is the direct model provided by NMT, p(x|y) is computed via back-translation and p(y) is an LM. Our approach differs from the previous methods that use LMs for re-ranking as we train our energy-based model to be consistent with the task measure instead of using pre-trained LMs. In our experiments, we only explore the effect of using the direct model plus LM, nevertheless, backtranslation can also be added into our model for further improvement. Recently, Salazar et al. (2020) use masked language models (MLM) such as BERT to score hypotheses from NMT. Salazar et al. (2020) describe the score of a MLM as pseudo-log-likelihood score (PLL). To calculate PLL score of a sentence, each token wi in the sentence is sequentially masked, which allows the calculation of log p(wi |wi ) from the output of the MLM. The normalized pseudolog-probability of the sentence is the average of logprobability of the masked words P given the rest of the words in the sentence: N1 N i=1 log p(wi |wi ), where N is the length of the sentence. We use this approach as one of our baselines. In"
2021.acl-long.349,P16-1009,0,0.217941,"describe using language model (LM) to improve the performance of NMT using shallow and deep fusion. Shallow models combine the marginal probability of predicting each word in NMT and LM: log PNMT (yi |y<i ) + λ log PLM (yi |y<i ), while deep fusion concatenates the hidden states of two models before predicting each word and uses parallel data to fine-tune the weights. Similar to deep fusion, Domhan and Hieber (2017) feed the unnormalized output of LM to the decoder of NMT. Domhan and Hieber (2017) jointly train the LM and NMT using monolingual target-side data and parallel data, respectively. Sennrich et al. (2016a) augment the parallel training data with monolingual data with the target language and back-translation. Re-ranking with LM has also been explored by Ng et al. (2019), where they decode the output 4530 based on log p(y|x) + λ1 log p(x|y) + λ2 log p(y), where p(y|x) is the direct model provided by NMT, p(x|y) is computed via back-translation and p(y) is an LM. Our approach differs from the previous methods that use LMs for re-ranking as we train our energy-based model to be consistent with the task measure instead of using pre-trained LMs. In our experiments, we only explore the effect of usi"
2021.acl-long.349,P16-1162,0,0.316259,"describe using language model (LM) to improve the performance of NMT using shallow and deep fusion. Shallow models combine the marginal probability of predicting each word in NMT and LM: log PNMT (yi |y<i ) + λ log PLM (yi |y<i ), while deep fusion concatenates the hidden states of two models before predicting each word and uses parallel data to fine-tune the weights. Similar to deep fusion, Domhan and Hieber (2017) feed the unnormalized output of LM to the decoder of NMT. Domhan and Hieber (2017) jointly train the LM and NMT using monolingual target-side data and parallel data, respectively. Sennrich et al. (2016a) augment the parallel training data with monolingual data with the target language and back-translation. Re-ranking with LM has also been explored by Ng et al. (2019), where they decode the output 4530 based on log p(y|x) + λ1 log p(x|y) + λ2 log p(y), where p(y|x) is the direct model provided by NMT, p(x|y) is computed via back-translation and p(y) is an LM. Our approach differs from the previous methods that use LMs for re-ranking as we train our energy-based model to be consistent with the task measure instead of using pre-trained LMs. In our experiments, we only explore the effect of usi"
2021.acl-long.349,N04-1023,0,0.210541,"valuated at y. The log-linear model simplifies training the parameters θ: ∇θ pθ (y) = P φ(y)−E y). The expectation term ˆ ∼pθ (.) φ(ˆ y y∈D can be estimated using rejecting sampling or importance sampling given the proposal distribution q. Deng et al. (2020) extend this approach for text generation by using unrestricted EBMs instead of log-linear models: ωθ (y) = exp(−Eθ (y)). They train the EBM using noise contrastive estimation (Gutmann and Hyv¨arinen, 2010). We find this less suitable for re-ranking in the translation tasks (see Section 4). Discriminative re-ranking was first introduced by Shen et al. (2004) for improving the performance of machine translation (MT). They have trained a linear separator using the perceptron learning algorithm to distinguish the top r translations from the rest of the translations in the n-best possible outputs. The features for the discriminator are extracted from both source and target sentences. Mizumoto and Matsumoto (2016) combine the score of MT and the linear model using more complex syntactical features to re-rank the target sentences. Here, we rely on the features learned by BERT, and given the high capacity of the energy model, we train the energy model t"
2021.acl-long.349,P16-1159,0,0.171551,"partment of Computer Science, College of Computing and Informatics University of North Carolina Charlotte {sbhatta9,rooshenas}@uncc.edu Subhajit Naskar, Simeng Sun, Mohit Iyyer, and Andrew McCallum College of Information and Computer Science, University of Massachusetts Amherst {snaskar,simeng,miyyer,mccallum}@cs.umass.edu Abstract The discrepancy between maximum likelihood estimation (MLE) and task measures such as BLEU score has been studied before for autoregressive neural machine translation (NMT) and resulted in alternative training algorithms (Ranzato et al., 2016; Norouzi et al., 2016; Shen et al., 2016; Wu et al., 2018). However, MLE training remains the de facto approach for autoregressive NMT because of its computational efficiency and stability. Despite this mismatch between the training objective and task measure, we notice that the samples drawn from an MLE-based trained NMT support the desired distribution – there are samples with much higher BLEU score comparing to the beam decoding output. To benefit from this observation, we train an energybased model to mimic the behavior of the task measure (i.e., the energy-based model assigns lower energy to samples with higher BLEU score), whi"
2021.acl-long.349,D18-1397,0,0.0699061,"r Science, College of Computing and Informatics University of North Carolina Charlotte {sbhatta9,rooshenas}@uncc.edu Subhajit Naskar, Simeng Sun, Mohit Iyyer, and Andrew McCallum College of Information and Computer Science, University of Massachusetts Amherst {snaskar,simeng,miyyer,mccallum}@cs.umass.edu Abstract The discrepancy between maximum likelihood estimation (MLE) and task measures such as BLEU score has been studied before for autoregressive neural machine translation (NMT) and resulted in alternative training algorithms (Ranzato et al., 2016; Norouzi et al., 2016; Shen et al., 2016; Wu et al., 2018). However, MLE training remains the de facto approach for autoregressive NMT because of its computational efficiency and stability. Despite this mismatch between the training objective and task measure, we notice that the samples drawn from an MLE-based trained NMT support the desired distribution – there are samples with much higher BLEU score comparing to the beam decoding output. To benefit from this observation, we train an energybased model to mimic the behavior of the task measure (i.e., the energy-based model assigns lower energy to samples with higher BLEU score), which is resulted in"
2021.acl-long.349,N18-2021,1,0.368379,", thus each local region is trained to have similar ranking as that BLEU score for the samples in the region. 2 Energy-Based Reranking Using EBM Eθ to reweight the samples from an NMT defines a new probability distribution over the output sentences (see Grover et al. (2019)): Pθ (y|x) ∝ PNMT (y|x) exp( −EθT(y,x) ), where T is temperature. The ideal re-ranker requires an EBM with the energy function Eθ (y, x) such that Pθ (y|x) and BLEU(y, yi ) have similar modes for all (xi , yi ) ∈ D, where D is an empirical data distribution. To train θ we use rank-based training (Rohanimanesh et al., 2011; Rooshenas et al., 2018, 2019). Rank-based training enforces that the samples from Pθ (.) have similar ranking with respect to both the energy score and task measure (see Figure 2). To sample from Pθ (y|x), we sample k sentences from PNMT (y|x) using multinomial sampling from locally normalized distributions over the output and reweight the samples based on the energy network exp( −EθT(y,x) ). Then we resample two sentences, y1 and y2 , from the renormalized set, which defines a conditional distribution: θ (y,x)/T ) P i (y|x) = P exp(−E (a similar samk exp(−Eθ (yk ,x)/T ) pling approach has been used in Deng et al."
2021.eacl-main.223,2020.emnlp-main.525,1,0.804005,"chieved impressive performance in language generation tasks (Radford et al., 2019; Dai et al., 2019) such as open-domain story generation (See et al., 2019a). When writing with the LM, users often desire an intuitive and effective way to control what a LM is going to generate (Keskar et al., 2019). To address this need, interactive writing assistants provide options to reveal possible developments of the story and generate continuations guided by the user-selected options. Interactive writing assistants have wide applications in creative writing (Roemmele and Gordon, 2015; Clark et al., 2018; Akoury et al., 2020), education (Luo et al., 2015), and gaming (Walton, 2020). Nevertheless, the existing systems’ options usually do not provide fine-grained control and/or require substantial human labor. In some prior work (Keskar et al., 2019; Tu et al., 2019), users choose among a static set of predefined attributes (e.g., sentiment) that only provide coarse-grained control. Other work (Roemmele and Gordon, 2015; Clark et al., 2018) presents users with multiple generated continuations, which requires substantial reading effort and might not contain topics that users want to see. Finally, options could be nod"
2021.eacl-main.223,P19-1285,0,0.0671411,"Missing"
2021.eacl-main.223,N19-1423,0,0.0116174,"on generator uses GPT2 to encode the input prompt x1 , ..., xI and passes the output embedding to K different linear layers L1 , ..., LK . To model the dependency of clusters, a Transformer (Vaswani et al., 2017) takes the K embeddings as input and predicts the cluster centers c1 , ...cK in GloVe (Pennington et al., 2014) space. During testing, each predicted cluster center is normalized by its L2 norm, and we use the M closest words in the normalized GloVe space to represent the topic Ti , which users can choose. We choose to learn the cluster centers in GloVe space rather than GPT2 or BERT (Devlin et al., 2019) space because the non-contextualized word embeddings are easier to visualize. Users can easily understand the meaning of a cluster center by seeing nearby words. We normalize GloVe space in this work to make the squared L2 distance equal to twice the cosine distance between two embeddings. Our architecture is similar to the one in Chang et al. (2021), but we use a pretrained GPT2 encoder rather than train a BERT-like Transformer from scratch. Another difference is that we ignore the connection between the second Transformer and the output of GPT2 to save GPU memory for handling a longer input"
2021.eacl-main.223,P18-1082,0,0.150572,"loVe embeddings are first passed through a linear layer to make their dimension become the same as the hidden state size of GPT2. Then, the transformed embeddings are added with special positional embeddings pfI , which are different from those for the prompt pw . i The special positional embedding tells GPT2 that the inserted embeddings have a different meaning and where the conditional generation starts. The GPT2 encoder’s output goes through a softmax layer, which computes the probability of each token being observed as the first word piece in the continuation y1 . We adopt top-k sampling (Fan et al., 2018), which reduces the chance of sampling words with low probability, to pick the next word, and autoregressively sample one token ybo at a time to generate the continuation yb1 , ..., ybO . 2.2.2 Model Training We train the generator using the continuation of a prompt and some randomly selected nonstop words in the continuation as its generation conditions. Since the continuation contains the randomly-selected words, the generator would be heavily penalized if it ignores the conditions by assigning low probabilities to the selected words in all the continuation positions. An example is illustrat"
2021.eacl-main.223,P19-1254,0,0.0478183,"Missing"
2021.eacl-main.223,W19-2405,0,0.0167825,"n et al., 2019), a sequence of event structure (subject, verb, object, preposition, modifier) (Ammanabrolu et al., 2020), a story premise (Fan et al., 2018), or a story summary (Chen et al., 2019). Users can revise the skeleton to control the generated text, but the approaches assume the existence of the skeleton extractor or labels in the training corpus. Besides, the systems cannot suggest options given the partial text, which is one of the main focuses of our interactive writing assistant. The skeleton could also be multiple keyphrases. The keyphrases are extracted based on word frequency (Ippolito et al., 2019; Tan et al., 2020; Wu et al., 2020), an off-the-shelf keyword extraction method (Peng et al., 2018; Goldfarb-Tarrant et al., 2019; Yao et al., 2019; Rashkin et al., 2020; Zhang et al., 2020), a sentence compression dataset and reinforcement learning (Xu et al., 2018), or image caption datasets and ConceptNet (Lin et al., 2020). Most of the studies focus on modeling the longterm dependency among the keyphrases and/or forcing the generation to contain the keyphrases. Instead, we focus on allowing users to determine the topical directions of the generation. Compared with conditioning on keyphras"
2021.eacl-main.223,N16-1014,0,0.0319295,"46 ± 3.47 56.41 ± 4.41 4.07 ± 0.10 24.49 ± 2.77 48.69 ± 4.61 4.15 ± 0.11 Table 5: Comparison of conditional text generators. The numbers in Dist-1, Dist-2, Recall, and Precision are percentages. Lower perplexity (PPL) and inference time are better. The better performances between PPLM and our method are highlighted. In human evaluation, we report the mean ± standard error of each method. words related to the topics. The fluency of the generated text is measured using the perplexity (Serban et al., 2016) of the original GPT2 (with 345M parameters) without being fine-tuned on Wikipedia. Dist-n (Li et al., 2016) is the ratio between the number of unique n-grams and the number of all n-grams in the continuations, where n=1 or 2. Higher Dist-n implies more diverse generations. The average inference time per input prompt is also presented. 3.3.2 Human Evaluation We present the prompt and the generated continuation and ask the worker to score the generation’s fluency from 1 (not fluent at all) to 5 (very fluent). Next, we show K topics and ask which topics are mentioned in the generation. Treating the worker’s choices as prediction and the topics our model conditions on as ground truth, we report the ave"
2021.eacl-main.223,2020.findings-emnlp.165,0,0.0377742,"s in the training corpus. Besides, the systems cannot suggest options given the partial text, which is one of the main focuses of our interactive writing assistant. The skeleton could also be multiple keyphrases. The keyphrases are extracted based on word frequency (Ippolito et al., 2019; Tan et al., 2020; Wu et al., 2020), an off-the-shelf keyword extraction method (Peng et al., 2018; Goldfarb-Tarrant et al., 2019; Yao et al., 2019; Rashkin et al., 2020; Zhang et al., 2020), a sentence compression dataset and reinforcement learning (Xu et al., 2018), or image caption datasets and ConceptNet (Lin et al., 2020). Most of the studies focus on modeling the longterm dependency among the keyphrases and/or forcing the generation to contain the keyphrases. Instead, we focus on allowing users to determine the topical directions of the generation. Compared with conditioning on keyphrases, our interactive writing assistant is especially helpful when users do not know the exact phrases they want to see or when the given keyphrase extractor does not detect the desired topics. 5 Conclusion We propose an interactive writing assistant that generates topic options given an input prompt and generates the continuatio"
2021.eacl-main.223,P17-4008,0,0.0257131,"n We present the prompt and the generated continuation and ask the worker to score the generation’s fluency from 1 (not fluent at all) to 5 (very fluent). Next, we show K topics and ask which topics are mentioned in the generation. Treating the worker’s choices as prediction and the topics our model conditions on as ground truth, we report the average precision and recall of the prediction. 3.3.3 Conditional Text Generator Baselines We compare our method with PPLM (Plug and Play Language Models) (Dathathri et al., 2020) due to its strong performance against the weighted decoding approach from Ghazvininejad et al. (2017) when the condition is a bag of words. The condition for PPLM is the union of the top M words in the chosen topics and each word’s weight is neglected. We use our generation model without conditioning on any word (i.e., n = 0) during testing3 as the base model of PPLM. We also present the performance of the base model itself as a reference to know the significance of our improvement (denoted as GPT2). 3.3.4 Results Table 5 indicates that our model outperforms PPLM in all metrics except in Dist-1 and Dist-2. We suspect that our model generates slightly less 3 We find the model performs similarl"
2021.eacl-main.223,N19-4016,0,0.0212796,"a story premise (Fan et al., 2018), or a story summary (Chen et al., 2019). Users can revise the skeleton to control the generated text, but the approaches assume the existence of the skeleton extractor or labels in the training corpus. Besides, the systems cannot suggest options given the partial text, which is one of the main focuses of our interactive writing assistant. The skeleton could also be multiple keyphrases. The keyphrases are extracted based on word frequency (Ippolito et al., 2019; Tan et al., 2020; Wu et al., 2020), an off-the-shelf keyword extraction method (Peng et al., 2018; Goldfarb-Tarrant et al., 2019; Yao et al., 2019; Rashkin et al., 2020; Zhang et al., 2020), a sentence compression dataset and reinforcement learning (Xu et al., 2018), or image caption datasets and ConceptNet (Lin et al., 2020). Most of the studies focus on modeling the longterm dependency among the keyphrases and/or forcing the generation to contain the keyphrases. Instead, we focus on allowing users to determine the topical directions of the generation. Compared with conditioning on keyphrases, our interactive writing assistant is especially helpful when users do not know the exact phrases they want to see or when the"
2021.eacl-main.223,Q18-1027,0,0.0183595,"let users express their preferences. The options could be manually defined classes (e.g., sentiment) (Keskar et al., 2019; Dathathri et al., 2020), semantic frames (Tu et al., 2019), or event structures such as (subject, verb, object, modifier) (Martin et al., 2018; Tambwekar et al., 2019; Ammanabrolu et al., 2020). The forms of options allow users to control the attributes of the generated text but require labels or classifiers that map the text to the attributes/options. The options could also be a single query word at the beginning (Austin, 2019), the article title (Yan, 2016), politeness (Niu and Bansal, 2018) or specificity (See et al., 2019b) of the text, or the length of the generated sentence (Tu et al., 2019). However, the options cannot provide fine-grained control on topical directions of the generated contents. A related research direction is the multi-stage story generation. To make a long story more coherent, recent work proposes to generate a skeleton and then generate the full text guided by 2608 the skeleton. The skeleton could be a sequence of SRL frames (Fan et al., 2019), a sequence of event structure (subject, verb, object, preposition, modifier) (Ammanabrolu et al., 2020), a story"
2021.eacl-main.223,W18-1505,0,0.0217192,"olu et al., 2020), a story premise (Fan et al., 2018), or a story summary (Chen et al., 2019). Users can revise the skeleton to control the generated text, but the approaches assume the existence of the skeleton extractor or labels in the training corpus. Besides, the systems cannot suggest options given the partial text, which is one of the main focuses of our interactive writing assistant. The skeleton could also be multiple keyphrases. The keyphrases are extracted based on word frequency (Ippolito et al., 2019; Tan et al., 2020; Wu et al., 2020), an off-the-shelf keyword extraction method (Peng et al., 2018; Goldfarb-Tarrant et al., 2019; Yao et al., 2019; Rashkin et al., 2020; Zhang et al., 2020), a sentence compression dataset and reinforcement learning (Xu et al., 2018), or image caption datasets and ConceptNet (Lin et al., 2020). Most of the studies focus on modeling the longterm dependency among the keyphrases and/or forcing the generation to contain the keyphrases. Instead, we focus on allowing users to determine the topical directions of the generation. Compared with conditioning on keyphrases, our interactive writing assistant is especially helpful when users do not know the exact phrase"
2021.eacl-main.223,D14-1162,0,0.0873552,"al., 2019) if the corresponding label data are available in the training corpus. Model Prediction The goal of our option generator is to predict the K cluster centers of words in the possible continuations and use the cluster centers as the topics user could choose from. As in Figure 3 (b), the option generator uses GPT2 to encode the input prompt x1 , ..., xI and passes the output embedding to K different linear layers L1 , ..., LK . To model the dependency of clusters, a Transformer (Vaswani et al., 2017) takes the K embeddings as input and predicts the cluster centers c1 , ...cK in GloVe (Pennington et al., 2014) space. During testing, each predicted cluster center is normalized by its L2 norm, and we use the M closest words in the normalized GloVe space to represent the topic Ti , which users can choose. We choose to learn the cluster centers in GloVe space rather than GPT2 or BERT (Devlin et al., 2019) space because the non-contextualized word embeddings are easier to visualize. Users can easily understand the meaning of a cluster center by seeing nearby words. We normalize GloVe space in this work to make the squared L2 distance equal to twice the cosine distance between two embeddings. Our archite"
2021.eacl-main.223,2020.emnlp-main.349,0,0.0234431,"Missing"
2021.eacl-main.223,K19-1079,0,0.20838,"tates United 10 Step 1: Let’s see what language models would say Input Prompt: “Barack Obama writes a new book” Step 3: Please talk more about these topics User Figure 1: Given an input prompt, the Transformerbased LM provides K = 10 topics that might be mentioned next and each topic is represented by M = 3 words. The user could guide the generation process by choosing a subset of topics. Introduction Recently, Transformer-based language models (LMs) have achieved impressive performance in language generation tasks (Radford et al., 2019; Dai et al., 2019) such as open-domain story generation (See et al., 2019a). When writing with the LM, users often desire an intuitive and effective way to control what a LM is going to generate (Keskar et al., 2019). To address this need, interactive writing assistants provide options to reveal possible developments of the story and generate continuations guided by the user-selected options. Interactive writing assistants have wide applications in creative writing (Roemmele and Gordon, 2015; Clark et al., 2018; Akoury et al., 2020), education (Luo et al., 2015), and gaming (Walton, 2020). Nevertheless, the existing systems’ options usually do not provide fine-grai"
2021.eacl-main.223,N19-1170,0,0.226503,"tates United 10 Step 1: Let’s see what language models would say Input Prompt: “Barack Obama writes a new book” Step 3: Please talk more about these topics User Figure 1: Given an input prompt, the Transformerbased LM provides K = 10 topics that might be mentioned next and each topic is represented by M = 3 words. The user could guide the generation process by choosing a subset of topics. Introduction Recently, Transformer-based language models (LMs) have achieved impressive performance in language generation tasks (Radford et al., 2019; Dai et al., 2019) such as open-domain story generation (See et al., 2019a). When writing with the LM, users often desire an intuitive and effective way to control what a LM is going to generate (Keskar et al., 2019). To address this need, interactive writing assistants provide options to reveal possible developments of the story and generate continuations guided by the user-selected options. Interactive writing assistants have wide applications in creative writing (Roemmele and Gordon, 2015; Clark et al., 2018; Akoury et al., 2020), education (Luo et al., 2015), and gaming (Walton, 2020). Nevertheless, the existing systems’ options usually do not provide fine-grai"
2021.eacl-main.223,D19-5605,0,0.253381,"at a LM is going to generate (Keskar et al., 2019). To address this need, interactive writing assistants provide options to reveal possible developments of the story and generate continuations guided by the user-selected options. Interactive writing assistants have wide applications in creative writing (Roemmele and Gordon, 2015; Clark et al., 2018; Akoury et al., 2020), education (Luo et al., 2015), and gaming (Walton, 2020). Nevertheless, the existing systems’ options usually do not provide fine-grained control and/or require substantial human labor. In some prior work (Keskar et al., 2019; Tu et al., 2019), users choose among a static set of predefined attributes (e.g., sentiment) that only provide coarse-grained control. Other work (Roemmele and Gordon, 2015; Clark et al., 2018) presents users with multiple generated continuations, which requires substantial reading effort and might not contain topics that users want to see. Finally, options could be nodes in a plot graph that are handcrafted (Luo et al., 2015) or derived from a collaboration between humans and machine (Li et al., 2013), but such choices are usually limited due to the high cost of preparing the options. To address these limita"
2021.eacl-main.223,D18-1462,0,0.0244819,"ches assume the existence of the skeleton extractor or labels in the training corpus. Besides, the systems cannot suggest options given the partial text, which is one of the main focuses of our interactive writing assistant. The skeleton could also be multiple keyphrases. The keyphrases are extracted based on word frequency (Ippolito et al., 2019; Tan et al., 2020; Wu et al., 2020), an off-the-shelf keyword extraction method (Peng et al., 2018; Goldfarb-Tarrant et al., 2019; Yao et al., 2019; Rashkin et al., 2020; Zhang et al., 2020), a sentence compression dataset and reinforcement learning (Xu et al., 2018), or image caption datasets and ConceptNet (Lin et al., 2020). Most of the studies focus on modeling the longterm dependency among the keyphrases and/or forcing the generation to contain the keyphrases. Instead, we focus on allowing users to determine the topical directions of the generation. Compared with conditioning on keyphrases, our interactive writing assistant is especially helpful when users do not know the exact phrases they want to see or when the given keyphrase extractor does not detect the desired topics. 5 Conclusion We propose an interactive writing assistant that generates topi"
2021.eacl-main.223,2020.emnlp-main.698,0,0.0197423,"19). Users can revise the skeleton to control the generated text, but the approaches assume the existence of the skeleton extractor or labels in the training corpus. Besides, the systems cannot suggest options given the partial text, which is one of the main focuses of our interactive writing assistant. The skeleton could also be multiple keyphrases. The keyphrases are extracted based on word frequency (Ippolito et al., 2019; Tan et al., 2020; Wu et al., 2020), an off-the-shelf keyword extraction method (Peng et al., 2018; Goldfarb-Tarrant et al., 2019; Yao et al., 2019; Rashkin et al., 2020; Zhang et al., 2020), a sentence compression dataset and reinforcement learning (Xu et al., 2018), or image caption datasets and ConceptNet (Lin et al., 2020). Most of the studies focus on modeling the longterm dependency among the keyphrases and/or forcing the generation to contain the keyphrases. Instead, we focus on allowing users to determine the topical directions of the generation. Compared with conditioning on keyphrases, our interactive writing assistant is especially helpful when users do not know the exact phrases they want to see or when the given keyphrase extractor does not detect the desired topics."
2021.emnlp-main.395,2020.emnlp-main.389,0,0.0853548,"t supervision ⇤ Equal contribution. through span constraints. These span constraints in4818 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 4818–4831 c November 7–11, 2021. 2021 Association for Computational Linguistics dicate that a certain sequence of words in a sentence form a constituent span in its parse tree, and we obtain these partial ground-truths without explicit user annotation. We take inspiration from previous work incorporating distant supervision into parsing (Haghighi and Klein, 2006; Finkel and Manning, 2009; Ganchev et al., 2010; Cao et al., 2020), and design a novel fully neural system that improves a competitive neural unsupervised parser (DIORA; Drozdov et al. 2019) using span constraints defined on a portion of the training data. In the large majority of cases, the number of spans constraints per sentence is much lower than that specified by a full parse tree. We find that entity spans are effective as constraints, and can readily be acquired from existing data or derived from a gazetteer. In our experiments, we use DIORA as our baseline and improve upon it by injecting these span constraints as a source of distant supervision. We"
2021.emnlp-main.395,N19-1116,1,0.927948,"rence on Empirical Methods in Natural Language Processing, pages 4818–4831 c November 7–11, 2021. 2021 Association for Computational Linguistics dicate that a certain sequence of words in a sentence form a constituent span in its parse tree, and we obtain these partial ground-truths without explicit user annotation. We take inspiration from previous work incorporating distant supervision into parsing (Haghighi and Klein, 2006; Finkel and Manning, 2009; Ganchev et al., 2010; Cao et al., 2020), and design a novel fully neural system that improves a competitive neural unsupervised parser (DIORA; Drozdov et al. 2019) using span constraints defined on a portion of the training data. In the large majority of cases, the number of spans constraints per sentence is much lower than that specified by a full parse tree. We find that entity spans are effective as constraints, and can readily be acquired from existing data or derived from a gazetteer. In our experiments, we use DIORA as our baseline and improve upon it by injecting these span constraints as a source of distant supervision. We introduce a new method for training DIORA that leverages the structured SVM loss often used in supervised constituency parsi"
2021.emnlp-main.395,D13-1057,0,0.0285768,"traint injection methods were usually used as an added loss to the supervised loss function. In this work, we show that the distant supervision through constraint injection is beneficial for unsupervised setting as well. Structural SVM with Latent Variables The PS-SVM loss we introduce in this work can be loosely thought of as an application-specific instantiation of Structural SVM with Latent Variables (Yu and Joachims, 2009). Various works have extended Structural SVM with Latent Variables to incorporate constraints for tasks such as sequence labeling (Yu, 2012) and co-reference resolution (Chang et al., 2013), although none we have seen focus on unsupervised constituency parsing. Perhaps a more clear distinction is that Yu and Joachims (2009) focuses on latent variables within supervised tasks, and PS-SVM is meant to improve convergence of an unsupervised learning algorithm (i.e., DIORA). Additional Related Work In Appendix A.3 we list additional work in unsupervised parsing not already mentioned. 7 Conclusion In this work, we present a method for enhancing DIORA with distant supervision from span constraints. We call this approach Partially Structured SVM (PS-SVM). We find that span constraints b"
2021.emnlp-main.395,W01-0713,0,0.419018,"Missing"
2021.emnlp-main.395,N04-4028,1,0.276541,"us properties of the above sentence make it difficult to parse. For instance, the sentence construction lacks syntactic cues and there is no verb in the sentence. There is also substantial ambiguity with respect to hyphenation, and the second hyphen is acting as a colon. These properties make it difficult to capture the spans (skeletal - muscle) or the second (HIF - 1↵) despite being constraints. 6 Related Work Learning from Partially Labeled Corpora Pereira and Schabes (1992) modify the insideoutside algorithm to respect span constraints. Similar methods have been explored for training CRFs (Culotta and McCallum, 2004; Bellare and McCallum, 2007). Rather than modify the weight assignment in DIORA, which is inspired by the inside-outside algorithm, we supervise the tree predicted from the inside-pass. Concurrent work to ours in distant supervision trains RoBERTa for constituency parsing using answer spans from question-answering datasets and wikipedia hyperlinks (Shi et al., 2021). Although effective, their approach depends entirely on the set of constraints. In contrast, PS-SVM enhances DIORA, which is a model that outputs a parse tree without any supervision. The span constraints in this work are derived"
2021.emnlp-main.395,N19-1114,0,0.0193451,"xt, since it is a shift in domain from the DIORA pre-training, we first train for 20 epochs using a concatenation of MedMentions and CRAFT data with only the reconstruction loss5 (called DIORAf t for “fine-tune”). Then, we train for 40 epochs like previously mentioned, using performance on a subset of 3k random sentences from the CRAFT training data for early stopping. Hyperparameters are in Appendix A.2. 5 The training jointly with MedMentions and CRAFT is a special case of “intermediate fine-tuning” (Phang et al., 2018). F1 General Purpose Ordered Neuron† (Shen et al., 2019) Compound PCFG† (Kim et al., 2019a) DIORA‡ (Drozdov et al., 2019) S-DIORA† (Drozdov et al., 2020) 48.1 ±1.0 55.2 ±2.5 56.8 57.6 ±3.2 Constituency Tests RoBERTa† (Cao et al., 2020) 62.8 ±1.6 DIORA Span Constraints +CCKY +PS-SVM NCBL +PS-SVM M INDIFF +PS-SVM R ESCALE +PS-SVM S TRUCTURE R AMP 57.5 60.4 ±0.1 59.0 ±0.8 61.2 ±0.6 59.9 ±1.0 Table 2: Parsing F1 on PTB. The average F1 across random seeds is measured on the test set, and the standard deviation is shown as subscript when applicable. †: Indicates that standard deviation is the approximate lower bound derived from the mean, max, and number of random seeds. ‡: Indicates no"
2021.emnlp-main.395,P18-1249,0,0.144117,"defined on a portion of the training data. In the large majority of cases, the number of spans constraints per sentence is much lower than that specified by a full parse tree. We find that entity spans are effective as constraints, and can readily be acquired from existing data or derived from a gazetteer. In our experiments, we use DIORA as our baseline and improve upon it by injecting these span constraints as a source of distant supervision. We introduce a new method for training DIORA that leverages the structured SVM loss often used in supervised constituency parsing (Stern et al., 2017; Kitaev and Klein, 2018), but only depends on partial structure. We refer to this method as partially structured SVM (PS-SVM). Our experiments indicate PS-SVM improves upon unsupervised parsing performance as the model adjusts its prediction to incorporate span constraints (depicted in Figure 1). Using ground-truth entities from Ontonotes (Pradhan et al., 2012) as constraints, we achieve more than 5 F1 improvement over DIORA when parsing English WSJ Penn Treebank (Marcus et al., 1993). Using automatically extracted span constraints from an entity-based lexicon (i.e. gazetteer) is an easy alternative to ground truth a"
2021.emnlp-main.395,P04-1061,0,0.226258,"Missing"
2021.emnlp-main.395,P92-1017,0,0.729626,"ructure. Here is a typical sentence and ground truth parse for that case: ((HIF - 1↵) KO) - ((skeletal - muscle) (HIF - 1↵) knockout mouse) Various properties of the above sentence make it difficult to parse. For instance, the sentence construction lacks syntactic cues and there is no verb in the sentence. There is also substantial ambiguity with respect to hyphenation, and the second hyphen is acting as a colon. These properties make it difficult to capture the spans (skeletal - muscle) or the second (HIF - 1↵) despite being constraints. 6 Related Work Learning from Partially Labeled Corpora Pereira and Schabes (1992) modify the insideoutside algorithm to respect span constraints. Similar methods have been explored for training CRFs (Culotta and McCallum, 2004; Bellare and McCallum, 2007). Rather than modify the weight assignment in DIORA, which is inspired by the inside-outside algorithm, we supervise the tree predicted from the inside-pass. Concurrent work to ours in distant supervision trains RoBERTa for constituency parsing using answer spans from question-answering datasets and wikipedia hyperlinks (Shi et al., 2021). Although effective, their approach depends entirely on the set of constraints. In co"
2021.emnlp-main.395,P11-1108,0,0.0782594,"Missing"
2021.emnlp-main.395,W12-4501,0,0.265538,"as our baseline and improve upon it by injecting these span constraints as a source of distant supervision. We introduce a new method for training DIORA that leverages the structured SVM loss often used in supervised constituency parsing (Stern et al., 2017; Kitaev and Klein, 2018), but only depends on partial structure. We refer to this method as partially structured SVM (PS-SVM). Our experiments indicate PS-SVM improves upon unsupervised parsing performance as the model adjusts its prediction to incorporate span constraints (depicted in Figure 1). Using ground-truth entities from Ontonotes (Pradhan et al., 2012) as constraints, we achieve more than 5 F1 improvement over DIORA when parsing English WSJ Penn Treebank (Marcus et al., 1993). Using automatically extracted span constraints from an entity-based lexicon (i.e. gazetteer) is an easy alternative to ground truth annotation and gives 2 F1 improvement over DIORA. Importantly, training DIORA with PS-SVM is more effective than simply injecting available constraints into parse tree decoding at test time. We also conduct experiments with different types of span constraints. Our detailed analysis shows that entity-based constraints are similarly useful"
2021.emnlp-main.395,W09-1119,0,0.0117968,"is not available, but partial span constraints are readily available. PMI Constraints We use the phrases defined in the vocab from Mikolov et al. (2013) as a lexicon, treating exact matches found in Ontonotes as constraints. The phrases are learned through word statistics by applying pointwise mutual information (PMI) to find relevant bi-grams, then replacing these bi-grams with a new special token representing the phrase — applied multiple times this technique is used to find arbitrarily long phrases. Gazetteer We use a list of 1.5 million entity names automatically extracted from Wikipedia (Ratinov and Roth, 2009), which has been effective for supervised entity-centric tasks with both log-linear and neural models (Liu et al., 2019a). We derive constraints by finding exact matches in the Ontonotes corpus that are in the gazetteer. A lexicon containing entity names is often called a gazetteer. 4.2 Training Details In all cases, we initialize our model’s parameters from pre-trained DIORA (Drozdov et al., 2019). We then continue training using a combination of the reconstruction and PS-SVM loss. Given sentence x and constraints z, the instance loss is: J(x, z) = Jrec (x) + JP S (x, z) For the newswire doma"
2021.emnlp-main.395,2020.acl-main.722,0,0.0197238,"ts. We hope our findings will help “bridge the gap” between supervised and unsupervised parsing. Broader Impact be acquired at reduced cost or even automatically extracted. The gazetteer used in our experiments is automatically extracted from Wikipedia, and our experiments are only for English, which is the language with by far the most Wikipedia entries. Although, similarly sized gazetteers may be difficult to attain in other languages, Mikheev et al. (1999) point out larger gazetteers do not necessarily boost performance, and gazetteers have already proven effective in low-resource domains (Rijhwani et al., 2020). In any case, we use gazetteers in the most naive way by finding exact text matches. When extending our approach to other languages, an entity recognition model may be a suitable replacement for the gazetteer. Acknowledgements We are grateful to our colleagues at UMass NLP and the anonymous reviewers for feedback on drafts of this work. This work was supported in part by the Center for Intelligent Information Retrieval, in part by the Chan Zuckerberg Initiative, in part by the IBM Research AI through the AI Horizons Network, and in part by the National Science Foundation (NSF) grant numbers D"
2021.emnlp-main.395,P05-1044,0,0.324887,"Missing"
2021.emnlp-main.395,P09-1009,0,0.0732978,"Missing"
2021.emnlp-main.395,P17-1076,0,0.0235161,"ng span constraints defined on a portion of the training data. In the large majority of cases, the number of spans constraints per sentence is much lower than that specified by a full parse tree. We find that entity spans are effective as constraints, and can readily be acquired from existing data or derived from a gazetteer. In our experiments, we use DIORA as our baseline and improve upon it by injecting these span constraints as a source of distant supervision. We introduce a new method for training DIORA that leverages the structured SVM loss often used in supervised constituency parsing (Stern et al., 2017; Kitaev and Klein, 2018), but only depends on partial structure. We refer to this method as partially structured SVM (PS-SVM). Our experiments indicate PS-SVM improves upon unsupervised parsing performance as the model adjusts its prediction to incorporate span constraints (depicted in Figure 1). Using ground-truth entities from Ontonotes (Pradhan et al., 2012) as constraints, we achieve more than 5 F1 improvement over DIORA when parsing English WSJ Penn Treebank (Marcus et al., 1993). Using automatically extracted span constraints from an entity-based lexicon (i.e. gazetteer) is an easy alte"
2021.emnlp-main.395,D18-1412,0,0.0219915,"an 100 span recall in Table 4. This approach to model training is often called “distant supervision” (Mintz et al., 2009; Shi et al., 2021). In contrast, “partial supervision” implies gold partial labels are available, which we explore as synthetic data (§5.4), but in general do not make this assumption. Joint Supervision An implicit way to incorporate constraints is through multi-task learning (MTL; Caruana, 1997). Even when relations between the tasks are not modeled explicitly, MTL has shown promise throughout a range of text processing tasks with neural models (Collobert and Weston, 2008; Swayamdipta et al., 2018; Kuncoro et al., 2020). Preliminary experiments with joint NER did not improving parsing results. This is in-line with DIORA’s relative weakness in representing fine-grained entity types. Modifications of DIORA to improve its semantic representation may prove to make joint NER more viable. 5.5.3 Parsing of PTB vs. CRAFT As mentioned in §5.5.1, there is considerable dif- Constraint Injection Methods There exists a ference in the text between PTB and CRAFT. It rich literature in constraint injection (Ganchev follows that there would be a difference in diffi- et al., 2010; Chang et al., 2012) ."
2021.emnlp-main.395,N12-1087,0,0.0252074,"nt Injection Methods There exists a ference in the text between PTB and CRAFT. It rich literature in constraint injection (Ganchev follows that there would be a difference in diffi- et al., 2010; Chang et al., 2012) . Both methods culty when parsing these two types of data. After are based on Expectation Maximization (EM) algorunning the parser from Kitaev and Klein (2018) on rithm (Dempster et al., 1977) where the constraint each dataset, it appears CRAFT is more difficult to is injected in the E-step of calculating the posterior parse than PTB. For CRAFT, the unlabeled parsing distribution (Samdani et al., 2012). Another line F1 is 81.3 and the span recall for entities is 37.6. of work focuses injecting constraint in the M-step For PTB, the unlabeled parsing F1 is 95. (Lee et al., 2019; Mehta et al., 2018) by reflecting 4825 the degree of constraint satisfaction of prediction as the weight of the gradient. Our approach is similar to Chang et al. (2012) as we select the highest scoring output that satisfies constraints and learn from it. PS-SVMR ESCALE is based on Lee et al. (2019). The aforementioned constraint injection methods were usually used as an added loss to the supervised loss function. In t"
2021.emnlp-main.395,Q18-1019,1,0.8471,"Missing"
2021.emnlp-main.395,2020.emnlp-main.196,0,0.0235256,"ically more effective. a large endeavor. For example, the 20k sentences of biomedical treebanking in the CRAFT corpus 1 Introduction required 80 annotator hours per week for 2.5 years, Syntactic parse trees are helpful for various down- include 6 months for annotator training (Verspoor stream tasks such as speech recognition (Moore et al., 2011). However, although many domains et al., 1995), machine translation (Akoury et al., and many languages lack full treebanks, they do 2019), paraphrase generation (Iyyer et al., 2018), often have access to other annotated resources such semantic parsing (Xu et al., 2020), and informa- as NER, whose spans might provide some partial tion extraction (Naradowsky, 2014). While super- syntactic supervision. We explore whether unsuvised syntactic parsers are state-of-the-art models pervised parsing methods can be enhanced with for creating these parse trees, their performance distant supervision from such spans to enable the does not transfer well across domains. Moreover, types of benefits afforded by supervised syntactic new syntactic annotations are prohibitively expen- parsers without the need for expensive syntactic sive; the original Penn Treebank required eig"
2021.emnlp-main.395,P19-1180,0,0.0240996,"Missing"
2021.emnlp-main.395,2021.naacl-main.234,0,0.0588775,"Missing"
2021.emnlp-main.462,2021.emnlp-main.468,0,0.0368326,"ne-tuning: Finetuning has been the most common approach for applying pre-trained language models to downstream tasks. However, it typically requires a target dataset of thousands to hundreds of thousands of examples to work well (Yogatama et al., 2019; Brown et al., 2020). Many methods have been proposed to improve performance and stability of pre-trained language models on small datasets, including language model fine-tuning on unlabeled data from the target domain (Howard and Ruder, 2018; Gururangan et al., 2020), intermediate-task fine-tuning (Phang et al., 2019), multi-task prefinetuning (Aghajanyan et al., 2021a), better design choices and training strategies (Mosbach et al., 2021; Zhang et al., 2021), and regularizationoriented techniques (Jiang et al., 2020; Aghajanyan et al., 2021b). More related to our work is research on intermediate-task fine-tuning that makes use of data-rich tasks (Phang et al., 2019), tasks that require complex reasoning and inference (Pruksachatkun et al., 2020), and beneficial relationships among tasks (Vu et al., 2020). Few-shot learning: Our work also relates to research in few-shot learning. In previous work, fine-tuning is combined with other learning strategies to im"
2021.emnlp-main.462,2020.emnlp-main.38,0,0.0349921,"and regularizationoriented techniques (Jiang et al., 2020; Aghajanyan et al., 2021b). More related to our work is research on intermediate-task fine-tuning that makes use of data-rich tasks (Phang et al., 2019), tasks that require complex reasoning and inference (Pruksachatkun et al., 2020), and beneficial relationships among tasks (Vu et al., 2020). Few-shot learning: Our work also relates to research in few-shot learning. In previous work, fine-tuning is combined with other learning strategies to improve few-shot performance, including consistency training (Xie et al., 2020a), metalearning (Bansal et al., 2020), self-training (Du et al., 2021; Sun et al., 2020), and contrastive learning (Gunel et al., 2021). Other work has focused on prompt-based/entailment-based few-shot learning approaches (Brown et al., 2020; Schick and Schütze, 2021; Gao et al., 2021; Tam et al., 2021; Wang et al., 2021). Notably, Brown et al. (2020) demonstrate remarkable few-shot learning performance with a single frozen GPT-3 model, although its performance still lags far behind state-of-the-art fine-tuning results. question answering (Puri et al., 2020), and commonsense reasoning (Yang et al., 2020). Yang et al. (2020) show"
2021.emnlp-main.462,D15-1075,0,0.119149,"Missing"
2021.emnlp-main.462,P19-1439,0,0.0546236,"Missing"
2021.emnlp-main.462,D19-1670,0,0.028611,"ich allows us to train a reliable data generator. Generating synthetic NLI data: To obtain an NLI data generator, we fine-tune the pre-trained T53B model (Raffel et al., 2020) on MNLI, which contains 393K sentence pairs labeled as {entailment, contradiction, neutral}. We cast each MNLI training example (sentA , sentB ) → label into a textto-text format (label, sentA ) → sentB to ob3 This process differs from traditional data augmentation approaches (e.g., lexical substitution, or back-translation), which yield negligible improvements when combined with large-scale pre-trained language models (Wei and Zou, 2019; Yang et al., 2020). 4 Traditional data augmentation is a special case of our framework where the auxiliary task is identical to the target task (A ≡ T ). tain fine-tuning examples that look like [entailment, the facts are accessible to you → you have access to the facts].5 We fine-tune T5 on this dataset with a constant learning rate of 0.001 for 216 = 65, 536 steps using the Adafactor optimizer (Shazeer and Stern, 2018). The fine-tuned T5 data generator produces augmented examples for all target datasets. Specifically, at inference time, we feed the model an NLI label (e.g., entailment) and"
2021.emnlp-main.462,N18-1101,0,0.14579,"TA, an approach that combines two complementary methods, SelfTraining and Task Augmentation, to effectively leverage unlabeled data, which is comparatively cheaper to obtain.1 At a high level, task augmentation exploits unlabeled text from the domain of a given target task to simulate a large amount of in-domain training data for the auxiliary task of natural language inference (NLI), which is then used to train a given model before applying it to the target task. To achieve this, we first build an NLI data generator by fine-tuning a pre-trained generative language model on the MNLI data set (Williams et al., 2018) in a text-to-text format. Then, given a target task (e.g., sentiment analysis) with unlabeled 1 Our code and pre-trained models will be available at https://github.com/google-research/google-research/tree/ master/STraTA. 5715 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5715–5731 c November 7–11, 2021. 2021 Association for Computational Linguistics Task Augmentation Self-training Teacher Model Inference Auxiliary-task Model Pre-trained Language Model Labeled Data Pseudo-labeled Data Use a broad distribution Task-specific Unlabeled Texts Data Ge"
2021.emnlp-main.462,2020.findings-emnlp.90,0,0.0435509,"Missing"
2021.emnlp-main.483,N19-1124,0,0.0255408,"and coherent text has been demonstrated in several prior works (Radford et al., 2019; Brown et al., 2020; Zellers et al., 2019) when given only a few words or a sentence as a prompt. More recent research has addressed the inability of these models to infill text, or insert new words/tokens between tokens that already exist (Donahue et al., 2020; Zhu et al., 2019; Huang et al., 2020; Stern et al., 2019; Welleck et al., 2019; Zhang et al., 2020; Liao et al., 2020; Moryossef et al., 2019). In this vein, Rashkin et al. (2020) generate coherent stories given just bullet-point plot outlines, while Cai et al. (2019) perform token insertion using a retrieval engine in combination with a language model for dialogue agents. Unlike IGA, however, none of this prior work can control generation using high-level rhetorical directives specified by an author. 2.3 Controllable Text Generation IGA conditions its generated text on tags, which has previously been done for left-to-right language models. For example, Dathathri et al. Numerous studies within the humanities focus on (2020) combine a large language model with an modeling the process of effective writing (Flower attribute discriminator to generate text that"
2021.emnlp-main.483,P02-1040,0,0.111848,"onstrained outputs. Since the infilled spans of BIO are strictly post-modifiers that follow a very specific structure (i.e., enclosed by two commas), the superior performance of ILM indicates that it memorizes this simple form of construction without requiring a separate tag input. PARA is the only substitution-based tag in our system and is not supported by ILM. Therefore, we compare performance of PARA with the stateof-the-art paraphraser STRAP released by Krishna et al. (2020) with the default nucleus sampling p = 0.6. We compute BLEURT scores to check semantic similarity, as well as BLEU (Papineni et al., 2002), self-BLEU (Sun and Zhou, 2012), and iBLEU (Sun and Zhou, 2012) with α = 0.8 to check the diversity of output. Table 4 indicates that IGA outperforms STRAP in all dimensions. We hypothesize that this is primarily because the diverse paraphraser in STRAP normalizes (and often simplifies) stylized text, while our PARA tag is associated with complex, embellished paraphrases during fine-tuning. 7 All automatic metrics are computed only on the infilled spans, excluding the context. Intrinsic crowdsourced evaluation The above automatic evaluations can only tell us so much about IGA’s capabilities."
2021.emnlp-main.483,2020.emnlp-main.55,1,0.819061,"Missing"
2021.emnlp-main.483,2020.emnlp-main.349,0,0.0598188,"Missing"
2021.emnlp-main.483,2020.acl-main.703,0,0.0275975,"Given context, by specifying different writing intents, the system generates output satisfying the intent. In addition to wellformed sentence fragments, keywords can also be part of user input, serving as arguments for the intents, and are preserved in the output. specifically, we build an authoring assistant capable of following fine-grained user directives (e.g., add descriptive text, use idiomatic language, or paraphrase a clunky bit of wording). Our system, the Intent-Guided Assistant (IGA), combines controllable text generation with text infilling (Zhu et al., 2019; Keskar et al., 2019a; Lewis et al., 2020; Donahue et al., 2020); more specifically, we adapt the tag-based control of Keskar et al. (2019b) to include a set of rhetorical directives that our model learns to infill with relevant and fluent text. Our system can handle the following authorguided tags: cause, effect, concession (contrast), description, biography, idiom, and rephrase. User input to IGA can be as simple as a list of keywords and does not have to include well-formed text (Figure 1). We train IGA in supervised fashion by creating a large multi-domain dataset in which spans corresponding to particular directives are ∗ Most o"
2021.emnlp-main.483,2020.acl-main.24,0,0.0248466,"actual implementation of IGA relies on controllable text infilling via language modeling. The ability of large-scale language models to generate fluent and coherent text has been demonstrated in several prior works (Radford et al., 2019; Brown et al., 2020; Zellers et al., 2019) when given only a few words or a sentence as a prompt. More recent research has addressed the inability of these models to infill text, or insert new words/tokens between tokens that already exist (Donahue et al., 2020; Zhu et al., 2019; Huang et al., 2020; Stern et al., 2019; Welleck et al., 2019; Zhang et al., 2020; Liao et al., 2020; Moryossef et al., 2019). In this vein, Rashkin et al. (2020) generate coherent stories given just bullet-point plot outlines, while Cai et al. (2019) perform token insertion using a retrieval engine in combination with a language model for dialogue agents. Unlike IGA, however, none of this prior work can control generation using high-level rhetorical directives specified by an author. 2.3 Controllable Text Generation IGA conditions its generated text on tags, which has previously been done for left-to-right language models. For example, Dathathri et al. Numerous studies within the humanities"
2021.emnlp-main.483,2020.acl-main.704,0,0.015412,"Tufte, 2006). Unlike sentence simplification, the intent of our &lt;paraphrase&gt; tag is to paraphrase with improved writing quality, similar to embellishment. We construct parallel data for this tag by combining ParaNMT-50M (Wieting and Gimpel, 2018), a large corpus consisting of back-translated sentence pairs, with WikiLarge, a sentence simplification dataset with parallel simple and complex sentences. The original sentence in ParaNMT-50M and complex sentence in WikiLarge are treated as targets, while the back-translated sentence and the simplified sentence are used as the source. We use BLEURT (Sellam et al., 2020) to filter noisy pairs from ParaNMT-50M,6 discarding pairs whose word-level edit distance is less than five. To further encourage complex paraphrases, we require the reference sentence to have more lowfrequency words than the candidate sentence. 5 Evaluation against references As an initial comparison of IGA and ILM, we evaluate the generated outputs of each model against reference completions from our dataset, both automatically and through a crowdsourced evaluation. We acknowledge that this type of evaluation (especially using automatic metrics) is limited for open-ended generation tasks lik"
2021.emnlp-main.483,N19-1351,0,0.0370365,"Missing"
2021.emnlp-main.483,W04-1013,0,0.0174926,"a crowdsourced evaluation. We acknowledge that this type of evaluation (especially using automatic metrics) is limited for open-ended generation tasks like ours (Fan et al., 2018; Akoury et al., 2020; Rashkin et al., 2020), which is why we also conduct an in-depth user study in Section 6. While results of these evaluations cannot reflect how practical IGA can be used as an authoring assistant, they do indicate that IGA is more constrained than ILM and produces output that better fulfills the writing intents. 5.1 Automatic evaluation We compare IGA with ILM on automatic metrics such as ROUGE (Lin, 2004) and self-BLEU (Zhu et al., 2018) following Rashkin et al. (2020), computing both scores against reference completions 6 We set BLEURT threshold to be (0.7, 0.9) to avoid semantically dissimilar sentences and sentences without too much change. 5976 ROUGE-2 BIO CAUSE EFFECT CNTRA DESCP IDIOM BLEU-2 5.2 Length ILM IGA ILM IGA ILM IGA 10.4 4.1 5.2 4.2 2.1 33.7 9.9 9.0 6.6 4.9 2.2 37.8 47.7 35.0 37.2 32.3 23.4 62.3 44.2 37.1 37.8 34.6 23.6 64.5 6.3 10.1 13.2 10.3 8.9 3.0 6.0 10.2 13.4 10.3 8.9 2.7 Table 3: ROUGE-2, self-BLEU2, and total number of infilled tokens of each example on test set. STRAP"
2021.emnlp-main.483,2020.acl-main.173,0,0.0189018,"ask. Ethics statement Our data collection is for research purposes only, and thus consistent with the terms of use of all source corpora we mined. For the evaluation process, we strive to compensate the Mechanical Turk workers as well as participants of our user study with competitive payments. The intended use of IGA is for creative writing. Although generating factually-correct output is not a major focus of creative writing tasks, IGA often hallucinates facts about real-world entities, a phenomenon that raises ethical concerns and has become an increasing focus in text generation research (Maynez et al., 2020; Wang and Sennrich, 2020). The model can on rare occasions produce offensive outputs, due in large part to GPT-2’s pretraining corpora. One potential way to reduce the toxicity of output is to apply profanity filter as a post-processing step before final output is returned. Acknowledgements We thank the reviewers for the thoughtful comments. We thank Andrew Drozdov, Katherine Thai, Nicholas Monath and other UMass computer science graduate students for helping us with the user study. We thank UMass NLP group for the great advice on the initial draft of this paper. MI was partially supported by"
2021.emnlp-main.483,P12-2008,0,0.00983197,"led spans of BIO are strictly post-modifiers that follow a very specific structure (i.e., enclosed by two commas), the superior performance of ILM indicates that it memorizes this simple form of construction without requiring a separate tag input. PARA is the only substitution-based tag in our system and is not supported by ILM. Therefore, we compare performance of PARA with the stateof-the-art paraphraser STRAP released by Krishna et al. (2020) with the default nucleus sampling p = 0.6. We compute BLEURT scores to check semantic similarity, as well as BLEU (Papineni et al., 2002), self-BLEU (Sun and Zhou, 2012), and iBLEU (Sun and Zhou, 2012) with α = 0.8 to check the diversity of output. Table 4 indicates that IGA outperforms STRAP in all dimensions. We hypothesize that this is primarily because the diverse paraphraser in STRAP normalizes (and often simplifies) stylized text, while our PARA tag is associated with complex, embellished paraphrases during fine-tuning. 7 All automatic metrics are computed only on the infilled spans, excluding the context. Intrinsic crowdsourced evaluation The above automatic evaluations can only tell us so much about IGA’s capabilities. Many of our tags (e.g., DESCP, C"
2021.emnlp-main.483,2020.acl-main.326,0,0.012549,"Our data collection is for research purposes only, and thus consistent with the terms of use of all source corpora we mined. For the evaluation process, we strive to compensate the Mechanical Turk workers as well as participants of our user study with competitive payments. The intended use of IGA is for creative writing. Although generating factually-correct output is not a major focus of creative writing tasks, IGA often hallucinates facts about real-world entities, a phenomenon that raises ethical concerns and has become an increasing focus in text generation research (Maynez et al., 2020; Wang and Sennrich, 2020). The model can on rare occasions produce offensive outputs, due in large part to GPT-2’s pretraining corpora. One potential way to reduce the toxicity of output is to apply profanity filter as a post-processing step before final output is returned. Acknowledgements We thank the reviewers for the thoughtful comments. We thank Andrew Drozdov, Katherine Thai, Nicholas Monath and other UMass computer science graduate students for helping us with the user study. We thank UMass NLP group for the great advice on the initial draft of this paper. MI was partially supported by award IIS-1955567 from th"
2021.emnlp-main.483,W19-3620,0,0.0285998,"rocess. 2.2 Infilling language models The actual implementation of IGA relies on controllable text infilling via language modeling. The ability of large-scale language models to generate fluent and coherent text has been demonstrated in several prior works (Radford et al., 2019; Brown et al., 2020; Zellers et al., 2019) when given only a few words or a sentence as a prompt. More recent research has addressed the inability of these models to infill text, or insert new words/tokens between tokens that already exist (Donahue et al., 2020; Zhu et al., 2019; Huang et al., 2020; Stern et al., 2019; Welleck et al., 2019; Zhang et al., 2020; Liao et al., 2020; Moryossef et al., 2019). In this vein, Rashkin et al. (2020) generate coherent stories given just bullet-point plot outlines, while Cai et al. (2019) perform token insertion using a retrieval engine in combination with a language model for dialogue agents. Unlike IGA, however, none of this prior work can control generation using high-level rhetorical directives specified by an author. 2.3 Controllable Text Generation IGA conditions its generated text on tags, which has previously been done for left-to-right language models. For example, Dathathri et al."
2021.emnlp-main.483,P18-1042,0,0.0643661,"put: There are individual and social beliefs that should lead us to be skeptical of the facts and the wrong. Input: This report only shows the &lt;idiom&gt; , as many details can only be uncovered if you sign the document. Output: This report only shows the tip of the iceberg , as many details can only be uncovered if you sign the document. BIO CAUSE EFFECT CNTRA DESCP IDIOM Table 2: Example output of each tag from IGA. tically mining the N EWSROOM corpus (Grusky et al., 2018), the largest available summarization dataset with 1.3 million news articles. We also collect partial data from ParaNMT-50M (Wieting and Gimpel, 2018), WikiLarge (Zhang and Lapata, 2017) for “sentence embellishment” writing intent, and PoMo (Kang et al., 2019) to extract postmodifier that comes after an entity. Our dataset (statistics shown in Table 1) contains 75M tokens with a mean example length of 60.5 words tokenized with NLTK (Bird et al., 2009). volve open-ended generation loosely constrained by keywords and intent. 4.1 Data collection for each writing intent CAUSE: This tag helps an author invent a reason for the occurrence of an event. Clauses with CAUSE intent usually follow words/phrases like ‘because’ or ‘due to’. We manually ex"
2021.emnlp-main.483,2020.emnlp-main.226,0,0.0426399,"crip&gt; roof. (2) Post-processing: Identify tags The wind blew over the farm, the rain came down and &lt;descrip&gt; pings &lt;descrip&gt; roof. &lt;sep&gt; she heard ominous &lt;answer&gt; on the &lt;answer&gt; &lt;eos&gt; The wind blew over the farm, the rain came down and &lt;descrip&gt; pings &lt;descrip&gt; roof. &lt;sep&gt; (1) Fine-tuned GPT-2 generates output prefixed with input and &lt;sep&gt; Inference time Figure 2: On the left, we show how each example is constructed for fine-tuning. On the right, we show how the final output is constructed by post-processing the output of a fine-tuned GPT-2 model at inference time. the Megatron-CNTRL model (Xu et al., 2020) control the output with predicted keyword. In contrast to these works, IGA focuses on finegrained, intra-sentential controlled infilling. Previous work has also explored controlling stylistic parameters (Ficler and Goldberg, 2017) and syntactic structures (Iyyer et al., 2018; Goyal and Durrett, 2020). 3 Intent-Guided Assistant IGA extends text infilling models with finegrained rhetorical control. Specifically, we build on the Infilling Language Model (ILM) of Donahue et al. (2020), which fine-tunes an off-theshelf language model such as GPT-2 on a dataset of text with masked spans. To continu"
2021.emnlp-main.483,D17-1062,0,0.0245485,"eliefs that should lead us to be skeptical of the facts and the wrong. Input: This report only shows the &lt;idiom&gt; , as many details can only be uncovered if you sign the document. Output: This report only shows the tip of the iceberg , as many details can only be uncovered if you sign the document. BIO CAUSE EFFECT CNTRA DESCP IDIOM Table 2: Example output of each tag from IGA. tically mining the N EWSROOM corpus (Grusky et al., 2018), the largest available summarization dataset with 1.3 million news articles. We also collect partial data from ParaNMT-50M (Wieting and Gimpel, 2018), WikiLarge (Zhang and Lapata, 2017) for “sentence embellishment” writing intent, and PoMo (Kang et al., 2019) to extract postmodifier that comes after an entity. Our dataset (statistics shown in Table 1) contains 75M tokens with a mean example length of 60.5 words tokenized with NLTK (Bird et al., 2009). volve open-ended generation loosely constrained by keywords and intent. 4.1 Data collection for each writing intent CAUSE: This tag helps an author invent a reason for the occurrence of an event. Clauses with CAUSE intent usually follow words/phrases like ‘because’ or ‘due to’. We manually extracted 16 markers, many from the di"
2021.emnlp-main.483,2020.emnlp-main.698,0,0.0280605,"Missing"
2021.emnlp-main.483,J11-1005,0,0.024604,"writing intent CAUSE: This tag helps an author invent a reason for the occurrence of an event. Clauses with CAUSE intent usually follow words/phrases like ‘because’ or ‘due to’. We manually extracted 16 markers, many from the discourse marker list in Sileo et al. (2019), and then mine N EWSROOM (Grusky et al., 2018) to find sentences that match any of the markers. For all mined examples, we also preserve the previous sentence as the context of the matched sentence. Simple declarative clauses that start with matched discourse markers are extracted through shift-reduce constituency parser ZPar (Zhang and Clark, 2011). The YAKE algorithm is later applied to those clauses for keyword extraction. Choosing a collection of tags: Before we start collecting data, we conduct an internal survey with potential users of our system to determine what writing assistance functions they would most benefit from. We surveyed nine NLP researchers about their opinions on the ideal functionality of an authoring assistant. After removing simple functions such as generating synonyms, antonyms, adjectives, and adverbs, which are already impleEFFECT: As a conjugate writing intent of CAUSE, EFFECT is used when one needs to demente"
2021.emnlp-main.62,J95-2003,0,0.831751,"duct a series of fine-grained analysis experiments to answer these questions, several of which are inspired by the context analysis of LSTM LMs conducted by Khandelwal et al. (2018). We focus specifically on analyzing the behavior of the state-of-the-art Routing Transformer and a simpler baseline model in the presence of various perturbations (e.g., word shuffling, random document replacement), and look closely at how different types of tokens are affected. Our results show that: Understanding long documents requires modeling various discourse-level phenomena, including anaphora (Hobbs, 1979; Grosz et al., 1995), argument structure (Grimshaw, 1990), narrative scripts and trajectories (Schank and Abelson, 1977; Labov and Waletzky, 1997), and causal links between concepts (Mooney and DeJong, 1985). Unfortunately, most language models (LMs) are trained to predict the next word given only a small window of local context, which prevents them from using long-range discourse structure to improve their predictions. Many research efforts over the 807 • Providing long-range context (i.e., further than 2K tokens away) to these models has negProceedings of the 2021 Conference on Empirical Methods in Natural Lang"
2021.emnlp-main.62,P18-1027,0,0.124084,"(Roy et al., 2021, Routing Transformer). When evaluated on the PG-19 benchmark dataset (Rae et al., 2020), which contains long documents in the public domain, these “long-range” Transformer LMs also reach lower perplexities than baseline models on held-out data. How do “long-range” Transformer LMs make use of the long-range context? Do they actually encode important discourse information to improve their predictions? In this paper, we conduct a series of fine-grained analysis experiments to answer these questions, several of which are inspired by the context analysis of LSTM LMs conducted by Khandelwal et al. (2018). We focus specifically on analyzing the behavior of the state-of-the-art Routing Transformer and a simpler baseline model in the presence of various perturbations (e.g., word shuffling, random document replacement), and look closely at how different types of tokens are affected. Our results show that: Understanding long documents requires modeling various discourse-level phenomena, including anaphora (Hobbs, 1979; Grosz et al., 1995), argument structure (Grimshaw, 1990), narrative scripts and trajectories (Schank and Abelson, 1977; Labov and Waletzky, 1997), and causal links between concepts"
2021.emnlp-main.62,2021.naacl-main.393,1,0.712246,"x), a language model (LM) computes the probability distribution of the next token p(wi |w&lt;i ). LMs are commonly evaluated using perplexity, which is the exponentiated negative log likelihood of a held-out corpus:  ppl = exp  N 1 X log p(wi |w&lt;i ) − N i=1 X Aij Vj j:Kj ∈µ(Qi ), j&lt;i In contrast to the position-based local attention, this clustering-based attention takes the content of the token representations into account. This sparse self-attention mechanism reduces the complexity from O(N 2 ) to O(N 1.5 ) and has led to state-ofthe-art results on tasks such as long-form question answering (Krishna et al., 2021). 2.2 Experimental setup While the previously-described models can be trained with longer inputs than standard Transformers, it remains unclear how they make use of the additional context tokens to improve their predictions of the next word. To shed light on the behavior of long-range Transformer LMs, we perform a series of experiments in which we manipulate the input sequence (both length and content). For token-level experiments (§ 3, § 4), we only evaluate the perplexity of k tokens near the end1 of an N Modern LMs are most often implemented with Transformers (Vaswani et al., 2017), which c"
2021.emnlp-main.62,2020.findings-emnlp.338,0,0.0409865,"ong-range context. for more details. 814 Hofstätter et al., 2020; Zhang et al., 2020). This work is also similar to other analysis of language models, especially for long-range context. Khandelwal et al. (2018) analyze the usage of long-term context of smaller LSTM LMs. Sharan et al. (2018) prove long-term context is not needed for HMM LM due to teacher forcing. Rae and Razavi (2020) conduct an analysis exclusively for the Transformer-XL (Dai et al., 2019) model. Rae et al. (2020) show that Compressive Transformer improves the performance of infrequent tokens. Our work also relates to that of Lai et al. (2020), who investigate the impact of context for pretrained masked LMs. More recently, Press et al. (2020) also observe negligible benefits of long-term context; we step further in this direction by exploring larger models with more fine-grained analysis. 7 Conclusion We perform a fine-grained analysis of the impact of long-range context to both token- and sequencelevel improvements on two long-range Transformer language models, using the PG-19 dataset as a testbed. Our results suggest these models rarely take advantage of the long-term context, and when they do it is mostly in superficial ways (e."
2021.emnlp-main.62,D15-1166,0,0.0622056,"ture research on long-range LMs includes analysis experiments such as those in our work to shed light on how and when they are using the distant context. 2 Local Transformer A simple way to improve the efficiency of self-attention blocks is to constrain the attention at each layer to a local window of the previous k tokens. Such Transformers, which we refer to as Local Transformers, can be feasibly scaled up to large input sequence lengths. The receptive field of a Local Transformer scales linearly with the number of layers, as the lth layer of this model can access the previous k × l tokens (Luong et al., 2015; Child et al., 2019; Sukhbaatar et al., 2019). Routing Transformer The Routing Transformer (Roy et al., 2021, RT) takes a more intelligent approach to scaling self-attention. Specifically, the RT assigns keys and queries in self-attention to k clusters, the centroids of which are learned during training. A routing attention strategy computes attention A only over the queries Qi and keys Kj that belong to the same cluster µ(Qi ) (i.e., those whose centroid is closest to query Qi ). Xi = Background & Setup In this section, we first provide an overview of the long-range language models analyzed"
2021.emnlp-main.62,1985.tmi-1.17,0,0.24367,"We focus specifically on analyzing the behavior of the state-of-the-art Routing Transformer and a simpler baseline model in the presence of various perturbations (e.g., word shuffling, random document replacement), and look closely at how different types of tokens are affected. Our results show that: Understanding long documents requires modeling various discourse-level phenomena, including anaphora (Hobbs, 1979; Grosz et al., 1995), argument structure (Grimshaw, 1990), narrative scripts and trajectories (Schank and Abelson, 1977; Labov and Waletzky, 1997), and causal links between concepts (Mooney and DeJong, 1985). Unfortunately, most language models (LMs) are trained to predict the next word given only a small window of local context, which prevents them from using long-range discourse structure to improve their predictions. Many research efforts over the 807 • Providing long-range context (i.e., further than 2K tokens away) to these models has negProceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 807–822 c November 7–11, 2021. 2021 Association for Computational Linguistics ligible impact on the perplexity of tokens near the end of a sequence in aggregate. Ho"
2021.emnlp-main.62,D18-1009,0,0.0241255,"turn for the worse. . . look!!!&quot;Her voice had fallen suddenly to a quivering whisper and she was. . . Again the Admiral burst out cheering.&quot;There remains, therefore. . . Table 1: An example of suffix identification task, the full version of this example is included in Appendix E. previously. Next, we examine the models’ ability to identify which of six 128-token suffixes follows a given prefix, which examines their behavior outside the standard teacher-forced setting. Suffix identification: To move beyond tokenlevel experiments, we adopt a similar setting as the multiple choice task in SWAG (Zellers et al., 2018). Specifically, a prefix is paired with the ground-truth next 128 tokens (or suffix) as well as five randomly sampled sequences of length 128 that come from the same book and do not occur in the prefix or gold suffix. We constrain the prefix to end at a full stop and each candidate suffix to start from a new sentence so that the difference in perplexity is not due to ungrammaticality. An example is shown in Table 1. We construct 7K examples and compute the accuracy of both models at correctly choosing the correct suffix. The model makes a correct prediction when the gold suffix has lower perpl"
2021.emnlp-main.846,2020.emnlp-main.525,1,0.884931,"sed neural topic model (PNTM). Despite its simple architecture, PNTM outperforms other topic model baselines in our human evaluation studies in terms of topic coherence and topicto-document relatedness (Table 5). 6.1 Building a topic model with Phrase-BERT Using Phrase-BERT for topic modeling We have shown that Phrase-BERT produces meaningful embeddings of variable-length phrases and a lexically diverse nearest neighbor space. In this section, we demonstrate Phrase-BERT’s utility in the downstream application of phrase-based corpus 6 We randomly choose source phrases from the Storium dataset (Akoury et al., 2020), which contains a diverse set of stories that does not appear in either the pretraining or finetuning data of Phrase-BERT, and use a vocabulary of 125K most frequent words and phrases from this dataset to compute the nearest neighbors. We integrate Phrase-BERT into previous unigrambased neural topic models (Iyyer et al., 2016; Akoury et al., 2020) that try to reconstruct a document representation through an interpretable bottleneck layer. Unlike prior implementations, computing text representations using Phrase-BERT allows us to produce high quality topic descriptions (with a mixture of words"
2021.emnlp-main.846,N19-1050,0,0.0251606,"of training steps are used as warm-up steps, following the linear warm-up schedule used by Reimers and Gurevych (2019). PPDBfiltered (global affairs, world affairs) “positive” (world affairs, domestic affairs) ‘negative” PPDB (actively participate, play an activate role) “positive&quot; PAWSshort (a variable version of the basic lyrics, a basic version of the variable lyrics) “negative&quot; 4 Experimental setup We evaluate our phrase embeddings on a diverse collection of phrase-level semantic relatedness tasks following previous works on evaluating phrase embeddings (Turney, 2012; Yu and Dredze, 2015; Asaadi et al., 2019; Yu and Ettinger, 2020). Due to a lack of benchmarks like SentEval (Conneau et al., 2018) at the phrase level, we create filtered versions of some datasets by removing lexical overlap cues. 4.1 Datasets We compare the performance of Phrase-BERT against baselines on a variety of phrases tasks involving phrases of different length and types. Turney: The dataset of Turney (2012) contains 2,180 examples that test bigram compositionality by asking models to select which of five unigrams is most similar in meaning to a given bigram. BiRD: The bigram-relatedness judgment dataset (Asaadi et al., 2019"
2021.emnlp-main.846,P15-1153,0,0.0245057,"in particular to approaches that leverage large-scale pretrained language models. Like most prior approaches, PhraseBERT learns a composition function that combines component word embeddings together into a single phrase embedding. This function has previously been implemented with rule-based composition over word vectors (Yu and Dredze, 2015) and recurrent models (Zhou et al., 2017) that use a pair-wise GRU model using datasets such as PPDB (Pavlick et al., 2015). Other work learns task-specific phrase embeddings such as those for semantic parsing (Socher et al., 2011), machine translation (Bing et al., 2015) and question answering (Lee et al., 2021); in contrast, Phrase-BERT produces general-purpose embeddings useful for any task. The advent of huge-scale pretrained language models such as BERT (Devlin et al., 2018) has opened a new direction of phrase representation learning. Yu and Ettinger (2020) highlight BERT’s struggles to meaningfully represent short linguistic units (words, phrases). Several papers hypothesize that this is because BERT is trained on longer texts (512 tokens) and with a pairwise text objective that may be irrelevant for shorter texts (Reimers and 1 https://github.com/sf-wa"
2021.emnlp-main.846,P15-2070,0,0.0768244,"Missing"
2021.emnlp-main.846,2020.emnlp-main.55,1,0.709537,"ses: Our first fine-tuning objective encourages BERT to capture semantic relatedness between phrases without overly relying on lexical similarity between those phrases. To accomplish this, we create a dataset by extracting 100K phrases from WikiText-103 (Merity et al., 2017) using the shift-reduce parser from CoreNLP (Manning et al., 2014).3 Then, given a phrase p “complete control” from the sentence “The local authorities have complete control over the allocation of building materials”, we create a positive example p+ by passing p through the GPT2-based diverse paraphrasing model released by Krishna et al. (2020). This model was trained by fine-tuning GPT2-large (Radford et al., 2019) on a filtered version of the PARANMT-50M sentence-level paraphrase dataset (Wieting and Gimpel, 2017), using an encoder-free seq2seq modeling approach as proposed by Wolf et al. (2019). We decode from this model using nucleus sampling with the nucleus probability mass of 0.8 (Holtzman et al., 2019), applying lexical constraints to avoid producing any non-stopword tokens that also occur in p. This yields phrases such as “full power of the system”, which are quasiparaphrases of p with no lexical overlap. We create a negati"
2021.emnlp-main.846,2021.acl-long.518,0,0.0341923,"large-scale pretrained language models. Like most prior approaches, PhraseBERT learns a composition function that combines component word embeddings together into a single phrase embedding. This function has previously been implemented with rule-based composition over word vectors (Yu and Dredze, 2015) and recurrent models (Zhou et al., 2017) that use a pair-wise GRU model using datasets such as PPDB (Pavlick et al., 2015). Other work learns task-specific phrase embeddings such as those for semantic parsing (Socher et al., 2011), machine translation (Bing et al., 2015) and question answering (Lee et al., 2021); in contrast, Phrase-BERT produces general-purpose embeddings useful for any task. The advent of huge-scale pretrained language models such as BERT (Devlin et al., 2018) has opened a new direction of phrase representation learning. Yu and Ettinger (2020) highlight BERT’s struggles to meaningfully represent short linguistic units (words, phrases). Several papers hypothesize that this is because BERT is trained on longer texts (512 tokens) and with a pairwise text objective that may be irrelevant for shorter texts (Reimers and 1 https://github.com/sf-wa-326/phrase-bert-topic-model Phrase Embedd"
2021.emnlp-main.846,D19-1410,0,0.27854,"ive that intuitively places as sentences and phrases. An out-of-the-box BERT phrase embeddings close to both their paraphrases sentence embedding model often underperforms and the contexts in which they appear (Figure 1). simple baselines such as averaging GloVe vectors in semantic textual similarity tasks (Reimers and Phrase-BERT outperforms strong baselines such 10837 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 10837–10851 c November 7–11, 2021. 2021 Association for Computational Linguistics as SpanBERT (Joshi et al., 2019) and SentenceBERT (Reimers and Gurevych, 2019) across a suite of phrase-level semantic relatedness tasks. Additionally, we show that its nearest neighbor space exhibits increased lexical diversity, which signals that compositionality plays a larger role in its vector space (Table 1). Such phrasal diversity is an important component of models built for corpus exploration such as phrase-based topic modeling (Wang et al., 2007; Griffiths et al., 2007). To investigate Phrase-BERT’s potential role in such applications, we integrate it into a neural topic model that represents topics as mixtures of words, phrases, and even sentences. A series o"
2021.emnlp-main.846,2020.emnlp-main.733,0,0.321669,"representations derived from BERT do not exhibit complex phrasal compositionality. In this paper, we develop Phrase-BERT, which fine-tunes BERT using contrastive learning to induce more powerful phrase embeddings. Our approach directly targets two major weaknesses of out-of-the-box BERT phrase embeddings: (1) BERT never sees short texts (e.g., phrases) during pretraining, as its inputs are chunks of 512 1 Introduction tokens; and (2) BERT relies heavily on lexical similarity (word overlap) between input texts to deLearning representations of phrases is important termine semantic relatedness (Li et al., 2020; Yu for many tasks, such as semantic parsing (Socher et al., 2011), machine translation (Ramisch et al., and Ettinger, 2020; Zhang et al., 2019). To combat 2013), and question answering (Seo et al., 2018). these issues, we automatically generate a dataset of lexically-diverse phrasal paraphrase pairs, and While pretrained language models such as BERT we additionally extract a large-scale dataset of 300 (Devlin et al., 2018) have significantly pushed forward the state of the art on a variety of NLP tasks, million phrases in context from the Books3 dataset they still struggle to produce semanti"
2021.emnlp-main.846,2021.ccl-1.108,0,0.0947228,"Missing"
2021.emnlp-main.846,P14-5010,0,0.00523289,"le for short documents, but it struggles to model the semantics of words and phrases, 2 We use the 12-layer BERT-base-uncased for all experiments. 10838 as shown by Yu and Ettinger (2020) and also by our evaluations in Section 4.1. Creating lexically diverse phrase-level paraphrases: Our first fine-tuning objective encourages BERT to capture semantic relatedness between phrases without overly relying on lexical similarity between those phrases. To accomplish this, we create a dataset by extracting 100K phrases from WikiText-103 (Merity et al., 2017) using the shift-reduce parser from CoreNLP (Manning et al., 2014).3 Then, given a phrase p “complete control” from the sentence “The local authorities have complete control over the allocation of building materials”, we create a positive example p+ by passing p through the GPT2-based diverse paraphrasing model released by Krishna et al. (2020). This model was trained by fine-tuning GPT2-large (Radford et al., 2019) on a filtered version of the PARANMT-50M sentence-level paraphrase dataset (Wieting and Gimpel, 2017), using an encoder-free seq2seq modeling approach as proposed by Wolf et al. (2019). We decode from this model using nucleus sampling with the nu"
2021.emnlp-main.846,D19-1666,1,0.892035,"Missing"
2021.emnlp-main.846,D18-1052,0,0.0250059,"embeddings. Our approach directly targets two major weaknesses of out-of-the-box BERT phrase embeddings: (1) BERT never sees short texts (e.g., phrases) during pretraining, as its inputs are chunks of 512 1 Introduction tokens; and (2) BERT relies heavily on lexical similarity (word overlap) between input texts to deLearning representations of phrases is important termine semantic relatedness (Li et al., 2020; Yu for many tasks, such as semantic parsing (Socher et al., 2011), machine translation (Ramisch et al., and Ettinger, 2020; Zhang et al., 2019). To combat 2013), and question answering (Seo et al., 2018). these issues, we automatically generate a dataset of lexically-diverse phrasal paraphrase pairs, and While pretrained language models such as BERT we additionally extract a large-scale dataset of 300 (Devlin et al., 2018) have significantly pushed forward the state of the art on a variety of NLP tasks, million phrases in context from the Books3 dataset they still struggle to produce semantically mean- from the Pile (Gao et al., 2020). We then use this paraphrase data and contextual information to fineingful embeddings for shorter linguistic units such tune BERT with an objective that intuiti"
2021.emnlp-main.846,2020.repl4nlp-1.20,0,0.0319078,"deling (Wang et al., 2007; Griffiths et al., 2007). To investigate Phrase-BERT’s potential role in such applications, we integrate it into a neural topic model that represents topics as mixtures of words, phrases, and even sentences. A series of human evaluations reveals that our phrase-level topic model produces more meaningful and coherent topics compared to baseline models such as LDA (Blei et al., 2003) and its phrase-augmented variants. We have publicly released code and pretrained models for PhraseBERT to spur future research on phrase-based NLP tasks.1 Gurevych, 2019; Liu et al., 2019; Toshniwal et al., 2020). Without task-specific fine-tuning, the performance of BERT on phrases and sentences is often worse than simple baselines such as mean-pooling over GloVe vectors (Reimers and Gurevych, 2019; Li et al., 2020). Furthermore, Li et al. (2020) draw theoretical connections between BERT’s pretraining objective and its non-smooth anisotropic semantic embedding space, which make it more reliant on lexical overlap to determine phrase and sentence similarity. Previously proposed methods to address these issues include predicting spans during pretraining instead of words (Joshi et al., 2019), fine-tuning"
2021.emnlp-main.846,2020.emnlp-main.397,0,0.371523,"nd phrase-level topic models, further validating the utility of Phrase-BERT. GloVe his trigger, the trigger, a trigger BERT pulled the trigger, squeezed the trigger, scoots closer SpanBERT pulled the trigger, pulling the trigger, seize the day SentBERT pulling the trigger, pulled the trigger, the trigger PhraseBERT picks up his gun, squeezes off a quick burst of shots, takes aim Table 1: Nearest neighbors of the phrase “pulls the trigger”. While baselines rely heavily on lexical overlap, Phrase-BERT’s neighbors are both semantically similar and lexically diverse. Gurevych, 2019). Furthermore, Yu and Ettinger (2020) have shown that phrasal representations derived from BERT do not exhibit complex phrasal compositionality. In this paper, we develop Phrase-BERT, which fine-tunes BERT using contrastive learning to induce more powerful phrase embeddings. Our approach directly targets two major weaknesses of out-of-the-box BERT phrase embeddings: (1) BERT never sees short texts (e.g., phrases) during pretraining, as its inputs are chunks of 512 1 Introduction tokens; and (2) BERT relies heavily on lexical similarity (word overlap) between input texts to deLearning representations of phrases is important termin"
2021.emnlp-main.846,Q15-1017,0,0.0115996,"t of this paper, Phrase-BERT produces more semantically meaningful phrase representations than these competing approaches while also promoting a lexically diverse vector space. 2 3 Related work Our work relates to a long history of learning dense phrase representations, and in particular to approaches that leverage large-scale pretrained language models. Like most prior approaches, PhraseBERT learns a composition function that combines component word embeddings together into a single phrase embedding. This function has previously been implemented with rule-based composition over word vectors (Yu and Dredze, 2015) and recurrent models (Zhou et al., 2017) that use a pair-wise GRU model using datasets such as PPDB (Pavlick et al., 2015). Other work learns task-specific phrase embeddings such as those for semantic parsing (Socher et al., 2011), machine translation (Bing et al., 2015) and question answering (Lee et al., 2021); in contrast, Phrase-BERT produces general-purpose embeddings useful for any task. The advent of huge-scale pretrained language models such as BERT (Devlin et al., 2018) has opened a new direction of phrase representation learning. Yu and Ettinger (2020) highlight BERT’s struggles to"
2021.emnlp-main.846,N19-1131,0,0.152759,"ERT using contrastive learning to induce more powerful phrase embeddings. Our approach directly targets two major weaknesses of out-of-the-box BERT phrase embeddings: (1) BERT never sees short texts (e.g., phrases) during pretraining, as its inputs are chunks of 512 1 Introduction tokens; and (2) BERT relies heavily on lexical similarity (word overlap) between input texts to deLearning representations of phrases is important termine semantic relatedness (Li et al., 2020; Yu for many tasks, such as semantic parsing (Socher et al., 2011), machine translation (Ramisch et al., and Ettinger, 2020; Zhang et al., 2019). To combat 2013), and question answering (Seo et al., 2018). these issues, we automatically generate a dataset of lexically-diverse phrasal paraphrase pairs, and While pretrained language models such as BERT we additionally extract a large-scale dataset of 300 (Devlin et al., 2018) have significantly pushed forward the state of the art on a variety of NLP tasks, million phrases in context from the Books3 dataset they still struggle to produce semantically mean- from the Pile (Gao et al., 2020). We then use this paraphrase data and contextual information to fineingful embeddings for shorter li"
2021.emnlp-main.846,W17-5603,0,0.0186903,"semantically meaningful phrase representations than these competing approaches while also promoting a lexically diverse vector space. 2 3 Related work Our work relates to a long history of learning dense phrase representations, and in particular to approaches that leverage large-scale pretrained language models. Like most prior approaches, PhraseBERT learns a composition function that combines component word embeddings together into a single phrase embedding. This function has previously been implemented with rule-based composition over word vectors (Yu and Dredze, 2015) and recurrent models (Zhou et al., 2017) that use a pair-wise GRU model using datasets such as PPDB (Pavlick et al., 2015). Other work learns task-specific phrase embeddings such as those for semantic parsing (Socher et al., 2011), machine translation (Bing et al., 2015) and question answering (Lee et al., 2021); in contrast, Phrase-BERT produces general-purpose embeddings useful for any task. The advent of huge-scale pretrained language models such as BERT (Devlin et al., 2018) has opened a new direction of phrase representation learning. Yu and Ettinger (2020) highlight BERT’s struggles to meaningfully represent short linguistic u"
2021.emnlp-main.97,2020.emnlp-main.525,1,0.851487,"Missing"
2021.emnlp-main.97,2020.acl-main.399,0,0.0357538,"Missing"
2021.emnlp-main.97,N18-1016,0,0.0606353,"Missing"
2021.emnlp-main.97,2020.emnlp-main.426,0,0.0780324,"Missing"
2021.emnlp-main.97,2020.emnlp-main.745,0,0.0377891,"form (AMT) to minimize cost and time. Most existing AMT studies ask crowdworkers to provide Likert scale ratings of various properties of generated text, such as fluency and likability. In this paper, we study the reliability and reproducibility of AMT evaluations of open-ended text generation. We first conduct a survey of papers on open-ended text generation between 2018-2020 and find many critical details often go unreported (e.g., worker qualifications, payment, task descriptions, annotator agreement), a finding in line with prior reproducibility studies outside open-ended text generation (Card et al., 2020; Howcroft et al., 2020; van der Lee et al., 2021). Next, we perform a series of story generation evaluations with both AMT workers and expert raters (English teachers), applying a variant of the most common task configuration that appeared in our survey (5 point Likert scale ratings of 200 examples with three annotators per example) to the paragraph-length WritingPrompts dataset of Fan et al. (2018). Unlike prior work in this area, we ask raters to evaluate both stories generated by a fine-tuned GPT-2 language model (Radford et al., 2019) and human-written reference stories on the same scale,"
2021.emnlp-main.97,2021.humeval-1.4,0,0.0189824,"likability than the GPT-2 generated stories (all p’s<0.001). Interestingly, their IAA was higher than the English teachers recruited from the authors’ personal networks. The details of this experiment are provided in the Appendix B. 5 Related Work Our work is related to previous studies of human evaluation of text quality as well as collecting judgments using Amazon Mechanical Turk. Human evaluation of text quality: Most previous studies on human evaluation concentrate on constrained generation domains, such as machine translation (Guzmán et al., 2015; Graham et al., 2017; Toral et al., 2018; Castilho, 2021) or summarization (Gillick and Liu, 2010; Iskender et al., 2020). Other studies evaluate very short, often one sentence long, outputs (Grundkiewicz et al., 2015; Mori et al., 2019; Khashabi et al., 2021). Even professional translators struggle when evaluating longer machine translated texts (Castilho, 2021). Creative texts, such as stories, are less constrained than translated texts, but researchers continue to employ crowd workers to evaluate creative texts, often without evaluating reference texts (see Section 2). Previous studies have asked workers to choose from (Mori et al., 2019) or dist"
2021.emnlp-main.97,2020.acl-main.711,0,0.0229845,"Missing"
2021.emnlp-main.97,2020.emnlp-main.524,0,0.0858367,"Missing"
2021.emnlp-main.97,2021.acl-long.565,0,0.0334149,"n one sentence long, outputs (Grundkiewicz et al., 2015; Mori et al., 2019; Khashabi et al., 2021). Even professional translators struggle when evaluating longer machine translated texts (Castilho, 2021). Creative texts, such as stories, are less constrained than translated texts, but researchers continue to employ crowd workers to evaluate creative texts, often without evaluating reference texts (see Section 2). Previous studies have asked workers to choose from (Mori et al., 2019) or distinguish between human-written and machine-generated texts (Garbacea et al., 2019; Ippolito et al., 2020; Clark et al., 2021). 6 Recommendations & Conclusion Our experiments show that evaluating open-ended generated text is an incredibly challenging task even for expert raters. While AMT is a convenient and affordable solution, we observe that high variance between workers, poor calibration, and cognitively-demanding tasks can lead researchers to draw misleading scientific conclusions (e.g., that human-written text is “worse” than GPT-2’s). Simple fixes such as adding strict worker qualifications do not address the root of the problem. As such, we recommend future AMT evaluations implement additional quality control"
2021.emnlp-main.97,N18-1204,0,0.0620442,"Missing"
2021.emnlp-main.97,2020.acl-main.225,0,0.038267,"Missing"
2021.emnlp-main.97,P18-1082,0,0.176849,"details often go unreported (e.g., worker qualifications, payment, task descriptions, annotator agreement), a finding in line with prior reproducibility studies outside open-ended text generation (Card et al., 2020; Howcroft et al., 2020; van der Lee et al., 2021). Next, we perform a series of story generation evaluations with both AMT workers and expert raters (English teachers), applying a variant of the most common task configuration that appeared in our survey (5 point Likert scale ratings of 200 examples with three annotators per example) to the paragraph-length WritingPrompts dataset of Fan et al. (2018). Unlike prior work in this area, we ask raters to evaluate both stories generated by a fine-tuned GPT-2 language model (Radford et al., 2019) and human-written reference stories on the same scale, as we expect the latter to consistently score higher on all evaluations. Our experiments expose and quantify several troubling trends: 1 Nevertheless, such metrics are commonly reported in research papers on open-ended text generation. 1. AMT ratings do not reliably distinguish model-generated text from human-generated text unless workers are asked to rate both sideby-side, which allows them to bett"
2021.emnlp-main.97,P19-1254,0,0.0568155,"Missing"
2021.emnlp-main.97,2020.emnlp-main.61,0,0.0478452,"Missing"
2021.emnlp-main.97,J11-2010,0,0.0316819,"sites like Reddit may contain works raise concerns about the reliability of data collected on AMT (Necka et al., 2016; Matherly, racist, sexist, and other forms of vulgar content. Additionally, neural language models like GPT2019; Ahler et al., 2020). Reluctance of requesters to reject HITs leads to positive bias in workers’ 2, which have been trained on open domain text crawled from the web, have been shown to generate qualifications (Matherly, 2019). Furthermore, a similarly offensive content. As such, we advocate large number of responses are provided by small number of productive workers (Fort et al., 2011; adequately warning any humans who take part in open-ended text evaluation of the potential for such Robinson et al., 2019). Researchers also report an harms (as we did in our research). increasing number of workers use VPNs to mask their location (Bauer et al., 2020) and contribute Additionally, crowd workers are frequently unlower-quality data (Moss and Litman; Ahler et al., derpaid for their labor, which harms both the qual2020). Hence, simple quality control measures, ity of the research, and more importantly, the ability such as approval rate or the country of residence of these crowd wo"
2021.emnlp-main.97,D19-1409,0,0.0191019,"2020). Other studies evaluate very short, often one sentence long, outputs (Grundkiewicz et al., 2015; Mori et al., 2019; Khashabi et al., 2021). Even professional translators struggle when evaluating longer machine translated texts (Castilho, 2021). Creative texts, such as stories, are less constrained than translated texts, but researchers continue to employ crowd workers to evaluate creative texts, often without evaluating reference texts (see Section 2). Previous studies have asked workers to choose from (Mori et al., 2019) or distinguish between human-written and machine-generated texts (Garbacea et al., 2019; Ippolito et al., 2020; Clark et al., 2021). 6 Recommendations & Conclusion Our experiments show that evaluating open-ended generated text is an incredibly challenging task even for expert raters. While AMT is a convenient and affordable solution, we observe that high variance between workers, poor calibration, and cognitively-demanding tasks can lead researchers to draw misleading scientific conclusions (e.g., that human-written text is “worse” than GPT-2’s). Simple fixes such as adding strict worker qualifications do not address the root of the problem. As such, we recommend future AMT eval"
2021.emnlp-main.97,W10-0722,0,0.0456676,"d stories (all p’s<0.001). Interestingly, their IAA was higher than the English teachers recruited from the authors’ personal networks. The details of this experiment are provided in the Appendix B. 5 Related Work Our work is related to previous studies of human evaluation of text quality as well as collecting judgments using Amazon Mechanical Turk. Human evaluation of text quality: Most previous studies on human evaluation concentrate on constrained generation domains, such as machine translation (Guzmán et al., 2015; Graham et al., 2017; Toral et al., 2018; Castilho, 2021) or summarization (Gillick and Liu, 2010; Iskender et al., 2020). Other studies evaluate very short, often one sentence long, outputs (Grundkiewicz et al., 2015; Mori et al., 2019; Khashabi et al., 2021). Even professional translators struggle when evaluating longer machine translated texts (Castilho, 2021). Creative texts, such as stories, are less constrained than translated texts, but researchers continue to employ crowd workers to evaluate creative texts, often without evaluating reference texts (see Section 2). Previous studies have asked workers to choose from (Mori et al., 2019) or distinguish between human-written and machin"
2021.emnlp-main.97,2020.emnlp-main.351,0,0.0804412,"Missing"
2021.emnlp-main.97,2020.acl-main.164,0,0.0429135,"aluate very short, often one sentence long, outputs (Grundkiewicz et al., 2015; Mori et al., 2019; Khashabi et al., 2021). Even professional translators struggle when evaluating longer machine translated texts (Castilho, 2021). Creative texts, such as stories, are less constrained than translated texts, but researchers continue to employ crowd workers to evaluate creative texts, often without evaluating reference texts (see Section 2). Previous studies have asked workers to choose from (Mori et al., 2019) or distinguish between human-written and machine-generated texts (Garbacea et al., 2019; Ippolito et al., 2020; Clark et al., 2021). 6 Recommendations & Conclusion Our experiments show that evaluating open-ended generated text is an incredibly challenging task even for expert raters. While AMT is a convenient and affordable solution, we observe that high variance between workers, poor calibration, and cognitively-demanding tasks can lead researchers to draw misleading scientific conclusions (e.g., that human-written text is “worse” than GPT-2’s). Simple fixes such as adding strict worker qualifications do not address the root of the problem. As such, we recommend future AMT evaluations implement addit"
2021.emnlp-main.97,N18-1169,0,0.056443,"Missing"
2021.emnlp-main.97,W04-1013,0,0.0220767,"Finally, interviews with the English teachers provide deeper insights into the challenges of the evaluation process, particularly when rating model-generated text. 1 Introduction Recent advances in neural language modeling have spurred research into open-ended text generation tasks such as story generation (Peng et al., 2018a), style transfer (Krishna et al., 2020), and pun generation (He et al., 2019). Since the space of possible outputs for these tasks is huge compared to more constrained problems such as machine translation, automatic metrics such as BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) that measure similarity to reference texts are mostly uninformative (Akoury et al., 2020).1 Human evaluation of model-generated text, which is critical for openended tasks given the unreliability of automatic metrics (Peng et al., 2017; Reiter, 2018; See et al., 2019), is frequently conducted on Amazon’s popular Mechanical Turk platform (AMT) to minimize cost and time. Most existing AMT studies ask crowdworkers to provide Likert scale ratings of various properties of generated text, such as fluency and likability. In this paper, we study the reliability and reproducibility of AMT evaluations"
2021.emnlp-main.97,W19-2405,0,0.0438025,"Missing"
2021.emnlp-main.97,2020.ngt-1.2,0,0.0489484,"Missing"
2021.emnlp-main.97,2020.eval4nlp-1.16,0,0.0187475,"1). Interestingly, their IAA was higher than the English teachers recruited from the authors’ personal networks. The details of this experiment are provided in the Appendix B. 5 Related Work Our work is related to previous studies of human evaluation of text quality as well as collecting judgments using Amazon Mechanical Turk. Human evaluation of text quality: Most previous studies on human evaluation concentrate on constrained generation domains, such as machine translation (Guzmán et al., 2015; Graham et al., 2017; Toral et al., 2018; Castilho, 2021) or summarization (Gillick and Liu, 2010; Iskender et al., 2020). Other studies evaluate very short, often one sentence long, outputs (Grundkiewicz et al., 2015; Mori et al., 2019; Khashabi et al., 2021). Even professional translators struggle when evaluating longer machine translated texts (Castilho, 2021). Creative texts, such as stories, are less constrained than translated texts, but researchers continue to employ crowd workers to evaluate creative texts, often without evaluating reference texts (see Section 2). Previous studies have asked workers to choose from (Mori et al., 2019) or distinguish between human-written and machine-generated texts (Garba"
2021.emnlp-main.97,P19-1560,0,0.0608981,"Missing"
2021.emnlp-main.97,2020.acl-main.709,0,0.0454694,"Missing"
2021.emnlp-main.97,2020.emnlp-main.55,1,0.82434,"Missing"
2021.emnlp-main.97,N19-1317,0,0.0606701,"Missing"
2021.emnlp-main.97,2020.emnlp-main.415,0,0.0486756,"Missing"
2021.emnlp-main.97,D19-1615,0,0.0245109,"p We first describe the parameters of our experiments before later analyzing the results. Dataset: We use the WritingPrompts dataset collected by Fan et al. (2018), which is a collection of 303,358 English language stories written by Reddit users on the r/WritingPrompts subreddit.5 This dataset, which consists of short prompts paired with user-written stories (e.g., “There are 10 legendary dentists who review every toothpaste. You are the 10th... being hunted by the other 9...”), has been used in multiple previous works on paragraph-length story generation (Fan et al., 2019; See et al., 2019; Mao et al., 2019). We randomly select 200 prompts from the test set for all of our experiments. Since the human-written stories in the dataset are already tokenized, we first de-tokenized the stories, cleaned up artifacts from lemmatization, and manually truncated each story so that it ends with a full sentence and is no longer than 150 words in order to make the length comparable with the machine-generated story.6 We use the resulting stories for all experiments with reference text. Model-generated stories: We follow a similar modeling approach to prior story generation work (Mao et al., 2019; Guan et al., 20"
2021.emnlp-main.97,2020.emnlp-main.348,0,0.0552769,"Missing"
2021.emnlp-main.97,N19-1049,0,0.0567142,"Missing"
2021.emnlp-main.97,W18-1505,0,0.0261327,"workers (unlike teachers) fail to distinguish between model-generated text and human-generated references. We show that AMT worker judgments improve when they are shown model-generated output alongside human-generated references, which enables the workers to better calibrate their ratings. Finally, interviews with the English teachers provide deeper insights into the challenges of the evaluation process, particularly when rating model-generated text. 1 Introduction Recent advances in neural language modeling have spurred research into open-ended text generation tasks such as story generation (Peng et al., 2018a), style transfer (Krishna et al., 2020), and pun generation (He et al., 2019). Since the space of possible outputs for these tasks is huge compared to more constrained problems such as machine translation, automatic metrics such as BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) that measure similarity to reference texts are mostly uninformative (Akoury et al., 2020).1 Human evaluation of model-generated text, which is critical for openended tasks given the unreliability of automatic metrics (Peng et al., 2017; Reiter, 2018; See et al., 2019), is frequently conducted on Amazon’s popular M"
2021.emnlp-main.97,D18-1359,0,0.0288666,"Missing"
2021.emnlp-main.97,D19-1509,0,0.0594648,"Missing"
2021.emnlp-main.97,N18-1012,0,0.0671392,"Missing"
2021.emnlp-main.97,2020.emnlp-main.349,0,0.0336494,"Missing"
2021.emnlp-main.97,J18-3002,0,0.0275431,"o open-ended text generation tasks such as story generation (Peng et al., 2018a), style transfer (Krishna et al., 2020), and pun generation (He et al., 2019). Since the space of possible outputs for these tasks is huge compared to more constrained problems such as machine translation, automatic metrics such as BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) that measure similarity to reference texts are mostly uninformative (Akoury et al., 2020).1 Human evaluation of model-generated text, which is critical for openended tasks given the unreliability of automatic metrics (Peng et al., 2017; Reiter, 2018; See et al., 2019), is frequently conducted on Amazon’s popular Mechanical Turk platform (AMT) to minimize cost and time. Most existing AMT studies ask crowdworkers to provide Likert scale ratings of various properties of generated text, such as fluency and likability. In this paper, we study the reliability and reproducibility of AMT evaluations of open-ended text generation. We first conduct a survey of papers on open-ended text generation between 2018-2020 and find many critical details often go unreported (e.g., worker qualifications, payment, task descriptions, annotator agreement), a fi"
2021.emnlp-main.97,K19-1079,0,0.0742684,"ext generation tasks such as story generation (Peng et al., 2018a), style transfer (Krishna et al., 2020), and pun generation (He et al., 2019). Since the space of possible outputs for these tasks is huge compared to more constrained problems such as machine translation, automatic metrics such as BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) that measure similarity to reference texts are mostly uninformative (Akoury et al., 2020).1 Human evaluation of model-generated text, which is critical for openended tasks given the unreliability of automatic metrics (Peng et al., 2017; Reiter, 2018; See et al., 2019), is frequently conducted on Amazon’s popular Mechanical Turk platform (AMT) to minimize cost and time. Most existing AMT studies ask crowdworkers to provide Likert scale ratings of various properties of generated text, such as fluency and likability. In this paper, we study the reliability and reproducibility of AMT evaluations of open-ended text generation. We first conduct a survey of papers on open-ended text generation between 2018-2020 and find many critical details often go unreported (e.g., worker qualifications, payment, task descriptions, annotator agreement), a finding in line with"
2021.emnlp-main.97,P19-1200,0,0.0447291,"Missing"
2021.emnlp-main.97,2020.emnlp-main.226,0,0.0885123,"Missing"
2021.emnlp-main.97,N19-4013,0,0.0250486,"Missing"
2021.findings-acl.352,W19-1909,0,0.0492542,"Missing"
2021.findings-acl.352,D19-1678,0,0.209479,"019), while some use various other machine learning techniques for outcome prediction on data collected from the ICU (Jin et al., 2018; Boag et al., 2018; Ghorbani et al., 2020). More recently, BERT-based models have been adopted for this domain following their incredible success in Natural Language Processing (NLP). A number of studies trained and used BERT-based models for clinical applications (Lee et al., 2020; Huang et al., 2019; Darabi et al., 2020; Li et al., 2020; Alsentzer et al., 2019). All these models only use one source of available data when predicting medical outcomes. However, Khadanga et al. (2019) showed the usefulness of combining time-series and clinical notes for ICU outcome prediction. They used a convolution neural network (CNN) on top of pretrained word embedding from (Zhang et al., 2019) for getting a representation of clinical notes and a long short-term memory network (LSTM) for embedding the time series part of the data. The two representations were then concatenated to make the predictions. Concurrently to our work, Yang et al. (2021) also showed the usefulness of combining time-series data with information from clinical notes. They used an LSTM model for the time-series par"
2021.naacl-main.270,2020.acl-main.398,0,0.333998,"upt! real France 3.6 real corrupt! Italy 5 real real real real Spain 4 step 2: embed the table with TABBIE Figure 1: TABBIE is a table embedding model trained to detect corrupted cells, inspired by the ELECTRA (Clark et al., 2020) objective function. This simple pretraining objective results in powerful embeddings of cells, columns, and rows, and it yields stateof-the-art results on downstream table-based tasks. allows easy access to representations for different tabular substructures (cells, rows, and columns). Existing table representation models such as TaBERT (Yin et al., 2020) and TaPas (Herzig et al., 2020) concatenate tabular data with an associated piece of text and then use BERT’s masked language modeling objective for pretraining. These approaches are computationally expensive due to the long sequences that arise from concatenating text with linearized tables, which necessitates truncating the input sequences1 to make training feasible. We show that TaBERT underperforms on downstream table-based applications that operate independent of external text (e.g., deciding whether cell text was corrupted while extracting a table from a PDF), which motivates us to investigate an approach that preserv"
2021.naacl-main.270,P15-1142,0,0.210161,"we additionally specify that half of the intratable swaps must come from the same row or column to make the objective more challenging. Experiments We validate TABBIE’s table representation quality through its performance on three downstream tablecentric benchmarks (column population, row population, and column type prediction) that measure semantic table understanding. In most configurations of these tasks, TABBIE outperforms TaBERT and other baselines to set new state-of-the-art numbers. Note that we do not investigate TABBIE’s performance on table-and-text tasks such as WikiTableQuestions (Pasupat and Liang, 2015), as our focus is not on integrating TABBIE into complex taskspecific pipelines (Liang et al., 2018), although this is an interesting avenue for future work. 3.1 Fine-tuning TABBIE Column population Row population Col. type prediction Batch size LR Max epochs 12 48 12 1e-05 2e-05 2e-05 20 30 15 Table 1: Fine-tuning hyperparameters of each downstream task for TABBIE and TaBERT. [CLS] [CLS] 3 Task tures used in the downstream task, and we place a classifier over these representations to predict the training labels. We select task-specific hyperparameters based on the size of each dataset (full d"
2021.naacl-main.270,N18-1202,1,0.789788,"external text (e.g., deciding whether cell text was corrupted while extracting a table from a PDF), which motivates us to investigate an approach that preserves the full table during pretraining. Our TABBIE architecture relies on two Transformers that independently encode rows and columns, respectively; their representations are pooled at each layer. This setup reduces the sequence length of each Transformer’s input, which cuts down on its complexity, while also allowing us Large-scale self-supervised pretraining has substantially advanced the state-of-the-art in natural language processing (Peters et al., 2018; Devlin et al., 2018; Liu et al., 2019). More recently, these pretraining methods have been extended to jointly learn representations of tables as well as text (Herzig et al., 2020; Yin et al., 2020), which enables improved modeling of tasks such as question answering over tables. However, many practical problems involve semantic understanding of tabular data without additional text-based input, such as extracting tables from documents, retrieving similar columns or cells, and filling in missing information (Zhang and Balog, 2020). In this work, we design a pretraining methodology specificall"
2021.naacl-main.270,D19-1534,0,0.0252409,"vely), as well as embeddings for individual columns cj and rows r i . Initialization: We begin by initializing the cell embeddings xij using a pretrained BERT model (Devlin et al., 2018).3 Specifically, for each cell (i, j), we feed its contents into BERT and extract the 768-d [ CLS ] token representation. This step allows us to leverage the powerful semantic text encoder of BERT to compute representations of cells out-of-context, which is important because many tables contain cells with long-form text (e.g., Notes columns). Additionally, BERT has been shown to encode some degree of numeracy (Wallace et al., 2019), which helps represent cells with numerical content. We keep this BERT encoder fixed during training to reduce computational expense. Finally, we add learned positional embeddings to each of the [ CLS ] vectors to form the initialization of xij . More specifically, we have (r) two sets of positional embeddings, pi ∈ RH and (c) pj ∈ RH , which model the position of rows and columns, respectively, and are randomly initialized and fine-tuned via TABBIE’s self-supervised objective. Contextualizing the cell embeddings: The cell embeddings we get from BERT are uncontextualized: they are computed in"
2021.naacl-main.270,2020.acl-main.745,0,0.379368,"rupted cells Size Medals corrupt! real France 3.6 real corrupt! Italy 5 real real real real Spain 4 step 2: embed the table with TABBIE Figure 1: TABBIE is a table embedding model trained to detect corrupted cells, inspired by the ELECTRA (Clark et al., 2020) objective function. This simple pretraining objective results in powerful embeddings of cells, columns, and rows, and it yields stateof-the-art results on downstream table-based tasks. allows easy access to representations for different tabular substructures (cells, rows, and columns). Existing table representation models such as TaBERT (Yin et al., 2020) and TaPas (Herzig et al., 2020) concatenate tabular data with an associated piece of text and then use BERT’s masked language modeling objective for pretraining. These approaches are computationally expensive due to the long sequences that arise from concatenating text with linearized tables, which necessitates truncating the input sequences1 to make training feasible. We show that TaBERT underperforms on downstream table-based applications that operate independent of external text (e.g., deciding whether cell text was corrupted while extracting a table from a PDF), which motivates us to inve"
2021.naacl-main.393,2020.emnlp-main.525,1,0.767317,"s will be better judges of factual correctness of answers. (2) Length of Answers: Annotators mentioned the paragraph-long length of answers made the task quite challenging. Annotators reported taking an average of 2 minutes per answer pair, many of which required careful thought & concentration. This was especially difficult when only part of the answer was correct and the rest had contradictions or repetitions, a common theme in our generations. Takeaway: Human evaluation is challenging but necessary for evaluating LFQA. Crowd-workers are unlikely to spend time reading & analyzing long text (Akoury et al., 2020). Hence, it is imperative to design simpler evaluations. One effort in this direction is Dugan et al. (2020), who reveal one generated sentence at a time and estimate system quality based on the number of sentences which fooled humans. Another promising direction is extrinsic evaluation (Celikyilmaz et al., 2020) where humans actually interact with systems in real-world scenarios such as the Alexa Prize (Ram et al., 2018) or STORIUM (Akoury et al., 2020). 4 Conclusion First and foremost, we thank the twenty people who volunteered to help out with with the human annotation experiments. We are v"
2021.naacl-main.393,N19-1423,0,0.0280777,"nditions answer generation on ROUGE-L. On the other hand, our system achieves Wikipedia articles identified by a pretrained rehigher ROUGE-L than reference human-written triever. We use a dense retriever trained by scaling answers, which is misleading since human A/B up a distantly supervised algorithm from Jernite testers strongly prefer reference answers to our sys- (2020). Since retrieved articles can be quite long tem’s. We conclude that ROUGE-L is not a reliable and often exceed the maximum sequence length of metric to evaluate LFQA due to its large and rela- pretrained models like BERT (Devlin et al., 2019), tively unconstrained output space (e.g., compared we use a sparse-attention variant of the Transformer to translation or summarization), and we offer sug- to allow modeling over longer sequences. While gestions for better automatic & human evaluations our system sets a new state-of-the-art on ELI5, we to enable meaningful progress on this task. question the significance of this result in Section 3. 4941 2.1 2.2 Retriever We begin by specifying our dense retriever (“contrastive REALM” or C -REALM), which returns documents related to an input question. Consider a corpus of long-form questions"
2021.naacl-main.393,2020.emnlp-demos.25,0,0.0206345,"ragraph-long length of answers made the task quite challenging. Annotators reported taking an average of 2 minutes per answer pair, many of which required careful thought & concentration. This was especially difficult when only part of the answer was correct and the rest had contradictions or repetitions, a common theme in our generations. Takeaway: Human evaluation is challenging but necessary for evaluating LFQA. Crowd-workers are unlikely to spend time reading & analyzing long text (Akoury et al., 2020). Hence, it is imperative to design simpler evaluations. One effort in this direction is Dugan et al. (2020), who reveal one generated sentence at a time and estimate system quality based on the number of sentences which fooled humans. Another promising direction is extrinsic evaluation (Celikyilmaz et al., 2020) where humans actually interact with systems in real-world scenarios such as the Alexa Prize (Ram et al., 2018) or STORIUM (Akoury et al., 2020). 4 Conclusion First and foremost, we thank the twenty people who volunteered to help out with with the human annotation experiments. We are very grateful to Vidhisha Balachandran, Niki Parmar, and Ashish Vaswani for weekly meetings discussing progre"
2021.naacl-main.393,P19-1612,0,0.0357186,"open-domain QA, which involves searching a large external knowledge source for documents relevant to a given question, with a text generation component to produce paragraph-length answers. Significant progress has been made on open-domain QA datasets such as Natural Questions (Kwiatkowski et al., 2019), * Work done during an internship at Google Research. Resources accompanying our paper can be found in https://github.com/martiansideofthemoon/ hurdles-longform-qa 1 Mohit Iyyer♠ whose questions are answerable with short phrases and entities, by leveraging dense retrieval techniques like ORQA (Lee et al., 2019), REALM (Guu et al., 2020), and DPR (Karpukhin et al., 2020; Lewis et al., 2020c; Izacard and Grave, 2020). Methods inspired by these results have recently been combined with pretrained language models (Lewis et al., 2020b; Petroni et al., 2020) and applied to the Reddit-derived “Explain Like I’m Five” (ELI5) dataset (Fan et al., 2019), which is the only publicly-available large-scale LFQA dataset. The recently proposed KILT benchmark (Petroni et al., 2020), which compares retrieval-augmented models across a variety of knowledge-intensive tasks including ELI5, automatically evaluates LFQA mode"
2021.naacl-main.393,2020.acl-main.703,0,0.158199,"r documents relevant to a given question, with a text generation component to produce paragraph-length answers. Significant progress has been made on open-domain QA datasets such as Natural Questions (Kwiatkowski et al., 2019), * Work done during an internship at Google Research. Resources accompanying our paper can be found in https://github.com/martiansideofthemoon/ hurdles-longform-qa 1 Mohit Iyyer♠ whose questions are answerable with short phrases and entities, by leveraging dense retrieval techniques like ORQA (Lee et al., 2019), REALM (Guu et al., 2020), and DPR (Karpukhin et al., 2020; Lewis et al., 2020c; Izacard and Grave, 2020). Methods inspired by these results have recently been combined with pretrained language models (Lewis et al., 2020b; Petroni et al., 2020) and applied to the Reddit-derived “Explain Like I’m Five” (ELI5) dataset (Fan et al., 2019), which is the only publicly-available large-scale LFQA dataset. The recently proposed KILT benchmark (Petroni et al., 2020), which compares retrieval-augmented models across a variety of knowledge-intensive tasks including ELI5, automatically evaluates LFQA models by the quality of both generated answers (ROUGE-L against reference answers)"
2021.naacl-main.393,W04-1013,0,0.0628141,"ALM RT + C -REALM 6.7 10.7 15.7 24.6 23.1 22.9 23.4 23.2 1.5 2.4 KRL Table 1: Results on the KILT test set for ELI5 for (1) retrieval performance, using R-precision and Recall@5 (RPrec, R@5), and (2) generation quality, using ROUGE-L (R-L). These scores are combined to produce the final metric KILT R-L (KRL). We outperform prior work on both generation & combined scores. are hidden, and hosted on a public leaderboard in the EvalAI platform (Yadav et al., 2019). Answer quality is measured by the maximum overlap of generations with a set of gold answers in terms of unigram F1 score and ROUGE-L (Lin, 2004). Petroni et al. (2020) collected human annotations of Wikipedia articles which support ELI5 gold answers, which enables measuring retrieval quality by computing R-precision (if the top-1 retrieval matches the annotation) and Recall@5 using the top-5 retrievals. Finally, the KILT benchmark combines R-prec. and ROUGE-L to measure the overall performance of the system by “KILT ROUGE-L”. This metric is similar to ROUGE-L, but assigns a score of 0 whenever the top-1 retrieval does not match the gold annotation. Baselines: We compare our model with the other entries on the ELI5 KILT leaderboard whi"
2021.naacl-main.393,2021.ccl-1.108,0,0.0577049,"Missing"
2021.naacl-main.393,W19-2303,0,0.0176012,"ems despite not using retrievals? While our model has similar capacity as the BART/RAG baselines (comparison in Appendix A.3), we hypothesize that our improvements in ROUGE-L are due to a different pretraining objective. BART is pretrained on a masked infilling task on short sequences. Instead, we pretrain our model to perform next-word prediction on long sequences from Project Gutenberg, which encourages long & fluent generations. To illustrate this length effect, in Appendix A.6 we show that truncated outputs from our model get lower ROUGE-L scores on ELI5.11 Prior summarization literature (Sun et al., 2019) has also shown that ROUGE scores vary heavily by length. To compare the same systems on shorter length outputs, we also tried finetuning the pretrained model on Wizard of Wikipedia (Dinan et al., 2019), an unconstrained dialogue generation task with single sentence dialogues (much shorter than ELI5). As seen on the public KILT leaderboard,12 our system has lower ROUGE-L scores than the BART / RAG baselines. Another possible explanation is issues with ROUGE-L itself, as discussed in Section 3.3. Our experiments in Section 3.1 show that model performance is mostly unchanged by conditioning gene"
2021.naacl-main.393,W18-1819,0,0.0365769,"Missing"
2021.naacl-main.393,2020.acl-main.450,0,0.020599,"rge output space. The ELI5 dataset has several open-ended questions with many plausible answers (like What causes traffic?), often involving analogies. A possible fix is a sentence-level evaluation and then aggregating scores across generated sentences, but appropriate penalties are needed for lack of diversity (Zhu et al., 2018) and short lengths. Other possible fixes 4947 17 Human A/B testing details in Appendix A.5. include learning task-specific metrics to measure semantic overlap (Sellam et al., 2020) or metrics to check factual correctness (Zhang et al., 2020) and faithfulness to input (Wang et al., 2020; Durmus et al., 2020; Zhou et al., 2020). Ultimately, all automatic metrics have their limitations, and human evaluation is necessary (Celikyilmaz et al., 2020). the ELI5 long-form question answering dataset. However, an in-depth analysis reveals several issues not only with our model, but also with the ELI5 dataset & evaluation metrics. We hope that the community works towards solving these issues so that we can climb the right hills and make meaningful progress on this important task. 3.4 Acknowledgements Difficulty of Human Evaluation To better understand the inherent difficulty of evaluat"
2021.naacl-main.393,2020.acl-main.458,0,0.0418058,"ngth of answers and fairly unconstrained and large output space. The ELI5 dataset has several open-ended questions with many plausible answers (like What causes traffic?), often involving analogies. A possible fix is a sentence-level evaluation and then aggregating scores across generated sentences, but appropriate penalties are needed for lack of diversity (Zhu et al., 2018) and short lengths. Other possible fixes 4947 17 Human A/B testing details in Appendix A.5. include learning task-specific metrics to measure semantic overlap (Sellam et al., 2020) or metrics to check factual correctness (Zhang et al., 2020) and faithfulness to input (Wang et al., 2020; Durmus et al., 2020; Zhou et al., 2020). Ultimately, all automatic metrics have their limitations, and human evaluation is necessary (Celikyilmaz et al., 2020). the ELI5 long-form question answering dataset. However, an in-depth analysis reveals several issues not only with our model, but also with the ELI5 dataset & evaluation metrics. We hope that the community works towards solving these issues so that we can climb the right hills and make meaningful progress on this important task. 3.4 Acknowledgements Difficulty of Human Evaluation To better"
2021.naacl-main.407,2020.emnlp-main.103,0,0.0431068,"se models improve over the Transformer. This analysis reveals that the gains stem mainly from three types of target tokens: (1) context-freqeunt (CF) tokens that appear more than twice in the prefix; (2) low frequency tokens (LF) with frequency below 1500; and (3) named entity tokens (Ent) detected by the spaCy (Honnibal et al., 2020) NER tagger. The three right-most columns of Table 3 shows that both Transformer variants are more accurate at predicting these tokens, which demonstrates the benefits of enforcing local focus at the first layer. 4 Related work work conceptually resembles that of Chiu and Rush (2020), who modernize HMM language models, as well as simple RNN-based language models (Merity et al., 2018). Our linguistic analysis is inspired by experiments from Khandelwal et al. (2018). 5 Conclusion We discover that general-purpose advances in neural architecture design, hardware, and optimization significantly improve the NPLM, a classic language model. An analysis of our upgraded NPLM inspires us to hybridize it with a modern Transformer LM and obtain perplexity decreases across three word-level LM datasets. Ethics statement Misuse of language models Our research involves training large lang"
2021.naacl-main.407,P19-1285,0,0.0395922,"Missing"
2021.naacl-main.407,P18-1027,0,0.0244057,"wice in the prefix; (2) low frequency tokens (LF) with frequency below 1500; and (3) named entity tokens (Ent) detected by the spaCy (Honnibal et al., 2020) NER tagger. The three right-most columns of Table 3 shows that both Transformer variants are more accurate at predicting these tokens, which demonstrates the benefits of enforcing local focus at the first layer. 4 Related work work conceptually resembles that of Chiu and Rush (2020), who modernize HMM language models, as well as simple RNN-based language models (Merity et al., 2018). Our linguistic analysis is inspired by experiments from Khandelwal et al. (2018). 5 Conclusion We discover that general-purpose advances in neural architecture design, hardware, and optimization significantly improve the NPLM, a classic language model. An analysis of our upgraded NPLM inspires us to hybridize it with a modern Transformer LM and obtain perplexity decreases across three word-level LM datasets. Ethics statement Misuse of language models Our research involves training large language models on publicly available benchmark datasets. They share the same issues faced by many pretrained language models, such as being used maliciously to generate unfaithful, biased"
2021.naacl-main.407,P16-1144,0,0.0270704,"l. Valid bpc. Test bpc. 44.8 42.1 42.0 41.8 44.5 41.8 41.7 41.5 1.63 1.14 1.14 1.14 1.63 1.12 1.12 1.12 Table 2: Our Transformer variants improve on the baseline Transformer across three word-level LM datasets. The # of model parameters is shown in brackets (same for all models). For model details, see Appendix B. 25.0 24.8 24.6 24.4 24.2 viously applied at all layers in prior Transformer variants (Beltagy et al., 2020; Roy et al., 2020).6 3.3 Experimental details Datasets We evaluate our models on four language modeling datasets: W IKITEXT-2 and W IKITEXT-103 (Merity et al., 2016), LAM BADA (Paperno et al., 2016), and the characterlevel ENWIK 8 benchmark (Merity et al., 2017). For W IKITEXT-2 and W IKITEXT-103 (Merity et al., 2016), we insert an &lt;eos&gt; token after each line, following Merity et al. (2018). We use adaptive softmax (Grave et al., 2017) on W IKITEXT-103 with cutoffs (2e4, 4e4, 2e5). On LAMBADA, we follow Paperno et al. (2016) by considering only the most frequent 60K words and replacing the rest with &lt;unk&gt; tokens. We use the preprocessing script released by Merity et al. (2017) to process ENWIK 8. Models We train 16-layer (16L) models on the larger W IKITEXT-103 and LAMBADA datasets, 12L"
2021.naacl-main.407,2020.acl-main.270,0,0.0542914,"Missing"
2021.naacl-main.407,P19-1032,0,0.0524279,"Missing"
D14-1070,W13-3214,0,0.0242311,"vector spaces with neural networks have been successful, although simpler functions have been proposed for short phrases (Mitchell and Lapata, 2008). Recursive neural networks have achieved state-of-the-art performance in sentiment analysis and parsing (Socher et al., 2013c; Hermann and Blunsom, 2013; Socher et al., 2013a). rnns have not been previously used for learning attribute spaces as we do here, although recursive tensor networks were unsuccessfully applied to a knowledge base completion task (Socher et al., 2013b). More relevant to this work are the dialogue analysis model proposed by Kalchbrenner & Blunsom (2013) and the paragraph vector model described in Le and Mikolov (2014), both of which are able to generate distributed representations of paragraphs. Here we present a simpler approach where a single model is able to learn complex sentence representations and average them across paragraphs. 6.2 Factoid Question-Answering Factoid question answering is often functionally equivalent to information retrieval. Given a knowledge base and a query, the goal is to Thomas Mann Henrik Ibsen Joseph Conrad Henry James Franz Kafka Figure 6: A question on the German novelist Thomas Mann that contains no named en"
D14-1070,D12-1118,1,0.767192,"s have a property called pyramidality, which means that sentences early in a question contain harder, more obscure clues, while later sentences are “giveaways”. This design rewards players with deep knowledge of a particular subject and thwarts bag of words methods. Sometimes the first sentence contains no named entities—answering the question correctly requires an actual understanding of the sentence (Figure 1). Later sentences, however, progressively reveal more well-known and uniquely identifying terms. Previous work answers quiz bowl questions using a bag of words (na¨ıve Bayes) approach (Boyd-Graber et al., 2012). These models fail on sentences like the first one in Figure 1, a typical hard, initial clue. Recursive neural networks (rnns), in contrast to simpler models, can capture the compositional aspect of such sentences (Hermann et al., 2013). rnns require many redundant training examples to learn meaningful representations, which in the quiz bowl setting means we need multiple questions about the same answer. Fortunately, hundreds of questions are produced during the school year for quiz bowl competitions, yielding many different examples of questions asking about any entity of note (see Section 4"
D14-1070,de-marneffe-etal-2006-generating,0,0.0211844,"Missing"
D14-1070,W13-0112,0,0.0150837,"ronald_reagan woodrow_wilson Wars, rebellions, and battles U.S. presidents Prime ministers Explorers & emperors Policies Other Figure 5: t-SNE 2-D projections of 451 answer vectors divided into six major clusters. The blue cluster is predominantly populated by U.S. presidents. The zoomed plot reveals temporal clustering among the presidents based on the years they spent in office. from the meaning of the words that it contains as well as the syntax that glues those words together. Many computational models of compositionality focus on learning vector spaces (Zanzotto et al., 2010; Erk, 2012; Grefenstette et al., 2013; Yessenalina and Cardie, 2011). Recent approaches towards modeling compositional vector spaces with neural networks have been successful, although simpler functions have been proposed for short phrases (Mitchell and Lapata, 2008). Recursive neural networks have achieved state-of-the-art performance in sentiment analysis and parsing (Socher et al., 2013c; Hermann and Blunsom, 2013; Socher et al., 2013a). rnns have not been previously used for learning attribute spaces as we do here, although recursive tensor networks were unsuccessfully applied to a knowledge base completion task (Socher et al"
D14-1070,P13-1088,0,0.00846746,". from the meaning of the words that it contains as well as the syntax that glues those words together. Many computational models of compositionality focus on learning vector spaces (Zanzotto et al., 2010; Erk, 2012; Grefenstette et al., 2013; Yessenalina and Cardie, 2011). Recent approaches towards modeling compositional vector spaces with neural networks have been successful, although simpler functions have been proposed for short phrases (Mitchell and Lapata, 2008). Recursive neural networks have achieved state-of-the-art performance in sentiment analysis and parsing (Socher et al., 2013c; Hermann and Blunsom, 2013; Socher et al., 2013a). rnns have not been previously used for learning attribute spaces as we do here, although recursive tensor networks were unsuccessfully applied to a knowledge base completion task (Socher et al., 2013b). More relevant to this work are the dialogue analysis model proposed by Kalchbrenner & Blunsom (2013) and the paragraph vector model described in Le and Mikolov (2014), both of which are able to generate distributed representations of paragraphs. Here we present a simpler approach where a single model is able to learn complex sentence representations and average them acr"
D14-1070,W13-3209,0,0.0158745,"rts bag of words methods. Sometimes the first sentence contains no named entities—answering the question correctly requires an actual understanding of the sentence (Figure 1). Later sentences, however, progressively reveal more well-known and uniquely identifying terms. Previous work answers quiz bowl questions using a bag of words (na¨ıve Bayes) approach (Boyd-Graber et al., 2012). These models fail on sentences like the first one in Figure 1, a typical hard, initial clue. Recursive neural networks (rnns), in contrast to simpler models, can capture the compositional aspect of such sentences (Hermann et al., 2013). rnns require many redundant training examples to learn meaningful representations, which in the quiz bowl setting means we need multiple questions about the same answer. Fortunately, hundreds of questions are produced during the school year for quiz bowl competitions, yielding many different examples of questions asking about any entity of note (see Section 4.1 for more details). Thus, we have built-in redundancy (the number of “askable” entities is limited), but also built-in diversity, as difficult clues cannot appear in every question without becoming well-known. Dependency-Tree Recursive"
D14-1070,P14-1136,0,0.00701547,"ector hs . The error for the sentence is C(S, θ) = XX L(rank(c, s, Z))max(0, s∈S z∈Z 1 − xc · hs + xz · hs ), (5) where the function rank(c, s, Z) provides the rank of correct answer c with respect to the incorrect answers Z. We transform this rank into a loss function4 shown by Usunier et al. (2009) to optimize the top of the ranked list, r P L(r) = 1/i. i=1 Since rank(c, s, Z) is expensive to compute, we approximate it by randomly sampling K incorrect answers until a violation is observed (xc · hs < 1 + xz · hs ) and set rank(c, s, Z) = (|Z |− 1)/K, as in previous work (Weston et al., 2011; Hermann et al., 2014). The model minimizes the sum of the error over all sentences T normalized by the number of nodes N in the training set, J(θ) = 1 X C(t, θ). N (6) t∈T The parameters θ = (Wr∈R , Wv , We , b), where R represents all dependency relations in the data, are optimized using AdaGrad(Duchi et al., 2011).5 In Section 4 we compare performance to an identical model (fixed-qanta) that excludes answer vectors from We and show that training them as part of θ produces significantly better results. The gradient of the objective function, ∂C 1 X ∂J(t) = , ∂θ N ∂θ (7) t∈T is computed using backpropagation throu"
D14-1070,P14-1105,1,0.253879,"ion described below. While we are not interested in obtaining a ranked list of answers,3 we observe better performance by adding the weighted approximaterank pairwise (warp) loss proposed in Weston et al. (2011) to our objective function. where R(n, k) is the dependency relation between node n and child node k. 3.2 Training Our goal is to map questions to their corresponding answer entities. Because there are a limited number of possible answers, we can view this as a multi-class classification task. While a softmax layer over every node in the tree could predict answers (Socher et al., 2011; Iyyer et al., 2014), this method overlooks that most answers are themselves words (features) in other questions (e.g., a question on World 635 Given a sentence paired with its correct answer c, we randomly select j incorrect answers from the set of all incorrect answers and denote this subset as Z. Since c is part of the vocabulary, it has a vector xc ∈ We . An incorrect answer z ∈ Z is also associated with a vector xz ∈ We . We define S to be the set of all nodes in the sentence’s dependency tree, where an individual node s ∈ S is associated with the 2 Of course, questions never contain their own answer as part"
D14-1070,P08-1028,0,0.0784578,"er is predominantly populated by U.S. presidents. The zoomed plot reveals temporal clustering among the presidents based on the years they spent in office. from the meaning of the words that it contains as well as the syntax that glues those words together. Many computational models of compositionality focus on learning vector spaces (Zanzotto et al., 2010; Erk, 2012; Grefenstette et al., 2013; Yessenalina and Cardie, 2011). Recent approaches towards modeling compositional vector spaces with neural networks have been successful, although simpler functions have been proposed for short phrases (Mitchell and Lapata, 2008). Recursive neural networks have achieved state-of-the-art performance in sentiment analysis and parsing (Socher et al., 2013c; Hermann and Blunsom, 2013; Socher et al., 2013a). rnns have not been previously used for learning attribute spaces as we do here, although recursive tensor networks were unsuccessfully applied to a knowledge base completion task (Socher et al., 2013b). More relevant to this work are the dialogue analysis model proposed by Kalchbrenner & Blunsom (2013) and the paragraph vector model described in Le and Mikolov (2014), both of which are able to generate distributed repr"
D14-1070,P14-1037,0,0.028984,"Missing"
D14-1070,N12-1085,1,0.502253,"641 Q A Q A Q A Q A Q Akbar Muhammad Shah Jahan Ghana A Babur Figure 7: An extremely misleading question about John Cabot, at least to computer models. The words muslim and mecca lead to three Mughal emperors in the top five guesses from qanta; other models are similarly led awry. with ir-wiki. A promising avenue for future work would be to incorporate Wikipedia data into qanta by transforming sentences to look like quiz bowl questions (Wang et al., 2007) and to select relevant sentences, as not every sentence in a Wikipedia article directly describes its subject. Syntax-specific annotation (Sayeed et al., 2012) may help in this regard. Finally, we could adapt the attribute space learned by the dt-rnn to use information from knowledge bases and to aid in knowledge base completion. Having learned many facts about entities that occur in question text, a dt-rnn could add new facts to a knowledge base or check existing relationships. 8 Conclusion We present qanta, a dependency-tree recursive neural network for factoid question answering that outperforms bag of words and information retrieval baselines. Our model improves upon a contrastive max-margin objective function from previous work to dynamically u"
D14-1070,D07-1002,0,0.141453,"z Kafka Figure 6: A question on the German novelist Thomas Mann that contains no named entities, along with the five top answers as scored by qanta. Each cell in the heatmap corresponds to the score (inner product) between a node in the parse tree and the given answer, and the dependency parse of the sentence is shown on the left. All of our baselines, including irwiki, are wrong, while qanta uses the plot description to make a correct guess. return the answer. Many approaches to this problem rely on hand-crafted pattern matching and answer-type classification to narrow down the search space (Shen, 2007; Bilotti et al., 2010; Wang, 2006). More recent factoid qa systems incorporate the web and social media into their retrieval systems (Bian et al., 2008). In contrast to these approaches, we place the burden of learning answer types and patterns on the model. 7 Future Work While we have shown that dt-rnns are effective models for quiz bowl question answering, other factoid qa tasks are more challenging. Questions like what does the aarp stand for? from trec qa data require additional infrastructure. A more apt comparison would be to IBM’s proprietary Watson system (Lally et al., 2012) for Jeop"
D14-1070,D11-1014,1,0.167867,"argin objective function described below. While we are not interested in obtaining a ranked list of answers,3 we observe better performance by adding the weighted approximaterank pairwise (warp) loss proposed in Weston et al. (2011) to our objective function. where R(n, k) is the dependency relation between node n and child node k. 3.2 Training Our goal is to map questions to their corresponding answer entities. Because there are a limited number of possible answers, we can view this as a multi-class classification task. While a softmax layer over every node in the tree could predict answers (Socher et al., 2011; Iyyer et al., 2014), this method overlooks that most answers are themselves words (features) in other questions (e.g., a question on World 635 Given a sentence paired with its correct answer c, we randomly select j incorrect answers from the set of all incorrect answers and denote this subset as Z. Since c is part of the vocabulary, it has a vector xc ∈ We . An incorrect answer z ∈ Z is also associated with a vector xz ∈ We . We define S to be the set of all nodes in the sentence’s dependency tree, where an individual node s ∈ S is associated with the 2 Of course, questions never contain the"
D14-1070,P13-1045,1,0.470518,"the Holy Roman Empire. The first sentence contains no words or named entities that by themselves are indicative of the answer, while subsequent sentences contain more and more obvious clues. where inputs are typically a single sentence and outputs are either continuous or a limited discrete set. Neural networks have not yet shown to be useful for tasks that require mapping paragraph-length inputs to rich output spaces. Introduction Deep neural networks have seen widespread use in natural language processing tasks such as parsing, language modeling, and sentiment analysis (Bengio et al., 2003; Socher et al., 2013a; Socher et al., 2013c). The vector spaces learned by these models cluster words and phrases together based on similarity. For example, a neural network trained for a sentiment analysis task such as restaurant review classification might learn that “tasty” and “delicious” should have similar representations since they are synonymous adjectives. These models have so far only seen success in a limited range of text-based prediction tasks, Consider factoid question answering: given a description of an entity, identify the person, place, or thing discussed. We describe a task with high-quality ma"
D14-1070,D13-1170,1,0.042951,"the Holy Roman Empire. The first sentence contains no words or named entities that by themselves are indicative of the answer, while subsequent sentences contain more and more obvious clues. where inputs are typically a single sentence and outputs are either continuous or a limited discrete set. Neural networks have not yet shown to be useful for tasks that require mapping paragraph-length inputs to rich output spaces. Introduction Deep neural networks have seen widespread use in natural language processing tasks such as parsing, language modeling, and sentiment analysis (Bengio et al., 2003; Socher et al., 2013a; Socher et al., 2013c). The vector spaces learned by these models cluster words and phrases together based on similarity. For example, a neural network trained for a sentiment analysis task such as restaurant review classification might learn that “tasty” and “delicious” should have similar representations since they are synonymous adjectives. These models have so far only seen success in a limited range of text-based prediction tasks, Consider factoid question answering: given a description of an entity, identify the person, place, or thing discussed. We describe a task with high-quality ma"
D14-1070,Q14-1017,1,0.765477,"te (see Section 4.1 for more details). Thus, we have built-in redundancy (the number of “askable” entities is limited), but also built-in diversity, as difficult clues cannot appear in every question without becoming well-known. Dependency-Tree Recursive Neural Networks To compute distributed representations for the individual sentences within quiz bowl questions, we use a dependency-tree rnn (dt-rnn). These representations are then aggregated and fed into a multinomial logistic regression classifier, where class labels are the answers associated with each question instance. In previous work, Socher et al. (2014) use dt-rnns to map text descriptions to images. dt-rnns are robust to similar sentences with slightly different syntax, which is ideal for our problem since answers are often described by many sentences that are similar in meaning but different in structure. Our model improves upon the existing dt-rnn model by jointly learning answer and question representations in the same vector space rather than learning them separately. 3.1 Model Description As in other rnn models, we begin by associating each word w in our vocabulary with a vector representation xw ∈ Rd . These vectors are stored as the"
D14-1070,D07-1003,0,0.0464695,"al., 2012) for Jeopardy, which is limited to single sentences, or to models trained on Yago (Hoffart et al., 2013). We would also like to fairly compare qanta 641 Q A Q A Q A Q A Q Akbar Muhammad Shah Jahan Ghana A Babur Figure 7: An extremely misleading question about John Cabot, at least to computer models. The words muslim and mecca lead to three Mughal emperors in the top five guesses from qanta; other models are similarly led awry. with ir-wiki. A promising avenue for future work would be to incorporate Wikipedia data into qanta by transforming sentences to look like quiz bowl questions (Wang et al., 2007) and to select relevant sentences, as not every sentence in a Wikipedia article directly describes its subject. Syntax-specific annotation (Sayeed et al., 2012) may help in this regard. Finally, we could adapt the attribute space learned by the dt-rnn to use information from knowledge bases and to aid in knowledge base completion. Having learned many facts about entities that occur in question text, a dt-rnn could add new facts to a knowledge base or check existing relationships. 8 Conclusion We present qanta, a dependency-tree recursive neural network for factoid question answering that outpe"
D14-1070,D11-1016,0,0.0348678,"on Wars, rebellions, and battles U.S. presidents Prime ministers Explorers & emperors Policies Other Figure 5: t-SNE 2-D projections of 451 answer vectors divided into six major clusters. The blue cluster is predominantly populated by U.S. presidents. The zoomed plot reveals temporal clustering among the presidents based on the years they spent in office. from the meaning of the words that it contains as well as the syntax that glues those words together. Many computational models of compositionality focus on learning vector spaces (Zanzotto et al., 2010; Erk, 2012; Grefenstette et al., 2013; Yessenalina and Cardie, 2011). Recent approaches towards modeling compositional vector spaces with neural networks have been successful, although simpler functions have been proposed for short phrases (Mitchell and Lapata, 2008). Recursive neural networks have achieved state-of-the-art performance in sentiment analysis and parsing (Socher et al., 2013c; Hermann and Blunsom, 2013; Socher et al., 2013a). rnns have not been previously used for learning attribute spaces as we do here, although recursive tensor networks were unsuccessfully applied to a knowledge base completion task (Socher et al., 2013b). More relevant to thi"
D14-1070,C10-1142,0,0.00402061,"ane_addams hull_house jimmy_carter ronald_reagan woodrow_wilson Wars, rebellions, and battles U.S. presidents Prime ministers Explorers & emperors Policies Other Figure 5: t-SNE 2-D projections of 451 answer vectors divided into six major clusters. The blue cluster is predominantly populated by U.S. presidents. The zoomed plot reveals temporal clustering among the presidents based on the years they spent in office. from the meaning of the words that it contains as well as the syntax that glues those words together. Many computational models of compositionality focus on learning vector spaces (Zanzotto et al., 2010; Erk, 2012; Grefenstette et al., 2013; Yessenalina and Cardie, 2011). Recent approaches towards modeling compositional vector spaces with neural networks have been successful, although simpler functions have been proposed for short phrases (Mitchell and Lapata, 2008). Recursive neural networks have achieved state-of-the-art performance in sentiment analysis and parsing (Socher et al., 2013c; Hermann and Blunsom, 2013; Socher et al., 2013a). rnns have not been previously used for learning attribute spaces as we do here, although recursive tensor networks were unsuccessfully applied to a knowle"
D18-1241,P18-1078,0,0.0356121,"r all other dialog acts (neither for affirmation and don’t follow up for continuation). Transition matrix We divide the supporting text into 12 chunks (with a special chunk for no answer) and use the transition matrix (computed from the training set) in Figure 5b to select an answer given the position of the previous answer. This baseline does not output other dialog acts. 5.2 Upper bounds Gold NA + TM This is the same transition matrix (TM) baseline as before, except that for questions whose gold annotations are no answer, we always output no answer. et al., 2016, BiDAF) with self-attention (Clark and Gardner, 2018) and contextualized embeddings.16 A token for no answer is appended to s to enable its prediction following Levy et al. (2017). Additionally, we modify the model for our task to also predict dialog acts, placing a classifier over the same representation used to predict the end position of the predicted span. BiDAF++ w/ k-ctx As BiDAF++ does not model any dialog context, we modify the passage and question embedding processes to consider the dialog history. We consider context from the previous k QA pairs.17 • Passage embedding We explicitly identify the previous k answers within the section tex"
D18-1241,D17-1070,0,0.0153231,"ng Naively prepending the previous k questions to the current question did not show gains in initial experiments. We opt instead to simply encode the dialog turn number within the question embedding. 5.4 Results Table 4 summarizes our results (each cell displays dev/test scores), where dialog acts are Yes/No (affirmation) and Follow up (continuation). For comparison to other datasets, we report F1 without filtering low-agreement QA pairs (F1’). Pretrained InferSent To test the importance of lexical matching in our dataset, we output the sentence in s whose pretrained InferSent representation (Conneau et al., 2017) has the highest cosine similarity to that of the question. Sanity check Overall, the poor sanity check results imply that is very challenging. Of these, following the transition matrix (TM) gives the best performance, reinforcing the observation that the dialog context plays a significant role in the task. Feature-rich logistic regression We train a logistic regression using Vowpal Wabbit (Langford et al., 2007) to select answer sentences. We use simple matching features (e.g., n-gram overlap between questions and candidate answers), bias features (position and length of a candidate), and con"
D18-1241,H94-1010,0,0.675779,"Missing"
D18-1241,K17-1034,1,0.798034,"ext into 12 chunks (with a special chunk for no answer) and use the transition matrix (computed from the training set) in Figure 5b to select an answer given the position of the previous answer. This baseline does not output other dialog acts. 5.2 Upper bounds Gold NA + TM This is the same transition matrix (TM) baseline as before, except that for questions whose gold annotations are no answer, we always output no answer. et al., 2016, BiDAF) with self-attention (Clark and Gardner, 2018) and contextualized embeddings.16 A token for no answer is appended to s to enable its prediction following Levy et al. (2017). Additionally, we modify the model for our task to also predict dialog acts, placing a classifier over the same representation used to predict the end position of the predicted span. BiDAF++ w/ k-ctx As BiDAF++ does not model any dialog context, we modify the passage and question embedding processes to consider the dialog history. We consider context from the previous k QA pairs.17 • Passage embedding We explicitly identify the previous k answers within the section text by concatenating marker embeddings to the existing word embeddings. Gold sentence + NA To see if can be treated as an answer"
D18-1241,D17-1259,0,0.0131978,"of CoQA answers are named entities or short noun phrases, much like those in SQuAD. In contrast, the asymmetric nature of forces students to ask more exploratory questions whose answers can be potentially be followed up on.19 Dialog fits into an increasing interest in open domain dialog, mostly studied in the context of social chit-chat (Li et al., 2016; Ritter et al., 2011; Fang et al., 2017; Ghazvininejad et al., 2018). Most related to our effort is visual dialog (Das et al., 2017), which relies on images as evidence instead of text. More explicit goal driven scenarios, such as bargaining (Lewis et al., 2017) and item guessing (He et al., 2017) have also been explored, but the language is more constrained than in . Information-seeking dialog specifically was studied in Stede and Schlangen (2004). 7 Conclusion In this paper, we introduce , a large scale dataset of information-seeking dialogs over sections from Wikipedia articles. Our data collection process, which takes the form of a teacher-student interaction between two crowd workers, encourages questions that are highly contextual, openended, and even unanswerable from the text. Our baselines, which include top performers on existing machine co"
D18-1241,D16-1127,0,0.0402491,"xt (such as traffic laws) by interacting with a user through dialog. Also concurrently, Reddy et al. (2018) propose conversational question answering (CoQA) from text but allow both students and questioners to see the evidence. As a result, a large percentage of CoQA answers are named entities or short noun phrases, much like those in SQuAD. In contrast, the asymmetric nature of forces students to ask more exploratory questions whose answers can be potentially be followed up on.19 Dialog fits into an increasing interest in open domain dialog, mostly studied in the context of social chit-chat (Li et al., 2016; Ritter et al., 2011; Fang et al., 2017; Ghazvininejad et al., 2018). Most related to our effort is visual dialog (Das et al., 2017), which relies on images as evidence instead of text. More explicit goal driven scenarios, such as bargaining (Lewis et al., 2017) and item guessing (He et al., 2017) have also been explored, but the language is more constrained than in . Information-seeking dialog specifically was studied in Stede and Schlangen (2004). 7 Conclusion In this paper, we introduce , a large scale dataset of information-seeking dialogs over sections from Wikipedia articles. Our data c"
D18-1241,P18-2124,1,0.903645,"the evidence text, as in existing reading comprehension tasks (Rajpurkar et al., 2016). Additionally, the teacher uses dialog acts to provide the student with feedback (e.g., “ask a follow up ques2174 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2174–2184 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics Dataset QuAC CoQA (Reddy et al., 2018) CSQA (Saha et al., 2018) CQA (Talmor and Berant, 2018) SQA (Iyyer et al., 2017) NarrativeQA (Kocisk´y et al., 2017) TriviaQA (Joshi et al., 2017) SQuAD 2.0 (Rajpurkar et al., 2018) MS Marco (Nguyen et al., 2016) NewsQA (Trischler et al., 2016) Multi turn Textbased Dialog Acts Simple Evaluation Unanswerable Questions Asker Can’t See Evidence 4 4 4 4 4 4 4 4 4 4 7 7 7 7 7 4 7 4 7 4 4 4 4 4 7 7 7 7 7 7 7 7 7 4 7 4 4 7 4 4 7 4 4 4 7 7 7 7 4 4 4 7 7 4 7 4 4 7 4 4 Table 1: Comparison of the QUAC dataset to other question answering datasets. tion”), which makes the dialogs more productive. We collect the dataset in an interactive setting where two crowd workers play the roles of teacher and student. To encourage natural and diverse questions, we do not follow previous dialogst"
D18-1241,P17-1162,1,0.821322,"short noun phrases, much like those in SQuAD. In contrast, the asymmetric nature of forces students to ask more exploratory questions whose answers can be potentially be followed up on.19 Dialog fits into an increasing interest in open domain dialog, mostly studied in the context of social chit-chat (Li et al., 2016; Ritter et al., 2011; Fang et al., 2017; Ghazvininejad et al., 2018). Most related to our effort is visual dialog (Das et al., 2017), which relies on images as evidence instead of text. More explicit goal driven scenarios, such as bargaining (Lewis et al., 2017) and item guessing (He et al., 2017) have also been explored, but the language is more constrained than in . Information-seeking dialog specifically was studied in Stede and Schlangen (2004). 7 Conclusion In this paper, we introduce , a large scale dataset of information-seeking dialogs over sections from Wikipedia articles. Our data collection process, which takes the form of a teacher-student interaction between two crowd workers, encourages questions that are highly contextual, openended, and even unanswerable from the text. Our baselines, which include top performers on existing machine comprehension datasets, significantly"
D18-1241,D16-1264,1,0.889651,", who does not see the section text, asks questions. The teacher provides a response in the form of a text span (or No answer ), optionally yes or no ( Yes / No ), and encouragement about continuing a ¯ , or should line of questioning (should, ,→ , could ,→ not 6,→ ask a follow-up question). Wikipedia page), which only the teacher can access. Given just the section’s heading, “Origin & History”, the student aims to learn as much as possible about its contents by asking questions. The teacher answers these questions with spans from the evidence text, as in existing reading comprehension tasks (Rajpurkar et al., 2016). Additionally, the teacher uses dialog acts to provide the student with feedback (e.g., “ask a follow up ques2174 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2174–2184 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics Dataset QuAC CoQA (Reddy et al., 2018) CSQA (Saha et al., 2018) CQA (Talmor and Berant, 2018) SQA (Iyyer et al., 2017) NarrativeQA (Kocisk´y et al., 2017) TriviaQA (Joshi et al., 2017) SQuAD 2.0 (Rajpurkar et al., 2018) MS Marco (Nguyen et al., 2016) NewsQA (Trischler et al., 2016)"
D18-1241,P17-1167,1,0.880503,"ible about its contents by asking questions. The teacher answers these questions with spans from the evidence text, as in existing reading comprehension tasks (Rajpurkar et al., 2016). Additionally, the teacher uses dialog acts to provide the student with feedback (e.g., “ask a follow up ques2174 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2174–2184 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics Dataset QuAC CoQA (Reddy et al., 2018) CSQA (Saha et al., 2018) CQA (Talmor and Berant, 2018) SQA (Iyyer et al., 2017) NarrativeQA (Kocisk´y et al., 2017) TriviaQA (Joshi et al., 2017) SQuAD 2.0 (Rajpurkar et al., 2018) MS Marco (Nguyen et al., 2016) NewsQA (Trischler et al., 2016) Multi turn Textbased Dialog Acts Simple Evaluation Unanswerable Questions Asker Can’t See Evidence 4 4 4 4 4 4 4 4 4 4 7 7 7 7 7 4 7 4 7 4 4 4 4 4 7 7 7 7 7 7 7 7 7 4 7 4 4 7 4 4 7 4 4 4 7 7 7 7 4 4 4 7 7 4 7 4 4 7 4 4 Table 1: Comparison of the QUAC dataset to other question answering datasets. tion”), which makes the dialogs more productive. We collect the dataset in an interactive setting where two crowd workers play the roles o"
D18-1241,P17-1147,1,0.902703,"hese questions with spans from the evidence text, as in existing reading comprehension tasks (Rajpurkar et al., 2016). Additionally, the teacher uses dialog acts to provide the student with feedback (e.g., “ask a follow up ques2174 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2174–2184 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics Dataset QuAC CoQA (Reddy et al., 2018) CSQA (Saha et al., 2018) CQA (Talmor and Berant, 2018) SQA (Iyyer et al., 2017) NarrativeQA (Kocisk´y et al., 2017) TriviaQA (Joshi et al., 2017) SQuAD 2.0 (Rajpurkar et al., 2018) MS Marco (Nguyen et al., 2016) NewsQA (Trischler et al., 2016) Multi turn Textbased Dialog Acts Simple Evaluation Unanswerable Questions Asker Can’t See Evidence 4 4 4 4 4 4 4 4 4 4 7 7 7 7 7 4 7 4 7 4 4 4 4 4 7 7 7 7 7 7 7 7 7 4 7 4 4 7 4 4 7 4 4 4 7 7 7 7 4 4 4 7 7 4 7 4 4 7 4 4 Table 1: Comparison of the QUAC dataset to other question answering datasets. tion”), which makes the dialogs more productive. We collect the dataset in an interactive setting where two crowd workers play the roles of teacher and student. To encourage natural and diverse questions,"
D18-1241,D11-1054,0,0.0542229,"ic laws) by interacting with a user through dialog. Also concurrently, Reddy et al. (2018) propose conversational question answering (CoQA) from text but allow both students and questioners to see the evidence. As a result, a large percentage of CoQA answers are named entities or short noun phrases, much like those in SQuAD. In contrast, the asymmetric nature of forces students to ask more exploratory questions whose answers can be potentially be followed up on.19 Dialog fits into an increasing interest in open domain dialog, mostly studied in the context of social chit-chat (Li et al., 2016; Ritter et al., 2011; Fang et al., 2017; Ghazvininejad et al., 2018). Most related to our effort is visual dialog (Das et al., 2017), which relies on images as evidence instead of text. More explicit goal driven scenarios, such as bargaining (Lewis et al., 2017) and item guessing (He et al., 2017) have also been explored, but the language is more constrained than in . Information-seeking dialog specifically was studied in Stede and Schlangen (2004). 7 Conclusion In this paper, we introduce , a large scale dataset of information-seeking dialogs over sections from Wikipedia articles. Our data collection process, wh"
D18-1241,D18-1233,0,0.203396,"are the first to incorporate these into information-seeking dialog. Sequential QA Our work is similar to sequential question answering against knowledge bases (Iyyer et al., 2017) and the web (Talmor and Berant, 2018), but instead of decomposing a single question into smaller questions, we rely on the curiosity of the student to generate a sequence of questions. Such open information seeking was studied in semantic parsing on knowledge bases (Dahl et al., 1994) and more recently with modern approaches (Saha et al., 2018), but with questions paraphrased from templates. Concurrent to our work, Saeidi et al. (2018) proposed a task of generating and answering yes/no questions for rule focused text (such as traffic laws) by interacting with a user through dialog. Also concurrently, Reddy et al. (2018) propose conversational question answering (CoQA) from text but allow both students and questioners to see the evidence. As a result, a large percentage of CoQA answers are named entities or short noun phrases, much like those in SQuAD. In contrast, the asymmetric nature of forces students to ask more exploratory questions whose answers can be potentially be followed up on.19 Dialog fits into an increasing in"
D18-1241,N18-1059,0,0.1584,"aims to learn as much as possible about its contents by asking questions. The teacher answers these questions with spans from the evidence text, as in existing reading comprehension tasks (Rajpurkar et al., 2016). Additionally, the teacher uses dialog acts to provide the student with feedback (e.g., “ask a follow up ques2174 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2174–2184 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics Dataset QuAC CoQA (Reddy et al., 2018) CSQA (Saha et al., 2018) CQA (Talmor and Berant, 2018) SQA (Iyyer et al., 2017) NarrativeQA (Kocisk´y et al., 2017) TriviaQA (Joshi et al., 2017) SQuAD 2.0 (Rajpurkar et al., 2018) MS Marco (Nguyen et al., 2016) NewsQA (Trischler et al., 2016) Multi turn Textbased Dialog Acts Simple Evaluation Unanswerable Questions Asker Can’t See Evidence 4 4 4 4 4 4 4 4 4 4 7 7 7 7 7 4 7 4 7 4 4 4 4 4 7 7 7 7 7 7 7 7 7 4 7 4 4 7 4 4 7 4 4 4 7 7 7 7 4 4 4 7 7 4 7 4 4 7 4 4 Table 1: Comparison of the QUAC dataset to other question answering datasets. tion”), which makes the dialogs more productive. We collect the dataset in an interactive setting where two crowd"
D18-1407,W16-1601,0,0.122618,"Missing"
D18-1407,D15-1075,0,0.24531,"rather than providing explanations of the original prediction, our reduced examples more closely resemble adversarial examples. In Figure 1, the reduced input is meaningless to a human but retains the same model prediction with high confidence. Gradient-based input reduction exposes pathological model behaviors that contradict what one expects based on existing interpretation methods. In Section 2, we construct more of these counterintuitive examples by augmenting input reduction with beam search and experiment with three tasks: SQ UAD (Rajpurkar et al., 2016) for reading comprehension, SNLI (Bowman et al., 2015) for textual entailment, and VQA (Antol et al., 2015) for visual question answering. Input reduction with beam search consistently reduces the input sentence to very short lengths—often only one or two words—without lowering model confidence on its original prediction. The reduced examples appear nonsensical to humans, which we verify with crowdsourced experiments. In Section 3, we draw connections to adversarial examples and confidence calibration; we explain why the observed pathologies are a consequence of the overconfidence of neural models. This elucidates limitations of interpretation me"
D18-1407,D12-1118,1,0.90524,"Missing"
D18-1407,P17-1171,0,0.0140056,"e model changes its prediction. We experi3720 ment with three popular datasets: SQ UAD (Rajpurkar et al., 2016) for reading comprehension, SNLI (Bowman et al., 2015) for textual entailment, and VQA (Antol et al., 2015) for visual question answering. We describe each of these tasks and the model we use below, providing full details in the Supplement. In SQ UAD, each example is a context paragraph and a question. The task is to predict a span in the paragraph as the answer. We reduce only the question while keeping the context paragraph unchanged. The model we use is the D R QA Document Reader (Chen et al., 2017). In SNLI, each example consists of two sentences: a premise and a hypothesis. The task is to predict one of three relationships: entailment, neutral, or contradiction. We reduce only the hypothesis while keeping the premise unchanged. The model we use is Bilateral Multi-Perspective Matching (B I MPM) (Wang et al., 2017). In VQA, each example consists of an image and a natural language question. We reduce only the question while keeping the image unchanged. The model we use is Show, Ask, Attend, and Answer (Kazemi and Elqursh, 2017). During the iterative reduction process, we ensure that the p"
D18-1407,N18-2017,0,0.083614,"ut—cause significant changes in the interpretation. Ghorbani et al. (2017) make a similar observation about secondorder sensitivity, that “the fragility of interpretation is orthogonal to fragility of the prediction”. Previous work studies biases in the annotation process that lead to datasets easier than desired or expected which eventually induce pathological models. We attribute our observed pathologies primarily to the lack of accurate uncertainty estimates in neural models trained with maximum likelihood. SNLI hypotheses contain artifacts that allow training a model without the premises (Gururangan et al., 2018); we apply input reduction at test time to the hypothesis. Similarly, VQA images are surprisingly unimportant for training a model; we reduce the question. The recent SQ UAD 2.0 (Rajpurkar et al., 2018) augments the original reading comprehension task with an uncertainty modeling requirement, the goal being to make the task more realistic and challenging. Section 3.1 explains the pathologies from the overconfidence perspective. One explanation for overconfidence is overfitting: Guo et al. (2017) show that, late in maximum likelihood training, 3725 the model learns to minimize loss by outputtin"
D18-1407,P15-1162,1,0.896952,"Missing"
D18-1407,N18-1170,1,0.844269,"n et al., 2015), but to our knowledge not for NLP. Our input reduction process gradually transforms a valid input into a rubbish example. We can often determine which word’s removal causes the transition to occur—for example, removing “Broncos” in Figure 5. These rubbish examples are particularly interesting, as they are also adversarial: the difference from a valid example is small, unlike image rubbish examples generated from pure noise which are far outside the training data distribution. The robustness of NLP models has been studied extensively (Papernot et al., 2016; Jia and Liang, 2017; Iyyer et al., 2018; Ribeiro et al., 2018), and most studies define adversarial examples similar to the image domain: small perturbations to the input lead to large changes in the output. HotFlip (Ebrahimi et al., 2017) uses a gradient-based approach, similar to image adversarial examples, to flip the model prediction by perturbing a few characters or words. Our work and Belinkov and Bisk (2018) both identify cases where noisy user inputs become adversarial by accident: common misspellings break neural machine translation models; we show that incomplete user input can lead to unreasonably high model confidence."
D18-1407,D17-1215,0,0.0558403,"w et al., 2015; Nguyen et al., 2015), but to our knowledge not for NLP. Our input reduction process gradually transforms a valid input into a rubbish example. We can often determine which word’s removal causes the transition to occur—for example, removing “Broncos” in Figure 5. These rubbish examples are particularly interesting, as they are also adversarial: the difference from a valid example is small, unlike image rubbish examples generated from pure noise which are far outside the training data distribution. The robustness of NLP models has been studied extensively (Papernot et al., 2016; Jia and Liang, 2017; Iyyer et al., 2018; Ribeiro et al., 2018), and most studies define adversarial examples similar to the image domain: small perturbations to the input lead to large changes in the output. HotFlip (Ebrahimi et al., 2017) uses a gradient-based approach, similar to image adversarial examples, to flip the model prediction by perturbing a few characters or words. Our work and Belinkov and Bisk (2018) both identify cases where noisy user inputs become adversarial by accident: common misspellings break neural machine translation models; we show that incomplete user input can lead to unreasonably hig"
D18-1407,N16-1082,0,0.389222,"he instability of neural network predictions by showing how small perturbations to the input dramatically change the output. A common, non-adversarial form of model interpretation is feature attribution: features that are crucial for predictions are highlighted in a heatmap. One can measure a feature’s importance by input perturbation. Given an input for text classification, a word’s importance can be measured by the difference in model confidence before and after that word is removed from the input—the word is important if confidence decreases significantly. This is the leave-one-out method (Li et al., 2016b). Gradients can also measure feature importance; for example, a feature is influential to the prediction if its gradient is a large positive value. Both perturbation and gradient-based methods can generate heatmaps, implying that the model’s prediction is highly influenced by the highlighted, important words. Instead, we study how the model’s prediction is influenced by the unimportant words. We use input reduction, a process that iteratively removes the unimportant words from the input while maintaining the model’s prediction. Intuitively, the words remaining after input reduction should be"
D18-1407,P11-1015,0,0.175621,"Missing"
D18-1407,D16-1244,0,0.109526,"Missing"
D18-1407,P18-2124,0,0.0185617,"e prediction”. Previous work studies biases in the annotation process that lead to datasets easier than desired or expected which eventually induce pathological models. We attribute our observed pathologies primarily to the lack of accurate uncertainty estimates in neural models trained with maximum likelihood. SNLI hypotheses contain artifacts that allow training a model without the premises (Gururangan et al., 2018); we apply input reduction at test time to the hypothesis. Similarly, VQA images are surprisingly unimportant for training a model; we reduce the question. The recent SQ UAD 2.0 (Rajpurkar et al., 2018) augments the original reading comprehension task with an uncertainty modeling requirement, the goal being to make the task more realistic and challenging. Section 3.1 explains the pathologies from the overconfidence perspective. One explanation for overconfidence is overfitting: Guo et al. (2017) show that, late in maximum likelihood training, 3725 the model learns to minimize loss by outputting low-entropy distributions without improving validation accuracy. To examine if overfitting can explain the input reduction results, we run input reduction using D R QA model checkpoints from every tra"
D18-1407,N16-3020,0,0.549742,"he reduced examples appear nonsensical to humans, which we verify with crowdsourced experiments. In Section 3, we draw connections to adversarial examples and confidence calibration; we explain why the observed pathologies are a consequence of the overconfidence of neural models. This elucidates limitations of interpretation methods that rely on model confidence. In Section 4, we encourage high model uncertainty on reduced examples with entropy regularization. The pathological model behavior under input reduction is mitigated, leading to more reasonable reduced examples. 2 Input Reduction 2.1 Ribeiro et al. (2016) and Li et al. (2016b) define importance by seeing how confidence changes when a feature is removed; a natural approximation is to use the gradient (Baehrens et al., 2010; Simonyan et al., 2014). We formally define these importance metrics in natural language contexts and introduce the efficient gradient-based approximation. For each word in an input sentence, we measure its importance by the change in the confidence of the original prediction when we remove that word from the sentence. We switch the sign so that when the confidence decreases, the importance value is positive. Formally, let x"
D18-1407,P18-1079,0,0.0395479,"to our knowledge not for NLP. Our input reduction process gradually transforms a valid input into a rubbish example. We can often determine which word’s removal causes the transition to occur—for example, removing “Broncos” in Figure 5. These rubbish examples are particularly interesting, as they are also adversarial: the difference from a valid example is small, unlike image rubbish examples generated from pure noise which are far outside the training data distribution. The robustness of NLP models has been studied extensively (Papernot et al., 2016; Jia and Liang, 2017; Iyyer et al., 2018; Ribeiro et al., 2018), and most studies define adversarial examples similar to the image domain: small perturbations to the input lead to large changes in the output. HotFlip (Ebrahimi et al., 2017) uses a gradient-based approach, similar to image adversarial examples, to flip the model prediction by perturbing a few characters or words. Our work and Belinkov and Bisk (2018) both identify cases where noisy user inputs become adversarial by accident: common misspellings break neural machine translation models; we show that incomplete user input can lead to unreasonably high model confidence. Other failures of inter"
D18-1407,D14-1162,0,\N,Missing
D18-1407,P16-1223,0,\N,Missing
D18-1407,D16-1264,0,\N,Missing
D18-1407,D16-1011,0,\N,Missing
D18-1407,W17-5401,0,\N,Missing
D18-1505,D14-1162,0,0.0804832,"Missing"
D18-1505,N18-1202,1,0.85799,".ac.in Abstract We analyze the performance of different sentiment classification models on syntacticallycomplex inputs like A-but-B sentences. The first contribution of this analysis addresses reproducible research: to meaningfully compare different models, their accuracies must be averaged over far more random seeds than what has traditionally been reported. With proper averaging in place, we notice that the distillation model described in Hu et al. (2016), which incorporates explicit logic rules for sentiment classification, is ineffective. In contrast, using contextualized ELMo embeddings (Peters et al., 2018a) instead of logic rules yields significantly better performance. Additionally, we provide analysis and visualizations that demonstrate ELMo’s ability to implicitly learn logic rules. Finally, a crowdsourced analysis reveals how ELMo outperforms baseline models even on sentences with ambiguous sentiment labels. 1 Introduction In this paper, we explore the effectiveness of methods designed to improve sentiment classification (positive vs. negative) of sentences that contain complex syntactic structures. While simple bag-of-words or lexicon-based methods (Pang and Lee, 2005; Wang and Manning, 2"
D18-1505,D18-1179,0,0.228692,".ac.in Abstract We analyze the performance of different sentiment classification models on syntacticallycomplex inputs like A-but-B sentences. The first contribution of this analysis addresses reproducible research: to meaningfully compare different models, their accuracies must be averaged over far more random seeds than what has traditionally been reported. With proper averaging in place, we notice that the distillation model described in Hu et al. (2016), which incorporates explicit logic rules for sentiment classification, is ineffective. In contrast, using contextualized ELMo embeddings (Peters et al., 2018a) instead of logic rules yields significantly better performance. Additionally, we provide analysis and visualizations that demonstrate ELMo’s ability to implicitly learn logic rules. Finally, a crowdsourced analysis reveals how ELMo outperforms baseline models even on sentences with ambiguous sentiment labels. 1 Introduction In this paper, we explore the effectiveness of methods designed to improve sentiment classification (positive vs. negative) of sentences that contain complex syntactic structures. While simple bag-of-words or lexicon-based methods (Pang and Lee, 2005; Wang and Manning, 2"
D19-1161,N19-1116,1,0.848341,"lin et al., 2019). We analyze the clustered constituents and observe they are separated syntactically (i.e. past tense vs. present participle verbs) and semantically (i.e. time-related phrases vs. references to people). 2 DIORA: Deep Inside-Outside Recursive Autoencoders DIORA is a recursive autoencoder that learns to reconstruct an input sentence. A fundamental step in the reconstruction is to build a chart using the inside-outside algorithm (Baker, 1979), which represents a soft weighting over all possible binary trees of the input sentence. For all the model details, we refer the reader to Drozdov et al. (2019). For this work, it is key to understand two capabilities that DIORA provides: each span in a sentence is represented as a vector and DIORA induces a maximally likely binary tree for the sentence. We can directly label the constituents of a sentence by clustering the learned span vectors from DIORA and assigning a label to each cluster. DIORA’s autoencoder objective incentivizes the model to learn representations that compress the sentence well in order to reconstruct the input leading to the discovery of syntactic structure. To encourage phrase representations to be easily clusterable into a"
D19-1161,D15-1075,0,0.101709,"Missing"
D19-1161,N19-1423,0,0.162955,"he previous best model, Haghighi and Klein 2006, which unlike our approach uses gold partof-speech tags). Furthermore, we show DIORA is competitive when a ground truth bracketing is not provided, and instead must be induced. On 1507 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1507–1512, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics the full WSJ test set, DIORA outperforms two strong baselines, ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2019). We analyze the clustered constituents and observe they are separated syntactically (i.e. past tense vs. present participle verbs) and semantically (i.e. time-related phrases vs. references to people). 2 DIORA: Deep Inside-Outside Recursive Autoencoders DIORA is a recursive autoencoder that learns to reconstruct an input sentence. A fundamental step in the reconstruction is to build a chart using the inside-outside algorithm (Baker, 1979), which represents a soft weighting over all possible binary trees of the input sentence. For all the model details, we refer the reader to Drozdov et al. (2"
D19-1161,P06-1111,0,0.0477938,"eled constituency parsing. In this paper, we instead focus on labeled constituency parsing for English. The small number of previous works that exist in this area suffer from substantial weaknesses: 1) the models depend on ground-truth part-of-speech tags, which are not always available and known to boost constituency parsing scores (Kitaev and Klein, 2018), 2) none can simultaneously identify and label constituents (instead they typically depend on an external latent parser), and 3) they ignore sentences longer than ten tokens because previous latent parsers do not scale to longer sentences (Haghighi and Klein, 2006; Borensztajn and Zuidema, 2007; Reichart and Rappoport, 2008). Unlike previous work, we achieve strong results in unlabeled constituency parsing using a single model for both bracketing and labeling. Our approach relies on clustering span representations, which are fixed-length continuous vectors learned end-to-end using DIORA and do not require external resources such as part-of-speech tags. Furthermore, we enhance the DIORA architecture with latent codes: the model learns a distribution over these codes that loosely aligns with the groundtruth assignment of phrase types and, more importantl"
D19-1161,N19-1419,0,0.0712323,"Missing"
D19-1161,W18-5452,0,0.026974,"Missing"
D19-1161,N19-1114,0,0.0756686,"Missing"
D19-1161,P18-1249,0,0.0998597,"Missing"
D19-1161,P02-1017,0,0.496704,"ibed in (Drozdov et al., 2019). We are interested in clustering the learned vectors a(i, j) such that each span may be mapped to a phrase type. To enhance this clustering based approach, we augment the DIORA architecture with latent codes, shown in the right half of the figure. Introduction The deep inside-outside recursive autoencoder (Drozdov et al., 2019, DIORA) is part of a recent trend in fully unsupervised neural constituency parsers (Shen et al., 2018; Williams et al., 2018a; Htut et al., 2018; Shen et al., 2019; Kim et al., 2019). However, these works and nearly all previous research (Klein and Manning, 2002; Seginer, 2007; Ponvert et al., 2011; Spitkovsky et al., 2013) have focused on unlabeled constituency parsing. In this paper, we instead focus on labeled constituency parsing for English. The small number of previous works that exist in this area suffer from substantial weaknesses: 1) the models depend on ground-truth part-of-speech tags, which are not always available and known to boost constituency parsing scores (Kitaev and Klein, 2018), 2) none can simultaneously identify and label constituents (instead they typically depend on an external latent parser), and 3) they ignore sentences long"
D19-1161,N18-1202,1,0.843099,"relative error reduction over the previous best model, Haghighi and Klein 2006, which unlike our approach uses gold partof-speech tags). Furthermore, we show DIORA is competitive when a ground truth bracketing is not provided, and instead must be induced. On 1507 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1507–1512, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics the full WSJ test set, DIORA outperforms two strong baselines, ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2019). We analyze the clustered constituents and observe they are separated syntactically (i.e. past tense vs. present participle verbs) and semantically (i.e. time-related phrases vs. references to people). 2 DIORA: Deep Inside-Outside Recursive Autoencoders DIORA is a recursive autoencoder that learns to reconstruct an input sentence. A fundamental step in the reconstruction is to build a chart using the inside-outside algorithm (Baker, 1979), which represents a soft weighting over all possible binary trees of the input sentence. For all the model details, we refe"
D19-1161,D18-1179,0,0.123893,"relative error reduction over the previous best model, Haghighi and Klein 2006, which unlike our approach uses gold partof-speech tags). Furthermore, we show DIORA is competitive when a ground truth bracketing is not provided, and instead must be induced. On 1507 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1507–1512, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics the full WSJ test set, DIORA outperforms two strong baselines, ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2019). We analyze the clustered constituents and observe they are separated syntactically (i.e. past tense vs. present participle verbs) and semantically (i.e. time-related phrases vs. references to people). 2 DIORA: Deep Inside-Outside Recursive Autoencoders DIORA is a recursive autoencoder that learns to reconstruct an input sentence. A fundamental step in the reconstruction is to build a chart using the inside-outside algorithm (Baker, 1979), which represents a soft weighting over all possible binary trees of the input sentence. For all the model details, we refe"
D19-1161,P11-1108,0,0.0567434,"Missing"
D19-1161,D13-1204,0,0.0508066,"Missing"
D19-1161,P19-1452,0,0.0161425,"and BERT do not induce structure whatsoever and depend on DIORA for the Induced evaluation. ELMoCI uses only the context-insensitive character embeddings produced by ELMo. pervised labeled parsing. When a reference parse is provided, it is only necessary to derive ad-hoc phrase vectors using the contextualized token vectors from these models. Peters et al. (2018b) describe an effective way to do so for ELMo, which involves concatenating the token vectors at the beginning and end of the phrase.4 For BERT, it is critical to look at all layers as lower layers tend to be more syntactic in nature (Tenney et al., 2019). For both models, we report the max F1 and mean F1, and for the Induced evaluation we use the parses extracted from DIORA. 4.3 WSJ Unsupervised constituency parsing has often been evaluated on different splits of the WSJ. For labeled constituency parsing, models that produce 4 We tried many combinations (4 variants for ELMo and nearly 200 for BERT). They are described in Appendix A.2. 1509 4.4 Model Ablations As an alternative to clustering the constituent vectors with K-means, one can treat the codebook affinity scores, (C > W x), as a soft assignment over the clusters represented by each co"
D19-1161,Q18-1019,1,0.893278,"Missing"
D19-1161,N18-1101,0,0.0273822,"Missing"
D19-1161,C08-1091,0,0.0760276,"s on labeled constituency parsing for English. The small number of previous works that exist in this area suffer from substantial weaknesses: 1) the models depend on ground-truth part-of-speech tags, which are not always available and known to boost constituency parsing scores (Kitaev and Klein, 2018), 2) none can simultaneously identify and label constituents (instead they typically depend on an external latent parser), and 3) they ignore sentences longer than ten tokens because previous latent parsers do not scale to longer sentences (Haghighi and Klein, 2006; Borensztajn and Zuidema, 2007; Reichart and Rappoport, 2008). Unlike previous work, we achieve strong results in unlabeled constituency parsing using a single model for both bracketing and labeling. Our approach relies on clustering span representations, which are fixed-length continuous vectors learned end-to-end using DIORA and do not require external resources such as part-of-speech tags. Furthermore, we enhance the DIORA architecture with latent codes: the model learns a distribution over these codes that loosely aligns with the groundtruth assignment of phrase types and, more importantly, improves the quality of the clusters. Our code-enhanced DIO"
D19-1161,D10-1001,0,0.0405301,"ntence where the leaves of the tree are the words in the sentence. The tree is not labeled. This may be derived from the ground truth parse or induced using DIORA. When induced, we extract a binary tree by running the CKY algorithm2 over DIORA’s learned compatibility scores. 1 More details about the equation C > (CW x) are discussed in Appendix A.3. It’s worth noting that can be an arbitrary function, in this work we use the identity function. 2 The CKY algorithm is an efficient dynamic programming approach for recognizing constituency trees using exact inference (Kasami, 1966; Younger, 1967; Rush et al., 2010). 1508 – WSJ (Test) – Gold Induced Model F1µ F1max F1µ F1max – WSJ-10 – Model Upper Bound Majority (NP) 76.3 30.6 76.3 30.6 59.7 24.5 59.7 24.5 Upper Bound 86.0 Majority (NP) 32.0 86.0 32.0 64.6 25.2 64.6 25.2 ELMo ELMoCI BERT 58.5 53.4 41.8 59.4 56.3 42.2 43.5 38.5 38.1 48.2 40.2 38.3 ELMo ELMoCI BERT 67.8 65.9 54.6 68.9 67.3 57.8 50.1 46.0 44.5 53.0 47.6 45.2 DIORA DIORACB DIORA⇤CB 62.5 ±0.5 64.5 ±0.6 66.4 ±0.7 63.4 65.5 67.8 50.2 ±0.5 49.8 ±0.7 50.4 ±0.7 51.4 50.6 51.5 DIORA DIORACB DIORA⇤CB 72.7 ±1.5 73.2 ±1.7 74.9 ±1.1 76.2 75.7 76.7 55.2 ±0.7 54.5 ±1.2 53.9 ±0.8 56.3 56.6 55.1 PCFG† - 51"
D19-1161,P07-1049,0,0.588301,"2019). We are interested in clustering the learned vectors a(i, j) such that each span may be mapped to a phrase type. To enhance this clustering based approach, we augment the DIORA architecture with latent codes, shown in the right half of the figure. Introduction The deep inside-outside recursive autoencoder (Drozdov et al., 2019, DIORA) is part of a recent trend in fully unsupervised neural constituency parsers (Shen et al., 2018; Williams et al., 2018a; Htut et al., 2018; Shen et al., 2019; Kim et al., 2019). However, these works and nearly all previous research (Klein and Manning, 2002; Seginer, 2007; Ponvert et al., 2011; Spitkovsky et al., 2013) have focused on unlabeled constituency parsing. In this paper, we instead focus on labeled constituency parsing for English. The small number of previous works that exist in this area suffer from substantial weaknesses: 1) the models depend on ground-truth part-of-speech tags, which are not always available and known to boost constituency parsing scores (Kitaev and Klein, 2018), 2) none can simultaneously identify and label constituents (instead they typically depend on an external latent parser), and 3) they ignore sentences longer than ten tok"
D19-1666,D16-1057,0,0.114781,"Missing"
D19-1666,W16-5615,1,0.815825,"Missing"
D19-1666,I17-1093,0,0.0425704,"Missing"
D19-1666,N16-1130,0,0.0192761,"channels.4 YouTube automatically captions many videos, allowing us to scrape caption transcripts from 601 NFL games and 854 NCAA games. We next identify the teams playing and game’s year by searching for exact string matches in the video title and manually labeling any videos with underspecified titles. After downloading videos, we tokenize transcripts using spaCy.5 As part-of-speech tags predicted by spaCy are unreliable on our transcript text, we tag FOOTBALL using the ARK TweetNLP POS tagger (Owoputi et al., 2013), which is more robust to noisy and fragmented text, including TV subtitles (Jørgensen et al., 2016). Additionally, we use phrasemachine (Handler et al., 2016) to identify all corpus noun phrases. Finally, we identify player mentions in the transcript text using exact string matches of first, last, and full names to roster information from online archives; these rosters also contain the player’s position.6 Although we initially had concerns about the reliability of transcriptions of player names, we noticed minimal errors on more common names. Qualitatively, we noticed that even uncommon names were often correctly transcribed and capitalized. We leave a more systematic study for future work."
D19-1666,W11-0139,0,0.537391,"Missing"
D19-1666,S18-2005,0,0.0700206,"Missing"
D19-1666,N13-1039,1,0.837169,"Missing"
D19-1666,N18-1202,1,0.592705,"Missing"
N15-1117,D08-1031,0,0.0289439,"demonstrates good performance by using Wikipedia knowledge to strengthen a multi-pass rule based system. In a more recent work, Durrett and Klein (2014) outperform previous systems by building a joint model that matches mentions to Wikipedia entities while doing named entity resolution and coreference resolution simultaneously. We take a different approach by approximating semantic and world knowledge through our word embedding features. Our simple classifier yields a bi1116 nary decision for each mention pair, a method that had been very popular before the last five years (Soon et al., 2001; Bengtson and Roth, 2008; Stoyanov et al., 2010). Recently, better results have been obtained with mention-ranking systems (Luo et al., 2004; Haghighi and Klein, 2010; Durrett and Klein, 2013; Bj¨orkelund and Kuhn, 2014). However, on quiz bowl data, our experiments show that binary classifiers can outperform mention-ranking approaches. 7 Embracing Harder Coreference This paper introduces a new, naturally-occuring coreference dataset that is easy to annotate but difficult for computers to solve. We show that active learning allows us to create a dataset that is rich in different types of coreference. We develop an end"
N15-1117,W12-4503,0,0.0262708,"Missing"
N15-1117,P14-1005,0,0.0433592,"Missing"
N15-1117,D12-1118,1,0.860225,"sentence—while quiz bowl contains 1.16 mentions per sentence (Figure 2). Examples of nested mentions can be seen in in Table 1. Since quiz bowl is a game, it makes the task of solving coreference interesting and challenging for an annotator. In the next section, we use the intrinsic fun of this task to create a new annotated coreference dataset. 4 Intelligent Annotation Here we describe our annotation process. Each document is a single quiz bowl question containing an average of 5.2 sentences. While quiz bowl covers all areas of academic knowledge, we focus on questions about literature from Boyd-Graber et al. (2012), as annotation standards are more straightforward. Our webapp (Figure 3) allows users to annotate a question by highlighting a phrase using their mouse and then pressing a number corresponding to the coreference group to which it belongs. Each group is highlighted with a single color in the interface. The webapp displays a single question at a time, and for some questions, users can compare their answers against gold annotations by the authors. We provide annotators the ability to see if their tags match the gold labels for a few documents as we need to provide a mechanism to help them learn"
N15-1117,de-marneffe-etal-2006-generating,0,0.020866,"Missing"
N15-1117,doddington-etal-2004-automatic,0,0.15856,"Klein, 2013). These results motivate us to develop a very simple end-to-end coreference resolution system consisting of a crfbased mention detector and a pairwise classifier. Our system outperforms the Berkeley system when both have been trained on our new dataset. This result motivates further exploration into complex coreference types absent in newswire data, which we discuss at length in Section 7. 2 Newswire’s Limitations for Coreference Newswire text is widely used as training data for coreference resolution systems. The standard datasets used in the muc (MUC-6, 1995; MUC-7, 1997), ace (Doddington et al., 2004), and CoNLL shared tasks (Pradhan et al., 2011) contain only such text. In this section we argue why this monoculture, despite its many past successes, offer diminishing results for advancing the coreference subfield. First, newswire text has sparse references, and those that it has are mainly identity coreferences and appositives. In the CoNLL 2011 shared task (Pradhan et al., 2007) based on OntoNotes 4.0 (Hovy et al., 2006),2 there are 2.1 mentions per sentence; in the next section we present a dataset with 3.7 mentions per sentence.3 In newswire text, most nominal entities (not including pr"
N15-1117,D13-1203,0,0.166578,"Introduction Coreference resolution—adding annotations to an input text where multiple strings refer to the same entity—is a fundamental problem in computational linguistics. It is challenging because it requires the application of syntactic, semantic, and world knowledge (Ng, 2010). For example, in the sentence Monsieur Poirot assured Hastings that he ought to have faith in him, the strings Monsieur Poirot and him refer to the same person, while Hastings and he refer to a different character. There are a panoply of sophisticated coreference systems, both data-driven (Fernandes et al., 2012; Durrett and Klein, 2013; Durrett and Klein, 2014; Bj¨orkelund and Kuhn, 2014) and rule-based (Pradhan et al., 2011; Lee et al., 2011). Recent CoNLL shared tasks provide the opportunity to make a fair comparison between these systems. However, because all of these shared tasks contain strictly newswire data,1 it is unclear how existing systems perform on more diverse data. We argue in Section 2 that to truly solve coreference resolution, the research community needs high-quality datasets that contain many challenging cases such as nested coreferences and coreferences that can only be resolved using external knowledge"
N15-1117,N10-1061,0,0.104663,"t amount of external knowledge, but existing models lack information about worlds (both real and imaginary) and thus cannot confidently mark these coreferences. We discuss coreference work that incorporates external resources such as Wikipedia in the next section; our aim is to provide a dataset that benefits more from this type of information than newswire does. 6 Related Work We describe relevant data-driven coreference research in this section, all of which train and evaluate on only newswire text. Despite efforts to build better rule-based (Luo et al., 2004) or hybrid statistical systems (Haghighi and Klein, 2010), data-driven systems currently dominate the field. The 2012 CoNLL shared task led to improved data-driven systems for coreference resolution that finally outperformed both the Stanford system (Lee et al., 2011) and the ims system (Bj¨orkelund and Farkas, 2012), the latter of which was the best available publiclyavailable English coreference system at the time. The recently-released Berkeley coreference system (Durrett and Klein, 2013) is especially striking: it performs well with only a sparse set of carefully-chosen features. Semantic knowledge sources—especially WordNet (Miller, 1995) and W"
N15-1117,N06-2015,0,0.0614004,"Coreference Newswire text is widely used as training data for coreference resolution systems. The standard datasets used in the muc (MUC-6, 1995; MUC-7, 1997), ace (Doddington et al., 2004), and CoNLL shared tasks (Pradhan et al., 2011) contain only such text. In this section we argue why this monoculture, despite its many past successes, offer diminishing results for advancing the coreference subfield. First, newswire text has sparse references, and those that it has are mainly identity coreferences and appositives. In the CoNLL 2011 shared task (Pradhan et al., 2007) based on OntoNotes 4.0 (Hovy et al., 2006),2 there are 2.1 mentions per sentence; in the next section we present a dataset with 3.7 mentions per sentence.3 In newswire text, most nominal entities (not including pronouns) are singletons; in other words, they do not corefer to anything. OntoNotes 4.0 development data contains 25.4K singleton nominal entities (Durrett and Klein, 2013), compared to only 7.6K entities which corefer to something (anaphora). On the other hand, most pronominals are anaphoric, which makes them easy to resolve as pronouns are single token entities. While 2 As our representative for “newswire” data, the English"
N15-1117,D14-1070,1,0.878909,"Missing"
N15-1117,W11-1916,0,0.0213565,"plore the linguistic phenomena that make quiz bowl coreference so hard and draw insights from our analysis that may help to guide the next generation of coreference systems. 11 We use default options, including hyperparameters tuned on OntoNotes 1113 Evaluating the Berkeley System on Quiz Bowl Data A Simple Mention Detector Detecting mentions is done differently by different coreference systems. The Berkeley system does rule-based mention detection to detect every NP span, every pronoun, and every named entity, which leads to many spurious mentions. This process is based on an earlier work of Kummerfeld et al. (2011), which assumes that every maximal projection of a noun or a pronoun is a mention and uses rules to weed out spurious mentions. Instead of using such a rule-based mention detector, our system detects mentions via sequence labeling, as detecting mentions is essentially a problem of detecting start and stop points in spans of text. We solve this sequence tagging problem using the mallet (McCallum, 2002) implementation of conditional random fields (Lafferty et al., 2001). Since our data contain nested mentions, the sequence labels are bio markers (Ratinov and Roth, 2009). The features we use, whi"
N15-1117,W12-2409,0,0.053764,"a hundred documents and an evaluation set of fifty documents10 we sample 250 more 9 These numbers do not include singletons as OntoNotes does not have them tagged, while ours does. 10 These were documents tagged by the quiz bowl com1112 documents from our set of 7,000 quiz bowl questions. We use the Berkeley coreference system (described in the next section) for the training phase. In Figure 4 we show the effectiveness of our iteration procedure. Unlike the result shown by Miller et al. (2012), we find that for our dataset voting sampling beats random sampling, which supports the findings of Laws et al. (2012). Voting sampling works by dividing the seed set into multiple parts and using each to train a model. Then, from the rest of the dataset we select the document that has the most variance in results after predicting using all of the models. Once that document gets tagged, we add it to the seed set, retrain, and repeat the procedure. This process is impractical with instance-level active learning methods, as there are 116,125 mention pairs (instances) for just 400 documents. Even with document-level sampling, the procedure of training on all documents in the seed set and then testing every docum"
N15-1117,W11-1902,0,0.30178,"tity—is a fundamental problem in computational linguistics. It is challenging because it requires the application of syntactic, semantic, and world knowledge (Ng, 2010). For example, in the sentence Monsieur Poirot assured Hastings that he ought to have faith in him, the strings Monsieur Poirot and him refer to the same person, while Hastings and he refer to a different character. There are a panoply of sophisticated coreference systems, both data-driven (Fernandes et al., 2012; Durrett and Klein, 2013; Durrett and Klein, 2014; Bj¨orkelund and Kuhn, 2014) and rule-based (Pradhan et al., 2011; Lee et al., 2011). Recent CoNLL shared tasks provide the opportunity to make a fair comparison between these systems. However, because all of these shared tasks contain strictly newswire data,1 it is unclear how existing systems perform on more diverse data. We argue in Section 2 that to truly solve coreference resolution, the research community needs high-quality datasets that contain many challenging cases such as nested coreferences and coreferences that can only be resolved using external knowledge. In contrast, newswire is deliberately written to contain few coreferences, and those coreferences should be"
N15-1117,P04-1018,0,0.275517,"’s Tale Humans solve cases like these using a vast amount of external knowledge, but existing models lack information about worlds (both real and imaginary) and thus cannot confidently mark these coreferences. We discuss coreference work that incorporates external resources such as Wikipedia in the next section; our aim is to provide a dataset that benefits more from this type of information than newswire does. 6 Related Work We describe relevant data-driven coreference research in this section, all of which train and evaluate on only newswire text. Despite efforts to build better rule-based (Luo et al., 2004) or hybrid statistical systems (Haghighi and Klein, 2010), data-driven systems currently dominate the field. The 2012 CoNLL shared task led to improved data-driven systems for coreference resolution that finally outperformed both the Stanford system (Lee et al., 2011) and the ims system (Bj¨orkelund and Farkas, 2012), the latter of which was the best available publiclyavailable English coreference system at the time. The recently-released Berkeley coreference system (Durrett and Klein, 2013) is especially striking: it performs well with only a sparse set of carefully-chosen features. Semantic"
N15-1117,H05-1004,0,0.0608093,"at our lr model outperforms Berkeley by a wide margin when both are trained on the mentions found by our mention detector (crf). For four metrics, the crf mentions actually improve over training on the gold mentions. Why does the lr model outperform Berkeley 13 The muc (Vilain et al., 1995) score is the minimum number of links between mentions to be inserted or deleted when mapping the output to a gold standard key set. bcub (Bagga and Baldwin, 1998) computes the precision and recall for all mentions separately and then combines them to get the final precision and recall of the output. ceafe (Luo, 2005) is an improvement on bcub and does not use entities multiple times to compute scores. Coreference LR CRF Mentions Gold Mentions 75 50 25 0 75 50 25 0 75 50 25 0 BCUB CEAFE Score Berkeley Mentions QB Final (Berkeley trained on QB) MUC F1 P R F1 P R F1 P R Figure 5: All models are trained and evaluated on quiz bowl data via five fold cross validation on F1 , precision, and recall. Berkeley/crf/Gold refers to the mention detection used, lr refers to our logistic regression model and QB Final refers to the Berkeley model trained on quiz bowl data. Our model outperforms the Berkeley model on every"
N15-1117,M95-1025,0,0.71932,"Missing"
N15-1117,P02-1014,0,0.241297,"Missing"
N15-1117,P10-1142,0,0.0243244,"coreferences and named entities. We also successfully integrate active learning into this annotation pipeline to collect documents maximally useful to coreference models. State-of-the-art coreference systems underperform a simple classifier on our new dataset, motivating non-newswire data for future coreference research. 1 Introduction Coreference resolution—adding annotations to an input text where multiple strings refer to the same entity—is a fundamental problem in computational linguistics. It is challenging because it requires the application of syntactic, semantic, and world knowledge (Ng, 2010). For example, in the sentence Monsieur Poirot assured Hastings that he ought to have faith in him, the strings Monsieur Poirot and him refer to the same person, while Hastings and he refer to a different character. There are a panoply of sophisticated coreference systems, both data-driven (Fernandes et al., 2012; Durrett and Klein, 2013; Durrett and Klein, 2014; Bj¨orkelund and Kuhn, 2014) and rule-based (Pradhan et al., 2011; Lee et al., 2011). Recent CoNLL shared tasks provide the opportunity to make a fair comparison between these systems. However, because all of these shared tasks contain"
N15-1117,D14-1162,0,0.080163,"arser (De Marneffe et al., 2006). 1114     m1 and m2 concatenated with their partsof-speech same as above except for an n-word window before and after m1 and m2 how many tokens separate m1 and m2 how many sentences separate m1 and m2 the cosine similarity of word2vec (Mikolov et al., 2013) vector representations of m1 and m2 ; we obtain these vectors by averaging the word embeddings for all words in each mention. We use publicly-available 300dimensional embeddings that have been pretrained on 100B tokens from Google News. same as above except with publicly-available 300-dimensional GloVe (Pennington et al., 2014) vector embeddings trained on 840B tokens from the Common Crawl The first four features are standard in coreference literature and similar to some of the surface features used by the Berkeley system, while the word embedding similarity scores increase our F-measure by about 5 points on the quiz bowl data. Since they have been trained on huge corpora, the word embeddings allow us to infuse world knowledge into our model; for instance, the vector for Russian is more similar to Dostoevsky than Hemingway. Figure 5 shows that our logistic regression model (lr) outperforms the Berkeley system on num"
N15-1117,N06-1025,0,0.0158281,"ld. The 2012 CoNLL shared task led to improved data-driven systems for coreference resolution that finally outperformed both the Stanford system (Lee et al., 2011) and the ims system (Bj¨orkelund and Farkas, 2012), the latter of which was the best available publiclyavailable English coreference system at the time. The recently-released Berkeley coreference system (Durrett and Klein, 2013) is especially striking: it performs well with only a sparse set of carefully-chosen features. Semantic knowledge sources—especially WordNet (Miller, 1995) and Wikipedia—have been used in coreference engines (Ponzetto and Strube, 2006). A system by Ratinov and Roth (2012) demonstrates good performance by using Wikipedia knowledge to strengthen a multi-pass rule based system. In a more recent work, Durrett and Klein (2014) outperform previous systems by building a joint model that matches mentions to Wikipedia entities while doing named entity resolution and coreference resolution simultaneously. We take a different approach by approximating semantic and world knowledge through our word embedding features. Our simple classifier yields a bi1116 nary decision for each mention pair, a method that had been very popular before th"
N15-1117,W11-1901,0,0.374848,"s refer to the same entity—is a fundamental problem in computational linguistics. It is challenging because it requires the application of syntactic, semantic, and world knowledge (Ng, 2010). For example, in the sentence Monsieur Poirot assured Hastings that he ought to have faith in him, the strings Monsieur Poirot and him refer to the same person, while Hastings and he refer to a different character. There are a panoply of sophisticated coreference systems, both data-driven (Fernandes et al., 2012; Durrett and Klein, 2013; Durrett and Klein, 2014; Bj¨orkelund and Kuhn, 2014) and rule-based (Pradhan et al., 2011; Lee et al., 2011). Recent CoNLL shared tasks provide the opportunity to make a fair comparison between these systems. However, because all of these shared tasks contain strictly newswire data,1 it is unclear how existing systems perform on more diverse data. We argue in Section 2 that to truly solve coreference resolution, the research community needs high-quality datasets that contain many challenging cases such as nested coreferences and coreferences that can only be resolved using external knowledge. In contrast, newswire is deliberately written to contain few coreferences, and those core"
N15-1117,W09-1119,0,0.0602804,"ed on an earlier work of Kummerfeld et al. (2011), which assumes that every maximal projection of a noun or a pronoun is a mention and uses rules to weed out spurious mentions. Instead of using such a rule-based mention detector, our system detects mentions via sequence labeling, as detecting mentions is essentially a problem of detecting start and stop points in spans of text. We solve this sequence tagging problem using the mallet (McCallum, 2002) implementation of conditional random fields (Lafferty et al., 2001). Since our data contain nested mentions, the sequence labels are bio markers (Ratinov and Roth, 2009). The features we use, which are similar to those used in Kummerfeld et al. (2011), are: muc System Train P R F1 Surface Final OntoN OntoN 47.22 50.79 27.97 30.77 35.13 38.32 Surface Final QB QB 60.44 60.21 31.31 33.41 41.2 42.35  Table 3: The top half of the table represents Berkeley models trained on OntoNotes 4.0 data, while the bottom half shows models trained on quiz bowl data. The muc F1 -score of the Berkeley system on OntoNotes text is 66.4, which when compared to these results prove that quiz bowl coreference is significantly different than OntoNotes coreference.     the token it"
N15-1117,D12-1113,0,0.0130123,"proved data-driven systems for coreference resolution that finally outperformed both the Stanford system (Lee et al., 2011) and the ims system (Bj¨orkelund and Farkas, 2012), the latter of which was the best available publiclyavailable English coreference system at the time. The recently-released Berkeley coreference system (Durrett and Klein, 2013) is especially striking: it performs well with only a sparse set of carefully-chosen features. Semantic knowledge sources—especially WordNet (Miller, 1995) and Wikipedia—have been used in coreference engines (Ponzetto and Strube, 2006). A system by Ratinov and Roth (2012) demonstrates good performance by using Wikipedia knowledge to strengthen a multi-pass rule based system. In a more recent work, Durrett and Klein (2014) outperform previous systems by building a joint model that matches mentions to Wikipedia entities while doing named entity resolution and coreference resolution simultaneously. We take a different approach by approximating semantic and world knowledge through our word embedding features. Our simple classifier yields a bi1116 nary decision for each mention pair, a method that had been very popular before the last five years (Soon et al., 2001;"
N15-1117,J01-4004,0,0.143123,"ov and Roth (2012) demonstrates good performance by using Wikipedia knowledge to strengthen a multi-pass rule based system. In a more recent work, Durrett and Klein (2014) outperform previous systems by building a joint model that matches mentions to Wikipedia entities while doing named entity resolution and coreference resolution simultaneously. We take a different approach by approximating semantic and world knowledge through our word embedding features. Our simple classifier yields a bi1116 nary decision for each mention pair, a method that had been very popular before the last five years (Soon et al., 2001; Bengtson and Roth, 2008; Stoyanov et al., 2010). Recently, better results have been obtained with mention-ranking systems (Luo et al., 2004; Haghighi and Klein, 2010; Durrett and Klein, 2013; Bj¨orkelund and Kuhn, 2014). However, on quiz bowl data, our experiments show that binary classifiers can outperform mention-ranking approaches. 7 Embracing Harder Coreference This paper introduces a new, naturally-occuring coreference dataset that is easy to annotate but difficult for computers to solve. We show that active learning allows us to create a dataset that is rich in different types of coref"
N15-1117,P10-2029,0,0.018002,"ance by using Wikipedia knowledge to strengthen a multi-pass rule based system. In a more recent work, Durrett and Klein (2014) outperform previous systems by building a joint model that matches mentions to Wikipedia entities while doing named entity resolution and coreference resolution simultaneously. We take a different approach by approximating semantic and world knowledge through our word embedding features. Our simple classifier yields a bi1116 nary decision for each mention pair, a method that had been very popular before the last five years (Soon et al., 2001; Bengtson and Roth, 2008; Stoyanov et al., 2010). Recently, better results have been obtained with mention-ranking systems (Luo et al., 2004; Haghighi and Klein, 2010; Durrett and Klein, 2013; Bj¨orkelund and Kuhn, 2014). However, on quiz bowl data, our experiments show that binary classifiers can outperform mention-ranking approaches. 7 Embracing Harder Coreference This paper introduces a new, naturally-occuring coreference dataset that is easy to annotate but difficult for computers to solve. We show that active learning allows us to create a dataset that is rich in different types of coreference. We develop an end-to-end coreference syst"
N15-1117,uryupina-2006-coreference,0,0.0707799,"Missing"
N15-1117,M95-1005,0,0.528265,"ar to Dostoevsky than Hemingway. Figure 5 shows that our logistic regression model (lr) outperforms the Berkeley system on numerous metrics when trained and evaluated on the quiz bowl dataset. We use precision, recall, and F1 , metrics applied to muc, bcub, and ceafe measures used for comparing coreference systems.13 We find that our lr model outperforms Berkeley by a wide margin when both are trained on the mentions found by our mention detector (crf). For four metrics, the crf mentions actually improve over training on the gold mentions. Why does the lr model outperform Berkeley 13 The muc (Vilain et al., 1995) score is the minimum number of links between mentions to be inserted or deleted when mapping the output to a gold standard key set. bcub (Bagga and Baldwin, 1998) computes the precision and recall for all mentions separately and then combines them to get the final precision and recall of the output. ceafe (Luo, 2005) is an improvement on bcub and does not use entities multiple times to compute scores. Coreference LR CRF Mentions Gold Mentions 75 50 25 0 75 50 25 0 75 50 25 0 BCUB CEAFE Score Berkeley Mentions QB Final (Berkeley trained on QB) MUC F1 P R F1 P R F1 P R Figure 5: All models are"
N15-1117,W12-4502,0,\N,Missing
N15-1117,N12-1055,0,\N,Missing
N15-1117,Q14-1037,0,\N,Missing
N16-1180,P13-1035,0,0.618432,"Missing"
N16-1180,P14-1035,0,0.635859,"and other topic model baselines in two crowdsourced evaluations described in Section 4. In Section 5 we show qualitative results and make connections to existing literary scholarship. 2 A Dataset of Character Interactions Our dataset consists of 1,383 fictional works pulled from Project Gutenberg and other Internet sources. Project Gutenberg has a limited selection (outside of science fiction) of mostly classic literature, so we add more contemporary novels from various genres such as mystery, romance, and fantasy to our dataset. To identify character mentions, we run the BookNLP pipeline of Bamman et al. (2014), which includes character name clustering, quoted speaker identification, and coreference resolution.1 For ev1 While this pipeline works reasonably well, it is unreliable for first-person narratives; we leave the necessary improvements 1535 ery detected character mention, we define a span as beginning 100 tokens before the mention and ending 100 tokens after the mention. We do not use sentence or paragraph boundaries because they vary considerably depending on the author (e.g., William Faulkner routinely wrote single sentences longer than many of Hemingway’s paragraphs). All spans in our data"
N16-1180,P08-1090,0,0.0134561,"ric, others are more events-based, depicting actions rather than feelings; Figure 6: Clusters from PCA visualizations of the RMN’s learned book (left) and character (right) embeddings. We see a cluster of books about war and violence (many of which are authored by Tom Clancy) as well as a cluster of lead female characters from primarily romance novels. These visualizations show that the RMN can recover useful static representations of characters and books in addition to the dynamic relationship trajectories. such descriptors have been the focus of much previous work (Schank and Abelson, 1977; Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Orr et al., 2014). Our model is more closely related to the plot units framework (Lehnert, 1981; Goyal et al., 2013), which annotates events with emotional states. The RMN builds on deep recurrent autoencoders such as the hierarchical LSTM autoencoder of Li et al. (2015); however, it is more efficient because of the span-level vector averaging. It is also similar to recent neural topic model architectures (Cao et al., 2015; Das et al., 2015), although these models are limited to static document representations. We hope to apply the RMN to nonfictional datasets as"
N16-1180,P09-1068,0,0.015139,"ased, depicting actions rather than feelings; Figure 6: Clusters from PCA visualizations of the RMN’s learned book (left) and character (right) embeddings. We see a cluster of books about war and violence (many of which are authored by Tom Clancy) as well as a cluster of lead female characters from primarily romance novels. These visualizations show that the RMN can recover useful static representations of characters and books in addition to the dynamic relationship trajectories. such descriptors have been the focus of much previous work (Schank and Abelson, 1977; Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Orr et al., 2014). Our model is more closely related to the plot units framework (Lehnert, 1981; Goyal et al., 2013), which annotates events with emotional states. The RMN builds on deep recurrent autoencoders such as the hierarchical LSTM autoencoder of Li et al. (2015); however, it is more efficient because of the span-level vector averaging. It is also similar to recent neural topic model architectures (Cao et al., 2015; Das et al., 2015), although these models are limited to static document representations. We hope to apply the RMN to nonfictional datasets as well; in this vein, Iyyer et"
N16-1180,P15-1077,0,0.0237195,"relationship trajectories. such descriptors have been the focus of much previous work (Schank and Abelson, 1977; Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Orr et al., 2014). Our model is more closely related to the plot units framework (Lehnert, 1981; Goyal et al., 2013), which annotates events with emotional states. The RMN builds on deep recurrent autoencoders such as the hierarchical LSTM autoencoder of Li et al. (2015); however, it is more efficient because of the span-level vector averaging. It is also similar to recent neural topic model architectures (Cao et al., 2015; Das et al., 2015), although these models are limited to static document representations. We hope to apply the RMN to nonfictional datasets as well; in this vein, Iyyer et al. (2014) apply a neural network to sentences from nonfiction political books for ideology prediction. More generally, topic models and related generative models are a central tool for understanding large corpora from science (Talley et al., 2011) to politics (Nguyen et al., 2014). We show representation learning models like RMN can be just as interpretable as LDA-based models. Other applications for which researchers have prioritized interp"
N16-1180,E12-1065,0,0.296948,"ships. Because these models lack temporal components (the focus of our task), we compare instead against the HTMM of Gruber et al. (2007). Closest to our own work is the supervised structured prediction problem of Chaturvedi et al. (2016), in which features are designed to predict dynamic sequences of positive and negative interactions between two characters in plot summaries. Other research in this area includes social network construction from novels (Elson et al., 2010; Srivastava et al., 2016) and film (Krishnan and Eisenstein, 2015), as well as attempts to summarize and generate stories (Elsner, 2012). While some of the relationship descriptors learned by our model are character-centric, others are more events-based, depicting actions rather than feelings; Figure 6: Clusters from PCA visualizations of the RMN’s learned book (left) and character (right) embeddings. We see a cluster of books about war and violence (many of which are authored by Tom Clancy) as well as a cluster of lead female characters from primarily romance novels. These visualizations show that the RMN can recover useful static representations of characters and books in addition to the dynamic relationship trajectories. su"
N16-1180,P10-1015,0,0.202155,"summaries (Bamman et al., 2013). The NUBBI model of Chang et al. (2009a) learns topics that statically describe characters and their relationships. Because these models lack temporal components (the focus of our task), we compare instead against the HTMM of Gruber et al. (2007). Closest to our own work is the supervised structured prediction problem of Chaturvedi et al. (2016), in which features are designed to predict dynamic sequences of positive and negative interactions between two characters in plot summaries. Other research in this area includes social network construction from novels (Elson et al., 2010; Srivastava et al., 2016) and film (Krishnan and Eisenstein, 2015), as well as attempts to summarize and generate stories (Elsner, 2012). While some of the relationship descriptors learned by our model are character-centric, others are more events-based, depicting actions rather than feelings; Figure 6: Clusters from PCA visualizations of the RMN’s learned book (left) and character (right) embeddings. We see a cluster of books about war and violence (many of which are authored by Tom Clancy) as well as a cluster of lead female characters from primarily romance novels. These visualizations sho"
N16-1180,P15-1144,0,0.0178133,"; in this vein, Iyyer et al. (2014) apply a neural network to sentences from nonfiction political books for ideology prediction. More generally, topic models and related generative models are a central tool for understanding large corpora from science (Talley et al., 2011) to politics (Nguyen et al., 2014). We show representation learning models like RMN can be just as interpretable as LDA-based models. Other applications for which researchers have prioritized interpretable vector representations include text-to-vision mappings (Lazaridou et al., 2014) and word embeddings (Fyshe et al., 2015; Faruqui et al., 2015). 1542 7 Conclusion We formalize the task of unsupervised relationship modeling, which involves learning a set of relationship descriptors as well as a trajectory over these descriptors for each relationship in an input dataset. We present the RMN, a novel neural network architecture for this task that generates more interpretable descriptors and trajectories than topic model baselines. Finally, we show that the output of our model can lead to interesting insights when combined with annotations in an existing dataset. Acknowledgments We thank Jonathan Chang and Amit Gruber for providing baseli"
N16-1180,D15-1208,0,0.0777085,"ts even when we reduce the number of topics. 1541 reasonably. Finally, returning to the “extra” meaning of meals discussed in Section 1, food occurs slightly more frequently in positive relationships. 6 Related Work There are two major areas upon which our work builds: computational literary analysis and deep neural networks for natural language processing. Most previous work in computational literary analysis has focused either on characters or events. In the former category, graphical models and classifiers have been proposed for learning character personas from novels (Bamman et al., 2014; Flekova and Gurevych, 2015) and film summaries (Bamman et al., 2013). The NUBBI model of Chang et al. (2009a) learns topics that statically describe characters and their relationships. Because these models lack temporal components (the focus of our task), we compare instead against the HTMM of Gruber et al. (2007). Closest to our own work is the supervised structured prediction problem of Chaturvedi et al. (2016), in which features are designed to predict dynamic sequences of positive and negative interactions between two characters in plot summaries. Other research in this area includes social network construction from"
N16-1180,N15-1004,0,0.0148287,"nal datasets as well; in this vein, Iyyer et al. (2014) apply a neural network to sentences from nonfiction political books for ideology prediction. More generally, topic models and related generative models are a central tool for understanding large corpora from science (Talley et al., 2011) to politics (Nguyen et al., 2014). We show representation learning models like RMN can be just as interpretable as LDA-based models. Other applications for which researchers have prioritized interpretable vector representations include text-to-vision mappings (Lazaridou et al., 2014) and word embeddings (Fyshe et al., 2015; Faruqui et al., 2015). 1542 7 Conclusion We formalize the task of unsupervised relationship modeling, which involves learning a set of relationship descriptors as well as a trajectory over these descriptors for each relationship in an input dataset. We present the RMN, a novel neural network architecture for this task that generates more interpretable descriptors and trajectories than topic model baselines. Finally, we show that the output of our model can lead to interesting insights when combined with annotations in an existing dataset. Acknowledgments We thank Jonathan Chang and Amit Grub"
N16-1180,P14-1105,1,0.80738,"ky, 2009; Orr et al., 2014). Our model is more closely related to the plot units framework (Lehnert, 1981; Goyal et al., 2013), which annotates events with emotional states. The RMN builds on deep recurrent autoencoders such as the hierarchical LSTM autoencoder of Li et al. (2015); however, it is more efficient because of the span-level vector averaging. It is also similar to recent neural topic model architectures (Cao et al., 2015; Das et al., 2015), although these models are limited to static document representations. We hope to apply the RMN to nonfictional datasets as well; in this vein, Iyyer et al. (2014) apply a neural network to sentences from nonfiction political books for ideology prediction. More generally, topic models and related generative models are a central tool for understanding large corpora from science (Talley et al., 2011) to politics (Nguyen et al., 2014). We show representation learning models like RMN can be just as interpretable as LDA-based models. Other applications for which researchers have prioritized interpretable vector representations include text-to-vision mappings (Lazaridou et al., 2014) and word embeddings (Fyshe et al., 2015; Faruqui et al., 2015). 1542 7 Concl"
N16-1180,P15-1162,1,0.0293637,"Missing"
N16-1180,N15-1185,0,0.0223603,"ang et al. (2009a) learns topics that statically describe characters and their relationships. Because these models lack temporal components (the focus of our task), we compare instead against the HTMM of Gruber et al. (2007). Closest to our own work is the supervised structured prediction problem of Chaturvedi et al. (2016), in which features are designed to predict dynamic sequences of positive and negative interactions between two characters in plot summaries. Other research in this area includes social network construction from novels (Elson et al., 2010; Srivastava et al., 2016) and film (Krishnan and Eisenstein, 2015), as well as attempts to summarize and generate stories (Elsner, 2012). While some of the relationship descriptors learned by our model are character-centric, others are more events-based, depicting actions rather than feelings; Figure 6: Clusters from PCA visualizations of the RMN’s learned book (left) and character (right) embeddings. We see a cluster of books about war and violence (many of which are authored by Tom Clancy) as well as a cluster of lead female characters from primarily romance novels. These visualizations show that the RMN can recover useful static representations of charact"
N16-1180,P14-1132,0,0.0147958,"ations. We hope to apply the RMN to nonfictional datasets as well; in this vein, Iyyer et al. (2014) apply a neural network to sentences from nonfiction political books for ideology prediction. More generally, topic models and related generative models are a central tool for understanding large corpora from science (Talley et al., 2011) to politics (Nguyen et al., 2014). We show representation learning models like RMN can be just as interpretable as LDA-based models. Other applications for which researchers have prioritized interpretable vector representations include text-to-vision mappings (Lazaridou et al., 2014) and word embeddings (Fyshe et al., 2015; Faruqui et al., 2015). 1542 7 Conclusion We formalize the task of unsupervised relationship modeling, which involves learning a set of relationship descriptors as well as a trajectory over these descriptors for each relationship in an input dataset. We present the RMN, a novel neural network architecture for this task that generates more interpretable descriptors and trajectories than topic model baselines. Finally, we show that the output of our model can lead to interesting insights when combined with annotations in an existing dataset. Acknowledgmen"
N16-1180,P15-1107,0,0.00991822,"characters from primarily romance novels. These visualizations show that the RMN can recover useful static representations of characters and books in addition to the dynamic relationship trajectories. such descriptors have been the focus of much previous work (Schank and Abelson, 1977; Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Orr et al., 2014). Our model is more closely related to the plot units framework (Lehnert, 1981; Goyal et al., 2013), which annotates events with emotional states. The RMN builds on deep recurrent autoencoders such as the hierarchical LSTM autoencoder of Li et al. (2015); however, it is more efficient because of the span-level vector averaging. It is also similar to recent neural topic model architectures (Cao et al., 2015; Das et al., 2015), although these models are limited to static document representations. We hope to apply the RMN to nonfictional datasets as well; in this vein, Iyyer et al. (2014) apply a neural network to sentences from nonfiction political books for ideology prediction. More generally, topic models and related generative models are a central tool for understanding large corpora from science (Talley et al., 2011) to politics (Nguyen et"
N16-1180,D14-1162,0,0.127245,"Missing"
N16-1180,D15-1088,0,0.0242198,"of the RMN is a set of relationship descriptors (topics) and—for each relationship in the dataset—a trajectory, or a sequence of probability distributions over these descriptors (document-topic assignments). However, the RMN uses recent advances in deep learning to achieve better control over descriptor coherence and trajectory smoothness (Section 4). 3.1 Formalizing the Problem Assume we have two characters c1 and c2 in book b. We define Sc1 ,c2 as a sequence of token spans where each span st ∈ Sc1 ,c2 is itself a set of tokens to character name clustering, which are further expanded upon in Vala et al. (2015), for future work. 2 Code and span data available at http://github.com/ miyyer/rmn. Each input to the RMN is a tuple that contains identifiers for a book and two characters, as well as the spans corresponding to their relationship: (b, c1 , c2 , Sc1 ,c2 ). Given one such input, our objective is to reconstruct Sc1 ,c2 using a linear combination of relationship descriptors from R as shown in Figure 2; we now describe this process formally. r t = RT dt : reconstruction of input span R: descriptor matrix dt dt = ↵ · softmax(Wd · [ht ; dt 1: previous state (1 ↵) · dt 1 ])+ 1: distribution over desc"
N16-1180,Q14-1017,0,\N,Missing
N18-1170,P05-1074,0,0.748798,"inal test sets (§5). Together these results not only establish the first general purpose syntactically controlled paraphrase approach, but also suggest that this general paradigm could be used for controlling many other aspects of the target text. 2 Collecting labeled paraphrase pairs In this section, we describe a general purpose process for gathering and labeling training data for controlled paraphrase generation. 2.1 Paraphrase data via backtranslation Inducing paraphrases from bilingual data has long been an effective method to overcome data limitations. In particular, bilingual pivoting (Bannard and Callison-Burch, 2005) finds quality para1 Code, labeled data, and pretrained models available at https://github.com/miyyer/scpn. phrases by pivoting through a different language. Mallinson et al. (2017) show that neural machine translation (NMT) systems outperform phrasebased MT on several paraphrase evaluation metrics. In this paper, we use the PARA NMT-50M corpus from Wieting and Gimpel (2017). This corpus consists of over 50 million paraphrases obtained by backtranslating the Czech side of the CzEng (Bojar et al., 2016) parallel corpus. The pretrained Czech-English model used for translation came from the Nemat"
N18-1170,N03-1003,0,0.138616,"Missing"
N18-1170,D08-1021,0,0.0149106,"Missing"
N18-1170,2005.mtsummit-ebmt.3,0,0.272082,"Missing"
N18-1170,D17-1091,0,0.091119,"Missing"
N18-1170,W17-5401,0,0.101857,"Missing"
N18-1170,D12-1139,0,0.0195225,"odel is trained to produce s2 . To overcome learned biases of the NMT system, we also include reversed pairs hs2 , s1 i during training. 2.2.1 Syntactic templates To provide syntactic control, we linearize the bracketed parse structure without leaf nodes (i.e., tokens). For example, the corresponding linearized parse 2 Syntactic diversity was measured by the entropy of the top two levels of parse trees in the corpora. 3 Similar automated filtering could be used to produce data for many other transformations, such as tense changes, pointof-view shifts, and even stylometric pattern differences (Feng et al., 2012). This is an interesting area for future work. 4 Because of the large dataset size, we use the faster but less accurate shift-reduce parser written by John Bauer. 1876 tree for the sentence “She drove home.” is (S(NP(PRP))(VP(VBD)(NP(NN)))(.)). A system that requires a complete linearized target parse at test-time is unwieldy; how do we go about choosing the target parse? To simplify test-time usage, we relax the target syntactic form to a parse template, which we define as the top two levels of the linearized parse tree (the level immediately below the root along with the root); the prior exa"
N18-1170,W17-4912,0,0.0524541,"Missing"
N18-1170,D17-1215,0,0.169957,"nguage processing datasets often suffer from a dearth of linguistic variation, which can hurt the generalization of models trained on them. Recent work has shown it is possible to easily “break” many learned models by evaluating them on adversarial examples (Goodfellow et al., 2015), which are generated by manually introducing lexical, pragmatic, and syntactic variation not seen in the training set (Ettinger et al., 2017). Robustness to such adversarial examples can potentially be improved by augmenting the training data, as shown by prior work that introduces rulebased lexical substitutions (Jia and Liang, 2017; FAuthors contributed equally. The man is standing in the water at the base of a waterfall Liang et al., 2017). However, more complex transformations, such as generating syntactically adversarial examples, remain an open challenge, as input semantics must be preserved in the face of potentially substantial structural modifications. In this paper, we introduce a new approach for learning to do syntactically controlled paraphrase generation: given a sentence and a target syntactic form (e.g., a constituency parse), a system must produce a paraphrase of the sentence whose syntax conforms to the"
N18-1170,N10-1017,0,0.045833,"22.3 18.3 17.7 Table 1: A crowdsourced paraphrase evaluation on a three-point scale (0 = no paraphrase, 1 = ungrammatical paraphrase, 2 = grammatical paraphrase) shows both that NMT- BT and SCPN produce mostly grammatical paraphrases. Feeding parse templates to SCPN instead of full parses does not impact its quality. 4.1 Intrinsic Experiments w/ full parses w/ templates NMT- BT SCPN 2 Paraphrase quality & grammaticality To measure paraphrase quality and grammaticality, we perform a crowdsourced experiment in which workers are asked to rate a paraphrase pair hs, gi on the three-point scale of Kok and Brockett (2010), where s is the source sentence and g is the generated sentence. A 0 on this scale indicates no paraphrase relationship, while 1 means that g is an ungrammatical paraphrase of s and 2 means that g is a grammatical paraphrase of s. We select 100 paraphrase pairs from the development set of our PARA NMT-50M split (after the postprocessing steps detailed in Section 3.3) and have three workers rate each pair.7 To focus the evaluation on the effect of syntactic manipulation on quality, we minimum paraphrastic similarity to 0.7. 7 We use the Crowdflower platform for our experiments. 1878 only selec"
N18-1170,P16-1094,0,0.0138315,"is handled by our decoder language model. Recent efforts involve neural methods. Iyyer et al. (2014) generate paraphrases with dependency tree recursive autoencoders by randomly selecting parse trees at test time. Li et al. (2017) generate paraphrases using deep reinforcement learning. Gupta et al. (2017) use variational autoencoders to generate multiple paraphrases. These methods differ from our approach in that none offer fine-grained control over the syntactic form of the paraphrase. control the level of formality while Sennrich et al. (2016) control the level of politeness. For dialogue, Li et al. (2016a) affect the output using speaker identity, while Wang et al. (2017) develop models to influence topic and style of the output. Shen et al. (2017) perform style transfer on non-parallel texts, while Guu et al. (2017) generate novel sentences from prototypes; again, these methods are not necessarily seeking to generate meaning-preserving paraphrases, merely transformed sentences that have an altered style. 7.2 We propose SCPN, an encoder-decoder model for syntactically controlled paraphrase generation, and show that it is an effective way of generating adversarial examples. Using a parser, we"
N18-1170,J10-3003,0,0.0423198,"ency of paraphrase output. Callison-Burch (2008) constrains paraphrases to be the same syntactic type as the input, though he was focused on phrase-level, not sentential, paraphrasing. Pang et al. (2003) learn finite-state automata from translation pairs that generate syntactic paraphrases, though this requires multiple translations into the same language and cannot be used to generate paraphrases outside this dataset. Shen et al. (2006) extend this to deeper syntactic analysis. All of these approaches use syntax to 7 Related Work Paraphrase generation (Androutsopoulos and Malakasiotis, 2010; Madnani and Dorr, 2010) has been tackled using many different methods, including those based on hand-crafted rules (McKeown, 1983), synonym substitution (Bolshakov and Gelbukh, 2004), machine translation (Quirk et al., 2004), and, most recently, deep learning (Prakash et al., 2016; Mallinson et al., 2017; Dong et al., 2017). Our syntactically controlled setting also relates to controlled language generation tasks in which one desires to generate or rewrite a sentence with particular characteristics. We review related work in both 13 A configuration without the copy mechanism copies input syntax even more, with a 47."
N18-1170,E17-1083,0,0.0546964,"d for controlling many other aspects of the target text. 2 Collecting labeled paraphrase pairs In this section, we describe a general purpose process for gathering and labeling training data for controlled paraphrase generation. 2.1 Paraphrase data via backtranslation Inducing paraphrases from bilingual data has long been an effective method to overcome data limitations. In particular, bilingual pivoting (Bannard and Callison-Burch, 2005) finds quality para1 Code, labeled data, and pretrained models available at https://github.com/miyyer/scpn. phrases by pivoting through a different language. Mallinson et al. (2017) show that neural machine translation (NMT) systems outperform phrasebased MT on several paraphrase evaluation metrics. In this paper, we use the PARA NMT-50M corpus from Wieting and Gimpel (2017). This corpus consists of over 50 million paraphrases obtained by backtranslating the Czech side of the CzEng (Bojar et al., 2016) parallel corpus. The pretrained Czech-English model used for translation came from the Nematus NMT system (Sennrich et al., 2017). The training data of this system includes four sources: Common Crawl, CzEng 1.6, Europarl, and News Commentary. The CzEng corpus is the larges"
N18-1170,S14-2001,0,0.00800909,"he problem, assume a pretrained model for some downstream task produces prediction yx given test-time instance x. An adversarial example x0 can be formed by making label-preserving modifications to x such that yx 6= yx0 . Our results demonstrate that controlled paraphrase generation with appropriate template selection produces far more valid adversarial examples than backtranslation on sentiment analysis and entailment tasks. 5.1 Experimental setup We evaluate our syntactically adversarial paraphrases on the Stanford Sentiment Treebank (Socher et al., 2013, SST) and SICK entailment detection (Marelli et al., 2014). While both are relatively small datasets, we select them because they offer different experimental conditions: SST contains complicated sentences with high syntactic variance, while SICK almost exclusively consists of short, simple sentences. As a baseline, we compare the ten most probable beams from NMT- BT to controlled paraphrases generated by SCPN using ten templates randomly sampled from the template set described in Section 3.3.9 We also need pretrained models 9 We also experimented with the diverse beam search modification proposed by Li et al. (2016b) for NMT- BT but found that it dr"
N18-1170,P06-2096,0,0.106453,"Missing"
N18-1170,J83-1001,0,0.445484,"rial examples, remain an open challenge, as input semantics must be preserved in the face of potentially substantial structural modifications. In this paper, we introduce a new approach for learning to do syntactically controlled paraphrase generation: given a sentence and a target syntactic form (e.g., a constituency parse), a system must produce a paraphrase of the sentence whose syntax conforms to the target. General purpose syntactically controlled paraphrase generation is a challenging task. Approaches that rely on handcrafted rules and grammars, such as the question generation system of McKeown (1983), support only a limited number of syntactic targets. We introduce the first learning approach for this problem, building on the generality of neural encoder-decoder models to support a wide range of transformations. In doing 1875 Proceedings of NAACL-HLT 2018, pages 1875–1885 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics so, we face two new challenges: (1) obtaining a large amount of paraphrase pairs for training, and (2) defining syntactic transformations with which to label these pairs. Since no large-scale dataset of sentential paraphrases exist"
N18-1170,N16-3013,0,0.122415,"Missing"
N18-1170,D17-1299,0,0.0213328,"Missing"
N18-1170,D13-1170,0,0.0143659,"hrases for adversarial example generation. To formalize the problem, assume a pretrained model for some downstream task produces prediction yx given test-time instance x. An adversarial example x0 can be formed by making label-preserving modifications to x such that yx 6= yx0 . Our results demonstrate that controlled paraphrase generation with appropriate template selection produces far more valid adversarial examples than backtranslation on sentiment analysis and entailment tasks. 5.1 Experimental setup We evaluate our syntactically adversarial paraphrases on the Stanford Sentiment Treebank (Socher et al., 2013, SST) and SICK entailment detection (Marelli et al., 2014). While both are relatively small datasets, we select them because they offer different experimental conditions: SST contains complicated sentences with high syntactic variance, while SICK almost exclusively consists of short, simple sentences. As a baseline, we compare the ten most probable beams from NMT- BT to controlled paraphrases generated by SCPN using ten templates randomly sampled from the template set described in Section 3.3.9 We also need pretrained models 9 We also experimented with the diverse beam search modification pro"
N18-1170,N03-1024,0,0.454247,"Missing"
N18-1170,P17-3007,0,0.0644294,"Missing"
N18-1170,D14-1162,0,0.12426,"k and ignore all phrase-level labels (because our paraphrase models are trained on only sentences). Table 4 shows that for both datasets, SCPN breaks many more examples than NMT- BT. Moreover, as shown in Table 5, NMT- BT’s paraphrases differ from the original example mainly by lexical substitutions, while SCPN often produces dramatically different syntactic structures. 5.3 Are the adversarial examples valid? We have shown that we can break pretrained models with controlled paraphrases, but are these paraon the three-point scale. 10 We initialize both models using pretrained GloVe embeddings (Pennington et al., 2014) and set the LSTM hidden dimensionality to 300. 11 Since the SICK development dataset is tiny, we additionally generate adversarial examples on its test set. phrases actually valid adversarial examples? After all, it is possible that the syntactic modifications cause informative clauses or words (e.g., negations) to go missing. To measure the validity of our adversarial examples, we turn again to crowdsourced experiments. We ask workers to choose the appropriate label for a given sentence or sentence pair (e.g., positive or negative for SST), and then we compare the worker’s judgment to the or"
N18-1170,P15-1150,0,0.0186117,"ime comes . ” you said . can i get a good burglar when the time comes ? look at the time the thief comes . Table 3: Syntactically controlled paraphrases generated by SCPN for two examples from the PARA NMT-50M development set. For each input sentence, we show the outputs of four different templates; the fourth template is a failure case (highlighted in green) exhibiting semantic divergence and/or ungrammaticality, which occurs when the target template is unsuited for the input. for which to generate adversarial examples; we use the bidirectional LSTM baseline for both SST and SICK outlined in Tai et al. (2015) since it is a relatively simple architecture that has proven to work well for a variety of problems.10 Since the SICK task involves characterizing the relationship between two sentences, for simplicity we only generate adversarial examples for the first sentence and keep the second sentence fixed to the ground truth. 5.2 Breaking pretrained models For each dataset, we generate paraphrases for held-out examples and then run a pretrained model over them.11 We consider a development example x broken if the original prediction yx is correct, but the prediction yx0 for at least one paraphrase x0 i"
N18-1170,C16-1275,0,0.301174,"Missing"
N18-1170,D17-1228,0,0.0560075,"Missing"
N18-1170,W04-3219,0,0.398798,"paraphrase generation, noting two primary families: template-based and translationbased. The first family includes approaches that use hand-crafted rules (McKeown, 1983), thesaurus-based substitution (Bolshakov and Gelbukh, 2004; Zhang and LeCun, 2015), lattice matching (Barzilay and Lee, 2003), and templatebased “shake & bake” paraphrasing (Carl et al., 2005). These methods often yield grammatical outputs but they can be limited in diversity. The second family includes methods that rewrite the input using methods based on parallel text (Bannard and Callison-Burch, 2005), machine translation (Quirk et al., 2004; Napoles et al., 2016; Suzuki et al., 2017), or related statistical techniques (Zhao et al., 2009). Of particular relevance to our work are methods that incorporate syntax to improve fluency of paraphrase output. Callison-Burch (2008) constrains paraphrases to be the same syntactic type as the input, though he was focused on phrase-level, not sentential, paraphrasing. Pang et al. (2003) learn finite-state automata from translation pairs that generate syntactic paraphrases, though this requires multiple translations into the same language and cannot be used to generate paraphrases outside this"
N18-1170,P17-1099,0,0.0157426,"either parse templates or full parses depending on their desired level of control. 3 Syntactically Controlled Paraphrase Networks The SCPN encoder-decoder architecture is built from standard neural modules, as we describe in this section. 3.1 Neural controlled paraphrase generation Given a sentential paraphrase pair hs1 , s2 i and a corresponding target syntax tree p2 for s2 , we encode s1 using a bidirectional LSTM (Hochreiter and Schmidhuber, 1997), and our decoder is a two-layer LSTM augmented with soft attention over the encoded states (Bahdanau et al., 2014) as well as a copy mechanism (See et al., 2017). Following existing work in NMT (Sennrich et al., 2015), we preprocess s1 and s2 into subword units using byte pair encoding, and we perform decoding using beam search. For all attention computations, we use a bilinear product with a learned parameter matrix W: given vectors u and v, we score them by uT Wv. We incorporate the target syntax p2 into the generation process by modifying the inputs to the decoder. In particular, a standard decoder LSTM receives two inputs at every time step: (1) the embedding wt−1 of the ground-truth previous word in s2 , and (2) an attention-weighted average at o"
N18-1170,D17-1026,1,0.789198,"ed number of syntactic targets. We introduce the first learning approach for this problem, building on the generality of neural encoder-decoder models to support a wide range of transformations. In doing 1875 Proceedings of NAACL-HLT 2018, pages 1875–1885 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics so, we face two new challenges: (1) obtaining a large amount of paraphrase pairs for training, and (2) defining syntactic transformations with which to label these pairs. Since no large-scale dataset of sentential paraphrases exists publicly, we follow Wieting et al. (2017) and automatically generate millions of paraphrase pairs using neural backtranslation. Backtranslation naturally injects linguistic variation between the original sentence and its backtranslated counterpart. By running the process at a very large scale and testing for the specific variations we want to produce, we can gather ample input-output pairs for a wide range of phenomena. Our focus is on syntactic transformations, which we define using templates derived from linearized constituency parses (§2). Given such parallel data, we can easily train an encoder-decoder model that takes a sentence"
N18-1170,E17-3017,0,0.0397283,"Missing"
N18-1170,D11-1038,0,0.00804775,"ation than existing uncontrolled paraphrase generation systems, instead preferring purely syntactic modifications. It is capable of generating adversarial examples that fool pretrained NLP models. Furthermore, by training on such examples, we increase the robustness of these models to syntactic variation. Controlled language generation There is growing interest in generating language with the ability to influence the topic, style, or other properties of the output. Most related to our methods are those based on syntactic transformations, like the tree-to-tree sentence simplification method of Woodsend and Lapata (2011) based on quasi-synchronous grammar (Smith and Eisner, 2006). Our method is more general since we do not require a grammar and there are only soft constraints. Perhaps the closest to the proposed method is the conditioned recurrent language model of Ficler and Goldberg (2017), which produces language with user-selected properties such as sentence length and formality but is incapable of generating paraphrases. For machine translation output, Niu et al. (2017) 8 Conclusion Acknowledgments We thank the reviewers for their insightful comments. We would also like to thank Mark Yatskar for many use"
N18-1170,N16-1005,0,0.00722262,"generator to produce viable full parses. improve grammaticality, which is handled by our decoder language model. Recent efforts involve neural methods. Iyyer et al. (2014) generate paraphrases with dependency tree recursive autoencoders by randomly selecting parse trees at test time. Li et al. (2017) generate paraphrases using deep reinforcement learning. Gupta et al. (2017) use variational autoencoders to generate multiple paraphrases. These methods differ from our approach in that none offer fine-grained control over the syntactic form of the paraphrase. control the level of formality while Sennrich et al. (2016) control the level of politeness. For dialogue, Li et al. (2016a) affect the output using speaker identity, while Wang et al. (2017) develop models to influence topic and style of the output. Shen et al. (2017) perform style transfer on non-parallel texts, while Guu et al. (2017) generate novel sentences from prototypes; again, these methods are not necessarily seeking to generate meaning-preserving paraphrases, merely transformed sentences that have an altered style. 7.2 We propose SCPN, an encoder-decoder model for syntactically controlled paraphrase generation, and show that it is an effect"
N18-1170,P09-1094,0,0.169719,"Missing"
N18-1170,W06-3104,0,\N,Missing
N18-1170,P14-5010,0,\N,Missing
N18-1202,P17-1080,0,0.874363,"eneralize these approaches to deep contextual representations, which we show work well across a broad range of diverse NLP tasks. 1 http://allennlp.org/elmo Previous work has also shown that different layers of deep biRNNs encode different types of information. For example, introducing multi-task syntactic supervision (e.g., part-of-speech tags) at the lower levels of a deep LSTM can improve overall performance of higher level tasks such as dependency parsing (Hashimoto et al., 2017) or CCG super tagging (Søgaard and Goldberg, 2016). In an RNN-based encoder-decoder machine translation system, Belinkov et al. (2017) showed that the representations learned at the first layer in a 2layer LSTM encoder are better at predicting POS tags then second layer. Finally, the top layer of an LSTM for encoding word context (Melamud et al., 2016) has been shown to learn representations of word sense. We show that similar signals are also induced by the modified language model objective of our ELMo representations, and it can be very beneficial to learn models for downstream tasks that mix these different types of semi-supervision. Dai and Le (2015) and Ramachandran et al. (2017) pretrain encoder-decoder pairs using lan"
N18-1202,Q17-1010,0,0.531036,"eled text, pretrained word vectors (Turian et al., 2010; Mikolov et al., 2013; Pennington et al., 2014) are a standard component of most state-ofthe-art NLP architectures, including for question answering (Liu et al., 2017), textual entailment (Chen et al., 2017) and semantic role labeling (He et al., 2017). However, these approaches for learning word vectors only allow a single contextindependent representation for each word. Previously proposed methods overcome some of the shortcomings of traditional word vectors by either enriching them with subword information (e.g., Wieting et al., 2016; Bojanowski et al., 2017) or learning separate vectors for each word sense (e.g., Neelakantan et al., 2014). Our approach also benefits from subword units through the use of character convolutions, and we seamlessly incorporate multi-sense information into downstream tasks without explicitly training to predict predefined sense classes. Other recent work has also focused on learning context-dependent representations. context2vec (Melamud et al., 2016) uses a bidirectional Long Short Term Memory (LSTM; Hochreiter and Schmidhuber, 1997) to encode the context around a pivot word. Other approaches for learning contextual"
N18-1202,D15-1075,0,0.770513,"Missing"
N18-1202,P17-1152,0,0.665298,"- 6, 2018. 2018 Association for Computational Linguistics those derived from just the top layer of an LSTM. Our trained models and code are publicly available, and we expect that ELMo will provide similar gains for many other NLP problems.1 2 Related work Due to their ability to capture syntactic and semantic information of words from large scale unlabeled text, pretrained word vectors (Turian et al., 2010; Mikolov et al., 2013; Pennington et al., 2014) are a standard component of most state-ofthe-art NLP architectures, including for question answering (Liu et al., 2017), textual entailment (Chen et al., 2017) and semantic role labeling (He et al., 2017). However, these approaches for learning word vectors only allow a single contextindependent representation for each word. Previously proposed methods overcome some of the shortcomings of traditional word vectors by either enriching them with subword information (e.g., Wieting et al., 2016; Bojanowski et al., 2017) or learning separate vectors for each word sense (e.g., Neelakantan et al., 2014). Our approach also benefits from subword units through the use of character convolutions, and we seamlessly incorporate multi-sense information into downstr"
N18-1202,Q16-1026,0,0.779805,"Missing"
N18-1202,W14-4012,0,0.0858828,"Missing"
N18-1202,D16-1245,0,0.0232835,"Missing"
N18-1202,D13-1203,0,0.0178787,"Missing"
N18-1202,D17-1206,0,0.793935,"to plentiful monolingual data, and train our biLM on a corpus with approximately 30 million sentences (Chelba et al., 2014). We also generalize these approaches to deep contextual representations, which we show work well across a broad range of diverse NLP tasks. 1 http://allennlp.org/elmo Previous work has also shown that different layers of deep biRNNs encode different types of information. For example, introducing multi-task syntactic supervision (e.g., part-of-speech tags) at the lower levels of a deep LSTM can improve overall performance of higher level tasks such as dependency parsing (Hashimoto et al., 2017) or CCG super tagging (Søgaard and Goldberg, 2016). In an RNN-based encoder-decoder machine translation system, Belinkov et al. (2017) showed that the representations learned at the first layer in a 2layer LSTM encoder are better at predicting POS tags then second layer. Finally, the top layer of an LSTM for encoding word context (Melamud et al., 2016) has been shown to learn representations of word sense. We show that similar signals are also induced by the modified language model objective of our ELMo representations, and it can be very beneficial to learn models for downstream tasks that mi"
N18-1202,P17-1044,1,0.395137,"Linguistics those derived from just the top layer of an LSTM. Our trained models and code are publicly available, and we expect that ELMo will provide similar gains for many other NLP problems.1 2 Related work Due to their ability to capture syntactic and semantic information of words from large scale unlabeled text, pretrained word vectors (Turian et al., 2010; Mikolov et al., 2013; Pennington et al., 2014) are a standard component of most state-ofthe-art NLP architectures, including for question answering (Liu et al., 2017), textual entailment (Chen et al., 2017) and semantic role labeling (He et al., 2017). However, these approaches for learning word vectors only allow a single contextindependent representation for each word. Previously proposed methods overcome some of the shortcomings of traditional word vectors by either enriching them with subword information (e.g., Wieting et al., 2016; Bojanowski et al., 2017) or learning separate vectors for each word sense (e.g., Neelakantan et al., 2014). Our approach also benefits from subword units through the use of character convolutions, and we seamlessly incorporate multi-sense information into downstream tasks without explicitly training to pred"
N18-1202,P16-1085,0,0.422547,"nd “player”, “game” as nouns) but concentrated in the sportsrelated senses of “play”. In contrast, the bottom two rows show nearest neighbor sentences from the SemCor dataset (see below) using the biLM’s context representation of “play” in the source sentence. In these cases, the biLM is able to disambiguate both the part of speech and word sense in the source sentence. These observations can be quantified using an 2233 resentations have F1 of 69.0 and are better at WSD then the first layer. This is competitive with a state-of-the-art WSD-specific supervised model using hand crafted features (Iacobacci et al., 2016) and a task specific biLSTM that is also trained with auxiliary coarse-grained semantic labels and POS tags (Raganato et al., 2017a). The CoVe biLSTM layers follow a similar pattern to those from the biLM (higher overall performance at the second layer compared to the first); however, our biLM outperforms the CoVe biLSTM, which trails the WordNet first sense baseline. POS tagging To examine whether the biLM captures basic syntax, we used the context representations as input to a linear classifier that predicts POS tags with the Wall Street Journal portion of the Penn Treebank (PTB) (Marcus et"
N18-1202,P16-1101,0,0.700562,"compared to the first); however, our biLM outperforms the CoVe biLSTM, which trails the WordNet first sense baseline. POS tagging To examine whether the biLM captures basic syntax, we used the context representations as input to a linear classifier that predicts POS tags with the Wall Street Journal portion of the Penn Treebank (PTB) (Marcus et al., 1993). As the linear classifier adds only a small amount of model capacity, this is direct test of the biLM’s representations. Similar to WSD, the biLM representations are competitive with carefully tuned, task specific biLSTMs (Ling et al., 2015; Ma and Hovy, 2016). However, unlike WSD, accuracies using the first biLM layer are higher than the top layer, consistent with results from deep biLSTMs in multi-task training (Søgaard and Goldberg, 2016; Hashimoto et al., 2017) and MT (Belinkov et al., 2017). CoVe POS tagging accuracies follow the same pattern as those from the biLM, and just like for WSD, the biLM achieves higher accuracies than the CoVe encoder. Implications for supervised tasks Taken together, these experiments confirm different layers in the biLM represent different types of information and explain why including all biLM layers is important"
N18-1202,J93-2004,0,0.116591,"al., 2016) and a task specific biLSTM that is also trained with auxiliary coarse-grained semantic labels and POS tags (Raganato et al., 2017a). The CoVe biLSTM layers follow a similar pattern to those from the biLM (higher overall performance at the second layer compared to the first); however, our biLM outperforms the CoVe biLSTM, which trails the WordNet first sense baseline. POS tagging To examine whether the biLM captures basic syntax, we used the context representations as input to a linear classifier that predicts POS tags with the Wall Street Journal portion of the Penn Treebank (PTB) (Marcus et al., 1993). As the linear classifier adds only a small amount of model capacity, this is direct test of the biLM’s representations. Similar to WSD, the biLM representations are competitive with carefully tuned, task specific biLSTMs (Ling et al., 2015; Ma and Hovy, 2016). However, unlike WSD, accuracies using the first biLM layer are higher than the top layer, consistent with results from deep biLSTMs in multi-task training (Søgaard and Goldberg, 2016; Hashimoto et al., 2017) and MT (Belinkov et al., 2017). CoVe POS tagging accuracies follow the same pattern as those from the biLM, and just like for WSD"
N18-1202,K16-1006,0,0.86068,"eviously proposed methods overcome some of the shortcomings of traditional word vectors by either enriching them with subword information (e.g., Wieting et al., 2016; Bojanowski et al., 2017) or learning separate vectors for each word sense (e.g., Neelakantan et al., 2014). Our approach also benefits from subword units through the use of character convolutions, and we seamlessly incorporate multi-sense information into downstream tasks without explicitly training to predict predefined sense classes. Other recent work has also focused on learning context-dependent representations. context2vec (Melamud et al., 2016) uses a bidirectional Long Short Term Memory (LSTM; Hochreiter and Schmidhuber, 1997) to encode the context around a pivot word. Other approaches for learning contextual embeddings include the pivot word itself in the representation and are computed with the encoder of either a supervised neural machine translation (MT) system (CoVe; McCann et al., 2017) or an unsupervised language model (Peters et al., 2017). Both of these approaches benefit from large datasets, although the MT approach is limited by the size of parallel corpora. In this paper, we take full advantage of access to plentiful mo"
N18-1202,N16-1030,0,0.746938,"Missing"
N18-1202,D17-1018,1,0.121179,"Missing"
N18-1202,H94-1046,0,0.123123,"2017). To isolate the information encoded by the biLM, the representations are used to directly make predictions for a fine grained word sense disambiguation (WSD) task and a POS tagging task. Using this approach, it is also possible to compare to CoVe, and across each of the individual layers. Word sense disambiguation Given a sentence, we can use the biLM representations to predict the sense of a target word using a simple 1nearest neighbor approach, similar to Melamud et al. (2016). To do so, we first use the biLM to compute representations for all words in SemCor 3.0, our training corpus (Miller et al., 1994), and then take the average representation for each sense. At test time, we again use the biLM to compute representations for a given target word and take the nearest neighbor sense from the training set, falling back to the first sense from WordNet for lemmas not observed during training. Table 5 compares WSD results using the evaluation framework from Raganato et al. (2017b) across the same suite of four test sets in Raganato et al. (2017a). Overall, the biLM top layer rep5.3 What information is captured by the biLM’s representations? Since adding ELMo improves task performance over word vec"
N18-1202,E17-1002,0,0.0183651,"Missing"
N18-1202,D14-1113,0,0.0176931,"nnington et al., 2014) are a standard component of most state-ofthe-art NLP architectures, including for question answering (Liu et al., 2017), textual entailment (Chen et al., 2017) and semantic role labeling (He et al., 2017). However, these approaches for learning word vectors only allow a single contextindependent representation for each word. Previously proposed methods overcome some of the shortcomings of traditional word vectors by either enriching them with subword information (e.g., Wieting et al., 2016; Bojanowski et al., 2017) or learning separate vectors for each word sense (e.g., Neelakantan et al., 2014). Our approach also benefits from subword units through the use of character convolutions, and we seamlessly incorporate multi-sense information into downstream tasks without explicitly training to predict predefined sense classes. Other recent work has also focused on learning context-dependent representations. context2vec (Melamud et al., 2016) uses a bidirectional Long Short Term Memory (LSTM; Hochreiter and Schmidhuber, 1997) to encode the context around a pivot word. Other approaches for learning contextual embeddings include the pivot word itself in the representation and are computed wi"
N18-1202,J05-1004,0,0.0609658,"Missing"
N18-1202,D14-1162,0,0.137952,"the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals. 1 Introduction Pre-trained word representations (Mikolov et al., 2013; Pennington et al., 2014) are a key component in many neural language understanding models. However, learning high quality representations can be challenging. They should ideally model both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). In this paper, we introduce a new type of deep contextualized word representation that directly addresses both challenges, can be easily integrated into existing models, and significantly improves the state of the art in every considered case across a range of challenging language u"
N18-1202,P17-1161,1,0.697802,"ting models, and significantly improves the state of the art in every considered case across a range of challenging language understanding problems. Our representations differ from traditional word type embeddings in that each token is assigned a representation that is a function of the entire input sentence. We use vectors derived from a bidirectional LSTM that is trained with a coupled language model (LM) objective on a large text corpus. For this reason, we call them ELMo (Embeddings from Language Models) representations. Unlike previous approaches for learning contextualized word vectors (Peters et al., 2017; McCann et al., 2017), ELMo representations are deep, in the sense that they are a function of all of the internal layers of the biLM. More specifically, we learn a linear combination of the vectors stacked above each input word for each end task, which markedly improves performance over just using the top LSTM layer. Combining the internal states in this manner allows for very rich word representations. Using intrinsic evaluations, we show that the higher-level LSTM states capture context-dependent aspects of word meaning (e.g., they can be used without modification to perform well on superv"
N18-1202,W12-4501,0,0.343829,"Missing"
N18-1202,D17-1120,0,0.865474,"der {. . . } Olivia De Havilland signed to do a Broadway play for Garson {. . . } Nearest Neighbors playing, game, games, played, players, plays, player, Play, football, multiplayer Kieffer , the only junior in the group , was commended for his ability to hit in the clutch , as well as his all-round excellent play . {. . . } they were actors who had been handed fat roles in a successful play , and had talent enough to fill the roles competently , with nice understatement . Table 4: Nearest neighbors to “play” using GloVe and the context embeddings from a biLM. Model WordNet 1st Sense Baseline Raganato et al. (2017a) Iacobacci et al. (2016) CoVe, First Layer CoVe, Second Layer biLM, First layer biLM, Second layer F1 65.9 69.9 70.1 59.4 64.7 67.4 69.0 Model Collobert et al. (2011) Ma and Hovy (2016) Ling et al. (2015) CoVe, First Layer CoVe, Second Layer biLM, First Layer biLM, Second Layer Acc. 97.3 97.6 97.8 93.3 92.8 97.3 96.8 Table 5: All-words fine grained WSD F1 . For CoVe and the biLM, we report scores for both the first and second layer biLSTMs. Table 6: Test set POS tagging accuracies for PTB. For CoVe and the biLM, we report scores for both the first and second layer biLSTMs. the task-specific"
N18-1202,E17-1010,0,0.821604,"der {. . . } Olivia De Havilland signed to do a Broadway play for Garson {. . . } Nearest Neighbors playing, game, games, played, players, plays, player, Play, football, multiplayer Kieffer , the only junior in the group , was commended for his ability to hit in the clutch , as well as his all-round excellent play . {. . . } they were actors who had been handed fat roles in a successful play , and had talent enough to fill the roles competently , with nice understatement . Table 4: Nearest neighbors to “play” using GloVe and the context embeddings from a biLM. Model WordNet 1st Sense Baseline Raganato et al. (2017a) Iacobacci et al. (2016) CoVe, First Layer CoVe, Second Layer biLM, First layer biLM, Second layer F1 65.9 69.9 70.1 59.4 64.7 67.4 69.0 Model Collobert et al. (2011) Ma and Hovy (2016) Ling et al. (2015) CoVe, First Layer CoVe, Second Layer biLM, First Layer biLM, Second Layer Acc. 97.3 97.6 97.8 93.3 92.8 97.3 96.8 Table 5: All-words fine grained WSD F1 . For CoVe and the biLM, we report scores for both the first and second layer biLSTMs. Table 6: Test set POS tagging accuracies for PTB. For CoVe and the biLM, we report scores for both the first and second layer biLSTMs. the task-specific"
N18-1202,N16-1114,0,0.0336672,"Missing"
N18-1202,P15-1109,0,0.0952398,"Missing"
N18-1202,C16-1329,0,0.694006,"Missing"
N18-1202,D13-1170,0,0.0814613,"Missing"
N18-1202,P16-2038,0,0.361634,"biLM on a corpus with approximately 30 million sentences (Chelba et al., 2014). We also generalize these approaches to deep contextual representations, which we show work well across a broad range of diverse NLP tasks. 1 http://allennlp.org/elmo Previous work has also shown that different layers of deep biRNNs encode different types of information. For example, introducing multi-task syntactic supervision (e.g., part-of-speech tags) at the lower levels of a deep LSTM can improve overall performance of higher level tasks such as dependency parsing (Hashimoto et al., 2017) or CCG super tagging (Søgaard and Goldberg, 2016). In an RNN-based encoder-decoder machine translation system, Belinkov et al. (2017) showed that the representations learned at the first layer in a 2layer LSTM encoder are better at predicting POS tags then second layer. Finally, the top layer of an LSTM for encoding word context (Melamud et al., 2016) has been shown to learn representations of word sense. We show that similar signals are also induced by the modified language model objective of our ELMo representations, and it can be very beneficial to learn models for downstream tasks that mix these different types of semi-supervision. Dai a"
N18-1202,P10-1040,0,0.0362285,"neural machine translation encoder. Finally, an analysis of both ELMo and CoVe reveals that deep representations outperform 2227 Proceedings of NAACL-HLT 2018, pages 2227–2237 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics those derived from just the top layer of an LSTM. Our trained models and code are publicly available, and we expect that ELMo will provide similar gains for many other NLP problems.1 2 Related work Due to their ability to capture syntactic and semantic information of words from large scale unlabeled text, pretrained word vectors (Turian et al., 2010; Mikolov et al., 2013; Pennington et al., 2014) are a standard component of most state-ofthe-art NLP architectures, including for question answering (Liu et al., 2017), textual entailment (Chen et al., 2017) and semantic role labeling (He et al., 2017). However, these approaches for learning word vectors only allow a single contextindependent representation for each word. Previously proposed methods overcome some of the shortcomings of traditional word vectors by either enriching them with subword information (e.g., Wieting et al., 2016; Bojanowski et al., 2017) or learning separate vectors f"
N18-1202,P17-1018,0,0.676855,"Missing"
N18-1202,D16-1157,0,0.311052,"from large scale unlabeled text, pretrained word vectors (Turian et al., 2010; Mikolov et al., 2013; Pennington et al., 2014) are a standard component of most state-ofthe-art NLP architectures, including for question answering (Liu et al., 2017), textual entailment (Chen et al., 2017) and semantic role labeling (He et al., 2017). However, these approaches for learning word vectors only allow a single contextindependent representation for each word. Previously proposed methods overcome some of the shortcomings of traditional word vectors by either enriching them with subword information (e.g., Wieting et al., 2016; Bojanowski et al., 2017) or learning separate vectors for each word sense (e.g., Neelakantan et al., 2014). Our approach also benefits from subword units through the use of character convolutions, and we seamlessly incorporate multi-sense information into downstream tasks without explicitly training to predict predefined sense classes. Other recent work has also focused on learning context-dependent representations. context2vec (Melamud et al., 2016) uses a bidirectional Long Short Term Memory (LSTM; Hochreiter and Schmidhuber, 1997) to encode the context around a pivot word. Other approache"
N18-1202,D15-1176,0,\N,Missing
N18-1202,W13-3516,0,\N,Missing
N18-1202,D16-1264,0,\N,Missing
N18-2120,N16-1147,0,0.0917693,"Missing"
N19-1116,P18-2058,0,0.0217324,"der all possible binary trees over the sentence. At test time the CKY algorithm extracts the highest scoring parse. DIORA achieves a new state-of-the-art F1 in unsupervised binary constituency parsing (unlabeled) in two benchmark datasets, WSJ and MultiNLI. 1 Under the current circumstances he says their scenario no longer seems unrealistic Figure 1: An unlabeled binary constituency parse from DIORA matching the ground truth. Introduction Syntactic parse trees are useful for downstream tasks such as relation extraction (Gamallo et al., 2012), semantic role labeling (Sutton and McCallum, 2005; He et al., 2018), machine translation (Aharoni and Goldberg, 2017; Eriguchi et al., 2017; Zaremoodi and Haffari, 2018), and text classification (Li and Roth, 2006; Tai et al., 2015). Traditionally, supervised parsers trained on datasets such as the Penn Treebank (Marcus et al., 1993) are used to obtain syntactic trees. However, the treebanks used to train these supervised parsers are typically small and restricted to the newswire domain. Unfortunately, models trained on newswire treebanks tend to perform considerably worse when applied to new types of data, and creating new domain specific treebanks with synt"
N19-1116,W18-5452,0,0.386886,"ments 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: i,j∈{k} πki , πkj ← arg max[xi + xj + e(i, j)] i,j∈{k} Backtrack to get the maximal tree. procedure BACKTRACK(k) if SIZE(k) = 1 then return k i ← BACKTRACK(πki ) j ← BACKTRACK(πkj ) return (i, j) return BACKTRACK(k ← root) vised segment recall, and phrase similarity. The model has been implemented in PyTorch (Team, 2018) and the code is published online.3 For training details, see Appendix A.1. 3.1 Unsupervised Parsing We first evaluate how well our model predicts a full unlabeled constituency parse. We look at two data sets used in prior work (Htut et al., 2018), The Wall Street Journal (WSJ) section of Penn Treebank (Marcus et al., 1993), and the automatic parses from MultiNLI (Williams et al., 2018b). WSJ has gold human-annotated parses and MultiNLI contains automatic parses derived from a supervised parser (Manning et al., 2014). In addition to PRPN (Shen et al., 2018),4 we compare our model to deterministically constructed left branching, right branching, balanced, and random trees. We also compare to ON-LSTM (Shen et al., 2019), an extension of the PRPN model, RL-SPINN (Yogatama et al., 2017), an unsupervised shift-reduce parser, and ST-Gumbel ("
N19-1116,N18-1202,1,0.808351,"rained on datasets such as the Penn Treebank (Marcus et al., 1993) are used to obtain syntactic trees. However, the treebanks used to train these supervised parsers are typically small and restricted to the newswire domain. Unfortunately, models trained on newswire treebanks tend to perform considerably worse when applied to new types of data, and creating new domain specific treebanks with syntactic annotations is expensive and timeconsuming. Motivated by the desire to address the limitations of supervised parsing and by the success of large-scale unsupervised modeling such as ELMo and BERT (Peters et al., 2018a; Devlin et al., ∗ Equal contribution, randomly ordered. 2019), we propose a new deep learning method of unsupervised parser training that can extract both shallow parses (i.e., noun phrases or entities) and full syntactic trees from any domain or language automatically without requiring any labeled training data. In addition to producing parses, our model simultaneously builds representations for internal constituents that reflect syntactic and semantic regularities which can be leveraged by downstream tasks. Our model builds on existing work developing latent tree chart parsers (Socher et a"
N19-1116,D18-1179,0,0.0665688,"rained on datasets such as the Penn Treebank (Marcus et al., 1993) are used to obtain syntactic trees. However, the treebanks used to train these supervised parsers are typically small and restricted to the newswire domain. Unfortunately, models trained on newswire treebanks tend to perform considerably worse when applied to new types of data, and creating new domain specific treebanks with syntactic annotations is expensive and timeconsuming. Motivated by the desire to address the limitations of supervised parsing and by the success of large-scale unsupervised modeling such as ELMo and BERT (Peters et al., 2018a; Devlin et al., ∗ Equal contribution, randomly ordered. 2019), we propose a new deep learning method of unsupervised parser training that can extract both shallow parses (i.e., noun phrases or entities) and full syntactic trees from any domain or language automatically without requiring any labeled training data. In addition to producing parses, our model simultaneously builds representations for internal constituents that reflect syntactic and semantic regularities which can be leveraged by downstream tasks. Our model builds on existing work developing latent tree chart parsers (Socher et a"
N19-1116,P02-1017,0,0.884927,"ndom Balanced Table 2: NLI unsupervised unlabeled binary constituency parsing comparing to CoreNLP predicted parses. PRPN F1 was calculated using the parse trees and results provided by Htut et al. (2018). F1 median and max are calculated over five random seeds and the top F1 value in each column is bolded. Note that we use median rather than mean in order to compare with previous work. details on WSJ split differences). Not binarizing the target trees sets an upper-bound on the performance of our models, denoted as UB in Table 3. We compare against previous notable models for this task: CCM (Klein and Manning, 2002) uses the EM algorithm to learn probable nested 1133 bracketings over a sentence using gold or induced part-of-speech tags, and PRLG (Ponvert et al., 2011) performs constituent parsing through consecutive rounds of sentence chunking. In Table 3, we see that DIORA outperforms the previous state of the art for WSJ-40, PRLG, in max F1. The WSJ-10 split has been difficult for latent tree parsers such as DIORA, PRPN, and ONLSTM, none of which (including our model) are able to improve upon previous non-neural methods. However, when we compare trends between WSJ-10 and WSJ-40, we see that DIORA does"
N19-1116,P04-1061,0,0.60752,"any pair of constituents and does not use structural supervision. Learning from Raw Text Unsupervised learning of syntactic structure has been an active research area (Brill et al., 1990), including for unsupervised segmentation (Ando and Lee, 2000; Goldwater et al., 2009; Ponvert et al., 2011) and unsupervised dependency parsing (Spitkovsky et al., 2013). Some models exploit the availability of parallel corpora in multiple languages (Das and Petrov, 2011; Cohen et al., 2011). Others have shown that dependency parsing can be used for unsupervised constituency parsing (Spitkovsky et al., 2013; Klein and Manning, 2004), or that it’s effective to prune a random subset of possible trees (Bod, 2006). These approaches aren’t necessarily orthogonal to DIORA. For instance, our model may benefit when combined with an unsupervised dependency parser. 5 Conclusion In this work we presented DIORA, an unsupervised method for inducing syntactic trees and representations of constituent spans. We showed 1136 inside-outside representations constructed with a latent tree chart parser and trained with an autoencoder language modeling objective learns syntactic structure of language effectively. In experiments on unsupervised"
N19-1116,P11-1108,0,0.719209,"ees and results provided by Htut et al. (2018). F1 median and max are calculated over five random seeds and the top F1 value in each column is bolded. Note that we use median rather than mean in order to compare with previous work. details on WSJ split differences). Not binarizing the target trees sets an upper-bound on the performance of our models, denoted as UB in Table 3. We compare against previous notable models for this task: CCM (Klein and Manning, 2002) uses the EM algorithm to learn probable nested 1133 bracketings over a sentence using gold or induced part-of-speech tags, and PRLG (Ponvert et al., 2011) performs constituent parsing through consecutive rounds of sentence chunking. In Table 3, we see that DIORA outperforms the previous state of the art for WSJ-40, PRLG, in max F1. The WSJ-10 split has been difficult for latent tree parsers such as DIORA, PRPN, and ONLSTM, none of which (including our model) are able to improve upon previous non-neural methods. However, when we compare trends between WSJ-10 and WSJ-40, we see that DIORA does a better job at extending to longer sequences. 3.2 Unsupervised Phrase Segmentation In many scenarios, one is only concerned with extracting particular con"
N19-1116,W12-4501,0,0.0169464,"meaningful representations for spans of text. Most language modeling methods focus only on explicitly modeling token representations and rely on ad-hoc postprocessing to generate representations for longer spans, typically relying on simple arithmetic functions of the individual tokens. To evaluate our model’s learned phrase representations, we look at the similarity between spans of the same type within labeled phrase datasets. We look at two datasets. CoNLL 2000 (Tjong Kim Sang and Buchholz, 2000) is a shallow parsing dataset containing spans of noun phrases, verb phrases, etc. CoNLL 2012 (Pradhan et al., 2012) Model WSJ-10 WSJ-40 F1µ F1max F1µ F1max UB LB RB 87.8 28.7 61.7 87.8 28.7 61.7 85.7 12.0 40.7 85.7 12.0 40.7 CCM† CCMgold † PRLG † - 63.2 71.9 72.1 - 33.7 54.6 PRPNN LI PRPN‡ ON-LSTM‡ DIORA 66.3 ±0.8 70.5 ±0.4 65.1 ±1.7 67.7 ±0.7 68.5 71.3 66.8 68.5 60.6 ±0.2 52.4 60.9 Table 3: WSJ-10 and WSJ-40 unsupervised non-binary unlabeled constituency parsing with punctuation removed. † indicates that the model predicts a full, nonbinary parse with additional resources. ‡ indicates model was trained on WSJ data and PRPNN LI was trained on MultiNLI data. CCM uses predicted POS tags while CCMgold uses go"
N19-1116,D14-1081,0,0.0460728,"lculate marginal probabilities in order to align spans between sentences in entailment. Composition Loss TreeLSTM TreeLSTM MLP MLP MLPKernel MLPShared Margin Softmax Margin Softmax Softmax Softmax F1µ ∅ +PP 49.9 52.0 49.7 52.6 51.8 50.8 53.1 52.9 54.4 55.5 54.8 56.7 Table 6: F1 for different model variants on the binary WSJ validation set with included punctuation. The binary trees are as-is (∅) or modified according to the post-processing heuristic (+P P ). The mean F1 is shown across three random seeds. Neural Inside-Outside Parsers The InsideOutside Recursive Neural Network (IORNN) (Le and Zuidema, 2014) is closest to ours. It is a graph-based dependency parser that uses beam search and can reliably find accurate parses when retaining a k-best list. In contrast, our model produces the most likely parse given the learned compatibility of the constituents. The Neural CRF Parser (Durrett and Klein, 2015), similar to DIORA, performs exact inference on the structure of a sentence, although requires a set of grammar rules and labeled parse trees during training. DIORA, like Liu et al. (2018), has a single grammar rule that applies to any pair of constituents and does not use structural supervision."
N19-1116,D15-1137,0,0.129869,"∗ Equal contribution, randomly ordered. 2019), we propose a new deep learning method of unsupervised parser training that can extract both shallow parses (i.e., noun phrases or entities) and full syntactic trees from any domain or language automatically without requiring any labeled training data. In addition to producing parses, our model simultaneously builds representations for internal constituents that reflect syntactic and semantic regularities which can be leveraged by downstream tasks. Our model builds on existing work developing latent tree chart parsers (Socher et al., 2011b; Le and Zuidema, 2015; Yogatama et al., 2017; Maillard et al., 2017; Choi et al., 2018). These methods produce representations for all internal nodes in the tree (cells in the chart), each generated as a soft weighting over all possible sub-trees (§2). Unfortunately, they still require sentence-level annotations during training, as they are all trained to optimize a downstream task, typically natural language inference. To address these limitations, we present deep inside-outside recursive autoencoders (DIORA) which enable unsupervised discovery and representation of constituents without requiring any supervised t"
N19-1116,D11-1014,0,0.0974649,"t al., 2018a; Devlin et al., ∗ Equal contribution, randomly ordered. 2019), we propose a new deep learning method of unsupervised parser training that can extract both shallow parses (i.e., noun phrases or entities) and full syntactic trees from any domain or language automatically without requiring any labeled training data. In addition to producing parses, our model simultaneously builds representations for internal constituents that reflect syntactic and semantic regularities which can be leveraged by downstream tasks. Our model builds on existing work developing latent tree chart parsers (Socher et al., 2011b; Le and Zuidema, 2015; Yogatama et al., 2017; Maillard et al., 2017; Choi et al., 2018). These methods produce representations for all internal nodes in the tree (cells in the chart), each generated as a soft weighting over all possible sub-trees (§2). Unfortunately, they still require sentence-level annotations during training, as they are all trained to optimize a downstream task, typically natural language inference. To address these limitations, we present deep inside-outside recursive autoencoders (DIORA) which enable unsupervised discovery and representation of constituents without req"
N19-1116,D18-1184,0,0.0423187,"ur model’s output, we see that some trees are an exact replication of the binarized ground truth (Fig. 3), or very close (Fig. 4). For future work we intend to explore common patterns in DIORA’s learned structure, although some patterns are already recognizable, such as the affinity to group particles and verbs (Fig. 5). 4 Related Work Latent Tree Learning A brief survey of neural latent tree learning models was covered in (Williams et al., 2018a). The first positive result for neural latent tree parsing was shown in (Htut et al., 2018), which used a language modeling objective. The model in (Liu et al., 2018) uses an inside chart and an outside procedure to calculate marginal probabilities in order to align spans between sentences in entailment. Composition Loss TreeLSTM TreeLSTM MLP MLP MLPKernel MLPShared Margin Softmax Margin Softmax Softmax Softmax F1µ ∅ +PP 49.9 52.0 49.7 52.6 51.8 50.8 53.1 52.9 54.4 55.5 54.8 56.7 Table 6: F1 for different model variants on the binary WSJ validation set with included punctuation. The binary trees are as-is (∅) or modified according to the post-processing heuristic (+P P ). The mean F1 is shown across three random seeds. Neural Inside-Outside Parsers The Ins"
N19-1116,P14-5010,0,0.0130058,"vised segment recall, and phrase similarity. The model has been implemented in PyTorch (Team, 2018) and the code is published online.3 For training details, see Appendix A.1. 3.1 Unsupervised Parsing We first evaluate how well our model predicts a full unlabeled constituency parse. We look at two data sets used in prior work (Htut et al., 2018), The Wall Street Journal (WSJ) section of Penn Treebank (Marcus et al., 1993), and the automatic parses from MultiNLI (Williams et al., 2018b). WSJ has gold human-annotated parses and MultiNLI contains automatic parses derived from a supervised parser (Manning et al., 2014). In addition to PRPN (Shen et al., 2018),4 we compare our model to deterministically constructed left branching, right branching, balanced, and random trees. We also compare to ON-LSTM (Shen et al., 2019), an extension of the PRPN model, RL-SPINN (Yogatama et al., 2017), an unsupervised shift-reduce parser, and ST-Gumbel (Choi et al., 2018), an unsupervised chart parser. The latter two of these models are trained to predict the downstream task of natural language inference (NLI). 3 To evaluate the effectiveness of DIORA, we run experiments on unsupervised parsing, unsuperprocedure CKY(chart)"
N19-1116,D13-1204,0,0.150575,"RF Parser (Durrett and Klein, 2015), similar to DIORA, performs exact inference on the structure of a sentence, although requires a set of grammar rules and labeled parse trees during training. DIORA, like Liu et al. (2018), has a single grammar rule that applies to any pair of constituents and does not use structural supervision. Learning from Raw Text Unsupervised learning of syntactic structure has been an active research area (Brill et al., 1990), including for unsupervised segmentation (Ando and Lee, 2000; Goldwater et al., 2009; Ponvert et al., 2011) and unsupervised dependency parsing (Spitkovsky et al., 2013). Some models exploit the availability of parallel corpora in multiple languages (Das and Petrov, 2011; Cohen et al., 2011). Others have shown that dependency parsing can be used for unsupervised constituency parsing (Spitkovsky et al., 2013; Klein and Manning, 2004), or that it’s effective to prune a random subset of possible trees (Bod, 2006). These approaches aren’t necessarily orthogonal to DIORA. For instance, our model may benefit when combined with an unsupervised dependency parser. 5 Conclusion In this work we presented DIORA, an unsupervised method for inducing syntactic trees and rep"
N19-1116,W05-0636,1,0.65145,"ynamic programming to consider all possible binary trees over the sentence. At test time the CKY algorithm extracts the highest scoring parse. DIORA achieves a new state-of-the-art F1 in unsupervised binary constituency parsing (unlabeled) in two benchmark datasets, WSJ and MultiNLI. 1 Under the current circumstances he says their scenario no longer seems unrealistic Figure 1: An unlabeled binary constituency parse from DIORA matching the ground truth. Introduction Syntactic parse trees are useful for downstream tasks such as relation extraction (Gamallo et al., 2012), semantic role labeling (Sutton and McCallum, 2005; He et al., 2018), machine translation (Aharoni and Goldberg, 2017; Eriguchi et al., 2017; Zaremoodi and Haffari, 2018), and text classification (Li and Roth, 2006; Tai et al., 2015). Traditionally, supervised parsers trained on datasets such as the Penn Treebank (Marcus et al., 1993) are used to obtain syntactic trees. However, the treebanks used to train these supervised parsers are typically small and restricted to the newswire domain. Unfortunately, models trained on newswire treebanks tend to perform considerably worse when applied to new types of data, and creating new domain specific t"
N19-1116,N18-1101,0,0.298174,"procedure BACKTRACK(k) if SIZE(k) = 1 then return k i ← BACKTRACK(πki ) j ← BACKTRACK(πkj ) return (i, j) return BACKTRACK(k ← root) vised segment recall, and phrase similarity. The model has been implemented in PyTorch (Team, 2018) and the code is published online.3 For training details, see Appendix A.1. 3.1 Unsupervised Parsing We first evaluate how well our model predicts a full unlabeled constituency parse. We look at two data sets used in prior work (Htut et al., 2018), The Wall Street Journal (WSJ) section of Penn Treebank (Marcus et al., 1993), and the automatic parses from MultiNLI (Williams et al., 2018b). WSJ has gold human-annotated parses and MultiNLI contains automatic parses derived from a supervised parser (Manning et al., 2014). In addition to PRPN (Shen et al., 2018),4 we compare our model to deterministically constructed left branching, right branching, balanced, and random trees. We also compare to ON-LSTM (Shen et al., 2019), an extension of the PRPN model, RL-SPINN (Yogatama et al., 2017), an unsupervised shift-reduce parser, and ST-Gumbel (Choi et al., 2018), an unsupervised chart parser. The latter two of these models are trained to predict the downstream task of natural langua"
N19-1116,C18-1120,0,0.0259508,"he highest scoring parse. DIORA achieves a new state-of-the-art F1 in unsupervised binary constituency parsing (unlabeled) in two benchmark datasets, WSJ and MultiNLI. 1 Under the current circumstances he says their scenario no longer seems unrealistic Figure 1: An unlabeled binary constituency parse from DIORA matching the ground truth. Introduction Syntactic parse trees are useful for downstream tasks such as relation extraction (Gamallo et al., 2012), semantic role labeling (Sutton and McCallum, 2005; He et al., 2018), machine translation (Aharoni and Goldberg, 2017; Eriguchi et al., 2017; Zaremoodi and Haffari, 2018), and text classification (Li and Roth, 2006; Tai et al., 2015). Traditionally, supervised parsers trained on datasets such as the Penn Treebank (Marcus et al., 1993) are used to obtain syntactic trees. However, the treebanks used to train these supervised parsers are typically small and restricted to the newswire domain. Unfortunately, models trained on newswire treebanks tend to perform considerably worse when applied to new types of data, and creating new domain specific treebanks with syntactic annotations is expensive and timeconsuming. Motivated by the desire to address the limitations o"
N19-1116,J93-2004,0,\N,Missing
N19-1116,D15-1075,0,\N,Missing
N19-1116,H94-1020,0,\N,Missing
N19-1116,W00-0726,0,\N,Missing
N19-1116,P11-1061,0,\N,Missing
N19-1116,P06-1109,0,\N,Missing
N19-1116,A00-2032,0,\N,Missing
N19-1116,P17-2012,0,\N,Missing
N19-1116,Q18-1019,1,\N,Missing
N19-1116,D18-1544,0,\N,Missing
N19-1116,N19-1423,0,\N,Missing
N19-1116,W12-0702,0,\N,Missing
N19-1130,I13-1171,0,0.0130145,"ood, 2016; Jockers and Kirilloff, 2016; Long and So, 2016), but these attempts largely rely on techniques such as word counting, topic modeling, and naive Bayes classifiers and are therefore not able to capture the meaning of sentences or paragraphs (Da, 2019). While these works discover general patterns from multiple literary works, we are the first to use cutting-edge NLP techniques to engage with specific literary criticism about a single narrative. There has been other computational work that focuses on just a single book or a small number of books, much of it focused on network analysis: Agarwal et al. (2013) extract character social networks from Alice in Wonderland, while Elson et al. (2010) recover social networks from 19th century British novels. Wallace (2012) disentangles multiple narrative threads within the novel Infinite Jest, while Eve (2019) provides several automated statistical methods for close reading and test them on the award-winning novel Cloud Atlas (2004). Compared to this work, we push further on modeling the content of the narrative by leveraging pretrained language models. 7 Conclusion Our work takes a first step towards computationally engaging with literary criticism on a"
N19-1130,P14-1035,0,0.0207568,"groups. use recent advances in text representation learning to test a single literary theory about the novel Invisible Cities by Italo Calvino. Introduction Literary critics form interpretations of meaning in works of literature. Building computational models that can help form and test these interpretations is a fundamental goal of digital humanities research (Benzon and Hays, 1976). Within natural language processing, most previous work that engages with literature relies on “distant reading” (Jockers, 2013), which involves discovering high-level patterns from large collections of stories (Bamman et al., 2014; Chaturvedi et al., 2018). We depart from this trend by showing that computational techniques can also engage with literary criticism at a closer distance: concretely, we Framed as a dialogue between the traveler Marco Polo and the emperor Kublai Khan, Invisible Cities consists of 55 prose poems, each of which describes an imaginary city. Calvino categorizes these cities into eleven thematic groups that deal with human emotions (e.g., desires, memories), general objects (eyes, sky, signs), and unusual properties (continuous, hidden, thin). Many critics argue that Calvino’s labels are not mean"
N19-1130,P09-1068,0,0.249031,"Missing"
N19-1130,N18-2106,0,0.0625723,"dvances in text representation learning to test a single literary theory about the novel Invisible Cities by Italo Calvino. Introduction Literary critics form interpretations of meaning in works of literature. Building computational models that can help form and test these interpretations is a fundamental goal of digital humanities research (Benzon and Hays, 1976). Within natural language processing, most previous work that engages with literature relies on “distant reading” (Jockers, 2013), which involves discovering high-level patterns from large collections of stories (Bamman et al., 2014; Chaturvedi et al., 2018). We depart from this trend by showing that computational techniques can also engage with literary criticism at a closer distance: concretely, we Framed as a dialogue between the traveler Marco Polo and the emperor Kublai Khan, Invisible Cities consists of 55 prose poems, each of which describes an imaginary city. Calvino categorizes these cities into eleven thematic groups that deal with human emotions (e.g., desires, memories), general objects (eyes, sky, signs), and unusual properties (continuous, hidden, thin). Many critics argue that Calvino’s labels are not meaningful, while others belie"
N19-1130,D17-1287,0,0.0512157,"Missing"
N19-1130,P10-1015,0,0.0360815,"ely on techniques such as word counting, topic modeling, and naive Bayes classifiers and are therefore not able to capture the meaning of sentences or paragraphs (Da, 2019). While these works discover general patterns from multiple literary works, we are the first to use cutting-edge NLP techniques to engage with specific literary criticism about a single narrative. There has been other computational work that focuses on just a single book or a small number of books, much of it focused on network analysis: Agarwal et al. (2013) extract character social networks from Alice in Wonderland, while Elson et al. (2010) recover social networks from 19th century British novels. Wallace (2012) disentangles multiple narrative threads within the novel Infinite Jest, while Eve (2019) provides several automated statistical methods for close reading and test them on the award-winning novel Cloud Atlas (2004). Compared to this work, we push further on modeling the content of the narrative by leveraging pretrained language models. 7 Conclusion Our work takes a first step towards computationally engaging with literary criticism on a single book using state-of-the-art text representation methods. While we demonstrate t"
N19-1130,D10-1008,0,0.192517,"Missing"
N19-1130,N16-1180,1,0.935262,"Missing"
N19-1130,P10-1158,0,0.0554715,"Missing"
N19-1130,D14-1162,0,0.108324,"iptions While each of the city descriptions is relatively short, Calvino’s writing is filled with rare words, 1292 complex syntactic structures, and figurative language.1 Capturing the essential components of each city in a single vector is thus not as simple as it is with more standard forms of text. Nevertheless, we hope that representations from language models trained over billions of words of text can extract some meaningful semantics from these descriptions. We experiment with three different pretrained representations: ELMo (Peters et al., 2018a), BERT (Devlin et al., 2018), and GloVe (Pennington et al., 2014). To produce a single city embedding, we compute the TF-IDF weighted element-wise mean of the token-level representations.2 For all pretrained methods, we additionally reduce the dimensionality of the city embeddings to 40 using PCA for increased compatibility with our clustering algorithm. 3.2 Clustering city representations Given 55 city representations, how do we group them into eleven clusters of five cities each? Initially, we experimented with a graph-based community detection algorithm that maximizes cluster modularity (Newman, 2006), but we found no simple way to constrain this method"
N19-1130,D18-1179,0,0.243497,"uman emotions (e.g., desires, memories), general objects (eyes, sky, signs), and unusual properties (continuous, hidden, thin). Many critics argue that Calvino’s labels are not meaningful, while others believe that there is a distinct thematic separation between the groups, including the author himself (Calvino, 2004). The unique structure of this novel — each city’s description is short and self-contained (Figure 1) — allows us to computationally examine this debate. As the book is too small to train any models, we leverage recent advances in large-scale language model-based representations (Peters et al., 2018a; Devlin et al., 2018) to compute a representation of each city. We feed these representa1291 Proceedings of NAACL-HLT 2019, pages 1291–1297 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics tions into a clustering algorithm that produces exactly eleven clusters of five cities each and evaluate them against both Calvino’s original labels and crowdsourced human judgments. While the overall correlation with Calvino’s labels is low, both computers and humans can reliably identify some thematic groups associated with concrete objects. While prior work"
N19-1130,N12-1001,0,0.0183585,"ifiers and are therefore not able to capture the meaning of sentences or paragraphs (Da, 2019). While these works discover general patterns from multiple literary works, we are the first to use cutting-edge NLP techniques to engage with specific literary criticism about a single narrative. There has been other computational work that focuses on just a single book or a small number of books, much of it focused on network analysis: Agarwal et al. (2013) extract character social networks from Alice in Wonderland, while Elson et al. (2010) recover social networks from 19th century British novels. Wallace (2012) disentangles multiple narrative threads within the novel Infinite Jest, while Eve (2019) provides several automated statistical methods for close reading and test them on the award-winning novel Cloud Atlas (2004). Compared to this work, we push further on modeling the content of the narrative by leveraging pretrained language models. 7 Conclusion Our work takes a first step towards computationally engaging with literary criticism on a single book using state-of-the-art text representation methods. While we demonstrate that NLP techniques can be used to support literary analyses and obtain ne"
P14-1105,D10-1111,0,0.0321643,"campaign cycle. They use an HMM -based model, defining the states as a set of fine-grained political ideologies, and rely on a closed set of lexical bigram features associated with each ideology, inferred from a manually labeled ideological books corpus. Although it takes elements of discourse structure into account (capturing the“burstiness” of ideological terminology usage), their model explicitly ignores intrasentential contextual influences of the kind seen in Figure 1. Other approaches on the document level use topic models to analyze bias in news articles, blogs, and political speeches (Ahmed and Xing, 2010; Lin et al., 2008; Nguyen et al., 2013). 6.2 Subjectivity Detection Detecting subjective language, which conveys opinion or speculation, is a related NLP problem. While sentences lacking subjective language may contain ideological bias (e.g., the topic of the sentence), highly-opinionated sentences likely have obvious ideological leanings. In addition, sentiment and subjectivity analysis offers methodological approaches that can be applied to automatic bias detection. Wiebe et al. (2004) show that low-frequency words and some collocations are a good indicators of subjectivity. More recently,"
P14-1105,J92-4003,0,0.211075,"th liberal, Republicans with conservative) lends confidence that this dataset contains a rich mix of ideological 1115 1 Available at http://cs.umd.edu/˜miyyer/ibc statements. However, the raw Convote dataset contains a low percentage of sentences with explicit ideological bias.2 We therefore use the features in Yano et al. (2010), which correlate with political bias, to select sentences to annotate that have a higher likelihood of containing bias. Their features come from the Linguistic Inquiry and Word Count lexicon (LIWC) (Pennebaker et al., 2001), as well as from lists of “sticky bigrams” (Brown et al., 1992) strongly associated with one party or another (e.g., “illegal aliens” implies conservative, “universal healthcare” implies liberal). We first extract the subset of sentences that contains any words in the LIWC categories of Negative Emotion, Positive Emotion, Causation, Anger, and Kill verbs.3 After computing a list of the top 100 sticky bigrams for each category, ranked by loglikelihood ratio, and selecting another subset from the original data that included only sentences containing at least one sticky bigram, we take the union of the two subsets. Finally, we balance the resulting dataset s"
P14-1105,P13-1162,0,0.282011,"; Lin et al., 2008; Nguyen et al., 2013). 6.2 Subjectivity Detection Detecting subjective language, which conveys opinion or speculation, is a related NLP problem. While sentences lacking subjective language may contain ideological bias (e.g., the topic of the sentence), highly-opinionated sentences likely have obvious ideological leanings. In addition, sentiment and subjectivity analysis offers methodological approaches that can be applied to automatic bias detection. Wiebe et al. (2004) show that low-frequency words and some collocations are a good indicators of subjectivity. More recently, Recasens et al. (2013) detect biased words in sentences using indicator features for bias cues such as hedges and factive verbs in addition to standard bag-of-words and part-of-speech features. They show that this type of linguistic information dramatically improves performance over several standard baselines. Greene and Resnik (2009) also emphasize the connection between syntactic and semantic relationships in their work on “implicit sentiment”, 1120 n 1 Most conservative n-grams Salt, Mexico, housework, speculated, consensus, lawyer, pharmaceuticals, ruthless, deadly, Clinton, redistribution 3 prize individual li"
P14-1105,N12-1085,1,0.308395,"ences, and selected typical partisan unigrams to represent the “biased” class. DUALIST labels 11,555 sentences as politically biased, 5,434 of which come from conservative authors and 6,121 of which come from liberal authors. 3.2.1 Annotating the IBC For purposes of annotation, we define the task of political ideology detection as identifying, if possible, the political position of a given sentence’s author, where position is either liberal or conservative.5 We used the Crowdflower crowdsourcing platform (crowdflower.com), which has previously been used for subsentential sentiment annotation (Sayeed et al., 2012), to obtain human annotations of the filtered IBC dataset for political bias on both the sentence and phrase level. While members of the Crowdflower workforce are certainly not experts in political science, our simple task and the ubiquity of political bias allows us to acquire useful annotations. Crowdflower Task First, we parse the filtered IBC sentences using the Stanford constituency parser (Socher et al., 2013a). Because of the expense of labeling every node in a sentence, we only label one path in each sentence. The process for selecting paths is as follows: first, if any paths contain o"
P14-1105,D11-1136,0,0.00970619,"also substantial parliamentary boilerplate language. 3 While Kill verbs are not a category in LIWC, Yano et al. (2010) adopted it from Greene and Resnik (2009) and showed it to be a useful predictor of political bias. It includes words such as “slaughter” and “starve”. 4 This difference can be mainly attributed to a historical topics in the IBC (e.g., the Crusades, American Civil War). In Convote, every sentence is part of a debate about 2005 political policy. bias, instead of the more general task of classifying sentences as “neutral” or “biased”, we filter the dataset further using DUALIST (Settles, 2011), an active learning tool, to reduce the proportion of neutral sentences in our dataset. To train the DUALIST classifier, we manually assigned class labels of “neutral” or “biased” to 200 sentences, and selected typical partisan unigrams to represent the “biased” class. DUALIST labels 11,555 sentences as politically biased, 5,434 of which come from conservative authors and 6,121 of which come from liberal authors. 3.2.1 Annotating the IBC For purposes of annotation, we define the task of political ideology detection as identifying, if possible, the political position of a given sentence’s auth"
P14-1105,D13-1010,0,0.0532505,"the liberal-to-conservative switch. In D, negation confuses our model. the sake of simplicity. For example, Gentzkow and Shapiro (2010) derive a “slant index” to rate the ideological leaning of newspapers. A newspaper’s slant index is governed by the frequency of use of partisan collocations of 2-3 tokens. Similarly, authors have relied on simple models of language when leveraging inferred ideological positions. E.g., Gerrish and Blei (2011) predict the voting patterns of Congress members based on bagof-words representations of bills and inferred political leanings of those members. Recently, Sim et al. (2013) have proposed a model to infer mixtures of ideological positions in documents, applied to understanding the evolution of ideological rhetoric used by political candidates during the campaign cycle. They use an HMM -based model, defining the states as a set of fine-grained political ideologies, and rely on a closed set of lexical bigram features associated with each ideology, inferred from a manually labeled ideological books corpus. Although it takes elements of discourse structure into account (capturing the“burstiness” of ideological terminology usage), their model explicitly ignores intras"
P14-1105,N09-1057,1,0.545424,"sentences in the IBC, most of which have no noticeable political bias. Therefore we use the filtering procedure outlined in Section 3.1.1 to obtain a subset of 55,932 sentences. Compared to our final Convote dataset, an even larger percentage of the IBC sentences exhibit no noticeable political bias.4 Because our goal is to distinguish between liberal and conservative 2 Many sentences in Convote are variations on “I think this is a good/bad bill”, and there is also substantial parliamentary boilerplate language. 3 While Kill verbs are not a category in LIWC, Yano et al. (2010) adopted it from Greene and Resnik (2009) and showed it to be a useful predictor of political bias. It includes words such as “slaughter” and “starve”. 4 This difference can be mainly attributed to a historical topics in the IBC (e.g., the Crusades, American Civil War). In Convote, every sentence is part of a debate about 2005 political policy. bias, instead of the more general task of classifying sentences as “neutral” or “biased”, we filter the dataset further using DUALIST (Settles, 2011), an active learning tool, to reduce the proportion of neutral sentences in our dataset. To train the DUALIST classifier, we manually assigned cl"
P14-1105,D11-1014,0,0.786987,"e sentence in Figure 1 includes phrases typically associated with conservatives, such as “small businesses” and “death tax”. When we take more of the structure into account, however, we find that scare quotes and a negative propositional attitude (a lie about X) yield an evident liberal bias. Existing approaches toward bias detection have not gone far beyond “bag of words” classifiers, thus ignoring richer linguistic context of this kind and often operating at the level of whole documents. In contrast, recent work in sentiment analysis has used deep learning to discover compositional effects (Socher et al., 2011b; Socher et al., 2013b). Building from those insights, we introduce a recursive neural network (RNN) to detect ideological bias on the sentence level. This model requires 1113 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1113–1122, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics pe = so-called climate change xe= WL xd= wd = so-called WR pc = climate change xc= WL xa= wa = climate WR xb= wb = change Figure 2: An example RNN for the phrase “socalled climate change”. Two d-dimensional word vectors (here"
P14-1105,D13-1137,0,0.00652278,"data; the discrepancy between categorical predictions and annotations is measured 1114 through the cross-entropy loss. We optimize the model parameters to minimize the cross-entropy loss over all sentences in the corpus. The crossentropy loss of a single sentence is the sum over the true labels yi in the sentence, `(ˆ ys ) = k X yp ∗ log(ˆ yp ). (4) p=1 This induces a supervised objective function over all sentences: a regularized sum over all node losses normalized by the number of nodes N in the training set, C= 1 N N X `(predi ) + i λ kθk2 . 2 (5) We use L - BFGS with parameter averaging (Hashimoto et al., 2013) to optimize the model parameters θ = (WL , WR , Wcat , We , b1 , b2 ). The gradient of the objective, shown in Eq. (6), is computed using backpropagation through structure (Goller and Kuchler, 1996), N ∂C 1 X ∂`(ˆ yi ) = + λθ. ∂θ N ∂θ (6) i 2.2 Initialization When initializing our model, we have two choices: we can initialize all of our parameters randomly or provide the model some prior knowledge. As we see in Section 4, these choices have a significant effect on final performance. Random The most straightforward choice is to initialize the word embedding matrix We and composition matrices W"
P14-1105,P13-1088,0,0.0090081,"ove further when given additional phrase-level annotations. RNNs are quantitatively more effective than existing methods that use syntactic and semantic features separately, and we also illustrate how our model correctly identifies ideological bias in complex syntactic constructions. 2 Recursive Neural Networks Recursive neural networks (RNNs) are machine learning models that capture syntactic and semantic composition. They have achieved state-of-the-art performance on a variety of sentence-level NLP tasks, including sentiment analysis, paraphrase detection, and parsing (Socher et al., 2011a; Hermann and Blunsom, 2013). RNN models represent a shift from previous research on ideological bias detection in that they do not rely on hand-made lexicons, dictionaries, or rule sets. In this section, we describe a supervised RNN model for bias detection and highlight differences from previous work in training procedure and initialization. 2.1 ological bias becomes identifiable only at higher levels of sentence trees (as verified by our annotation, Figure 4), models relying primarily on wordlevel distributional statistics are not desirable for our problem. The basic idea behind the standard RNN model is that each wor"
P14-1105,P13-1045,0,0.411506,"includes phrases typically associated with conservatives, such as “small businesses” and “death tax”. When we take more of the structure into account, however, we find that scare quotes and a negative propositional attitude (a lie about X) yield an evident liberal bias. Existing approaches toward bias detection have not gone far beyond “bag of words” classifiers, thus ignoring richer linguistic context of this kind and often operating at the level of whole documents. In contrast, recent work in sentiment analysis has used deep learning to discover compositional effects (Socher et al., 2011b; Socher et al., 2013b). Building from those insights, we introduce a recursive neural network (RNN) to detect ideological bias on the sentence level. This model requires 1113 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1113–1122, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics pe = so-called climate change xe= WL xd= wd = so-called WR pc = climate change xc= WL xa= wa = climate WR xb= wb = change Figure 2: An example RNN for the phrase “socalled climate change”. Two d-dimensional word vectors (here, d = 6) are composed"
P14-1105,D13-1170,0,0.148607,"includes phrases typically associated with conservatives, such as “small businesses” and “death tax”. When we take more of the structure into account, however, we find that scare quotes and a negative propositional attitude (a lie about X) yield an evident liberal bias. Existing approaches toward bias detection have not gone far beyond “bag of words” classifiers, thus ignoring richer linguistic context of this kind and often operating at the level of whole documents. In contrast, recent work in sentiment analysis has used deep learning to discover compositional effects (Socher et al., 2011b; Socher et al., 2013b). Building from those insights, we introduce a recursive neural network (RNN) to detect ideological bias on the sentence level. This model requires 1113 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1113–1122, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics pe = so-called climate change xe= WL xd= wd = so-called WR pc = climate change xc= WL xa= wa = climate WR xb= wb = change Figure 2: An example RNN for the phrase “socalled climate change”. Two d-dimensional word vectors (here, d = 6) are composed"
P14-1105,W06-1639,0,0.0602891,"wo terms are highly correlated (e.g., a member of the Republican party likely agrees with conservative stances on most issues), they are not identical. For example, a moderate Republican might agree with the liberal position on increased gun control but take conservative positions on other issues. To avoid conflating partisanship and ideology we create a new dataset annotated for ideological bias on the sentence and phrase level. In this section we describe our initial dataset (Convote) and explain the procedure we followed for creating our new dataset (IBC).1 3.1 Convote The Convote dataset (Thomas et al., 2006) consists of US Congressional floor debate transcripts from 2005 in which all speakers have been labeled with their political party (Democrat, Republican, or independent). We propagate party labels down from the speaker to all of their individual sentences and map from party label to ideology label (Democrat → liberal, Republican → conservative). This is an expedient choice; in future work we plan to make use of work in political science characterizing candidates’ ideological positions empirically based on their behavior (Carroll et al., 2009). While the Convote dataset has seen widespread use"
P14-1105,J04-3002,0,0.0126291,"s on the document level use topic models to analyze bias in news articles, blogs, and political speeches (Ahmed and Xing, 2010; Lin et al., 2008; Nguyen et al., 2013). 6.2 Subjectivity Detection Detecting subjective language, which conveys opinion or speculation, is a related NLP problem. While sentences lacking subjective language may contain ideological bias (e.g., the topic of the sentence), highly-opinionated sentences likely have obvious ideological leanings. In addition, sentiment and subjectivity analysis offers methodological approaches that can be applied to automatic bias detection. Wiebe et al. (2004) show that low-frequency words and some collocations are a good indicators of subjectivity. More recently, Recasens et al. (2013) detect biased words in sentences using indicator features for bias cues such as hedges and factive verbs in addition to standard bag-of-words and part-of-speech features. They show that this type of linguistic information dramatically improves performance over several standard baselines. Greene and Resnik (2009) also emphasize the connection between syntactic and semantic relationships in their work on “implicit sentiment”, 1120 n 1 Most conservative n-grams Salt, M"
P14-1105,H05-1044,0,0.0342694,"“be used as an instrument to achieve charitable or social ends” reflects a liberal ideology, which the model predicts correctly. However, our model is unable to detect the polarity switch when this phrase is negated with “should not”. Since many different issues are discussed in the IBC, it is likely that our dataset has too few examples of some of these issues for the model to adequately learn the appropriate ideological positions, and more training data would resolve many of these errors. 6 Related Work A growing NLP subfield detects private states such as opinions, sentiment, and beliefs (Wilson et al., 2005; Pang and Lee, 2008) from text. In general, work in this category tends to combine traditional surface lexical modeling (e.g., bag-of-words) with hand-designed syntactic features or lexicons. Here we review the most salient literature related to the present paper. 6.1 Automatic Ideology Detection Most previous work on ideology detection ignores the syntactic structure of the language in use in favor of familiar bag-of-words representations for 1119 A B Thus , the harsh made worse by of free-market the implementing ideology conditions for farmers caused by a number of factors , , have created"
P14-1105,W10-0723,1,0.935118,"al., 2009). While the Convote dataset has seen widespread use for document-level political classification, we are unaware of similar efforts at the sentence level. 3.1.1 Biased Sentence Selection The strong correlation between US political parties and political ideologies (Democrats with liberal, Republicans with conservative) lends confidence that this dataset contains a rich mix of ideological 1115 1 Available at http://cs.umd.edu/˜miyyer/ibc statements. However, the raw Convote dataset contains a low percentage of sentences with explicit ideological bias.2 We therefore use the features in Yano et al. (2010), which correlate with political bias, to select sentences to annotate that have a higher likelihood of containing bias. Their features come from the Linguistic Inquiry and Word Count lexicon (LIWC) (Pennebaker et al., 2001), as well as from lists of “sticky bigrams” (Brown et al., 1992) strongly associated with one party or another (e.g., “illegal aliens” implies conservative, “universal healthcare” implies liberal). We first extract the subset of sentences that contains any words in the LIWC categories of Negative Emotion, Positive Emotion, Causation, Anger, and Kill verbs.3 After computing"
P15-1162,S14-2098,0,0.0113706,"Missing"
P15-1162,D10-1115,0,0.0107131,"ible”. However, we find no significant differences between the L2 norms of stopwords and words in the sentiment lexicon of Hu and Liu (2004). 6 Related Work Our DAN model builds on the successes of both simple vector operations and neural network-based models for compositionality. There are a variety of element-wise vector operations that could replace the average used in the DAN . Mitchell and Lapata (2008) experiment with many of them to model the compositionality of short phrases. Later, their work was extended to take into account the syntactic relation between words (Erk and Pad´o, 2008; Baroni and Zamparelli, 2010; Kartsaklis and Sadrzadeh, 2013) and grammars (Coecke et al., 2010; Grefenstette and Sadrzadeh, 2011). While the average works best for the tasks that we consider, Banea et al. (2014) find that simply summing word2vec embeddings outperforms all other methods on the SemEval 2014 phrase-to-word and sentence-to-phrase similarity tasks. Once we compute the embedding average in a DAN , we feed it to a deep neural network. In contrast, most previous work on neural network-based methods for NLP tasks explicitly model word order. Outside of sentiment analysis, RecNN-based approaches have been success"
P15-1162,D14-1070,1,0.582478,"OW, g averages word embeddings1 1 X z = g(w ∈ X) = vw . (1) |X| w∈X Feeding z to a softmax layer induces estimated probabilities for each output label yˆ = softmax(Ws · z + b), (2) where the softmax function is exp q softmax(q) = Pk j=1 exp qj (3) Ws is a k × d matrix for a dataset with k output labels, and b is a bias term. We train the NBOW model to minimize crossentropy error, which for a single training instance with ground-truth label y is `(ˆ y) = k X yp log(ˆ yp ). (4) p=1 1 Preliminary experiments indicate that averaging outperforms the vector sum used in NBOW from Kalchbrenner et al. (2014). Before we describe our deep extension of the model, we take a quick detour to discuss syntactic composition functions. Connections to other representation frameworks are discussed further in Section 4. NBOW 2.2 Considering Syntax for Composition Given a sentence like “You’ll be more entertained getting hit by a bus”, an unordered model like NBOW might be deceived by the word “entertained” to return a positive prediction. In contrast, syntactic composition functions rely on the order and structure of the input to learn how one word or phrase affects another, sacrificing computational efficien"
P15-1162,D12-1118,1,0.276157,"Missing"
P15-1162,D14-1082,0,0.0813431,"ide of sentiment analysis, RecNN-based approaches have been successful for tasks such as parsing (Socher et al., 2013a), machine translation (Liu et al., 2014), and paraphrase detection (Socher et al., 2011a). Convolutional networks also model word order in local windows and have achieved performance comparable to or better than that of RecNNs on many tasks (Collobert and Weston, 2008; Kim, 2014). Meanwhile, feedforward architectures like that of the DAN have been used for language modeling (Bengio et al., 2003), selectional preference acquisition (Van de Cruys, 2014), and dependency parsing (Chen and Manning, 2014). 7 Future Work can also extend the DAN’s success at incorporating out-of-domain training data to sentiment analysis: imagine training a DAN on labeled tweets for classification on newspaper reviews. Another potentially interesting application is to add gated units to a DAN,as has been done for recurrent and recursive neural networks (Hochreiter and Schmidhuber, 1997; Cho et al., 2014; Sutskever et al., 2014; Tai et al., 2015), to drop useless words rather than randomly-selected ones. 8 Conclusion In this paper, we introduce the deep averaging network, which feeds an unweighted average of word"
P15-1162,D14-1179,0,0.0113963,"Missing"
P15-1162,D08-1094,0,0.0145513,"Missing"
P15-1162,D11-1129,0,0.0213482,"n the sentiment lexicon of Hu and Liu (2004). 6 Related Work Our DAN model builds on the successes of both simple vector operations and neural network-based models for compositionality. There are a variety of element-wise vector operations that could replace the average used in the DAN . Mitchell and Lapata (2008) experiment with many of them to model the compositionality of short phrases. Later, their work was extended to take into account the syntactic relation between words (Erk and Pad´o, 2008; Baroni and Zamparelli, 2010; Kartsaklis and Sadrzadeh, 2013) and grammars (Coecke et al., 2010; Grefenstette and Sadrzadeh, 2011). While the average works best for the tasks that we consider, Banea et al. (2014) find that simply summing word2vec embeddings outperforms all other methods on the SemEval 2014 phrase-to-word and sentence-to-phrase similarity tasks. Once we compute the embedding average in a DAN , we feed it to a deep neural network. In contrast, most previous work on neural network-based methods for NLP tasks explicitly model word order. Outside of sentiment analysis, RecNN-based approaches have been successful for tasks such as parsing (Socher et al., 2013a), machine translation (Liu et al., 2014), and para"
P15-1162,W13-3209,0,0.0832105,"state-of-the-art accuracies on sentiment analysis tasks (Tai et al., 2015). As in NBOW , each word type has an associated embedding. However, the composition function g now depends on a parse tree of the input sequence. The representation for any internal node in a binary parse tree is computed as a nonlinear function of the representations of its children (Figure 1, left). A more powerful RecNN variant is the recursive neural tensor network (RecNTN), which modifies g to include a costly tensor product (Socher et al., 2013b). While RecNNs can model complex linguistic phenomena like negation (Hermann et al., 2013), they require much more training time than NBOW models. The nonlinearities and matrix/tensor products at each node of the parse tree are expensive, especially as model dimensionality increases. R ec NN s also require an error signal at every node. One root softmax is not strong enough for the model to learn compositional relations and leads to worse accuracies than standard bag-of-words models (Li, 2014). Finally, RecNNs require relatively consistent syntax between training and test data due to their reliance on parse trees and thus cannot effectively incorporate out-of-domain data, as we sho"
P15-1162,P14-1105,1,0.666888,"rameters selected on the SST also work well for the IMDB task. 4.1.5 Results The DAN achieves the second best reported result on the RT dataset, behind only the significantly slower CNN - MC model. It’s also competitive with more complex models on the SST and outperforms the DCNN and WRRBM on the document-level IMDB task. Interestingly, the DAN achieves good performance on the SST when trained with only sentence-level labels, indicating that it does not suffer from the vanishing error signal problem that plagues RecNNs. Since acquiring labelled phrases is often expensive (Sayeed et al., 2012; Iyyer et al., 2014b), this result is promising for large or messy datasets where fine-grained annotation is infeasible. 4.1.6 Timing Experiments DAN s require less time per epoch and—in general— require fewer epochs than their syntactic counterparts. We compare DAN runtime on the SST to publicly-available implementations of syntactic baselines in the last column of Table 1; the reported times are for a single epoch to control for hyperparameter choices such as learning rate, and all models use 300-d word vectors. Training a DAN on just sentence-level labels on the SST takes under five minutes on a single core o"
P15-1162,W13-3214,0,0.0468386,"Missing"
P15-1162,P14-1062,0,0.482625,"instantiation of NBOW, g averages word embeddings1 1 X z = g(w ∈ X) = vw . (1) |X| w∈X Feeding z to a softmax layer induces estimated probabilities for each output label yˆ = softmax(Ws · z + b), (2) where the softmax function is exp q softmax(q) = Pk j=1 exp qj (3) Ws is a k × d matrix for a dataset with k output labels, and b is a bias term. We train the NBOW model to minimize crossentropy error, which for a single training instance with ground-truth label y is `(ˆ y) = k X yp log(ˆ yp ). (4) p=1 1 Preliminary experiments indicate that averaging outperforms the vector sum used in NBOW from Kalchbrenner et al. (2014). Before we describe our deep extension of the model, we take a quick detour to discuss syntactic composition functions. Connections to other representation frameworks are discussed further in Section 4. NBOW 2.2 Considering Syntax for Composition Given a sentence like “You’ll be more entertained getting hit by a bus”, an unordered model like NBOW might be deceived by the word “entertained” to return a positive prediction. In contrast, syntactic composition functions rely on the order and structure of the input to learn how one word or phrase affects another, sacrificing computational efficien"
P15-1162,D13-1166,0,0.0216649,"gnificant differences between the L2 norms of stopwords and words in the sentiment lexicon of Hu and Liu (2004). 6 Related Work Our DAN model builds on the successes of both simple vector operations and neural network-based models for compositionality. There are a variety of element-wise vector operations that could replace the average used in the DAN . Mitchell and Lapata (2008) experiment with many of them to model the compositionality of short phrases. Later, their work was extended to take into account the syntactic relation between words (Erk and Pad´o, 2008; Baroni and Zamparelli, 2010; Kartsaklis and Sadrzadeh, 2013) and grammars (Coecke et al., 2010; Grefenstette and Sadrzadeh, 2011). While the average works best for the tasks that we consider, Banea et al. (2014) find that simply summing word2vec embeddings outperforms all other methods on the SemEval 2014 phrase-to-word and sentence-to-phrase similarity tasks. Once we compute the embedding average in a DAN , we feed it to a deep neural network. In contrast, most previous work on neural network-based methods for NLP tasks explicitly model word order. Outside of sentiment analysis, RecNN-based approaches have been successful for tasks such as parsing (So"
P15-1162,D14-1181,0,0.0448337,"an NBOW models. The nonlinearities and matrix/tensor products at each node of the parse tree are expensive, especially as model dimensionality increases. R ec NN s also require an error signal at every node. One root softmax is not strong enough for the model to learn compositional relations and leads to worse accuracies than standard bag-of-words models (Li, 2014). Finally, RecNNs require relatively consistent syntax between training and test data due to their reliance on parse trees and thus cannot effectively incorporate out-of-domain data, as we show in our question-answering experiments. Kim (2014) shows that some of these issues can be avoided by using a convolutional network instead of a RecNN, but the computational complexity increases even further (see Section 4 for runtime comparisons). What contributes most to the power of syntactic 1682 RecNN softmax z3 = f (W   DAN softmax c1 + b) z2 softmax z2 = f (W  softmax  c2 + b) z1 z1 = f (W h2 = f (W2 · h1 + b2 ) h1 = f (W1 · av + b1 )   c3 + b) c4 av = 4 P i=1 Predator c1 is c2 a c3 masterpiece c4 Predator c1 is c2 a c3 ci 4 masterpiece c4 Figure 1: On the left, a RecNN is given an input sentence for sentiment classification. Soft"
P15-1162,P14-1140,0,0.010475,"stette and Sadrzadeh, 2011). While the average works best for the tasks that we consider, Banea et al. (2014) find that simply summing word2vec embeddings outperforms all other methods on the SemEval 2014 phrase-to-word and sentence-to-phrase similarity tasks. Once we compute the embedding average in a DAN , we feed it to a deep neural network. In contrast, most previous work on neural network-based methods for NLP tasks explicitly model word order. Outside of sentiment analysis, RecNN-based approaches have been successful for tasks such as parsing (Socher et al., 2013a), machine translation (Liu et al., 2014), and paraphrase detection (Socher et al., 2011a). Convolutional networks also model word order in local windows and have achieved performance comparable to or better than that of RecNNs on many tasks (Collobert and Weston, 2008; Kim, 2014). Meanwhile, feedforward architectures like that of the DAN have been used for language modeling (Bengio et al., 2003), selectional preference acquisition (Van de Cruys, 2014), and dependency parsing (Chen and Manning, 2014). 7 Future Work can also extend the DAN’s success at incorporating out-of-domain training data to sentiment analysis: imagine training a"
P15-1162,P11-1015,0,0.19269,"och on the SST fine-grained task). Asterisked models are initialized either with different pretrained embeddings or randomly. 4.1 Sentiment Analysis Recently, syntactic composition functions have revolutionized both fine-grained and binary (positive or negative) sentiment analysis. We conduct sentence-level sentiment experiments on the Rotten Tomatoes (RT) movie reviews dataset (Pang and Lee, 2005) and its extension with phrase-level labels, the Stanford Sentiment Treebank (SST) introduced by Socher et al. (2013b). Our model is also effective on the document-level IMDB movie review dataset of Maas et al. (2011). 4.1.1 Neural Baselines Most neural approaches to sentiment analysis are variants of either recursive or convolutional networks. Our recursive neural network baselines include standard RecNNs (Socher et al., 2011b), R ec NTNs, the deep recursive network ( DR ec NN ) proposed by ˙Irsoy and Cardie (2014), and the TREE - LSTM of (Tai et al., 2015). Convolutional network baselines include the dynamic convolutional network (Kalchbrenner et al., 2014, DCNN ) and the convolutional neural network multichannel (Kim, 2014, CNN - MC). Our other neural baselines are the sliding-window based paragraph vec"
P15-1162,P08-1028,0,0.0096376,"sentiment. Intuitively, after the embeddings are fine-tuned during DAN training, we might expect a decrease in the norms of stopwords and an increase in the 1688 norms of sentiment-rich words like “awesome” or “horrible”. However, we find no significant differences between the L2 norms of stopwords and words in the sentiment lexicon of Hu and Liu (2004). 6 Related Work Our DAN model builds on the successes of both simple vector operations and neural network-based models for compositionality. There are a variety of element-wise vector operations that could replace the average used in the DAN . Mitchell and Lapata (2008) experiment with many of them to model the compositionality of short phrases. Later, their work was extended to take into account the syntactic relation between words (Erk and Pad´o, 2008; Baroni and Zamparelli, 2010; Kartsaklis and Sadrzadeh, 2013) and grammars (Coecke et al., 2010; Grefenstette and Sadrzadeh, 2011). While the average works best for the tasks that we consider, Banea et al. (2014) find that simply summing word2vec embeddings outperforms all other methods on the SemEval 2014 phrase-to-word and sentence-to-phrase similarity tasks. Once we compute the embedding average in a DAN ,"
P15-1162,P05-1015,0,0.56308,"— — — — — 89.4 92.6 — 89.2 — — 431 — — — 2,452 — Table 1: DANs achieve comparable sentiment accuracies to syntactic functions (bottom third of table) but require much less training time (measured as time of a single epoch on the SST fine-grained task). Asterisked models are initialized either with different pretrained embeddings or randomly. 4.1 Sentiment Analysis Recently, syntactic composition functions have revolutionized both fine-grained and binary (positive or negative) sentiment analysis. We conduct sentence-level sentiment experiments on the Rotten Tomatoes (RT) movie reviews dataset (Pang and Lee, 2005) and its extension with phrase-level labels, the Stanford Sentiment Treebank (SST) introduced by Socher et al. (2013b). Our model is also effective on the document-level IMDB movie review dataset of Maas et al. (2011). 4.1.1 Neural Baselines Most neural approaches to sentiment analysis are variants of either recursive or convolutional networks. Our recursive neural network baselines include standard RecNNs (Socher et al., 2011b), R ec NTNs, the deep recursive network ( DR ec NN ) proposed by ˙Irsoy and Cardie (2014), and the TREE - LSTM of (Tai et al., 2015). Convolutional network baselines in"
P15-1162,D14-1162,0,0.123646,"paces of size |V |2 . 4.1.3 DAN Configurations In Table 1, we compare a variety of DAN and NBOW configurations5 to the baselines described above. In particular, we are interested in not only comparing DAN accuracies to those of the baselines, but also how initializing with pretrained embeddings and restricting the model to only root-level labels affects performance. With this in mind, the NBOW- RAND and DAN - RAND models are initialized with random 300-dimensional word embeddings, while the other models are initialized with publicly-available 300-d GloVe vectors trained over the Common Crawl (Pennington et al., 2014). The DAN - ROOT model only has access to sentence-level labels for SST experiments, while all other models are trained on labeled phrases (if they exist) in addition to sentences. We train all NBOW and DAN models using AdaGrad (Duchi et al., 2011). We apply DANs to documents by averaging the embeddings for all of a document’s tokens and then feeding that average through multiple layers as before. Since the representations computed by DAN s are always d-dimensional vectors regardless of the input size, they are efficient with respect to both memory and computational cost. We find that the hype"
P15-1162,N12-1085,1,0.265487,"find that the hyperparameters selected on the SST also work well for the IMDB task. 4.1.5 Results The DAN achieves the second best reported result on the RT dataset, behind only the significantly slower CNN - MC model. It’s also competitive with more complex models on the SST and outperforms the DCNN and WRRBM on the document-level IMDB task. Interestingly, the DAN achieves good performance on the SST when trained with only sentence-level labels, indicating that it does not suffer from the vanishing error signal problem that plagues RecNNs. Since acquiring labelled phrases is often expensive (Sayeed et al., 2012; Iyyer et al., 2014b), this result is promising for large or messy datasets where fine-grained annotation is infeasible. 4.1.6 Timing Experiments DAN s require less time per epoch and—in general— require fewer epochs than their syntactic counterparts. We compare DAN runtime on the SST to publicly-available implementations of syntactic baselines in the last column of Table 1; the reported times are for a single epoch to control for hyperparameter choices such as learning rate, and all models use 300-d word vectors. Training a DAN on just sentence-level labels on the SST takes under five minute"
P15-1162,D11-1014,0,0.0224104,"oth fine-grained and binary (positive or negative) sentiment analysis. We conduct sentence-level sentiment experiments on the Rotten Tomatoes (RT) movie reviews dataset (Pang and Lee, 2005) and its extension with phrase-level labels, the Stanford Sentiment Treebank (SST) introduced by Socher et al. (2013b). Our model is also effective on the document-level IMDB movie review dataset of Maas et al. (2011). 4.1.1 Neural Baselines Most neural approaches to sentiment analysis are variants of either recursive or convolutional networks. Our recursive neural network baselines include standard RecNNs (Socher et al., 2011b), R ec NTNs, the deep recursive network ( DR ec NN ) proposed by ˙Irsoy and Cardie (2014), and the TREE - LSTM of (Tai et al., 2015). Convolutional network baselines include the dynamic convolutional network (Kalchbrenner et al., 2014, DCNN ) and the convolutional neural network multichannel (Kim, 2014, CNN - MC). Our other neural baselines are the sliding-window based paragraph vector (Le and Mikolov, 2014, PVEC)3 and 3 PVEC is computationally expensive at both training and test time and requires enough memory to store a vector for every paragraph in the training data. 1684 the word-represe"
P15-1162,P13-1045,0,0.0214218,"NNs) are syntactic functions that rely on natural language’s inherent structure to achieve state-of-the-art accuracies on sentiment analysis tasks (Tai et al., 2015). As in NBOW , each word type has an associated embedding. However, the composition function g now depends on a parse tree of the input sequence. The representation for any internal node in a binary parse tree is computed as a nonlinear function of the representations of its children (Figure 1, left). A more powerful RecNN variant is the recursive neural tensor network (RecNTN), which modifies g to include a costly tensor product (Socher et al., 2013b). While RecNNs can model complex linguistic phenomena like negation (Hermann et al., 2013), they require much more training time than NBOW models. The nonlinearities and matrix/tensor products at each node of the parse tree are expensive, especially as model dimensionality increases. R ec NN s also require an error signal at every node. One root softmax is not strong enough for the model to learn compositional relations and leads to worse accuracies than standard bag-of-words models (Li, 2014). Finally, RecNNs require relatively consistent syntax between training and test data due to their r"
P15-1162,D13-1170,0,0.156033,"NNs) are syntactic functions that rely on natural language’s inherent structure to achieve state-of-the-art accuracies on sentiment analysis tasks (Tai et al., 2015). As in NBOW , each word type has an associated embedding. However, the composition function g now depends on a parse tree of the input sequence. The representation for any internal node in a binary parse tree is computed as a nonlinear function of the representations of its children (Figure 1, left). A more powerful RecNN variant is the recursive neural tensor network (RecNTN), which modifies g to include a costly tensor product (Socher et al., 2013b). While RecNNs can model complex linguistic phenomena like negation (Hermann et al., 2013), they require much more training time than NBOW models. The nonlinearities and matrix/tensor products at each node of the parse tree are expensive, especially as model dimensionality increases. R ec NN s also require an error signal at every node. One root softmax is not strong enough for the model to learn compositional relations and leads to worse accuracies than standard bag-of-words models (Li, 2014). Finally, RecNNs require relatively consistent syntax between training and test data due to their r"
P15-1162,P15-1150,0,0.409406,"hit by a bus”, an unordered model like NBOW might be deceived by the word “entertained” to return a positive prediction. In contrast, syntactic composition functions rely on the order and structure of the input to learn how one word or phrase affects another, sacrificing computational efficiency in the process. In subsequent sections, we argue that this complexity is not matched by a corresponding gain in performance. Recursive neural networks (RecNNs) are syntactic functions that rely on natural language’s inherent structure to achieve state-of-the-art accuracies on sentiment analysis tasks (Tai et al., 2015). As in NBOW , each word type has an associated embedding. However, the composition function g now depends on a parse tree of the input sequence. The representation for any internal node in a binary parse tree is computed as a nonlinear function of the representations of its children (Figure 1, left). A more powerful RecNN variant is the recursive neural tensor network (RecNTN), which modifies g to include a costly tensor product (Socher et al., 2013b). While RecNNs can model complex linguistic phenomena like negation (Hermann et al., 2013), they require much more training time than NBOW model"
P15-1162,D14-1004,0,0.00882624,"Missing"
P15-1162,P12-2018,0,0.150115,"014, CNN - MC). Our other neural baselines are the sliding-window based paragraph vector (Le and Mikolov, 2014, PVEC)3 and 3 PVEC is computationally expensive at both training and test time and requires enough memory to store a vector for every paragraph in the training data. 1684 the word-representation restricted Boltzmann machine (Dahl et al., 2012, WRRBM), which only works on the document-level IMDB task.4 4.1.2 Non-Neural Baselines We also compare to non-neural baselines, specifically the bigram na¨ıve Bayes (BINB) and na¨ıve Bayes support vector machine (NBSVM - BI) models introduced by Wang and Manning (2012), both of which are memory-intensive due to huge feature spaces of size |V |2 . 4.1.3 DAN Configurations In Table 1, we compare a variety of DAN and NBOW configurations5 to the baselines described above. In particular, we are interested in not only comparing DAN accuracies to those of the baselines, but also how initializing with pretrained embeddings and restricting the model to only root-level labels affects performance. With this in mind, the NBOW- RAND and DAN - RAND models are initialized with random 300-dimensional word embeddings, while the other models are initialized with publicly-ava"
P17-1167,N16-1181,0,0.0369064,"ring function. In our design, the score of a state is determined by the scores of actions taken from the initial state to the target state, which are predicted by different neural network modules based on action type. By leveraging a margin-based objective function, the model learning procedure resembles several structured-output learning algorithms such as structured SVMs (Tsochantaridis et al., 2005), but can take either strong or weak supervision seamlessly. DynSP is inspired by STAGG, a search-based semantic parser (Yih et al., 2015), as well as the dynamic neural module network (DNMN) of Andreas et al. (2016). Much like STAGG, DynSP chains together different modules as search progresses; however, these modules are implemented as neural networks, which enables end-to-end training as in DNMN. The key difference between DynSP and DNMN is that in DynSP the network structure of an example is not predetermined. Instead, different network structures are constructed dynamically as our learning procedure explores the state space. It is straightforward to answer sequential questions using our framework: we allow the model to take the previous question and its answers as input, with a slightly modified actio"
P17-1167,D11-1039,0,0.0126661,"y implemented in our language due to concerns with the search space size. Increasing the number of complex actions requires designing smarter optimization procedures, which we leave to future work. 5 Related Work Previous work on conversational QA has focused on small, single-domain datasets. Perhaps most related to our task is the context-dependent sentence analysis described in (Zettlemoyer and Collins, 2009), where conversations between customers and travel agents are mapped to logical forms after resolving referential expressions. Another dataset of travel booking conversations is used by Artzi and Zettlemoyer (2011) to learn a semantic parser for complicated queries given user clarifications. More recently, Long et al. (2016) collect three contextual semantic parsing datasets (from synthetic domains) that contain coreferences to entities and MAX SUBSEQUENT 1 SUBSEQUENT 2 Figure 3: Parses computed by DynSP for three test sequences (actions in blue boxes, values from table in white boxes). Top: all three questions are parsed correctly. Middle: semantic matching errors cause the model to select incorrect columns and conditions. Bottom: The final question is unanswerable due to limitations of our parse langu"
P17-1167,D14-1179,0,0.00414407,"Missing"
P17-1167,P05-1026,0,0.0994602,"rse language. actions. We differentiate ourselves from these prior works in two significant ways: first, our dataset is not restricted to a particular domain, and second, a major goal of our work is to analyze the different types of sequence progressions people create when they are trying to express a complicated intent. Complex, interactive QA tasks have also been proposed in the information retrieval community, where the data source is a corpus of newswire text (Kelly and Lin, 2007). We also build on aspects of some existing interactive question-answering systems. For example, the system of Harabagiu et al. (2005) includes a module that predicts what a user will ask next given their current question. Other than FP and NP, the work of Neural Symbolic Machines (NSM) (Liang et al., 2017) is perhaps the closest to ours. NSM aims to generate formal semantic parses of questions that can be executed on Freebase to retrieve answers, and is trained using the REINFORCE algorithm (Williams, 1992) augmented with approximate gold parses found in a separate curriculum learning stage. In comparison, finding reference parses is an integral part of our algorithm. Our non1828 probabilistic, margin-based objective functi"
P17-1167,P17-1003,0,0.117723,"goal of our work is to analyze the different types of sequence progressions people create when they are trying to express a complicated intent. Complex, interactive QA tasks have also been proposed in the information retrieval community, where the data source is a corpus of newswire text (Kelly and Lin, 2007). We also build on aspects of some existing interactive question-answering systems. For example, the system of Harabagiu et al. (2005) includes a module that predicts what a user will ask next given their current question. Other than FP and NP, the work of Neural Symbolic Machines (NSM) (Liang et al., 2017) is perhaps the closest to ours. NSM aims to generate formal semantic parses of questions that can be executed on Freebase to retrieve answers, and is trained using the REINFORCE algorithm (Williams, 1992) augmented with approximate gold parses found in a separate curriculum learning stage. In comparison, finding reference parses is an integral part of our algorithm. Our non1828 probabilistic, margin-based objective function also helps avoid the need for empirical tricks to handle normalization and proper sampling, which are crucial when applying REINFORCE in practice. 6 Conclusion & Future Wo"
P17-1167,P16-1138,0,0.279417,"s designing smarter optimization procedures, which we leave to future work. 5 Related Work Previous work on conversational QA has focused on small, single-domain datasets. Perhaps most related to our task is the context-dependent sentence analysis described in (Zettlemoyer and Collins, 2009), where conversations between customers and travel agents are mapped to logical forms after resolving referential expressions. Another dataset of travel booking conversations is used by Artzi and Zettlemoyer (2011) to learn a semantic parser for complicated queries given user clarifications. More recently, Long et al. (2016) collect three contextual semantic parsing datasets (from synthetic domains) that contain coreferences to entities and MAX SUBSEQUENT 1 SUBSEQUENT 2 Figure 3: Parses computed by DynSP for three test sequences (actions in blue boxes, values from table in white boxes). Top: all three questions are parsed correctly. Middle: semantic matching errors cause the model to select incorrect columns and conditions. Bottom: The final question is unanswerable due to limitations of our parse language. actions. We differentiate ourselves from these prior works in two significant ways: first, our dataset is n"
P17-1167,P09-1110,0,0.0506913,"e surface, the final question in the bottom sequence of Figure 3 is one such example; the correct semantic parse requires access to the answers of both the first and second question, actions that we have not currently implemented in our language due to concerns with the search space size. Increasing the number of complex actions requires designing smarter optimization procedures, which we leave to future work. 5 Related Work Previous work on conversational QA has focused on small, single-domain datasets. Perhaps most related to our task is the context-dependent sentence analysis described in (Zettlemoyer and Collins, 2009), where conversations between customers and travel agents are mapped to logical forms after resolving referential expressions. Another dataset of travel booking conversations is used by Artzi and Zettlemoyer (2011) to learn a semantic parser for complicated queries given user clarifications. More recently, Long et al. (2016) collect three contextual semantic parsing datasets (from synthetic domains) that contain coreferences to entities and MAX SUBSEQUENT 1 SUBSEQUENT 2 Figure 3: Parses computed by DynSP for three test sequences (actions in blue boxes, values from table in white boxes). Top: a"
P17-1167,D17-1252,1,0.0819152,"time to converge. In this work, we propose a conceptually simple learning algorithm for weakly supervised training that sidesteps the inefficient learning problem. Our key insight is to conduct inference using a beam search procedure guided by an approximate reward function. The search procedure is executed twice for each training example, one for finding the best possible reference semantic parse and the other for finding the predicted semantic parse to update the model. Our framework is suitable for learning from either implicit or explicit supervision, and is detailed in a companion paper (Peng et al., 2017). Below we describe how we adapt it to the semantic parsing problem in this work. Approximate reward Let A(s) be the answers retrieved by executing the semantic parse represented by state s, and let A∗ be the set of gold answers of a given question. We define the reward R(s; A∗ ) = 1[A(s) = A∗ ], or the accuracy of the retrieved answers. We use R(s) as the abbreviation for R(s; A∗ ). A state s with R(s) = 1 is called a goal state. Directly using this reward function in search of goal states can be difficult, as rewards of most states are 0. However, even when the answers from a semantic parse"
P17-1167,D14-1162,0,0.11017,"nal constraints. 4.1 DynSP implementation details Unlike previous dynamic neural network frameworks (Andreas et al., 2016; Looks et al., 2017), where each example can have different but predetermined structure, DynSP needs to dynamically explores and constructs different neural network structures for each question. Therefore, we choose DyNet (Neubig et al., 2017) as our implementation platform for its flexibility in composing computation graphs. We optimize our model parameters using standard stochastic gradient descent. The word embeddings are initialized with 100-d pretrained GloVe vectors (Pennington et al., 2014) and fine-tuned during training with dropout rate 0.5. For follow-up questions, we choose uniformly at random to use either gold answers to the previous question or the model’s previous predictions.8 We constrain the maximum length of actions to 3 for computational efficiency and set the beam size to 15 in our reported models, as accuracy gains are negligible with larger beam sizes. We train our model for 30 epochs, although the best model on the validation set is usually found within the first 20 epochs. Only CPU is used in model training, and each epoch in the beam size 15 setting takes abou"
P17-1167,P15-1128,1,0.52009,"se. The quality of the induced semantic parse obviously depends on the scoring function. In our design, the score of a state is determined by the scores of actions taken from the initial state to the target state, which are predicted by different neural network modules based on action type. By leveraging a margin-based objective function, the model learning procedure resembles several structured-output learning algorithms such as structured SVMs (Tsochantaridis et al., 2005), but can take either strong or weak supervision seamlessly. DynSP is inspired by STAGG, a search-based semantic parser (Yih et al., 2015), as well as the dynamic neural module network (DNMN) of Andreas et al. (2016). Much like STAGG, DynSP chains together different modules as search progresses; however, these modules are implemented as neural networks, which enables end-to-end training as in DNMN. The key difference between DynSP and DNMN is that in DynSP the network structure of an example is not predetermined. Instead, different network structures are constructed dynamically as our learning procedure explores the state space. It is straightforward to answer sequential questions using our framework: we allow the model to take"
P17-1167,P16-2033,1,0.0431227,"cores of actions in the sequence, the goal of model optimization is to learn the parameters in the neural networks behind the policy function. Let θ be the collection of all the model parameters. Then thePstate value function can be written as: Vθ (st ) = ti=1 πθ (si−1 , ai ). In a fully supervised setting where the correct semantic parse of each question is available, learning the policy function can be reduced to a sequence prediction problem. However, while having full supervision leads to a better semantic parser, collecting the correct parses requires a much more sophisticated UI design (Yih et al., 2016). In many scenarios, such as the one in the SQA dataset, it is often the case that only the answers to the questions are available. Adapting a learning algorithm to this weakly supervised setting is thus critical. Generally speaking, weakly supervised semantic parsers operate on one assumption — a candidate semantic parse is treated as a correct one if it results in answers that are identical to the gold answers. Therefore, a straightforward modification of existing structured learning algorithms in our setting is to use any semantic parse found to evaluate to the correct answers during beam s"
P17-1167,P15-1142,0,\N,Missing
P19-1122,D16-1139,0,0.040255,"t with multiple decoders (Lee et al., 2018). As fully non-autoregressive decoding results in poor translation quality, another class of methods produce k tokens at a single time step where 1 < k < m. The semi-autoregressive Transformer (SAT) of Wang et al. (2018) produces a fixed k tokens per time step, thus modifying the target sequence probability to: p(t1,··· , tm |s) = |G| Y p(Gt |G1,··· , Gt−1 , x), i=1 where each of G1,··· , Gb m−1 c+1 is a group of k contiguous non-overlapping target tokens of the form ti,··· , ti+k . In conjunction with training techniques like knowledge distillation (Kim and Rush, 2016) and initialization with an autoregressive model, SATs maintain better translation quality than non-autoregressive approaches with competitive speedups. Stern et al. (2018) follow a similar approach but dynamically select a different k at each step, which results in further quality improvements with a corresponding decrease in speed. 2.4 Latent Transformer While current semi-autoregressive methods achieve both better quality and faster speedups than their non-autoregressive counterparts, largely due to the number of tricks required to train the latter, the 1271 theoretical speedup for non-auto"
P19-1122,W06-1628,0,0.0655215,"Missing"
P19-1122,D18-1149,0,0.462127,"h predict each token ti in the target language one by one conditioned on all previously-generated target tokens t1···i−1 and the source sentence s. For downstream applications of NMT that prioritize low latency (e.g., real-time translation), autoregressive decoding proves expensive, as decoding time in stateof-the-art attentional models such as the Transformer (Vaswani et al., 2017) scales quadratically with the number of target tokens. In order to speed up test-time translation, nonautoregressive decoding methods produce all target tokens at once independently of each other (Gu et al., 2018; Lee et al., 2018), while semiautoregressive decoding (Wang et al., 2018; Stern et al., 2018) trades off speed for quality by reducing (but not completely eliminating) the number of sequential computations in the decoder (Figure 1). We choose the latent Transformer (LT) of Kaiser et al. (2018) as a starting point, which merges both of these approaches by autoregressively generating a short sequence of discrete latent variables before non-autoregressively producing all target tokens conditioned on the generated latent sequence. Kaiser et al. (2018) experiment with increasingly complex ways of learning their disc"
P19-1122,N19-1116,1,0.836599,", a variant of the Transformer architecture that achieves decoding speedups by autoregressively generating a constituency chunk sequence before non-autoregressively producing all tokens in the target sentence. Controlled experiments show that SynST outperforms competing non- and semi-autoregressive approaches in terms of both BLEU and wall-clock speedup on En-De and En-Fr language pairs. While our method is currently restricted to languages that have reliable constituency parsers, an exciting future direction is to explore unsupervised tree induction methods for low-resource target languages (Drozdov et al., 2019). Finally, we hope that future work in this area will follow our lead in using carefully-controlled experiments to enable meaningful comparisons. Acknowledgements We thank the anonymous reviewers for their insightful comments. We also thank Justin Payan and the rest of the UMass NLP group for helpful comments on earlier drafts. Finally, we thank Weiqiu You for additional experimentation efforts. 1277 References Roee Aharoni and Yoav Goldberg. 2017. Towards string-to-tree neural machine translation. In Proceedings of the Association for Computational Linguistics. Dzmitry Bahdanau, Kyunghyun Cho"
P19-1122,D18-1336,0,0.0365532,"iguchi et al., 2017) directly motivated this work. 6 7 Related Work Our work builds on the existing body of literature in both fast decoding methods for neural generation models as well as syntax-based MT; we review each area below. 6.1 Fast neural decoding While all of the prior work described in Section 2 is relatively recent, non-autoregressive methods for decoding in NMT have been around for longer, although none relies on syntax like SynST. Schwenk (2012) translate short phrases non-autoregressively, while Kaiser and Bengio (2016) implement a nonautoregressive neural GPU architecture and Libovick and Helcl (2018) explore a CTC approach. Guo et al. (2019) use phrase tables and word-level adversarial methods to improve upon the NAT model of Gu et al. (2018), while Wang et al. (2019) regularize NAT by introducing similarity and backtranslation terms to the training objective. 6.2 Syntax-based translation There is a rich history of integrating syntax into machine translation systems. Wu (1997) pioneered Conclusions & Future Work We propose SynST, a variant of the Transformer architecture that achieves decoding speedups by autoregressively generating a constituency chunk sequence before non-autoregressivel"
P19-1122,P17-2012,0,0.0672742,"ate the setup of Kaiser et al. (2018) to the best of our ability, other work in this area does not adhere to the same set of datasets, base models, or “training tricks”, so a legitimate comparison with published results is difficult. For a more rigorous comparison, we re-implement another related model within our framework, the semiautoregressive transformer (SAT) of Wang et al. (2018), and observe improvements in BLEU and decoding speed on both En↔ De and En→ Fr language pairs (Section 4). While we build on a rich line of work that integrates syntax into both NMT (Aharoni and Goldberg, 2017; Eriguchi et al., 2017) and other language processing tasks (Strubell et al., 2018; Swayamdipta et al., 2018), we aim to use syntax to speed up decoding, not improve downstream performance (i.e., translation quality). An in-depth analysis (Section 5) reveals that syntax is a powerful abstraction for non-autoregressive translation: for example, removing information about the constituent type of each chunk results in a drop of 15 BLEU on IWSLT En→De. 2 Decoding in Transformers Our work extends the Transformer architecture (Vaswani et al., 2017), which is an instance of the encoder-decoder framework for language genera"
P19-1122,P06-1121,0,0.0645936,"he same sentence during training allows it to choose a sequence of chunks that has a higher likelihood at test time, improving BLEU by 1.5 while decreasing the speedup from 3.8× to 3.1×; this is an exciting result for future work (see Table A3 for additional analysis). this direction by proposing an inverse transduction grammar for building word aligners. Yamada and Knight (2001) convert an externally-derived source parse tree to a target sentence, the reverse of what we do with SynST’s parse decoder; later, other variations such as string-to-tree and tree-to-tree translation models followed (Galley et al., 2006; Cowan et al., 2006). The Hiero system of Chiang (2005) employs a learned synchronous context free grammar within phrase-based translation, which follow-up work augmented with syntactic supervision (Zollmann and Venugopal, 2006; Marton and Resnik, 2008; Chiang et al., 2008). Syntax took a back seat with the advent of neural MT, as early sequence to sequence models (Sutskever et al., 2014; Luong et al., 2015) focused on architectures and optimization. Sennrich and Haddow (2016) demonstrate that augmenting word embeddings with dependency relations helps NMT, while Shi et al. (2016) show that NM"
P19-1122,D15-1166,0,0.0689436,"source parse tree to a target sentence, the reverse of what we do with SynST’s parse decoder; later, other variations such as string-to-tree and tree-to-tree translation models followed (Galley et al., 2006; Cowan et al., 2006). The Hiero system of Chiang (2005) employs a learned synchronous context free grammar within phrase-based translation, which follow-up work augmented with syntactic supervision (Zollmann and Venugopal, 2006; Marton and Resnik, 2008; Chiang et al., 2008). Syntax took a back seat with the advent of neural MT, as early sequence to sequence models (Sutskever et al., 2014; Luong et al., 2015) focused on architectures and optimization. Sennrich and Haddow (2016) demonstrate that augmenting word embeddings with dependency relations helps NMT, while Shi et al. (2016) show that NMT systems do not automatically learn subtle syntactic properties. Stahlberg et al. (2016) incorporate Hiero’s translation grammar into NMT systems with improvements; similar follow-up results (Aharoni and Goldberg, 2017; Eriguchi et al., 2017) directly motivated this work. 6 7 Related Work Our work builds on the existing body of literature in both fast decoding methods for neural generation models as well as"
P19-1122,P14-5010,0,0.0056768,"from Moses SMT. 8 SacreBLEU signature: BLEU+case.mixed+lang.LANG +numrefs.1+smooth.exp+test.TEST+tok.intl+version.1.2.11, with LANG ∈ {en-de, de-en, en-fr} and TEST ∈ {wmt14/full, iwslt2017/tst2013} 9 We attempted to use the publicly available code in Tensor2Tensor, but were unable to successfully train a model. 10 The published SAT results use knowledge distillation and k = 6 and compare this model to the SAT trained with k = 2, 4, 6. 4.2 Datasets We experiment with English-German and EnglishFrench datasets, relying on constituency parsers in all three languages. We use the Stanford CoreNLP (Manning et al., 2014) shift-reduce parsers for English, German, and French. For English-German, we evaluate on WMT 2014 En↔De as well as IWSLT 2016 En→De, while for English-French we train on the Europarl / Common Crawl subset of the full WMT 2014 En→Fr data and evaluate over the full dev/test sets. WMT 2014 En↔De consists of around 4.5 million sentence pairs encoded using byte pair encoding (Sennrich et al., 2016) with a shared source-target vocabulary of roughly 37000 tokens. We use the same preprocessed dataset used in the original Transformer paper and also by many subsequent papers that have investigated impr"
P19-1122,P08-1114,0,0.0182986,"dditional analysis). this direction by proposing an inverse transduction grammar for building word aligners. Yamada and Knight (2001) convert an externally-derived source parse tree to a target sentence, the reverse of what we do with SynST’s parse decoder; later, other variations such as string-to-tree and tree-to-tree translation models followed (Galley et al., 2006; Cowan et al., 2006). The Hiero system of Chiang (2005) employs a learned synchronous context free grammar within phrase-based translation, which follow-up work augmented with syntactic supervision (Zollmann and Venugopal, 2006; Marton and Resnik, 2008; Chiang et al., 2008). Syntax took a back seat with the advent of neural MT, as early sequence to sequence models (Sutskever et al., 2014; Luong et al., 2015) focused on architectures and optimization. Sennrich and Haddow (2016) demonstrate that augmenting word embeddings with dependency relations helps NMT, while Shi et al. (2016) show that NMT systems do not automatically learn subtle syntactic properties. Stahlberg et al. (2016) incorporate Hiero’s translation grammar into NMT systems with improvements; similar follow-up results (Aharoni and Goldberg, 2017; Eriguchi et al., 2017) directly"
P19-1122,W18-6319,0,0.0172772,"directly comparable to the other results. • • • • chitectural upgrades.7 We use all of the hyperparameter values from the original Transformer paper and do not attempt to tune them further, except for: (1) the number of layers in the parse decoder, (2) the decoders do not use label smoothing. We do not use sequence-level knowledge distillation, which augments the training data with translations produced by an external autoregressive model. The choice of model used for distillation plays a part in the final BLEU score, so we remove this variable. We report all our BLEU numbers using sacreBLEU (Post, 2018) to ensure comparability with future work.8 We report wall-clock speedups by measuring the average time to decode one sentence (batch size of one) in the dev/test set. As the code for LT is not readily available9 , we also reimplement the SAT model using our setup, as it is the most similar model outside of LT to our own.10 For SynST, we set the maximum chunk size 7 As the popular Tensor2Tensor implementation is constantly being tweaked, we instead re-implement the Transformer as originally published and verify that its results closely match the published ones. Our implementation achieves a BL"
P19-1122,C12-2104,0,0.0350591,"rg et al. (2016) incorporate Hiero’s translation grammar into NMT systems with improvements; similar follow-up results (Aharoni and Goldberg, 2017; Eriguchi et al., 2017) directly motivated this work. 6 7 Related Work Our work builds on the existing body of literature in both fast decoding methods for neural generation models as well as syntax-based MT; we review each area below. 6.1 Fast neural decoding While all of the prior work described in Section 2 is relatively recent, non-autoregressive methods for decoding in NMT have been around for longer, although none relies on syntax like SynST. Schwenk (2012) translate short phrases non-autoregressively, while Kaiser and Bengio (2016) implement a nonautoregressive neural GPU architecture and Libovick and Helcl (2018) explore a CTC approach. Guo et al. (2019) use phrase tables and word-level adversarial methods to improve upon the NAT model of Gu et al. (2018), while Wang et al. (2019) regularize NAT by introducing similarity and backtranslation terms to the training objective. 6.2 Syntax-based translation There is a rich history of integrating syntax into machine translation systems. Wu (1997) pioneered Conclusions & Future Work We propose SynST,"
P19-1122,W16-2209,0,0.0273253,"do with SynST’s parse decoder; later, other variations such as string-to-tree and tree-to-tree translation models followed (Galley et al., 2006; Cowan et al., 2006). The Hiero system of Chiang (2005) employs a learned synchronous context free grammar within phrase-based translation, which follow-up work augmented with syntactic supervision (Zollmann and Venugopal, 2006; Marton and Resnik, 2008; Chiang et al., 2008). Syntax took a back seat with the advent of neural MT, as early sequence to sequence models (Sutskever et al., 2014; Luong et al., 2015) focused on architectures and optimization. Sennrich and Haddow (2016) demonstrate that augmenting word embeddings with dependency relations helps NMT, while Shi et al. (2016) show that NMT systems do not automatically learn subtle syntactic properties. Stahlberg et al. (2016) incorporate Hiero’s translation grammar into NMT systems with improvements; similar follow-up results (Aharoni and Goldberg, 2017; Eriguchi et al., 2017) directly motivated this work. 6 7 Related Work Our work builds on the existing body of literature in both fast decoding methods for neural generation models as well as syntax-based MT; we review each area below. 6.1 Fast neural decoding W"
P19-1122,P16-1162,0,0.0647659,"model to the SAT trained with k = 2, 4, 6. 4.2 Datasets We experiment with English-German and EnglishFrench datasets, relying on constituency parsers in all three languages. We use the Stanford CoreNLP (Manning et al., 2014) shift-reduce parsers for English, German, and French. For English-German, we evaluate on WMT 2014 En↔De as well as IWSLT 2016 En→De, while for English-French we train on the Europarl / Common Crawl subset of the full WMT 2014 En→Fr data and evaluate over the full dev/test sets. WMT 2014 En↔De consists of around 4.5 million sentence pairs encoded using byte pair encoding (Sennrich et al., 2016) with a shared source-target vocabulary of roughly 37000 tokens. We use the same preprocessed dataset used in the original Transformer paper and also by many subsequent papers that have investigated improving decoding speed, evaluating on the newstest2013 dataset for validation and the newstest2014 dataset for testing. For the IWSLT dataset we use tst2013 for validation and utilize the same hyperparameters as Lee et al. (2018). 4.3 Results Table 1 contains the results on all four datasets. SynST achieves speedups of ∼ 4 − 5× that of the vanilla Transformer, which is larger than nearly all diff"
P19-1122,J97-3002,0,0.014579,"onger, although none relies on syntax like SynST. Schwenk (2012) translate short phrases non-autoregressively, while Kaiser and Bengio (2016) implement a nonautoregressive neural GPU architecture and Libovick and Helcl (2018) explore a CTC approach. Guo et al. (2019) use phrase tables and word-level adversarial methods to improve upon the NAT model of Gu et al. (2018), while Wang et al. (2019) regularize NAT by introducing similarity and backtranslation terms to the training objective. 6.2 Syntax-based translation There is a rich history of integrating syntax into machine translation systems. Wu (1997) pioneered Conclusions & Future Work We propose SynST, a variant of the Transformer architecture that achieves decoding speedups by autoregressively generating a constituency chunk sequence before non-autoregressively producing all tokens in the target sentence. Controlled experiments show that SynST outperforms competing non- and semi-autoregressive approaches in terms of both BLEU and wall-clock speedup on En-De and En-Fr language pairs. While our method is currently restricted to languages that have reliable constituency parsers, an exciting future direction is to explore unsupervised tree"
P19-1122,P01-1067,0,0.127533,"T ), where T is the target sentence length (to ensure short inputs do not collapse into a single chunk) and randomly sampling k ∈ {1 . . . 6}. The final row of Table 4 shows that exposing the parse decoder to multiple possible chunkings of the same sentence during training allows it to choose a sequence of chunks that has a higher likelihood at test time, improving BLEU by 1.5 while decreasing the speedup from 3.8× to 3.1×; this is an exciting result for future work (see Table A3 for additional analysis). this direction by proposing an inverse transduction grammar for building word aligners. Yamada and Knight (2001) convert an externally-derived source parse tree to a target sentence, the reverse of what we do with SynST’s parse decoder; later, other variations such as string-to-tree and tree-to-tree translation models followed (Galley et al., 2006; Cowan et al., 2006). The Hiero system of Chiang (2005) employs a learned synchronous context free grammar within phrase-based translation, which follow-up work augmented with syntactic supervision (Zollmann and Venugopal, 2006; Marton and Resnik, 2008; Chiang et al., 2008). Syntax took a back seat with the advent of neural MT, as early sequence to sequence mo"
P19-1122,W06-3119,0,0.0131095,"uture work (see Table A3 for additional analysis). this direction by proposing an inverse transduction grammar for building word aligners. Yamada and Knight (2001) convert an externally-derived source parse tree to a target sentence, the reverse of what we do with SynST’s parse decoder; later, other variations such as string-to-tree and tree-to-tree translation models followed (Galley et al., 2006; Cowan et al., 2006). The Hiero system of Chiang (2005) employs a learned synchronous context free grammar within phrase-based translation, which follow-up work augmented with syntactic supervision (Zollmann and Venugopal, 2006; Marton and Resnik, 2008; Chiang et al., 2008). Syntax took a back seat with the advent of neural MT, as early sequence to sequence models (Sutskever et al., 2014; Luong et al., 2015) focused on architectures and optimization. Sennrich and Haddow (2016) demonstrate that augmenting word embeddings with dependency relations helps NMT, while Shi et al. (2016) show that NMT systems do not automatically learn subtle syntactic properties. Stahlberg et al. (2016) incorporate Hiero’s translation grammar into NMT systems with improvements; similar follow-up results (Aharoni and Goldberg, 2017; Eriguch"
P19-1122,D16-1159,0,0.0232021,"followed (Galley et al., 2006; Cowan et al., 2006). The Hiero system of Chiang (2005) employs a learned synchronous context free grammar within phrase-based translation, which follow-up work augmented with syntactic supervision (Zollmann and Venugopal, 2006; Marton and Resnik, 2008; Chiang et al., 2008). Syntax took a back seat with the advent of neural MT, as early sequence to sequence models (Sutskever et al., 2014; Luong et al., 2015) focused on architectures and optimization. Sennrich and Haddow (2016) demonstrate that augmenting word embeddings with dependency relations helps NMT, while Shi et al. (2016) show that NMT systems do not automatically learn subtle syntactic properties. Stahlberg et al. (2016) incorporate Hiero’s translation grammar into NMT systems with improvements; similar follow-up results (Aharoni and Goldberg, 2017; Eriguchi et al., 2017) directly motivated this work. 6 7 Related Work Our work builds on the existing body of literature in both fast decoding methods for neural generation models as well as syntax-based MT; we review each area below. 6.1 Fast neural decoding While all of the prior work described in Section 2 is relatively recent, non-autoregressive methods for de"
P19-1122,P16-2049,0,0.0192366,"learned synchronous context free grammar within phrase-based translation, which follow-up work augmented with syntactic supervision (Zollmann and Venugopal, 2006; Marton and Resnik, 2008; Chiang et al., 2008). Syntax took a back seat with the advent of neural MT, as early sequence to sequence models (Sutskever et al., 2014; Luong et al., 2015) focused on architectures and optimization. Sennrich and Haddow (2016) demonstrate that augmenting word embeddings with dependency relations helps NMT, while Shi et al. (2016) show that NMT systems do not automatically learn subtle syntactic properties. Stahlberg et al. (2016) incorporate Hiero’s translation grammar into NMT systems with improvements; similar follow-up results (Aharoni and Goldberg, 2017; Eriguchi et al., 2017) directly motivated this work. 6 7 Related Work Our work builds on the existing body of literature in both fast decoding methods for neural generation models as well as syntax-based MT; we review each area below. 6.1 Fast neural decoding While all of the prior work described in Section 2 is relatively recent, non-autoregressive methods for decoding in NMT have been around for longer, although none relies on syntax like SynST. Schwenk (2012) t"
P19-1122,D18-1548,0,0.0255622,"lity, other work in this area does not adhere to the same set of datasets, base models, or “training tricks”, so a legitimate comparison with published results is difficult. For a more rigorous comparison, we re-implement another related model within our framework, the semiautoregressive transformer (SAT) of Wang et al. (2018), and observe improvements in BLEU and decoding speed on both En↔ De and En→ Fr language pairs (Section 4). While we build on a rich line of work that integrates syntax into both NMT (Aharoni and Goldberg, 2017; Eriguchi et al., 2017) and other language processing tasks (Strubell et al., 2018; Swayamdipta et al., 2018), we aim to use syntax to speed up decoding, not improve downstream performance (i.e., translation quality). An in-depth analysis (Section 5) reveals that syntax is a powerful abstraction for non-autoregressive translation: for example, removing information about the constituent type of each chunk results in a drop of 15 BLEU on IWSLT En→De. 2 Decoding in Transformers Our work extends the Transformer architecture (Vaswani et al., 2017), which is an instance of the encoder-decoder framework for language generation that uses stacked layers of self-attention to both enc"
P19-1122,D18-1412,0,0.0283664,"s area does not adhere to the same set of datasets, base models, or “training tricks”, so a legitimate comparison with published results is difficult. For a more rigorous comparison, we re-implement another related model within our framework, the semiautoregressive transformer (SAT) of Wang et al. (2018), and observe improvements in BLEU and decoding speed on both En↔ De and En→ Fr language pairs (Section 4). While we build on a rich line of work that integrates syntax into both NMT (Aharoni and Goldberg, 2017; Eriguchi et al., 2017) and other language processing tasks (Strubell et al., 2018; Swayamdipta et al., 2018), we aim to use syntax to speed up decoding, not improve downstream performance (i.e., translation quality). An in-depth analysis (Section 5) reveals that syntax is a powerful abstraction for non-autoregressive translation: for example, removing information about the constituent type of each chunk results in a drop of 15 BLEU on IWSLT En→De. 2 Decoding in Transformers Our work extends the Transformer architecture (Vaswani et al., 2017), which is an instance of the encoder-decoder framework for language generation that uses stacked layers of self-attention to both encode a source sentence and d"
P19-1122,D18-1044,0,0.0629748,"Missing"
P19-1224,P17-1123,0,0.454475,"documents into specificity-based hierarchies of QA pairs. A4: A pipelined system to tackle SQUASH along with crowdsourced methods to evaluate it. Q: How can the community build on this work? A: We have released our codebase, crowdsourcing templates for evaluation, and a live demonstration of our system at http://squash.cs. umass.edu/. Additionally, we outline guidelines for future work in Section 7. 2 Obtaining training data for SQUASH The proliferation of reading comprehension datasets like SQuAD (Rajpurkar et al., 2016, 2018) has enabled state-of-the-art neural question generation systems (Du et al., 2017; Kim et al., 2018). However, these systems are trained for individual question generation, while the goal of SQUASH is to produce a general-to-specific hierarchy of QA pairs. Recently-released conversational QA datasets like QuAC (Choi et al., 2018) and CoQA (Reddy et al., 2018) contain a sequential arrangement of QA pairs, but question specificity is not explicitly marked.2 Motivated by the lack of hierarchical QA datasets, we automatically classify questions in SQuAD, QuAC and CoQA according to their specificity using a combination of rule-based and automatic approaches. 2.1 Rules for speci"
P19-1224,P18-1082,0,0.0392375,"ions (e.g., “where was he born?”) are duplicated many times in the dataset; we downsample such questions to a maximum limit of 10. Finally, we preprocess both paragraphs and questions using byte-pair encoding (Sennrich et al., 2016). class labels as described in Section 3.1. To promote diversity, we over-generate prospective candidates (Heilman and Smith, 2010) for every answer span and later prune them. Specifically, we use beam search with a beam size of 3 to generate three highly-probable question candidates. As these candidates are often generic, we additionally use top-k random sampling (Fan et al., 2018) with k = 10, a recently-proposed diversity-promoting decoding algorithm, to generate ten more question candidates per answer span. Hence, for every answer span we generate 13 question candidates. We discuss issues with using just standard beam search for question generation in Section 5.1. 3.3 Answering generated questions Architecture details: We use a two-layer biLSTM encoder and a single-layer LSTM (Hochreiter and Schmidhuber, 1997) decoder with soft attention (Bahdanau et al., 2015) to generate questions, similar to Du et al. (2017). Our architecture is augmented with a copy mechanism (Se"
P19-1224,W18-2501,0,0.0274331,"Missing"
P19-1224,N10-1086,0,0.0311499,"kar, 2019). In all datasets, we remove unanswerable questions and questions whose answers span multiple paragraphs. A few very generic questions (e.g. “what happened in this article?”) were manually identified removed from the training dataset. Some other questions (e.g., “where was he born?”) are duplicated many times in the dataset; we downsample such questions to a maximum limit of 10. Finally, we preprocess both paragraphs and questions using byte-pair encoding (Sennrich et al., 2016). class labels as described in Section 3.1. To promote diversity, we over-generate prospective candidates (Heilman and Smith, 2010) for every answer span and later prune them. Specifically, we use beam search with a beam size of 3 to generate three highly-probable question candidates. As these candidates are often generic, we additionally use top-k random sampling (Fan et al., 2018) with k = 10, a recently-proposed diversity-promoting decoding algorithm, to generate ten more question candidates per answer span. Hence, for every answer span we generate 13 question candidates. We discuss issues with using just standard beam search for question generation in Section 5.1. 3.3 Answering generated questions Architecture details"
P19-1224,P18-1152,0,0.0630042,"Missing"
P19-1224,D14-1181,0,0.00866301,"Missing"
P19-1224,D15-1166,0,0.0207483,"Missing"
P19-1224,N18-1202,1,0.659189,"Missing"
P19-1224,C69-0201,0,0.267195,"list is provided in Table 1.4 Classifying questions not covered by templates: If a question does not satisfy any template or rule, how do we assign it a label? We manage to clas2 “Teachers” in the QuAC set-up can encourage “students” to ask a follow-up question, but we cannot use these annotations to infer a hierarchy because students are not required to actually follow their teachers’ directions. 3 We add a third category for YES - NO questions as they are difficult to classify as either GENERAL or SPECIFIC. 4 Questions in Lehnert (1978) were classified using a conceptual dependency parser (Schank, 1972). We could not find a modern implementation of this parser and thus decided to use a rule-based approach that relies on spaCy 2.0 (Honnibal and Montani, 2017) for all preprocessing. 2322 Conceptual class Specificity Question asks for... Sample templates Causal Antecendent, Goal Oriented, Enablement, Causal Consequent, Expectational GENERAL the reason for occurrence of an event and the consequences of it Why ..., What happened after / before ..., What was the cause / reason / purpose ..., What led to ... Instrumental GENERAL a procedure / mechanism How question with VERB parent for How in depen"
P19-1224,P17-1099,0,0.0330935,"8) with k = 10, a recently-proposed diversity-promoting decoding algorithm, to generate ten more question candidates per answer span. Hence, for every answer span we generate 13 question candidates. We discuss issues with using just standard beam search for question generation in Section 5.1. 3.3 Answering generated questions Architecture details: We use a two-layer biLSTM encoder and a single-layer LSTM (Hochreiter and Schmidhuber, 1997) decoder with soft attention (Bahdanau et al., 2015) to generate questions, similar to Du et al. (2017). Our architecture is augmented with a copy mechanism (See et al., 2017) over the encoded paragraph representations. Answer spans are marked with &lt;SOA&gt; and &lt;EOA&gt; tokens in the paragraph, and representations for tokens within the answer span are attended to by a separate attention head. We condition the decoder on the specificity class (GENERAL, SPECIFIC and YES - NO)8 by concatenating an embedding for the ground-truth class to the input of each time step. We implement models in PyTorch v0.4 (Paszke et al., 2017), and the best-performing model achieves a perplexity of 11.1 on the validation set. Other hyperparameters details are provided in Appendix B. While we con"
P19-1224,P16-1162,0,0.0340314,"xtractive evidence spans for CoQA instances (Reddy et al., 2018) instead of the shorter, partially abstractive answer spans (Yatskar, 2019). In all datasets, we remove unanswerable questions and questions whose answers span multiple paragraphs. A few very generic questions (e.g. “what happened in this article?”) were manually identified removed from the training dataset. Some other questions (e.g., “where was he born?”) are duplicated many times in the dataset; we downsample such questions to a maximum limit of 10. Finally, we preprocess both paragraphs and questions using byte-pair encoding (Sennrich et al., 2016). class labels as described in Section 3.1. To promote diversity, we over-generate prospective candidates (Heilman and Smith, 2010) for every answer span and later prune them. Specifically, we use beam search with a beam size of 3 to generate three highly-probable question candidates. As these candidates are often generic, we additionally use top-k random sampling (Fan et al., 2018) with k = 10, a recently-proposed diversity-promoting decoding algorithm, to generate ten more question candidates per answer span. Hence, for every answer span we generate 13 question candidates. We discuss issues"
P19-1224,D17-1090,0,0.0748867,"to Harold B. and Agnes R.” the model incorrectly answers the question “Who was born in New Haven?” as “Harold B. and Agnes R.” 6 Related Work Question Generation: Our work builds upon neural question generation systems (Du et al., 2017; Du and Cardie, 2018). Our work conditions generation on specificity, similar to difficultyconditioned question generation (Gao et al., 2018). QA pair generation has previously been used for 2328 dataset creation (Serban et al., 2016; Du and Cardie, 2018). Joint modeling of question generation and answering has improved the performance of individual components (Tang et al., 2017; Wang et al., 2017; Sachan and Xing, 2018) and enabled visual dialog generation (Jain et al., 2018). Information Retrieval: Our hierarchies are related to interactive retrieval setting (Hardtke et al., 2009; Brandt et al., 2011) where similar webpages are grouped together. SQUASH is also related to exploratory (Marchionini, 2006) and faceted search (Yee et al., 2003). Summarization: Our work is related to queryfocused summarization (Dang, 2005; Baumel et al., 2018) which conditions an output summary on an input query. Hierarchies have also been applied to summarization (Christensen et al., 20"
P19-1224,L18-1503,0,0.0253559,"han and Xing, 2018) and enabled visual dialog generation (Jain et al., 2018). Information Retrieval: Our hierarchies are related to interactive retrieval setting (Hardtke et al., 2009; Brandt et al., 2011) where similar webpages are grouped together. SQUASH is also related to exploratory (Marchionini, 2006) and faceted search (Yee et al., 2003). Summarization: Our work is related to queryfocused summarization (Dang, 2005; Baumel et al., 2018) which conditions an output summary on an input query. Hierarchies have also been applied to summarization (Christensen et al., 2014; Zhang et al., 2017; Tauchmann et al., 2018). 7 Future Work While Section 5.2 focused on shortcomings in our modeling process and steps to fix them, this section focuses on broader guidelines for future work involving the SQUASH format and its associated text generation task. Evaluation of the SQUASH format: As discussed in Section 1, previous research shows support for the usefulness of hierarchies and QA in pedagogical applications. We did not directly evaluate this claim in the context of SQUASH, focusing instead on evaluating the quality of QA pairs and their hierarchies. Moving forward, careful user studies are needed to evaluate t"
P19-1224,P18-2124,0,0.090131,"Missing"
P19-1224,D16-1264,0,0.0774935,", unlike prior work on QA generation. A3: A novel text generation task (SQUASH), which converts documents into specificity-based hierarchies of QA pairs. A4: A pipelined system to tackle SQUASH along with crowdsourced methods to evaluate it. Q: How can the community build on this work? A: We have released our codebase, crowdsourcing templates for evaluation, and a live demonstration of our system at http://squash.cs. umass.edu/. Additionally, we outline guidelines for future work in Section 7. 2 Obtaining training data for SQUASH The proliferation of reading comprehension datasets like SQuAD (Rajpurkar et al., 2016, 2018) has enabled state-of-the-art neural question generation systems (Du et al., 2017; Kim et al., 2018). However, these systems are trained for individual question generation, while the goal of SQUASH is to produce a general-to-specific hierarchy of QA pairs. Recently-released conversational QA datasets like QuAC (Choi et al., 2018) and CoQA (Reddy et al., 2018) contain a sequential arrangement of QA pairs, but question specificity is not explicitly marked.2 Motivated by the lack of hierarchical QA datasets, we automatically classify questions in SQuAD, QuAC and CoQA according to their spe"
P19-1224,N18-1058,0,0.0466101,"ncorrectly answers the question “Who was born in New Haven?” as “Harold B. and Agnes R.” 6 Related Work Question Generation: Our work builds upon neural question generation systems (Du et al., 2017; Du and Cardie, 2018). Our work conditions generation on specificity, similar to difficultyconditioned question generation (Gao et al., 2018). QA pair generation has previously been used for 2328 dataset creation (Serban et al., 2016; Du and Cardie, 2018). Joint modeling of question generation and answering has improved the performance of individual components (Tang et al., 2017; Wang et al., 2017; Sachan and Xing, 2018) and enabled visual dialog generation (Jain et al., 2018). Information Retrieval: Our hierarchies are related to interactive retrieval setting (Hardtke et al., 2009; Brandt et al., 2011) where similar webpages are grouped together. SQUASH is also related to exploratory (Marchionini, 2006) and faceted search (Yee et al., 2003). Summarization: Our work is related to queryfocused summarization (Dang, 2005; Baumel et al., 2018) which conditions an output summary on an input query. Hierarchies have also been applied to summarization (Christensen et al., 2014; Zhang et al., 2017; Tauchmann et al., 2"
P19-1224,N19-1241,0,0.0170857,"(Similar Pipeline) SPECIFIC Building QA Hierarchy Figure 2: An overview of the process by which we generate a pair of GENERAL-SPECIFIC questions , which consists of feeding input data (“RC” is Reading Comprehension) through various modules, including a question classifier and a multi-stage pipeline for question generation, answering, and filtering. as input to the question generator. To improve the quality of SPECIFIC questions generated from sentence spans, we use the extractive evidence spans for CoQA instances (Reddy et al., 2018) instead of the shorter, partially abstractive answer spans (Yatskar, 2019). In all datasets, we remove unanswerable questions and questions whose answers span multiple paragraphs. A few very generic questions (e.g. “what happened in this article?”) were manually identified removed from the training dataset. Some other questions (e.g., “where was he born?”) are duplicated many times in the dataset; we downsample such questions to a maximum limit of 10. Finally, we preprocess both paragraphs and questions using byte-pair encoding (Sennrich et al., 2016). class labels as described in Section 3.1. To promote diversity, we over-generate prospective candidates (Heilman an"
P19-1224,P14-1085,0,\N,Missing
P19-1224,D17-1219,0,\N,Missing
P19-1224,D18-1241,1,\N,Missing
P19-1224,N19-1423,0,\N,Missing
P19-1638,P17-1080,0,0.0410793,"Missing"
P19-1638,I17-1001,0,0.0366588,"Missing"
P19-1638,P15-1162,1,0.899084,"Missing"
P19-1638,P15-1107,0,0.456747,"e objective to 1 Source code and data are available at https://github.com/ tuvuumass/SCoPE. fix this issue improves classification performance, training speed, and generalization ability. We specifically investigate the paragraph embedding method of Zhang et al. (2017), which consists of a CNN-based encoder-decoder model (Sutskever et al., 2014) paired with a reconstruction objective to learn powerful paragraph embeddings that are capable of accurately reconstructing long paragraphs. This model significantly improves downstream classification accuracies, outperforming LSTM-based alternatives (Li et al., 2015). How well do these embeddings encode whether or not a given sentence appears in the paragraph? Conneau et al. (2018) show that such identity information is correlated with performance on downstream sentence-level tasks. We thus design a probe task to measure the extent to which this sentence content property is captured in a paragraph embedding. Surprisingly, our experiments (Section 2) reveal that despite its impressive downstream performance, the model of Zhang et al. (2017) substantially underperforms a simple bagof-words model on our sentence content probe. Given this result, it is natura"
P19-1638,P11-1015,0,0.164248,"Missing"
P19-1638,P82-1020,0,0.69842,"Missing"
P19-1638,P19-1442,0,0.043527,"Missing"
P19-1638,P18-1031,0,0.0157982,"distribution over the whole vocabulary for every token of the paragraph, making it prohibitively slow to train. Yelp DBPedia Yahoo purely supervised w/o external data ngrams TFIDF 95.4 98.7 Large Word ConvNet 95.1 98.3 Small Word ConvNet 94.5 98.2 Large Char ConvNet 94.1 98.3 Small Char ConvNet 93.5 98.0 SA-LSTM (word level) NA 98.6 Deep ConvNet 95.7 98.7 CNN (Zhang et al., 2017) 95.4 98.2 68.5 70.9 70.0 70.5 70.2 NA 73.4 72.6 pre-training + fine-tuning w/o external data CNN-R (Zhang et al., 2017) 96.0 98.8 CNN-SC (ours) 96.6 99.0 74.2 74.9 pre-training + fine-tuning w/ external data ULMFiT (Howard and Ruder, 2018) 97.8 99.2 NA Table 4: CNN-SC outperforms other baseline models that do not use external data, including CNN-R. All baseline models are taken from Zhang et al. (2017). now to our fine-tuning experiments. Specifically, we take the CNN encoder pre-trained using our sentence content objective and then fine-tune it on downstream classification tasks with supervised labels. While our previous version of CNNSC created just a single positive/negative pair of examples from a single paragraph, for our finetuning experiments we create a pair of examples from every sentence in the paragraph to maximize t"
P19-1638,D14-1162,0,0.0895317,"ricts the class label from which negative sentence candidates s− are sampled. We experiment with two sources of s− : (1) paragraphs of the same class label as the probe paragraph (CNN-SC− ), and (2) paragraphs from a different class label (CNN-SC+ ). Figure 4 reveals that the performance of CNN-SC drops dramatically when trained on the first dataset and improves when trained on the second dataset, which confirms our hypothesis. 4 Related work Text embeddings and probe tasks A variety of methods exist for obtaining fixed-length dense vector representations of words (e.g., Mikolov et al., 2013; Pennington et al., 2014; Peters et al., 2018), sentences (e.g., Kiros et al., 2015; Conneau et al., 2017; Subramanian et al., 2018; Cer et al., 2018), and larger bodies of text (e.g., Le and Mikolov, 2014; Dai et al., 2015; Iyyer et al., 2015; Li et al., 2015; Chen, 2017; Zhang et al., 2017) that Yelp test accuracy available. Finally, when all labeled training data is used, CNN-SC achieves higher classification accuracy than CNN-R on all three datasets (Table 4). While CNN-SC exhibits a clear preference for target task unlabeled data (see Table 3), we can additionally leverage large amounts of unlabeled general-doma"
P19-1638,N18-1202,1,0.807701,"m which negative sentence candidates s− are sampled. We experiment with two sources of s− : (1) paragraphs of the same class label as the probe paragraph (CNN-SC− ), and (2) paragraphs from a different class label (CNN-SC+ ). Figure 4 reveals that the performance of CNN-SC drops dramatically when trained on the first dataset and improves when trained on the second dataset, which confirms our hypothesis. 4 Related work Text embeddings and probe tasks A variety of methods exist for obtaining fixed-length dense vector representations of words (e.g., Mikolov et al., 2013; Pennington et al., 2014; Peters et al., 2018), sentences (e.g., Kiros et al., 2015; Conneau et al., 2017; Subramanian et al., 2018; Cer et al., 2018), and larger bodies of text (e.g., Le and Mikolov, 2014; Dai et al., 2015; Iyyer et al., 2015; Li et al., 2015; Chen, 2017; Zhang et al., 2017) that Yelp test accuracy available. Finally, when all labeled training data is used, CNN-SC achieves higher classification accuracy than CNN-R on all three datasets (Table 4). While CNN-SC exhibits a clear preference for target task unlabeled data (see Table 3), we can additionally leverage large amounts of unlabeled general-domain data by incorporati"
P19-1638,P17-1190,0,0.0263881,"prove various downstream tasks. To analyze word and sentence embeddings, recent work has studied classification tasks that probe them for various linguistic properties (Shi et al., 2016; Adi et al., 2017; Belinkov et al., 2017a,b; Conneau et al., 2018; Tenney et al., 2019). In this paper, we extend the notion of probe tasks to the paragraph level. Transfer learning Another line of related work is transfer learning, which has been the driver of recent successes in NLP. Recently-proposed objectives for transfer learning include surrounding sentence prediction (Kiros et al., 2015), paraphrasing (Wieting and Gimpel, 2017), entailment (Conneau et al., 2017), machine translation (McCann et al., 2017), discourse (Jernite et al., 2017; Nie et al., 2017), and language modeling (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018). 5 Conclusions and Future work In this paper, we evaluate a state-of-the-art paragraph embedding model, based on how well it captures the sentence identity within a paragraph. Our results indicate that the model is not fully aware of this basic property, and that implementing a simple objective to fix this issue improves classification performance, training speed, and generaliza"
P19-1638,D16-1159,0,0.0290844,"Missing"
P19-1638,N19-1423,0,\N,Missing
W16-0107,D12-1118,1,0.835636,"Missing"
W16-0107,N12-1094,0,0.0275354,"paintings. For example, in a question about The Holy Trinity by 46 Related Work Our work is specifically related to previous work on visual question answering and more generally to multimodal applications of vision and language. Visual QA has previously focused on content questions (Antol et al., 2015; Ren et al., 2015; Andreas et al., 2015), while we focus on identity questions. Relatedly, Zhu et al. (2015) find semantic links between images and text via an attention model. We use coreference to connect text and image regions, similar to Kong et al. (2014). However, not all text is “visual” (Dodge et al., 2012) and not all image regions can be described textually (Berg et al., 2012). While we focus on meaning, structure of text (Elsner et al., 2014) can also be inferred from images. Socher et al. (2014) match sentences to images; however, our dataset is unique in that the text is intentionally oblique (rather than direct descriptions) and our images—paintings—are more varied visually. Aside from QA, images have been successfully used to generate captions (Karpathy and Fei-Fei, 2014; Mao et al., 2014; Vinyals et al., 2014; Xu et al., 2015; Chen and Zitnick, 2014). While we use vision to aid an NLP ta"
W16-0107,E14-1055,0,0.0273154,"question answering and more generally to multimodal applications of vision and language. Visual QA has previously focused on content questions (Antol et al., 2015; Ren et al., 2015; Andreas et al., 2015), while we focus on identity questions. Relatedly, Zhu et al. (2015) find semantic links between images and text via an attention model. We use coreference to connect text and image regions, similar to Kong et al. (2014). However, not all text is “visual” (Dodge et al., 2012) and not all image regions can be described textually (Berg et al., 2012). While we focus on meaning, structure of text (Elsner et al., 2014) can also be inferred from images. Socher et al. (2014) match sentences to images; however, our dataset is unique in that the text is intentionally oblique (rather than direct descriptions) and our images—paintings—are more varied visually. Aside from QA, images have been successfully used to generate captions (Karpathy and Fei-Fei, 2014; Mao et al., 2014; Vinyals et al., 2014; Xu et al., 2015; Chen and Zitnick, 2014). While we use vision to aid an NLP task, others have gone in the opposite direction, inducing correspondences between words and video clips (Yu and Siskind, 2013), words and acti"
W16-0107,P15-1162,1,0.826101,"Missing"
W16-0107,P13-1006,0,0.0582621,"Missing"
W16-0107,N15-1117,1,\N,Missing
