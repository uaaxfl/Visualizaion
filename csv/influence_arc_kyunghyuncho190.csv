2020.aacl-main.36,E06-1040,0,0.0581807,"a lower self-BLEU score means that the samples have better diversity. In our experiments, we feed the first ten subwords of every sample from test set to the model, and compare the model-generated sequences to the reference samples in the validation set. We use 10,000 samples to compute corpus-BLEU or selfBLEU, i.e., |Sgen |= |Sref |= 10, 000. Automatic evaluation enables us to do a finegrained sweep of the hyperparameters for each sampling algorithm, and compare them in the qualitydiversity trade-off. However, observations from automatic evaluation could be misaligned with human evaluation (Belz and Reiter, 2006). Therefore, we confirm our key observations with human evaluation. 4.1.2 Human Evaluation Quality We ask a pool of 602 crowdworkers on Amazon Mechanical Turk to evaluate various sampling configurations in the quality aspect. Each worker is presented a set of ten samples along with the prompts (prefixes). They are then asked to rate how likely the sentence would appear in a news article between 0 and 5 (Invalid, Confusing, Unspecific, Average, Expected, and Very Expected respectively). We focus on the Gigaword dataset for human evaluation since news articles are ubiquitous and do not often req"
2020.aacl-main.36,P18-1082,0,0.175591,"translation, summarization, image captioning, and other subfields. However, in the task of open-ended language generation (which is the focus of this work), a significant degree of diversity is required. For example, conditioned on the prompt “The news says that ...”, the LM is expected to be able to generate a wide range of interesting continuations. While the deterministic behavior of decoding algorithms could give high-quality samples, they suffer from a serious lack of diversity. This need for diversity gives rise to a wide adoption of various sampling algorithms. Notably, topk sampling (Fan et al., 2018), nucleus sampling (Holtzman et al., 2020), and tempered sampling (Caccia et al., 2020) have been used in open-ended 334 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 334–346 c December 4 - 7, 2020. 2020 Association for Computational Linguistics generation (Radford et al., 2018; Caccia et al., 2020), story generation (Fan et al., 2018), and dialogue response generation (Zhang et al., 2020b). However, the sampling algorithm and the hyperparameter are"
2020.aacl-main.36,2020.acl-main.164,0,0.34324,"Missing"
2020.aacl-main.36,P19-1365,0,0.123812,"py algorithms, tend to be less semantically and syntatically coherent. In particular, the target entropy sampling algorithm, which obtains the lowest quality score measured by corpusBLEU, lacks basic language structure. In comparison to target entropy, noised top-k is syntatically coherent, but exhibits logical and factual inconsistencies. These observations aligns with the results we get from automatic evaluation. 6 Related Works Despite the popularity of sampling algorithms in natural language generation, a rigorous comparison or scrutiny of existing algorithms is lacking in the literature. Ippolito et al. (2019) provides a comparison between sampling and decoding algorithms. Holtzman et al. (2020) proposes nucleus sampling, and compare it with top-k sampling (Fan et al., 2018). However, only a few hyperparameter configurations are tested. In Hashimoto et al. (2019) and Caccia et al. (2020), temperature sampling is used and the hyperparameter T is tuned to trade-off between diversity and quality, but it lacks comparisons with other sampling algorithms. Welleck et al. (2020) studies the consistency of existing sampling and decoding algorithms, without comparing the generation performance. In this work"
2020.aacl-main.36,D17-1230,0,0.0190267,"formance degradation, as measured by the Q-D trade-off. On the other hand, we find that the set of sampling algorithms that satisfies these properties performs on par with the existing sampling algorithms.1 1 Figure 1: Human evaluation (y-axis: quality, x-axis: diversity, both are the bigger the better) shows that the generation performance of existing sampling algorithms are on par with each other. Introduction A language model (LM) is a central module for natural language generation (NLG) tasks (Young et al., 2018) such as machine translation (Wu et al., 2018), dialogue response generation (Li et al., 2017), image captioning (Lin et al.), and related tasks. Given a trained LM, finding the best way to generate a sample from it has been an important challenge for NLG applications. ∗ Equal contribution. Our data and code are available https://github.com/moinnadeem/ characterizing-sampling-algorithms. 1 at Decoding, i.e., finding the most probable output sequence from a trained model, is a natural principle for generation. The beam-search decoding algorithm approximately finds the most likely sequence by performing breadth-first search over a restricted search space. It has achieved success in machi"
2020.aacl-main.36,W12-3018,0,0.0159211,"Missing"
2020.aacl-main.36,P02-1040,0,0.106858,"trics, with one pair using automatic evaluation and the other using human evaluation. In the next two sections, we describe the metrics we use, and refer readers to Caccia et al. (2020) for more intuition behind the Q-D trade-off. 4.1.1 Automatic Evaluation For automatic metrics, we adopt the corpus-BLEU (Yu et al., 2016) metric to measure quality and the self-BLEU (Zhu et al., 2018) metric to measure diversity. We formulate them below. Given a batch of generated sentences Sgen and a batch of sentences from ground-truth data as references Sref , corpus-BLEU returns the average 337 BLEU score (Papineni et al., 2002) of every model generated sentence against the reference set: corpus-BLEU(Sgen , Sref ) = of the rating. Therefore, we estimate the human judgement score for a sample as the average rating of the 20 crowdworkers (out of 25) who took the most time to rate the samples. We provide further details about our setup in Appendix C and D. X 1 BLEU(W, Sref ). |Sgen |W ∈S gen (11) Diversity It is difficult for human annotators to estimate diversity of text (Hashimoto et al., 2019). Therefore, we use the n-gram entropy metric (Zhang et al., 2018; He and Glass, 2019) . Given Sgen which contains a large num"
2020.aacl-main.36,P16-1162,0,0.00548779,"we rearrange the elements such that if i &lt; j → pi &gt;= pj .2 We list the transformations and their intuition below: Definition 2.1. (Top-k) In top-k sampling, we only sample from the top K tokens: pˆi = Autoregressive Language Modeling The task of autoregressive language modeling is to learn the probability distribution of the (l + 1)-th word Wl+1 in a sentence W conditioned on the word history W1:l := (W1 , . . . , Wl ) and context C. Here, we use Wi ∈ V to denote a discrete random variable distributed across a fixed vocabulary V . In this work, the vocabulary is constructed on subword level (Sennrich et al., 2016). Given a training set D, maximum likelihood es1 |D| pi · 1{i ≤ K} , PK j=1 pj (2) Definition 2.2. (Nucleus) With a hyperparameter P (0 &lt; P ≤ 1), in nucleus sampling, we sample from the top-P mass of p: p0 pˆi = P|V i| 0 j=1 pj , Pi−1 where p0i = pi · 1{ j=1 pj &lt; P }. 335 2 The token indexes are also permutated accordingly. (3) Definition 2.3. (Tempered) In tempered sampling, the log probabilities are scaled by T1 : exp(log(pi )/T ) . pˆi = P|V | j=1 exp(log(pj )/T ) Property 3. (Slope Preservation): The “slope” of the distribution is preserved. Formally, ∀ˆ pi &gt; pˆj &gt; pˆk &gt; 0 (i.e., they are"
2020.aacl-main.36,2020.emnlp-main.448,1,0.7212,"lgorithms in natural language generation, a rigorous comparison or scrutiny of existing algorithms is lacking in the literature. Ippolito et al. (2019) provides a comparison between sampling and decoding algorithms. Holtzman et al. (2020) proposes nucleus sampling, and compare it with top-k sampling (Fan et al., 2018). However, only a few hyperparameter configurations are tested. In Hashimoto et al. (2019) and Caccia et al. (2020), temperature sampling is used and the hyperparameter T is tuned to trade-off between diversity and quality, but it lacks comparisons with other sampling algorithms. Welleck et al. (2020) studies the consistency of existing sampling and decoding algorithms, without comparing the generation performance. In this work we mainly use the quality-diversity trade-off (Caccia et al., 2020) to conduct a comparison of different sampling algorithms. Parallel to our work, Zhang et al. (2020a) also uses the qualitydiversity trade-off to compare top-k, nucleus, and tempered sampling. Their observation is similar to ours: The performance of the existing algorithms are close with no significant gap. More importantly, the underlying reasons for the success of various sampling algorithms remain"
2020.aacl-main.36,2020.acl-demos.30,0,0.0885999,"option of various sampling algorithms. Notably, topk sampling (Fan et al., 2018), nucleus sampling (Holtzman et al., 2020), and tempered sampling (Caccia et al., 2020) have been used in open-ended 334 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 334–346 c December 4 - 7, 2020. 2020 Association for Computational Linguistics generation (Radford et al., 2018; Caccia et al., 2020), story generation (Fan et al., 2018), and dialogue response generation (Zhang et al., 2020b). However, the sampling algorithm and the hyperparameter are usually chosen via heuristics, and a comprehensive comparison between existing sampling algorithm is lacking in the literature. More importantly, the underlying reasons behind the success of the existing sampling algorithms still remains poorly understood. In this work, we begin by using the qualitydiversity (Q-D) trade-off (Caccia et al., 2020) to compare the three existing sampling algorithms. For automatic metrics, we use the BLEU score for quality and n-gram entropy for diversity. We also correlate these automatic metrics with"
2020.acl-main.428,D15-1075,0,0.0399269,"anguage they produce. That is, they can produce logically or factually inaccurate, or contradicting statements (Welleck et al., 2019b; Zhang et al., 2018; Hayashi et al., 2019; Petroni et al., 2019). Here, we show how the unlikelihood objective can be used to train such models to assign low probability to inconsistent and contradictory utterances. To do so, we assume the existence of training data of both positive and negative examples of coherent behavior. There is a raft of recent largescale, high quality data that can be massaged into this form, from natural language inference (NLI) tasks (Bowman et al., 2015; Williams et al., 2018; Welleck et al., 2019b) to commonsense reasoning tasks (Zellers et al., 2019; Qin et al., 2019). Two collections of data can be derived from the labels of such a supervised task: D+ = {(x(i) , y(i)+ )}, D− = {(x(i) , y(i)− )}, where D+ is coherent behavior, e.g. neutral or entailing data in NLI, and D− is incoherent behavior, e.g. contradictions. In general, many forms of this type of data can be collected, not just NLI, and it is also not necessary for the contexts x(i) to overlap as we have written here. Standard likelihood training can then be performed on coherent d"
2020.acl-main.428,W18-2706,0,0.0192674,"logue models. Outside of that work, the use of negative training in dialogue retrieval, rather than generation, has been previously extensively studied, see e.g. (Humeau et al., 2019; Nugmanova 4717 et al., 2019). In the area of generative dialogue, a number of works have focused on improving the standard likelihood training approach. Closer to our work is that of He and Glass (2019) which developed the approach of negative training to prevent generic and malicious responses in dialogue models. In terms of improving repetition and specificity, a recent alternative approach is that of control (Fan et al., 2018; Ficler and Goldberg, 2017; Ghazvininejad et al., 2017; See et al., 2019). Nucleus sampling (Holtzman et al., 2019) can help to remove generic or repetitive utterances at the expense of accuracy, but was shown to be inferior to beam blocking, which in turn was shown to be inferior to unlikelihood in Welleck et al. (2019a). In terms of dialogue coherence, Welleck et al. (2019b) showed that retrieval, but not generative models, could be improved with NLI as a rescorer, while Yang et al. (2018) multi-tasked with NLI. The work of Gabriel et al. (2019) has also studied improving narrative flow wit"
2020.acl-main.428,P19-1346,1,0.925994,"ith minimal changes in perplexity and F1. Repetition Model PPL F1 Context Label Human MLE Baseline 8.3 .368 .160 .441 .001 .014 UL (Context only) UL (Label only) UL (Context + Label) 8.8 .346 8.3 .371 8.5 .358 .229 .426 .313 .037 .001 .009 Table 2: Evaluation on the Wizard of Wikipedia test set, comparing standard likelihood (MLE) with context and label repetition unlikelihood loss training. The repetition types can be decreased depending on the type of unlikelihood loss used, while minimally impacting F1. knowledge-grounded dialogue (Dinan et al., 2019) and ELI5 long-form question answering (Fan et al., 2019) datasets to evaluate the effect of using unlikelihood to reduce copying and repetition in model generated utterances. On each dataset, we fine-tune the pre-trained pushshift.io Reddit model, then evaluate by generating nextutterances for dialogue contexts from the test set (or validation in ConvAI2, as the test set is hidden). We use greedy decoding in our main experiments for simplicity and scalability, but we also obtained similar results with beam search, shown in Appendix A. To measure label repetition in a sequence y, we use the portion of duplicate n-grams: 1.0 − |unique n-grams(y)| , |"
2020.acl-main.428,W17-4912,0,0.0218234,"ide of that work, the use of negative training in dialogue retrieval, rather than generation, has been previously extensively studied, see e.g. (Humeau et al., 2019; Nugmanova 4717 et al., 2019). In the area of generative dialogue, a number of works have focused on improving the standard likelihood training approach. Closer to our work is that of He and Glass (2019) which developed the approach of negative training to prevent generic and malicious responses in dialogue models. In terms of improving repetition and specificity, a recent alternative approach is that of control (Fan et al., 2018; Ficler and Goldberg, 2017; Ghazvininejad et al., 2017; See et al., 2019). Nucleus sampling (Holtzman et al., 2019) can help to remove generic or repetitive utterances at the expense of accuracy, but was shown to be inferior to beam blocking, which in turn was shown to be inferior to unlikelihood in Welleck et al. (2019a). In terms of dialogue coherence, Welleck et al. (2019b) showed that retrieval, but not generative models, could be improved with NLI as a rescorer, while Yang et al. (2018) multi-tasked with NLI. The work of Gabriel et al. (2019) has also studied improving narrative flow with a discriminative rescorer"
2020.acl-main.428,P17-4008,0,0.0276722,"f negative training in dialogue retrieval, rather than generation, has been previously extensively studied, see e.g. (Humeau et al., 2019; Nugmanova 4717 et al., 2019). In the area of generative dialogue, a number of works have focused on improving the standard likelihood training approach. Closer to our work is that of He and Glass (2019) which developed the approach of negative training to prevent generic and malicious responses in dialogue models. In terms of improving repetition and specificity, a recent alternative approach is that of control (Fan et al., 2018; Ficler and Goldberg, 2017; Ghazvininejad et al., 2017; See et al., 2019). Nucleus sampling (Holtzman et al., 2019) can help to remove generic or repetitive utterances at the expense of accuracy, but was shown to be inferior to beam blocking, which in turn was shown to be inferior to unlikelihood in Welleck et al. (2019a). In terms of dialogue coherence, Welleck et al. (2019b) showed that retrieval, but not generative models, could be improved with NLI as a rescorer, while Yang et al. (2018) multi-tasked with NLI. The work of Gabriel et al. (2019) has also studied improving narrative flow with a discriminative rescorer, but in that case for gener"
2020.acl-main.428,D17-2014,1,0.852974,"rticular tasks with the objectives outlined in Section 2 and specified in each experiment below. Following previous work (Humeau et al., 2019), we pre-train our model on dialogue data, using a previously existing Reddit dataset extracted and obtained by a third party and made available on pushshift.io, training to generate a comment conditioned on the full thread leading up to the comment, spanning ∼ 2200M training examples. Our Transformer model consists of an 8 layer encoder, 8 layer decoder with 512-dimensional embeddings and 16 attention heads, and is based on the ParlAI implementation of Miller et al. (2017). The model was trained with a batch size of 3072 sequences for approximately 3M updates using a learning rate of 5e-4, and an inverse square root scheduler. This pre-training took approximately two weeks using 64 NVIDIA V100s. 4.1 Repetition and Copying We use the ConvAI2 persona-based dialogue (Zhang et al., 2018), Wizard of Wikipedia Repetition Model PPL F1 Context Label Human MLE Baseline 11.4 .199 .0223 .0004 .1131 .0210 UL (Context only) 11.8 .194 UL (Label only) 11.4 .203 UL (Context & Label) 11.9 .193 .0330 .0069 .0984 .0005 .0352 .0023 Table 1: Evaluation on the ConvAI2 task valid set"
2020.acl-main.428,D19-1250,0,0.0682843,"Missing"
2020.acl-main.428,D19-1509,0,0.0496421,"Missing"
2020.acl-main.428,N19-1170,1,0.854072,"gue retrieval, rather than generation, has been previously extensively studied, see e.g. (Humeau et al., 2019; Nugmanova 4717 et al., 2019). In the area of generative dialogue, a number of works have focused on improving the standard likelihood training approach. Closer to our work is that of He and Glass (2019) which developed the approach of negative training to prevent generic and malicious responses in dialogue models. In terms of improving repetition and specificity, a recent alternative approach is that of control (Fan et al., 2018; Ficler and Goldberg, 2017; Ghazvininejad et al., 2017; See et al., 2019). Nucleus sampling (Holtzman et al., 2019) can help to remove generic or repetitive utterances at the expense of accuracy, but was shown to be inferior to beam blocking, which in turn was shown to be inferior to unlikelihood in Welleck et al. (2019a). In terms of dialogue coherence, Welleck et al. (2019b) showed that retrieval, but not generative models, could be improved with NLI as a rescorer, while Yang et al. (2018) multi-tasked with NLI. The work of Gabriel et al. (2019) has also studied improving narrative flow with a discriminative rescorer, but in that case for generated language. In o"
2020.acl-main.428,P19-1363,1,0.689817,"Facebook AI Research 2 New York University {margaretli,roller,ylan,kyunghyuncho,jase}@fb.com wellecks@nyu.edu,kulikov@cs.nyu.edu Abstract Generative dialogue models currently suffer from a number of problems which standard maximum likelihood training does not address. They tend to produce generations that (i) rely too much on copying from the context, (ii) contain repetitions within utterances, (iii) overuse frequent words, and (iv) at a deeper level, contain logical flaws. In this work we show how all of these problems can be addressed by extending the recently introduced unlikelihood loss (Welleck et al., 2019a) to these cases. We show that appropriate loss functions which regularize generated outputs to match human distributions are effective for the first three issues. For the last important general issue, we show applying unlikelihood to collected data of what a model should not do is effective for improving logical consistency, potentially paving the way to generative models with greater reasoning ability. We demonstrate the efficacy of our approach across several dialogue tasks. 1 Figure 1: GPT-2 345M model completions can show lack of coherence, e.g. direct contradictions. Introduction Open-e"
2020.acl-main.428,N18-1101,0,0.0267128,"That is, they can produce logically or factually inaccurate, or contradicting statements (Welleck et al., 2019b; Zhang et al., 2018; Hayashi et al., 2019; Petroni et al., 2019). Here, we show how the unlikelihood objective can be used to train such models to assign low probability to inconsistent and contradictory utterances. To do so, we assume the existence of training data of both positive and negative examples of coherent behavior. There is a raft of recent largescale, high quality data that can be massaged into this form, from natural language inference (NLI) tasks (Bowman et al., 2015; Williams et al., 2018; Welleck et al., 2019b) to commonsense reasoning tasks (Zellers et al., 2019; Qin et al., 2019). Two collections of data can be derived from the labels of such a supervised task: D+ = {(x(i) , y(i)+ )}, D− = {(x(i) , y(i)− )}, where D+ is coherent behavior, e.g. neutral or entailing data in NLI, and D− is incoherent behavior, e.g. contradictions. In general, many forms of this type of data can be collected, not just NLI, and it is also not necessary for the contexts x(i) to overlap as we have written here. Standard likelihood training can then be performed on coherent data D+ , while the unli"
2020.acl-main.428,W18-3022,0,0.0230585,"els. In terms of improving repetition and specificity, a recent alternative approach is that of control (Fan et al., 2018; Ficler and Goldberg, 2017; Ghazvininejad et al., 2017; See et al., 2019). Nucleus sampling (Holtzman et al., 2019) can help to remove generic or repetitive utterances at the expense of accuracy, but was shown to be inferior to beam blocking, which in turn was shown to be inferior to unlikelihood in Welleck et al. (2019a). In terms of dialogue coherence, Welleck et al. (2019b) showed that retrieval, but not generative models, could be improved with NLI as a rescorer, while Yang et al. (2018) multi-tasked with NLI. The work of Gabriel et al. (2019) has also studied improving narrative flow with a discriminative rescorer, but in that case for generated language. In our work, the improvements are tightly integrated into the training of the model itself. 4 Experiments In all of our experiments we employ a large pre-trained seq2seq Transformer (Vaswani et al., 2017) as our base model, which we then fine-tune for particular tasks with the objectives outlined in Section 2 and specified in each experiment below. Following previous work (Humeau et al., 2019), we pre-train our model on dia"
2020.acl-main.428,P19-1472,0,0.026468,"statements (Welleck et al., 2019b; Zhang et al., 2018; Hayashi et al., 2019; Petroni et al., 2019). Here, we show how the unlikelihood objective can be used to train such models to assign low probability to inconsistent and contradictory utterances. To do so, we assume the existence of training data of both positive and negative examples of coherent behavior. There is a raft of recent largescale, high quality data that can be massaged into this form, from natural language inference (NLI) tasks (Bowman et al., 2015; Williams et al., 2018; Welleck et al., 2019b) to commonsense reasoning tasks (Zellers et al., 2019; Qin et al., 2019). Two collections of data can be derived from the labels of such a supervised task: D+ = {(x(i) , y(i)+ )}, D− = {(x(i) , y(i)− )}, where D+ is coherent behavior, e.g. neutral or entailing data in NLI, and D− is incoherent behavior, e.g. contradictions. In general, many forms of this type of data can be collected, not just NLI, and it is also not necessary for the contexts x(i) to overlap as we have written here. Standard likelihood training can then be performed on coherent data D+ , while the unlikelihood objective is applied to D− as we wish to push down the probability o"
2020.acl-main.428,P18-1205,1,0.914215,"loss, each step’s candidate is thus the current token, Ctidentity = {yt }, and each token’s unlikelihood loss is scaled according to the mismatch between the approximated model and human distributions,  β(yc ) = pmodel(yc ) log pmodel (yc ) p∗ (yc )  . (i) identity LUL (pθ , C1:|y| , x(i) , y) where y is a model-generated sequence. Neural generation models appear fluent, especially when pre-trained on large datasets, but are still poor at understanding the language they produce. That is, they can produce logically or factually inaccurate, or contradicting statements (Welleck et al., 2019b; Zhang et al., 2018; Hayashi et al., 2019; Petroni et al., 2019). Here, we show how the unlikelihood objective can be used to train such models to assign low probability to inconsistent and contradictory utterances. To do so, we assume the existence of training data of both positive and negative examples of coherent behavior. There is a raft of recent largescale, high quality data that can be massaged into this form, from natural language inference (NLI) tasks (Bowman et al., 2015; Williams et al., 2018; Welleck et al., 2019b) to commonsense reasoning tasks (Zellers et al., 2019; Qin et al., 2019). Two collectio"
2020.acl-main.450,W15-4918,0,0.101643,"Missing"
2020.acl-main.450,P17-1152,0,0.0163367,"against these methods by evaluating on the sentence ranking experiment from Falke et al. (2019). The experiment uses 373 triplets of source sentences from CNN/DM and two summary sentences generated from the model from Chen and Bansal (2018). One summary sentence is factually consistent with the source sentence, and the other is inconsistent. A metric (or model) is evaluated based on how often it ranks the consistent sentence higher than the inconsistent sentence. We present the results in Table 5. Results using two NLI models fine-tuned on MultiNLI (Williams et al., 2018), BERT NLI, and ESIM (Chen et al., 2017), are from Falke et al. (2019). FactCC (Kryscinski et al., 2019b) is an NLI-based factchecking model that is trained on a dataset tailor made for detecting factual inconsistencies in generated text. QAGS outperforms these methods, while requiring no special supervision for this task. 7 Qualitative Analysis Interpreting QAGS The questions and answers produced in computing QAGS are directly interpretable, and highlight errors in summaries. We present examples of articles, summaries, and the QAGS questions and answers in Table 6. On the first example (Table 6, top), QAGS detects several factual i"
2020.acl-main.450,P18-1063,0,0.0583558,"entence ranking task from Falke et al. (2019). Results using BERT NLI and ESIM are from Falke et al. (2019); FactCC is from Kryscinski et al. (2019b). QAGS outperforms previous work. 6 Re-ranking with QAGS Several works explore the use of natural language inference (NLI) models to detect factual consistency in generated text (Welleck et al., 2019; Falke et al., 2019). We compare against these methods by evaluating on the sentence ranking experiment from Falke et al. (2019). The experiment uses 373 triplets of source sentences from CNN/DM and two summary sentences generated from the model from Chen and Bansal (2018). One summary sentence is factually consistent with the source sentence, and the other is inconsistent. A metric (or model) is evaluated based on how often it ranks the consistent sentence higher than the inconsistent sentence. We present the results in Table 5. Results using two NLI models fine-tuned on MultiNLI (Williams et al., 2018), BERT NLI, and ESIM (Chen et al., 2017), are from Falke et al. (2019). FactCC (Kryscinski et al., 2019b) is an NLI-based factchecking model that is trained on a dataset tailor made for detecting factual inconsistencies in generated text. QAGS outperforms these"
2020.acl-main.450,N19-1423,0,0.108235,"ramework for evaluating conditional text generation that is designed to detect factual inconsistencies in generated text with respect to some input. Our framework consists of three steps: (1) Given a generated text, a question generation (QG) model generates a set of questions about the text. (2) We then use question answering (QA) models to answer these questions given both the input and the generated text. (3) A quality score is computed based on the similarity of corresponding answers. This approach leverages recent progress in QA and QG to ask and answer human readable, ontopic questions (Devlin et al., 2019; Song et al., 2019). It only assumes access to a question answering dataset to train the QG and QA models, and is applicable to any modality where a QA model is available, e.g. text, images, or knowledge graphs. We use this framework to develop QAGS (Question Answering and Generation for Summarization), a metric for evaluating the factual consistency of abstractive document summaries. Compared to commonly used automatic metrics such as ROUGE (Lin, 2004), QAGS shows dramatically higher correlations with human judgements of factuality, for example achieving a Pearson correlation coefficient of"
2020.acl-main.450,P17-1123,0,0.0376324,"tions, e.g. as produced by beam search, which may be biased in the limit, but will require fewer questions to estimate because of the higher quality of the questions. 4 QAGS Using this framework requires specifying the question distribution p(Q|Y ), the answer distributions p(A|Q, ∗), and the answer similarity function D. We apply this framework to summarization to develop QAGS and describe our instantiations of these components. Question Generation To instantiate p(Q|Y ), we draw on recent work on automatic question generation (QG), which models this distribution using neural seq2seq models (Du et al., 2017; Krishna and Iyyer, 2019). We over-sample questions, and then filter out low quality questions as follows. First, we train and generate from answerconditional QG models. During training, the model receives both the answer and the source article, and is trained to maximize the likelihood of the paired question. At test time, given a summary Y , we determine candidate answers. We condition on these answers and the summary to generate questions. Next, we filter out low-quality questions using a number of heuristics, such as duplicates and questions less than three tokens long. We also found it e"
2020.acl-main.450,N19-1395,0,0.0918069,"Missing"
2020.acl-main.450,P19-1213,0,0.20808,"Missing"
2020.acl-main.450,P18-1082,1,0.824869,"r, and is trained to predict the question. The answer, source article, and question are concatenated with intervening special tokens to mark the boundaries. At test time, the model receives the concatentation of a summary and an expected answer, and outputs question candidates. For each summary, we extract 10 named entities and noun phrases as answer candidates using the en-web-sm spaCy model.3 For each summary-answer pair, we generate questions using beam search with width 10, for a total of 100 question candidates. We experimented with generating via top-k (Holtzman et al., 2019) and top-p (Fan et al., 2018) sampling, but the generated questions, while diverse, were noisy and frequently nongrammatical. After filtering, we use the K = 20 most probable questions. If a summary has too few filtered questions, we randomly sample questions to reach the required number. For additional filtering and training details, see Appendix B. We implement these models with fairseq (Ott et al., 2019). 5.3 We present Pearson correlations between humanjudged consistency scores and various automatic metrics in Table 1. For CNN/DM, all results are significant with p < 0.01; for XSUM, all results are significant with p"
2020.acl-main.450,D18-1443,0,0.0808046,"lack of such consistency has plagued abstractive neural summarization models (Cao et al., 2018; Falke et al., 2019; Kryscinski et al., 2019b, i.a.). To compare with prior work on evaluating summarization, we use two common abstractive summarization datasets, CNN/Daily Mail (CNNDM, Hermann et al., 2015; Nallapati et al., 2016) and XSUM (Narayan et al., 2018). CNN/DM is a standard dataset for summarization that consists of CNN and DailyMail articles. Each reference summary consists of the concatenation of three editor-written, bullet point highlights. For summaries, we use 235 test outputs from Gehrmann et al. (2018). XSUM was created by taking the first sentence of a news article as the summary, and using the rest of the article as the source. Consequently, XSUM summaries are significantly more abstractive than Table 1: Summary-level Pearson correlation coefficients between various automatic metrics and human judgments of correctness for summarization datasets. All correlations are significant at p < .01 and p < .05 for CNN/DM and XSUM, respectively. QAGS obtains substantially higher correlations than all other automatic metrics. those of CNN/DM, and extractive summarization models perform poorly on this"
2020.acl-main.450,D18-1208,0,0.0334291,"onal text generation tasks. For example, we expect that extractive summarization models may obtain nearly perfect QAGS scores because facts and statements are directly copied from the source article. 8 Related Work Automatic summarization and its evaluation are long-standing lines of work in NLP, dating at least as far back as the Document Understanding Conferences (Chali and Kolla, 2004). The primary evaluation metric then and now is ROUGE (Lin, 2004), though much work has demonstrated the limited ability of ROUGE and its relatives to evaluate summaries (Dorr et al., 2004; Liu and Liu, 2009; Kedzie et al., 2018, i.a.). Other metrics have focused on specific aspects of summarization quality, including content selection (Nenkova and Passonneau, 2004), relevance prediction (Daume III and Marcu, 2005), and many more. The idea of evaluating summaries by their ability to answer a set of questions is also long-standing (Mani et al., 1999). Like our work, Eyal et al. 5015 (2019) and Scialom et al. (2019) extend this line of work by incorporating neural network modules. We diverge from these works in two important ways. First, both works use Cloze-style questions, which are generated by masking entities in e"
2020.acl-main.450,P19-1224,0,0.0276054,"oduced by beam search, which may be biased in the limit, but will require fewer questions to estimate because of the higher quality of the questions. 4 QAGS Using this framework requires specifying the question distribution p(Q|Y ), the answer distributions p(A|Q, ∗), and the answer similarity function D. We apply this framework to summarization to develop QAGS and describe our instantiations of these components. Question Generation To instantiate p(Q|Y ), we draw on recent work on automatic question generation (QG), which models this distribution using neural seq2seq models (Du et al., 2017; Krishna and Iyyer, 2019). We over-sample questions, and then filter out low quality questions as follows. First, we train and generate from answerconditional QG models. During training, the model receives both the answer and the source article, and is trained to maximize the likelihood of the paired question. At test time, given a summary Y , we determine candidate answers. We condition on these answers and the summary to generate questions. Next, we filter out low-quality questions using a number of heuristics, such as duplicates and questions less than three tokens long. We also found it especially useful to run th"
2020.acl-main.450,D19-1051,0,0.0976494,"Missing"
2020.acl-main.450,W07-0734,0,0.603778,", 2004) was developed specifically for evaluating automatic summarization, and its variants are the de facto standard for such. The most common variant is ROUGE-n (typically n ∈ {1, 2}), which computes the F1 score for all reference n-grams in the generated summary. ROUGEL, another commonly used variant, is the length of the longest common subsequence (possibly nonconsecutive) between a summary and references. BLEU (Papineni et al., 2002) is closely related to ROUGE but was developed for machine translation. BLEU computes the precision of the reference ngrams in the generated summary. METEOR (Lavie and Agarwal, 2007) extends BLEU by using an alignment between the generated text and a reference, as well as using stemming and synonym A Framework for Automatically Evaluating Factual Consistency We introduce a framework for automatically detecting factual inconsistencies in generated text while also addressing the deficiencies of current approaches. Let X and Y be sequences of tokens coming from a vocabulary V where X is a source text and Y is a summary of X. We define p(Q|Y ) as a distribution over all possible questions Q given summary Y , and p(A|Q, X) and p(A|Q, Y ) as distributions over all possible answ"
2020.acl-main.450,D19-5413,0,0.0577701,"nificant with p < .05. QAGS strongly outperforms other automatic evaluation metrics in terms of correlation with the summary-level human judgments of factual consistency. BLEU and ROUGE perform comparably, and lower order n-gram metrics work better. BERTScore matches the best ngram metrics on CNN/DM, but the worst overall on XSUM. On CNN/DM, QAGS obtains nearly twice the correlation of the next best automatic metric (BLEU-1). We speculate that this large increase is due to the sensitivity of the QA model to the sentence fusing behavior exhibited in many summarization models trained on CNN/DM (Lebanoff et al., 2019). When two sentences are fused to produce an incorrect summary statement, the QA model produces different answers when using the source article than when using the summary. On XSUM, all metrics correlate worse with human judgments than on CNN/DM, which reflects the fact that XSUM is more abstractive. QAGS still outperforms the next best automatic metric. 5.4 Question Answering We train extractive QA models by fine-tuning BERT (Devlin et al., 2019) on SQuAD2.0 (Rajpurkar et al., 2018). We use the large-uncased BERT variant via the transformers library (Wolf et al., 2019). We found that allowing"
2020.acl-main.450,D16-1230,0,0.0928985,"Missing"
2020.acl-main.450,D19-1320,0,0.341541,"Missing"
2020.acl-main.450,E99-1011,0,0.390452,"the Document Understanding Conferences (Chali and Kolla, 2004). The primary evaluation metric then and now is ROUGE (Lin, 2004), though much work has demonstrated the limited ability of ROUGE and its relatives to evaluate summaries (Dorr et al., 2004; Liu and Liu, 2009; Kedzie et al., 2018, i.a.). Other metrics have focused on specific aspects of summarization quality, including content selection (Nenkova and Passonneau, 2004), relevance prediction (Daume III and Marcu, 2005), and many more. The idea of evaluating summaries by their ability to answer a set of questions is also long-standing (Mani et al., 1999). Like our work, Eyal et al. 5015 (2019) and Scialom et al. (2019) extend this line of work by incorporating neural network modules. We diverge from these works in two important ways. First, both works use Cloze-style questions, which are generated by masking entities in either the source document or the reference summary. We instead generate the questions with a model, allowing a much greater range of questions. Second, we produce questions conditioned on the generated summary, rather than the reference summary or source article. Producing questions from the generated summary is more appropri"
2020.acl-main.450,D17-2014,0,0.0401354,"models perform poorly on this dataset. We found that while the XSUM summaries are more abstractive, frequently there are facts (e.g. first names) in the summary that are not available in the “article”. This quirk made it especially difficult for humans and QAGS to tell when factual errors were being made by the summarization model. To remedy this, for human evaluation and QAGS, we prepend the summary back to the “article”. We use a subset of 239 test outputs from BART fine-tuned on XSUM (Lewis et al., 2019). Annotation Protocol We collect human judgments on Amazon Mechanical Turk2 via ParlAI (Miller et al., 2017). We present summaries one sentence at a time, along with the entire article. For each summary sentence, the annotator makes a binary decision as to whether the sentence is factually consistent with the article. Workers are instructed to mark non-grammatical sentences as not consistent, and copies of article sentences as consistent. Workers are paid $1 per full summary annotated. See Appendix A for further details. We collect 3 annotations per summary. To obtain a single consistency score per summary, we first take the majority vote for each sentence, then average the binary scores across summ"
2020.acl-main.450,K16-1028,0,0.226416,"Missing"
2020.acl-main.450,D18-1206,0,0.277794,"al inconsistencies with respect to their input. Existing automatic evaluation metrics for summarization are largely insensitive to such errors. We propose QAGS,1 an automatic evaluation protocol that is designed to identify factual inconsistencies in a generated summary. QAGS is based on the intuition that if we ask questions about a summary and its source, we will receive similar answers if the summary is factually consistent with the source. To evaluate QAGS, we collect human judgments of factual consistency on model-generated summaries for the CNN/DailyMail (Hermann et al., 2015) and XSUM (Narayan et al., 2018) summarization datasets. QAGS has substantially higher correlations with these judgments than other automatic evaluation metrics. Also, QAGS offers a natural form of interpretability: The answers and questions generated while computing QAGS indicate which tokens of a summary are inconsistent and why. We believe QAGS is a promising tool in automatically generating usable and factually consistent text. Code for QAGS will be available at https://github. com/W4ngatang/qags. 1 Introduction Automatic summarization aims to produce summaries that are succinct, coherent, relevant, and — crucially — fac"
2020.acl-main.450,N04-1019,0,0.251111,"use facts and statements are directly copied from the source article. 8 Related Work Automatic summarization and its evaluation are long-standing lines of work in NLP, dating at least as far back as the Document Understanding Conferences (Chali and Kolla, 2004). The primary evaluation metric then and now is ROUGE (Lin, 2004), though much work has demonstrated the limited ability of ROUGE and its relatives to evaluate summaries (Dorr et al., 2004; Liu and Liu, 2009; Kedzie et al., 2018, i.a.). Other metrics have focused on specific aspects of summarization quality, including content selection (Nenkova and Passonneau, 2004), relevance prediction (Daume III and Marcu, 2005), and many more. The idea of evaluating summaries by their ability to answer a set of questions is also long-standing (Mani et al., 1999). Like our work, Eyal et al. 5015 (2019) and Scialom et al. (2019) extend this line of work by incorporating neural network modules. We diverge from these works in two important ways. First, both works use Cloze-style questions, which are generated by masking entities in either the source document or the reference summary. We instead generate the questions with a model, allowing a much greater range of questio"
2020.acl-main.450,N19-4009,0,0.0220547,"sm spaCy model.3 For each summary-answer pair, we generate questions using beam search with width 10, for a total of 100 question candidates. We experimented with generating via top-k (Holtzman et al., 2019) and top-p (Fan et al., 2018) sampling, but the generated questions, while diverse, were noisy and frequently nongrammatical. After filtering, we use the K = 20 most probable questions. If a summary has too few filtered questions, we randomly sample questions to reach the required number. For additional filtering and training details, see Appendix B. We implement these models with fairseq (Ott et al., 2019). 5.3 We present Pearson correlations between humanjudged consistency scores and various automatic metrics in Table 1. For CNN/DM, all results are significant with p < 0.01; for XSUM, all results are significant with p < .05. QAGS strongly outperforms other automatic evaluation metrics in terms of correlation with the summary-level human judgments of factual consistency. BLEU and ROUGE perform comparably, and lower order n-gram metrics work better. BERTScore matches the best ngram metrics on CNN/DM, but the worst overall on XSUM. On CNN/DM, QAGS obtains nearly twice the correlation of the next"
2020.acl-main.450,W17-2623,0,0.0422429,"these agreement numbers are in-line with similar figures from previous work on summarization evaluation (Daume III and Marcu, 2005). 5.2 METEOR (Lavie and Agarwal, 2007), BLEU (Papineni et al., 2002), and BERTScore (Zhang et al., 2019). The latter uses BERT representations to compute an alignment between generation and reference tokens, and which is then used to compute a soft version of unigram F1. We use the large-uncased BERT variant. Experimental Details Question Generation We train answerconditional QG models by fine-tuning a pretrained BART language model (Lewis et al., 2019) on NewsQA (Trischler et al., 2017), a dataset consisting of CNN articles and crowdsourced questions. During training, the model receives the concatenation of the source article and an answer, and is trained to predict the question. The answer, source article, and question are concatenated with intervening special tokens to mark the boundaries. At test time, the model receives the concatentation of a summary and an expected answer, and outputs question candidates. For each summary, we extract 10 named entities and noun phrases as answer candidates using the en-web-sm spaCy model.3 For each summary-answer pair, we generate quest"
2020.acl-main.450,P19-1363,1,0.909723,"Missing"
2020.acl-main.450,N18-1101,0,0.0515945,"al., 2019; Falke et al., 2019). We compare against these methods by evaluating on the sentence ranking experiment from Falke et al. (2019). The experiment uses 373 triplets of source sentences from CNN/DM and two summary sentences generated from the model from Chen and Bansal (2018). One summary sentence is factually consistent with the source sentence, and the other is inconsistent. A metric (or model) is evaluated based on how often it ranks the consistent sentence higher than the inconsistent sentence. We present the results in Table 5. Results using two NLI models fine-tuned on MultiNLI (Williams et al., 2018), BERT NLI, and ESIM (Chen et al., 2017), are from Falke et al. (2019). FactCC (Kryscinski et al., 2019b) is an NLI-based factchecking model that is trained on a dataset tailor made for detecting factual inconsistencies in generated text. QAGS outperforms these methods, while requiring no special supervision for this task. 7 Qualitative Analysis Interpreting QAGS The questions and answers produced in computing QAGS are directly interpretable, and highlight errors in summaries. We present examples of articles, summaries, and the QAGS questions and answers in Table 6. On the first example (Table"
2020.acl-main.450,P02-1040,0,0.111226,"ll reference n-grams in the generated summary. We briefly describe the most common metrics in this family, and refer readers to Liu et al. (2016) for further discussion. ROUGE (Lin, 2004) was developed specifically for evaluating automatic summarization, and its variants are the de facto standard for such. The most common variant is ROUGE-n (typically n ∈ {1, 2}), which computes the F1 score for all reference n-grams in the generated summary. ROUGEL, another commonly used variant, is the length of the longest common subsequence (possibly nonconsecutive) between a summary and references. BLEU (Papineni et al., 2002) is closely related to ROUGE but was developed for machine translation. BLEU computes the precision of the reference ngrams in the generated summary. METEOR (Lavie and Agarwal, 2007) extends BLEU by using an alignment between the generated text and a reference, as well as using stemming and synonym A Framework for Automatically Evaluating Factual Consistency We introduce a framework for automatically detecting factual inconsistencies in generated text while also addressing the deficiencies of current approaches. Let X and Y be sequences of tokens coming from a vocabulary V where X is a source"
2020.acl-main.450,P18-2124,0,0.03989,"tivity of the QA model to the sentence fusing behavior exhibited in many summarization models trained on CNN/DM (Lebanoff et al., 2019). When two sentences are fused to produce an incorrect summary statement, the QA model produces different answers when using the source article than when using the summary. On XSUM, all metrics correlate worse with human judgments than on CNN/DM, which reflects the fact that XSUM is more abstractive. QAGS still outperforms the next best automatic metric. 5.4 Question Answering We train extractive QA models by fine-tuning BERT (Devlin et al., 2019) on SQuAD2.0 (Rajpurkar et al., 2018). We use the large-uncased BERT variant via the transformers library (Wolf et al., 2019). We found that allowing the model to predict that a question is unanswerable, as is the case in SQuAD2.0, is particularly useful in filtering out bad questions, as questions based on hallucinated facts in the summary should be unanswerable using the source article. Baselines We compare against a number of automatic evaluation metrics: ROUGE (Lin, 2004), 3 https://spacy.io/api/entityrecognizer Results Ablations A potential issue with model-based evaluation is that the quality of the evaluation metric may de"
2020.acl-main.450,W04-1013,0,\N,Missing
2020.acl-main.450,2015.eamt-1.19,0,\N,Missing
2020.coling-main.230,2020.acl-main.652,1,0.878955,"r makes a set of interrelated questions to the system, which extracts the answers from reference text (Choi et al., 2018). These systems are trained on datasets of human-human dialogues collected using Wizard-of-Oz techniques, where two crowdsourcers are paired at random to emulate the questioner and the answerer. Several projects have shown that it is possible to train effective systems using such datasets. For instance, QuAC includes question and answers about popular people in Wikipedia (Choi et al., 2018), and DoQA includes question-answer conversations on cooking, movies and travel FAQs (Campos et al., 2020). Building such datasets comes at a cost, which limits the widespread use of conversational systems built using supervised learning. The fact that conversational systems interact naturally with users poses an exciting opportunity to improve them after deployment. Given enough training data, a company can deploy a basic conversational system, enough to be accepted and used by users. Once the system is deployed, the interaction with users and their feedback can be used to improve the system. In this work we focus on the case where a CQA system trained off-line is deployed and receives explicit b"
2020.coling-main.230,D18-1241,0,0.0512798,"ack is derived from gold annotations. The results show that our method is able to improve over the initial supervised system, getting close to a fully-supervised system that has access to the same labeled examples in in-domain experiments (QuAC), and even matching in out-of-domain experiments (DoQA). Our work opens the prospect to exploit interactions with real users and improve conversational systems after deployment. 1 Introduction In Conversational Question Answering (CQA) systems, the user makes a set of interrelated questions to the system, which extracts the answers from reference text (Choi et al., 2018). These systems are trained on datasets of human-human dialogues collected using Wizard-of-Oz techniques, where two crowdsourcers are paired at random to emulate the questioner and the answerer. Several projects have shown that it is possible to train effective systems using such datasets. For instance, QuAC includes question and answers about popular people in Wikipedia (Choi et al., 2018), and DoQA includes question-answer conversations on cooking, movies and travel FAQs (Campos et al., 2020). Building such datasets comes at a cost, which limits the widespread use of conversational systems b"
2020.coling-main.230,N19-1423,0,0.00573438,"t understands the passage. There are two main methods: the extractive method, in which the answer is selected as a contiguous span in the reference passage, and the abstractive method, in which the answer text is generated. Many datasets (Rajpurkar et al., 2016; Rajpurkar et al., 2018; Dunn et al., 2017; Koˇcisk`y et al., 2018; Trischler et al., 2017; Bajaj et al., 2016) and systems have been proposed to address this task, where the extractive scenario has drawn special attention (Wang and Jiang, 2017; Seo et al., 2017). Lately, with the incursion of large pre-trained language models as BERT (Devlin et al., 2019), XLNet (Yang et al., 2019) and their relatives, the state of the art has been dominated by systems that use the representations obtained with these pre-trained language models. The systems learn answer pointer networks that consist of two 1 https://github.com/jjacampos/FeedbackWeightedLearning 2562 classifiers, one for spotting the start token of the answer span and another for spotting the end token of the answer span. In reading comprehension, the questions are individual and isolated, that is, they do not have any dialogue structure. Due to the increasing interest on modelling the conversa"
2020.coling-main.230,P19-1358,0,0.101709,"Within this framework of lifelong learning, we particularly focus on building a system that adapts to changes in the data distribution after deployment (Agirre et al., 2019). There have been efforts for learning actively from dialogue during deployment. The question answering (QA) setting was explored in Weston (2016) and Li et al. (2017), where they analyzed a variety of learning strategies for different dialogue tasks with diverse types of feedback. In these studies they also touch on forward prediction, which uses explicit user correction. This idea was later applied to chit-chat systems (Hancock et al., 2019). These works relied on users explicitly providing the correct answer. This strong assumption was relaxed in Weston (2016), where the user provides binary feedback on correct and incorrect answers in a synthetic question answering task (Weston et al., 2015). Our work also uses binary feedback and tests it in more realistic CQA datasets. In a similar online setup to ours, Liu et al. (2018b) explored contextual multi-armed bandits for dialogue response selection using a customized version of Thompson sampling. In this work they use the Ubuntu Dialogue Corpus (Lowe et al., 2015) for user simulati"
2020.coling-main.230,N18-1187,0,0.182184,"Missing"
2020.coling-main.230,W15-4640,0,0.020904,"-chat systems (Hancock et al., 2019). These works relied on users explicitly providing the correct answer. This strong assumption was relaxed in Weston (2016), where the user provides binary feedback on correct and incorrect answers in a synthetic question answering task (Weston et al., 2015). Our work also uses binary feedback and tests it in more realistic CQA datasets. In a similar online setup to ours, Liu et al. (2018b) explored contextual multi-armed bandits for dialogue response selection using a customized version of Thompson sampling. In this work they use the Ubuntu Dialogue Corpus (Lowe et al., 2015) for user simulation. In the case of task-oriented dialogue systems, Liu et al. (2018a) propose a hybrid learning method with supervised pre-training and further improvement using human teaching and feedback. For the human teaching case they use imitation learning with explicit corrections done by an expert. After that, they resort to reinforcement learning for further improvement thanks to long term rewards defined by task completion. 4 Experiments In this section we present the experiments with feedback-weighted learning (FWL). In the experiments we first build a supervised system (S0 ), and"
2020.coling-main.230,W19-4102,0,0.031364,"QA datasets where questions and answers are interrelated have been created following the Wizard-ofOz technique. Among all the datasets we can highlight QuAC (Choi et al., 2018), CoQA (Reddy et al., 2019) and DoQA (Campos et al., 2020). While the first two datasets cover more formal domains as Wikipedia articles and literature, the latter covers different domains extracted from online forums as StackExchange. Contextual versions of the previously mentioned reading comprehension models have successfully modelled the conversational structure in those datasets (Qu et al., 2019b; Qu et al., 2019a; Ohsugi et al., 2019; Ju et al., 2019). 3 Importance Sampling for Learning After Deployment In our learning after deployment scenario we start by training an initial S0 system in an off-line and supervised way. This first system follows the traditional workflow where we have access to limited supervised training and development data. Then, we take the best performing system on the development data and deploy it to serve user queries. In this deployment phase, every time a user makes a query x, the system generates an answer y and the user gives binary feedback to it. Over time, the system generates different answ"
2020.coling-main.230,D14-1162,0,0.0861155,"Missing"
2020.coling-main.230,D16-1264,0,0.120899,"t interactions with real users and improve conversational systems after deployment. All the code and dataset splits are made publicly available 1 . 2 Related Work on Conversational Question Answering CQA research builds on reading comprehension. In reading comprehension the system has to answer questions about a certain passage of text in order to show that it understands the passage. There are two main methods: the extractive method, in which the answer is selected as a contiguous span in the reference passage, and the abstractive method, in which the answer text is generated. Many datasets (Rajpurkar et al., 2016; Rajpurkar et al., 2018; Dunn et al., 2017; Koˇcisk`y et al., 2018; Trischler et al., 2017; Bajaj et al., 2016) and systems have been proposed to address this task, where the extractive scenario has drawn special attention (Wang and Jiang, 2017; Seo et al., 2017). Lately, with the incursion of large pre-trained language models as BERT (Devlin et al., 2019), XLNet (Yang et al., 2019) and their relatives, the state of the art has been dominated by systems that use the representations obtained with these pre-trained language models. The systems learn answer pointer networks that consist of two 1"
2020.coling-main.230,P18-2124,0,0.0164386,"users and improve conversational systems after deployment. All the code and dataset splits are made publicly available 1 . 2 Related Work on Conversational Question Answering CQA research builds on reading comprehension. In reading comprehension the system has to answer questions about a certain passage of text in order to show that it understands the passage. There are two main methods: the extractive method, in which the answer is selected as a contiguous span in the reference passage, and the abstractive method, in which the answer text is generated. Many datasets (Rajpurkar et al., 2016; Rajpurkar et al., 2018; Dunn et al., 2017; Koˇcisk`y et al., 2018; Trischler et al., 2017; Bajaj et al., 2016) and systems have been proposed to address this task, where the extractive scenario has drawn special attention (Wang and Jiang, 2017; Seo et al., 2017). Lately, with the incursion of large pre-trained language models as BERT (Devlin et al., 2019), XLNet (Yang et al., 2019) and their relatives, the state of the art has been dominated by systems that use the representations obtained with these pre-trained language models. The systems learn answer pointer networks that consist of two 1 https://github.com/jjac"
2020.coling-main.230,Q19-1016,0,0.0433041,"of two 1 https://github.com/jjacampos/FeedbackWeightedLearning 2562 classifiers, one for spotting the start token of the answer span and another for spotting the end token of the answer span. In reading comprehension, the questions are individual and isolated, that is, they do not have any dialogue structure. Due to the increasing interest on modelling the conversational structure behind user questions, several CQA datasets where questions and answers are interrelated have been created following the Wizard-ofOz technique. Among all the datasets we can highlight QuAC (Choi et al., 2018), CoQA (Reddy et al., 2019) and DoQA (Campos et al., 2020). While the first two datasets cover more formal domains as Wikipedia articles and literature, the latter covers different domains extracted from online forums as StackExchange. Contextual versions of the previously mentioned reading comprehension models have successfully modelled the conversational structure in those datasets (Qu et al., 2019b; Qu et al., 2019a; Ohsugi et al., 2019; Ju et al., 2019). 3 Importance Sampling for Learning After Deployment In our learning after deployment scenario we start by training an initial S0 system in an off-line and supervise"
2020.coling-main.230,W17-2623,0,0.0124032,"code and dataset splits are made publicly available 1 . 2 Related Work on Conversational Question Answering CQA research builds on reading comprehension. In reading comprehension the system has to answer questions about a certain passage of text in order to show that it understands the passage. There are two main methods: the extractive method, in which the answer is selected as a contiguous span in the reference passage, and the abstractive method, in which the answer text is generated. Many datasets (Rajpurkar et al., 2016; Rajpurkar et al., 2018; Dunn et al., 2017; Koˇcisk`y et al., 2018; Trischler et al., 2017; Bajaj et al., 2016) and systems have been proposed to address this task, where the extractive scenario has drawn special attention (Wang and Jiang, 2017; Seo et al., 2017). Lately, with the incursion of large pre-trained language models as BERT (Devlin et al., 2019), XLNet (Yang et al., 2019) and their relatives, the state of the art has been dominated by systems that use the representations obtained with these pre-trained language models. The systems learn answer pointer networks that consist of two 1 https://github.com/jjacampos/FeedbackWeightedLearning 2562 classifiers, one for spotting t"
2020.eamt-1.22,W17-4772,0,0.042315,"Missing"
2020.eamt-1.22,W16-2377,0,0.0616652,"Missing"
2020.eamt-1.22,W18-6452,0,0.0182467,"Martins (2019) is a monotonic model following the sequence-to-sequence architecture and pre-trained on BERT (seq2seq BERT). B´erard et al. (2017) predict a sequence of actions in a left-to-right order. path. We compare different ground-truth action sequences based on minimum edit actions, all arriving at the same pe:  = 0.1 (Pereyra et al., 2017). We use batch size of 512 tokens and save checkpoints every 10,000 steps. • Left-to-right (l2r); 5 • Randomly shuffled (shuff ); We compare the effect of using different action orders on the development set and test sets of WMT 2018 APE shared task (Chatterjee et al., 2018). By using only training data overlapping with WMT’s training sets (as described in §2.1), we are able to use WMT’s development and test sets for evaluation. This allows to compare the performance of our model with that of previous submissions. Note however that our systems are in disadvantage, due to being trained on fewer data: out of the original 23,000 training samples we only found 16,068 in Specia et al. (2017). • Human-ordered (h-ord). Minimum edit actions are generated using the dynamic programming algorithm to compute Levenshtein distance. The algorithm is set to output left-to-right"
2020.eamt-1.22,P19-1292,1,0.791822,"rs and use them to train an automatic postediting system. We compare the resulting system with those trained with left-to-right and random post-editing orderings. We observe that humans tend to follow a nearly left-to-right order, but with interesting deviations, such as preferring to start by correcting punctuation or verbs. 1 Introduction Neural sequence generation models have been widely adopted for tasks such as machine translation (MT) (Bahdanau et al., 2015; Vaswani et al., 2017) and automatic post-editing of translations (Junczys-Dowmunt and Grundkiewicz, 2016; Chatterjee et al., 2016; Correia and Martins, 2019; Lopes et al., 2019). These models typically generate one word at a time, and rely on a factorizac 2020 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. * Work partly done during a research visit at New York University. 0: Die LMS ge¨offnet ist . 1: Die LMS ist ge¨offnet ist . 2: Die LMS ist ge¨offnet . [ I:2:ist ] [ D:4:ist ] Table 1: Example of a small post-edit from the training set. Each action is represented by three features: its type (I for insert and D for delete), its position in the sentence and the token to inser"
2020.eamt-1.22,N19-1423,0,0.0228577,"ans counts by left-to-right counts, discard words with a difference lower than 5, and group results by part-ofspeech tag. 3 Model Inspired by recent work in non-monotonic generation (Stern et al., 2019; Gu et al., 2019a; Emelianenko et al., 2019), we propose a model that receives a src, mt pair and outputs one action at a time. When a new action is predicted, there is no explicit memory of previous time-steps. The model can only observe the current state of the mt, which may have been changed by previous actions of the model. This model is based on a Transformer-Encoder pre-trained with BERT (Devlin et al., 2019). After producing one hidden state for each token, a linear transformation outputs two values per token: the logit of deleting that token and of inserting a word to its right. Out of all possible DEL or INS positions, the most likely operation is selected. A special operation is reserved to represent End-ofDecoding. If an INS operation is chosen, we still need to choose which token to insert. Another linear transformation is applied to the hidden state of the chosen position. We obtain a distribution over the vocabulary and select the most likely token. Figure 3 illustrates this procedure. Aft"
2020.eamt-1.22,W19-6605,1,0.893081,"Missing"
2020.eamt-1.22,W16-2378,0,0.144712,"this paper, we analyze the orderings produced by human post-editors and use them to train an automatic postediting system. We compare the resulting system with those trained with left-to-right and random post-editing orderings. We observe that humans tend to follow a nearly left-to-right order, but with interesting deviations, such as preferring to start by correcting punctuation or verbs. 1 Introduction Neural sequence generation models have been widely adopted for tasks such as machine translation (MT) (Bahdanau et al., 2015; Vaswani et al., 2017) and automatic post-editing of translations (Junczys-Dowmunt and Grundkiewicz, 2016; Chatterjee et al., 2016; Correia and Martins, 2019; Lopes et al., 2019). These models typically generate one word at a time, and rely on a factorizac 2020 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. * Work partly done during a research visit at New York University. 0: Die LMS ge¨offnet ist . 1: Die LMS ist ge¨offnet ist . 2: Die LMS ist ge¨offnet . [ I:2:ist ] [ D:4:ist ] Table 1: Example of a small post-edit from the training set. Each action is represented by three features: its type (I for insert and D for delete),"
2020.eamt-1.22,P17-4012,0,0.0177875,"f the chosen action is an INS, we obtain a distribution over the vocabulary by applying another linear transformation to H’s row of the chosen action position. sentence, we obtain the logit of INS (on the position to its right) and DEL (of the token itself) using a learnable matrix W ∈ Rh×2 . The distribution probability over all possible edit-operations is defined as: • When the model enters a loop; • When a limit of 50 actions is reached. Once decoding ends, the model outputs the final post-edited mt. Model details. We use BERT’s implementation from Wolf et al. (2019) together with OpenNMT (Klein et al., 2017), both based on PyTorch (Paszke et al., 2019). The pretrained BERT-Encoder contains 12 layers, embedding size and hidden size of 768. We begin with an input sequence x. This sequence is the concatenation of: src + [SEP] + &lt;S&gt; + mt + &lt;T&gt; where &lt;S&gt; and &lt;T&gt; are auxiliary tokens used to allow INS in position 0 and to represent End-ofDecoding. Tokens before and after [SEP] have a different segment embedding, to help differentiate between src and mt tokens. Let N be the length of x after applying a BERT pre-trained tokenizer (Wolf et al., 2019). This sequence is the input of the BERT-Encoder with hi"
2020.eamt-1.22,P07-2045,0,0.00552749,"e average count of actions computed from human keystrokes. Both average action counts exclude samples with zero actions. training set with keystroke logging information. Out of 23,000 training samples provided by the WMT 2016-17 shared tasks, 16,068 are also present in the dataset from Specia et al. (2017). This intersection is obtained by requiring the same triplet (src, mt, pe) to be present in both datasets. Since the WMT dataset comes already pre-processed, the following pre-processing is applied to the dataset containing keystrokes, to increase their intersection: using tools from Moses (Koehn et al., 2007), we apply En punctuationnormalization to the whole triplet, followed by tokenization of the corresponding language (either En or De). Additionally, we preprocess the raw keystrokes to obtain word-level DEL and INS actions (detailed in §2.2). Table 2 shows statistics from WMT’s original data and training set after intersecting with the keystrokes dataset from Specia et al. (2017). We denote by min-edit the average count of DEL and INS obtained from the Levenshtein distance. Average count of human actions (human-edit) is only available for the subset of the training data found in the keystrokes"
2020.eamt-1.22,2012.amta-wptp.3,0,0.0541014,"everaging on pre-trained models (Correia and Martins, 2019) and using conservativeness penalties (Lopes et al., 2019) to avoid over-editing. B´erard et al. (2017) post-edit by predicting a sequence of actions with an imposed left-to-right order. Another recent work directly models edits, without including order information but allowing to re-use edits in unseen contexts (Yin et al., 2019). Human post-editing. Previous work has explored keystrokes to understand the behavior of human editors. O’Brien (2006) investigates the relationship between pauses and cognitive effort, while later research (Lacruz et al., 2012; Lacruz and Shreve, 2014) examines keystroke logs for the same effect. Specia et al. (2017) introduce a dataset of human post-edits, containing information on keytrokes. Recently it was shown that detailed information from post-editing, such as sequences of edit-operations combined with mouseclicks and waiting times, contain structured information (G´ois and Martins, 2019). The same work provides evidence that this kind of information allows to identify and profile editors, and may be helpful in downstream tasks. 7 Conclusions In this work we explored different ways to order the edit operatio"
2020.eamt-1.22,D19-1001,0,0.0123917,"arching for orders that maximize the sequence likelihood, given the current model parameters. Emelianenko et al. (2019) train using sampled orders instead, to better escape local optima. They also drop the relative attention mechanism together with its better theoretical bound on time complexity – showing that, in practice, inference remains feasible. Welleck et al. (2019) propose a model that generates text as a binary tree. They learn order from a uniform distribution that slowly shifts to search among the model’s own preferences, or alternatively using a deterministic left-to-right oracle. Lawrence et al. (2019) use placeholders to represent yet-to-insert tokens, allowing for bidirectional attention without exposing future tokens. Decoding is either done left-to-right or by picking the most certain prediction. Alternatively all tokens can be decided in parallel, but with significant loss in performance. Non-autoregressive models. Another class of models focuses on parallel decoding of multiple tokens, moving away from the traditional autoregressive paradigm. This unlocks faster inference, but brings the difficult challenge of learning dependencies between tokens (Gu et al., 2018). Stern et al. (2019)"
2020.eamt-1.22,Q19-1042,1,0.915618,". 1: Die LMS ist ge¨offnet ist . 2: Die LMS ist ge¨offnet . [ I:2:ist ] [ D:4:ist ] Table 1: Example of a small post-edit from the training set. Each action is represented by three features: its type (I for insert and D for delete), its position in the sentence and the token to insert/delete. In this example, the token marked in red needs to be removed since it is incorrectly placed. The blue token is inserted to obtain the correct pe. tion that imposes a left-to-right generation ordering. Recent alternatives allow for different generation orderings (Welleck et al., 2019; Stern et al., 2019; Gu et al., 2019a), or even for parallel generation of multiple tokens (Gu et al., 2018; Stern et al., 2019; Gu et al., 2019b; Zhou et al., 2020), which allows exploiting dependencies among nonconsecutive tokens. One potential difficulty when training non-monotonic models is how to learn a good generation ordering. There are exponentially many valid orderings to generate a given sequence, and a model should prefer those that lead to accurate translations and can be efficiently learned. In previous work, to guide the search for a good ordering, oracle policies have been provided (Welleck et al., 2019), or anot"
2020.eamt-1.22,W19-5413,1,0.890645,"Missing"
2020.eamt-1.22,W07-0728,0,0.152433,"umans typing. It is known that edit operations performed by human translators are not arbitrary (G´ois and Martins, 2019). But it is not known how the orderings preferred by humans look like, or how they compare to orders learned by models. To investigate this question, we propose a model that learns generation orderings in a supervised manner from human keystrokes. Since a human is free to move back and forth arbitrarily while editing text, the chosen order of operations can be used as an additional learning signal. More specifically, we do this in the context of automatic postediting (APE) (Simard et al., 2007). APE consists in improving the output of a blackbox MT system by automatically fixing its mistakes. The act of post-editing text can be fully specified as a sequence of delete (DEL) and insert (INS) actions in given positions. Furthermore, if we do not include redundant actions in a sequence, that sequence can be arbitrarily reordered while still producing the same output. For instance, in Table 1, we can switch the order of the two actions, as long as we rectify to delete position 3 instead of position 4. We compare a model trained with human orderings to others trained with left-to-right an"
2020.eamt-1.22,W19-3620,1,0.814701,"Missing"
2020.emnlp-demos.7,S17-2001,0,0.0526606,"Missing"
2020.emnlp-demos.7,2020.acl-main.747,0,0.143648,"Missing"
2020.emnlp-demos.7,N19-1423,0,0.0455398,"state-of-the-art pre-trained models (e.g., BERT, RoBERTa, XLM-R) across tasks and languages. Downloading, sharing, and training adapters is as seamless as possible using minimal changes to the training scripts and a specialized infrastructure. Our framework enables scalable and easy access to sharing of task-specific models, particularly in lowresource scenarios. AdapterHub includes all recent adapter architectures and can be found at AdapterHub.ml. 1 Introduction Recent advances in NLP leverage transformerbased language models (Vaswani et al., 2017), pretrained on large amounts of text data (Devlin et al., 2019; Liu et al., 2019; Conneau et al., 2020). These models are fine-tuned on a target task and achieve state-of-the-art (SotA) performance for most natural language understanding tasks. Their performance has been shown to scale with their size (Kaplan et al., 2020) and recent models have reached ∗ 1 *Equal contribution. https://github.com/huggingface/transformers 46 Proceedings of the 2020 EMNLP (Systems Demonstrations), pages 46–54 c November 16-20, 2020. 2020 Association for Computational Linguistics hance transformers with adapter modules that can be combined with existing SotA models with min"
2020.emnlp-demos.7,N16-1181,0,0.0279735,"l´ 2 Aishwarya Kamath , Ivan Vuli´c4 , Sebastian Ruder5 , Kyunghyun Cho2,3 , Iryna Gurevych1 1 Technical University of Darmstadt 2 New York University 3 CIFAR Associate Fellow 4 University of Cambridge 5 DeepMind AdapterHub.ml Abstract billions of parameters (Raffel et al., 2019; Brown et al., 2020). While fine-tuning large pre-trained models on target task data can be done fairly efficiently (Howard and Ruder, 2018), training them for multiple tasks and sharing trained models is often prohibitive. This precludes research on more modular architectures (Shazeer et al., 2017), task composition (Andreas et al., 2016), and injecting biases and external information (e.g., world or linguistic knowledge) into large models (Lauscher et al., 2019; Wang et al., 2020). Adapters (Houlsby et al., 2019) have been introduced as an alternative lightweight fine-tuning strategy that achieves on-par performance to full fine-tuning (Peters et al., 2019) on most tasks. They consist of a small set of additional newly initialized weights at every layer of the transformer. These weights are then trained during fine-tuning, while the pre-trained parameters of the large model are kept frozen/fixed. This enables efficient parame"
2020.emnlp-demos.7,I05-5002,0,0.0149582,"Missing"
2020.emnlp-demos.7,D19-1165,0,0.313909,"e been trained for particular tasks, domains, and languages. This opens up the possibility of building on and combining information from many more sources than was previously possible, and makes research such as intermediate task training (Pruksachatkun et al., 2020), composing information from many tasks (Pfeiffer et al., 2020a), and training models for very low-resource languages (Pfeiffer et al., 2020b) much more accessible. representations in intermediate layers of the pretrained model. Current work predominantly focuses on training adapters for each task separately (Houlsby et al., 2019; Bapna and Firat, 2019; Pfeiffer et al., 2020a,b), which enables parallel training and subsequent combination of the weights. In NLP, adapters have been mainly used within deep transformer-based architectures (Vaswani et al., 2017). At each transformer layer l, a set of adapter parameters Φl is introduced. The placement and architecture of adapter parameters Φ within a pre-trained model is non-trivial and may impact their efficacy: Houlsby et al. (2019) experiment with different adapter architectures, empirically validating that a two-layer feed-forward neural network with a bottleneck works well. While this down-"
2020.emnlp-demos.7,W07-1401,0,0.115017,"Missing"
2020.emnlp-demos.7,2020.acl-main.740,0,0.0195176,"no longer arises. By separating knowledge extraction and composition, adapters mitigate the two most common pitfalls of multi-task learning, catastrophic forgetting and catastrophic interference. Overcoming these problems together with the availability of readily available trained task-specific adapters enables researchers and practitioners to leverage information from specific tasks, domains, or languages that is often more relevant for a specific application—rather than more general pretrained counterparts. Recent work (Howard and Ruder, 2018; Phang et al., 2018; Pruksachatkun et al., 2020; Gururangan et al., 2020) has shown the benefits of such information, which was previously only available by fully fine-tuning a model on the data of interest prior to task-specific fine-tuning. 3 & pr Loa e- di tra ng Θ ined mod ad el ap te Φ’ rs el ers od apt m d ng a di new Θ a Lo ing d Φ ad 1 & 5 Finding adapters 4 2 Training adapters 3 Φ’ Θ,Φ’ Extracting and uploading adapters 6 Inference Figure 1: The AdapterHub Process graph. Adapters Φ are introduced into a pre-trained transformer Θ (step ¬) and are trained (). They can then be extracted and open-sourced (®) and visualized (¯). Pre-trained adapters are downlo"
2020.emnlp-demos.7,2020.acl-main.467,0,0.0786619,"Missing"
2020.emnlp-demos.7,P18-1031,1,0.821346,"tely, the necessity of sampling heuristics due to skewed data set sizes no longer arises. By separating knowledge extraction and composition, adapters mitigate the two most common pitfalls of multi-task learning, catastrophic forgetting and catastrophic interference. Overcoming these problems together with the availability of readily available trained task-specific adapters enables researchers and practitioners to leverage information from specific tasks, domains, or languages that is often more relevant for a specific application—rather than more general pretrained counterparts. Recent work (Howard and Ruder, 2018; Phang et al., 2018; Pruksachatkun et al., 2020; Gururangan et al., 2020) has shown the benefits of such information, which was previously only available by fully fine-tuning a model on the data of interest prior to task-specific fine-tuning. 3 & pr Loa e- di tra ng Θ ined mod ad el ap te Φ’ rs el ers od apt m d ng a di new Θ a Lo ing d Φ ad 1 & 5 Finding adapters 4 2 Training adapters 3 Φ’ Θ,Φ’ Extracting and uploading adapters 6 Inference Figure 1: The AdapterHub Process graph. Adapters Φ are introduced into a pre-trained transformer Θ (step ¬) and are trained (). They can then be extracte"
2020.emnlp-demos.7,D16-1264,0,0.0151158,"Missing"
2020.emnlp-demos.7,N19-5004,1,0.857724,"Missing"
2020.emnlp-demos.7,W19-4302,1,0.887479,"Missing"
2020.emnlp-demos.7,2020.emnlp-main.617,1,0.706042,"Missing"
2020.emnlp-demos.7,D13-1170,0,0.0449199,"020; Pfeiffer et al., 2020a,b). 2.1 Why Adapters? Adapter Architecture Adapters are neural modules with a small amount of additional newly introduced parameters Φ within a large pre-trained model with parameters Θ. The parameters Φ are learnt on a target task while keeping Θ fixed; Φ thus learn to encode task-specific 2 Layer normalization learns to normalize the inputs across the features. This is usually done by introducing a new set of features for mean and variance. 47 Full RTE (Wang et al., 2018) MRPC (Dolan and Brockett, 2005) STS-B (Cer et al., 2017) CoLA (Warstadt et al., 2019) SST-2 (Socher et al., 2013) QNLI (Rajpurkar et al., 2016) MNLI (Williams et al., 2018) QQP (Iyer et al., 2017) 66.2 90.5 88.8 59.5 92.6 91.3 84.1 91.4 Pfeif. Houl. 70.8 89.7 89.0 58.9 92.2 91.3 84.1 90.5 CRate 69.8 91.5 89.2 59.1 92.8 91.2 84.1 90.8 64 16 2 Base #Params Size Large #Params Size 0.2M 0.9M 7.1M 0.9Mb 3.5Mb 28Mb 0.8M 3.1M 25.2M 3.2Mb 13Mb 97Mb Table 2: Number of additional parameters and compressed storage space of the adapter of Pfeiffer et al. (2020a) in (Ro)BERT(a)-Base and Large transformer architectures. The adapter of Houlsby et al. (2019) requires roughly twice as much space. CRate refers to the adap"
2020.emnlp-demos.7,P19-1355,0,0.0848206,"Missing"
2020.emnlp-demos.7,W18-5446,0,0.202668,"scalability, modularity, and composition. We now provide a few use-cases for adapters to illustrate their usefulness in practice. Task-specific Layer-wise Representation Learning. Prior to the introduction of adapters, in order to achieve SotA performance on downstream tasks, the entire pre-trained transformer model needs to be fine-tuned (Peters et al., 2019). Adapters have been shown to work on-par with full fine-tuning, by adapting the representations at every layer. We present the results of fully fine-tuning the model compared to two different adapter architectures on the GLUE benchmark (Wang et al., 2018) in Table 1. The adapters of Houlsby et al. (2019, Figure 3c) and Pfeiffer et al. (2020a, Figure 3b) comprise two and one down- and up-projection Adapters While the predominant methodology for transfer learning is to fine-tune all weights of the pre-trained model, adapters have recently been introduced as an alternative approach, with applications in computer vision (Rebuffi et al., 2017) as well as the NLP domain (Houlsby et al., 2019; Bapna and Firat, 2019; Wang et al., 2020; Pfeiffer et al., 2020a,b). 2.1 Why Adapters? Adapter Architecture Adapters are neural modules with a small amount of"
2020.emnlp-demos.7,Q19-1040,0,0.0324564,"Missing"
2020.emnlp-demos.7,N18-1101,0,0.0345727,"Missing"
2020.emnlp-main.448,W18-6322,0,0.0464601,"ne, 2019) and degenerate repetition (Holtzman et al., 2019). ∗ Equal contribution. Correspondence to: Sean Welleck wellecks@nyu.edu. † Work done at New York University. These issues are suspected to be related to the maximum likelihood objective’s local normalization, which results in a discrepancy between the learned model’s distribution and the distribution induced by the decoding algorithm used to generate sequences (Lafferty et al., 2001; Andor et al., 2016). This has prompted the development of alternative decoding methods (Wu et al., 2016; Holtzman et al., 2019) and training objectives (Murray and Chiang, 2018; Welleck et al., 2019). In this paper, we formalize and study this discrepancy between the model and the decoding algorithm. We begin by formally defining recurrent neural language models, a family that encompasses neural models used in practice, such as recurrent neural networks (Elman, 1990; Cho et al., 2014; Hochreiter and Schmidhuber, 1997), and transformers (Vaswani et al., 2017). Next, we formally define a decoding algorithm – a function that induces a distribution over sequences given a recurrent language model and a context distribution – which is used to obtain probable sequences fro"
2020.emnlp-main.448,D16-1158,0,0.0543104,"d a selfterminating recurrent language model. Empirical results show that inconsistency occurs in practice, and that the proposed methods prevent inconsistency. 1 Introduction Neural sequence models trained with maximum likelihood estimation (MLE) have become a standard approach to modeling sequences in a variety of natural language applications such as machine translation (Bahdanau et al., 2015), dialogue modeling (Vinyals et al., 2015), and language modeling (Radford et al., 2019). Despite this success, MLEtrained neural sequence models have been shown to exhibit issues such as length bias (Sountsov and Sarawagi, 2016; Stahlberg and Byrne, 2019) and degenerate repetition (Holtzman et al., 2019). ∗ Equal contribution. Correspondence to: Sean Welleck wellecks@nyu.edu. † Work done at New York University. These issues are suspected to be related to the maximum likelihood objective’s local normalization, which results in a discrepancy between the learned model’s distribution and the distribution induced by the decoding algorithm used to generate sequences (Lafferty et al., 2001; Andor et al., 2016). This has prompted the development of alternative decoding methods (Wu et al., 2016; Holtzman et al., 2019) and tr"
2020.emnlp-main.448,P82-1020,0,0.77635,"Missing"
2020.emnlp-main.50,D13-1185,1,0.75481,"Missing"
2020.emnlp-main.50,P08-1090,1,0.872582,"Missing"
2020.emnlp-main.50,P09-1068,1,0.7437,"tory provides significant improvement to downstream end-to-end Information Extraction over a state-of-the-art joint neural extraction model, when used as additional global features to unfold instance graphs.1 1 Introduction Existing approaches to automated event extraction retain the overly simplistic assumption that events are atomic occurrences. Understanding events requires knowledge in the form of a repository of abstracted event schemas (complex event templates). Scripts (Schank and Abelson, 1977) encode frequently recurring event sequences, where events are ordered by temporal relation (Chambers and Jurafsky, 2009), causal relation (Mostafazadeh et al., 2016b), or narrative order (Jans et al., 2012). Event schemas have become increasingly important for natural language understanding tasks such as story 1 Our code and data are publicly available for research purpose at http://blender.cs.illinois.edu/software/ pathlm. ending prediction (Mostafazadeh et al., 2016a) and reading comprehension (Koˇcisk´y et al., 2018; Ostermann et al., 2019). Previous schema induction methods mostly ignore uncertainty, re-occurring events and multiple hypotheses, with limited attention to capture complex relations among event"
2020.emnlp-main.50,chambers-jurafsky-2010-database,1,0.801355,"Missing"
2020.emnlp-main.50,N13-1104,0,0.133398,"Missing"
2020.emnlp-main.50,N19-1423,0,0.0311453,"Missing"
2020.emnlp-main.50,N15-1165,0,0.026451,"Missing"
2020.emnlp-main.50,W16-1701,1,0.907272,"Missing"
2020.emnlp-main.50,P16-1025,1,0.902933,"Missing"
2020.emnlp-main.50,E12-1034,0,0.126036,"Missing"
2020.emnlp-main.50,W19-3311,0,0.0837118,"Missing"
2020.emnlp-main.50,Q18-1023,0,0.0569661,"Missing"
2020.emnlp-main.50,2020.acl-main.713,1,0.645499,"Missing"
2020.emnlp-main.50,N16-1098,1,0.812451,"ream end-to-end Information Extraction over a state-of-the-art joint neural extraction model, when used as additional global features to unfold instance graphs.1 1 Introduction Existing approaches to automated event extraction retain the overly simplistic assumption that events are atomic occurrences. Understanding events requires knowledge in the form of a repository of abstracted event schemas (complex event templates). Scripts (Schank and Abelson, 1977) encode frequently recurring event sequences, where events are ordered by temporal relation (Chambers and Jurafsky, 2009), causal relation (Mostafazadeh et al., 2016b), or narrative order (Jans et al., 2012). Event schemas have become increasingly important for natural language understanding tasks such as story 1 Our code and data are publicly available for research purpose at http://blender.cs.illinois.edu/software/ pathlm. ending prediction (Mostafazadeh et al., 2016a) and reading comprehension (Koˇcisk´y et al., 2018; Ostermann et al., 2019). Previous schema induction methods mostly ignore uncertainty, re-occurring events and multiple hypotheses, with limited attention to capture complex relations among events, other than temporal or causal relations."
2020.emnlp-main.50,W16-1007,1,0.880849,"ream end-to-end Information Extraction over a state-of-the-art joint neural extraction model, when used as additional global features to unfold instance graphs.1 1 Introduction Existing approaches to automated event extraction retain the overly simplistic assumption that events are atomic occurrences. Understanding events requires knowledge in the form of a repository of abstracted event schemas (complex event templates). Scripts (Schank and Abelson, 1977) encode frequently recurring event sequences, where events are ordered by temporal relation (Chambers and Jurafsky, 2009), causal relation (Mostafazadeh et al., 2016b), or narrative order (Jans et al., 2012). Event schemas have become increasingly important for natural language understanding tasks such as story 1 Our code and data are publicly available for research purpose at http://blender.cs.illinois.edu/software/ pathlm. ending prediction (Mostafazadeh et al., 2016a) and reading comprehension (Koˇcisk´y et al., 2018; Ostermann et al., 2019). Previous schema induction methods mostly ignore uncertainty, re-occurring events and multiple hypotheses, with limited attention to capture complex relations among events, other than temporal or causal relations."
2020.emnlp-main.50,P15-1019,0,0.145444,"Missing"
2020.emnlp-main.50,S19-1012,0,0.0429448,"ent schemas (complex event templates). Scripts (Schank and Abelson, 1977) encode frequently recurring event sequences, where events are ordered by temporal relation (Chambers and Jurafsky, 2009), causal relation (Mostafazadeh et al., 2016b), or narrative order (Jans et al., 2012). Event schemas have become increasingly important for natural language understanding tasks such as story 1 Our code and data are publicly available for research purpose at http://blender.cs.illinois.edu/software/ pathlm. ending prediction (Mostafazadeh et al., 2016a) and reading comprehension (Koˇcisk´y et al., 2018; Ostermann et al., 2019). Previous schema induction methods mostly ignore uncertainty, re-occurring events and multiple hypotheses, with limited attention to capture complex relations among events, other than temporal or causal relations. Temporal relations exist between almost all events, even those that are not semantically related; while research in identifying causal relations has been hobbled by low inter-annotator agreement (Hong et al., 2016). In this paper, we hypothesize that two events are connected when their entity arguments are coreferential or semantically related. For example, in Figure 1, (a) and (b)"
2020.emnlp-main.50,P17-1178,1,0.846469,"ument roles. We follow our recent work on ACE IE (Lin et al., 2020) to split the data. We consider the training set as historical data to train the LM, and the test set as our target data to induce schema for target scenarios. The instance graphs of the target data set are constructed from manual annotations. For historical data, we construct event instance graphs from both manual annotations (Historicalann ) and system extraction results (Historicalsys ) from the state-ofthe-art IE model (Lin et al., 2020). We perform cross-document entity coreference resolution by applying an entity linker (Pan et al., 2017) for both annotated and system generated instance graphs. Table 2 shows the data statistics. Split Historicalann Historicalsys Validation Target The cardinality for an instance graph and a schema will be the number of substructures in each, i.e., X |g|I = count(hvm , emn , vn i), hvm ,emn ,vn i∈g |s|S = 47,525 48,664 3,422 3,673 7,152 7,018 728 802 4,419 4,426 468 424 By extension, each path of length l=5 in a graph schema [φi , ψij , φj , ψjk , φk ] contains two consecutive triples hφi , ψij ,φj i, hφj , ψjk , φk i∈s, and a matched instance path contains two consecutive instance triples hvm ,"
2020.emnlp-main.50,K19-1051,0,0.111184,"Missing"
2020.emnlp-main.50,N18-1202,0,0.129853,"event but fails to extract the I NVESTIGATE C RIME triggered by “discovered” and its D EFENDANT argument “Mohammed A. Salameh”. Event graph scehmas can inform the model that a person who is arrested was usually investigated, our IE system can fix this missing error. Therefore we also conduct extrinsic evaluations and show the effectiveness of the induced schema repository in enhancing downstream end-to-end IE tasks. PART- WHOLE PLACE−1 −−−−−−−→ GPE −−−−−→ ATTACK. We train the path language model on two tasks: learning an auto-regressive language model (Ponte and Croft, 1998; Dai and Le, 2015; Peters et al., 2018; Radford et al.; Yang et al., 2019) to predict an edge or a node, given previous edges and nodes in a path, and a neighboring path classification task to predict how likely two paths co-occur. The path language model is trained from all the paths between two event instances from the same document, based on the assumption that events from the same document (especially news document) tell a coherent story. We propose two intrinsic evaluation metrics, instance coverage and instance coherence, to assess when event instance graphs are covered by each 685 In summary, we make the following novel con"
2020.emnlp-main.50,E14-1024,0,0.100189,"Missing"
2020.emnlp-main.50,D15-1195,0,0.203725,"Missing"
2020.emnlp-main.50,N16-1049,0,0.151836,"Missing"
2020.emnlp-main.50,W17-0901,0,0.0336757,"Missing"
2020.emnlp-main.50,W19-3404,0,0.0428587,"Missing"
2020.emnlp-main.713,N16-1181,0,0.0387079,"Missing"
2020.emnlp-main.713,D18-1549,0,0.0223216,"seudo-Decompositions Since subquestions are retrieval-based, they are often not about the same entities as q. Inspired by retrieve-and-edit methods (e.g., Guu et al., 2018), we replace each sub-question entity not in q with an entity from q of the same type (e.g., “Date” or “Location”) if possible.5 This step is important for PseudoD and Seq2Seq (which would learn to hallucinate entities) but not ONUS (which must reconstruct entities in q from its own decomposition, as discussed next). 3.2.3 Unsupervised Decomposition Models Pretraining Pretraining is crucial for unsupervised Seq2Seq methods (Artetxe et al., 2018; Lample et al., 2018), so we initialize all decomposition models (Seq2Seq or ONUS) with the same pretrained weights. We warm-start our pretraining with the pretrained, English Masked Language Model (MLM) from Lample and Conneau (2019), a 12-block transformer (Vaswani et al., 2017). We do MLM finetuning for one epoch on Q and pseudodecompositions D formed via random retrieval, using the final weights to initialize a pretrained encoder-decoder. See Appendix §B.2 for details. Seq2Seq We finetune the pretrained encoderdecoder using maximum likelihood. We stop training based on validation BLEU bet"
2020.emnlp-main.713,P19-1309,0,0.0593854,"rvision, we find it possible to decompose questions in a fully unsupervised way. We propose an algorithm for One-to-N Unsupervised Sequence transduction (ONUS) that learns to map from the distribution of hard questions to that of many simple questions. First, we automatically create a noisy “pseudo-decomposition” for each hard question by using embedding similarity to retrieve sub-question candidates. We mine over 10M possible sub-questions from Common Crawl with a classifier, showcasing the effectiveness of parallel corpus mining, a common approach in machine translation (Xu and Koehn, 2017; Artetxe and Schwenk, 2019), for QA. Second, we train a decomposition model on the mined data with unsupervised sequence-to-sequence learning, allowing ONUS to improve over pseudo-decompositions. As a result, we are able to train a large transformer model to generate decompositions, surpassing the fluency of heuristic/extractive decompositions. Figure 2 overviews our approach to decomposition. We validate ONUS on multi-hop QA, where questions require reasoning over multiple pieces of evidence. We use an off-the-shelf single-hop QA model to answer decomposed sub-questions. Then, we give sub-questions and their answers to"
2020.emnlp-main.713,Q17-1010,0,0.0230205,"y increases the number of single-hop questions (130K → 10.1M) and multi-hop questions (90K → 2.4M), showing the power of parallel corpus mining in QA. 2 3.2.2 Creating Pseudo-Decompositions To create pseudo-decompositions (retrieval-based sub-questions for a given question), we experimented with using a variable number of subquestions N per question (Appendix §A.1), but we found similar QA results with a fixed N = 2, which we use in the remainder for simplicity. Similarity-based Retrieval To retrieve relevant sub-questions, we embed any text t into a vector vt by summing the FastText vectors (Bojanowski et al., 2017)3 for words in t and use cosine as our similarity metric f .4 Let q be a multi-hop question ˆ be the with a pseudo-decomposition (s∗1 , s∗2 ) and v unit vector of v. Since N = 2, Eq. 1 simplifies to: h i ˆ s2 − v ˆq> v ˆs1 + v ˆq> v ˆs>1 v ˆ s2 (s∗1 , s∗2 ) = argmax v {s1 ,s2 }∈S The last term requires O(|S|2 ) comparisons, which is expensive as |S |> 10M. Instead of solving the above equation exactly, we find an approximate 0 , s0 ) by computing over pseudo-decomposition (s 1 2   S 0 = topK{s∈S} v ˆq> v ˆs with K = 1000. We efficiently build S 0 with FAISS (Johnson et al., 2017a). Random Re"
2020.emnlp-main.713,P17-1171,0,0.0844335,"Missing"
2020.emnlp-main.713,P18-1078,0,0.0373823,"hop) 7 (Baseline) 66.7 77.0±.2 63.7 65.2±.2 66.5 67.1±.5 PseudoD Model Architecture Our model takes in a question and several paragraphs to predict the answer. We compute a separate forward pass on each paragraph (with the question). For each paragraph, the model learns to predict the answer span if the paragraph contains the answer and to predict “no answer” otherwise. We treat yes or no predictions as spans within the passage (prepended to each paragraph), as in Nie et al. (2019) on H OTPOT QA. During inference, for the final softmax, we consider all paragraphs as a single chunk. Similar to Clark and Gardner (2018), we subtract a paragraph’s “no answer” logit from the logits of all spans in that paragraph, to reduce or increase span probabilities accordingly. In other words, we compute the probability p(sp ) of each span sp in a paragraph p ∈ {1, . . . , P } using the predicted span logit l(sp ) and “no answer” paragraph logit n(p) with p(sp ) ∝ el(sp )−n(p) . RO BERTA LARGE (Liu et al., 2019) is used as our pretrained model. Seq2Seq Random FastText Random FastText Random FastText 78.4±.2 78.9±.2 77.7±.2 78.9±.2 79.8±.1 80.1±.2 70.9±.2 72.4±.1 69.4±.3 73.1±.2 76.0±.2 76.2±.1 70.7±.4 72.0±.1 70.0±.7 73.0"
2020.emnlp-main.713,N19-1423,0,0.0236315,"set versions: (1) the original version,9 (2) the multi-hop version from Jiang and Bansal (2019a) who created some distractor paragraphs adversarially to test multi-hop reasoning, and (3) the out-of-domain (OOD) version from Min et al. (2019b) who retrieved distractor paragraphs with the same procedure as the original version but excluded the original paragraphs. Main Results Table 1 shows how unsupervised decompositions affect QA. Our RO BERTA baseline does quite well on H OTPOT QA (77.0 F1), in line with Min et al. (2019a) who achieved strong results using a BERT-based version of the model (Devlin et al., 2019). We achieve large gains over the RO BERTA baseline by simply adding sub-questions and sub-answers to the input. Using decompositions from ONUS trained on FastText 9 Test set is private, so we randomly halve the dev set to form validation/held-out dev sets. Our codebase has our splits. 8868 QType Using Decomps. 7 X SQs SAs QA F1 7 7 77.0±.2 Bridge Comp. Inters. 1-hop 80.1±.2 73.8±.4 79.4±.6 73.9±.6 81.7±.4 80.1±.3 82.3±.5 76.9±.6 X X X X 7 Sent. Span Rand. 7 Sent. 80.1±.2 77.8±.3 76.9±.2 76.9±.2 80.2±.1 Table 2: Left: Decompositions improve QA F1 for all 4 H OTPOT QA types. Right (Ablation): Q"
2020.emnlp-main.713,E17-2068,0,0.0335695,"m large training corpora. A larger simple question corpus will also improve the relevance of retrieved simple questions to the hard 8866 question. Thus, we take inspiration from parallel corpus mining in machine translation (Xu and Koehn, 2017; Artetxe and Schwenk, 2019). We use questions from SQ UAD 2 and H OTPOT QA to form our initial corpora S (single-hop questions) and Q (multi-hop questions), respectively, and we augment Q and S by mining more questions from Common Crawl. First, we select sentences that start with “wh”-words or end in “?” Next, we train an efficient, FastText classifier (Joulin et al., 2017) to classify between questions sampled from Common Crawl, SQ UAD 2, and H OTPOT QA (60K in total). Then, we classify our Common Crawl questions, adding those classified as SQ UAD 2 questions to S and those classified as H OTPOT QA questions to Q. Mining greatly increases the number of single-hop questions (130K → 10.1M) and multi-hop questions (90K → 2.4M), showing the power of parallel corpus mining in QA. 2 3.2.2 Creating Pseudo-Decompositions To create pseudo-decompositions (retrieval-based sub-questions for a given question), we experimented with using a variable number of subquestions N p"
2020.emnlp-main.713,P16-1004,0,0.0292627,"COMP RC decomposes a multi-hop question using a heuristic algorithm or not at all. Watson and D E COMP RC use special case handling to decompose different questions, while our algorithm is fully automated and requires little hand-engineering. More traditional, semantic parsing methods map questions to compositional programs, whose subprograms can be viewed as question decompositions in a formal language (Talmor and Berant, 2018; Wolfson et al., 2020). Examples include classical QA systems like SHRDLU (Winograd, 1972) and LUNAR (Woods et al., 1974), as well as neural Seq2Seq semantic parsers (Dong and Lapata, 2016) and neural module networks (Andreas et al., 2015, 2016). Such methods usually require 8871 strong, program-level supervision to generate programs, as in visual QA (Johnson et al., 2017c) and on H OTPOT QA (Jiang and Bansal, 2019b). Some models use other forms of strong supervision, e.g., the sentences needed to answer a question, as annotated by H OTPOT QA. Such an approach is taken by SAE (Tu et al., 2020) and HGN (Fang et al., 2019), whose methods may be combined with ours. Unsupervised decomposition complements strongly and weakly supervised decomposition approaches. Our unsupervised appro"
2020.emnlp-main.713,D18-1091,0,0.0218187,"Q2 What is the name of the variation on a popular anecdote? x “Mrs. Bixby and the Colonel’s Coat” is a short story by Roald Dahl that first appeared in the 1959 issue of Nugget. ˆ more than 250 million A: 4.4 GPT2 NLL Unsupervised Decomposition Model Intrinsic Evaluation of Decompositions We evaluate the quality of decompositions on other metrics aside from downstream QA. To measure the fluency of decompositions, we compute the likelihood of decompositions using the pretrained GPT-2 language model (Radford et al., 2019). We train a BERT BASE classifier on the questionwellformedness dataset of Faruqui and Das (2018), and we use the classifier to estimate the proportion of sub-questions that are well-formed. We measure how abstractive decompositions are by computing (i) the token Levenstein distance between the multihop question and its generated decomposition and (ii) the ratio between the length of the decomposition and the length of the multi-hop question. We compare ONUS to D ECOMP RC (Min et al., 2019b), a supervised+heuristic decomposition method. As shown in Table 4, ONUS decompositions are more natural and well-formed than D ECOMP RC decompositions. As an example, for Table 3 Q3, D ECOMP RC produc"
2020.emnlp-main.713,Q18-1031,0,0.0158481,"h FAISS (Johnson et al., 2017a). Random Retrieval For comparison, we test a random pseudo-decomposition baseline, where we retrieve s1 , . . . , sN by sampling uniformly from S. 2 See Appendix §A.3 for details on question classifier. 300-dim. English Common Crawl vectors: https:// fasttext.cc/docs/en/english-vectors.html 4 We also tried TFIDF and BERT representations but did not see improvements over FastText (see Appendix §A.4). 3 Editing Pseudo-Decompositions Since subquestions are retrieval-based, they are often not about the same entities as q. Inspired by retrieve-and-edit methods (e.g., Guu et al., 2018), we replace each sub-question entity not in q with an entity from q of the same type (e.g., “Date” or “Location”) if possible.5 This step is important for PseudoD and Seq2Seq (which would learn to hallucinate entities) but not ONUS (which must reconstruct entities in q from its own decomposition, as discussed next). 3.2.3 Unsupervised Decomposition Models Pretraining Pretraining is crucial for unsupervised Seq2Seq methods (Artetxe et al., 2018; Lample et al., 2018), so we initialize all decomposition models (Seq2Seq or ONUS) with the same pretrained weights. We warm-start our pretraining with"
2020.emnlp-main.713,P19-1262,0,0.275833,"er decomposed sub-questions. Then, we give sub-questions and their answers to a recomposition model to combine into a final answer. We evaluate on three dev sets for H OTPOT QA, a standard benchmark for multi-hop QA (Yang et al., 2018), including two challenge sets. ONUS proves to be a powerful tool for QA in the following ways. First, QA models that use decompositions outperform a strong RoBERTa baseline (Liu et al., 2019; Min et al., 2019a) by 3.1 points in F1 on the original dev set, 10 points on the out-of-domain dev set from Min et al. (2019b), and 11 points on the multi-hop dev set from Jiang and Bansal (2019a). Our method is competitive with state-of-the-art methods SAE (Tu et al., 2020) and HGN (Fang et al., 2019) that use additional, strong supervision on which sentences are relevant to the question. Second, our analysis shows that sub-questions improve multi-hop QA by using the single-hop QA model to retrieve question-relevant text. Qualitative examples illustrate how the retrieved text adds a level of interpretability to otherwise black-box, neural QA models. Third, ONUS automatically learns to generate useful decompositions for all four question types in H OTPOT QA, highlighting the general"
2020.emnlp-main.713,P19-1484,1,0.847698,") between predicted and gold spans. 3.2 3.2.1 2.1.2 Learning to Decompose With the above pseudo-decompositions, we explore various decomposition methods (details in §3.2.3): PseudoD We use sub-questions from pseudodecompositions directly in downstream QA. Sequence-to-Sequence (Seq2Seq) We train a Seq2Seq model pθ to maximize log pθ (d0 |q). Question Answering Task Unsupervised Decomposition Training Data and Question Mining Supervised decomposition methods are limited by the amount of available human annotation, but our unsupervised method faces no such limitation, similar to unsupervised QA (Lewis et al., 2019). Since we need to train data-hungry Seq2Seq models, we would benefit from large training corpora. A larger simple question corpus will also improve the relevance of retrieved simple questions to the hard 8866 question. Thus, we take inspiration from parallel corpus mining in machine translation (Xu and Koehn, 2017; Artetxe and Schwenk, 2019). We use questions from SQ UAD 2 and H OTPOT QA to form our initial corpora S (single-hop questions) and Q (multi-hop questions), respectively, and we augment Q and S by mining more questions from Common Crawl. First, we select sentences that start with “w"
2020.emnlp-main.713,2021.ccl-1.108,0,0.209817,"Missing"
2020.emnlp-main.713,P19-1416,0,0.0732106,"Albert Camus?” (Petrochuk and Zettlemoyer, 2018). Thus, a promising strategy to answer hard questions is divide-and-conquer: decompose a hard question into simpler sub-questions, answer the sub-questions with a QA system, and recompose the resulting answers into a final answer, as shown in Figure 1. This approach leverages strong performance on simple questions to help answer harder questions (Christiano et al., 2018). Existing work decomposes questions using a combination of hand-crafted heuristics, rule-based algorithms, and learning from supervised decompositions (Talmor and Berant, 2018; Min et al., 2019b), which each require significant human effort. For example, D ECOMP RC (Min et al., 2019b) decomposes some questions using supervision and other questions using a heuristic algorithm with fine-grained, special case handling based on part8864 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 8864–8880, c November 16–20, 2020. 2020 Association for Computational Linguistics ? ? ? Hard Question ? Simple Question ? ? Step 1 Pseudo Decomp. ? ? ? ? Step 2 ONUS ? ? ? ? or Step 2 Seq2Seq ? ? ? ? ? ? ? ? ? ? ? ? ? Figure 2: One-to-N Unsupervised Sequence tra"
2020.emnlp-main.713,D19-1455,0,0.314797,"er decomposed sub-questions. Then, we give sub-questions and their answers to a recomposition model to combine into a final answer. We evaluate on three dev sets for H OTPOT QA, a standard benchmark for multi-hop QA (Yang et al., 2018), including two challenge sets. ONUS proves to be a powerful tool for QA in the following ways. First, QA models that use decompositions outperform a strong RoBERTa baseline (Liu et al., 2019; Min et al., 2019a) by 3.1 points in F1 on the original dev set, 10 points on the out-of-domain dev set from Min et al. (2019b), and 11 points on the multi-hop dev set from Jiang and Bansal (2019a). Our method is competitive with state-of-the-art methods SAE (Tu et al., 2020) and HGN (Fang et al., 2019) that use additional, strong supervision on which sentences are relevant to the question. Second, our analysis shows that sub-questions improve multi-hop QA by using the single-hop QA model to retrieve question-relevant text. Qualitative examples illustrate how the retrieved text adds a level of interpretability to otherwise black-box, neural QA models. Third, ONUS automatically learns to generate useful decompositions for all four question types in H OTPOT QA, highlighting the general"
2020.emnlp-main.713,P19-1613,0,0.175238,"Albert Camus?” (Petrochuk and Zettlemoyer, 2018). Thus, a promising strategy to answer hard questions is divide-and-conquer: decompose a hard question into simpler sub-questions, answer the sub-questions with a QA system, and recompose the resulting answers into a final answer, as shown in Figure 1. This approach leverages strong performance on simple questions to help answer harder questions (Christiano et al., 2018). Existing work decomposes questions using a combination of hand-crafted heuristics, rule-based algorithms, and learning from supervised decompositions (Talmor and Berant, 2018; Min et al., 2019b), which each require significant human effort. For example, D ECOMP RC (Min et al., 2019b) decomposes some questions using supervision and other questions using a heuristic algorithm with fine-grained, special case handling based on part8864 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 8864–8880, c November 16–20, 2020. 2020 Association for Computational Linguistics ? ? ? Hard Question ? Simple Question ? ? Step 1 Pseudo Decomp. ? ? ? ? Step 2 ONUS ? ? ? ? or Step 2 Seq2Seq ? ? ? ? ? ? ? ? ? ? ? ? ? Figure 2: One-to-N Unsupervised Sequence tra"
2020.emnlp-main.713,D19-1258,0,0.0617453,"tune a pretrained model for single-hop QA following prior work from Min et al. (2019b) on H OTPOT QA, as described below.8 7 7 7 (1hop) 7 (Baseline) 66.7 77.0±.2 63.7 65.2±.2 66.5 67.1±.5 PseudoD Model Architecture Our model takes in a question and several paragraphs to predict the answer. We compute a separate forward pass on each paragraph (with the question). For each paragraph, the model learns to predict the answer span if the paragraph contains the answer and to predict “no answer” otherwise. We treat yes or no predictions as spans within the passage (prepended to each paragraph), as in Nie et al. (2019) on H OTPOT QA. During inference, for the final softmax, we consider all paragraphs as a single chunk. Similar to Clark and Gardner (2018), we subtract a paragraph’s “no answer” logit from the logits of all spans in that paragraph, to reduce or increase span probabilities accordingly. In other words, we compute the probability p(sp ) of each span sp in a paragraph p ∈ {1, . . . , P } using the predicted span logit l(sp ) and “no answer” paragraph logit n(p) with p(sp ) ∝ el(sp )−n(p) . RO BERTA LARGE (Liu et al., 2019) is used as our pretrained model. Seq2Seq Random FastText Random FastText Ra"
2020.emnlp-main.713,D17-1061,1,0.84176,"thods to leverage millions of otherwise unusable questions, similar to work on unsupervised QA (Lewis et al., 2019). When decomposition examples exist, supervised and unsupervised learning can be used in tandem to learn from both labeled and unlabeled examples. Such semi-supervised methods outperform supervised learning for tasks like machine translation (Sennrich et al., 2016). Other work on weakly supervised question generation uses a downstream QA model’s accuracy as a signal for learning to generate useful questions. Weakly supervised question generation often uses reinforcement learning (Nogueira and Cho, 2017; Wang and Lake, 2019; Strub et al., 2017; Das et al., 2017; Liang et al., 2018), where an unsupervised initialization can greatly mitigate the issues of exploring from scratch (Jaderberg et al., 2017). 7 Conclusion We proposed a QA system that answers a question via decomposition, without supervised question decompositions, using three stages: (1) decompose a question into many sub-questions using One-toN Unsupervised Sequence transduction (ONUS), (2) answer sub-questions with an off-the-shelf QA system, and (3) recompose sub-answers into a final answer. When evaluated on three H OTPOT QA dev"
2020.emnlp-main.713,D18-1051,0,0.0253497,"Question answering (QA) systems struggle to answer complex questions such as “What profession do H. L. Mencken and Albert Camus have in common?” since the required information is scattered in different places (Yang et al., 2018). However, QA systems accurately answer ∗ journalist KC was a part-time research scientist at Facebook AI Research while working on this paper. 1 Our code, data, and pretrained models are available at https://github.com/facebookresearch/ UnsupervisedDecomposition. simpler, related questions such as “What profession does H. L. Mencken have?” and “Who was Albert Camus?” (Petrochuk and Zettlemoyer, 2018). Thus, a promising strategy to answer hard questions is divide-and-conquer: decompose a hard question into simpler sub-questions, answer the sub-questions with a QA system, and recompose the resulting answers into a final answer, as shown in Figure 1. This approach leverages strong performance on simple questions to help answer harder questions (Christiano et al., 2018). Existing work decomposes questions using a combination of hand-crafted heuristics, rule-based algorithms, and learning from supervised decompositions (Talmor and Berant, 2018; Min et al., 2019b), which each require significan"
2020.emnlp-main.713,P16-1009,0,0.155251,"tart our pretraining with the pretrained, English Masked Language Model (MLM) from Lample and Conneau (2019), a 12-block transformer (Vaswani et al., 2017). We do MLM finetuning for one epoch on Q and pseudodecompositions D formed via random retrieval, using the final weights to initialize a pretrained encoder-decoder. See Appendix §B.2 for details. Seq2Seq We finetune the pretrained encoderdecoder using maximum likelihood. We stop training based on validation BLEU between generated decompositions and pseudo-decompositions. ONUS We finetune the pretrained encoderdecoder with back-translation (Sennrich et al., 2016) and denoising objectives simultaneously, similar to Lample and Conneau (2019) in unsupervised one-to-one translation.6 For denoising, we produce a noisy input d0 by randomly masking, dropping, and locally shuffling tokens in d ∼ D, and we train a model with parameters θ to maximize log pθ (d|d0 ). We likewise maximize log pθ (q|q 0 ) for a noised version q 0 of q ∼ Q. For back-translation, we generate a multihop question qˆ for a decomposition d ∼ D, and we maximize log pθ (d|ˆ q ). Similarly, we maximize ˆ for a model-generated decomposition dˆ log pθ (q|d) of q ∼ Q. To stop training without"
2020.emnlp-main.713,2020.tacl-1.13,0,0.126801,"frames subquestions as extractive spans of a question, learning to predict span-based sub-questions via supervised learning on human annotations. In other cases, D E COMP RC decomposes a multi-hop question using a heuristic algorithm or not at all. Watson and D E COMP RC use special case handling to decompose different questions, while our algorithm is fully automated and requires little hand-engineering. More traditional, semantic parsing methods map questions to compositional programs, whose subprograms can be viewed as question decompositions in a formal language (Talmor and Berant, 2018; Wolfson et al., 2020). Examples include classical QA systems like SHRDLU (Winograd, 1972) and LUNAR (Woods et al., 1974), as well as neural Seq2Seq semantic parsers (Dong and Lapata, 2016) and neural module networks (Andreas et al., 2015, 2016). Such methods usually require 8871 strong, program-level supervision to generate programs, as in visual QA (Johnson et al., 2017c) and on H OTPOT QA (Jiang and Bansal, 2019b). Some models use other forms of strong supervision, e.g., the sentences needed to answer a question, as annotated by H OTPOT QA. Such an approach is taken by SAE (Tu et al., 2020) and HGN (Fang et al.,"
2020.emnlp-main.713,P02-1040,0,\N,Missing
2020.emnlp-main.713,D17-1319,0,\N,Missing
2020.emnlp-main.713,D18-1259,0,\N,Missing
2020.emnlp-main.73,D19-1633,0,0.565761,"e translation systems are autoregressive, hence decoding latency grows linearly with respect to the length of the target sentence. For faster generation, several work proposed nonautoregressive models with sub-linear decoding latency given sufficient parallel computation (Gu et al., 2018a; Lee et al., 2018; Kaiser et al., 2018). As it is challenging to precisely model the dependencies among the tokens without autoregression, many existing non-autoregressive models first generate an initial translation which is then iteratively refined to yield better output (Lee et al., 2018; Gu et al., 2019; Ghazvininejad et al., 2019). While various training objectives are used to admit refinement (e.g. denoising, evidence lowerbound maximization and mask language modeling), the generation process of these models is similar in that the refinement process happens in the discrete space of sentences. Meanwhile, another line of work proposed to use continuous latent variables for non-autoregressive translation, such that the distribution of the target sentences can be factorized over time given the latent variables (Ma et al., 2019; Shu et al., 2020). Unlike the models discussed above, finding the most likely target sentence u"
2020.emnlp-main.73,N18-2081,0,0.245024,") it is more effective, resulting in higher marginal probabilities and BLEU scores with the same number of refinement steps. On WMT’14 En→De, for instance, our approach is able to decode 6.2 times faster than the autoregressive model with minimal degradation to translation quality (0.9 BLEU). 1 Introduction Most neural machine translation systems are autoregressive, hence decoding latency grows linearly with respect to the length of the target sentence. For faster generation, several work proposed nonautoregressive models with sub-linear decoding latency given sufficient parallel computation (Gu et al., 2018a; Lee et al., 2018; Kaiser et al., 2018). As it is challenging to precisely model the dependencies among the tokens without autoregression, many existing non-autoregressive models first generate an initial translation which is then iteratively refined to yield better output (Lee et al., 2018; Gu et al., 2019; Ghazvininejad et al., 2019). While various training objectives are used to admit refinement (e.g. denoising, evidence lowerbound maximization and mask language modeling), the generation process of these models is similar in that the refinement process happens in the discrete space of sen"
2020.emnlp-main.73,Q19-1042,1,0.927338,"ost neural machine translation systems are autoregressive, hence decoding latency grows linearly with respect to the length of the target sentence. For faster generation, several work proposed nonautoregressive models with sub-linear decoding latency given sufficient parallel computation (Gu et al., 2018a; Lee et al., 2018; Kaiser et al., 2018). As it is challenging to precisely model the dependencies among the tokens without autoregression, many existing non-autoregressive models first generate an initial translation which is then iteratively refined to yield better output (Lee et al., 2018; Gu et al., 2019; Ghazvininejad et al., 2019). While various training objectives are used to admit refinement (e.g. denoising, evidence lowerbound maximization and mask language modeling), the generation process of these models is similar in that the refinement process happens in the discrete space of sentences. Meanwhile, another line of work proposed to use continuous latent variables for non-autoregressive translation, such that the distribution of the target sentences can be factorized over time given the latent variables (Ma et al., 2019; Shu et al., 2020). Unlike the models discussed above, finding the"
2020.emnlp-main.73,D17-1014,0,0.0227985,"g interpretation of the Parzen score objective (Vincent, 2011) that avoids estimating the Hessian. Although score function estimation that bypasses energy estimation was found to be unstable (Alain and Bengio, 2014; Saremi et al., 2018), it has been successfully applied to generative modeling of images (Song and Ermon, 2019). Inference While we categorize inference methods for machine translation as (1) discrete search, (2) hybrid optimization (Shu et al., 2020) and (3) continuous optimization (this work) in Section 2, another line of work relaxes discrete search into continuous optimization (Hoang et al., 2017; Gu et al., 2018b; Tu et al., 2020). By using Gumbelsoftmax relaxation (Maddison et al., 2017; Jang et al., 2017), they train an inference network to generate target tokens that maximize the log probability under a pretrained model. 1012 Example 1 Source Reference Original Refined There aren ’t many doctors in the west African country ; just one for every 5,000 people ¨ In dem westafrikanischen Land gibt es nicht viele Arzte, nur einen f¨ur 5.000 Menschen ¨ Es gibt nicht viele Arzte im westafrikanischen Land, nur eine f¨ur 5.000 Menschen. ¨ Im westafrikanischen Land gibt es nicht viele Arzte,"
2020.emnlp-main.73,D16-1139,0,0.168881,"Missing"
2020.emnlp-main.73,D18-2012,0,0.0203332,"see Alg. 1). As our inference procedure only involves optimization in the continuous space each step, we avoid having to search over a large vocabulary. We can either perform iterative refinement for a fixed number of steps, or until some convergence condition is satisfied. 4 Experimental Setup 4.1 Datasets and Preprocessing We evaluate our approach on three widely used machine translation datasets: IWSLT’16 De→En2 (containing 197K training, 2K development and 2K test sentence pairs), WMT’16 Ro→En3 (612K, 2K, 2K pairs) and WMT’14 En→De4 (4.5M, 3K, 3K pairs). We use sentencepiece tokenization (Kudo and Richardson, 2018) with 32K sentencepieces on all datasets. For WMT’16 Ro→En, we follow Sennrich et al. (2016) and normalize Romanian and remove diacritics before applying tokenization. For training, we discard sentence pairs if either the source or the target length exceeds 64 tokens. Following Lee et al. (2018), we remove repetitions from the translations with a simple postprocessing step before computing BLEU scores. We use detokenized BLEU with Sacrebleu (Post, 2018). Distillation Following previous work on non-autoregressive translation, we train non2 Models and Baselines https://wit3.fbk.eu/ 3 www.statmt."
2020.emnlp-main.73,D18-1149,1,0.91593,"tive, resulting in higher marginal probabilities and BLEU scores with the same number of refinement steps. On WMT’14 En→De, for instance, our approach is able to decode 6.2 times faster than the autoregressive model with minimal degradation to translation quality (0.9 BLEU). 1 Introduction Most neural machine translation systems are autoregressive, hence decoding latency grows linearly with respect to the length of the target sentence. For faster generation, several work proposed nonautoregressive models with sub-linear decoding latency given sufficient parallel computation (Gu et al., 2018a; Lee et al., 2018; Kaiser et al., 2018). As it is challenging to precisely model the dependencies among the tokens without autoregression, many existing non-autoregressive models first generate an initial translation which is then iteratively refined to yield better output (Lee et al., 2018; Gu et al., 2019; Ghazvininejad et al., 2019). While various training objectives are used to admit refinement (e.g. denoising, evidence lowerbound maximization and mask language modeling), the generation process of these models is similar in that the refinement process happens in the discrete space of sentences. Meanwhile,"
2020.emnlp-main.73,2020.spnlp-1.10,1,0.817251,"Missing"
2020.emnlp-main.73,D19-1437,0,0.438954,"then iteratively refined to yield better output (Lee et al., 2018; Gu et al., 2019; Ghazvininejad et al., 2019). While various training objectives are used to admit refinement (e.g. denoising, evidence lowerbound maximization and mask language modeling), the generation process of these models is similar in that the refinement process happens in the discrete space of sentences. Meanwhile, another line of work proposed to use continuous latent variables for non-autoregressive translation, such that the distribution of the target sentences can be factorized over time given the latent variables (Ma et al., 2019; Shu et al., 2020). Unlike the models discussed above, finding the most likely target sentence under these models requires searching over continuous latent variables. To this end, Shu et al. (2020) proposed an EM-like inference procedure that optimizes over a hybrid space consisting of both continuous and discrete variables. By introducing a deterministic delta posterior, it maximizes a proxy lowerbound by alternating between matching the delta posterior to the original approximate posterior (continuous optimization), and finding a target sentence that maximizes the proxy lowerbound (discrete"
2020.emnlp-main.73,N19-4009,0,0.0769184,"Missing"
2020.emnlp-main.73,W18-6319,0,0.0217458,"sentence pairs), WMT’16 Ro→En3 (612K, 2K, 2K pairs) and WMT’14 En→De4 (4.5M, 3K, 3K pairs). We use sentencepiece tokenization (Kudo and Richardson, 2018) with 32K sentencepieces on all datasets. For WMT’16 Ro→En, we follow Sennrich et al. (2016) and normalize Romanian and remove diacritics before applying tokenization. For training, we discard sentence pairs if either the source or the target length exceeds 64 tokens. Following Lee et al. (2018), we remove repetitions from the translations with a simple postprocessing step before computing BLEU scores. We use detokenized BLEU with Sacrebleu (Post, 2018). Distillation Following previous work on non-autoregressive translation, we train non2 Models and Baselines https://wit3.fbk.eu/ 3 www.statmt.org/wmt16/translation-task. html 4 www.statmt.org/wmt14/translation-task. html Non-autoregressive latent variable models We closely follow the implementation details from (Shu et al., 2020). The prior and the approximate posterior distributions are spherical Gaussian distributions with learned mean and variance, and the decoder is factorized over time. The only difference is at inference time, the target sentence length is predicted once and fixed throu"
2020.emnlp-main.73,W16-2323,0,0.0300139,"step, we avoid having to search over a large vocabulary. We can either perform iterative refinement for a fixed number of steps, or until some convergence condition is satisfied. 4 Experimental Setup 4.1 Datasets and Preprocessing We evaluate our approach on three widely used machine translation datasets: IWSLT’16 De→En2 (containing 197K training, 2K development and 2K test sentence pairs), WMT’16 Ro→En3 (612K, 2K, 2K pairs) and WMT’14 En→De4 (4.5M, 3K, 3K pairs). We use sentencepiece tokenization (Kudo and Richardson, 2018) with 32K sentencepieces on all datasets. For WMT’16 Ro→En, we follow Sennrich et al. (2016) and normalize Romanian and remove diacritics before applying tokenization. For training, we discard sentence pairs if either the source or the target length exceeds 64 tokens. Following Lee et al. (2018), we remove repetitions from the translations with a simple postprocessing step before computing BLEU scores. We use detokenized BLEU with Sacrebleu (Post, 2018). Distillation Following previous work on non-autoregressive translation, we train non2 Models and Baselines https://wit3.fbk.eu/ 3 www.statmt.org/wmt16/translation-task. html 4 www.statmt.org/wmt14/translation-task. html Non-autoregre"
2020.emnlp-main.73,P91-1024,0,0.382149,"fully applied to many structured prediction tasks (Belanger and McCallum, 2016; Wang et al., 2016; Belanger et al., 2017). Other work performed gradient descent over the latent variables to optimize objectives for a wide variety of tasks, including chemical design (G´omez-Bombarelli et al., 2018) and text generation (Mueller et al., 2017) Generation by Refinement Refinement has a long history in text generation. The retrieve-andrefine framework retrieves an (input, output) pair from the training set that is similar to the test example, and performs edit operations on the corresponding output (Sumita and Iida, 1991; Song et al., 2016; Hashimoto et al., 2018; Weston et al., 2018; Gu et al., 2018c). The idea of refinement has also been applied in automatic post-editing (Novak et al., 2016; Grangier and Auli, 2017). 8 Conclusion We propose an efficient inference procedure for non-autoregressive machine translation that refines translations purely in the continuous space. Given a latent variable model for machine translation, we train an inference network to approximate the gradient of the marginal log probability with respect to the target sentence, using only the latent variable. This allows us to use gra"
2020.emnlp-main.73,2020.acl-main.251,0,0.0112911,"bjective (Vincent, 2011) that avoids estimating the Hessian. Although score function estimation that bypasses energy estimation was found to be unstable (Alain and Bengio, 2014; Saremi et al., 2018), it has been successfully applied to generative modeling of images (Song and Ermon, 2019). Inference While we categorize inference methods for machine translation as (1) discrete search, (2) hybrid optimization (Shu et al., 2020) and (3) continuous optimization (this work) in Section 2, another line of work relaxes discrete search into continuous optimization (Hoang et al., 2017; Gu et al., 2018b; Tu et al., 2020). By using Gumbelsoftmax relaxation (Maddison et al., 2017; Jang et al., 2017), they train an inference network to generate target tokens that maximize the log probability under a pretrained model. 1012 Example 1 Source Reference Original Refined There aren ’t many doctors in the west African country ; just one for every 5,000 people ¨ In dem westafrikanischen Land gibt es nicht viele Arzte, nur einen f¨ur 5.000 Menschen ¨ Es gibt nicht viele Arzte im westafrikanischen Land, nur eine f¨ur 5.000 Menschen. ¨ Im westafrikanischen Land gibt es nicht viele Arzte, nur eine f¨ur 5.000 Menschen. Examp"
2020.emnlp-main.73,W18-5713,0,0.0249331,"Callum, 2016; Wang et al., 2016; Belanger et al., 2017). Other work performed gradient descent over the latent variables to optimize objectives for a wide variety of tasks, including chemical design (G´omez-Bombarelli et al., 2018) and text generation (Mueller et al., 2017) Generation by Refinement Refinement has a long history in text generation. The retrieve-andrefine framework retrieves an (input, output) pair from the training set that is similar to the test example, and performs edit operations on the corresponding output (Sumita and Iida, 1991; Song et al., 2016; Hashimoto et al., 2018; Weston et al., 2018; Gu et al., 2018c). The idea of refinement has also been applied in automatic post-editing (Novak et al., 2016; Grangier and Auli, 2017). 8 Conclusion We propose an efficient inference procedure for non-autoregressive machine translation that refines translations purely in the continuous space. Given a latent variable model for machine translation, we train an inference network to approximate the gradient of the marginal log probability with respect to the target sentence, using only the latent variable. This allows us to use gradient based optimization to find a target sentence at inference"
2020.emnlp-main.97,P07-1056,0,0.208948,"cific knowledge, and does not rely on class- or dataset-specific fine-tuning. We investigate the use of SSMBA in the natural language domain on 3 diverse tasks spanning both classification and sequence modelling: sentiment analysis, natural language inference, and machine translation. In experiments across 9 datasets and 4 model types, we show SSMBA consistently outperforms baseline models and other data augmentation methods on both in-domain and OOD data. 2 2.1 Background and Related Work Data Augmentation in NLP The problem of domain adaptation and OOD robustness is well established in NLP (Blitzer et al., 2007; Daum´e III, 2007; Hendrycks et al., 2020). Existing work on improving generalization has focused on data augmentation, where synthetically generated training examples are used to augment an existing dataset. It is hypothesized that these examples induce robustness to local perturbations, which has been shown to be effective in semi-supervised and self-supervised settings (Bachman et al., 2014; Szegedy et al., 2014; Sajjadi et al., 2016). Existing task-specific methods (Kafle et al., 2017) and word-level methods (Zhang et al., 2015; Xie et al., 2017; Wei and Zou, 2019) are based on human-desi"
2020.emnlp-main.97,2014.iwslt-evaluation.1,0,0.0430303,"Missing"
2020.emnlp-main.97,P07-1033,0,0.303907,"Missing"
2020.emnlp-main.97,N18-1033,0,0.0658056,"Missing"
2020.emnlp-main.97,P17-2090,0,0.0653812,"effective in semi-supervised and self-supervised settings (Bachman et al., 2014; Szegedy et al., 2014; Sajjadi et al., 2016). Existing task-specific methods (Kafle et al., 2017) and word-level methods (Zhang et al., 2015; Xie et al., 2017; Wei and Zou, 2019) are based on human-designed heuristics. Back-translation from or through another language has been applied in the context of machine translation (Rico Sennrich, 2016), question answering (Yu et al., 2018), and consistency training (Xie et al., 2019). More recent work has used word embeddings (Wang and Yang, 2015) and LSTM language models (Fadaee et al., 2017) to perform word replacement. Other methods focus on fine-tuning contextual language models (Kobayashi, 2018; Wu et al., 2019b; Kumar et al., 2020) or large generative models (AnabyTavor et al., 2020; Yang et al., 2020; Kumar et al., 2020) to generate synthetic examples. 2.2 VRM and the Manifold Assumption Vicinal Risk Minimization (VRM) (Chapelle et al., 2000) formalizes data augmentation as enlarging the training set support by drawing samples from a vicinity of existing training examples. Typically the vicinity of a training example is defined using dataset-dependent heuristics. For example"
2020.emnlp-main.97,D19-1018,0,0.0401354,"Missing"
2020.emnlp-main.97,W17-3529,0,0.0208255,"ata Augmentation in NLP The problem of domain adaptation and OOD robustness is well established in NLP (Blitzer et al., 2007; Daum´e III, 2007; Hendrycks et al., 2020). Existing work on improving generalization has focused on data augmentation, where synthetically generated training examples are used to augment an existing dataset. It is hypothesized that these examples induce robustness to local perturbations, which has been shown to be effective in semi-supervised and self-supervised settings (Bachman et al., 2014; Szegedy et al., 2014; Sajjadi et al., 2016). Existing task-specific methods (Kafle et al., 2017) and word-level methods (Zhang et al., 2015; Xie et al., 2017; Wei and Zou, 2019) are based on human-designed heuristics. Back-translation from or through another language has been applied in the context of machine translation (Rico Sennrich, 2016), question answering (Yu et al., 2018), and consistency training (Xie et al., 2019). More recent work has used word embeddings (Wang and Yang, 2015) and LSTM language models (Fadaee et al., 2017) to perform word replacement. Other methods focus on fine-tuning contextual language models (Kobayashi, 2018; Wu et al., 2019b; Kumar et al., 2020) or large"
2020.emnlp-main.97,D14-1181,0,0.00750826,"Missing"
2020.emnlp-main.97,W04-3250,0,0.547337,"Missing"
2020.emnlp-main.97,2020.lifelongnlp-1.3,0,0.0459964,"Missing"
2020.emnlp-main.97,N19-4009,1,0.897192,"Missing"
2020.emnlp-main.97,P02-1040,0,0.106883,"Missing"
2020.emnlp-main.97,D13-1170,0,0.0111734,"plit the samples into separate domains or similar datasets that are treated as separate domains. 4.1 Sentiment Analysis The Amazon Review Dataset (Jianmo Ni, 2019) contains product reviews from Amazon. Following Hendrycks et al. 2020, we form two datasets: AR-Full contains reviews from the 10 largest categories, and AR-Clothing contains reviews in the clothing category separated into subcategories by metadata. Since the reviews in AR-Clothing come from the same top-level category, the amount of domain shift is much less than that of AR-Full. Models predict a review’s 1 to 5 star rating. SST2 (Socher et al., 2013) contains movie review excerpts. Following Hendrycks et al. 2020 we pair this dataset with the IMDb dataset (Maas et al., 1270 Dataset AR-Clothing AR-Full Domain * * n 4 10 l 35 67 Train In the de→en direction, we use IWSLT14 de→en (Cettolo et al., 2014) as a widely-used benchmark to test in-domain performance. We also use the OPUS (Tiedemann, 2012) dataset to test OOD generalization. We train on highly specific in-domain data (medical texts) and disparate out-of-domain data (Koran text, Ubuntu localization files, movie subtitles, and legal text). Since domains share very little similarities i"
2020.emnlp-main.97,W18-6319,0,0.0232389,"Missing"
2020.emnlp-main.97,tiedemann-2012-parallel,0,0.0135968,"lothing category separated into subcategories by metadata. Since the reviews in AR-Clothing come from the same top-level category, the amount of domain shift is much less than that of AR-Full. Models predict a review’s 1 to 5 star rating. SST2 (Socher et al., 2013) contains movie review excerpts. Following Hendrycks et al. 2020 we pair this dataset with the IMDb dataset (Maas et al., 1270 Dataset AR-Clothing AR-Full Domain * * n 4 10 l 35 67 Train In the de→en direction, we use IWSLT14 de→en (Cettolo et al., 2014) as a widely-used benchmark to test in-domain performance. We also use the OPUS (Tiedemann, 2012) dataset to test OOD generalization. We train on highly specific in-domain data (medical texts) and disparate out-of-domain data (Koran text, Ubuntu localization files, movie subtitles, and legal text). Since domains share very little similarities in language, generalization to out-of-domain text is extremely difficult. In the de→rm direction, we use a training set consisting of the Allegra corpus (Scherrer and Cartoni, 2012) and Swiss press releases. We use blog posts from Convivenza as a test domain. Test 25k † 2k 25k † 2k † 2k Yelp * 4 138 25k Movies SST2 IMDb - 11 296 66k 46k 1k 2k MNLI *"
2020.emnlp-main.97,P16-1009,0,0.11105,"Missing"
2020.emnlp-main.97,scherrer-cartoni-2012-trilingual,0,0.0301216,"l Domain * * n 4 10 l 35 67 Train In the de→en direction, we use IWSLT14 de→en (Cettolo et al., 2014) as a widely-used benchmark to test in-domain performance. We also use the OPUS (Tiedemann, 2012) dataset to test OOD generalization. We train on highly specific in-domain data (medical texts) and disparate out-of-domain data (Koran text, Ubuntu localization files, movie subtitles, and legal text). Since domains share very little similarities in language, generalization to out-of-domain text is extremely difficult. In the de→rm direction, we use a training set consisting of the Allegra corpus (Scherrer and Cartoni, 2012) and Swiss press releases. We use blog posts from Convivenza as a test domain. Test 25k † 2k 25k † 2k † 2k Yelp * 4 138 25k Movies SST2 IMDb - 11 296 66k 46k 1k 2k MNLI * 10 36 80k 1k ANLI R1 R2 R3 - 92 90 82 17k 46k 100k 1k 1k 1k IWSLT - 1 24 160k 7k OPUS Medical 5 15 1.1m 2k de-rm Law Blogs - 22 25 100k - 2k 2k Table 1: Dataset summary statistics. n: number of domains. l: average tokenized input length. A * in the domain column indicates that the statistics are identical across domains within that dataset. Training sets marked with a † are sampled randomly from a larger dataset. Refer to App"
2020.emnlp-main.97,W16-2323,0,0.03234,"hat randomly replaces synonyms and inserts, swaps, and deletes words. Conditional Bert Contextual Augmentation (CBERT) (Wu et al., 2019b) finetunes a class-conditional BERT model and uses it to generate sentences in a process similar to our own. Unsupervised Data Augmentation (UDA) (Xie et al., 2020) translates data to and from a pivot language to generate paraphrases. We adapt UDA for supervised classification tasks by training directly on the backtranslated data. On translation tasks, we compare only against methods which do not require additional target side monolingual data. Word dropout (Sennrich et al., 2016) randomly chooses words in the source sentence to set to zero embeddings. Reward Augmented Maximum Likelihood (RAML) (Norouzi et al., 2016) samples noisy target sentences based on an exponential of their Hamming distance from the original sentence. SwitchOut (Wang et al., 2018) applies a noise function similar to RAML to both the source and target side. We use publicly available implementations for all methods. 5.4 Evaluation Method We train LSTM and CNN models with 10 random seeds, RoBERTa models with 5 random seeds, and transformer models with 3 random seeds. Models are trained separately on"
2020.emnlp-main.97,D15-1306,0,0.0478462,"ocal perturbations, which has been shown to be effective in semi-supervised and self-supervised settings (Bachman et al., 2014; Szegedy et al., 2014; Sajjadi et al., 2016). Existing task-specific methods (Kafle et al., 2017) and word-level methods (Zhang et al., 2015; Xie et al., 2017; Wei and Zou, 2019) are based on human-designed heuristics. Back-translation from or through another language has been applied in the context of machine translation (Rico Sennrich, 2016), question answering (Yu et al., 2018), and consistency training (Xie et al., 2019). More recent work has used word embeddings (Wang and Yang, 2015) and LSTM language models (Fadaee et al., 2017) to perform word replacement. Other methods focus on fine-tuning contextual language models (Kobayashi, 2018; Wu et al., 2019b; Kumar et al., 2020) or large generative models (AnabyTavor et al., 2020; Yang et al., 2020; Kumar et al., 2020) to generate synthetic examples. 2.2 VRM and the Manifold Assumption Vicinal Risk Minimization (VRM) (Chapelle et al., 2000) formalizes data augmentation as enlarging the training set support by drawing samples from a vicinity of existing training examples. Typically the vicinity of a training example is defined"
2020.emnlp-main.97,D18-1100,0,0.0247582,"(Xie et al., 2020) translates data to and from a pivot language to generate paraphrases. We adapt UDA for supervised classification tasks by training directly on the backtranslated data. On translation tasks, we compare only against methods which do not require additional target side monolingual data. Word dropout (Sennrich et al., 2016) randomly chooses words in the source sentence to set to zero embeddings. Reward Augmented Maximum Likelihood (RAML) (Norouzi et al., 2016) samples noisy target sentences based on an exponential of their Hamming distance from the original sentence. SwitchOut (Wang et al., 2018) applies a noise function similar to RAML to both the source and target side. We use publicly available implementations for all methods. 5.4 Evaluation Method We train LSTM and CNN models with 10 random seeds, RoBERTa models with 5 random seeds, and transformer models with 3 random seeds. Models are trained separately on each domain then evaluated on all domains, and performance is averaged across seeds and test domains. We report the average in-domain (ID) and OOD performance across all train domains. On sentiment analysis and NLI tasks we report accuracy, and on translation we report uncased"
2020.emnlp-main.97,D19-1670,0,0.444406,"l established in NLP (Blitzer et al., 2007; Daum´e III, 2007; Hendrycks et al., 2020). Existing work on improving generalization has focused on data augmentation, where synthetically generated training examples are used to augment an existing dataset. It is hypothesized that these examples induce robustness to local perturbations, which has been shown to be effective in semi-supervised and self-supervised settings (Bachman et al., 2014; Szegedy et al., 2014; Sajjadi et al., 2016). Existing task-specific methods (Kafle et al., 2017) and word-level methods (Zhang et al., 2015; Xie et al., 2017; Wei and Zou, 2019) are based on human-designed heuristics. Back-translation from or through another language has been applied in the context of machine translation (Rico Sennrich, 2016), question answering (Yu et al., 2018), and consistency training (Xie et al., 2019). More recent work has used word embeddings (Wang and Yang, 2015) and LSTM language models (Fadaee et al., 2017) to perform word replacement. Other methods focus on fine-tuning contextual language models (Kobayashi, 2018; Wu et al., 2019b; Kumar et al., 2020) or large generative models (AnabyTavor et al., 2020; Yang et al., 2020; Kumar et al., 2020"
2020.emnlp-main.97,N18-1101,0,0.0309799,"nized input length. A * in the domain column indicates that the statistics are identical across domains within that dataset. Training sets marked with a † are sampled randomly from a larger dataset. Refer to Appendix A for more information. 2011), which contains full length movie reviews. We call this pair the Movies dataset. Models predict a movie review’s binary sentiment. The Yelp Review Dataset contains restaurant reviews with associated business metadata which we preprocess following Hendrycks et al. 2020. Models predict a review’s 1 to 5 star rating. 4.2 Natural Language Inference MNLI (Williams et al., 2018) is a corpus of NLI data from 10 distinct genres of written and spoken English. We train on the 5 genres with training data and test on all 10 genres. Since the dataset does not include labeled test data, we use the validation set as our test set and sample 2000 examples from each training set for validation. ANLI (Nie et al., 2019) is a corpus of NLI data designed adversarially by humans such that stateof-the-art models fail to classify examples correctly. The dataset consists of three different levels of difficulty which we treat as separate textual domains. 4.3 Machine Translation Following"
2020.emnlp-main.97,2020.findings-emnlp.90,0,0.0607463,"Missing"
2020.nlpcovid19-acl.2,2020.nlpcovid19-acl.2,1,0.0609097,"aset Edwin Zhang,1 Nikhil Gupta,1 Rodrigo Nogueira,1 Kyunghyun Cho,2,3,4 and Jimmy Lin1 1 David R. Cheriton School of Computer Science, University of Waterloo 2 Courant Institute of Mathematical Sciences, New York University 3 Center for Data Science, New York University 4 CIFAR Associate Fellow This extended abstract represents an abridged version of Zhang et al. (2020a), posted on arXiv April 10, 2020 and concurrently submitted to this workshop. We have intentionally decided for this short piece to reflect the state of our work at that time. The latest updates on our project can be found in Zhang et al. (2020b). The Neural Covidex is a search engine that exploits the latest neural ranking architectures to provide information access to the COVID-19 Open Research Dataset (CORD-19) curated by the Allen Institute for AI (Wang et al., 2020). It exists as part of a suite of tools we have developed to help domain experts tackle the ongoing global pandemic. We hope that improved information access capabilities to the scientific literature can inform evidencebased decision making and insight generation. The first version of CORD-19 was released on March 13, 2020. Within a couple of weeks, our team was able"
2020.nlpcovid19-acl.2,N19-1423,0,\N,Missing
2020.nlpcovid19-acl.2,N19-4013,1,\N,Missing
2020.nlpcovid19-acl.2,D19-1352,1,\N,Missing
2020.nlpcovid19-acl.2,2020.findings-emnlp.63,1,\N,Missing
2020.repl4nlp-1.5,D19-1384,1,0.825455,"ages systematic generalization. While we empirically see evidence for the bottom of this range, we curiously do not find evidence for the top part of the range and believe that this is an open question for the community. 1 Introduction Compositional language learning in the context of multi agent emergent communication has been extensively studied (Foerster et al., 2016; Lazaridou et al., 2017; Baroni, 2020). These works have found that while most emergent languages do not tend to be compositional, they can be guided towards this attribute through artificial task-specific constraints (Harding Graesser et al., 2019; Lee et al., 2018; Słowik et al., 2020). In this paper, we focus on how a neural network, specifically a generative one, can learn a compositional language. Moreover, we ask how this can occur without task-specific constraints. To accomplish this, we first define what is a language and what we mean by compositionality. In tandem, we introduce precision and recall, two metrics that help us measure how well a generative model at ∗ These two authors contributed equally. Cinjon Resnick∗ New York University cinjon@nyu.edu Kyunghyun Cho New York University Facebook AI Research kyunghyun.cho@nyu.edu"
2020.repl4nlp-1.5,D17-1321,0,0.0251689,". Cinjon Resnick∗ New York University cinjon@nyu.edu Kyunghyun Cho New York University Facebook AI Research kyunghyun.cho@nyu.edu large has learned a grammar from a finite set of training instances. We then use a variational autoencoder with a discrete sequence bottleneck to investigate how well the model learns a compositional language, in addition to what affects that learning. This allows us to derive residual entropy, a third metric that reliably measures compositionality in our particular environment. We use this metric to cross-validate precision and recall. Our paper is most similar to Kottur et al. (2017), which showed that compositional language arose only when certain constraints on the agents are satisfied. While the constraints they examined were either making their models memoryless or having a minimal vocabulary in the language, we hypothesized about the importance for agents to have small capacity relative to the number of concepts to which they are exposed. Each of Verhoef et al. (2016); Kirby et al. (2015); Zaslavsky et al. (2018) examine the trade-off between expression and compression in both emergent and natural languages, in addition to how that trade-off affects the learners. We"
2020.sdp-1.5,W19-5034,0,0.0333516,"ovidex.r5.d2q.1s (= expando + monoT5) r5.fusion2 r5.fusion1 Table 1: Selected TREC-COVID results. Our submissions are under teams “covidex” and “anserini”. All runs notated with † incorporate our infrastructure components in some way. Note that the metrics used in the first three rounds are different from those used in the final two rounds. 37 tion technique proved to be effective: when constructing the keyword query for a given topic, we take the non-stopwords from the query field and further expand them with terms belonging to named entities extracted from the question field using ScispaCy (Neumann et al., 2019). demonstrated our impact not only in developing effective ranking models, but also our service to the community in providing infrastructure. As another point of comparison, UIowaSRun3 (2c) represented a fusion of two traditional (i.e., term-based) relevance feedback runs, and did not use any neural networks. Interestingly, its effectiveness is not very far behind SparseDenseSciBert (2b), the best run in the feedback category (which does take advantage of BERT). It seems that BERTbased methods for exploiting relevance judgments yielded only modest improvements, likely due to the paucity of rel"
2020.sdp-1.5,2020.findings-emnlp.63,1,0.823597,"Missing"
2020.sdp-1.5,D19-1352,1,0.925726,"as follows: each full-text article was segmented into paragraphs and for each paragraph, we created a “document” comprising the title, abstract, and that paragraph. The title and abstract alone comprised an additional “document”. Thus, a fulltext article with n paragraphs yielded n + 1 separate retrieval units in the index. Keyword Search In our design, initial retrieval is performed by the Anserini IR toolkit (Yang et al., 2017, 2018),1 which we have been developing for several years and powers a number of our previous systems that incorporate various neural architectures (Yang et al., 2019; Yilmaz et al., 2019). Anserini represents an effort to better align real-world search applications with academic information retrieval research: under the covers, it builds on the popular and widely-deployed open-source Lucene search library, on top of which we provide a number of 1 To be consistent with standard IR parlance, we call each of these retrieval units a “document”, in a generic sense, despite their composite structure. In addition to the above indexing schemes, we considered three more based on our doc2query document expansion technique (Nogueira et al., 2019b; Nogueira and Lin, 2019). The idea behind"
2020.sdp-1.5,2020.nlpcovid19-acl.2,1,0.236165,"scientific articles (as of October, 2020), including most with full text, about COVID-19 and coronavirus-related research more broadly (for example, SARS and MERS). These articles are gathered from a variety of sources, including PubMed, a curated list of articles from the WHO, as well as preprints from arXiv, bioRxiv, and medRxiv. The goal of the effort is “to mobilize researchers to apply recent advances in natural language process3. Finally, we package the previous two components into Covidex, an end-to-end search engine and browsing interface deployed at covidex.ai, initially described in Zhang et al. (2020a). All three efforts have been successful. In the TRECCOVID challenge, our infrastructure and baselines have been adopted by many teams, which in some 31 Proceedings of the First Workshop on Scholarly Document Processing, pages 31–41 c Online, November 19, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 cases have submitted runs that scored higher than our own submissions. This illustrates the success of our infrastructure-building efforts (1). In round 3, we submitted the highest-scoring run that took advantage of previous training data and the secondhigh"
2020.sdp-1.5,N19-4013,1,0.878427,"Missing"
2020.spnlp-1.10,P82-1020,0,0.525451,"Missing"
2020.spnlp-1.10,W05-0909,0,0.227013,"y Dustin Tran Google AI Orhan Firat Google AI Kyunghyun Cho New York University jason@cs.nyu.edu trandustin@google.com orhanf@google.com kyunghyun.cho@nyu.edu Abstract the conditional log-likelihood, and also evaluated using log-likelihood on a test set. However, many sequence generation tasks require finding the best output yˆ given an input x at test time, and the output is evaluated against a set of references y ∗ on a task-specific metric: R(ˆ y , y ∗ |x). For example, machine translation systems are evaluated using BLEU scores (Papineni et al., 2002), image captioning systems use METEOR (Banerjee and Lavie, 2005) and text-to-speech systems use MOS (mean opinion scores). As density estimators are optimized on log-likelihood, we want models with higher held-out log-likelihoods to give better generation quality, but the correlation has not been well studied for sequence generation tasks. In this work, we investigate the correlation between rankings of density estimators based on (1) test log-likelihood and (2) the downstream metric for machine translation. On five language pairs from three machine translation datasets (WMT’14 En↔De, WMT’16 En↔Ro, IWSLT’16 De→En), we compare the held-out log-likelihood an"
2020.spnlp-1.10,D11-1125,0,0.0387008,"tion quality while being smaller and faster. For sequence generation, the gap between loglikelihood and downstream metric has long been recognized. To address this discrepancy between density estimation and approximate inference (generation), there has largely been two lines of prior work: (1) structured perceptron training for conditional random fields (Lafferty et al., 2001; Collins, 2002; Liang et al., 2006) and (2) empirical risk minimization with approximate inference (Valtchev et al., 1997; Povey and Woodland, 2002; Och, 2003; Qiang Fu and Biing-Hwang Juang, 2007; Stoyanov et al., 2011; Hopkins and May, 2011; Shen et al., 2016). More recent work proposed to train neural sequence models directly on task-specific losses using reinforcement learning (Ranzato et al., 2016; Bahdanau et al., 2017; Jaques et al., 2017) or adversarial training (Goyal et al., 2016). Despite such a plethora of work in bridging the gap between log-likelihood and the downstream task, the exact correlation between the two has not been established well. Our work investigates the correlation for neural sequence models (autoregressive models and latent variable models) in machine translation. Among autoregressive models for open"
2020.spnlp-1.10,K16-1002,0,0.0198994,"Missing"
2020.spnlp-1.10,W02-1001,0,0.0486256,"with a diagonal Gaussian prior (Shu et al., 2019) and a normalizing flow prior (Ma et al., 2019). We find that while having an expressive prior is beneficial for density estimation, a simple prior delivers better generation quality while being smaller and faster. For sequence generation, the gap between loglikelihood and downstream metric has long been recognized. To address this discrepancy between density estimation and approximate inference (generation), there has largely been two lines of prior work: (1) structured perceptron training for conditional random fields (Lafferty et al., 2001; Collins, 2002; Liang et al., 2006) and (2) empirical risk minimization with approximate inference (Valtchev et al., 1997; Povey and Woodland, 2002; Och, 2003; Qiang Fu and Biing-Hwang Juang, 2007; Stoyanov et al., 2011; Hopkins and May, 2011; Shen et al., 2016). More recent work proposed to train neural sequence models directly on task-specific losses using reinforcement learning (Ranzato et al., 2016; Bahdanau et al., 2017; Jaques et al., 2017) or adversarial training (Goyal et al., 2016). Despite such a plethora of work in bridging the gap between log-likelihood and the downstream task, the exact correla"
2020.spnlp-1.10,D16-1139,0,0.0363847,"at outputs the logits of all target tokens in parallel. The approximate posterior qφ (z|y, x) is implemented as a Transformer decoder with a final Linear layer with weight normalization (Salimans where yˆm = argmaxy log pF (y|xm ). To perform model selection, we can rank a set of density estimators {F1 , . . . , FK } based on either the held-out log-likelihood or the downstream metric. We measure the correlation between the rankings given by the log-likelihood L(F ) and the downstream metric R(F ). 4 Autoregressive Models https://wit3.fbk.eu/ 87 Knowledge Distillation Following previous work (Kim and Rush, 2016; Gu et al., 2018; Lee et al., 2018), we construct a distilled dataset by decoding the training set using Transformer-base with beam width 4. For IWSLT’16 De→En, we use Transformer-small. and Kingma, 2016) to output the mean and standard deviation (having dimensionality dlatent ). Both the decoder and the approximate posterior attend to the source hidden states. Diagonal Gaussian Prior The diagonal Gaussian prior is implemented with a Transformer decoder which receives a sequence of positional encodings of length T as input, and outputs the mean and standard deviation of each target token (of"
2020.spnlp-1.10,P02-1040,0,0.11653,"timation and Sequence Generation Jason Lee New York University Dustin Tran Google AI Orhan Firat Google AI Kyunghyun Cho New York University jason@cs.nyu.edu trandustin@google.com orhanf@google.com kyunghyun.cho@nyu.edu Abstract the conditional log-likelihood, and also evaluated using log-likelihood on a test set. However, many sequence generation tasks require finding the best output yˆ given an input x at test time, and the output is evaluated against a set of references y ∗ on a task-specific metric: R(ˆ y , y ∗ |x). For example, machine translation systems are evaluated using BLEU scores (Papineni et al., 2002), image captioning systems use METEOR (Banerjee and Lavie, 2005) and text-to-speech systems use MOS (mean opinion scores). As density estimators are optimized on log-likelihood, we want models with higher held-out log-likelihoods to give better generation quality, but the correlation has not been well studied for sequence generation tasks. In this work, we investigate the correlation between rankings of density estimators based on (1) test log-likelihood and (2) the downstream metric for machine translation. On five language pairs from three machine translation datasets (WMT’14 En↔De, WMT’16 E"
2020.spnlp-1.10,D18-1149,1,0.948403,"okens in parallel. The approximate posterior qφ (z|y, x) is implemented as a Transformer decoder with a final Linear layer with weight normalization (Salimans where yˆm = argmaxy log pF (y|xm ). To perform model selection, we can rank a set of density estimators {F1 , . . . , FK } based on either the held-out log-likelihood or the downstream metric. We measure the correlation between the rankings given by the log-likelihood L(F ) and the downstream metric R(F ). 4 Autoregressive Models https://wit3.fbk.eu/ 87 Knowledge Distillation Following previous work (Kim and Rush, 2016; Gu et al., 2018; Lee et al., 2018), we construct a distilled dataset by decoding the training set using Transformer-base with beam width 4. For IWSLT’16 De→En, we use Transformer-small. and Kingma, 2016) to output the mean and standard deviation (having dimensionality dlatent ). Both the decoder and the approximate posterior attend to the source hidden states. Diagonal Gaussian Prior The diagonal Gaussian prior is implemented with a Transformer decoder which receives a sequence of positional encodings of length T as input, and outputs the mean and standard deviation of each target token (of dimensionality dlatent ). We train t"
2020.spnlp-1.10,P06-1096,0,0.0859911,"Missing"
2020.spnlp-1.10,D19-1437,0,0.314019,"n between rankings of density estimators based on (1) test log-likelihood and (2) the downstream metric for machine translation. On five language pairs from three machine translation datasets (WMT’14 En↔De, WMT’16 En↔Ro, IWSLT’16 De→En), we compare the held-out log-likelihood and BLEU scores of several density estimators: (1) autoregressive models (Vaswani et al., 2017), (2) latent variable models with a non-autoregressive decoder and a simple (diagonal Gaussian) prior (Shu et al., 2019), and (3) latent variable models with a non-autoregressive decoder and a flexible (normalizing flow) prior (Ma et al., 2019). We present two key observations. First, among models within the same family, we find that loglikelihood is strongly correlated with BLEU. The correlation is almost perfect for autoregressive models and high for latent variable models with the same prior. Between models of different families, however, log-likelihood and BLEU are not correlated. Latent variable models with a flow prior are in fact the best density estimators (even better than autoregressive models), but they give the Many sequence-to-sequence generation tasks, including machine translation and text-tospeech, can be posed as es"
2020.spnlp-1.10,P03-1021,0,0.126588,"beneficial for density estimation, a simple prior delivers better generation quality while being smaller and faster. For sequence generation, the gap between loglikelihood and downstream metric has long been recognized. To address this discrepancy between density estimation and approximate inference (generation), there has largely been two lines of prior work: (1) structured perceptron training for conditional random fields (Lafferty et al., 2001; Collins, 2002; Liang et al., 2006) and (2) empirical risk minimization with approximate inference (Valtchev et al., 1997; Povey and Woodland, 2002; Och, 2003; Qiang Fu and Biing-Hwang Juang, 2007; Stoyanov et al., 2011; Hopkins and May, 2011; Shen et al., 2016). More recent work proposed to train neural sequence models directly on task-specific losses using reinforcement learning (Ranzato et al., 2016; Bahdanau et al., 2017; Jaques et al., 2017) or adversarial training (Goyal et al., 2016). Despite such a plethora of work in bridging the gap between log-likelihood and the downstream task, the exact correlation between the two has not been established well. Our work investigates the correlation for neural sequence models (autoregressive models and"
2020.spnlp-1.10,W16-2323,0,0.0365402,"rminant is multiplicative, increasingly flexible coupling flows can be constructed by stacking multiple flow layers and reordering such that all the variables are transformed. 2.4 training, 2K development and 2K test sentence pairs), WMT’16 En↔Ro2 (612K, 2K, 2K pairs) and WMT’14 En↔De3 (4.5M, 3K, 3K pairs). For WMT’14 En↔De and WMT’16 En↔Ro, both directions are used. We use the preprocessing scripts with default hyperparameters from the tensor2tensor framework.4 Namely, we use wordpiece tokenization (Schuster and Nakajima, 2012) with 32K wordpieces on all datasets. For WMT’16 En↔Ro, we follow Sennrich et al. (2016) and normalize Romanian and remove diacritics before applying wordpiece tokenization. For training, we discard sentence pairs if either the source or the target length exceeds 64 tokens. As splitting along the time dimension (Ma et al., 2019) in the coupling flow layer requires that the length of the output sequence is a multiple of 2 at each level, <EOS&gt; tokens are appended to the target sentence until its length is a multiple of 4. Knowledge Distillation While most density estimators for sequence generation tasks are trained to maximize the loglikelihood of the training data, recent work hav"
2020.spnlp-1.10,P16-1159,0,0.022288,"g smaller and faster. For sequence generation, the gap between loglikelihood and downstream metric has long been recognized. To address this discrepancy between density estimation and approximate inference (generation), there has largely been two lines of prior work: (1) structured perceptron training for conditional random fields (Lafferty et al., 2001; Collins, 2002; Liang et al., 2006) and (2) empirical risk minimization with approximate inference (Valtchev et al., 1997; Povey and Woodland, 2002; Och, 2003; Qiang Fu and Biing-Hwang Juang, 2007; Stoyanov et al., 2011; Hopkins and May, 2011; Shen et al., 2016). More recent work proposed to train neural sequence models directly on task-specific losses using reinforcement learning (Ranzato et al., 2016; Bahdanau et al., 2017; Jaques et al., 2017) or adversarial training (Goyal et al., 2016). Despite such a plethora of work in bridging the gap between log-likelihood and the downstream task, the exact correlation between the two has not been established well. Our work investigates the correlation for neural sequence models (autoregressive models and latent variable models) in machine translation. Among autoregressive models for open-domain dialogue, a"
2020.spnlp-1.11,W19-5321,0,0.0412112,"Missing"
2020.spnlp-1.11,D18-1512,0,0.15085,"Missing"
2020.spnlp-1.11,2020.spnlp-1.10,1,0.720275,"Missing"
2020.spnlp-1.11,W17-4811,0,0.213389,"oach as a log-linear combination of translation, sentence-level and documentlevel language model probabilities. In addition to using static coefficients for each term, this formulation alternatively allows for the learning of dynamic per-token weights to more finely control the impact of the language models. Using both static or dynamic coefficients leads to improvements over a context-agnostic baseline and a context-aware concatenation model. 1 With either static or dynamic coefficients, we observe improvements over a context-agnostic baseline, as well as a context-aware concatenation model (Tiedemann and Scherrer, 2017). Similarly to the noisy channel model, our approach reuses off-the-shelf models and benefits from future translation or language modelling improvements. Introduction Neural machine translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015) has been reported to reach near human-level performance on sentence-by-sentence translation (Läubli et al., 2018). Going beyond sentence-level, documentlevel NMT aims to translate sentences by taking into account neighboring source or target sentences in order to produce a more cohesive output (Jean et al., 2017; Wang et al., 2017; Maruf et al., 2019"
2020.spnlp-1.11,Q18-1029,0,0.175988,"Missing"
2020.spnlp-1.11,D19-1081,0,0.200661,"Missing"
2020.spnlp-1.11,P19-1116,0,0.409921,"on or language modelling improvements. Introduction Neural machine translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015) has been reported to reach near human-level performance on sentence-by-sentence translation (Läubli et al., 2018). Going beyond sentence-level, documentlevel NMT aims to translate sentences by taking into account neighboring source or target sentences in order to produce a more cohesive output (Jean et al., 2017; Wang et al., 2017; Maruf et al., 2019). These approaches often train new models from scratch using parallel data. In this paper, in a similar spirit to Voita et al. (2019a); Yu et al. (2020), we seek a document-level approach that maximally uses various available corpora, such as parallel and monolingual data, leveraging models trained at the sentence and document levels, while also striving for computational efficiency. We start from the noisy channel model (Yu et al., 2020) which combines a target-to-source 2 Log-linear reformulation of the noisy channel model Given the availability of various heterogeneous data sources that could be used for document-level translation, we seek a strategy to maximally use them. These sources include parallel data, at either"
2020.spnlp-1.11,P18-1117,0,0.0447489,"tlevel linguistic phenomena better. Future directions include combining our approach with MT models trained on back-translated documents, exploring its applicability to other modalities such as vision and speech, and considering deeper fusion of the models. Related work Document-level NMT Neural machine translation may be extended to include extra-sentential information in many ways, as surveyed by Maruf et al. (2019). The model architecture may be modified, for example by encoding previous source sentences or generated translations and attending to them (Jean et al., 2017; Wang et al., 2017; Voita et al., 2018; Zhang et al., 2018; Miculicich et al., 2018; Maruf and Haffari, 2018; Tu 1 Using the difference of language models scores gives higher accuracy, but they cannot be used in isolation to generate relevant translations. 98 Acknowledgements sentence alignments in large, noisy parallel corpora. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC-2018). This work was supported by Samsung Advanced Institute of Technology (Next Generation Deep Learning: from pattern recognition to AI), Samsung Research (Improving Deep Learning using Latent Structure) an"
2020.spnlp-1.11,D17-1301,0,0.162623,"model (Tiedemann and Scherrer, 2017). Similarly to the noisy channel model, our approach reuses off-the-shelf models and benefits from future translation or language modelling improvements. Introduction Neural machine translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015) has been reported to reach near human-level performance on sentence-by-sentence translation (Läubli et al., 2018). Going beyond sentence-level, documentlevel NMT aims to translate sentences by taking into account neighboring source or target sentences in order to produce a more cohesive output (Jean et al., 2017; Wang et al., 2017; Maruf et al., 2019). These approaches often train new models from scratch using parallel data. In this paper, in a similar spirit to Voita et al. (2019a); Yu et al. (2020), we seek a document-level approach that maximally uses various available corpora, such as parallel and monolingual data, leveraging models trained at the sentence and document levels, while also striving for computational efficiency. We start from the noisy channel model (Yu et al., 2020) which combines a target-to-source 2 Log-linear reformulation of the noisy channel model Given the availability of various heterogeneous"
2020.spnlp-1.11,D18-1049,0,0.129768,"Missing"
2021.acl-long.508,2020.emnlp-main.19,0,0.0810152,"Missing"
2021.acl-long.508,2020.sustainlp-1.19,0,0.0231267,"notable improvements in various natural language processing (NLP) tasks. Most of them rely on transformers (Vaswani et al., 2017), and the number of model parameters ranges from hundreds of millions to billions (Shoeybi et al., 2019; Raffel et al., 2019; Kaplan et al., 2020; Brown et al., 2020). Despite this high accuracy, excessive computational overhead during inference, both in terms of time and memory, has hindered its use in real applications. This level of excessive computation has further raised the concern over energy consumption as well (Schwartz et al., 2019; Strubell et al., 2019; Cao et al., 2020). Recent studies have attempted to address these concerns regarding large-scale transformers’ computational and energy efficiency (see §7 for a more extensive discussion.) Among these, we focus on PoWER-BERT (Goyal et al., 2020) which progressively reduces sequence length by eliminating word-vectors based on the attention values as passing layers. PoWER-BERT establishes the superiority of accuracy-time trade-off over earlier approaches (Sanh et al., 2019; Sun et al., 2019; Michel et al., 2019). However, it requires us to train a separate model for each efficiency constraint. In this paper, we"
2021.acl-long.508,N18-1202,0,0.0265893,". Additionally, we significantly extend the applicability of PoWER-BERT beyond sequence-level classification into tokenlevel classification with Drop-and-Restore process that drops word-vectors temporarily in intermediate layers and restores at the last layer if necessary. We empirically verify the utility of the proposed approach by demonstrating the superior accuracy-efficiency trade-off under various setups, including span-based question answering and text classification. Code is available at https://github.com/clovaai/lengthadaptive-transformer. 1 Introduction Pre-trained language models (Peters et al., 2018; Devlin et al., 2018; Radford et al., 2019; Yang et al., 2019; He et al., 2020) have achieved notable improvements in various natural language processing (NLP) tasks. Most of them rely on transformers (Vaswani et al., 2017), and the number of model parameters ranges from hundreds of millions to billions (Shoeybi et al., 2019; Raffel et al., 2019; Kaplan et al., 2020; Brown et al., 2020). Despite this high accuracy, excessive computational overhead during inference, both in terms of time and memory, has hindered its use in real applications. This level of excessive computation has further rais"
2021.acl-long.508,2021.acl-long.427,0,0.0672741,"Missing"
2021.acl-long.508,D16-1264,0,0.468643,"l Joint Conference on Natural Language Processing, pages 6501–6511 August 1–6, 2021. ©2021 Association for Computational Linguistics in this work, we propose to use an evolutionary search algorithm, which further allows us to obtain a full Pareto frontier of accuracy-efficiency trade-off of each model. PoWER-BERT, which forms the foundation of the proposed two-stage procedure, is only applicable to sequence-level classification, because it eliminates some of the word vectors at each layer by design. In other words, it cannot be used for token-level tasks such as span-based question answering (Rajpurkar et al., 2016) because these tasks require hidden representations of the entire input sequence at the final layer. We thus propose to extend PoWER-BERT with a novel Drop-and-Restore process (§3.3), which eliminates this inherent limitation. Word vectors are dropped and set aside, rather than eliminated, in intermediate layers to maintain the saving of computational cost, as was with the original PoWER-BERT. These set-aside vectors are then restored at the final hidden layer and provided as an input to a subsequent task-specific layer, unlike the original PoWER-BERT. The main contributions of this work are t"
2021.acl-long.508,2020.acl-main.537,0,0.0171546,"mer language models (Subramanian et al., 2020) also successfully reduce sequence length in the middle and rescale to full length for the final computation. However, their inference complexity is fixed differently with PoWER-BERT because they are not designed to control efficiency. More recently, TR-BERT (Ye et al., 2021) introduces a policy network trained via reinforcement learning to decide which vectors to skip. LayerDrop (Fan et al., 2019) drops random layers during the training to be robust to pruning inspired by Huang et al. (2016). Word-level adaptive depth in Elbayad et al. (2019) and Liu et al. (2020b) might seemingly resemble with length reduction, but word vectors that reached the maximal layer are used for self-attention computation without updating themselves. Escaping a network early (Teerapittayanon et al., 2016; Huang et al., 2017) based on the confidence of the prediction (Xin et al., 2020, 2021; Schwartz et al., 2020; Liu et al., 2020a; Li et al., 2021) also offers a control over accuracyefficiency trade-off by changing a threshold, but it is difficult to tune a threshold for a desired computational budget because of the example-wise adaptive computation. Slimmable neural network"
2021.acl-long.508,2020.acl-main.593,0,0.0196062,"oduces a policy network trained via reinforcement learning to decide which vectors to skip. LayerDrop (Fan et al., 2019) drops random layers during the training to be robust to pruning inspired by Huang et al. (2016). Word-level adaptive depth in Elbayad et al. (2019) and Liu et al. (2020b) might seemingly resemble with length reduction, but word vectors that reached the maximal layer are used for self-attention computation without updating themselves. Escaping a network early (Teerapittayanon et al., 2016; Huang et al., 2017) based on the confidence of the prediction (Xin et al., 2020, 2021; Schwartz et al., 2020; Liu et al., 2020a; Li et al., 2021) also offers a control over accuracyefficiency trade-off by changing a threshold, but it is difficult to tune a threshold for a desired computational budget because of the example-wise adaptive computation. Slimmable neural networks (Yu et al., 2018; Lee and Shin, 2018) reduce the hidden dimension for the any-time prediction. DynaBERT (Hou et al., 2020) can run at adaptive width (the number of attention heads and intermediate hidden dimension) and depth. Hardware-aware Transformers (Wang et al., 2020a) construct a design space with arbitrary encoder-decoder"
2021.acl-long.508,K16-1029,0,0.0298644,"e number of transformer layers is four. Evaluation metrics We use the number of floating operations (FLOPs) as a main metric to measure the inference efficiency given any length configuration, as it is agnostic to the choice of the underlying hardware, unlike other alternatives such as hardware-aware latency (Wang et al., 2020a) or energy consumption (Henderson et al., 2020). We later demonstrate that FLOPs and wall-clock time on GPU and CPU correlate well with the proposed approach, which is not necessarily the case for other approaches, such as unstructured weight pruning (Han et al., 2015; See et al., 2016). Pre-trained transformers Since BERT was introduced by Devlin et al. (2018), it has become a standard practice to start from a pre-trained (masked) language model and fine-tune it for each downstream task. We follow the same strategy in this paper and test two pre-trained transformerbased language models; BERTBase (Devlin et al., 2018) and DistilBERT (Sanh et al., 2019), which allows us to demonstrate that the usefulness and applicability of our approach are not tied to any specific architectural choice, such as the number of layers and the maximum input sequence length. Although we focus on"
2021.acl-long.508,2020.acl-main.686,0,0.40767,"n models with length reduction. They are trained to their prediction close to the full model’s prediction (inplace distillation). 3.2 Evolutionary Search of Length Configurations After training a Length-Adaptive Transformer with LengthDrop, we search for appropriate length configurations for possible target computational budgets that will be given at inference time. The length configuration determines the model performance in terms of both accuracy and efficiency. In order to search for the optimal length configuration, we propose to use evolutionary search, similarly to Cai et al. (2019) and Wang et al. (2020a). This procedure is efficient, as it only requires a single pass through the relatively small validation set for each length configuration, unlike re-training for a new computational budget which requires multiple passes through a significantly larger training set for each budget. We initialize the population with constant-ratio configurations. Each configuration is created by li+1 = (1 − r)li for each layer i with r so that the amount of computation within the initial population is uniformly distributed between those of the smallest and full models. At each iteration, we evolve the populati"
2021.acl-long.508,P19-1355,0,0.0514067,"Missing"
2021.acl-long.508,D19-1441,0,0.0353428,"Missing"
2021.acl-long.508,2020.acl-main.204,0,0.0206562,"T (Ye et al., 2021) introduces a policy network trained via reinforcement learning to decide which vectors to skip. LayerDrop (Fan et al., 2019) drops random layers during the training to be robust to pruning inspired by Huang et al. (2016). Word-level adaptive depth in Elbayad et al. (2019) and Liu et al. (2020b) might seemingly resemble with length reduction, but word vectors that reached the maximal layer are used for self-attention computation without updating themselves. Escaping a network early (Teerapittayanon et al., 2016; Huang et al., 2017) based on the confidence of the prediction (Xin et al., 2020, 2021; Schwartz et al., 2020; Liu et al., 2020a; Li et al., 2021) also offers a control over accuracyefficiency trade-off by changing a threshold, but it is difficult to tune a threshold for a desired computational budget because of the example-wise adaptive computation. Slimmable neural networks (Yu et al., 2018; Lee and Shin, 2018) reduce the hidden dimension for the any-time prediction. DynaBERT (Hou et al., 2020) can run at adaptive width (the number of attention heads and intermediate hidden dimension) and depth. Hardware-aware Transformers (Wang et al., 2020a) construct a design space w"
2021.acl-long.508,2021.eacl-main.8,0,0.0440472,"Missing"
2021.acl-long.508,2021.naacl-main.463,0,0.0429024,"posed approach, based on PoWER-BERT, adaptively reduces the input length as the input sequence is pro6507 cessed by the transformer layers. In our knowledge, Goyal et al. (2020) is the first work in this direction for transformers. Funnel-Transformer (Dai et al., 2020) and multi-scale transformer language models (Subramanian et al., 2020) also successfully reduce sequence length in the middle and rescale to full length for the final computation. However, their inference complexity is fixed differently with PoWER-BERT because they are not designed to control efficiency. More recently, TR-BERT (Ye et al., 2021) introduces a policy network trained via reinforcement learning to decide which vectors to skip. LayerDrop (Fan et al., 2019) drops random layers during the training to be robust to pruning inspired by Huang et al. (2016). Word-level adaptive depth in Elbayad et al. (2019) and Liu et al. (2020b) might seemingly resemble with length reduction, but word vectors that reached the maximal layer are used for self-attention computation without updating themselves. Escaping a network early (Teerapittayanon et al., 2016; Huang et al., 2017) based on the confidence of the prediction (Xin et al., 2020, 2"
2021.acl-long.508,W18-5446,0,0.034504,"Missing"
2021.acl-long.92,2020.acl-main.747,0,0.119934,"Missing"
2021.acl-long.92,2020.acl-main.130,0,0.019878,"138 139 550,152 10,000 10,000 392,702 9,823 9,824 250 28 28 1,105,719 3,200 3,200 400 554 9,741 3,026 33,410 5,428 169,654 16,113 40,398 50 52 610 757 977 319 766 919 633 50 52 611 9,442 977 319 766 919 634 2,251 1,119 1,211 14,191 9,427 25,262 39,905 7,088 7,088 10,246 570 299 317 2,020 1,635 1,492 5,021 443 443 2,164 2,376 1,172 445 3,610 1,635 1,493 5,021 443 443 556 ARC-Easy (Clark et al., 2018) ARC-Challenge (Clark et al., 2018) ARCT (Habernal et al., 2018) MCScript (Ostermann et al., 2018) BoolQ (Clark et al., 2019) Cosmos QA (Huang et al., 2019) HellaSwag (Zellers et al., 2019) MuTual (Cui et al., 2020) MuTual+ (Cui et al., 2020) QuAIL (Rogers et al., 2020) QAMR (Michael et al., 2018) NewsQA (Trischler et al., 2017) SQuAD2.0 (Rajpurkar et al., 2018) MRQA-NQ (Kwiatkowski et al., 2019) Quoref (Dasigi et al., 2019) 50,615 18,908 18,770 76,568 4,343 4,293 130,319 5,675 6,198 104,071 6,418 6,418 19,399 1,209 1,209 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 Acc. Acc. Acc. Acc. Acc. 87.6 92.7 89.7 90.5 50.8 93.6 – 92.0 95.8 – Acc. Acc. Acc. EM Acc. Acc. Acc. Acc. Acc. 86.0 78.8 74.6 55.9 79.9 71.5 85.0 77.6 77.3 100.0 100.0 88.9 75.8 88.1 80.0 92.9 94.9 94.0 Acc. Acc. Acc. Acc. Acc. Acc. Acc. Acc. Acc"
2021.acl-long.92,D19-1606,0,0.0495168,"Missing"
2021.acl-long.92,D15-1075,1,0.813148,"Missing"
2021.acl-long.92,N19-1423,0,0.0754342,"Missing"
2021.acl-long.92,N19-1300,0,0.0154629,"2020) Sentence-Level Multiple Choice |Train| |Dev ||Test |Cust. Metric RoBERTa Human 2,490 138 139 550,152 10,000 10,000 392,702 9,823 9,824 250 28 28 1,105,719 3,200 3,200 400 554 9,741 3,026 33,410 5,428 169,654 16,113 40,398 50 52 610 757 977 319 766 919 633 50 52 611 9,442 977 319 766 919 634 2,251 1,119 1,211 14,191 9,427 25,262 39,905 7,088 7,088 10,246 570 299 317 2,020 1,635 1,492 5,021 443 443 2,164 2,376 1,172 445 3,610 1,635 1,493 5,021 443 443 556 ARC-Easy (Clark et al., 2018) ARC-Challenge (Clark et al., 2018) ARCT (Habernal et al., 2018) MCScript (Ostermann et al., 2018) BoolQ (Clark et al., 2019) Cosmos QA (Huang et al., 2019) HellaSwag (Zellers et al., 2019) MuTual (Cui et al., 2020) MuTual+ (Cui et al., 2020) QuAIL (Rogers et al., 2020) QAMR (Michael et al., 2018) NewsQA (Trischler et al., 2017) SQuAD2.0 (Rajpurkar et al., 2018) MRQA-NQ (Kwiatkowski et al., 2019) Quoref (Dasigi et al., 2019) 50,615 18,908 18,770 76,568 4,343 4,293 130,319 5,675 6,198 104,071 6,418 6,418 19,399 1,209 1,209 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 Acc. Acc. Acc. Acc. Acc. 87.6 92.7 89.7 90.5 50.8 93.6 – 92.0 95.8 – Acc. Acc. Acc. EM Acc. Acc. Acc. Acc. Acc. 86.0 78.8 74.6 55.9 79.9 71.5 85.0 77.6 77.3"
2021.acl-long.92,D16-1062,0,0.606927,"robability that a model will correctly handle an example in a test set depends on the model’s latent ability parameter and three example-specific parameters, typically measuring example difficulty (how strong does a model have to be to get it right), discrimination (how effective the example is for differentiating between similar models), and guessing (how likely a weak model is to get the example right for spurious reasons). This paper presents a large-scale IRT analysis of existing English NLU datasets. Unlike previous work which focuses on example-level analysis within individual datasets (Lalor et al., 2016, 2018), here we analyze example characteristics from a larger perspective by comparing individual examples across datasets. We evaluate test sets from 29 datasets in different formats—classification, multiple-choice QA, and span-selection QA. As responses, we use model predictions from 18 Transformer-based models, including some limited-capacity models chosen to expose better the dataset’s ability to discriminate weaker from stronger predictors. We then fit a single IRT model on these responses using a variational inference method.2 2 Our data and code can be found at https://github. com/nyu-"
2021.acl-long.92,P13-1139,0,0.0307078,"and False (Table 2). For SQuAD2.0 and QuAIL, we analyze the context length, the answerability of a question, and the lexical overlap between context and questions. However, we do not find any clear evidence that any of them might indicate the difficulty level of test examples. For BoolQ, we observe that the 20 most discriminating examples are all labeled False while 13 of the 20 least discriminating examples are labeled True. Table 2 shows the hardest and the easiest examples of MNLI and MC-TACO. 5 Related Work Prior work on using IRT to evaluate NLP systems mostly relies on human responses. Hopkins and May (2013) use IRT to estimate the relative ability of a set of machine translation systems using responses from pairwise comparison of system outputs by human judges. Otani et al. (2016) extend this work by including a baseline translation to the pairwise comparison. Lalor et al. (2016, 2018) use IRT to identify hard examples in natural language inference data based on human responses. In a follow-up study, Lalor et al. (2019) compare human versus model responses and find that both are positively correlated and demonstrate the use cases of IRT parameters in training set filtering. Sedoc and Ungar (2020"
2021.acl-long.92,P19-1439,1,0.900182,"Missing"
2021.acl-long.92,W18-5446,1,0.87719,"Missing"
2021.acl-long.92,N19-1421,0,0.132588,"Missing"
2021.acl-long.92,N18-1101,1,0.832608,"t as our training set and the rest as a our validation set while leaving the original test set untouched. 3 https://github.com/mrqa/ MRQA-Shared-Task-2019 1143 Classification COPA (Roemmele et al., 2011) WSC (Levesque et al., 2012) CommonsenseQA (CSQA; Talmor et al., 2019) MC-TACO (Zhou et al., 2019) SocialIQA (Sap et al., 2019) WiC (Pilehvar and Camacho-Collados, 2019) Abductive NLI (AbductNLI; Bhagavatula et al., 2020) PIQA (Bisk et al., 2020) WinoGrande (Sakaguchi et al., 2020) Span Selection Paragraph-Level Multiple Choice RTE (Dagan et al., 2005, et seq.) SNLI (Bowman et al., 2015) MNLI (Williams et al., 2018) CommitmentBank (CB; De Marneffe et al., 2019) ANLI (Nie et al., 2020) Sentence-Level Multiple Choice |Train| |Dev ||Test |Cust. Metric RoBERTa Human 2,490 138 139 550,152 10,000 10,000 392,702 9,823 9,824 250 28 28 1,105,719 3,200 3,200 400 554 9,741 3,026 33,410 5,428 169,654 16,113 40,398 50 52 610 757 977 319 766 919 633 50 52 611 9,442 977 319 766 919 634 2,251 1,119 1,211 14,191 9,427 25,262 39,905 7,088 7,088 10,246 570 299 317 2,020 1,635 1,492 5,021 443 443 2,164 2,376 1,172 445 3,610 1,635 1,493 5,021 443 443 556 ARC-Easy (Clark et al., 2018) ARC-Challenge (Clark et al., 2018) ARCT ("
2021.acl-long.92,W17-2623,0,0.0295077,"5,428 169,654 16,113 40,398 50 52 610 757 977 319 766 919 633 50 52 611 9,442 977 319 766 919 634 2,251 1,119 1,211 14,191 9,427 25,262 39,905 7,088 7,088 10,246 570 299 317 2,020 1,635 1,492 5,021 443 443 2,164 2,376 1,172 445 3,610 1,635 1,493 5,021 443 443 556 ARC-Easy (Clark et al., 2018) ARC-Challenge (Clark et al., 2018) ARCT (Habernal et al., 2018) MCScript (Ostermann et al., 2018) BoolQ (Clark et al., 2019) Cosmos QA (Huang et al., 2019) HellaSwag (Zellers et al., 2019) MuTual (Cui et al., 2020) MuTual+ (Cui et al., 2020) QuAIL (Rogers et al., 2020) QAMR (Michael et al., 2018) NewsQA (Trischler et al., 2017) SQuAD2.0 (Rajpurkar et al., 2018) MRQA-NQ (Kwiatkowski et al., 2019) Quoref (Dasigi et al., 2019) 50,615 18,908 18,770 76,568 4,343 4,293 130,319 5,675 6,198 104,071 6,418 6,418 19,399 1,209 1,209 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 Acc. Acc. Acc. Acc. Acc. 87.6 92.7 89.7 90.5 50.8 93.6 – 92.0 95.8 – Acc. Acc. Acc. EM Acc. Acc. Acc. Acc. Acc. 86.0 78.8 74.6 55.9 79.9 71.5 85.0 77.6 77.3 100.0 100.0 88.9 75.8 88.1 80.0 92.9 94.9 94.0 Acc. Acc. Acc. Acc. Acc. Acc. Acc. Acc. Acc. Acc. 62.5 37.5 86.7 92.8 85.7 79.4 84.1 87.8 77.9 73.3 – – 79.8 98.2 89.0 94.0 95.6 93.8 93.0 – EM EM EM EM EM 79"
2021.acl-long.92,P19-1472,0,0.0660269,"t. Metric RoBERTa Human 2,490 138 139 550,152 10,000 10,000 392,702 9,823 9,824 250 28 28 1,105,719 3,200 3,200 400 554 9,741 3,026 33,410 5,428 169,654 16,113 40,398 50 52 610 757 977 319 766 919 633 50 52 611 9,442 977 319 766 919 634 2,251 1,119 1,211 14,191 9,427 25,262 39,905 7,088 7,088 10,246 570 299 317 2,020 1,635 1,492 5,021 443 443 2,164 2,376 1,172 445 3,610 1,635 1,493 5,021 443 443 556 ARC-Easy (Clark et al., 2018) ARC-Challenge (Clark et al., 2018) ARCT (Habernal et al., 2018) MCScript (Ostermann et al., 2018) BoolQ (Clark et al., 2019) Cosmos QA (Huang et al., 2019) HellaSwag (Zellers et al., 2019) MuTual (Cui et al., 2020) MuTual+ (Cui et al., 2020) QuAIL (Rogers et al., 2020) QAMR (Michael et al., 2018) NewsQA (Trischler et al., 2017) SQuAD2.0 (Rajpurkar et al., 2018) MRQA-NQ (Kwiatkowski et al., 2019) Quoref (Dasigi et al., 2019) 50,615 18,908 18,770 76,568 4,343 4,293 130,319 5,675 6,198 104,071 6,418 6,418 19,399 1,209 1,209 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 Acc. Acc. Acc. Acc. Acc. 87.6 92.7 89.7 90.5 50.8 93.6 – 92.0 95.8 – Acc. Acc. Acc. EM Acc. Acc. Acc. Acc. Acc. 86.0 78.8 74.6 55.9 79.9 71.5 85.0 77.6 77.3 100.0 100.0 88.9 75.8 88.1 80.0 92.9 94.9 94.0 Acc. Acc. Acc. Ac"
2021.acl-long.92,2021.acl-long.90,1,0.452405,"Missing"
2021.eacl-main.39,2020.deelio-1.5,1,0.647692,"Missing"
2021.eacl-main.39,Q17-1026,1,0.818565,"of each target task simultaneously. Using this approach, the network captures the common structure underlying all the target tasks. However, multi-task learning requires simul487 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 487–503 April 19 - 23, 2021. ©2021 Association for Computational Linguistics taneous access to all tasks during training. Adding new tasks thus requires complete joint retraining. Further, it is difficult to balance multiple tasks and train a model that solves each task equally well. As has been shown in Lee et al. (2017), these models often overfit on low resource tasks and underfit on high resource tasks. This makes it difficult to effectively transfer knowledge across tasks with all the tasks being solved equally well (Pfeiffer et al., 2020b), thus considerably limiting the applicability of multi-task learning in many scenarios. Recently, adapters (Rebuffi et al., 2017; Houlsby et al., 2019) have emerged as an alternative training strategy. Adapters do not require fine-tuning of all parameters of the pretrained model, and instead introduce a small number of task specific parameters — while keeping the under"
2021.eacl-main.39,2021.acl-long.353,0,0.19641,"distributed across multiple tasks than adapters in earlier layers. The potential reason for this is that the last adapters are not encapsulated between frozen pretrained layers, and can thus be considered as an extension of the preContemporary Work In contemporaneous work, other approaches for parameter efficient fine-tuning have been proposed. Guo et al. (2020) train sparse “diff” vectors which are applied on top of pretrained frozen parameter vectors. Ravfogel and Goldberg (2021) only finetune bias terms of the pretrained language models, achieving similar results as full model finetuning. Li and Liang (2021) propose prefix-tuning for natural language generation tasks. Here, continuous task-specific vectors are trained while the remaining model is kept frozen. These alternative, parameter-efficient fine-tuning strategies all encapsulate the idiosyncratic task-specific information in designated parameters, creating the potential for new composition approaches of multiple tasks. R¨uckl´e et al. (2020a) analyse the training and inference efficiency of adapters and AdapterFusion. For AdapterFusion, they find that adding more tasks to the set of adapters results in a linear increase of computational co"
2021.eacl-main.39,P17-1001,0,0.0343179,"Missing"
2021.eacl-main.39,P19-1441,0,0.0245151,"terFusion Adapter Add & Norm Feed Forward Add & Norm Multi-Head Attention Figure 1: AdapterFusion architecture inside a transformer (Vaswani et al., 2017). The AdapterFusion component takes as input the representations of multiple adapters trained on different tasks and learns a parameterized mixer of the encoded information. Introduction The most commonly used method for solving NLU tasks is to leverage pretrained models, with the dominant architecture being a transformer (Vaswani et al., 2017), typically trained with a language modelling objective (Devlin et al., 2019; Radford et al., 2018; Liu et al., 2019b). Transfer to a task of interest is achieved by fine-tuning all the weights of the pretrained model on that single task, often yielding state-of-the-art results (Zhang and Yang, 2017; Ruder, 2017; Howard and Ruder, 2018; Peters et al., 2019). However, each task of interest requires all the parameters of the network to be fine-tuned, which results in a specialized model for each task. There are two approaches for sharing information across multiple tasks. The first consists of starting from the pretrained language model and sequentially fine-tuning on each of the tasks one by one (Phang et al"
2021.eacl-main.39,P11-1015,0,0.0269545,"oid introducing additional capacity. 4.2 Tasks and Datasets We briefly summarize the different types of tasks that we include in our experiments, and reference the related datasets accordingly. A detailed descriptions can be found in Appendix A.1. Commonsense reasoning is used to gauge whether the model can perform basic reasoning skills: Hellaswag (Zellers et al., 2018, 2019), Winogrande (Sakaguchi et al., 2020), CosmosQA (Huang et al., 2019), CSQA (Talmor et al., 2019), SocialIQA (Sap et al., 2019). Sentiment analysis predicts whether a given text has a positive or negative sentiment: IMDb (Maas et al., 2011), SST (Socher et al., 2013). Natural language inference predicts whether one sentence entails, contradicts, or is neutral to another: MNLI (Williams et al., 2018), SciTail (Khot et al., 2018), SICK (Marelli et al., 2014), RTE (as combined by Wang et al. (2018)), CB (De Marneffe et al., 2019). Sentence relatedness captures whether two sentences include similar content: MRPC (Dolan and Brockett, 2005), QQP5 . We also use an argument mining Argument (Stab et al., 2018) and reading comprehension BoolQ (Clark et al., 2019) dataset. 5 Results We present results for all 16 datasets in Table 1. For re"
2021.eacl-main.39,marelli-etal-2014-sick,0,0.0208113,"can be found in Appendix A.1. Commonsense reasoning is used to gauge whether the model can perform basic reasoning skills: Hellaswag (Zellers et al., 2018, 2019), Winogrande (Sakaguchi et al., 2020), CosmosQA (Huang et al., 2019), CSQA (Talmor et al., 2019), SocialIQA (Sap et al., 2019). Sentiment analysis predicts whether a given text has a positive or negative sentiment: IMDb (Maas et al., 2011), SST (Socher et al., 2013). Natural language inference predicts whether one sentence entails, contradicts, or is neutral to another: MNLI (Williams et al., 2018), SciTail (Khot et al., 2018), SICK (Marelli et al., 2014), RTE (as combined by Wang et al. (2018)), CB (De Marneffe et al., 2019). Sentence relatedness captures whether two sentences include similar content: MRPC (Dolan and Brockett, 2005), QQP5 . We also use an argument mining Argument (Stab et al., 2018) and reading comprehension BoolQ (Clark et al., 2019) dataset. 5 Results We present results for all 16 datasets in Table 1. For reference, we also include the adapter architecture of Houlsby et al. (2019), ST-AHoulsby , which has twice as many parameters compared to ST-A. To provide a fair comparison to Stickland and Murray (2019) we primarily expe"
2021.eacl-main.39,2020.emnlp-main.361,0,0.22089,"Missing"
2021.eacl-main.39,2020.acl-main.467,0,0.0195111,"for which we learn the optimal parameters Θm through minimizing the task’s loss on its training data. 2.1 Current Approaches to Transfer Learning There are two predominant approaches to achieve sharing of information from one task to another. 2.1.1 Sequential Fine-Tuning This involves sequentially updating all the weights of the model on each task. For a set of N tasks, the order of fine-tuning is defined and at each step the model is initialized with the parameters learned through the previous step. However, this approach does not perform well beyond two sequential tasks (Phang et al., 2018; Pruksachatkun et al., 2020) due to catastrophic forgetting. 2.1.2 Multi-Task Learning (MTL) All tasks are trained simultaneously with the aim of learning a shared representation that will enable the model to generalize better on each task (Caruana, 1997; Collobert and Weston, 2008; Nam et al., 2014; Liu et al., 2016, 2017; Zhang and Yang, 2017; Ruder, 2017; Ruder et al., 2019; Sanh et al., 2019; Pfeiffer et al., 2020b, inter alia). ! N X Θ0→{1,...,N } ← argmin Ln (Dn ; Θ0 ) Θ 2.2.1 Single-Task Adapters (ST-A) For each of the N tasks, the model is initialized with parameters Θ0 . In addition, a set of new and randomly in"
2021.eacl-main.39,W19-4302,0,0.0615418,"Missing"
2021.eacl-main.39,2020.emnlp-demos.7,1,0.852029,"Missing"
2021.eacl-main.39,D19-1410,1,0.848301,", and 12 and ST-A in Figure 4 (see Appendix Figure 6 for the remaining layers). We find that tasks which do not benefit from AdapterFusion tend to more strongly activate their own adapter at every layer (e.g. Argument, HellaSwag, MNLI, QQP, SciTail). This confirms that AdapterFusion only extracts information from adapters if they are beneficial for the target task m. We further find that MNLI is a useful intermediate task that benefits a large number of target tasks, e.g. BoolQ, SICK, CSQA, SST-2, CB, MRPC, RTE, which is in line with previous work (Phang et al., 2018; Conneau and Kiela, 2018; Reimers and Gurevych, 2019). Similarly, QQP is utilized by a large number of tasks, e.g. SICK, IMDB, RTE, CB, MRPC, SST-2. Most importantly, tasks with small datasets such as CB, RTE, and MRPC often strongly rely on adapters trained on large datasets such as MNLI and QQP. Interestingly, we find that the activations in layer 12 are considerably more distributed across multiple tasks than adapters in earlier layers. The potential reason for this is that the last adapters are not encapsulated between frozen pretrained layers, and can thus be considered as an extension of the preContemporary Work In contemporaneous work, ot"
2021.eacl-main.39,2020.emnlp-main.617,1,0.585903,"Missing"
2021.eacl-main.39,2020.emnlp-main.194,1,0.847232,"Missing"
2021.eacl-main.39,W18-5446,0,0.037738,"reasoning is used to gauge whether the model can perform basic reasoning skills: Hellaswag (Zellers et al., 2018, 2019), Winogrande (Sakaguchi et al., 2020), CosmosQA (Huang et al., 2019), CSQA (Talmor et al., 2019), SocialIQA (Sap et al., 2019). Sentiment analysis predicts whether a given text has a positive or negative sentiment: IMDb (Maas et al., 2011), SST (Socher et al., 2013). Natural language inference predicts whether one sentence entails, contradicts, or is neutral to another: MNLI (Williams et al., 2018), SciTail (Khot et al., 2018), SICK (Marelli et al., 2014), RTE (as combined by Wang et al. (2018)), CB (De Marneffe et al., 2019). Sentence relatedness captures whether two sentences include similar content: MRPC (Dolan and Brockett, 2005), QQP5 . We also use an argument mining Argument (Stab et al., 2018) and reading comprehension BoolQ (Clark et al., 2019) dataset. 5 Results We present results for all 16 datasets in Table 1. For reference, we also include the adapter architecture of Houlsby et al. (2019), ST-AHoulsby , which has twice as many parameters compared to ST-A. To provide a fair comparison to Stickland and Murray (2019) we primarily experiment with BERT-baseuncased. We additio"
2021.eacl-main.95,N16-1014,0,0.284479,"neural language generation (NLG). In particular, we focus on the opendomain dialogue response task, for the following reasons: (1) There is high similarity between the target dialogue response task (conditional NLG) and the pretraining language modeling (LM) objective, so we expect that language generation skills learnt during pretraining can be well transferred to the down-stream target task. (2) The sequence-tosequence (seq2seq) nature of the model allows us to characterize the model’s generation behavior in various ways (e.g., context sensitivity). End-to-end dialogue response generation (Li et al., 2016) can be formulated as a sequence-tosequence (seq2seq) task: Given a dialogue context (previous utterances), the model is asked to generate a high-quality response. In this work we adopt the encoder-decoder model architecture (Sutskever et al., 2014; Cho et al., 2014), which is widely used in NLG applications like dialogue response generation (Li et al., 2016), machine translation (Luong et al., 2015), etc. In particular, we use the transformer model (Vaswani et al., 2017), which has currently become the most popular encoderdecoder model architecture (Young et al., 2017). We use the same config"
2021.eacl-main.95,I17-1099,0,0.0177841,"large-scale CCNEWS data (Bakhtin et al., 2019) which is a de-duplicated subset of the English portion of the CommonCrawl news dataset1 . The dataset contains news articles published worldwide between September 2016 and February 2019. It has in total around 1 billion sentences or 27 billion words. To be able to complete experiments in a reasonable amount of time, we use the first 10 percent of the CCNEWS data for pretraining, which contains 100 million sentences and 2.7 billion words. For finetuning, three open-domain conversational dialogue datasets are used: Dailydialog (1.3 million words) (Li et al., 2017), Switchboard (1.2 million words), and Cornell Movie (Danescu-Niculescu-Mizil and Lee, 2011) (4.5 million words). To save space, we defer the details of the data-sets to Appendix B. To construct the vocabulary, we learn codes of Byte Pair Encoding (BPE) (Sennrich et al., 2016) from the CCNEWS-100m data with 50k merges. This results in a vocabulary of size 62k. We then apply the same BPE codes to all target dialogue datasets. 4.2 Implementation Our code is based on the Fairseq toolkit (Ott et al., 2019). The Adam optimizer (Kingma and Ba, 2014) is used for all experiments. For pretraining of bo"
2021.eacl-main.95,P18-1138,0,0.0213915,"ledge-grounded datasets such as Topical-chat (Gopalakrishnan et al., 2019). 7 Related Works Behavior of pretrained NLG Models Recently, multiple works (Radford et al., 2019; Jiang et al., 2020; Roberts et al., 2020; Talmor et al., 2019; Trinh and Le, 2019) have reported that pre-trained language models (LM) have implicitly stored large amounts of “world knowledge” in its parameters, and are able to answer common-sense questions. However, whether the world knowledge is well preserved after finetuning on target task dataset is not discussed. 1128 On the other hand, knowledge-grounded NLG model (Liu et al., 2018; Guu et al., 2020; Zhou et al., 2018) has been an important and exciting research topic. These studies usually involve additional retrieval modules or external knowledge bases to provide the model with relevant information. In contrast to these works, we study whether the model can conduct knowledgeable dialogues by itself. Forgetting As discussed in Section 3.2, in contrast to the “catastrophic forgetting” problem in sequential learning (Atkinson et al., 2018; Robins, 1995), the performance drop on pretraining data is not necessarily bad for the NLP pretrain-finetune framework, and its impli"
2021.eacl-main.95,2021.ccl-1.108,0,0.063944,"Missing"
2021.eacl-main.95,D15-1166,0,0.0459636,") The sequence-tosequence (seq2seq) nature of the model allows us to characterize the model’s generation behavior in various ways (e.g., context sensitivity). End-to-end dialogue response generation (Li et al., 2016) can be formulated as a sequence-tosequence (seq2seq) task: Given a dialogue context (previous utterances), the model is asked to generate a high-quality response. In this work we adopt the encoder-decoder model architecture (Sutskever et al., 2014; Cho et al., 2014), which is widely used in NLG applications like dialogue response generation (Li et al., 2016), machine translation (Luong et al., 2015), etc. In particular, we use the transformer model (Vaswani et al., 2017), which has currently become the most popular encoderdecoder model architecture (Young et al., 2017). We use the same configuration as (Vaswani et al., 2017), which has 6 encoder/decoder layers, 16 attention heads, with an embedding dimension of 1024 and a feed-forward dimension of 4096. During standard finetuning, the Adam optimizer (Kingma and Ba, 2014) is used to minimize the negative log-likelihood (NLL) of the reference target sentence y given the input context x in the data distribution (denoted as Pdata ): Lfinetun"
2021.eacl-main.95,N19-4009,1,0.840787,"three open-domain conversational dialogue datasets are used: Dailydialog (1.3 million words) (Li et al., 2017), Switchboard (1.2 million words), and Cornell Movie (Danescu-Niculescu-Mizil and Lee, 2011) (4.5 million words). To save space, we defer the details of the data-sets to Appendix B. To construct the vocabulary, we learn codes of Byte Pair Encoding (BPE) (Sennrich et al., 2016) from the CCNEWS-100m data with 50k merges. This results in a vocabulary of size 62k. We then apply the same BPE codes to all target dialogue datasets. 4.2 Implementation Our code is based on the Fairseq toolkit (Ott et al., 2019). The Adam optimizer (Kingma and Ba, 2014) is used for all experiments. For pretraining of both MASS and NS, we use a mini-batch size of 2048, with the learning rate (LR) set to 0.0001. Following (Vaswani et al., 2017), the “inverse square root” LR scheduler with a warm-up stage is used. Pretraining is conducted on 32 GPUs and half-precision (float16) speed-up is used. For both MASS and NS, we stop the pretraining after the CCNEWS data is swept 20 times. For all our experiments, a dropout rate of 0.1 is applied to the transformer model. We follow Song et al. (2019) for the recommended hyper-pa"
2021.eacl-main.95,N18-1202,0,0.214203,"from the perspectives of knowledge transfer, context sensitivity, and function space projection. As a preliminary attempt to alleviate the forgetting problem, we propose an intuitive finetuning strategy named “mix-review”. We find that mix-review effectively regularizes the finetuning process, and the forgetting problem is alleviated to some extent. Finally, we discuss interesting behavior of the resulting dialogue model and its implications. 1 Figure 1: During finetuning, the model’s performance on the pretraining data drastically degrades. Introduction Large-scale unsupervised pretraining (Peters et al., 2018; Devlin et al., 2018; Song et al., 2019; Yang et al., 2019; Liu et al., 2019) has recently been shown to greatly boost the performance of natural language processing (NLP) models. On a high level, the pretrain-finetune framework can be viewed as a simple two-stage procedure: (1) Use large-scale unsupervised text data to pretrain the model; (2) Use target task data to finetune the model. Recently, multiple works (Radford et al., 2019; Jiang et al., 2020; Roberts et al., 2020; Talmor et al., 2019) have reported that pretrained language models (LM) have implicitly stored large amounts of “world"
2021.eacl-main.95,P19-1004,0,0.0212955,"e dialogue responses more informative and engaging (e.g., the model can learn about the “Avengers” movie, and use it as a topic). To quantify how knowledgeable the finetuned model is, we prepare a set of knowledge terms such as iphone, pokemon, etc., and the corresponding reference description. We then query the model about these knowledge terms, and compare its output against the reference. We also conduct multi-turn human evaluation in the setting of knowledgeable conversations. More details will be given in Section 5.1. The other ability is the utilization of contextual input: as shown by (Sankar et al., 2019), the current open-domain dialogue models (without pretraining) are insensitive to contextual input, which gives rise to the generic response problem (Li et al., 2016). In our preliminary experiments with NS pretraining, we find that similarly to the GPT model (Radford et al., 2019) the pretrained model has the ability to generate closely related responses given the previous sentences as input. Ideally during finetuning, the model can transfer this skill to the target dialogue task. To quantify the model’s sensitivity to context, following (Sankar et al., 2019), we add noise to the input, and"
2021.eacl-main.95,P16-1162,0,0.0261507,"tences or 27 billion words. To be able to complete experiments in a reasonable amount of time, we use the first 10 percent of the CCNEWS data for pretraining, which contains 100 million sentences and 2.7 billion words. For finetuning, three open-domain conversational dialogue datasets are used: Dailydialog (1.3 million words) (Li et al., 2017), Switchboard (1.2 million words), and Cornell Movie (Danescu-Niculescu-Mizil and Lee, 2011) (4.5 million words). To save space, we defer the details of the data-sets to Appendix B. To construct the vocabulary, we learn codes of Byte Pair Encoding (BPE) (Sennrich et al., 2016) from the CCNEWS-100m data with 50k merges. This results in a vocabulary of size 62k. We then apply the same BPE codes to all target dialogue datasets. 4.2 Implementation Our code is based on the Fairseq toolkit (Ott et al., 2019). The Adam optimizer (Kingma and Ba, 2014) is used for all experiments. For pretraining of both MASS and NS, we use a mini-batch size of 2048, with the learning rate (LR) set to 0.0001. Following (Vaswani et al., 2017), the “inverse square root” LR scheduler with a warm-up stage is used. Pretraining is conducted on 32 GPUs and half-precision (float16) speed-up is used"
2021.emnlp-main.422,D13-1185,0,0.0147787,"on all evaluation tasks, since it ignores the coreferential arguments and their relations, but 6 Related Work relies solely on the overly simplistic temporal order to connect events. This is especially apparent from The definition of a complex event schema septhe instance graph perplexity in Table 3. arates us from related lines of work, namely Learning Corpus Size. An average of 113 in- schema induction and script learning. Previous stance graphs is used for each complex event type work on schema induction aims to characterize 5210 event triggers and participants of individual atomic events (Chambers, 2013; Cheung et al., 2013; Nguyen et al., 2015; Sha et al., 2016; Yuan et al., 2018), ignoring inter-event relations. Work on script learning, on the other hand, originally limited attention to event chains with a single protagonist (Chambers and Jurafsky, 2008, 2009; Rudinger et al., 2015; Jans et al., 2012; Granroth-Wilding and Clark, 2016) and later extended to multiple participants (Pichotta and Mooney, 2014, 2016; Weber et al., 2018). Recent efforts rely on distributed representations encoded from the compositional nature of events (Modi, 2016; Granroth-Wilding and Clark, 2016; Weber et al.,"
2021.emnlp-main.422,P08-1090,0,0.742122,"event schema septhe instance graph perplexity in Table 3. arates us from related lines of work, namely Learning Corpus Size. An average of 113 in- schema induction and script learning. Previous stance graphs is used for each complex event type work on schema induction aims to characterize 5210 event triggers and participants of individual atomic events (Chambers, 2013; Cheung et al., 2013; Nguyen et al., 2015; Sha et al., 2016; Yuan et al., 2018), ignoring inter-event relations. Work on script learning, on the other hand, originally limited attention to event chains with a single protagonist (Chambers and Jurafsky, 2008, 2009; Rudinger et al., 2015; Jans et al., 2012; Granroth-Wilding and Clark, 2016) and later extended to multiple participants (Pichotta and Mooney, 2014, 2016; Weber et al., 2018). Recent efforts rely on distributed representations encoded from the compositional nature of events (Modi, 2016; Granroth-Wilding and Clark, 2016; Weber et al., 2018, 2020; Zhang et al., 2020), and language modeling (Rudinger et al., 2015; Pichotta and Mooney, 2016; Peng and Roth, 2016). All of these methods still assume that events follow linear order in a single chain. They also overlook the relations between par"
2021.emnlp-main.422,chambers-jurafsky-2010-database,0,0.0434858,"Missing"
2021.emnlp-main.422,N13-1104,0,0.0305441,"n tasks, since it ignores the coreferential arguments and their relations, but 6 Related Work relies solely on the overly simplistic temporal order to connect events. This is especially apparent from The definition of a complex event schema septhe instance graph perplexity in Table 3. arates us from related lines of work, namely Learning Corpus Size. An average of 113 in- schema induction and script learning. Previous stance graphs is used for each complex event type work on schema induction aims to characterize 5210 event triggers and participants of individual atomic events (Chambers, 2013; Cheung et al., 2013; Nguyen et al., 2015; Sha et al., 2016; Yuan et al., 2018), ignoring inter-event relations. Work on script learning, on the other hand, originally limited attention to event chains with a single protagonist (Chambers and Jurafsky, 2008, 2009; Rudinger et al., 2015; Jans et al., 2012; Granroth-Wilding and Clark, 2016) and later extended to multiple participants (Pichotta and Mooney, 2014, 2016; Weber et al., 2018). Recent efforts rely on distributed representations encoded from the compositional nature of events (Modi, 2016; Granroth-Wilding and Clark, 2016; Weber et al., 2018, 2020; Zhang et"
2021.emnlp-main.422,P16-1154,0,0.011558,"ained the new event ei is an A RREST event, so we add LDC Schema Learning Ontology. three argument nodes for D ETAINEE, JAILOR, and 5 Compared to (Liao et al., 2019), we do not use the posiP LACE respectively. The edges between these ar- tional embedding mask because the newly generated nodes guments and event ei are also added into the graph. have distinct roles. 5206 3.5 Coreferential Argument Generation After updating the node representations, we detect the entity type of each argument, and also predict whether the argument is coreferential to existing entities. Inspired by copy mechanism (Gu et al., 2016), we classify each argument node vj to either a new entity with entity type φ(vj ), or an existing entity node in the previous graph G&lt;i . For example, in Figure 2, the D ETAINEE should be classified to the existing ATTACKER node, while JAILOR node is classified as P ERSON. Namely, p(hei , aj , vj i|ei , aj ) ( p(hei , aj , vj i, g|ei , aj ) if vj is new, = p(hei , aj , vj i, c|ei , aj ) otherwise, where p(hei , aj , vj i, g|ei , aj ) is the generation probability, classifying the new node to its entity type φ(vj ):  p(hei , aj , vj i, g|ei , aj ) = exp(W φ(vj ) v j ) Z The copy probability p"
2021.emnlp-main.422,E12-1034,0,0.0677415,"Missing"
2021.emnlp-main.422,D19-6014,0,0.0172787,"lso overlook the relations between participants which are critical for understanding the complex event. However, we induce a comprehensive event graph schema, capturing both the temporal dependency and the multi-hop argument dependency across events. Recent work on event graph schema induction (Li et al., 2020) only considers the connections between a pair of two events. Similarly, their event prediction task is designed to automatically generate a missing event (e.g., a word sequence) given a single or a sequence of prerequisite events (Nguyen et al., 2017; Hu et al., 2017; Li et al., 2018b; Kiyomaru et al., 2019; Lv et al., 2019), or predict a pre-condition event given the current events (Kwon et al., 2020). In contrast, we leverage the automatically discovered temporal event schema as guidance to forecast the future events. Existing script annotations (Chambers and Jurafsky, 2008, 2010; Modi et al., 2016; Wanzare et al., 2016; Mostafazadeh et al., 2016a,b; Kwon et al., 2020) cannot support a comprehensive graph schema induction due to the missing of critical event graph structures, such as argument relations. Furthermore, in real-world applications, complex event schemas are expected to be induced f"
2021.emnlp-main.422,2020.findings-emnlp.340,0,0.0275404,"nt. However, we induce a comprehensive event graph schema, capturing both the temporal dependency and the multi-hop argument dependency across events. Recent work on event graph schema induction (Li et al., 2020) only considers the connections between a pair of two events. Similarly, their event prediction task is designed to automatically generate a missing event (e.g., a word sequence) given a single or a sequence of prerequisite events (Nguyen et al., 2017; Hu et al., 2017; Li et al., 2018b; Kiyomaru et al., 2019; Lv et al., 2019), or predict a pre-condition event given the current events (Kwon et al., 2020). In contrast, we leverage the automatically discovered temporal event schema as guidance to forecast the future events. Existing script annotations (Chambers and Jurafsky, 2008, 2010; Modi et al., 2016; Wanzare et al., 2016; Mostafazadeh et al., 2016a,b; Kwon et al., 2020) cannot support a comprehensive graph schema induction due to the missing of critical event graph structures, such as argument relations. Furthermore, in real-world applications, complex event schemas are expected to be induced from large-scale historical data, which is not feasible to annotate manually. We propose a data-dr"
2021.emnlp-main.422,2021.naacl-main.274,1,0.713586,"complex event type, such as car-bombing, we construct a set of instance graphs, where each instance graph is about one complex event, such as Kabul ambulance bombing. We first identify a cluster of documents that describes the same complex event. In this paper, we treat all documents linked to a single Wikipedia page as belonging to the same complex event, detailed in §4.1. We use OneIE, a state-of-the-art Information Extraction system (Lin et al., 2020), to extract entities, relations and events, and then perform crossdocument entity (Pan et al., 2015, 2017) and event coreference resolution (Lai et al., 2021) over the document cluster of each complex event. We further conduct event-event temporal relation extraction (Ning et al., 2019; Wen et al., 2021b) to determine the order of event pairs. We run the entire 2 For simplification purposes, we mention “schema graphs” as “schemas”, and “events” in schemas are only “event types”. pipeline following (Wen et al., 2021a) 3 , and the detailed extraction performance is reported in the paper. After extraction, we construct one instance graph for each complex event, where coreferential events or entities are merged. We consider the isolated events as irrel"
2021.emnlp-main.422,2020.emnlp-main.50,1,0.80141,"tional nature of events (Modi, 2016; Granroth-Wilding and Clark, 2016; Weber et al., 2018, 2020; Zhang et al., 2020), and language modeling (Rudinger et al., 2015; Pichotta and Mooney, 2016; Peng and Roth, 2016). All of these methods still assume that events follow linear order in a single chain. They also overlook the relations between participants which are critical for understanding the complex event. However, we induce a comprehensive event graph schema, capturing both the temporal dependency and the multi-hop argument dependency across events. Recent work on event graph schema induction (Li et al., 2020) only considers the connections between a pair of two events. Similarly, their event prediction task is designed to automatically generate a missing event (e.g., a word sequence) given a single or a sequence of prerequisite events (Nguyen et al., 2017; Hu et al., 2017; Li et al., 2018b; Kiyomaru et al., 2019; Lv et al., 2019), or predict a pre-condition event given the current events (Kwon et al., 2020). In contrast, we leverage the automatically discovered temporal event schema as guidance to forecast the future events. Existing script annotations (Chambers and Jurafsky, 2008, 2010; Modi et a"
2021.emnlp-main.422,2021.naacl-main.69,1,0.668757,"ns between arguments, so we only compute this metric for the IED dataset. IED Schema Learning Corpus: The same type of complex events may have many variants, which depends on the different types of conditions and participants. In order to evaluate our model’s capability at capturing uncertainty and multiple hypotheses, we decided to dive deeper into one scenario and chose the improvised explosive device (IED) as our case study. We first collected Wikipedia articles that describe 4 types of complex events, i.e., Car-bombing IED, Drone Strikes IED, Suicide IED and General IED. Then we followed (Li et al., 2021) to exploit the external links to collect the additional news documents with the corresponding complex event type. The ground-truth schemas for this IED corpus are created manually, through a schema curation 4.3 Instance Graph Perplexity Evaluation tool (Mishra et al., 2021). Only one human schema graph was created for each complex event type, To evaluate our temporal event graph model, we resulting in 4 schemas. In detail, for each com- compute the instance graph perplexity by predictplex event type, we presented example instance ing the instance graphs in the test set, graphs and the ranked"
2021.emnlp-main.422,L16-1555,0,0.0194662,"l., 2020) only considers the connections between a pair of two events. Similarly, their event prediction task is designed to automatically generate a missing event (e.g., a word sequence) given a single or a sequence of prerequisite events (Nguyen et al., 2017; Hu et al., 2017; Li et al., 2018b; Kiyomaru et al., 2019; Lv et al., 2019), or predict a pre-condition event given the current events (Kwon et al., 2020). In contrast, we leverage the automatically discovered temporal event schema as guidance to forecast the future events. Existing script annotations (Chambers and Jurafsky, 2008, 2010; Modi et al., 2016; Wanzare et al., 2016; Mostafazadeh et al., 2016a,b; Kwon et al., 2020) cannot support a comprehensive graph schema induction due to the missing of critical event graph structures, such as argument relations. Furthermore, in real-world applications, complex event schemas are expected to be induced from large-scale historical data, which is not feasible to annotate manually. We propose a data-driven schema induction approach, and choose to use IE systems instead of using manual annotation, to induce schemas that are robust and can tolerate extraction errors. modeling and generation of graphs ("
2021.emnlp-main.422,2020.acl-main.713,1,0.793931,"rded as a summary abstraction of instance graphs, capturing the reoccurring structures. 3 3.1 Our Approach Instance Graph Construction To induce schemas for a complex event type, such as car-bombing, we construct a set of instance graphs, where each instance graph is about one complex event, such as Kabul ambulance bombing. We first identify a cluster of documents that describes the same complex event. In this paper, we treat all documents linked to a single Wikipedia page as belonging to the same complex event, detailed in §4.1. We use OneIE, a state-of-the-art Information Extraction system (Lin et al., 2020), to extract entities, relations and events, and then perform crossdocument entity (Pan et al., 2015, 2017) and event coreference resolution (Lai et al., 2021) over the document cluster of each complex event. We further conduct event-event temporal relation extraction (Ning et al., 2019; Wen et al., 2021b) to determine the order of event pairs. We run the entire 2 For simplification purposes, we mention “schema graphs” as “schemas”, and “events” in schemas are only “event types”. pipeline following (Wen et al., 2021a) 3 , and the detailed extraction performance is reported in the paper. After"
2021.emnlp-main.422,K16-1008,0,0.0141284,"and participants of individual atomic events (Chambers, 2013; Cheung et al., 2013; Nguyen et al., 2015; Sha et al., 2016; Yuan et al., 2018), ignoring inter-event relations. Work on script learning, on the other hand, originally limited attention to event chains with a single protagonist (Chambers and Jurafsky, 2008, 2009; Rudinger et al., 2015; Jans et al., 2012; Granroth-Wilding and Clark, 2016) and later extended to multiple participants (Pichotta and Mooney, 2014, 2016; Weber et al., 2018). Recent efforts rely on distributed representations encoded from the compositional nature of events (Modi, 2016; Granroth-Wilding and Clark, 2016; Weber et al., 2018, 2020; Zhang et al., 2020), and language modeling (Rudinger et al., 2015; Pichotta and Mooney, 2016; Peng and Roth, 2016). All of these methods still assume that events follow linear order in a single chain. They also overlook the relations between participants which are critical for understanding the complex event. However, we induce a comprehensive event graph schema, capturing both the temporal dependency and the multi-hop argument dependency across events. Recent work on event graph schema induction (Li et al., 2020) only considers the"
2021.emnlp-main.422,W16-1007,0,0.0187693,"between a pair of two events. Similarly, their event prediction task is designed to automatically generate a missing event (e.g., a word sequence) given a single or a sequence of prerequisite events (Nguyen et al., 2017; Hu et al., 2017; Li et al., 2018b; Kiyomaru et al., 2019; Lv et al., 2019), or predict a pre-condition event given the current events (Kwon et al., 2020). In contrast, we leverage the automatically discovered temporal event schema as guidance to forecast the future events. Existing script annotations (Chambers and Jurafsky, 2008, 2010; Modi et al., 2016; Wanzare et al., 2016; Mostafazadeh et al., 2016a,b; Kwon et al., 2020) cannot support a comprehensive graph schema induction due to the missing of critical event graph structures, such as argument relations. Furthermore, in real-world applications, complex event schemas are expected to be induced from large-scale historical data, which is not feasible to annotate manually. We propose a data-driven schema induction approach, and choose to use IE systems instead of using manual annotation, to induce schemas that are robust and can tolerate extraction errors. modeling and generation of graphs (Li et al., 2018a; Jin et al., 2018; Grover et al."
2021.emnlp-main.422,I17-2007,0,0.0262425,"hat events follow linear order in a single chain. They also overlook the relations between participants which are critical for understanding the complex event. However, we induce a comprehensive event graph schema, capturing both the temporal dependency and the multi-hop argument dependency across events. Recent work on event graph schema induction (Li et al., 2020) only considers the connections between a pair of two events. Similarly, their event prediction task is designed to automatically generate a missing event (e.g., a word sequence) given a single or a sequence of prerequisite events (Nguyen et al., 2017; Hu et al., 2017; Li et al., 2018b; Kiyomaru et al., 2019; Lv et al., 2019), or predict a pre-condition event given the current events (Kwon et al., 2020). In contrast, we leverage the automatically discovered temporal event schema as guidance to forecast the future events. Existing script annotations (Chambers and Jurafsky, 2008, 2010; Modi et al., 2016; Wanzare et al., 2016; Mostafazadeh et al., 2016a,b; Kwon et al., 2020) cannot support a comprehensive graph schema induction due to the missing of critical event graph structures, such as argument relations. Furthermore, in real-world applic"
2021.emnlp-main.422,P15-1019,0,0.0538438,"Missing"
2021.emnlp-main.422,D19-1642,0,0.0347696,"Missing"
2021.emnlp-main.422,N15-1119,1,0.748927,"roach Instance Graph Construction To induce schemas for a complex event type, such as car-bombing, we construct a set of instance graphs, where each instance graph is about one complex event, such as Kabul ambulance bombing. We first identify a cluster of documents that describes the same complex event. In this paper, we treat all documents linked to a single Wikipedia page as belonging to the same complex event, detailed in §4.1. We use OneIE, a state-of-the-art Information Extraction system (Lin et al., 2020), to extract entities, relations and events, and then perform crossdocument entity (Pan et al., 2015, 2017) and event coreference resolution (Lai et al., 2021) over the document cluster of each complex event. We further conduct event-event temporal relation extraction (Ning et al., 2019; Wen et al., 2021b) to determine the order of event pairs. We run the entire 2 For simplification purposes, we mention “schema graphs” as “schemas”, and “events” in schemas are only “event types”. pipeline following (Wen et al., 2021a) 3 , and the detailed extraction performance is reported in the paper. After extraction, we construct one instance graph for each complex event, where coreferential events or en"
2021.emnlp-main.422,P17-1178,1,0.891563,"Missing"
2021.emnlp-main.422,L16-1556,0,0.0245624,"iders the connections between a pair of two events. Similarly, their event prediction task is designed to automatically generate a missing event (e.g., a word sequence) given a single or a sequence of prerequisite events (Nguyen et al., 2017; Hu et al., 2017; Li et al., 2018b; Kiyomaru et al., 2019; Lv et al., 2019), or predict a pre-condition event given the current events (Kwon et al., 2020). In contrast, we leverage the automatically discovered temporal event schema as guidance to forecast the future events. Existing script annotations (Chambers and Jurafsky, 2008, 2010; Modi et al., 2016; Wanzare et al., 2016; Mostafazadeh et al., 2016a,b; Kwon et al., 2020) cannot support a comprehensive graph schema induction due to the missing of critical event graph structures, such as argument relations. Furthermore, in real-world applications, complex event schemas are expected to be induced from large-scale historical data, which is not feasible to annotate manually. We propose a data-driven schema induction approach, and choose to use IE systems instead of using manual annotation, to induce schemas that are robust and can tolerate extraction errors. modeling and generation of graphs (Li et al., 2018a; Jin"
2021.emnlp-main.422,P16-1028,0,0.0157009,"ations. Work on script learning, on the other hand, originally limited attention to event chains with a single protagonist (Chambers and Jurafsky, 2008, 2009; Rudinger et al., 2015; Jans et al., 2012; Granroth-Wilding and Clark, 2016) and later extended to multiple participants (Pichotta and Mooney, 2014, 2016; Weber et al., 2018). Recent efforts rely on distributed representations encoded from the compositional nature of events (Modi, 2016; Granroth-Wilding and Clark, 2016; Weber et al., 2018, 2020; Zhang et al., 2020), and language modeling (Rudinger et al., 2015; Pichotta and Mooney, 2016; Peng and Roth, 2016). All of these methods still assume that events follow linear order in a single chain. They also overlook the relations between participants which are critical for understanding the complex event. However, we induce a comprehensive event graph schema, capturing both the temporal dependency and the multi-hop argument dependency across events. Recent work on event graph schema induction (Li et al., 2020) only considers the connections between a pair of two events. Similarly, their event prediction task is designed to automatically generate a missing event (e.g., a word sequence) given a single o"
2021.emnlp-main.422,E14-1024,0,0.0227831,"duction and script learning. Previous stance graphs is used for each complex event type work on schema induction aims to characterize 5210 event triggers and participants of individual atomic events (Chambers, 2013; Cheung et al., 2013; Nguyen et al., 2015; Sha et al., 2016; Yuan et al., 2018), ignoring inter-event relations. Work on script learning, on the other hand, originally limited attention to event chains with a single protagonist (Chambers and Jurafsky, 2008, 2009; Rudinger et al., 2015; Jans et al., 2012; Granroth-Wilding and Clark, 2016) and later extended to multiple participants (Pichotta and Mooney, 2014, 2016; Weber et al., 2018). Recent efforts rely on distributed representations encoded from the compositional nature of events (Modi, 2016; Granroth-Wilding and Clark, 2016; Weber et al., 2018, 2020; Zhang et al., 2020), and language modeling (Rudinger et al., 2015; Pichotta and Mooney, 2016; Peng and Roth, 2016). All of these methods still assume that events follow linear order in a single chain. They also overlook the relations between participants which are critical for understanding the complex event. However, we induce a comprehensive event graph schema, capturing both the temporal depen"
2021.emnlp-main.422,2020.emnlp-main.612,0,0.0293622,"Missing"
2021.emnlp-main.422,2021.naacl-demos.16,1,0.847555,"Missing"
2021.emnlp-main.422,D15-1195,0,0.0450596,"Missing"
2021.emnlp-main.422,N16-1049,0,0.0199393,"l arguments and their relations, but 6 Related Work relies solely on the overly simplistic temporal order to connect events. This is especially apparent from The definition of a complex event schema septhe instance graph perplexity in Table 3. arates us from related lines of work, namely Learning Corpus Size. An average of 113 in- schema induction and script learning. Previous stance graphs is used for each complex event type work on schema induction aims to characterize 5210 event triggers and participants of individual atomic events (Chambers, 2013; Cheung et al., 2013; Nguyen et al., 2015; Sha et al., 2016; Yuan et al., 2018), ignoring inter-event relations. Work on script learning, on the other hand, originally limited attention to event chains with a single protagonist (Chambers and Jurafsky, 2008, 2009; Rudinger et al., 2015; Jans et al., 2012; Granroth-Wilding and Clark, 2016) and later extended to multiple participants (Pichotta and Mooney, 2014, 2016; Weber et al., 2018). Recent efforts rely on distributed representations encoded from the compositional nature of events (Modi, 2016; Granroth-Wilding and Clark, 2016; Weber et al., 2018, 2020; Zhang et al., 2020), and language modeling (Rudi"
2021.emnlp-main.422,2021.naacl-main.6,1,0.815069,"Missing"
2021.emnlp-main.422,2020.emnlp-main.374,0,0.0992327,"Missing"
2021.mrl-1.13,2020.acl-main.425,0,0.2416,"are linked to Wikipedia articles, WordNet synsets, and (when available) highquality images from ImageNet (Deng et al., 2009). VisualSem integrates seamlessly with existing resources. Compared to existing multimodal KGs, VisualSem includes data from different sources and thus it is also more diverse in terms of the domains. We source the images in VisualSem using BabelNet (Navigli and Ponzetto, 2012), a large multilingual and multimodal resource that semiautomatically aggregates information from many different sources. We address the known issue of noisy images in BabelNet (Colla et al., 2018; Calabrese et al., 2020) by applying multiple filtering steps to ensure that we remove noise while maintaining breadth of coverage and image diversity. We also release pre-trained models to retrieve entities from VisualSem using either images or sentences as queries in a k-nearest neighbor search. This effectively allows researchers to integrate entities and facts in VisualSem into their (neural) model pipelines. Code to generate and download VisualSem, as well as retrieval models, is publicly available.2 2 https://github.com/iacercalixto/ 138 Proceedings of the 1st Workshop on Multilingual Representation Learning, p"
2021.mrl-1.13,2020.tacl-1.29,0,0.0379378,"Missing"
2021.mrl-1.13,P19-1081,0,0.0242076,"pipeline that filters out noisy images linked in BabelNet, and we source the remaining high-quality images in VisualSem. All the aforementioned KBs have in common the fact they are mostly “entity-centric”: nodes denote concepts and are associated with multimodal information. This is in contrast to “vision-centric” KBs such as Visual Genome (Krishna et al., 2017) and Visual KB (Zhu et al., 2015), where each instance in the dataset is an image with a scene graph representation to support visual query tasks. Besides visual queries, multimodal KBs have been designed for conversational reasoning (Moon et al., 2019) and node classification (Bloem et al., 2021). VisualSem is an entity-centric KG designed to support tasks at the intersection of vision and language. 6 Conclusions and Future work We present the VisualSem knowledge graph with ∼ 90k nodes, over 1.3M glosses in up to 14 diverse languages, and around 938k curated and cleaned Multi-modal knowledge bases. Two recently proposed multimodal knowledge bases are WN9- images illustrating nodes’ concepts. VisualSem IMG (Xie et al., 2017) and FB15-IMG (Mous- bridges a gap in the available resources to train grounded models of language, and is designed to"
2021.mrl-1.13,S18-2027,0,0.359078,"sks such as open-ended question answering (Guu et al., 2020; Lewis et al., 2020b,a). However, Wikipedia’s wealth of visual information is typically hard to use,1 which prevents models from including rich multi-modal data. Although a small number of structured knowledge graphs (KGs) with images exist and are publicly available, they either cover lim∗ Yibo Liu2 Work initiated while in the University of Amsterdam during her MSc. research. 1 See for instance https://en.wikipedia.org/ wiki/Wikipedia:Images and https://commons. wikimedia.org/wiki/Main_Page. ited domains (Xie et al., 2017; Mousselly Sergieh et al., 2018) or span multiple domains but with noisy images (Navigli and Ponzetto, 2012). To facilitate further progress in this direction, we introduce a new resource to enable research on LMs that efficiently retrieve textual and visual contextual information from KGs, where this information can be used for grounding, data augmentation, among other uses. In this paper, we introduce VisualSem, a multilingual and multimodal KG with ∼ 90k nodes, ∼ 1.3M glosses and ∼ 938k images. VisualSem’s nodes denote concepts and named entities, include multiple curated illustrative images, as well as glosses in up to 1"
2021.mrl-1.13,D19-1410,0,0.0805512,"ranking problem, and use all English glosses available for retrieval. Experiments in this section use the gloss and image splits in Table 4. Training, validation and test splits for glosses and images are chosen randomly in order to facilitate their use by the research community in future experiments. Gloss validation and test splits are balanced regarding different languages, and each split includes 2, 000 examples for each of the 14 languages in VisualSem. 4.1 Sentence retrieval VisualSem has N = 89, 896 nodes with glosses in up to 14 languages. We encode glosses using Sentence BERT (SBERT; Reimers and Gurevych, 2019), which is a family of models based on bi-encoder architectures trained on the task of sentence similarity that have strong performance when used for retrieval/indexing. SBERT models are by default trained on English-only sentences to map semantically similar sentences to points close in the embedding space. We use a multilingual SBERT model trained using knowledge distillation to transfer knowledge from English to other languages, more specifically the paraphrasemultilingual-mpnet-base-v2 model (Reimers and Gurevych, 2020) trained on over 8 50 languages. We select the model from all We use th"
2021.mrl-1.13,2020.emnlp-main.365,0,0.0210701,"Missing"
2021.mrl-1.13,speer-havasi-2012-representing,0,0.0357876,"nd KGs have a long and rich history, and many publicly available KBs exist and have been built for different purposes.11 A seminal example is Cyc (Lenat et al., 1986), an early effort towards building a general-purpose knowledge base to store common sense facts and rules about how the world works. More recent examples of knowledge bases built with different purposes include WordNet (Miller, 1995; Bond and Paik, 2012), DBPedia (Auer et al., 2007), Wikidata (Vrandeˇci´c and Krötzsch, 2014), Freebase (Bollacker et al., 2008), YAGO (Rebele et al., 2016; Pellissier Tanon et al., 2020), ConceptNet (Speer and Havasi, 2012), ATOMIC (Sap et al., 2019), among many others (see Wang et al., 2017; Ji et al., 2020 for a detailed list of knowledge bases and algorithms). Our main goal is to design a KG to support research in vision & language, which none of the abovementioned KBs are designed to do. varied sources—such as Freebase, WordNet and Wikipedia in several languages, among many others—and includes over 54 million images mined mostly from Wikipedia and ImageNet. BabelNet can be seen as a high coverage KB, and in its version v4.0 has more than 15.7 million concepts in 284 languages. At times, images linked in Babe"
2021.spnlp-1.5,N19-1388,0,0.0638364,"Missing"
2021.spnlp-1.5,W18-6322,0,0.021349,"iori inference). However, recent studies suggest that the most likely sequences may not resemble training sequences at all. For instance, the learning stage can yield a distribution pmodel which places high probability on empty (Stahlberg and Byrne, 2019) or repetitive (Holtzman et al., 2019) sequences, while the decoding stage can yield a distribution pF which places non-zero mass on infinite-length sequences (Welleck et al., 2020a). As a result, various workarounds have been proposed in the form of alternative learning or decoding algorithms (Andor et al., 2016; Sountsov and Sarawagi, 2016; Murray and Chiang, 2018; Welleck et al., 2020b; Welleck and Cho, 2020; Martins et al., 2020; Deng et al., 2020; Basu et al., 2021; Shi et al., 2020). A particularly relevant work by Eikema and Aziz (2020) argues that the modes of neural sequence models are inadequate and thus we must discard maximum-a-posteriori inference altogether. Rather than advocating for a particular solution, we instead seek an understanding of why the conventional approach displays these peculiar behaviors. While we do not claim to provide a full explanation, the first step is developing a way of quantifying the problem, then localizing it."
2021.spnlp-1.5,P16-1231,0,0.0247609,"quences under the trained model (maximum a posteriori inference). However, recent studies suggest that the most likely sequences may not resemble training sequences at all. For instance, the learning stage can yield a distribution pmodel which places high probability on empty (Stahlberg and Byrne, 2019) or repetitive (Holtzman et al., 2019) sequences, while the decoding stage can yield a distribution pF which places non-zero mass on infinite-length sequences (Welleck et al., 2020a). As a result, various workarounds have been proposed in the form of alternative learning or decoding algorithms (Andor et al., 2016; Sountsov and Sarawagi, 2016; Murray and Chiang, 2018; Welleck et al., 2020b; Welleck and Cho, 2020; Martins et al., 2020; Deng et al., 2020; Basu et al., 2021; Shi et al., 2020). A particularly relevant work by Eikema and Aziz (2020) argues that the modes of neural sequence models are inadequate and thus we must discard maximum-a-posteriori inference altogether. Rather than advocating for a particular solution, we instead seek an understanding of why the conventional approach displays these peculiar behaviors. While we do not claim to provide a full explanation, the first step is developing"
2021.spnlp-1.5,P16-1009,0,0.088677,"Missing"
2021.spnlp-1.5,D16-1158,0,0.019247,"ained model (maximum a posteriori inference). However, recent studies suggest that the most likely sequences may not resemble training sequences at all. For instance, the learning stage can yield a distribution pmodel which places high probability on empty (Stahlberg and Byrne, 2019) or repetitive (Holtzman et al., 2019) sequences, while the decoding stage can yield a distribution pF which places non-zero mass on infinite-length sequences (Welleck et al., 2020a). As a result, various workarounds have been proposed in the form of alternative learning or decoding algorithms (Andor et al., 2016; Sountsov and Sarawagi, 2016; Murray and Chiang, 2018; Welleck et al., 2020b; Welleck and Cho, 2020; Martins et al., 2020; Deng et al., 2020; Basu et al., 2021; Shi et al., 2020). A particularly relevant work by Eikema and Aziz (2020) argues that the modes of neural sequence models are inadequate and thus we must discard maximum-a-posteriori inference altogether. Rather than advocating for a particular solution, we instead seek an understanding of why the conventional approach displays these peculiar behaviors. While we do not claim to provide a full explanation, the first step is developing a way of quantifying the prob"
2021.spnlp-1.5,2020.coling-main.398,0,0.014898,"pmodel which places high probability on empty (Stahlberg and Byrne, 2019) or repetitive (Holtzman et al., 2019) sequences, while the decoding stage can yield a distribution pF which places non-zero mass on infinite-length sequences (Welleck et al., 2020a). As a result, various workarounds have been proposed in the form of alternative learning or decoding algorithms (Andor et al., 2016; Sountsov and Sarawagi, 2016; Murray and Chiang, 2018; Welleck et al., 2020b; Welleck and Cho, 2020; Martins et al., 2020; Deng et al., 2020; Basu et al., 2021; Shi et al., 2020). A particularly relevant work by Eikema and Aziz (2020) argues that the modes of neural sequence models are inadequate and thus we must discard maximum-a-posteriori inference altogether. Rather than advocating for a particular solution, we instead seek an understanding of why the conventional approach displays these peculiar behaviors. While we do not claim to provide a full explanation, the first step is developing a way of quantifying the problem, then localizing it. To this end, we develop the mode recovery cost and measure it along the learning chain argtop-k selects all the elements within Ω whose probabilities p(s) are greater than the proba"
2021.spnlp-1.5,D19-1331,0,0.0892692,"the product of the conditional probability of each token given the previous tokens. Each conditional probability is modeled by a shared neural network, typically implemented as a recurrent neural network (Hochreiter and Schmidhuber, 1997) or a transformer (Vaswani et al., 2017). Despite its success, recent studies have identified peculiarities in neural autoregressive sequence models. Lee et al. (2018) identify hallucinations in neural machine translation, in which a well-trained model suddenly generates a nonsense translation when a rare token is artificially introduced to a source sentence. Stahlberg and Byrne (2019) observe that a vast portion of probability mass is concentrated on the empty sequence in neural machine translation, although the models they studied were never presented with empty sequences during training. Holtzman et al. (2019) report that large-scale language models often produce pathological sequences with many n-gram repetitions, at a rate which far exceeds that of the training data. Welleck et al. (2020a) show that neural language models can generate infinite-length sequences despite being trained on only finite sequences. A common theme underlying these findings is that well-trained"
2021.spnlp-1.5,D16-1139,0,0.0749369,"Missing"
2021.spnlp-1.5,2020.emnlp-main.448,1,0.925556,"inations in neural machine translation, in which a well-trained model suddenly generates a nonsense translation when a rare token is artificially introduced to a source sentence. Stahlberg and Byrne (2019) observe that a vast portion of probability mass is concentrated on the empty sequence in neural machine translation, although the models they studied were never presented with empty sequences during training. Holtzman et al. (2019) report that large-scale language models often produce pathological sequences with many n-gram repetitions, at a rate which far exceeds that of the training data. Welleck et al. (2020a) show that neural language models can generate infinite-length sequences despite being trained on only finite sequences. A common theme underlying these findings is that well-trained models can assign unreasonably high probabilities to sequences that are dissimilar to any sequence from the training set. In particular, the modes of the model’s distribution appear to be undesired, implying that the model failed to recover the modes of the empirical distribution, which we term mode recovery degradation. The situation is further complicated by the fact that we only approximate the model’s modes"
2021.spnlp-1.5,2020.emnlp-main.348,0,0.0367818,"Missing"
C16-1011,W12-3102,0,0.0122747,"set using crowdsourcing. CrowdFlower (https://make.crowdflower. com) was used as the crowdsourcing platform and they solicited one French and one German translation for each of the 5000 captions using native speakers. Note that (Rajendran et al., 2015) report results for cross modal search and do not address the problem of crosslingual image captioning. In our model, for D1 we use the same train(118K), validation (1K) and test sets (1K) as defined in (Rajendran et al., 2015) and explained above. Choosing D2 was a bit more tricky. Initially we considered the corpus released as part of WMT’12 (Callison-Burch et al., 2012) which contains roughly 44M English-French parallel sentences from various sources including News, parliamentary proceedings, etc. However, our initial small scale experiments showed that this does not work well because there is a clear mismatch between the vocabulary of this corpus and the vocabulary that we need for generating captions. Also the vocabulary is much larger (at least an order higher than what we need for image captioning) and it thus hampers training. Further, the average length and structure of these sentences is also very different from captions. Domain shift in MT is itself"
C16-1011,P05-1033,0,0.105885,"idea is not just limited to translation but could be applicable to any kind of conversion involving multiple source and target languages and/or modalities (for example, transliteration, multilingual image captioning, multilingual image Question Answering, etc.). Even though this idea has had limited success, it is still fascinating and considered by many as the holy grail of multilingual multimodal processing. It is interesting to consider the implications of this idea when viewed in the statistical context. For example, current state of the art statistical systems for MT (Koehn et al., 2003; Chiang, 2005; Luong et al., 2015b), transliteration (Finch et al., 2015; Shao et al., 2015; Nicolai et al., 2015), image captioning (Vinyals et al., 2015b; Xu et al., 2015), etc. require parallel data between the source and target views (where a view could be a language or some other modality like image). Thus, given n views, we require n C parallel datasets to build systems to convert from any source view to any target view. Obviously, this 2 does not scale well in practice because it is hard to find parallel data between all n C2 views. For example, publicly available parallel datasets for transliterati"
C16-1011,D14-1179,1,0.0874101,"Missing"
C16-1011,W15-3909,0,0.0154288,"applicable to any kind of conversion involving multiple source and target languages and/or modalities (for example, transliteration, multilingual image captioning, multilingual image Question Answering, etc.). Even though this idea has had limited success, it is still fascinating and considered by many as the holy grail of multilingual multimodal processing. It is interesting to consider the implications of this idea when viewed in the statistical context. For example, current state of the art statistical systems for MT (Koehn et al., 2003; Chiang, 2005; Luong et al., 2015b), transliteration (Finch et al., 2015; Shao et al., 2015; Nicolai et al., 2015), image captioning (Vinyals et al., 2015b; Xu et al., 2015), etc. require parallel data between the source and target views (where a view could be a language or some other modality like image). Thus, given n views, we require n C parallel datasets to build systems to convert from any source view to any target view. Obviously, this 2 does not scale well in practice because it is hard to find parallel data between all n C2 views. For example, publicly available parallel datasets for transliteration (Zhang et al., 2012) cater to < 20 languages. Similarly,"
C16-1011,N16-1101,1,0.92347,"ng two independent models using the datasets between XZ and ZY , the model jointly learns from the two datasets. The resulting common representation learned for X and Z can be viewed as a vectorial analogue of the linguistic representation sought by interlingua based approaches. Of course, by no means do we suggest that this vectorial representation is a substitute for the rich linguistic representation but its easier to learn from parallel data (as opposed to a linguistic representation which requires hand crafted resources). Note that our work should not be confused with the recent work of (Firat et al., 2016), (Zoph and Knight, 2016) and (Elliott et al., 2015). The last two works in fact require 3-way parallel data between X, Z and Y and learn to decode sequences in Y given both X and Z. For example, at test time, (Elliott et al., 2015) generate captions in German, given both (i) the image and (ii) its corresponding English caption. This is indeed very different from the problem addressed in this paper. Similarly, even though (Firat et al., 2016) learn a single encoder per language and a single decoder per language they do not learn shared representations for multiple languages (only the attention"
C16-1011,P14-1006,0,0.042996,"ly, (Wu and Wang, 2007; Zhu et al., 2014) do pivot based machine translation. Lastly, we would also like to mention the work in interlingua based Machine Translation (Nirenburg, 1994; Dorr et al., 2010) which is clearly the inspiration for this work even though the focus of this work is not on MT. The main theme explored in this paper is to learn a common representation for two views with the end goal of generating a target sequence in a third view. The idea of learning common representations for multiple views has been explored well in the past (Klementiev et al., 2012; Chandar et al., 2014; Hermann and Blunsom, 2014; Chandar et al., 2016; Rajendran et al., 2015). For example, Andrew et al. (2013) propose Deep CCA for learning a common representation for two views. (Chandar et al., 2014; Chandar et al., 2016) propose correlational neural networks for common representation learning and Rajendran et al. (2015) propose bridge correlational networks for multilingual multimodal representation learning. From the point of view of representation learning, the work of Rajendran et al. (2015) is very similar to our work except that it focuses only on representation learning and does not consider the end goal of gen"
C16-1011,N10-1065,1,0.82703,", Firat et al. (2016) focus on multi-task learning with a shared attention mechanism and the goal is to improve the MT performance for a pair of languages for which parallel data is available. This is clearly different from the goal of this paper which is to design encoder decoder models for a pair of languages where no parallel data is available but data is available only between each of these languages and a bridge language. Of course, in general the idea of pivot/bridge/interlingua based conversion is not new and has been used previously in several non-neural network settings. For example (Khapra et al., 2010) use a bridge language or pivot language to do machine transliteration. Similarly, (Wu and Wang, 2007; Zhu et al., 2014) do pivot based machine translation. Lastly, we would also like to mention the work in interlingua based Machine Translation (Nirenburg, 1994; Dorr et al., 2010) which is clearly the inspiration for this work even though the focus of this work is not on MT. The main theme explored in this paper is to learn a common representation for two views with the end goal of generating a target sequence in a third view. The idea of learning common representations for multiple views has"
C16-1011,C12-1089,0,0.0446559,"language to do machine transliteration. Similarly, (Wu and Wang, 2007; Zhu et al., 2014) do pivot based machine translation. Lastly, we would also like to mention the work in interlingua based Machine Translation (Nirenburg, 1994; Dorr et al., 2010) which is clearly the inspiration for this work even though the focus of this work is not on MT. The main theme explored in this paper is to learn a common representation for two views with the end goal of generating a target sequence in a third view. The idea of learning common representations for multiple views has been explored well in the past (Klementiev et al., 2012; Chandar et al., 2014; Hermann and Blunsom, 2014; Chandar et al., 2016; Rajendran et al., 2015). For example, Andrew et al. (2013) propose Deep CCA for learning a common representation for two views. (Chandar et al., 2014; Chandar et al., 2016) propose correlational neural networks for common representation learning and Rajendran et al. (2015) propose bridge correlational networks for multilingual multimodal representation learning. From the point of view of representation learning, the work of Rajendran et al. (2015) is very similar to our work except that it focuses only on representation l"
C16-1011,N03-1017,0,0.0137786,"e believe that this idea is not just limited to translation but could be applicable to any kind of conversion involving multiple source and target languages and/or modalities (for example, transliteration, multilingual image captioning, multilingual image Question Answering, etc.). Even though this idea has had limited success, it is still fascinating and considered by many as the holy grail of multilingual multimodal processing. It is interesting to consider the implications of this idea when viewed in the statistical context. For example, current state of the art statistical systems for MT (Koehn et al., 2003; Chiang, 2005; Luong et al., 2015b), transliteration (Finch et al., 2015; Shao et al., 2015; Nicolai et al., 2015), image captioning (Vinyals et al., 2015b; Xu et al., 2015), etc. require parallel data between the source and target views (where a view could be a language or some other modality like image). Thus, given n views, we require n C parallel datasets to build systems to convert from any source view to any target view. Obviously, this 2 does not scale well in practice because it is hard to find parallel data between all n C2 views. For example, publicly available parallel datasets for"
C16-1011,P07-2045,0,0.0074932,"Missing"
C16-1011,D15-1166,0,0.16404,"st limited to translation but could be applicable to any kind of conversion involving multiple source and target languages and/or modalities (for example, transliteration, multilingual image captioning, multilingual image Question Answering, etc.). Even though this idea has had limited success, it is still fascinating and considered by many as the holy grail of multilingual multimodal processing. It is interesting to consider the implications of this idea when viewed in the statistical context. For example, current state of the art statistical systems for MT (Koehn et al., 2003; Chiang, 2005; Luong et al., 2015b), transliteration (Finch et al., 2015; Shao et al., 2015; Nicolai et al., 2015), image captioning (Vinyals et al., 2015b; Xu et al., 2015), etc. require parallel data between the source and target views (where a view could be a language or some other modality like image). Thus, given n views, we require n C parallel datasets to build systems to convert from any source view to any target view. Obviously, this 2 does not scale well in practice because it is hard to find parallel data between all n C2 views. For example, publicly available parallel datasets for transliteration (Zhang et al., 20"
C16-1011,W15-3911,0,0.0204691,"volving multiple source and target languages and/or modalities (for example, transliteration, multilingual image captioning, multilingual image Question Answering, etc.). Even though this idea has had limited success, it is still fascinating and considered by many as the holy grail of multilingual multimodal processing. It is interesting to consider the implications of this idea when viewed in the statistical context. For example, current state of the art statistical systems for MT (Koehn et al., 2003; Chiang, 2005; Luong et al., 2015b), transliteration (Finch et al., 2015; Shao et al., 2015; Nicolai et al., 2015), image captioning (Vinyals et al., 2015b; Xu et al., 2015), etc. require parallel data between the source and target views (where a view could be a language or some other modality like image). Thus, given n views, we require n C parallel datasets to build systems to convert from any source view to any target view. Obviously, this 2 does not scale well in practice because it is hard to find parallel data between all n C2 views. For example, publicly available parallel datasets for transliteration (Zhang et al., 2012) cater to < 20 languages. Similarly, publicly available image caption datasets"
C16-1011,W15-3908,0,0.012526,"nd of conversion involving multiple source and target languages and/or modalities (for example, transliteration, multilingual image captioning, multilingual image Question Answering, etc.). Even though this idea has had limited success, it is still fascinating and considered by many as the holy grail of multilingual multimodal processing. It is interesting to consider the implications of this idea when viewed in the statistical context. For example, current state of the art statistical systems for MT (Koehn et al., 2003; Chiang, 2005; Luong et al., 2015b), transliteration (Finch et al., 2015; Shao et al., 2015; Nicolai et al., 2015), image captioning (Vinyals et al., 2015b; Xu et al., 2015), etc. require parallel data between the source and target views (where a view could be a language or some other modality like image). Thus, given n views, we require n C parallel datasets to build systems to convert from any source view to any target view. Obviously, this 2 does not scale well in practice because it is hard to find parallel data between all n C2 views. For example, publicly available parallel datasets for transliteration (Zhang et al., 2012) cater to < 20 languages. Similarly, publicly available"
C16-1011,P07-1108,0,0.0328358,"improve the MT performance for a pair of languages for which parallel data is available. This is clearly different from the goal of this paper which is to design encoder decoder models for a pair of languages where no parallel data is available but data is available only between each of these languages and a bridge language. Of course, in general the idea of pivot/bridge/interlingua based conversion is not new and has been used previously in several non-neural network settings. For example (Khapra et al., 2010) use a bridge language or pivot language to do machine transliteration. Similarly, (Wu and Wang, 2007; Zhu et al., 2014) do pivot based machine translation. Lastly, we would also like to mention the work in interlingua based Machine Translation (Nirenburg, 1994; Dorr et al., 2010) which is clearly the inspiration for this work even though the focus of this work is not on MT. The main theme explored in this paper is to learn a common representation for two views with the end goal of generating a target sequence in a third view. The idea of learning common representations for multiple views has been explored well in the past (Klementiev et al., 2012; Chandar et al., 2014; Hermann and Blunsom, 2"
C16-1011,D14-1174,0,0.0188242,"ormance for a pair of languages for which parallel data is available. This is clearly different from the goal of this paper which is to design encoder decoder models for a pair of languages where no parallel data is available but data is available only between each of these languages and a bridge language. Of course, in general the idea of pivot/bridge/interlingua based conversion is not new and has been used previously in several non-neural network settings. For example (Khapra et al., 2010) use a bridge language or pivot language to do machine transliteration. Similarly, (Wu and Wang, 2007; Zhu et al., 2014) do pivot based machine translation. Lastly, we would also like to mention the work in interlingua based Machine Translation (Nirenburg, 1994; Dorr et al., 2010) which is clearly the inspiration for this work even though the focus of this work is not on MT. The main theme explored in this paper is to learn a common representation for two views with the end goal of generating a target sequence in a third view. The idea of learning common representations for multiple views has been explored well in the past (Klementiev et al., 2012; Chandar et al., 2014; Hermann and Blunsom, 2014; Chandar et al."
C16-1011,N16-1004,0,0.0694392,"els using the datasets between XZ and ZY , the model jointly learns from the two datasets. The resulting common representation learned for X and Z can be viewed as a vectorial analogue of the linguistic representation sought by interlingua based approaches. Of course, by no means do we suggest that this vectorial representation is a substitute for the rich linguistic representation but its easier to learn from parallel data (as opposed to a linguistic representation which requires hand crafted resources). Note that our work should not be confused with the recent work of (Firat et al., 2016), (Zoph and Knight, 2016) and (Elliott et al., 2015). The last two works in fact require 3-way parallel data between X, Z and Y and learn to decode sequences in Y given both X and Z. For example, at test time, (Elliott et al., 2015) generate captions in German, given both (i) the image and (ii) its corresponding English caption. This is indeed very different from the problem addressed in this paper. Similarly, even though (Firat et al., 2016) learn a single encoder per language and a single decoder per language they do not learn shared representations for multiple languages (only the attention mechanism is shared). Fu"
C16-1011,H94-1093,0,\N,Missing
C16-1011,W12-4402,0,\N,Missing
D14-1179,D13-1106,0,0.10598,"Missing"
D14-1179,D11-1033,0,0.125416,"Missing"
D14-1179,P14-1129,0,0.106494,"Missing"
D14-1179,2006.iwslt-papers.2,1,0.141042,"Missing"
D14-1179,D13-1176,0,0.236308,"Missing"
D14-1179,C12-2104,1,0.137075,"Missing"
D14-1179,N03-1017,0,0.145005,"Missing"
D14-1179,2005.mtsummit-papers.11,0,0.132804,"Missing"
D14-1179,W02-1018,0,0.124674,"Missing"
D14-1179,P10-2041,0,0.126969,"Missing"
D14-1179,N12-1005,0,0.143701,"Missing"
D14-1179,D13-1140,0,0.122241,"Missing"
D14-1179,D13-1141,0,0.0877611,"Missing"
D16-1026,W16-2358,0,0.00810698,"Missing"
D16-1026,P16-1160,1,0.423331,"the same language pair and (2) better than pivotbased translation strategy, while keeping only one additional copy of attention-related parameters. Introduction A recently introduced neural machine transla˜ tion (Forcada and Neco, 1997; Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014) has proven to be a platform for new opportunities in machine translation research. Rather than word-level translation with language-specific preprocessing, neural machine translation has found to work well with statistically segmented subword sequences as well as sequences of characters (Chung et al., 2016; Luong and Manning, 2016; Sennrich et al., 2015b; Ling et al., 2015). Also, recent works show that neural machine translation provides a seamless way to incorporate multiple modalities ? Kyunghyun Cho New York University other than natural language text in translation (Luong et al., 2015a; Caglayan et al., 2016). Furthermore, neural machine translation has been found to translate between multiple languages, achieving better translation quality by exploiting positive language transfer (Dong et al., 2015; Firat et al., 2016; Zoph and Knight, 2016). Abstract 1 Baskaran Sankaran IBM T.J. Watson R"
D16-1026,P15-1166,0,0.153138,"rk well with statistically segmented subword sequences as well as sequences of characters (Chung et al., 2016; Luong and Manning, 2016; Sennrich et al., 2015b; Ling et al., 2015). Also, recent works show that neural machine translation provides a seamless way to incorporate multiple modalities ? Kyunghyun Cho New York University other than natural language text in translation (Luong et al., 2015a; Caglayan et al., 2016). Furthermore, neural machine translation has been found to translate between multiple languages, achieving better translation quality by exploiting positive language transfer (Dong et al., 2015; Firat et al., 2016; Zoph and Knight, 2016). Abstract 1 Baskaran Sankaran IBM T.J. Watson Research Center Work carried out while the author was at IBM Research. In this paper, we conduct in-depth investigation into the recently proposed multi-way, multilingual neural machine translation (Firat et al., 2016). Specifically, we are interested in its potential for zero-resource machine translation, in which there does not exist any direct parallel examples between a target language pair. Zero-resource translation has been addressed by pivot-based translation in traditional machine translation res"
D16-1026,N16-1101,1,0.252294,"tically segmented subword sequences as well as sequences of characters (Chung et al., 2016; Luong and Manning, 2016; Sennrich et al., 2015b; Ling et al., 2015). Also, recent works show that neural machine translation provides a seamless way to incorporate multiple modalities ? Kyunghyun Cho New York University other than natural language text in translation (Luong et al., 2015a; Caglayan et al., 2016). Furthermore, neural machine translation has been found to translate between multiple languages, achieving better translation quality by exploiting positive language transfer (Dong et al., 2015; Firat et al., 2016; Zoph and Knight, 2016). Abstract 1 Baskaran Sankaran IBM T.J. Watson Research Center Work carried out while the author was at IBM Research. In this paper, we conduct in-depth investigation into the recently proposed multi-way, multilingual neural machine translation (Firat et al., 2016). Specifically, we are interested in its potential for zero-resource machine translation, in which there does not exist any direct parallel examples between a target language pair. Zero-resource translation has been addressed by pivot-based translation in traditional machine translation research (Wu and Wang,"
D16-1026,W09-0431,0,0.235261,"t 1 Baskaran Sankaran IBM T.J. Watson Research Center Work carried out while the author was at IBM Research. In this paper, we conduct in-depth investigation into the recently proposed multi-way, multilingual neural machine translation (Firat et al., 2016). Specifically, we are interested in its potential for zero-resource machine translation, in which there does not exist any direct parallel examples between a target language pair. Zero-resource translation has been addressed by pivot-based translation in traditional machine translation research (Wu and Wang, 2007; Utiyama and Isahara, 2007; Habash and Hu, 2009), but we explore a way to use the multi-way, multilingual neural model to translate directly from a source to target language. In doing so, we begin by studying different translation strategies available in the multi-way, multilingual model in Sec. 3–4. The strategies include a usual one-to-one translation as well as variants of many-to-one translation for multi-source translation (Zoph and Knight, 2016). We empirically show that the many-to-one strategies significantly outperform the one-to-one strategy. We move on to zero-resource translation by first evaluating a vanilla multi-way, multilin"
D16-1026,W15-3014,1,0.090981,"urce output distribution: p(yt = w|y<t , X1 , X2 ) = (7) 1 (p(yt = w|y<t , X1 ) + p(yt = w|y<t )). 2 An advantage of this late averaging strategy over the early averaging one is that this can work even when those two translation paths were not from a single multilingual model. They can be two separately trained single-pair models. In fact, if X1 and X2 are same and the two translation paths are simply two different models trained on the same language pair– direction, this is equivalent to constructing an ensemble, which was found to greatly improve translation quality (Sutskever et al., 2014; Jean et al., 2015) Early+Late Average The two strategies above can be further combined by late-averaging the output distributions from the early averaged model and the late averaged one. We empirically evaluate this early+late average strategy as well. 4 Experiments: Translation Strategies and Multi-Source Translation Before continuing on with zero-resource machine translation, we first evaluate the translation strategies described in the previous section on multisource translation, as these translation strategies form a basic foundation on which we extend the multi-way, multilingual model for zero-resource mac"
D16-1026,D13-1176,0,0.0445487,"translate that enables zero-resource machine translation. When used together with novel manyto-one translation strategies, we empirically show that this finetuning algorithm allows the multi-way, multilingual model to translate a zero-resource language pair (1) as well as a single-pair neural translation model trained with up to 1M direct parallel sentences of the same language pair and (2) better than pivotbased translation strategy, while keeping only one additional copy of attention-related parameters. Introduction A recently introduced neural machine transla˜ tion (Forcada and Neco, 1997; Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014) has proven to be a platform for new opportunities in machine translation research. Rather than word-level translation with language-specific preprocessing, neural machine translation has found to work well with statistically segmented subword sequences as well as sequences of characters (Chung et al., 2016; Luong and Manning, 2016; Sennrich et al., 2015b; Ling et al., 2015). Also, recent works show that neural machine translation provides a seamless way to incorporate multiple modalities ? Kyunghyun Cho New York University other than natural language"
D16-1026,P07-2045,0,0.0073134,"Missing"
D16-1026,P16-1100,0,0.00557872,"ir and (2) better than pivotbased translation strategy, while keeping only one additional copy of attention-related parameters. Introduction A recently introduced neural machine transla˜ tion (Forcada and Neco, 1997; Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014) has proven to be a platform for new opportunities in machine translation research. Rather than word-level translation with language-specific preprocessing, neural machine translation has found to work well with statistically segmented subword sequences as well as sequences of characters (Chung et al., 2016; Luong and Manning, 2016; Sennrich et al., 2015b; Ling et al., 2015). Also, recent works show that neural machine translation provides a seamless way to incorporate multiple modalities ? Kyunghyun Cho New York University other than natural language text in translation (Luong et al., 2015a; Caglayan et al., 2016). Furthermore, neural machine translation has been found to translate between multiple languages, achieving better translation quality by exploiting positive language transfer (Dong et al., 2015; Firat et al., 2016; Zoph and Knight, 2016). Abstract 1 Baskaran Sankaran IBM T.J. Watson Research Center Work carri"
D16-1026,D15-1166,0,0.465968,"14; Cho et al., 2014) has proven to be a platform for new opportunities in machine translation research. Rather than word-level translation with language-specific preprocessing, neural machine translation has found to work well with statistically segmented subword sequences as well as sequences of characters (Chung et al., 2016; Luong and Manning, 2016; Sennrich et al., 2015b; Ling et al., 2015). Also, recent works show that neural machine translation provides a seamless way to incorporate multiple modalities ? Kyunghyun Cho New York University other than natural language text in translation (Luong et al., 2015a; Caglayan et al., 2016). Furthermore, neural machine translation has been found to translate between multiple languages, achieving better translation quality by exploiting positive language transfer (Dong et al., 2015; Firat et al., 2016; Zoph and Knight, 2016). Abstract 1 Baskaran Sankaran IBM T.J. Watson Research Center Work carried out while the author was at IBM Research. In this paper, we conduct in-depth investigation into the recently proposed multi-way, multilingual neural machine translation (Firat et al., 2016). Specifically, we are interested in its potential for zero-resource mac"
D16-1026,N07-1061,0,0.843698,"and Knight, 2016). Abstract 1 Baskaran Sankaran IBM T.J. Watson Research Center Work carried out while the author was at IBM Research. In this paper, we conduct in-depth investigation into the recently proposed multi-way, multilingual neural machine translation (Firat et al., 2016). Specifically, we are interested in its potential for zero-resource machine translation, in which there does not exist any direct parallel examples between a target language pair. Zero-resource translation has been addressed by pivot-based translation in traditional machine translation research (Wu and Wang, 2007; Utiyama and Isahara, 2007; Habash and Hu, 2009), but we explore a way to use the multi-way, multilingual neural model to translate directly from a source to target language. In doing so, we begin by studying different translation strategies available in the multi-way, multilingual model in Sec. 3–4. The strategies include a usual one-to-one translation as well as variants of many-to-one translation for multi-source translation (Zoph and Knight, 2016). We empirically show that the many-to-one strategies significantly outperform the one-to-one strategy. We move on to zero-resource translation by first evaluating a vanil"
D16-1026,P07-1108,0,0.498016,"et al., 2016; Zoph and Knight, 2016). Abstract 1 Baskaran Sankaran IBM T.J. Watson Research Center Work carried out while the author was at IBM Research. In this paper, we conduct in-depth investigation into the recently proposed multi-way, multilingual neural machine translation (Firat et al., 2016). Specifically, we are interested in its potential for zero-resource machine translation, in which there does not exist any direct parallel examples between a target language pair. Zero-resource translation has been addressed by pivot-based translation in traditional machine translation research (Wu and Wang, 2007; Utiyama and Isahara, 2007; Habash and Hu, 2009), but we explore a way to use the multi-way, multilingual neural model to translate directly from a source to target language. In doing so, we begin by studying different translation strategies available in the multi-way, multilingual model in Sec. 3–4. The strategies include a usual one-to-one translation as well as variants of many-to-one translation for multi-source translation (Zoph and Knight, 2016). We empirically show that the many-to-one strategies significantly outperform the one-to-one strategy. We move on to zero-resource translation"
D16-1026,N09-2006,0,0.0782224,"Missing"
D16-1026,N16-1004,0,0.487144,"bword sequences as well as sequences of characters (Chung et al., 2016; Luong and Manning, 2016; Sennrich et al., 2015b; Ling et al., 2015). Also, recent works show that neural machine translation provides a seamless way to incorporate multiple modalities ? Kyunghyun Cho New York University other than natural language text in translation (Luong et al., 2015a; Caglayan et al., 2016). Furthermore, neural machine translation has been found to translate between multiple languages, achieving better translation quality by exploiting positive language transfer (Dong et al., 2015; Firat et al., 2016; Zoph and Knight, 2016). Abstract 1 Baskaran Sankaran IBM T.J. Watson Research Center Work carried out while the author was at IBM Research. In this paper, we conduct in-depth investigation into the recently proposed multi-way, multilingual neural machine translation (Firat et al., 2016). Specifically, we are interested in its potential for zero-resource machine translation, in which there does not exist any direct parallel examples between a target language pair. Zero-resource translation has been addressed by pivot-based translation in traditional machine translation research (Wu and Wang, 2007; Utiyama and Isahar"
D16-1026,P16-1162,0,\N,Missing
D16-1209,P16-1020,0,0.103775,"Missing"
D16-1209,N16-1030,0,0.102001,"Missing"
D16-1209,D15-1161,0,0.0271606,"Missing"
D16-1209,P16-1100,0,0.0720266,"Missing"
D16-1209,P11-1015,0,0.144357,"Missing"
D16-1209,J93-2004,0,0.0691883,"Missing"
D16-1209,N16-1145,0,0.0156099,"Missing"
D16-1209,D15-1176,0,\N,Missing
D17-1061,D14-1181,0,0.003699,"Missing"
D17-1061,D16-1261,0,0.0274596,"ed on supervised learning is to learn a common latent representation of queries and relevant documents terms by using a click-through dataset (Sordoni et al., 2014). Neighboring document terms of a query in the latent space are selected to form an expanded query. Instead of using a click-through dataset, which is often proprietary, it is possible to use an alternative dataset consisting of anchor text/title pairs. In contrast, our approach does not require a dataset of paired queries as it learns term selection strategies via reinforcement learning. Perhaps the closest work to ours is that by Narasimhan et al. (2016), in which a reinforcement learning based approach is used to reformu4 Experiments In this section we describe our experimental setup, including baselines against which we compare the proposed method, metrics, reward for RL-based models, datasets and implementation details. 4.1 Baseline Methods Raw: The original query is given to a search engine without any modification. We evaluate two search engines in their default configuration: Lucene3 (Raw-Lucene) and Google Search4 (Raw-Google). Pseudo Relevance Feedback (PRF-TFIDF): A query is expanded with terms from the documents retrieved by a searc"
D17-1210,W16-2309,1,0.738135,"to train it using a variant of deterministic policy gradient. We extensively evaluate the proposed algorithm using four language pairs and two decoding objectives, and show that we can indeed train a trainable greedy decoder that generates a better translation (in terms of a target decoding objective) with minimal computational overhead. 1 Introduction Neural machine translation has recently become a method of choice in machine translation research. Besides its success in traditional settings of machine translation, that is one-to-one translation between two languages, (Sennrich et al., 2016; Chung et al., 2016), neural machine translation has ventured into more sophisticated settings of machine translation. For instance, neural machine translation has successfully proven itself to be capable of handling subword-level representation of sentences (Lee et al., 2016; Luong and Manning, 2016; Sennrich et al., 2015; Costa-Jussa and Fonollosa, 2016; Ling et al., 2015). Furthermore, several research groups have shown its potential in seamlessly handling multiple languages (Dong et al., 2015; Luong et al., 2015a; Firat et al., 2016a,b; Lee et al., 2016; Ha et al., 2016; Viégas et al., 2016). A typical scenar"
D17-1210,P16-2058,0,0.0282862,"Missing"
D17-1210,P15-1166,0,0.0177346,"ional settings of machine translation, that is one-to-one translation between two languages, (Sennrich et al., 2016; Chung et al., 2016), neural machine translation has ventured into more sophisticated settings of machine translation. For instance, neural machine translation has successfully proven itself to be capable of handling subword-level representation of sentences (Lee et al., 2016; Luong and Manning, 2016; Sennrich et al., 2015; Costa-Jussa and Fonollosa, 2016; Ling et al., 2015). Furthermore, several research groups have shown its potential in seamlessly handling multiple languages (Dong et al., 2015; Luong et al., 2015a; Firat et al., 2016a,b; Lee et al., 2016; Ha et al., 2016; Viégas et al., 2016). A typical scenario of neural machine translation starts with training a model to maximize its log-likelihood. That is, we often train a model to maximize the conditional probability of a reference translation given a source sentence over a large parallel corpus. Once the model is trained in this way, it defines the conditional distribution over all possible translations given a source sentence, and the task of translation becomes equivalent to finding a translation to which the model assigns"
D17-1210,N16-1101,1,0.733763,"hat is one-to-one translation between two languages, (Sennrich et al., 2016; Chung et al., 2016), neural machine translation has ventured into more sophisticated settings of machine translation. For instance, neural machine translation has successfully proven itself to be capable of handling subword-level representation of sentences (Lee et al., 2016; Luong and Manning, 2016; Sennrich et al., 2015; Costa-Jussa and Fonollosa, 2016; Ling et al., 2015). Furthermore, several research groups have shown its potential in seamlessly handling multiple languages (Dong et al., 2015; Luong et al., 2015a; Firat et al., 2016a,b; Lee et al., 2016; Ha et al., 2016; Viégas et al., 2016). A typical scenario of neural machine translation starts with training a model to maximize its log-likelihood. That is, we often train a model to maximize the conditional probability of a reference translation given a source sentence over a large parallel corpus. Once the model is trained in this way, it defines the conditional distribution over all possible translations given a source sentence, and the task of translation becomes equivalent to finding a translation to which the model assigns the highest conditional probability. Sinc"
D17-1210,W04-3250,0,0.0132212,"jective (b) Beam Search + Trainable Greedy Decoding Figure 2: The plots draw the improvements by the trainable greedy decoding on the test set. The x-axes correspond to the objectives used to train trainable greedy decoding, and the y-axes to the changes in the achieved objectives (BLEU for the figures on the left, and negative perplexity on the right.) The top row (a) shows the cases when the trainable greedy decoder is used on its own, and the bottom row (b) when it is used together with beam search. When training and evaluation are both done with BLEU, we test the statistical significance (Koehn, 2004), and we mark significant cases with red stars (p &lt; 0.05.) The underlying neural machine translation models achieved the BLEU scores of 14.49/16.20 for En-Cs, 18.90/21.20 for Cs-En, 18.97/21.33 for En-De, 21.63/24.46 for De-En, 16.97/19.68 for En-Ru, 21.06/23.34 for Ru-En, 7.53/8.82 for En-Fi and 9.79/11.03 for Fi-En (greedy/beam). ceive again a reasonable return. time step, similar to (Heess et al., 2015): Although this property of dense reward makes the problem of trainable greedy decoding more manageable, we have observed other issues during our preliminary experiment with the vanilla deter"
D17-1210,P16-1160,1,0.853522,"n (in terms of a target decoding objective) with minimal computational overhead. 1 Introduction Neural machine translation has recently become a method of choice in machine translation research. Besides its success in traditional settings of machine translation, that is one-to-one translation between two languages, (Sennrich et al., 2016; Chung et al., 2016), neural machine translation has ventured into more sophisticated settings of machine translation. For instance, neural machine translation has successfully proven itself to be capable of handling subword-level representation of sentences (Lee et al., 2016; Luong and Manning, 2016; Sennrich et al., 2015; Costa-Jussa and Fonollosa, 2016; Ling et al., 2015). Furthermore, several research groups have shown its potential in seamlessly handling multiple languages (Dong et al., 2015; Luong et al., 2015a; Firat et al., 2016a,b; Lee et al., 2016; Ha et al., 2016; Viégas et al., 2016). A typical scenario of neural machine translation starts with training a model to maximize its log-likelihood. That is, we often train a model to maximize the conditional probability of a reference translation given a source sentence over a large parallel corpus. Once the"
D17-1210,P04-1077,0,0.0340817,"(Tieleman and Hinton, 2012) with the initial learning rates of 2 × 10−6 and 2 × 10−4 , respectively, for the actor and critic. We monitor the progress of learning by measuring the decoding objective on the validation set. After training, we pick the actor that results in the best decoding objective on the validation set, and test it on the test set. Decoding Objectives For each neural machine translation model, pretrained using maximum likelihood criterion, we train two trainable greedy decoding actors. One actor is trained to maximize BLEU (or its smoothed version for sentence-level scoring (Lin and Och, 2004)) as its decoding objective, and the other to minimize perplexity (or equivalently the negative log-probability normalized by the length.) We have chosen the first two decoding objectives for two purposes. First, we demonstrate that it is possible to build multiple trainable decoders with a single underlying model trained using maximum likelihood learning. Second, the comparison between these two objectives provides a glimpse into the relationship between BLEU (the most widely used automatic metric for evaluating translation systems) and log-likelihood (the most widely used learning criterion"
D17-1210,D16-1026,1,0.74228,"Missing"
D17-1210,P16-1100,0,0.0363273,"Missing"
D17-1210,D15-1166,0,0.0626159,"achine translation, that is one-to-one translation between two languages, (Sennrich et al., 2016; Chung et al., 2016), neural machine translation has ventured into more sophisticated settings of machine translation. For instance, neural machine translation has successfully proven itself to be capable of handling subword-level representation of sentences (Lee et al., 2016; Luong and Manning, 2016; Sennrich et al., 2015; Costa-Jussa and Fonollosa, 2016; Ling et al., 2015). Furthermore, several research groups have shown its potential in seamlessly handling multiple languages (Dong et al., 2015; Luong et al., 2015a; Firat et al., 2016a,b; Lee et al., 2016; Ha et al., 2016; Viégas et al., 2016). A typical scenario of neural machine translation starts with training a model to maximize its log-likelihood. That is, we often train a model to maximize the conditional probability of a reference translation given a source sentence over a large parallel corpus. Once the model is trained in this way, it defines the conditional distribution over all possible translations given a source sentence, and the task of translation becomes equivalent to finding a translation to which the model assigns the highest conditio"
D17-1210,D13-1176,0,0.0555751,"practice to resort to approximate search/decoding algorithms such as greedy decoding or beam search. In this scenario, we have identified two points where improvements could be made. They are (1) training (including the selection of a model architecture) and (2) decoding. Much of the research on neural machine translation has focused solely on the former, that is, on improving the model architecture. Neural machine translation started with with a simple encoderdecoder architecture in which a source sentence is encoded into a single, fixed-size vector (Cho et al., 2014; Sutskever et al., 2014; Kalchbrenner and Blunsom, 2013). It soon evolved with the attention mechanism (Bahdanau et al., 2014). A few variants of the attention mechanism, or its regularization, have been proposed recently to improve both the translation quality as well as the computational efficiency (Luong et al., 2015b; Cohn et al., 2016; Tu et al., 2016b). More recently, convolutional net1968 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1968–1978 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics works have been adopted either as a replacement of or a compl"
D17-1210,W16-2323,0,0.0152969,"on decoder and propose to train it using a variant of deterministic policy gradient. We extensively evaluate the proposed algorithm using four language pairs and two decoding objectives, and show that we can indeed train a trainable greedy decoder that generates a better translation (in terms of a target decoding objective) with minimal computational overhead. 1 Introduction Neural machine translation has recently become a method of choice in machine translation research. Besides its success in traditional settings of machine translation, that is one-to-one translation between two languages, (Sennrich et al., 2016; Chung et al., 2016), neural machine translation has ventured into more sophisticated settings of machine translation. For instance, neural machine translation has successfully proven itself to be capable of handling subword-level representation of sentences (Lee et al., 2016; Luong and Manning, 2016; Sennrich et al., 2015; Costa-Jussa and Fonollosa, 2016; Ling et al., 2015). Furthermore, several research groups have shown its potential in seamlessly handling multiple languages (Dong et al., 2015; Luong et al., 2015a; Firat et al., 2016a,b; Lee et al., 2016; Ha et al., 2016; Viégas et al., 20"
D17-1210,P16-1008,0,0.127485,"ation has focused solely on the former, that is, on improving the model architecture. Neural machine translation started with with a simple encoderdecoder architecture in which a source sentence is encoded into a single, fixed-size vector (Cho et al., 2014; Sutskever et al., 2014; Kalchbrenner and Blunsom, 2013). It soon evolved with the attention mechanism (Bahdanau et al., 2014). A few variants of the attention mechanism, or its regularization, have been proposed recently to improve both the translation quality as well as the computational efficiency (Luong et al., 2015b; Cohn et al., 2016; Tu et al., 2016b). More recently, convolutional net1968 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1968–1978 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics works have been adopted either as a replacement of or a complement to a recurrent network in order to efficiently utilize parallel computing (Kalchbrenner et al., 2016; Lee et al., 2016; Gehring et al., 2016). On the aspect of decoding, only a few research groups have tackled this problem by incorporating a target decoding algorithm into training. Wiseman and R"
D17-1210,D16-1137,0,0.0726698,"Missing"
D18-1023,P17-1042,0,0.0533221,"versity, CIFAR Global Scholar kyunghyun.cho@nyu.edu 3 Didi Labs and University of Southern California knight@isi.edu Abstract (MT) is only available for dozens of dominant languages. In this paper we aim to construct a multilingual common semantic space where words in multiple languages are mapped into a distributed, language-agnostic semantic continuous space, so that resources and knowledge can be shared across languages. Previous multilingual embedding methods align the semantic distributions of words from multiple languages within the common semantic space. Though several recent attempts (Artetxe et al., 2017, 2018; Conneau et al., 2017) have shown that it is possible to extract multilingual word embedding from a pair of potentially unaligned corpora in multiple languages, we claim that it is necessary to impose more constraints to preserve linguistic properties and facilitate downstream NLP tasks, such as cross-lingual IE, and MT. We find that words also can be clustered through explicit (e.g., sharing affixes of certain linguistic functions) or implicit clues (e.g., sharing neighbors from monolingual word embedding) and such clusters should also be consistent across multiple languages. To do so,"
D18-1023,P18-1073,0,0.02517,"Missing"
D18-1023,P15-1027,0,0.0335093,"ow-resource languages. Experiments demonstrate that our framework is effective at capturing linguistic properties and significantly outperforms state-of-the-art multi-lingual embedding learning methods. pendency parsing (Guo et al., 2015; Ammar et al., 2016a), and name tagging (Zhang et al., 2017a; Tsai and Roth, 2016; Zhang et al., 2018; Cheung et al.; Zhang et al., 2017b; Feng et al., 2017). Using bilingual aligned words, previous methods project multiple monolingual embeddings into a shared semantic space using linear mappings (Mikolov et al., 2013b; Rothe et al., 2016; Zhang et al., 2016; Baroni et al., 2015; Xing et al., 2015; Smith et al., 2017) or canonical correlation analysis (CCA) (Ammar et al., 2016b; Faruqui and Dyer, 2014; Lu et al., 2015). Compared with CCA, which only optimizes the correlation for each individual pair of languages, linear mapping based methods can jointly optimize all the languages in the common semantic space. We focus on learning linear mappings to construct the common semantic space and adopt correlational neural networks (CorrNet) (Chandar et al., 2016; Rajendran et al., 2015) as the basic model. In contrast to previous work which only exploited monolingual word se"
D18-1023,N16-1030,0,0.0149937,"n (PER), Location (LOC), Organization (ORG), and Geo-Political Entities (GPE). We experiment with two sets of languages. The first set Amh+Tig consists of Amharic and Tigrinya. Both languages share the same Ge’ez script and descend from the proto-Semitic language family. The other set Eng+Uig+Tur consists of one high-resource language (English), one mediumresource language (Turkish) and one low-resource language (Uighur). It also consists of two distinct language scripts: English and Turkish use Latin script while Uighur uses Arabic script. We use an LSTM-CRF architecture (Huang et al., 2015; Lample et al., 2016; Ma and Hovy, 2016) for name tagging. It takes only word embedding as input and predict a tag for each word. Table 5 shows the statistics of training, development, and test sets for each language released by Linguistic Data Consortium (LDC).15 For each language pair, we combine the bilingual aligned words extracted from Wiktionary and monolingual dictionaries based on identical strings.16 We evaluate the quality from several aspects: Impact of Bilingual Dictionary Size In order to show the impact of the size of bilingual lexicons, we use three languages as a case study, and gradually reduce t"
D18-1023,C16-1171,0,0.0188394,"ers. Another branch of approaches for multilingual word embeddings are based on parallel or comparable data, such as parallel sentences (AP Chandar et al., 2014; Gouws et al., 2015; Luong et al., 2015; Hermann and Blunsom, 2014; Schwenk et al., 2017), phrase translations (Duong et al., 2016) and comparable documents (Vulic and Moens, 2015). Moreover, to reduce the need of bilingual alignment, several approaches have been designed to learn cross-lingual embeddings based on a small seed dictionary (Vulic and Korhonen, 2016; Zhang et al., 2016; Artetxe et al., 2017), or even with no supervision (Cao et al., 2016; Zhang et al., 2017d,c; Conneau et al., 2017; Artetxe et al., 2018). However, such methods are still limited to bilingual word embedding learning and remaining to be explored for common semantic space construction. 3 2 Related Work 3.1 Multilingual word embeddings have advanced many multilingual NLP tasks, such as machine translation (Zou et al., 2013; Mikolov et al., 2013b; Madhyastha and Espa˜na-Bonet, 2017), de2 Approach Overview Figure 1 shows the overview of our neural architecture. We project all monolingual word embeddings into a common semantic space based on word-level as well as clu"
D18-1023,N15-1028,0,0.0226675,"ate-of-the-art multi-lingual embedding learning methods. pendency parsing (Guo et al., 2015; Ammar et al., 2016a), and name tagging (Zhang et al., 2017a; Tsai and Roth, 2016; Zhang et al., 2018; Cheung et al.; Zhang et al., 2017b; Feng et al., 2017). Using bilingual aligned words, previous methods project multiple monolingual embeddings into a shared semantic space using linear mappings (Mikolov et al., 2013b; Rothe et al., 2016; Zhang et al., 2016; Baroni et al., 2015; Xing et al., 2015; Smith et al., 2017) or canonical correlation analysis (CCA) (Ammar et al., 2016b; Faruqui and Dyer, 2014; Lu et al., 2015). Compared with CCA, which only optimizes the correlation for each individual pair of languages, linear mapping based methods can jointly optimize all the languages in the common semantic space. We focus on learning linear mappings to construct the common semantic space and adopt correlational neural networks (CorrNet) (Chandar et al., 2016; Rajendran et al., 2015) as the basic model. In contrast to previous work which only exploited monolingual word semantics, we introduce multiple cluster-level alignments and design a new cluster consistent CorrNet to align both words and clusters. Another b"
D18-1023,W15-1521,0,0.173801,"the common semantic space. We focus on learning linear mappings to construct the common semantic space and adopt correlational neural networks (CorrNet) (Chandar et al., 2016; Rajendran et al., 2015) as the basic model. In contrast to previous work which only exploited monolingual word semantics, we introduce multiple cluster-level alignments and design a new cluster consistent CorrNet to align both words and clusters. Another branch of approaches for multilingual word embeddings are based on parallel or comparable data, such as parallel sentences (AP Chandar et al., 2014; Gouws et al., 2015; Luong et al., 2015; Hermann and Blunsom, 2014; Schwenk et al., 2017), phrase translations (Duong et al., 2016) and comparable documents (Vulic and Moens, 2015). Moreover, to reduce the need of bilingual alignment, several approaches have been designed to learn cross-lingual embeddings based on a small seed dictionary (Vulic and Korhonen, 2016; Zhang et al., 2016; Artetxe et al., 2017), or even with no supervision (Cao et al., 2016; Zhang et al., 2017d,c; Conneau et al., 2017; Artetxe et al., 2018). However, such methods are still limited to bilingual word embedding learning and remaining to be explored for comm"
D18-1023,P16-1101,0,0.0607558,"C), Organization (ORG), and Geo-Political Entities (GPE). We experiment with two sets of languages. The first set Amh+Tig consists of Amharic and Tigrinya. Both languages share the same Ge’ez script and descend from the proto-Semitic language family. The other set Eng+Uig+Tur consists of one high-resource language (English), one mediumresource language (Turkish) and one low-resource language (Uighur). It also consists of two distinct language scripts: English and Turkish use Latin script while Uighur uses Arabic script. We use an LSTM-CRF architecture (Huang et al., 2015; Lample et al., 2016; Ma and Hovy, 2016) for name tagging. It takes only word embedding as input and predict a tag for each word. Table 5 shows the statistics of training, development, and test sets for each language released by Linguistic Data Consortium (LDC).15 For each language pair, we combine the bilingual aligned words extracted from Wiktionary and monolingual dictionaries based on identical strings.16 We evaluate the quality from several aspects: Impact of Bilingual Dictionary Size In order to show the impact of the size of bilingual lexicons, we use three languages as a case study, and gradually reduce the size of the lexic"
D18-1023,W17-2617,0,0.0411181,"Missing"
D18-1023,D16-1136,0,0.0372422,"antic space and adopt correlational neural networks (CorrNet) (Chandar et al., 2016; Rajendran et al., 2015) as the basic model. In contrast to previous work which only exploited monolingual word semantics, we introduce multiple cluster-level alignments and design a new cluster consistent CorrNet to align both words and clusters. Another branch of approaches for multilingual word embeddings are based on parallel or comparable data, such as parallel sentences (AP Chandar et al., 2014; Gouws et al., 2015; Luong et al., 2015; Hermann and Blunsom, 2014; Schwenk et al., 2017), phrase translations (Duong et al., 2016) and comparable documents (Vulic and Moens, 2015). Moreover, to reduce the need of bilingual alignment, several approaches have been designed to learn cross-lingual embeddings based on a small seed dictionary (Vulic and Korhonen, 2016; Zhang et al., 2016; Artetxe et al., 2017), or even with no supervision (Cao et al., 2016; Zhang et al., 2017d,c; Conneau et al., 2017; Artetxe et al., 2018). However, such methods are still limited to bilingual word embedding learning and remaining to be explored for common semantic space construction. 3 2 Related Work 3.1 Multilingual word embeddings have advan"
D18-1023,E17-1084,0,0.0871444,"A, MultiSkip, and MultiCross)12 . MultiCluster (Ammar et al., 2016b) groups multilingual words into clusters based on bilingual dictionaries and forces all the words from various languages within one cluster share the same embedding. MultiCCA (Ammar et al., 2016b; Faruqui and Dyer, 2014) uses CCA to estimate linear projections for each pair of languages. MultiSkip is an extension of the multilingual skip-gram model (Luong et al., 2015), which requires parallel data. MultiCross is an approach to unify bilingual word embeddings into a shared semantic space using post hoc linear transformations (Duong et al., 2017). Table 2 lists the hyper-parameters used in the experiments. 512 512 20 1, 2, 3 500 0.5 Adadelta Table 2: Hyper-parameters. X OR = L(HlRi , HlRj ) , {li ,lj }∈A where W is the same as the W used in Section 3.3 for each language. We finally optimize the sum of the losses by finding the parameters 0 θ = {Wl , bl , bl , Ul , b∗l , CNNl , bR l }, where l denotes a specific language: Oθ = OW + ON + Ochar + OR 4 Experiments 4.1 Experiment Setup 4.2 Previous work (Ammar et al., 2016b; Duong et al., 2017) evaluated multilingual word embeddings on a series of intrinsic (e.g., monolingual and crossling"
D18-1023,E14-1049,0,0.1096,"ificantly outperforms state-of-the-art multi-lingual embedding learning methods. pendency parsing (Guo et al., 2015; Ammar et al., 2016a), and name tagging (Zhang et al., 2017a; Tsai and Roth, 2016; Zhang et al., 2018; Cheung et al.; Zhang et al., 2017b; Feng et al., 2017). Using bilingual aligned words, previous methods project multiple monolingual embeddings into a shared semantic space using linear mappings (Mikolov et al., 2013b; Rothe et al., 2016; Zhang et al., 2016; Baroni et al., 2015; Xing et al., 2015; Smith et al., 2017) or canonical correlation analysis (CCA) (Ammar et al., 2016b; Faruqui and Dyer, 2014; Lu et al., 2015). Compared with CCA, which only optimizes the correlation for each individual pair of languages, linear mapping based methods can jointly optimize all the languages in the common semantic space. We focus on learning linear mappings to construct the common semantic space and adopt correlational neural networks (CorrNet) (Chandar et al., 2016; Rajendran et al., 2015) as the basic model. In contrast to previous work which only exploited monolingual word semantics, we introduce multiple cluster-level alignments and design a new cluster consistent CorrNet to align both words and c"
D18-1023,H93-1061,0,0.177294,"h, French, Hungarian, Italian, Swedish) respectively. The monolingual data for each language is the combination of the Leipzig Corpora Collection9 and Europarl.10 The bilingual dictionaries are the same as those used in Ammar et al. (2016b).11 Intrinsic Evaluation: QVEC In order to evaluate the quality of multilingual embeddings, we adopt QVEC (Tsvetkov et al., 2015) as the intrinsic evaluation measure. It evaluates the quality of word embeddings based on the alignment of distributional word vectors to linguistic feature vectors extracted from manually crafted lexical resources, e.g., SemCor (Miller et al., 1993). For each word, each dimension of its linguistic feature vector defines the probability of that word belongs to a supersense (e.g., NN.FOOD) which is summarized from WordNet (Fellbaum, 1998). QVEC is computed as QVEC = Pmax j aij ≤1 D X P X r(xi , sj ) × aij , i=1 j=1 where x ∈ RD×1 denotes a distributional word vector and s ∈ RP ×1 denotes a linguistic word vector. D and P denote the sizes of vectors respectively. aij = 1 iff xi is aligned to sj , otherwise aij = 0. r(xi , sj ) is the Pearson’s correlation between xi and sj . QVEC-CCA (Ammar et al., 2016b) is extended from QVEC by using CCA"
D18-1023,N16-1091,0,0.0625978,"Missing"
D18-1023,P15-1119,0,0.0308306,"nt between clusters for common semantic space construction. We evaluate our approach on monolingual and multilingual QVEC (Tsvetkov et al., 2015) tasks, which measure the quality of word embeddings based on the alignment of the embeddings to linguistic feature vectors extracted from manually crafted linguistic resources, as well as an extrinsic evaluation on name tagging for low-resource languages. Experiments demonstrate that our framework is effective at capturing linguistic properties and significantly outperforms state-of-the-art multi-lingual embedding learning methods. pendency parsing (Guo et al., 2015; Ammar et al., 2016a), and name tagging (Zhang et al., 2017a; Tsai and Roth, 2016; Zhang et al., 2018; Cheung et al.; Zhang et al., 2017b; Feng et al., 2017). Using bilingual aligned words, previous methods project multiple monolingual embeddings into a shared semantic space using linear mappings (Mikolov et al., 2013b; Rothe et al., 2016; Zhang et al., 2016; Baroni et al., 2015; Xing et al., 2015; Smith et al., 2017) or canonical correlation analysis (CCA) (Ammar et al., 2016b; Faruqui and Dyer, 2014; Lu et al., 2015). Compared with CCA, which only optimizes the correlation for each individu"
D18-1023,W17-2619,0,0.0200454,"linear mappings to construct the common semantic space and adopt correlational neural networks (CorrNet) (Chandar et al., 2016; Rajendran et al., 2015) as the basic model. In contrast to previous work which only exploited monolingual word semantics, we introduce multiple cluster-level alignments and design a new cluster consistent CorrNet to align both words and clusters. Another branch of approaches for multilingual word embeddings are based on parallel or comparable data, such as parallel sentences (AP Chandar et al., 2014; Gouws et al., 2015; Luong et al., 2015; Hermann and Blunsom, 2014; Schwenk et al., 2017), phrase translations (Duong et al., 2016) and comparable documents (Vulic and Moens, 2015). Moreover, to reduce the need of bilingual alignment, several approaches have been designed to learn cross-lingual embeddings based on a small seed dictionary (Vulic and Korhonen, 2016; Zhang et al., 2016; Artetxe et al., 2017), or even with no supervision (Cao et al., 2016; Zhang et al., 2017d,c; Conneau et al., 2017; Artetxe et al., 2018). However, such methods are still limited to bilingual word embedding learning and remaining to be explored for common semantic space construction. 3 2 Related Work 3"
D18-1023,N18-5009,1,0.881187,"Missing"
D18-1023,N16-1072,0,0.0330789,"roach on monolingual and multilingual QVEC (Tsvetkov et al., 2015) tasks, which measure the quality of word embeddings based on the alignment of the embeddings to linguistic feature vectors extracted from manually crafted linguistic resources, as well as an extrinsic evaluation on name tagging for low-resource languages. Experiments demonstrate that our framework is effective at capturing linguistic properties and significantly outperforms state-of-the-art multi-lingual embedding learning methods. pendency parsing (Guo et al., 2015; Ammar et al., 2016a), and name tagging (Zhang et al., 2017a; Tsai and Roth, 2016; Zhang et al., 2018; Cheung et al.; Zhang et al., 2017b; Feng et al., 2017). Using bilingual aligned words, previous methods project multiple monolingual embeddings into a shared semantic space using linear mappings (Mikolov et al., 2013b; Rothe et al., 2016; Zhang et al., 2016; Baroni et al., 2015; Xing et al., 2015; Smith et al., 2017) or canonical correlation analysis (CCA) (Ammar et al., 2016b; Faruqui and Dyer, 2014; Lu et al., 2015). Compared with CCA, which only optimizes the correlation for each individual pair of languages, linear mapping based methods can jointly optimize all the la"
D18-1023,D15-1243,0,0.120914,"/ -ler), Somali (-o)). Linguists have created a wide variety of linguistic property knowledge bases, which are readily available for thousands of languages. For example, the CLDR (Unicode Common Locale Data Repository)2 includes closed word classes and affixes indicating various linguistic properties. We propose to take advantage of these languageuniversal resources to create clusters, where the words within one cluster share the same linguistic property, and build alignment between clusters for common semantic space construction. We evaluate our approach on monolingual and multilingual QVEC (Tsvetkov et al., 2015) tasks, which measure the quality of word embeddings based on the alignment of the embeddings to linguistic feature vectors extracted from manually crafted linguistic resources, as well as an extrinsic evaluation on name tagging for low-resource languages. Experiments demonstrate that our framework is effective at capturing linguistic properties and significantly outperforms state-of-the-art multi-lingual embedding learning methods. pendency parsing (Guo et al., 2015; Ammar et al., 2016a), and name tagging (Zhang et al., 2017a; Tsai and Roth, 2016; Zhang et al., 2018; Cheung et al.; Zhang et a"
D18-1023,P17-1179,0,0.0840215,"Missing"
D18-1023,P16-1024,0,0.0317905,"cluster-level alignments and design a new cluster consistent CorrNet to align both words and clusters. Another branch of approaches for multilingual word embeddings are based on parallel or comparable data, such as parallel sentences (AP Chandar et al., 2014; Gouws et al., 2015; Luong et al., 2015; Hermann and Blunsom, 2014; Schwenk et al., 2017), phrase translations (Duong et al., 2016) and comparable documents (Vulic and Moens, 2015). Moreover, to reduce the need of bilingual alignment, several approaches have been designed to learn cross-lingual embeddings based on a small seed dictionary (Vulic and Korhonen, 2016; Zhang et al., 2016; Artetxe et al., 2017), or even with no supervision (Cao et al., 2016; Zhang et al., 2017d,c; Conneau et al., 2017; Artetxe et al., 2018). However, such methods are still limited to bilingual word embedding learning and remaining to be explored for common semantic space construction. 3 2 Related Work 3.1 Multilingual word embeddings have advanced many multilingual NLP tasks, such as machine translation (Zou et al., 2013; Mikolov et al., 2013b; Madhyastha and Espa˜na-Bonet, 2017), de2 Approach Overview Figure 1 shows the overview of our neural architecture. We project all m"
D18-1023,D17-1207,0,0.120181,"Missing"
D18-1023,P15-2118,0,0.0245502,"works (CorrNet) (Chandar et al., 2016; Rajendran et al., 2015) as the basic model. In contrast to previous work which only exploited monolingual word semantics, we introduce multiple cluster-level alignments and design a new cluster consistent CorrNet to align both words and clusters. Another branch of approaches for multilingual word embeddings are based on parallel or comparable data, such as parallel sentences (AP Chandar et al., 2014; Gouws et al., 2015; Luong et al., 2015; Hermann and Blunsom, 2014; Schwenk et al., 2017), phrase translations (Duong et al., 2016) and comparable documents (Vulic and Moens, 2015). Moreover, to reduce the need of bilingual alignment, several approaches have been designed to learn cross-lingual embeddings based on a small seed dictionary (Vulic and Korhonen, 2016; Zhang et al., 2016; Artetxe et al., 2017), or even with no supervision (Cao et al., 2016; Zhang et al., 2017d,c; Conneau et al., 2017; Artetxe et al., 2018). However, such methods are still limited to bilingual word embedding learning and remaining to be explored for common semantic space construction. 3 2 Related Work 3.1 Multilingual word embeddings have advanced many multilingual NLP tasks, such as machine"
D18-1023,N16-1156,0,0.121959,"n name tagging for low-resource languages. Experiments demonstrate that our framework is effective at capturing linguistic properties and significantly outperforms state-of-the-art multi-lingual embedding learning methods. pendency parsing (Guo et al., 2015; Ammar et al., 2016a), and name tagging (Zhang et al., 2017a; Tsai and Roth, 2016; Zhang et al., 2018; Cheung et al.; Zhang et al., 2017b; Feng et al., 2017). Using bilingual aligned words, previous methods project multiple monolingual embeddings into a shared semantic space using linear mappings (Mikolov et al., 2013b; Rothe et al., 2016; Zhang et al., 2016; Baroni et al., 2015; Xing et al., 2015; Smith et al., 2017) or canonical correlation analysis (CCA) (Ammar et al., 2016b; Faruqui and Dyer, 2014; Lu et al., 2015). Compared with CCA, which only optimizes the correlation for each individual pair of languages, linear mapping based methods can jointly optimize all the languages in the common semantic space. We focus on learning linear mappings to construct the common semantic space and adopt correlational neural networks (CorrNet) (Chandar et al., 2016; Rajendran et al., 2015) as the basic model. In contrast to previous work which only exploite"
D18-1023,D13-1141,0,0.0354099,"o reduce the need of bilingual alignment, several approaches have been designed to learn cross-lingual embeddings based on a small seed dictionary (Vulic and Korhonen, 2016; Zhang et al., 2016; Artetxe et al., 2017), or even with no supervision (Cao et al., 2016; Zhang et al., 2017d,c; Conneau et al., 2017; Artetxe et al., 2018). However, such methods are still limited to bilingual word embedding learning and remaining to be explored for common semantic space construction. 3 2 Related Work 3.1 Multilingual word embeddings have advanced many multilingual NLP tasks, such as machine translation (Zou et al., 2013; Mikolov et al., 2013b; Madhyastha and Espa˜na-Bonet, 2017), de2 Approach Overview Figure 1 shows the overview of our neural architecture. We project all monolingual word embeddings into a common semantic space based on word-level as well as cluster-level alignments and learn the transformation functions. First, on cldr.unicode.org 251 Figure 1: Architecture Overview. In each monolingual semantic space, the words within solid rectangle denote a neighbor based cluster and the words within dotted rectangle denote a linguistic property based cluster. where Hl1 ∈ R|Vl1 |×h and Hl2 ∈ R|Vl2 |×h are"
D18-1023,N15-1104,0,0.0397766,". Experiments demonstrate that our framework is effective at capturing linguistic properties and significantly outperforms state-of-the-art multi-lingual embedding learning methods. pendency parsing (Guo et al., 2015; Ammar et al., 2016a), and name tagging (Zhang et al., 2017a; Tsai and Roth, 2016; Zhang et al., 2018; Cheung et al.; Zhang et al., 2017b; Feng et al., 2017). Using bilingual aligned words, previous methods project multiple monolingual embeddings into a shared semantic space using linear mappings (Mikolov et al., 2013b; Rothe et al., 2016; Zhang et al., 2016; Baroni et al., 2015; Xing et al., 2015; Smith et al., 2017) or canonical correlation analysis (CCA) (Ammar et al., 2016b; Faruqui and Dyer, 2014; Lu et al., 2015). Compared with CCA, which only optimizes the correlation for each individual pair of languages, linear mapping based methods can jointly optimize all the languages in the common semantic space. We focus on learning linear mappings to construct the common semantic space and adopt correlational neural networks (CorrNet) (Chandar et al., 2016; Rajendran et al., 2015) as the basic model. In contrast to previous work which only exploited monolingual word semantics, we introdu"
D18-1023,Q16-1031,0,\N,Missing
D18-1035,P17-1176,1,0.850225,"the size of the vocabulary. Our primary evaluations use tokenized and cased BLEU. For METEOR and TER evaluations, we use multeval4 with tokenized and case-insensitive scoring. All the underlying models are trained from scratch, except for ConvS2S WMT14 EnglishGerman translation, for which we use the trained model (as well as training data) provided by Gehring et al. (2017).5 Training To overcome the severe instability reported by Gu et al. (2017), we introduce the use of a pseudo-parallel corpus generated from the underlying NMT model (Gao and He, 2013; Auli and Gao, 2014; Kim and Rush, 2016; Chen et al., 2017; Freitag et al., 2017; Zhang et al., 2017) for actor training. This corpus includes pairs that both (i) have a high model likelihood, so that we can coerce the model to generate them without much additional training or many new parameters and, (ii) represent high-quality translations, measured according to a target metric like BLEU. We do this by generating sentences from the original unaugmented model with large-beam beam search and selecting the best sentence from the resulting kbest list according to the decoding objective. More specifically, let hx, yi be a sentence pair in the training d"
D18-1035,P16-2007,0,0.0243728,"ves are all well correlated with each other, training with different objectives do not differ dramatically. 5 Figure 4: (a) The effect of beam size on the IWSLT16 De-En validation with Transformer and (b) the effect of the training corpus composition in the same setting. para: parallel corpus; full: all 35 beam search outputs; thd: beam search outputs that score higher than the base model’s greedy decoding output; top1: beam search output with the highest bleu score; comb.: top1+para. 0.0 corresponds to 33.04 BLEU. machine translation (SMT) (Chiang, 2012; Gao and He, 2013; Auli and Gao, 2014; Dakwale and Monz, 2016). Gao and He (2013) integrate a recurrent neural network language model as an additional feature into a trained phrase-based SMT system and train it by maximizing the expected BLEU on k-best list from the underlying model. Our work revisits a similar idea in the context trainable greedy decoding for neural MT. Related Work Data Distillation Our work is directly inspired by work on knowledge distillation, which uses a similar pseudo-parallel corpus strategy, but aims at training a compact model to approximate the function learned by a larger model or an ensemble of models (Hinton et al., 2015)."
D18-1035,D18-1549,0,0.0224809,"onjecture that this gives the decoder more flexibility with which to guide decoding. In cases where model throughput is less important, our method can also be combined with beam search at test time to yield results somewhat better than either could achieve alone. Table 2 shows the result when combining our method with beam search. RNN We use OpenNMT-py (Klein et al., 2017)6 to implement our model. It is composed of an encoder with two-layer bidirectional RNN, and a decoder with another twolayer RNN. We refer to OpenNMT’s default setting (rnn size = 500, word vec size = 500) and the setting in Artetxe et al. (2018) (rnn size = 600, word vec size = 300), and choose similar hyper-parameters: rnn size = 500, word vec size = 300 for IWSLT16 and rnn size = 600, word vec size = 500 for WMT. We use the input-feeding decoder and global attention with the general alignment function (Luong et al., 2015). 7 6 Results and Analysis 8 https://github.com/OpenNMT/OpenNMT-py 384 https://github.com/facebookresearch/fairseq-py https://github.com/salesforce/nonauto-nmt src ref Am Vormittag wollte auch die Arbeitsgruppe Migration und Integration ihre Beratungen fortsetzen . During the morning , the Migration and Integration"
D18-1035,P14-2023,0,0.0547932,"BPE; Sennrich et al., 2016) to restrict the size of the vocabulary. Our primary evaluations use tokenized and cased BLEU. For METEOR and TER evaluations, we use multeval4 with tokenized and case-insensitive scoring. All the underlying models are trained from scratch, except for ConvS2S WMT14 EnglishGerman translation, for which we use the trained model (as well as training data) provided by Gehring et al. (2017).5 Training To overcome the severe instability reported by Gu et al. (2017), we introduce the use of a pseudo-parallel corpus generated from the underlying NMT model (Gao and He, 2013; Auli and Gao, 2014; Kim and Rush, 2016; Chen et al., 2017; Freitag et al., 2017; Zhang et al., 2017) for actor training. This corpus includes pairs that both (i) have a high model likelihood, so that we can coerce the model to generate them without much additional training or many new parameters and, (ii) represent high-quality translations, measured according to a target metric like BLEU. We do this by generating sentences from the original unaugmented model with large-beam beam search and selecting the best sentence from the resulting kbest list according to the decoding objective. More specifically, let hx,"
D18-1035,N13-1048,0,0.120542,"te-pair encoding (BPE; Sennrich et al., 2016) to restrict the size of the vocabulary. Our primary evaluations use tokenized and cased BLEU. For METEOR and TER evaluations, we use multeval4 with tokenized and case-insensitive scoring. All the underlying models are trained from scratch, except for ConvS2S WMT14 EnglishGerman translation, for which we use the trained model (as well as training data) provided by Gehring et al. (2017).5 Training To overcome the severe instability reported by Gu et al. (2017), we introduce the use of a pseudo-parallel corpus generated from the underlying NMT model (Gao and He, 2013; Auli and Gao, 2014; Kim and Rush, 2016; Chen et al., 2017; Freitag et al., 2017; Zhang et al., 2017) for actor training. This corpus includes pairs that both (i) have a high model likelihood, so that we can coerce the model to generate them without much additional training or many new parameters and, (ii) represent high-quality translations, measured according to a target metric like BLEU. We do this by generating sentences from the original unaugmented model with large-beam beam search and selecting the best sentence from the resulting kbest list according to the decoding objective. More sp"
D18-1035,D17-1210,1,0.839035,"Missing"
D18-1035,P02-1040,0,0.104736,"represent high-quality translations, measured according to a target metric like BLEU. We do this by generating sentences from the original unaugmented model with large-beam beam search and selecting the best sentence from the resulting kbest list according to the decoding objective. More specifically, let hx, yi be a sentence pair in the training data and Z = {z1 , ..., zk } be the kbest list from beam search on the pretrained NMT model, where k is the beam size. We define the objective score of the translation z w.r.t. the goldstandard translation y according to a target metric such as BLEU (Papineni et al., 2002), NIST 4 https://github.com/jhclark/multeval https://s3.amazonaws.com/fairseqpy/models/wmt14.v2.en-de.fconv-py.tar.bz2 5 383 greedy BLEU↑ beam4 23.57 27.44 27.15 24.90 28.80 28.74 23.59 28.74 28.36 12.45 15.43 13.76 13.22 16.86 14.61 13.02 17.17 14.49 23.08 27.52 26.44 24.62 28.79 27.31 24.54 28.56 26.96 tg greedy BLEU↑ beam4 tg greedy tok/s↑ beam4 tg 32.5 64.0 26.5 45.7 124.0 51.2 31.2 11.7 12.8 43.8 16.9 27.9 22.4 9.1 12.2 32.5 13.6 26.1 En → De 62.8 191.1 63.9 45.0 87.2 31.0 60.4 167.5 59.8 20.05 22.88 23.87 21.11 24.02 25.03 19.88 24.42 25.46 48.1 136.5 57.9 En → Fi 51.5 24.8 31.4 33.1 11."
D18-1035,D17-1227,0,0.0157199,"ecoder—while beam search requires an equivalent of k such runs, as well as substantial additional overhead for data management. However, beam search often leads to substantial improvement over greedy decoding. For example, Ranzato et al. (2015) report that beam search (with k = 10) gives a 2.2 BLEU improvement in translation and a 3.5 ROUGE-2 improvement in summarization over greedy decoding. Various approaches have been explored recently to improve beam search by improving the method by which candidate sequences are scored (Li et al., 2016; Shu and Nakayama, 2017), the termination criterion (Huang et al., 2017), or the search function itself (Li et al., 2017). In contrast, Gu et al. (2017) have tried to directly improve greedy decoding to decode for an arbitrary decoding objective. They add a small actor network to the decoder and train it with a version of policy gradient to optimize sequence objectives like BLEU. However, they report that they are seriously limited by the instability of this approach to training. In this paper, we propose a procedure to modify a trained decoder to allow it to generate text greedily with the level of quality (according to metrics like BLEU) that would otherwise req"
D18-1035,D16-1139,0,0.150165,", 2016) to restrict the size of the vocabulary. Our primary evaluations use tokenized and cased BLEU. For METEOR and TER evaluations, we use multeval4 with tokenized and case-insensitive scoring. All the underlying models are trained from scratch, except for ConvS2S WMT14 EnglishGerman translation, for which we use the trained model (as well as training data) provided by Gehring et al. (2017).5 Training To overcome the severe instability reported by Gu et al. (2017), we introduce the use of a pseudo-parallel corpus generated from the underlying NMT model (Gao and He, 2013; Auli and Gao, 2014; Kim and Rush, 2016; Chen et al., 2017; Freitag et al., 2017; Zhang et al., 2017) for actor training. This corpus includes pairs that both (i) have a high model likelihood, so that we can coerce the model to generate them without much additional training or many new parameters and, (ii) represent high-quality translations, measured according to a target metric like BLEU. We do this by generating sentences from the original unaugmented model with large-beam beam search and selecting the best sentence from the resulting kbest list according to the decoding objective. More specifically, let hx, yi be a sentence pai"
D18-1035,D15-1044,0,0.0503676,"like BLEU. Our method is inspired by earlier work on this problem, but requires no reinforcement learning, and can be trained reliably on a range of models. Experiments on three parallel corpora and three architectures show that the method yields substantial improvements in translation quality and speed over each base system. 1 Introduction Neural network sequence decoders yield stateof-the-art results for many text generation tasks, including machine translation (Bahdanau et al., 2015; Luong et al., 2015; Gehring et al., 2017; Vaswani et al., 2017; Dehghani et al., 2018), text summarization (Rush et al., 2015; Ranzato et al., 2015; See et al., 2017; Paulus et al., 2017) and image captioning (Vinyals et al., 2015; Xu et al., 2015). These decoders generate tokens from left to right, at each step giving a distribution over possible next tokens, conditioned on both the input and all the tokens generated so far. However, since the space of all possible output sequences is infinite and grows exponentially with sequence length, heuristic search methods such as greedy decod380 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 380–390 c Brussels, Belgium, October"
D18-1035,P17-4012,0,0.039023,"ose little or no performance, and doing so yields an increase in decoding efficiency, even accounting for the small overhead added by the actor. Among the three architectures, ConvS2S—the one with the most and largest layers—performs best. We conjecture that this gives the decoder more flexibility with which to guide decoding. In cases where model throughput is less important, our method can also be combined with beam search at test time to yield results somewhat better than either could achieve alone. Table 2 shows the result when combining our method with beam search. RNN We use OpenNMT-py (Klein et al., 2017)6 to implement our model. It is composed of an encoder with two-layer bidirectional RNN, and a decoder with another twolayer RNN. We refer to OpenNMT’s default setting (rnn size = 500, word vec size = 500) and the setting in Artetxe et al. (2018) (rnn size = 600, word vec size = 300), and choose similar hyper-parameters: rnn size = 500, word vec size = 300 for IWSLT16 and rnn size = 600, word vec size = 500 for WMT. We use the input-feeding decoder and global attention with the general alignment function (Luong et al., 2015). 7 6 Results and Analysis 8 https://github.com/OpenNMT/OpenNMT-py 384"
D18-1035,P17-1099,0,0.0398925,"ier work on this problem, but requires no reinforcement learning, and can be trained reliably on a range of models. Experiments on three parallel corpora and three architectures show that the method yields substantial improvements in translation quality and speed over each base system. 1 Introduction Neural network sequence decoders yield stateof-the-art results for many text generation tasks, including machine translation (Bahdanau et al., 2015; Luong et al., 2015; Gehring et al., 2017; Vaswani et al., 2017; Dehghani et al., 2018), text summarization (Rush et al., 2015; Ranzato et al., 2015; See et al., 2017; Paulus et al., 2017) and image captioning (Vinyals et al., 2015; Xu et al., 2015). These decoders generate tokens from left to right, at each step giving a distribution over possible next tokens, conditioned on both the input and all the tokens generated so far. However, since the space of all possible output sequences is infinite and grows exponentially with sequence length, heuristic search methods such as greedy decod380 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 380–390 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association"
D18-1035,W17-3204,0,0.0211464,"P (y|x; θ) = T Y P (yt |y&lt;t , x; θ), Beam Search Beam search decodes from left to right, and maintains k > 1 hypotheses at each step. At each step t, beam search considers all possible next tokens conditioned on the current hypotheses, the overall highest Q and picks the k with ˆ When all the hyscores tt0 =1 P (yt0 |y&lt;t0 , x; θ). potheses are complete (they end in an end-of-thesentence symbol or reach a predetermined length limit), it returns the hypothesis with the highest likelihood. Tuning to find a roughly optimal beam size k can yield improvements in performance with sizes as high as 30 (Koehn and Knowles, 2017; Britz et al., 2017). However, the complexity of beam search grows linearly in beam size, with high constant terms, making it undesirable in some applications where latency is important, such as in on-device real-time translation. (1) t=1 where θ is the set of model parameters. Given a parallel corpus Dx,y of source–target sentence pairs, the neural machine translation model can be trained by maximizing the loglikelihood: ( ) X θˆ = argmax log P (y|x; θ) . (2) θ 2.2 NPAD Noisy parallel approximate decoding (NPAD; Cho, 2016) is a parallel decoding algorithm that can be used to improve greedy d"
D18-1035,P16-1162,0,0.0524523,"g outputs under greedy or small-beam decoding. 4 Experiments 4.1 Setting We evaluate our approach on IWSLT16 GermanEnglish, WMT15 Finnish-English, and WMT14 De-En translation in both directions with three strong translation model architectures. For IWSLT16, we use tst2013 and tst2014 for validation and testing, respectively. For WMT15, we use newstest2013 and newstest2015 for validation and testing, respectively. For WMT14, we use newstest2013 and newstest2014 for validation and testing, respectively. All the data are tokenized and segmented into subword symbols using byte-pair encoding (BPE; Sennrich et al., 2016) to restrict the size of the vocabulary. Our primary evaluations use tokenized and cased BLEU. For METEOR and TER evaluations, we use multeval4 with tokenized and case-insensitive scoring. All the underlying models are trained from scratch, except for ConvS2S WMT14 EnglishGerman translation, for which we use the trained model (as well as training data) provided by Gehring et al. (2017).5 Training To overcome the severe instability reported by Gu et al. (2017), we introduce the use of a pseudo-parallel corpus generated from the underlying NMT model (Gao and He, 2013; Auli and Gao, 2014; Kim and"
D18-1035,P16-1159,0,0.0256975,"Zhang et al. (2017) propose a new strategy to generate a pseudo-corpus, namely, fast sequenceinterpolation based on the greedy output of the teacher model and the parallel corpus. Freitag et al. (2017) extend knowledge distillation on an ensemble and oracle BLEU teacher model. However, all these approaches require the expensive procedure of retraining the full student network. Decoding for Multiple Objectives Several works have proposed to incorporate different decoding objectives into training. Ranzato et al. (2015) and Bahdanau et al. (2016) use reinforcement learning to achieve this goal. Shen et al. (2016) and Norouzi et al. (2016) train the model by defining an objective-dependent loss function. Wiseman and Rush (2016) propose a learning algorithm tailored for beam search. Unlike these works that optimize the entire model, Li et al. (2017) introduce an additional network that predicts an arbitrary decoding objective given a source sentence and a prefix of translation. This prediction is used as an auxiliary score in beam search. All of these methods focus primarily on improving beam search results, rather than those with greedy decoding. Pseudo-Parallel Corpora in Statistical MT Pseudo-paralle"
D18-1035,2006.amta-papers.25,0,0.0353439,"the architecture of the actor network. zt = σ([ht , et ]W i + bi ), at = tanh(zt W o + bo ), (5) the ff2 actor is computed as z1t = σ([ht , et ]W i + bi ), z2t = σ(z1t W z + bz ), at = tanh(z2t W o + bo ), (6) the rnn actor is computed as zt = σ([ht , et ]U z + st−1 W z ), 3 Methods rt = σ([ht , et ]U r + st−1 W r ),  ˜ st = tanh [ht , et ]U h + (st−1 ◦ rt )W h , We propose a method for training a small actor neural network, following the trainable greedy decoding approach of Gu et al. (2017). This actor st = (1 − zt ) ◦ ˜ st + zt ◦ st−1 , at = st U, 382 (7) (Doddington, 2002), negative TER (Snover et al., 2006), or METEOR (Lavie and Denkowski, 2009) ˜ that as O(z, y). Then we choose the sentence z has the highest score to become our new target sentence: and the gate actor is computed as zt = σ([ht , et ]U z ), at = zt ◦ tanh([ht , et ]U ). (8) Once the action at has been computed, the hidden state ht is simply replaced with the updated ˜t : state h ˜ t = f (ht , et ) + at . h (9) ˜ = argmax O(z, y). z (10) z=z1 ,..,zk Figure 1 shows a single step of the actor interacting with the underlying neural decoder of each of the three NMT architectures we use: the RNNbased model of Luong et al. (2015), ConvS"
D18-1035,D16-1137,0,0.0578165,"Missing"
D18-1035,D17-1154,0,0.0616288,"valuations use tokenized and cased BLEU. For METEOR and TER evaluations, we use multeval4 with tokenized and case-insensitive scoring. All the underlying models are trained from scratch, except for ConvS2S WMT14 EnglishGerman translation, for which we use the trained model (as well as training data) provided by Gehring et al. (2017).5 Training To overcome the severe instability reported by Gu et al. (2017), we introduce the use of a pseudo-parallel corpus generated from the underlying NMT model (Gao and He, 2013; Auli and Gao, 2014; Kim and Rush, 2016; Chen et al., 2017; Freitag et al., 2017; Zhang et al., 2017) for actor training. This corpus includes pairs that both (i) have a high model likelihood, so that we can coerce the model to generate them without much additional training or many new parameters and, (ii) represent high-quality translations, measured according to a target metric like BLEU. We do this by generating sentences from the original unaugmented model with large-beam beam search and selecting the best sentence from the resulting kbest list according to the decoding objective. More specifically, let hx, yi be a sentence pair in the training data and Z = {z1 , ..., zk } be the kbest li"
D18-1035,D17-1151,0,\N,Missing
D18-1149,N13-1073,0,0.0453925,"16) investigated neural GPU (Kaiser and Sutskever, 2015), for machine translation. They evaluated both non-autoregressive and autoregressive approaches, and found that the nonautoregressive approach significantly lags behind the autoregressive variants. It however differs from our approach that each iteration does not output a refined version from the previous iteration. The recent paper by Gu et al. (2017) is most relevant to the proposed work. They similarly introduced a sequence of discrete latent variables. They however use supervised learning for inference, using the word alignment tool (Dyer et al., 2013). To achieve the best result, Gu et al. (2017) stochastically sample the latent variables and rerank the corresponding target sequences with an external, autoregressive model. This is in contrast to the proposed approach which is fully deterministic during decoding and does not rely on any extra reranking mechanism. Parallel WaveNet Simultaneously with Gu et al. (2017), Oord et al. (2017) presented a nonautoregressive sequence model for speech generation. They use inverse autoregressive flow (IAF, Kingma et al., 2016) to map a sequence of independent random variables to a target sequence. They"
D18-1149,P16-5005,1,0.898246,"Missing"
D18-1149,N16-1162,1,0.728491,"mes as an input to each conditional in Eq. (1). Then, the goal of learning is to maximize the log-probability of the original reference Y ⇤ given the corrupted version. That is, to minimize JDAE (✓) = T X t=1 log p✓ (yt⇤ |Y˜ , X). (3) Once this cost JDAE is minimized, we can recursively perform the maximum-a-posterior inference, i.e., Yˆ = arg maxY log p✓ (Y |X), to find Yˆ that (approximately) maximizes log p(Y |X). Corruption Process C There is little consensus on the best corruption process for a sequence, especially of discrete tokens. In this work, we use a corruption process proposed by Hill et al. (2016), which has recently become more widely adopted (see, e.g., Artetxe et al., 2017; Lample et al., 2017). Each yt⇤ in a reference target Y ⇤ = (y1⇤ , . . . , yT⇤ ) is corrupted with a probability 2 [0, 1]. If decided to corrupt, we either (1) re⇤ place yt+1 with this token yt⇤ , (2) replace yt⇤ with a token uniformly selected from a vocabulary of all ⇤ . unique tokens at random, or (3) swap yt⇤ and yt+1 ⇤ ⇤ This is done sequentially from y1 until yT . 3.3 Learning Cost function Although it is possible to train the proposed non-autoregressive sequence model using either of the cost functions abov"
D18-1149,P84-1044,0,0.663159,"Missing"
D18-1149,D13-1176,0,0.0521644,"g (Mikolov et al., 1 We release the implementation, preprocessed datasets as well as trained models online at https://github.com/ nyu-dl/dl4mt-nonauto. 1173 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1173–1182 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics 2010). When this is conditioned on an extra variable X, it becomes a conditional sequence model log p(Y |X) which serves as a basis on which many recent advances in, e.g., machine translation (Bahdanau et al., 2014; Sutskever et al., 2014; Kalchbrenner and Blunsom, 2013) and speech recognition (Chorowski et al., 2015; Chiu et al., 2017) have been made. Despite the recent success, autoregressive sequence modeling has a weakness due to its nature of sequential processing. This weakness shows itself especially when we try to decode the most likely sequence from a trained model, i.e., Yˆ = arg max log p(Y |X). Y There is no known polynomial algorithm for solving it exactly, and practitioners have relied on approximate decoding algorithms (see, e.g., Cho, 2016; Hoang et al., 2017). Among these, beam search has become the method of choice, due to its superior perfo"
D18-1149,D16-1139,0,0.133895,"m Yˆ l 1 in Eq. (2) with Y˜ in Eq. (3): J(✓) = L+1 X ↵l T X t=1 l=0 +(1 log p✓ (yt⇤ |Yˆ l ↵l ) T X t=1 1 , X) log p✓ (yt⇤ |Y˜ , X) (4) ! , where Y˜ ⇠ C(Y |Y ⇤ ), and ↵l is a sample from a Bernoulli distribution with the probability pDAE . pDAE is a hyperparameter. As the first conditional p(Y 0 |X) in Eq. (1) does not take as input any target Y , we set ↵0 = 1 always. Distillation Gu et al. (2017), in the context of machine translation, and Oord et al. (2017), in the context of speech generation, have recently discovered that it is important to use knowledge distillation (Hinton et al., 2015; Kim and Rush, 2016) to successfully train a non-autoregressive sequence model. Following Gu et al. (2017), we also use knowledge distillation by replacing the reference target Y ⇤ of each training example 1175 (X, Y ⇤ ) with a target Y AR generated from a welltrained autoregressive counterpart. Other than this replacement, the cost function in Eq (4) and the model architecture remain unchanged. Target Length Prediction One difference between the autoregressive and non-autoregressive models is that the former naturally models the length of a target sequence without any arbitrary upper-bound, while the latter does"
D18-1149,P07-2045,0,0.0130806,"ch on two sequence modeling tasks: machine translation and image caption generation. We compare the proposed non-autoregressive model against the autoregressive counterpart both in terms of generation quality, measured in terms of BLEU (Papineni et al., 2002), and generation efficiency, measured in terms of (source) tokens and images per second for translation and image captioning, respectively. Machine Translation We choose three tasks of different sizes: IWSLT’16 En$De (196k pairs), WMT’16 En$Ro (610k pairs) and WMT’14 En$De (4.5M pairs). We tokenize each sentence using a script from Moses (Koehn et al., 2007) and segment each word into subword units using BPE (Sennrich et al., 2016). We use 40k tokens from both source and target for all the tasks. For WMT’14 En-De, we use newstest-2013 and newstest-2014 as development and test sets. For WMT’16 En-Ro, we use newsdev-2016 and newstest-2016 as development and test sets. For IWSLT’16 En-De, we use test2013 for validation. We closely follow the setting by Gu et al. (2017). In the case of IWSLT’16 En-De, we use the small model (dmodel = 278, dhidden = 507, pdropout = 0.1, nlayer = 5 and nhead = 2).2 For WMT’14 En-De and WMT’16 En-Ro, we use the base tra"
D18-1149,P02-1040,0,0.101224,"al layer (He et al., 2016). The original input X is padded or shortned to fit the length of the reference target sequence before being fed to Decoder 1. At each refinement step l, Decoder 2 takes as input the predicted target sequence Yˆ l 1 and the sequence of final activation vectors from the previous step. Experimental Setting We evaluate the proposed approach on two sequence modeling tasks: machine translation and image caption generation. We compare the proposed non-autoregressive model against the autoregressive counterpart both in terms of generation quality, measured in terms of BLEU (Papineni et al., 2002), and generation efficiency, measured in terms of (source) tokens and images per second for translation and image captioning, respectively. Machine Translation We choose three tasks of different sizes: IWSLT’16 En$De (196k pairs), WMT’16 En$Ro (610k pairs) and WMT’14 En$De (4.5M pairs). We tokenize each sentence using a script from Moses (Koehn et al., 2007) and segment each word into subword units using BPE (Sennrich et al., 2016). We use 40k tokens from both source and target for all the tasks. For WMT’14 En-De, we use newstest-2013 and newstest-2014 as development and test sets. For WMT’16"
D18-1149,C12-2104,0,0.0197322,"l one, are modeled by a single, shared neural network, this refinement can be performed as many iterations as necessary until a predefined stopping criterion is met. A criterion can be based either on the amount of change in a target sequence after each iteration (i.e., D(Yˆ l 1 , Yˆ l )  ✏), or on the amount of change in the conditional log-probabilities (i.e., |log p(Yˆ l 1 |X) log p(Yˆ l 1 |X) | ✏) or on the computational budget. In our experiments, we use the first criterion and use Jaccard distance as our distance function D. 4 Related Work Non-Autoregressive Neural Machine Translation Schwenk (2012) proposed a continuousspace translation model to estimate the conditional distribution over a target phrase given a source phrase, while dropping the conditional dependencies among target tokens. The evaluation was however limited to reranking and to short phrase pairs (up to 7 words on each side) only. Kaiser and Bengio (2016) investigated neural GPU (Kaiser and Sutskever, 2015), for machine translation. They evaluated both non-autoregressive and autoregressive approaches, and found that the nonautoregressive approach significantly lags behind the autoregressive variants. It however differs f"
D18-1149,P16-1162,0,0.351135,"generation. We compare the proposed non-autoregressive model against the autoregressive counterpart both in terms of generation quality, measured in terms of BLEU (Papineni et al., 2002), and generation efficiency, measured in terms of (source) tokens and images per second for translation and image captioning, respectively. Machine Translation We choose three tasks of different sizes: IWSLT’16 En$De (196k pairs), WMT’16 En$Ro (610k pairs) and WMT’14 En$De (4.5M pairs). We tokenize each sentence using a script from Moses (Koehn et al., 2007) and segment each word into subword units using BPE (Sennrich et al., 2016). We use 40k tokens from both source and target for all the tasks. For WMT’14 En-De, we use newstest-2013 and newstest-2014 as development and test sets. For WMT’16 En-Ro, we use newsdev-2016 and newstest-2016 as development and test sets. For IWSLT’16 En-De, we use test2013 for validation. We closely follow the setting by Gu et al. (2017). In the case of IWSLT’16 En-De, we use the small model (dmodel = 278, dhidden = 507, pdropout = 0.1, nlayer = 5 and nhead = 2).2 For WMT’14 En-De and WMT’16 En-Ro, we use the base transformer by Vaswani et al. (2017) (dmodel = 512, dhidden = 512, pdropout ="
D18-1176,N18-2031,0,0.157046,"ning many NLP systems is selecting what kinds of word embeddings to use, with people often resorting to freely available pre-trained embeddings. While this is often a sensible thing to do, the usefulness of word embeddings for downstream tasks tends to be hard to predict, as downstream tasks can be poorly correlated with word-level benchmarks. An alternative is to try to combine the strengths of different word embeddings. Recent work in socalled “meta-embeddings”, which ensembles embedding sets, has been gaining traction (Yin and Sch¨utze, 2015; Bollegala et al., 2017; Murom¨agi et al., 2017; Coates and Bollegala, 2018). Metaembeddings are usually created in a separate preprocessing step, rather than in a process that is dynamically adapted to the task. In this work, we explore the supervised learning of task-specific, dynamic meta-embeddings, and apply the technique to sentence representations. The proposed approach turns out to be highly effective, leading to state-of-the-art performance within the same model class on a variety of tasks, opening up new areas for exploration and yielding insights into the usage of word embeddings. Why Is This a Good Idea? Our technique brings several important benefits to N"
D18-1176,D17-1070,1,0.911117,"word embeddings. • Interpretability and Linguistic Analysis Different word embeddings work well on different tasks. This is well-known in the field, but knowing why this happens is less wellunderstood. Our method sheds light on which embeddings are preferred in which linguistic contexts, for different tasks, and allows us to speculate as to why that is the case. Outline In what follows, we explore dynamic meta-embeddings and show that this method outperforms the naive concatenation of various word embeddings, while being more efficient. We apply the technique in a BiLSTM-max sentence encoder (Conneau et al., 2017) and evaluate it on wellknown tasks in the field: natural language inference (SNLI and MultiNLI; §4), sentiment analysis (SST; §5), and image-caption retrieval (Flickr30k; §6). In each case we show state-of-the-art performance within the class of single sentence encoder models. Furthermore, we include an extensive analysis (§7) to highlight the general usefulness of our technique and to illustrate how it can lead to new insights. 2 Related Work Thanks to their widespread popularity in NLP, a sprawling literature has emerged about learning and applying word embeddings—much too large to fully co"
D18-1176,W16-2524,0,0.0134293,"explored (Wang et al., 2018; Kiros et al., 2018). There has also been work on unifying multi-view embeddings from different data sources (Luo et al., 2014). The usefulness of different embeddings as initialization has been explored (Kocmi and Bojar, 2017), and different architectures and hyperparameters have been extensively examined (Levy et al., 2015). Problems with evaluating word embeddings intrinsically are well known (Faruqui et al., 2016), and various alternatives for evaluating word embeddings in downstream tasks have been proposed (e.g., Tsvetkov et al., 2015; Schnabel et al., 2015; Ettinger et al., 2016). For more related work with regard to word embeddings and their evaluation, see Bakarov (2017). Our work can be seen as an instance of the wellknown attention mechanism (Bahdanau et al., 2014), and its recent sentence-level incarnations of self-attention (Lin et al., 2017) and inner-attention (Cheng et al., 2016; Liu et al., 2016), where the attention mechanism is applied within the same sentence instead of for aligning multiple sentences. Here, we learn (optionally contextualized) attention weights for different embedding sets and apply the technique in sentence representations (Kiros et al."
D18-1176,W16-2506,0,0.0196495,"(1 − α)v] (Bruni et al., 2014). Here, we 1467 dynamically learn the weights to combine representations. Recently, related dynamic multi-modal fusion methods have also been explored (Wang et al., 2018; Kiros et al., 2018). There has also been work on unifying multi-view embeddings from different data sources (Luo et al., 2014). The usefulness of different embeddings as initialization has been explored (Kocmi and Bojar, 2017), and different architectures and hyperparameters have been extensively examined (Levy et al., 2015). Problems with evaluating word embeddings intrinsically are well known (Faruqui et al., 2016), and various alternatives for evaluating word embeddings in downstream tasks have been proposed (e.g., Tsvetkov et al., 2015; Schnabel et al., 2015; Ettinger et al., 2016). For more related work with regard to word embeddings and their evaluation, see Bakarov (2017). Our work can be seen as an instance of the wellknown attention mechanism (Bahdanau et al., 2014), and its recent sentence-level incarnations of self-attention (Lin et al., 2017) and inner-attention (Cheng et al., 2016; Liu et al., 2016), where the attention mechanism is applied within the same sentence instead of for aligning mul"
D18-1176,D14-1005,1,0.923595,"here has also been work on learning multiple embeddings per word (Chen et al., 2014; Neelakantan et al., 2015; Vu and Parker, 2016), including a lot of work in sense embeddings where the senses of a word have their own individual embeddings (Iacobacci et al., 2015; Qiu et al., 2016), as well as on how to apply such sense embeddings in downstream NLP tasks (Pilehvar et al., 2017). The question of combining multiple word embeddings is related to multi-modal and multi-view learning. For instance, combining visual features from convolutional neural networks with word embeddings has been examined (Kiela and Bottou, 2014; Lazaridou et al., 2015), see Baltruˇsaitis et al. (2018) for an overview. In multi-modal semantics, for instance, word-level embeddings from different modalities are often mixed via concatenation r = [αu, (1 − α)v] (Bruni et al., 2014). Here, we 1467 dynamically learn the weights to combine representations. Recently, related dynamic multi-modal fusion methods have also been explored (Wang et al., 2018; Kiros et al., 2018). There has also been work on unifying multi-view embeddings from different data sources (Luo et al., 2014). The usefulness of different embeddings as initialization has bee"
D18-1176,D15-1242,1,0.879536,"Missing"
D18-1176,P18-1085,0,0.0569833,"ngs is related to multi-modal and multi-view learning. For instance, combining visual features from convolutional neural networks with word embeddings has been examined (Kiela and Bottou, 2014; Lazaridou et al., 2015), see Baltruˇsaitis et al. (2018) for an overview. In multi-modal semantics, for instance, word-level embeddings from different modalities are often mixed via concatenation r = [αu, (1 − α)v] (Bruni et al., 2014). Here, we 1467 dynamically learn the weights to combine representations. Recently, related dynamic multi-modal fusion methods have also been explored (Wang et al., 2018; Kiros et al., 2018). There has also been work on unifying multi-view embeddings from different data sources (Luo et al., 2014). The usefulness of different embeddings as initialization has been explored (Kocmi and Bojar, 2017), and different architectures and hyperparameters have been extensively examined (Levy et al., 2015). Problems with evaluating word embeddings intrinsically are well known (Faruqui et al., 2016), and various alternatives for evaluating word embeddings in downstream tasks have been proposed (e.g., Tsvetkov et al., 2015; Schnabel et al., 2015; Ettinger et al., 2016). For more related work wit"
D18-1176,W17-7508,0,0.0217995,"et al., 2015), see Baltruˇsaitis et al. (2018) for an overview. In multi-modal semantics, for instance, word-level embeddings from different modalities are often mixed via concatenation r = [αu, (1 − α)v] (Bruni et al., 2014). Here, we 1467 dynamically learn the weights to combine representations. Recently, related dynamic multi-modal fusion methods have also been explored (Wang et al., 2018; Kiros et al., 2018). There has also been work on unifying multi-view embeddings from different data sources (Luo et al., 2014). The usefulness of different embeddings as initialization has been explored (Kocmi and Bojar, 2017), and different architectures and hyperparameters have been extensively examined (Levy et al., 2015). Problems with evaluating word embeddings intrinsically are well known (Faruqui et al., 2016), and various alternatives for evaluating word embeddings in downstream tasks have been proposed (e.g., Tsvetkov et al., 2015; Schnabel et al., 2015; Ettinger et al., 2016). For more related work with regard to word embeddings and their evaluation, see Bakarov (2017). Our work can be seen as an instance of the wellknown attention mechanism (Bahdanau et al., 2014), and its recent sentence-level incarnati"
D18-1176,J81-4005,0,0.677773,"Missing"
D18-1176,P16-1160,1,0.812507,"they are now ubiquitous in the field (Goldberg, 2016). A sprawling literature has emerged about what types of embeddings are most useful for which tasks. For instance, there has been extensive work on understanding what word embeddings learn (Levy and Goldberg, 2014b), evaluating their performance (Milajevs et al., 2014; Schnabel et al., 2015; Bakarov, 2017), specializing them for certain tasks (Maas et al., 2011; Faruqui et al., 2014; Kiela et al., 2015; Mrkˇsi´c et al., 2016; Vuli´c and Mrkˇsi´c, 2017), learning sub-word level representations (Wieting et al., 2016; Bojanowski et al., 2016; Lee et al., 2016), et cetera. One of the first steps in designing many NLP systems is selecting what kinds of word embeddings to use, with people often resorting to freely available pre-trained embeddings. While this is often a sensible thing to do, the usefulness of word embeddings for downstream tasks tends to be hard to predict, as downstream tasks can be poorly correlated with word-level benchmarks. An alternative is to try to combine the strengths of different word embeddings. Recent work in socalled “meta-embeddings”, which ensembles embedding sets, has been gaining traction (Yin and Sch¨utze, 2015; Boll"
D18-1176,P14-2050,0,0.597324,"distributional semantic models (Turney and Pantel, 2010; Erk, 2012; Clark, 2015) to deep learningbased word embeddings (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014; Bojanowski et al., 2016), word-level meaning representations have found applications in a wide variety of core NLP tasks, to the extent that they are now ubiquitous in the field (Goldberg, 2016). A sprawling literature has emerged about what types of embeddings are most useful for which tasks. For instance, there has been extensive work on understanding what word embeddings learn (Levy and Goldberg, 2014b), evaluating their performance (Milajevs et al., 2014; Schnabel et al., 2015; Bakarov, 2017), specializing them for certain tasks (Maas et al., 2011; Faruqui et al., 2014; Kiela et al., 2015; Mrkˇsi´c et al., 2016; Vuli´c and Mrkˇsi´c, 2017), learning sub-word level representations (Wieting et al., 2016; Bojanowski et al., 2016; Lee et al., 2016), et cetera. One of the first steps in designing many NLP systems is selecting what kinds of word embeddings to use, with people often resorting to freely available pre-trained embeddings. While this is often a sensible thing to do, the usefulness of"
D18-1176,N16-1162,1,0.898778,"Missing"
D18-1176,P15-1010,0,0.0313794,"eprocessing step, rather than learning them dynamically in a supervised setting, as we do here. Similarly to Peters et al. (2018), who proposed deep contextualized word representations derived from language models and which led to impressive performance on a variety of tasks, our method allows for contextualization, in this case of embedding set weights. There has also been work on learning multiple embeddings per word (Chen et al., 2014; Neelakantan et al., 2015; Vu and Parker, 2016), including a lot of work in sense embeddings where the senses of a word have their own individual embeddings (Iacobacci et al., 2015; Qiu et al., 2016), as well as on how to apply such sense embeddings in downstream NLP tasks (Pilehvar et al., 2017). The question of combining multiple word embeddings is related to multi-modal and multi-view learning. For instance, combining visual features from convolutional neural networks with word embeddings has been examined (Kiela and Bottou, 2014; Lazaridou et al., 2015), see Baltruˇsaitis et al. (2018) for an overview. In multi-modal semantics, for instance, word-level embeddings from different modalities are often mixed via concatenation r = [αu, (1 − α)v] (Bruni et al., 2014). Her"
D18-1176,Q15-1016,0,0.0455047,"word-level embeddings from different modalities are often mixed via concatenation r = [αu, (1 − α)v] (Bruni et al., 2014). Here, we 1467 dynamically learn the weights to combine representations. Recently, related dynamic multi-modal fusion methods have also been explored (Wang et al., 2018; Kiros et al., 2018). There has also been work on unifying multi-view embeddings from different data sources (Luo et al., 2014). The usefulness of different embeddings as initialization has been explored (Kocmi and Bojar, 2017), and different architectures and hyperparameters have been extensively examined (Levy et al., 2015). Problems with evaluating word embeddings intrinsically are well known (Faruqui et al., 2016), and various alternatives for evaluating word embeddings in downstream tasks have been proposed (e.g., Tsvetkov et al., 2015; Schnabel et al., 2015; Ettinger et al., 2016). For more related work with regard to word embeddings and their evaluation, see Bakarov (2017). Our work can be seen as an instance of the wellknown attention mechanism (Bahdanau et al., 2014), and its recent sentence-level incarnations of self-attention (Lin et al., 2017) and inner-attention (Cheng et al., 2016; Liu et al., 2016),"
D18-1176,N16-1018,0,0.0568473,"Missing"
D18-1176,E17-1038,0,0.0531809,"Missing"
D18-1176,P11-1015,0,0.277151,"eston, 2008; Mikolov et al., 2013; Pennington et al., 2014; Bojanowski et al., 2016), word-level meaning representations have found applications in a wide variety of core NLP tasks, to the extent that they are now ubiquitous in the field (Goldberg, 2016). A sprawling literature has emerged about what types of embeddings are most useful for which tasks. For instance, there has been extensive work on understanding what word embeddings learn (Levy and Goldberg, 2014b), evaluating their performance (Milajevs et al., 2014; Schnabel et al., 2015; Bakarov, 2017), specializing them for certain tasks (Maas et al., 2011; Faruqui et al., 2014; Kiela et al., 2015; Mrkˇsi´c et al., 2016; Vuli´c and Mrkˇsi´c, 2017), learning sub-word level representations (Wieting et al., 2016; Bojanowski et al., 2016; Lee et al., 2016), et cetera. One of the first steps in designing many NLP systems is selecting what kinds of word embeddings to use, with people often resorting to freely available pre-trained embeddings. While this is often a sensible thing to do, the usefulness of word embeddings for downstream tasks tends to be hard to predict, as downstream tasks can be poorly correlated with word-level benchmarks. An alterna"
D18-1176,K16-1006,0,0.0287886,"supervised embeddings with supervised ones for sentiment classification. Yang et al. (2016) and Miyamoto and Cho (2016) learn to combine word-level and character-level embeddings. Contextual representations have been used in neural machine translation as well, e.g. for learning contextual word vectors and applying them in other tasks (McCann et al., 2017) or for learning context-dependent representations to solve disambiguation problems in machine translation Choi et al. (2016). Neural tensor skip-gram models learn to combine word, topic and context embeddings (Liu et al., 2015); context2vec (Melamud et al., 2016) learns a more sophisticated context representation separately from target embeddings; and Li et al. (2016) learn word representations with distributed word representation with multi-contextual mixed embedding. Recent work in “meta-embeddings”, which ensembles embedding sets, has been gaining traction (Yin and Sch¨utze, 2015; Bollegala et al., 2017; Murom¨agi et al., 2017; Coates and Bollegala, 2018)—here, we show that the idea can be applied in context, and to sentence representations. Furthermore, these works obtain metaembeddings as a preprocessing step, rather than learning them dynamicall"
D18-1176,L18-1550,0,0.0314326,"ic representations in that setting: SNLI (Bowman et al., 2015) and the more recent MultiNLI (Williams et al., 2017). The SNLI dataset consists of 570k humangenerated English sentence pairs, manually labeled for entailment, contradiction and neutral. The MultiNLI dataset can be seen as an extension of SNLI: it contains 433k sentence pairs, taken from ten different genres (e.g. fiction, government text or spoken telephone conversations), with the same entailment labeling scheme. We train sentence encoders with dynamic metaembeddings using two well-known and often-used embedding types: FastText (Mikolov et al., 2018; Bojanowski et al., 2016) and GloVe (Pennington et al., 2014). Specifically, we make use of the 300-dimensional embeddings trained on a similar WebCrawl corpus, and compare three scenarios: when used individually, when naively concatenated or in the dynamic meta-embedding setting (unweighted, context-independent DME and contextualized CDME). We also compare our approach against other models in the same class—in this case, models that encode sentences individually and do not allow attention across the two sentences.1 We include InferSent (Conneau et al., 2017), which also makes use of a BiLSTM"
D18-1176,D14-1079,0,0.0318848,"Erk, 2012; Clark, 2015) to deep learningbased word embeddings (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014; Bojanowski et al., 2016), word-level meaning representations have found applications in a wide variety of core NLP tasks, to the extent that they are now ubiquitous in the field (Goldberg, 2016). A sprawling literature has emerged about what types of embeddings are most useful for which tasks. For instance, there has been extensive work on understanding what word embeddings learn (Levy and Goldberg, 2014b), evaluating their performance (Milajevs et al., 2014; Schnabel et al., 2015; Bakarov, 2017), specializing them for certain tasks (Maas et al., 2011; Faruqui et al., 2014; Kiela et al., 2015; Mrkˇsi´c et al., 2016; Vuli´c and Mrkˇsi´c, 2017), learning sub-word level representations (Wieting et al., 2016; Bojanowski et al., 2016; Lee et al., 2016), et cetera. One of the first steps in designing many NLP systems is selecting what kinds of word embeddings to use, with people often resorting to freely available pre-trained embeddings. While this is often a sensible thing to do, the usefulness of word embeddings for downstream tasks tends to be hard"
D18-1176,W17-0212,0,0.1399,"Missing"
D18-1176,D14-1113,0,0.0551116,"Missing"
D18-1176,W17-5308,0,0.0499894,"Missing"
D18-1176,D14-1162,0,0.0954351,"ta-embeddings, a simple yet effective method for the supervised learning of embedding ensembles, which leads to stateof-the-art performance within the same model class on a variety of tasks. We subsequently show how the technique can be used to shed new light on the usage of word embeddings in NLP systems. 1 Introduction It is no exaggeration to say that word embeddings have revolutionized NLP. From early distributional semantic models (Turney and Pantel, 2010; Erk, 2012; Clark, 2015) to deep learningbased word embeddings (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014; Bojanowski et al., 2016), word-level meaning representations have found applications in a wide variety of core NLP tasks, to the extent that they are now ubiquitous in the field (Goldberg, 2016). A sprawling literature has emerged about what types of embeddings are most useful for which tasks. For instance, there has been extensive work on understanding what word embeddings learn (Levy and Goldberg, 2014b), evaluating their performance (Milajevs et al., 2014; Schnabel et al., 2015; Bakarov, 2017), specializing them for certain tasks (Maas et al., 2011; Faruqui et al., 2014; Kiela et al., 201"
D18-1176,N18-1202,0,0.0764596,"arately from target embeddings; and Li et al. (2016) learn word representations with distributed word representation with multi-contextual mixed embedding. Recent work in “meta-embeddings”, which ensembles embedding sets, has been gaining traction (Yin and Sch¨utze, 2015; Bollegala et al., 2017; Murom¨agi et al., 2017; Coates and Bollegala, 2018)—here, we show that the idea can be applied in context, and to sentence representations. Furthermore, these works obtain metaembeddings as a preprocessing step, rather than learning them dynamically in a supervised setting, as we do here. Similarly to Peters et al. (2018), who proposed deep contextualized word representations derived from language models and which led to impressive performance on a variety of tasks, our method allows for contextualization, in this case of embedding set weights. There has also been work on learning multiple embeddings per word (Chen et al., 2014; Neelakantan et al., 2015; Vu and Parker, 2016), including a lot of work in sense embeddings where the senses of a word have their own individual embeddings (Iacobacci et al., 2015; Qiu et al., 2016), as well as on how to apply such sense embeddings in downstream NLP tasks (Pilehvar et"
D18-1176,P17-1170,0,0.0200153,"t al. (2018), who proposed deep contextualized word representations derived from language models and which led to impressive performance on a variety of tasks, our method allows for contextualization, in this case of embedding set weights. There has also been work on learning multiple embeddings per word (Chen et al., 2014; Neelakantan et al., 2015; Vu and Parker, 2016), including a lot of work in sense embeddings where the senses of a word have their own individual embeddings (Iacobacci et al., 2015; Qiu et al., 2016), as well as on how to apply such sense embeddings in downstream NLP tasks (Pilehvar et al., 2017). The question of combining multiple word embeddings is related to multi-modal and multi-view learning. For instance, combining visual features from convolutional neural networks with word embeddings has been examined (Kiela and Bottou, 2014; Lazaridou et al., 2015), see Baltruˇsaitis et al. (2018) for an overview. In multi-modal semantics, for instance, word-level embeddings from different modalities are often mixed via concatenation r = [αu, (1 − α)v] (Bruni et al., 2014). Here, we 1467 dynamically learn the weights to combine representations. Recently, related dynamic multi-modal fusion met"
D18-1176,D16-1018,0,0.0187376,"than learning them dynamically in a supervised setting, as we do here. Similarly to Peters et al. (2018), who proposed deep contextualized word representations derived from language models and which led to impressive performance on a variety of tasks, our method allows for contextualization, in this case of embedding set weights. There has also been work on learning multiple embeddings per word (Chen et al., 2014; Neelakantan et al., 2015; Vu and Parker, 2016), including a lot of work in sense embeddings where the senses of a word have their own individual embeddings (Iacobacci et al., 2015; Qiu et al., 2016), as well as on how to apply such sense embeddings in downstream NLP tasks (Pilehvar et al., 2017). The question of combining multiple word embeddings is related to multi-modal and multi-view learning. For instance, combining visual features from convolutional neural networks with word embeddings has been examined (Kiela and Bottou, 2014; Lazaridou et al., 2015), see Baltruˇsaitis et al. (2018) for an overview. In multi-modal semantics, for instance, word-level embeddings from different modalities are often mixed via concatenation r = [αu, (1 − α)v] (Bruni et al., 2014). Here, we 1467 dynamica"
D18-1176,D15-1036,0,0.0879763,"to deep learningbased word embeddings (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014; Bojanowski et al., 2016), word-level meaning representations have found applications in a wide variety of core NLP tasks, to the extent that they are now ubiquitous in the field (Goldberg, 2016). A sprawling literature has emerged about what types of embeddings are most useful for which tasks. For instance, there has been extensive work on understanding what word embeddings learn (Levy and Goldberg, 2014b), evaluating their performance (Milajevs et al., 2014; Schnabel et al., 2015; Bakarov, 2017), specializing them for certain tasks (Maas et al., 2011; Faruqui et al., 2014; Kiela et al., 2015; Mrkˇsi´c et al., 2016; Vuli´c and Mrkˇsi´c, 2017), learning sub-word level representations (Wieting et al., 2016; Bojanowski et al., 2016; Lee et al., 2016), et cetera. One of the first steps in designing many NLP systems is selecting what kinds of word embeddings to use, with people often resorting to freely available pre-trained embeddings. While this is often a sensible thing to do, the usefulness of word embeddings for downstream tasks tends to be hard to predict, as downstre"
D18-1176,D13-1170,0,0.0085803,"ce. Finally, we obtain results for using the six different embedding types (marked *), and show that adding in more embeddings increases performance further. To our knowledge, these numbers constitute the state of the art within the model class of single sentence encoders on these tasks. 5 1 Sentiment To showcase the general applicability of the proposed approach, we also apply it to a case where we have to classify a single sentence, namely, sentiment classification. Sentiment analysis and opinion mining have become important applications for NLP research. We evaluate on the binary SST task (Socher et al., 2013), consisting of 70k sentences with a corresponding binary (positive or negative) sentiment label. 5.1 Implementation Details We use 256-dimensional embedding projections, 512-dimensional BiLSTM encoders and an MLP with 512-dimensional hidden layer in the classifier. The initial learning rate is set to 0.0004 and dropped by a factor of 0.2 when dev accuracy stops improving, dropout to 0.5, and we use Adam for optimization. The loss is standard crossentropy. We calculate the mean accuracy and standard deviation based on ten random seeds. 5.2 Results Table 2 shows a similar pattern as we observed"
D18-1176,P15-1150,0,0.103114,"Missing"
D18-1176,D15-1243,0,0.0223349,"amic multi-modal fusion methods have also been explored (Wang et al., 2018; Kiros et al., 2018). There has also been work on unifying multi-view embeddings from different data sources (Luo et al., 2014). The usefulness of different embeddings as initialization has been explored (Kocmi and Bojar, 2017), and different architectures and hyperparameters have been extensively examined (Levy et al., 2015). Problems with evaluating word embeddings intrinsically are well known (Faruqui et al., 2016), and various alternatives for evaluating word embeddings in downstream tasks have been proposed (e.g., Tsvetkov et al., 2015; Schnabel et al., 2015; Ettinger et al., 2016). For more related work with regard to word embeddings and their evaluation, see Bakarov (2017). Our work can be seen as an instance of the wellknown attention mechanism (Bahdanau et al., 2014), and its recent sentence-level incarnations of self-attention (Lin et al., 2017) and inner-attention (Cheng et al., 2016; Liu et al., 2016), where the attention mechanism is applied within the same sentence instead of for aligning multiple sentences. Here, we learn (optionally contextualized) attention weights for different embedding sets and apply the tech"
D18-1176,N16-1151,0,0.0220506,"ow that the idea can be applied in context, and to sentence representations. Furthermore, these works obtain metaembeddings as a preprocessing step, rather than learning them dynamically in a supervised setting, as we do here. Similarly to Peters et al. (2018), who proposed deep contextualized word representations derived from language models and which led to impressive performance on a variety of tasks, our method allows for contextualization, in this case of embedding set weights. There has also been work on learning multiple embeddings per word (Chen et al., 2014; Neelakantan et al., 2015; Vu and Parker, 2016), including a lot of work in sense embeddings where the senses of a word have their own individual embeddings (Iacobacci et al., 2015; Qiu et al., 2016), as well as on how to apply such sense embeddings in downstream NLP tasks (Pilehvar et al., 2017). The question of combining multiple word embeddings is related to multi-modal and multi-view learning. For instance, combining visual features from convolutional neural networks with word embeddings has been examined (Kiela and Bottou, 2014; Lazaridou et al., 2015), see Baltruˇsaitis et al. (2018) for an overview. In multi-modal semantics, for ins"
D18-1176,J17-4004,1,0.875438,"Missing"
D18-1176,D16-1157,0,0.0189657,"e variety of core NLP tasks, to the extent that they are now ubiquitous in the field (Goldberg, 2016). A sprawling literature has emerged about what types of embeddings are most useful for which tasks. For instance, there has been extensive work on understanding what word embeddings learn (Levy and Goldberg, 2014b), evaluating their performance (Milajevs et al., 2014; Schnabel et al., 2015; Bakarov, 2017), specializing them for certain tasks (Maas et al., 2011; Faruqui et al., 2014; Kiela et al., 2015; Mrkˇsi´c et al., 2016; Vuli´c and Mrkˇsi´c, 2017), learning sub-word level representations (Wieting et al., 2016; Bojanowski et al., 2016; Lee et al., 2016), et cetera. One of the first steps in designing many NLP systems is selecting what kinds of word embeddings to use, with people often resorting to freely available pre-trained embeddings. While this is often a sensible thing to do, the usefulness of word embeddings for downstream tasks tends to be hard to predict, as downstream tasks can be poorly correlated with word-level benchmarks. An alternative is to try to combine the strengths of different word embeddings. Recent work in socalled “meta-embeddings”, which ensembles embedding sets, has been ga"
D18-1176,D17-1056,0,0.084221,"various genres. This allows us to inspect the applicability of source domain data for a specific genre. We train embeddings on three kinds of data: Wikipedia, the Toronto Books Corpus (Zhu et al., 2015) and the English OpenSubtitles4 . We examine the atten1472 4 http://opus.nlpl.eu/OpenSubtitles.php Figure 5: Multi-domain weights on MultiNLI. Model Levy LEAR SNLI CDME 0.33 0.67 85.3±.9 Model GloVe Refined SST CDME 0.59 0.41 89.0±.4 Table 4: Accuracy and learned weights on SNLI using LEAR (Vuli´c and Mrkˇsi´c, 2017) or SST using sentiment-refined embeddings using the specialization method from Yu et al. (2017). tion weights on the five genres in the in-domain (matched) set, consisting of fiction; transcriptions of spoken telephone conversations; government reports, speeches, letters and press releases; popular culture articles from the Slate Magazine archive; and travel guides. Figure 5 shows the average attention weights for the three embedding types over the five genres. We observe that Toronto Books, which consists of fiction, is very appropriate for the fiction genre, while Wikipedia is highly preferred for the travel genre, perhaps because it contains a lot of factual information about geograp"
D18-1398,P17-1042,0,0.109699,"data is limited (Koehn and Knowles, 2017). In general, there are two ways for handling the problem of low resource translation: (1) utilizing the resource of unlabeled monolingual data, and (2) sharing the knowledge between low- and high-resource language pairs. Many research efforts have been spent on incorporating the monolingual corpora into machine translation, such as multi-task learning (Gulcehre et al., 2015; Zhang and Zong, 2016), back-translation (Sennrich et al., 2015), dual learning (He et al., 2016) and unsupervised machine translation with monolingual corpora only for both sides (Artetxe et al., 2017b; Lample et al., 2017; Yang et al., 2018). For the second approach, prior researches have worked on methods to exploit the knowledge of auxiliary translations, or even auxiliary tasks. For instance, Cheng et al. (2016); Chen et al. (2017); Lee et al. (2017); Chen et al. (2018) investigate the use of a pivot to build a translation path between two languages even without any directed resource. The pivot can be a third language or even an image in multimodal domains. When pivots are not easy to obtain, Firat et al. (2016a); Lee et al. (2016); Johnson et al. (2016) have shown that the structure o"
D18-1398,J82-2005,0,0.558951,"Missing"
D18-1398,P17-1176,1,0.799414,"ource language pairs. Many research efforts have been spent on incorporating the monolingual corpora into machine translation, such as multi-task learning (Gulcehre et al., 2015; Zhang and Zong, 2016), back-translation (Sennrich et al., 2015), dual learning (He et al., 2016) and unsupervised machine translation with monolingual corpora only for both sides (Artetxe et al., 2017b; Lample et al., 2017; Yang et al., 2018). For the second approach, prior researches have worked on methods to exploit the knowledge of auxiliary translations, or even auxiliary tasks. For instance, Cheng et al. (2016); Chen et al. (2017); Lee et al. (2017); Chen et al. (2018) investigate the use of a pivot to build a translation path between two languages even without any directed resource. The pivot can be a third language or even an image in multimodal domains. When pivots are not easy to obtain, Firat et al. (2016a); Lee et al. (2016); Johnson et al. (2016) have shown that the structure of NMT is suitable for multilingual machine translation. Gu et al. (2018b) also showed that such a multilingual NMT system could improve the performance of low resource translation by using a universal lexical representation to share embedd"
D18-1398,W14-4012,1,0.658063,"Missing"
D18-1398,N16-1101,1,0.834667,"Koehn et al., 2003), for low-resource language pairs (see, e.g., Koehn and Knowles, 2017). In the past few years, various approaches have been proposed to address this issue. The first attempts at tackling this problem exploited the availability of monolingual corpora (Gulcehre * Equal contribution. et al., 2015; Sennrich et al., 2015; Zhang and Zong, 2016). It was later followed by approaches based on multilingual translation, in which the goal was to exploit knowledge from high-resource language pairs by training a single NMT system on a mix of high-resource and low-resource language pairs (Firat et al., 2016a,b; Lee et al., 2016; Johnson et al., 2016; Ha et al., 2016b). Its variant, transfer learning, was also proposed by Zoph et al. (2016), in which an NMT system is pretrained on a high-resource language pair before being finetuned on a target low-resource language pair. In this paper, we follow up on these latest approaches based on multilingual NMT and propose a meta-learning algorithm for low-resource neural machine translation. We start by arguing that the recently proposed model-agnostic meta-learning algorithm (MAML, Finn et al., 2017) could be applied to low-resource machine translation b"
D18-1398,D16-1026,1,0.832171,"Missing"
D18-1398,N18-1032,1,0.723871,"hen , Kyunghyun Cho and Victor O.K. Li † The University of Hong Kong New York University, CIFAR Azrieli Global Scholar † {jiataogu, wangyong, vli}@eee.hku.hk † yun.chencreek@gmail.com ‡ kyunghyun.cho@nyu.edu ‡ Abstract In this paper, we propose to extend the recently introduced model-agnostic meta-learning algorithm (MAML, Finn et al., 2017) for lowresource neural machine translation (NMT). We frame low-resource translation as a metalearning problem, and we learn to adapt to low-resource languages based on multilingual high-resource language tasks. We use the universal lexical representation (Gu et al., 2018b) to overcome the input-output mismatch across different languages. We evaluate the proposed meta-learning strategy using eighteen European languages (Bg, Cs, Da, De, El, Es, Et, Fr, Hu, It, Lt, Nl, Pl, Pt, Sk, Sl, Sv and Ru) as source tasks and five diverse languages (Ro, Lv, Fi, Tr and Ko) as target tasks. We show that the proposed approach significantly outperforms the multilingual, transfer learning based approach (Zoph et al., 2016) and enables us to train a competitive NMT system with only a fraction of training examples. For instance, the proposed approach can achieve as high as 22.04"
D18-1398,W17-3204,0,0.189611,"MT system with only a fraction of training examples. For instance, the proposed approach can achieve as high as 22.04 BLEU on Romanian-English WMT’16 by seeing only 16,000 translated words (⇠ 600 parallel sentences). 1 Introduction Despite the massive success brought by neural machine translation (NMT, Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017), it has been noticed that the vanilla NMT often lags behind conventional machine translation systems, such as statistical phrase-based translation systems (PBMT, Koehn et al., 2003), for low-resource language pairs (see, e.g., Koehn and Knowles, 2017). In the past few years, various approaches have been proposed to address this issue. The first attempts at tackling this problem exploited the availability of monolingual corpora (Gulcehre * Equal contribution. et al., 2015; Sennrich et al., 2015; Zhang and Zong, 2016). It was later followed by approaches based on multilingual translation, in which the goal was to exploit knowledge from high-resource language pairs by training a single NMT system on a mix of high-resource and low-resource language pairs (Firat et al., 2016a,b; Lee et al., 2016; Johnson et al., 2016; Ha et al., 2016b). Its var"
D18-1398,N03-1017,0,0.0323047,"roach (Zoph et al., 2016) and enables us to train a competitive NMT system with only a fraction of training examples. For instance, the proposed approach can achieve as high as 22.04 BLEU on Romanian-English WMT’16 by seeing only 16,000 translated words (⇠ 600 parallel sentences). 1 Introduction Despite the massive success brought by neural machine translation (NMT, Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017), it has been noticed that the vanilla NMT often lags behind conventional machine translation systems, such as statistical phrase-based translation systems (PBMT, Koehn et al., 2003), for low-resource language pairs (see, e.g., Koehn and Knowles, 2017). In the past few years, various approaches have been proposed to address this issue. The first attempts at tackling this problem exploited the availability of monolingual corpora (Gulcehre * Equal contribution. et al., 2015; Sennrich et al., 2015; Zhang and Zong, 2016). It was later followed by approaches based on multilingual translation, in which the goal was to exploit knowledge from high-resource language pairs by training a single NMT system on a mix of high-resource and low-resource language pairs (Firat et al., 2016a"
D18-1398,P16-1160,1,0.923942,"r low-resource language pairs (see, e.g., Koehn and Knowles, 2017). In the past few years, various approaches have been proposed to address this issue. The first attempts at tackling this problem exploited the availability of monolingual corpora (Gulcehre * Equal contribution. et al., 2015; Sennrich et al., 2015; Zhang and Zong, 2016). It was later followed by approaches based on multilingual translation, in which the goal was to exploit knowledge from high-resource language pairs by training a single NMT system on a mix of high-resource and low-resource language pairs (Firat et al., 2016a,b; Lee et al., 2016; Johnson et al., 2016; Ha et al., 2016b). Its variant, transfer learning, was also proposed by Zoph et al. (2016), in which an NMT system is pretrained on a high-resource language pair before being finetuned on a target low-resource language pair. In this paper, we follow up on these latest approaches based on multilingual NMT and propose a meta-learning algorithm for low-resource neural machine translation. We start by arguing that the recently proposed model-agnostic meta-learning algorithm (MAML, Finn et al., 2017) could be applied to low-resource machine translation by viewing language pa"
D18-1398,D16-1147,0,0.0139611,"ot apply to machine translation in general due to the vocabulary mismatch across different languages. In multilingual translation, this issue has been tackled by using a vocabulary of sub-words (Sennrich et al., 2015) or characters (Lee et al., 2016) shared across multiple languages. This surface-level sharing is however limited, as it cannot be applied to languages exhibiting distinct orthography (e.g., IndoEuroepan languages vs. Korean.) Universal Lexical Representation (ULR) We tackle this issue by dynamically building a vocabulary specific to each language using a keyvalue memory network (Miller et al., 2016; Gulcehre et al., 2018), as was done successfully for low-resource machine translation recently by Gu et al. (2018b). We start with multilingual word embedding matrices ✏kquery 2 R|Vk |⇥d pretrained on large monolingual corpora, where Vk is the vocabulary of the k-th language. These embedding vectors can be obtained with small dictionaries of seed word pairs (Artetxe et al., 2017a; Smith et al., 2017) or in a fully unsupervised manner (Zhang et al., 2017; Conneau et al., 2018). We take one of these languages k 0 to build universal lexical representation consisting of a universal embedding mat"
D18-1398,P16-1009,0,0.154622,"Missing"
D18-1398,W16-2323,0,0.023515,". Validation We pick either Ro-En or Lv-En as a validation set for meta-learning and test the generalization capability on the remaining target tasks. This allows us to study the strict form of metalearning, in which target tasks are unknown during both training and model selection. Preprocessing and ULR Initialization As described in §3.3, we initialize the query embedding vectors ✏kquery of all the languages. For each language, we use the monolingual corpora built from Wikipedia7 and the parallel corpus. The concatenated corpus is first tokenized and segmented using byte-pair encoding (BPE, Sennrich et al., 2016), resulting in 40, 000 subwords for each language. We then estimate word vectors using fastText (Bojanowski et al., 2016) and align them across all the languages in an unsupervised way 6 A subsample of approximately 2M pairs from WMT’17. We use the most recent Wikipedia dump (2018.5) from https://dumps.wikimedia.org/backup-index.html. using MUSE (Conneau et al., 2018) to get multilingual word vectors. We use the multilingual word vectors of the 20,000 most frequent words in English to form the universal embedding matrix ✏u . 4.2 Model and Learning Model We utilize the recently proposed Transfo"
D18-1398,P18-1005,0,0.0189134,"n general, there are two ways for handling the problem of low resource translation: (1) utilizing the resource of unlabeled monolingual data, and (2) sharing the knowledge between low- and high-resource language pairs. Many research efforts have been spent on incorporating the monolingual corpora into machine translation, such as multi-task learning (Gulcehre et al., 2015; Zhang and Zong, 2016), back-translation (Sennrich et al., 2015), dual learning (He et al., 2016) and unsupervised machine translation with monolingual corpora only for both sides (Artetxe et al., 2017b; Lample et al., 2017; Yang et al., 2018). For the second approach, prior researches have worked on methods to exploit the knowledge of auxiliary translations, or even auxiliary tasks. For instance, Cheng et al. (2016); Chen et al. (2017); Lee et al. (2017); Chen et al. (2018) investigate the use of a pivot to build a translation path between two languages even without any directed resource. The pivot can be a third language or even an image in multimodal domains. When pivots are not easy to obtain, Firat et al. (2016a); Lee et al. (2016); Johnson et al. (2016) have shown that the structure of NMT is suitable for multilingual machine"
D18-1398,D16-1160,0,0.138447,"ural machine translation (NMT, Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017), it has been noticed that the vanilla NMT often lags behind conventional machine translation systems, such as statistical phrase-based translation systems (PBMT, Koehn et al., 2003), for low-resource language pairs (see, e.g., Koehn and Knowles, 2017). In the past few years, various approaches have been proposed to address this issue. The first attempts at tackling this problem exploited the availability of monolingual corpora (Gulcehre * Equal contribution. et al., 2015; Sennrich et al., 2015; Zhang and Zong, 2016). It was later followed by approaches based on multilingual translation, in which the goal was to exploit knowledge from high-resource language pairs by training a single NMT system on a mix of high-resource and low-resource language pairs (Firat et al., 2016a,b; Lee et al., 2016; Johnson et al., 2016; Ha et al., 2016b). Its variant, transfer learning, was also proposed by Zoph et al. (2016), in which an NMT system is pretrained on a high-resource language pair before being finetuned on a target low-resource language pair. In this paper, we follow up on these latest approaches based on multili"
D18-1398,D17-1207,0,0.0196912,"l Representation (ULR) We tackle this issue by dynamically building a vocabulary specific to each language using a keyvalue memory network (Miller et al., 2016; Gulcehre et al., 2018), as was done successfully for low-resource machine translation recently by Gu et al. (2018b). We start with multilingual word embedding matrices ✏kquery 2 R|Vk |⇥d pretrained on large monolingual corpora, where Vk is the vocabulary of the k-th language. These embedding vectors can be obtained with small dictionaries of seed word pairs (Artetxe et al., 2017a; Smith et al., 2017) or in a fully unsupervised manner (Zhang et al., 2017; Conneau et al., 2018). We take one of these languages k 0 to build universal lexical representation consisting of a universal embedding matrix ✏u 2 RM ⇥d and a corresponding key matrix ✏key 2 RM ⇥d , where M < |Vk0 |. Both ✏kquery and ✏key are fixed during meta-learning. We then compute the language-specific embedding of token x from the language k as the convex sum of the universal embedding vectors by ✏0 [x] = M X ↵i ✏u [i], i=1 1 &gt; k where ↵i / exp ⌧ ✏key [i] A✏query [x] and ⌧ is set to 0.05. This approach allows us to handle languages with different vocabularies using a fixed number of s"
D18-1398,D16-1163,0,0.448325,"arning problem, and we learn to adapt to low-resource languages based on multilingual high-resource language tasks. We use the universal lexical representation (Gu et al., 2018b) to overcome the input-output mismatch across different languages. We evaluate the proposed meta-learning strategy using eighteen European languages (Bg, Cs, Da, De, El, Es, Et, Fr, Hu, It, Lt, Nl, Pl, Pt, Sk, Sl, Sv and Ru) as source tasks and five diverse languages (Ro, Lv, Fi, Tr and Ko) as target tasks. We show that the proposed approach significantly outperforms the multilingual, transfer learning based approach (Zoph et al., 2016) and enables us to train a competitive NMT system with only a fraction of training examples. For instance, the proposed approach can achieve as high as 22.04 BLEU on Romanian-English WMT’16 by seeing only 16,000 translated words (⇠ 600 parallel sentences). 1 Introduction Despite the massive success brought by neural machine translation (NMT, Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017), it has been noticed that the vanilla NMT often lags behind conventional machine translation systems, such as statistical phrase-based translation systems (PBMT, Koehn et al., 2003), for"
D18-1527,D16-1057,0,0.0890313,"nal word embedding distributions. Our model allows for (a) hypothesis tests about the meanings of terms, (b) assessments as to whether a word is near or far from another conditioned on different covariate values, and (c) assessments as to whether estimated differences are statistically significant. 1 2 Introduction Whether a word’s meaning varies across contexts has become a major focus of NLP, linguistics, and social science research in recent years. For example, since the early 20th century, the word “gay” has evolved from describing an emotion to being more aligned with sexual orientation (Hamilton et al., 2016b). Popular word embedding techniques (e.g., Mikolov et al., 2013a; Pennington et al., 2014) have proven useful for analyzing language evolution. But to use these models for such research, scholars often divide a corpus into distinct training sets (e.g., train independent language models on different decades of text) and compare model output across specifications in an ad hoc way (Garg et al., 2018). Such splitting inhibits many within- and across-word comparisons, since embeddings are only comparable within a given model. Additionally, most methods ignore the variance of words, mechanically t"
D18-1527,D16-1229,0,0.0627599,"nal word embedding distributions. Our model allows for (a) hypothesis tests about the meanings of terms, (b) assessments as to whether a word is near or far from another conditioned on different covariate values, and (c) assessments as to whether estimated differences are statistically significant. 1 2 Introduction Whether a word’s meaning varies across contexts has become a major focus of NLP, linguistics, and social science research in recent years. For example, since the early 20th century, the word “gay” has evolved from describing an emotion to being more aligned with sexual orientation (Hamilton et al., 2016b). Popular word embedding techniques (e.g., Mikolov et al., 2013a; Pennington et al., 2014) have proven useful for analyzing language evolution. But to use these models for such research, scholars often divide a corpus into distinct training sets (e.g., train independent language models on different decades of text) and compare model output across specifications in an ad hoc way (Garg et al., 2018). Such splitting inhibits many within- and across-word comparisons, since embeddings are only comparable within a given model. Additionally, most methods ignore the variance of words, mechanically t"
D18-1527,P16-1141,0,0.343032,"nal word embedding distributions. Our model allows for (a) hypothesis tests about the meanings of terms, (b) assessments as to whether a word is near or far from another conditioned on different covariate values, and (c) assessments as to whether estimated differences are statistically significant. 1 2 Introduction Whether a word’s meaning varies across contexts has become a major focus of NLP, linguistics, and social science research in recent years. For example, since the early 20th century, the word “gay” has evolved from describing an emotion to being more aligned with sexual orientation (Hamilton et al., 2016b). Popular word embedding techniques (e.g., Mikolov et al., 2013a; Pennington et al., 2014) have proven useful for analyzing language evolution. But to use these models for such research, scholars often divide a corpus into distinct training sets (e.g., train independent language models on different decades of text) and compare model output across specifications in an ad hoc way (Garg et al., 2018). Such splitting inhibits many within- and across-word comparisons, since embeddings are only comparable within a given model. Additionally, most methods ignore the variance of words, mechanically t"
D18-1527,D14-1162,0,0.0866388,"nings of terms, (b) assessments as to whether a word is near or far from another conditioned on different covariate values, and (c) assessments as to whether estimated differences are statistically significant. 1 2 Introduction Whether a word’s meaning varies across contexts has become a major focus of NLP, linguistics, and social science research in recent years. For example, since the early 20th century, the word “gay” has evolved from describing an emotion to being more aligned with sexual orientation (Hamilton et al., 2016b). Popular word embedding techniques (e.g., Mikolov et al., 2013a; Pennington et al., 2014) have proven useful for analyzing language evolution. But to use these models for such research, scholars often divide a corpus into distinct training sets (e.g., train independent language models on different decades of text) and compare model output across specifications in an ad hoc way (Garg et al., 2018). Such splitting inhibits many within- and across-word comparisons, since embeddings are only comparable within a given model. Additionally, most methods ignore the variance of words, mechanically treating words equally regardless of the volatility, or uncertainty, in their meanings. If on"
D18-1544,P06-1109,0,0.348752,"Accuracy columns represent the fraction of ground truth constituents of a given type that correspond to constituents in the model parses. Italics mark results that are worse than the random baseline. Underlining marks the best results from our runs. Results with RL-SPINN and ST-Gumbel are from Williams et al. (2018a), and are evaluated on the full WSJ. We run the model with 5 different random seeds to calculate the average F1. We use the model with the best F1 score to report ADJP, NP, PP, and INTJ. WSJ10 baselines are from Klein and Manning (2002, CCM), Klein and Manning (2005, DMV+CCM), and Bod (2006, UML-DOP). As the WSJ10 baselines are trained using additional information such as POS tags and dependency parser, they are not strictly comparable with the latent tree learning results. of WSJ) splits. To compare PRPN to the models studied in Williams et al. (2018a), we also retrain it on AllNLI. As the MultiNLI test set is not publicly available, we follow Williams et al. (2018a) and use the development set for testing. The parsing evaluation code in the original codebase does not support PRPN-LM, and we modify it in our experiments only to add this support. For early stopping, we remove 10"
D18-1544,D15-1075,1,0.844422,"ure and tend to identify noun phrases correctly. ing decisions greedily and with no access to any words to the right of the point where each parsing decision must be made (Collins and Roark, 2004); (2) As RNN language models are known to be insufficient for capturing syntax-sensitive dependencies (Linzen et al., 2016), language modeling as the downstream task may not be well-suited to latent tree learning. In this replication we train PRPN on two corpora: The full WSJ, a staple in work on grammar induction, and AllNLI, the concatenation of the Stanford Natural Language Inference Corpus (SNLI; Bowman et al., 2015) and the Multi-Genre NLI Corpus (MultiNLI; Williams et al., 2018b), which is used in other latent tree learning work for its non-syntactic classification labels for the task of textual entailment, and which we include for comparison. We then evaluate the constituency trees produced by these models on the WSJ test set, full WSJ10,1 and the MultiNLI development set. Our results indicate that PRPN-LM achieves better parsing performance than PRPN-UP on both WSJ and WSJ10 even though PRPN-UP was tuned—at least to some extent—for parsing. Surprisingly, a PRPN-LM model trained on the large out-of-dom"
D18-1544,P04-1015,0,0.214201,"rist offices . The entire Minoan civilization was destroyed by a volcanic eruption . There ’s nothing worth seeing in the tourist offices . The entire Minoan civilization was destroyed by a volcanic eruption . Figure 1: Left Parses from PRPN-LM trained on AllNLI. Right Parses from PRPN-UP trained on AllNLI (stopping criterion: parsing). We can observe that both sets of parses tend to have roughly reasonable high-level structure and tend to identify noun phrases correctly. ing decisions greedily and with no access to any words to the right of the point where each parsing decision must be made (Collins and Roark, 2004); (2) As RNN language models are known to be insufficient for capturing syntax-sensitive dependencies (Linzen et al., 2016), language modeling as the downstream task may not be well-suited to latent tree learning. In this replication we train PRPN on two corpora: The full WSJ, a staple in work on grammar induction, and AllNLI, the concatenation of the Stanford Natural Language Inference Corpus (SNLI; Bowman et al., 2015) and the Multi-Genre NLI Corpus (MultiNLI; Williams et al., 2018b), which is used in other latent tree learning work for its non-syntactic classification labels for the task of"
D18-1544,N18-1101,1,0.809114,"greedily and with no access to any words to the right of the point where each parsing decision must be made (Collins and Roark, 2004); (2) As RNN language models are known to be insufficient for capturing syntax-sensitive dependencies (Linzen et al., 2016), language modeling as the downstream task may not be well-suited to latent tree learning. In this replication we train PRPN on two corpora: The full WSJ, a staple in work on grammar induction, and AllNLI, the concatenation of the Stanford Natural Language Inference Corpus (SNLI; Bowman et al., 2015) and the Multi-Genre NLI Corpus (MultiNLI; Williams et al., 2018b), which is used in other latent tree learning work for its non-syntactic classification labels for the task of textual entailment, and which we include for comparison. We then evaluate the constituency trees produced by these models on the WSJ test set, full WSJ10,1 and the MultiNLI development set. Our results indicate that PRPN-LM achieves better parsing performance than PRPN-UP on both WSJ and WSJ10 even though PRPN-UP was tuned—at least to some extent—for parsing. Surprisingly, a PRPN-LM model trained on the large out-of-domain AllNLI dataset achieves the best parsing performance on WSJ"
D18-1544,P02-1017,0,0.317857,"e results of this work are robust: All variants of the model under study outperform all latent tree learning baselines, and perform competitively with symbolic grammar induction systems. We find that this model represents the first empirical success for latent tree learning, and that neural network language modeling warrants further study as a setting for grammar induction. 1 Introduction and Background Work on grammar induction attempts to find methods for syntactic parsing that do not require expensive and difficult-to-design expertlabeled treebanks for training (Charniak and Carroll, 1992; Klein and Manning, 2002; Smith and Eisner, 2005). Recent work on latent tree learning offers a new family of approaches to the problem (Yogatama et al., 2017; Maillard et al., 2017; Choi et al., 2018). Latent tree learning models attempt to induce syntactic structure using the supervision from a downstream NLP task such as textual entailment. Though these models tend to show good task performance, they are often not evaluated using standard parsing metrics, and Williams et al. 3 (2018a) report that the parses they produce tend to be no better than random trees in a standard evaluation on the full Wall Street Journal"
D18-1544,Q16-1037,0,0.0789784,"st offices . The entire Minoan civilization was destroyed by a volcanic eruption . Figure 1: Left Parses from PRPN-LM trained on AllNLI. Right Parses from PRPN-UP trained on AllNLI (stopping criterion: parsing). We can observe that both sets of parses tend to have roughly reasonable high-level structure and tend to identify noun phrases correctly. ing decisions greedily and with no access to any words to the right of the point where each parsing decision must be made (Collins and Roark, 2004); (2) As RNN language models are known to be insufficient for capturing syntax-sensitive dependencies (Linzen et al., 2016), language modeling as the downstream task may not be well-suited to latent tree learning. In this replication we train PRPN on two corpora: The full WSJ, a staple in work on grammar induction, and AllNLI, the concatenation of the Stanford Natural Language Inference Corpus (SNLI; Bowman et al., 2015) and the Multi-Genre NLI Corpus (MultiNLI; Williams et al., 2018b), which is used in other latent tree learning work for its non-syntactic classification labels for the task of textual entailment, and which we include for comparison. We then evaluate the constituency trees produced by these models"
D18-1544,J93-2004,0,0.0624137,"rk on latent tree learning offers a new family of approaches to the problem (Yogatama et al., 2017; Maillard et al., 2017; Choi et al., 2018). Latent tree learning models attempt to induce syntactic structure using the supervision from a downstream NLP task such as textual entailment. Though these models tend to show good task performance, they are often not evaluated using standard parsing metrics, and Williams et al. 3 (2018a) report that the parses they produce tend to be no better than random trees in a standard evaluation on the full Wall Street Journal section of the Penn Treebank (WSJ; Marcus et al., 1993). This paper addresses the Parsing-ReadingPredict Network (PRPN; Shen et al., 2018), which was recently published at ICLR, and which reports near-state-of-the-art results on language modeling and strong results on grammar induction, a first for latent tree models (though they do not use that term). PRPN is built around a substantially novel architecture, and uses convolutional networks with a form of structured attention (Kim et al., 2017) rather than recursive neural networks (Goller and Kuchler, 1996; Socher et al., 2011) to evaluate and learn trees while performing straightforward backpropa"
D18-1544,Q18-1019,1,0.714537,"New York University 60 Fifth Avenue New York, NY 10011 Kyunghyun Cho1,2 CIFAR Global Scholar kyunghyun.cho@nyu.edu Samuel R. Bowman1,2,3 bowman@nyu.edu Dept. of Computer Science New York University 60 Fifth Avenue New York, NY 10011 Dept. of Linguistics New York University 10 Washington Place New York, NY 10003 2 Abstract A substantial thread of recent work on latent tree learning has attempted to develop neural network models with parse-valued latent variables and train them on non-parsing tasks, in the hope of having them discover interpretable tree structure. In a recent paper, Shen et al. (2018) introduce such a model and report nearstate-of-the-art results on the target task of language modeling, and the first strong latent tree learning result on constituency parsing. In an attempt to reproduce these results, we discover issues that make the original results hard to trust, including tuning and even training on what is effectively the test set. Here, we attempt to reproduce these results in a fair experiment and to extend them to two new datasets. We find that the results of this work are robust: All variants of the model under study outperform all latent tree learning baselines, an"
D19-1244,N19-1264,0,0.0485276,"Missing"
D19-1244,P17-1020,0,0.0218616,"scussed in §4.1 on (dis)similarities between patterns learned by humans and neural networks. Evidence Extraction Various papers have explored the related problem of extracting evidence or summaries to aid downstream QA. Wang et al. (2018a) concurrently introduced a neural model that extracts evidence specifically for the correct answer, as an intermediate step in a QA pipeline. Prior work uses similar methods to explain what a specific model has learned (Lei et al., 2016; Li et al., 2016; Yu et al., 2019). Others extract evidence to improve downstream QA efficiency over large amounts of text (Choi et al., 2017; Kratzwald and Feuerriegel, 2019; Wang et al., 2018b). More broadly, extracting evidence can facilitate fact verification (Thorne et al., 2018) and debate.2 2 IBM Project Debater: www.research.ibm.com/ artificial-intelligence/project-debater 2409 Generic Summarization In contrast, various papers focus primarily on summarization rather than QA, using downstream QA accuracy only as a reward to optimize generic (question-agnostic) summarization models (Arumae and Liu, 2018, 2019; Eyal et al., 2019). Debate Evidence extraction can be viewed as a form of debate, in which multiple agents support di"
D19-1244,N19-1423,0,0.0404619,"rity between two using Adam (Kingma and Ba, 2015) on one loss texts (Perone et al., 2018). Using this funcfrom Table 1. For t &gt; 1, we find it effective to tion, we define a model class that selects the ansimply predict the judge model at t = 1 and use swer most similar to the input passage context: this distribution for all time steps during inference. L(i) = fastText(S, A(i)). This trick speeds up training by enabling us to precompute prediction targets using the judge model, BERT L(i) is computed using the multipleinstead of querying it constantly during training. choice adaptation of BERT (Devlin et al., 2019; Radford et al., 2018; Si, 2019), a pre-trained We use BERTBASE for all learned agents. transformer network (Vaswani et al., 2017). We Learned agents predict the BERTBASE judge, as fine-tune all BERT parameters during trainit is more efficient to compute than BERTLARGE . ing. This model predicts L(i) using a trainEach agent AGENT(i) is assigned the answer A(i) able vector v and BERT’s first token embedding: that it should support. We train one learned agent L(i) = v &gt; · BERT([S; Q; A(i)]). to find evidence for an arbitrary answer i. We We experiment with both the BERTBASE model condition AGEN"
D19-1244,N19-1395,0,0.043278,"Missing"
D19-1244,W18-2501,0,0.0315432,"Missing"
D19-1244,D18-1316,0,0.0260321,"Missing"
D19-1244,N18-2017,0,0.0434092,"Missing"
D19-1244,P18-3015,0,0.0467999,"Missing"
D19-1244,D17-1215,0,0.052598,"Missing"
D19-1244,E17-2068,0,0.093309,"Missing"
D19-1244,D16-1011,0,0.0337657,"duction There is great value in understanding the fundamental nature of a question (Chalmers, 2015). Distilling the core of an issue, however, is timeconsuming. Finding the correct answer to a given question may require reading large volumes of text or understanding complex arguments. Here, we examine if we can automatically discover the underlying properties of problems such as question answering by examining how machine learning models learn to solve that task. We examine this question in the context of passage-based question-answering (QA). Inspired by work in interpreting neural networks (Lei et al., 2016), we have agents find a subset of the passage (i.e., supporting evidence) that maximizes a QA model’s probability of a particular answer. Each agent (one agent per answer) finds the sentences that a QA model regards as strong evidence for its answer, using either exhaustive search or learned prediction. Figure 1 shows an example. Figure 1: Evidence agents quote sentences from the passage to convince a question-answering judge model of an answer. To examine to what extent evidence is general and independent of the model, we evaluate if humans and other models find selected evidence to be valid"
D19-1244,D17-2014,1,0.848518,"acy. However, each judge model’s accuracy is useful to know for analysis purposes. Table 2 shows model accuracies, which cover a broad range. BERT models significantly outperform word-based baselines (TFIDF and fastText), and BERTLARGE achieves the best overall accuracy. No model achieves the estimated human ceiling for either RACE (Lai et al., 2017) or DREAM (Sun et al., 2019). Our code is available at https://github. com/ethanjperez/convince. We build off AllenNLP (Gardner et al., 2018) using PyTorch (Paszke et al., 2017). For all human evaluations, we use Amazon Mechanical Turk via ParlAI (Miller et al., 2017). Appendix B describes preprocessing and training details. 4 4.1 Agents Select General Evidence Human Evaluation of Evidence Would evidence that convinces a model also be valid evidence to humans? On one hand, there is ample work suggesting that neural networks can learn similar patterns as humans do. Convolutional networks trained on ImageNet share similarities with the human visual cortex (Cadieu et al., 2014). In machine translation, attention learns to align foreign words with their native counterparts (Bahdanau et al., 2015). On the other hand, neural networks often do not behave as human"
D19-1244,P18-1079,0,0.0321551,"Missing"
D19-1244,Q19-1014,0,0.057546,"convince the judge model when supporting the correct answer (one answer per question). 3.2 Training and Evaluating Models Our setup is not directly comparable to standard QA setups, as we aim to evaluate evidence rather than raw QA accuracy. However, each judge model’s accuracy is useful to know for analysis purposes. Table 2 shows model accuracies, which cover a broad range. BERT models significantly outperform word-based baselines (TFIDF and fastText), and BERTLARGE achieves the best overall accuracy. No model achieves the estimated human ceiling for either RACE (Lai et al., 2017) or DREAM (Sun et al., 2019). Our code is available at https://github. com/ethanjperez/convince. We build off AllenNLP (Gardner et al., 2018) using PyTorch (Paszke et al., 2017). For all human evaluations, we use Amazon Mechanical Turk via ParlAI (Miller et al., 2017). Appendix B describes preprocessing and training details. 4 4.1 Agents Select General Evidence Human Evaluation of Evidence Would evidence that convinces a model also be valid evidence to humans? On one hand, there is ample work suggesting that neural networks can learn similar patterns as humans do. Convolutional networks trained on ImageNet share similari"
D19-1244,N18-1074,0,0.0201572,"the related problem of extracting evidence or summaries to aid downstream QA. Wang et al. (2018a) concurrently introduced a neural model that extracts evidence specifically for the correct answer, as an intermediate step in a QA pipeline. Prior work uses similar methods to explain what a specific model has learned (Lei et al., 2016; Li et al., 2016; Yu et al., 2019). Others extract evidence to improve downstream QA efficiency over large amounts of text (Choi et al., 2017; Kratzwald and Feuerriegel, 2019; Wang et al., 2018b). More broadly, extracting evidence can facilitate fact verification (Thorne et al., 2018) and debate.2 2 IBM Project Debater: www.research.ibm.com/ artificial-intelligence/project-debater 2409 Generic Summarization In contrast, various papers focus primarily on summarization rather than QA, using downstream QA accuracy only as a reward to optimize generic (question-agnostic) summarization models (Arumae and Liu, 2018, 2019; Eyal et al., 2019). Debate Evidence extraction can be viewed as a form of debate, in which multiple agents support different stances (Irving et al., 2018; Irving and Askell, 2019). Chen et al. (2018) show that evidence-based debate improves the accuracy of crow"
D19-1244,D16-1264,0,0.115473,"Missing"
D19-1329,P17-1183,0,0.0266486,", while staying as close to the original pronunciation as possible. Unlike for translation, focus lies on the sound; the target language meaning is usually ignored. Data. For our transliteration experiments, we follow Upadhyay et al. (2018). We experiment on datasets from the Named Entities Workshop 2015 (Duan et al., 2015) in Hindi, Kannada, Bengali, Tamil, and Hebrew. For this task, all languages are both development and target languages. Model. The last featured model is an LSTM sequence-to-sequence model similar to that by Bahdanau et al. (2015), except for using hard monotonic attention (Aharoni and Goldberg, 2017). It attends to a single character at a time, and attention moves monotonically over the input. We take hyperparameters and code from Upadhyay et al. (2018).5 Early stopping is done by training for 20 epochs and applying the best model regarding development accuracy to the test data. 4.4 Experimental Setup We run all experiments using the implementations from previous work or OpenNMT as described above. Existing code is only modified where necessary. Most importantly, we add storing of the DevLang model during the main training phase. 5 Results Development sets vs. development languages. We ar"
D19-1329,P17-1031,0,0.0565016,"to compare between tasks, we further limit this study to sequence-to-sequence tasks. 4.1 Historical Text Normalization (NORM) Task. The goal of historical text normalization is to convert old texts into a form that conforms with contemporary spelling conventions. Historical text normalization is a specific case of the general task of text normalization, which additionally encompasses, e.g., correction of spelling mistakes or normalization of social media text. Data. We experiment on the ten datasets from Bollmann et al. (2018), which represent eight different languages: German (two datasets; Bollmann et al., 2017; Odebrecht et al., 2017); English, Hungarian, Icelandic, and Swedish (Pettersson, 2016); Slovene (two datasets; Ljubeˇsic et al., 2016); and Spanish and Portuguese (Vaamonde, 2015). We treat the two datasets for German and Slovene as different languages. All languages serve both as development languages for all other languages and as target languages. Model. Our model for this task is an LSTM (Hochreiter and Schmidhuber, 1997) encoderdecoder model with attention (Bahdanau et al., 2015). Both encoder and decoder have a single hidden layer. We use the default model in OpenNMT (Klein et al., 201"
D19-1329,W18-3403,0,0.284664,"nd multi-task learning (Caruana, 1997; Ruder, 2017; Wang et al., 2019) approaches, neural models are showing promising results on various natural language processing (NLP) tasks also in lowresource or few-shot settings (Johnson et al., 2017; Kann et al., 2017; Yu et al., 2018). Often, the high-resource experimental setup and training procedure are kept unchanged, and the size of the original training set is reduced to simulate limited data. This leads to settings where validation examples may outnumber training examples. Table 1 shows such cases for the tasks of historical text normalization (Bollmann et al., 2018), morphological segmentation (Kann et al., 2018), morphological inflection (Makarov and Clematide, 2018; Sharma et al., 2018), argument component identification (Schulz et al., 2018), and transliteration (Upadhyay et al., 2018). 3342 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 3342–3349, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics However, in a real-world setting with limited resources, it is unlikely that such a development set"
D19-1329,K18-3001,1,0.837587,"ding development accuracy is applied to the test set. 4.2 Morphological Inflection (MORPH) Task. Morphological inflection consists of mapping the canonical form of a word, the lemma, to an indicated inflected form. This task gets very complex for morphologically rich languages, where a single lemma can have hundreds or thousand of inflected forms. Recently, morphological inflection has frequently been cast as a sequenceto-sequence task, mapping the characters of the input word together with the morphological features specifying the target to the characters of the corresponding inflected form (Cotterell et al., 2018). Data. We experiment on the datasets released for a 2018 shared task (Cotterell et al., 2018), which cover 103 languages and feature an explicit low-resource setting. We randomly choose ten development languages: Armenian, Basque, Galician, Georgian, Greenlandic, Icelandic, Karbadian, Kannada, Latin, and Lithuanian. Model. For MORPH, we experiment with a pointer-generator network architecture (Gu et al., 2016; See et al., 2017). This is a sequence-tosequence model similar to that for NORM, but employs separate encoders for characters and features. It is further equipped with a copy mechanism:"
D19-1329,P07-1033,0,0.202739,"Missing"
D19-1329,N19-1423,0,0.0128989,"er, since T is finite, overfitting the training set might lead to poor generalization performance. One way to avoid fitting Equation 1 too closely is early stopping: a separate development or validation set is used to end training as soon as the loss on the development set LD (θ) starts increasing or model performance on the development set D starts decreasing. The best set of parameters θ is used in the final model. This works well when large amounts of data are available to create training, development and test splits. Recently, however, with the success of pretraining (Peters et al., 2018; Devlin et al., 2019) and multi-task learning (Caruana, 1997; Ruder, 2017; Wang et al., 2019) approaches, neural models are showing promising results on various natural language processing (NLP) tasks also in lowresource or few-shot settings (Johnson et al., 2017; Kann et al., 2017; Yu et al., 2018). Often, the high-resource experimental setup and training procedure are kept unchanged, and the size of the original training set is reduced to simulate limited data. This leads to settings where validation examples may outnumber training examples. Table 1 shows such cases for the tasks of historical text normalization"
D19-1329,P13-1057,0,0.0892694,"Oliver et al. (2018) investigate how to evaluate semi-supervised training algorithms in a realistic way; they differ from us in that they focus exclusively on semi-supervised learning (SSL) algorithms, and do not consider NLP explicitly. However, in line with our conclusion, they report that recent practices for evaluating SSL techniques do not address the question of the algorithms’ real-word applicability in a satisfying way. In NLP, several earlier works have explicitly investigated real-world low-resource settings as opposed to artificial proxy settings, e.g., for part-of-speech tagging (Garrette et al., 2013) or machine translation (Irvine and Callison-Burch, 2013). While those mostly focus on real data-poor languages, we explicitly investigate the effect of the common practice to assume a relatively large development set for early stopping in the low-resource setting. Low-resource settings in NLP. Research in the area of neural methods for low-resource NLP has gained popularity in recent years, with a dedicated Experimental Design • Main training phase. We train models for all languages keeping both the model resulting from the original early stopping strategy (DevSet) and that from the epoch com"
D19-1329,P16-1154,0,0.02812,"ceto-sequence task, mapping the characters of the input word together with the morphological features specifying the target to the characters of the corresponding inflected form (Cotterell et al., 2018). Data. We experiment on the datasets released for a 2018 shared task (Cotterell et al., 2018), which cover 103 languages and feature an explicit low-resource setting. We randomly choose ten development languages: Armenian, Basque, Galician, Georgian, Greenlandic, Icelandic, Karbadian, Kannada, Latin, and Lithuanian. Model. For MORPH, we experiment with a pointer-generator network architecture (Gu et al., 2016; See et al., 2017). This is a sequence-tosequence model similar to that for NORM, but employs separate encoders for characters and features. It is further equipped with a copy mechanism: using attention to decide on what element from the input sequence to copy, the model computes a probability for either copying or generation while producing an output. The final probability distribution over the target vocabulary is a combination of both. Hyperparameters are taken from Sharma et al. (2018).4 For early stopping, we also follow Sharma et al. (2018): all models are trained 3 github.com/OpenNMT/O"
D19-1329,W18-3400,0,0.187709,"Missing"
D19-1329,W13-2233,0,0.0304037,"te semi-supervised training algorithms in a realistic way; they differ from us in that they focus exclusively on semi-supervised learning (SSL) algorithms, and do not consider NLP explicitly. However, in line with our conclusion, they report that recent practices for evaluating SSL techniques do not address the question of the algorithms’ real-word applicability in a satisfying way. In NLP, several earlier works have explicitly investigated real-world low-resource settings as opposed to artificial proxy settings, e.g., for part-of-speech tagging (Garrette et al., 2013) or machine translation (Irvine and Callison-Burch, 2013). While those mostly focus on real data-poor languages, we explicitly investigate the effect of the common practice to assume a relatively large development set for early stopping in the low-resource setting. Low-resource settings in NLP. Research in the area of neural methods for low-resource NLP has gained popularity in recent years, with a dedicated Experimental Design • Main training phase. We train models for all languages keeping both the model resulting from the original early stopping strategy (DevSet) and that from the epoch computed in the stopping point selection phase (DevLang).2 T"
D19-1329,P17-1182,1,0.882068,"Missing"
D19-1329,N18-1005,1,0.903263,"Missing"
D19-1329,P17-4012,0,0.0215587,"mann et al., 2017; Odebrecht et al., 2017); English, Hungarian, Icelandic, and Swedish (Pettersson, 2016); Slovene (two datasets; Ljubeˇsic et al., 2016); and Spanish and Portuguese (Vaamonde, 2015). We treat the two datasets for German and Slovene as different languages. All languages serve both as development languages for all other languages and as target languages. Model. Our model for this task is an LSTM (Hochreiter and Schmidhuber, 1997) encoderdecoder model with attention (Bahdanau et al., 2015). Both encoder and decoder have a single hidden layer. We use the default model in OpenNMT (Klein et al., 2017)3 as our implementation and employ the hyperparameters from Bollmann et al. (2018). In the original paper, early stopping is done by training for 50 epochs, and the best model regarding development accuracy is applied to the test set. 4.2 Morphological Inflection (MORPH) Task. Morphological inflection consists of mapping the canonical form of a word, the lemma, to an indicated inflected form. This task gets very complex for morphologically rich languages, where a single lemma can have hundreds or thousand of inflected forms. Recently, morphological inflection has frequently been cast as a sequ"
D19-1329,D18-1314,0,0.0237466,"re showing promising results on various natural language processing (NLP) tasks also in lowresource or few-shot settings (Johnson et al., 2017; Kann et al., 2017; Yu et al., 2018). Often, the high-resource experimental setup and training procedure are kept unchanged, and the size of the original training set is reduced to simulate limited data. This leads to settings where validation examples may outnumber training examples. Table 1 shows such cases for the tasks of historical text normalization (Bollmann et al., 2018), morphological segmentation (Kann et al., 2018), morphological inflection (Makarov and Clematide, 2018; Sharma et al., 2018), argument component identification (Schulz et al., 2018), and transliteration (Upadhyay et al., 2018). 3342 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 3342–3349, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics However, in a real-world setting with limited resources, it is unlikely that such a development set would be available for early stopping, since it would be more effective to use at least part of it for"
D19-1329,N18-1202,0,0.0117913,"examples in T . However, since T is finite, overfitting the training set might lead to poor generalization performance. One way to avoid fitting Equation 1 too closely is early stopping: a separate development or validation set is used to end training as soon as the loss on the development set LD (θ) starts increasing or model performance on the development set D starts decreasing. The best set of parameters θ is used in the final model. This works well when large amounts of data are available to create training, development and test splits. Recently, however, with the success of pretraining (Peters et al., 2018; Devlin et al., 2019) and multi-task learning (Caruana, 1997; Ruder, 2017; Wang et al., 2019) approaches, neural models are showing promising results on various natural language processing (NLP) tasks also in lowresource or few-shot settings (Johnson et al., 2017; Kann et al., 2017; Yu et al., 2018). Often, the high-resource experimental setup and training procedure are kept unchanged, and the size of the original training set is reduced to simulate limited data. This leads to settings where validation examples may outnumber training examples. Table 1 shows such cases for the tasks of histori"
D19-1329,N18-2006,0,0.0573727,"n Cho and Samuel R. Bowman New York University, USA {kann, kyunghyun.cho, bowman}@nyu.edu Abstract # train Table 1: Number of examples used for training and development in recent low-resource NLP experiments; ES=early stopping on the development set. Experiments from papers in bold will be revisited here. Introduction Parametric machine learning models are frequently trained by minimizing the loss on the training set T , LT (θ) = X l (θ, x) , ES Bollmann et al. (2018) 5k 12k-46k Yes Kann et al. (2018) 400-700 100-200 Yes Makarov and Clematide (2018) 100 1k Yes Sharma et al. (2018) 100 100 Yes Schulz et al. (2018) 1k-21k 9k N/A Upadhyay et al. (2018) 500 1k Yes Development sets are impractical to obtain for real low-resource languages, since using all available data for training is often more effective. However, development sets are widely used in research papers that purport to deal with low-resource natural language processing (NLP). Here, we aim to answer the following questions: Does using a development set for early stopping in the low-resource setting influence results as compared to a more realistic alternative, where the number of training epochs is tuned on development languages? And does it l"
D19-1384,D17-1259,0,0.0502329,"rouchy et al., 2016), often used in the framework of Artificial Life (Bedau, 2003), or a gradient-based optimization algorithm, as is often used for training deep neural networks with a supervised or reinforcement learning objective function. The former simulates generations developing complex behavior over time, while the latter enables more sophisticated agents thanks to the recent advances in deep learning (LeCun et al., 2015). Recent years have seen intriguing new results in emergent communication, starting with Lazaridou et al. (2016) and Foerster et al. (2016), using deep neural agents (Lewis et al., 2017; Havrylov and Titov, 2017; Jorge et al., 2016; Evtimova et al., 2018; Das et al., 2018; Cao et al., 2018). Often, these approaches could be framed as special or generalized cases of Lewis’s signalling game (Lewis, 2008), in which agents exchange signals to achieve a common goal. In this work, deep neural agents play games within communities of similar agents, where the aim is for agents to communicate about their perceptual input. 3 Multi-agent communication In order to study emergent linguistic phenomena in a simplified but realistic setting, the communication game needs to have several prop"
D19-1384,D14-1162,0,\N,Missing
D19-1447,P17-1022,0,0.0362282,"ion, game-playing or dialogue—which either don’t have clearly defined metrics or easily available natural language data— this pivot-based translation allows us to check exactly whether the communicated sequence corresponds to the intended meaning, as well as to the gold standard sequence. In addition, every single utterance has very clear and well-known metrics such as BLEU and log-likelihood, allowing us to measure performance at every single step. Our work is inspired by recent work in protocols or languages that emerge from multi-agent interaction (Lazaridou et al., 2017; Lee et al., 2018; Andreas et al., 2017; Evtimova et al., 2018; Kottur et al., 2017; Havrylov and Titov, 2017; Mordatch and Abbeel, 2017). Work on the emergence of language in multi-agent settings goes back a long way (Steels, 1997; Nowak and Krakauer, 1999; Kirby, 2001; Briscoe, 2002; Skyrms, 2010). In our case, we are specifically interested in tabula inscripta agents that are already pre-trained to generate natural language, and we are primarily concerned with keeping their generated language as natural as possible during further training. Reinforcement Learning has been applied to finetuning models for various natural language"
D19-1447,I17-1014,0,0.0378313,"Missing"
D19-1447,D16-1026,1,0.908898,"Missing"
D19-1447,2012.eamt-1.60,0,0.0637196,"Missing"
D19-1447,D17-1303,0,0.0182176,"u et al., 2016) and dialogue (Li et al., 2017). Our work can be viewed as fine-tuning MT systems using an intermediary pivot language. In MT, there is a long line of work of pivot-based approaches, most notably Muraki (1986) and more recently with neural approaches (Wang et al., 2017; Cheng et al., 2017; Chen et al., 2018). There has also been work on using visual pivots directly (Hitschler et al., 2016; Nakayama and Nishida, 2017; Lee et al., 2018). Grounded language learning in general has been shown to give significant practical improvements in various natural language understanding tasks (Gella et al., 2017; Elliott and K´ad´ar, 2017; Chrupała et al., 2015; Kiela et al., 2017; K´ad´ar et al., 2018). In what follows, we show that language drift happens, and quite dramatically so, when fine-tuning using policy gradients. Next, we investigate imposing syntactic conformity (i.e., “Englishness”) via language model constraints, and show that this does somewhat mitigate drift, but does not lead to semantic correspondence. We then show that additionally imposing semantic constraints via (visual) grounding leads to the best retention of original syntax and intended semantics, and minimizes drift while im"
D19-1447,D17-1210,1,0.905964,"Missing"
D19-1447,P16-1227,0,0.029764,"ed to finetuning models for various natural language generation tasks, including summarization (Ranzato et al., 2015; Paulus et al., 2017), information retrieval (Nogueira and Cho, 2017), MT (Gu et al., 2017; Bahdanau et al., 2016) and dialogue (Li et al., 2017). Our work can be viewed as fine-tuning MT systems using an intermediary pivot language. In MT, there is a long line of work of pivot-based approaches, most notably Muraki (1986) and more recently with neural approaches (Wang et al., 2017; Cheng et al., 2017; Chen et al., 2018). There has also been work on using visual pivots directly (Hitschler et al., 2016; Nakayama and Nishida, 2017; Lee et al., 2018). Grounded language learning in general has been shown to give significant practical improvements in various natural language understanding tasks (Gella et al., 2017; Elliott and K´ad´ar, 2017; Chrupała et al., 2015; Kiela et al., 2017; K´ad´ar et al., 2018). In what follows, we show that language drift happens, and quite dramatically so, when fine-tuning using policy gradients. Next, we investigate imposing syntactic conformity (i.e., “Englishness”) via language model constraints, and show that this does somewhat mitigate drift, but does not lead"
D19-1447,K18-1039,0,0.049502,"Missing"
D19-1447,N18-1038,1,0.897595,"Missing"
D19-1447,P07-2045,0,0.00539243,"French, German and Czech (of which we only use the first three). To ensure our findings are robust, we compare four different language models, trained on WikiText103, MS COCO, Flickr30k and all of the above. The grounding model is trained on Flickr30k (Young et al., 2014). Following Faghri et al. (2018), we randomly crop training images for data augmentation. We use 2048-dimensional features from a pretrained and fixed ResNet-152 (He et al., 2016). Preprocessing The same tokenization and vocabulary are used across different tasks and datasets. We lowercase and tokenize our corpora with Moses (Koehn et al., 2007) and use subword tokenization with Byte Pair Encoding (BPE) (Sennrich et al., 2016) with 10k merge operations. This allows us to use the same vocabulary across different models seamlessly (translation, language model, image-caption ranker model). Controling the English message length When fine-tuning the agents, we observe that the length of English messages becomes excessively long. As Agent A has no explicit incentive to output the end-of-sentence (EOS) symbol, it tends to keep transmitting the same token repeatedly. While redundancy might be beneficial for communication, excessively long me"
D19-1447,D17-1321,0,0.0483672,"n’t have clearly defined metrics or easily available natural language data— this pivot-based translation allows us to check exactly whether the communicated sequence corresponds to the intended meaning, as well as to the gold standard sequence. In addition, every single utterance has very clear and well-known metrics such as BLEU and log-likelihood, allowing us to measure performance at every single step. Our work is inspired by recent work in protocols or languages that emerge from multi-agent interaction (Lazaridou et al., 2017; Lee et al., 2018; Andreas et al., 2017; Evtimova et al., 2018; Kottur et al., 2017; Havrylov and Titov, 2017; Mordatch and Abbeel, 2017). Work on the emergence of language in multi-agent settings goes back a long way (Steels, 1997; Nowak and Krakauer, 1999; Kirby, 2001; Briscoe, 2002; Skyrms, 2010). In our case, we are specifically interested in tabula inscripta agents that are already pre-trained to generate natural language, and we are primarily concerned with keeping their generated language as natural as possible during further training. Reinforcement Learning has been applied to finetuning models for various natural language generation tasks, including summarization (R"
D19-1447,D17-1259,0,0.0533192,"s been a renewed interest in multi-agent communication (Foerster et al., 2016; Lazaridou et al., 2016). While agents can be very effective in solving the tasks that they were trained on, their multi-agent communication protocols bear little resemblance to human languages. A major open question revolves around training multi-agent systems such that their communication protocols can be interpreted by humans. One option is to pre-train in a supervised fashion with human language, but even then it is found that the protocols diverge quickly when the agents are fine-tuned on an external reward, as Lewis et al. (2017) showed on a negotiation task. Indeed, language drift is to be expected if we are optimizing for an external non-linguistic reward, such as a reward based on whether or not two agents successfully accomplish a negotiation. Language drift might be avoided by imposing a “naturalness” constraint, e.g. by factoring language model likelihood into the reward function. However, such a constraint only acts on the syntax of the generated language, ignoring its semantics. See Table 1 for an illustration of different constraints. As has been advocated by multi-modal semantics (Baroni, 2016; Kiela, 2017),"
D19-1447,D17-1230,0,0.0380497,"Nowak and Krakauer, 1999; Kirby, 2001; Briscoe, 2002; Skyrms, 2010). In our case, we are specifically interested in tabula inscripta agents that are already pre-trained to generate natural language, and we are primarily concerned with keeping their generated language as natural as possible during further training. Reinforcement Learning has been applied to finetuning models for various natural language generation tasks, including summarization (Ranzato et al., 2015; Paulus et al., 2017), information retrieval (Nogueira and Cho, 2017), MT (Gu et al., 2017; Bahdanau et al., 2016) and dialogue (Li et al., 2017). Our work can be viewed as fine-tuning MT systems using an intermediary pivot language. In MT, there is a long line of work of pivot-based approaches, most notably Muraki (1986) and more recently with neural approaches (Wang et al., 2017; Cheng et al., 2017; Chen et al., 2018). There has also been work on using visual pivots directly (Hitschler et al., 2016; Nakayama and Nishida, 2017; Lee et al., 2018). Grounded language learning in general has been shown to give significant practical improvements in various natural language understanding tasks (Gella et al., 2017; Elliott and K´ad´ar, 2017;"
D19-1447,D17-1061,1,0.824177,"e emergence of language in multi-agent settings goes back a long way (Steels, 1997; Nowak and Krakauer, 1999; Kirby, 2001; Briscoe, 2002; Skyrms, 2010). In our case, we are specifically interested in tabula inscripta agents that are already pre-trained to generate natural language, and we are primarily concerned with keeping their generated language as natural as possible during further training. Reinforcement Learning has been applied to finetuning models for various natural language generation tasks, including summarization (Ranzato et al., 2015; Paulus et al., 2017), information retrieval (Nogueira and Cho, 2017), MT (Gu et al., 2017; Bahdanau et al., 2016) and dialogue (Li et al., 2017). Our work can be viewed as fine-tuning MT systems using an intermediary pivot language. In MT, there is a long line of work of pivot-based approaches, most notably Muraki (1986) and more recently with neural approaches (Wang et al., 2017; Cheng et al., 2017; Chen et al., 2018). There has also been work on using visual pivots directly (Hitschler et al., 2016; Nakayama and Nishida, 2017; Lee et al., 2018). Grounded language learning in general has been shown to give significant practical improvements in various natural"
D19-1447,P16-1162,0,0.015582,"indings are robust, we compare four different language models, trained on WikiText103, MS COCO, Flickr30k and all of the above. The grounding model is trained on Flickr30k (Young et al., 2014). Following Faghri et al. (2018), we randomly crop training images for data augmentation. We use 2048-dimensional features from a pretrained and fixed ResNet-152 (He et al., 2016). Preprocessing The same tokenization and vocabulary are used across different tasks and datasets. We lowercase and tokenize our corpora with Moses (Koehn et al., 2007) and use subword tokenization with Byte Pair Encoding (BPE) (Sennrich et al., 2016) with 10k merge operations. This allows us to use the same vocabulary across different models seamlessly (translation, language model, image-caption ranker model). Controling the English message length When fine-tuning the agents, we observe that the length of English messages becomes excessively long. As Agent A has no explicit incentive to output the end-of-sentence (EOS) symbol, it tends to keep transmitting the same token repeatedly. While redundancy might be beneficial for communication, excessively long messages obscure evaluation of the communication protocol. For instance, BLEU score q"
D19-1447,I17-1039,0,0.0188084,"eeping their generated language as natural as possible during further training. Reinforcement Learning has been applied to finetuning models for various natural language generation tasks, including summarization (Ranzato et al., 2015; Paulus et al., 2017), information retrieval (Nogueira and Cho, 2017), MT (Gu et al., 2017; Bahdanau et al., 2016) and dialogue (Li et al., 2017). Our work can be viewed as fine-tuning MT systems using an intermediary pivot language. In MT, there is a long line of work of pivot-based approaches, most notably Muraki (1986) and more recently with neural approaches (Wang et al., 2017; Cheng et al., 2017; Chen et al., 2018). There has also been work on using visual pivots directly (Hitschler et al., 2016; Nakayama and Nishida, 2017; Lee et al., 2018). Grounded language learning in general has been shown to give significant practical improvements in various natural language understanding tasks (Gella et al., 2017; Elliott and K´ad´ar, 2017; Chrupała et al., 2015; Kiela et al., 2017; K´ad´ar et al., 2018). In what follows, we show that language drift happens, and quite dramatically so, when fine-tuning using policy gradients. Next, we investigate imposing syntactic conformit"
D19-1447,Q14-1006,0,0.0421369,"sets Agents A and B are initially pre-trained on IWSLT Fr→En and En→De, respectively (Cettolo et al., 2012). Fine-tuning is performed on Multi30k Task 1 (Elliott et al., 2016). That is, importantly, there is no overlap in the pre-training data and the fine-tuning data. Multi30k Task 1 consists of 30k images and one caption per image in English, French, German and Czech (of which we only use the first three). To ensure our findings are robust, we compare four different language models, trained on WikiText103, MS COCO, Flickr30k and all of the above. The grounding model is trained on Flickr30k (Young et al., 2014). Following Faghri et al. (2018), we randomly crop training images for data augmentation. We use 2048-dimensional features from a pretrained and fixed ResNet-152 (He et al., 2016). Preprocessing The same tokenization and vocabulary are used across different tasks and datasets. We lowercase and tokenize our corpora with Moses (Koehn et al., 2007) and use subword tokenization with Byte Pair Encoding (BPE) (Sennrich et al., 2016) with 10k merge operations. This allows us to use the same vocabulary across different models seamlessly (translation, language model, image-caption ranker model). Contro"
D19-1447,N16-1004,0,0.0417725,"ts (αpg , αentr , αb ) for agent A and (βLM , βG ) for agent B, respectively (see previous section). For 4388 our joint systems with policy gradient fine-tuning, we run every model three times with different random seeds and report averaged results. Baseline and Upper Bound Our main quantitative experiment has three baselines: • Pretrained models : models pretrained on IWSLT are used without finetuning. • Ensembling : Given Fr, we let Agent A generate K En hypotheses with beam search. Then, we let Agent B generate the translation De using an ensemble of K source sentences (Firat et al., 2016; Zoph and Knight, 2016). • Agent A fixed : We fix Agent A (Fr→En) and only fine-tune Agent B using LB . This shows the communication performance achievable when Agent A cannot drift. Meanwhile, we also train an NMT model of the same architecture and size directly on the Fr→De task in Multi30k Task 1 (without English intermediary). This serves as an upper bound on the Fr→De performance achievable with available data. 5 Quantitative Results In Table 2, the top three rows are the baselines described above. The pretrained-only baseline performs relatively poorly on Fr→De, conceivably because it was pretrained on a diffe"
D19-6123,W18-3020,0,0.0143063,"be interesting to observe whether similar trends in relative performance among languages hold for other models mentioned above. We leave this for future work. A closely related line of research, which is often referred to as latent tree learning, aims to create a parse structure which is well-suited for a particular NLP application. Common choices are sentence classification tasks like natural language inference (Yogatama et al., 2017; Maillard et al., 2017; Choi et al., 2018), machine translation (Bisk and Tran, 2018), or toy datasets where the correct parse can trivially be found by humans (Jacob et al., 2018; Nangia and Bowman, 2018). Latent tree learning models have been shown to outperform sequential models and TreeRNNs on multiple datasets (Maillard et al., 2017; Choi et al., 2018). However, the parses predicted by latent tree models have been shown to mostly be nonsensical (Williams et al., 2018). Supervised parsing. This research is further related to the line of work on supervised parsing. Two main parsing paradigms exist: dependency parsing, which is concerned with the relationships between words in a sentence, and constituency parsing (or phrase-structure parsing), which is what we are in"
D19-6123,D16-1073,0,0.0204782,"rmance on other languages in the case of limited training data. Such cross-lingual transfer has been successfully employed for a variety of tasks, e.g., for supervised dependency parsing (de Lhoneux et al., 2018), for machine translation (Johnson et al., 2017a), or for paradigm completion (McCarthy et al., 2019). 5.2 6 Related Work Unsupervised parsing. Previous work on nonneural models for unsupervised parsing includes Clark (2001) and Klein and Manning (2002) for constituency parsing and Carroll and Charniak (1992); Klein and Manning (2004); Cohn et al. (2010); Spitkovsky et al. (2011); and Jiang et al. (2016) for dependency parsing. For Chinese, German, and English, previous work also observed differences in F1 scores; an overview can be found in Bod (2006). We have not included these nonneural baselines as part of our results due to lack of availability of trustworthy implementations. Following the success of the neural PRPN model in 2018, various other neural unsupervised parsing approaches have been developed and shown promising results. Shen et al. (2018c) enhance the vanilla LSTM network with master forget and input gates to learn the tree structure through soft gating. Drozdov et al. (2019)"
D19-6123,P17-2021,0,0.0709194,"Missing"
D19-6123,P19-1234,0,0.0156458,"ve been developed and shown promising results. Shen et al. (2018c) enhance the vanilla LSTM network with master forget and input gates to learn the tree structure through soft gating. Drozdov et al. (2019) use a recursive autoencoder-based architecture. Kim et al. (2019b) employ unsupervised recurrent neural network grammars, and Kim et al. (2019a) employ compound probabilistic context free grammars. Shi et al. (2019) show how image captions can be successfully leveraged to identify constituents in sentences. None of these papers performs an explicit analysis of differences between languages. Jin et al. (2019) extend the PCFG approach to show results on Chinese, English and German. There are certain question that remain unanswered about multilingual grammar induction, especially related to cross-lingual transfer and difference in hyper parameters. In this work, we focus on adaptSetup Since transfer learning is mostly needed and also particularly effective in the low-data regime, we combine the training sets of 2,500 examples for English and German—the two of our languages which are related—to form a multilingual training set. We then train PRPN models on this combined training set, sharing all para"
D19-6123,N19-1388,0,0.0324253,"al., 2019): One way is to translate the test data into a high-resource language and to solve the task using a system for that second language. Another way is to translate large amount of training data into a low-resource language and train a system in that language. Other methods have been developed as well, many based on parameter sharing—as we do in this work—, e.g., for cross-lingual natural language inference (Conneau et al., 2018), morphological generation (Kann et al., 2017; McCarthy et al., 2019), dialogue systems (Schuster et al., 2019), or machine translation (Johnson et al., 2017b; Aharoni et al., 2019). While we are not aware of any previous work exploring cross-lingual transfer for unsupervised parsing as done in this paper, approaches have been developed which leverage high-resource language data for supervised parsing in low-resource languages (Søgaard, 2011; Naseem et al., 2012). ing the PRPN to a multilingual setting, since it is the first neural model which has been shown to obtain robust unsupervised parsing results. Although we have primarily focused on PRPN due to its overall success, it would be interesting to observe whether similar trends in relative performance among languages"
D19-6123,W18-2704,0,0.0249191,"arsing results. Although we have primarily focused on PRPN due to its overall success, it would be interesting to observe whether similar trends in relative performance among languages hold for other models mentioned above. We leave this for future work. A closely related line of research, which is often referred to as latent tree learning, aims to create a parse structure which is well-suited for a particular NLP application. Common choices are sentence classification tasks like natural language inference (Yogatama et al., 2017; Maillard et al., 2017; Choi et al., 2018), machine translation (Bisk and Tran, 2018), or toy datasets where the correct parse can trivially be found by humans (Jacob et al., 2018; Nangia and Bowman, 2018). Latent tree learning models have been shown to outperform sequential models and TreeRNNs on multiple datasets (Maillard et al., 2017; Choi et al., 2018). However, the parses predicted by latent tree models have been shown to mostly be nonsensical (Williams et al., 2018). Supervised parsing. This research is further related to the line of work on supervised parsing. Two main parsing paradigms exist: dependency parsing, which is concerned with the relationships between words"
D19-6123,P06-1109,0,0.0575599,"ervised dependency parsing (de Lhoneux et al., 2018), for machine translation (Johnson et al., 2017a), or for paradigm completion (McCarthy et al., 2019). 5.2 6 Related Work Unsupervised parsing. Previous work on nonneural models for unsupervised parsing includes Clark (2001) and Klein and Manning (2002) for constituency parsing and Carroll and Charniak (1992); Klein and Manning (2004); Cohn et al. (2010); Spitkovsky et al. (2011); and Jiang et al. (2016) for dependency parsing. For Chinese, German, and English, previous work also observed differences in F1 scores; an overview can be found in Bod (2006). We have not included these nonneural baselines as part of our results due to lack of availability of trustworthy implementations. Following the success of the neural PRPN model in 2018, various other neural unsupervised parsing approaches have been developed and shown promising results. Shen et al. (2018c) enhance the vanilla LSTM network with master forget and input gates to learn the tree structure through soft gating. Drozdov et al. (2019) use a recursive autoencoder-based architecture. Kim et al. (2019b) employ unsupervised recurrent neural network grammars, and Kim et al. (2019a) employ"
D19-6123,W01-0713,0,0.232608,"mobile devices. Second, neural models which have been trained simultaneously on multiple languages have been shown to leverage knowledge from related languages to improve performance on other languages in the case of limited training data. Such cross-lingual transfer has been successfully employed for a variety of tasks, e.g., for supervised dependency parsing (de Lhoneux et al., 2018), for machine translation (Johnson et al., 2017a), or for paradigm completion (McCarthy et al., 2019). 5.2 6 Related Work Unsupervised parsing. Previous work on nonneural models for unsupervised parsing includes Clark (2001) and Klein and Manning (2002) for constituency parsing and Carroll and Charniak (1992); Klein and Manning (2004); Cohn et al. (2010); Spitkovsky et al. (2011); and Jiang et al. (2016) for dependency parsing. For Chinese, German, and English, previous work also observed differences in F1 scores; an overview can be found in Bod (2006). We have not included these nonneural baselines as part of our results due to lack of availability of trustworthy implementations. Following the success of the neural PRPN model in 2018, various other neural unsupervised parsing approaches have been developed and s"
D19-6123,P17-1182,1,0.8623,"Missing"
D19-6123,P19-1228,0,0.221934,"Missing"
D19-6123,D18-1269,1,0.821128,"ask in another (usually low-resource) language, is very common when working on resource-poor languages in NLP. There are two very intuitive ways of realizing such a transfer (Liu et al., 2019): One way is to translate the test data into a high-resource language and to solve the task using a system for that second language. Another way is to translate large amount of training data into a low-resource language and train a system in that language. Other methods have been developed as well, many based on parameter sharing—as we do in this work—, e.g., for cross-lingual natural language inference (Conneau et al., 2018), morphological generation (Kann et al., 2017; McCarthy et al., 2019), dialogue systems (Schuster et al., 2019), or machine translation (Johnson et al., 2017b; Aharoni et al., 2019). While we are not aware of any previous work exploring cross-lingual transfer for unsupervised parsing as done in this paper, approaches have been developed which leverage high-resource language data for supervised parsing in low-resource languages (Søgaard, 2011; Naseem et al., 2012). ing the PRPN to a multilingual setting, since it is the first neural model which has been shown to obtain robust unsupervised parsi"
D19-6123,N19-1114,0,0.122249,"erarchical syntactic structure from a large amount of unlabeled text, has been widely studied in natural language processing (NLP) (Carroll and Charniak, 1992; Pereira and Schabes, 1992; Klein and Manning, 2002, 2004). Work on this task bears on open research questions involving human language learning and grammar design by demonstrating what can be learned without substantial prior knowledge. Further, it can also be practically relevant for low-resource languages or language styles. Recently, multiple types of neural network models have been added to the line of research on 1 Some work, e.g. Kim et al. (2019a), present PRPN results on Chinese, but without further analysis of languagedependent differences. 209 Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo), pages 209–218 c Hong Kong, China, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 tion? From a practical NLP perspective, grammar induction enables us to obtain syntactic information without labeled data, i.e., even in lowresource settings and for resource-poor languages. This information can then be of help for downstream tasks like machine translation"
D19-6123,D16-1001,0,0.0169883,"lliams et al., 2018). Supervised parsing. This research is further related to the line of work on supervised parsing. Two main parsing paradigms exist: dependency parsing, which is concerned with the relationships between words in a sentence, and constituency parsing (or phrase-structure parsing), which is what we are interested in here (cf. Figure 1). Neural network models have pushed the state of the art for supervised constituency parsing in the last years. Possible approaches include methods to either build parse trees sequentially by estimating transition probabilities (Zhu et al., 2013; Cross and Huang, 2016), employ a chart-based approach, which performs exact structured inference via dynamic programming (Durrett and Klein, 2015; Stern et al., 2017), or cast the problem as a sequence labeling task (G´omez-Rodr´ıguez and Vilares, 2018). Another, rather new option is to predict syntactic distances between words, which can then be converted into trees (Shen et al., 2018b). This is the same core concept that the PRPN is based on. Thus, we consider Shen et al. (2018b)’s approach one of our upper bounds on the unsupervised parsing performance of the PRPN. 7 Conclusion We investigated the behavior of th"
D19-6123,P02-1017,0,0.435434,"odels, which perform unsupervised parsing—show language-dependent performance variation (Snyder et al., 2009), which motivates our investigation of recent neural models. In this work, we first aim to answer the following research questions, focusing on the parsingreading-predict network (PRPN; Shen et al., 2018a) and experimenting with Arabic, Chinese, Introduction Unsupervised parsing, the task of inducing hierarchical syntactic structure from a large amount of unlabeled text, has been widely studied in natural language processing (NLP) (Carroll and Charniak, 1992; Pereira and Schabes, 1992; Klein and Manning, 2002, 2004). Work on this task bears on open research questions involving human language learning and grammar design by demonstrating what can be learned without substantial prior knowledge. Further, it can also be practically relevant for low-resource languages or language styles. Recently, multiple types of neural network models have been added to the line of research on 1 Some work, e.g. Kim et al. (2019a), present PRPN results on Chinese, but without further analysis of languagedependent differences. 209 Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo),"
D19-6123,N19-1116,0,0.0197397,"nd Jiang et al. (2016) for dependency parsing. For Chinese, German, and English, previous work also observed differences in F1 scores; an overview can be found in Bod (2006). We have not included these nonneural baselines as part of our results due to lack of availability of trustworthy implementations. Following the success of the neural PRPN model in 2018, various other neural unsupervised parsing approaches have been developed and shown promising results. Shen et al. (2018c) enhance the vanilla LSTM network with master forget and input gates to learn the tree structure through soft gating. Drozdov et al. (2019) use a recursive autoencoder-based architecture. Kim et al. (2019b) employ unsupervised recurrent neural network grammars, and Kim et al. (2019a) employ compound probabilistic context free grammars. Shi et al. (2019) show how image captions can be successfully leveraged to identify constituents in sentences. None of these papers performs an explicit analysis of differences between languages. Jin et al. (2019) extend the PCFG approach to show results on Chinese, English and German. There are certain question that remain unanswered about multilingual grammar induction, especially related to cros"
D19-6123,P04-1061,0,0.16553,"ave been shown to leverage knowledge from related languages to improve performance on other languages in the case of limited training data. Such cross-lingual transfer has been successfully employed for a variety of tasks, e.g., for supervised dependency parsing (de Lhoneux et al., 2018), for machine translation (Johnson et al., 2017a), or for paradigm completion (McCarthy et al., 2019). 5.2 6 Related Work Unsupervised parsing. Previous work on nonneural models for unsupervised parsing includes Clark (2001) and Klein and Manning (2002) for constituency parsing and Carroll and Charniak (1992); Klein and Manning (2004); Cohn et al. (2010); Spitkovsky et al. (2011); and Jiang et al. (2016) for dependency parsing. For Chinese, German, and English, previous work also observed differences in F1 scores; an overview can be found in Bod (2006). We have not included these nonneural baselines as part of our results due to lack of availability of trustworthy implementations. Following the success of the neural PRPN model in 2018, various other neural unsupervised parsing approaches have been developed and shown promising results. Shen et al. (2018c) enhance the vanilla LSTM network with master forget and input gates"
D19-6123,D18-1543,0,0.0392102,"Missing"
D19-6123,P03-1013,0,0.0582167,"splits: the development and test sets consist of 300 files each, while the training set consists of the remaining 2407. Balanced trees baseline (BTB). Finally, this baseline is similar to LBR/RBR, but considering balanced binary trees, which are created by recursively splitting each span into halves. For odd lengths, the middle word becomes a part of the right subtree. German. We use the NEGRA corpus (Skut et al., 1997), which consists of approximately 350, 000 words of German newspaper text (20,602 sentences). We divide the dataset into training, development, and test splits as suggested by Dubey and Keller (2003). 4 4.1 Monolingual Experiments Language-Dependence of Hyperparameters Best binary tree upper bound (BB). Since our datasets contain n-ary trees, but the PRPN only produces binary trees, obtaining a perfect F1 score is impossible. This upper bound represents the best score which can be obtained with binary trees. It is of practical importance to know whether a set of hyperparameters found for one language transfers to another one without any changes, especially for low-resource language without annotated (development) data. Therefore, we ask the following questions: (i) Do hyperparameters depe"
D19-6123,P19-1227,0,0.0247975,"ces for 12,500 examples and for the entire training sets for all languages. Thus, we conclude that disposing of more than 12,500 examples is generally beneficial. 5 0.186 (.08) 0.326 (.02) Results Results for single-language models as well as the multilingual versions are shown in Table 3. The parsing performance of the multilingual model is 215 ing knowledge gained from one (usually highresource) language for solving a task in another (usually low-resource) language, is very common when working on resource-poor languages in NLP. There are two very intuitive ways of realizing such a transfer (Liu et al., 2019): One way is to translate the test data into a high-resource language and to solve the task using a system for that second language. Another way is to translate large amount of training data into a low-resource language and train a system in that language. Other methods have been developed as well, many based on parameter sharing—as we do in this work—, e.g., for cross-lingual natural language inference (Conneau et al., 2018), morphological generation (Kann et al., 2017; McCarthy et al., 2019), dialogue systems (Schuster et al., 2019), or machine translation (Johnson et al., 2017b; Aharoni et"
D19-6123,D18-1162,0,0.0249036,"Missing"
D19-6123,D11-1118,0,0.0309377,"ted languages to improve performance on other languages in the case of limited training data. Such cross-lingual transfer has been successfully employed for a variety of tasks, e.g., for supervised dependency parsing (de Lhoneux et al., 2018), for machine translation (Johnson et al., 2017a), or for paradigm completion (McCarthy et al., 2019). 5.2 6 Related Work Unsupervised parsing. Previous work on nonneural models for unsupervised parsing includes Clark (2001) and Klein and Manning (2002) for constituency parsing and Carroll and Charniak (1992); Klein and Manning (2004); Cohn et al. (2010); Spitkovsky et al. (2011); and Jiang et al. (2016) for dependency parsing. For Chinese, German, and English, previous work also observed differences in F1 scores; an overview can be found in Bod (2006). We have not included these nonneural baselines as part of our results due to lack of availability of trustworthy implementations. Following the success of the neural PRPN model in 2018, various other neural unsupervised parsing approaches have been developed and shown promising results. Shen et al. (2018c) enhance the vanilla LSTM network with master forget and input gates to learn the tree structure through soft gatin"
D19-6123,P17-1076,0,0.0617362,"ist: dependency parsing, which is concerned with the relationships between words in a sentence, and constituency parsing (or phrase-structure parsing), which is what we are interested in here (cf. Figure 1). Neural network models have pushed the state of the art for supervised constituency parsing in the last years. Possible approaches include methods to either build parse trees sequentially by estimating transition probabilities (Zhu et al., 2013; Cross and Huang, 2016), employ a chart-based approach, which performs exact structured inference via dynamic programming (Durrett and Klein, 2015; Stern et al., 2017), or cast the problem as a sequence labeling task (G´omez-Rodr´ıguez and Vilares, 2018). Another, rather new option is to predict syntactic distances between words, which can then be converted into trees (Shen et al., 2018b). This is the same core concept that the PRPN is based on. Thus, we consider Shen et al. (2018b)’s approach one of our upper bounds on the unsupervised parsing performance of the PRPN. 7 Conclusion We investigated the behavior of the PRPN, a neural unsupervised constituency parsing model, for the languages Arabic, Chinese, English, and German. While, overall, our experiment"
D19-6123,Q18-1019,1,0.795528,"suited for a particular NLP application. Common choices are sentence classification tasks like natural language inference (Yogatama et al., 2017; Maillard et al., 2017; Choi et al., 2018), machine translation (Bisk and Tran, 2018), or toy datasets where the correct parse can trivially be found by humans (Jacob et al., 2018; Nangia and Bowman, 2018). Latent tree learning models have been shown to outperform sequential models and TreeRNNs on multiple datasets (Maillard et al., 2017; Choi et al., 2018). However, the parses predicted by latent tree models have been shown to mostly be nonsensical (Williams et al., 2018). Supervised parsing. This research is further related to the line of work on supervised parsing. Two main parsing paradigms exist: dependency parsing, which is concerned with the relationships between words in a sentence, and constituency parsing (or phrase-structure parsing), which is what we are interested in here (cf. Figure 1). Neural network models have pushed the state of the art for supervised constituency parsing in the last years. Possible approaches include methods to either build parse trees sequentially by estimating transition probabilities (Zhu et al., 2013; Cross and Huang, 201"
D19-6123,W19-4226,0,0.146783,"s needed for a set of languages, less memory is required to store all parameters. This facilitates, for instance, the application on mobile devices. Second, neural models which have been trained simultaneously on multiple languages have been shown to leverage knowledge from related languages to improve performance on other languages in the case of limited training data. Such cross-lingual transfer has been successfully employed for a variety of tasks, e.g., for supervised dependency parsing (de Lhoneux et al., 2018), for machine translation (Johnson et al., 2017a), or for paradigm completion (McCarthy et al., 2019). 5.2 6 Related Work Unsupervised parsing. Previous work on nonneural models for unsupervised parsing includes Clark (2001) and Klein and Manning (2002) for constituency parsing and Carroll and Charniak (1992); Klein and Manning (2004); Cohn et al. (2010); Spitkovsky et al. (2011); and Jiang et al. (2016) for dependency parsing. For Chinese, German, and English, previous work also observed differences in F1 scores; an overview can be found in Bod (2006). We have not included these nonneural baselines as part of our results due to lack of availability of trustworthy implementations. Following t"
D19-6123,J97-3002,0,0.261135,"Missing"
D19-6123,N18-4013,1,0.832279,"serve whether similar trends in relative performance among languages hold for other models mentioned above. We leave this for future work. A closely related line of research, which is often referred to as latent tree learning, aims to create a parse structure which is well-suited for a particular NLP application. Common choices are sentence classification tasks like natural language inference (Yogatama et al., 2017; Maillard et al., 2017; Choi et al., 2018), machine translation (Bisk and Tran, 2018), or toy datasets where the correct parse can trivially be found by humans (Jacob et al., 2018; Nangia and Bowman, 2018). Latent tree learning models have been shown to outperform sequential models and TreeRNNs on multiple datasets (Maillard et al., 2017; Choi et al., 2018). However, the parses predicted by latent tree models have been shown to mostly be nonsensical (Williams et al., 2018). Supervised parsing. This research is further related to the line of work on supervised parsing. Two main parsing paradigms exist: dependency parsing, which is concerned with the relationships between words in a sentence, and constituency parsing (or phrase-structure parsing), which is what we are interested in here (cf. Figu"
D19-6123,P12-1066,0,0.0262921,"been developed as well, many based on parameter sharing—as we do in this work—, e.g., for cross-lingual natural language inference (Conneau et al., 2018), morphological generation (Kann et al., 2017; McCarthy et al., 2019), dialogue systems (Schuster et al., 2019), or machine translation (Johnson et al., 2017b; Aharoni et al., 2019). While we are not aware of any previous work exploring cross-lingual transfer for unsupervised parsing as done in this paper, approaches have been developed which leverage high-resource language data for supervised parsing in low-resource languages (Søgaard, 2011; Naseem et al., 2012). ing the PRPN to a multilingual setting, since it is the first neural model which has been shown to obtain robust unsupervised parsing results. Although we have primarily focused on PRPN due to its overall success, it would be interesting to observe whether similar trends in relative performance among languages hold for other models mentioned above. We leave this for future work. A closely related line of research, which is often referred to as latent tree learning, aims to create a parse structure which is well-suited for a particular NLP application. Common choices are sentence classificati"
D19-6123,H01-1035,0,0.216396,"Missing"
D19-6123,P92-1017,0,0.57111,"r grammar induction—i.e., models, which perform unsupervised parsing—show language-dependent performance variation (Snyder et al., 2009), which motivates our investigation of recent neural models. In this work, we first aim to answer the following research questions, focusing on the parsingreading-predict network (PRPN; Shen et al., 2018a) and experimenting with Arabic, Chinese, Introduction Unsupervised parsing, the task of inducing hierarchical syntactic structure from a large amount of unlabeled text, has been widely studied in natural language processing (NLP) (Carroll and Charniak, 1992; Pereira and Schabes, 1992; Klein and Manning, 2002, 2004). Work on this task bears on open research questions involving human language learning and grammar design by demonstrating what can be learned without substantial prior knowledge. Further, it can also be practically relevant for low-resource languages or language styles. Recently, multiple types of neural network models have been added to the line of research on 1 Some work, e.g. Kim et al. (2019a), present PRPN results on Chinese, but without further analysis of languagedependent differences. 209 Proceedings of the 2nd Workshop on Deep Learning Approaches for L"
D19-6123,N19-1380,0,0.0180437,"There are two very intuitive ways of realizing such a transfer (Liu et al., 2019): One way is to translate the test data into a high-resource language and to solve the task using a system for that second language. Another way is to translate large amount of training data into a low-resource language and train a system in that language. Other methods have been developed as well, many based on parameter sharing—as we do in this work—, e.g., for cross-lingual natural language inference (Conneau et al., 2018), morphological generation (Kann et al., 2017; McCarthy et al., 2019), dialogue systems (Schuster et al., 2019), or machine translation (Johnson et al., 2017b; Aharoni et al., 2019). While we are not aware of any previous work exploring cross-lingual transfer for unsupervised parsing as done in this paper, approaches have been developed which leverage high-resource language data for supervised parsing in low-resource languages (Søgaard, 2011; Naseem et al., 2012). ing the PRPN to a multilingual setting, since it is the first neural model which has been shown to obtain robust unsupervised parsing results. Although we have primarily focused on PRPN due to its overall success, it would be interesting to o"
D19-6123,P13-1043,0,0.02848,"be nonsensical (Williams et al., 2018). Supervised parsing. This research is further related to the line of work on supervised parsing. Two main parsing paradigms exist: dependency parsing, which is concerned with the relationships between words in a sentence, and constituency parsing (or phrase-structure parsing), which is what we are interested in here (cf. Figure 1). Neural network models have pushed the state of the art for supervised constituency parsing in the last years. Possible approaches include methods to either build parse trees sequentially by estimating transition probabilities (Zhu et al., 2013; Cross and Huang, 2016), employ a chart-based approach, which performs exact structured inference via dynamic programming (Durrett and Klein, 2015; Stern et al., 2017), or cast the problem as a sequence labeling task (G´omez-Rodr´ıguez and Vilares, 2018). Another, rather new option is to predict syntactic distances between words, which can then be converted into trees (Shen et al., 2018b). This is the same core concept that the PRPN is based on. Thus, we consider Shen et al. (2018b)’s approach one of our upper bounds on the unsupervised parsing performance of the PRPN. 7 Conclusion We investi"
D19-6123,P18-1108,0,0.207133,",416 18,598 1,000 1,000 see it as the supervised approach which is most comparable to the PRPN. Since our model does not predict labels which are used to recover nary trees in the original work, we compute the F1 score for this approach only with respect to binary gold trees. This is acceptable for our purposes, since we are interested in the supervised parser upper bound only to get an idea of the difficulty of the datasets in our different languages. For the supervised SUB baseline, we use hidden state and embedding dimensions of 100 and 300, respectively, and keep the default settings from Shen et al. (2018b) for all other hyperparameters. Table 1: Dataset statistics. English. We perform English constituency parsing experiments for comparison with the original work by Shen et al. (2018a). We use the Wall Street Journal Section of the Penn Treebank (Marcus et al., 1999). We use parts 00-21 for training, 22 for validation and 23 for testing. Left/right-branching trees baseline (LBR/RBR). Our next baseline consists of purely left- or right-branching trees. LBR refers to the F1 score strictly left-branching binary trees obtain compared to the gold annotations, and RBR denotes the score of strictly r"
D19-6123,P19-1180,0,0.0163381,"s as part of our results due to lack of availability of trustworthy implementations. Following the success of the neural PRPN model in 2018, various other neural unsupervised parsing approaches have been developed and shown promising results. Shen et al. (2018c) enhance the vanilla LSTM network with master forget and input gates to learn the tree structure through soft gating. Drozdov et al. (2019) use a recursive autoencoder-based architecture. Kim et al. (2019b) employ unsupervised recurrent neural network grammars, and Kim et al. (2019a) employ compound probabilistic context free grammars. Shi et al. (2019) show how image captions can be successfully leveraged to identify constituents in sentences. None of these papers performs an explicit analysis of differences between languages. Jin et al. (2019) extend the PCFG approach to show results on Chinese, English and German. There are certain question that remain unanswered about multilingual grammar induction, especially related to cross-lingual transfer and difference in hyper parameters. In this work, we focus on adaptSetup Since transfer learning is mostly needed and also particularly effective in the low-data regime, we combine the training set"
D19-6123,A97-1014,0,0.0531374,"ed for each of test and development, and the remaining 1434 constitute our training set. Chinese. We use the Chinese Penn Treebank v8.0 (Xue et al., 2005). Again, we randomly separate files into splits: the development and test sets consist of 300 files each, while the training set consists of the remaining 2407. Balanced trees baseline (BTB). Finally, this baseline is similar to LBR/RBR, but considering balanced binary trees, which are created by recursively splitting each span into halves. For odd lengths, the middle word becomes a part of the right subtree. German. We use the NEGRA corpus (Skut et al., 1997), which consists of approximately 350, 000 words of German newspaper text (20,602 sentences). We divide the dataset into training, development, and test splits as suggested by Dubey and Keller (2003). 4 4.1 Monolingual Experiments Language-Dependence of Hyperparameters Best binary tree upper bound (BB). Since our datasets contain n-ary trees, but the PRPN only produces binary trees, obtaining a perfect F1 score is impossible. This upper bound represents the best score which can be obtained with binary trees. It is of practical importance to know whether a set of hyperparameters found for one l"
D19-6123,P09-1009,0,0.028359,"l., 2018a,c). While the latter model family has been able to generate parse trees which show a high accordance with expert annotations, its members, with the prominent Parsing Reading Predict-Network (PRPN) being no exception, have mostly been evaluated on English.1 Thus, it is not obvious whether and when obtained results would hold true for other languages, especially if they are unrelated to English or dispose of significantly smaller training corpora. Some nonneural models for grammar induction—i.e., models, which perform unsupervised parsing—show language-dependent performance variation (Snyder et al., 2009), which motivates our investigation of recent neural models. In this work, we first aim to answer the following research questions, focusing on the parsingreading-predict network (PRPN; Shen et al., 2018a) and experimenting with Arabic, Chinese, Introduction Unsupervised parsing, the task of inducing hierarchical syntactic structure from a large amount of unlabeled text, has been widely studied in natural language processing (NLP) (Carroll and Charniak, 1992; Pereira and Schabes, 1992; Klein and Manning, 2002, 2004). Work on this task bears on open research questions involving human language l"
D19-6123,P11-2120,0,0.0220928,"r methods have been developed as well, many based on parameter sharing—as we do in this work—, e.g., for cross-lingual natural language inference (Conneau et al., 2018), morphological generation (Kann et al., 2017; McCarthy et al., 2019), dialogue systems (Schuster et al., 2019), or machine translation (Johnson et al., 2017b; Aharoni et al., 2019). While we are not aware of any previous work exploring cross-lingual transfer for unsupervised parsing as done in this paper, approaches have been developed which leverage high-resource language data for supervised parsing in low-resource languages (Søgaard, 2011; Naseem et al., 2012). ing the PRPN to a multilingual setting, since it is the first neural model which has been shown to obtain robust unsupervised parsing results. Although we have primarily focused on PRPN due to its overall success, it would be interesting to observe whether similar trends in relative performance among languages hold for other models mentioned above. We leave this for future work. A closely related line of research, which is often referred to as latent tree learning, aims to create a parse structure which is well-suited for a particular NLP application. Common choices are"
E17-1099,N12-1048,0,0.596074,"ITE Figure 1: Example output from the proposed framework in DE → EN simultaneous translation. The heat-map represents the soft alignment between the incoming source sentence (left, upto-down) and the emitted translation (top, leftto-right). The length of each column represents the number of source words being waited for before emitting the translation. Best viewed when zoomed digitally. Introduction Simultaneous translation, the task of translating content in real-time as it is produced, is an important tool for real-time understanding of spoken lectures or conversations (F¨ugen et al., 2007; Bangalore et al., 2012). Different from the typical machine translation (MT) task, in which translation quality is paramount, simultaneous translation requires balancing the trade-off between translation quality and time delay to ensure that users receive translated content in an expeditious manner (Mieno et al., 2015). A number of methods have been proposed to solve this problem, mostly in the context of phrase-based machine translation. These methods are based on a segmenter, which receives the input one word at a time, then decides when to send it to a MT system that translates each 1 Code and data can be found a"
E17-1099,D14-1140,0,0.667149,"Missing"
E17-1099,P14-2090,1,0.870912,"anslation quality is paramount, simultaneous translation requires balancing the trade-off between translation quality and time delay to ensure that users receive translated content in an expeditious manner (Mieno et al., 2015). A number of methods have been proposed to solve this problem, mostly in the context of phrase-based machine translation. These methods are based on a segmenter, which receives the input one word at a time, then decides when to send it to a MT system that translates each 1 Code and data can be found at https://github. com/nyu-dl/dl4mt-simul-trans. segment independently (Oda et al., 2014) or with a minimal amount of language model context (Bangalore et al., 2012). Independently of simultaneous translation, accuracy of standard MT systems has greatly improved with the introduction of neural-networkbased MT systems (NMT) (Sutskever et al., 2014; Bahdanau et al., 2014). Very recently, there have been a few efforts to apply NMT to simultaneous translation either through heuristic modifications to the decoding process (Cho and Esipova, 2016), or through the training of an independent segmentation network that chooses when to perform output using a standard NMT model (Satija and Pin"
E17-1099,P02-1040,0,0.113931,"put words X = {x1 , ..., xTs } to be translated in real-time. We define the simultaneous translation task as sequentially making two interleaved decisions: READ or WRITE . More precisely, the translator READ s a source word xη from the input buffer in chronological order as translation context, or WRITEs a translated word yτ onto the output buffer, resulting in output sentence Y = {y1 , ..., yTt }, and action sequence A = {a1 , ..., aT } consists of Ts READs and Tt WRITEs, so T = Ts + Tt . Similar to standard MT, we have a measure Q(Y ) to evaluate the translation quality, such as BLEU score (Papineni et al., 2002). For simultaneous translation we are also concerned with the fact that each action incurs a time delay D(A). D(A) will mainly be influenced by delay caused by READ, as this entails waiting for a human speaker to continue speaking (about 0.3s per word for an average speaker), while WRITE consists of generating a few words from a machine translaFigure 2: Illustration of the proposed framework: at each step, the NMT environment (left) computes a candidate translation. The recurrent agent (right) will the observation including the candidates and send back decisions–READ or WRITE. tion system, whi"
E17-1099,N13-1023,0,0.369226,"the decoder’s hypothesis. This is one of the limitations of the proposed framework, as the NMT environment is trained on complete source sentences and it may be difficult to predict the verb that has not been seen in the source sentence. One possible way is to fine-tune the NMT model on incomplete sentences to boost its prediction ability. We will leave this as future work. 7 Related Work Researchers commonly consider the problem of simultaneous machine translation in the scenario of real-time speech interpretation (F¨ugen et al., 2007; Bangalore et al., 2012; Fujita et al., 2013; Rangarajan Sridhar et al., 2013; Yarmohammadi et al., 2013). In this approach, the incoming speech stream required to be translated are first recognized and segmented based on an automatic speech recognition (ASR) system. The translation model then works independently based on each of these segments, potentially limiting the quality of translation. To avoid using a fixed segmentation algorithm, Oda et al. (2014) introduced a trainable segmentation component into their system, so that the segmentation leads to better translation quality. Grissom II et al. (2014) proposed a similar framework, however, based on reinforcement l"
E17-1099,P04-1077,0,0.0655476,"simultaneous machine translation, a reward must consider both quality and delay. Quality We evaluate the translation quality using metrics such as BLEU (Papineni et al., 2002). The BLEU score is defined as the weighted geometric average of the modified n-gram precision BLEU0 , multiplied by the brevity penalty BP to punish a short translation. In practice, the vanilla 1055 BLEU score is not a good metric at sentence level because being a geometric average, the score will reduce to zero if one of the precisions is zero. To avoid this, we used a smoothed version of BLEU for our implementation (Lin and Och, 2004). BLEU(Y, Y ∗ ) = BP · BLEU0 (Y, Y ∗ ), (5) where Y ∗ is the reference and Y is the output. We decompose BLEU and use the difference of partial BLEU scores as the reward, that is:  ∆BLEU0 (Y, Y ∗ , t) t<T rtQ = (6) BLEU(Y, Y ∗ ) t=T where Y t is the cumulative output at t (Y 0 = ∅), and ∆BLEU0 (Y, Y ∗ , t) = BLEU0 (Y t , Y ∗ ) − BLEU0 (Y t−1 , Y ∗ ). Obviously, if at = READ, no new words are written into Y , yielding rtQ = 0. Note that we do not multiply BP until the end of the sentence, as it would heavily penalize partial translation results. Delay As another critical feature, delay judges"
E17-1099,D16-1138,0,0.0160829,"SEQ) learning. Jaitly et al. (2015) proposed a SEQ 2 SEQ ASR model that takes fixedsized segments of the input sequence and outputs tokens based on each segment in real-time. It is trained with alignment information using supervised learning. A similar idea for online ASR is proposed by Luo et al. (2016). Similar to Satija and Pineau (2016), they also used reinforcement learning to decide whether to emit a token while reading a new input at each step. Although sharing some similarities, ASR is very different from simultaneous MT with a more intuitive definition for segmentation. In addition, Yu et al. (2016) recently proposed an online alignment model to help sentence compression and morphological inflection. They regarded the alignment between the input and output sequences as a hidden variable, and performed transitions over the input and output sequence. By contrast, the proposed READ and WRITE actions do not necessarily to be performed on aligned words (e.g. in Fig. 1), and are learned to balance the trade-off of quality and delay. 8 Conclusion We propose a unified framework to do neural simultaneous machine translation. To trade off quality and delay, we extensively explore various targets f"
E17-3017,W14-3346,0,0.0173175,"2016). Nematus is available under a permissive BSD license. By default, the training objective in Nematus is cross-entropy minimization on a parallel training corpus. Training is performed via stochastic gradient descent, or one of its variants with adaptive learning rate (Adadelta (Zeiler, 2012), RmsProp (Tieleman and Hinton, 2012), Adam (Kingma and Ba, 2014)). Additionally, Nematus supports minimum risk training (MRT) (Shen et al., 2016) to optimize towards an arbitrary, sentence-level loss function. Various MT metrics are supported as loss function, including smoothed sentence-level B LEU (Chen and Cherry, 2014), METEOR (Denkowski and Lavie, 2011), BEER (Stanojevic and Sima’an, 2014), and any interpolation of implemented metrics. To stabilize training, Nematus supports early stopping based on cross entropy, or an arbitrary loss function defined by the user. 4 Conclusion Acknowledgments This project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant agreements 645452 (QT21), 644333 (TraMOOC), 644402 (HimL) and 688139 (SUMMA). Usability Features References In addition to the main algorithms to train and decode with an NMT model, Nematus includes fe"
E17-3017,W11-2107,0,0.0197012,"r a permissive BSD license. By default, the training objective in Nematus is cross-entropy minimization on a parallel training corpus. Training is performed via stochastic gradient descent, or one of its variants with adaptive learning rate (Adadelta (Zeiler, 2012), RmsProp (Tieleman and Hinton, 2012), Adam (Kingma and Ba, 2014)). Additionally, Nematus supports minimum risk training (MRT) (Shen et al., 2016) to optimize towards an arbitrary, sentence-level loss function. Various MT metrics are supported as loss function, including smoothed sentence-level B LEU (Chen and Cherry, 2014), METEOR (Denkowski and Lavie, 2011), BEER (Stanojevic and Sima’an, 2014), and any interpolation of implemented metrics. To stabilize training, Nematus supports early stopping based on cross entropy, or an arbitrary loss function defined by the user. 4 Conclusion Acknowledgments This project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant agreements 645452 (QT21), 644333 (TraMOOC), 644402 (HimL) and 688139 (SUMMA). Usability Features References In addition to the main algorithms to train and decode with an NMT model, Nematus includes features aimed towards facilitating ex"
E17-3017,W14-3354,0,0.0551932,"Missing"
E17-3017,E17-2025,0,0.133573,"Missing"
E17-3017,W16-2209,1,0.131218,"axout before the softmax layer. • In both encoder and decoder word embedding layers, we do not use additional biases. • Compared to Look, Generate, Update decoder phases in Bahdanau et al. (2015), we implement Look, Update, Generate which drastically simplifies the decoder implementation (see Table 1). • Optionally, we perform recurrent Bayesian dropout (Gal, 2015). • Instead of a single word embedding at each source position, our input representations allows multiple features (or “factors”) at each time step, with the final embedding being the concatenation of the embeddings of each feature (Sennrich and Haddow, 2016). • We allow tying of embedding matrices (Press and Wolf, 2017; Inan et al., 2016). We will here describe some differences in more detail: available at https://github.com/rsennrich/nematus https://github.com/nyu-dl/dl4mt-tutorial 65 Proceedings of the EACL 2017 Software Demonstrations, Valencia, Spain, April 3-7 2017, pages 65–68 c 2017 Association for Computational Linguistics z0j being the reset and update gate activations. In this formulation, W0 , U0 , Wr0 , U0r , Wz0 , U0z are trained model parameters; σ is the logistic sigmoid activation function. The attention mechanism, ATT, inputs the"
E17-3017,W16-2323,1,0.216734,"Sutskever et al., 2014) has recently established itself as a new state-of-the art in machine translation. We present Nematus1 , a new toolkit for Neural Machine Translation. Nematus has its roots in the dl4mt-tutorial.2 We found the codebase of the tutorial to be compact, simple and easy to extend, while also producing high translation quality. These characteristics make it a good starting point for research in NMT. Nematus has been extended to include new functionality based on recent research, and has been used to build top-performing systems to last year’s shared translation tasks at WMT (Sennrich et al., 2016) and IWSLT (Junczys-Dowmunt and Birch, 2016). Nematus is implemented in Python, and based on the Theano framework (Theano Development Team, 2016). It implements an attentional encoder–decoder architecture similar to Bahdanau et al. (2015). Our neural network architecture differs in some aspect from theirs, and we will discuss differences in more detail. We will also describe additional functionality, aimed to enhance usability and performance, which has been implemented in Nematus. 1 2 Neural Network Architecture • In the decoder, we use a feedforward hidden layer with tanh non-linearity rathe"
E17-3017,P16-1159,0,0.0772511,"ll documented toolkit to support their research. The toolkit is by no means limited to research, and has been used to train MT systems that are currently in production (WIPO, 2016). Nematus is available under a permissive BSD license. By default, the training objective in Nematus is cross-entropy minimization on a parallel training corpus. Training is performed via stochastic gradient descent, or one of its variants with adaptive learning rate (Adadelta (Zeiler, 2012), RmsProp (Tieleman and Hinton, 2012), Adam (Kingma and Ba, 2014)). Additionally, Nematus supports minimum risk training (MRT) (Shen et al., 2016) to optimize towards an arbitrary, sentence-level loss function. Various MT metrics are supported as loss function, including smoothed sentence-level B LEU (Chen and Cherry, 2014), METEOR (Denkowski and Lavie, 2011), BEER (Stanojevic and Sima’an, 2014), and any interpolation of implemented metrics. To stabilize training, Nematus supports early stopping based on cross entropy, or an arbitrary loss function defined by the user. 4 Conclusion Acknowledgments This project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant agreements 645452 (QT2"
K18-3006,E14-1060,0,0.0205784,"icit low-resource settings were first introduced to the shared task. These settings demonstrated the effectiveness of hard attention in neural sequence-to-sequence models if training data are limited (Makarov et al., 2017). Research not immediately done for the shared tasks included papers on multi-source reinflection (Cotterell et al., 2017b; Kann et al., 2017a), cross-lingual transfer for reinflection (Kann et al., 2017b), or first intents of neural inflection systems which make use of context for lemmatization (Bergmanis and Goldwater, 2018). Older work on morphological inflection includes Ahlberg et al. (2014); Durrett and DeNero (2013); Nicolai et al. (2015); Faruqui et al. (2016), inter alia. Table 1: Test accuracies when considering only the gold solution; BL = BASELINE; CPH = COPENHAGEN; CUB = CUBoulder. Best results per language in bold; our results in italic. BL BME-HAS CPH CUB NYU UZH de en es fi ru sv 0.10 2.92 11.08 0.89 2.71 0.96 31.14 62.64 33.52 11.18 21.29 27.34 21.54 66.87 37.31 16.14 24.40 36.38 11.53 66.36 31.42 10.04 22.59 19.04 48.43 72.21 31.98 18.68 23.29 37.13 61.38 74.02 37.17 28.21 30.42 39.36 av. 3.11 31.18 33.77 26.83 38.62 45.09 Related Work Table 2: Test accuracies when c"
K18-3006,N15-1093,0,0.018475,"o the shared task. These settings demonstrated the effectiveness of hard attention in neural sequence-to-sequence models if training data are limited (Makarov et al., 2017). Research not immediately done for the shared tasks included papers on multi-source reinflection (Cotterell et al., 2017b; Kann et al., 2017a), cross-lingual transfer for reinflection (Kann et al., 2017b), or first intents of neural inflection systems which make use of context for lemmatization (Bergmanis and Goldwater, 2018). Older work on morphological inflection includes Ahlberg et al. (2014); Durrett and DeNero (2013); Nicolai et al. (2015); Faruqui et al. (2016), inter alia. Table 1: Test accuracies when considering only the gold solution; BL = BASELINE; CPH = COPENHAGEN; CUB = CUBoulder. Best results per language in bold; our results in italic. BL BME-HAS CPH CUB NYU UZH de en es fi ru sv 0.10 2.92 11.08 0.89 2.71 0.96 31.14 62.64 33.52 11.18 21.29 27.34 21.54 66.87 37.31 16.14 24.40 36.38 11.53 66.36 31.42 10.04 22.59 19.04 48.43 72.21 31.98 18.68 23.29 37.13 61.38 74.02 37.17 28.21 30.42 39.36 av. 3.11 31.18 33.77 26.83 38.62 45.09 Related Work Table 2: Test accuracies when counting all plausible forms as correct; BL = BASEL"
K18-3006,N18-1126,0,0.150462,"in the 2017 edition of the shared task (Cotterell et al., 2017a). In 2017, explicit low-resource settings were first introduced to the shared task. These settings demonstrated the effectiveness of hard attention in neural sequence-to-sequence models if training data are limited (Makarov et al., 2017). Research not immediately done for the shared tasks included papers on multi-source reinflection (Cotterell et al., 2017b; Kann et al., 2017a), cross-lingual transfer for reinflection (Kann et al., 2017b), or first intents of neural inflection systems which make use of context for lemmatization (Bergmanis and Goldwater, 2018). Older work on morphological inflection includes Ahlberg et al. (2014); Durrett and DeNero (2013); Nicolai et al. (2015); Faruqui et al. (2016), inter alia. Table 1: Test accuracies when considering only the gold solution; BL = BASELINE; CPH = COPENHAGEN; CUB = CUBoulder. Best results per language in bold; our results in italic. BL BME-HAS CPH CUB NYU UZH de en es fi ru sv 0.10 2.92 11.08 0.89 2.71 0.96 31.14 62.64 33.52 11.18 21.29 27.34 21.54 66.87 37.31 16.14 24.40 36.38 11.53 66.36 31.42 10.04 22.59 19.04 48.43 72.21 31.98 18.68 23.29 37.13 61.38 74.02 37.17 28.21 30.42 39.36 av. 3.11 31."
K18-3006,K18-3001,1,0.854511,"Missing"
K18-3006,K17-2001,0,0.0946098,"Missing"
K18-3006,E17-2120,0,0.0118469,"CoNLL–SIGMORPHON 2017 shared tasks. The first edition of the shared task in 2016 (Cotterell et al., 2016) resulted in 3 different types of systems: “pipeline approaches” (unsupervised alignment algorithms applied to the source-target pairs, followed by a model which predicts edit operations), “neural approaches”, and “linguistically inspired systems”. The winning system was a neural network, namely a character-based RNN encoder-decoder model with attention, similar to the one we use here (Kann and Sch¨utze, 2016). Hence, neural models gained popularity in the 2017 edition of the shared task (Cotterell et al., 2017a). In 2017, explicit low-resource settings were first introduced to the shared task. These settings demonstrated the effectiveness of hard attention in neural sequence-to-sequence models if training data are limited (Makarov et al., 2017). Research not immediately done for the shared tasks included papers on multi-source reinflection (Cotterell et al., 2017b; Kann et al., 2017a), cross-lingual transfer for reinflection (Kann et al., 2017b), or first intents of neural inflection systems which make use of context for lemmatization (Bergmanis and Goldwater, 2018). Older work on morphological inf"
K18-3006,N13-1138,0,0.0309843,"ngs were first introduced to the shared task. These settings demonstrated the effectiveness of hard attention in neural sequence-to-sequence models if training data are limited (Makarov et al., 2017). Research not immediately done for the shared tasks included papers on multi-source reinflection (Cotterell et al., 2017b; Kann et al., 2017a), cross-lingual transfer for reinflection (Kann et al., 2017b), or first intents of neural inflection systems which make use of context for lemmatization (Bergmanis and Goldwater, 2018). Older work on morphological inflection includes Ahlberg et al. (2014); Durrett and DeNero (2013); Nicolai et al. (2015); Faruqui et al. (2016), inter alia. Table 1: Test accuracies when considering only the gold solution; BL = BASELINE; CPH = COPENHAGEN; CUB = CUBoulder. Best results per language in bold; our results in italic. BL BME-HAS CPH CUB NYU UZH de en es fi ru sv 0.10 2.92 11.08 0.89 2.71 0.96 31.14 62.64 33.52 11.18 21.29 27.34 21.54 66.87 37.31 16.14 24.40 36.38 11.53 66.36 31.42 10.04 22.59 19.04 48.43 72.21 31.98 18.68 23.29 37.13 61.38 74.02 37.17 28.21 30.42 39.36 av. 3.11 31.18 33.77 26.83 38.62 45.09 Related Work Table 2: Test accuracies when counting all plausible forms"
K18-3006,N16-1077,0,0.0191269,"e settings demonstrated the effectiveness of hard attention in neural sequence-to-sequence models if training data are limited (Makarov et al., 2017). Research not immediately done for the shared tasks included papers on multi-source reinflection (Cotterell et al., 2017b; Kann et al., 2017a), cross-lingual transfer for reinflection (Kann et al., 2017b), or first intents of neural inflection systems which make use of context for lemmatization (Bergmanis and Goldwater, 2018). Older work on morphological inflection includes Ahlberg et al. (2014); Durrett and DeNero (2013); Nicolai et al. (2015); Faruqui et al. (2016), inter alia. Table 1: Test accuracies when considering only the gold solution; BL = BASELINE; CPH = COPENHAGEN; CUB = CUBoulder. Best results per language in bold; our results in italic. BL BME-HAS CPH CUB NYU UZH de en es fi ru sv 0.10 2.92 11.08 0.89 2.71 0.96 31.14 62.64 33.52 11.18 21.29 27.34 21.54 66.87 37.31 16.14 24.40 36.38 11.53 66.36 31.42 10.04 22.59 19.04 48.43 72.21 31.98 18.68 23.29 37.13 61.38 74.02 37.17 28.21 30.42 39.36 av. 3.11 31.18 33.77 26.83 38.62 45.09 Related Work Table 2: Test accuracies when counting all plausible forms as correct; BL = BASELINE; CPH = COPENHAGEN;"
K18-3006,E17-1049,1,0.884614,"Missing"
K18-3006,P17-1182,1,0.895074,"Missing"
K18-3006,W16-2010,1,0.8906,"Missing"
K18-3006,P16-1162,0,0.0295864,"ter level, i.e., the input to the system is the character sequence of the input lemma, represented by embeddings. The output is the (predicted) character sequence of the inflected form. Additionally, we include the sentence context as follows: Given a sentence s = [w1 , w2 , . . . , wi−1 , l, wi+1 , . . . , wn ], where l is the lemma of the inflected form of interest, and w1 , . . . , wn with n 6= i are the surrounding context words, we split the past context cprev = [w1 , w2 , . . . , wi−1 ] and the future context cf ut = [wi+1 , . . . , wn ] into subword units using byte pair encoding (BPE, Sennrich et al. (2016)). We then use two additional encoders to encode the sequences of subword units of both contexts. Using bidirectional encoders, the final hidden states produced by each encoder are concatena4 4.1 Official System Evaluation Datasets The data for Task 2, track 2, LOW consists of sentences taken from the Universal Dependencies 59 Decoder (characters) ... Lemma (characters) ... ... Past context (BPE) Future context (BPE) Attention mechanism Attention mechanism Attention mechanism Figure 1: Overview of our employed model architecture. the lemma of the word at the next position in the sentence, (v)"
K18-3006,P15-2111,0,\N,Missing
K18-3006,K17-2003,1,\N,Missing
K18-3006,K17-2002,1,\N,Missing
K18-3006,K17-3001,0,\N,Missing
K18-3006,W17-4110,1,\N,Missing
K18-3006,N18-2087,0,\N,Missing
K18-3006,P18-1245,0,\N,Missing
K18-3006,L18-1293,0,\N,Missing
K18-3006,W18-6011,0,\N,Missing
K18-3006,K18-3004,0,\N,Missing
K18-3006,K18-3010,0,\N,Missing
K18-3006,K18-3013,0,\N,Missing
K18-3006,K18-3003,0,\N,Missing
K18-3006,K18-3016,0,\N,Missing
K18-3006,K18-3005,0,\N,Missing
N16-1034,W06-0901,0,0.0632536,"Missing"
N16-1034,P14-1023,0,0.0168519,"indow of 2 for the local features, and the feed-forward neural networks with one hidden layer for F trg , F arg and F binary (the size of the hidden layers are 600, 600 and 300 respectively). Finally, for training, we use the mini-batch size = 50 and the parameter for the Frobenius norms = 3. These parameter values are either inherited from the prior research (Nguyen and Grishman, 2015b; Chen et al., 2015) or selected according to the validation data. We pre-train the word embeddings from the English Gigaword corpus utilizing the word2vec toolkit4 (modified to add the C-CBOW model). Following Baroni et al. (2014), we employ the context window of 5, the subsampling of the frequent words set to 1e-05 and 10 negative samples. We evaluate the model with the ACE 2005 corpus. For the purpose of comparison, we use the same data split as the previous work (Ji and Grishman, 2008; Liao and Grishman, 2010; Li et al., 2013; Nguyen and Grishman, 2015b; Chen et al., 2015). This data split includes 40 newswire articles (672 sentences) for the test set, 30 other documents (836 sentences) for the development set and 529 remaining documents (14,849 sentences) for the training set. Also, 4 https://code.google.com/p/word"
N16-1034,P15-1017,0,0.470371,"cameraman as the Target argument for the event Attack with their local features. The joint approach can overcome this issue by relying on the global features to encode the fact that a Victim argument for the Die event is often the Target argument for the Attack event in the same sentence. Despite the advantages presented above, the joint system by Li et al. (2013) suffers from the lack of generalization over the unseen words/features and the inability to extract the underlying structures for EE (due to its discrete representation from the handcrafted feature set) (Nguyen and Grishman, 2015b; Chen et al., 2015). The most successful pipelined system for EE to date (Chen et al., 2015) addresses these drawbacks of the joint system by Li et al. (2013) via dynamic multi-pooling convolutional neural networks (DMCNN). In this system, words are represented by the continuous representations (Bengio et al., 2003; Turian et al., 2010; Mikolov et al., 2013a) and features are automatically learnt from data by the DMCNN, thereby alleviating the unseen word/feature problem and extracting more effective features for the given dataset. However, as the system by Chen et al. (2015) is pipelined, it still suffers from"
N16-1034,P09-2093,0,0.0233377,"Missing"
N16-1034,P11-1113,0,0.350837,"Missing"
N16-1034,P08-1030,1,0.809113,"ter for the Frobenius norms = 3. These parameter values are either inherited from the prior research (Nguyen and Grishman, 2015b; Chen et al., 2015) or selected according to the validation data. We pre-train the word embeddings from the English Gigaword corpus utilizing the word2vec toolkit4 (modified to add the C-CBOW model). Following Baroni et al. (2014), we employ the context window of 5, the subsampling of the frequent words set to 1e-05 and 10 negative samples. We evaluate the model with the ACE 2005 corpus. For the purpose of comparison, we use the same data split as the previous work (Ji and Grishman, 2008; Liao and Grishman, 2010; Li et al., 2013; Nguyen and Grishman, 2015b; Chen et al., 2015). This data split includes 40 newswire articles (672 sentences) for the test set, 30 other documents (836 sentences) for the development set and 529 remaining documents (14,849 sentences) for the training set. Also, 4 https://code.google.com/p/word2vec/ we follow the criteria of the previous work (Ji and Grishman, 2008; Liao and Grishman, 2010; Li et al., 2013; Chen et al., 2015) to judge the correctness of the predicted event mentions. 5.2 Memory Vector/Matrices This section evaluates the effectiveness o"
N16-1034,D14-1181,0,0.00360714,"triggers and argument roles: C(T, A, X, E) = − log P (T, A|X, E) = − log P (T |X, E) − log P (A|T, X, E) n X trg =− log Pi;t∗ − i=1 n X i I(ti 6= “Other”) i=1 k X j=1 arg log Pij;a∗ ij where I is the indicator function. We apply the stochastic gradient descent algorithm with mini-batches and the AdaDelta update rule (Zeiler, 2012). The gradients are computed using back-propagation. During training, besides the weight matrices, we also optimize the word and entity type embedding tables to achieve the optimal states. Finally, we rescale the weights whose Frobenius norms exceed a hyperparameter (Kim, 2014; Nguyen and Grishman, 2015a). 4 Word Representation Following the prior work (Nguyen and Grishman, 2015b; Chen et al., 2015), we pre-train word embeddings from a large corpus and employ them to initialize the word embedding table. One of the models to train word embeddings have been proposed in Mikolov et al. (2013a; 2013b) that introduce two log-linear models, i.e the continuous bag305 of-words model (CBOW) and the continuous skipgram model (SKIP-GRAM). The CBOW model attempts to predict the current word based on the average of the context word vectors while the SKIPGRAM model aims to predic"
N16-1034,P13-1008,0,0.157368,"articipating into such events. This is an important and challenging task of information extraction in natural language processing (NLP), as the same event might be present in various expressions, and an expression might expresses different events in different contexts. There are two main approaches to EE: (i) the joint approach that predicts event triggers and arguments for sentences simultaneously as a structured prediction problem, and (ii) the pipelined approach that first performs trigger prediction and then identifies arguments in separate stages. The most successful joint system for EE (Li et al., 2013) is based on the structured perceptron algorithm with a large set of local and global features1 . These features are designed to capture the discrete structures that are intuitively helpful for EE using the NLP toolkits (e.g., part of speech tags, dependency and constituent tags). The advantages of such a joint system are twofold: (i) mitigating the error propagation from the upstream component (trigger identification) to the downstream classifier (argument identification), and (ii) benefiting from the the inter-dependencies among event triggers and argument roles via global features. For exam"
N16-1034,D14-1198,0,0.0615326,"Missing"
N16-1034,P10-1081,1,0.602342,"orms = 3. These parameter values are either inherited from the prior research (Nguyen and Grishman, 2015b; Chen et al., 2015) or selected according to the validation data. We pre-train the word embeddings from the English Gigaword corpus utilizing the word2vec toolkit4 (modified to add the C-CBOW model). Following Baroni et al. (2014), we employ the context window of 5, the subsampling of the frequent words set to 1e-05 and 10 negative samples. We evaluate the model with the ACE 2005 corpus. For the purpose of comparison, we use the same data split as the previous work (Ji and Grishman, 2008; Liao and Grishman, 2010; Li et al., 2013; Nguyen and Grishman, 2015b; Chen et al., 2015). This data split includes 40 newswire articles (672 sentences) for the test set, 30 other documents (836 sentences) for the development set and 529 remaining documents (14,849 sentences) for the training set. Also, 4 https://code.google.com/p/word2vec/ we follow the criteria of the previous work (Ji and Grishman, 2008; Liao and Grishman, 2010; Li et al., 2013; Chen et al., 2015) to judge the correctness of the predicted event mentions. 5.2 Memory Vector/Matrices This section evaluates the effectiveness of the memory vector and m"
N16-1034,R11-1002,1,0.897403,"mention. ACE annotates 8 types and 33 subtypes (e.g., Attack, Die, Start-Position) for event mentions that also correspond to the types and subtypes of the event triggers. Each event subtype has its own set of roles to be filled by the event arguments. For instance, the roles for the Die event include Place, Victim and Time. The total number of roles for all the event subtypes is 36. Given an English text document, an event extraction system needs to recognize event triggers with specific subtypes and their corresponding arguments with the roles for each sentence. Following the previous work (Liao and Grishman, 2011; Li et al., 2013; Chen et al., 2015), we assume that the argument candidates (i.e, the entity mentions, temporal expressions and values) are provided (by the ACE annotation) to the event extraction systems. 2 http://projects.ldc.upenn.edu/ace 3 Model We formalize the EE task as follow. Let W = w1 w2 . . . wn be a sentence where n is the sentence length and wi is the i-th token. Also, let E = e1 , e2 , . . . , ek be the entity mentions3 in this sentence (k is the number of the entity mentions and can be zero). Each entity mention comes with the offsets of the head and the entity type. We furth"
N16-1034,P11-1163,0,0.265714,"Missing"
N16-1034,W15-1506,1,0.433652,"oach might fail to recognize cameraman as the Target argument for the event Attack with their local features. The joint approach can overcome this issue by relying on the global features to encode the fact that a Victim argument for the Die event is often the Target argument for the Attack event in the same sentence. Despite the advantages presented above, the joint system by Li et al. (2013) suffers from the lack of generalization over the unseen words/features and the inability to extract the underlying structures for EE (due to its discrete representation from the handcrafted feature set) (Nguyen and Grishman, 2015b; Chen et al., 2015). The most successful pipelined system for EE to date (Chen et al., 2015) addresses these drawbacks of the joint system by Li et al. (2013) via dynamic multi-pooling convolutional neural networks (DMCNN). In this system, words are represented by the continuous representations (Bengio et al., 2003; Turian et al., 2010; Mikolov et al., 2013a) and features are automatically learnt from data by the DMCNN, thereby alleviating the unseen word/feature problem and extracting more effective features for the given dataset. However, as the system by Chen et al. (2015) is pipelined, i"
N16-1034,P15-2060,1,0.885827,"oach might fail to recognize cameraman as the Target argument for the event Attack with their local features. The joint approach can overcome this issue by relying on the global features to encode the fact that a Victim argument for the Die event is often the Target argument for the Attack event in the same sentence. Despite the advantages presented above, the joint system by Li et al. (2013) suffers from the lack of generalization over the unseen words/features and the inability to extract the underlying structures for EE (due to its discrete representation from the handcrafted feature set) (Nguyen and Grishman, 2015b; Chen et al., 2015). The most successful pipelined system for EE to date (Chen et al., 2015) addresses these drawbacks of the joint system by Li et al. (2013) via dynamic multi-pooling convolutional neural networks (DMCNN). In this system, words are represented by the continuous representations (Bengio et al., 2003; Turian et al., 2010; Mikolov et al., 2013a) and features are automatically learnt from data by the DMCNN, thereby alleviating the unseen word/feature problem and extracting more effective features for the given dataset. However, as the system by Chen et al. (2015) is pipelined, i"
N16-1034,D09-1016,0,0.0598437,"Missing"
N16-1034,N10-1123,0,0.0610405,"Missing"
N16-1034,D11-1001,0,0.0188606,"Missing"
N16-1034,W11-1807,0,0.0351499,"Missing"
N16-1034,W09-1406,0,0.0324773,"Missing"
N16-1034,P10-1040,0,0.0736485,"sented above, the joint system by Li et al. (2013) suffers from the lack of generalization over the unseen words/features and the inability to extract the underlying structures for EE (due to its discrete representation from the handcrafted feature set) (Nguyen and Grishman, 2015b; Chen et al., 2015). The most successful pipelined system for EE to date (Chen et al., 2015) addresses these drawbacks of the joint system by Li et al. (2013) via dynamic multi-pooling convolutional neural networks (DMCNN). In this system, words are represented by the continuous representations (Bengio et al., 2003; Turian et al., 2010; Mikolov et al., 2013a) and features are automatically learnt from data by the DMCNN, thereby alleviating the unseen word/feature problem and extracting more effective features for the given dataset. However, as the system by Chen et al. (2015) is pipelined, it still suffers from the inherent limitations of error propagation and failure to exploit the inter-dependencies between event triggers and argument roles (Li et al., 2013). Finally, we notice that the discrete features, shown to be helpful in the previous studies for EE (Li et al., 2013), are not considered in Chen et al. (2015). Guided"
N16-1034,D14-1090,0,0.135969,"Missing"
N16-1101,W14-4012,1,0.0614141,"Missing"
N16-1101,D14-1179,1,0.0760334,"Missing"
N16-1101,P15-1166,0,0.691319,"to build a system that maps a source sentence in any language to a common continuous representation space and decodes the representation into any of the target languages, allowing us to make a multilingual machine translation system. This possibility is straightforward to implement and has been validated in the case of basic encoderdecoder networks (Luong et al., 2015a). It is however not so, in the case of the attention-based encoder-decoder network, as the attention mechanism, or originally called the alignment function in (Bahdanau et al., 2014), is conceptually language pair-specific. In (Dong et al., 2015), the authors cleverly avoided this issue of language pair-specific attention mechanism by considering only a one-tomany translation, where each target language decoder embedded its own attention mechanism. Also, we notice that both of these works have only evaluated their models on relatively small-scale tasks, making it difficult to assess whether multilingual neural machine translation can scale beyond lowresource language translation. Multi-Way, Multilingual Neural Machine Translation In this paper, we first step back from the currently available multilingual neural translation systems pro"
N16-1101,W15-3014,1,0.223449,"), a remedy to this issue was proposed by incorporating an attention mechanism to the basic encoder-decoder network. The attention mechanism in the encoder-decoder network frees the network from having to map a sequence of arbitrary length to a single, fixed-dimensional vector. Since this attention mechanism was introduced to the encoder-decoder network for machine translation, neural machine translation, which is purely based on neural networks to perform full end-to-end translation, has become competitive with the existing phrase-based statistical machine translation in many language pairs (Jean et al., 2015; Gulcehre et al., 2015; Luong et al., 2015b). Multilingual Neural Machine Translation Existing machine translation systems, mostly based on a phrase-based system or its variants, work by directly mapping a symbol or a subsequence of symbols in a source language to its corresponding symbol or subsequence in a target language. This kind of mapping is strictly specific to a given language pair, and it is not trivial to extend this mapping to work on multiple pairs of languages. A system based on neural machine translation, on the other hand, can be decomposed into two mod866 Proceedings of NAACL"
N16-1101,D15-1166,0,0.46911,"incorporating an attention mechanism to the basic encoder-decoder network. The attention mechanism in the encoder-decoder network frees the network from having to map a sequence of arbitrary length to a single, fixed-dimensional vector. Since this attention mechanism was introduced to the encoder-decoder network for machine translation, neural machine translation, which is purely based on neural networks to perform full end-to-end translation, has become competitive with the existing phrase-based statistical machine translation in many language pairs (Jean et al., 2015; Gulcehre et al., 2015; Luong et al., 2015b). Multilingual Neural Machine Translation Existing machine translation systems, mostly based on a phrase-based system or its variants, work by directly mapping a symbol or a subsequence of symbols in a source language to its corresponding symbol or subsequence in a target language. This kind of mapping is strictly specific to a given language pair, and it is not trivial to extend this mapping to work on multiple pairs of languages. A system based on neural machine translation, on the other hand, can be decomposed into two mod866 Proceedings of NAACL-HLT 2016, pages 866–875, c San Diego, Cali"
N16-1101,P16-1162,0,\N,Missing
N16-1162,S14-2010,0,0.0519061,". (2015): a logistic regression classifier is trained on top of sentence representations, with 10-fold cross-validation used when a train-test split is not pre-defined. 3.2 Unsupervised Evaluations We also measure how well representation spaces reflect human intuitions of the semantic sentence relatedness, by computing the cosine distance between vectors for the two sentences in each test pair, and correlating these distances with gold-standard human judgements. The SICK dataset (Marelli et al., 1371 2014) consists of 10,000 pairs of sentences and relatedness judgements. The STS 2014 dataset (Agirre et al., 2014) consists of 3,750 pairs and ratings from six linguistic domains. Example ratings are shown in Table 2. All available pairs are used for testing apart from the 500 SICK ‘trial’ pairs, which are held-out for tuning hyperparameters (representation size of log-bilinear models, and noise parameters in SDAE). The optimal settings on this task are then applied to both supervised and unsupervised evaluations. 4 Results Performance of the models on the supervised evaluations (grouped according to the data required by their objective) is shown in Table 3. Overall, SkipThought vectors perform best on th"
N16-1162,2014.lilt-9.5,0,0.0108545,"so propose two new unsupervised representation-learning objectives designed to optimise the trade-off between training time, domain portability and performance. 1 Introduction Distributed representations - dense real-valued vectors that encode the semantics of linguistic units are ubiquitous in today’s NLP research. For singlewords or word-like entities, there are established ways to acquire such representations from naturally occurring (unlabelled) training data based on comparatively task-agnostic objectives (such as predicting adjacent words). These methods are well understood empirically (Baroni et al., 2014b) and theoretically (Levy and Goldberg, 2014). The best word representation spaces reflect consistently-observed aspects of human conceptual organisation (Hill et al., 2015b), and can be added as features to improve the performance of numerous language processing systems (Collobert et al., 2011). By contrast, there is comparatively little consensus on the best ways to learn distributed representations of phrases or sentences.1 With the advent of deeper language processing techniques, it is relatively common for models to represent phrases or sentences as continuous-valued vectors. Examples in"
N16-1162,P14-1023,0,0.643214,"so propose two new unsupervised representation-learning objectives designed to optimise the trade-off between training time, domain portability and performance. 1 Introduction Distributed representations - dense real-valued vectors that encode the semantics of linguistic units are ubiquitous in today’s NLP research. For singlewords or word-like entities, there are established ways to acquire such representations from naturally occurring (unlabelled) training data based on comparatively task-agnostic objectives (such as predicting adjacent words). These methods are well understood empirically (Baroni et al., 2014b) and theoretically (Levy and Goldberg, 2014). The best word representation spaces reflect consistently-observed aspects of human conceptual organisation (Hill et al., 2015b), and can be added as features to improve the performance of numerous language processing systems (Collobert et al., 2011). By contrast, there is comparatively little consensus on the best ways to learn distributed representations of phrases or sentences.1 With the advent of deeper language processing techniques, it is relatively common for models to represent phrases or sentences as continuous-valued vectors. Examples in"
N16-1162,C04-1051,0,0.805554,"g with human relatedness judgements - unspervised evaluation (Hill et al., 2015a; Baroni et al., 2014b; Levy et al., 2015). The former setting reflects a scenario in which representations are used to inject general knowledge (sometimes considered as pre-training) into a supervised model. The latter pertains to applications in which the sentence representation space is used for direct comparisons, lookup or retrieval. Here, we apply and compare both evaluation paradigms. 3.1 Supervised Evaluations Representations are applied to 6 sentence classification tasks: paraphrase identification (MSRP) (Dolan et al., 2004), movie review sentiment (MR) (Pang and Lee, 2005), product reviews (CR) (Hu and Liu, 2004), subjectivity classification (SUBJ) (Pang and Lee, 2004), opinion polarity (MPQA) (Wiebe et al., 2005) and question type classification (TREC) (Voorhees, 2002). We follow the procedure (and code) of Kiros et al. (2015): a logistic regression classifier is trained on top of sentence representations, with 10-fold cross-validation used when a train-test split is not pre-defined. 3.2 Unsupervised Evaluations We also measure how well representation spaces reflect human intuitions of the semantic sentence rel"
N16-1162,J15-4004,1,0.722961,"Distributed representations - dense real-valued vectors that encode the semantics of linguistic units are ubiquitous in today’s NLP research. For singlewords or word-like entities, there are established ways to acquire such representations from naturally occurring (unlabelled) training data based on comparatively task-agnostic objectives (such as predicting adjacent words). These methods are well understood empirically (Baroni et al., 2014b) and theoretically (Levy and Goldberg, 2014). The best word representation spaces reflect consistently-observed aspects of human conceptual organisation (Hill et al., 2015b), and can be added as features to improve the performance of numerous language processing systems (Collobert et al., 2011). By contrast, there is comparatively little consensus on the best ways to learn distributed representations of phrases or sentences.1 With the advent of deeper language processing techniques, it is relatively common for models to represent phrases or sentences as continuous-valued vectors. Examples include machine translation (Sutskever et al., 2014), image captioning (Mao et al., 2015) and dialogue systems (Serban et al., 2015). While it has been observed informally tha"
N16-1162,P15-1162,0,0.0615922,"Missing"
N16-1162,D13-1090,0,0.0341145,"ttings on this task are then applied to both supervised and unsupervised evaluations. 4 Results Performance of the models on the supervised evaluations (grouped according to the data required by their objective) is shown in Table 3. Overall, SkipThought vectors perform best on three of the six evaluations, the BOW DictRep model with pretrained word embeddings performs best on two, and the SDAE on one. SDAEs perform notably well on the paraphrasing task, going beyond SkipThought by three percentage points and approaching stateof-the-art performance of models designed specifically for the task (Ji and Eisenstein, 2013). SDAE is also consistently better than SAE, which aligns with other findings that adding noise to AEs produces richer representations (Vincent et al., 2008). Results on the unsupervised evaluations are shown in Table 4. The same DictRep model performs best on four of the six STS categories (and overall) and is joint-top performer on SICK. Of the models trained on raw text, simply adding CBOW word vectors works best on STS. The best performing raw text model on SICK is FastSent, which achieves almost identical performance to CPHRASE’s state-of-the-art performance for a distributed model (Pham"
N16-1162,P14-1062,0,0.00809071,"ral language models that compute sentence representations from unlabelled, naturally-ocurring data, as with the predominant methods for word representations.2 Likewise, we do not focus on ‘bottom up’ models where phrase or sentence representations are built from fixed mathe proposed bymatical operations on word vectors (although we do consider a canonical case - see CBOW below); these were already compared by Milajevs et al. (2014). Most space is devoted to our novel approaches, and we refer the 2 This excludes innovative supervised sentence-level architectures including (Socher et al., 2011; Kalchbrenner et al., 2014) and many others. 1368 reader to the original papers for more details of existing models. 2.1 Existing Models Trained on Text SkipThought Vectors For consecutive sentences Si−1 , Si , Si+1 in some document, the SkipThought model (Kiros et al., 2015) is trained to predict target sentences Si−1 and Si+1 given source sentence Si . As with all sequence-to-sequence models, in training the source sentence is ‘encoded’ by a Recurrent Neural Network (RNN) (with Gated Recurrent uUnits (Cho et al., 2014)) and then ‘decoded’ into the two target sentences in turn. Importantly, because RNNs employ a single"
N16-1162,Q15-1016,0,0.0319935,"ion. A man is jumping into a full pool. /5 4 3.8 3.6 1.6 4.4 0.4 1.7 Table 2: Example sentence pairs and ‘similarity’ ratings from the unsupervised evaluations used in this study. 3 Evaluating Sentence Representations In previous work, distributed representations of language were evaluated either by measuring the effect of adding representations as features in some classification task - supervised evaluation (Collobert et al., 2011; Mikolov et al., 2013a; Kiros et al., 2015) - or by comparing with human relatedness judgements - unspervised evaluation (Hill et al., 2015a; Baroni et al., 2014b; Levy et al., 2015). The former setting reflects a scenario in which representations are used to inject general knowledge (sometimes considered as pre-training) into a supervised model. The latter pertains to applications in which the sentence representation space is used for direct comparisons, lookup or retrieval. Here, we apply and compare both evaluation paradigms. 3.1 Supervised Evaluations Representations are applied to 6 sentence classification tasks: paraphrase identification (MSRP) (Dolan et al., 2004), movie review sentiment (MR) (Pang and Lee, 2005), product reviews (CR) (Hu and Liu, 2004), subjectivi"
N16-1162,marelli-etal-2014-sick,0,0.0889193,"Missing"
N16-1162,D14-1079,0,0.0295207,"t of most current language understanding systems. We address this issue with a systematic comparison of cutting-edge methods for learning distributed representations of sentences. We focus on methods that do not require labelled data gathered for the purpose of training models, since such methods are more cost-effective and applicable across languages and domains. We also propose two new phrase or sentence representation learning objectives - Sequential Denoising Autoencoders (SDAEs) 1 See the contrasting conclusions in (Mitchell and Lapata, 2008; Clark and Pulman, 2007; Baroni et al., 2014a; Milajevs et al., 2014) among others. 1367 Proceedings of NAACL-HLT 2016, pages 1367–1377, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics and FastSent, a sentence-level log-bilinear bag-ofwords model. We compare all methods on two types of task - supervised and unsupervised evaluations - reflecting different ways in which representations are ultimately to be used. In the former setting, a classifier or regression model is applied to representations and trained with task-specific labelled data, while in the latter, representation spaces are directly queried using cosine dist"
N16-1162,P08-1028,0,0.0164017,"owledge (or ‘common sense’) (Norman, 1972) that is a critical missing part of most current language understanding systems. We address this issue with a systematic comparison of cutting-edge methods for learning distributed representations of sentences. We focus on methods that do not require labelled data gathered for the purpose of training models, since such methods are more cost-effective and applicable across languages and domains. We also propose two new phrase or sentence representation learning objectives - Sequential Denoising Autoencoders (SDAEs) 1 See the contrasting conclusions in (Mitchell and Lapata, 2008; Clark and Pulman, 2007; Baroni et al., 2014a; Milajevs et al., 2014) among others. 1367 Proceedings of NAACL-HLT 2016, pages 1367–1377, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics and FastSent, a sentence-level log-bilinear bag-ofwords model. We compare all methods on two types of task - supervised and unsupervised evaluations - reflecting different ways in which representations are ultimately to be used. In the former setting, a classifier or regression model is applied to representations and trained with task-specific labelled data, while in th"
N16-1162,P04-1035,0,0.083734,"cts a scenario in which representations are used to inject general knowledge (sometimes considered as pre-training) into a supervised model. The latter pertains to applications in which the sentence representation space is used for direct comparisons, lookup or retrieval. Here, we apply and compare both evaluation paradigms. 3.1 Supervised Evaluations Representations are applied to 6 sentence classification tasks: paraphrase identification (MSRP) (Dolan et al., 2004), movie review sentiment (MR) (Pang and Lee, 2005), product reviews (CR) (Hu and Liu, 2004), subjectivity classification (SUBJ) (Pang and Lee, 2004), opinion polarity (MPQA) (Wiebe et al., 2005) and question type classification (TREC) (Voorhees, 2002). We follow the procedure (and code) of Kiros et al. (2015): a logistic regression classifier is trained on top of sentence representations, with 10-fold cross-validation used when a train-test split is not pre-defined. 3.2 Unsupervised Evaluations We also measure how well representation spaces reflect human intuitions of the semantic sentence relatedness, by computing the cosine distance between vectors for the two sentences in each test pair, and correlating these distances with gold-standa"
N16-1162,P05-1015,0,0.0593828,"evaluation (Hill et al., 2015a; Baroni et al., 2014b; Levy et al., 2015). The former setting reflects a scenario in which representations are used to inject general knowledge (sometimes considered as pre-training) into a supervised model. The latter pertains to applications in which the sentence representation space is used for direct comparisons, lookup or retrieval. Here, we apply and compare both evaluation paradigms. 3.1 Supervised Evaluations Representations are applied to 6 sentence classification tasks: paraphrase identification (MSRP) (Dolan et al., 2004), movie review sentiment (MR) (Pang and Lee, 2005), product reviews (CR) (Hu and Liu, 2004), subjectivity classification (SUBJ) (Pang and Lee, 2004), opinion polarity (MPQA) (Wiebe et al., 2005) and question type classification (TREC) (Voorhees, 2002). We follow the procedure (and code) of Kiros et al. (2015): a logistic regression classifier is trained on top of sentence representations, with 10-fold cross-validation used when a train-test split is not pre-defined. 3.2 Unsupervised Evaluations We also measure how well representation spaces reflect human intuitions of the semantic sentence relatedness, by computing the cosine distance between"
N16-1162,P15-1094,0,0.0499898,"stimate sentence representations s for arbitrary sentences based on the vw . This additional computation is reflected in the higher encoding time in Table 1 (TE). ILSVRC 2014 object recognition task (Russakovsky et al., 2014). Multi-modal distributed representations can be encoded by feeding test sentences forward through the trained model. Bottom-Up Methods We train CBOW and SkipGram word embeddings (Mikolov et al., 2013b) on the same text corpus as the SkipThought and ParagraphVector models, and compose by elementwise addition as per Mitchell and Lapata (2010).4 We also compare to C-PHRASE (Pham et al., 2015), an approach that exploits a (supervised) parser to infer distributed semantic representations based on a syntactic parse of sentences. C-PHRASE achieves state-of-the-art results for distributed representations on several evaluations used in this study.5 NMT We consider the sentence representations learned by neural MT models. These models have identical architecture to SkipThought, but are trained on sentence-aligned translated texts. We used a standard architecture (Cho et al., 2014) on all available En-Fr and En-De data from the 2015 Workshop on Statistical MT (WMT).7 Non-Distributed Basel"
N16-1162,W15-2701,0,0.0147471,"an others in this study. 6 https://www.cl.cam.ac.uk/˜fh295/. Definitions from the training data matching those in the WordNet STS 2014 evaluation (used in this study) were excluded. 5 1369 2.3 Novel Text-Based Models We introduce two new approaches designed to address certain limitations with the existing models. 7 www.statmt.org/wmt15/translation-task.html FastSent The performance of SkipThought vectors shows that rich sentence semantics can be inferred from the content of adjacent sentences. The model could be said to exploit a type of sentence-level Distributional Hypothesis (Harris, 1954; Polajnar et al., 2015). Nevertheless, like many deep neural language models, SkipThought is very slow to train (see Table 1). FastSent is a simple additive (log-bilinear) sentence model designed to exploit the same signal, but at much lower computational expense. Given a BOW representation of some sentence in context, the model simply predicts adjacent sentences (also represented as BOW) . More formally, FastSent learns a source uw and target vw embedding for each word in the model vocabulary. For a training example Si−1 , Si , Si+1 of consecutive sentences, Si is represented as the sum P of its source embeddings s"
N16-1162,D11-1014,0,0.0156144,"lysis, we compare neural language models that compute sentence representations from unlabelled, naturally-ocurring data, as with the predominant methods for word representations.2 Likewise, we do not focus on ‘bottom up’ models where phrase or sentence representations are built from fixed mathe proposed bymatical operations on word vectors (although we do consider a canonical case - see CBOW below); these were already compared by Milajevs et al. (2014). Most space is devoted to our novel approaches, and we refer the 2 This excludes innovative supervised sentence-level architectures including (Socher et al., 2011; Kalchbrenner et al., 2014) and many others. 1368 reader to the original papers for more details of existing models. 2.1 Existing Models Trained on Text SkipThought Vectors For consecutive sentences Si−1 , Si , Si+1 in some document, the SkipThought model (Kiros et al., 2015) is trained to predict target sentences Si−1 and Si+1 given source sentence Si . As with all sequence-to-sequence models, in training the source sentence is ‘encoded’ by a Recurrent Neural Network (RNN) (with Gated Recurrent uUnits (Cho et al., 2014)) and then ‘decoded’ into the two target sentences in turn. Importantly,"
N18-4017,P15-1162,0,0.0763168,"Missing"
N18-4017,D15-1075,1,0.692716,"a machine reader as shown in Figure 1. The focus of our work is to improve the ranker for QA performance. We use DrQA’s Document Reader as our reader. We train our ranker and reader models on QUASAR-T (Dhingra et al., 2017b) dataset. QUASAR-T provides a collection top 100 short paragraphs returned by search engine for each question in the dataset. Our goal is to find the correct answer span for a given question. 3 3.1 3.2.1 InferSent Ranker InferSent (Conneau et al., 2017) provides distributed representations for sentences.3 It is trained on Stanford Natural Language Inference Dataset (SNLI; Bowman et al., 2015) and MultiGenre NLI Corpus (MultiNLI; Williams et al., 2017) using supervised learning. It generalizes well and outperforms unsupervised sentence representations such as Skip-Thought Vectors (Kiros et al., 2015) in a variety of tasks. As InferSent representation captures the general semantics of a sentence, we use it to implement the ranker that ranks based on semantic similarity. To compose sentence representations into a paragraph representation, we simply sum the InferSent representations of all the sentences in the paragraph. This approach is inspired by the sum of word representations as"
N18-4017,P17-1171,0,0.461914,"e New York, NY 10003 2 Abstract 3 Dept. of Computer Science New York University 60 Fifth Avenue New York, NY 10011 looks for the answer in the unstructured text corpus (Brill et al., 2001). This approach eliminates the need to build and update knowledge bases by taking advantage of the large amount of text data available on the web. Complex parsing rules and information extraction methods are required to extract answers from unstructured text. As machine readers are excellent at this task, there have been attempts to combine search engines with machine reading for corpus-based open-domain QA (Chen et al., 2017; Wang et al., 2017). To achieve high accuracy in this setting, the top documents retrieved by the search engine must be relevant to the question. As the top ranked documents returned from search engine might not contain the answer that the machine reader is looking for, reranking the documents based on the likelihood of containing answer will improve the overall QA performance. Our focus is on building a neural network ranker to re-rank the documents retrieved by a search engine to improve overall QA performance. Semantic similarity is crucial in QA as the passage containing the answer may be"
N18-4017,P16-2022,0,0.0133531,"entence, we use it to implement the ranker that ranks based on semantic similarity. To compose sentence representations into a paragraph representation, we simply sum the InferSent representations of all the sentences in the paragraph. This approach is inspired by the sum of word representations as composition function for forming sentence representations (Iyyer et al., 2015). We implement a feed-forward neural network as our scoring function. The input feature vector is constructed by concatenating the question embedding, paragraph embedding, their difference, and their element-wise product (Mou et al., 2016): Model Architecture Overall Setup The overall pipeline consists of a search engine, ranker and reader. We do not build our own search engine as QUASAR-T provides 100 short passages already retrieved by the search engine for each question. We build two different rankers: InferSent ranker to evaluate the performance of semantic similarity in ranking for QA, and RelationNetworks ranker to evaluate the performance of relevance matching in ranking for QA. We use the Document Reader of DrQA (Chen et al., 2017) as our machine reader. 3.2 Ranker Given a question and a paragraph, the ranker model acts"
N18-4017,D14-1162,0,0.0809574,"ven a paragraph P (answerj |pij ), where answerj stands for the answer span of j th question in dataset and pij indicates the corresponding top 5 paragraphs. We can thus calculate the overall confidence of each answer span and corresponding paragraph P (pij , answerj ) by multiplying P (answerj |pij ) with P (pij ). We then choose the answer span with the highest P (answerj , pij ) as the output of our model. i,j where q = {q1 , q2 , ..., qn } is the question that contains n words and p = {p1 , p2 , ..., pm } is the paragraph that contains m words; E(qi ) is a 300 dimensional GloVe embedding (Pennington et al., 2014) of word qi , and [·; ·] is the concatenation operator. fφ and gθ are 3 layer feed-forward neural networks with ReLU activation function. The role of gθ is to infer the relation between two words while fφ serves as the scoring function. As we directly compare the word embeddings, this model will lose the contextual information and word order, which can provide us some semantic information. We do not fine-tune the word embeddings during training as we want to preserve the generalized meaning of GloVe embeddings. We hypothesize that this ranker will achieve a high retrieval recall as relevance m"
N18-4017,D17-1070,0,0.0663539,"acting the correct answer by the reader. We follow a similar pipeline as Wang et al. (2017). Our system consists of a neural network ranker and a machine reader as shown in Figure 1. The focus of our work is to improve the ranker for QA performance. We use DrQA’s Document Reader as our reader. We train our ranker and reader models on QUASAR-T (Dhingra et al., 2017b) dataset. QUASAR-T provides a collection top 100 short paragraphs returned by search engine for each question in the dataset. Our goal is to find the correct answer span for a given question. 3 3.1 3.2.1 InferSent Ranker InferSent (Conneau et al., 2017) provides distributed representations for sentences.3 It is trained on Stanford Natural Language Inference Dataset (SNLI; Bowman et al., 2015) and MultiGenre NLI Corpus (MultiNLI; Williams et al., 2017) using supervised learning. It generalizes well and outperforms unsupervised sentence representations such as Skip-Thought Vectors (Kiros et al., 2015) in a variety of tasks. As InferSent representation captures the general semantics of a sentence, we use it to implement the ranker that ranks based on semantic similarity. To compose sentence representations into a paragraph representation, we si"
N18-4017,P17-1168,0,0.271802,"estion and a document, relevance matching measures the word or phrase level local interactions between pieces of texts in a question and a document. As fixed size representations encode the general meaning of the whole sentence or document, they lose some distinctions about the keywords that are crucial for retrieval and question answering. To analyze the importance of relevance matching in QA, we build another ranker model that focuses on local interactions between words in the question and words in the document. We evaluate and analyze the performance of the two rankers on QUASAR-T dataset (Dhingra et al., 2017b). We observe that the ranker model that focuses on relevance matching (RelationNetworks ranker) achieves significantly higher retrieval recall but the ranker model that focuses on semantic similarity (InferSent ranker) has better overall QA performance. We achieve 11.6 percent improvement in overall QA performance by integrating InferSent ranker (6.4 percent improvement by Relation-Networks ranker). 2 Related Work With the introduction of large-scale datasets for machine reading such as CNN/DailyMail (Hermann et al., 2015) and The Stanford Question Answering Dataset (SQuAD; Rajpurkar et al.,"
N18-4017,D16-1264,0,\N,Missing
N18-4017,N18-1101,1,\N,Missing
P15-1001,buck-etal-2014-n,0,0.223514,"Missing"
P15-1001,W14-4012,1,0.433124,"Missing"
P15-1001,D14-1179,1,0.286316,"Missing"
P15-1001,W14-3309,0,0.106484,"by different models on (a) English→French and (b) English→German translation tasks. RNNsearch is the model proposed in (Bahdanau et al., 2015), RNNsearch-LV is the RNNsearch trained with the approach proposed in this paper, and Google is the LSTM-based model proposed in (Sutskever et al., 2014). Unless mentioned otherwise, we report singlemodel RNNsearch-LV scores using τ = 30k (English→French) and τ = 50k (English→German). For the experiments we have run ourselves, we show the scores on the development set as well in the brackets. (?) (Sutskever et al., 2014), (◦) (Luong et al., 2015), (•) (Durrani et al., 2014), (∗) Standard Moses Setting (Cho et al., 2014b), () (Buck et al., 2014). performed the baseline before unknown word replacement, but after doing so, the two systems performed similarly. We could reach higher largevocabulary single-model performance by reshuffling the dataset, but this step could potentially also help the baseline. In this case, we were able to surpass the previously reported best translation result on this task by building an ensemble of 8 models. With τ = 15k, the RNNsearch-LV performance worsened a little, with best BLEU scores, without reshuffling, of 33.76 and 18.59 resp"
P15-1001,N13-1073,0,0.168633,"Missing"
P15-1001,W14-3310,0,0.0134458,"his task by building an ensemble of 8 models. With τ = 15k, the RNNsearch-LV performance worsened a little, with best BLEU scores, without reshuffling, of 33.76 and 18.59 respectively for English→French and English→German. The English→German ensemble described in this paper has also been used for the shared translation task of the 10th Workshop on Statistical Machine Translation (WMT’15), where it was ranked first in terms of BLEU score. The translations by this ensemble can be found online.7 such ensembles may be sub-optimal. This is supported by the fact that higher cross-model BLEU scores (Freitag et al., 2014) are observed for models that were partially trained together. 4.2 Translation Performance In Table 2, we present the results obtained by the trained models with very large target vocabularies, and alongside them, the previous results reported in (Sutskever et al., 2014), (Luong et al., 2015), (Buck et al., 2014) and (Durrani et al., 2014). Without translation-specific strategies, we can clearly see that the RNNsearch-LV outperforms the baseline RNNsearch. In the case of the English→French task, RNNsearch-LV approached the performance level of the previous best single neural machine translatio"
P15-1001,D13-1176,0,0.962493,"ained to maximize the conditional probability of a correct translation given a source sentence, using the bilingual corpus. The NMT models have shown to perform as well as the most widely used conventional translation systems (Sutskever et al., 2014; Bahdanau et al., 2015). Neural machine translation has a number of advantages over the existing statistical machine translation system, specifically, the phrase-based system (Koehn et al., 2003). First, NMT requires a minimal set of domain knowledge. For instance, all of the models proposed in (Sutskever et al., 2014), (Bahdanau et al., 2015) or (Kalchbrenner and Blunsom, 2013) do not assume any linguistic property in both source and target sentences except that they are sequences of words. Second, the whole system is jointly trained to maximize the translation performance, unlike the existing phrase-based system which consists of many separately trained features whose weights are then tuned jointly. Lastly, the memory footprint of the NMT model is often much smaller than the existing system which relies on maintaining large tables of phrase pairs. Despite these advantages and promising results, there is a major limitation in NMT compared to the existing phrase-base"
P15-1001,N03-1017,0,0.0691514,"hyun Cho Roland Memisevic Universit´e de Montr´eal Yoshua Bengio Universit´e de Montr´eal CIFAR Senior Fellow Abstract its translation. The whole neural network is jointly trained to maximize the conditional probability of a correct translation given a source sentence, using the bilingual corpus. The NMT models have shown to perform as well as the most widely used conventional translation systems (Sutskever et al., 2014; Bahdanau et al., 2015). Neural machine translation has a number of advantages over the existing statistical machine translation system, specifically, the phrase-based system (Koehn et al., 2003). First, NMT requires a minimal set of domain knowledge. For instance, all of the models proposed in (Sutskever et al., 2014), (Bahdanau et al., 2015) or (Kalchbrenner and Blunsom, 2013) do not assume any linguistic property in both source and target sentences except that they are sequences of words. Second, the whole system is jointly trained to maximize the translation performance, unlike the existing phrase-based system which consists of many separately trained features whose weights are then tuned jointly. Lastly, the memory footprint of the NMT model is often much smaller than the existin"
P15-1001,J10-4005,0,0.0246337,"to infer the source word to which each target word was most aligned (indicated by the largest αt in Eq. (5)). This is especially useful when the model generated an [UNK] token. Once a translation is generated given a source sentence, each [UNK] may be replaced using a translation-specific technique based on the aligned source word. For instance, in the experiment, we try replacing each [UNK] token with the aligned source word or its most likely translation determined by another word alignment model. Other techniques such as transliteration may also be used to further improve the performance (Koehn, 2010). Decoding Once the model is trained using the proposed approximation, we can use the full target vocabulary when decoding a translation given a new source sentence. Although this is advantageous as it allows the trained model to utilize the whole vocabulary when generating a translation, doing so may be too computationally expensive, e.g., for realtime applications. Since training puts the target word vectors in the space so that they align well with the hidden state of the decoder only when they are likely to be a correct word, we can use only a subset of candidate target words during decodi"
P15-1001,P15-1002,0,0.694642,"., 2015). do not often result in speed-up when decoding a translation during test time.1 2.2 Limited Vocabulary Issue and Other than these model-specific approaches, Conventional Solutions there exist translation-specific approaches. A One of the main difficulties in training this neutranslation-specific approach exploits the properral machine translation model is the computational ties of the rare target words. For instance, Luong complexity involved in computing the target word et al. proposed such an approach for neural maprobability (Eq. (6)). More specifically, we need chine translation (Luong et al., 2015). They reto compute the dot product between the feature place rare words (the words that are not included φ (yt−1 , zt , ct ) and the word vector wt as many in the shortlist) in both source and target sentences times as there are words in a target vocabulary in into corresponding hOOVn i tokens using the word order to compute the normalization constant (the alignment model. Once a source sentence is transdenominator in Eq. (6)). This has to be done for, lated, each hOOVn i in the translation will be reon average, 20–30 words per sentence, which easplaced based on the source word marked by the"
P15-1001,P02-1040,0,0.111409,"on sentences, is identical to the one used in (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2015; Sutskever et al., 2014). As for English→German, the corpus was preprocessed, in a manner similar to (Peitz et al., 2014; Li et al., 2014), in order to remove many poorly translated sentences. We evaluate the models on the WMT’14 test set (news-test 2014),3 while the concatenation of news-test-2012 and news-test-2013 is used for model selection (development set). Table 1 presents data coverage w.r.t. the vocabulary size, on the target side. Unless mentioned otherwise, all reported BLEU scores (Papineni et al., 2002) are computed with the multi-bleu.perl script4 on the cased tokenized translations. 4.1 Settings As a baseline for English→French translation, we use the RNNsearch model proposed by (Bahdanau et al., 2015), with 30k source and target words.5 Another RNNsearch model is trained for English→German translation with 50k source and target words. For each language pair, we train another set of RNNsearch models with much larger vocabularies of 500k source and target words, using the proposed approach. We call these models RNNsearch-LV. We vary the size of the shortlist used during training (τ in Sec."
P15-1001,W14-3317,0,0.0319046,"Missing"
P15-1001,W14-3314,0,\N,Missing
P16-1125,W14-4012,1,0.0384304,"Missing"
P16-1125,D14-1179,1,0.0159737,"Missing"
P16-1125,P13-2121,0,0.0656156,"Missing"
P16-1125,D13-1176,0,0.0132294,"el. Kiros et al. (2015) fully focused on using their model to extract a good, generic sentence vector, while in this paper we are focused on obtaining a good language model. There are less major technical differences. First, the skip-thought vector model conditions only on the immediate preceding sentence, while we extend this to multiple preceding sentences. Second, similarly to the previous works by Mikolov and Zweig (2012), the skip-thought vector model only implements early fusion. Neural Machine Translation Neural machine translation is another related approach (Forcada ˜ and Neco, 1997; Kalchbrenner and Blunsom, 2013; Cho et al., 2014b; Sutskever et al., 2014; Bahdanau et al., 2014). In neural machine translation, often two recurrent neural networks are used. The first recurrent neural network, called an encoder, reads a source sentence, represented as a sequence of words in a source language, to form a context vector, or a set of context vectors. The 1323 other recurrent neural network, called a decoder, then, models the target translation conditioned on this source context. This is similar to the proposed larger-context recurrent language model, if we consider the source sentence as a preceding sentence"
P16-1125,P11-1015,0,0.0527062,"Missing"
P16-1125,N03-1033,0,0.0497879,"compress a long sequence into a single vector has been observed earlier, for instance, in neural machine translation (Cho et al., 2014a). Attention mechanism, which was found to avoid this problem in machine translation (Bahdanau et al., 2014), is found to solve this problem in our task as well. Perplexity per Part-of-Speech Tag Next, we attempted at discovering why the larger-context recurrent language model outperforms the unconditional one. In order to do so, we computed the perplexity per part-of-speech (POS) tag. We used the Stanford log-linear part-of-speech tagger (Stanford POS Tagger, Toutanova et al., 2003) to tag each word of each sentence in the corpora.5 We then computed the perplexity of each word and averaged them for each tag type separately. Among the 36 POS tags used by the Stanford POS Tagger, we looked at the perplexities of the ten most frequent tags (NN, IN, DT, JJ, RB, NNS, VBZ, VB, PRP, CC), of which we combined NN and NNS into a new tag Noun and VB and VBZ into a new tag Verb. We show the results using the RLM-BoWLF and RLM-SeqBoW-ATT-LF on three corpora– IMDB, BBC and Penn Treebank– in Fig. 3. We observe that the predictability, measured by the perplexity (negatively correlated),"
P16-1160,D15-1249,0,0.0523868,"work, each and every token in the vocabulary is equal distance away from every other token. The semantics of those tokens are simply learned (into the embeddings) to maximize the translation quality, or the log-likelihood of the model. This property allows us great freedom in the choice of tokens’ unit. Neural networks have been shown to work well with word tokens (Bengio et al., 2001; Schwenk, 2007; Mikolov et al., 2010) but also with finer units, such as subwords (Sennrich et al., 2015; Botha and Blunsom, 2014; Lu1694 ong et al., 2013) as well as symbols resulting from compression/encoding (Chitnis and DeNero, 2015). Although there have been a number of previous research reporting the use of neural networks with characters (see, e.g., Mikolov et al. (2012) and Santos and Zadrozny (2014)), the dominant approach has been to preprocess the text into a sequence of symbols, each associated with a sequence of characters, after which the neural network is presented with those symbols rather than with characters. More recently in the context of neural machine translation, two research groups have proposed to directly use characters. Kim et al. (2015) proposed to represent each word not as a single integer index"
P16-1160,P16-2058,0,0.113159,"Missing"
P16-1160,W14-3309,0,0.0144672,"– 8.979.17 11.73 8.88 10.9311.56 13.48 10.11 10.2410.63 13.32 9.71 12.70(7) Table 1: BLEU scores of the subword-level, character-level base and character-level bi-scale decoders for both single models and ensembles. The best scores among the single models per language pair are bold-faced, and those among the ensembles are underlined. When available, we report the median value, and the minimum and maximum values as a subscript and a superscript, respectively. (∗) http: //matrix.statmt.org/ as of 11 March 2016 (constrained only). (1) Freitag et al. (2014). (2, 6) Williams et al. (2015). (3, 5) Durrani et al. (2014). (4) Haddow et al. (2015). (7) Rubino et al. (2015). Models and Training We test three models settings: (1) BPE→BPE, (2) BPE→Char (base) and (3) BPE→Char (bi-scale). The latter two differ by the type of recurrent neural network we use. We use GRUs for the encoder in all the settings. We used GRUs for the decoders in the first two settings, (1) and (2), while the proposed bi-scale recurrent network was used in the last setting, (3). The encoder has 512 hidden units for each direction (forward and reverse), and the decoder has 1024 hidden units per layer. We train each model using stochastic gr"
P16-1160,W14-3310,0,0.012861,"911.55 11.09 10.7311.04 10.40 – 11.92 13.72 13.39 – – – – – – – 8.979.17 11.73 8.88 10.9311.56 13.48 10.11 10.2410.63 13.32 9.71 12.70(7) Table 1: BLEU scores of the subword-level, character-level base and character-level bi-scale decoders for both single models and ensembles. The best scores among the single models per language pair are bold-faced, and those among the ensembles are underlined. When available, we report the median value, and the minimum and maximum values as a subscript and a superscript, respectively. (∗) http: //matrix.statmt.org/ as of 11 March 2016 (constrained only). (1) Freitag et al. (2014). (2, 6) Williams et al. (2015). (3, 5) Durrani et al. (2014). (4) Haddow et al. (2015). (7) Rubino et al. (2015). Models and Training We test three models settings: (1) BPE→BPE, (2) BPE→Char (base) and (3) BPE→Char (bi-scale). The latter two differ by the type of recurrent neural network we use. We use GRUs for the encoder in all the settings. We used GRUs for the decoders in the first two settings, (1) and (2), while the proposed bi-scale recurrent network was used in the last setting, (3). The encoder has 512 hidden units for each direction (forward and reverse), and the decoder has 1024 hi"
P16-1160,W15-3013,0,0.00918929,"311.56 13.48 10.11 10.2410.63 13.32 9.71 12.70(7) Table 1: BLEU scores of the subword-level, character-level base and character-level bi-scale decoders for both single models and ensembles. The best scores among the single models per language pair are bold-faced, and those among the ensembles are underlined. When available, we report the median value, and the minimum and maximum values as a subscript and a superscript, respectively. (∗) http: //matrix.statmt.org/ as of 11 March 2016 (constrained only). (1) Freitag et al. (2014). (2, 6) Williams et al. (2015). (3, 5) Durrani et al. (2014). (4) Haddow et al. (2015). (7) Rubino et al. (2015). Models and Training We test three models settings: (1) BPE→BPE, (2) BPE→Char (base) and (3) BPE→Char (bi-scale). The latter two differ by the type of recurrent neural network we use. We use GRUs for the encoder in all the settings. We used GRUs for the decoders in the first two settings, (1) and (2), while the proposed bi-scale recurrent network was used in the last setting, (3). The encoder has 512 hidden units for each direction (forward and reverse), and the decoder has 1024 hidden units per layer. We train each model using stochastic gradient descent with Adam ("
P16-1160,P15-1001,1,0.248061,"en have a priori belief that a word, or its segmented-out lexeme, is a basic unit of meaning, making it natural to approach translation as mapping from a sequence of source-language words to a sequence of target-language words. This has continued with the more recently proposed paradigm of neural machine translaYoshua Bengio Universit´e de Montr´eal CIFAR Senior Fellow tion, although neural networks do not suffer from character-level modelling and rather suffer from the issues specific to word-level modelling, such as the increased computational complexity from a very large target vocabulary (Jean et al., 2015; Luong et al., 2015b). Therefore, in this paper, we address a question of whether neural machine translation can be done directly on a sequence of characters without any explicit word segmentation. To answer this question, we focus on representing the target side as a character sequence. We evaluate neural machine translation models with a character-level decoder on four language pairs from WMT’15 to make our evaluation as convincing as possible. We represent the source side as a sequence of subwords extracted using byte-pair encoding from Sennrich et al. (2015), and vary the target side to b"
P16-1160,W15-5008,0,0.00934046,"equence of characters, and use a convolutional network followed by a highway network (Srivastava et al., 2015) to extract a continuous representation of the word. This approach, which effectively replaces the embedding function ex , was adopted by Costa-Juss`a and Fonollosa (2016) for neural machine translation. Similarly, Ling et al. (2015b) use a bidirectional recurrent neural network to replace the embedding functions ex and ey to respectively encode a character sequence to and from the corresponding continuous word representation. A similar, but slightly different approach was proposed by Lee et al. (2015), where they explicitly mark each character with its relative location in a word (e.g., “B”eginning and “I”ntermediate). Despite the fact that these recent approaches work at the level of characters, it is less satisfying that they all rely on knowing how to segment characters into words. Although it is generally easy for languages like English, this is not always the case. This word segmentation procedure can be as simple as tokenization followed by some punctuation normalization, but also can be as complicated as morpheme segmentation requiring a separate model to be trained in advance (Creu"
P16-1160,W13-3512,0,0.118743,"Missing"
P16-1160,D15-1166,0,0.188984,"lief that a word, or its segmented-out lexeme, is a basic unit of meaning, making it natural to approach translation as mapping from a sequence of source-language words to a sequence of target-language words. This has continued with the more recently proposed paradigm of neural machine translaYoshua Bengio Universit´e de Montr´eal CIFAR Senior Fellow tion, although neural networks do not suffer from character-level modelling and rather suffer from the issues specific to word-level modelling, such as the increased computational complexity from a very large target vocabulary (Jean et al., 2015; Luong et al., 2015b). Therefore, in this paper, we address a question of whether neural machine translation can be done directly on a sequence of characters without any explicit word segmentation. To answer this question, we focus on representing the target side as a character sequence. We evaluate neural machine translation models with a character-level decoder on four language pairs from WMT’15 to make our evaluation as convincing as possible. We represent the source side as a sequence of subwords extracted using byte-pair encoding from Sennrich et al. (2015), and vary the target side to be either a sequence"
P16-1160,P15-1002,0,0.266409,"lief that a word, or its segmented-out lexeme, is a basic unit of meaning, making it natural to approach translation as mapping from a sequence of source-language words to a sequence of target-language words. This has continued with the more recently proposed paradigm of neural machine translaYoshua Bengio Universit´e de Montr´eal CIFAR Senior Fellow tion, although neural networks do not suffer from character-level modelling and rather suffer from the issues specific to word-level modelling, such as the increased computational complexity from a very large target vocabulary (Jean et al., 2015; Luong et al., 2015b). Therefore, in this paper, we address a question of whether neural machine translation can be done directly on a sequence of characters without any explicit word segmentation. To answer this question, we focus on representing the target side as a character sequence. We evaluate neural machine translation models with a character-level decoder on four language pairs from WMT’15 to make our evaluation as convincing as possible. We represent the source side as a sequence of subwords extracted using byte-pair encoding from Sennrich et al. (2015), and vary the target side to be either a sequence"
P16-1160,W15-3022,0,0.0534567,"Missing"
P16-1160,W07-0705,0,0.0385677,"nd phrase tables, are a count-based estimator of probabilities. In other words, a probability of a subsequence of symbols, or pairs of symbols, is estimated by counting the number of its occurrences in a training corpus. This approach severely suffers from the issue of data sparsity, which is due to a large state space which grows exponentially w.r.t. the length of subsequences while growing only linearly w.r.t. the corpus size. This poses a great challenge to character-level modelling, as any subsequence will be on average 4–5 times longer when characters, instead of words, are used. Indeed, Vilar et al. (2007) reported worse performance when the character sequence was directly used by a phrase-based machine translation system. More recently, Neubig et al. (2013) proposed a method to improve character-level translation with phrasebased translation systems, however, with only a limited success. 3 For instance, “quit”, “quite” and “quiet” are one editdistance away from each other but have distinct meanings. 1695 (3) Vanishing Gradient Specifically to neural machine translation, a major reason behind the wide adoption of word-level modelling is due to the difficulty in modelling long-term dependencies"
P16-1160,W15-3024,0,0.0342085,"– 11.92 13.72 13.39 – – – – – – – 8.979.17 11.73 8.88 10.9311.56 13.48 10.11 10.2410.63 13.32 9.71 12.70(7) Table 1: BLEU scores of the subword-level, character-level base and character-level bi-scale decoders for both single models and ensembles. The best scores among the single models per language pair are bold-faced, and those among the ensembles are underlined. When available, we report the median value, and the minimum and maximum values as a subscript and a superscript, respectively. (∗) http: //matrix.statmt.org/ as of 11 March 2016 (constrained only). (1) Freitag et al. (2014). (2, 6) Williams et al. (2015). (3, 5) Durrani et al. (2014). (4) Haddow et al. (2015). (7) Rubino et al. (2015). Models and Training We test three models settings: (1) BPE→BPE, (2) BPE→Char (base) and (3) BPE→Char (bi-scale). The latter two differ by the type of recurrent neural network we use. We use GRUs for the encoder in all the settings. We used GRUs for the decoders in the first two settings, (1) and (2), while the proposed bi-scale recurrent network was used in the last setting, (3). The encoder has 512 hidden units for each direction (forward and reverse), and the decoder has 1024 hidden units per layer. We train"
P16-1160,D13-1176,0,\N,Missing
P16-1160,P16-1162,0,\N,Missing
P16-1160,W14-3324,0,\N,Missing
P16-5005,D13-1176,0,\N,Missing
P16-5005,W04-3248,0,\N,Missing
P16-5005,W03-1502,0,\N,Missing
P16-5005,P04-1021,0,\N,Missing
P16-5005,P15-1001,1,\N,Missing
P16-5005,P02-1040,0,\N,Missing
P16-5005,P15-1002,1,\N,Missing
P16-5005,P08-1045,0,\N,Missing
P16-5005,W09-0438,0,\N,Missing
P16-5005,P13-1059,0,\N,Missing
P16-5005,J98-4003,0,\N,Missing
P16-5005,P16-1162,0,\N,Missing
P16-5005,D07-1091,0,\N,Missing
P16-5005,P09-1067,0,\N,Missing
P16-5005,P07-2045,0,\N,Missing
P16-5005,W04-3250,0,\N,Missing
P17-2012,N16-1024,0,0.06925,"oposed a doubly-recurrent neural network that can generate a tree-structured sentence, but its effectiveness in a full scale NMT task is yet to be shown. Aharoni and Goldberg (2017) introduced a method to serialize a parsed tree and to train the serialized parsed sentences. We propose to implicitly incorporate linguistic prior based on the idea of multi-task learning (Caruana, 1998; Collobert et al., 2011). More specifically, we design a hybrid decoder for NMT, called NMT+RNNG1 , that combines a usual conditional language model and a recently proposed recurrent neural network grammars (RNNGs, Dyer et al., 2016). This is done by plugging in the conventional language model decoder in the place of the buffer in RNNG, while sharing a subset of parameters, such as word vectors, between the language model and RNNG. We train this hybrid model to maximize both the log-probability of a target sentence and the log-probability of a parse action sequence. We use an external parser (Andor et al., 2016) to generate target parse actions, but unlike the previous explicit approaches, we do not need it during test time. There has been relatively little attention to incorporating linguistic prior to neural machine tra"
P17-2012,P17-2021,0,0.109183,"ing process of neural machine translation, which results in two separate models rather than a single end-to-end one. Despite the promising improvements, these explicit approaches are limited in that the trained translation model strictly requires the availability of external tools during inference time. More recently, researchers have proposed methods to incorporate target-side syntax into NMT models. Alvarez-Melis and Jaakkola (2017) have proposed a doubly-recurrent neural network that can generate a tree-structured sentence, but its effectiveness in a full scale NMT task is yet to be shown. Aharoni and Goldberg (2017) introduced a method to serialize a parsed tree and to train the serialized parsed sentences. We propose to implicitly incorporate linguistic prior based on the idea of multi-task learning (Caruana, 1998; Collobert et al., 2011). More specifically, we design a hybrid decoder for NMT, called NMT+RNNG1 , that combines a usual conditional language model and a recently proposed recurrent neural network grammars (RNNGs, Dyer et al., 2016). This is done by plugging in the conventional language model decoder in the place of the buffer in RNNG, while sharing a subset of parameters, such as word vector"
P17-2012,P16-1078,1,0.77136,"Missing"
P17-2012,W16-3905,0,0.0143141,"We call this hybrid model NMT+RNNG. 4.2 Knowledge Distillation for Parsing A major challenge in training the proposed hybrid model is that there is not a parallel corpus augmented with gold-standard target-side parse, and vice versa. In other words, we must either parse the target-side sentences of an existing parallel corpus or translate sentences with existing goldstandard parses. As the target task of the proposed model is translation, we start with a parallel corpus and annotate the target-side sentences. It is however costly to manually annotate any corpus of reasonable size (Table 6 in Alonso et al., 2016). We instead resort to noisy, but automated annotation using an existing parser. This approach of automated annotation can be considered along the line of recently proposed techniques of knowledge distillation (Hinton et al., 2015) and distant supervision (Mintz et al., 2009). In knowledge distillation, a teacher network is trained purely on a training set with ground-truth annotations, and the annotations predicted by this teacher are used to train a student network, which is similar to our approach where the external parser could be thought of as a teacher and the proposed hybrid network’s R"
P17-2012,P84-1044,0,0.104662,"Missing"
P17-2012,P16-1231,0,0.0361626,"Collobert et al., 2011). More specifically, we design a hybrid decoder for NMT, called NMT+RNNG1 , that combines a usual conditional language model and a recently proposed recurrent neural network grammars (RNNGs, Dyer et al., 2016). This is done by plugging in the conventional language model decoder in the place of the buffer in RNNG, while sharing a subset of parameters, such as word vectors, between the language model and RNNG. We train this hybrid model to maximize both the log-probability of a target sentence and the log-probability of a parse action sequence. We use an external parser (Andor et al., 2016) to generate target parse actions, but unlike the previous explicit approaches, we do not need it during test time. There has been relatively little attention to incorporating linguistic prior to neural machine translation. Much of the previous work was further constrained to considering linguistic prior on the source side. In this paper, we propose a hybrid model, called NMT+RNNG, that learns to parse and translate by combining the recurrent neural network grammar into the attention-based neural machine translation. Our approach encourages the neural machine translation model to incorporate l"
P17-2012,D10-1092,0,0.0116664,"tree. The translated sentence was generated by using beam search, which is the same setting of NMT+RNNG shown in Table 3. The parsing actions were obtained by greedy search. The resulting dependency structure is mostly correct but contains a few errors; for example, dependency relation between “The” and “ transition” should not be “pobj”. BLEU 18.60 18.02 17.94 17.58 17.75 Table 3: Effect of each component in RNNG. 5.3 Results and Analysis In Table 2, we report the translation qualities of the tested models on all the four language pairs. We report both BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010). Except for DeEn, measured in BLEU, we observe the statistically significant improvement by the proposed NMT+RNNG over the baseline model. It is worthwhile to note that these significant improvements have been achieved without any additional parameters nor computational overhead in the inference time. 6 Conclusion We propose a hybrid model, to which we refer as NMT+RNNG, that combines the decoder of an attention-based neural translation model with the RNNG. This model learns to parse and translate simultaneously, and training it encourages both the encoder and decoder to better incorporate li"
P17-2012,P16-1160,1,0.918573,"encourages the neural machine translation model to incorporate linguistic prior during training, and lets it translate on its own afterward. Extensive experiments with four language pairs show the effectiveness of the proposed NMT+RNNG. 1 Introduction Neural Machine Translation (NMT) has enjoyed impressive success without relying on much, if any, prior linguistic knowledge. Some of the most recent studies have for instance demonstrated that NMT systems work comparably to other systems even when the source and target sentences are given simply as flat sequences of characters (Lee et al., 2016; Chung et al., 2016) or statistically, not linguistically, motivated subword units (Sennrich et al., 2016; Wu et al., 2016). Shi et al. (2016) recently made an observation that the encoder of NMT captures syntactic properties of a source sentence automatically, indirectly suggesting that explicit linguistic prior may not be necessary. On the other hand, there have only been a couple of recent studies showing the potential benefit of explicitly encoding the linguistic prior into NMT. Sennrich and Haddow (2016) for instance proposed to augment each source word with its corresponding part-of-speech tag, lemmatized 1"
P17-2012,W04-3250,0,0.0135413,"ed as data preprocessing, we use all the vocabularies in a corpus and do not cut off any words. We use the plain SyntaxNet and do not train it furthermore. 4 75 We run all the experiments on multi-core CPUs (10 De-En NMT NMT+RNNG NMT NMT+RNNG Ru-En BLEU 16.61 12.03 16.41 12.46† RIBES 73.75 69.56 75.03† 71.04† Cs-En Jp-En 11.22 12.06† 17.88 18.84† 69.59 70.39† 71.27 72.25† Figure 1: An example of translation and its dependency relations obtained by our proposed model. Table 2: BLEU and RIBES scores by the baseline and proposed models on the test set. We use the bootstrap resampling method from Koehn (2004) to compute the statistical significance. We use † to mark those significant cases with p < 0.005. Jp-En (Dev) NMT+RNNG w/o Buffer w/o Action w/o Stack NMT ate a translated sentence and the RNNG decoder to predict its parsing actions. The proposed model can therefore output a dependency structure along with a translated sentence. Figure 1 shows an example of JP-EN translation in the development dataset and its dependency parse tree obtained by the proposed model. The special symbol (“EOS”) is treated as the root node (“ROOT”) of the parsed tree. The translated sentence was generated by using b"
P17-2012,P07-2045,0,0.00557262,"the ASPEC corpus (“train1.txt”) from the WAT’16 Jp-En translation task. We tokenize each Japanese sentence with KyTea (Neubig et al., 2011) and preprocess according to the recommendations from WAT’16 (WAT, 2016). We use the first 100K sentence pairs of length shorter than 50 for training. The vocabulary is constructed with all the unique tokens that appear at least twice in the training corpus. We use “dev.txt” and “test.txt” provided by WAT’16 respectively as development and test sets. Cs, De and Ru We use News Commentary v8. We removed noisy metacharacters and used the tokenizer from Moses (Koehn et al., 2007) to build a vocabulary of each language using unique tokens that appear at least 6, 6 and 5 times respectively for Cs, Ru and De. The target-side (English) vocabulary was constructed with all the unique tokens 3 When the target sentence is parsed as data preprocessing, we use all the vocabularies in a corpus and do not cut off any words. We use the plain SyntaxNet and do not train it furthermore. 4 75 We run all the experiments on multi-core CPUs (10 De-En NMT NMT+RNNG NMT NMT+RNNG Ru-En BLEU 16.61 12.03 16.41 12.46† RIBES 73.75 69.56 75.03† 71.04† Cs-En Jp-En 11.22 12.06† 17.88 18.84† 69.59 7"
P17-2012,P15-1033,0,0.0181116,", xN ). The encoder returns a sequence of hidden states h = (h1 , h2 , . . . , hN ). Each hidden state hi is a concatenation of those from the forward and backward recurrent network: hi = h→ − ← −i h i ; h i , where 3 A recurrent neural network grammar (RNNG, Dyer et al., 2016) is a probabilistic syntax-based language model. Unlike a usual recurrent language model (see, e.g., Mikolov et al., 2010), an RNNG simultaneously models both tokens and their tree-based composition. This is done by having a (output) buffer, stack and action history, each of which is implemented as a stack LSTM (sLSTM, Dyer et al., 2015). At each time step, the action sLSTM predicts the next action based on the (current) hidden states of the buffer, stack and action sLSTM. That is, >f p(at = a|a<t ) ∝ eWa Vx (xi ) refers to the word vector of the i-th source word. The decoder is implemented as a conditional recurrent language model which models the target sentence, or translation, as X log p(y|x) = log p(yj |y<j , x), hbuffer = StackLSTM(hbuffer t top , Vy (yt−1 )), where y = (y1 , . . . , yM ). Each of the conditional probabilities in the r.h.s is computed by (2) sj = fdec (sj−1 , [Vy (yj−1 ); s˜j−1 ]), (3) (5) where Vy and"
P17-2012,E17-1117,0,0.0517758,"Missing"
P17-2012,P16-2049,0,0.118476,"Missing"
P17-2012,D15-1166,0,0.0475373,"Missing"
P17-2012,P09-1113,0,0.00428942,"arget-side sentences of an existing parallel corpus or translate sentences with existing goldstandard parses. As the target task of the proposed model is translation, we start with a parallel corpus and annotate the target-side sentences. It is however costly to manually annotate any corpus of reasonable size (Table 6 in Alonso et al., 2016). We instead resort to noisy, but automated annotation using an existing parser. This approach of automated annotation can be considered along the line of recently proposed techniques of knowledge distillation (Hinton et al., 2015) and distant supervision (Mintz et al., 2009). In knowledge distillation, a teacher network is trained purely on a training set with ground-truth annotations, and the annotations predicted by this teacher are used to train a student network, which is similar to our approach where the external parser could be thought of as a teacher and the proposed hybrid network’s RNNG as a student. On the other hand, what we 2 The j-th hidden state in Eq. (3) is calculated only when the action (shift) is predicted by the RNNG. This is why our proposed model can handle the sequences of words and actions which have different lengths. 74 Cs-En De-En Ru-En"
P17-2012,N16-1145,0,0.0234197,"ining. We use “newstest2015” and “newstest2016” as development and test sets respectively. 5.2 Models, Learning and Inference In all our experiments, each recurrent network has a single layer of LSTM units of 256 dimensions, and the word vectors and the action vectors are of 256 and 128 dimensions, respectively. To reduce computational overhead, we use BlackOut (Ji et al., 2015) with 2000 negative samples and α = 0.4. When employing BlackOut, we shared the negative samples of each target word in a sentence in training time (Hashimoto and Tsuruoka, 2017), which is similar to the previous work (Zoph et al., 2016). For the proposed NMT+RNNG, we share the target word vectors between the decoder (buffer) and the stack sLSTM. Each weight is initialized from the uniform distribution [−0.1, 0.1]. The bias vectors and the weights of the softmax and BlackOut are initialized to be zero. The forget gate biases of LSTMs and Stack-LSTMs are initialized to 1 as recommended in J´ozefowicz et al. (2015). We use stochastic gradient descent with minibatches of 128 examples. The learning rate starts from 1.0, and is halved each time the perplexity on the development set increases. We clip the norm of the gradient (Pasc"
P17-2012,P11-2093,0,0.0512957,"eline and the proposed model to train a full JP-EN parallel corpus in our implementation.4 Experiments Language Pairs and Corpora We compare the proposed NMT+RNNG against the baseline model on four different language pairs–Jp-En, Cs-En, De-En and Ru-En. The basic statistics of the training data are presented in Table 1. We mapped all the low-frequency words to the unique symbol “UNK” and inserted a special symbol “EOS” at the end of both source and target sentences. Ja We use the ASPEC corpus (“train1.txt”) from the WAT’16 Jp-En translation task. We tokenize each Japanese sentence with KyTea (Neubig et al., 2011) and preprocess according to the recommendations from WAT’16 (WAT, 2016). We use the first 100K sentence pairs of length shorter than 50 for training. The vocabulary is constructed with all the unique tokens that appear at least twice in the training corpus. We use “dev.txt” and “test.txt” provided by WAT’16 respectively as development and test sets. Cs, De and Ru We use News Commentary v8. We removed noisy metacharacters and used the tokenizer from Moses (Koehn et al., 2007) to build a vocabulary of each language using unique tokens that appear at least 6, 6 and 5 times respectively for Cs, R"
P17-2012,P02-1040,0,0.128416,"root node (“ROOT”) of the parsed tree. The translated sentence was generated by using beam search, which is the same setting of NMT+RNNG shown in Table 3. The parsing actions were obtained by greedy search. The resulting dependency structure is mostly correct but contains a few errors; for example, dependency relation between “The” and “ transition” should not be “pobj”. BLEU 18.60 18.02 17.94 17.58 17.75 Table 3: Effect of each component in RNNG. 5.3 Results and Analysis In Table 2, we report the translation qualities of the tested models on all the four language pairs. We report both BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010). Except for DeEn, measured in BLEU, we observe the statistically significant improvement by the proposed NMT+RNNG over the baseline model. It is worthwhile to note that these significant improvements have been achieved without any additional parameters nor computational overhead in the inference time. 6 Conclusion We propose a hybrid model, to which we refer as NMT+RNNG, that combines the decoder of an attention-based neural translation model with the RNNG. This model learns to parse and translate simultaneously, and training it encourages both the encoder and"
P17-2012,W16-2209,0,0.0501205,"ms even when the source and target sentences are given simply as flat sequences of characters (Lee et al., 2016; Chung et al., 2016) or statistically, not linguistically, motivated subword units (Sennrich et al., 2016; Wu et al., 2016). Shi et al. (2016) recently made an observation that the encoder of NMT captures syntactic properties of a source sentence automatically, indirectly suggesting that explicit linguistic prior may not be necessary. On the other hand, there have only been a couple of recent studies showing the potential benefit of explicitly encoding the linguistic prior into NMT. Sennrich and Haddow (2016) for instance proposed to augment each source word with its corresponding part-of-speech tag, lemmatized 1 Our code is available at https://github.com/ tempra28/nmtrnng. 72 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 72–78 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2012 state sj against each of the hidden states and assigns a scalar score: βi,j = exp(h> i Wd sj ) (Luong et al., 2015). These scores are then normalized across the hidden states to su"
P17-2012,P16-1162,0,0.0282931,"ing training, and lets it translate on its own afterward. Extensive experiments with four language pairs show the effectiveness of the proposed NMT+RNNG. 1 Introduction Neural Machine Translation (NMT) has enjoyed impressive success without relying on much, if any, prior linguistic knowledge. Some of the most recent studies have for instance demonstrated that NMT systems work comparably to other systems even when the source and target sentences are given simply as flat sequences of characters (Lee et al., 2016; Chung et al., 2016) or statistically, not linguistically, motivated subword units (Sennrich et al., 2016; Wu et al., 2016). Shi et al. (2016) recently made an observation that the encoder of NMT captures syntactic properties of a source sentence automatically, indirectly suggesting that explicit linguistic prior may not be necessary. On the other hand, there have only been a couple of recent studies showing the potential benefit of explicitly encoding the linguistic prior into NMT. Sennrich and Haddow (2016) for instance proposed to augment each source word with its corresponding part-of-speech tag, lemmatized 1 Our code is available at https://github.com/ tempra28/nmtrnng. 72 Proceedings of the"
P17-2012,D16-1159,0,\N,Missing
P18-1201,W13-2322,0,0.048347,"for Computational Linguistics (Long Papers), pages 2160–2170 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics Figure 1: Event Mention Example: dispatching is the trigger of a Transport-Person event with four arguments: the solid lines show the event annotations for the sentence while the dotted lines show the Abstract Meaning Representation parsing output. patching is the trigger for the event mention of type Transport Person and in E2, conflict is the trigger for the event mention of type Attack. We make use of Abstract Meaning Representations (AMR) (Banarescu et al., 2013) to identify the candidate arguments and construct event mention structures as shown in Figure 2 (top). Figure 2 (bottom) also shows event type structures defined in the Automatic Content Extraction (ACE) guideline.2 We can see that a trigger and its event type name usually have some shared meaning. Furthermore, their structures also tend to be similar: a Transport Person event typically involves a Person as its patient role, while an Attack event involves a Person or Location as an Attacker. This observation matches the theory by Pustejovsky (1991): “the semantics of an event structure can be"
P18-1201,N07-4013,0,0.12327,"Missing"
P18-1201,P08-1004,0,0.122949,"Missing"
P18-1201,P15-2061,1,0.915193,"Missing"
P18-1201,P15-1017,0,0.661906,"Missing"
P18-1201,P16-2011,1,0.921764,"Missing"
P18-1201,C16-1017,0,0.0883192,"Missing"
P18-1201,P11-1163,0,0.389091,"Missing"
P18-1201,P11-1113,0,0.557271,"Missing"
P18-1201,P16-1025,1,0.911858,"Missing"
P18-1201,D09-1013,0,0.0959611,"Missing"
P18-1201,N16-1034,1,0.915997,"Missing"
P18-1201,P08-1030,1,0.902064,"Missing"
P18-1201,P15-2060,0,0.36822,"Missing"
P18-1201,P11-1115,1,0.767935,"Missing"
P18-1201,K17-1034,0,0.0861971,"Missing"
P18-1201,P13-1008,1,0.94998,"Missing"
P18-1201,P10-1081,0,0.668857,"Missing"
P18-1201,C10-2087,0,0.0604086,"Missing"
P18-1201,D16-1038,0,0.0734626,"Missing"
P18-1201,D16-1087,0,0.0557832,"Missing"
P18-1201,D11-1001,1,0.900064,"Missing"
P18-1201,P16-1201,0,0.0472642,"Missing"
P18-1201,P06-2094,0,0.0941745,"Missing"
P18-1201,P12-1088,0,0.128376,"Missing"
P18-1201,N06-1039,0,0.0621108,"Missing"
P18-1201,D13-1170,0,0.00421772,"xtraction as a classification problem, by assigning event triggers to event types from a pre-defined fixed set. These methods rely heavily on manual annotations and features specific to each event type, and thus are not easily adapted to new event types without extra annotation effort. Handling new event types may even entail starting over, without being able to re-use annotations from previous event types. To make event extraction effective as new realworld scenarios emerge, we take a look at this task from the perspective of zero-shot learning, ZSL (Frome et al., 2013; Norouzi et al., 2013; Socher et al., 2013a). ZSL, as a type of transfer learning, makes use of separate, pre-existing classifiers to build a semantic, cross-concept space that maps between their respective classes. The resulting shared semantic space then allows for building a novel “zero-shot” classifier, i,e,, requiring no (zero) additional training examples, to handle unseen cases. We observe that each event mention has a structure consisting of a candidate trigger and arguments, with corresponding predefined name labels for the event type and argument roles. We propose to enrich the semantic representations of each event mention"
P18-1201,W15-0812,0,0.0620811,"Missing"
P18-1201,N15-1040,0,0.0767342,"Missing"
P18-1201,P10-4014,0,0.0766931,"Missing"
P19-1121,D18-1549,0,0.0436996,"is also closely related to zero-resource translation which is a general task to translate between languages without parallel resources. Possible solutions include pivot-based translation, multilingual or unsupervised NMT. For instance, there have been attempts to train a single-pair model with a pivotlanguage (Cheng et al., 2016; Chen et al., 2017) or a pivot-image (Lee et al., 2017; Chen et al., 2018). Unsupervised Translation Unlike the focus of this work, unsupervised translation usually refers to a zero-resource problem where many monolingual corpora are available. Lample et al. (2018a); Artetxe et al. (2018) proposed to enforce a shared 1265 latent space to improve unsupervised translation quality which was shown not necessary by Lample et al. (2018b) in which a more effective initialization method for related languages was proposed. Neural Machine Translation Pre-training As a standard transfer learning approach, pre-training significantly improves the translation quality of low resource languages by fine-tuning the parameters trained on high-resource languages (Zoph et al., 2016; Gu et al., 2018c; Lample and Conneau, 2019). Our proposed LM pre-training can also be included in the same scope whi"
P19-1121,P17-1176,1,0.946434,"have fewer pipelined components and significantly outperform phrasebased systems (Koehn et al., 2003), Neural Machine Translation (NMT) still works poorly when the available number of training examples is limited. Research on low-resource languages is drawing increasing attention, and it has been found promising to train a multilingual NMT (Firat et al., 2016a) model for high- and row-resource languages to deal with low-resource translation (Gu et al., 2018b). As an extreme in terms of the number of supervised examples, prior works dug into translation with zero-resource (Firat et al., 2016b; Chen et al., 2017; Lample et al., 2018a,b) where the language pairs in interest do not have any parallel corpora between them. In particular, Johnson et al. (2017) observed an emergent property of zero-shot translation where a trained multilingual NMT model is able to automatically do translation on unseen language pairs; we refer to this setting as zero-shot NMT from here on. In this work, we start with a typical degeneracy issue of zero-shot NMT, reported in several recent works (Arivazhagan et al., 2018; Sestorain et al., 2018), that zero-shot NMT is sensitive to training conditions, and the translation qua"
P19-1121,W14-4012,1,0.750206,"Missing"
P19-1121,N16-1101,1,0.903048,"and achieve similar or better results than the pivot-based approach. 1 Introduction Despite the recent domination of neural networkbased models (Sutskever et al., 2014; Bahdanau et al., 2014; Vaswani et al., 2017) in the field of machine translation, which have fewer pipelined components and significantly outperform phrasebased systems (Koehn et al., 2003), Neural Machine Translation (NMT) still works poorly when the available number of training examples is limited. Research on low-resource languages is drawing increasing attention, and it has been found promising to train a multilingual NMT (Firat et al., 2016a) model for high- and row-resource languages to deal with low-resource translation (Gu et al., 2018b). As an extreme in terms of the number of supervised examples, prior works dug into translation with zero-resource (Firat et al., 2016b; Chen et al., 2017; Lample et al., 2018a,b) where the language pairs in interest do not have any parallel corpora between them. In particular, Johnson et al. (2017) observed an emergent property of zero-shot translation where a trained multilingual NMT model is able to automatically do translation on unseen language pairs; we refer to this setting as zero-shot"
P19-1121,D16-1026,1,0.882838,"Missing"
P19-1121,N18-1032,1,0.896791,"domination of neural networkbased models (Sutskever et al., 2014; Bahdanau et al., 2014; Vaswani et al., 2017) in the field of machine translation, which have fewer pipelined components and significantly outperform phrasebased systems (Koehn et al., 2003), Neural Machine Translation (NMT) still works poorly when the available number of training examples is limited. Research on low-resource languages is drawing increasing attention, and it has been found promising to train a multilingual NMT (Firat et al., 2016a) model for high- and row-resource languages to deal with low-resource translation (Gu et al., 2018b). As an extreme in terms of the number of supervised examples, prior works dug into translation with zero-resource (Firat et al., 2016b; Chen et al., 2017; Lample et al., 2018a,b) where the language pairs in interest do not have any parallel corpora between them. In particular, Johnson et al. (2017) observed an emergent property of zero-shot translation where a trained multilingual NMT model is able to automatically do translation on unseen language pairs; we refer to this setting as zero-shot NMT from here on. In this work, we start with a typical degeneracy issue of zero-shot NMT, reported"
P19-1121,D18-1398,1,0.63184,"domination of neural networkbased models (Sutskever et al., 2014; Bahdanau et al., 2014; Vaswani et al., 2017) in the field of machine translation, which have fewer pipelined components and significantly outperform phrasebased systems (Koehn et al., 2003), Neural Machine Translation (NMT) still works poorly when the available number of training examples is limited. Research on low-resource languages is drawing increasing attention, and it has been found promising to train a multilingual NMT (Firat et al., 2016a) model for high- and row-resource languages to deal with low-resource translation (Gu et al., 2018b). As an extreme in terms of the number of supervised examples, prior works dug into translation with zero-resource (Firat et al., 2016b; Chen et al., 2017; Lample et al., 2018a,b) where the language pairs in interest do not have any parallel corpora between them. In particular, Johnson et al. (2017) observed an emergent property of zero-shot translation where a trained multilingual NMT model is able to automatically do translation on unseen language pairs; we refer to this setting as zero-shot NMT from here on. In this work, we start with a typical degeneracy issue of zero-shot NMT, reported"
P19-1121,N03-1017,0,0.0757703,"s analysis, we propose to use two simple but effective approaches: (1) decoder pre-training; (2) backtranslation. These methods show significant improvement (4 ∼ 22 BLEU points) over the vanilla zero-shot translation on three challenging multilingual datasets, and achieve similar or better results than the pivot-based approach. 1 Introduction Despite the recent domination of neural networkbased models (Sutskever et al., 2014; Bahdanau et al., 2014; Vaswani et al., 2017) in the field of machine translation, which have fewer pipelined components and significantly outperform phrasebased systems (Koehn et al., 2003), Neural Machine Translation (NMT) still works poorly when the available number of training examples is limited. Research on low-resource languages is drawing increasing attention, and it has been found promising to train a multilingual NMT (Firat et al., 2016a) model for high- and row-resource languages to deal with low-resource translation (Gu et al., 2018b). As an extreme in terms of the number of supervised examples, prior works dug into translation with zero-resource (Firat et al., 2016b; Chen et al., 2017; Lample et al., 2018a,b) where the language pairs in interest do not have any paral"
P19-1121,J82-2005,0,0.59927,"Missing"
P19-1121,P16-1160,1,0.779222,"raining sets Di,j of all available language pairs S. That is: max θ 1 |S |· |Di,j | X Ljθ (xi , y j ), (2) (xi ,y j )∈Di,j ,(i,j)∈S where we denote Ljθ (xi , y j ) = log p(y j |xi , j; θ). Specifically, the target language ID j is given to the model so that it knows to which language it translates, and this can be readily implemented by setting the initial token y0 = j for the target sentence to start with.1 The multilingual NMT model shares a single representation space across multiple languages, which has been found to facilitate translating low-resource language pairs (Firat et al., 2016a; Lee et al., 2016; Gu et al., 2018b,c). Pivot-based NMT In practise, it is almost impossible for the training set to contain all K × (K+1) combinations of translation pairs to learn a multilingual model. Often only one (e.g. English) or a few out of the K + 1 languages have parallel sentence pairs with the remaining languages. For instance, we may only have parallel pairs between English & French, and Spanish & English, but not between French & Spanish. What happens if we evaluate on an unseen direction e.g. Spanish to French? A simple but commonly used solution is pivoting: we first translate from Spanish to"
P19-1121,P12-3005,0,0.0166452,"g. Learning the decoder language model increases I(j; z) which facilitates zero-shot translation. Once the model captures the correct dependency that guides the model to output the desired language, it is more likely for the model to ignore the spurious correlation during standard NMT training. That is, we pre-train the decoder as a multilingual language model. Similar to Eq. (4): i,j where the summation is over all possible language pairs, and p˜(·) represents frequency. The latent language identity z = φ(y j ) is estimated by an external language identification tool given the actual output (Lui and Baldwin, 2012). In Fig. 3, the trend of zero-shot performance is inversely proportional to I(i; z), which indicates that the degeneracy is from the spurious correlation. The analysis of the mutual information also explains the sensitivity issue of zero-shot NMT during training. As a side effect of learning translation, I(i; z) tends to increase more when the training conditions make MT training easier (e.g. large batch-size). The performance of zero-shot NMT becomes more unstable and fails to produce translation in the desired language (j). 20 Figure 3: The learning curves of the mutual information between"
P19-1121,D16-1163,0,0.0192561,"on usually refers to a zero-resource problem where many monolingual corpora are available. Lample et al. (2018a); Artetxe et al. (2018) proposed to enforce a shared 1265 latent space to improve unsupervised translation quality which was shown not necessary by Lample et al. (2018b) in which a more effective initialization method for related languages was proposed. Neural Machine Translation Pre-training As a standard transfer learning approach, pre-training significantly improves the translation quality of low resource languages by fine-tuning the parameters trained on high-resource languages (Zoph et al., 2016; Gu et al., 2018c; Lample and Conneau, 2019). Our proposed LM pre-training can also be included in the same scope while following a different motivation. 7 Conclusion In this paper, we analyzed the issue of zero-shot translation quantitatively and successfully close the gap of the performance of between zero-shot translation and pivot-based zero-resource translation. We proposed two simple and effective strategies for zero-shot translation. Experiments on the Europarl, IWSLT and MultiUN corpora show that our proposed methods significantly improve the vanilla zero-shot NMT and consistently out"
P19-1121,D18-1039,0,0.0227884,"hot BLEU scores on validation set of Multi-UN with the corresponded checkpoints marked. after a small number of iterations on large-bs. Case Study We also show a randomly selected example for Ru → Zh from the validation set of MultiUN dataset in Fig. 5. We can see that at the beginning, the output sentence of ZS+LM is fluent while ZS learns translation faster than ZS+LM. Then, En tokens starts to appear in the output sentence of ZS, and it totally shifts to En eventually. 6 Related Works Zero-shot Neural Machine Translation Zeroshot NMT has received increasingly more interest in recent years. Platanios et al. (2018) introduced the contextual parameter generator, which generated the parameters of the system and performed zero-shot translation. Arivazhagan et al. (2018) conjectured the solution towards the degeneracy in zero-shot NMT was to guide an NMT encoder to learn language agnostic representations. Sestorain et al. (2018) combined dual learning to improve zero-shot NMT. However, unlike our work, none of these prior works performed quantitative investigation of the underlying cause. Zero Resource Translation This work is also closely related to zero-resource translation which is a general task to tran"
P19-1121,W16-2323,0,0.0200964,"ure 2: A conceptual illustration of decoupling the output translation (y j ) into two latent factors (language type and the semantics) where the undesired spurious correlation (in red) will be wrongly captured if i is always translated to j during training. 0.150 20 I(X; Z) &lt;latexit sha1_base64=""(null)"">(null)&lt;/latexit> Zero-shot Average BLEU 25 1 |S |· |Di,j | X L˜jθ (y j ), (4) (xi ,y j )∈Di,j ,(i,j)∈S where L˜jθ (y j ) = log p(y j |0, j; θ), which represents that pre-training can be implemented by simply replacing all the source representations by zero vectors during standard NMT training (Sennrich et al., 2016). In Transformer, it is equivalent to ignoring the attention modules between the encoder and decoder. The proposed LM pre-training can be seen as a rough approximation of marginalizing all possible source sentences, while empirically we found it worked well. After a few gradient descent steps, the pre-trained model continues with MT training. In this work, we only consider using the same parallel data for pre-training. We summarize the pros and cons as follows: Pros: Efficient (a few LM training steps + NMT training); no additional data needed; Cons: The LM pre-training objective does not nece"
P19-1121,N07-1061,0,\N,Missing
P19-1121,Q17-1026,1,\N,Missing
P19-1177,P07-2045,0,0.0078378,"search approach to generate diverse dialogs. Comparing to these works, we focus on generating translations with different sentence structures. We still use beam search to search for best words in every decoding steps under the constraint of code assignment. Our approach also comes with 4 Experiments 4.1 Experimental Settings We evaluate our models on two machine translation datasets: ASPEC Japanese-to-English dataset (Nakazawa et al., 2016) and WMT14 Germanto-English dataset. The datasets contain 3M and 4.5M bilingual pairs respectively. For the ASPEC Ja-En dataset, we use the Moses toolkit (Koehn et al., 2007) to tokenize the English side and Kytea (Neubig et al., 2011) to tokenize the Japanese side. After tokenization, we apply byte-pair encoding (Sennrich et al., 2016) to segment the texts into subwords, forcing the vocabulary size of each language to be 40k. For WMT14 De-En dataset, we use sentencepiece (Kudo and Richardson, 2018) to segment the words to ensure a vocabulary size of 32k. In evaluation, we report tokenized BLEU for ASPEC Ja-En dataset. For WMT14 De-En dataset, BLEU scores are generated using SacreBleu toolkit (Post, 2018). For models that produce sentence codes during decoding, th"
P19-1177,D18-2012,0,0.0216905,"ings We evaluate our models on two machine translation datasets: ASPEC Japanese-to-English dataset (Nakazawa et al., 2016) and WMT14 Germanto-English dataset. The datasets contain 3M and 4.5M bilingual pairs respectively. For the ASPEC Ja-En dataset, we use the Moses toolkit (Koehn et al., 2007) to tokenize the English side and Kytea (Neubig et al., 2011) to tokenize the Japanese side. After tokenization, we apply byte-pair encoding (Sennrich et al., 2016) to segment the texts into subwords, forcing the vocabulary size of each language to be 40k. For WMT14 De-En dataset, we use sentencepiece (Kudo and Richardson, 2018) to segment the words to ensure a vocabulary size of 32k. In evaluation, we report tokenized BLEU for ASPEC Ja-En dataset. For WMT14 De-En dataset, BLEU scores are generated using SacreBleu toolkit (Post, 2018). For models that produce sentence codes during decoding, the codes are removed from translation results before evaluating BLEU scores. 4.2 Obtaining Sentence Codes For the semantic coding model based on BERT, we cluster the hidden state of “[CLS]” token into 256 clusters with k-means algorithm. The cluster ids are then used as sentence codes. For models using FastText Embeddings, pre-tr"
P19-1177,P11-2093,0,0.0111102,"hese works, we focus on generating translations with different sentence structures. We still use beam search to search for best words in every decoding steps under the constraint of code assignment. Our approach also comes with 4 Experiments 4.1 Experimental Settings We evaluate our models on two machine translation datasets: ASPEC Japanese-to-English dataset (Nakazawa et al., 2016) and WMT14 Germanto-English dataset. The datasets contain 3M and 4.5M bilingual pairs respectively. For the ASPEC Ja-En dataset, we use the Moses toolkit (Koehn et al., 2007) to tokenize the English side and Kytea (Neubig et al., 2011) to tokenize the Japanese side. After tokenization, we apply byte-pair encoding (Sennrich et al., 2016) to segment the texts into subwords, forcing the vocabulary size of each language to be 40k. For WMT14 De-En dataset, we use sentencepiece (Kudo and Richardson, 2018) to segment the words to ensure a vocabulary size of 32k. In evaluation, we report tokenized BLEU for ASPEC Ja-En dataset. For WMT14 De-En dataset, BLEU scores are generated using SacreBleu toolkit (Post, 2018). For models that produce sentence codes during decoding, the codes are removed from translation results before evaluatin"
P19-1177,W18-6319,0,0.024455,"the ASPEC Ja-En dataset, we use the Moses toolkit (Koehn et al., 2007) to tokenize the English side and Kytea (Neubig et al., 2011) to tokenize the Japanese side. After tokenization, we apply byte-pair encoding (Sennrich et al., 2016) to segment the texts into subwords, forcing the vocabulary size of each language to be 40k. For WMT14 De-En dataset, we use sentencepiece (Kudo and Richardson, 2018) to segment the words to ensure a vocabulary size of 32k. In evaluation, we report tokenized BLEU for ASPEC Ja-En dataset. For WMT14 De-En dataset, BLEU scores are generated using SacreBleu toolkit (Post, 2018). For models that produce sentence codes during decoding, the codes are removed from translation results before evaluating BLEU scores. 4.2 Obtaining Sentence Codes For the semantic coding model based on BERT, we cluster the hidden state of “[CLS]” token into 256 clusters with k-means algorithm. The cluster ids are then used as sentence codes. For models using FastText Embeddings, pre-trained vectors (Common Crawl, 2M words) are used. Please note that the number of clusters is a hyperparameter, here we choose the number of clusters to match the number of unique codes in the syntax-based model."
P19-1177,P16-1162,0,0.0752029,"search to search for best words in every decoding steps under the constraint of code assignment. Our approach also comes with 4 Experiments 4.1 Experimental Settings We evaluate our models on two machine translation datasets: ASPEC Japanese-to-English dataset (Nakazawa et al., 2016) and WMT14 Germanto-English dataset. The datasets contain 3M and 4.5M bilingual pairs respectively. For the ASPEC Ja-En dataset, we use the Moses toolkit (Koehn et al., 2007) to tokenize the English side and Kytea (Neubig et al., 2011) to tokenize the Japanese side. After tokenization, we apply byte-pair encoding (Sennrich et al., 2016) to segment the texts into subwords, forcing the vocabulary size of each language to be 40k. For WMT14 De-En dataset, we use sentencepiece (Kudo and Richardson, 2018) to segment the words to ensure a vocabulary size of 32k. In evaluation, we report tokenized BLEU for ASPEC Ja-En dataset. For WMT14 De-En dataset, BLEU scores are generated using SacreBleu toolkit (Post, 2018). For models that produce sentence codes during decoding, the codes are removed from translation results before evaluating BLEU scores. 4.2 Obtaining Sentence Codes For the semantic coding model based on BERT, we cluster the"
P19-1177,Q17-1010,0,0.011275,"from target sentences, we explore two methods. Semantic Coding Model The first method extracts sentence codes from unsupervisedly learned semantic information. We cluster the sentence embeddings produced by pre-trained models into a fixed number of clusters, then use the cluster ids as discrete priors to condition the sentence generation. In this work, we test two semantic coding models. The first model is based on BERT (Devlin et al., 2018), where the vectors corresponding to the “[CLS]” token are clustered. The second model produces sentence embeddings by averaging FastText word embeddings (Bojanowski et al., 2017). Comparing to the hidden states of BERT, word embeddings are expected to contain less syntactic information as the word order is ignored during training. Syntactic Coding Model To explicitly capture the syntactic diversity, we also consider to derive the sentence codes from the parse trees produced by a constituency parser. As the utterance-level information is not desired, the terminal nodes are removed from the parse trees. To obtain the sentence codes, we use a TreeLSTM-based auto-encoder similar to Socher et al. (2011), which encodes the syntactic information into a single discrete code."
P19-1177,P15-1150,0,0.0331554,"Missing"
P19-1363,D15-1075,0,0.726082,"jarring, and because semantic plausibility is not enough to root them out, preventing them is challenging. One approach to increasing the consistency of a chit-chat dialogue model was proposed in (Zhang et al., 2018), where the dialogue agent was given a set of personal facts describing its character (a persona) and produces utterances that reflect the persona. The intended outcome is that the agent produces utterances consistent with its given persona. However, these models still face the consistency issue, as shown in Figure 1. Separately, the framework of Natural Language Inference (NLI) (Bowman et al., 2015; Dagan et al., 2006; Maccartney and Manning, 2009) involves learning a mapping between a sentence pair and an entailment category. It is hypothesized that the NLI task is a proxy for general goals in natural language processing, such as language understanding (Bowman et al., 2015; Williams et al., 2018). Thus, the NLI task has been used for learning general sentence representations (Conneau et al., 2017) and for evaluating NLP models (Poliak et al., 2018a; Wang et al., 2018), with the expectation that such models will be useful in downstream tasks. Despite this expectation, leveraging an NLI"
P19-1363,P17-1152,0,0.057996,"s can be categorized into sentence encoding based methods of the form fMLP (genc (s1 ), genc (s2 )), and attention-based methods of the form fMLP (gattn (s1 , s2 )) (Lan and Xu, 2018). We thus choose and train representative models of each type which have achieved competitive performance on existing NLI benchmark datasets. For the sentence encoding method, we use InferSent (Conneau et al., 2017), which encodes a sentence using a bidirectional LSTM followed by max-pooling over the output states. As the representative attention-based method we use the enhanced sequential inference model (ESIM, (Chen et al., 2017)), which computes an attention score for each word pair. We also report results from a model trained and evaluated using the hypothesis sentence only (InferSent Hyp. Only) (Gururangan et al., 2018; Poliak et al., 2018c), a model trained on the existing SNLI dataset (Bowman et al., 2015) but evaluated 5 In our experiments, the softmax output corresponding to the contradiction class from Dialogue NLI. 6 Future work could consider filtering previous-utterance contradictions (ui , uj ) as well. i,j 3735 Data Type Matching Triple (p, p) Example Pred. Actual i am a hopeless bookworm. when i have som"
P19-1363,D17-1070,0,0.0635909,"oduces utterances consistent with its given persona. However, these models still face the consistency issue, as shown in Figure 1. Separately, the framework of Natural Language Inference (NLI) (Bowman et al., 2015; Dagan et al., 2006; Maccartney and Manning, 2009) involves learning a mapping between a sentence pair and an entailment category. It is hypothesized that the NLI task is a proxy for general goals in natural language processing, such as language understanding (Bowman et al., 2015; Williams et al., 2018). Thus, the NLI task has been used for learning general sentence representations (Conneau et al., 2017) and for evaluating NLP models (Poliak et al., 2018a; Wang et al., 2018), with the expectation that such models will be useful in downstream tasks. Despite this expectation, leveraging an NLI model for a downstream task remains an underexplored research direction. An NLI model may improve downstream task performance if properly used, while downstream tasks may yield new datasets or identify issues with existing NLI models, thus expanding the NLI research domain. In this paper, we reduce the problem of consistency in dialogue to natural language inference. We first create a dataset, Dialogue NL"
P19-1363,N18-2017,0,0.0198173,"s choose and train representative models of each type which have achieved competitive performance on existing NLI benchmark datasets. For the sentence encoding method, we use InferSent (Conneau et al., 2017), which encodes a sentence using a bidirectional LSTM followed by max-pooling over the output states. As the representative attention-based method we use the enhanced sequential inference model (ESIM, (Chen et al., 2017)), which computes an attention score for each word pair. We also report results from a model trained and evaluated using the hypothesis sentence only (InferSent Hyp. Only) (Gururangan et al., 2018; Poliak et al., 2018c), a model trained on the existing SNLI dataset (Bowman et al., 2015) but evaluated 5 In our experiments, the softmax output corresponding to the contradiction class from Dialogue NLI. 6 Future work could consider filtering previous-utterance contradictions (ui , uj ) as well. i,j 3735 Data Type Matching Triple (p, p) Example Pred. Actual i am a hopeless bookworm. when i have some spare time i read. Neutral Entail Matching Triple (u, p) i am from italy. i love the early mornings. i like getting up bright and early. Neutral Entail Misc. Utterance i do not understand footba"
P19-1363,C18-1328,0,0.0155167,"g to sre-rank . Hyper-parameters λ and k control the NLI model’s influence in re-ranking. For example, if the top candidate has a contradiction score of 1.0, then with λ = 1, it will be moved to the k’th position in the ranking. λ = 0 corresponds to no re-ranking. 5 5.1 Experiments Experiment 1: NLI That is, if the candidate ui does not contradict any persona sentence pj according to the NLI Models Many recently proposed NLI models can be categorized into sentence encoding based methods of the form fMLP (genc (s1 ), genc (s2 )), and attention-based methods of the form fMLP (gattn (s1 , s2 )) (Lan and Xu, 2018). We thus choose and train representative models of each type which have achieved competitive performance on existing NLI benchmark datasets. For the sentence encoding method, we use InferSent (Conneau et al., 2017), which encodes a sentence using a bidirectional LSTM followed by max-pooling over the output states. As the representative attention-based method we use the enhanced sequential inference model (ESIM, (Chen et al., 2017)), which computes an attention score for each word pair. We also report results from a model trained and evaluated using the hypothesis sentence only (InferSent Hyp."
P19-1363,P16-1094,0,0.0582053,"Consistency is a long standing issue faced by dialogue models. In this paper, we frame the consistency of dialogue agents as natural language inference (NLI) and create a new natural language inference dataset called Dialogue NLI. We propose a method which demonstrates that a model trained on Dialogue NLI can be used to improve the consistency of a dialogue model, and evaluate the method with human evaluation and with automatic metrics on a suite of evaluation sets designed to measure a dialogue model’s consistency. 1 Introduction A long standing issue faced by dialogue models is consistency (Li et al., 2016; Vinyals et al., 2015; Zhang et al., 2018). An example from (Vinyals et al., 2015) shows a two-round dialogue in which their neural sequence model first responds to what is your job? with i’m a lawyer, then responds to what do you do? with i’m a doctor. Even when inconsistencies are relatively rare and semantically plausible, they are jarring, and because semantic plausibility is not enough to root them out, preventing them is challenging. One approach to increasing the consistency of a chit-chat dialogue model was proposed in (Zhang et al., 2018), where the dialogue agent was given a set of"
P19-1363,W09-3714,0,0.0232086,"ty is not enough to root them out, preventing them is challenging. One approach to increasing the consistency of a chit-chat dialogue model was proposed in (Zhang et al., 2018), where the dialogue agent was given a set of personal facts describing its character (a persona) and produces utterances that reflect the persona. The intended outcome is that the agent produces utterances consistent with its given persona. However, these models still face the consistency issue, as shown in Figure 1. Separately, the framework of Natural Language Inference (NLI) (Bowman et al., 2015; Dagan et al., 2006; Maccartney and Manning, 2009) involves learning a mapping between a sentence pair and an entailment category. It is hypothesized that the NLI task is a proxy for general goals in natural language processing, such as language understanding (Bowman et al., 2015; Williams et al., 2018). Thus, the NLI task has been used for learning general sentence representations (Conneau et al., 2017) and for evaluating NLP models (Poliak et al., 2018a; Wang et al., 2018), with the expectation that such models will be useful in downstream tasks. Despite this expectation, leveraging an NLI model for a downstream task remains an underexplore"
P19-1363,marelli-etal-2014-sick,0,0.0285929,"dsourcing three label annotations for each example in the test set. We keep each test example for which two or more annotators agreed with its dataset label. All sentences in Dialogue NLI were generated by humans during the crowdsourced dialogue collection process of the Persona-Chat dataset (Zhang et al., 2018). The resulting sentence pairs are thus drawn from a natural dialogue domain that differs from existing NLI datasets, which are either drawn from different domains such as image captions or created using synthetic templates (Bowman et al., 2015; Demszky et al., 2018; Khot et al., 2018; Marelli et al., 2014; Poliak et al., 2018b; Wang et al., 2018; Williams et al., 2018). 4 Consistent Dialogue Agents via Natural Language Inference We now present a method which demonstrates that natural language inference can be used to improve the consistency of dialogue agents. Candidate utterances are re-ranked based on whether the candidate is predicted to contradict a persona sentence. If the NLI model predicts that a candidate contradicts a persona sentence, the candidate’s score is penalized, with the penalty weighted by the NLI model’s confidence5 scaled by a constant. Specifically, assume a dialogue mode"
P19-1363,D18-1298,0,0.0851225,"Missing"
P19-1363,D17-2014,1,0.933196,"sona consistency in dialogue. The reported metrics are percentages computed over each validation set. Figure 3: Example from the Likes Evaluation Set, showing dialogue model candidates, NLI model predictions, and reranked candidates using the method proposed in Section 4. model and the Dialogue NLI model. For the dialogue model we train a key-value memory network (Zhang et al., 2018) on the Persona-Chat dataset, which uses persona sentences and the conversation prefix as context. This model achieved the best performance on Persona-Chat in (Zhang et al., 2018). We train the model using ParlAI (Miller et al., 2017) on the personachat:self original task, using the hyper-parameters given for the KVMemnnAgent in the ConvAI2 competition. For the NLI model we use the ESIM model trained on Dialogue NLI, based on the results of Experiment 5. To study the effect of re-ranking on persona consistency, we form evaluation sets which contain next-utterances which are likely to yield persona contradiction or entailment, as follows. Evaluation Sets Each example is formed by first finding a next-utterance ut+1 in the Persona-Chat validation set which has an associated triple (e1 , r, e2 ) of interest, e.g. (i, like mus"
P19-1363,D14-1162,0,0.0828782,"triple (e1 , r, e2 ), where r ∈ R, e1 ∈ E1 , and e2 ∈ E2 . Here E1 is the set of all annotated e1 from the drop-downs or the text-box, and E2 is similarly defined. Finally, utterances are associated with a triple as follows. Let p be a persona sentence with triple (e1 , r, e2 ). We start with all utterances, U , from agents that have p in their persona. An utterance u ∈ U is then associated with the triple (e1 , r, e2 ) and persona sentence p when e2 is a sub-string of u, or word similarity4 sim(u, p) ≥ τ is suitably large. 4 We use cosine similarity between the mean of TF-IDF weighted GloVe (Pennington et al., 2014) word vectors and set τ = 0.9. 3734 3.3 Statistics Table 2 summarizes the dataset and its underlying data types. The label, triple, and data type are supplied as annotations for each sentence pair. We additionally create a gold-standard test set (Test Gold) by crowdsourcing three label annotations for each example in the test set. We keep each test example for which two or more annotators agreed with its dataset label. All sentences in Dialogue NLI were generated by humans during the crowdsourced dialogue collection process of the Persona-Chat dataset (Zhang et al., 2018). The resulting senten"
P19-1363,N18-2082,0,0.0380422,"Missing"
P19-1363,W18-5441,0,0.0533604,"Missing"
P19-1363,W18-5446,0,0.0492296,"Missing"
P19-1363,N18-1101,0,0.248564,"(a persona) and produces utterances that reflect the persona. The intended outcome is that the agent produces utterances consistent with its given persona. However, these models still face the consistency issue, as shown in Figure 1. Separately, the framework of Natural Language Inference (NLI) (Bowman et al., 2015; Dagan et al., 2006; Maccartney and Manning, 2009) involves learning a mapping between a sentence pair and an entailment category. It is hypothesized that the NLI task is a proxy for general goals in natural language processing, such as language understanding (Bowman et al., 2015; Williams et al., 2018). Thus, the NLI task has been used for learning general sentence representations (Conneau et al., 2017) and for evaluating NLP models (Poliak et al., 2018a; Wang et al., 2018), with the expectation that such models will be useful in downstream tasks. Despite this expectation, leveraging an NLI model for a downstream task remains an underexplored research direction. An NLI model may improve downstream task performance if properly used, while downstream tasks may yield new datasets or identify issues with existing NLI models, thus expanding the NLI research domain. In this paper, we reduce the p"
P19-1363,P18-1205,1,0.903158,"ced by dialogue models. In this paper, we frame the consistency of dialogue agents as natural language inference (NLI) and create a new natural language inference dataset called Dialogue NLI. We propose a method which demonstrates that a model trained on Dialogue NLI can be used to improve the consistency of a dialogue model, and evaluate the method with human evaluation and with automatic metrics on a suite of evaluation sets designed to measure a dialogue model’s consistency. 1 Introduction A long standing issue faced by dialogue models is consistency (Li et al., 2016; Vinyals et al., 2015; Zhang et al., 2018). An example from (Vinyals et al., 2015) shows a two-round dialogue in which their neural sequence model first responds to what is your job? with i’m a lawyer, then responds to what do you do? with i’m a doctor. Even when inconsistencies are relatively rare and semantically plausible, they are jarring, and because semantic plausibility is not enough to root them out, preventing them is challenging. One approach to increasing the consistency of a chit-chat dialogue model was proposed in (Zhang et al., 2018), where the dialogue agent was given a set of personal facts describing its character (a"
Q16-1002,P14-1133,0,0.0161394,"wledge questionanswering (open QA), generally require large teams of researchers, modular design and powerful infrastructure, exemplified by IBM’s Watson (Ferrucci et al., 2010). For this reason, much academic research focuses on settings in which the scope of the 25 task is reduced. This has been achieved by restricting questions to a specific topic or domain (Moll´a and Vicedo, 2007), allowing systems access to prespecified passages of text from which the answer can be inferred (Iyyer et al., 2014; Weston et al., 2015), or centering both questions and answers on a particular knowledge base (Berant and Liang, 2014; Bordes et al., 2014). In what follows, we show that the dictionary embedding models introduced in the previous sections may form a useful component of an open QA system. Given the absence of a knowledge base or web-scale information in our architecture, we narrow the scope of the task by focusing on general knowledge crossword questions. General knowledge (non-cryptic, or quick) crosswords appear in national newspapers in many countries. Crossword question answering is more tractable than general open QA for two reasons. First, models know the length of the correct answer (in letters), reduc"
Q16-1002,D14-1067,0,0.0958821,"(open QA), generally require large teams of researchers, modular design and powerful infrastructure, exemplified by IBM’s Watson (Ferrucci et al., 2010). For this reason, much academic research focuses on settings in which the scope of the 25 task is reduced. This has been achieved by restricting questions to a specific topic or domain (Moll´a and Vicedo, 2007), allowing systems access to prespecified passages of text from which the answer can be inferred (Iyyer et al., 2014; Weston et al., 2015), or centering both questions and answers on a particular knowledge base (Berant and Liang, 2014; Bordes et al., 2014). In what follows, we show that the dictionary embedding models introduced in the previous sections may form a useful component of an open QA system. Given the absence of a knowledge base or web-scale information in our architecture, we narrow the scope of the task by focusing on general knowledge crossword questions. General knowledge (non-cryptic, or quick) crosswords appear in national newspapers in many countries. Crossword question answering is more tractable than general open QA for two reasons. First, models know the length of the correct answer (in letters), reducing the search space."
Q16-1002,P12-1092,0,0.0381888,"omposition engine’. 2.4 Training Objective We train all neural language models M to map the input definition phrase sc defining word c to a location close to the the pre-trained embedding vc of c. We experiment with two different cost functions for the word-phrase pair (c, sc ) from the training data. The first is simply the cosine distance between M (sc ) and vc . The second is the rank loss max(0, m − cos(M (sc ), vc ) − cos(M (sc ), vr )) where vr is the embedding of a randomly-selected word from the vocabulary other than c. This loss function was used for language models, for example, in (Huang et al., 2012). In all experiments we apply a margin m = 0.1, which has been shown to work well on word-retrieval tasks (Bordes et al., 2015). 2.5 Implementation Details 3 Since training on the dictionary data took 6-10 hours, we did not conduct a hyper-parameter search on any validation sets over the space of possible model configurations such as embedding dimension, or size of hidden layers. Instead, we chose these parameters to be as standard as possible based on previous research. For fair comparison, any aspects of model design that are not specific to a particular class of model were kept constant acr"
Q16-1002,D14-1070,0,0.0316547,"Missing"
Q16-1002,P15-1162,0,0.0141736,"Missing"
Q16-1002,C12-1089,0,0.0178615,"under a CC-BY 4.0 license. For instance, a travel-writer might look to enhance her prose by searching for examples of a country that people associate with warm weather or an activity that is mentally or physically demanding. We show that an NLM-based reverse dictionary trained on only a handful of dictionaries identifies novel definitions and concept descriptions comparably or better than commercial systems, which rely on significant task-specific engineering and access to much more dictionary data. Moreover, by exploiting models that learn bilingual word representations (Vulic et al., 2011; Klementiev et al., 2012; Hermann and Blunsom, 2013; Gouws et al., 2014), we show that the NLM approach can be easily extended to produce a potentially useful cross-lingual reverse dictionary. The second application of our models is as a general-knowledge crossword question answerer. When trained on both dictionary definitions and the opening sentences of Wikipedia articles, NLMs produce plausible answers to (non-cryptic) crossword clues, even those that apparently require detailed world knowledge. Both BOW and RNN models can outperform bespoke commercial crossword solvers, particularly when clues contain a greater n"
Q16-1002,C94-1103,0,0.0104201,"uation settings. *Low variance in mult models is due to consistently poor scores, so not highlighted. unseen evaluation, we randomly selected 500 words from WordNet and excluded all definitions of these words from the training data of all models. Finally, for a fair comparison with OneLook, which has both the seen and unseen pairs in its internal database, we built a new dataset of concept descriptions that do not appear in the training data for any model. To do so, we randomly selected 200 adjectives, nouns or verbs from among the top 3000 most frequent tokens in the British National Corpus (Leech et al., 1994) (but outside the top 100). We then asked ten native English speakers to write a single-sentence ‘description’ of these words. To ensure the resulting descriptions were good quality, for each description we asked two participants who did not produce that description to list any words that fitted the description (up to a maximum of three). If the target word was not produced by one of the two checkers, the original participant was asked to re-write the description until the validation was passed.9 These concept descriptions, together with other evaluation sets, can be downloaded from our websit"
Q16-1002,J07-1004,0,0.0194807,"Missing"
Q16-1002,P11-2084,0,0.0169042,"Missing"
Q16-1002,W04-2105,0,0.35876,"purpose we experiment with two broad classes of neural language models (NLMs): Recurrent Neural Networks (RNNs), which naturally encode the order of input words, and simpler (feedforward) bag-of-words (BOW) embedding models. Prior to training these NLMs, we learn target lexical representations by training the Word2Vec software (Mikolov et al., 2013) on billions of words of raw text. We demonstrate the usefulness of our approach by building and releasing two applications. The first is a reverse dictionary or concept finder: a system that returns words based on user descriptions or definitions (Zock and Bilac, 2004). Reverse dictionaries are used by copywriters, novelists, translators and other professional writers to find words for notions or ideas that might be on the tip of their tongue. 17 Transactions of the Association for Computational Linguistics, vol. 4, pp. 17–30, 2016. Action Editor: Chris Callison-Burch. Submission batch: 9/2015; revised 12/2015; revised 1/2016; Published 2/2016. c 2016 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. For instance, a travel-writer might look to enhance her prose by searching for examples of a country that people associate with"
Q16-1002,N15-1184,0,\N,Missing
Q16-1002,D14-1179,1,\N,Missing
Q16-1002,D14-1079,0,\N,Missing
Q17-1026,D09-1030,0,0.00731951,"model is the best performer in CS-EN, FI-EN and RU-EN (Table 5 (j, o, t)), and is the runner-up in DE-EN (Table 5 (e)). The fact that the multilingual char2char model outperforms the single-pair models goes to show the parameter efficiency of character-level translation: instead of training N separate models for N language pairs, it is possible to get a better performance with a single multilingual character-level model. 6.2 Human Evaluation It is well known that automatic evaluation metrics such as BLEU encourage reference-like translations and do not fully capture true translation quality (Callison-Burch, 2009; Graham et al., 2015). Therefore, we also carry out a recently proposed evaluation from Graham et al. (2017) where we have human assessors rate both (1) adequacy; and (2) fluency of each system translation on a scale from 0 to 100 via Amazon Mechanical Turk. Adequacy is the degree to which assessors agree that the system translation expresses the meaning of the reference translation. Fluency is evaluated using system translation alone without any reference translation. 373 Approximately 1K Turkers assessed a single test set (3K sentences in newstest-2014) for each system and language pair. Ea"
Q17-1026,W14-4012,1,0.0529888,"Missing"
Q17-1026,D14-1179,1,0.0246211,"Missing"
Q17-1026,P16-1160,1,0.833084,"huber, 1997) or gated recurrent units (GRUs) (Cho et al., 2014b). The encoder constructs a set of continuous source sentence representations C by concatenating the forward and backward hid den states at each timestep: C = h1 , . . . , hTX , → − ← − where ht = h t ; h t . Attention First introduced in Bahdanau et al. (2015), the attention mechanism lets the decoder attend more to different source symbols for each target symbol. More concretely, it computes the context vector ct0 at each decoding time step t0 as Pa Xweighted αt0 t ht . sum of the source hidden states: ct0 = Tt=1 Similarly to Chung et al. (2016) and Firat et al. (2016a), each attentional weight αt0 t represents how relevant the t-th source token xt is to the t0 -th target token yt0 , and is computed as: αt0 t    1 = exp score Ey (yt0 −1 ), st0 −1 , ht , (1) Z  P X exp score(Ey (yt0 −1 ), st0 −1 , hk ) where Z = Tk=1 is the normalization constant. score() is a feedforward neural network with a single hidden layer that scores how well the source symbol xt and the target symbol yt0 match. Ey is the target embedding lookup table and st0 is the target hidden state at time t0 . Decoder Given a source context vector ct0 , the de0 coder"
Q17-1026,P16-2058,0,0.132846,"Missing"
Q17-1026,N16-1101,1,0.77853,"current units (GRUs) (Cho et al., 2014b). The encoder constructs a set of continuous source sentence representations C by concatenating the forward and backward hid den states at each timestep: C = h1 , . . . , hTX , → − ← − where ht = h t ; h t . Attention First introduced in Bahdanau et al. (2015), the attention mechanism lets the decoder attend more to different source symbols for each target symbol. More concretely, it computes the context vector ct0 at each decoding time step t0 as Pa Xweighted αt0 t ht . sum of the source hidden states: ct0 = Tt=1 Similarly to Chung et al. (2016) and Firat et al. (2016a), each attentional weight αt0 t represents how relevant the t-th source token xt is to the t0 -th target token yt0 , and is computed as: αt0 t    1 = exp score Ey (yt0 −1 ), st0 −1 , ht , (1) Z  P X exp score(Ey (yt0 −1 ), st0 −1 , hk ) where Z = Tk=1 is the normalization constant. score() is a feedforward neural network with a single hidden layer that scores how well the source symbol xt and the target symbol yt0 match. Ey is the target embedding lookup table and st0 is the target hidden state at time t0 . Decoder Given a source context vector ct0 , the de0 coder computes its hidden"
Q17-1026,D16-1026,1,0.257075,"Missing"
Q17-1026,N15-1124,0,0.00875968,"former in CS-EN, FI-EN and RU-EN (Table 5 (j, o, t)), and is the runner-up in DE-EN (Table 5 (e)). The fact that the multilingual char2char model outperforms the single-pair models goes to show the parameter efficiency of character-level translation: instead of training N separate models for N language pairs, it is possible to get a better performance with a single multilingual character-level model. 6.2 Human Evaluation It is well known that automatic evaluation metrics such as BLEU encourage reference-like translations and do not fully capture true translation quality (Callison-Burch, 2009; Graham et al., 2015). Therefore, we also carry out a recently proposed evaluation from Graham et al. (2017) where we have human assessors rate both (1) adequacy; and (2) fluency of each system translation on a scale from 0 to 100 via Amazon Mechanical Turk. Adequacy is the degree to which assessors agree that the system translation expresses the meaning of the reference translation. Fluency is evaluated using system translation alone without any reference translation. 373 Approximately 1K Turkers assessed a single test set (3K sentences in newstest-2014) for each system and language pair. Each Turker conducted a"
Q17-1026,P15-1001,1,0.634591,", one reason behind this is that sequences are significantly longer when represented in characters, compounding the problem of data sparsity and modeling long-range dependencies. This has driven NMT research to be almost exclusively word-level (Bahdanau et al., 2015; Sutskever et al., 2014). Despite their remarkable success, word-level NMT models suffer from several major weaknesses. For one, they are unable to model rare, out-ofvocabulary words, making them limited in translating languages with rich morphology such as Czech, Finnish and Turkish. If one uses a large vocabulary to combat this (Jean et al., 2015), the complexity of training and decoding grows linearly with respect to the target vocabulary size, leading to a vicious cycle. To address this, we present a fully character-level NMT model that maps a character sequence in a source language to a character sequence in a target language. We show that our model outperforms a baseline with a subword-level encoder on DE-EN and CS-EN, and achieves a comparable result on FI-EN and RU-EN. A purely character-level NMT model with a basic encoder was proposed as a baseline by Luong and Manning (2016), but training it was prohibitively slow. We were abl"
Q17-1026,P16-1100,0,0.198409,"Missing"
Q17-1026,D15-1166,0,0.105673,"Missing"
Q17-1026,W16-2323,0,0.0371469,"n takes the source context vector from the attention mechanism and predicts each target character. This decoder was described as base decoder by Chung et al. (2016). 5 Experiment Settings 5.1 Task and Models We evaluate the proposed character-to-character (char2char) translation model against subwordlevel baselines (bpe2bpe and bpe2char) on the WMT’15 DE→EN, CS→EN, FI→EN and RU→EN translation tasks.1 We do not consider word-level models, as it has already been shown that subword-level models outperform them by mitigating issues inherent to closed-vocabulary translation (Sennrich et al., 2015; Sennrich et al., 2016). Indeed, subword-level NMT models have been the de-facto state-of-the-art and are now used in a very large-scale industry NMT system to serve millions of users per day (Wu et al., 2016). 1 http://www.statmt.org/wmt15/translation -task.html 370 bilingual bpe2bpe: from (Firat et al., 2016a) bilingual bpe2char: from (Chung et al., 2016) bilingual char2char multilingual bpe2char multilingual char2char Datasets and Preprocessing We use all available parallel data on the four language pairs from WMT’15: DE-EN, CS-EN, FI-EN and RU-EN. For the bpe2char baselines, we only use sentence pairs where the"
Q17-1026,N16-1161,0,0.0207425,"Missing"
Q19-1042,P02-1040,0,\N,Missing
Q19-1042,W05-0909,0,\N,Missing
Q19-1042,P05-1033,0,\N,Missing
Q19-1042,D10-1092,0,\N,Missing
Q19-1042,P16-1057,0,\N,Missing
Q19-1042,P17-2012,1,\N,Missing
Q19-1042,D18-1509,0,\N,Missing
Q19-1042,D18-1396,0,\N,Missing
Q19-1042,D18-1044,0,\N,Missing
Q19-1042,Q19-1006,0,\N,Missing
Q19-1042,W19-3620,1,\N,Missing
Q19-1042,P16-1162,0,\N,Missing
Q19-1042,D18-1149,1,\N,Missing
Q19-1042,N16-1035,0,\N,Missing
R19-1153,P16-1001,0,0.0150775,"et al., 2018). Developing models with relaxed or learned generation orders has picked up recent interest (Welleck et al., 2018, 2019; Gu et al., 2019; Stern et al., 2019). We investigate this for dependency parsing, framing the problem as sequential set generation without a prespecified order. Finally, our work is inspired by techniques for improving upon maximum likelihood training through error exploration and dynamic oracles (Goldberg and Nivre, 2012, 2013), and related techniques in imitation learning for structured prediction (Daum´e III et al., 2009; Ross et al., 2011; He et al., 2012; Goodman et al., 2016). In particular, our formulation is closely related to the framework of (Chang et al., 2015), where our oracle can be seen as an optimal roll-out policy which computes action costs without explicit roll-outs. 6 Conclusion We described a family of dependency parsers which construct a dependency tree by generating a sequence of edge sets, and a learning method that does not presuppose a generation order. Ex1343 perimentally, we found that a ‘coaching’ method, which weights actions in the loss according to the model, improves parsing accuracy compared to a uniform weighting and allows the parser"
R19-1153,Q19-1042,1,0.834872,"fferent languages. The proposed model trained with coaching achieves a higher UAS than the Qi et al. (2018) model on 12 of the 19 datasets, plus two ties. A separate thread of research in sequential modeling has demonstrated that generation order can affect performance (Vinyals et al., 2015), both in tasks with set-structured outputs such as objects (Welleck et al., 2017, 2018) or graphs (Li et al., 2018), and in sequential tasks such as language modeling (Ford et al., 2018). Developing models with relaxed or learned generation orders has picked up recent interest (Welleck et al., 2018, 2019; Gu et al., 2019; Stern et al., 2019). We investigate this for dependency parsing, framing the problem as sequential set generation without a prespecified order. Finally, our work is inspired by techniques for improving upon maximum likelihood training through error exploration and dynamic oracles (Goldberg and Nivre, 2012, 2013), and related techniques in imitation learning for structured prediction (Daum´e III et al., 2009; Ross et al., 2011; He et al., 2012; Goodman et al., 2016). In particular, our formulation is closely related to the framework of (Chang et al., 2015), where our oracle can be seen as an"
R19-1153,N19-1076,0,0.0384171,"Missing"
R19-1153,D18-1324,0,0.0143061,"oNLL 2018 competition, reporting test UAS evaluated using their pre-trained models. Table 3 shows the results on the 19 datasets from 17 different languages. The proposed model trained with coaching achieves a higher UAS than the Qi et al. (2018) model on 12 of the 19 datasets, plus two ties. A separate thread of research in sequential modeling has demonstrated that generation order can affect performance (Vinyals et al., 2015), both in tasks with set-structured outputs such as objects (Welleck et al., 2017, 2018) or graphs (Li et al., 2018), and in sequential tasks such as language modeling (Ford et al., 2018). Developing models with relaxed or learned generation orders has picked up recent interest (Welleck et al., 2018, 2019; Gu et al., 2019; Stern et al., 2019). We investigate this for dependency parsing, framing the problem as sequential set generation without a prespecified order. Finally, our work is inspired by techniques for improving upon maximum likelihood training through error exploration and dynamic oracles (Goldberg and Nivre, 2012, 2013), and related techniques in imitation learning for structured prediction (Daum´e III et al., 2009; Ross et al., 2011; He et al., 2012; Goodman et al."
R19-1153,N10-1115,0,0.0418332,"bal Scholar Facebook AI Research kyunghyun.cho@nyu.edu Abstract transition-based approaches by viewing parsing as sequential graph generation. In this view, a graph is incrementally built by adding edges to an edge set. No distinction between projective and non-projective trees is necessary. Since edges do not have a pre-specified order, we propose a set-based learning method. Like (Fern´andezGonz´alez and G´omez-Rodrguez, 2019), our parser runs in n steps. However, our learning method and transitions do not impose a left-to-right parsing order, allowing easy-first (Tsuruoka and Tsujii, 2005; Goldberg and Elhadad, 2010) behavior. Experimentally, we find that the proposed method can yield a sequential parser with preferred, inputdependent generation orders and performance gains over strong one-step methods.1 We propose a method for non-projective dependency parsing by incrementally predicting a set of edges. Since the edges do not have a pre-specified order, we propose a set-based learning method. Our method blends graph, transition, and easyfirst parsing, including a prior state of the parser as a special case. The proposed transition-based method successfully parses near the state of the art on both project"
R19-1153,C12-1059,0,0.0240798,"in tasks with set-structured outputs such as objects (Welleck et al., 2017, 2018) or graphs (Li et al., 2018), and in sequential tasks such as language modeling (Ford et al., 2018). Developing models with relaxed or learned generation orders has picked up recent interest (Welleck et al., 2018, 2019; Gu et al., 2019; Stern et al., 2019). We investigate this for dependency parsing, framing the problem as sequential set generation without a prespecified order. Finally, our work is inspired by techniques for improving upon maximum likelihood training through error exploration and dynamic oracles (Goldberg and Nivre, 2012, 2013), and related techniques in imitation learning for structured prediction (Daum´e III et al., 2009; Ross et al., 2011; He et al., 2012; Goodman et al., 2016). In particular, our formulation is closely related to the framework of (Chang et al., 2015), where our oracle can be seen as an optimal roll-out policy which computes action costs without explicit roll-outs. 6 Conclusion We described a family of dependency parsers which construct a dependency tree by generating a sequence of edge sets, and a learning method that does not presuppose a generation order. Ex1343 perimentally, we found t"
R19-1153,Q13-1033,0,0.0479092,"Missing"
R19-1153,N19-1114,0,0.0192163,"rying by the choice of transition system and feature representation. Traditional stack-based arc-standard and arc-eager (Yamada and Matsumoto, 2003; Nivre, 2003) transition systems only parse projectively, requiring additional operations for pseudo-nonprojectivity (G´omez-Rodr´ıguez et al., 2014) or projectivity (Nivre, 2009), while list-based nonprojective systems have been developed (Nivre, 2008). Recent variations assume a generation order such as top-down (Ma et al., 2018) or left-toright (Fern´andez-Gonz´alez and G´omez-Rodrguez, 2019). Other recent models focus on unsupervised settings (Kim et al., 2019). Our focus here is a non-projective transition system and learning method which does not assume a particular generation order. Table 3: Test set results (UAS) on datasets from the CoNLL 2018 shared task with greater than 200k examples, plus the Ancient Greek (GRC) and Chinese (ZH) datasets. Bold denotes the highest UAS on each dataset. to those preferred by the policy may be more effective than sampling uniformly from the set of all correct trajectories. Based on these results, we use the coaching oracle and valid roll-in for training our final model in the next experiment. 4.3 Related Work C"
R19-1153,P18-1130,0,0.1088,"se. The proposed transition-based method successfully parses near the state of the art on both projective and non-projective languages, without assuming a certain parsing order. 1 Introduction Dependency parsing methods can be categorized as graph-based and transition-based. Typical graph-based methods support non-projective parsing, but introduce independence assumptions and rely on external decoding algorithms. Conversely, transition-based methods model joint dependencies, but without modification are typically limited to projective parses. There are two recent exceptions of interest here. (Ma et al., 2018) developed the Stack-Pointer parser, a transition-based, non-projective parser that maintains a stack populated in a top-down, depth-first manner, and uses a pointer network to determine the dependent of the stack’s top node, resulting in a transition sequence of length 2n − 1. Recently, (Fern´andez-Gonz´alez and G´omezRodrguez, 2019) developed a variant of the StackPointer parser which parses in n steps by traversing the sentence left-to-right, selecting the head of the current node in the traversal, while incrementally checking for, and prohibiting, cycles. We take inspiration from both grap"
R19-1153,W03-3017,0,0.182931,"N (EWT) ES ET FR (GSD) GRC (Perseus) HI IT (ISDT) KO (KAIST) LA (ITTB) NO (Bokmaal) NO (Nynorsk) PT RU (SynTagRus) ZH Ours Qi et al. (2018) 88.22 94.13 93.53 93.80 88.39 91.28 93.70 89.56 91.07 80.90 96.78 94.06 91.02 93.66 94.63 94.44 91.22 94.57 87.31 88.35 94.13 93.22 93.21 87.21 91.21 93.38 89.40 90.90 82.77 96.78 94.24 90.55 93.00 94.27 94.02 91.67 94.42 88.49 5 Transition-based dependency parsing has a rich history, with methods generally varying by the choice of transition system and feature representation. Traditional stack-based arc-standard and arc-eager (Yamada and Matsumoto, 2003; Nivre, 2003) transition systems only parse projectively, requiring additional operations for pseudo-nonprojectivity (G´omez-Rodr´ıguez et al., 2014) or projectivity (Nivre, 2009), while list-based nonprojective systems have been developed (Nivre, 2008). Recent variations assume a generation order such as top-down (Ma et al., 2018) or left-toright (Fern´andez-Gonz´alez and G´omez-Rodrguez, 2019). Other recent models focus on unsupervised settings (Kim et al., 2019). Our focus here is a non-projective transition system and learning method which does not assume a particular generation order. Table 3: Test se"
R19-1153,J08-4003,0,0.0647354,"2 94.57 87.31 88.35 94.13 93.22 93.21 87.21 91.21 93.38 89.40 90.90 82.77 96.78 94.24 90.55 93.00 94.27 94.02 91.67 94.42 88.49 5 Transition-based dependency parsing has a rich history, with methods generally varying by the choice of transition system and feature representation. Traditional stack-based arc-standard and arc-eager (Yamada and Matsumoto, 2003; Nivre, 2003) transition systems only parse projectively, requiring additional operations for pseudo-nonprojectivity (G´omez-Rodr´ıguez et al., 2014) or projectivity (Nivre, 2009), while list-based nonprojective systems have been developed (Nivre, 2008). Recent variations assume a generation order such as top-down (Ma et al., 2018) or left-toright (Fern´andez-Gonz´alez and G´omez-Rodrguez, 2019). Other recent models focus on unsupervised settings (Kim et al., 2019). Our focus here is a non-projective transition system and learning method which does not assume a particular generation order. Table 3: Test set results (UAS) on datasets from the CoNLL 2018 shared task with greater than 200k examples, plus the Ancient Greek (GRC) and Chinese (ZH) datasets. Bold denotes the highest UAS on each dataset. to those preferred by the policy may be more"
R19-1153,P09-1040,0,0.0457979,".39 91.28 93.70 89.56 91.07 80.90 96.78 94.06 91.02 93.66 94.63 94.44 91.22 94.57 87.31 88.35 94.13 93.22 93.21 87.21 91.21 93.38 89.40 90.90 82.77 96.78 94.24 90.55 93.00 94.27 94.02 91.67 94.42 88.49 5 Transition-based dependency parsing has a rich history, with methods generally varying by the choice of transition system and feature representation. Traditional stack-based arc-standard and arc-eager (Yamada and Matsumoto, 2003; Nivre, 2003) transition systems only parse projectively, requiring additional operations for pseudo-nonprojectivity (G´omez-Rodr´ıguez et al., 2014) or projectivity (Nivre, 2009), while list-based nonprojective systems have been developed (Nivre, 2008). Recent variations assume a generation order such as top-down (Ma et al., 2018) or left-toright (Fern´andez-Gonz´alez and G´omez-Rodrguez, 2019). Other recent models focus on unsupervised settings (Kim et al., 2019). Our focus here is a non-projective transition system and learning method which does not assume a particular generation order. Table 3: Test set results (UAS) on datasets from the CoNLL 2018 shared task with greater than 200k examples, plus the Ancient Greek (GRC) and Chinese (ZH) datasets. Bold denotes the"
R19-1153,K18-2016,0,0.117031,"al., 2015) or narrow the set of training trajectories. We evaluate several alternatives: ∗ 1. uniform (i, j) ∼ πunif ∗ 2. coaching (i, j) ∼ πθ πunif 3. valid-policy (i, j) ∼ valid(πθ ) where valid(πθ ) is the set of edges that keeps the predicted tree as a valid dependency tree. The coaching and valid-policy roll-ins choose edge permutations that are preferred by the policy, with valid-policy resembling test-time behavior. 4 Experiments In Experiments 4.1 and 4.2 we evaluate on English, German, Chinese, and Ancient Greek since they vary with respect to projectivity, size, and performance in (Qi et al., 2018). Based on these development set results, we then test our strongest model on a large suite of languages (4.3). Experimental Setup Experiments are done using datasets from the CoNLL 2018 Shared Task (Zeman et al., 2018). We build our implementation from the open-source version of (Qi et al., 2018)3 , and use their experimental setup (e.g. 3 https://github.com/stanfordnlp/ stanfordnlp. Figure 1: Per-step edge distributions from recurrent weight models trained with the given oracle. pre-processing, data-loading, pre-trained vectors, evaluation) which follows the shared task setup. Our model uses"
R19-1153,H05-1059,0,0.107028,"niversity CIFAR Azrieli Global Scholar Facebook AI Research kyunghyun.cho@nyu.edu Abstract transition-based approaches by viewing parsing as sequential graph generation. In this view, a graph is incrementally built by adding edges to an edge set. No distinction between projective and non-projective trees is necessary. Since edges do not have a pre-specified order, we propose a set-based learning method. Like (Fern´andezGonz´alez and G´omez-Rodrguez, 2019), our parser runs in n steps. However, our learning method and transitions do not impose a left-to-right parsing order, allowing easy-first (Tsuruoka and Tsujii, 2005; Goldberg and Elhadad, 2010) behavior. Experimentally, we find that the proposed method can yield a sequential parser with preferred, inputdependent generation orders and performance gains over strong one-step methods.1 We propose a method for non-projective dependency parsing by incrementally predicting a set of edges. Since the edges do not have a pre-specified order, we propose a set-based learning method. Our method blends graph, transition, and easyfirst parsing, including a prior state of the parser as a special case. The proposed transition-based method successfully parses near the sta"
R19-1153,W19-3620,1,0.918733,"this paper, we restrict to the case of predicting a single edge (i,ˆj) per step, so that the recurrent weight model generates a sequence of edges with SN theˆ goal of matching a target edge set, i.e. t=1 (i, j)t = E. Since the target edges E are a set, the model’s generation order is not determined a priori. As a result, we propose to use a learning method that does not require a pre-specified generation order and allows the model to learn inputdependent orderings. Our proposed method is based on the multiset loss (Welleck et al., 2018) and its recent extensions for non-monotonic generation (Welleck et al., 2019). The method is motivated from the perspective of learning-to-search (Daum´e III et al., 2009; Chang et al., 2015), which involves learning a policy πθ that mimics an oracle policy π ∗ . The policy maps states to distributions over actions. For the proposed graph parser, an action is an edge (i, j) ∈ E, and a state st is an input sentence x ˆ<t . The along with the edges predicted so far, E policy is a conditional distribution over E, ˆ<t , x), πθ ((i, j)|E such as the distribution in equation (5). Learning consists of minimizing a cost, computed by first sampling states from a roll-in policy"
R19-1153,W03-3023,0,0.0770683,"AR CA CS (CAC) CS (PDT) DE EN (EWT) ES ET FR (GSD) GRC (Perseus) HI IT (ISDT) KO (KAIST) LA (ITTB) NO (Bokmaal) NO (Nynorsk) PT RU (SynTagRus) ZH Ours Qi et al. (2018) 88.22 94.13 93.53 93.80 88.39 91.28 93.70 89.56 91.07 80.90 96.78 94.06 91.02 93.66 94.63 94.44 91.22 94.57 87.31 88.35 94.13 93.22 93.21 87.21 91.21 93.38 89.40 90.90 82.77 96.78 94.24 90.55 93.00 94.27 94.02 91.67 94.42 88.49 5 Transition-based dependency parsing has a rich history, with methods generally varying by the choice of transition system and feature representation. Traditional stack-based arc-standard and arc-eager (Yamada and Matsumoto, 2003; Nivre, 2003) transition systems only parse projectively, requiring additional operations for pseudo-nonprojectivity (G´omez-Rodr´ıguez et al., 2014) or projectivity (Nivre, 2009), while list-based nonprojective systems have been developed (Nivre, 2008). Recent variations assume a generation order such as top-down (Ma et al., 2018) or left-toright (Fern´andez-Gonz´alez and G´omez-Rodrguez, 2019). Other recent models focus on unsupervised settings (Kim et al., 2019). Our focus here is a non-projective transition system and learning method which does not assume a particular generation order. Ta"
R19-1153,K18-2001,0,0.136618,"Missing"
W14-4009,D11-1033,0,0.0189847,"ion-based approach together with another RNNenc trained to translate from French to English. The two RNNenc’s are used in the proposed segmentation algorithm to compute the confidence score of each phrase (See Eqs. (2)–(3)). We also compare with the translations of a conventional phrase-based machine translation system, which we expect to be more robust when translating long sentences. Experiment Settings 4.1 Models and Approaches Dataset We evaluate the proposed approach on the task of English-to-French translation. We use a bilingual, parallel corpus of 348M words selected by the method of (Axelrod et al., 2011) from a combination of Europarl (61M), news commentary (5.5M), UN (421M) and two crawled corpora of 90M and 780M words respectively.2 The performance of our models was tested on news-test2012, news-test2013, and news-test2014. When comparing with the phrase-based SMT system Moses (Koehn et al., 2007), the first two were used as a development set for tuning Moses while news-test2014 was used as our test set. To train the neural network models, we use only the sentence pairs in the parallel corpus, where both English and French sentences are at most 30 words long. Furthermore, we limit our vocab"
W14-4009,W14-4012,1,0.708259,"Missing"
W14-4009,D14-1179,1,0.231926,"Missing"
W14-4009,D13-1176,0,0.0611189,"anslated by the neural network translation model. Once each segment has been independently translated by the neural machine translation model, the translated clauses are concatenated to form a final translation. Empirical results show a significant improvement in translation quality for long sentences. 1 Introduction Up to now, most research efforts in statistical machine translation (SMT) research have relied on the use of a phrase-based system as suggested in (Koehn et al., 2003). Recently, however, an entirely new, neural network based approach has been proposed by several research groups (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014b), showing promising results, both as a standalone system or as an additional component in the existing phrase-based system. In this neural network based approach, an encoder ‘encodes’ a variable-length input sentence into a fixed-length vector and a decoder ‘decodes’ a variable-length target sentence from the fixedlength encoded vector. It has been observed in (Sutskever et al., 2014), (Kalchbrenner and Blunsom, 2013) and (Cho et al., 2014a) that this neural network approach 2 Background: RNN Encoder–Decoder for Translation The RNN Encoder–Decoder (R"
W14-4009,N03-1017,0,0.00893003,"n this paper, we propose a way to address this issue by automatically segmenting an input sentence into phrases that can be easily translated by the neural network translation model. Once each segment has been independently translated by the neural machine translation model, the translated clauses are concatenated to form a final translation. Empirical results show a significant improvement in translation quality for long sentences. 1 Introduction Up to now, most research efforts in statistical machine translation (SMT) research have relied on the use of a phrase-based system as suggested in (Koehn et al., 2003). Recently, however, an entirely new, neural network based approach has been proposed by several research groups (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014b), showing promising results, both as a standalone system or as an additional component in the existing phrase-based system. In this neural network based approach, an encoder ‘encodes’ a variable-length input sentence into a fixed-length vector and a decoder ‘decodes’ a variable-length target sentence from the fixedlength encoded vector. It has been observed in (Sutskever et al., 2014), (Kalchbrenner and Bluns"
W14-4012,D11-1033,0,0.0378693,"x(0, x)) are used as the element-wise nonlinear functions for the RNNenc and grConv respectively. The grConv has 2000 hidden neurons, whereas the RNNenc has 1000 hidden neurons. The word embeddings are 620-dimensional in both cases.2 Both models were trained for approximately 110 hours, which is equivalent to 296,144 updates and 846,322 updates for the grConv and RNNenc, respectively. Experiment Settings 4.1 Models Dataset We evaluate the encoder–decoder models on the task of English-to-French translation. We use the bilingual, parallel corpus which is a set of 348M selected by the method in (Axelrod et al., 2011) from a combination of Europarl (61M words), news commentary (5.5M), UN (421M) and two crawled corpora of 90M and 780M words respectively.1 We did not use separate monolingual data. The performance of the neural machien translation models was measured on the news-test2012, news-test2013 and news-test2014 sets ( 3000 lines each). When comparing to the SMT system, we use news-test2012 and news-test2013 as our development set for tuning the SMT system, and news-test2014 as our test set. Among all the sentence pairs in the prepared parallel corpus, for reasons of computational efficiency we only u"
W14-4012,D13-1176,0,0.891685,"translation using two models; RNN Encoder–Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically. 1 Introduction A new approach for statistical machine translation based purely on neural networks has recently been proposed (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014). This new approach, which we refer to as neural machine translation, is inspired by the recent trend of deep representational learning. All the neural network models used in (Sutskever et al., 2014; Cho et al., 2014) consist of an encoder and a decoder. The encoder extracts a fixed-length vector representation from a variablelength input sentence, and from this representation the decoder generates a correct, variable-length target translation. ∗ Research done while visiting Universit´e de Montr´eal 103 Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and St"
W14-4012,N03-1017,0,0.0857637,"overall translation performance (see Table 1 (a)). This analysis suggests that that the current neural translation approach has its weakness in handling long sentences. The most obvious explanatory hypothesis is that the fixed-length vector representation does not have enough capacity to encode a long sentence with complicated structure and meaning. In order to encode a variable-length sequence, a neural network may “sacrifice” some of the important topics in the input sentence in order to remember others. This is in stark contrast to the conventional phrase-based machine translation system (Koehn et al., 2003). As we can see from Fig. 5, the conventional system trained on the same dataset (with additional monolingual data for the language model) tends to get a higher BLEU score on longer When we use the beam-search to find the k best translations, we do not use a usual log-probability but one normalized with respect to the length of the translation. This prevents the RNN decoder from favoring shorter translations, behavior which was observed earlier in, e.g., (Graves, 2013). 5 5.1 Results and Analysis Quantitative Analysis In this paper, we are interested in the properties of the neural machine tra"
W14-4012,D11-1035,0,0.0426355,"se models. 5.2 40 BLEU score 35 30 25 20 15 Source text Reference text Both 10 5 0 0 10 20 30 40 50 60 Sentence length 70 80 Qualitative Analysis Figure 5: The BLEU scores achieved by an SMT system for sentences of a given length. The plot is smoothed by taking a window of size 10. We use the solid, dotted and dashed lines to show the effect of different lengths of source, reference or both of them, respectively. Although BLEU score is used as a de-facto standard metric for evaluating the performance of a machine translation system, it is not the perfect metric (see, e.g., (Song et al., 2013; Liu et al., 2011)). Hence, here we present some of the actual translations generated from the two models, RNNenc and grConv. In Table. 2 (a)–(b), we show the translations of some randomly selected sentences from the development and test sets. We chose the ones that have no unknown words. (a) lists long sentences (longer than 30 words), and (b) short sentences (shorter than 10 words). We can see that, despite the difference in the BLEU scores, all three models (RNNenc, grConv and Moses) do a decent job at translating, especially, short sentences. When the source sentences are long, however, we notice the perfor"
W15-3014,J93-2003,0,0.0594725,"training, we choose a smaller vocabulary size τ and divide the training set into partitions, each of which contains approximately τ unique target words. For each partition, we train the model as if only the unique words within it existed, leaving the embeddings of all the other words fixed. At test time, the corresponding subset of target words for each source sentence is not known in advance, yet we still want to keep computational complexity manageable. To overcome this, we run an existing word alignment tool on the training corpus in advance to obtain word-based conditional probabilities (Brown et al., 1993). During decoding, we start with an initial target vocabulary containing the K most frequent words. Then, reading a few sentences at once, we arbitrarily replace some of these initial words by the K 0 most 1 This step differs very slightly from (Jean et al., 2015), where the sentence-specific words were added on top of the K common ones instead of replacing them. 136 where σ is a logistic sigmoid function, vg , wg and bg are model parameters. The output of the controller network is multiplied to the LM’s hidden state sLM i : pLM = sLM t t gt . Knight, 2003), as implemented in the Moses toolkit"
W15-3014,W11-2123,0,0.0196365,"lary comprising of the 42K most frequent words in the English side of the intersection of the parallel corpora of Fi→En, De→En and Cs→En. Importantly, we use the same RNN-LM for both Fi→En, Cs→En and De→En. In the experiments with deep fusion, we used the randomly selected 2/3 of newsdev2015 as a validation set and the rest as a held-out set. In the case of De→En, we used newstest2013 for validation and newstest2014 for test. For all language pairs except Fi→En, we also simply built 5-gram language models, this time on all appropriate provided data, with the exception of the English Gigaword (Heafield, 2011). In our contrastive submissions only, we re-ranked our 20best lists with the LM log-probabilities, once again divided by sentence length. The relative weight of the language model was manually chosen to maximize BLEU on the development set. 4 Results Results for single systems and primary ensemble submissions are presented in Table 2.5 When translating from English to another language, neural machine translation works particularly well, achieving the best BLEU-c scores among all the constrained systems. On the other hand, NMT is generally competitive even in the case of translating to English"
W15-3014,P15-1001,1,0.340179,"5 use the RNNsearch architecture from (Bahdanau et al., 2015). In this case, the encoder assigns a context-dependent vector, or annotation, to every source word. The decoder then selectively combines the most relevant annotations to generate each target word. NMT systems often use a limited vocabulary of approximately 30, 000 to 80, 000 target words, which leads them to generate many outof-vocabulary tokens (hUNKi). This may easily lead to the degraded quality of the translations. To sidestep this problem, we employ a variant of importance sampling to help increase the target vocabulary size (Jean et al., 2015). Even with a larger vocabulary, there will almost assuredly be words in the test set that were unseen during training. As such, we replace generated out-ofvocaulbary tokens with the corresponding source words with a technique similar to those proposed by (Luong et al., 2015). Most NMT systems rely only on parallel data, ignoring the wealth of information found in large monolingual corpora. On Finnish→English, we combine our systems with a recurrent neural netNeural machine translation (NMT) systems have recently achieved results comparable to the state of the art on a few translation tasks, i"
W15-3014,D13-1176,0,0.047794,"Cho University of Montreal Roland Memisevic University of Montreal Yoshua Bengio University of Montreal CIFAR Senior Fellow firstname.lastname@umontreal.ca Abstract variety of language pairs. As such, we trained systems on Czech↔English, German↔English and Finnish→English. Furthermore, the human evaluation campaign of WMT’15 will help us better understand the quality of NMT systems which have mainly been evaluated using the automatic evaluation metric such as BLEU (Papineni et al., 2002). Most NMT systems are based on the encoderdecoder architecture (Cho et al., 2014; Sutskever et al., 2014; Kalchbrenner and Blunsom, 2013). The source sentence is first read by the encoder, which compresses it into a real-valued vector. From this vector representation the decoder may then generate a translation word-by-word. One limitation of this approach is that a source sentence of any length must be encoded into a fixedlength vector. To address this issue, our systems for WMT’15 use the RNNsearch architecture from (Bahdanau et al., 2015). In this case, the encoder assigns a context-dependent vector, or annotation, to every source word. The decoder then selectively combines the most relevant annotations to generate each targe"
W15-3014,E03-1076,0,0.0280529,"Missing"
W15-3014,P15-1002,0,0.260546,"en use a limited vocabulary of approximately 30, 000 to 80, 000 target words, which leads them to generate many outof-vocabulary tokens (hUNKi). This may easily lead to the degraded quality of the translations. To sidestep this problem, we employ a variant of importance sampling to help increase the target vocabulary size (Jean et al., 2015). Even with a larger vocabulary, there will almost assuredly be words in the test set that were unseen during training. As such, we replace generated out-ofvocaulbary tokens with the corresponding source words with a technique similar to those proposed by (Luong et al., 2015). Most NMT systems rely only on parallel data, ignoring the wealth of information found in large monolingual corpora. On Finnish→English, we combine our systems with a recurrent neural netNeural machine translation (NMT) systems have recently achieved results comparable to the state of the art on a few translation tasks, including English→French and English→German. The main purpose of the Montreal Institute for Learning Algorithms (MILA) submission to WMT’15 is to evaluate this new approach on a greater variety of language pairs. Furthermore, the human evaluation campaign may help us and the r"
W15-3014,P02-1040,0,0.0950986,"University of Montreal Middle East Technical University, Turkey jeasebas@iro.umontreal.ca orhan.firat@ceng.metu.edu.tr Kyunghyun Cho University of Montreal Roland Memisevic University of Montreal Yoshua Bengio University of Montreal CIFAR Senior Fellow firstname.lastname@umontreal.ca Abstract variety of language pairs. As such, we trained systems on Czech↔English, German↔English and Finnish→English. Furthermore, the human evaluation campaign of WMT’15 will help us better understand the quality of NMT systems which have mainly been evaluated using the automatic evaluation metric such as BLEU (Papineni et al., 2002). Most NMT systems are based on the encoderdecoder architecture (Cho et al., 2014; Sutskever et al., 2014; Kalchbrenner and Blunsom, 2013). The source sentence is first read by the encoder, which compresses it into a real-valued vector. From this vector representation the decoder may then generate a translation word-by-word. One limitation of this approach is that a source sentence of any length must be encoded into a fixedlength vector. To address this issue, our systems for WMT’15 use the RNNsearch architecture from (Bahdanau et al., 2015). In this case, the encoder assigns a context-depende"
W15-3014,P07-2045,0,\N,Missing
W15-3014,D14-1179,1,\N,Missing
W16-1618,W06-0901,0,0.0379377,"concepts or topics covered by the corresponding event types in the ACE 2005 corpus. As we can see from the 5 Taken from the ACE 2005 Annotation Guideline 163 Defined by the annotation guideline. plan to apply the two-stage algorithm to other tasks such as relation extension to further verify its effectiveness. table, the Meet and Phone-Write subtypes or topics of Contact are quite separate from those of the other types. 7 Related Work References Early research on event extraction has primarily focused on local sentence-level representations in a pipelined architecture (Grishman et al., 2005; Ahn, 2006). Afterward, higher level features have been found to improve the performance (Ji and Grishman, 2008; Gupta and Ji, 2009; Patwardhan and Riloff, 2009; Liao and Grishman, 2010; Liao and Grishman, 2011; Hong et al., 2011; McClosky et al., 2011; Huang and Riloff, 2012; Li et al., 2013). Some recent research has proposed joint models for EE, including the methods based on Markov Logic Networks (Riedel et al., 2009; Poon and Vanderwende, 2010; Venugopal et al., 2014), structured perceptron (Li et al., 2013; Li et al., 2014b), and dual decomposition (Riedel et al. (2009; 2011b)). The application of"
W16-1618,W06-1615,0,0.0242467,"abeled instances for each event type in the training data. Unfortunately, this setting does not reflect the real world situation very well. In practice, we often have a large amount of training data for some old event types but are interested in extracting instances of a new event type. The new event type is only specified by a small set of seed instances provided by clients (the event type extension setting). How can we effectively leverage the training data of old event types to facilitate the extraction of the new event type? Inspired by the work on transfer learning and domain adaptation (Blitzer et al., 2006; Jiang and Zhai, 2007; Daume III, 2007; Jiang, 2009), in this paper, we systematically evaluate the representative methods (i.e, the feature based model and the CNN model) for ED to gain an insight into which kind of method performs better in the new extension setting. In addition, we propose a two-stage algorithm to train a CNN model that effectively learns and transfers the knowledge from the old event types for the extraction of the target type. We study the event detection problem in the new type extension setting. In particular, our task involves identifying the event instances of a targ"
W16-1618,D14-1199,0,0.034208,"Missing"
W16-1618,P15-1017,0,0.447118,"he feature-based approach that has dominated the ED research in the last decade (Ji and Grishman, 2008; Gupta and Ji, 2009; Liao and Grishman, 2011; McClosky et al., 2011; Riedel and McCallum, 2011; Li et al., 2013; Venugopal et al., 2014). The second approach, on the other hand, is proposed very recently and uses convolutional neural networks (CNN) to exploit the continuous representations of words. These continuous representations have been shown to effectively capture the underlying structures of a sentence, thereby significantly improving the performance for ED (Nguyen and Grishman, 2015; Chen et al., 2015). The previous research has mainly focused on building an ED system in a supervised setting. The performance of such systems strongly depends on a sufficient amount of labeled instances for each event type in the training data. Unfortunately, this setting does not reflect the real world situation very well. In practice, we often have a large amount of training data for some old event types but are interested in extracting instances of a new event type. The new event type is only specified by a small set of seed instances provided by clients (the event type extension setting). How can we effect"
W16-1618,P07-1033,0,0.0333017,"Missing"
W16-1618,P09-2093,0,0.0127275,"ken from the ACE 2005 Annotation Guideline 163 Defined by the annotation guideline. plan to apply the two-stage algorithm to other tasks such as relation extension to further verify its effectiveness. table, the Meet and Phone-Write subtypes or topics of Contact are quite separate from those of the other types. 7 Related Work References Early research on event extraction has primarily focused on local sentence-level representations in a pipelined architecture (Grishman et al., 2005; Ahn, 2006). Afterward, higher level features have been found to improve the performance (Ji and Grishman, 2008; Gupta and Ji, 2009; Patwardhan and Riloff, 2009; Liao and Grishman, 2010; Liao and Grishman, 2011; Hong et al., 2011; McClosky et al., 2011; Huang and Riloff, 2012; Li et al., 2013). Some recent research has proposed joint models for EE, including the methods based on Markov Logic Networks (Riedel et al., 2009; Poon and Vanderwende, 2010; Venugopal et al., 2014), structured perceptron (Li et al., 2013; Li et al., 2014b), and dual decomposition (Riedel et al. (2009; 2011b)). The application of neural networks to EE is very recent. In particular, Zhou et al. (2014) and Boros et al. (2014) use neural networks to l"
W16-1618,P11-1113,0,0.20635,"he two-stage algorithm to other tasks such as relation extension to further verify its effectiveness. table, the Meet and Phone-Write subtypes or topics of Contact are quite separate from those of the other types. 7 Related Work References Early research on event extraction has primarily focused on local sentence-level representations in a pipelined architecture (Grishman et al., 2005; Ahn, 2006). Afterward, higher level features have been found to improve the performance (Ji and Grishman, 2008; Gupta and Ji, 2009; Patwardhan and Riloff, 2009; Liao and Grishman, 2010; Liao and Grishman, 2011; Hong et al., 2011; McClosky et al., 2011; Huang and Riloff, 2012; Li et al., 2013). Some recent research has proposed joint models for EE, including the methods based on Markov Logic Networks (Riedel et al., 2009; Poon and Vanderwende, 2010; Venugopal et al., 2014), structured perceptron (Li et al., 2013; Li et al., 2014b), and dual decomposition (Riedel et al. (2009; 2011b)). The application of neural networks to EE is very recent. In particular, Zhou et al. (2014) and Boros et al. (2014) use neural networks to learn word embeddings from a corpus of specific domains and then directly utilize these embeddings"
W16-1618,P08-1030,1,0.37819,"e can see from the 5 Taken from the ACE 2005 Annotation Guideline 163 Defined by the annotation guideline. plan to apply the two-stage algorithm to other tasks such as relation extension to further verify its effectiveness. table, the Meet and Phone-Write subtypes or topics of Contact are quite separate from those of the other types. 7 Related Work References Early research on event extraction has primarily focused on local sentence-level representations in a pipelined architecture (Grishman et al., 2005; Ahn, 2006). Afterward, higher level features have been found to improve the performance (Ji and Grishman, 2008; Gupta and Ji, 2009; Patwardhan and Riloff, 2009; Liao and Grishman, 2010; Liao and Grishman, 2011; Hong et al., 2011; McClosky et al., 2011; Huang and Riloff, 2012; Li et al., 2013). Some recent research has proposed joint models for EE, including the methods based on Markov Logic Networks (Riedel et al., 2009; Poon and Vanderwende, 2010; Venugopal et al., 2014), structured perceptron (Li et al., 2013; Li et al., 2014b), and dual decomposition (Riedel et al. (2009; 2011b)). The application of neural networks to EE is very recent. In particular, Zhou et al. (2014) and Boros et al. (2014) use"
W16-1618,P09-1114,0,0.107311,"fortunately, this setting does not reflect the real world situation very well. In practice, we often have a large amount of training data for some old event types but are interested in extracting instances of a new event type. The new event type is only specified by a small set of seed instances provided by clients (the event type extension setting). How can we effectively leverage the training data of old event types to facilitate the extraction of the new event type? Inspired by the work on transfer learning and domain adaptation (Blitzer et al., 2006; Jiang and Zhai, 2007; Daume III, 2007; Jiang, 2009), in this paper, we systematically evaluate the representative methods (i.e, the feature based model and the CNN model) for ED to gain an insight into which kind of method performs better in the new extension setting. In addition, we propose a two-stage algorithm to train a CNN model that effectively learns and transfers the knowledge from the old event types for the extraction of the target type. We study the event detection problem in the new type extension setting. In particular, our task involves identifying the event instances of a target type that is only specified by a small set of seed"
W16-1618,P13-1008,0,0.566443,"involves event argument discovery. There have been two major approaches to ED in the literature. The first approach extensively leverages linguistic analysis and knowledge resources to capture the discrete structures for ED, focusing on the combination of various properties 1 most often a single verb or nominalization 158 Proceedings of the 1st Workshop on Representation Learning for NLP, pages 158–165, c Berlin, Germany, August 11th, 2016. 2016 Association for Computational Linguistics entropy (MaxEnt) and classified as the type T or not. In this work, we employ the feature set for ED from (Li et al., 2013), which is the state-of-the-art FET. The experimental results show that this two-stage algorithm significantly outperforms the traditional methods in the type extension setting for ED and demonstrates the benefit of CNN in transfer learning. To our knowledge, this is the first work on the type extension setting as well as on transferring knowledge with neural networks for ED of natural language processing. 2 3.2 In a CNN for ED, we limit the context of the trigger candidates to a fixed window size by trimming longer sentences and padding shorter sentences with a special token when necessary. L"
W16-1618,D14-1198,0,0.0351828,"entence-level representations in a pipelined architecture (Grishman et al., 2005; Ahn, 2006). Afterward, higher level features have been found to improve the performance (Ji and Grishman, 2008; Gupta and Ji, 2009; Patwardhan and Riloff, 2009; Liao and Grishman, 2010; Liao and Grishman, 2011; Hong et al., 2011; McClosky et al., 2011; Huang and Riloff, 2012; Li et al., 2013). Some recent research has proposed joint models for EE, including the methods based on Markov Logic Networks (Riedel et al., 2009; Poon and Vanderwende, 2010; Venugopal et al., 2014), structured perceptron (Li et al., 2013; Li et al., 2014b), and dual decomposition (Riedel et al. (2009; 2011b)). The application of neural networks to EE is very recent. In particular, Zhou et al. (2014) and Boros et al. (2014) use neural networks to learn word embeddings from a corpus of specific domains and then directly utilize these embeddings as features in statistical classifiers. Chen et al. (2015) apply dynamic multi-pooling CNNs for EE in a pipelined framework, while Nguyen et al. (2016) propose joint event extraction using recurrent neural networks. Finally, domain adaptation and transfer learning have been studied extensively for variou"
W16-1618,P10-1081,1,0.725734,"fined by the annotation guideline. plan to apply the two-stage algorithm to other tasks such as relation extension to further verify its effectiveness. table, the Meet and Phone-Write subtypes or topics of Contact are quite separate from those of the other types. 7 Related Work References Early research on event extraction has primarily focused on local sentence-level representations in a pipelined architecture (Grishman et al., 2005; Ahn, 2006). Afterward, higher level features have been found to improve the performance (Ji and Grishman, 2008; Gupta and Ji, 2009; Patwardhan and Riloff, 2009; Liao and Grishman, 2010; Liao and Grishman, 2011; Hong et al., 2011; McClosky et al., 2011; Huang and Riloff, 2012; Li et al., 2013). Some recent research has proposed joint models for EE, including the methods based on Markov Logic Networks (Riedel et al., 2009; Poon and Vanderwende, 2010; Venugopal et al., 2014), structured perceptron (Li et al., 2013; Li et al., 2014b), and dual decomposition (Riedel et al. (2009; 2011b)). The application of neural networks to EE is very recent. In particular, Zhou et al. (2014) and Boros et al. (2014) use neural networks to learn word embeddings from a corpus of specific domains"
W16-1618,R11-1002,1,0.880877,"uideline. plan to apply the two-stage algorithm to other tasks such as relation extension to further verify its effectiveness. table, the Meet and Phone-Write subtypes or topics of Contact are quite separate from those of the other types. 7 Related Work References Early research on event extraction has primarily focused on local sentence-level representations in a pipelined architecture (Grishman et al., 2005; Ahn, 2006). Afterward, higher level features have been found to improve the performance (Ji and Grishman, 2008; Gupta and Ji, 2009; Patwardhan and Riloff, 2009; Liao and Grishman, 2010; Liao and Grishman, 2011; Hong et al., 2011; McClosky et al., 2011; Huang and Riloff, 2012; Li et al., 2013). Some recent research has proposed joint models for EE, including the methods based on Markov Logic Networks (Riedel et al., 2009; Poon and Vanderwende, 2010; Venugopal et al., 2014), structured perceptron (Li et al., 2013; Li et al., 2014b), and dual decomposition (Riedel et al. (2009; 2011b)). The application of neural networks to EE is very recent. In particular, Zhou et al. (2014) and Boros et al. (2014) use neural networks to learn word embeddings from a corpus of specific domains and then directly utiliz"
W16-1618,P10-1040,0,0.0179284,"might only be partial and not necessarily include all the trigger words of type T in D. Also, we call DN the set of the negative instances generated from D under this setting (to be discussed in more details later). In general, DN might contains unannotated trigger words of T (false negatives), making this task more challenging. Eventually, our goal is to learn an event detector for T , leveraging the training data DT , DA and DN for both the target and auxiliary types. Note that our work is related to Jiang (2009) who studies the relation type extension problem. 3 1. Word Embedding Table E (Turian et al., 2010; Mikolov et al., 2013a; Mikolov et al., 2013b). 2. Position Embedding Table: to embed the relative distance i of xi to the current token x0 . 3. Entity Type Embedding Table: to capture the entity type information for each token. Following Nguyen and Grishman (2015), we assign the entity type labels to each token using the heads of the entity mentions in x with the BIO schema. As a result, the original event trigger candidate x is transformed into a matrix x = [x−w , x−w+1 , . . . , x0 , . . . , xw−1 , xw ]. This matrix will serve as the input for CNN. Models for Event Detection In this sectio"
W16-1618,N10-1004,0,0.0148524,"Missing"
W16-1618,D14-1090,0,0.109324,"rly research on event extraction has primarily focused on local sentence-level representations in a pipelined architecture (Grishman et al., 2005; Ahn, 2006). Afterward, higher level features have been found to improve the performance (Ji and Grishman, 2008; Gupta and Ji, 2009; Patwardhan and Riloff, 2009; Liao and Grishman, 2010; Liao and Grishman, 2011; Hong et al., 2011; McClosky et al., 2011; Huang and Riloff, 2012; Li et al., 2013). Some recent research has proposed joint models for EE, including the methods based on Markov Logic Networks (Riedel et al., 2009; Poon and Vanderwende, 2010; Venugopal et al., 2014), structured perceptron (Li et al., 2013; Li et al., 2014b), and dual decomposition (Riedel et al. (2009; 2011b)). The application of neural networks to EE is very recent. In particular, Zhou et al. (2014) and Boros et al. (2014) use neural networks to learn word embeddings from a corpus of specific domains and then directly utilize these embeddings as features in statistical classifiers. Chen et al. (2015) apply dynamic multi-pooling CNNs for EE in a pipelined framework, while Nguyen et al. (2016) propose joint event extraction using recurrent neural networks. Finally, domain adaptation and t"
W16-1618,P11-1163,0,0.0172154,"thm to other tasks such as relation extension to further verify its effectiveness. table, the Meet and Phone-Write subtypes or topics of Contact are quite separate from those of the other types. 7 Related Work References Early research on event extraction has primarily focused on local sentence-level representations in a pipelined architecture (Grishman et al., 2005; Ahn, 2006). Afterward, higher level features have been found to improve the performance (Ji and Grishman, 2008; Gupta and Ji, 2009; Patwardhan and Riloff, 2009; Liao and Grishman, 2010; Liao and Grishman, 2011; Hong et al., 2011; McClosky et al., 2011; Huang and Riloff, 2012; Li et al., 2013). Some recent research has proposed joint models for EE, including the methods based on Markov Logic Networks (Riedel et al., 2009; Poon and Vanderwende, 2010; Venugopal et al., 2014), structured perceptron (Li et al., 2013; Li et al., 2014b), and dual decomposition (Riedel et al. (2009; 2011b)). The application of neural networks to EE is very recent. In particular, Zhou et al. (2014) and Boros et al. (2014) use neural networks to learn word embeddings from a corpus of specific domains and then directly utilize these embeddings as features in statisti"
W16-1618,P14-2012,1,0.527304,"Missing"
W16-1618,P15-2060,1,0.307446,"azetteers. This is called the feature-based approach that has dominated the ED research in the last decade (Ji and Grishman, 2008; Gupta and Ji, 2009; Liao and Grishman, 2011; McClosky et al., 2011; Riedel and McCallum, 2011; Li et al., 2013; Venugopal et al., 2014). The second approach, on the other hand, is proposed very recently and uses convolutional neural networks (CNN) to exploit the continuous representations of words. These continuous representations have been shown to effectively capture the underlying structures of a sentence, thereby significantly improving the performance for ED (Nguyen and Grishman, 2015; Chen et al., 2015). The previous research has mainly focused on building an ED system in a supervised setting. The performance of such systems strongly depends on a sufficient amount of labeled instances for each event type in the training data. Unfortunately, this setting does not reflect the real world situation very well. In practice, we often have a large amount of training data for some old event types but are interested in extracting instances of a new event type. The new event type is only specified by a small set of seed instances provided by clients (the event type extension setting"
W16-1618,P15-1062,1,0.900189,"Missing"
W16-1618,N16-1034,1,0.371819,"the methods based on Markov Logic Networks (Riedel et al., 2009; Poon and Vanderwende, 2010; Venugopal et al., 2014), structured perceptron (Li et al., 2013; Li et al., 2014b), and dual decomposition (Riedel et al. (2009; 2011b)). The application of neural networks to EE is very recent. In particular, Zhou et al. (2014) and Boros et al. (2014) use neural networks to learn word embeddings from a corpus of specific domains and then directly utilize these embeddings as features in statistical classifiers. Chen et al. (2015) apply dynamic multi-pooling CNNs for EE in a pipelined framework, while Nguyen et al. (2016) propose joint event extraction using recurrent neural networks. Finally, domain adaptation and transfer learning have been studied extensively for various NLP tasks, including part of speech tagging (Blitzer et al., 2006), name tagging (Daume III, 2007), parsing (McClosky et al., 2010), relation extraction (Plank and Moschitti, 2013; Nguyen and Grishman, 2014; Nguyen et al., 2015a), to name a few. For event extraction, Miwa et al. (2013) study instance weighting and stacking models while Riedel and McCallum (2011b) examine joint models with domain adaptation. However, none of them studies the"
W16-1618,D09-1016,0,0.0113689,"5 Annotation Guideline 163 Defined by the annotation guideline. plan to apply the two-stage algorithm to other tasks such as relation extension to further verify its effectiveness. table, the Meet and Phone-Write subtypes or topics of Contact are quite separate from those of the other types. 7 Related Work References Early research on event extraction has primarily focused on local sentence-level representations in a pipelined architecture (Grishman et al., 2005; Ahn, 2006). Afterward, higher level features have been found to improve the performance (Ji and Grishman, 2008; Gupta and Ji, 2009; Patwardhan and Riloff, 2009; Liao and Grishman, 2010; Liao and Grishman, 2011; Hong et al., 2011; McClosky et al., 2011; Huang and Riloff, 2012; Li et al., 2013). Some recent research has proposed joint models for EE, including the methods based on Markov Logic Networks (Riedel et al., 2009; Poon and Vanderwende, 2010; Venugopal et al., 2014), structured perceptron (Li et al., 2013; Li et al., 2014b), and dual decomposition (Riedel et al. (2009; 2011b)). The application of neural networks to EE is very recent. In particular, Zhou et al. (2014) and Boros et al. (2014) use neural networks to learn word embeddings from a c"
W16-1618,P13-1147,0,0.0300964,"Missing"
W16-1618,N10-1123,0,0.0141137,"7 Related Work References Early research on event extraction has primarily focused on local sentence-level representations in a pipelined architecture (Grishman et al., 2005; Ahn, 2006). Afterward, higher level features have been found to improve the performance (Ji and Grishman, 2008; Gupta and Ji, 2009; Patwardhan and Riloff, 2009; Liao and Grishman, 2010; Liao and Grishman, 2011; Hong et al., 2011; McClosky et al., 2011; Huang and Riloff, 2012; Li et al., 2013). Some recent research has proposed joint models for EE, including the methods based on Markov Logic Networks (Riedel et al., 2009; Poon and Vanderwende, 2010; Venugopal et al., 2014), structured perceptron (Li et al., 2013; Li et al., 2014b), and dual decomposition (Riedel et al. (2009; 2011b)). The application of neural networks to EE is very recent. In particular, Zhou et al. (2014) and Boros et al. (2014) use neural networks to learn word embeddings from a corpus of specific domains and then directly utilize these embeddings as features in statistical classifiers. Chen et al. (2015) apply dynamic multi-pooling CNNs for EE in a pipelined framework, while Nguyen et al. (2016) propose joint event extraction using recurrent neural networks. Finally"
W16-1618,D11-1001,0,0.0243931,"Missing"
W16-1618,W11-1807,0,0.0518334,"Missing"
W16-1618,W09-1406,0,0.0191274,"of the other types. 7 Related Work References Early research on event extraction has primarily focused on local sentence-level representations in a pipelined architecture (Grishman et al., 2005; Ahn, 2006). Afterward, higher level features have been found to improve the performance (Ji and Grishman, 2008; Gupta and Ji, 2009; Patwardhan and Riloff, 2009; Liao and Grishman, 2010; Liao and Grishman, 2011; Hong et al., 2011; McClosky et al., 2011; Huang and Riloff, 2012; Li et al., 2013). Some recent research has proposed joint models for EE, including the methods based on Markov Logic Networks (Riedel et al., 2009; Poon and Vanderwende, 2010; Venugopal et al., 2014), structured perceptron (Li et al., 2013; Li et al., 2014b), and dual decomposition (Riedel et al. (2009; 2011b)). The application of neural networks to EE is very recent. In particular, Zhou et al. (2014) and Boros et al. (2014) use neural networks to learn word embeddings from a corpus of specific domains and then directly utilize these embeddings as features in statistical classifiers. Chen et al. (2015) apply dynamic multi-pooling CNNs for EE in a pipelined framework, while Nguyen et al. (2016) propose joint event extraction using recurr"
W16-2309,P16-1160,1,0.897656,"xemes and morphemes. The other issue, which is specific to neural network approaches, is that neural machine translation systems suffers from increased complexity due to the large vocabulary size (Jean et al., 2015; Luong et al., 2015), which does not happen with character-level modelling. Most issues of word-level modelling can be addressed to certain extent by switching into finer tokens, e.g., characters. In fact, to neural networks, each and every token in the vocabulary is treated as an independent entity, and the semantics of tokens are simply learned to maximize the objective function (Chung et al., 2016). This property allows a lot of freedom to the neural machine translation system in the choice of tokens. The NYU-MILA neural machine translation system is built on the idea of directly generating characters, instead of words, that can possibly unlink a machine translation system from the need of explicit segmentation as a preprocessing step, which is often suboptimal in solving translation tasks. We focus on representing the target sentence as a sequence of characters, and the source sentence as a sequence of subwords (Sennrich et al., 2015). We describe the neural machine translation system"
W16-2309,P15-1001,1,0.870012,"Missing"
W16-2309,D13-1176,0,0.0640476,"words or their segmented-out lexemes are usually considered as basic units of meaning, which makes words to be more suitable when solving natural language processing tasks. There are however two pressing issues here. The first issue is the absence of a perfect segmentation algorithm for any single language. A perfect 2 System Description In this section, we describe the details of the NYUMILA neural machine translation system. In our system, we closely follow the neural machine translation model proposed by Bahdanau et al. (2015). A neural machine translation model (For˜ cada and Neco, 1997; Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014) aims at building an end-to-end neural network that takes as input a source sentence X = (x1 , . . . , xTx ) and outputs its translation Y = (y1 , . . . , yTy ), where This system description paper summarizes and details the experimental procedure described in Chung et al. (2016) 268 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 268–271, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics xt and yt0 are respectively source and target tokens. The neural network is const"
W16-2309,P15-1002,0,0.0595715,"Missing"
W16-2309,D14-1179,1,\N,Missing
W16-2309,P16-1162,0,\N,Missing
W17-4806,W17-4801,0,0.0403717,"Missing"
W17-4806,D17-1301,0,0.0256907,"=1 exp(eij ) αij = PTx k=1 exp(eik ) ∗ This work was done during his visit to NYU. Now at Google (orhanf@google.com). 54 Proceedings of the Third Workshop on Discourse in Machine Translation, pages 54–57, c Copenhagen, Denmark, September 8, 2017. 2017 Association for Computational Linguistics. (4) 3.2 eij = a(si−1 , yi−1 , hj ) Double-Gated Context Model (DGCM) Our second approach is very similar to the first with the exception that, for both functions f and g, distinct gates (g1 and g2 ) are applied to the context representation cci . Similar context-modulating gates were previously used by (Wang et al., 2017). (5) where eij is the attention model score, which represents how well the output at time i aligns with the input around time j. si = f (si−1 , yi−1 , ci , g1 cci ) 3 Larger-Context Neural Machine Translation As the antecedent needed to correctly translate a pronoun may be in a different sentence (intersentential anaphora) (Guillou et al., 2016), we added the previous sentence as a auxiliary input to the neural machine translation system, using an additional encoder and attention model. Similarly to the source sentence encoding, we apply a bidirectional recurrent  network to generate context"
W17-4806,N16-1101,1,0.592754,"Missing"
W17-5409,P14-1062,0,0.0975468,"Missing"
W17-5409,D15-1176,0,0.0199434,"Missing"
W17-5409,D16-1209,1,0.890223,"Missing"
W17-5409,D13-1170,0,0.0186626,"Missing"
W18-3221,W18-3219,0,0.175796,"Missing"
W18-3221,L18-1550,0,0.0418527,"Missing"
W18-3221,N16-1030,0,0.353212,"s “solved”, performance deteriorates proportional to the degree of code-switching in the data. The shared task for the third workshop on Computational Approaches on Linguistic Code-Switching concerned named entity recognition (NER) for two code-switched language pairs (Aguilar et al., 2018): Modern Standard Arabic and Egyptian (MSA-EGY); and English-Spanish (ENG-SPA). Here, we describe our work on the shared task. Traditional NER systems used to rely heavily on hand-crafted features and gazetteers, but have since been replaced by neural architectures that combine bidirectional LSTMs and CRFs (Lample et al., 2016). Equipped with supervised characterlevel representations and pre-trained unsupervised word embeddings, such neural architectures have not only come to dominate named entity recognition, but have also successfully been applied to code-switched language identification (Samih et al., 2016), which makes them highly suitable for the current task as well. 2 Approach The input data consists of noisy user-generated social media text collected from Twitter. Codeswitching can occur between different tweets in the training data, with many tweets being monolingual, but can also occur within tweets (e.g."
W18-3221,P16-1101,0,0.0380217,"er • O: Any other token that is not an NE 154 Proceedings of The Third Workshop on Computational Approaches to Code-Switching, pages 154–158 c Melbourne, Australia, July 19, 2018. 2018 Association for Computational Linguistics The train/valid/test split for MSA-EGY was 10102/1122/1110. The train/valid/test split for ENG-SPA was 50757/832/15634. The first work to combine CRFs with modern neural representation learning for NER is, to our knowledge, by Collobert et al. (2011). Our architecture is similar to more recent neural architectures for NER, e.g. Huang et al. (2015); Lample et al. (2016); Ma and Hovy (2016). Instead of using a straightforward bidirectional LSTM (BiLSTM), we use several layers and add shortcut connections. Instead of simply feeding in word (and/or character) embeddings, we add a selfattention mechanism. 2.1 is, we combine the language-specific word embeddings wL1 and wL2 with the character-level word representation via a simple self-attention mechanism: αi = softmax(U tanh(V [wL1 , wL2 , wchar ])), wword+char = [α1 wL1 , α2 wL2 , α3 wchar ] 2.2 Capitalization Additionally, we concatenate an embedding to indicate the capitalization of the word, which be either no-capitals, startin"
W18-3221,W17-5308,0,0.0581651,"Missing"
W18-3221,W16-5806,0,0.051127,"8): Modern Standard Arabic and Egyptian (MSA-EGY); and English-Spanish (ENG-SPA). Here, we describe our work on the shared task. Traditional NER systems used to rely heavily on hand-crafted features and gazetteers, but have since been replaced by neural architectures that combine bidirectional LSTMs and CRFs (Lample et al., 2016). Equipped with supervised characterlevel representations and pre-trained unsupervised word embeddings, such neural architectures have not only come to dominate named entity recognition, but have also successfully been applied to code-switched language identification (Samih et al., 2016), which makes them highly suitable for the current task as well. 2 Approach The input data consists of noisy user-generated social media text collected from Twitter. Codeswitching can occur between different tweets in the training data, with many tweets being monolingual, but can also occur within tweets (e.g. “[USER]: en los finales be like [URL]”) or even morphologically within words (e.g. “pero esta twitteando y pitchandome los textos”). The goal is to predict the correct IOB entity type for the following categories: • [BI]-PER: Person • [BI]-LOC: Location • [BI]-ORG: Organization • [BI]-GR"
W18-5452,N18-1101,1,0.850568,"g, and PRPN-UP for (unsupervised) parsing. PRPN-LM is much larger than PRPN-UP, with embedding layer that is 4 times larger and the number of units per layer that is 3 times larger. In the PRPN-UP experiments, we observe that the WSJ data is not split, such that the test data is used without parse information for training. This implies that the parsing results of PRPN-UP may not be generalizable in the way usually expected of machine learning evaluation results. We train PRPN on sentences from two datasets: The full WSJ and AllNLI, the concatenation of SNLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2018b). We then evaluate the constituency trees produced by these models on the full WSJ, WSJ101 , and the MultiNLI development set. 3 Results Table 1 shows results with all the models under study, plus several baselines, on WSJ and WSJ10. Unexpectedly, the PRPN-LM models achieve higher parsing performance than PRPNUP. This shows that any tuning done to separate PRPN-UP from PRPN-LM was not necessary, and that the results described in the paper can be largely reproduced by a unified model in a fair setting. Moreover, the PRPN models trained on the larger, out-of-domain AllNLI perform better than t"
W18-5452,D15-1075,1,0.746123,"—PRPN-LM tuned for language modeling, and PRPN-UP for (unsupervised) parsing. PRPN-LM is much larger than PRPN-UP, with embedding layer that is 4 times larger and the number of units per layer that is 3 times larger. In the PRPN-UP experiments, we observe that the WSJ data is not split, such that the test data is used without parse information for training. This implies that the parsing results of PRPN-UP may not be generalizable in the way usually expected of machine learning evaluation results. We train PRPN on sentences from two datasets: The full WSJ and AllNLI, the concatenation of SNLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2018b). We then evaluate the constituency trees produced by these models on the full WSJ, WSJ101 , and the MultiNLI development set. 3 Results Table 1 shows results with all the models under study, plus several baselines, on WSJ and WSJ10. Unexpectedly, the PRPN-LM models achieve higher parsing performance than PRPNUP. This shows that any tuning done to separate PRPN-UP from PRPN-LM was not necessary, and that the results described in the paper can be largely reproduced by a unified model in a fair setting. Moreover, the PRPN models trained on the larger, out-of"
W18-5452,P02-1017,0,0.842674,"ees – – – – – – 34.7 21.3 (0.0) 21.4 – 21.3 (0.0) 21.3 5.3 4.6 17.4 22.1 – – Parsing F1 Depth Accuracy on WSJ by Tag WSJ10 WSJ WSJ ADJP NP PP INTJ µ (σ) max µ (σ) max 22.3 16.0 20.2 9.3 40.4 55.9 Table 1: Unlabeled parsing F1 test results broken down by training data and by early stopping criterion. The Accuracy columns represent the fraction of ground truth constituents of a given type that correspond to constituents in the model parses. Italics mark results that are worse than the random baseline. Results with RL-SPINN and ST-Gumbel are from Williams et al. (2018a). WSJ10 baselines are from Klein and Manning (2002, CCM), Klein and Manning (2005, DMV+CCM), and Bod (2006, UML-DOP). Model 300D SPINN w/o Leaf GRU 300D SPINN-NC w/o Leaf GRU NLI NLI NLI NLI 19.3 21.2 19.2 20.6 36.9 39.0 36.2 38.9 70.2 63.5 70.5 64.1 6.2 6.4 6.1 6.3 300D ST-Gumbel w/o Leaf GRU 300D RL-SPINN w/o Leaf GRU NLI NLI NLI NLI 32.6 30.8 95.0 99.1 37.5 35.6 13.5 10.7 23.7 27.5 18.8 18.1 4.1 4.6 8.6 8.6 PRPN-LM PRPN-UP PRPN-UP LM UP LM 25.6 26.9 45.7 19.4 41.0 46.3 19.9 37.4 48.6 4.9 4.9 4.9 – – 27.9 28.0 27.0 21.7 36.8 21.3 4.4 3.9 Random Trees Balanced Trees et al. (2018, called ST-Gumbel), despite the fact that the model was tuned e"
W19-2304,P02-1040,0,0.135422,"0 98.01 98.14 87.28 99.05 98.73 59.91 57.90 33.71 56.19 55.30 91.86 91.72 72.86 88.05 91.08 98.43 98.55 91.12 97.44 98.81 64.59 60.94 25.74 68.35 44.75 93.27 92.04 65.04 94.20 82.06 98.59 98.56 88.42 99.23 96.31 Table 2: Self-BLEU and percent of generated n-grams that are unique relative to own generations (left) WikiText103 test set (middle) a sample of 5000 sentences from Toronto Book Corpus (right). For the WT103 and TBC rows, we sample 1000 sentences from the respective datasets. Quality To automatically measure the quality of the generations, we follow Yu et al. (2017) by computing BLEU (Papineni et al., 2002) between the generations and the original data distributions to measure how similar the generations are. We use a random sample of 5000 sentences from the test set of WikiText-103 (WT103, Merity et al., 2016) and a random sample of 5000 sentences from TBC as references. We also use the perplexity of a trained language model evaluated on the generations as a rough proxy for fluency. Specifically, we use the Gated Convolutional Language Model (Dauphin et al., 2016) pretrained on WikiText-1035 . Model BERT (large) BERT (base) GPT WT103 TBC Corpus-BLEU (↑) WT103 TBC 5.05 7.80 10.81 17.48 10.05 7.6"
W19-2304,P18-1082,0,\N,Missing
W19-3620,D15-1166,0,\N,Missing
W19-3620,H05-1059,0,\N,Missing
W19-3620,J90-2002,0,\N,Missing
W19-3620,P15-1150,0,\N,Missing
W19-3620,P07-2045,0,\N,Missing
W19-3620,W07-0734,0,\N,Missing
W19-3620,D10-1092,0,\N,Missing
W19-3620,W14-4012,1,\N,Missing
W19-3620,D14-1162,0,\N,Missing
W19-3620,N16-1024,0,\N,Missing
W19-3620,C12-1154,0,\N,Missing
W19-3620,P17-2021,0,\N,Missing
W19-3620,D18-1324,0,\N,Missing
W19-3620,P16-1162,0,\N,Missing
W19-3620,N16-1035,0,\N,Missing
W19-3620,N10-1115,0,\N,Missing
W19-8609,P17-4012,0,0.0352946,"tice to use the score s(h) used during the search to select the final sequence, but it is an open question whether there are better selection strategies for choosing between these final candidate responses. Beam search Instead of maintaining a single hypothesis at a time, as in greedy search above, at time step t beam search maintains a set of K hypotheses Ht : Ht = {(y11 , . . . , yt1 ), . . . , (y1K , . . . , ytK )}. (3) Avoiding repeating n-grams Although this has not been reported in a formal publication in the context of neural dialogue modeling to our knowledge, Paulus et al. (2017) and Klein et al. (2017) implement so-called n-gram blocking. In n-gram blocking, a hypothesis in a beam Ht is discarded if there is an n-gram that appears more than once within it. Each hypothesis hiyi , i ∈ {1, . . . , K} from Ht is t expanded with all possible next tokens v from the vocabulary V to form candidate hypotheses. Each candidate is in the form of ˜ i = hi k(v) = (y i , . . . , y i , v), h v yt 1 t (4) and is assigned a score: ˜ i ) = s(hi i ) + log p(v|y i ). s(h v ≤t y t 3 (5) We now propose an improved search strategy. To address the locality issue in beam search, we propose an iterative beam search t"
W19-8609,D18-1035,1,0.837739,"aximum length T . It is thus necessary to resort to approximate search algorithms. Greedy search Greedy search has been the search algorithm of choice among the recent papers on neural dialogue modeling (Gu et al., 2018; Zhao et al., 2017; Xu et al., 2017; Weston et al., 2018; Zhang et al., 2018). It moves from left to right selecting one token at a time, simply choosing the most likely token at the current time step: s s¯ yˆt = arg max log p(yt = v|ˆ y&lt;t , Y&lt;l , Y&lt;l , U ). v∈V Greedy search has been found significantly suboptimal within the field of machine translation (see, e.g., Table 1 in Chen et al., 2018), where similar neural sequence models are frequently used. Final sequence selection We consider search strategies to produce a set of candidate responses for the model to choose from. While greedy search provides only a single possible sequence, beam search generates a candidate set of size |M|. It is usual practice to use the score s(h) used during the search to select the final sequence, but it is an open question whether there are better selection strategies for choosing between these final candidate responses. Beam search Instead of maintaining a single hypothesis at a time, as in greedy"
W19-8609,D17-1230,0,0.0326471,"hese two learning algorithms, variational lower-bound maximization and adversarial learning, have been combined into a single model by Shen et al. (2018), which has been followed by Gu et al. (2018). Despite abundant endeavors on modeling and learning, search has received only a little attention (Dinan et al., 2019). Most of the work on search has focused on training an additional neural network that provides a supplementary score to guide either greedy or beam search. Li et al. (2015) propose a maximum mutual information criterion for decoding using a reverse model. This has been extended by Li et al. (2017a), where an extra neural network is trained to predict an arbitrary reward given a partial hypothesis and used during decoding. Similarly, Zemlyanskiy and Sha (2018) train a neural network that predicts the other participant’s personality given a partial conversation and use its predictability as an auxiliary score for re-ranking a set of candidate responses. None of these approaches study how the choice of the underlying search algorithm, rather than its scoring function, affects the quality of the neural dialogue model. In this paper, we investigate the effects of varying search and selecti"
W19-8609,D16-1230,0,0.0376594,"se the subscript 0 to indicate that beam search has been done without any other constraint. Re-running beam search with an increased beam width K would result in the search space that overlaps significantly with S0 , and would not give us Beam search terminates when |∪tt0 =1 Mt |≥ where K 0 is the maximum number of candidate sequences to be returned, or when t ≥ Lmax , where Lmax is the maximum length allowed for each candidate sequence. When terminated, beam K 0, 78 a set of (often human generated) reference responses and compare a single generated response against them (Serban et al., 2015; Liu et al., 2016). There are several methods for this comparison: (1) measure the perplexity of reference responses using the neural dialogue model, (2) compute a string match-based metric of a generated response against reference responses, and (3) use human annotators to compare model generated responses against reference or other models’ responses. None of these approaches capture the effectiveness of a neural sequence model in conducting a full conversation, because the model responses are computed given a human-written context, i.e., it does not see its own responses in the dialogue history, but gold resp"
W19-8609,D15-1166,0,0.157793,"Missing"
W19-8609,D18-1298,0,0.0292964,"Missing"
W19-8609,D17-2014,1,0.832109,"rning maximizes the log-probabilities of all the conversations in the training set: 1 X L= log p(C), (2) |D| C∈D often done using stochastic gradient descent with backpropagation (Rumelhart et al., 1985). Neural dialogue modeling Since Vinyals and Le (2015), a neural autoregressive sequence model based on sequence-tosequence models Sutskever et al. (2014); Cho et al. (2014) have become one of the most widely studied approaches to dialogue modeling (see, e.g., Serban et al., 2016, 2017; Zhao et al., 2017; Xu et al., 2017; Li et al., 2016, 2017a,b; Zemlyanskiy and Sha, 2018; Zhang et al., 2018; Miller et al., 2017; Shen et al., 2018; Gu et al., 2018). In this approach, a neural sequence model is used to model 2.2 Inference (generation) In this paper, we generate a response to the current state of the conversation (but do not attempt to plan ahead to future exchanges), maximizing s s¯ p(Y |Y&lt;l , Y&lt;l , U) = T Y s s¯ log p(yt |y&lt;t , Y&lt;l , Y&lt;l , U ). t=1 Unfortunately, it is intractable to solve this problem due to the exponentially-growing space of all 77 search returns all the candidate sequences in M = ∪tt0 =1 Mt . One can increase the size of the subspace over which beam search searches for a response"
W19-8609,W18-5713,1,0.926484,"e problem that most of the hypotheses discovered in M are near each other in the response space (Li et al., 2016, 2015). For tasks such as dialogue modeling, which are much more open-ended than e.g. machine translation, this is particularly troublesome as many high quality responses may be missing in the beam. possible responses w.r.t. the maximum length T . It is thus necessary to resort to approximate search algorithms. Greedy search Greedy search has been the search algorithm of choice among the recent papers on neural dialogue modeling (Gu et al., 2018; Zhao et al., 2017; Xu et al., 2017; Weston et al., 2018; Zhang et al., 2018). It moves from left to right selecting one token at a time, simply choosing the most likely token at the current time step: s s¯ yˆt = arg max log p(yt = v|ˆ y&lt;t , Y&lt;l , Y&lt;l , U ). v∈V Greedy search has been found significantly suboptimal within the field of machine translation (see, e.g., Table 1 in Chen et al., 2018), where similar neural sequence models are frequently used. Final sequence selection We consider search strategies to produce a set of candidate responses for the model to choose from. While greedy search provides only a single possible sequence, beam search"
W19-8609,D14-1162,0,0.0807947,"Missing"
W19-8609,1983.tc-1.13,0,0.174556,"Missing"
W19-8609,D17-1065,0,0.111631,"og p(Yls |Y&lt;l , Y≤l , U ), s∈{a,b} l=1 (1) where s¯ = a if s = b and otherwise s¯ = b. Learning maximizes the log-probabilities of all the conversations in the training set: 1 X L= log p(C), (2) |D| C∈D often done using stochastic gradient descent with backpropagation (Rumelhart et al., 1985). Neural dialogue modeling Since Vinyals and Le (2015), a neural autoregressive sequence model based on sequence-tosequence models Sutskever et al. (2014); Cho et al. (2014) have become one of the most widely studied approaches to dialogue modeling (see, e.g., Serban et al., 2016, 2017; Zhao et al., 2017; Xu et al., 2017; Li et al., 2016, 2017a,b; Zemlyanskiy and Sha, 2018; Zhang et al., 2018; Miller et al., 2017; Shen et al., 2018; Gu et al., 2018). In this approach, a neural sequence model is used to model 2.2 Inference (generation) In this paper, we generate a response to the current state of the conversation (but do not attempt to plan ahead to future exchanges), maximizing s s¯ p(Y |Y&lt;l , Y&lt;l , U) = T Y s s¯ log p(yt |y&lt;t , Y&lt;l , Y&lt;l , U ). t=1 Unfortunately, it is intractable to solve this problem due to the exponentially-growing space of all 77 search returns all the candidate sequences in M = ∪tt0 =1"
W19-8609,W18-3022,0,0.0576322,"Missing"
W19-8609,K18-1053,0,0.120184,"which has been followed by Gu et al. (2018). Despite abundant endeavors on modeling and learning, search has received only a little attention (Dinan et al., 2019). Most of the work on search has focused on training an additional neural network that provides a supplementary score to guide either greedy or beam search. Li et al. (2015) propose a maximum mutual information criterion for decoding using a reverse model. This has been extended by Li et al. (2017a), where an extra neural network is trained to predict an arbitrary reward given a partial hypothesis and used during decoding. Similarly, Zemlyanskiy and Sha (2018) train a neural network that predicts the other participant’s personality given a partial conversation and use its predictability as an auxiliary score for re-ranking a set of candidate responses. None of these approaches study how the choice of the underlying search algorithm, rather than its scoring function, affects the quality of the neural dialogue model. In this paper, we investigate the effects of varying search and selection strategies on the quality of generated dialogue utterances. We start with an attention-based sequence-tosequence model (Bahdanau et al., 2014) trained on the recen"
W19-8609,P18-1205,1,0.531282,"cts the other participant’s personality given a partial conversation and use its predictability as an auxiliary score for re-ranking a set of candidate responses. None of these approaches study how the choice of the underlying search algorithm, rather than its scoring function, affects the quality of the neural dialogue model. In this paper, we investigate the effects of varying search and selection strategies on the quality of generated dialogue utterances. We start with an attention-based sequence-tosequence model (Bahdanau et al., 2014) trained on the recently-released PersonaChat dataset (Zhang et al., 2018). We evaluate three search algorithms: greedy search, beam search and iterative beam search, the last of which we design based on earWe investigate the impact of search strategies in neural dialogue modeling. We first compare two standard search algorithms, greedy and beam search, as well as our newly proposed iterative beam search which produces a more diverse set of candidate responses. We evaluate these strategies in realistic full conversations with humans and propose a modelbased Bayesian calibration to address annotator bias. These conversations are analyzed using two automatic metrics:"
W19-8609,P17-1061,0,0.0904272,"p(C) = L X X s s¯ log p(Yls |Y&lt;l , Y≤l , U ), s∈{a,b} l=1 (1) where s¯ = a if s = b and otherwise s¯ = b. Learning maximizes the log-probabilities of all the conversations in the training set: 1 X L= log p(C), (2) |D| C∈D often done using stochastic gradient descent with backpropagation (Rumelhart et al., 1985). Neural dialogue modeling Since Vinyals and Le (2015), a neural autoregressive sequence model based on sequence-tosequence models Sutskever et al. (2014); Cho et al. (2014) have become one of the most widely studied approaches to dialogue modeling (see, e.g., Serban et al., 2016, 2017; Zhao et al., 2017; Xu et al., 2017; Li et al., 2016, 2017a,b; Zemlyanskiy and Sha, 2018; Zhang et al., 2018; Miller et al., 2017; Shen et al., 2018; Gu et al., 2018). In this approach, a neural sequence model is used to model 2.2 Inference (generation) In this paper, we generate a response to the current state of the conversation (but do not attempt to plan ahead to future exchanges), maximizing s s¯ p(Y |Y&lt;l , Y&lt;l , U) = T Y s s¯ log p(yt |y&lt;t , Y&lt;l , Y&lt;l , U ). t=1 Unfortunately, it is intractable to solve this problem due to the exponentially-growing space of all 77 search returns all the candidate sequence"
