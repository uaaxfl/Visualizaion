2016.gwc-1.59,W04-2214,0,0.0601204,"adhere to some version of the Princeton WordNet also means that the fund of concepts is biased towards an AngloSaxon worldview and is not open to concepts from other languages and cultures. It could be argued that these problems would go away if a single multilingual database was developed instead. This would, in theory, solve problems of incompatible formats and coordination. In practice, however there is no single group that has expertise in all the world’s languages. Further, much experimentation is done in the different projects; adding new relations (Vossen, 1998), adding richer domains (Bentivogli et al., 2004), adding new parts-of-speech (Seah and Bond, 2014) and so forth. This would be harder to do in one monolithic project. As time passes, and PWN now celebrates its 25th anniversary, the need for implementing the GWG becomes more urgent. The GWG should be a platform for achieving linguistic and conceptual interoperability across wordnets and all related machinery. It should allow researchers to study the universals and idiosyncracies in lexicalisation across languages, to address fundamental questions about what is a word and what is a concept (Fellbaum and Vossen, 2010; Vossen and Fellbaum, 2011"
2016.gwc-1.59,2016.gwc-1.9,1,0.711705,"ike license (CC BY SA). In order to keep compatibility across the grid, any projects in the Global Wordnet Grid must have a license compatible with CC BY SA (such as the original wordnet license, CC BY, MIT and many others), and the entire grid will be released under this license. The individual projects, starting with PWN, are the foundations upon which the GWG is built, the CILI links them and the platform ties them together, allows for versioning and adaptation through the community. 4 The Collaborative ILI (CILI) The Collaborative ILI is an extension of the ILI defined in EuroWordNet (see Bond et al., 2016, for more details). As a base for the CILI we take the synsets currently in Princeton Wordnet 3.0, the de facto ILI for the Open Multilingual Wordnet. This shows its central position in the current wordnet community. Each synset in PWN 3.0 gives rise to a concept in the CILI. The CILI is just a collection of concepts to which all wordnets are linked. It does not duplicate the relations between these concepts as represented in any wordnet and it does not have any lexicalizations. Concepts and concept identifiers in the CILI are permanent. They will never be removed or changed. However, new con"
2016.gwc-1.59,P15-4013,1,0.881799,"Missing"
2016.gwc-1.59,2016.gwc-1.43,1,0.805029,"Missing"
2016.gwc-1.59,Q14-1018,0,0.021166,"Missing"
2016.gwc-1.59,vossen-etal-2014-newsreader,1,0.789611,"Missing"
2018.gwc-1.10,W14-0101,0,0.0195947,"anguages to form the IndoWordNet. Mohanty et al. (2017) built SentiWordNet for the Odia language, which is one of the official languages of India. Being an under-resourced language, Odia lacks proper machine translation system to translate the vocabulary of the available resource from English into Odia. The authors have created SentiWordNet for Odia using resources of other Indian languages and the IndoWordNet. Although the IndoWordNet structure does not map directly to the SentiWordNet, instead synsets are matched. The authors used these for translation from source lexicon to target lexicon. Aliabadi et al. (2014) have created a wordnet for the Kurdish language, one of the under-resourced languages in western Iranian language family. They have created Kurdish translation for the “core” wordnet synsets (Vossen, 1997), which is a set of 5,000 essential concepts. They used a dictionary to translate its literals (words), adopted an indirect evaluation alternative in which they look at the effectiveness of using KurdNet for rewriting Information Retrieval queries. Similarly, the work by Horv´ath et al. (2016) focuses on the semiautomatic construction of wordnet for the Mansi language, which is spoken by Man"
2018.gwc-1.10,C16-1010,1,0.934104,"xt from scratch, we use the available parallel corpora from multiple sources, like OPUS,2 to create a machine translation system to translate the wordnet senses in the Princeton WordNet into the mentioned under-resourced languages. Translation tools such as Google Translate,3 or open source SMT systems such as Moses (Koehn et 1 http://globalwordnet.org/ http://opus.lingfil.uu.se/ 3 http://translate.google.com/ 2 al., 2007) trained on generic data are the most common solutions, but they often result in unsatisfactory translations of domain-specific expressions. Therefore, we follow the idea of Arcan et al. (2016b), where the authors automatically identify relevant sentences in English containing the WordNet senses and translate them within the context, which showed translation quality improvement of the targeted entries. The effectiveness of our approach is evaluated by comparing the generated translations with the IndoWordNet entries, automatically and manually, respectively. This paper reports our first outcomes in improving wordnet for under-resourced Dravidian languages such as Tamil(ISO 639-2: tam), Telugu (ISO 639-2: tel) and Kannada (ISO 639-2: kan). 2 Related work Scannell (2007) describes th"
2018.gwc-1.10,W14-3902,0,0.0477905,"ll corpora used for training lead to incomplete word coverage, which may cause the out-of-vocabulary (OOV) issues. Besides the resource scarceness, another issue observed with the corpus for Dravidian languages was code-switching contents in the data. Codeswitching is an act of alternating between elements of two or more languages, which is prevalent in English→Tamil English→Telugu English→Kannada Original Non-Code mixing 20.29 28.81 14.64 20.61 28.25 14.45 Table 2: Automatic translation evaluation of the of 1000 randomly selected sentences in terms of the BLEU metric. multilingual countries (Barman et al., 2014). With English being the most used language in the digital world, people tend to mix English words with their native languages. That might be the case in other languages as well. 4 Methodology The principle approaches for constructing wordnets are the merge approach or the expand approach. In the merge approach, the synsets and relations are built independently and then aligned with WordNet. The drawbacks of the merge approach are that it is time-consuming and requires a lot of manual effort to build. On the contrary in the expand model, wordnet can be created automatically by translating syns"
2018.gwc-1.10,eisele-chen-2010-multiun,0,0.0245692,"languages than English were used to select the most relevant sentences for wordnet senses from a large set of generic parallel corpora. The goal is to identify sentences that share the same semantic information in respect to the synset of the WordNet entry that we want to translate. To ensure a broad lexical and domain coverage of English sentences, existing parallel corpora for various language pairs were merged into one parallel data set, i.e., Europarl (Koehn, 2005), DGT - translation memories generated by the Directorate-General for Translation (Steinberger et al., 2014), MultiUN corpus (Eisele and Chen, 2010), EMEA, KDE4, OpenOffice (Tiedemann, 2009), OpenSubtitles2012 (Tiedemann, 2012). Similarly, wordnets in a variety of languages, provided by the Open Multilingual Wordnet web page,9 were used. As a motivating example, we consider the word vessel, which is a member of three synsets in Princeton WordNet, whereby the most frequent translation, e.g., as given by Google Translate, is Schiff in German and nave in Italian, corresponding to i6083310 ‘a craft designed for water transportation’. For the second sense, i65336 ‘a tube in which a body fluid circulates’, we assume that we know the German tran"
2018.gwc-1.10,2012.amta-papers.22,0,0.0188799,"e followed for different languages. The IndoWordNet (Bhattacharyya, 2010) was compiled for eighteen out of the twenty-two official languages of India and made available for public use. It is based on the expand approach like EuroWordNet, but from the Hindi wordnet, which is then linked to English. On the Global WordNet Association website,1 a comprehensive list of wordnets available for different languages can be found, including IndoWordNet and EuroWordNet etc. This paper describes the effort towards generating and improving wordnets for the underresourced Dravidian languages. Since studies (Federico et al., 2012; L¨aubli et al., 2013; Green et al., 2013) have shown significant productivity gains when human translators post-edit machine translation output rather than translating text from scratch, we use the available parallel corpora from multiple sources, like OPUS,2 to create a machine translation system to translate the wordnet senses in the Princeton WordNet into the mentioned under-resourced languages. Translation tools such as Google Translate,3 or open source SMT systems such as Moses (Koehn et 1 http://globalwordnet.org/ http://opus.lingfil.uu.se/ 3 http://translate.google.com/ 2 al., 2007) t"
2018.gwc-1.10,W11-2123,0,0.0208039,"anslating synsets using different strategies, whereby the synsets are built in correspondence with the existing wordnet synsets. We followed the expand approach and created a machine translation systems to translate the sentences, which contained the WordNet senses in English to the target language 4.1 Training Machine Translation parameters In the following section, we takes as a baseline a parallel text, that has been aligned at the sentence level. To obtain the translations, we use Moses SMT toolkit with of baseline setup with 5-gram language model created using the training data by KenLM (Heafield, 2011). The baseline SMT system was built for three language pairs, English-Tamil, English-Telugu, and English-Kannada. The test set mentioned in Section 3.3 was used to evaluate our system. From Table 1 and Table 2 we can see that size of the parallel corpus has an impact on the BLEU score for test set which is evaluation criteria for the translation model. 4.2 Context Identification Since manual translation of wordnets using the extend approach is a very time consuming and expensive process, we apply SMT to automatically translate WordNet entries into the targeted Dravidian languages. While an dom"
2018.gwc-1.10,2016.gwc-1.20,0,0.102139,"Missing"
2018.gwc-1.10,P07-2045,0,0.00600865,"aximizing separately a language model p(t) and the inverse translation model p(s|t). A language model assigns a probability p(t) for any sentence t and translation model assigns a conditional probability p(s|t) to source / target pair of sentence. By Bayes rule p(t|s) ∝ p(t)p(s|t) (1) This decomposition into a translation and a language model improves the fluency of generated texts by making full use of available corpora. The language model is not only meant to ensure a fluent output, but also supports difficult decisions about word order and word translation (Koehn, 2010). We used the Moses (Koehn et al., 2007) toolkit that provides end-to-end support for the creation and evaluation of machine translation system based on BLEU (Papineni et al., 2002) score. There are two major criteria for automatic SMT evaluation: completeness and correctness, which are considered by BLEU, an automatic evaluation technique, which is a geometric mean of n-gram precision. BLEU score is language independent, fast, and shows good correlation with human evaluation campaigns. Therefore we plan to use this metric to evaluate our work. 3.3 Available Corpora for Machine Translation This section describes the data collection"
2018.gwc-1.10,2005.mtsummit-papers.11,0,0.0150759,"disambiguated context of a sentence (Arcan et al., 2016a; Arcan et al., 2016b). Therefore existing translations of WordNet senses in other languages than English were used to select the most relevant sentences for wordnet senses from a large set of generic parallel corpora. The goal is to identify sentences that share the same semantic information in respect to the synset of the WordNet entry that we want to translate. To ensure a broad lexical and domain coverage of English sentences, existing parallel corpora for various language pairs were merged into one parallel data set, i.e., Europarl (Koehn, 2005), DGT - translation memories generated by the Directorate-General for Translation (Steinberger et al., 2014), MultiUN corpus (Eisele and Chen, 2010), EMEA, KDE4, OpenOffice (Tiedemann, 2009), OpenSubtitles2012 (Tiedemann, 2012). Similarly, wordnets in a variety of languages, provided by the Open Multilingual Wordnet web page,9 were used. As a motivating example, we consider the word vessel, which is a member of three synsets in Princeton WordNet, whereby the most frequent translation, e.g., as given by Google Translate, is Schiff in German and nave in Italian, corresponding to i6083310 ‘a craf"
2018.gwc-1.10,2016.gwc-1.9,1,0.878429,"Missing"
2018.gwc-1.10,J10-4005,0,0.0238948,"t given a source sentence s by maximizing separately a language model p(t) and the inverse translation model p(s|t). A language model assigns a probability p(t) for any sentence t and translation model assigns a conditional probability p(s|t) to source / target pair of sentence. By Bayes rule p(t|s) ∝ p(t)p(s|t) (1) This decomposition into a translation and a language model improves the fluency of generated texts by making full use of available corpora. The language model is not only meant to ensure a fluent output, but also supports difficult decisions about word order and word translation (Koehn, 2010). We used the Moses (Koehn et al., 2007) toolkit that provides end-to-end support for the creation and evaluation of machine translation system based on BLEU (Papineni et al., 2002) score. There are two major criteria for automatic SMT evaluation: completeness and correctness, which are considered by BLEU, an automatic evaluation technique, which is a geometric mean of n-gram precision. BLEU score is language independent, fast, and shows good correlation with human evaluation campaigns. Therefore we plan to use this metric to evaluate our work. 3.3 Available Corpora for Machine Translation Thi"
2018.gwc-1.10,W16-5814,0,0.0573178,"languages with in the same sentence or between sentences. In many bilingual or multilingual communities like India, Hong Kong, Malaysia or Singapore, language interaction often happens in which two or more languages are mixed. Furthermore, it increasingly occurs in monolingual cultures due to globalization. In many contexts and domains, English is mixed with native languages within their utterance than in the past due to Internet boom. Due to the history and popularity of the English language, on the Internet Indian languages are more frequently mixed with English than other native languages (Chanda et al., 2016). A major part of our corpora comes from movie subtitles and technical documents, which makes it even more prone to code-mixing of English in the Dravidian languages. In our corpus, movie speeches are transcribed to text and they differ from that in other written genres: the vocabulary is informal, non-linguistics sounds like ah, and mixing of scripts in case of English and native languages (Tiedemann, 2008). Two example of codeswitching are demonstrated in Figure 1.The parallel corpus is initially segregated into English script and native script. All of the annotations are done using an autom"
2018.gwc-1.10,2013.mtsummit-wptp.10,0,0.0213978,"Missing"
2018.gwc-1.10,W17-5219,0,0.0211052,". The IndoWordNet entries are updated frequently. For the Tamil language, Rajendran et al. (2002) proposed a design template for the Tamil wordnet. 4 http://www.cfilt.iitb.ac.in/ indowordnet/index.jsp In their further work (Rajendran et al., 2010), they emphasize the need for an independent wordnet for the Dravidian languages, based on EuroWordNet. This is due the observation that the morphology and lexical concepts of these languages are different compared to other Indian languages. The authors have combined the Tamil wordnet and wordnets in other Dravidian languages to form the IndoWordNet. Mohanty et al. (2017) built SentiWordNet for the Odia language, which is one of the official languages of India. Being an under-resourced language, Odia lacks proper machine translation system to translate the vocabulary of the available resource from English into Odia. The authors have created SentiWordNet for Odia using resources of other Indian languages and the IndoWordNet. Although the IndoWordNet structure does not map directly to the SentiWordNet, instead synsets are matched. The authors used these for translation from source lexicon to target lexicon. Aliabadi et al. (2014) have created a wordnet for the K"
2018.gwc-1.10,P02-1040,0,0.102499,"entence t and translation model assigns a conditional probability p(s|t) to source / target pair of sentence. By Bayes rule p(t|s) ∝ p(t)p(s|t) (1) This decomposition into a translation and a language model improves the fluency of generated texts by making full use of available corpora. The language model is not only meant to ensure a fluent output, but also supports difficult decisions about word order and word translation (Koehn, 2010). We used the Moses (Koehn et al., 2007) toolkit that provides end-to-end support for the creation and evaluation of machine translation system based on BLEU (Papineni et al., 2002) score. There are two major criteria for automatic SMT evaluation: completeness and correctness, which are considered by BLEU, an automatic evaluation technique, which is a geometric mean of n-gram precision. BLEU score is language independent, fast, and shows good correlation with human evaluation campaigns. Therefore we plan to use this metric to evaluate our work. 3.3 Available Corpora for Machine Translation This section describes the data collection and the pre-processing process steps. The English-Tamil parallel corpus, which we used to train our SMT system is collected from various sour"
2018.gwc-1.10,W12-3152,0,0.0252879,".8 13,543 Table 1: Statistics of the parallel corpora used to train the translation systems. contains text from the news domain,5 sentences from the Tamil cinema articles6 and the Bible.7 For the news corpus, the authors downloaded web pages that have matching file names in both English and Tamil. For the cinema corpus, all the English articles had a link to the corresponding Tamil translation. The collection of the Bible corpus followed a similar pattern. We also took the English-Tamil parallel corpora for six Indian languages created with the help of Mechanical Turk for Wikipedia documents (Post et al., 2012). Since the data was created by non-expert translators hired over the Mechanical Turk, it is of mixed quality. From the OPUS website, we have collected the Gnome, KDE, Ubuntu and movie subtitles (Tiedemann, 2012). We furthermore manually aligned Tamil text Tirukkural,8 and combined all the parallel corpora into a single corpus. We first tokenized sentences in English and Tamil and then true-cased only the English side of the parallel corpus, since the Tamil language does not have a casing. Finally, we cleaned up the data by eliminating the sentences whose length is above 80 words. To obtain th"
2018.gwc-1.10,W12-5611,0,0.0802512,"Missing"
2018.gwc-1.10,tiedemann-2008-synchronizing,0,0.0423089,"st due to Internet boom. Due to the history and popularity of the English language, on the Internet Indian languages are more frequently mixed with English than other native languages (Chanda et al., 2016). A major part of our corpora comes from movie subtitles and technical documents, which makes it even more prone to code-mixing of English in the Dravidian languages. In our corpus, movie speeches are transcribed to text and they differ from that in other written genres: the vocabulary is informal, non-linguistics sounds like ah, and mixing of scripts in case of English and native languages (Tiedemann, 2008). Two example of codeswitching are demonstrated in Figure 1.The parallel corpus is initially segregated into English script and native script. All of the annotations are done using an automatic process. All words from a language other than the native script of our experiment are taken out on both sides of corpus if it occurs in native language side of the parallel corpus. The sentences are removed from both sides if the target language side does not contain native script words in it. Table 3 show the percentage of code-mixed text removed from original corpus. The goal of this approach is to in"
2018.gwc-1.10,tiedemann-2012-parallel,0,0.18819,"uthors downloaded web pages that have matching file names in both English and Tamil. For the cinema corpus, all the English articles had a link to the corresponding Tamil translation. The collection of the Bible corpus followed a similar pattern. We also took the English-Tamil parallel corpora for six Indian languages created with the help of Mechanical Turk for Wikipedia documents (Post et al., 2012). Since the data was created by non-expert translators hired over the Mechanical Turk, it is of mixed quality. From the OPUS website, we have collected the Gnome, KDE, Ubuntu and movie subtitles (Tiedemann, 2012). We furthermore manually aligned Tamil text Tirukkural,8 and combined all the parallel corpora into a single corpus. We first tokenized sentences in English and Tamil and then true-cased only the English side of the parallel corpus, since the Tamil language does not have a casing. Finally, we cleaned up the data by eliminating the sentences whose length is above 80 words. To obtain the parallel corpora for Telugu and Kannada, we used the corpora available on the OPUS website. The same pre-processing procedure was followed for Telugu and Kannada language, since both languages are close to the"
2018.gwc-1.10,W17-2911,0,0.0429524,"Missing"
2018.gwc-1.40,2016.gwc-1.9,1,0.86482,"Missing"
2018.gwc-1.40,bel-etal-2000-simple,0,0.161384,"Missing"
2018.gwc-1.40,2018.gwc-1.21,1,0.813552,"Missing"
2018.gwc-1.51,W15-4319,0,0.0322998,"Missing"
2018.gwc-1.51,2016.gwc-1.9,1,0.89092,"Missing"
2018.gwc-1.51,L16-1686,0,0.0336037,"Missing"
2018.gwc-1.51,2016.gwc-1.19,1,0.829233,"Missing"
2018.gwc-1.8,E09-1005,0,0.0682276,"Missing"
2018.gwc-1.8,P98-1013,0,0.372483,"Missing"
2018.gwc-1.8,2016.gwc-1.9,1,0.845867,"Missing"
2018.gwc-1.8,W13-5503,0,0.184119,"Missing"
2018.gwc-1.8,fernando-stevenson-2012-mapping,0,0.0535991,"Missing"
2018.gwc-1.8,E12-1059,0,0.688244,"Missing"
2018.gwc-1.8,L16-1386,1,0.894238,"Missing"
2018.gwc-1.8,W11-0122,0,0.228957,"Missing"
2018.gwc-1.8,I11-1099,0,0.517072,"Missing"
2018.gwc-1.8,W00-1104,0,0.015752,"Missing"
2019.gwc-1.31,L16-1686,0,0.0607604,"Missing"
2019.gwc-1.31,D16-1041,0,0.0173731,"kipedia and this may require the project to adopt the more restrictive CCBY-SA license of Wikipedia. Moreover, it is not Figure 1: Screenshot of the new English WordNet interface clear how many of the entries have been reviewed by native speakers of English. Finally, a long term goal would be to introduce a principled method for introducing new synsets, which are of high quality and this would have to involve reviewing of all the links between synsets that have been introduced. It is expected that this could be achieved by a semi-automatic procedure where potential links are learnt from text (Espinosa-Anke et al., 2016) combined with a crowd-sourced reviews. Another important aspect of each synset is also its definition and as many of the definitions in WordNet are of poor quality (McCrae and Prangnawarat, 2016), it is necessary to adopt some general guidelines for writing definitions that can ensure high quality, such as those defined for ontological definitions (Sepp¨al¨a et al., 2017). Further, we will implement and further extend the validations that are available and automate the checking such that it is clear if any changes are breaking issues. In particular, we currently implement simple DTD validatio"
2019.gwc-1.31,francopoulo-etal-2006-lexical,0,0.0760954,"y of the minor errors into their own resources. 3 The Open English WordNet Project The Open English WordNet Project2 takes the form of a single Git repository, published on GitHub, and consisting for the most part of a collection of XML files describing the synsets and lexical entries in the resource. These XML files represent each of the lexicographer file sections of the original resource and a simple script is provided to stitch them together into a single XML file. The XML files are compliant with the GWC LMF model (McCrae et al., 2019)3 , which is itself based partially on the LMF model (Francopoulo et al., 2006) and in particular the WordNet (a.k.a 2 https://github.com/globalwordnet/ english-wordnet 3 https://globalwordnet.github.io/ schemas/ Kyoto) LMF variant (Soria et al., 2009). Due to its basis on LMF, a particular challenge was that the entire wordnet should be represented as a single XML document. However, due to the relative verbosity of the LMF format, the final data ended up as 97 MB, exceeding the upload limits of GitHub, so instead the single XML file was divided by lexicographer sections. Even still, this creates several very large files (over 10 MB) and this has resulted in some challen"
2019.gwc-1.31,E12-1059,0,0.150764,"Missing"
2019.gwc-1.31,N18-1014,0,0.0137142,"ordNet 2019, which has been developed by multiple people around the world through GitHub, fixes many errors in previous wordnets for English. We give some details of the changes that have been made in this version and give some perspectives about likely future changes that will be made as this project continues to evolve. 1 Introduction WordNet (Miller, 1995; Fellbaum, 1998) is one of the most widely-used language resources in natural language processing and continues to find usage in a wide variety of applications including sentiment analysis (Wang et al., 2018), natural language generation (Juraska et al., 2018) and textual entailment (Silva et al., 2018). However, in the recent few years there has been only one update since version 3.0 was released in 2006, in spite of its wide use and the interest in the data. In the meantime, a number of other wordnet teams working with the WordNet data have proposed modifications or extensions to its latest release. These two facts have provided the chief motivation for our present initiative, namely developing an opensource WordNet for English on the basis of Princeton WordNet (to be released under the name English WordNet 2019). In order to allow for meaningful"
2019.gwc-1.31,C16-1213,1,0.92792,"ton WordNet (Miller, 1995; Fellbaum, 2010) is the first wordnet for English, however it is not the only one that has been developed for this language. Moreover, it has been the case that during the development of several wordnets for other languages signficant changes and/or additions were made to the underlying structure and content of the English section of the wordnet. In at least one case, namely the development of the Polish wordnet, plWordNet, the additions to the underlying English wordnet have been so numerous that they were released as a new wordnet, enWordnet (Rudnicka et al., 2015; Maziarz et al., 2016). These involved the addition of new lemmas (over 11k), lexical units (over 11k) and synsets (7.5k). The latter were linked to WordNet 3.1 synsets via hyponymy relation. Still, no alterations to the original WordNet synsets or relations were made within this project. Currently, enWordnet is only available as part of the plWordNet project and does not constitute a ‘drop-in’ replacement for Princeton WordNet. Some projects have attempted to expand Princeton WordNet with new terminology in other directions, for example the Colloquial WordNet project (McCrae et al., 2017), has been working on addi"
2019.gwc-1.31,2018.gwc-1.8,1,0.739086,"r by providing a suitable modification, for example the example of ‘double negative’ was ‘I don’t never go’ and was updated to ‘double negative such as ‘I don’t never go” to include the lemma. 6 7 corrected to ‘certain’ https://www.sketchengine.eu/ • An issue was logged, as it was identified that this example shows a more signficant change. This was often the case when the example used a lemma or a hypernym and it was not clear if the distinction between synsets was meaningful. A third major change was to introduce new synset members based on a previously calculated WordNet-Wikipedia mapping (McCrae, 2018). In particular, if this mapping, which has already been manually verified, linked to a page title that did not match the lemma, the page title was added as a new lemma to the synset. This was, as with all changes, manually verified in its entirety before the change was made. Finally, the repository has been open to new suggestions of changes and there have been many suggestions already contributed about sporadic and various changes to the wordnet. A sample of these include: • The sense of ‘threepenny’ as a size was incorrect in the actual length in inches of a threepenny. • Grammatical errors"
2019.gwc-1.31,C12-3044,1,0.889346,"Missing"
2019.gwc-1.31,2018.gwc-1.40,1,0.702996,"ent and further extend the validations that are available and automate the checking such that it is clear if any changes are breaking issues. In particular, we currently implement simple DTD validation of the merged XML, which also catches many other issues, such as senses without synsets, but we are working to extend this validation to include issues, such as hypernyms without hyponyms, etc. In order to achieve this, it is important that strong tools are available for the creation and maintenance of the resource and it is likely that tools coming out of the ELEXIS project (Krek et al., 2018; Pedersen et al., 2018) will be adapted to this task. 6 Results for this release This release represents a mostly maintenance release where obvious errors have been fixed. In Table 2 we see that most of the updates are to the definitions and examples used to describe the synsets in English WordNet. There have also been a number of removals relative to the previous version of Princeton WordNet: mispelled lemmas were removed and replaced with a correctly spelled variant and these were counted as both a removal and addition of a lemma. Secondly, due to an issue11 two links were removed as they were deemed clearly incor"
2019.gwc-1.31,strapparava-valitutti-2004-wordnet,0,0.125518,"Missing"
2020.cogalex-1.8,P14-1023,0,0.0230895,"rns. The later mentioned method has been explored in an unsupervised manner (Weeds and Weir, 2003); (Lenci and Benotto, 2012); (Santus et al., 2015) and didn’t measure up in terms of efficacy. Thereafter supervised methods have been adopted (Kruszewski et al., 2015); (Roller and Erk, 2016); (Nguyen et al., 2016); (Shwartz et al., 2016) in the very same direction aiming for classifying the multiclass relationships better. Count-based vectors have been substituting the prediction based ones in some recent approaches, which apparently performed better in some task, such as similarity estimation (Baroni et al., 2014), even though (Levy et al., 2015) demonstrated that these improvements were most likely due to the optimization of hyper-parameters that were instead left unoptimized in count based models. (Shwartz et al., 2016) had an approach combining patterns and distributional information reflected promising parameters in hypernymy recognition. 2.2 Shared Task regarding Semantic Relations Identification Several shared tasks has been emerged from the NLP related conferences in this decade and the following covers a brief survey on such tasks. Seven “encyclopedic” semantic relations between nouns (cause-ef"
2020.cogalex-1.8,S15-2151,0,0.0200445,"Missing"
2020.cogalex-1.8,S16-1168,0,0.0226091,"Missing"
2020.cogalex-1.8,S07-1003,0,0.0721188,"Missing"
2020.cogalex-1.8,W09-2415,0,0.134469,"Missing"
2020.cogalex-1.8,Q15-1027,0,0.0182681,"were pattern based multiclass classification task carried out by (Turney, 2008) on similarity, antonymy and analogy, and by (Pantel and Pennacchiotti, 2006) on generic pattern recognition and filtering. These approaches resulted in higher precision and lower recall compared to distributionalsemantic-model-based methods due to their sole dependency on patterns. The later mentioned method has been explored in an unsupervised manner (Weeds and Weir, 2003); (Lenci and Benotto, 2012); (Santus et al., 2015) and didn’t measure up in terms of efficacy. Thereafter supervised methods have been adopted (Kruszewski et al., 2015); (Roller and Erk, 2016); (Nguyen et al., 2016); (Shwartz et al., 2016) in the very same direction aiming for classifying the multiclass relationships better. Count-based vectors have been substituting the prediction based ones in some recent approaches, which apparently performed better in some task, such as similarity estimation (Baroni et al., 2014), even though (Levy et al., 2015) demonstrated that these improvements were most likely due to the optimization of hyper-parameters that were instead left unoptimized in count based models. (Shwartz et al., 2016) had an approach combining pattern"
2020.cogalex-1.8,S12-1012,0,0.271986,", while Section 5 describes the results and Section 6 lays out the conclusion and future direction. 2 Related Work 2.1 Identifications of semantic relations Recognizing the semantic meaning of words in terms of connecting them with semantic relationships has become a key direction to grow knowledge base and many further practices in NLP. This connects to a wide range of applications, such as textual entailment, text summarization, sentiment analysis, ontology learning, and so on. Following this, several supervised and unsupervised approaches have been initiated and for reference the works of (Lenci and Benotto, 2012) and (Shwartz et al., 2016). (Mohammad et al., 2013) and (Santus et al., 2014) on antonymy are of relevance here. One key commonality amongst these were that these approaches targeted one semantic relationship discovery at once amongst the words rather multiple. There were pattern based multiclass classification task carried out by (Turney, 2008) on similarity, antonymy and analogy, and by (Pantel and Pennacchiotti, 2006) on generic pattern recognition and filtering. These approaches resulted in higher precision and lower recall compared to distributionalsemantic-model-based methods due to the"
2020.cogalex-1.8,N15-1098,0,0.0284777,"been explored in an unsupervised manner (Weeds and Weir, 2003); (Lenci and Benotto, 2012); (Santus et al., 2015) and didn’t measure up in terms of efficacy. Thereafter supervised methods have been adopted (Kruszewski et al., 2015); (Roller and Erk, 2016); (Nguyen et al., 2016); (Shwartz et al., 2016) in the very same direction aiming for classifying the multiclass relationships better. Count-based vectors have been substituting the prediction based ones in some recent approaches, which apparently performed better in some task, such as similarity estimation (Baroni et al., 2014), even though (Levy et al., 2015) demonstrated that these improvements were most likely due to the optimization of hyper-parameters that were instead left unoptimized in count based models. (Shwartz et al., 2016) had an approach combining patterns and distributional information reflected promising parameters in hypernymy recognition. 2.2 Shared Task regarding Semantic Relations Identification Several shared tasks has been emerged from the NLP related conferences in this decade and the following covers a brief survey on such tasks. Seven “encyclopedic” semantic relations between nouns (cause-effect, instrument-agency, product-"
2020.cogalex-1.8,J10-3003,0,0.0133096,"ords are analysed here which shows a balanced performance in overall characteristics with some scope for improvement. 1 Introduction Predicting the relationship between two words in terms of semantics has become a quintessential problem to be solved in the present day NLP world and reflect great impacts on the theoretical psycholinguistic modeling of the mental lexicon as well. The field of NLP finds many useful applications through tackling this direction, such as thesaurus generation (Grefenstette, 1994), ontology learning (Zouaq and Nkambou, 2008), paraphrase generation and identification (Madnani and Dorr, 2010), question answering and recognizing textual entailment (Dagan et al., 2013), as well as drawing inferences (Mart´ınez-G´omez et al., 2016). Many NLP applications make use of handcrafted resources such as WordNet (Fellbaum, 1998). As a matter of fact, WordNet came from a similar direction with a substantial manual effort. Creating such resources is expensive and time consuming; thus efforts of this sort do not cover the variety of languages equally. Practically, coverage of a wide range of languages through such manual initiatives also far from completion. Many organizations and institutes, wh"
2020.cogalex-1.8,P16-4015,0,0.0219626,"Missing"
2020.cogalex-1.8,J13-3004,0,0.0172652,"lays out the conclusion and future direction. 2 Related Work 2.1 Identifications of semantic relations Recognizing the semantic meaning of words in terms of connecting them with semantic relationships has become a key direction to grow knowledge base and many further practices in NLP. This connects to a wide range of applications, such as textual entailment, text summarization, sentiment analysis, ontology learning, and so on. Following this, several supervised and unsupervised approaches have been initiated and for reference the works of (Lenci and Benotto, 2012) and (Shwartz et al., 2016). (Mohammad et al., 2013) and (Santus et al., 2014) on antonymy are of relevance here. One key commonality amongst these were that these approaches targeted one semantic relationship discovery at once amongst the words rather multiple. There were pattern based multiclass classification task carried out by (Turney, 2008) on similarity, antonymy and analogy, and by (Pantel and Pennacchiotti, 2006) on generic pattern recognition and filtering. These approaches resulted in higher precision and lower recall compared to distributionalsemantic-model-based methods due to their sole dependency on patterns. The later mentioned"
2020.cogalex-1.8,P16-2074,0,0.137706,"arried out by (Turney, 2008) on similarity, antonymy and analogy, and by (Pantel and Pennacchiotti, 2006) on generic pattern recognition and filtering. These approaches resulted in higher precision and lower recall compared to distributionalsemantic-model-based methods due to their sole dependency on patterns. The later mentioned method has been explored in an unsupervised manner (Weeds and Weir, 2003); (Lenci and Benotto, 2012); (Santus et al., 2015) and didn’t measure up in terms of efficacy. Thereafter supervised methods have been adopted (Kruszewski et al., 2015); (Roller and Erk, 2016); (Nguyen et al., 2016); (Shwartz et al., 2016) in the very same direction aiming for classifying the multiclass relationships better. Count-based vectors have been substituting the prediction based ones in some recent approaches, which apparently performed better in some task, such as similarity estimation (Baroni et al., 2014), even though (Levy et al., 2015) demonstrated that these improvements were most likely due to the optimization of hyper-parameters that were instead left unoptimized in count based models. (Shwartz et al., 2016) had an approach combining patterns and distributional information reflected prom"
2020.cogalex-1.8,P06-1015,0,0.3327,"t summarization, sentiment analysis, ontology learning, and so on. Following this, several supervised and unsupervised approaches have been initiated and for reference the works of (Lenci and Benotto, 2012) and (Shwartz et al., 2016). (Mohammad et al., 2013) and (Santus et al., 2014) on antonymy are of relevance here. One key commonality amongst these were that these approaches targeted one semantic relationship discovery at once amongst the words rather multiple. There were pattern based multiclass classification task carried out by (Turney, 2008) on similarity, antonymy and analogy, and by (Pantel and Pennacchiotti, 2006) on generic pattern recognition and filtering. These approaches resulted in higher precision and lower recall compared to distributionalsemantic-model-based methods due to their sole dependency on patterns. The later mentioned method has been explored in an unsupervised manner (Weeds and Weir, 2003); (Lenci and Benotto, 2012); (Santus et al., 2015) and didn’t measure up in terms of efficacy. Thereafter supervised methods have been adopted (Kruszewski et al., 2015); (Roller and Erk, 2016); (Nguyen et al., 2016); (Shwartz et al., 2016) in the very same direction aiming for classifying the multic"
2020.cogalex-1.8,D16-1234,0,0.227976,"ss classification task carried out by (Turney, 2008) on similarity, antonymy and analogy, and by (Pantel and Pennacchiotti, 2006) on generic pattern recognition and filtering. These approaches resulted in higher precision and lower recall compared to distributionalsemantic-model-based methods due to their sole dependency on patterns. The later mentioned method has been explored in an unsupervised manner (Weeds and Weir, 2003); (Lenci and Benotto, 2012); (Santus et al., 2015) and didn’t measure up in terms of efficacy. Thereafter supervised methods have been adopted (Kruszewski et al., 2015); (Roller and Erk, 2016); (Nguyen et al., 2016); (Shwartz et al., 2016) in the very same direction aiming for classifying the multiclass relationships better. Count-based vectors have been substituting the prediction based ones in some recent approaches, which apparently performed better in some task, such as similarity estimation (Baroni et al., 2014), even though (Levy et al., 2015) demonstrated that these improvements were most likely due to the optimization of hyper-parameters that were instead left unoptimized in count based models. (Shwartz et al., 2016) had an approach combining patterns and distributional inf"
2020.cogalex-1.8,W15-4208,0,0.635733,"at these approaches targeted one semantic relationship discovery at once amongst the words rather multiple. There were pattern based multiclass classification task carried out by (Turney, 2008) on similarity, antonymy and analogy, and by (Pantel and Pennacchiotti, 2006) on generic pattern recognition and filtering. These approaches resulted in higher precision and lower recall compared to distributionalsemantic-model-based methods due to their sole dependency on patterns. The later mentioned method has been explored in an unsupervised manner (Weeds and Weir, 2003); (Lenci and Benotto, 2012); (Santus et al., 2015) and didn’t measure up in terms of efficacy. Thereafter supervised methods have been adopted (Kruszewski et al., 2015); (Roller and Erk, 2016); (Nguyen et al., 2016); (Shwartz et al., 2016) in the very same direction aiming for classifying the multiclass relationships better. Count-based vectors have been substituting the prediction based ones in some recent approaches, which apparently performed better in some task, such as similarity estimation (Baroni et al., 2014), even though (Levy et al., 2015) demonstrated that these improvements were most likely due to the optimization of hyper-paramet"
2020.cogalex-1.8,P16-1226,0,0.358202,"e results and Section 6 lays out the conclusion and future direction. 2 Related Work 2.1 Identifications of semantic relations Recognizing the semantic meaning of words in terms of connecting them with semantic relationships has become a key direction to grow knowledge base and many further practices in NLP. This connects to a wide range of applications, such as textual entailment, text summarization, sentiment analysis, ontology learning, and so on. Following this, several supervised and unsupervised approaches have been initiated and for reference the works of (Lenci and Benotto, 2012) and (Shwartz et al., 2016). (Mohammad et al., 2013) and (Santus et al., 2014) on antonymy are of relevance here. One key commonality amongst these were that these approaches targeted one semantic relationship discovery at once amongst the words rather multiple. There were pattern based multiclass classification task carried out by (Turney, 2008) on similarity, antonymy and analogy, and by (Pantel and Pennacchiotti, 2006) on generic pattern recognition and filtering. These approaches resulted in higher precision and lower recall compared to distributionalsemantic-model-based methods due to their sole dependency on patte"
2020.cogalex-1.8,C08-1114,0,0.0125222,"wide range of applications, such as textual entailment, text summarization, sentiment analysis, ontology learning, and so on. Following this, several supervised and unsupervised approaches have been initiated and for reference the works of (Lenci and Benotto, 2012) and (Shwartz et al., 2016). (Mohammad et al., 2013) and (Santus et al., 2014) on antonymy are of relevance here. One key commonality amongst these were that these approaches targeted one semantic relationship discovery at once amongst the words rather multiple. There were pattern based multiclass classification task carried out by (Turney, 2008) on similarity, antonymy and analogy, and by (Pantel and Pennacchiotti, 2006) on generic pattern recognition and filtering. These approaches resulted in higher precision and lower recall compared to distributionalsemantic-model-based methods due to their sole dependency on patterns. The later mentioned method has been explored in an unsupervised manner (Weeds and Weir, 2003); (Lenci and Benotto, 2012); (Santus et al., 2015) and didn’t measure up in terms of efficacy. Thereafter supervised methods have been adopted (Kruszewski et al., 2015); (Roller and Erk, 2016); (Nguyen et al., 2016); (Shwar"
2020.cogalex-1.8,W03-1011,0,0.21574,"nce here. One key commonality amongst these were that these approaches targeted one semantic relationship discovery at once amongst the words rather multiple. There were pattern based multiclass classification task carried out by (Turney, 2008) on similarity, antonymy and analogy, and by (Pantel and Pennacchiotti, 2006) on generic pattern recognition and filtering. These approaches resulted in higher precision and lower recall compared to distributionalsemantic-model-based methods due to their sole dependency on patterns. The later mentioned method has been explored in an unsupervised manner (Weeds and Weir, 2003); (Lenci and Benotto, 2012); (Santus et al., 2015) and didn’t measure up in terms of efficacy. Thereafter supervised methods have been adopted (Kruszewski et al., 2015); (Roller and Erk, 2016); (Nguyen et al., 2016); (Shwartz et al., 2016) in the very same direction aiming for classifying the multiclass relationships better. Count-based vectors have been substituting the prediction based ones in some recent approaches, which apparently performed better in some task, such as similarity estimation (Baroni et al., 2014), even though (Levy et al., 2015) demonstrated that these improvements were mo"
2020.coling-main.141,W06-3812,0,0.0133325,"unlabelled data and situations where the languages are not known in advance. Unsupervised LI is a challenging task. Selamat and Ching (2008) used Fuzzy ART NNs for clustering of documents written in Arabic, Persian, and Urdu. In Fuzzy ART, they have shown how to update clusters dynamically. Amine et al. (2010) used a character n-gram representation for text. Later k-means algorithm followed by particle-swarm optimization is applied to cluster the languages which results in different small clusters. Shiells and Pham (2010) worked on unsupervised tweet LI with the Chinese Whispers algorithm of Biemann (2006) and Graclus clustering. Elfardy and Diab (2012) deal with code-switching identification between Modern Standard Arabic (MSA) and dialectal Arabic. Wan (2016) used word-level k-means clustering for LI and clustered languages based on co-occurring words. Poulston et al. (2017) used word2Vec word embeddings and k-means clustering to make their LI model. Rijhwani et al. (2017) researched short-text tweets for unsupervised language detection which mainly focuses on seven languages. These algorithms work well for some specific languages and long texts. There is some research on dialect identificati"
2020.coling-main.141,2018.gwc-1.10,1,0.782737,"17) and consists of short texts for 11 South African languages, many of which are related; these are the Nguni languages (zul, xho, nbl, ssw), Afrikaans (afr) and English (en), the Sotho languages (nso, sot, tsn), tshiVenda (ven) and Xitsonga (tso). The texts are on average 15-20 characters long. We have taken 15,000 samples for training and 5,000 for testing. Dravidian languages are under-resourced languages spoken mainly in the southern part of India. This dataset contains the languages Tamil (ISO 639-3: tam), Telugu (ISO 639-3: tel), Malayalam (ISO 639-3: mal) and Kannada (ISO 639-3: kan) (Chakravarthi et al., 2018). They have different orthographies with a unique script for each language (Chakravarthi et al., 2019). The training set contains a total of around 14,000 instances and the test set contains a total of 3,000 instances. Swiss German. This dataset is based on the ArchiMob corpus (Samardˇzi´c et al., 2016) and is used for dialect identification. It contains transcriptions of 34 interviews with native speakers of various German 1610 dialects spoken in Switzerland. The subset used for German Dialect Identification contains 18 interviews (14 for training and 4 for testing) from four Swiss German dia"
2020.coling-main.141,W18-3933,0,0.0152149,"ts starting from House and Neuburg (1977). Unsupervised LI is an under-explored area, but is highly useful as it can exploit the large amount of unlabelled data and, more importantly, can be employed when the languages to be identified are not known in advance. However, unsupervised LI for short texts is a very difficult task, for which performance is still comparatively poor. Zhang et al. (2016) have explored unsupervised language identification but this does not work well with closely related short texts. Zaidan and Callison-Burch (2014) have worked on the identification of Arabic dialects. Ciobanu et al. (2018) studied German dialect identification. These works are mostly supervised works. The task is even harder when it comes to unsupervised DI. These previous works have generated some questions: What is the best way to construct sentence embeddings and how to cluster them efficiently for short texts when there is no labelled training data? How to address the hard task of DI and LI for closely related languages in an unsupervised way without any manual intervention? In this paper, we address these problems by taking inspiration from iterative clustering (Xie et al., 2016) and self-attention-based s"
2020.coling-main.141,W17-1221,0,0.0782416,"Missing"
2020.coling-main.141,C12-2029,0,0.0323,"he languages are not known in advance. Unsupervised LI is a challenging task. Selamat and Ching (2008) used Fuzzy ART NNs for clustering of documents written in Arabic, Persian, and Urdu. In Fuzzy ART, they have shown how to update clusters dynamically. Amine et al. (2010) used a character n-gram representation for text. Later k-means algorithm followed by particle-swarm optimization is applied to cluster the languages which results in different small clusters. Shiells and Pham (2010) worked on unsupervised tweet LI with the Chinese Whispers algorithm of Biemann (2006) and Graclus clustering. Elfardy and Diab (2012) deal with code-switching identification between Modern Standard Arabic (MSA) and dialectal Arabic. Wan (2016) used word-level k-means clustering for LI and clustered languages based on co-occurring words. Poulston et al. (2017) used word2Vec word embeddings and k-means clustering to make their LI model. Rijhwani et al. (2017) researched short-text tweets for unsupervised language detection which mainly focuses on seven languages. These algorithms work well for some specific languages and long texts. There is some research on dialect identification (DI). DI has been explored in Serbo-Croatian"
2020.coling-main.141,Y08-1042,0,0.0215706,"(2017) used word2Vec word embeddings and k-means clustering to make their LI model. Rijhwani et al. (2017) researched short-text tweets for unsupervised language detection which mainly focuses on seven languages. These algorithms work well for some specific languages and long texts. There is some research on dialect identification (DI). DI has been explored in Serbo-Croatian dialects (Zeˇcevi´c and Vujicic-Stankovic, 2013), English varieties (Simaki et al., 2017), Dutch dialects (Trieschnigg et al., 2012), German dialects (Hollenstein and Aepli, 2015), Mainland–Singaporean–Taiwanese Chinese (Huang and Lee, 2008), Arabic dialects in social media (Huang, 2015), Portuguese varieties (Zampieri and Gebre, 2012). One of the more famous dialect identification systems is an Arabic dialect system which was given much emphasis in 2015 (Zampieri et al., 2015). But most of these works deal with supervised dialect identification. Our model bridges the gap between DI and unsupervised learning. 1607 Figure 1: Unsupervised deep LI and DI model. The left-hand side shows the architecture design of the UDLDI mechanism; the sentence embedding mechanism is explained on the right-hand side 3 Unsupervised Deep Language and"
2020.coling-main.141,D15-1254,0,0.023762,"ering to make their LI model. Rijhwani et al. (2017) researched short-text tweets for unsupervised language detection which mainly focuses on seven languages. These algorithms work well for some specific languages and long texts. There is some research on dialect identification (DI). DI has been explored in Serbo-Croatian dialects (Zeˇcevi´c and Vujicic-Stankovic, 2013), English varieties (Simaki et al., 2017), Dutch dialects (Trieschnigg et al., 2012), German dialects (Hollenstein and Aepli, 2015), Mainland–Singaporean–Taiwanese Chinese (Huang and Lee, 2008), Arabic dialects in social media (Huang, 2015), Portuguese varieties (Zampieri and Gebre, 2012). One of the more famous dialect identification systems is an Arabic dialect system which was given much emphasis in 2015 (Zampieri et al., 2015). But most of these works deal with supervised dialect identification. Our model bridges the gap between DI and unsupervised learning. 1607 Figure 1: Unsupervised deep LI and DI model. The left-hand side shows the architecture design of the UDLDI mechanism; the sentence embedding mechanism is explained on the right-hand side 3 Unsupervised Deep Language and Dialect Identification (UDLDI) Unsupervised De"
2020.coling-main.141,N13-1131,0,0.0349213,"DI models. Language identification of closely related languages are explored in Malay-Indonesian languages (Ranaivo-Malanc¸on, 2006), Indian languages (Murthy and Kumar, 2006), South Slavic languages (Tiedemann and Ljubeˇsi´c, 2012; Ljubeˇsic and Kranjcic, 2014; Ljubeˇsi´c and Kranjˇci´c, 2015). Zampieri and Gebre (2014) made a LI system which can identify 27 languages. There is some research on LI for short texts. Vatanen et al. (2010) explored an LI method based on ngram character sequences for messages that are 5-21 characters long. Bergsma et al. (2012) have explored LI for Twitter data. King and Abney (2013) used Conditional Random Fields (CRF) (Lafferty et al., 2001) to build word-level LI method. Giwa and Davel (2013) have investigated LI in terms of code-switching. These methods are all supervised and this does not allow them to exploit the large amount of unlabelled data and situations where the languages are not known in advance. Unsupervised LI is a challenging task. Selamat and Ching (2008) used Fuzzy ART NNs for clustering of documents written in Arabic, Persian, and Urdu. In Fuzzy ART, they have shown how to update clusters dynamically. Amine et al. (2010) used a character n-gram represe"
2020.coling-main.141,E17-1087,0,0.177989,"Missing"
2020.coling-main.141,W17-1220,0,0.610394,"Missing"
2020.coling-main.141,W17-1219,0,0.321191,"oes not require any manual intervention and it works well for short texts in any dataset consisting of closely related languages, for any language family. Our contributions are: (1) the mentioned UDLDI model for language and dialect identification in short texts of closely related languages, (2) efficient sentence embedding construction for short texts, and (3) efficient clustering of closely related languages with a very small dataset. 2 Related Work There are some LI and DI models which work efficiently for short texts and closely related languages on different datasets (Hammarstr¨om, 2007; Medvedeva et al., 2017). Jauhiainen et al. (2019) have described different LI and DI models. Language identification of closely related languages are explored in Malay-Indonesian languages (Ranaivo-Malanc¸on, 2006), Indian languages (Murthy and Kumar, 2006), South Slavic languages (Tiedemann and Ljubeˇsi´c, 2012; Ljubeˇsic and Kranjcic, 2014; Ljubeˇsi´c and Kranjˇci´c, 2015). Zampieri and Gebre (2014) made a LI system which can identify 27 languages. There is some research on LI for short texts. Vatanen et al. (2010) explored an LI method based on ngram character sequences for messages that are 5-21 characters long."
2020.coling-main.141,P17-1180,0,0.0173133,"k-means algorithm followed by particle-swarm optimization is applied to cluster the languages which results in different small clusters. Shiells and Pham (2010) worked on unsupervised tweet LI with the Chinese Whispers algorithm of Biemann (2006) and Graclus clustering. Elfardy and Diab (2012) deal with code-switching identification between Modern Standard Arabic (MSA) and dialectal Arabic. Wan (2016) used word-level k-means clustering for LI and clustered languages based on co-occurring words. Poulston et al. (2017) used word2Vec word embeddings and k-means clustering to make their LI model. Rijhwani et al. (2017) researched short-text tweets for unsupervised language detection which mainly focuses on seven languages. These algorithms work well for some specific languages and long texts. There is some research on dialect identification (DI). DI has been explored in Serbo-Croatian dialects (Zeˇcevi´c and Vujicic-Stankovic, 2013), English varieties (Simaki et al., 2017), Dutch dialects (Trieschnigg et al., 2012), German dialects (Hollenstein and Aepli, 2015), Mainland–Singaporean–Taiwanese Chinese (Huang and Lee, 2008), Arabic dialects in social media (Huang, 2015), Portuguese varieties (Zampieri and Geb"
2020.coling-main.141,L16-1641,0,0.0516066,"Missing"
2020.coling-main.141,simaki-etal-2017-identifying,0,0.0155007,"ic (MSA) and dialectal Arabic. Wan (2016) used word-level k-means clustering for LI and clustered languages based on co-occurring words. Poulston et al. (2017) used word2Vec word embeddings and k-means clustering to make their LI model. Rijhwani et al. (2017) researched short-text tweets for unsupervised language detection which mainly focuses on seven languages. These algorithms work well for some specific languages and long texts. There is some research on dialect identification (DI). DI has been explored in Serbo-Croatian dialects (Zeˇcevi´c and Vujicic-Stankovic, 2013), English varieties (Simaki et al., 2017), Dutch dialects (Trieschnigg et al., 2012), German dialects (Hollenstein and Aepli, 2015), Mainland–Singaporean–Taiwanese Chinese (Huang and Lee, 2008), Arabic dialects in social media (Huang, 2015), Portuguese varieties (Zampieri and Gebre, 2012). One of the more famous dialect identification systems is an Arabic dialect system which was given much emphasis in 2015 (Zampieri et al., 2015). But most of these works deal with supervised dialect identification. Our model bridges the gap between DI and unsupervised learning. 1607 Figure 1: Unsupervised deep LI and DI model. The left-hand side sho"
2020.coling-main.141,C12-1160,0,0.391726,"Missing"
2020.coling-main.141,vatanen-etal-2010-language,0,0.0847896,"Missing"
2020.coling-main.141,W16-1206,0,0.0127973,"or clustering of documents written in Arabic, Persian, and Urdu. In Fuzzy ART, they have shown how to update clusters dynamically. Amine et al. (2010) used a character n-gram representation for text. Later k-means algorithm followed by particle-swarm optimization is applied to cluster the languages which results in different small clusters. Shiells and Pham (2010) worked on unsupervised tweet LI with the Chinese Whispers algorithm of Biemann (2006) and Graclus clustering. Elfardy and Diab (2012) deal with code-switching identification between Modern Standard Arabic (MSA) and dialectal Arabic. Wan (2016) used word-level k-means clustering for LI and clustered languages based on co-occurring words. Poulston et al. (2017) used word2Vec word embeddings and k-means clustering to make their LI model. Rijhwani et al. (2017) researched short-text tweets for unsupervised language detection which mainly focuses on seven languages. These algorithms work well for some specific languages and long texts. There is some research on dialect identification (DI). DI has been explored in Serbo-Croatian dialects (Zeˇcevi´c and Vujicic-Stankovic, 2013), English varieties (Simaki et al., 2017), Dutch dialects (Tri"
2020.coling-main.141,J14-1006,0,0.0187839,". Many researchers have carried out experiments in these domains both in speech and texts starting from House and Neuburg (1977). Unsupervised LI is an under-explored area, but is highly useful as it can exploit the large amount of unlabelled data and, more importantly, can be employed when the languages to be identified are not known in advance. However, unsupervised LI for short texts is a very difficult task, for which performance is still comparatively poor. Zhang et al. (2016) have explored unsupervised language identification but this does not work well with closely related short texts. Zaidan and Callison-Burch (2014) have worked on the identification of Arabic dialects. Ciobanu et al. (2018) studied German dialect identification. These works are mostly supervised works. The task is even harder when it comes to unsupervised DI. These previous works have generated some questions: What is the best way to construct sentence embeddings and how to cluster them efficiently for short texts when there is no labelled training data? How to address the hard task of DI and LI for closely related languages in an unsupervised way without any manual intervention? In this paper, we address these problems by taking inspira"
2020.coling-main.141,zampieri-gebre-2014-varclass,0,0.0298929,"ering of closely related languages with a very small dataset. 2 Related Work There are some LI and DI models which work efficiently for short texts and closely related languages on different datasets (Hammarstr¨om, 2007; Medvedeva et al., 2017). Jauhiainen et al. (2019) have described different LI and DI models. Language identification of closely related languages are explored in Malay-Indonesian languages (Ranaivo-Malanc¸on, 2006), Indian languages (Murthy and Kumar, 2006), South Slavic languages (Tiedemann and Ljubeˇsi´c, 2012; Ljubeˇsic and Kranjcic, 2014; Ljubeˇsi´c and Kranjˇci´c, 2015). Zampieri and Gebre (2014) made a LI system which can identify 27 languages. There is some research on LI for short texts. Vatanen et al. (2010) explored an LI method based on ngram character sequences for messages that are 5-21 characters long. Bergsma et al. (2012) have explored LI for Twitter data. King and Abney (2013) used Conditional Random Fields (CRF) (Lafferty et al., 2001) to build word-level LI method. Giwa and Davel (2013) have investigated LI in terms of code-switching. These methods are all supervised and this does not allow them to exploit the large amount of unlabelled data and situations where the lang"
2020.coling-main.141,W15-5401,0,0.0604417,"Missing"
2020.coling-main.141,W13-5307,0,0.0436173,"Missing"
2020.coling-main.369,D19-1522,0,0.0556029,"Missing"
2020.coling-main.369,D19-1189,0,0.170871,"ges 4179–4189 Barcelona, Spain (Online), December 8-13, 2020 KGs, there is a need for constructing appropriate subgraphs for the targeted domain, thereby reducing the amount of redundant information made available to the system. Subgraphs that are rich in domain information containing a low amount of noise are desirable. We, therefore, study the benefits of subgraphs created using N -hop and PageRank (Page et al., 1999) approaches. Furthermore, we analyse which subgraphs are best suited for the task at hand. In this work, we build upon the work of Knowledge-Based Recommendation Dialog (KBRD) (Chen et al., 2019). The authors extract movie entities from dialogues and utilise information from a KG to suggest movies to users. We incorporate pre-trained entity embeddings and make use of positional embeddings to improve the performance of the system. The main contributions of our work are as follows: • We conduct extensive experiments on different subgraph extracted from DBpedia(Auer et al., 2007) to show that the entire information contained in this resource is not beneficial and that there is a need for optimal subgraph creation technique. • We show that using pre-trained entity embeddings supplemented"
2020.coling-main.369,N19-1423,0,0.00571995,"d probability of a relation φ(es , r, eo ) over known relations. 4.2.2 Positional Embeddings During the dialogue flow, entities are mentioned sequentially. By virtue of this, entities mentioned later have a larger effect on future recommendations. Wu et al. (2019) constructed a session graph to represent the sequential nature of items belonging to a user. By leveraging the properties of such graphs, they learned latent item embeddings using graph neural networks. To avoid the construction of session graph and increasing the complexity of the model, we use positional embeddings as described in Devlin et al. (2019), which allow us to infuse sequence information into the entity embeddings. The pattern of embeddings induced by Equation 3 and 4 encodes positional information of entities in the model. POS(pos,2i+1) = cos(pos/100002i/dmodel ) (3) POS(pos,2i) = sin(pos/100002i/dmodel ) (4) The subscript pos in Equation 3 and Equation 4 refers to the index position of the entity in the dialogue, while dmodel refers to the dimensionality of the entity embeddings. The subscript i refers to the index in the vector representing the positional embedding. The final entity representation of the entity ei seen at posi"
2020.figlang-1.22,E06-1042,0,0.0739157,"aphor identification explicitly analyses the tenor and the relation between The levels of processing metaphors should be taken into consideration when designing and developing a computational model to identify metaphors and hence choosing the annotated dataset accordingly for evaluation and comparison. Shutova (2015), Parde and Nielsen (2018) and Zayed et al. (2019) provided extensive details about existing datasets for metaphor identification in English text. The authors highlighted the level of annotation for each dataset among other properties. The widely used benchmark datasets are TroFi (Birke and Sarkar, 2006), VU Amsterdam metaphor corpus (VUAMC) (Steen et al., 2010) and MOH (Mohammad et al., 2016) for word-level metaphor identification, whereas TSV (Tsvetkov et al., 2014), the adaptation of MOH by Shutova et al. (2016), and 155 Figure 1: An illustration of the difference between word-level and relation-level metaphor identification. Stanford CoreNLP is used to generate the dependencies. Zayed’s Tweets (Zayed et al., 2019) datasets are utilised for relation-level metaphor identification. Approaches addressing the task on the wordlevel are not fairly comparable to relation-level approaches since ea"
2020.figlang-1.22,W07-0104,0,0.0372383,"Missing"
2020.figlang-1.22,P06-4020,0,0.0317125,"aches using this dataset. In this paper, we introduce the first adapted version of the VUAMC. Furthermore, we adapt the TroFi and the TSV datasets to better suit relation-level metaphor processing. Related Work 4 This work is inspired by Tsvetkov et al. (2014) and Shutova et al. (2016) who attempted to adapt existing word-level metaphor identification datasets to suit their relation-level (phrase-level) identification approaches. Shutova et al. (2010) was the first to create an annotated dataset for relation-level metaphor identification. The Robust Accurate Statistical Parsing (RASP) parser (Briscoe et al., 2006) was utilised to extract verb-subject and verb-direct object grammar relations from the British National Corpus (BNC) (Burnard, 2007). The dataset comprises around 62 verb-noun pairs of metaphoric expressions, where the verb is used metaphorically given the complement noun (tenor). The TroFi dataset, which was designed to classify particular literal and metaphoric verbs on the word-level, was adapted by Tsvetkov et al. (2014) in order to extract metaphoric expressions on the relation-level. The authors parsed the original dataset using the Turbo dependency parser (Martins et al., 2010) to extr"
2020.figlang-1.22,E17-2084,0,0.0131724,"n the wordlevel, to suit relation-level metaphor identification of verb-noun relations. Verb-direct object and verbs-subject dependencies were extracted and filtered yielding a dataset of 647 verb–noun pairs, out of which 316 instances are metaphorical and 331 instances are literal. To the best of our knowledge, there is no attempt to adapt the benchmark VU Amsterdam metaphor corpus, referred to as VUAMC, to suit relation-level metaphor identification. This has discouraged other researchers focusing on relationlevel approaches to employ this dataset such as the work done by Rei et al. (2017), Bulat et al. (2017), Shutova et al. (2016) and Tsvetkov et al. (2014) who did not evaluate or compare their approaches using this dataset. In this paper, we introduce the first adapted version of the VUAMC. Furthermore, we adapt the TroFi and the TSV datasets to better suit relation-level metaphor processing. Related Work 4 This work is inspired by Tsvetkov et al. (2014) and Shutova et al. (2016) who attempted to adapt existing word-level metaphor identification datasets to suit their relation-level (phrase-level) identification approaches. Shutova et al. (2010) was the first to create an annotated dataset for r"
2020.figlang-1.22,D14-1082,0,0.0115165,"e) labelled as a metaphor regardless of its tenor since it is word-by-word classification. Therefore, in order to adapt them to suit relation-level processing, the associated target domain words (tenor) need to be identified. 3 1. select the benchmark dataset which is originally annotated on the word-level; 2. extract particular grammatical relations focusing on the vehicle as the head of the relation (e.g. the verb in a dobj or adjective in amod relation); 4. verify the correctness of the retrieved relations and the assigned gold label. In this work, we employ the Stanford dependency parser (Chen and Manning, 2014) to identify grammar relations. The recurrent neural network (RNN) parser, pre-trained on the WSJ corpus, is used from within the Stanford CoreNLP toolkit (Manning et al., 2014). For the VUAMC adaptation, as discussed in Section 4, we utilise the training and test splits provided by the NAACL metaphor shared task in the Verbs track. We focus on this track since we are interested in verb-noun relations. The verbs dataset consists of 17,240 annotated verbs in the training set and 5,874 annotated verbs in the test set. First, we retrieved the original sentences of these verbs from the VUAMC since"
2020.figlang-1.22,W19-4444,0,0.0230971,"guage in terms of linguistic metaphors such as “shattered my emotions”,“break his soul”,“crushed her happiness”, “fragile emotions” and “brittle feelings”. Due to their nebulous nature, metaphors are quite challenging to comprehend and process by humans, let alone computational models. This intrigued many researchers to develop various automatic techniques to process metaphor in text. Metaphor processing has many potential applications, either as part of natural language processing (NLP) tasks such as machine translation (Koglin and Cunha, 2019), text simplification (Wolska and Clausen, 2017; Clausen and Nastase, 2019) and sentiment analysis (Rentoumi et al., 2012) or in more general discourse analysis use cases such as in analysing political discourse (Charteris-Black, 2011), financial reporting (Ho and Cheng, 2016) and health communication (Semino et al., 2018). The computational processing of metaphors can be divided into two tasks, namely metaphor identification and its interpretation. While the former is concerned with recognising the metaphoric word or expression in a given sentence, the latter focuses on discerning the meaning of the metaphor. Metaphor identification is studied more extensively than"
2020.figlang-1.22,W18-0907,0,0.0375345,"Missing"
2020.figlang-1.22,P14-5010,0,0.00253319,"et domain words (tenor) need to be identified. 3 1. select the benchmark dataset which is originally annotated on the word-level; 2. extract particular grammatical relations focusing on the vehicle as the head of the relation (e.g. the verb in a dobj or adjective in amod relation); 4. verify the correctness of the retrieved relations and the assigned gold label. In this work, we employ the Stanford dependency parser (Chen and Manning, 2014) to identify grammar relations. The recurrent neural network (RNN) parser, pre-trained on the WSJ corpus, is used from within the Stanford CoreNLP toolkit (Manning et al., 2014). For the VUAMC adaptation, as discussed in Section 4, we utilise the training and test splits provided by the NAACL metaphor shared task in the Verbs track. We focus on this track since we are interested in verb-noun relations. The verbs dataset consists of 17,240 annotated verbs in the training set and 5,874 annotated verbs in the test set. First, we retrieved the original sentences of these verbs from the VUAMC since the shared task released their ids and the corresponding gold labels. This yielded around 10,570 sentences in both sets. Then, we parsed these sentences using the Stanford pars"
2020.figlang-1.22,D10-1004,0,0.0110796,"parser (Briscoe et al., 2006) was utilised to extract verb-subject and verb-direct object grammar relations from the British National Corpus (BNC) (Burnard, 2007). The dataset comprises around 62 verb-noun pairs of metaphoric expressions, where the verb is used metaphorically given the complement noun (tenor). The TroFi dataset, which was designed to classify particular literal and metaphoric verbs on the word-level, was adapted by Tsvetkov et al. (2014) in order to extract metaphoric expressions on the relation-level. The authors parsed the original dataset using the Turbo dependency parser (Martins et al., 2010) to extract subject-verb-object (SVO) grammar relations. The final dataset consists of 953 metaphorical and 656 literal instances. In the same work, Tsvetkov et al. also prepared a relationlevel metaphor identification dataset, referred to as the TSV dataset, focusing on adjective-noun grammar relations. We will further describe this dataset in Section 4. More recently, Shutova et al. (2016) adapted the benchmark MOH dataset, which was initially Datasets As mentioned in Section 2, the widely used benchmark datasets for word-level metaphor identification are TroFi, VUAMC and MOH datasets, while"
2020.figlang-1.22,S16-2003,0,0.0668767,"Missing"
2020.figlang-1.22,L18-1243,0,0.0110519,"comprehension of metaphors. Thus, processing metaphors on the word-level could be seen as a more general approach where the tenor of the metaphor is not explicitly highlighted as well as the relation between the source and the target domains. On the other hand, relation-level metaphor identification explicitly analyses the tenor and the relation between The levels of processing metaphors should be taken into consideration when designing and developing a computational model to identify metaphors and hence choosing the annotated dataset accordingly for evaluation and comparison. Shutova (2015), Parde and Nielsen (2018) and Zayed et al. (2019) provided extensive details about existing datasets for metaphor identification in English text. The authors highlighted the level of annotation for each dataset among other properties. The widely used benchmark datasets are TroFi (Birke and Sarkar, 2006), VU Amsterdam metaphor corpus (VUAMC) (Steen et al., 2010) and MOH (Mohammad et al., 2016) for word-level metaphor identification, whereas TSV (Tsvetkov et al., 2014), the adaptation of MOH by Shutova et al. (2016), and 155 Figure 1: An illustration of the difference between word-level and relation-level metaphor ident"
2020.figlang-1.22,D17-1162,0,0.0117747,"metaphoric verbs on the wordlevel, to suit relation-level metaphor identification of verb-noun relations. Verb-direct object and verbs-subject dependencies were extracted and filtered yielding a dataset of 647 verb–noun pairs, out of which 316 instances are metaphorical and 331 instances are literal. To the best of our knowledge, there is no attempt to adapt the benchmark VU Amsterdam metaphor corpus, referred to as VUAMC, to suit relation-level metaphor identification. This has discouraged other researchers focusing on relationlevel approaches to employ this dataset such as the work done by Rei et al. (2017), Bulat et al. (2017), Shutova et al. (2016) and Tsvetkov et al. (2014) who did not evaluate or compare their approaches using this dataset. In this paper, we introduce the first adapted version of the VUAMC. Furthermore, we adapt the TroFi and the TSV datasets to better suit relation-level metaphor processing. Related Work 4 This work is inspired by Tsvetkov et al. (2014) and Shutova et al. (2016) who attempted to adapt existing word-level metaphor identification datasets to suit their relation-level (phrase-level) identification approaches. Shutova et al. (2010) was the first to create an an"
2020.figlang-1.22,N16-1020,0,0.253398,"rs and hence choosing the annotated dataset accordingly for evaluation and comparison. Shutova (2015), Parde and Nielsen (2018) and Zayed et al. (2019) provided extensive details about existing datasets for metaphor identification in English text. The authors highlighted the level of annotation for each dataset among other properties. The widely used benchmark datasets are TroFi (Birke and Sarkar, 2006), VU Amsterdam metaphor corpus (VUAMC) (Steen et al., 2010) and MOH (Mohammad et al., 2016) for word-level metaphor identification, whereas TSV (Tsvetkov et al., 2014), the adaptation of MOH by Shutova et al. (2016), and 155 Figure 1: An illustration of the difference between word-level and relation-level metaphor identification. Stanford CoreNLP is used to generate the dependencies. Zayed’s Tweets (Zayed et al., 2019) datasets are utilised for relation-level metaphor identification. Approaches addressing the task on the wordlevel are not fairly comparable to relation-level approaches since each task deals with metaphor identification differently. Therefore, given the distinction of the tasks definition, the tradition of previous work in this area is to compare the wordlevel metaphor identification appro"
2020.figlang-1.22,C10-1113,0,0.0966965,"Missing"
2020.figlang-1.22,W18-0903,0,0.0112705,"ric or literal given the context. Many approaches are designed to identify metaphors of different syntactic types on the word-level but the most frequently studied ones are verbs. In this paper, we are interested in relation-level metaphor identification focusing on the data availability for this level of processing. The next section explains, in detail, the difference between wordlevel and relation-level metaphor analysis highlighting the research gap that we aim to tackle. 2 the source and the target domains. Figure 1 illustrates the difference between the levels of metaphor identification. Stowe and Palmer (2018) highlighted the importance of integrating syntax and semantics to process metaphors in text. Through a corpus-based analysis focusing on verb metaphors, the authors showed that the type of syntactic construction (dependency/grammar relation) a verb occurs in influences its metaphoricity. Relation-level metaphor processing requires an extra step to identify the grammatical relations (i.e. dependencies) that highlight both the tenor and the vehicle. Thus, it might be seen that processing metaphors on the word-level is more straight forward and raises the question: why do we need relation-level"
2020.figlang-1.22,P14-1024,0,0.21611,"eloping a computational model to identify metaphors and hence choosing the annotated dataset accordingly for evaluation and comparison. Shutova (2015), Parde and Nielsen (2018) and Zayed et al. (2019) provided extensive details about existing datasets for metaphor identification in English text. The authors highlighted the level of annotation for each dataset among other properties. The widely used benchmark datasets are TroFi (Birke and Sarkar, 2006), VU Amsterdam metaphor corpus (VUAMC) (Steen et al., 2010) and MOH (Mohammad et al., 2016) for word-level metaphor identification, whereas TSV (Tsvetkov et al., 2014), the adaptation of MOH by Shutova et al. (2016), and 155 Figure 1: An illustration of the difference between word-level and relation-level metaphor identification. Stanford CoreNLP is used to generate the dependencies. Zayed’s Tweets (Zayed et al., 2019) datasets are utilised for relation-level metaphor identification. Approaches addressing the task on the wordlevel are not fairly comparable to relation-level approaches since each task deals with metaphor identification differently. Therefore, given the distinction of the tasks definition, the tradition of previous work in this area is to com"
2020.figlang-1.22,W17-5035,0,0.0235765,"ressed in our everyday language in terms of linguistic metaphors such as “shattered my emotions”,“break his soul”,“crushed her happiness”, “fragile emotions” and “brittle feelings”. Due to their nebulous nature, metaphors are quite challenging to comprehend and process by humans, let alone computational models. This intrigued many researchers to develop various automatic techniques to process metaphor in text. Metaphor processing has many potential applications, either as part of natural language processing (NLP) tasks such as machine translation (Koglin and Cunha, 2019), text simplification (Wolska and Clausen, 2017; Clausen and Nastase, 2019) and sentiment analysis (Rentoumi et al., 2012) or in more general discourse analysis use cases such as in analysing political discourse (Charteris-Black, 2011), financial reporting (Ho and Cheng, 2016) and health communication (Semino et al., 2018). The computational processing of metaphors can be divided into two tasks, namely metaphor identification and its interpretation. While the former is concerned with recognising the metaphoric word or expression in a given sentence, the latter focuses on discerning the meaning of the metaphor. Metaphor identification is st"
2020.findings-emnlp.36,W13-0901,0,0.0148166,"uestion-answering (de Vries et al., 2017), dependency parsing (Dozat and Manning, 2017), semantic role labelling (Cai et al., 2018), coreference resolution (Zhang et al., 2018), visual reasoning (Perez et al., 2018) and lexicon features integration (Margatina et al., 2019). 2 Related Work Over the last decades, the focus of computational metaphor identification has shifted from rule-based (Fass, 1991) and knowledge-based approaches (Krishnakumaran and Zhu, 2007; Wilks et al., 2013) to statistical and machine learning approaches including supervised (Gedigian et al., 2006; Turney et al., 2011; Dunn, 2013a,b; Tsvetkov et al., 2013; Hovy et al., 2013; Mohler et al., 2013; Klebanov et al., 2014; Bracewell et al., 2014; Jang et al., 2015; Gargett and Barnden, 2015; Rai et al., 2016; Bulat et al., 2017; K¨oper and Schulte im Walde, 2017), semi-supervised (Birke and Sarkar, 2006; Shutova et al., 2010; Zayed et al., 2018) and unsupervised methods (Shutova and Sun, 2013; Heintz et al., 2013; Strzalkowski et al., 2013). These approaches employed a variety of features to design their models. With the advances in neu389 ral networks, the focus started to shift towards employing more sophisticated models"
2020.findings-emnlp.36,J91-1003,0,0.827084,"ation in a novel way to condition the neural network computation on the contextualised features of the given expression. The idea of affine transformations has been used in NLP-related tasks such as visual question-answering (de Vries et al., 2017), dependency parsing (Dozat and Manning, 2017), semantic role labelling (Cai et al., 2018), coreference resolution (Zhang et al., 2018), visual reasoning (Perez et al., 2018) and lexicon features integration (Margatina et al., 2019). 2 Related Work Over the last decades, the focus of computational metaphor identification has shifted from rule-based (Fass, 1991) and knowledge-based approaches (Krishnakumaran and Zhu, 2007; Wilks et al., 2013) to statistical and machine learning approaches including supervised (Gedigian et al., 2006; Turney et al., 2011; Dunn, 2013a,b; Tsvetkov et al., 2013; Hovy et al., 2013; Mohler et al., 2013; Klebanov et al., 2014; Bracewell et al., 2014; Jang et al., 2015; Gargett and Barnden, 2015; Rai et al., 2016; Bulat et al., 2017; K¨oper and Schulte im Walde, 2017), semi-supervised (Birke and Sarkar, 2006; Shutova et al., 2010; Zayed et al., 2018) and unsupervised methods (Shutova and Sun, 2013; Heintz et al., 2013; Strzal"
2020.findings-emnlp.36,D18-1060,0,0.0842365,"ere a traditional fully-connected feed-forward neural network is trained using pre-trained word embeddings. The authors highlighted the limitation of this approach when dealing with short and noisy conversational texts. As part of the NAACL 2018 Metaphor Shared Task (Leong et al., 2018), many researchers proposed neural models that mainly employ LSTMs (Hochreiter and Schmidhuber, 1997) with pre-trained word embeddings to identify metaphors on the word-level. The best performing systems are: THU NGN (Wu et al., 2018), OCOTA (Bizzoni and Ghanimifard, 2018) and bot.zen (Stemle and Onysko, 2018). Gao et al. (2018) were the first to employ the deep contextualised word representation ELMo (Peters et al., 2018), combined with pre-trained GloVe (Pennington et al., 2014) embeddings to train bidirectional LSTM-based models. The authors introduced a sequence labelling model and a single-word classification model for verbs. They showed that incorporating the context-dependent representation of ELMo with context-independent word embeddings improved metaphor identification. Mu et al. (2019) proposed a system that utilises a gradient boosting decision tree classifier. Document embeddings were employed in an attem"
2020.findings-emnlp.36,W15-1403,0,0.0459358,"resolution (Zhang et al., 2018), visual reasoning (Perez et al., 2018) and lexicon features integration (Margatina et al., 2019). 2 Related Work Over the last decades, the focus of computational metaphor identification has shifted from rule-based (Fass, 1991) and knowledge-based approaches (Krishnakumaran and Zhu, 2007; Wilks et al., 2013) to statistical and machine learning approaches including supervised (Gedigian et al., 2006; Turney et al., 2011; Dunn, 2013a,b; Tsvetkov et al., 2013; Hovy et al., 2013; Mohler et al., 2013; Klebanov et al., 2014; Bracewell et al., 2014; Jang et al., 2015; Gargett and Barnden, 2015; Rai et al., 2016; Bulat et al., 2017; K¨oper and Schulte im Walde, 2017), semi-supervised (Birke and Sarkar, 2006; Shutova et al., 2010; Zayed et al., 2018) and unsupervised methods (Shutova and Sun, 2013; Heintz et al., 2013; Strzalkowski et al., 2013). These approaches employed a variety of features to design their models. With the advances in neu389 ral networks, the focus started to shift towards employing more sophisticated models to identify metaphors. This section focuses on current research that employs neural models for metaphor identification on both word and relation levels. Word-"
2020.findings-emnlp.36,W06-3506,0,0.0721933,"n used in NLP-related tasks such as visual question-answering (de Vries et al., 2017), dependency parsing (Dozat and Manning, 2017), semantic role labelling (Cai et al., 2018), coreference resolution (Zhang et al., 2018), visual reasoning (Perez et al., 2018) and lexicon features integration (Margatina et al., 2019). 2 Related Work Over the last decades, the focus of computational metaphor identification has shifted from rule-based (Fass, 1991) and knowledge-based approaches (Krishnakumaran and Zhu, 2007; Wilks et al., 2013) to statistical and machine learning approaches including supervised (Gedigian et al., 2006; Turney et al., 2011; Dunn, 2013a,b; Tsvetkov et al., 2013; Hovy et al., 2013; Mohler et al., 2013; Klebanov et al., 2014; Bracewell et al., 2014; Jang et al., 2015; Gargett and Barnden, 2015; Rai et al., 2016; Bulat et al., 2017; K¨oper and Schulte im Walde, 2017), semi-supervised (Birke and Sarkar, 2006; Shutova et al., 2010; Zayed et al., 2018) and unsupervised methods (Shutova and Sun, 2013; Heintz et al., 2013; Strzalkowski et al., 2013). These approaches employed a variety of features to design their models. With the advances in neu389 ral networks, the focus started to shift towards em"
2020.findings-emnlp.36,W13-0908,0,0.0160554,"rom rule-based (Fass, 1991) and knowledge-based approaches (Krishnakumaran and Zhu, 2007; Wilks et al., 2013) to statistical and machine learning approaches including supervised (Gedigian et al., 2006; Turney et al., 2011; Dunn, 2013a,b; Tsvetkov et al., 2013; Hovy et al., 2013; Mohler et al., 2013; Klebanov et al., 2014; Bracewell et al., 2014; Jang et al., 2015; Gargett and Barnden, 2015; Rai et al., 2016; Bulat et al., 2017; K¨oper and Schulte im Walde, 2017), semi-supervised (Birke and Sarkar, 2006; Shutova et al., 2010; Zayed et al., 2018) and unsupervised methods (Shutova and Sun, 2013; Heintz et al., 2013; Strzalkowski et al., 2013). These approaches employed a variety of features to design their models. With the advances in neu389 ral networks, the focus started to shift towards employing more sophisticated models to identify metaphors. This section focuses on current research that employs neural models for metaphor identification on both word and relation levels. Word-Level Processing: Do Dinh and Gurevych (2016) were the first to utilise a neural architecture to identify metaphors. They approached the problem as sequence labelling where a traditional fully-connected feed-forward neural netw"
2020.findings-emnlp.36,P82-1020,0,0.740393,"Missing"
2020.findings-emnlp.36,P18-1113,0,0.217732,"Missing"
2020.findings-emnlp.36,W13-0907,0,0.0169984,"17), dependency parsing (Dozat and Manning, 2017), semantic role labelling (Cai et al., 2018), coreference resolution (Zhang et al., 2018), visual reasoning (Perez et al., 2018) and lexicon features integration (Margatina et al., 2019). 2 Related Work Over the last decades, the focus of computational metaphor identification has shifted from rule-based (Fass, 1991) and knowledge-based approaches (Krishnakumaran and Zhu, 2007; Wilks et al., 2013) to statistical and machine learning approaches including supervised (Gedigian et al., 2006; Turney et al., 2011; Dunn, 2013a,b; Tsvetkov et al., 2013; Hovy et al., 2013; Mohler et al., 2013; Klebanov et al., 2014; Bracewell et al., 2014; Jang et al., 2015; Gargett and Barnden, 2015; Rai et al., 2016; Bulat et al., 2017; K¨oper and Schulte im Walde, 2017), semi-supervised (Birke and Sarkar, 2006; Shutova et al., 2010; Zayed et al., 2018) and unsupervised methods (Shutova and Sun, 2013; Heintz et al., 2013; Strzalkowski et al., 2013). These approaches employed a variety of features to design their models. With the advances in neu389 ral networks, the focus started to shift towards employing more sophisticated models to identify metaphors. This section focuses"
2020.findings-emnlp.36,W15-4650,0,0.277476,"2018), coreference resolution (Zhang et al., 2018), visual reasoning (Perez et al., 2018) and lexicon features integration (Margatina et al., 2019). 2 Related Work Over the last decades, the focus of computational metaphor identification has shifted from rule-based (Fass, 1991) and knowledge-based approaches (Krishnakumaran and Zhu, 2007; Wilks et al., 2013) to statistical and machine learning approaches including supervised (Gedigian et al., 2006; Turney et al., 2011; Dunn, 2013a,b; Tsvetkov et al., 2013; Hovy et al., 2013; Mohler et al., 2013; Klebanov et al., 2014; Bracewell et al., 2014; Jang et al., 2015; Gargett and Barnden, 2015; Rai et al., 2016; Bulat et al., 2017; K¨oper and Schulte im Walde, 2017), semi-supervised (Birke and Sarkar, 2006; Shutova et al., 2010; Zayed et al., 2018) and unsupervised methods (Shutova and Sun, 2013; Heintz et al., 2013; Strzalkowski et al., 2013). These approaches employed a variety of features to design their models. With the advances in neu389 ral networks, the focus started to shift towards employing more sophisticated models to identify metaphors. This section focuses on current research that employs neural models for metaphor identification on both word"
2020.findings-emnlp.36,W14-2302,0,0.0189081,"ng, 2017), semantic role labelling (Cai et al., 2018), coreference resolution (Zhang et al., 2018), visual reasoning (Perez et al., 2018) and lexicon features integration (Margatina et al., 2019). 2 Related Work Over the last decades, the focus of computational metaphor identification has shifted from rule-based (Fass, 1991) and knowledge-based approaches (Krishnakumaran and Zhu, 2007; Wilks et al., 2013) to statistical and machine learning approaches including supervised (Gedigian et al., 2006; Turney et al., 2011; Dunn, 2013a,b; Tsvetkov et al., 2013; Hovy et al., 2013; Mohler et al., 2013; Klebanov et al., 2014; Bracewell et al., 2014; Jang et al., 2015; Gargett and Barnden, 2015; Rai et al., 2016; Bulat et al., 2017; K¨oper and Schulte im Walde, 2017), semi-supervised (Birke and Sarkar, 2006; Shutova et al., 2010; Zayed et al., 2018) and unsupervised methods (Shutova and Sun, 2013; Heintz et al., 2013; Strzalkowski et al., 2013). These approaches employed a variety of features to design their models. With the advances in neu389 ral networks, the focus started to shift towards employing more sophisticated models to identify metaphors. This section focuses on current research that employs neural mode"
2020.findings-emnlp.36,P19-1378,0,0.0806395,"9) proposed a system that utilises a gradient boosting decision tree classifier. Document embeddings were employed in an attempt to exploit wider context to improve metaphor detection in addition to other word representations including GLoVe, ELMo and skip-thought (Kiros et al., 2015). Mao et al. (2018, 2019) explored the idea of selectional preferences violation (Wilks, 1978) in a neural architecture to identify metaphoric words. Mao’s proposed approaches emphasised the importance of the context to identify metaphoricity by employing context-dependent and context-independent word embeddings. Mao et al. (2019) also proposed employing multi-head attention to compare the targeted word representation with its context. An interesting approach was introduced by Dankers et al. (2019) to model the interplay between metaphor identification and emotion regression. The authors introduced multiple multi-task learning techniques that employ hard and soft parameter sharing methods to optimise LSTM-based and BERT-based models. Relation-Level Processing: Shutova et al. (2016) focused on identifying the metaphoricity of adjective/verb-noun pairs. This work employed multimodal embeddings of visual and linguistic fe"
2020.findings-emnlp.36,P19-1385,0,0.0179672,"ne transformations. In order to integrate the interaction of the metaphor components in the identification process, we utilise affine transformation in a novel way to condition the neural network computation on the contextualised features of the given expression. The idea of affine transformations has been used in NLP-related tasks such as visual question-answering (de Vries et al., 2017), dependency parsing (Dozat and Manning, 2017), semantic role labelling (Cai et al., 2018), coreference resolution (Zhang et al., 2018), visual reasoning (Perez et al., 2018) and lexicon features integration (Margatina et al., 2019). 2 Related Work Over the last decades, the focus of computational metaphor identification has shifted from rule-based (Fass, 1991) and knowledge-based approaches (Krishnakumaran and Zhu, 2007; Wilks et al., 2013) to statistical and machine learning approaches including supervised (Gedigian et al., 2006; Turney et al., 2011; Dunn, 2013a,b; Tsvetkov et al., 2013; Hovy et al., 2013; Mohler et al., 2013; Klebanov et al., 2014; Bracewell et al., 2014; Jang et al., 2015; Gargett and Barnden, 2015; Rai et al., 2016; Bulat et al., 2017; K¨oper and Schulte im Walde, 2017), semi-supervised (Birke and S"
2020.findings-emnlp.36,S16-2003,0,0.391371,"Missing"
2020.findings-emnlp.36,W13-0904,0,0.274685,"sing (Dozat and Manning, 2017), semantic role labelling (Cai et al., 2018), coreference resolution (Zhang et al., 2018), visual reasoning (Perez et al., 2018) and lexicon features integration (Margatina et al., 2019). 2 Related Work Over the last decades, the focus of computational metaphor identification has shifted from rule-based (Fass, 1991) and knowledge-based approaches (Krishnakumaran and Zhu, 2007; Wilks et al., 2013) to statistical and machine learning approaches including supervised (Gedigian et al., 2006; Turney et al., 2011; Dunn, 2013a,b; Tsvetkov et al., 2013; Hovy et al., 2013; Mohler et al., 2013; Klebanov et al., 2014; Bracewell et al., 2014; Jang et al., 2015; Gargett and Barnden, 2015; Rai et al., 2016; Bulat et al., 2017; K¨oper and Schulte im Walde, 2017), semi-supervised (Birke and Sarkar, 2006; Shutova et al., 2010; Zayed et al., 2018) and unsupervised methods (Shutova and Sun, 2013; Heintz et al., 2013; Strzalkowski et al., 2013). These approaches employed a variety of features to design their models. With the advances in neu389 ral networks, the focus started to shift towards employing more sophisticated models to identify metaphors. This section focuses on current research t"
2020.findings-emnlp.36,N19-1059,0,0.522106,"rming systems are: THU NGN (Wu et al., 2018), OCOTA (Bizzoni and Ghanimifard, 2018) and bot.zen (Stemle and Onysko, 2018). Gao et al. (2018) were the first to employ the deep contextualised word representation ELMo (Peters et al., 2018), combined with pre-trained GloVe (Pennington et al., 2014) embeddings to train bidirectional LSTM-based models. The authors introduced a sequence labelling model and a single-word classification model for verbs. They showed that incorporating the context-dependent representation of ELMo with context-independent word embeddings improved metaphor identification. Mu et al. (2019) proposed a system that utilises a gradient boosting decision tree classifier. Document embeddings were employed in an attempt to exploit wider context to improve metaphor detection in addition to other word representations including GLoVe, ELMo and skip-thought (Kiros et al., 2015). Mao et al. (2018, 2019) explored the idea of selectional preferences violation (Wilks, 1978) in a neural architecture to identify metaphoric words. Mao’s proposed approaches emphasised the importance of the context to identify metaphoricity by employing context-dependent and context-independent word embeddings. Ma"
2020.findings-emnlp.36,D14-1162,0,0.0823574,"Missing"
2020.findings-emnlp.36,N18-1202,0,0.0374712,"rd embeddings. The authors highlighted the limitation of this approach when dealing with short and noisy conversational texts. As part of the NAACL 2018 Metaphor Shared Task (Leong et al., 2018), many researchers proposed neural models that mainly employ LSTMs (Hochreiter and Schmidhuber, 1997) with pre-trained word embeddings to identify metaphors on the word-level. The best performing systems are: THU NGN (Wu et al., 2018), OCOTA (Bizzoni and Ghanimifard, 2018) and bot.zen (Stemle and Onysko, 2018). Gao et al. (2018) were the first to employ the deep contextualised word representation ELMo (Peters et al., 2018), combined with pre-trained GloVe (Pennington et al., 2014) embeddings to train bidirectional LSTM-based models. The authors introduced a sequence labelling model and a single-word classification model for verbs. They showed that incorporating the context-dependent representation of ELMo with context-independent word embeddings improved metaphor identification. Mu et al. (2019) proposed a system that utilises a gradient boosting decision tree classifier. Document embeddings were employed in an attempt to exploit wider context to improve metaphor detection in addition to other word representati"
2020.findings-emnlp.36,W16-1103,0,0.029423,"Missing"
2020.findings-emnlp.36,D17-1162,0,0.10501,"del the interplay between metaphor identification and emotion regression. The authors introduced multiple multi-task learning techniques that employ hard and soft parameter sharing methods to optimise LSTM-based and BERT-based models. Relation-Level Processing: Shutova et al. (2016) focused on identifying the metaphoricity of adjective/verb-noun pairs. This work employed multimodal embeddings of visual and linguistic features. Their model employs the cosine similarity of the candidate expression components based on word embeddings to classify metaphors using an optimised similarity threshold. Rei et al. (2017) introduced a supervised similarity network to detect adjective/verb-noun metaphoric expressions. Their system utilises word gating, vector representation mapping and a weighted similarity function. Pre-trained word embeddings and attribute-based embeddings (Bulat et al., 2017) were employed as features. This work explicitly models the interaction between the metaphor components. Gating is used to modify the vector of the verb/adjective based on the noun, however the surrounding context is ignored by feeding only the candidates as input to the neural model which might lead to loosing important"
2020.findings-emnlp.36,N16-1020,0,0.128659,"ches emphasised the importance of the context to identify metaphoricity by employing context-dependent and context-independent word embeddings. Mao et al. (2019) also proposed employing multi-head attention to compare the targeted word representation with its context. An interesting approach was introduced by Dankers et al. (2019) to model the interplay between metaphor identification and emotion regression. The authors introduced multiple multi-task learning techniques that employ hard and soft parameter sharing methods to optimise LSTM-based and BERT-based models. Relation-Level Processing: Shutova et al. (2016) focused on identifying the metaphoricity of adjective/verb-noun pairs. This work employed multimodal embeddings of visual and linguistic features. Their model employs the cosine similarity of the candidate expression components based on word embeddings to classify metaphors using an optimised similarity threshold. Rei et al. (2017) introduced a supervised similarity network to detect adjective/verb-noun metaphoric expressions. Their system utilises word gating, vector representation mapping and a weighted similarity function. Pre-trained word embeddings and attribute-based embeddings (Bulat e"
2020.findings-emnlp.36,N13-1118,0,0.238925,"ification has shifted from rule-based (Fass, 1991) and knowledge-based approaches (Krishnakumaran and Zhu, 2007; Wilks et al., 2013) to statistical and machine learning approaches including supervised (Gedigian et al., 2006; Turney et al., 2011; Dunn, 2013a,b; Tsvetkov et al., 2013; Hovy et al., 2013; Mohler et al., 2013; Klebanov et al., 2014; Bracewell et al., 2014; Jang et al., 2015; Gargett and Barnden, 2015; Rai et al., 2016; Bulat et al., 2017; K¨oper and Schulte im Walde, 2017), semi-supervised (Birke and Sarkar, 2006; Shutova et al., 2010; Zayed et al., 2018) and unsupervised methods (Shutova and Sun, 2013; Heintz et al., 2013; Strzalkowski et al., 2013). These approaches employed a variety of features to design their models. With the advances in neu389 ral networks, the focus started to shift towards employing more sophisticated models to identify metaphors. This section focuses on current research that employs neural models for metaphor identification on both word and relation levels. Word-Level Processing: Do Dinh and Gurevych (2016) were the first to utilise a neural architecture to identify metaphors. They approached the problem as sequence labelling where a traditional fully-connected fee"
2020.findings-emnlp.36,C10-1113,0,0.0827264,"Missing"
2020.findings-emnlp.36,shutova-teufel-2010-metaphor,0,0.210902,"l expression, whereas in word-level identification only the source domain words (vehicle) are labelled. These levels of analysis (paradigms) are already established in literature and adopted by previous research in this area as will be explained in Section 2. The majority of existing approaches, as well as the available datasets, pertaining to metaphor processing focus on the metaphorical usage of verbs and adjectives either on the word or relation levels. This is because these syntactic types exhibit metaphoricity more frequently than others according to corpus-based analysis (Cameron, 2003; Shutova and Teufel, 2010). Inspired by the works on visual reasoning, we use the candidate expression of certain grammatical relations, represented by deep contextualised features, as an auxiliary input to modulate our computational model. Affine transformations can be utilised to process one source of information in the context of another. In our case, we want to integrate: 1) the deep contextualised-features of the candidate expression (represented by ELMo sentence embeddings) with 2) the syntactic/semantic features of a given sentence. Based on this task, affine transformations have a similar role to attention but"
2020.globalex-1.1,2020.lrec-1.695,1,0.81461,"Missing"
2020.globalex-1.15,D13-1179,1,0.872474,"Missing"
2020.globalex-1.15,P16-1162,0,0.0241246,"geted Romance language family, but excluded the English-Spanish, EnglishFrench, English-Portuguese and Portuguese-French language pair. 3. 3.1. Neural Machine Translation Setup We used OpenNMT (Klein et al., 2017), a generic deep learning framework mainly specialised in sequence-to-sequence models covering a variety of tasks such as machine translation, summarisation, speech processing and question answering as NMT framework. Due to computational complexity, the vocabulary in NMT models had to be limited. To overcome this limitation, we used byte pair encoding (BPE) to generate subword units (Sennrich et al., 2016). BPE is a data compression technique that iteratively replaces the most frequent pair of bytes in a sequence with a single, unused byte. We used the following default neural network training parameters: two hidden layers, 500 hidden LSTM (long short term memory) units per layer, input feeding enabled, 13 epochs, batch size of 64, 0.3 dropout probability, dynamic learning rate decay, 500 dimension embeddings. 1 Results Results on Apertium In order to develop and train our system, we used the available Apertium data as a gold standard. In this case, we held out the English-Spanish translation d"
2020.globalex-1.15,P17-4012,0,0.0214073,"set of parallel data, i.e. for less-resourced languages, we trained a multi-source and multi-target NMT model (Ha et al., 2016) with wellresourced language pairs. In our work, we have chosen parallel corpora in the Romance language family, i.e. Spanish, Italian, French, Portuguese, Romanian, as well as English. To train the multi-way NMT system, we used all possible language combinations within the targeted Romance language family, but excluded the English-Spanish, EnglishFrench, English-Portuguese and Portuguese-French language pair. 3. 3.1. Neural Machine Translation Setup We used OpenNMT (Klein et al., 2017), a generic deep learning framework mainly specialised in sequence-to-sequence models covering a variety of tasks such as machine translation, summarisation, speech processing and question answering as NMT framework. Due to computational complexity, the vocabulary in NMT models had to be limited. To overcome this limitation, we used byte pair encoding (BPE) to generate subword units (Sennrich et al., 2016). BPE is a data compression technique that iteratively replaces the most frequent pair of bytes in a sequence with a single, unused byte. We used the following default neural network training"
2020.globalex-1.15,steinberger-etal-2012-dgt,0,0.0234989,"Missing"
2020.globalex-1.15,C94-1048,0,0.0244137,"aluations. Table 3: Performance of our system on predicting EnglishSpanish Apertium data 3.2. Task Results The official results from the organizers are reproduced in Table 4. We can see from this that in all evaluations, the system described in this paper (labelled ‘NUIG’), produced the highest precision in its results. However, as we saw in the Apertium analysis we had a significant drop in recall compared to the baselines and these overall meant that the system was 2nd or 3rd in terms of F-Measure. We also note that the systems to beat ours were those based on one-time inverse consultation (Tanaka and Umemura, 1994), and it should be relatively easy to combine these results into our architecture, suggesting that we could easily obtain a much stronger result. 3.3. Conclusion Acknowledgements This publication has emanated from research supported in part by a research grant from Science Foundation Ireland (SFI) under Grant Number SFI/12/RC/2289 P2, co-funded by the European Regional Development Fund, as well as by the H2020 project Prˆet-`a-LLOD under Grant Agreement number 825182. Bibliographical References Discussion Arcan, M., Torregrosa, D., Ahmadi, S., and McCrae, J. P. (2019). Inferring translation ca"
2020.iwltp-1.15,W16-3503,1,0.91547,"PI Manager Workflow Manager Preprocessing Provisioning of Datasets and Content Language Identification Storage Knowledge Graph File Storage Duplicate Detection Document Structure Analysis Semantic Analysis Content Generation Summarization Named Entity Recognition and Linking Paraphrasing Temporal Expression Analysis Machine Translation Relation Extraction Semantic Storytelling Event Detection Security Figure 5: Technical architecture of the QURATOR platform velops a curation technology platform, which is also being populated with services, simplifying and accelerating the curation of content (Bourgonje et al., 2016a; Rehm et al., 2019a; Schneider and Rehm, 2018a; Schneider and Rehm, 2018b). The project develops, evaluates and integrates services for preprocessing, analyzing and generating content, spanning use cases from the sectors of culture, media, health and industry. To process and transform incoming data, text or multimedia streams into device-adapted, publishable content, various groups of components, services and technologies are applied. These include adapters to data, content and knowledge sources, as well as infrastructural tools and AI methods for the acquisition, analysis and generation of"
2020.iwltp-1.15,2020.lrec-1.696,1,0.735511,"nt annotation schemes can refer to. (2) Multiple OLiA Annotation Models formalize annotation schemes and tagsets. Fig. 8 illustrates this with an annotation model developed as part of the Korean NLP2RDF stack (Hahm et al., 2012). (3) For every annotation model, a linking model defines subclass-relationships between concepts in the annotation model and the reference model. Linking models are interpretations of annotation model concepts and properties in terms of the reference model. (4) Similarly, other community-maintained vocabularies are linked with OLiA, e. g., the CLARIN Concept Registry (Chiarcos et al., 2020). OLiA was developed as part of an infrastructure for the sustainable maintenance of linguistic resources (Wörner et al., 2006; Schmidt et al., 2006; Rehm et al., 2008b; Witt et al., 2009; Rehm et al., 2009). Its field of application included the formalization of annotation schemes and concept-based querying over heterogeneously annotated corpora (Rehm et al., 2008a). As several institutions and resources from various disciplines were involved, no holistic annotation standard could be enforced onto the contributors. 101 3.4. Figure 8: Modular OLiA ontologies 3.2. Level 1: Simple Cross-Platform"
2020.iwltp-1.15,W12-5201,0,0.0160466,"tral hub for linguistic annotation terminology in the web of data. OLiA was designed for mediating between various terminology repositories on the one hand and annotated resources (i. e., their annotation schemes), on the other. Four different types of ontologies are distinguished (Fig. 8): (1) The OLiA Reference Model is an OWL ontology that specifies the common terminology that different annotation schemes can refer to. (2) Multiple OLiA Annotation Models formalize annotation schemes and tagsets. Fig. 8 illustrates this with an annotation model developed as part of the Korean NLP2RDF stack (Hahm et al., 2012). (3) For every annotation model, a linking model defines subclass-relationships between concepts in the annotation model and the reference model. Linking models are interpretations of annotation model concepts and properties in terms of the reference model. (4) Similarly, other community-maintained vocabularies are linked with OLiA, e. g., the CLARIN Concept Registry (Chiarcos et al., 2020). OLiA was developed as part of an infrastructure for the sustainable maintenance of linguistic resources (Wörner et al., 2006; Schmidt et al., 2006; Rehm et al., 2008b; Witt et al., 2009; Rehm et al., 2009"
2020.iwltp-1.15,2020.lrec-1.420,1,0.820145,"Missing"
2020.iwltp-1.15,W17-4212,1,0.852914,". g., provisioning content, language and duplicate detection as well as document structure recognition. (2) Semantic analysis services process a document and add information in the form of annotations, e. g., NER, temporal expression analysis, relation extraction, event detection, fake news as well as discourse analysis (Bourgonje et al., 2016b; Srivastava et al., 2016; Rehm et al., 2017b; Ostendorff et al., 2019). (3) Content generation services enable the creation of a new piece of content, e. g., summarization, paraphrasing, and semantic storytelling (Rehm et al., 2019c; Rehm et al., 2018; Moreno-Schneider et al., 2017; Rehm et al., 2017a; Schneider et al., 2017; Schneider et al., 2016). ENVISIONED SOLUTION Pilot Contracts Pilot GeoThermal Pilot Labor Law Legal Knowledge Graph Workflows Ontologies ... Smart Services Vocabulary ETL linking annotation classification Legal resources Language resources Standards Linked Data Private documents Other open data Documents Figure 6: The Lynx technology platform The platform’s microservice architecture is a variant of the service-oriented architecture (SOA), in which an application is structured as a collection of loosely coupled services. It uses Docker containers ho"
2020.iwltp-1.15,2020.iwltp-1.12,1,0.914049,"ments, yet others provide a user interface. The Document Manager provides the storage and annotation of documents with an emphasis on keeping them synchronized, providing read and write access, as well as updates of documents and annotations. It can be queried in terms of annotations and documents, through REST APIs. The interface includes a set of create, read, update, and delete APIs to manage collections, documents and annotations. The orchestration and execution of services involved in more complex tasks is addressed by a Workflow Manager. It defines combinations of services as workflows (Moreno-Schneider et al., 2020b; Bourgonje et al., 2016a; Schneider and Rehm, 2018a; Schneider and Rehm, 2018b). Workflows are described using BPMN and executed using Camunda.8 Interoperability is addressed at the following levels: Since the QURATOR platform is a closed ecosystem, the platform can be thought of as an experimental toolbox with services customised by the partners for their own use cases. As the platform is used only by the QURATOR partners, it does not contain a catalogue or any kind or structured metadata. However, two of the ten QURATOR projects have a focus on service composition and workflows with protot"
2020.iwltp-1.15,2020.lrec-1.284,1,0.886537,"Missing"
2020.iwltp-1.15,piperidis-2012-meta,1,0.875093,"t least upon a certain (obligatory) subset (Labropoulou et al., 2020; McCrae et al., 2015). Such a more detailed, semantics-driven approach enables more efficient and more user-friendly search results from multiple platforms that can be visually aggregated and also easily ranked. The actual search can be performed through publicly available APIs but returned objects would be semantically richer. Alternatively, the metadata records of external repositories can be harvested using standard protocols such as OAI-PMH, which allow the construction of a master index out of decentralised inventories (Piperidis, 2012). A known issue that needs to be addressed using such an approach involves the detection of duplicate resources. Figure 9: A cross-platform workflow example A similar approach was implemented in the project OpenMinTeD (OMTD) (Labropoulou et al., 2018) using the Galaxy workflow management system.10 Three types of LT components are supported: (1) components packaged in Docker images that follow the OMTD specifications; (2) components wrapped with UIMA or GATE, available in a Maven repository; (3) Text and Data Mining web services that run outside the OMTD platform and that follow the OMTD specif"
2020.iwltp-1.15,L18-1519,1,0.396376,"ed in AI4EU Experiments. 2.2. European Language Grid (ELG) Multilingualism and cross-lingual communication in Europe can only be enabled through Language Technologies (LTs) (Rehm et al., 2016). The European LT landscape is fragmented (Vasiljevs et al., 2019), holding back its impact. Another crucial issue is that many languages are underresourced and, thus, in danger of digital extinction (Rehm and Uszkoreit, 2012; Kornai, 2013; Rehm et al., 2014). There is an enormous need for an European LT platform as a unifying umbrella (Rehm and Uszkoreit, 2013; Rehm et al., 2016; STOA, 2017; Rehm, 2017; Rehm and Hegele, 2018; European Parliament, 2018; Rehm et al., 2020c). The project European Language Grid (2019-2021) attempts to establish the primary platform and marketplace for the European LT community, both industry and research (Rehm et al., 2020a). This scalable cloud platform will provide access to hundreds of LTs for all European languages, including running services as well as data sets. ELG will enable the European LT community to upload their technologies and data sets, to deploy them, and to connect with other resources. ELG caters for commercial and non-commercial LTs (i. e., LTs with a high Technol"
2020.iwltp-1.15,rehm-etal-2008-ontology,1,0.781144,"Missing"
2020.iwltp-1.15,W17-2707,1,0.837412,"road groups: (1) Preprocessing encompasses services for obtaining and processing information from different content sources so that they can be used in the platform and integrated into other services (Schneider et al., 2018), e. g., provisioning content, language and duplicate detection as well as document structure recognition. (2) Semantic analysis services process a document and add information in the form of annotations, e. g., NER, temporal expression analysis, relation extraction, event detection, fake news as well as discourse analysis (Bourgonje et al., 2016b; Srivastava et al., 2016; Rehm et al., 2017b; Ostendorff et al., 2019). (3) Content generation services enable the creation of a new piece of content, e. g., summarization, paraphrasing, and semantic storytelling (Rehm et al., 2019c; Rehm et al., 2018; Moreno-Schneider et al., 2017; Rehm et al., 2017a; Schneider et al., 2017; Schneider et al., 2016). ENVISIONED SOLUTION Pilot Contracts Pilot GeoThermal Pilot Labor Law Legal Knowledge Graph Workflows Ontologies ... Smart Services Vocabulary ETL linking annotation classification Legal resources Language resources Standards Linked Data Private documents Other open data Documents Figure 6:"
2020.iwltp-1.15,2020.lrec-1.413,1,0.802365,"Missing"
2020.iwltp-1.15,2016.tc-1.14,1,0.737947,"n be divided into three broad groups: (1) Preprocessing encompasses services for obtaining and processing information from different content sources so that they can be used in the platform and integrated into other services (Schneider et al., 2018), e. g., provisioning content, language and duplicate detection as well as document structure recognition. (2) Semantic analysis services process a document and add information in the form of annotations, e. g., NER, temporal expression analysis, relation extraction, event detection, fake news as well as discourse analysis (Bourgonje et al., 2016b; Srivastava et al., 2016; Rehm et al., 2017b; Ostendorff et al., 2019). (3) Content generation services enable the creation of a new piece of content, e. g., summarization, paraphrasing, and semantic storytelling (Rehm et al., 2019c; Rehm et al., 2018; Moreno-Schneider et al., 2017; Rehm et al., 2017a; Schneider et al., 2017; Schneider et al., 2016). ENVISIONED SOLUTION Pilot Contracts Pilot GeoThermal Pilot Labor Law Legal Knowledge Graph Workflows Ontologies ... Smart Services Vocabulary ETL linking annotation classification Legal resources Language resources Standards Linked Data Private documents Other open data"
2020.iwltp-1.2,L16-1707,1,0.790295,"e applications tailored to the needs of linguists, lexicographers, researchers in NLP and knowledge engineering. Promising approaches in this direction do exist: Existing tools can be complemented with an RDF layer to facilitate their interoperability. Likewise, LLOD-native applications are possible, e.g., to use RDFa (RDF in attributes) (Herman et al., 2015) to complement an XML workflow with SPARQL-based semantic search by means of web services (Sabine Tittel and Chiarcos, 2018), to provide aggregation, enrichment and search routines for language resource metadata (McCrae and Cimiano, 2015; Chiarcos et al., 2016), to use RDF as a formalism for annotation integration and data management (Burchardt et al., 2008; Chiarcos et al., 2017), or to use RDF and SPARQL for manipulating and evaluating linguistic annotations (Chiarcos et al., 2018b; Chiarcos et al., 2018a). While these applications demonstrate the potential of LOD technology in linguistics, they come with a considerable entry barrier and they address the advanced user of RDF technology rather than a typical linguist. Even though concrete applications to exist, a long way is still to go to achieve the level of user-friendliness expected by occasion"
2020.iwltp-1.2,L18-1717,1,0.852139,"e their interoperability. Likewise, LLOD-native applications are possible, e.g., to use RDFa (RDF in attributes) (Herman et al., 2015) to complement an XML workflow with SPARQL-based semantic search by means of web services (Sabine Tittel and Chiarcos, 2018), to provide aggregation, enrichment and search routines for language resource metadata (McCrae and Cimiano, 2015; Chiarcos et al., 2016), to use RDF as a formalism for annotation integration and data management (Burchardt et al., 2008; Chiarcos et al., 2017), or to use RDF and SPARQL for manipulating and evaluating linguistic annotations (Chiarcos et al., 2018b; Chiarcos et al., 2018a). While these applications demonstrate the potential of LOD technology in linguistics, they come with a considerable entry barrier and they address the advanced user of RDF technology rather than a typical linguist. Even though concrete applications to exist, a long way is still to go to achieve the level of user-friendliness expected by occasional users of this technology. A notable exception in this regard is LexO (Bellandi et al., 2017), which is a graphical tool for the collaborative editing of lexical and ontological resources natively building on the OntoLex voc"
2020.iwltp-1.2,2020.lrec-1.395,1,0.811257,"Missing"
2020.iwltp-1.2,W17-7010,0,0.0379115,"t (Burchardt et al., 2008; Chiarcos et al., 2017), or to use RDF and SPARQL for manipulating and evaluating linguistic annotations (Chiarcos et al., 2018b; Chiarcos et al., 2018a). While these applications demonstrate the potential of LOD technology in linguistics, they come with a considerable entry barrier and they address the advanced user of RDF technology rather than a typical linguist. Even though concrete applications to exist, a long way is still to go to achieve the level of user-friendliness expected by occasional users of this technology. A notable exception in this regard is LexO (Bellandi et al., 2017), which is a graphical tool for the collaborative editing of lexical and ontological resources natively building on the OntoLex vocabulary and RDF, designed to conduct lexicographical work in a philological context (i.e., creating the Dictionnaire des Termes Médico-botaniques de l’Ancien Occitan). Other projects whose objective is to provide LLOD-based tools for specific areas of application have been recently approved, so that progress in this direction is to be expected within the next years. Ten years after the formation of the OWLG, the situation of linked data in language technology and l"
2020.iwltp-1.2,I08-1051,0,0.0564092,"e engineering. Promising approaches in this direction do exist: Existing tools can be complemented with an RDF layer to facilitate their interoperability. Likewise, LLOD-native applications are possible, e.g., to use RDFa (RDF in attributes) (Herman et al., 2015) to complement an XML workflow with SPARQL-based semantic search by means of web services (Sabine Tittel and Chiarcos, 2018), to provide aggregation, enrichment and search routines for language resource metadata (McCrae and Cimiano, 2015; Chiarcos et al., 2016), to use RDF as a formalism for annotation integration and data management (Burchardt et al., 2008; Chiarcos et al., 2017), or to use RDF and SPARQL for manipulating and evaluating linguistic annotations (Chiarcos et al., 2018b; Chiarcos et al., 2018a). While these applications demonstrate the potential of LOD technology in linguistics, they come with a considerable entry barrier and they address the advanced user of RDF technology rather than a typical linguist. Even though concrete applications to exist, a long way is still to go to achieve the level of user-friendliness expected by occasional users of this technology. A notable exception in this regard is LexO (Bellandi et al., 2017), w"
2020.iwltp-1.2,W07-1501,0,0.0696464,"ersion should be provided. Ecosystem RDF as a data exchange framework is maintained by an interdisciplinary, large and active community, and it comes with a developed infrastructure that provides APIs, database implementations, technical support and validators for various RDF-based languages, e.g., reasoners for OWL. For developers of linguistic resources, this ecosystem can provide technological support or off-the-shelf implementations for common problems, e.g., the development of a database that is capable of support flexible, graphbased data structures as necessary for multi-layer corpora (Ide and Suderman, 2007). Linked (Open) Data for Language Resources Publishing Linked Data allows resources to be globally and uniquely identified such that they can be retrieved through standard Web protocols. Moreover, resources can be easily linked to one another in a uniform fashion and thus become structurally interoperable. (Chiarcos et al., 2013) identified the five main benefits of Linked Data for Linguistics and NLP: Conceptual Interoperability Semantic Web technologies allow to provide, to maintain and to share centralized, but freely accessible terminology repositories. Reference to such terminology reposi"
2020.iwltp-1.2,wright-2004-global,0,0.123506,"ly referenced from any other resource on the Web through URIs. Similar to hyperlinks in the HTML web, the web of data created by these links allows navigation along these connections, and thereby to freely integrate information from different resources in the cloud. Dynamic Import When linguistic resources are interlinked by references to resolvable URIs instead of system-defined IDs (or static copies of parts from another resource), we always provide access to the most recent version of a resource. For communitymaintained terminology repositories like the ISO TC37/SC4 Data Category Registry (Wright, 2004; Windhouwer and Wright, 2012), for example, new categories, definitions or examples can be introduced occasionally, and this information is available immediately to anyone whose resources refer to ISOcat URIs. In order to preserve link consistency among Linguistic Linked Open Data resources, however, it is strongly advised to apply a proper versioning system such that backward-compatibility can be preserved: Adding concepts or examples is unproblematic, but when concepts are deleted, renamed or redefined, a new version should be provided. Ecosystem RDF as a data exchange framework is maintain"
2020.ldl-1.7,P06-1014,0,0.0672568,"ure used in the paper versions. The use of semantic standards enables the organization of vast amounts of lexical data in ontologies, Wordnets and other machine-readable lexical resources resorting to novel tools for the transformation and linking of multilingual datasets (McCrae and Declerck, 2019; Chiarcos et al., 2012). Linked Open Data (LLOD) promotes the use of the RDF data model to publish lexical data on the web for a global information system and interoperability issues. There have been many efforts underway on behalf of numerous researchers to align different lexical resources (e.g. (Navigli, 2006; Knight and Luk, 1994) dealing with the word sense alignment (WSA) task. We define this task as linking a list of pairs of senses from two or more lexical resources using semantic relationships. To mention a few previous projects, Meyer and Gurevych (2011) align the Princeton WordNet with the English Wiktionary1 , and Henrich et al. (2012) link the GermaNet–the German Wordnet with the German Wikipedia2 . WSA involves searching for matching senses within dictionary entries of different lexical resources and linking them, which poses significant challenges. The lexicographic criteria are not al"
2020.ldl-1.7,2020.lrec-1.395,1,0.729702,"Missing"
2020.lrec-1.390,P13-1133,1,0.892147,"Abstract In this paper we discuss the experience of bringing together over 40 different wordnets. We introduce some extensions to the GWA wordnet LMF format proposed in Vossen et al. (2016) and look at how this new information can be displayed. Notable extensions include: confidence, corpus frequency, orthographic variants, lexicalized and non-lexicalized synsets and lemmas, new parts of speech, and more. Many of these extensions already exist in multiple wordnets – the challenge was to find a compatible representation. To this end, we introduce a new version of the Open Multilingual Wordnet (Bond and Foster, 2013), that integrates a new set of tools that tests the extensions introduced by this new format, while also ensuring the integrity of the Collaborative Interlingual Index (CILI: Bond et al., 2016), avoiding the same new concept to be introduced through multiple projects. Keywords: multilingual lexicon, wordnet, collaborative development 1. Introduction This paper provides a summary and update of some of the issues involved with coordinating multiple lexical wordnets. The Princeton WordNet (PWN) is one of the most cited lexical resources in the world with over 1.8 as many citations as the most cit"
2020.lrec-1.390,2016.gwc-1.9,1,0.935642,"k at how this new information can be displayed. Notable extensions include: confidence, corpus frequency, orthographic variants, lexicalized and non-lexicalized synsets and lemmas, new parts of speech, and more. Many of these extensions already exist in multiple wordnets – the challenge was to find a compatible representation. To this end, we introduce a new version of the Open Multilingual Wordnet (Bond and Foster, 2013), that integrates a new set of tools that tests the extensions introduced by this new format, while also ensuring the integrity of the Collaborative Interlingual Index (CILI: Bond et al., 2016), avoiding the same new concept to be introduced through multiple projects. Keywords: multilingual lexicon, wordnet, collaborative development 1. Introduction This paper provides a summary and update of some of the issues involved with coordinating multiple lexical wordnets. The Princeton WordNet (PWN) is one of the most cited lexical resources in the world with over 1.8 as many citations as the most cited paper in the ACL anthology1 (Marcus et al. (2004) according to Joseph and Radev (2007)). This success has inspired wordnets in many languages, and many attempts to link them, such as EuroWor"
2020.lrec-1.390,2019.gwc-1.50,1,0.722496,"lems, then the projects will have the option to upload it onto the OMW system, which will make it immediately available on its online interface. This automated validation effort, though sometimes challenging to set up, has been a great way to catch problems that would otherwise most certainly be overlooked in our previous system. 2.3. Graph Checks When we attempted to use the OMW for sense disambiguation, it turned out that it had cycles. This led us to check the individual wordnets, and we found that some wordnets (including PWN 3.0!) had ill-formed graph structures such as loops and cycles (Lohk et al., 2019). These are not caught by the XML structure, but make the wordnet graph impossible to manage, so we added checks for these, and pro3191 Figure 2: Screenshot of Stage 3 of the Validation Report vided feedback to all wordnets for which we found errors. These checks are now done after upload as part of the validation, and must be passed for the wordnet to be accepted into OMW 2.0. We check for three things. The first is loops: does a synset in the new wordnet link to itself (using any semantic relation). The second is cycles in the hypernym graph of the new wordnet: if we claim A is-a B, B is-a C"
2020.lrec-1.390,W14-0116,0,0.0223601,"lexicalized=false in English. If a sense has lexicalized=true then it has been validated in some standard lexicon for the language. If it has lexicalized=false, then it is believed to be compositional and only added as an aid to multilingual users (similar to phrase in multiwordnet). For example harimau anak “young tiger” in the Indonesian synset for tiger cub is lexicalized=false, or dedos pedas “foot finger-andtoe” in the synset for toe in Spanish. These allow the lexicographers to put in useful translation equivalents while acknowledging that they are not necessarily part of the language. Vincze and Almázi (2014) discuss other synsets that may not be lexicalized in Hungarian, such as place names or culturally specific concepts. 4. Duplicate Sense Detection One difficult challenge with integrating many wordnets of different languages is that they may define identical concepts and as such this would lead to duplicates in the CILI index. As such, we have introduced a system for duplicate sense detection based on the Naisc system introduced by McCrae and Buitelaar (2018). This system is intended for dataset linking and is being specialized for sense linking in the ELEXIS project (Krek et al., 2018). In th"
2020.lrec-1.390,W99-0512,0,0.404156,"that most existing wordnets are built by translating the PWN: the extend model (Vossen, 1998). For example, dogn:1 is linked to the lemmas chien in French, anjing in Malay, and so on. The overall structure of PWN serves as a useful scaffold and the fact that, for example, a dogn:1 is an animaln:1 is language independent. The main innovations of the OMW 1.0 were an emphasis on open licenses (so that all the data could be legally shared) and a simple shared format (so that resources could be easily converted). The second version of the OMW (2.0) revived the idea of the InterLingual Index (ILI: Vossen et al., 1999) with the Collaborative Interlingual Index (CILI: Bond et al., 2016). In this vision, there is a shared set of concepts, which the wordnets agree to link to. In the Collaborative ILI, new wordnet projects can propose candidate ILI concepts, instantiated by synsets in the wordnet for that language. In this paper, we introduce other information added to the Open-Multilingual Wordnet and the motivation for it. The structure of the paper is as follows: in § 2 we talk about challenges in the process of integrating the wordnets; in § 3 we look at how the wordnet format has been extended; in § 4 we l"
2020.lrec-1.395,I13-1057,0,0.0253544,"reated dynamically for semantic relationship annotation. scribed our methodology in Section 3, we further elaborate on the challenges of sense annotation in Section 4. We evaluate the datasets in Section 5 and finally, conclude the paper in Section 6. 2. Related work Aligning senses across lexical resources has been attempted in several lexicographical milieus over the recent years. Such resources mainly include open-source dictionaries, WordNet and collaboratively-curated resources, such as Wikipedia. The latter has been shown to be reliable resources to construct accurate sense classifiers (Dandala et al., 2013). There has been a significant body of research in aligning English resources, particularly, Princeton WordNet with Wikipedia (including (Ruiz-Casado et al., 2005; Ponzetto and Navigli, 2010; Niemann and Gurevych, 2011; McCrae, 2018)), with the Longman Dictionary of Contemporary English and with Roget’s thesaurus (Kwong, 1998), with Wiktionary3 (Meyer and Gurevych, 2011) or with the Oxford Dictionary of English (Navigli, 2006). Meyer and Gurevych (2011) also present a manually-annotated dataset for WSA between the English WordNet and Wiktionary. On the other hand, there are a fewer number of m"
2020.lrec-1.395,erjavec-fiser-2006-building,0,0.137609,"Missing"
2020.lrec-1.395,E12-1059,0,0.155275,"geneity in content, which makes aligning information across resources and languages a challenging task. Word sense alignment (WSA) is a more specific task of linking dictionary content at sense level which has been proved to be beneficial in various NLP tasks, such as wordsense disambiguation (Navigli and Ponzetto, 2012), semantic role labeling (Palmer, 2009) and information extraction (Moro et al., 2013). Moreover, combining LSRs can enhance domain coverage in terms of the number of lexical items and types of lexical-semantic information (Shi and 1 Mihalcea, 2005; Ponzetto and Navigli, 2010; Gurevych et al., 2012). Given the current progress of artificial intelligence and the usage of data to train neural networks, annotated data with specific features play a crucial role to tackle data-driven challenges, particularly in NLP. In recent years, a few efforts have been made to create gold-standard dataset, i.e., a dataset of instances used for learning and fitting parameters, for aligning senses across monolingual resources including collaboratively-curated ones such as Wikipedia2 , and expert-made ones such as WordNet. However, the previous work is limited to a handful of languages and much of it is not"
2020.lrec-1.395,W97-0800,0,0.569498,"2011; McCrae, 2018)), with the Longman Dictionary of Contemporary English and with Roget’s thesaurus (Kwong, 1998), with Wiktionary3 (Meyer and Gurevych, 2011) or with the Oxford Dictionary of English (Navigli, 2006). Meyer and Gurevych (2011) also present a manually-annotated dataset for WSA between the English WordNet and Wiktionary. On the other hand, there are a fewer number of manually aligned monolingual resources in other languages. For instance, there have been considerable efforts in aligning lexical semantic resources (LSRs) in German, particularly, the GermaNet–the German Wordnet (Hamp and Feldweg, 1997) with the German Wiktionary (Henrich et al., 2011), with the German Wikipedia (Henrich et al., 2012) and with the Digital Dictionary of the German Language (Digitales W¨orterbuch der Deutschen Sprache (Klein and Geyken, 2010)) (Henrich et al., 2014). Gurevych et al. (2012) present UKB–a large-scale lexical-semantic resource containing pairwise sense alignments between a subset of nine resources in English and German which are mapped to a uniform representation. For Danish, aligning senses across modern lexical resources has been carried out in several projects in recent years (Pedersen et al.,"
2020.lrec-1.395,W14-0109,0,0.0200074,"resent a manually-annotated dataset for WSA between the English WordNet and Wiktionary. On the other hand, there are a fewer number of manually aligned monolingual resources in other languages. For instance, there have been considerable efforts in aligning lexical semantic resources (LSRs) in German, particularly, the GermaNet–the German Wordnet (Hamp and Feldweg, 1997) with the German Wiktionary (Henrich et al., 2011), with the German Wikipedia (Henrich et al., 2012) and with the Digital Dictionary of the German Language (Digitales W¨orterbuch der Deutschen Sprache (Klein and Geyken, 2010)) (Henrich et al., 2014). Gurevych et al. (2012) present UKB–a large-scale lexical-semantic resource containing pairwise sense alignments between a subset of nine resources in English and German which are mapped to a uniform representation. For Danish, aligning senses across modern lexical resources has been carried out in several projects in recent years (Pedersen et al., 2018), and a next natural step is to link these to historical Danish dictionaries. 3 https://www.wiktionary.org/ Pedersen et al. (2009) describe the semi-automatic compilation of a WordNet for Danish, DanNet, based on a monolingual dictionary, the"
2020.lrec-1.395,bel-etal-2000-simple,1,0.672745,"Missing"
2020.lrec-1.395,Q13-1013,0,0.110061,"nment of LSRs and applied it to the production of a three-way alignment of the English WordNet, Wikipedia and Wiktionary. Niemann and Gurevych (2011) propose a threshold-based Personalized PageRank method for extracting a set of Wikipedia articles as alignment candidates and automatically aligning them with WordNet synsets. This method yields a sense inventory of higher coverage in comparison to taxonomy mapping techniques where Wikipedia categories are aligned to WordNet synsets (Ponzetto and Navigli, 2009). Matuschek and Gurevych present the Dijkstra-WSA algorithm as a graph-based approach (Matuschek and Gurevych, 2013) and a machine learning approach where features such as sense distances and gloss similarities are used for the task of WSA (Matuschek and Gurevych, 2014). It should be noted that all of these approaches produce results that are of lower reliability than gold standard datasets such as the ones presented in this paper. 3233 3. Methodology The main goal of the current study is to provide semantic relationships between two sets of senses for the same lemmas in two monolingual dictionaries. As an example, Figure 1 illustrates the senses for the entry “clog” (verb) in the English WordNet (Miller, 1"
2020.lrec-1.395,C14-1025,0,0.31488,"ose a threshold-based Personalized PageRank method for extracting a set of Wikipedia articles as alignment candidates and automatically aligning them with WordNet synsets. This method yields a sense inventory of higher coverage in comparison to taxonomy mapping techniques where Wikipedia categories are aligned to WordNet synsets (Ponzetto and Navigli, 2009). Matuschek and Gurevych present the Dijkstra-WSA algorithm as a graph-based approach (Matuschek and Gurevych, 2013) and a machine learning approach where features such as sense distances and gloss similarities are used for the task of WSA (Matuschek and Gurevych, 2014). It should be noted that all of these approaches produce results that are of lower reliability than gold standard datasets such as the ones presented in this paper. 3233 3. Methodology The main goal of the current study is to provide semantic relationships between two sets of senses for the same lemmas in two monolingual dictionaries. As an example, Figure 1 illustrates the senses for the entry “clog” (verb) in the English WordNet (Miller, 1995) (left) and the Webster’s Dictionary 1913 (Webster and Slater, 1828) (right). For further clarification, we provide two case studies of Danish and Ita"
2020.lrec-1.395,2018.gwc-1.8,1,0.745977,"Section 6. 2. Related work Aligning senses across lexical resources has been attempted in several lexicographical milieus over the recent years. Such resources mainly include open-source dictionaries, WordNet and collaboratively-curated resources, such as Wikipedia. The latter has been shown to be reliable resources to construct accurate sense classifiers (Dandala et al., 2013). There has been a significant body of research in aligning English resources, particularly, Princeton WordNet with Wikipedia (including (Ruiz-Casado et al., 2005; Ponzetto and Navigli, 2010; Niemann and Gurevych, 2011; McCrae, 2018)), with the Longman Dictionary of Contemporary English and with Roget’s thesaurus (Kwong, 1998), with Wiktionary3 (Meyer and Gurevych, 2011) or with the Oxford Dictionary of English (Navigli, 2006). Meyer and Gurevych (2011) also present a manually-annotated dataset for WSA between the English WordNet and Wiktionary. On the other hand, there are a fewer number of manually aligned monolingual resources in other languages. For instance, there have been considerable efforts in aligning lexical semantic resources (LSRs) in German, particularly, the GermaNet–the German Wordnet (Hamp and Feldweg, 19"
2020.lrec-1.395,I11-1099,0,0.152048,"he recent years. Such resources mainly include open-source dictionaries, WordNet and collaboratively-curated resources, such as Wikipedia. The latter has been shown to be reliable resources to construct accurate sense classifiers (Dandala et al., 2013). There has been a significant body of research in aligning English resources, particularly, Princeton WordNet with Wikipedia (including (Ruiz-Casado et al., 2005; Ponzetto and Navigli, 2010; Niemann and Gurevych, 2011; McCrae, 2018)), with the Longman Dictionary of Contemporary English and with Roget’s thesaurus (Kwong, 1998), with Wiktionary3 (Meyer and Gurevych, 2011) or with the Oxford Dictionary of English (Navigli, 2006). Meyer and Gurevych (2011) also present a manually-annotated dataset for WSA between the English WordNet and Wiktionary. On the other hand, there are a fewer number of manually aligned monolingual resources in other languages. For instance, there have been considerable efforts in aligning lexical semantic resources (LSRs) in German, particularly, the GermaNet–the German Wordnet (Hamp and Feldweg, 1997) with the German Wiktionary (Henrich et al., 2011), with the German Wikipedia (Henrich et al., 2012) and with the Digital Dictionary of t"
2020.lrec-1.395,miller-gurevych-2014-wordnet,0,0.125981,"Missing"
2020.lrec-1.395,P06-1014,0,0.254458,"ies, WordNet and collaboratively-curated resources, such as Wikipedia. The latter has been shown to be reliable resources to construct accurate sense classifiers (Dandala et al., 2013). There has been a significant body of research in aligning English resources, particularly, Princeton WordNet with Wikipedia (including (Ruiz-Casado et al., 2005; Ponzetto and Navigli, 2010; Niemann and Gurevych, 2011; McCrae, 2018)), with the Longman Dictionary of Contemporary English and with Roget’s thesaurus (Kwong, 1998), with Wiktionary3 (Meyer and Gurevych, 2011) or with the Oxford Dictionary of English (Navigli, 2006). Meyer and Gurevych (2011) also present a manually-annotated dataset for WSA between the English WordNet and Wiktionary. On the other hand, there are a fewer number of manually aligned monolingual resources in other languages. For instance, there have been considerable efforts in aligning lexical semantic resources (LSRs) in German, particularly, the GermaNet–the German Wordnet (Hamp and Feldweg, 1997) with the German Wiktionary (Henrich et al., 2011), with the German Wikipedia (Henrich et al., 2012) and with the Digital Dictionary of the German Language (Digitales W¨orterbuch der Deutschen S"
2020.lrec-1.395,W11-0122,0,0.627617,"ally, conclude the paper in Section 6. 2. Related work Aligning senses across lexical resources has been attempted in several lexicographical milieus over the recent years. Such resources mainly include open-source dictionaries, WordNet and collaboratively-curated resources, such as Wikipedia. The latter has been shown to be reliable resources to construct accurate sense classifiers (Dandala et al., 2013). There has been a significant body of research in aligning English resources, particularly, Princeton WordNet with Wikipedia (including (Ruiz-Casado et al., 2005; Ponzetto and Navigli, 2010; Niemann and Gurevych, 2011; McCrae, 2018)), with the Longman Dictionary of Contemporary English and with Roget’s thesaurus (Kwong, 1998), with Wiktionary3 (Meyer and Gurevych, 2011) or with the Oxford Dictionary of English (Navigli, 2006). Meyer and Gurevych (2011) also present a manually-annotated dataset for WSA between the English WordNet and Wiktionary. On the other hand, there are a fewer number of manually aligned monolingual resources in other languages. For instance, there have been considerable efforts in aligning lexical semantic resources (LSRs) in German, particularly, the GermaNet–the German Wordnet (Hamp"
2020.lrec-1.395,P10-1154,0,0.289699,"nces in structure and heterogeneity in content, which makes aligning information across resources and languages a challenging task. Word sense alignment (WSA) is a more specific task of linking dictionary content at sense level which has been proved to be beneficial in various NLP tasks, such as wordsense disambiguation (Navigli and Ponzetto, 2012), semantic role labeling (Palmer, 2009) and information extraction (Moro et al., 2013). Moreover, combining LSRs can enhance domain coverage in terms of the number of lexical items and types of lexical-semantic information (Shi and 1 Mihalcea, 2005; Ponzetto and Navigli, 2010; Gurevych et al., 2012). Given the current progress of artificial intelligence and the usage of data to train neural networks, annotated data with specific features play a crucial role to tackle data-driven challenges, particularly in NLP. In recent years, a few efforts have been made to create gold-standard dataset, i.e., a dataset of instances used for learning and fitting parameters, for aligning senses across monolingual resources including collaboratively-curated ones such as Wikipedia2 , and expert-made ones such as WordNet. However, the previous work is limited to a handful of language"
2020.lrec-1.395,roventini-ruimy-2008-mapping,0,0.0409542,"the semantic level of a quadripartite Italian lexicon. Its structure is inspired by Generative Lexicon theory (Pustejovsky, 1995) and in particular the notion of qualia structure which is used to organise the Semantic Units (SemUs) which constitute the basic structures representing word-sense. SIMPLE contains 20,000 SemUs and we used the definitions of these SemUs for the task. Both lexicons share a set of common “base concepts” that provided the basis of a previous (semi-)automatic mapping of the two lexicons on the basis of their respective ontological organisations (Roventini et al., 2007; Roventini and Ruimy, 2008). Although this mapping did not make the five-fold distinction, i.e., exact, narrower, broader, related, and none, it did constitute a useful starting point and a basis for comparison for the task. The teams that had originally compiled IWN and SIMPLE shared many members in common and so, the definitions for corresponding senses across the two lexicons are sometimes very similar or differ solely on the basis of an extra clause. This made it easy to determine, in many cases, if two senses were ‘exact’ matches or if one was ‘broader’ or ‘narrower’ than the other by just comparing strings. The ap"
2020.lrec-1.395,roventini-etal-2000-italwordnet,0,0.0791879,"Missing"
2020.lrec-1.395,roventini-etal-2002-integrating,0,0.0935961,"n incapacitated adult). An opposite case where the historical sense is ‘narrower’ than the modern one can be illustrated by the adjective spids (‘sharp’) where ODS describes two specific senses, one about sound and another one about smell, while DDO merges the two senses into one: ‘pungent in an unpleasant way (about smell, taste or sound)’. 4.2. ItalWordNet and SIMPLE Regarding Italian, the team at ILC-CNR chose ItalWordNet (IWN) and SIMPLE, two Italian language lexical resources which had been previously developed in the institute. The former, IWN, is a lexical semantic network for Italian (Roventini et al., 2002) which is part of the WordNet family (Miller, 1995). As such it is organised around the notion of a synset of word senses and the network structure based on lexical-semantic relations which hold between senses across synsets. The 50,000 Italian synsets contained in IWN are linked to the Princeton Wordnet. The latter resource, SIMPLE, constitutes the semantic level of a quadripartite Italian lexicon. Its structure is inspired by Generative Lexicon theory (Pustejovsky, 1995) and in particular the notion of qualia structure which is used to organise the Semantic Units (SemUs) which constitute the"
2020.lrec-1.395,P07-2041,0,0.0611602,"ce, SIMPLE, constitutes the semantic level of a quadripartite Italian lexicon. Its structure is inspired by Generative Lexicon theory (Pustejovsky, 1995) and in particular the notion of qualia structure which is used to organise the Semantic Units (SemUs) which constitute the basic structures representing word-sense. SIMPLE contains 20,000 SemUs and we used the definitions of these SemUs for the task. Both lexicons share a set of common “base concepts” that provided the basis of a previous (semi-)automatic mapping of the two lexicons on the basis of their respective ontological organisations (Roventini et al., 2007; Roventini and Ruimy, 2008). Although this mapping did not make the five-fold distinction, i.e., exact, narrower, broader, related, and none, it did constitute a useful starting point and a basis for comparison for the task. The teams that had originally compiled IWN and SIMPLE shared many members in common and so, the definitions for corresponding senses across the two lexicons are sometimes very similar or differ solely on the basis of an extra clause. This made it easy to determine, in many cases, if two senses were ‘exact’ matches or if one was ‘broader’ or ‘narrower’ than the other by ju"
2020.lrec-1.395,2019.gwc-1.37,1,0.782709,"onary, the Danish Dictionary (Den Danske Ordbog (DDO)). Later, the semantic links between these two resources facilitated the compilation of a comprehensive thesaurus (Den Danske Begrebsordbog) (Nimb et al., 2014). The semantic links between thesaurus and dictionary made it possible to combine verb groups and dictionary valency information, used as input for the compilation of the Danish FrameNet Lexicon (Nimb, 2018). Furthermore, they constitute the basis for the automatically integrated information on related words in DDO, on the fly for each dictionary sense (Nimb et al., 2018). Similarly, Simov et al. (2019) report the manual mapping of the Bulgarian Word-Net BTB-WN with the Bulgarian Wikipedia. Given the amount of the effort required to construct and maintain expert-made resources, various solutions have been proposed to automatically link and merge existing LSRs at different levels. LSRs being very diverse in domain coverage (Meyer, 2010; Burgun and Bodenreider, 2001), previous works have focused on methods to increase domain coverage, enrich sense representations and decrease sense granularity (Miller, 2016). Miller and Gurevych (2014) describe a technique for constructing an n-way alignment o"
2020.lrec-1.695,P18-1073,0,0.016647,"ersion of 5 terminological resources in TBX to RDF. 5.3. Linking Finally, the project is developing (semi-)automated linking mechanisms. This concerns both the conceptual level of language descriptions as also the lexical data. We are working both in a mono- and in a cross-lingual set up. Since this work is still in progress, at this time we can only report on preliminary approaches. In the context of cross-lingual concept matching, we are updating an already existent ontology matching tool, CIDERCL (Gracia and Asooja, 2013) with contemporary techniques based on cross-lingual word embeddings (Artetxe et al., 2018). Regarding linking cross-lingual lexical data, the project has laid the groundwork for research in the topic of ”translation inference across dictionaries” by organising the TIAD’19 shared task (Gracia et al., 2019), in which a benchmark and evaluation framework were provided to allow for systematic comparisons between systems. Such systems were able to infer indirect translations between language pairs that were initially disconnected in the Apertium RDF graph (Gracia et al., 2018), showing promising results but also the need of further research. Ontology lexicalisation aims at developing te"
2020.lrec-1.695,2019.gwc-1.34,1,0.826147,"Missing"
2020.lrec-1.695,E09-2008,0,0.0376885,"ons for morphology (Klimek et al., 2019) and the representation of frequency, attestation and corpus information (Chiarcos and Ionov, 2019). The morphology module is specifically important for the cross-linguistic applicability of OntoLex-Lemon, as it aims to support languages with a lot of stem internal alternations: By using regular expressions to represent morphology generation rules, it provides implementation-independent means to generate inflected forms from lemma information, that can be subsequently incorporated in conventional morphology frameworks such as XFST (Ranta, 1998) or FOMA (Hulden, 2009). Specifications for phonological processes and mor3 See https://www.w3.org/2016/05/ontolex/ See (McCrae et al., 2012) and (Cimiano et al., 2016). 5 SKOS stands for “Simple Knowledge Organization System”. SKOS provides “a model for expressing the basic structure and content of concept schemes such as thesauri, classification schemes, subject heading lists, taxonomies, folksonomies, and other similar types of controlled vocabulary” (https://www. w3.org/TR/skos-primer/). 6 See http://www.elex.is/ for more detail. 4 Figure 2: The core Modules of OntoLex-Lemon. Graphic taken from https://www.w3.or"
2020.lrec-1.695,L16-1386,1,0.78537,"Missing"
2020.lrec-1.695,W98-1308,0,0.29918,"d emerging specifications for morphology (Klimek et al., 2019) and the representation of frequency, attestation and corpus information (Chiarcos and Ionov, 2019). The morphology module is specifically important for the cross-linguistic applicability of OntoLex-Lemon, as it aims to support languages with a lot of stem internal alternations: By using regular expressions to represent morphology generation rules, it provides implementation-independent means to generate inflected forms from lemma information, that can be subsequently incorporated in conventional morphology frameworks such as XFST (Ranta, 1998) or FOMA (Hulden, 2009). Specifications for phonological processes and mor3 See https://www.w3.org/2016/05/ontolex/ See (McCrae et al., 2012) and (Cimiano et al., 2016). 5 SKOS stands for “Simple Knowledge Organization System”. SKOS provides “a model for expressing the basic structure and content of concept schemes such as thesauri, classification schemes, subject heading lists, taxonomies, folksonomies, and other similar types of controlled vocabulary” (https://www. w3.org/TR/skos-primer/). 6 See http://www.elex.is/ for more detail. 4 Figure 2: The core Modules of OntoLex-Lemon. Graphic taken"
2020.lrec-1.712,C18-1021,0,0.0135773,"ession is provided (Martin, 1990) in a way similar to dictionaries or lexicons. Table 1 gives examples of the three aforementioned approaches of metaphor interpretation. The choice of the approach depends on the application. In this work, we view metaphor interpretation as a definition generation (explanation) task focusing on finding out the meaning of a given metaphoric expression and explain it in literal words. There are a variety of applications that can benefit from interpreting metaphors, including language learning and text simplification (Barbu et al., 2015; Wolska and Clausen, 2017; Bingel et al., 2018) as well as lexical resources creation and development (Krek et al., 2018). Approach lexical substitution (Shutova et al., 2010) Metaphor brush aside accusation Interpretation reject paraphrase generation (Bizzoni and Lappin, 2018) The crowd was a river in the street. The crowd was large and impetuous in the street. definition generation (Martin, 1990) How do I kill the process? to terminate computer process. Table 1: Metaphor interpretation approaches with examples from previous studies. Manually annotating a dataset for metaphor interpretation (either to provide a definition/explanation or t"
2020.lrec-1.712,W18-0906,0,0.0616811,"le datasets for metaphor interpretation, which in turn hinders the development of this topic. There are several approaches to address metaphor interpretation among which: 1. Lexical Substitution (lexical paraphrasing) where the metaphoric word/phrase is replaced with its literal counterpart to clarify its semantic meaning. This task is viewed as single-word (lexical) substitution (Shutova, 2010; Shutova et al., 2012; Bollegala and Shutova, 2013); 2. Paraphrase Generation (inference of meaning) where the full sentence including the metaphoric expression is transformed using more literal words (Bizzoni and Lappin, 2018); 3. Definition Generation (interpretation or definition assignment) where a full interpretation (explanation) of the metaphoric expression is provided (Martin, 1990) in a way similar to dictionaries or lexicons. Table 1 gives examples of the three aforementioned approaches of metaphor interpretation. The choice of the approach depends on the application. In this work, we view metaphor interpretation as a definition generation (explanation) task focusing on finding out the meaning of a given metaphoric expression and explain it in literal words. There are a variety of applications that can ben"
2020.lrec-1.712,S16-2003,0,0.0556793,"Missing"
2020.lrec-1.712,D14-1162,0,0.0833958,"Missing"
2020.lrec-1.712,shutova-teufel-2010-metaphor,0,0.0234301,"the automatically generated list of candidates. External knowledge resources including machine readable dictionaries and lexicons have been widely used in WSD (Ide and V´eronis, 1993; Agirre and Stevenson, 2007; Navigli, 2009). We will discuss the criteria of choosing the resources utilised in this work in Section 3.. Linguistic metaphors can be expressed in various syntactic structures. The majority of previous work focused on modelling verbal and adjectival metaphoric expression (Shutova, 2015). Corpus studies showed that verbs are the most frequent metaphorical expressions (Cameron, 2003; Shutova and Teufel, 2010) which encouraged the majority of systems pertained to metaphor processing to focus on the metaphorical usage of verbs. Thus, in this work, we focus on verb-direct object metaphoric expressions. We create our dataset of metaphor definitions by interpreting around 1,500 metaphoric expression identified in an existing tweets dataset (Zayed et al., 2019) and providing their literal meaning. To the best of our knowledge, there is no publicly available annotated dataset of this kind and we believe that this resource will be invaluable for the development and evaluation of computational models for m"
2020.lrec-1.712,C12-2109,0,0.064216,"Missing"
2020.lrec-1.712,N16-1020,0,0.194735,"Missing"
2020.lrec-1.712,N10-1147,0,0.108488,"onsciously grasp such interaction, asking a human annotator to translate such a cognitive process and interpret a metaphoric expression is a very demanding task. This is the reason behind the lack of publicly available datasets for metaphor interpretation, which in turn hinders the development of this topic. There are several approaches to address metaphor interpretation among which: 1. Lexical Substitution (lexical paraphrasing) where the metaphoric word/phrase is replaced with its literal counterpart to clarify its semantic meaning. This task is viewed as single-word (lexical) substitution (Shutova, 2010; Shutova et al., 2012; Bollegala and Shutova, 2013); 2. Paraphrase Generation (inference of meaning) where the full sentence including the metaphoric expression is transformed using more literal words (Bizzoni and Lappin, 2018); 3. Definition Generation (interpretation or definition assignment) where a full interpretation (explanation) of the metaphoric expression is provided (Martin, 1990) in a way similar to dictionaries or lexicons. Table 1 gives examples of the three aforementioned approaches of metaphor interpretation. The choice of the approach depends on the application. In this work,"
2020.lrec-1.712,J15-4002,0,0.0231285,"hat given a metaphoric verb the goal (of the human annotator) is to identify its closest (literal) meaning among the automatically generated list of candidates. External knowledge resources including machine readable dictionaries and lexicons have been widely used in WSD (Ide and V´eronis, 1993; Agirre and Stevenson, 2007; Navigli, 2009). We will discuss the criteria of choosing the resources utilised in this work in Section 3.. Linguistic metaphors can be expressed in various syntactic structures. The majority of previous work focused on modelling verbal and adjectival metaphoric expression (Shutova, 2015). Corpus studies showed that verbs are the most frequent metaphorical expressions (Cameron, 2003; Shutova and Teufel, 2010) which encouraged the majority of systems pertained to metaphor processing to focus on the metaphorical usage of verbs. Thus, in this work, we focus on verb-direct object metaphoric expressions. We create our dataset of metaphor definitions by interpreting around 1,500 metaphoric expression identified in an existing tweets dataset (Zayed et al., 2019) and providing their literal meaning. To the best of our knowledge, there is no publicly available annotated dataset of this"
2020.lrec-1.712,W17-5035,0,0.0220775,"on) of the metaphoric expression is provided (Martin, 1990) in a way similar to dictionaries or lexicons. Table 1 gives examples of the three aforementioned approaches of metaphor interpretation. The choice of the approach depends on the application. In this work, we view metaphor interpretation as a definition generation (explanation) task focusing on finding out the meaning of a given metaphoric expression and explain it in literal words. There are a variety of applications that can benefit from interpreting metaphors, including language learning and text simplification (Barbu et al., 2015; Wolska and Clausen, 2017; Bingel et al., 2018) as well as lexical resources creation and development (Krek et al., 2018). Approach lexical substitution (Shutova et al., 2010) Metaphor brush aside accusation Interpretation reject paraphrase generation (Bizzoni and Lappin, 2018) The crowd was a river in the street. The crowd was large and impetuous in the street. definition generation (Martin, 1990) How do I kill the process? to terminate computer process. Table 1: Metaphor interpretation approaches with examples from previous studies. Manually annotating a dataset for metaphor interpretation (either to provide a defin"
2020.mmw-1.3,W14-4724,1,0.709436,"satellite adjectives should be marked as similar to a head adjective; this is called the ‘dumbbell’ model. The distinction is made at the part-of-speech level in the resource, although no other part-of-speech catalogue or dictionary to our knowledge makes the distinction this way.9 This means that there is often fewer links to other synsets and also shorter definitions; in fact adjectives typically have 1.44 synset links against a general average of 2.43. The plan for a future version, is to revamp the adjective so that they follow a more conventional classification such as that proposed by (McCrae et al., 2014), where the formal categories are: Open Multilingual WordNet Intersective These refer to properties that the adjective indicates the presence of. The most significant group of these are pertainyms, which mean that a concept is of or pertaining to a noun, e.g., “French” pertaining to “France”. The existing pertainym relation marks many of these but can be expanded. The Open Multilingual WordNet (Bond and Paik, 2012; Bond and Foster, 2013) project has also introduced new synsets and made changes related to the English WordNet. We are in the process of integrating these changes, one of the most m"
2020.mmw-1.3,2019.gwc-1.31,1,0.589223,"second release of this resource entitled “English WordNet 2020”. The work has focused firstly, on the introduction of new synsets and senses and developing guidelines for this and secondly, on the integration of contributions from other projects. We present the changes in this edition, which total over 15,000 changes over the previous release. Keywords: WordNet, lexicons, open source, lexicography, NLP 1. Introduction improving the procedure for development of the resource, in particular with the format and the issue of ensuring backwards compatibility with Princeton WordNet. English WordNet (McCrae et al., 2019) is a fork of Princeton WordNet (Fellbaum, 2010; Miller, 1995), which aims to further the development of a wordnet for English. Wordnets are one of the most widely used resources in natural language processing1 and as the English language is not static, it is necessary to continually update the resource so that it remains relevant for these tasks. Wordnets group words into sets of synonyms, called synsets, and each sense of a word corresponds to its membership in one synset. These synsets are then organized in a graph containing relationships such as hypernym/hyponym (broader or narrower), ant"
2020.mmw-1.3,2018.gwc-1.8,1,0.793344,"y not an encyclopedia. For this reason, it should not contain long lists of people, places, organizations, etc. Proper nouns are generally not expected to be included in the resource and many kinds of common nouns for narrow domains or geographical usage should not be included, examples of this would include elements of different cuisines around the world. As a rule of thumb, if there is a Wikipedia page for this concept it should not be in English WordNet.4 For future releases a more complete alignment of the resource and Wikipedia is planned based on previous works(De Melo and Weikum, 2009; McCrae, 2018) to address the introduction of synsets already welldescribed in Wikipedia. New Synset A synset covering a new concept is being proposed; Synset Duplicate Two synsets are not possible to distinguish or refer to the same concept. This is fixed by either creating a new concept for all synsets or by deleting all but one of the duplicates; Synset Split A synset refers to two distinct concepts and should be split into two new synsets; Synset Member A word in a synset should be added or removed; Enhancement A request for an improvement in the tooling around English WordNet or for a new kind of data;"
2020.mmw-1.3,2018.gwc-1.17,1,0.830348,"Missing"
2020.mmw-1.3,C12-3044,1,0.903984,"Missing"
2020.mmw-1.3,W04-2807,0,0.127532,"for this concept that is distinct from other concepts in English WordNet. A good definition consists of a genus and a differentia. Genus The type of the thing, often the hypernym, Differentia Something that makes this word unique An example of a good definition is: a piece of furniture having a smooth flat top that is usually supported by one or more vertical legs Where a poor definition would be: a piece of furniture used for eating 2.2.7. Sense distinctions One particular issue that has been common in the reported set of issues is the issue of sense distinction. WordNet has been criticized (Palmer et al., 2004; Snow et al., 2007) for a long time for issues related to its sense granularity. As such, there have been many issues claiming that synsets are duplicated as the meanings are quite hard to distinguish. In order to simplify these decisions, we have developed a few key principles that help us in distinguishing senses.6 In addition an example should be provided with a link to a website where the example is used as follows: &lt;Synset id=&quot;ewn-...&quot;&gt; ... &lt;Example dc:source= &quot;https://en.wikipedia.org/wiki/Example.com&quot;&gt; The example domains have one subdomain name defined in the Domain Name System &lt;/Exam"
2020.mmw-1.3,D07-1107,0,0.102197,"Missing"
2020.mmw-1.3,2018.gwc-1.18,0,0.0367778,"Missing"
2020.mmw-1.3,2016.gwc-1.9,1,0.838125,"ded into a number of source files that correspond to the original lexicographer files in WordNet, but are now in XML. New synsets proposed from issues should be assigned to one of these lexicographer files as they are created. For contributed resources (see below), we merged them into the original resource according to the hypernym. Sense keys were a mechanism that provided stability between releases of WordNet, and sense keys were (mostly) stable identifiers between different versions of Princeton WordNet. Instead, English WordNet has adopted the CILI interlingual index (Vossen et al., 2016; Bond et al., 2016) as the principal method of providing cross-version stability. Moreover, for new senses the calculation of stable sense identifiers is complicated as the Princeton WordNet formula relied on information in lexicographer files that is no longer present. Initial proposals were just to jettison sense keys, however community feedback has encouraged the creation of new methodology for assigning sense keys.5 In addition, we now also track the changes of sense keys, caused for example changes in the spelling of a lemma or if a sense has been moved across lexicographer files. 2.2.4. Well-defined It sho"
2020.mmw-1.3,2018.gwc-1.14,0,0.056166,"Missing"
2020.mmw-1.3,W15-0904,0,0.0218255,"the term should not be derivable from its components, e.g., “French Army” could be tagged with the synsets for “French” and “Army”; in contrast “operational system” refers not to a system that is operational, but it is a computer science term for the system that runs on every computer. Another case of MWE is the conventionalized ones. Conventionalization refers to the situation where a sequence of words that refer to a particular concept is commonly accepted in such a way that its constituents cannot easily be substituted for near-synonyms, because of some cultural or historical conventions (Farahmand et al., 2015). Consider the expression “geologic fault”. It is compositional but no one would consider substituting it with “geologic defect”. There are many types of MWE and a extensive literature about them (Sag et al., 2002), here we just want to emphasize that expressions that could have their parts annotated with senses already in the resource don’t need to be explicitly added. For single words, the word should not be derived in a systematic manner, these include: Contribution Issues related to large external contributions (see Section 3.); Bug A technical flaw that needs to be addressed in the data f"
2020.semeval-1.125,2020.sltu-1.25,1,0.724263,"g and thus, sentiment analysis has been explored. Most of the data extracted from social media are code-mixed (Ranjan et al., 2016; Priyadharshini et al., 2020), which have become a common approach in most cases but also pose unique challenges. Analysis of short texts from micro-blogging platforms such as Twitter is in high demand as the analysis of these text distill and evaluate the moods and the sentiment of the users and are very useful for different organisations, be it government or business or NGO. Sentiment analysis for Indian code-mixed languages is relatively new (Jose et al., 2020; Chakravarthi et al., 2020a; Chakravarthi et al., 2020b; Rani et al., 2020). The significant difference in style of language, orthography (Chakravarthi et al., 2019) and grammar used in tweets presents specific challenges for English-Hindi code-mixed data. In this paper we aim to introduce a novel deep neural network system which was submitted for SemEval 2020 Task 9, Sub Task A for English-Hindi data (Patwa et al., 2020). We will also compare the system with other state-of-the-art systems and describe how the system has outperformed others. The systems were trained using only the Twitter data provided by the organiser"
2020.semeval-1.125,2020.sltu-1.28,1,0.753985,"g and thus, sentiment analysis has been explored. Most of the data extracted from social media are code-mixed (Ranjan et al., 2016; Priyadharshini et al., 2020), which have become a common approach in most cases but also pose unique challenges. Analysis of short texts from micro-blogging platforms such as Twitter is in high demand as the analysis of these text distill and evaluate the moods and the sentiment of the users and are very useful for different organisations, be it government or business or NGO. Sentiment analysis for Indian code-mixed languages is relatively new (Jose et al., 2020; Chakravarthi et al., 2020a; Chakravarthi et al., 2020b; Rani et al., 2020). The significant difference in style of language, orthography (Chakravarthi et al., 2019) and grammar used in tweets presents specific challenges for English-Hindi code-mixed data. In this paper we aim to introduce a novel deep neural network system which was submitted for SemEval 2020 Task 9, Sub Task A for English-Hindi data (Patwa et al., 2020). We will also compare the system with other state-of-the-art systems and describe how the system has outperformed others. The systems were trained using only the Twitter data provided by the organiser"
2020.semeval-1.125,C14-1008,0,0.0497024,"rformed empirical analysis comparing the performance of various state of the art models in sentiment analysis. They also introduced 1 https://competitions.codalab.org/competitions/20654#learn_the_details-results This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http:// creativecommons.org/licenses/by/4.0/. 968 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 968–974 Barcelona, Spain (Online), December 12, 2020. a sub-word level representation in an LSTM model instead of character or word level representation. Dos Santos and Gatti (2014) proposed a deep convolutional neural network that exploits character level and sentence level information to predict the sentiments in short texts. All these previous experiments were dependent on the word-level language tags, and this is a disadvantage as it is time-consuming to annotate at the word level. In our approach, we create a model without the need for word-level annotation. 3 Dataset The dataset used for the current task is provided by SentiMix English-Hindi Task 9 in SemEval-2020 (Patwa et al., 2020). It consists of English-Hindi code-mixed tweets annotated with sentiment labels:"
2020.semeval-1.125,C16-1234,0,0.0281011,"systems were trained using only the Twitter data provided by the organisers excluding the word-level language tags provided in the data. 2 Related Work Although the field of sentiment analysis is growing and several systems have advanced the state-of-the-art, the overall performance of systems to predict sentiment in code-mixed data is low. Sharma et al. (2015) predicted overall sentiment score for Hindi-English code-mixed data using a lexicon based approach. Go et al. (2009) were the first to look at the task as a query-driven classification problem. A Hindi-English data-set was introduce by Joshi et al. (2016) for sentiment analysis and they performed empirical analysis comparing the performance of various state of the art models in sentiment analysis. They also introduced 1 https://competitions.codalab.org/competitions/20654#learn_the_details-results This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http:// creativecommons.org/licenses/by/4.0/. 968 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 968–974 Barcelona, Spain (Online), December 12, 2020. a sub-word level representation in an LSTM model instead of character"
2020.semeval-1.125,W16-3710,0,0.0280166,"using word-level language tags instead inferring this automatically using a morphological model. The system is based on a novel deep neural network (DNN) architecture, which has outperformed the baseline F1-score on the test data-set as well as the validation data-set. Our results can be found under the user name “koustava” on the “Sentimix Hindi English”1 page. 1 Introduction Sentiment analysis refers to a process of predicting the emotion content from a given text. Sentiment analysis is usually seen as a categorization problem over a variable with three values: positive, negative, neutral (Phani et al., 2016). With the increase in the popularity of social media such as Twitter, a new area of study to the field of natural language processing and thus, sentiment analysis has been explored. Most of the data extracted from social media are code-mixed (Ranjan et al., 2016; Priyadharshini et al., 2020), which have become a common approach in most cases but also pose unique challenges. Analysis of short texts from micro-blogging platforms such as Twitter is in high demand as the analysis of these text distill and evaluate the moods and the sentiment of the users and are very useful for different organisa"
2020.semeval-1.125,2020.trac-1.7,1,0.708238,"f the data extracted from social media are code-mixed (Ranjan et al., 2016; Priyadharshini et al., 2020), which have become a common approach in most cases but also pose unique challenges. Analysis of short texts from micro-blogging platforms such as Twitter is in high demand as the analysis of these text distill and evaluate the moods and the sentiment of the users and are very useful for different organisations, be it government or business or NGO. Sentiment analysis for Indian code-mixed languages is relatively new (Jose et al., 2020; Chakravarthi et al., 2020a; Chakravarthi et al., 2020b; Rani et al., 2020). The significant difference in style of language, orthography (Chakravarthi et al., 2019) and grammar used in tweets presents specific challenges for English-Hindi code-mixed data. In this paper we aim to introduce a novel deep neural network system which was submitted for SemEval 2020 Task 9, Sub Task A for English-Hindi data (Patwa et al., 2020). We will also compare the system with other state-of-the-art systems and describe how the system has outperformed others. The systems were trained using only the Twitter data provided by the organisers excluding the word-level language tags provided"
2020.sltu-1.25,N12-3002,0,0.0115691,"” 177 Figure 1: Data collection process. This is an example of inter-sentential code-mixing (Barman et al., 2014). ‘Oola’ a slang word for ‘useless’ which is popular among the youth of Kerala.The viewer expressed strong dislike against the whole trailer and one aspect is ‘poor dialogue delivery’. This comment has been marked as a negative comment as the disapproval of the trailer is evident. Sentiment analysis is a topic of greater interest recently since business strategies can be enhanced with insights obtained from the opinion about the product or subject of interest from the users (Balage Filho et al., 2012; Suryawanshi et al., 2020a). As mentioned earlier, the greater part of comments in social media are code-mixed. The conducive nature of such platforms invits all users from different stratus of society to express their opinion about a subject with their own feeling. Hence it is true that the real sentiments about the subject can be extracted from the analysis of code-mixed data. Even with this massive enthusiasm for user-opinions, there is not much effort taken to analyse the sentiment of code-mixed content in under-resourced languages. The contribution of this paper is that we release the go"
2020.sltu-1.25,W14-3914,0,0.0948675,"hing between more than one language in the same conversation for the speaker’s convenience is referred to as code-mixing (Androutsopoulos, 2013; Chakravarthi et al., 2019a; Chakravarthi et al., 2020). Even though many languages have their own scripts, social media users use nonnative script, usually Roman script, (Saint-Jacques, 1987; Rosowsky, 2010) for convenience in some part of the world, like India. This causes difficulties in finding the languages involved and also makes it hard to execute various existing natural language processing tasks, as these were developed for a single language (Bali et al., 2014; Diab et al., 2014; Solorio et al., 2014). Malayalam is one of the Dravidian languages spoken in the southern region of India with nearly 38 million Malayalam speakers in India and other countries (Thottingal, 2019). Malayalam is a deeply agglutinating language (Sreelekha and Bhattacharyya, 2018). The Malayalam script is the Vatteluttu alphabet extended with symbols from the Grantha alphabet. It is an alphasyllabary (abugida), a writing system that is partially “alphabetic” and partially syllable-based (Krishnamurti, 2003; Lalitha Devi, 2019; Chakravarthi et al., 2019c). Still, social media u"
2020.sltu-1.25,W14-3902,0,0.0658266,"this scenario due to the absence of a proper dataset. To create resources for a MalayalamEnglish code-mixed scenario, we collected comments of various Malayalam movie trailers from YouTube. Malayalam code-mixed sample text from the proposed dataset is shown below with the corresponding English glosses. • Malayalam-English: Innaleyaaane kandath super Padam.....ellarum familyaaayi poyi kananam super abinayam English: “Watched yesterday only this super movie... everyone go and watch the movie with the family..super acting..”. The English words ‘super’ and ‘family’ intra-sententially code mixed (Barman et al., 2014) with the Malayalam language. Also, the word ‘familyaayi’ is a new word combining both English and Malayalam, which is another kind of code-mixing called Intra-word switching that happens at the word level (Das and Gamb¨ack, 2014). In this case, ‘with family’ is together said as a single word following Malayalam morphology. Although the main word ‘family’ is in English and as the sentence is in Malayalam, the new word takes Malayalam morphology. This comment can be considered as a positive comment from the viewer of the trailer of a Malayalam movie as it is clear that he enjoyed the movie and"
2020.sltu-1.25,Q17-1010,0,0.745394,"i, 2004; Chakravarthi et al., 2018; Chakravarthi et al., 2019b), SentiNet (Poria et al., 2012), and SentiWordNet (Esuli and Sebastiani, 2006) were primarily used. Although being famous for their simplicity, both traditional machine learning and lexicon-based approaches are not efficient when applied on user-generated data, due to the dynamic nature of such data. This is where deep learning approaches take the spotlight for being efficient in adapting to dynamic user-generated data. In the advent of transfer learning, GloVe (Pennington et al., 2014), Word2Vec (Mikolov et al., 2013b), fastText (Bojanowski et al., 2017a) comes with their pros and cons. Malayalam (Nair et al., 2014; Sarkar and Chakraborty, 2015; Se et al., 2015; Se et al., 2016; Mouthami et al., 2013) has official status in India and other countries. Several research activities on sentiment analysis and events are focused on Malayalam due to their population and use of this language. However, sentiment analysis on MalayalamEnglish is very low, and data are not easily available for the research. Code-mixed data contains informal language with numerous accidental, deliberate errors, mixing of language and grammatical mixing, which makes previo"
2020.sltu-1.25,2018.gwc-1.10,1,0.68501,"Missing"
2020.sltu-1.25,W19-7101,1,0.578617,"le, mostly in social media forums such as Youtube, Facebook, and Twitter, which opens up the ground for mixing languages in the same conversation for multilingual communities. Some people with different linguistic backgrounds and cultures mark their impressions about a subject with the individual feeling in mixed language as not all are comfortable with a single language alone (Scotton, 1982; Tay, 1989; Suryawanshi et al., 2020b). This unplanned switching between more than one language in the same conversation for the speaker’s convenience is referred to as code-mixing (Androutsopoulos, 2013; Chakravarthi et al., 2019a; Chakravarthi et al., 2020). Even though many languages have their own scripts, social media users use nonnative script, usually Roman script, (Saint-Jacques, 1987; Rosowsky, 2010) for convenience in some part of the world, like India. This causes difficulties in finding the languages involved and also makes it hard to execute various existing natural language processing tasks, as these were developed for a single language (Bali et al., 2014; Diab et al., 2014; Solorio et al., 2014). Malayalam is one of the Dravidian languages spoken in the southern region of India with nearly 38 million Mal"
2020.sltu-1.25,W19-6809,1,0.816549,"le, mostly in social media forums such as Youtube, Facebook, and Twitter, which opens up the ground for mixing languages in the same conversation for multilingual communities. Some people with different linguistic backgrounds and cultures mark their impressions about a subject with the individual feeling in mixed language as not all are comfortable with a single language alone (Scotton, 1982; Tay, 1989; Suryawanshi et al., 2020b). This unplanned switching between more than one language in the same conversation for the speaker’s convenience is referred to as code-mixing (Androutsopoulos, 2013; Chakravarthi et al., 2019a; Chakravarthi et al., 2020). Even though many languages have their own scripts, social media users use nonnative script, usually Roman script, (Saint-Jacques, 1987; Rosowsky, 2010) for convenience in some part of the world, like India. This causes difficulties in finding the languages involved and also makes it hard to execute various existing natural language processing tasks, as these were developed for a single language (Bali et al., 2014; Diab et al., 2014; Solorio et al., 2014). Malayalam is one of the Dravidian languages spoken in the southern region of India with nearly 38 million Mal"
2020.sltu-1.25,2020.sltu-1.28,1,0.362799,"orums such as Youtube, Facebook, and Twitter, which opens up the ground for mixing languages in the same conversation for multilingual communities. Some people with different linguistic backgrounds and cultures mark their impressions about a subject with the individual feeling in mixed language as not all are comfortable with a single language alone (Scotton, 1982; Tay, 1989; Suryawanshi et al., 2020b). This unplanned switching between more than one language in the same conversation for the speaker’s convenience is referred to as code-mixing (Androutsopoulos, 2013; Chakravarthi et al., 2019a; Chakravarthi et al., 2020). Even though many languages have their own scripts, social media users use nonnative script, usually Roman script, (Saint-Jacques, 1987; Rosowsky, 2010) for convenience in some part of the world, like India. This causes difficulties in finding the languages involved and also makes it hard to execute various existing natural language processing tasks, as these were developed for a single language (Bali et al., 2014; Diab et al., 2014; Solorio et al., 2014). Malayalam is one of the Dravidian languages spoken in the southern region of India with nearly 38 million Malayalam speakers in India and"
2020.sltu-1.25,W17-1106,0,0.0405588,"ent analysis and provide comprehensive results on popular classification methods. To the best of our knowledge, this is the first code-mixed dataset for Malayalam sentiment analysis. Our code implementing these models along with the dataset is available freely for research purposes1 . 2. Related Work The sentiment analysis task has become increasingly important due to the explosion of social media, and extensive research has been done for sentiment analysis of monolingual corpora such as English (Hu and Liu, 2004; Wiebe et al., 2005; Jiang et al., 2019), Russian (Rogers et al., 2018), German (Cieliebak et al., 2017), Norwegian (Mæhlum et al., 2019) and Indian languages (Agrawal et al., 2018; Rani et al., 2020). There have been two traditional approaches to solve sentiment analysis problem such as lexicon-based, and machine learning approaches (Habimana et al., 2019). With the increasing popularity of lexicons in the field of sentiment analysis since 1966, new lexicons namely WordNet (Fellbaum, 1998), WordNet-Affect (Valitutti, 2004; Chakravarthi et al., 2018; Chakravarthi et al., 2019b), SentiNet (Poria et al., 2012), and SentiWordNet (Esuli and Sebastiani, 2006) were primarily used. Although being famou"
2020.sltu-1.25,W14-5152,0,0.0654114,"Missing"
2020.sltu-1.25,esuli-sebastiani-2006-sentiwordnet,0,0.0191124,"2019), Russian (Rogers et al., 2018), German (Cieliebak et al., 2017), Norwegian (Mæhlum et al., 2019) and Indian languages (Agrawal et al., 2018; Rani et al., 2020). There have been two traditional approaches to solve sentiment analysis problem such as lexicon-based, and machine learning approaches (Habimana et al., 2019). With the increasing popularity of lexicons in the field of sentiment analysis since 1966, new lexicons namely WordNet (Fellbaum, 1998), WordNet-Affect (Valitutti, 2004; Chakravarthi et al., 2018; Chakravarthi et al., 2019b), SentiNet (Poria et al., 2012), and SentiWordNet (Esuli and Sebastiani, 2006) were primarily used. Although being famous for their simplicity, both traditional machine learning and lexicon-based approaches are not efficient when applied on user-generated data, due to the dynamic nature of such data. This is where deep learning approaches take the spotlight for being efficient in adapting to dynamic user-generated data. In the advent of transfer learning, GloVe (Pennington et al., 2014), Word2Vec (Mikolov et al., 2013b), fastText (Bojanowski et al., 2017a) comes with their pros and cons. Malayalam (Nair et al., 2014; Sarkar and Chakraborty, 2015; Se et al., 2015; Se et"
2020.sltu-1.25,D19-1654,0,0.0291531,"Missing"
2020.sltu-1.25,C16-1234,0,0.0457727,"y low, and data are not easily available for the research. Code-mixed data contains informal language with numerous accidental, deliberate errors, mixing of language and grammatical mixing, which makes previous corpora and methods less suitable to train a model for sentiment analysis in code-mixed data. In the past few years, there have been increasing efforts on a variety of task using code-mixed text. However, the number of a freely available code-mixed dataset (Ranjan et al., 2016; Jose et al., 2020) are still limited in number, size, availability. For few languages, such as English-Hindi (Joshi et al., 2016; Patra et al., 2018; Priyadharshini et al., 2020), English-Spanish (Solorio et al., 2014) , ChineseEnglish (Lee and Wang, 2015), and English-Bengali (Patra et al., 2018) datasets are available for research. There are no dataset for Malayalam-English, so inspired by Severyn et al. (2014) we collected and created a code-mixed dataset from YouTube. We provided a use case of the codemixed Malayalam-English dataset by laying down the baselines which make the use of state of the art techniques such as Dynamic Meta-Embeddings DME (Kiela et al., 2018), Contextualized DME CDME (Kiela et al., 2018), 1D"
2020.sltu-1.25,P14-1062,0,0.0425927,"core not be between precision and recall. We combined fastText (Bojanowski et al., 2017b) and word2vec (Mikolov et al., 2013a) in DME and CDME baselines. The fastText and word2vec were trained on our codeswitched dataset. In DME, we are combining the mentioned embeddings by doing a weighted sum. On the otherhand CDME is using self-attention based mechanism on top of DME to make the embeddings context-dependent. 1DConv makes use of a 1D convolution filter to represent 181 each word with the context of the neighbouring word in the range of the kernel. In this convolutional neural network (CNN) (Kalchbrenner et al., 2014) approach, we are trying to capture the standout features from the text. BERT makes use of encoder-decoder architecture with an attention mechanism which increases the flexibility to read sequence both (left to right and vice versa) ways. From the results shown in the Table 4, all the machine learning algorithms succeed in classifying all the classes except SVM. A recall of 1.00 and precision around 0.13 for non-Malayalam class shows that all the classes have been labelled as non-Malayalam irrespectively. Other than SVM, LR, DT, RF shows considerable macro average score for precision, recall a"
2020.sltu-1.25,D18-1176,0,0.187143,"ability. For few languages, such as English-Hindi (Joshi et al., 2016; Patra et al., 2018; Priyadharshini et al., 2020), English-Spanish (Solorio et al., 2014) , ChineseEnglish (Lee and Wang, 2015), and English-Bengali (Patra et al., 2018) datasets are available for research. There are no dataset for Malayalam-English, so inspired by Severyn et al. (2014) we collected and created a code-mixed dataset from YouTube. We provided a use case of the codemixed Malayalam-English dataset by laying down the baselines which make the use of state of the art techniques such as Dynamic Meta-Embeddings DME (Kiela et al., 2018), Contextualized DME CDME (Kiela et al., 2018), 1D Dimensional Convolution 1DConv (Zhou et al., 2016), and Bidirectional Encoder Representations for Transformers BERT (Devlin et al., 2018). 3. Corpus Creation and Annotation Our goal was to create a code-mixed dataset for Malayalam-English and to ensure that enough data are 1 https://github.com/bharathichezhiyan/MalayalamMixSentiment available for research purposes. We used youtube178 comment-scraper tool 2 to download the comments from YouTube. First, we collected 116,711 sentences for Malayalam from YouTube post comments. We collect the comme"
2020.sltu-1.25,R19-1072,0,0.061102,"Missing"
2020.sltu-1.25,W15-3116,0,0.0823302,"deliberate errors, mixing of language and grammatical mixing, which makes previous corpora and methods less suitable to train a model for sentiment analysis in code-mixed data. In the past few years, there have been increasing efforts on a variety of task using code-mixed text. However, the number of a freely available code-mixed dataset (Ranjan et al., 2016; Jose et al., 2020) are still limited in number, size, availability. For few languages, such as English-Hindi (Joshi et al., 2016; Patra et al., 2018; Priyadharshini et al., 2020), English-Spanish (Solorio et al., 2014) , ChineseEnglish (Lee and Wang, 2015), and English-Bengali (Patra et al., 2018) datasets are available for research. There are no dataset for Malayalam-English, so inspired by Severyn et al. (2014) we collected and created a code-mixed dataset from YouTube. We provided a use case of the codemixed Malayalam-English dataset by laying down the baselines which make the use of state of the art techniques such as Dynamic Meta-Embeddings DME (Kiela et al., 2018), Contextualized DME CDME (Kiela et al., 2018), 1D Dimensional Convolution 1DConv (Zhou et al., 2016), and Bidirectional Encoder Representations for Transformers BERT (Devlin et"
2020.sltu-1.25,W19-6113,0,0.0266416,"ve results on popular classification methods. To the best of our knowledge, this is the first code-mixed dataset for Malayalam sentiment analysis. Our code implementing these models along with the dataset is available freely for research purposes1 . 2. Related Work The sentiment analysis task has become increasingly important due to the explosion of social media, and extensive research has been done for sentiment analysis of monolingual corpora such as English (Hu and Liu, 2004; Wiebe et al., 2005; Jiang et al., 2019), Russian (Rogers et al., 2018), German (Cieliebak et al., 2017), Norwegian (Mæhlum et al., 2019) and Indian languages (Agrawal et al., 2018; Rani et al., 2020). There have been two traditional approaches to solve sentiment analysis problem such as lexicon-based, and machine learning approaches (Habimana et al., 2019). With the increasing popularity of lexicons in the field of sentiment analysis since 1966, new lexicons namely WordNet (Fellbaum, 1998), WordNet-Affect (Valitutti, 2004; Chakravarthi et al., 2018; Chakravarthi et al., 2019b), SentiNet (Poria et al., 2012), and SentiWordNet (Esuli and Sebastiani, 2006) were primarily used. Although being famous for their simplicity, both trad"
2020.sltu-1.25,W16-0429,0,0.044074,"n language identification at comment level with the langdect library 3 . That is if the comment is fully in one language than we discarded that comment since monolingual resources are available for these languages. Comments in Malayalam script was also discarded. We preprocessed the comments by removing the emoji’s, and sentence length longer than 15 or less than 5 words since sentence more than 15 words will be difficult for annotators. After cleaning, we got 6,738 sentences for Malayalam-English code-mixed post comments. 3.1. Annotation Setup For annotation, we adopted the approach taken by Mohammad (2016) and each sentence was annotated by a minimum of three annotators according to the following schema: • Positive state: There is an explicit or implicit clue in the text suggesting that the speaker is in a positive state, i.e., happy, admiring, relaxed, and forgiving. • Negative state: There is an explicit or implicit clue in the text suggesting that the speaker is in a negative state, i.e., sad, angry, anxious, and violent. • Mixed feelings: There is an explicit or implicit clue in the text suggesting that the speaker is experiencing both positive and negative feeling: Comparing two movies • N"
2020.sltu-1.25,D14-1162,0,0.0831817,"Missing"
2020.sltu-1.25,2020.trac-1.7,1,0.775004,"knowledge, this is the first code-mixed dataset for Malayalam sentiment analysis. Our code implementing these models along with the dataset is available freely for research purposes1 . 2. Related Work The sentiment analysis task has become increasingly important due to the explosion of social media, and extensive research has been done for sentiment analysis of monolingual corpora such as English (Hu and Liu, 2004; Wiebe et al., 2005; Jiang et al., 2019), Russian (Rogers et al., 2018), German (Cieliebak et al., 2017), Norwegian (Mæhlum et al., 2019) and Indian languages (Agrawal et al., 2018; Rani et al., 2020). There have been two traditional approaches to solve sentiment analysis problem such as lexicon-based, and machine learning approaches (Habimana et al., 2019). With the increasing popularity of lexicons in the field of sentiment analysis since 1966, new lexicons namely WordNet (Fellbaum, 1998), WordNet-Affect (Valitutti, 2004; Chakravarthi et al., 2018; Chakravarthi et al., 2019b), SentiNet (Poria et al., 2012), and SentiWordNet (Esuli and Sebastiani, 2006) were primarily used. Although being famous for their simplicity, both traditional machine learning and lexicon-based approaches are not e"
2020.sltu-1.25,C18-1064,0,0.0364948,"amEnglish annotated for sentiment analysis and provide comprehensive results on popular classification methods. To the best of our knowledge, this is the first code-mixed dataset for Malayalam sentiment analysis. Our code implementing these models along with the dataset is available freely for research purposes1 . 2. Related Work The sentiment analysis task has become increasingly important due to the explosion of social media, and extensive research has been done for sentiment analysis of monolingual corpora such as English (Hu and Liu, 2004; Wiebe et al., 2005; Jiang et al., 2019), Russian (Rogers et al., 2018), German (Cieliebak et al., 2017), Norwegian (Mæhlum et al., 2019) and Indian languages (Agrawal et al., 2018; Rani et al., 2020). There have been two traditional approaches to solve sentiment analysis problem such as lexicon-based, and machine learning approaches (Habimana et al., 2019). With the increasing popularity of lexicons in the field of sentiment analysis since 1966, new lexicons namely WordNet (Fellbaum, 1998), WordNet-Affect (Valitutti, 2004; Chakravarthi et al., 2018; Chakravarthi et al., 2019b), SentiNet (Poria et al., 2012), and SentiWordNet (Esuli and Sebastiani, 2006) were pri"
2020.sltu-1.25,P14-1118,0,0.135963,"in code-mixed data. In the past few years, there have been increasing efforts on a variety of task using code-mixed text. However, the number of a freely available code-mixed dataset (Ranjan et al., 2016; Jose et al., 2020) are still limited in number, size, availability. For few languages, such as English-Hindi (Joshi et al., 2016; Patra et al., 2018; Priyadharshini et al., 2020), English-Spanish (Solorio et al., 2014) , ChineseEnglish (Lee and Wang, 2015), and English-Bengali (Patra et al., 2018) datasets are available for research. There are no dataset for Malayalam-English, so inspired by Severyn et al. (2014) we collected and created a code-mixed dataset from YouTube. We provided a use case of the codemixed Malayalam-English dataset by laying down the baselines which make the use of state of the art techniques such as Dynamic Meta-Embeddings DME (Kiela et al., 2018), Contextualized DME CDME (Kiela et al., 2018), 1D Dimensional Convolution 1DConv (Zhou et al., 2016), and Bidirectional Encoder Representations for Transformers BERT (Devlin et al., 2018). 3. Corpus Creation and Annotation Our goal was to create a code-mixed dataset for Malayalam-English and to ensure that enough data are 1 https://git"
2020.sltu-1.25,W14-3907,0,0.242618,"the same conversation for the speaker’s convenience is referred to as code-mixing (Androutsopoulos, 2013; Chakravarthi et al., 2019a; Chakravarthi et al., 2020). Even though many languages have their own scripts, social media users use nonnative script, usually Roman script, (Saint-Jacques, 1987; Rosowsky, 2010) for convenience in some part of the world, like India. This causes difficulties in finding the languages involved and also makes it hard to execute various existing natural language processing tasks, as these were developed for a single language (Bali et al., 2014; Diab et al., 2014; Solorio et al., 2014). Malayalam is one of the Dravidian languages spoken in the southern region of India with nearly 38 million Malayalam speakers in India and other countries (Thottingal, 2019). Malayalam is a deeply agglutinating language (Sreelekha and Bhattacharyya, 2018). The Malayalam script is the Vatteluttu alphabet extended with symbols from the Grantha alphabet. It is an alphasyllabary (abugida), a writing system that is partially “alphabetic” and partially syllable-based (Krishnamurti, 2003; Lalitha Devi, 2019; Chakravarthi et al., 2019c). Still, social media users use Roman script for typing due to it"
2020.sltu-1.25,L18-1413,0,0.0279294,"ive script, usually Roman script, (Saint-Jacques, 1987; Rosowsky, 2010) for convenience in some part of the world, like India. This causes difficulties in finding the languages involved and also makes it hard to execute various existing natural language processing tasks, as these were developed for a single language (Bali et al., 2014; Diab et al., 2014; Solorio et al., 2014). Malayalam is one of the Dravidian languages spoken in the southern region of India with nearly 38 million Malayalam speakers in India and other countries (Thottingal, 2019). Malayalam is a deeply agglutinating language (Sreelekha and Bhattacharyya, 2018). The Malayalam script is the Vatteluttu alphabet extended with symbols from the Grantha alphabet. It is an alphasyllabary (abugida), a writing system that is partially “alphabetic” and partially syllable-based (Krishnamurti, 2003; Lalitha Devi, 2019; Chakravarthi et al., 2019c). Still, social media users use Roman script for typing due to it being easier to input. There is a lot of code-mixed data between Malayalam and English among the YouTube comments we surveyed. Monolingual datasets are available for Indian languages for various research aims (Agrawal et al., 2018). However, there are few"
2020.sltu-1.25,2020.trac-1.6,1,0.914888,"timent analysis 1. Introduction The Internet gave users the opportunities to express an opinion on any topic in the form of user reviews or comments. The comments are usually of an informal style, mostly in social media forums such as Youtube, Facebook, and Twitter, which opens up the ground for mixing languages in the same conversation for multilingual communities. Some people with different linguistic backgrounds and cultures mark their impressions about a subject with the individual feeling in mixed language as not all are comfortable with a single language alone (Scotton, 1982; Tay, 1989; Suryawanshi et al., 2020b). This unplanned switching between more than one language in the same conversation for the speaker’s convenience is referred to as code-mixing (Androutsopoulos, 2013; Chakravarthi et al., 2019a; Chakravarthi et al., 2020). Even though many languages have their own scripts, social media users use nonnative script, usually Roman script, (Saint-Jacques, 1987; Rosowsky, 2010) for convenience in some part of the world, like India. This causes difficulties in finding the languages involved and also makes it hard to execute various existing natural language processing tasks, as these were developed"
2020.sltu-1.25,W19-6801,0,0.0381548,"y languages have their own scripts, social media users use nonnative script, usually Roman script, (Saint-Jacques, 1987; Rosowsky, 2010) for convenience in some part of the world, like India. This causes difficulties in finding the languages involved and also makes it hard to execute various existing natural language processing tasks, as these were developed for a single language (Bali et al., 2014; Diab et al., 2014; Solorio et al., 2014). Malayalam is one of the Dravidian languages spoken in the southern region of India with nearly 38 million Malayalam speakers in India and other countries (Thottingal, 2019). Malayalam is a deeply agglutinating language (Sreelekha and Bhattacharyya, 2018). The Malayalam script is the Vatteluttu alphabet extended with symbols from the Grantha alphabet. It is an alphasyllabary (abugida), a writing system that is partially “alphabetic” and partially syllable-based (Krishnamurti, 2003; Lalitha Devi, 2019; Chakravarthi et al., 2019c). Still, social media users use Roman script for typing due to it being easier to input. There is a lot of code-mixed data between Malayalam and English among the YouTube comments we surveyed. Monolingual datasets are available for Indian"
2020.sltu-1.25,strapparava-valitutti-2004-wordnet,0,0.239735,"n done for sentiment analysis of monolingual corpora such as English (Hu and Liu, 2004; Wiebe et al., 2005; Jiang et al., 2019), Russian (Rogers et al., 2018), German (Cieliebak et al., 2017), Norwegian (Mæhlum et al., 2019) and Indian languages (Agrawal et al., 2018; Rani et al., 2020). There have been two traditional approaches to solve sentiment analysis problem such as lexicon-based, and machine learning approaches (Habimana et al., 2019). With the increasing popularity of lexicons in the field of sentiment analysis since 1966, new lexicons namely WordNet (Fellbaum, 1998), WordNet-Affect (Valitutti, 2004; Chakravarthi et al., 2018; Chakravarthi et al., 2019b), SentiNet (Poria et al., 2012), and SentiWordNet (Esuli and Sebastiani, 2006) were primarily used. Although being famous for their simplicity, both traditional machine learning and lexicon-based approaches are not efficient when applied on user-generated data, due to the dynamic nature of such data. This is where deep learning approaches take the spotlight for being efficient in adapting to dynamic user-generated data. In the advent of transfer learning, GloVe (Pennington et al., 2014), Word2Vec (Mikolov et al., 2013b), fastText (Bojanow"
2020.sltu-1.25,C16-1329,0,0.146954,"ini et al., 2020), English-Spanish (Solorio et al., 2014) , ChineseEnglish (Lee and Wang, 2015), and English-Bengali (Patra et al., 2018) datasets are available for research. There are no dataset for Malayalam-English, so inspired by Severyn et al. (2014) we collected and created a code-mixed dataset from YouTube. We provided a use case of the codemixed Malayalam-English dataset by laying down the baselines which make the use of state of the art techniques such as Dynamic Meta-Embeddings DME (Kiela et al., 2018), Contextualized DME CDME (Kiela et al., 2018), 1D Dimensional Convolution 1DConv (Zhou et al., 2016), and Bidirectional Encoder Representations for Transformers BERT (Devlin et al., 2018). 3. Corpus Creation and Annotation Our goal was to create a code-mixed dataset for Malayalam-English and to ensure that enough data are 1 https://github.com/bharathichezhiyan/MalayalamMixSentiment available for research purposes. We used youtube178 comment-scraper tool 2 to download the comments from YouTube. First, we collected 116,711 sentences for Malayalam from YouTube post comments. We collect the comments from the movie trailers of 2019 based on the YouTube search results for keyword ”Malayalam movie"
2020.sltu-1.28,W16-5812,0,0.0359972,"mixed text (Barman et al., 2014; Chanda et al., 2016; Pratapa et al., 2018a; Winata et al., 2019a). Our study focuses on sentiment analysis in Tamil, which has little annotated data for code-mixed scenarios (Phani et al., 2016; Jose et al., 2020). Features based on the lexical properties such as a dictionary of words and parts of speech tagging have less performance compared to the supervised learning (Kannan et al., 2016) approaches using annotated data. However, an annotated corpus developed for monolingual data cannot deal with code-mixed usage and therefore it fails to yield good results (AlGhamdi et al., 2016; Aguilar et al., 2018) due to mixture of languages at different levels of linguistic analysis. (2016) and without annotating the word level language tag. The instructions enabled light and speedy annotation while maintaining consistency. The overall inter-annotator agreement in terms of Kripendorffs’s α (Krippendorff, 1970) stands at 0.6. In total, 15,744 comments were annotated; this makes the largest general domain sentiment dataset for this relatively low-resource language with code-mixing phenomenon. We observed all the three types of code-mixed sentences - Inter-Sentential switch, Intra-"
2020.sltu-1.28,C12-2008,0,0.0809981,"Missing"
2020.sltu-1.28,W14-3902,0,0.0772007,"15,744 comment posts from YouTube. In this paper, we describe the process of creating the corpus and assigning polarities. We present inter-annotator agreement and show the results of sentiment analysis trained on this corpus as a benchmark. Keywords: code mixed, Tamil, sentiment, corpus, dataset 1. Introduction Sentiment analysis has become important in social media research (Yang and Eisenstein, 2017). Until recently these applications were created for high-resourced languages which analysed monolingual utterances. But social media in multilingual communities contains more code-mixed text (Barman et al., 2014; Chanda et al., 2016; Pratapa et al., 2018a; Winata et al., 2019a). Our study focuses on sentiment analysis in Tamil, which has little annotated data for code-mixed scenarios (Phani et al., 2016; Jose et al., 2020). Features based on the lexical properties such as a dictionary of words and parts of speech tagging have less performance compared to the supervised learning (Kannan et al., 2016) approaches using annotated data. However, an annotated corpus developed for monolingual data cannot deal with code-mixed usage and therefore it fails to yield good results (AlGhamdi et al., 2016; Aguilar"
2020.sltu-1.28,2018.gwc-1.10,1,0.616076,"Missing"
2020.sltu-1.28,W19-7101,1,0.522792,"n it comes to code-mixing, an English-Hindi corpus was created by (Sitaram et al., 2015; Joshi et al., 2016; Patra et al., 2018), an English-Spanish corpus was introduced by (Solorio et al., 2014; Vilares et al., 2015; Vilares et al., 2016), and a Chinese-English one (Lee and Wang, 2015) was collected from Weibo.com and English-Bengali data were released by Patra et al. (Patra et al., 2018). Tamil is a Dravidian language spoken by Tamil people in India, Sri Lanka and by the Tamil diaspora around the world, with official recognition in India, Sri Lanka and Singapore (Chakravarthi et al., 2018; Chakravarthi et al., 2019a; Chakravarthi et al., 2019b; Chakravarthi et al., 2019c). Several research activities on sentiment analysis in Tamil (Padmamala and Prema, 2017) and other Indian languages (Ranjan et al., 2016; Das and Bandyopadhyay, 2010; A.R. et al., 2012; Phani et al., 2016; Prasad et al., 2016; Priyadharshini et al., 2020; Chakravarthi et al., 2020) are happening because the sheer number of native speakers are a potential market for commercial NLP applications. However, sentiment analysis on Tamil-English code-mixed data (Patra et al., 2018) is under-developed and data tare not readily available for rese"
2020.sltu-1.28,W19-6809,1,0.73317,"n it comes to code-mixing, an English-Hindi corpus was created by (Sitaram et al., 2015; Joshi et al., 2016; Patra et al., 2018), an English-Spanish corpus was introduced by (Solorio et al., 2014; Vilares et al., 2015; Vilares et al., 2016), and a Chinese-English one (Lee and Wang, 2015) was collected from Weibo.com and English-Bengali data were released by Patra et al. (Patra et al., 2018). Tamil is a Dravidian language spoken by Tamil people in India, Sri Lanka and by the Tamil diaspora around the world, with official recognition in India, Sri Lanka and Singapore (Chakravarthi et al., 2018; Chakravarthi et al., 2019a; Chakravarthi et al., 2019b; Chakravarthi et al., 2019c). Several research activities on sentiment analysis in Tamil (Padmamala and Prema, 2017) and other Indian languages (Ranjan et al., 2016; Das and Bandyopadhyay, 2010; A.R. et al., 2012; Phani et al., 2016; Prasad et al., 2016; Priyadharshini et al., 2020; Chakravarthi et al., 2020) are happening because the sheer number of native speakers are a potential market for commercial NLP applications. However, sentiment analysis on Tamil-English code-mixed data (Patra et al., 2018) is under-developed and data tare not readily available for rese"
2020.sltu-1.28,2020.sltu-1.25,1,0.362799,"ata were released by Patra et al. (Patra et al., 2018). Tamil is a Dravidian language spoken by Tamil people in India, Sri Lanka and by the Tamil diaspora around the world, with official recognition in India, Sri Lanka and Singapore (Chakravarthi et al., 2018; Chakravarthi et al., 2019a; Chakravarthi et al., 2019b; Chakravarthi et al., 2019c). Several research activities on sentiment analysis in Tamil (Padmamala and Prema, 2017) and other Indian languages (Ranjan et al., 2016; Das and Bandyopadhyay, 2010; A.R. et al., 2012; Phani et al., 2016; Prasad et al., 2016; Priyadharshini et al., 2020; Chakravarthi et al., 2020) are happening because the sheer number of native speakers are a potential market for commercial NLP applications. However, sentiment analysis on Tamil-English code-mixed data (Patra et al., 2018) is under-developed and data tare not readily available for research. Until recently, word-level annotations were used for research in code-mixed corpora. Almost all the previous systems proposed were based on data annotated at the word-level. This is not only time-consuming but also expensive to create. However, neural networks and metaembeddings (Kiela et al., 2018) have shown great promise in code-"
2020.sltu-1.28,W16-5810,0,0.0241652,"from YouTube. In this paper, we describe the process of creating the corpus and assigning polarities. We present inter-annotator agreement and show the results of sentiment analysis trained on this corpus as a benchmark. Keywords: code mixed, Tamil, sentiment, corpus, dataset 1. Introduction Sentiment analysis has become important in social media research (Yang and Eisenstein, 2017). Until recently these applications were created for high-resourced languages which analysed monolingual utterances. But social media in multilingual communities contains more code-mixed text (Barman et al., 2014; Chanda et al., 2016; Pratapa et al., 2018a; Winata et al., 2019a). Our study focuses on sentiment analysis in Tamil, which has little annotated data for code-mixed scenarios (Phani et al., 2016; Jose et al., 2020). Features based on the lexical properties such as a dictionary of words and parts of speech tagging have less performance compared to the supervised learning (Kannan et al., 2016) approaches using annotated data. However, an annotated corpus developed for monolingual data cannot deal with code-mixed usage and therefore it fails to yield good results (AlGhamdi et al., 2016; Aguilar et al., 2018) due to"
2020.sltu-1.28,W17-1106,0,0.0532475,"n, naive Bayes, decision tree, random forest, SVM, dynamic meta-embedding, contextualized dynamic meta-embedding, 1DConv-LSTM and BERT on our code-mixed data for sentiment classification. 2. Related Work Recently, there has been a considerable amount of work and effort to collect resources for code-switched text. However, code-switched datasets and lexicons for sentiment analysis are still limited in number, size and availability. For monolingual analysis, there exist various corpora for English (Hu and Liu, 2004; Wiebe et al., 2005; Jiang et al., 2019), Russian (Rogers et al., 2018), German (Cieliebak et al., 2017), Norwegian (Mæhlum et al., 2019) and Indian languages (Agrawal et al., 2018; Rani et al., 2020). When it comes to code-mixing, an English-Hindi corpus was created by (Sitaram et al., 2015; Joshi et al., 2016; Patra et al., 2018), an English-Spanish corpus was introduced by (Solorio et al., 2014; Vilares et al., 2015; Vilares et al., 2016), and a Chinese-English one (Lee and Wang, 2015) was collected from Weibo.com and English-Bengali data were released by Patra et al. (Patra et al., 2018). Tamil is a Dravidian language spoken by Tamil people in India, Sri Lanka and by the Tamil diaspora aroun"
2020.sltu-1.28,D19-1383,0,0.0217615,"ivity. We evaluate the MNB model with our data using α=1 with TF-IDF vectors. 5.1.7. 1DConv-LSTM: The model we evaluated consists of Embedding layer, Dropout, 1DConv with activation ReLU, Max-pooling and LSTM. The embeddings are randomly initialized. 5.1.8. BERT-Multilingual: Devlin et al. (2019) introduced a language representation model which is Bidirectional Encoder Representation from Transforms. It is designed to pre-train from unlabelled text and can be fine-tuned by adding last layer. BERT has been used for many text classification tasks (Tayyar Madabushi et al., 2019; Ma et al., 2019; Cohan et al., 2019). We explore classification of a code-mixed data into their corresponding sentiment categories. 5.1.9. DME and CDME: We also implemented the Dynamic Meta Embedding (Kiela et al., 2018) to evaluate our model. As a first step, we used Word2Vec and FastText to train from our dataset since dy206 namic meta-embedding is an effective method for the supervised learning of embedding ensembles. 5.2. Experiment Results and Discussion The experimental results of the sentiment classification task using different methods are shown in terms of precision in Table 4, recall in Table 5, and F-score in Table 6."
2020.sltu-1.28,W10-3208,0,0.0344382,"lares et al., 2016), and a Chinese-English one (Lee and Wang, 2015) was collected from Weibo.com and English-Bengali data were released by Patra et al. (Patra et al., 2018). Tamil is a Dravidian language spoken by Tamil people in India, Sri Lanka and by the Tamil diaspora around the world, with official recognition in India, Sri Lanka and Singapore (Chakravarthi et al., 2018; Chakravarthi et al., 2019a; Chakravarthi et al., 2019b; Chakravarthi et al., 2019c). Several research activities on sentiment analysis in Tamil (Padmamala and Prema, 2017) and other Indian languages (Ranjan et al., 2016; Das and Bandyopadhyay, 2010; A.R. et al., 2012; Phani et al., 2016; Prasad et al., 2016; Priyadharshini et al., 2020; Chakravarthi et al., 2020) are happening because the sheer number of native speakers are a potential market for commercial NLP applications. However, sentiment analysis on Tamil-English code-mixed data (Patra et al., 2018) is under-developed and data tare not readily available for research. Until recently, word-level annotations were used for research in code-mixed corpora. Almost all the previous systems proposed were based on data annotated at the word-level. This is not only time-consuming but also ex"
2020.sltu-1.28,N19-1423,0,0.0394077,"rest, the classifier randomly generates trees without defining rules. We evaluate the RF model with same features as in DT. 5.1.6. Multinominal Naive Bayes (MNB): Naive-Bayes classifier is a probabilistic model, which is derived from Bayes Theorem that finds the probability of hypothesis activity to the given evidence activity. We evaluate the MNB model with our data using α=1 with TF-IDF vectors. 5.1.7. 1DConv-LSTM: The model we evaluated consists of Embedding layer, Dropout, 1DConv with activation ReLU, Max-pooling and LSTM. The embeddings are randomly initialized. 5.1.8. BERT-Multilingual: Devlin et al. (2019) introduced a language representation model which is Bidirectional Encoder Representation from Transforms. It is designed to pre-train from unlabelled text and can be fine-tuned by adding last layer. BERT has been used for many text classification tasks (Tayyar Madabushi et al., 2019; Ma et al., 2019; Cohan et al., 2019). We explore classification of a code-mixed data into their corresponding sentiment categories. 5.1.9. DME and CDME: We also implemented the Dynamic Meta Embedding (Kiela et al., 2018) to evaluate our model. As a first step, we used Word2Vec and FastText to train from our datas"
2020.sltu-1.28,D19-1654,0,0.0424139,"Missing"
2020.sltu-1.28,C16-1234,0,0.0466807,"there has been a considerable amount of work and effort to collect resources for code-switched text. However, code-switched datasets and lexicons for sentiment analysis are still limited in number, size and availability. For monolingual analysis, there exist various corpora for English (Hu and Liu, 2004; Wiebe et al., 2005; Jiang et al., 2019), Russian (Rogers et al., 2018), German (Cieliebak et al., 2017), Norwegian (Mæhlum et al., 2019) and Indian languages (Agrawal et al., 2018; Rani et al., 2020). When it comes to code-mixing, an English-Hindi corpus was created by (Sitaram et al., 2015; Joshi et al., 2016; Patra et al., 2018), an English-Spanish corpus was introduced by (Solorio et al., 2014; Vilares et al., 2015; Vilares et al., 2016), and a Chinese-English one (Lee and Wang, 2015) was collected from Weibo.com and English-Bengali data were released by Patra et al. (Patra et al., 2018). Tamil is a Dravidian language spoken by Tamil people in India, Sri Lanka and by the Tamil diaspora around the world, with official recognition in India, Sri Lanka and Singapore (Chakravarthi et al., 2018; Chakravarthi et al., 2019a; Chakravarthi et al., 2019b; Chakravarthi et al., 2019c). Several research activ"
2020.sltu-1.28,W16-6305,0,0.0205759,"nstein, 2017). Until recently these applications were created for high-resourced languages which analysed monolingual utterances. But social media in multilingual communities contains more code-mixed text (Barman et al., 2014; Chanda et al., 2016; Pratapa et al., 2018a; Winata et al., 2019a). Our study focuses on sentiment analysis in Tamil, which has little annotated data for code-mixed scenarios (Phani et al., 2016; Jose et al., 2020). Features based on the lexical properties such as a dictionary of words and parts of speech tagging have less performance compared to the supervised learning (Kannan et al., 2016) approaches using annotated data. However, an annotated corpus developed for monolingual data cannot deal with code-mixed usage and therefore it fails to yield good results (AlGhamdi et al., 2016; Aguilar et al., 2018) due to mixture of languages at different levels of linguistic analysis. (2016) and without annotating the word level language tag. The instructions enabled light and speedy annotation while maintaining consistency. The overall inter-annotator agreement in terms of Kripendorffs’s α (Krippendorff, 1970) stands at 0.6. In total, 15,744 comments were annotated; this makes the larges"
2020.sltu-1.28,D18-1176,0,0.0774937,"iyadharshini et al., 2020; Chakravarthi et al., 2020) are happening because the sheer number of native speakers are a potential market for commercial NLP applications. However, sentiment analysis on Tamil-English code-mixed data (Patra et al., 2018) is under-developed and data tare not readily available for research. Until recently, word-level annotations were used for research in code-mixed corpora. Almost all the previous systems proposed were based on data annotated at the word-level. This is not only time-consuming but also expensive to create. However, neural networks and metaembeddings (Kiela et al., 2018) have shown great promise in code-switched research without the need for word-level annotation. In particular, work by Winata et al. (2019a) learns to utilise information from pre-trained embeddings without explicit word-level language tags. A recent work by Winata et al. (2019b) utilised the subword-level information from closely related languages to improve the performance on the code-mixed text. (a) Example 1 (b) Example 2 Figure 1: Examples of Google Form. 203 3. Corpus Creation and Annotation Our goal was to create a code-mixed dataset for Tamil to ensure that enough data are available fo"
2020.sltu-1.28,W15-3116,0,0.0255315,"limited in number, size and availability. For monolingual analysis, there exist various corpora for English (Hu and Liu, 2004; Wiebe et al., 2005; Jiang et al., 2019), Russian (Rogers et al., 2018), German (Cieliebak et al., 2017), Norwegian (Mæhlum et al., 2019) and Indian languages (Agrawal et al., 2018; Rani et al., 2020). When it comes to code-mixing, an English-Hindi corpus was created by (Sitaram et al., 2015; Joshi et al., 2016; Patra et al., 2018), an English-Spanish corpus was introduced by (Solorio et al., 2014; Vilares et al., 2015; Vilares et al., 2016), and a Chinese-English one (Lee and Wang, 2015) was collected from Weibo.com and English-Bengali data were released by Patra et al. (Patra et al., 2018). Tamil is a Dravidian language spoken by Tamil people in India, Sri Lanka and by the Tamil diaspora around the world, with official recognition in India, Sri Lanka and Singapore (Chakravarthi et al., 2018; Chakravarthi et al., 2019a; Chakravarthi et al., 2019b; Chakravarthi et al., 2019c). Several research activities on sentiment analysis in Tamil (Padmamala and Prema, 2017) and other Indian languages (Ranjan et al., 2016; Das and Bandyopadhyay, 2010; A.R. et al., 2012; Phani et al., 2016;"
2020.sltu-1.28,D19-6109,0,0.0151866,"iven evidence activity. We evaluate the MNB model with our data using α=1 with TF-IDF vectors. 5.1.7. 1DConv-LSTM: The model we evaluated consists of Embedding layer, Dropout, 1DConv with activation ReLU, Max-pooling and LSTM. The embeddings are randomly initialized. 5.1.8. BERT-Multilingual: Devlin et al. (2019) introduced a language representation model which is Bidirectional Encoder Representation from Transforms. It is designed to pre-train from unlabelled text and can be fine-tuned by adding last layer. BERT has been used for many text classification tasks (Tayyar Madabushi et al., 2019; Ma et al., 2019; Cohan et al., 2019). We explore classification of a code-mixed data into their corresponding sentiment categories. 5.1.9. DME and CDME: We also implemented the Dynamic Meta Embedding (Kiela et al., 2018) to evaluate our model. As a first step, we used Word2Vec and FastText to train from our dataset since dy206 namic meta-embedding is an effective method for the supervised learning of embedding ensembles. 5.2. Experiment Results and Discussion The experimental results of the sentiment classification task using different methods are shown in terms of precision in Table 4, recall in Table 5, an"
2020.sltu-1.28,W19-6113,0,0.0256624,"m forest, SVM, dynamic meta-embedding, contextualized dynamic meta-embedding, 1DConv-LSTM and BERT on our code-mixed data for sentiment classification. 2. Related Work Recently, there has been a considerable amount of work and effort to collect resources for code-switched text. However, code-switched datasets and lexicons for sentiment analysis are still limited in number, size and availability. For monolingual analysis, there exist various corpora for English (Hu and Liu, 2004; Wiebe et al., 2005; Jiang et al., 2019), Russian (Rogers et al., 2018), German (Cieliebak et al., 2017), Norwegian (Mæhlum et al., 2019) and Indian languages (Agrawal et al., 2018; Rani et al., 2020). When it comes to code-mixing, an English-Hindi corpus was created by (Sitaram et al., 2015; Joshi et al., 2016; Patra et al., 2018), an English-Spanish corpus was introduced by (Solorio et al., 2014; Vilares et al., 2015; Vilares et al., 2016), and a Chinese-English one (Lee and Wang, 2015) was collected from Weibo.com and English-Bengali data were released by Patra et al. (Patra et al., 2018). Tamil is a Dravidian language spoken by Tamil people in India, Sri Lanka and by the Tamil diaspora around the world, with official recogn"
2020.sltu-1.28,W16-0429,0,0.0195048,"languages. We also identified if the sentences were written in other languages such as Hindi, Malayalam, Urdu, Telugu, and Kannada. We preprocessed the comments by removing the emoticons and applying a sentence length filter. We want to create a code-mixed corpus of reasonable size with sentences that have fairly defined sentiments which will be useful for future research. Thus our filter removed sentences with less than five words and more than 15 words after cleaning the data. In the end we got 15,744 Tanglish sentences. 3.1. Annotation Setup For annotation, we adopted the approach taken by Mohammad (2016), and a minimum of three annotators annotated each sentence in the dataset according to the following schema shown in the Figure 1. We added new category Other language: If the sentence is written in some other language other than Tamil or English. Examples for this are the comments written in other Indian languages using the Roman script. The annotation guidelines are given in English and Tamil. As we have collected data from YouTube we anonymized to keep the privacy of the users who commented on it. As the voluntary annotators’ personal information were collected to know about the them, this"
2020.sltu-1.28,W16-3710,0,0.305945,"nalysis trained on this corpus as a benchmark. Keywords: code mixed, Tamil, sentiment, corpus, dataset 1. Introduction Sentiment analysis has become important in social media research (Yang and Eisenstein, 2017). Until recently these applications were created for high-resourced languages which analysed monolingual utterances. But social media in multilingual communities contains more code-mixed text (Barman et al., 2014; Chanda et al., 2016; Pratapa et al., 2018a; Winata et al., 2019a). Our study focuses on sentiment analysis in Tamil, which has little annotated data for code-mixed scenarios (Phani et al., 2016; Jose et al., 2020). Features based on the lexical properties such as a dictionary of words and parts of speech tagging have less performance compared to the supervised learning (Kannan et al., 2016) approaches using annotated data. However, an annotated corpus developed for monolingual data cannot deal with code-mixed usage and therefore it fails to yield good results (AlGhamdi et al., 2016; Aguilar et al., 2018) due to mixture of languages at different levels of linguistic analysis. (2016) and without annotating the word level language tag. The instructions enabled light and speedy annotati"
2020.sltu-1.28,P18-1143,0,0.162974,"s paper, we describe the process of creating the corpus and assigning polarities. We present inter-annotator agreement and show the results of sentiment analysis trained on this corpus as a benchmark. Keywords: code mixed, Tamil, sentiment, corpus, dataset 1. Introduction Sentiment analysis has become important in social media research (Yang and Eisenstein, 2017). Until recently these applications were created for high-resourced languages which analysed monolingual utterances. But social media in multilingual communities contains more code-mixed text (Barman et al., 2014; Chanda et al., 2016; Pratapa et al., 2018a; Winata et al., 2019a). Our study focuses on sentiment analysis in Tamil, which has little annotated data for code-mixed scenarios (Phani et al., 2016; Jose et al., 2020). Features based on the lexical properties such as a dictionary of words and parts of speech tagging have less performance compared to the supervised learning (Kannan et al., 2016) approaches using annotated data. However, an annotated corpus developed for monolingual data cannot deal with code-mixed usage and therefore it fails to yield good results (AlGhamdi et al., 2016; Aguilar et al., 2018) due to mixture of languages a"
2020.sltu-1.28,D18-1344,0,0.108638,"s paper, we describe the process of creating the corpus and assigning polarities. We present inter-annotator agreement and show the results of sentiment analysis trained on this corpus as a benchmark. Keywords: code mixed, Tamil, sentiment, corpus, dataset 1. Introduction Sentiment analysis has become important in social media research (Yang and Eisenstein, 2017). Until recently these applications were created for high-resourced languages which analysed monolingual utterances. But social media in multilingual communities contains more code-mixed text (Barman et al., 2014; Chanda et al., 2016; Pratapa et al., 2018a; Winata et al., 2019a). Our study focuses on sentiment analysis in Tamil, which has little annotated data for code-mixed scenarios (Phani et al., 2016; Jose et al., 2020). Features based on the lexical properties such as a dictionary of words and parts of speech tagging have less performance compared to the supervised learning (Kannan et al., 2016) approaches using annotated data. However, an annotated corpus developed for monolingual data cannot deal with code-mixed usage and therefore it fails to yield good results (AlGhamdi et al., 2016; Aguilar et al., 2018) due to mixture of languages a"
2020.sltu-1.28,2020.trac-1.7,1,0.768204,"a-embedding, 1DConv-LSTM and BERT on our code-mixed data for sentiment classification. 2. Related Work Recently, there has been a considerable amount of work and effort to collect resources for code-switched text. However, code-switched datasets and lexicons for sentiment analysis are still limited in number, size and availability. For monolingual analysis, there exist various corpora for English (Hu and Liu, 2004; Wiebe et al., 2005; Jiang et al., 2019), Russian (Rogers et al., 2018), German (Cieliebak et al., 2017), Norwegian (Mæhlum et al., 2019) and Indian languages (Agrawal et al., 2018; Rani et al., 2020). When it comes to code-mixing, an English-Hindi corpus was created by (Sitaram et al., 2015; Joshi et al., 2016; Patra et al., 2018), an English-Spanish corpus was introduced by (Solorio et al., 2014; Vilares et al., 2015; Vilares et al., 2016), and a Chinese-English one (Lee and Wang, 2015) was collected from Weibo.com and English-Bengali data were released by Patra et al. (Patra et al., 2018). Tamil is a Dravidian language spoken by Tamil people in India, Sri Lanka and by the Tamil diaspora around the world, with official recognition in India, Sri Lanka and Singapore (Chakravarthi et al., 2"
2020.sltu-1.28,C18-1064,0,0.0402104,"analysis of logistic regression, naive Bayes, decision tree, random forest, SVM, dynamic meta-embedding, contextualized dynamic meta-embedding, 1DConv-LSTM and BERT on our code-mixed data for sentiment classification. 2. Related Work Recently, there has been a considerable amount of work and effort to collect resources for code-switched text. However, code-switched datasets and lexicons for sentiment analysis are still limited in number, size and availability. For monolingual analysis, there exist various corpora for English (Hu and Liu, 2004; Wiebe et al., 2005; Jiang et al., 2019), Russian (Rogers et al., 2018), German (Cieliebak et al., 2017), Norwegian (Mæhlum et al., 2019) and Indian languages (Agrawal et al., 2018; Rani et al., 2020). When it comes to code-mixing, an English-Hindi corpus was created by (Sitaram et al., 2015; Joshi et al., 2016; Patra et al., 2018), an English-Spanish corpus was introduced by (Solorio et al., 2014; Vilares et al., 2015; Vilares et al., 2016), and a Chinese-English one (Lee and Wang, 2015) was collected from Weibo.com and English-Bengali data were released by Patra et al. (Patra et al., 2018). Tamil is a Dravidian language spoken by Tamil people in India, Sri Lank"
2020.sltu-1.28,W14-3907,0,0.0321745,"-switched text. However, code-switched datasets and lexicons for sentiment analysis are still limited in number, size and availability. For monolingual analysis, there exist various corpora for English (Hu and Liu, 2004; Wiebe et al., 2005; Jiang et al., 2019), Russian (Rogers et al., 2018), German (Cieliebak et al., 2017), Norwegian (Mæhlum et al., 2019) and Indian languages (Agrawal et al., 2018; Rani et al., 2020). When it comes to code-mixing, an English-Hindi corpus was created by (Sitaram et al., 2015; Joshi et al., 2016; Patra et al., 2018), an English-Spanish corpus was introduced by (Solorio et al., 2014; Vilares et al., 2015; Vilares et al., 2016), and a Chinese-English one (Lee and Wang, 2015) was collected from Weibo.com and English-Bengali data were released by Patra et al. (Patra et al., 2018). Tamil is a Dravidian language spoken by Tamil people in India, Sri Lanka and by the Tamil diaspora around the world, with official recognition in India, Sri Lanka and Singapore (Chakravarthi et al., 2018; Chakravarthi et al., 2019a; Chakravarthi et al., 2019b; Chakravarthi et al., 2019c). Several research activities on sentiment analysis in Tamil (Padmamala and Prema, 2017) and other Indian langua"
2020.sltu-1.28,2020.trac-1.6,1,0.771907,"lease, everybody will celebrate the hero. Tamil words written in Roman script with no English switch. Code-mixing is common among speakers in a bilingual speech community. As English is seen as the language of prestige and education, the influence of lexicon, connectives and phrases from English language is common in spoken Tamil. It is largely observed in educated speakers although not completely absent amongst less educated and uneducated speakers (Krishnasamy, 2015). Due to their pervasiveness of English online, code-mixed Tamil-English (Tanglish) sentences are often typed in Roman script (Suryawanshi et al., 2020a; Suryawanshi et al., 2020b). We present TamilMixSentiment 1 , a dataset of YouTube video comments in Tanglish. TamilMixSentiment was developed with guidelines following the work of Mohammad 1 https://github.com/bharathichezhiyan/TamilMixSentiment • Trailer late ah parthavanga like podunga. - Those who watched the trailer late, please like it. Tag switching with English words. • Omg .. use head phones. Enna bgm da saami .. - OMG! Use your headphones. Good Lord, What a background score! Inter-sentential switch • I think sivakarthickku hero getup set aagala. - I think the hero role does not sui"
2020.sltu-1.28,D19-5018,0,0.0212976,"thesis activity to the given evidence activity. We evaluate the MNB model with our data using α=1 with TF-IDF vectors. 5.1.7. 1DConv-LSTM: The model we evaluated consists of Embedding layer, Dropout, 1DConv with activation ReLU, Max-pooling and LSTM. The embeddings are randomly initialized. 5.1.8. BERT-Multilingual: Devlin et al. (2019) introduced a language representation model which is Bidirectional Encoder Representation from Transforms. It is designed to pre-train from unlabelled text and can be fine-tuned by adding last layer. BERT has been used for many text classification tasks (Tayyar Madabushi et al., 2019; Ma et al., 2019; Cohan et al., 2019). We explore classification of a code-mixed data into their corresponding sentiment categories. 5.1.9. DME and CDME: We also implemented the Dynamic Meta Embedding (Kiela et al., 2018) to evaluate our model. As a first step, we used Word2Vec and FastText to train from our dataset since dy206 namic meta-embedding is an effective method for the supervised learning of embedding ensembles. 5.2. Experiment Results and Discussion The experimental results of the sentiment classification task using different methods are shown in terms of precision in Table 4, reca"
2020.sltu-1.28,W15-2902,0,0.0574732,"Missing"
2020.sltu-1.28,L16-1655,0,0.0389141,"Missing"
2020.sltu-1.28,W19-4320,0,0.0811764,"e process of creating the corpus and assigning polarities. We present inter-annotator agreement and show the results of sentiment analysis trained on this corpus as a benchmark. Keywords: code mixed, Tamil, sentiment, corpus, dataset 1. Introduction Sentiment analysis has become important in social media research (Yang and Eisenstein, 2017). Until recently these applications were created for high-resourced languages which analysed monolingual utterances. But social media in multilingual communities contains more code-mixed text (Barman et al., 2014; Chanda et al., 2016; Pratapa et al., 2018a; Winata et al., 2019a). Our study focuses on sentiment analysis in Tamil, which has little annotated data for code-mixed scenarios (Phani et al., 2016; Jose et al., 2020). Features based on the lexical properties such as a dictionary of words and parts of speech tagging have less performance compared to the supervised learning (Kannan et al., 2016) approaches using annotated data. However, an annotated corpus developed for monolingual data cannot deal with code-mixed usage and therefore it fails to yield good results (AlGhamdi et al., 2016; Aguilar et al., 2018) due to mixture of languages at different levels of"
2020.sltu-1.28,D19-1360,0,0.130653,"e process of creating the corpus and assigning polarities. We present inter-annotator agreement and show the results of sentiment analysis trained on this corpus as a benchmark. Keywords: code mixed, Tamil, sentiment, corpus, dataset 1. Introduction Sentiment analysis has become important in social media research (Yang and Eisenstein, 2017). Until recently these applications were created for high-resourced languages which analysed monolingual utterances. But social media in multilingual communities contains more code-mixed text (Barman et al., 2014; Chanda et al., 2016; Pratapa et al., 2018a; Winata et al., 2019a). Our study focuses on sentiment analysis in Tamil, which has little annotated data for code-mixed scenarios (Phani et al., 2016; Jose et al., 2020). Features based on the lexical properties such as a dictionary of words and parts of speech tagging have less performance compared to the supervised learning (Kannan et al., 2016) approaches using annotated data. However, an annotated corpus developed for monolingual data cannot deal with code-mixed usage and therefore it fails to yield good results (AlGhamdi et al., 2016; Aguilar et al., 2018) due to mixture of languages at different levels of"
2020.sltu-1.28,Q17-1021,0,0.0282999,"of annotated code-mixed data for a low-resourced language like Tamil also adds difficulty to this problem. To overcome this, we created a gold standard Tamil-English code-switched, sentiment-annotated corpus containing 15,744 comment posts from YouTube. In this paper, we describe the process of creating the corpus and assigning polarities. We present inter-annotator agreement and show the results of sentiment analysis trained on this corpus as a benchmark. Keywords: code mixed, Tamil, sentiment, corpus, dataset 1. Introduction Sentiment analysis has become important in social media research (Yang and Eisenstein, 2017). Until recently these applications were created for high-resourced languages which analysed monolingual utterances. But social media in multilingual communities contains more code-mixed text (Barman et al., 2014; Chanda et al., 2016; Pratapa et al., 2018a; Winata et al., 2019a). Our study focuses on sentiment analysis in Tamil, which has little annotated data for code-mixed scenarios (Phani et al., 2016; Jose et al., 2020). Features based on the lexical properties such as a dictionary of words and parts of speech tagging have less performance compared to the supervised learning (Kannan et al."
2020.trac-1.7,2020.sltu-1.28,1,0.768204,"arner and Hirschberg, 2012; Zimmerman et al., 2018; MacAvaney et al., 2019; Ibrohim and Budi, 2019; Nobata et al., 2016). Code mixing is a phenomenon which occurs when the speaker uses two languages together in the course of a single utterance (Wardhaugh, 1986; Chakravarthi et al., 2018; Chakravarthi et al., 2019). The speaker makes use of the grammar or lexicon from more than one language. It is considered as a natural and common phenomenon in multilingual societies and is reflected in usergenerated content on social media (Ranjan et al., 2016; Jose et al., 2020; Priyadharshini et al., 2020; Chakravarthi et al., 2020b; Chakravarthi et al., 2020a). The task of identifying hate speech becomes even more challenging when the content is code-mixed since lexical items, phrases and sentences from different languages may co-exist within a sequence, and computational models are required to recognize and process these simultaneously. Hate Speech is common on social media, and content generated by Indianlanguage speakers is no exception (Suryawanshi et al., 2020a; Suryawanshi et al., 2020b). It assumes an additional significance due to high internet infiltration and rich linguistic diversity. In addition to this, th"
2020.trac-1.7,W19-3506,0,0.0227701,"ethnicity, religion, disability, gender or sexual orientation (Schmidt and Wiegand, 2017). Due to the massive rise in user-generated content from social media, hate speech has also steadily increased. Hate speech, targeting a particular individual or group of people, can cause personal trauma, cyberbullying, panic in the society, and discrimination. In response to the growth in the hate content from social media, there has been a large number of works on automatic hate speech detection to alleviate online harassment (Warner and Hirschberg, 2012; Zimmerman et al., 2018; MacAvaney et al., 2019; Ibrohim and Budi, 2019; Nobata et al., 2016). Code mixing is a phenomenon which occurs when the speaker uses two languages together in the course of a single utterance (Wardhaugh, 1986; Chakravarthi et al., 2018; Chakravarthi et al., 2019). The speaker makes use of the grammar or lexicon from more than one language. It is considered as a natural and common phenomenon in multilingual societies and is reflected in usergenerated content on social media (Ranjan et al., 2016; Jose et al., 2020; Priyadharshini et al., 2020; Chakravarthi et al., 2020b; Chakravarthi et al., 2020a). The task of identifying hate speech becom"
2020.trac-1.7,L18-1226,0,0.051663,"Missing"
2020.trac-1.7,malmasi-zampieri-2017-detecting,0,0.0747166,"Missing"
2020.trac-1.7,W18-3504,0,0.0683268,"Missing"
2020.trac-1.7,W18-1105,0,0.424363,"s and familiarity with English, which adds to • An annotated Hindi-English code-mixed data set containing hate speech. To the best of our knowledge, this is the first Hindi-English code-mixed data set which contains posts/tweets written in both the Roman and the native Devanagari script. • A comparative study of performance of five different classifiers including machine learning and deep learning on the three different Hindi-English code-mixed data sets. • An extensive discussion of the micro F1 score of all the trained models for each data set, not provided in the experiments reported on by Bohra et al. (2018). We have also evaluated the performance of the classifiers and deep learning model on the same data set used by Bohra et al. (2018). The rest of the paper is organized as follows. We explain related works in Section 2. Section 3. presents the details of the data set. Section 4. reports on approaches we used to classify the hate speech content. In Section 5., we present our results accompanied by a detailed error analysis. Section 6. concludes the paper. 2. Related Work In the digital era of the global world, various areas of research have studied computer-mediated communication from different"
2020.trac-1.7,2018.gwc-1.10,1,0.714384,"Missing"
2020.trac-1.7,W19-6809,1,0.402546,"targeting a particular individual or group of people, can cause personal trauma, cyberbullying, panic in the society, and discrimination. In response to the growth in the hate content from social media, there has been a large number of works on automatic hate speech detection to alleviate online harassment (Warner and Hirschberg, 2012; Zimmerman et al., 2018; MacAvaney et al., 2019; Ibrohim and Budi, 2019; Nobata et al., 2016). Code mixing is a phenomenon which occurs when the speaker uses two languages together in the course of a single utterance (Wardhaugh, 1986; Chakravarthi et al., 2018; Chakravarthi et al., 2019). The speaker makes use of the grammar or lexicon from more than one language. It is considered as a natural and common phenomenon in multilingual societies and is reflected in usergenerated content on social media (Ranjan et al., 2016; Jose et al., 2020; Priyadharshini et al., 2020; Chakravarthi et al., 2020b; Chakravarthi et al., 2020a). The task of identifying hate speech becomes even more challenging when the content is code-mixed since lexical items, phrases and sentences from different languages may co-exist within a sequence, and computational models are required to recognize and proces"
2020.trac-1.7,2020.sltu-1.25,1,0.775004,"arner and Hirschberg, 2012; Zimmerman et al., 2018; MacAvaney et al., 2019; Ibrohim and Budi, 2019; Nobata et al., 2016). Code mixing is a phenomenon which occurs when the speaker uses two languages together in the course of a single utterance (Wardhaugh, 1986; Chakravarthi et al., 2018; Chakravarthi et al., 2019). The speaker makes use of the grammar or lexicon from more than one language. It is considered as a natural and common phenomenon in multilingual societies and is reflected in usergenerated content on social media (Ranjan et al., 2016; Jose et al., 2020; Priyadharshini et al., 2020; Chakravarthi et al., 2020b; Chakravarthi et al., 2020a). The task of identifying hate speech becomes even more challenging when the content is code-mixed since lexical items, phrases and sentences from different languages may co-exist within a sequence, and computational models are required to recognize and process these simultaneously. Hate Speech is common on social media, and content generated by Indianlanguage speakers is no exception (Suryawanshi et al., 2020a; Suryawanshi et al., 2020b). It assumes an additional significance due to high internet infiltration and rich linguistic diversity. In addition to this, th"
2020.trac-1.7,W17-1101,0,0.0352889,"Convolutional Neural Networks 1. Introduction the overall complexity of the problem. While there is some relevant and independent work on code-mixed social media content, few efforts have been made to detect hate speech in Hindi-English code-mixed data. In the light of the gap in this research area, our contributions described in this paper are the following: Hate speech is a direct or indirect statement targeted towards a person or group of people intended to demean and brutalize another or use derogatory language on the basis of ethnicity, religion, disability, gender or sexual orientation (Schmidt and Wiegand, 2017). Due to the massive rise in user-generated content from social media, hate speech has also steadily increased. Hate speech, targeting a particular individual or group of people, can cause personal trauma, cyberbullying, panic in the society, and discrimination. In response to the growth in the hate content from social media, there has been a large number of works on automatic hate speech detection to alleviate online harassment (Warner and Hirschberg, 2012; Zimmerman et al., 2018; MacAvaney et al., 2019; Ibrohim and Budi, 2019; Nobata et al., 2016). Code mixing is a phenomenon which occurs wh"
2020.trac-1.7,2020.trac-1.6,1,0.842073,"n in multilingual societies and is reflected in usergenerated content on social media (Ranjan et al., 2016; Jose et al., 2020; Priyadharshini et al., 2020; Chakravarthi et al., 2020b; Chakravarthi et al., 2020a). The task of identifying hate speech becomes even more challenging when the content is code-mixed since lexical items, phrases and sentences from different languages may co-exist within a sequence, and computational models are required to recognize and process these simultaneously. Hate Speech is common on social media, and content generated by Indianlanguage speakers is no exception (Suryawanshi et al., 2020a; Suryawanshi et al., 2020b). It assumes an additional significance due to high internet infiltration and rich linguistic diversity. In addition to this, the use of the Roman script for Indian languages mixed with native scripts is widespread among social networking sites due to difficulty in typing tools and familiarity with English, which adds to • An annotated Hindi-English code-mixed data set containing hate speech. To the best of our knowledge, this is the first Hindi-English code-mixed data set which contains posts/tweets written in both the Roman and the native Devanagari script. • A c"
2020.trac-1.7,W12-2103,0,0.106669,"d to demean and brutalize another or use derogatory language on the basis of ethnicity, religion, disability, gender or sexual orientation (Schmidt and Wiegand, 2017). Due to the massive rise in user-generated content from social media, hate speech has also steadily increased. Hate speech, targeting a particular individual or group of people, can cause personal trauma, cyberbullying, panic in the society, and discrimination. In response to the growth in the hate content from social media, there has been a large number of works on automatic hate speech detection to alleviate online harassment (Warner and Hirschberg, 2012; Zimmerman et al., 2018; MacAvaney et al., 2019; Ibrohim and Budi, 2019; Nobata et al., 2016). Code mixing is a phenomenon which occurs when the speaker uses two languages together in the course of a single utterance (Wardhaugh, 1986; Chakravarthi et al., 2018; Chakravarthi et al., 2019). The speaker makes use of the grammar or lexicon from more than one language. It is considered as a natural and common phenomenon in multilingual societies and is reflected in usergenerated content on social media (Ranjan et al., 2016; Jose et al., 2020; Priyadharshini et al., 2020; Chakravarthi et al., 2020b"
2020.trac-1.7,L18-1404,0,0.0269494,"ther or use derogatory language on the basis of ethnicity, religion, disability, gender or sexual orientation (Schmidt and Wiegand, 2017). Due to the massive rise in user-generated content from social media, hate speech has also steadily increased. Hate speech, targeting a particular individual or group of people, can cause personal trauma, cyberbullying, panic in the society, and discrimination. In response to the growth in the hate content from social media, there has been a large number of works on automatic hate speech detection to alleviate online harassment (Warner and Hirschberg, 2012; Zimmerman et al., 2018; MacAvaney et al., 2019; Ibrohim and Budi, 2019; Nobata et al., 2016). Code mixing is a phenomenon which occurs when the speaker uses two languages together in the course of a single utterance (Wardhaugh, 1986; Chakravarthi et al., 2018; Chakravarthi et al., 2019). The speaker makes use of the grammar or lexicon from more than one language. It is considered as a natural and common phenomenon in multilingual societies and is reflected in usergenerated content on social media (Ranjan et al., 2016; Jose et al., 2020; Priyadharshini et al., 2020; Chakravarthi et al., 2020b; Chakravarthi et al., 2"
2020.vardial-1.6,D16-1250,0,0.0266056,"he induced translation can improve MT systems (Golan et al., 1988) to expand the coverage of translation models by translating Out-Of-Vocabulary (OOV) words. Nevertheless, prior work in BLI treated it as stand-alone task (Irvine and Callison-Burch, 2017). Using monolingual word embeddings for BLI has attracted significant attention in recent years. Stateof-the-art BLI results are based on bilingual word embedding models (Irvine and Callison-Burch, 2017). Given the source and target language word embeddings trained independently on monolingual data, unsupervised models (Vuli´c and Moens, 2015; Artetxe et al., 2016; Zhang et al., 2017; Artetxe et al., 2017; Artetxe et al., 2018; Riley and Gildea, 2018; Artetxe et al., 2019) learn a linear mapping W between the source and target space such that: W ∗ = arg min W XX i Dij kXi∗ W − Zj∗ k2 , (1) j where X and Z are two aligned matrices of embedding size d containing the embeddings of the words in the parallel vocabulary. The vocabulary of each language are Vs and Vt , and D ∈ {0, 1}|Vs |×|Vt |is a binary matrix representing a dictionary such that Dij = 1 if the i-th word in the source language is aligned with the j-th word in the target language. Equation (1"
2020.vardial-1.6,P17-1042,0,0.391423,"ems (Golan et al., 1988) to expand the coverage of translation models by translating Out-Of-Vocabulary (OOV) words. Nevertheless, prior work in BLI treated it as stand-alone task (Irvine and Callison-Burch, 2017). Using monolingual word embeddings for BLI has attracted significant attention in recent years. Stateof-the-art BLI results are based on bilingual word embedding models (Irvine and Callison-Burch, 2017). Given the source and target language word embeddings trained independently on monolingual data, unsupervised models (Vuli´c and Moens, 2015; Artetxe et al., 2016; Zhang et al., 2017; Artetxe et al., 2017; Artetxe et al., 2018; Riley and Gildea, 2018; Artetxe et al., 2019) learn a linear mapping W between the source and target space such that: W ∗ = arg min W XX i Dij kXi∗ W − Zj∗ k2 , (1) j where X and Z are two aligned matrices of embedding size d containing the embeddings of the words in the parallel vocabulary. The vocabulary of each language are Vs and Vt , and D ∈ {0, 1}|Vs |×|Vt |is a binary matrix representing a dictionary such that Dij = 1 if the i-th word in the source language is aligned with the j-th word in the target language. Equation (1) is equivelent to:  W ∗ = arg max Tr XW"
2020.vardial-1.6,P18-1073,0,0.122315,"re languages from monolingual corpora (Irvine and Callison-Burch, 2017). It is a time-consuming process to do it manually so automatically inducing bilingual lexicons based on edit-distance (Haghighi et al., 2008), comparable corpora (Turcato, 1998), bilingual corpora (Rosner and Sultana, 2014) or pretrained embeddings from monolingual corpora (Vuli´c and Moens, 2015) is more suitable. However, sentence-aligned parallel data is not available for all languages. Methods based on unsupervised or semi-supervised learning can utilise readily available monolingual data to induce bilingual lexicons. Artetxe et al. (2018) showed that an iterative self-learning method could bootstrap this approach without the need of a seed dictionary by utilising numbers as seed dictionary through adversarial training. However, Patra et al. (2019) showed that with even a small seed dictionary, the results could be improved considerably. This task is further complicated by the fact that many languages use distinct scripts, and as such learning the similarities between cognates is a non-trivial task. As such, BLI is a challenging task for under-resourced languages due to lack of seed dictionaries and large monolingual corpora. F"
2020.vardial-1.6,P19-1494,0,0.0166961,"s by translating Out-Of-Vocabulary (OOV) words. Nevertheless, prior work in BLI treated it as stand-alone task (Irvine and Callison-Burch, 2017). Using monolingual word embeddings for BLI has attracted significant attention in recent years. Stateof-the-art BLI results are based on bilingual word embedding models (Irvine and Callison-Burch, 2017). Given the source and target language word embeddings trained independently on monolingual data, unsupervised models (Vuli´c and Moens, 2015; Artetxe et al., 2016; Zhang et al., 2017; Artetxe et al., 2017; Artetxe et al., 2018; Riley and Gildea, 2018; Artetxe et al., 2019) learn a linear mapping W between the source and target space such that: W ∗ = arg min W XX i Dij kXi∗ W − Zj∗ k2 , (1) j where X and Z are two aligned matrices of embedding size d containing the embeddings of the words in the parallel vocabulary. The vocabulary of each language are Vs and Vt , and D ∈ {0, 1}|Vs |×|Vt |is a binary matrix representing a dictionary such that Dij = 1 if the i-th word in the source language is aligned with the j-th word in the target language. Equation (1) is equivelent to:  W ∗ = arg max Tr XW Z T DT , (2) W where Tr(·) is the trace operator (the sum of all diag"
2020.vardial-1.6,P19-1302,0,0.0747503,"a similarity measure of two or more strings to find the longest subsequence common to all sequence in two or more strings. LCS was previously used to extract the morphological variations and generate lexicons (Hulden et al., 2014; Sorokin, 2016). LCS was also used to identify cognate candidates during the construction of N -best translation lexicons from parallel texts (Melamed, 1995; Kondrak et al., 2003), and for the automatic evaluation of translation quality (Lin and Och, 2004). Recent work on the creation of a large-scale multilingual lexical database based on cognates was introduced by Batsuren et al. (2019), called CogNet, which uses LCS ratio to find cognates. 60 Karakanta et al. (2018) also used the LCS ratio to extract cognate pairs from Wikipedia titles between Russian and Belarusian. Inspired by this, we use LCS for our work. A sequence Z = [zl , z2 , . . . , zn ] is a subsequence of another sequence X = [x1 , x2 , . . . , xm ] , if there exists a sequence [il , i2 , . . . , ik ] of indices of X such that for all j = 1, 2, . . . , k, where xij = zj . Given two sequences X and Y , the LCS of X and Y is a common subsequence with maximum length. More formally:  if i = 0 or j = 0  ∅ LCS (Xi−1"
2020.vardial-1.6,N10-1083,0,0.0778924,"Missing"
2020.vardial-1.6,Q17-1010,0,0.0329446,"acted from wikimedia.org for Dravidian languages 4 Experimental Settings Words that share a similar context are semantically related. Based on their word embeddings, methods represent words in a vector space by grouping semantically similar words near each other. Word embeddings are useful for several lexical-semantic tasks such as detecting synonyms and disambiguating word sense. Several pre-trained embedding models are publicly available such as word2vec4 (Mikolov et al., 2013a; Mikolov et al., 2013b), global word representation-based models (GloVe)5 (Pennington et al., 2014) and FastText6 (Bojanowski et al., 2017; Grave et al., 2018). FastText was used to create monolingual embeddings from Wikipedia articles. FastText enhances traditional word-based vectors by representing each word as a bag of character n-grams. Incorporating this subword information from FastText embeddings as well as semantic relatedness allows the capturing of orthographic and morphological similarity. We did not use the pre-trained embeddings from FastText since we also created embedding for transliteration. Given that the main focus of our work is on bringing closely related languages into a single script, we transcribed the Wik"
2020.vardial-1.6,2018.gwc-1.10,1,0.779128,"transliteration, and we demonstrate that it is an effective and necessary step which yields more isomorphic embeddings and obtains more robust BLI. Second, we show that the use of the longest common subsequence (LCS) is a superior method of assessing the cognate similarity. 2 Dravidian Languages Dravidian languages are the common terminology (Caldwell, 1856) used to represent the South Indian languages, which consist of around 26 languages divided into four branches: 11 in the Southern group, 7 in the South-Central group, 5 in the Central group and 3 in the Northern group (Krishnamurti, 2003; Chakravarthi et al., 2018). Out of the 26 Dravidian languages, many of them are non-literary languages except the four languages chosen for this paper. Indigenous minority populations primarily use the non-literary languages. The modern society widely uses the four literary languages in literature, public communications, government institutions, academic settings and many other places in the day-to-day life of an ordinary person (Chakravarthi et al., 2020b; Chakravarthi et al., 2020a). For many natural language processing tasks such as machine translation (MT) systems, it is essential to have a corpus of written docume"
2020.vardial-1.6,W19-7101,1,0.544357,"The Tamil, Malayalam and Telugu languages have their own written script symbols whereas Telugu and Kannada have significant similarities in their script symbols. Though Telugu and Kannada have these similarities, they are not readily intelligible for speakers of the language. The study of languages suggests that these languages formed a single language around late 4000 BCE and then started evolving on their own (Steever, 2015). Since the languages evolved sharing geographical, etymological and political borders, the cognates may have evolved similar meanings or borrowed words from each other. Chakravarthi et al. (2019a) have compared the Latin script and the International Phonetic Alphabet (IPA) for multilingual 58 translation systems and shown that bringing the Dravidian languages into Latin script outperforms a multilingual neural machine translation system trained on native script and IPA. Inspired by this, we transform the Dravidian language monolingual corpora into a single script (Latin script). 3 3.1 Our Approach Bilingual Lexicon Induction State-of-the-art approaches to BLI use monolingual (Haghighi et al., 2008) or comparable corpora (Fung, 1995; Tamura et al., 2012) to identify pairs of translate"
2020.vardial-1.6,W19-6809,1,0.894503,"The Tamil, Malayalam and Telugu languages have their own written script symbols whereas Telugu and Kannada have significant similarities in their script symbols. Though Telugu and Kannada have these similarities, they are not readily intelligible for speakers of the language. The study of languages suggests that these languages formed a single language around late 4000 BCE and then started evolving on their own (Steever, 2015). Since the languages evolved sharing geographical, etymological and political borders, the cognates may have evolved similar meanings or borrowed words from each other. Chakravarthi et al. (2019a) have compared the Latin script and the International Phonetic Alphabet (IPA) for multilingual 58 translation systems and shown that bringing the Dravidian languages into Latin script outperforms a multilingual neural machine translation system trained on native script and IPA. Inspired by this, we transform the Dravidian language monolingual corpora into a single script (Latin script). 3 3.1 Our Approach Bilingual Lexicon Induction State-of-the-art approaches to BLI use monolingual (Haghighi et al., 2008) or comparable corpora (Fung, 1995; Tamura et al., 2012) to identify pairs of translate"
2020.vardial-1.6,2020.sltu-1.25,1,0.768298,"languages divided into four branches: 11 in the Southern group, 7 in the South-Central group, 5 in the Central group and 3 in the Northern group (Krishnamurti, 2003; Chakravarthi et al., 2018). Out of the 26 Dravidian languages, many of them are non-literary languages except the four languages chosen for this paper. Indigenous minority populations primarily use the non-literary languages. The modern society widely uses the four literary languages in literature, public communications, government institutions, academic settings and many other places in the day-to-day life of an ordinary person (Chakravarthi et al., 2020b; Chakravarthi et al., 2020a). For many natural language processing tasks such as machine translation (MT) systems, it is essential to have a corpus of written documents, as well as well-defined lexicons and grammar for the selected languages (Chakravarthi et al., 2020c). Hence in this work, we will focus on the four chosen Dravidian languages Tamil, Malayalam, Kannada and Telugu which are spoken by approximately 210 million people (Steever, 2015) across the world either as their first or second language. Figure 1: Example of cognate words for the Dravidian language Even though these language"
2020.vardial-1.6,2020.sltu-1.28,1,0.746996,"languages divided into four branches: 11 in the Southern group, 7 in the South-Central group, 5 in the Central group and 3 in the Northern group (Krishnamurti, 2003; Chakravarthi et al., 2018). Out of the 26 Dravidian languages, many of them are non-literary languages except the four languages chosen for this paper. Indigenous minority populations primarily use the non-literary languages. The modern society widely uses the four literary languages in literature, public communications, government institutions, academic settings and many other places in the day-to-day life of an ordinary person (Chakravarthi et al., 2020b; Chakravarthi et al., 2020a). For many natural language processing tasks such as machine translation (MT) systems, it is essential to have a corpus of written documents, as well as well-defined lexicons and grammar for the selected languages (Chakravarthi et al., 2020c). Hence in this work, we will focus on the four chosen Dravidian languages Tamil, Malayalam, Kannada and Telugu which are spoken by approximately 210 million people (Steever, 2015) across the world either as their first or second language. Figure 1: Example of cognate words for the Dravidian language Even though these language"
2020.vardial-1.6,P11-1042,0,0.019873,"t the usage of Latin script outperforms the IPA for Multilingual NMT for Dravidian languages. This was proven with a cosine similarity of the corpus showing that transcribing the text into the Latin script retain more similarity. Inspired by this, we used the Indic-trans library by Bhat et al. (2015) to transliterate. We show the example of a comparison of NLCS and NL between languages for examples of cognate words in Table 1. Previous methods based on edit-distance and orthographic similarity are proposed for using linguist features for word alignments by supervised and unsupervised methods (Dyer et al., 2011; Berg-Kirkpatrick et al., 2010; Hauer et al., 2017). Hauer et al. (2017) created a seed dictionary based on the cognates of 61 Language Pairs kan-mal kan-mal kan-tam kan-tam kan-tel kan-tel mal-tam mal-tam mal-tel mal-tel tam-tel tam-tel Word Pair English Translation NLCS NL hajaradant-hajarulla rahasyadaan-rahasyadan navratn-navmani tandeilladant-tacoppanillat poojaniyavadantah-poojyaniyulu atyagatyavadant-atyavasaramin navaratnam-navmani tatanillat-tacoppanillat navaratnam-navratnalu tatanillat-tandriless tacoppanillat-tandriless sammadam-samardhinchada arriving secret nine gems having no l"
2020.vardial-1.6,W95-0114,0,0.299749,"gs or borrowed words from each other. Chakravarthi et al. (2019a) have compared the Latin script and the International Phonetic Alphabet (IPA) for multilingual 58 translation systems and shown that bringing the Dravidian languages into Latin script outperforms a multilingual neural machine translation system trained on native script and IPA. Inspired by this, we transform the Dravidian language monolingual corpora into a single script (Latin script). 3 3.1 Our Approach Bilingual Lexicon Induction State-of-the-art approaches to BLI use monolingual (Haghighi et al., 2008) or comparable corpora (Fung, 1995; Tamura et al., 2012) to identify pairs of translated words with or without a seed dictionary (Vuli´c and Korhonen, 2016). The induced translation can improve MT systems (Golan et al., 1988) to expand the coverage of translation models by translating Out-Of-Vocabulary (OOV) words. Nevertheless, prior work in BLI treated it as stand-alone task (Irvine and Callison-Burch, 2017). Using monolingual word embeddings for BLI has attracted significant attention in recent years. Stateof-the-art BLI results are based on bilingual word embedding models (Irvine and Callison-Burch, 2017). Given the source"
2020.vardial-1.6,C88-1042,0,0.579926,"and shown that bringing the Dravidian languages into Latin script outperforms a multilingual neural machine translation system trained on native script and IPA. Inspired by this, we transform the Dravidian language monolingual corpora into a single script (Latin script). 3 3.1 Our Approach Bilingual Lexicon Induction State-of-the-art approaches to BLI use monolingual (Haghighi et al., 2008) or comparable corpora (Fung, 1995; Tamura et al., 2012) to identify pairs of translated words with or without a seed dictionary (Vuli´c and Korhonen, 2016). The induced translation can improve MT systems (Golan et al., 1988) to expand the coverage of translation models by translating Out-Of-Vocabulary (OOV) words. Nevertheless, prior work in BLI treated it as stand-alone task (Irvine and Callison-Burch, 2017). Using monolingual word embeddings for BLI has attracted significant attention in recent years. Stateof-the-art BLI results are based on bilingual word embedding models (Irvine and Callison-Burch, 2017). Given the source and target language word embeddings trained independently on monolingual data, unsupervised models (Vuli´c and Moens, 2015; Artetxe et al., 2016; Zhang et al., 2017; Artetxe et al., 2017; Ar"
2020.vardial-1.6,L18-1550,0,0.0137973,"for Dravidian languages 4 Experimental Settings Words that share a similar context are semantically related. Based on their word embeddings, methods represent words in a vector space by grouping semantically similar words near each other. Word embeddings are useful for several lexical-semantic tasks such as detecting synonyms and disambiguating word sense. Several pre-trained embedding models are publicly available such as word2vec4 (Mikolov et al., 2013a; Mikolov et al., 2013b), global word representation-based models (GloVe)5 (Pennington et al., 2014) and FastText6 (Bojanowski et al., 2017; Grave et al., 2018). FastText was used to create monolingual embeddings from Wikipedia articles. FastText enhances traditional word-based vectors by representing each word as a bag of character n-grams. Incorporating this subword information from FastText embeddings as well as semantic relatedness allows the capturing of orthographic and morphological similarity. We did not use the pre-trained embeddings from FastText since we also created embedding for transliteration. Given that the main focus of our work is on bringing closely related languages into a single script, we transcribed the Wikidump corpus before c"
2020.vardial-1.6,P08-1088,0,0.203984,"equence is linguistically more sound and improves the performance of bilingual lexicon induction. We show that our approach can increase the accuracy of bilingual lexicon induction methods on these languages many times, making bilingual lexicon induction approaches feasible for such under-resourced languages. 1 Introduction Bilingual lexicon induction (BLI) is the process of creating lexicons for two or more languages from monolingual corpora (Irvine and Callison-Burch, 2017). It is a time-consuming process to do it manually so automatically inducing bilingual lexicons based on edit-distance (Haghighi et al., 2008), comparable corpora (Turcato, 1998), bilingual corpora (Rosner and Sultana, 2014) or pretrained embeddings from monolingual corpora (Vuli´c and Moens, 2015) is more suitable. However, sentence-aligned parallel data is not available for all languages. Methods based on unsupervised or semi-supervised learning can utilise readily available monolingual data to induce bilingual lexicons. Artetxe et al. (2018) showed that an iterative self-learning method could bootstrap this approach without the need of a seed dictionary by utilising numbers as seed dictionary through adversarial training. However"
2020.vardial-1.6,E17-2098,0,0.0223466,"r Multilingual NMT for Dravidian languages. This was proven with a cosine similarity of the corpus showing that transcribing the text into the Latin script retain more similarity. Inspired by this, we used the Indic-trans library by Bhat et al. (2015) to transliterate. We show the example of a comparison of NLCS and NL between languages for examples of cognate words in Table 1. Previous methods based on edit-distance and orthographic similarity are proposed for using linguist features for word alignments by supervised and unsupervised methods (Dyer et al., 2011; Berg-Kirkpatrick et al., 2010; Hauer et al., 2017). Hauer et al. (2017) created a seed dictionary based on the cognates of 61 Language Pairs kan-mal kan-mal kan-tam kan-tam kan-tel kan-tel mal-tam mal-tam mal-tel mal-tel tam-tel tam-tel Word Pair English Translation NLCS NL hajaradant-hajarulla rahasyadaan-rahasyadan navratn-navmani tandeilladant-tacoppanillat poojaniyavadantah-poojyaniyulu atyagatyavadant-atyavasaramin navaratnam-navmani tatanillat-tacoppanillat navaratnam-navratnalu tatanillat-tandriless tacoppanillat-tandriless sammadam-samardhinchada arriving secret nine gems having no living father worthy of adoration primary nine gems h"
2020.vardial-1.6,P18-4003,0,0.0247781,"s a phonetic alphabet, such as the International Phonetic Alphabet (IPA); however, transcribing into the Latin script or non-native script is prevalent due to the ubiquity of the US/UK keyboard. The IPA is an evolving standard initially developed by the International Phonetic Association in 1888 with the goal of transcribing the sounds of all human languages. Transliteration is used to help language learners to read words written in foreign scripts, by writing the sound of the word using the equivalent letters. Romanisation remains a popular technique for transliteration of various languages (Hermjakob et al., 2018). The use of the Latin script for text entry of South Asian languages is common, even though there is no standard orthography for these languages in the script (Wolf-Sonkin et al., 2019). The 107 symbols used for writing the IPA are taken primarily from the Latin and Greek scripts some are novel creations. Diacritics are used for subtle distinctions in sounds and to show nasalisation of vowels, length, stress, and tones. Using IPA symbols, one can represent the pronunciation of words. Nevertheless, the study by Chakravarthi et al. (2019a) and Chakravarthi et al. (2019c) shows that transliterat"
2020.vardial-1.6,E14-1060,0,0.0242626,"e to 0. 3.2 Longest Common Subsequence The Levenshtein distance is a standard measure of the distance between two sequences by a minimum number of single-character edits required to map one string from another based on deletions, additions and substitution. This approach makes a binary decision about whether a pair of characters match. LCS (Paterson and Danˇc´ık, 1994; Melamed, 1999) is a similarity measure of two or more strings to find the longest subsequence common to all sequence in two or more strings. LCS was previously used to extract the morphological variations and generate lexicons (Hulden et al., 2014; Sorokin, 2016). LCS was also used to identify cognate candidates during the construction of N -best translation lexicons from parallel texts (Melamed, 1995; Kondrak et al., 2003), and for the automatic evaluation of translation quality (Lin and Och, 2004). Recent work on the creation of a large-scale multilingual lexical database based on cognates was introduced by Batsuren et al. (2019), called CogNet, which uses LCS ratio to find cognates. 60 Karakanta et al. (2018) also used the LCS ratio to extract cognate pairs from Wikipedia titles between Russian and Belarusian. Inspired by this, we u"
2020.vardial-1.6,J17-2001,0,0.132888,"used linguistically sub-optimal measures such as the Levenshtein edit distance to detect cognates, whereby we demonstrate that the longest common sub-sequence is linguistically more sound and improves the performance of bilingual lexicon induction. We show that our approach can increase the accuracy of bilingual lexicon induction methods on these languages many times, making bilingual lexicon induction approaches feasible for such under-resourced languages. 1 Introduction Bilingual lexicon induction (BLI) is the process of creating lexicons for two or more languages from monolingual corpora (Irvine and Callison-Burch, 2017). It is a time-consuming process to do it manually so automatically inducing bilingual lexicons based on edit-distance (Haghighi et al., 2008), comparable corpora (Turcato, 1998), bilingual corpora (Rosner and Sultana, 2014) or pretrained embeddings from monolingual corpora (Vuli´c and Moens, 2015) is more suitable. However, sentence-aligned parallel data is not available for all languages. Methods based on unsupervised or semi-supervised learning can utilise readily available monolingual data to induce bilingual lexicons. Artetxe et al. (2018) showed that an iterative self-learning method cou"
2020.vardial-1.6,N03-2016,0,0.480793,"e to lack of seed dictionaries and large monolingual corpora. For this work, we proposed to use the IndoWordNet as a seed dictionary for the closely related Dravidian languages, namely Tamil, Telugu, Kannada, and Malayalam, which use different scripts. BLI between closely-related languages has shown to perform better than unrelated languages (Irvine and Callison-Burch, 2017), since closely related languages often share similar linguistics properties and cognates (Nasution et al., 2016). Cognates are words that have a similar meaning and similar orthography based on etymological relationships (Kondrak et al., 2003). Computational models of monolingual embeddings also exhibit isomorphism across closely related languages (Mikolov et al., 2013b; Ormazabal This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http:// creativecommons.org/licenses/by/4.0/. 57 Proceedings of the 7th VarDial Workshop on NLP for Similar Languages, Varieties and Dialects, pages 57–69 Barcelona, Spain (Online), December 13, 2020 et al., 2019) based on the assumption that word embeddings in different languages have approximately the same structure. This isomorphic property was exploi"
2020.vardial-1.6,P04-1077,0,0.0187852,"This approach makes a binary decision about whether a pair of characters match. LCS (Paterson and Danˇc´ık, 1994; Melamed, 1999) is a similarity measure of two or more strings to find the longest subsequence common to all sequence in two or more strings. LCS was previously used to extract the morphological variations and generate lexicons (Hulden et al., 2014; Sorokin, 2016). LCS was also used to identify cognate candidates during the construction of N -best translation lexicons from parallel texts (Melamed, 1995; Kondrak et al., 2003), and for the automatic evaluation of translation quality (Lin and Och, 2004). Recent work on the creation of a large-scale multilingual lexical database based on cognates was introduced by Batsuren et al. (2019), called CogNet, which uses LCS ratio to find cognates. 60 Karakanta et al. (2018) also used the LCS ratio to extract cognate pairs from Wikipedia titles between Russian and Belarusian. Inspired by this, we use LCS for our work. A sequence Z = [zl , z2 , . . . , zn ] is a subsequence of another sequence X = [x1 , x2 , . . . , xm ] , if there exists a sequence [il , i2 , . . . , ik ] of indices of X such that for all j = 1, 2, . . . , k, where xij = zj . Given t"
2020.vardial-1.6,W95-0115,0,0.299926,"edits required to map one string from another based on deletions, additions and substitution. This approach makes a binary decision about whether a pair of characters match. LCS (Paterson and Danˇc´ık, 1994; Melamed, 1999) is a similarity measure of two or more strings to find the longest subsequence common to all sequence in two or more strings. LCS was previously used to extract the morphological variations and generate lexicons (Hulden et al., 2014; Sorokin, 2016). LCS was also used to identify cognate candidates during the construction of N -best translation lexicons from parallel texts (Melamed, 1995; Kondrak et al., 2003), and for the automatic evaluation of translation quality (Lin and Och, 2004). Recent work on the creation of a large-scale multilingual lexical database based on cognates was introduced by Batsuren et al. (2019), called CogNet, which uses LCS ratio to find cognates. 60 Karakanta et al. (2018) also used the LCS ratio to extract cognate pairs from Wikipedia titles between Russian and Belarusian. Inspired by this, we use LCS for our work. A sequence Z = [zl , z2 , . . . , zn ] is a subsequence of another sequence X = [x1 , x2 , . . . , xm ] , if there exists a sequence [il"
2020.vardial-1.6,J99-1003,0,0.210571,"w2 )) . The edit distance for a subset of possible word pairs is just considered as by how far most of word sets are orthographically unique, resulting in a normalised edit distance close to 1 and an orthographic similarity close to 0. 3.2 Longest Common Subsequence The Levenshtein distance is a standard measure of the distance between two sequences by a minimum number of single-character edits required to map one string from another based on deletions, additions and substitution. This approach makes a binary decision about whether a pair of characters match. LCS (Paterson and Danˇc´ık, 1994; Melamed, 1999) is a similarity measure of two or more strings to find the longest subsequence common to all sequence in two or more strings. LCS was previously used to extract the morphological variations and generate lexicons (Hulden et al., 2014; Sorokin, 2016). LCS was also used to identify cognate candidates during the construction of N -best translation lexicons from parallel texts (Melamed, 1995; Kondrak et al., 2003), and for the automatic evaluation of translation quality (Lin and Och, 2004). Recent work on the creation of a large-scale multilingual lexical database based on cognates was introduced"
2020.vardial-1.6,L16-1524,0,0.0276039,"earning the similarities between cognates is a non-trivial task. As such, BLI is a challenging task for under-resourced languages due to lack of seed dictionaries and large monolingual corpora. For this work, we proposed to use the IndoWordNet as a seed dictionary for the closely related Dravidian languages, namely Tamil, Telugu, Kannada, and Malayalam, which use different scripts. BLI between closely-related languages has shown to perform better than unrelated languages (Irvine and Callison-Burch, 2017), since closely related languages often share similar linguistics properties and cognates (Nasution et al., 2016). Cognates are words that have a similar meaning and similar orthography based on etymological relationships (Kondrak et al., 2003). Computational models of monolingual embeddings also exhibit isomorphism across closely related languages (Mikolov et al., 2013b; Ormazabal This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http:// creativecommons.org/licenses/by/4.0/. 57 Proceedings of the 7th VarDial Workshop on NLP for Similar Languages, Varieties and Dialects, pages 57–69 Barcelona, Spain (Online), December 13, 2020 et al., 2019) based on th"
2020.vardial-1.6,P19-1492,0,0.0277965,"Missing"
2020.vardial-1.6,P19-1018,0,0.0596185,"comparable corpora (Turcato, 1998), bilingual corpora (Rosner and Sultana, 2014) or pretrained embeddings from monolingual corpora (Vuli´c and Moens, 2015) is more suitable. However, sentence-aligned parallel data is not available for all languages. Methods based on unsupervised or semi-supervised learning can utilise readily available monolingual data to induce bilingual lexicons. Artetxe et al. (2018) showed that an iterative self-learning method could bootstrap this approach without the need of a seed dictionary by utilising numbers as seed dictionary through adversarial training. However, Patra et al. (2019) showed that with even a small seed dictionary, the results could be improved considerably. This task is further complicated by the fact that many languages use distinct scripts, and as such learning the similarities between cognates is a non-trivial task. As such, BLI is a challenging task for under-resourced languages due to lack of seed dictionaries and large monolingual corpora. For this work, we proposed to use the IndoWordNet as a seed dictionary for the closely related Dravidian languages, namely Tamil, Telugu, Kannada, and Malayalam, which use different scripts. BLI between closely-rel"
2020.vardial-1.6,D14-1162,0,0.0832826,"r of sentences and number of tokens extracted from wikimedia.org for Dravidian languages 4 Experimental Settings Words that share a similar context are semantically related. Based on their word embeddings, methods represent words in a vector space by grouping semantically similar words near each other. Word embeddings are useful for several lexical-semantic tasks such as detecting synonyms and disambiguating word sense. Several pre-trained embedding models are publicly available such as word2vec4 (Mikolov et al., 2013a; Mikolov et al., 2013b), global word representation-based models (GloVe)5 (Pennington et al., 2014) and FastText6 (Bojanowski et al., 2017; Grave et al., 2018). FastText was used to create monolingual embeddings from Wikipedia articles. FastText enhances traditional word-based vectors by representing each word as a bag of character n-grams. Incorporating this subword information from FastText embeddings as well as semantic relatedness allows the capturing of orthographic and morphological similarity. We did not use the pre-trained embeddings from FastText since we also created embedding for transliteration. Given that the main focus of our work is on bringing closely related languages into"
2020.vardial-1.6,W15-5910,0,0.0190922,"pts, and used Levenshtein distance without considering morphological properties. 3.4 Data Lexicons such as WordNet (Miller, 1995; Miller, 1998) for English or EuroWordNet for European languages are lexical resources which were used to improve MT quality. EuroWordNet is a cross-lingual synonym resource that linked WordNet synsets across European languages (Vossen, 1997). Similarly, IndoWordNet (Bhattacharyya, 2010) links WordNet synsets across major Indian languages from the Indo-Aryan, Dravidian and Sino-Tibetan families. An online multilingual dictionary for Indian languages was developed by Redkar et al. (2015) from IndoWordNet. However, this dictionary is not publicly accessible. To train and evaluate the quality of BLI, a seed dictionary and a test set of the bilingual lexicon is required. For the Dravidian languages, there is no existing seed dictionary, so we used the IndoWordNet. To create a seed dictionary, we used the IndoWordNet ID to link the WordNet entries for Tamil, Telugu, Malayalam and Kannada. We map the one-to-many word mapping from IndoWordNet to one-to-one word mapping by replicating the source word. Table 2 shows the seed dictionary statistics and Table 3 shows the statistics for"
2020.vardial-1.6,P18-2062,0,0.115953,"This isomorphic property was exploited by Artetxe et al. (2018) and Lample et al. (2018) to map monolingual word embeddings in different languages to a shared space through a linear transformation. For closely-related languages, it follows that cognates can be used as a form of alignment as words that have a similar form are quite likely to be cognates and therefore could be used as a weak seed dictionary. Given previous work on the use of seed dictionaries (Patra et al., 2019), the usage of such alignments is likely to improve performance of BLI. Previous works used the Levenshtein distance (Riley and Gildea, 2018); however, this is not linguistically well-motivated as it allows for multiple changes that are not consistent with the kinds of changes seen etymologically. The goal of this work is to exploit the orthographic information between languages that use a different script. For that purpose, we bring the languages into a single script, which allows us to take advantage of the cognate properties of closely related languages. This paper has two principal contributions: first, we study the use of transliteration, and we demonstrate that it is an effective and necessary step which yields more isomorphi"
2020.vardial-1.6,rosner-sultana-2014-automatic,0,0.0296233,"lexicon induction. We show that our approach can increase the accuracy of bilingual lexicon induction methods on these languages many times, making bilingual lexicon induction approaches feasible for such under-resourced languages. 1 Introduction Bilingual lexicon induction (BLI) is the process of creating lexicons for two or more languages from monolingual corpora (Irvine and Callison-Burch, 2017). It is a time-consuming process to do it manually so automatically inducing bilingual lexicons based on edit-distance (Haghighi et al., 2008), comparable corpora (Turcato, 1998), bilingual corpora (Rosner and Sultana, 2014) or pretrained embeddings from monolingual corpora (Vuli´c and Moens, 2015) is more suitable. However, sentence-aligned parallel data is not available for all languages. Methods based on unsupervised or semi-supervised learning can utilise readily available monolingual data to induce bilingual lexicons. Artetxe et al. (2018) showed that an iterative self-learning method could bootstrap this approach without the need of a seed dictionary by utilising numbers as seed dictionary through adversarial training. However, Patra et al. (2019) showed that with even a small seed dictionary, the results c"
2020.vardial-1.6,W16-2009,0,0.0248562,"ommon Subsequence The Levenshtein distance is a standard measure of the distance between two sequences by a minimum number of single-character edits required to map one string from another based on deletions, additions and substitution. This approach makes a binary decision about whether a pair of characters match. LCS (Paterson and Danˇc´ık, 1994; Melamed, 1999) is a similarity measure of two or more strings to find the longest subsequence common to all sequence in two or more strings. LCS was previously used to extract the morphological variations and generate lexicons (Hulden et al., 2014; Sorokin, 2016). LCS was also used to identify cognate candidates during the construction of N -best translation lexicons from parallel texts (Melamed, 1995; Kondrak et al., 2003), and for the automatic evaluation of translation quality (Lin and Och, 2004). Recent work on the creation of a large-scale multilingual lexical database based on cognates was introduced by Batsuren et al. (2019), called CogNet, which uses LCS ratio to find cognates. 60 Karakanta et al. (2018) also used the LCS ratio to extract cognate pairs from Wikipedia titles between Russian and Belarusian. Inspired by this, we use LCS for our w"
2020.vardial-1.6,D12-1003,0,0.020776,"ed words from each other. Chakravarthi et al. (2019a) have compared the Latin script and the International Phonetic Alphabet (IPA) for multilingual 58 translation systems and shown that bringing the Dravidian languages into Latin script outperforms a multilingual neural machine translation system trained on native script and IPA. Inspired by this, we transform the Dravidian language monolingual corpora into a single script (Latin script). 3 3.1 Our Approach Bilingual Lexicon Induction State-of-the-art approaches to BLI use monolingual (Haghighi et al., 2008) or comparable corpora (Fung, 1995; Tamura et al., 2012) to identify pairs of translated words with or without a seed dictionary (Vuli´c and Korhonen, 2016). The induced translation can improve MT systems (Golan et al., 1988) to expand the coverage of translation models by translating Out-Of-Vocabulary (OOV) words. Nevertheless, prior work in BLI treated it as stand-alone task (Irvine and Callison-Burch, 2017). Using monolingual word embeddings for BLI has attracted significant attention in recent years. Stateof-the-art BLI results are based on bilingual word embedding models (Irvine and Callison-Burch, 2017). Given the source and target language w"
2020.vardial-1.6,P98-2212,0,0.352374,"roves the performance of bilingual lexicon induction. We show that our approach can increase the accuracy of bilingual lexicon induction methods on these languages many times, making bilingual lexicon induction approaches feasible for such under-resourced languages. 1 Introduction Bilingual lexicon induction (BLI) is the process of creating lexicons for two or more languages from monolingual corpora (Irvine and Callison-Burch, 2017). It is a time-consuming process to do it manually so automatically inducing bilingual lexicons based on edit-distance (Haghighi et al., 2008), comparable corpora (Turcato, 1998), bilingual corpora (Rosner and Sultana, 2014) or pretrained embeddings from monolingual corpora (Vuli´c and Moens, 2015) is more suitable. However, sentence-aligned parallel data is not available for all languages. Methods based on unsupervised or semi-supervised learning can utilise readily available monolingual data to induce bilingual lexicons. Artetxe et al. (2018) showed that an iterative self-learning method could bootstrap this approach without the need of a seed dictionary by utilising numbers as seed dictionary through adversarial training. However, Patra et al. (2019) showed that wi"
2020.vardial-1.6,P16-1024,0,0.0671408,"Missing"
2020.vardial-1.6,P15-2118,0,0.0592429,"Missing"
2020.vardial-1.6,W19-3114,0,0.0285754,"keyboard. The IPA is an evolving standard initially developed by the International Phonetic Association in 1888 with the goal of transcribing the sounds of all human languages. Transliteration is used to help language learners to read words written in foreign scripts, by writing the sound of the word using the equivalent letters. Romanisation remains a popular technique for transliteration of various languages (Hermjakob et al., 2018). The use of the Latin script for text entry of South Asian languages is common, even though there is no standard orthography for these languages in the script (Wolf-Sonkin et al., 2019). The 107 symbols used for writing the IPA are taken primarily from the Latin and Greek scripts some are novel creations. Diacritics are used for subtle distinctions in sounds and to show nasalisation of vowels, length, stress, and tones. Using IPA symbols, one can represent the pronunciation of words. Nevertheless, the study by Chakravarthi et al. (2019a) and Chakravarthi et al. (2019c) shows that transliteration into the Latin Script is best suited to take advantage of cognate information from closely related languages. For example, LCS of the input sequence “AABCDH” and “AABHEDE” is “AAB” o"
2020.vardial-1.6,P17-1179,0,0.0231617,"can improve MT systems (Golan et al., 1988) to expand the coverage of translation models by translating Out-Of-Vocabulary (OOV) words. Nevertheless, prior work in BLI treated it as stand-alone task (Irvine and Callison-Burch, 2017). Using monolingual word embeddings for BLI has attracted significant attention in recent years. Stateof-the-art BLI results are based on bilingual word embedding models (Irvine and Callison-Burch, 2017). Given the source and target language word embeddings trained independently on monolingual data, unsupervised models (Vuli´c and Moens, 2015; Artetxe et al., 2016; Zhang et al., 2017; Artetxe et al., 2017; Artetxe et al., 2018; Riley and Gildea, 2018; Artetxe et al., 2019) learn a linear mapping W between the source and target space such that: W ∗ = arg min W XX i Dij kXi∗ W − Zj∗ k2 , (1) j where X and Z are two aligned matrices of embedding size d containing the embeddings of the words in the parallel vocabulary. The vocabulary of each language are Vs and Vt , and D ∈ {0, 1}|Vs |×|Vt |is a binary matrix representing a dictionary such that Dij = 1 if the i-th word in the source language is aligned with the j-th word in the target language. Equation (1) is equivelent to:"
2020.wildre-1.2,K19-1096,0,0.0110666,"015b; Mihaylov et al., 2015a), troll comments in News Community (Mihaylov and Nakov, 2016), and the role of political trolls (Atanasov et al., 2019) have been studied. All these considered the trolling on text-only media. However, meme consist of images or images with text. 1 8 https://www.ethnologue.com/language/tam A related research area is on offensive content detection. Various works in the recent years have investigated Offensive and Aggression content in text (Clarke and Grieve, 2017; Mathur et al., 2018; Nogueira dos Santos et al., 2018; Galery et al., 2018). For images, Gandhi et al. (2019) deals with offensive images and non-compliant logos. They have developed a computer-vision driven offensive and non-compliant image detection algorithm that identifies the offensive content in the image. They have categorized images as offensive if it has nudity, sexually explicit content, abusive text, objects used to promote violence or racially inappropriate content. The classifier takes advantage of a pre-trained object detector to identify the type of object in the image and then sends the image to the unit which specializes in detecting objects in the image. The majority of memes do not"
2020.wildre-1.2,2018.gwc-1.10,1,0.835805,"Missing"
2020.wildre-1.2,W19-7101,1,0.674614,"is work at https: //github.com/sharduls007/TamilMemes. 2. (a) Example 3 (b) Example 4 Figure 2: Examples of troll and not-troll memes. memes then have been verified and annotated manually by the annotators. As the users who sent these troll memes belong to the Tamil speaking population, all the troll memes are in Tamil. The general format of the meme is the image and Tamil text embedded within the image. Most of the troll memes comes from the state of Tamil Nadu, in India. The Tamil language, which has 75 million speakers,1 belongs to the Dravidian language family (Rao and Lalitha Devi, 2013; Chakravarthi et al., 2019a; Chakravarthi et al., 2019b; Chakravarthi et al., 2019c) and is one of the 22 scheduled languages of India (Dash et al., 2015). As these troll memes can have a negative psychological effect on an individual, a constraint has to be in place for such a conversation. In this work, we are attempting to identify such troll memes by providing a dataset and image classifier to identify these memes. Troll Meme A troll meme is an implicit image that intents to demean or offend an individual on the Internet. Based on the definition “Trolling is the activity of posting a message via social media that t"
2020.wildre-1.2,W19-6809,1,0.756237,"is work at https: //github.com/sharduls007/TamilMemes. 2. (a) Example 3 (b) Example 4 Figure 2: Examples of troll and not-troll memes. memes then have been verified and annotated manually by the annotators. As the users who sent these troll memes belong to the Tamil speaking population, all the troll memes are in Tamil. The general format of the meme is the image and Tamil text embedded within the image. Most of the troll memes comes from the state of Tamil Nadu, in India. The Tamil language, which has 75 million speakers,1 belongs to the Dravidian language family (Rao and Lalitha Devi, 2013; Chakravarthi et al., 2019a; Chakravarthi et al., 2019b; Chakravarthi et al., 2019c) and is one of the 22 scheduled languages of India (Dash et al., 2015). As these troll memes can have a negative psychological effect on an individual, a constraint has to be in place for such a conversation. In this work, we are attempting to identify such troll memes by providing a dataset and image classifier to identify these memes. Troll Meme A troll meme is an implicit image that intents to demean or offend an individual on the Internet. Based on the definition “Trolling is the activity of posting a message via social media that t"
2020.wildre-1.2,2020.sltu-1.25,1,0.812156,"ation of troll memes with the existing methods. We found that the identification of a troll meme with such an image classifier is not feasible which has been corroborated with precision, recall and F1-score. Keywords: Tamil dataset, memes classification, trolling, Indian language data 1. Introduction often obscure due to fused image-text representation. The content in Indian memes might be written in English, in a native language (native or foreign script), or in a mixture of languages and scripts (Ranjan et al., 2016; Chakravarthi et al., 2018; Jose et al., 2020; Priyadharshini et al., 2020; Chakravarthi et al., 2020a; Chakravarthi et al., 2020b). This adds another challenge to the meme classification problem. Traditional media content distribution channels such as television, radio or newspapers are monitored and scrutinized for their content. Nevertheless, social media platforms on the Internet opened the door for people to contribute, leave a comment on existing content without any moderation. Although most of the time, the internet users are harmless, some produce offensive content due to anonymity and freedom provided by social networks. Due to this freedom, people are becoming creative in their joke"
2020.wildre-1.2,2020.sltu-1.28,1,0.822974,"ation of troll memes with the existing methods. We found that the identification of a troll meme with such an image classifier is not feasible which has been corroborated with precision, recall and F1-score. Keywords: Tamil dataset, memes classification, trolling, Indian language data 1. Introduction often obscure due to fused image-text representation. The content in Indian memes might be written in English, in a native language (native or foreign script), or in a mixture of languages and scripts (Ranjan et al., 2016; Chakravarthi et al., 2018; Jose et al., 2020; Priyadharshini et al., 2020; Chakravarthi et al., 2020a; Chakravarthi et al., 2020b). This adds another challenge to the meme classification problem. Traditional media content distribution channels such as television, radio or newspapers are monitored and scrutinized for their content. Nevertheless, social media platforms on the Internet opened the door for people to contribute, leave a comment on existing content without any moderation. Although most of the time, the internet users are harmless, some produce offensive content due to anonymity and freedom provided by social networks. Due to this freedom, people are becoming creative in their joke"
2020.wildre-1.2,W17-3001,0,0.0240583,"ga and Ng, 2018; Malmasi and Zampieri, 2017; Kumar et al., 2018; Kumar, 2019). Opinion manipulation trolling (Mihaylov et al., 2015b; Mihaylov et al., 2015a), troll comments in News Community (Mihaylov and Nakov, 2016), and the role of political trolls (Atanasov et al., 2019) have been studied. All these considered the trolling on text-only media. However, meme consist of images or images with text. 1 8 https://www.ethnologue.com/language/tam A related research area is on offensive content detection. Various works in the recent years have investigated Offensive and Aggression content in text (Clarke and Grieve, 2017; Mathur et al., 2018; Nogueira dos Santos et al., 2018; Galery et al., 2018). For images, Gandhi et al. (2019) deals with offensive images and non-compliant logos. They have developed a computer-vision driven offensive and non-compliant image detection algorithm that identifies the offensive content in the image. They have categorized images as offensive if it has nudity, sexually explicit content, abusive text, objects used to promote violence or racially inappropriate content. The classifier takes advantage of a pre-trained object detector to identify the type of object in the image and the"
2020.wildre-1.2,W15-5948,0,0.024406,"memes then have been verified and annotated manually by the annotators. As the users who sent these troll memes belong to the Tamil speaking population, all the troll memes are in Tamil. The general format of the meme is the image and Tamil text embedded within the image. Most of the troll memes comes from the state of Tamil Nadu, in India. The Tamil language, which has 75 million speakers,1 belongs to the Dravidian language family (Rao and Lalitha Devi, 2013; Chakravarthi et al., 2019a; Chakravarthi et al., 2019b; Chakravarthi et al., 2019c) and is one of the 22 scheduled languages of India (Dash et al., 2015). As these troll memes can have a negative psychological effect on an individual, a constraint has to be in place for such a conversation. In this work, we are attempting to identify such troll memes by providing a dataset and image classifier to identify these memes. Troll Meme A troll meme is an implicit image that intents to demean or offend an individual on the Internet. Based on the definition “Trolling is the activity of posting a message via social media that tend to be offensive, provocative, or menacing (Bishop, 2013; Bishop, 2014; Mojica de la Vega and Ng, 2018)”. Their main function"
2020.wildre-1.2,W18-4409,0,0.0195459,"pinion manipulation trolling (Mihaylov et al., 2015b; Mihaylov et al., 2015a), troll comments in News Community (Mihaylov and Nakov, 2016), and the role of political trolls (Atanasov et al., 2019) have been studied. All these considered the trolling on text-only media. However, meme consist of images or images with text. 1 8 https://www.ethnologue.com/language/tam A related research area is on offensive content detection. Various works in the recent years have investigated Offensive and Aggression content in text (Clarke and Grieve, 2017; Mathur et al., 2018; Nogueira dos Santos et al., 2018; Galery et al., 2018). For images, Gandhi et al. (2019) deals with offensive images and non-compliant logos. They have developed a computer-vision driven offensive and non-compliant image detection algorithm that identifies the offensive content in the image. They have categorized images as offensive if it has nudity, sexually explicit content, abusive text, objects used to promote violence or racially inappropriate content. The classifier takes advantage of a pre-trained object detector to identify the type of object in the image and then sends the image to the unit which specializes in detecting objects in the i"
2020.wildre-1.2,W18-4401,0,0.126908,"g in memes has yet to be investigated. One way to understand how meme varies from other image posts was studied by Wang and Wen (2015). According to the authors, memes combine two images or are a combination of an image and a witty, catchy or sarcastic text. In this work, we treat this task as an image classification problem. Due to the large population in India, the issue has emerged in the context of recent events. There have been several threats towards people or communities from memes. This is a serious threat which shames people or spreads hatred towards people or a particular community (Kumar et al., 2018; Rani et al., 2020; Suryawanshi et al., 2020). There have been several studies on moderating trolling, however, for a social media administrator memes are hard to monitor as they are region-specific. Furthermore, their meaning is (a) Example 1 (b) Example 2 Figure 1: Examples of Indian memes. 7 In Figure 1, Example 1 is written in Tamil with two images and Example 2 is written in English and Tamil (Roman Script) with two images. In the first example, the meme is trolling about the “Vim dis-washer” soap. The information in Example 1 can be translated into English as “the price of a lemon is fi"
2020.wildre-1.2,malmasi-zampieri-2017-detecting,0,0.0213379,"4 would be “Sorry my friend (girl)”. As this example does not contain any provoking or offensive content and is even funny, it should be listed in the not-troll category. As a troll meme is directed towards someone, it is easy to find such content in the comments section or group chat of social media. For our work, we collected memes from volunteers who sent them through WhatsApp, a social media for chatting and creating a group chat. The suspected troll 3. Related Work Trolling in social media for text has been studied extensively (Bishop, 2013; Bishop, 2014; Mojica de la Vega and Ng, 2018; Malmasi and Zampieri, 2017; Kumar et al., 2018; Kumar, 2019). Opinion manipulation trolling (Mihaylov et al., 2015b; Mihaylov et al., 2015a), troll comments in News Community (Mihaylov and Nakov, 2016), and the role of political trolls (Atanasov et al., 2019) have been studied. All these considered the trolling on text-only media. However, meme consist of images or images with text. 1 8 https://www.ethnologue.com/language/tam A related research area is on offensive content detection. Various works in the recent years have investigated Offensive and Aggression content in text (Clarke and Grieve, 2017; Mathur et al., 201"
2020.wildre-1.2,W18-3504,0,0.0321038,"and Zampieri, 2017; Kumar et al., 2018; Kumar, 2019). Opinion manipulation trolling (Mihaylov et al., 2015b; Mihaylov et al., 2015a), troll comments in News Community (Mihaylov and Nakov, 2016), and the role of political trolls (Atanasov et al., 2019) have been studied. All these considered the trolling on text-only media. However, meme consist of images or images with text. 1 8 https://www.ethnologue.com/language/tam A related research area is on offensive content detection. Various works in the recent years have investigated Offensive and Aggression content in text (Clarke and Grieve, 2017; Mathur et al., 2018; Nogueira dos Santos et al., 2018; Galery et al., 2018). For images, Gandhi et al. (2019) deals with offensive images and non-compliant logos. They have developed a computer-vision driven offensive and non-compliant image detection algorithm that identifies the offensive content in the image. They have categorized images as offensive if it has nudity, sexually explicit content, abusive text, objects used to promote violence or racially inappropriate content. The classifier takes advantage of a pre-trained object detector to identify the type of object in the image and then sends the image to"
2020.wildre-1.2,P16-2065,0,0.0169659,"troll meme is directed towards someone, it is easy to find such content in the comments section or group chat of social media. For our work, we collected memes from volunteers who sent them through WhatsApp, a social media for chatting and creating a group chat. The suspected troll 3. Related Work Trolling in social media for text has been studied extensively (Bishop, 2013; Bishop, 2014; Mojica de la Vega and Ng, 2018; Malmasi and Zampieri, 2017; Kumar et al., 2018; Kumar, 2019). Opinion manipulation trolling (Mihaylov et al., 2015b; Mihaylov et al., 2015a), troll comments in News Community (Mihaylov and Nakov, 2016), and the role of political trolls (Atanasov et al., 2019) have been studied. All these considered the trolling on text-only media. However, meme consist of images or images with text. 1 8 https://www.ethnologue.com/language/tam A related research area is on offensive content detection. Various works in the recent years have investigated Offensive and Aggression content in text (Clarke and Grieve, 2017; Mathur et al., 2018; Nogueira dos Santos et al., 2018; Galery et al., 2018). For images, Gandhi et al. (2019) deals with offensive images and non-compliant logos. They have developed a computer"
2020.wildre-1.2,K15-1032,0,0.0178824,"sive content and is even funny, it should be listed in the not-troll category. As a troll meme is directed towards someone, it is easy to find such content in the comments section or group chat of social media. For our work, we collected memes from volunteers who sent them through WhatsApp, a social media for chatting and creating a group chat. The suspected troll 3. Related Work Trolling in social media for text has been studied extensively (Bishop, 2013; Bishop, 2014; Mojica de la Vega and Ng, 2018; Malmasi and Zampieri, 2017; Kumar et al., 2018; Kumar, 2019). Opinion manipulation trolling (Mihaylov et al., 2015b; Mihaylov et al., 2015a), troll comments in News Community (Mihaylov and Nakov, 2016), and the role of political trolls (Atanasov et al., 2019) have been studied. All these considered the trolling on text-only media. However, meme consist of images or images with text. 1 8 https://www.ethnologue.com/language/tam A related research area is on offensive content detection. Various works in the recent years have investigated Offensive and Aggression content in text (Clarke and Grieve, 2017; Mathur et al., 2018; Nogueira dos Santos et al., 2018; Galery et al., 2018). For images, Gandhi et al. (20"
2020.wildre-1.2,R15-1058,0,0.0223515,"sive content and is even funny, it should be listed in the not-troll category. As a troll meme is directed towards someone, it is easy to find such content in the comments section or group chat of social media. For our work, we collected memes from volunteers who sent them through WhatsApp, a social media for chatting and creating a group chat. The suspected troll 3. Related Work Trolling in social media for text has been studied extensively (Bishop, 2013; Bishop, 2014; Mojica de la Vega and Ng, 2018; Malmasi and Zampieri, 2017; Kumar et al., 2018; Kumar, 2019). Opinion manipulation trolling (Mihaylov et al., 2015b; Mihaylov et al., 2015a), troll comments in News Community (Mihaylov and Nakov, 2016), and the role of political trolls (Atanasov et al., 2019) have been studied. All these considered the trolling on text-only media. However, meme consist of images or images with text. 1 8 https://www.ethnologue.com/language/tam A related research area is on offensive content detection. Various works in the recent years have investigated Offensive and Aggression content in text (Clarke and Grieve, 2017; Mathur et al., 2018; Nogueira dos Santos et al., 2018; Galery et al., 2018). For images, Gandhi et al. (20"
2020.wildre-1.2,L18-1585,0,0.0292017,"Missing"
2020.wildre-1.2,P18-2031,0,0.0198473,"2018; Kumar, 2019). Opinion manipulation trolling (Mihaylov et al., 2015b; Mihaylov et al., 2015a), troll comments in News Community (Mihaylov and Nakov, 2016), and the role of political trolls (Atanasov et al., 2019) have been studied. All these considered the trolling on text-only media. However, meme consist of images or images with text. 1 8 https://www.ethnologue.com/language/tam A related research area is on offensive content detection. Various works in the recent years have investigated Offensive and Aggression content in text (Clarke and Grieve, 2017; Mathur et al., 2018; Nogueira dos Santos et al., 2018; Galery et al., 2018). For images, Gandhi et al. (2019) deals with offensive images and non-compliant logos. They have developed a computer-vision driven offensive and non-compliant image detection algorithm that identifies the offensive content in the image. They have categorized images as offensive if it has nudity, sexually explicit content, abusive text, objects used to promote violence or racially inappropriate content. The classifier takes advantage of a pre-trained object detector to identify the type of object in the image and then sends the image to the unit which specializes in dete"
2020.wildre-1.2,2020.trac-1.7,1,0.686215,"o be investigated. One way to understand how meme varies from other image posts was studied by Wang and Wen (2015). According to the authors, memes combine two images or are a combination of an image and a witty, catchy or sarcastic text. In this work, we treat this task as an image classification problem. Due to the large population in India, the issue has emerged in the context of recent events. There have been several threats towards people or communities from memes. This is a serious threat which shames people or spreads hatred towards people or a particular community (Kumar et al., 2018; Rani et al., 2020; Suryawanshi et al., 2020). There have been several studies on moderating trolling, however, for a social media administrator memes are hard to monitor as they are region-specific. Furthermore, their meaning is (a) Example 1 (b) Example 2 Figure 1: Examples of Indian memes. 7 In Figure 1, Example 1 is written in Tamil with two images and Example 2 is written in English and Tamil (Roman Script) with two images. In the first example, the meme is trolling about the “Vim dis-washer” soap. The information in Example 1 can be translated into English as “the price of a lemon is five Rupees”, whereby"
2020.wildre-1.2,2020.trac-1.6,1,0.747165,"ial networks. Due to this freedom, people are becoming creative in their jokes by making memes. Although memes are meant to be humorous, sometimes it becomes threatening and offensive to specific people or community. On the Internet, a troll is a person who upsets or starts a hatred towards people or community. Trolling is the activity of posting a message via social media that is intended to be offensive, provocative, or menacing to distract which often has a digressive or off-topic content with the intent of provoking the audience (Bishop, 2013; Bishop, 2014; Mojica de la Vega and Ng, 2018; Suryawanshi et al., 2020). Despite this growing body of research in natural language processing, identifying trolling in memes has yet to be investigated. One way to understand how meme varies from other image posts was studied by Wang and Wen (2015). According to the authors, memes combine two images or are a combination of an image and a witty, catchy or sarcastic text. In this work, we treat this task as an image classification problem. Due to the large population in India, the issue has emerged in the context of recent events. There have been several threats towards people or communities from memes. This is a seri"
2020.wildre-1.2,Q14-1006,0,0.0309954,"r balance male and female annotators). Based on Landis and Koch (1977) and given the inherent obscure nature of memes, we got fair agreement amongst the annotators. K= 4.5. Data Statistics We collected 2,969 memes, of which most are images with text embedded on them. After the annotation, we learned that the majority (1,951) of these were annotated as troll memes, and 1,018 as not-troll memes. Furthermore, we observed that memes, which have more than one image have a high probability of being a troll, whereas those with only one image are likely to be not-troll. We included Flickr30K2 images (Young et al., 2014) to the not-troll category to address the class imbalance. Flickr30K is only added to training, while the test set is randomly chosen from our dataset. In all our experiments the test set remains the same. 5. 6. We experimented with ResNet and MobileNet. The variation in experiments comes in terms of the data on which the models have been trained on, while the test set (300 memes) remained the same for all experiments. In the first variation, TamilMemes in Table 1, we trained the ResNet and MobileNet models on our Tamil meme dataset(2,669 memes). The second variation, i.e. TamilMemes + ImageNe"
2020.wmt-1.49,2018.gwc-1.10,1,0.839896,"Raja Chakravarthi! , Ritesh Kumar* , John P. McCrae! ! Data Science Institute, NUIG, Galway, + Panlingua Language Processing LLP, New Delhi, * Dr. Bhimrao Ambedkar University, Agra (atulkumar.ojha,priya.rani,bharathi.raja)@insight-centre.org, panlingua@outlook.com, john.mccrae@nuigalway.ie, ritesh78 llh@jnu.ac.in Abstract as OpenNMT (Klein et al., 2017), Marian (JunczysDowmunt et al., 2018) and Neamtus (Sennrich et al., 2017), which provide various ways of experimenting with the use of different features and architectures, yet it fails to achieve the same results with low resource languages (Chakravarthi et al., 2018, 2019b). However, Sennrich and Zhang (2019) revisited the NMT models and tuned hyperparameters, changed network architectures to optimize NMT for low-resource conditions and concluded that low-resource NMT is very sensitive to hyper-parameters such as Byte Pair Encoding (BPE) vocabulary size, word dropout, and others. This paper is an extension of our work Ojha et al. (2019) submitted to WMT 2019 similar language translation task. Therefore our team adapted methods of the low resource setting for NMT proposed by Sennrich and Zhang (2019) to explore the following broad objectives: NUIG-Panling"
2020.wmt-1.49,W19-7101,1,0.838717,"it a significant overlap in their vocabularies and strong syntactic plus lexical similarities. These striking similarities seem promising in enhancing the possibility of mutual inter-comprehension within closely related languages. However, automated translation between such closely related languages is a rather challenging task. The linguistic similarities and regularities in morphological variations and orthography motivate the use of character-level translation models, which have been applied to translation (Vilar et al., 2007; Chakravarthi et al., 2020) and transliteration (Matthews, 2007; Chakravarthi et al., 2019a; Chakravarthi, 2020). In the past few years, neural machine translation systems have achieved outstanding performance with high resource languages, with the help of open source toolkit such • to compare the performance of SMT and NMT in case of closely related, relatively lowresourced language pairs, and • to findout how to leverage the accuracy of NMT in closely related languages using BPE into subwords. • to analyze the effects of data quality in performance of the systems. 2 System Description This section provides an overview of the systems developed for the WMT 2020 Shared Task. In thes"
2020.wmt-1.49,W11-2123,0,0.0135112,"ms, we use the Moses (Koehn et al., 2007) and Nematus (Sennrich et al., 2017) toolkit for developing statistical and neural 418 Proceedings of the 5th Conference on Machine Translation (WMT), pages 418–423 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics machine translation systems respectively. The preprocessing was done to handle noise in data (for example, different language sentences, non-UTF characters etc), the details of which are provided in section 3.1 2.1 Phrase-based SMT Systems These systems were built on the Moses open source toolkit using the KenLM (Heafield, 2011) language model and GIZA++ (Och and Ney, 2003) aligner. ‘Grow-diag-final-and heuristic’ parameters were used to extract phrases from the corresponding parallel corpora. In addition to this, KenLM was used to build 5-gram language models. 2.2 Out of 43274 training sentences, the Hindi corpus had Telugu sentences while the Marathi corpus had Meitei sentences intermingled as shown in first row (Figure 1). The parallel data had more than 1192 lines that were not comparable with each other as shown in second and third row (Figure 1), where some Hindi sentences had only half the sentences translated"
2020.wmt-1.49,D10-1092,0,0.254499,"to build 2 NMT systems. As we mentioned in an earlier section, at first data was pre-processed at subwords level with BPE for neural translation, and then the system was trained using Nematus toolkit. Most of the system features were adopted from (Sennrich et al., 2017; Koehn and Knowles, 2017) (see section 3.3.2). 2.3 Table 1: Statistics of Parallel and Monolingual Sentences of the Hindi and Marathi Languages 3.2 Assessment Assessment of these systems was done on the standard automatic evaluation metrics: BLEU (Papineni et al., 2002), Rank-based Intuitive Bilingual Evaluation Score (RIBES) (Isozaki et al., 2010) and Translation Error Rate (TER) (Snover et al., 2006). 3 3.1 The following pre-processing steps were performed as part of the experiments: a) Both corpora were tokenized and cleaned (sentences of length over 80 words were removed). b) For neural translation, training, validation and test data was prepossessed into subwords BPE format. This format was utilised to prepare BPE and vocabulary further used. Experiments This section briefly describes the experiment settings for developing the systems. Data Preparations The parallel data-set for these experiments was provided by the WMT Similar Tra"
2020.wmt-1.49,P18-4020,0,0.0244252,"Missing"
2020.wmt-1.49,P17-4012,0,0.117642,"Missing"
2020.wmt-1.49,P07-2045,0,0.0365011,"of closely related, relatively lowresourced language pairs, and • to findout how to leverage the accuracy of NMT in closely related languages using BPE into subwords. • to analyze the effects of data quality in performance of the systems. 2 System Description This section provides an overview of the systems developed for the WMT 2020 Shared Task. In these experiments, the NUIG-Panlingua-KMI team explored two different approaches: phrase-based statistical (Koehn et al., 2003), and neural method for Hindi-Marathi and Marathi-Hindi language pairs. In all the submitted systems, we use the Moses (Koehn et al., 2007) and Nematus (Sennrich et al., 2017) toolkit for developing statistical and neural 418 Proceedings of the 5th Conference on Machine Translation (WMT), pages 418–423 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics machine translation systems respectively. The preprocessing was done to handle noise in data (for example, different language sentences, non-UTF characters etc), the details of which are provided in section 3.1 2.1 Phrase-based SMT Systems These systems were built on the Moses open source toolkit using the KenLM (Heafield, 2011) language model and GIZA++"
2020.wmt-1.49,W17-3204,0,0.0115005,"to mark. In fact, the team could locate a few instances of synthetic data. There were a few sentences where character encoding was an issue, hence were completely unintelligible. Neural Machine Translation System Language Pair Training Tuning Monolingual Hindi ↔ Marathi 43274 1411 Marathi 326748 Hindi 75348193 Nematus was used to build 2 NMT systems. As we mentioned in an earlier section, at first data was pre-processed at subwords level with BPE for neural translation, and then the system was trained using Nematus toolkit. Most of the system features were adopted from (Sennrich et al., 2017; Koehn and Knowles, 2017) (see section 3.3.2). 2.3 Table 1: Statistics of Parallel and Monolingual Sentences of the Hindi and Marathi Languages 3.2 Assessment Assessment of these systems was done on the standard automatic evaluation metrics: BLEU (Papineni et al., 2002), Rank-based Intuitive Bilingual Evaluation Score (RIBES) (Isozaki et al., 2010) and Translation Error Rate (TER) (Snover et al., 2006). 3 3.1 The following pre-processing steps were performed as part of the experiments: a) Both corpora were tokenized and cleaned (sentences of length over 80 words were removed). b) For neural translation, training, vali"
2020.wmt-1.49,N03-1017,0,0.132954,"ing performance with high resource languages, with the help of open source toolkit such • to compare the performance of SMT and NMT in case of closely related, relatively lowresourced language pairs, and • to findout how to leverage the accuracy of NMT in closely related languages using BPE into subwords. • to analyze the effects of data quality in performance of the systems. 2 System Description This section provides an overview of the systems developed for the WMT 2020 Shared Task. In these experiments, the NUIG-Panlingua-KMI team explored two different approaches: phrase-based statistical (Koehn et al., 2003), and neural method for Hindi-Marathi and Marathi-Hindi language pairs. In all the submitted systems, we use the Moses (Koehn et al., 2007) and Nematus (Sennrich et al., 2017) toolkit for developing statistical and neural 418 Proceedings of the 5th Conference on Machine Translation (WMT), pages 418–423 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics machine translation systems respectively. The preprocessing was done to handle noise in data (for example, different language sentences, non-UTF characters etc), the details of which are provided in section 3.1 2.1 Ph"
2020.wmt-1.49,P03-1021,0,0.025394,"Figure 1: Examples of discrepancies in Hindi-Marathi parallel data Figure 2: Analysis of the PBSMT and NMT’s Systems 3.3.1 Building Primary MT Systems: As previously mentioned, the Hindi-Marathi and Marathi-Hindi PBSMT systems were built as the primary submission using Moses. The language model was built first, using KenLM. For MarathiHindi and Hindi-Marathi language pairs, the lan420 guage models were trained on 5-gram. After that, the systems were built independently and combined in a loglinear scheme in which each model was assigned a different weight using the Minimum Error Rate Training (Och, 2003) tuning algorithm. To train and tune the systems, we used 40454 and 1411 parallel sentences, respectively, for all language pairs. 3.3.2 Building Contrastive MT Systems: As mentioned in the previous section, Nematus toolkit was used to develop the NMT systems. The training was done on subword and character-level. All the NMT experiments were carried out only with a data-set that contained sentences with length of up to 80 words. The neural model is trained on 5000 epochs, using Adam with a default learning rate of 0.002, dropout at 0.01 and mini-batches of 80 and the batch size for the validat"
2020.wmt-1.49,J03-1002,0,0.0113605,"nd Nematus (Sennrich et al., 2017) toolkit for developing statistical and neural 418 Proceedings of the 5th Conference on Machine Translation (WMT), pages 418–423 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics machine translation systems respectively. The preprocessing was done to handle noise in data (for example, different language sentences, non-UTF characters etc), the details of which are provided in section 3.1 2.1 Phrase-based SMT Systems These systems were built on the Moses open source toolkit using the KenLM (Heafield, 2011) language model and GIZA++ (Och and Ney, 2003) aligner. ‘Grow-diag-final-and heuristic’ parameters were used to extract phrases from the corresponding parallel corpora. In addition to this, KenLM was used to build 5-gram language models. 2.2 Out of 43274 training sentences, the Hindi corpus had Telugu sentences while the Marathi corpus had Meitei sentences intermingled as shown in first row (Figure 1). The parallel data had more than 1192 lines that were not comparable with each other as shown in second and third row (Figure 1), where some Hindi sentences had only half the sentences translated in Marathi (second row) and some had blank sp"
2020.wmt-1.49,Y18-3011,1,0.717598,"WMT Similar Translation Shared Task 1 organisers and the Marathi monolingual data-set was taken from WMT 2020 Shared Task: Parallel Corpus Filtering for Low-Resource Conditions.2 The parallel data was sub-divided into training, tuning, and monolingual sets, as detailed in Table 1. However, the shared data was very noisy. To enhance the data quality, the team had to undertake an extensive pre-processing session focused on identifying and cleaning the data-sets. Pre-processing All these processes were performed using Moses scripts. However, the tokenization was done by the RGNLP team tokenizer (Ojha et al., 2018) and Indic nlp library.3 These tokenizers were used since Moses does not provide a tokenizer for Indic languages. Also the RGNLP tokenizer ensured that the canonical Unicode representation of the characters are retained. 3.3 Development of the NUIG-PanlinguaKMI MT Systems After removing noisy and pre-processing data, the following steps were followed to build the NUIGPanlingua-KMI MT systems: 1 http://www.statmt.org/wmt20/similar. html 2 https://wmt20similar.cs.upc.edu/ 3 https://github.com/anoopkunchukuttan/ indic_nlp_library 419 Figure 1: Examples of discrepancies in Hindi-Marathi parallel d"
2020.wmt-1.49,W19-5429,1,0.667592,"l., 2018) and Neamtus (Sennrich et al., 2017), which provide various ways of experimenting with the use of different features and architectures, yet it fails to achieve the same results with low resource languages (Chakravarthi et al., 2018, 2019b). However, Sennrich and Zhang (2019) revisited the NMT models and tuned hyperparameters, changed network architectures to optimize NMT for low-resource conditions and concluded that low-resource NMT is very sensitive to hyper-parameters such as Byte Pair Encoding (BPE) vocabulary size, word dropout, and others. This paper is an extension of our work Ojha et al. (2019) submitted to WMT 2019 similar language translation task. Therefore our team adapted methods of the low resource setting for NMT proposed by Sennrich and Zhang (2019) to explore the following broad objectives: NUIG-Panlingua-KMI submission to WMT 2020 seeks to push the state-of-the-art in the Similar language translation task for the Hindi ↔ Marathi language pair. As part of these efforts, we conducted a series of experiments to address the challenges for translation between similar languages. Among the 4 MT systems prepared for this task, 1 PBSMT systems were prepared for Hindi ↔ Marathi each"
2020.wmt-1.49,P02-1040,0,0.111692,"lingual Hindi ↔ Marathi 43274 1411 Marathi 326748 Hindi 75348193 Nematus was used to build 2 NMT systems. As we mentioned in an earlier section, at first data was pre-processed at subwords level with BPE for neural translation, and then the system was trained using Nematus toolkit. Most of the system features were adopted from (Sennrich et al., 2017; Koehn and Knowles, 2017) (see section 3.3.2). 2.3 Table 1: Statistics of Parallel and Monolingual Sentences of the Hindi and Marathi Languages 3.2 Assessment Assessment of these systems was done on the standard automatic evaluation metrics: BLEU (Papineni et al., 2002), Rank-based Intuitive Bilingual Evaluation Score (RIBES) (Isozaki et al., 2010) and Translation Error Rate (TER) (Snover et al., 2006). 3 3.1 The following pre-processing steps were performed as part of the experiments: a) Both corpora were tokenized and cleaned (sentences of length over 80 words were removed). b) For neural translation, training, validation and test data was prepossessed into subwords BPE format. This format was utilised to prepare BPE and vocabulary further used. Experiments This section briefly describes the experiment settings for developing the systems. Data Preparations"
2020.wmt-1.49,E17-3017,0,0.0350261,"Missing"
2020.wmt-1.49,P19-1021,0,0.0125417,". McCrae! ! Data Science Institute, NUIG, Galway, + Panlingua Language Processing LLP, New Delhi, * Dr. Bhimrao Ambedkar University, Agra (atulkumar.ojha,priya.rani,bharathi.raja)@insight-centre.org, panlingua@outlook.com, john.mccrae@nuigalway.ie, ritesh78 llh@jnu.ac.in Abstract as OpenNMT (Klein et al., 2017), Marian (JunczysDowmunt et al., 2018) and Neamtus (Sennrich et al., 2017), which provide various ways of experimenting with the use of different features and architectures, yet it fails to achieve the same results with low resource languages (Chakravarthi et al., 2018, 2019b). However, Sennrich and Zhang (2019) revisited the NMT models and tuned hyperparameters, changed network architectures to optimize NMT for low-resource conditions and concluded that low-resource NMT is very sensitive to hyper-parameters such as Byte Pair Encoding (BPE) vocabulary size, word dropout, and others. This paper is an extension of our work Ojha et al. (2019) submitted to WMT 2019 similar language translation task. Therefore our team adapted methods of the low resource setting for NMT proposed by Sennrich and Zhang (2019) to explore the following broad objectives: NUIG-Panlingua-KMI submission to WMT 2020 seeks to push"
2020.wmt-1.49,2006.amta-papers.25,0,0.491743,"ection, at first data was pre-processed at subwords level with BPE for neural translation, and then the system was trained using Nematus toolkit. Most of the system features were adopted from (Sennrich et al., 2017; Koehn and Knowles, 2017) (see section 3.3.2). 2.3 Table 1: Statistics of Parallel and Monolingual Sentences of the Hindi and Marathi Languages 3.2 Assessment Assessment of these systems was done on the standard automatic evaluation metrics: BLEU (Papineni et al., 2002), Rank-based Intuitive Bilingual Evaluation Score (RIBES) (Isozaki et al., 2010) and Translation Error Rate (TER) (Snover et al., 2006). 3 3.1 The following pre-processing steps were performed as part of the experiments: a) Both corpora were tokenized and cleaned (sentences of length over 80 words were removed). b) For neural translation, training, validation and test data was prepossessed into subwords BPE format. This format was utilised to prepare BPE and vocabulary further used. Experiments This section briefly describes the experiment settings for developing the systems. Data Preparations The parallel data-set for these experiments was provided by the WMT Similar Translation Shared Task 1 organisers and the Marathi monol"
2020.wmt-1.49,W07-0705,0,0.0316022,"concern especially in the domain of Machine Translation(MT). Hindi and Marathi exhibit a significant overlap in their vocabularies and strong syntactic plus lexical similarities. These striking similarities seem promising in enhancing the possibility of mutual inter-comprehension within closely related languages. However, automated translation between such closely related languages is a rather challenging task. The linguistic similarities and regularities in morphological variations and orthography motivate the use of character-level translation models, which have been applied to translation (Vilar et al., 2007; Chakravarthi et al., 2020) and transliteration (Matthews, 2007; Chakravarthi et al., 2019a; Chakravarthi, 2020). In the past few years, neural machine translation systems have achieved outstanding performance with high resource languages, with the help of open source toolkit such • to compare the performance of SMT and NMT in case of closely related, relatively lowresourced language pairs, and • to findout how to leverage the accuracy of NMT in closely related languages using BPE into subwords. • to analyze the effects of data quality in performance of the systems. 2 System Description This"
2021.dravidianlangtech-1.15,2020.vardial-1.6,1,0.661911,"Missing"
2021.dravidianlangtech-1.15,W18-6325,0,0.0150834,"ing shared lexical and sentence level representations across multiple source languages, thereby developing a system that performs well in low resource scenarios. Xia et al. (2019) propose a general framework for data augmentation in low-resource machine translation that not only uses target-side monolingual data but also pivots through a related high-resource language HRL. Zoph et al. (2016)’s key idea is first to train a high-resource language pair (the parent model), then transfer some of the learned parameters to the low-resource pair (the child model) to initialize and constrain training. Kocmi and Bojar (2018) 1 https://competitions.codalab.org/ competitions/27650 119 Proceedings of the First Workshop on Speech and Language Technologies for Dravidian Languages, pages 119–125 April 20, 2021 ©2021 Association for Computational Linguistics propose a transfer learning-based method, wherein they train a “parent” model with a high-resource language pair followed by which they train on a “child” model on a corpus of a low-resource language pair. It was observed that this model is better than the models trained on just the low-resource languages. Zoph et al. (2016) propose a transfer learning-based method,"
2021.dravidianlangtech-1.15,W14-4012,0,0.0203306,"Missing"
2021.dravidianlangtech-1.15,D16-1026,0,0.0377392,"Missing"
2021.dravidianlangtech-1.15,N18-1032,0,0.0366948,"this day and age when translation systems are increasingly being built upon neural network-based architectures (Bahdanau et al., 2014; Luong et al., 2015; Cho et al., 2014; Wu et al., 2016), the development of such systems for under-resourced languages is a challenging task due to the lack of availability of resources. Multilingual extensions to these architectures have been proposed (Firat et al., 2016; Ha et al.; Johnson et al., 2017) which have been shown to improve on low-resource languages. Recent studies have also extended this to a massively multilingual setting (Aharoni et al., 2019). Gu et al. (2018) demonstrated a transfer learning-based approach by utilizing shared lexical and sentence level representations across multiple source languages, thereby developing a system that performs well in low resource scenarios. Xia et al. (2019) propose a general framework for data augmentation in low-resource machine translation that not only uses target-side monolingual data but also pivots through a related high-resource language HRL. Zoph et al. (2016)’s key idea is first to train a high-resource language pair (the parent model), then transfer some of the learned parameters to the low-resource pai"
2021.dravidianlangtech-1.15,W19-5404,0,0.0316982,"Missing"
2021.dravidianlangtech-1.15,2020.peoples-1.6,1,0.769226,") MUCS Shared Task (Hegde et al., 2021) IRLAB-DAIICT (Prajapati et al., 2021) Languages English-Telugu English-Telugu English-Telugu English-Tamil English-Tamil English-Tamil English-Tamil English-Malayalam English-Malayalam English-Malayalam English-Malayalam Tamil-Telugu Tamil-Telugu Tamil-Telugu BLEU 38.86 6.25 0.29 36.66 28.27 6.04 1.66 19.84 15.31 8.43 0.48 35.29 0.43 0.0 Table 1: Results of the participating systems in BLEU score. hatta record of 700 CE is the oldest extant form of Kannada poetry in the Tripadi metre. It is based in part on Kavyadarsha, a Sanskrit text (Rawlinson, 1930; Hande et al., 2020). Dravidian is the name for the Tamil languages or Tamil people in Sanskrit, and all the current Dravidian languages were called a branch of Tamil in old Jain, Bhraminic, and Buddhist literature (Caldwell, 1875). 4 4.1 Task Description and Dataset Task The shared task was hosted on Codalab. Four translation sub-tasks were organized as a part of this task: English to Tamil, English to Malayalam, English to Telugu and Tamil to Telugu. Participants were given a choice to participate in the sub-tasks they wanted to. Training, development and test datasets of parallel sentences for each language pa"
2021.dravidianlangtech-1.15,D15-1166,0,0.0251105,"kers of Dravidian (Tamil) languages can readily understand each other without prior familiarity or special effort (Krishnamurti, 2016; Thavareesan and Mahesan, 2019). The performance of our tasks was evaluated using automatic measures BLEU (Papineni et al., 2002). This shared task’s primary objectives are to further state of the art in machine translation of Related Work Machine translation of under resource languages is an open and an active research area. In this day and age when translation systems are increasingly being built upon neural network-based architectures (Bahdanau et al., 2014; Luong et al., 2015; Cho et al., 2014; Wu et al., 2016), the development of such systems for under-resourced languages is a challenging task due to the lack of availability of resources. Multilingual extensions to these architectures have been proposed (Firat et al., 2016; Ha et al.; Johnson et al., 2017) which have been shown to improve on low-resource languages. Recent studies have also extended this to a massively multilingual setting (Aharoni et al., 2019). Gu et al. (2018) demonstrated a transfer learning-based approach by utilizing shared lexical and sentence level representations across multiple source la"
2021.dravidianlangtech-1.15,2021.dravidianlangtech-1.50,0,0.532793,"in Tamil Nadu to write Sanskrit and foreign words (Andronov, 1996). Telugu existed in the earliest time in the form of inscriptions from 575 CE onwards. Telugu literary works split from Tamil by the first grammar of Telugu in the 13th century CE which was written by Atharvana ´ Acharya, naming it the Trilinga Sabd¯ anus¯asana (or Trilinga Grammar). Similarly, Kannada also split from Tamil by 8th century CE. The Kappe Arab120 2 also called Damili or Tamil-Brahmi Tamil-inscription 4 Keeladi-Book-English-18-09-2019.pdf 3 Team GX (Xie, 2021) IRLAB-DAIICT (Prajapati et al., 2021) MUCS Shared Task (Hegde et al., 2021) GX (Xie, 2021) Spartans IRLAB-DAIICT (Prajapati et al., 2021) MUCS Shared Task (Hegde et al., 2021) GX (Xie, 2021) Spartans IRLAB-DAIICT (Prajapati et al., 2021) MUCS Shared Task (Hegde et al., 2021) GX (Xie, 2021) MUCS Shared Task (Hegde et al., 2021) IRLAB-DAIICT (Prajapati et al., 2021) Languages English-Telugu English-Telugu English-Telugu English-Tamil English-Tamil English-Tamil English-Tamil English-Malayalam English-Malayalam English-Malayalam English-Malayalam Tamil-Telugu Tamil-Telugu Tamil-Telugu BLEU 38.86 6.25 0.29 36.66 28.27 6.04 1.66 19.84 15.31 8.43 0.48 35.29 0.43 0.0 Table"
2021.dravidianlangtech-1.15,2020.loresmt-1.4,0,0.0287756,"corpus of a low-resource language pair. It was observed that this model is better than the models trained on just the low-resource languages. Zoph et al. (2016) propose a transfer learning-based method, wherein they train a parent model trained on a high-resource language pair, followed by which they transfer some of parameters to a child model that they subsequently train on the low-resource language pair. Lakew et al. (2017) leverage the use of duality of translations generated by the system for zero-shot; these translations are used along with the parallel data to train the neural network. Ojha et al. (2020) show the results of the LoResMT 2020 shared task. This workshop was held along with AACL and reported good BLEU scores in case of the low-resource Bhojpuri-Hindi language pair. Koehn et al. (2019) focussed on the translation of the low resource language pairs Nepali-English and Sinhala-English. They reported the results of these translation tasks for statistical as well as phrase-based methods. Chakravarthi et al. (2019c) created a multimodal corpora for Dravidian languages. M (2013) developed a statistical machine translation system for English-Tamil using transfer based on computational lin"
2021.dravidianlangtech-1.15,P02-1040,0,0.122324,"idian languages of the Workshop on Speech and Language Technologies for Dravidian Languages held at EACL 2021. The shared task features four sub-tasks: a translation task from English to Tamil, English to Telugu, English to Malayalam and Tamil to Telugu. They all closely related languages and are underresourced now. They are mutually intelligible since speakers of Dravidian (Tamil) languages can readily understand each other without prior familiarity or special effort (Krishnamurti, 2016; Thavareesan and Mahesan, 2019). The performance of our tasks was evaluated using automatic measures BLEU (Papineni et al., 2002). This shared task’s primary objectives are to further state of the art in machine translation of Related Work Machine translation of under resource languages is an open and an active research area. In this day and age when translation systems are increasingly being built upon neural network-based architectures (Bahdanau et al., 2014; Luong et al., 2015; Cho et al., 2014; Wu et al., 2016), the development of such systems for under-resourced languages is a challenging task due to the lack of availability of resources. Multilingual extensions to these architectures have been proposed (Firat et a"
2021.dravidianlangtech-1.15,P19-1579,0,0.014722,"ourced languages is a challenging task due to the lack of availability of resources. Multilingual extensions to these architectures have been proposed (Firat et al., 2016; Ha et al.; Johnson et al., 2017) which have been shown to improve on low-resource languages. Recent studies have also extended this to a massively multilingual setting (Aharoni et al., 2019). Gu et al. (2018) demonstrated a transfer learning-based approach by utilizing shared lexical and sentence level representations across multiple source languages, thereby developing a system that performs well in low resource scenarios. Xia et al. (2019) propose a general framework for data augmentation in low-resource machine translation that not only uses target-side monolingual data but also pivots through a related high-resource language HRL. Zoph et al. (2016)’s key idea is first to train a high-resource language pair (the parent model), then transfer some of the learned parameters to the low-resource pair (the child model) to initialize and constrain training. Kocmi and Bojar (2018) 1 https://competitions.codalab.org/ competitions/27650 119 Proceedings of the First Workshop on Speech and Language Technologies for Dravidian Languages, pa"
2021.dravidianlangtech-1.15,2021.dravidianlangtech-1.18,0,0.0603906,"Tamil and Sanskrit, using the Tamil Grantha script which was used in Tamil Nadu to write Sanskrit and foreign words (Andronov, 1996). Telugu existed in the earliest time in the form of inscriptions from 575 CE onwards. Telugu literary works split from Tamil by the first grammar of Telugu in the 13th century CE which was written by Atharvana ´ Acharya, naming it the Trilinga Sabd¯ anus¯asana (or Trilinga Grammar). Similarly, Kannada also split from Tamil by 8th century CE. The Kappe Arab120 2 also called Damili or Tamil-Brahmi Tamil-inscription 4 Keeladi-Book-English-18-09-2019.pdf 3 Team GX (Xie, 2021) IRLAB-DAIICT (Prajapati et al., 2021) MUCS Shared Task (Hegde et al., 2021) GX (Xie, 2021) Spartans IRLAB-DAIICT (Prajapati et al., 2021) MUCS Shared Task (Hegde et al., 2021) GX (Xie, 2021) Spartans IRLAB-DAIICT (Prajapati et al., 2021) MUCS Shared Task (Hegde et al., 2021) GX (Xie, 2021) MUCS Shared Task (Hegde et al., 2021) IRLAB-DAIICT (Prajapati et al., 2021) Languages English-Telugu English-Telugu English-Telugu English-Tamil English-Tamil English-Tamil English-Tamil English-Malayalam English-Malayalam English-Malayalam English-Malayalam Tamil-Telugu Tamil-Telugu Tamil-Telugu BLEU 38.86"
2021.dravidianlangtech-1.15,D16-1163,0,0.0370392,"e been shown to improve on low-resource languages. Recent studies have also extended this to a massively multilingual setting (Aharoni et al., 2019). Gu et al. (2018) demonstrated a transfer learning-based approach by utilizing shared lexical and sentence level representations across multiple source languages, thereby developing a system that performs well in low resource scenarios. Xia et al. (2019) propose a general framework for data augmentation in low-resource machine translation that not only uses target-side monolingual data but also pivots through a related high-resource language HRL. Zoph et al. (2016)’s key idea is first to train a high-resource language pair (the parent model), then transfer some of the learned parameters to the low-resource pair (the child model) to initialize and constrain training. Kocmi and Bojar (2018) 1 https://competitions.codalab.org/ competitions/27650 119 Proceedings of the First Workshop on Speech and Language Technologies for Dravidian Languages, pages 119–125 April 20, 2021 ©2021 Association for Computational Linguistics propose a transfer learning-based method, wherein they train a “parent” model with a high-resource language pair followed by which they trai"
2021.dravidianlangtech-1.17,2021.dravidianlangtech-1.45,0,0.0614889,"Missing"
2021.dravidianlangtech-1.17,2021.dravidianlangtech-1.47,0,0.0200416,"ith various machine learning algorithms. They achieved F1 scores of 0.95 for Malayalam, 0.7 for Kannada and 0.73 for task2-Tamil on the test-set. Malayalam • Garain et al. (2021) used IndicBERT and BERT architectures, to facilitate identification of offensive languages for Kannada-English, Malayalam-English, and Tamil-English codemixed language pairs extracted from social media. F1 score for language pair KannadaEnglish as 0.62, 0.71, and 0.66, respectively, for language pair Malayalam-English as 0.77, 0.43, and 0.53, respectively, and for TamilEnglish as 0.71, 0.74, and 0.72, respectively. • Balouchzahi et al. (2021) Two models, namely, COOLI-ensemble and COOLI-Keras were trained with the char sequences extracted from the sentences combined with words as features. Out of the two proposed models, the COOLI-Ensemble model (best among our models) obtained the first rank for the MaEn language pair with 0.97 weighted F1- and fourth and sixth rank with a 0.75 and a 0.69 weighted F1-score for Ta-En and Kn-En language pairs respectively. • Kedia and Nandy (2021) leveraged existing state of the art approaches in text classification by incorporating additional data and transfer learning on pre-trained models. Their"
2021.dravidianlangtech-1.17,2020.sltu-1.25,1,0.495454,"e infiltration depth and width of this ‘digital wonder’ equipped the erstwhile ‘invisible and socially paralysed’ communities to play cards, if not extravagantly noticeable (Barnidge et al., 2019). Research in hate speech detection (Kumar et al., 2018) or offensive language detection (Zampieri et al., 2020; Mandl et al., 2020) using Natural Language Processing (NLP) has significantly improved in recent years. However, the work on underresourced languages is still limited (Chakravarthi, 2020). For example, under-resourced languages such as Tamil, Malayalam, and Kannada lack tools and datasets (Chakravarthi et al., 2020a,c; Thavareesan and Mahesan, 2019, 2020a,b). Recently, shared sentiment analysis for Tamil and Malayalam by Chakravarthi et al. (2020d) and offensive language identification in Tamil and Malayalam by Chakravarthi et al. (2020b) paved the wave for more research on Dravidian languages. Tamil, Malayalam and Kannada belong to the Dravidian language family and are spoken by some 220 million people in the Indian subcontinent, Singapore and Sri Lanka (Krishnamurti, 2003). It is essential to create NLP systems such as hate speech detection or offensive language detection in local languages since most"
2021.dravidianlangtech-1.17,2020.sltu-1.28,1,0.401091,"e infiltration depth and width of this ‘digital wonder’ equipped the erstwhile ‘invisible and socially paralysed’ communities to play cards, if not extravagantly noticeable (Barnidge et al., 2019). Research in hate speech detection (Kumar et al., 2018) or offensive language detection (Zampieri et al., 2020; Mandl et al., 2020) using Natural Language Processing (NLP) has significantly improved in recent years. However, the work on underresourced languages is still limited (Chakravarthi, 2020). For example, under-resourced languages such as Tamil, Malayalam, and Kannada lack tools and datasets (Chakravarthi et al., 2020a,c; Thavareesan and Mahesan, 2019, 2020a,b). Recently, shared sentiment analysis for Tamil and Malayalam by Chakravarthi et al. (2020d) and offensive language identification in Tamil and Malayalam by Chakravarthi et al. (2020b) paved the wave for more research on Dravidian languages. Tamil, Malayalam and Kannada belong to the Dravidian language family and are spoken by some 220 million people in the Indian subcontinent, Singapore and Sri Lanka (Krishnamurti, 2003). It is essential to create NLP systems such as hate speech detection or offensive language detection in local languages since most"
2021.dravidianlangtech-1.17,2021.dravidianlangtech-1.37,0,0.0183528,"ibited better training performance, and they submitted the predictions based on the same. • Sharif et al. (2021) employed two machine learning techniques (LR, SVM), three deep learning (LSTM, LSTM+Attention) techniques and three transformers (m-BERT, Indic-BERT, XLM-R) based methods. They showed that XLM-R outperforms other techniques in Tamil and Malayalam languages while m-BERT achieves the highest score in the Kannada language. Their proposed models gained a weighted F1 score of 0.76 (for Tamil), 0.93 (for Malayalam ), and 0.71 (for Kannada) with a rank of 3rd , 5th and 4th respectively. • Dave et al. (2021) used TF-IDF character ngrams and pretrained MuRIL embeddings for text representation and Logistic Regression and Linear SVM for classification. Their best approach achieved ninth, third and eighth with weighted F1 score of 0.64, 0.95 and 0.71 in Kannada, Malayalam and Tamil on the test dataset respectively. • Saha et al. (2021) presented an exhaustive exploration of different transformer models, also provided a genetic algorithm technique for ensembling different models. Their ensembled models trained separately for each language secured the first position in Tamil, the second 140 position in"
2021.dravidianlangtech-1.17,2021.dravidianlangtech-1.19,0,0.0206022,". In total, 119 participant register for the task and downloaded the data. 2.3 Evaluation Phase In the second phase, test sets for evaluation are released for all three languages. Each participating team submitted their generated prediction for evaluation. Predictions are submitted via Google form to the organizing committer to evaluate the systems. CodaLab is an established platform to organize shared-tasks. However, we faced issues with running evaluation, so we choose to evaluate manually. The metrics used for evaluation is the weighted average F1 score. 3 Systems 3.1 System Descriptions • Dowlagar and Mamidi (2021) used a pretrained multilingual BERT transformer model with transliteration and class balancing loss for offensive content identification. They have ranked 2nd in Malayalam-English and 4th in Tamil-English languages. • Li (2021) participated in all three of offensive language identification. They explored multilingual models based on XLM-RoBERTa and multilingual BERT trained on mixed data of three code-mixed languages. They solved the class-imbalance problem existed in the training data by class weights and class combination. Their model achieved a weighted average F1 scores of 0.75 (ranked 4t"
2021.dravidianlangtech-1.17,2021.dravidianlangtech-1.46,0,0.0202878,"nd ranked 4th and 5th for Tamil and Kannada, respectively. • Jayanthi and Gupta (2021) system is an ensemble of mBERT and XLM-RoBERTa models which leverage task-adaptive pre-training of multilingual BERT models with a masked language modeling objective. They ranked 1st Language Tamil for Kannada, 2nd for Malayalam and 3rd for Tamil. • B and Silvia A (2021) described an automatic offensive language identification from Dravidian languages with various machine learning algorithms. They achieved F1 scores of 0.95 for Malayalam, 0.7 for Kannada and 0.73 for task2-Tamil on the test-set. Malayalam • Garain et al. (2021) used IndicBERT and BERT architectures, to facilitate identification of offensive languages for Kannada-English, Malayalam-English, and Tamil-English codemixed language pairs extracted from social media. F1 score for language pair KannadaEnglish as 0.62, 0.71, and 0.66, respectively, for language pair Malayalam-English as 0.77, 0.43, and 0.53, respectively, and for TamilEnglish as 0.71, 0.74, and 0.72, respectively. • Balouchzahi et al. (2021) Two models, namely, COOLI-ensemble and COOLI-Keras were trained with the char sequences extracted from the sentences combined with words as features. Ou"
2021.dravidianlangtech-1.17,2020.peoples-1.6,1,0.612923,"The first printed work in an Indian language and script was a Roman Catholic catechism translated by Henrique which is Thambiran Vanakkam (Doctrina Christam en Lingua Malauar Tamul in Portuguese) on 20 October 1,578 CE at Quilon, Venad (present day Kerala) (George, 1972). Similarly, Kannada also split from Tamil by 9th century CE; for example, Taayviru is a term of Kannada origin and is found in a Tamil inscription from the 4th-century CE. The Kappe Arabhatta record of 700 CE is the oldest extant form of Kannada poetry in the Tripadi metre, it is based in part on Kavyadarsha (Rawlinson, 1930; Hande et al., 2020). However, user-generated content is often adopted to Roman script for typing due to historical reason and modern computer keyboard layouts. Hence, the majority of user-generated data for these Dravidian languages are code-mixed (Priyadharshini et al., 2020; Jose et al., 2020). Due to the growth of social media platforms across the world and the possibility of writing content without any moderation, users write content in multilingual code-switching without grammatical restrictions and using non-native scripts (Chakravarthi et al., 2019). In linguistics, code-switching is switching between two"
2021.dravidianlangtech-1.17,2021.dravidianlangtech-1.44,0,0.110139,"he Malayalam language and a precision of 0.66, a recall of 0.67 and an F1-score of 0.63 for the Kannada language. • Que et al. (2021) used the XLM-Roberta model for pre-training for the Kannada language. They scored a very low score of 0.33 weighted • Tula et al. (2021) proposed a multilingual ensemble based model from ULMFiT, DistilmBERT, and IndicBERT. Their model is able to handle both code-mixed data as well as instances where the script used is mixed (for instance, Tamil and Latin). They ranked number one for Malayalam dataset and ranked 4th and 5th for Tamil and Kannada, respectively. • Jayanthi and Gupta (2021) system is an ensemble of mBERT and XLM-RoBERTa models which leverage task-adaptive pre-training of multilingual BERT models with a masked language modeling objective. They ranked 1st Language Tamil for Kannada, 2nd for Malayalam and 3rd for Tamil. • B and Silvia A (2021) described an automatic offensive language identification from Dravidian languages with various machine learning algorithms. They achieved F1 scores of 0.95 for Malayalam, 0.7 for Kannada and 0.73 for task2-Tamil on the test-set. Malayalam • Garain et al. (2021) used IndicBERT and BERT architectures, to facilitate identificati"
2021.dravidianlangtech-1.17,2021.dravidianlangtech-1.34,0,0.0393988,"Missing"
2021.dravidianlangtech-1.17,2021.dravidianlangtech-1.3,0,0.141865,"respectively, for language pair Malayalam-English as 0.77, 0.43, and 0.53, respectively, and for TamilEnglish as 0.71, 0.74, and 0.72, respectively. • Balouchzahi et al. (2021) Two models, namely, COOLI-ensemble and COOLI-Keras were trained with the char sequences extracted from the sentences combined with words as features. Out of the two proposed models, the COOLI-Ensemble model (best among our models) obtained the first rank for the MaEn language pair with 0.97 weighted F1- and fourth and sixth rank with a 0.75 and a 0.69 weighted F1-score for Ta-En and Kn-En language pairs respectively. • Kedia and Nandy (2021) leveraged existing state of the art approaches in text classification by incorporating additional data and transfer learning on pre-trained models. Their final submission is an ensemble of an AWD-LSTM based model along with 2 different transformer model architectures based on BERT and RoBERTa. They achieved weighted- average F1 scores of 0.97, 0.77, and 0.72 in the Malayalam-English, Tamil-English, and Kannada-English datasets ranking 1st, 2nd, and 3rd on the respective tasks. 4 Results and Discussion The offensive language identification shared task was organized for three languages Tamil, M"
2021.dravidianlangtech-1.17,W18-4401,0,0.0831958,"Missing"
2021.dravidianlangtech-1.17,2021.dravidianlangtech-1.21,0,0.0208503,"evaluation. Predictions are submitted via Google form to the organizing committer to evaluate the systems. CodaLab is an established platform to organize shared-tasks. However, we faced issues with running evaluation, so we choose to evaluate manually. The metrics used for evaluation is the weighted average F1 score. 3 Systems 3.1 System Descriptions • Dowlagar and Mamidi (2021) used a pretrained multilingual BERT transformer model with transliteration and class balancing loss for offensive content identification. They have ranked 2nd in Malayalam-English and 4th in Tamil-English languages. • Li (2021) participated in all three of offensive language identification. They explored multilingual models based on XLM-RoBERTa and multilingual BERT trained on mixed data of three code-mixed languages. They solved the class-imbalance problem existed in the training data by class weights and class combination. Their model achieved a weighted average F1 scores of 0.75 (ranked 4th), 0.94 (ranked 4th) and 0.72 (ranked 3rd) on offensive language identification in code-mixed Tamil-English language, Malayalam-English language and Kannada-English language, respectively. or comment in Tamil, Malayalam, and Ka"
2021.dravidianlangtech-1.17,2021.dravidianlangtech-1.38,0,0.15221,"niques in Tamil and Malayalam languages while m-BERT achieves the highest score in the Kannada language. Their proposed models gained a weighted F1 score of 0.76 (for Tamil), 0.93 (for Malayalam ), and 0.71 (for Kannada) with a rank of 3rd , 5th and 4th respectively. • Dave et al. (2021) used TF-IDF character ngrams and pretrained MuRIL embeddings for text representation and Logistic Regression and Linear SVM for classification. Their best approach achieved ninth, third and eighth with weighted F1 score of 0.64, 0.95 and 0.71 in Kannada, Malayalam and Tamil on the test dataset respectively. • Saha et al. (2021) presented an exhaustive exploration of different transformer models, also provided a genetic algorithm technique for ensembling different models. Their ensembled models trained separately for each language secured the first position in Tamil, the second 140 position in Kannada, and the first position in Malayalam sub-tasks. • Yang (2021) only participate in one of the language task- Malayalam. They used the transformer-based language model with BiGRU-Attention to complete this task. They ranked 5th in this task with a weighted average F1 score of 0.93 on the private leader board. • Awatramani"
2021.dravidianlangtech-1.17,2020.lrec-1.765,0,0.0363132,"esults of the competing systems. 1 Introduction The dawn of social media helped to bridge the gap between the borders and paved the way for the people to communicate or express their opinions more easily than any other time in human history (Edosomwan et al., 2011). But with this advent of social media, the platforms like YouTube, Facebook, and Twitter not only helped information sharing and networking but also became the place that target, defame and marginalize people merely on the basis of their physical appearance, religion or sexual orientation (Keipi et al., 2016; Benikova et al., 2018; Pamungkas et al., 2020). Social media has moulded itself into its specialised tool with which the people are verbally threatened and cornered not for what they have done but for who they are (Maitra and McGowan, 2012; Patton et al., 2013). Undoubtedly, the infiltration depth and width of this ‘digital wonder’ equipped the erstwhile ‘invisible and socially paralysed’ communities to play cards, if not extravagantly noticeable (Barnidge et al., 2019). Research in hate speech detection (Kumar et al., 2018) or offensive language detection (Zampieri et al., 2020; Mandl et al., 2020) using Natural Language Processing (NLP)"
2021.dravidianlangtech-1.17,2021.dravidianlangtech-1.20,0,0.0356375,"2021) used Indic-BERT to generate word embeddings which is then fed into a 4-layer Multi Layer Perceptron (MLP) which does the multi-class classification task and achieve an F1 score of 0.85 for Malayalam language. • Andrew (2021) did language-specific preprocessing before using several machine learning algorithms. For the Tamil language, they achieve a precision of 0.54, a recall of 0.73 and an F1-score of 0.61, a precision of 0.94, a recall of 0.94 and an F1-score of 0.93 for the Malayalam language and a precision of 0.66, a recall of 0.67 and an F1-score of 0.63 for the Kannada language. • Que et al. (2021) used the XLM-Roberta model for pre-training for the Kannada language. They scored a very low score of 0.33 weighted • Tula et al. (2021) proposed a multilingual ensemble based model from ULMFiT, DistilmBERT, and IndicBERT. Their model is able to handle both code-mixed data as well as instances where the script used is mixed (for instance, Tamil and Latin). They ranked number one for Malayalam dataset and ranked 4th and 5th for Tamil and Kannada, respectively. • Jayanthi and Gupta (2021) system is an ensemble of mBERT and XLM-RoBERTa models which leverage task-adaptive pre-training of multilin"
2021.dravidianlangtech-1.17,2021.dravidianlangtech-1.35,0,0.0231013,"icipating teams. • K et al. (2021) implemented three deep neural network architectures such as a hybrid network with a Convolutional layer, a Bidirectional Long Short- Term Memory network (BiLSTM) layer and a hidden layer, a network containing a Bi- LSTM and another with a Bidirectional Recurrent Neural Network (BiRNN). In addition to that, they incorporated a cost-sensitive learning approach to deal with the problem of class imbalance in the training data. Among the three models, the hybrid network exhibited better training performance, and they submitted the predictions based on the same. • Sharif et al. (2021) employed two machine learning techniques (LR, SVM), three deep learning (LSTM, LSTM+Attention) techniques and three transformers (m-BERT, Indic-BERT, XLM-R) based methods. They showed that XLM-R outperforms other techniques in Tamil and Malayalam languages while m-BERT achieves the highest score in the Kannada language. Their proposed models gained a weighted F1 score of 0.76 (for Tamil), 0.93 (for Malayalam ), and 0.71 (for Kannada) with a rank of 3rd , 5th and 4th respectively. • Dave et al. (2021) used TF-IDF character ngrams and pretrained MuRIL embeddings for text representation and Logi"
2021.dravidianlangtech-1.17,2021.dravidianlangtech-1.42,0,0.0207515,"ss classification task and achieve an F1 score of 0.85 for Malayalam language. • Andrew (2021) did language-specific preprocessing before using several machine learning algorithms. For the Tamil language, they achieve a precision of 0.54, a recall of 0.73 and an F1-score of 0.61, a precision of 0.94, a recall of 0.94 and an F1-score of 0.93 for the Malayalam language and a precision of 0.66, a recall of 0.67 and an F1-score of 0.63 for the Kannada language. • Que et al. (2021) used the XLM-Roberta model for pre-training for the Kannada language. They scored a very low score of 0.33 weighted • Tula et al. (2021) proposed a multilingual ensemble based model from ULMFiT, DistilmBERT, and IndicBERT. Their model is able to handle both code-mixed data as well as instances where the script used is mixed (for instance, Tamil and Latin). They ranked number one for Malayalam dataset and ranked 4th and 5th for Tamil and Kannada, respectively. • Jayanthi and Gupta (2021) system is an ensemble of mBERT and XLM-RoBERTa models which leverage task-adaptive pre-training of multilingual BERT models with a masked language modeling objective. They ranked 1st Language Tamil for Kannada, 2nd for Malayalam and 3rd for Tam"
2021.dravidianlangtech-1.17,2021.dravidianlangtech-1.26,0,0.033633,"e problem existed in the training data by class weights and class combination. Their model achieved a weighted average F1 scores of 0.75 (ranked 4th), 0.94 (ranked 4th) and 0.72 (ranked 3rd) on offensive language identification in code-mixed Tamil-English language, Malayalam-English language and Kannada-English language, respectively. or comment in Tamil, Malayalam, and Kannada. They have fine-tuned transformer models to get better performance. The relatively high F1-scores of 0.9603, 0.7895 on Malayalam, Tamil were achieved by ULMFiT and 0.7277 on Kannada was achieved by DistilBERT models. • Vasantharajan and Thayasivam (2021) participated in Tamil, Kannada, and Malayalam language using the BERT fine-tuning strategies. Their model got a 0.96 F1- score for Malayalam, 0.73 F1-score for Tamil, and 0.70 F1score for Kannada. Moreover, in the view of multilingual models, this modal ranked 3rd and achieved favorable results and confirmed the model as the best among all systems submitted to these shared tasks in these three languages. They got 2nd, 5th, 6th ranks in the leader-board for the Malayalam, Kannada, and Tamil test set respectively. • Huang and Bai (2021) used the multilingual BERT model to complete in all three"
2021.dravidianlangtech-1.17,2021.dravidianlangtech-1.40,0,0.0162106,"gs for text representation and Logistic Regression and Linear SVM for classification. Their best approach achieved ninth, third and eighth with weighted F1 score of 0.64, 0.95 and 0.71 in Kannada, Malayalam and Tamil on the test dataset respectively. • Saha et al. (2021) presented an exhaustive exploration of different transformer models, also provided a genetic algorithm technique for ensembling different models. Their ensembled models trained separately for each language secured the first position in Tamil, the second 140 position in Kannada, and the first position in Malayalam sub-tasks. • Yang (2021) only participate in one of the language task- Malayalam. They used the transformer-based language model with BiGRU-Attention to complete this task. They ranked 5th in this task with a weighted average F1 score of 0.93 on the private leader board. • Awatramani (2021) participated for Tamil language. They used mBERT-cased and XLMRoBERTa. Their system was ranked 3 rd in the task leaderboard achieving an F1-score 0.76 for detecting offensive Tamil YouTube comments. • Nair and Fernandes (2021) used Indic-BERT to generate word embeddings which is then fed into a 4-layer Multi Layer Perceptron (MLP)"
2021.dravidianlangtech-1.17,N19-1144,0,0.0510669,"t grammatical restrictions and using non-native scripts (Chakravarthi et al., 2019). In linguistics, code-switching is switching between two or more language in the same utterance. This explosion of code-mixed user-generated content in the social media platforms aroused interest in NLP. In this paper, we dicuss about the offensive language identification shared task for Tamil, Malayalam and Kannada languages and the participant’s submissions. 134 2 Task Description Offensive language identification for Dravidian languages at different levels of complexity were developed following the work of (Zampieri et al., 2019). It was customized to our annotation method from three-level hierarchical annotation schema. A new category Not in intended language was added to include comments written in a language other than the Dravidian languages. Annotations decision for offensive language categories were split into six labels to simplify the annotation process. • Not Offensive: Comment/post does not have offence, obscenity, swearing, or profanity. • Offensive Untargeted: Comment/post have offence, obscenity, swearing, or profanity not directed towards any target. These are the comments/posts which have inadmissible l"
2021.dravidianlangtech-1.17,2020.semeval-1.188,0,0.0610882,"al orientation (Keipi et al., 2016; Benikova et al., 2018; Pamungkas et al., 2020). Social media has moulded itself into its specialised tool with which the people are verbally threatened and cornered not for what they have done but for who they are (Maitra and McGowan, 2012; Patton et al., 2013). Undoubtedly, the infiltration depth and width of this ‘digital wonder’ equipped the erstwhile ‘invisible and socially paralysed’ communities to play cards, if not extravagantly noticeable (Barnidge et al., 2019). Research in hate speech detection (Kumar et al., 2018) or offensive language detection (Zampieri et al., 2020; Mandl et al., 2020) using Natural Language Processing (NLP) has significantly improved in recent years. However, the work on underresourced languages is still limited (Chakravarthi, 2020). For example, under-resourced languages such as Tamil, Malayalam, and Kannada lack tools and datasets (Chakravarthi et al., 2020a,c; Thavareesan and Mahesan, 2019, 2020a,b). Recently, shared sentiment analysis for Tamil and Malayalam by Chakravarthi et al. (2020d) and offensive language identification in Tamil and Malayalam by Chakravarthi et al. (2020b) paved the wave for more research on Dravidian languag"
2021.dravidianlangtech-1.17,2021.dravidianlangtech-1.29,0,0.0250988,"0.70 F1score for Kannada. Moreover, in the view of multilingual models, this modal ranked 3rd and achieved favorable results and confirmed the model as the best among all systems submitted to these shared tasks in these three languages. They got 2nd, 5th, 6th ranks in the leader-board for the Malayalam, Kannada, and Tamil test set respectively. • Huang and Bai (2021) used the multilingual BERT model to complete in all three languages. For all three language data sets, they combined the Tf-Idf algorithm and the multilingual BERT model’s output and introduced the CNN block as a shared layer. • Zhao (2021) proposed a system based on the multilingual model XLM-Roberta and DPCNN. The test results on the official test data set confirm the effectiveness of their system. They achieved a weighted average F1score of Kannada, Malayalam, and Tami language of 0.69, 0.92, and 0.76, respectively, ranked 6th, 6th, and 3rd. • Ghanghor et al. (2021) used multilingualBERT-base for offensive language identification, their system achieved a weighted F1 scores of 0.75 for Tamil, 0.95 for Malayalam, and 0.71 for Kannada. and ranked 3rd, 3rd and 4th for Tamil, Malayalam and Kannada respectively. • Chen and Kong (20"
2021.emnlp-main.716,D15-1075,0,0.0365137,"al. (2018) using a bidirectional dual encoder arels are even seen to outperform LASER (e.g., for chitecture. Chidambaram et al. (2019) proposed EN-DE our unsupervised model achieves a Spear- Multilingual Universal Sentence Encoder (mUSE), man rank correlation score of 64.6 and weakly- a dual-encoder model trained on large web-mined supervised model achieves a Spearman rank corre- translation parallel corpora, along with data from lation score of 69.4 compared to 64.2 for LASER). Reddit, Wikipedia, and Stanford Natural Language On the BUCC task (bitext mining task) (Zweigen- Inference (SNLI) (Bowman et al., 2015) to learn baum et al., 2017) our model achieves a better F1 more context, supporting 16 languages. A translascore compared to the existing unsupervised model tion ranking task was used to identify a correct of Kvapilíková et al. (2020). Interestingly, for translation pair, and the architecture assumes 5 certain under-resourced languages, we outperform hard negative pairs for each sample while training. both LASER and multilingual S-BERT (Reimers Subsequently, the LASER (Artetxe and Schwenk, 9100 2019; Schwenk et al., 2019) framework considered a sequence-to-sequence architecture using LSTM net"
2021.emnlp-main.716,2020.emnlp-main.365,0,0.367874,"ual encoder into a single vector for sentences. model mapping different languages onto the Supervised sentence embedding approaches map same vector space. We demonstrate the effiparallel sentences from source and target languages cacy of an unsupervised as well as a weakly into the same vector space by either maximising supervised variant of our framework on STS, their cosine similarity or minimising the distance BUCC and Tatoeba benchmark tasks. The proposed unsupervised sentence embedding between the generated embeddings (Artetxe and framework outperforms even supervised stateSchwenk, 2019; Reimers and Gurevych, 2020). For of-the-art methods for certain under-resourced example, recent supervised methods using parallel languages on the Tatoeba dataset and on a corpus rely on a teacher-student model to minimonolingual benchmark. Further, we show mize cross-lingual embedding distance (Reimers enhanced zero-shot learning capabilities for and Gurevych, 2020) or an additive margin softmore than 30 languages, with the model bemax function based dual sentence encoder to maxing trained on only 13 languages. Our model can be extended to a wide range of languages imally separate the sentences that are true transfrom"
2021.emnlp-main.716,P18-2037,0,0.0221223,"ection 6) on multi- methods are supervised approaches. There exlingual sentence similarity and parallel sentence ist some unsupervised sentence embedding framemining tasks have showcased the efficacy of our works (Zhang et al., 2020; Pagliardini et al., 2018), sentence embedding framework. For example, but are mostly for English sentence embeddings. on the cross-lingual STS benchmark (Reimers Initial methods generated sentence embeddings and Gurevych, 2020), our unsupervised ap- based on neural machine translation system with proach achieves state-of-the-art average Spearman a shared encoder (Schwenk, 2018; España-Bonet rank correlation score of 62.1, comparable to et al., 2017; Schwenk and Douze, 2017). The use the supervised sentence embedding approach of of cosine similarities between source and target LASER (Artetxe and Schwenk, 2019) with an aver- language parallel sentences was studied by Guo age of 65.8. In fact, for certain languages, our mod- et al. (2018) using a bidirectional dual encoder arels are even seen to outperform LASER (e.g., for chitecture. Chidambaram et al. (2019) proposed EN-DE our unsupervised model achieves a Spear- Multilingual Universal Sentence Encoder (mUSE), man r"
2021.emnlp-main.716,W17-2619,0,0.0248312,"ty and parallel sentence ist some unsupervised sentence embedding framemining tasks have showcased the efficacy of our works (Zhang et al., 2020; Pagliardini et al., 2018), sentence embedding framework. For example, but are mostly for English sentence embeddings. on the cross-lingual STS benchmark (Reimers Initial methods generated sentence embeddings and Gurevych, 2020), our unsupervised ap- based on neural machine translation system with proach achieves state-of-the-art average Spearman a shared encoder (Schwenk, 2018; España-Bonet rank correlation score of 62.1, comparable to et al., 2017; Schwenk and Douze, 2017). The use the supervised sentence embedding approach of of cosine similarities between source and target LASER (Artetxe and Schwenk, 2019) with an aver- language parallel sentences was studied by Guo age of 65.8. In fact, for certain languages, our mod- et al. (2018) using a bidirectional dual encoder arels are even seen to outperform LASER (e.g., for chitecture. Chidambaram et al. (2019) proposed EN-DE our unsupervised model achieves a Spear- Multilingual Universal Sentence Encoder (mUSE), man rank correlation score of 64.6 and weakly- a dual-encoder model trained on large web-mined supervise"
2021.emnlp-main.716,D13-1170,0,0.00528691,"oduce efficient results in monolingual settings. To evaluate the performance on monolingual classification tasks, we now study the performance of the unsupervised variant of DuEAM on the SentEval benchmark (Conneau and Kiela, 2018). We compare DuEAM to other sentence embedding frameworks on four tasks : (i) MR: Movie reviews positive/negative sentiment analysis (Pang and Lee, 2005), (ii) SUBJ: Subjectivity/objectivity prediction of reviews (Pang and Lee, 2004), (iii) TREC: Question type classification on six classes (Li and Roth, 2002), and (iv) SST2: Stanford binary sentiment classification (Socher et al., 2013; Reimers and Gurevych, 2019). In Table 6, we can see quite satisfactory result produced by DuEAM framework. On every task, the DuEAMunsupv model surpasses the performance of the supervised LASER model whereas in case of the TREC task the results are better than even the supervised multilingual S-BERT model. Overall, the monolingual and multi-lingual experimental results depict DuEAM to effectively capture cross-lingual semantic understanding (without parallel training data) to generate efficient sentence embeddings by alignment of multiple languages in the same vector space. Observe that DuEA"
2021.emnlp-main.716,2020.acl-demos.12,0,0.0310923,"fy a correct of Kvapilíková et al. (2020). Interestingly, for translation pair, and the architecture assumes 5 certain under-resourced languages, we outperform hard negative pairs for each sample while training. both LASER and multilingual S-BERT (Reimers Subsequently, the LASER (Artetxe and Schwenk, 9100 2019; Schwenk et al., 2019) framework considered a sequence-to-sequence architecture using LSTM networks, and was trained on parallel corpora designed for neural machine translation across 93 languages. Expanding beyond translation-based approaches, the multilingual sentence encoder model of Yang et al. (2020a) was trained for semantic retrieval on three different tasks: multi-feature question-answer prediction, translation ranking, and natural language inference (NLI). Recently, Yang et al. (2020b) proposed Conditional Masked Language Modeling (CMLM) to generate sentence embeddings, by co-training the system with bi-text retrieval and Natural Language Inference (NLI) tasks. To generate sentence embeddings beyond the naïve CLS token and simple pooling strategies of language models, sentence transformer architectures were proposed (Reimers and Gurevych, 2019). For multilingual S-BERT models, Reimer"
2021.gwc-1.11,2019.gwc-1.49,1,0.758591,"s requirement is partially just to satisfy XML validators, but can also serve as a check on the dependent lexicon’s assumptions about the structure of the primary wordnet. lishing IDs for linking, these elements allow for augmenting the elements themselves, such as for adding senses to an existing lexical entry or relations to a synset. However, these elements do not allow one to change information in the provider wordnet, so lemmas on lexical entries, ILIs on synsets, and other required information may not be speciﬁed on the corresponding external elements. For example, the Geonames Wordnet (Bond and Bond, 2019) provides additional synset relations on top of the PWN as well as an extended lexical hierarchy of location names in the PWN and many other wordnets. The extension would specify that it extends the PWN as follows: &lt;LexiconExtension id=&quot;geonames-pwn&quot; version=&quot;1.0&quot;&gt; &lt;Extends id=&quot;pwn&quot; version=&quot;3.0&quot;/&gt; &lt;/LexiconExtension&gt; In some cases it might make sense to use both the Extends and Requires elements. For instance, if we want to extend the Japanese Wordnet with its entries from the Geonames Wordnet and reuse the relations from the English Geonames extension, we could specify the relationships as f"
2021.gwc-1.11,P13-1133,1,0.913743,"me perspectives on how these changes help in the integration of wordnets. 1 Introduction The introduction of the Global WordNet Grid (Vossen et al., 2016) and the Collaborative Interlingual Index (Bond et al., 2016) presented a need for greater compatibility between individual wordnet projects through a common format for the representation of wordnets. As such the Global WordNet Association introduced a format with several serialization methods1 that have been used by several projects, including the new open English WordNet (EWN; McCrae et al., 2020, 2019), the Open Multilingual Wordnet (OMW; Bond and Foster, 2013) and the Wn Python library (Goodman and Bond, 2021). Along with the increased adoption came the perception of shortcomings in the format as it was initially deﬁned, such as the inability to capture all of the information present in Princeton WordNet (PWN; Miller, 1995; Fellbaum, 2012) or to capture some key information that other projects wished to use in their modelling. It was therefore deemed necessary to extend the model and, for this reason, we have introduced a new extended version (v1.1) of the format that covers some of these use cases. 1 https://globalwordnet.github.io/schemas In this"
2021.gwc-1.11,bond-etal-2008-boot,1,0.549775,"htforward to use a wordnet in isolation of the full OMW, e.g., for experimental purposes. This issue is even more pronounced for wordnets that are not included in the OMW. What we need, then, is a way for a wordnet to specify what other resources are required, much as how software projects specify their dependencies. We therefore introduce a new Requires element which selects the id and version attributes of an external lexicon that should be loaded along with the current lexicon for it to behave as expected. For example, the following speciﬁes that the Japanese Wordnet (Isahara et al., 2008; Bond et al., 2008) depends on the PWN for its synset relations: &lt;Lexicon id=&quot;wnja&quot; id=&quot;2.0&quot;&gt; &lt;Requires id=&quot;pwn&quot; version=&quot;3.0&quot;/&gt; &lt;/Lexicon&gt; The purpose is to declare what, exactly, is required so that an application that hosts the wordnets can signal to the user if dependencies are unmet, or to limit the wordnets that may be used when traversing external synset relations. It is left implicit which elements or kinds of elements from the external wordnet become available to the dependent wordnet but, following the OMW’s behaviour, an application may choose to only allow synset relations and not, say, synsets or le"
2021.gwc-1.11,2016.gwc-1.9,1,0.776872,"t can be integrated through the Global WordNet Grid. As a result of their adoption, a number of shortcomings of the format were identiﬁed, and in this paper we describe the extensions to the formats that address these issues. These include: ordering of senses, dependencies between wordnets, pronunciation, syntactic modelling, relations, sense keys, metadata and RDF support. Furthermore, we provide some perspectives on how these changes help in the integration of wordnets. 1 Introduction The introduction of the Global WordNet Grid (Vossen et al., 2016) and the Collaborative Interlingual Index (Bond et al., 2016) presented a need for greater compatibility between individual wordnet projects through a common format for the representation of wordnets. As such the Global WordNet Association introduced a format with several serialization methods1 that have been used by several projects, including the new open English WordNet (EWN; McCrae et al., 2020, 2019), the Open Multilingual Wordnet (OMW; Bond and Foster, 2013) and the Wn Python library (Goodman and Bond, 2021). Along with the increased adoption came the perception of shortcomings in the format as it was initially deﬁned, such as the inability to cap"
2021.gwc-1.11,2020.mmw-1.7,0,0.0268947,"mes-pwn&quot; version=&quot;1.0&quot;&gt; &lt;Extends id=&quot;pwn&quot; version=&quot;3.0&quot;/&gt; &lt;/LexiconExtension&gt; In some cases it might make sense to use both the Extends and Requires elements. For instance, if we want to extend the Japanese Wordnet with its entries from the Geonames Wordnet and reuse the relations from the English Geonames extension, we could specify the relationships as follows: &lt;LexiconExtension id=&quot;geonames-wnja&quot; version=&quot;1.0&quot;&gt; &lt;Extends id=&quot;wnja&quot; version=&quot;2.0&quot;/&gt; &lt;Requires id=&quot;geonames-pwn&quot; version=&quot;1.0&quot;/&gt; &lt;/LexiconExtension&gt; 3.3 Pronunciation One of the extensions that has been requested by other projects (Declerck et al., 2020) is the ability to represent phonetic information giving the pronunciation of lemmas in a schema such as the International Phonetic Alphabet. As well as giving the IPA text, it was also desired that we should be able to provide information about the speciﬁc variety, as well as further notes about the form of the pronunciation. In addition, we want to indicate whether the transcription is phonemic or phonetic, that is whether it includes expected features of the language such as aspiration. For ‘variety’, we decided to support the use of IETF language tags to indicate dialect, for example encod"
2021.gwc-1.11,francopoulo-etal-2006-lexical,0,0.416051,"inly inspired by plWordNet (Piasecki et al., 2009). Finally, there were some technical issues to do with the modelling of syntactic behaviours, and while the current formats could capture the information, they did so in a way that was quite verbose and lead to bloated ﬁles. In addition, we ﬁxed a few minor issues related to the representation of lexicographer ﬁles, sense keys and metadata. 2 Background The Global WordNet Association’s formats are a common data model with three(-plus) serializations in XML, JSON and various RDF formats.2 The XML format is based on the Lexical Markup Framework (Francopoulo et al., 2006) and in particular on the version developed in the Kyoto project (Soria and Monachini, 2008). This represents the wordnet as a LexicalResource with a number of Lexicons, one for each language, along with multiple metadata elements about the lexicon, including identiﬁers, version, language, license, contact email, 2 Any RDF serialization is valid, but for this paper we consider the Turtle form of RDF. citation, etc. The format splits the data into two distinct elements, the LexicalEntry, which contains the syntactic information about the usage of individual words, and the Synset, which provides"
2021.gwc-1.11,2021.gwc-1.12,1,0.667854,"ntegration of wordnets. 1 Introduction The introduction of the Global WordNet Grid (Vossen et al., 2016) and the Collaborative Interlingual Index (Bond et al., 2016) presented a need for greater compatibility between individual wordnet projects through a common format for the representation of wordnets. As such the Global WordNet Association introduced a format with several serialization methods1 that have been used by several projects, including the new open English WordNet (EWN; McCrae et al., 2020, 2019), the Open Multilingual Wordnet (OMW; Bond and Foster, 2013) and the Wn Python library (Goodman and Bond, 2021). Along with the increased adoption came the perception of shortcomings in the format as it was initially deﬁned, such as the inability to capture all of the information present in Princeton WordNet (PWN; Miller, 1995; Fellbaum, 2012) or to capture some key information that other projects wished to use in their modelling. It was therefore deemed necessary to extend the model and, for this reason, we have introduced a new extended version (v1.1) of the format that covers some of these use cases. 1 https://globalwordnet.github.io/schemas In this paper, we describe the model as a reference for us"
2021.gwc-1.11,isahara-etal-2008-development,1,0.669223,"ired, it is not straightforward to use a wordnet in isolation of the full OMW, e.g., for experimental purposes. This issue is even more pronounced for wordnets that are not included in the OMW. What we need, then, is a way for a wordnet to specify what other resources are required, much as how software projects specify their dependencies. We therefore introduce a new Requires element which selects the id and version attributes of an external lexicon that should be loaded along with the current lexicon for it to behave as expected. For example, the following speciﬁes that the Japanese Wordnet (Isahara et al., 2008; Bond et al., 2008) depends on the PWN for its synset relations: &lt;Lexicon id=&quot;wnja&quot; id=&quot;2.0&quot;&gt; &lt;Requires id=&quot;pwn&quot; version=&quot;3.0&quot;/&gt; &lt;/Lexicon&gt; The purpose is to declare what, exactly, is required so that an application that hosts the wordnets can signal to the user if dependencies are unmet, or to limit the wordnets that may be used when traversing external synset relations. It is left implicit which elements or kinds of elements from the external wordnet become available to the dependent wordnet but, following the OMW’s behaviour, an application may choose to only allow synset relations and not"
2021.gwc-1.11,2019.gwc-1.31,1,0.83942,"Missing"
2021.gwc-1.11,2020.mmw-1.3,1,0.730115,"sense keys, metadata and RDF support. Furthermore, we provide some perspectives on how these changes help in the integration of wordnets. 1 Introduction The introduction of the Global WordNet Grid (Vossen et al., 2016) and the Collaborative Interlingual Index (Bond et al., 2016) presented a need for greater compatibility between individual wordnet projects through a common format for the representation of wordnets. As such the Global WordNet Association introduced a format with several serialization methods1 that have been used by several projects, including the new open English WordNet (EWN; McCrae et al., 2020, 2019), the Open Multilingual Wordnet (OMW; Bond and Foster, 2013) and the Wn Python library (Goodman and Bond, 2021). Along with the increased adoption came the perception of shortcomings in the format as it was initially deﬁned, such as the inability to capture all of the information present in Princeton WordNet (PWN; Miller, 1995; Fellbaum, 2012) or to capture some key information that other projects wished to use in their modelling. It was therefore deemed necessary to extend the model and, for this reason, we have introduced a new extended version (v1.1) of the format that covers some of"
2021.gwc-1.11,C12-3044,1,0.81435,"Missing"
2021.gwc-1.11,van-assem-etal-2006-conversion,0,0.130729,"Missing"
2021.gwc-1.29,E09-1005,0,0.119209,"Missing"
2021.gwc-1.29,2019.gwc-1.49,0,0.0829696,"Missing"
2021.gwc-1.29,2016.gwc-1.9,1,0.889611,"Missing"
2021.gwc-1.29,W13-5503,0,0.0622611,"Missing"
2021.gwc-1.29,E12-1059,0,0.0691448,"Missing"
2021.gwc-1.29,2018.gwc-1.8,1,0.841818,"Missing"
2021.gwc-1.29,2019.gwc-1.31,1,0.762595,"gest knowledge graphs of entity and concepts available. While, there is a clear difference in the focus of these two resources, there is also a significant overlap and as such a complete linking of these resources would have many uses. We propose the development of such a linking, first by means of the hapax legomenon links and secondly by the use of natural language processing techniques. We show that these can be done with high accuracy but that human validation is still necessary. This has resulted in over 9,000 links being added between these two resources. 1 Introduction English WordNet (McCrae et al., 2019, 2020), derived from Princeton WordNet (Miller, 1995; Fellbaum, 2012, PWN)1 , is the most complete wordnet for English, while Wikidata2 provides one of the largest collection of encyclopedic facts in machine readable form. Moreover, as Wikidata is an open resource to which anyone can contribute and data is published without any license, it is quickly becoming a central database to which knowledge graphs can link. As such, a linking between WordNet and Wikipedia would provide value to users of both resources, and potentially make it easier to extend WordNet in the future with new synsets. Howe"
2021.gwc-1.29,2020.mmw-1.3,1,0.809819,"Missing"
2021.gwc-1.29,I11-1099,0,0.0744955,"Missing"
2021.gwc-1.29,P10-1023,0,0.0778125,"Missing"
2021.gwc-1.29,W14-3004,0,0.0608397,"Missing"
2021.gwc-1.29,R09-1080,0,0.0557362,"Missing"
2021.nllp-1.10,Q19-1038,0,0.0138736,"tures to the model. • SVM: Similar to the MLP model, we learn an SVM model for the classification task. We set the regularization parameter C and gamma to 1.0 and 0.1 respectively. • Sentence-BERT (Reimers and Gurevych, 2019): This setting is similar to our proposed approach. We encode each sentence into a fixed-sized vector using its Sentence-BERT embedding. The sentence embedding is then fed into a 3 layer fully connected neural network with ReLu activation in the first two layers. The model is learned by minimizing the cross-entropy loss of classification using the Adam optimizer. • Laser (Artetxe and Schwenk, 2019): In this setting, we encode each sentence using its Laser embeddings. The remaining architecture remains the same as that used in the Sentence-BERT model. We retrieved data from internal and external data In addition to the supervised approaches, we comsources in the financial services industry to cre- pare our few-shot learning approach against a zeroate the initial data sets for the approach. After shot learning approach. Yin et al. (2019) suggested data setup, we cleaned the data to remove dupli- method for using pre-trained natural language incate and irrelevant content to ensure data qua"
2021.nllp-1.10,N19-1423,0,0.0134402,"al texts has mostly been successful so far. Methods based on counting the words in the text and then classifying using machine learning approaches such as support vector machines (Cortes and Vapnik, 1995) for example by Sulea et al. (2017), where they applied this method to the classification of texts according to the legal area, ruling and time span of the text. Deep learning methods such as Convolutional Neural Networks (CNNs) have been shown to further improve the performance of such systems (Wei et al., 2018). More recently, the emergence of large pre-trained language models such as BERT (Devlin et al., 2019) has further increased the performance and Shaheen et al. (2020) showed that these models could be used to classify legal texts according to thousands of labels and even on multiple languages if sufficient training data exists. A criticism of such NLP-based approaches to 1 predictive coding, especially with the emergence https://www.finra.org/rules-guidance/ rulebooks/finra-rules/2210 of more sophisticated deep learning methods, is 102 Natural Legal Language Processing Workshop 2021, pages 102–106 November 10, 2021. ©2021 Association for Computational Linguistics that they can appear to be ‘bl"
2021.nllp-1.10,2020.acl-main.703,0,0.0291871,"the Sentence-BERT model. We retrieved data from internal and external data In addition to the supervised approaches, we comsources in the financial services industry to cre- pare our few-shot learning approach against a zeroate the initial data sets for the approach. After shot learning approach. Yin et al. (2019) suggested data setup, we cleaned the data to remove dupli- method for using pre-trained natural language incate and irrelevant content to ensure data quality ference models as sequence classifiers. Towards before review. Each data point was reviewed and this end, we use BART model (Lewis et al., 2020) labelled by both in-house licensed staff and con- as our zero-shot learning model. We consider the tractors to confirm the interpretation of regulatory sentences tagged as ‘promissory’ as the hypothesis. content standards. The probability of a sentence being the premise 104 1 2 3 4 5 6 Sentence Model Result Gold Label Stocks are an income source which main street is ignoring It is going up in all currencies Joe Smith picks the best stock in each sector for the fund All rights reserved. Save more now. There is no action required on your part. non-promissory non-promissory non-promissory promis"
2021.nllp-1.10,D08-1046,0,0.130807,"Missing"
2021.nllp-1.10,D19-1410,0,0.0364584,"Missing"
2021.nllp-1.10,2021.naacl-main.434,0,0.173662,"ovides explanations for why they make certain predictions. Similarly, some work has gone into the investigation of specific complexities of legal texts, such as in Nallapati and Manning (2008), who showed that for some legal texts the complex combination of negative and positive statements can confused machine learning approaches. They showed that by combining these machine learning approaches with propositional logic, text classification systems could handle intricate legal wording. 3 Methodology To solve the problem of legal text classification, we approach this with a triplet architecture (Wei et al., 2021) where an input sentence, s, is compared with a positive example s+ and a negative example s− as depicted in Figure 1. We begin by describing the model architecture. Then we discuss the triplet loss used for training the network. Finally, we describe the classification model used for the final classification. 3.1 Model Architecture Gurevych, 2019) encoder. Sentence-BERT captures the contextual information in a sentence in a fixed-size vector representation. The contextual sentence representation is then fed to a two-layer perceptron. The hidden layer of the perceptron has ReLU (Nair and Hinton"
2021.nllp-1.10,D19-1404,0,0.0207522,"tivation in the first two layers. The model is learned by minimizing the cross-entropy loss of classification using the Adam optimizer. • Laser (Artetxe and Schwenk, 2019): In this setting, we encode each sentence using its Laser embeddings. The remaining architecture remains the same as that used in the Sentence-BERT model. We retrieved data from internal and external data In addition to the supervised approaches, we comsources in the financial services industry to cre- pare our few-shot learning approach against a zeroate the initial data sets for the approach. After shot learning approach. Yin et al. (2019) suggested data setup, we cleaned the data to remove dupli- method for using pre-trained natural language incate and irrelevant content to ensure data quality ference models as sequence classifiers. Towards before review. Each data point was reviewed and this end, we use BART model (Lewis et al., 2020) labelled by both in-house licensed staff and con- as our zero-shot learning model. We consider the tractors to confirm the interpretation of regulatory sentences tagged as ‘promissory’ as the hypothesis. content standards. The probability of a sentence being the premise 104 1 2 3 4 5 6 Sentence"
2021.smm4h-1.33,S18-1100,0,0.0652855,"Missing"
2021.smm4h-1.33,2020.smm4h-1.5,0,0.0406325,"nguage processing field such as containing symptoms detection of medical jargons, named entity recogThe data set for the experiment was given by nition, multi-word expressions. Furthermore, the the organisers of Task 6 SMM4H’21.2 Like informal nature of tweets and short length, which Task 5 this data was also divided in training, often contain non-standard grammar, frequent mis1 https://healthlanguageprocessing.org/ spellings, many contractions, extensive slang, and smm4h-2021/task-5/ 2 use of emojis/emoticons to express emotion exhttps://healthlanguageprocessing.org/ acerbate the challenges (Dang et al., 2020). The smm4h-2021/task-6/ 149 Proceedings of the Sixth Social Media Mining for Health Workshop 2021, pages 149–152 June 10, 2021. ©2021 Association for Computational Linguistics validation and test set. The statistics of the data set is given in Table 1. The data is classified at three different levels: self-reports, nonpersonal reports, literature/news mentions. the model we have tested it on the held-out test data set given by the organizers. Wsteps = Task Training Validation Test Task 5 6,465 717 10,000 Task 6 9,068 501 6,500 3.2 Table 1: Statistics of Task 5 and 6 Dataset 2.2 Pre-processing"
2021.smm4h-1.33,N19-1423,0,0.0108042,"ation from the Generative Morphemes with Attention (GenMA) model (Goswami et al., 2020) to develop the model for Task 6. We have noted the model description below: 1. After a thorough manual evaluation of the data set, we came to the conclusion that emoticons, URLs along with the other special characters which are very common in social media data do not serve necessary purpose for our tasks. Therefore we removed emoticon, URLs and other special characters from the data set. 3 (len(trainingset ) ∗ epochstraining ) batchsizetraining ∗ r (1) Task 5 We have used the multilingual pre-trained BERT (Devlin et al., 2019; Turc et al., 2019) model to fine-tune our model on the given Task 5 training data set. The detailed model descriptions is given below: • The model has an embedding dimension of 768. We have used the Google-provided cased vocabulary. • The model takes the character sequence as the input sequence. It has one character embedding layer and two convolutions (CONV1D) layers. Each convolution layer has one maxpooling layer. After the convolution layers, there is one LSTM layer and one bidirectional LSTM layer, followed by two self-attention layers. The model has two hidden layers and one softmax la"
2021.smm4h-1.33,2020.semeval-1.125,1,0.881533,"rsonal reports, literature/news mentions. the model we have tested it on the held-out test data set given by the organizers. Wsteps = Task Training Validation Test Task 5 6,465 717 10,000 Task 6 9,068 501 6,500 3.2 Table 1: Statistics of Task 5 and 6 Dataset 2.2 Pre-processing We normalized the data through the following preprocessing steps as part of the experiment. 2. Lower casing all the tweets in the data set. After lower casing the tweets all extra spaces were removed from it. Experiments 3.1 Task 6 We have taken the inspiration from the Generative Morphemes with Attention (GenMA) model (Goswami et al., 2020) to develop the model for Task 6. We have noted the model description below: 1. After a thorough manual evaluation of the data set, we came to the conclusion that emoticons, URLs along with the other special characters which are very common in social media data do not serve necessary purpose for our tasks. Therefore we removed emoticon, URLs and other special characters from the data set. 3 (len(trainingset ) ∗ epochstraining ) batchsizetraining ∗ r (1) Task 5 We have used the multilingual pre-trained BERT (Devlin et al., 2019; Turc et al., 2019) model to fine-tune our model on the given Task"
C16-1010,2014.amta-researchers.5,1,0.855557,"Translations The SMT system is configured to return the t highest scoring translations, according to its model, and we select the translation as the most frequent translation of the context among this t-best list. In our experiments, we combined this with m disambiguations to give tm candidate translations from which the candidate is chosen. Target Side Lookup (TSL) We can also utilize the translation of our context into the target language xliT from the parallel corpus, however this cannot be applied directly as we do not know which word(s) in xliT correspond to the input and previous work (Arcan et al., 2014) has shown that automatic inference of this alignment (e.g., with GIZA++) can seriously affect performance. Instead we filter contexts to those that generate a translation candidate, wklT , such that wklT ∈ xliT , i.e., the machine translation agrees with the gold-standard translation for this context. 4 Experimental Setting This section gives an overview on the multilingual resources and the translation toolkit used in our experiment. Furthermore, we give insights into SMT evaluation techniques, considering the translation direction of the English WordNet entries into Italian, Slovene, Spanis"
C16-1010,P15-1069,1,0.524332,"ingwn.linguistic-lod.org/ 98 Since all these approaches use word alignment information, they are not able to generate any translation equivalents for multi-word expressions (MWE). In contrast, our approach use an SMT system trained on a large amount of parallel sentences, which allows us to align possible MWEs, such as commercial loan or take a breath, between source and target language. Furthermore, we engage the idea of identifying relevant contextual information to support an SMT system translating short expressions, which showed better performance compared to approaches without a context. Arcan et al. (2015) built small domainspecific translation models for ontology translation from relevant sentence pairs that were identified in a parallel corpus based on the ontology labels to be translated. With this approach they improve the translation quality over the usage of large generic translation models. Since the generation of translation models can be computational expensive, Arcan et al. (2016) use large generic translation models to translate ontology labels, which were placed into a disambiguated context. With this approach the authors demonstrate translation quality improvement over commercial s"
C16-1010,P13-1133,0,0.0310493,"orm of concepts, where new concepts may be added even if they are not represented (yet) in the Princeton WordNet or even lexicalized in English (e.g., many languages have distinct gendered role words, such as ‘male teacher’ and ‘female teacher’, but these meanings are not distinguished in English). Previous studies of generating non-English wordnets combined Wiktionary knowledge with existing wordnets to extend them or to create new ones (de Melo and Weikum, 2009). Bond and Paik (2012) describe in their work the creation of the Open Multilingual Wordnet and its extension with other resources (Bond and Foster, 2013). A different approach to expand English WordNet synsets with lexicalizations in other languages was proposed in de Melo and Weikum (2012). The authors do not directly match concepts in the two different language resources, but demonstrate an approach that learns how to determine the best translation for English synsets by taking bilingual dictionaries, structural information of the English WordNet and corpus frequency information into account. With the growing amount of parallel data, Kazakov and Shahid (2009) show an approach to acquire a set of synsets from parallel corpora. The synsets are"
C16-1010,2016.gwc-1.9,1,0.90782,"f a sentence. As a motivating example, we consider the word vessel, which is a member of three synsets in Princeton WordNet, whereby the most frequent translation, e.g., as given by Google Translate, is Schiff in German and nave in Italian, corresponding to i608331 ‘a craft designed for water transportation’. For the second sense, i65336 ‘a tube in which a body fluid circulates’, we assume that we know the This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/ 1 We use the CILI identifiers for synsets (Bond et al., 2016) 97 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 97–108, Osaka, Japan, December 11-17 2016. German translation for this sense is Gefäß. In our approach we look for sentences in a parallel corpus, where the words vessel and Gefäß both occur and obtain a context such as ‘blood vessel’ that allows the SMT system to translate this sense correctly. This alone is not sufficient as Gefäß is also a translation of i60834 ‘an object used as a container’, however in Italian these two senses are distinct (vaso and recipiente respective"
C16-1010,S07-1054,0,0.0243835,"kipedia entry, the authors use Google Translate to translate English sentences containing the synset in the sense annotated corpus. After that, the most frequent translation is included as a variant for the synset for the given language. The use of parallel corpora has been previously exploited for word sense disambiguation, for example to construct sense-tagged corpora in another language (Ng et al., 2003) or by using translations as a method to discriminate senses (Ide et al., 2002). It has been shown that the combination of these techniques can improve supervised word sense disambiguation (Chan et al., 2007). A similar approach to the one proposed in this paper is that of Tufi¸s et al. (2004), where they show that using the interlingual index of WordNet with the help of parallel text can improve word sense disambiguation of a monolingual approach and we generalize this result to generate wordnets for new languages. 3 Methodology Our approach takes the advantage of the increasing amount of parallel corpora in combination with wordnets in languages other than English for sense disambiguation, which will help us to improve automatic translations of English WordNet entries. We assume that we have a m"
C16-1010,P11-2031,0,0.0293983,"c produces good correlation with human judgement at the sentence or segment level. chrF3 is a character n-gram metric, which has shown very good correlations with human judgements on the WMT2015 shared metric task (Stanojevi´c et al., 2015), especially when translating from English into morphologically rich(er) languages. As there are multiple translations available for each sense in the target wordnet we use all translations as multiple references for BLEU, for the other two metrics we compare only to the most frequent member of the synset. The approximate randomization approach in MultEval (Clark et al., 2011) is used to test whether differences among system performances are statistically significant with a p-value < 0.05. 5 Evaluation In this section we present the evaluation of the translated English WordNet words into Italian, Slovene, Spanish and Croatian. We evaluate the quality of translations of the WordNet entries based on the provided contextual information as well as the impact on the number of languages and their effect on wordsense disambiguation. 5.1 Translation Quality Evaluation Based on Contextual Information Our main evaluation focuses on the importance of identifying relevant cont"
C16-1010,W14-3348,0,0.0558521,"corpus (Eisele and Chen, 2010), EMEA, KDE4, OpenOffice (Tiedemann, 2009), OpenSubtitles2012 (Tiedemann, 2012). Similarly, we concatenate parallel corpora for identifying relevant sentences containing WordNet entries, which are then translated into the targeted languages. Table 2 shows the number of parallel sentences used for the ten language pairs. 4.4 Translation Evaluation Metrics The automatic translation evaluation is based on the correspondence between the SMT output and reference translation (gold standard). For the automatic evaluation we used the BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014) and chrF (Popovi´c, 2015) metrics. BLEU (Bilingual Evaluation Understudy) is calculated for individual translated segments (n-grams) by comparing them with a data set of reference translations.6 The calculated scores, between 0 and 100 (perfect translation), are averaged over the whole evaluation data set to reach an estimate of the translation’s overall quality. Considering the short length of the terms in WordNet, while we report scores based on the unigram overlap (BLEU-1), this is in most cases only precision, so in addition we also report other metrics. METEOR (Metric for Evaluation of T"
C16-1010,eisele-chen-2010-multiun,0,0.142157,"word alignments, necessary for generating translation models, were built with the GIZA++ toolkit (Och and Ney, 2003). The Kenlm toolkit (Heafield, 2011) was used to build a 5-gram language model. 4.3 Parallel Resources for SMT training and Word-Sense-Disambiguation To ensure a broad lexical and domain coverage of our SMT system we merged the existing parallel corpora for each language pair from the OPUS web page5 into one parallel data set, i.e., Europarl (Koehn, 2005), DGT - translation memories generated by the Directorate-General for Translation (Steinberger et al., 2014), MultiUN corpus (Eisele and Chen, 2010), EMEA, KDE4, OpenOffice (Tiedemann, 2009), OpenSubtitles2012 (Tiedemann, 2012). Similarly, we concatenate parallel corpora for identifying relevant sentences containing WordNet entries, which are then translated into the targeted languages. Table 2 shows the number of parallel sentences used for the ten language pairs. 4.4 Translation Evaluation Metrics The automatic translation evaluation is based on the correspondence between the SMT output and reference translation (gold standard). For the automatic evaluation we used the BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014) and"
C16-1010,W11-2123,0,0.0256256,"37M 43M Table 2: Statistics on parallel data for translation model training and word-sense disambiguation. (parallel resources used for training the translation models1 and/or word-sense disambiguation2 ) The decoder, which is a search procedure, provides the most probable translation based on a statistical translation model learned from the training data. For our translation task, we use the statistical translation toolkit Moses (Koehn et al., 2007), where word alignments, necessary for generating translation models, were built with the GIZA++ toolkit (Och and Ney, 2003). The Kenlm toolkit (Heafield, 2011) was used to build a 5-gram language model. 4.3 Parallel Resources for SMT training and Word-Sense-Disambiguation To ensure a broad lexical and domain coverage of our SMT system we merged the existing parallel corpora for each language pair from the OPUS web page5 into one parallel data set, i.e., Europarl (Koehn, 2005), DGT - translation memories generated by the Directorate-General for Translation (Steinberger et al., 2014), MultiUN corpus (Eisele and Chen, 2010), EMEA, KDE4, OpenOffice (Tiedemann, 2009), OpenSubtitles2012 (Tiedemann, 2012). Similarly, we concatenate parallel corpora for ide"
C16-1010,W02-0808,0,0.183729,"Missing"
C16-1010,W09-4202,0,0.0213779,"the creation of the Open Multilingual Wordnet and its extension with other resources (Bond and Foster, 2013). A different approach to expand English WordNet synsets with lexicalizations in other languages was proposed in de Melo and Weikum (2012). The authors do not directly match concepts in the two different language resources, but demonstrate an approach that learns how to determine the best translation for English synsets by taking bilingual dictionaries, structural information of the English WordNet and corpus frequency information into account. With the growing amount of parallel data, Kazakov and Shahid (2009) show an approach to acquire a set of synsets from parallel corpora. The synsets are obtained by comparing aligned words in parallel corpora in several languages. Similarly, the sloWNet for Slovene (Fišer, 2007) and Wolf for French (Sagot and Fišer, 2008) are constructed using a multilingual corpus and word alignment techniques in combination with other existing lexical resources. 2 The Polylingual WordNet is available at http://polylingwn.linguistic-lod.org/ 98 Since all these approaches use word alignment information, they are not able to generate any translation equivalents for multi-word e"
C16-1010,N03-1017,0,0.0314089,"Missing"
C16-1010,2005.mtsummit-papers.11,0,0.191404,"del learned from the training data. For our translation task, we use the statistical translation toolkit Moses (Koehn et al., 2007), where word alignments, necessary for generating translation models, were built with the GIZA++ toolkit (Och and Ney, 2003). The Kenlm toolkit (Heafield, 2011) was used to build a 5-gram language model. 4.3 Parallel Resources for SMT training and Word-Sense-Disambiguation To ensure a broad lexical and domain coverage of our SMT system we merged the existing parallel corpora for each language pair from the OPUS web page5 into one parallel data set, i.e., Europarl (Koehn, 2005), DGT - translation memories generated by the Directorate-General for Translation (Steinberger et al., 2014), MultiUN corpus (Eisele and Chen, 2010), EMEA, KDE4, OpenOffice (Tiedemann, 2009), OpenSubtitles2012 (Tiedemann, 2012). Similarly, we concatenate parallel corpora for identifying relevant sentences containing WordNet entries, which are then translated into the targeted languages. Table 2 shows the number of parallel sentences used for the ten language pairs. 4.4 Translation Evaluation Metrics The automatic translation evaluation is based on the correspondence between the SMT output and"
C16-1010,P03-1058,0,0.073926,"ge of Wikipedia. This is done by assigning WordNet synsets to Wikipedia entries, and making these relations multilingual through the interlingual links. For languages, which do not have the corresponding Wikipedia entry, the authors use Google Translate to translate English sentences containing the synset in the sense annotated corpus. After that, the most frequent translation is included as a variant for the synset for the given language. The use of parallel corpora has been previously exploited for word sense disambiguation, for example to construct sense-tagged corpora in another language (Ng et al., 2003) or by using translations as a method to discriminate senses (Ide et al., 2002). It has been shown that the combination of these techniques can improve supervised word sense disambiguation (Chan et al., 2007). A similar approach to the one proposed in this paper is that of Tufi¸s et al. (2004), where they show that using the interlingual index of WordNet with the help of parallel text can improve word sense disambiguation of a monolingual approach and we generalize this result to generate wordnets for new languages. 3 Methodology Our approach takes the advantage of the increasing amount of par"
C16-1010,J03-1002,0,0.00552686,"7M 296M 377M 130M 378M 302M 34M 33M 13M 37M 43M Table 2: Statistics on parallel data for translation model training and word-sense disambiguation. (parallel resources used for training the translation models1 and/or word-sense disambiguation2 ) The decoder, which is a search procedure, provides the most probable translation based on a statistical translation model learned from the training data. For our translation task, we use the statistical translation toolkit Moses (Koehn et al., 2007), where word alignments, necessary for generating translation models, were built with the GIZA++ toolkit (Och and Ney, 2003). The Kenlm toolkit (Heafield, 2011) was used to build a 5-gram language model. 4.3 Parallel Resources for SMT training and Word-Sense-Disambiguation To ensure a broad lexical and domain coverage of our SMT system we merged the existing parallel corpora for each language pair from the OPUS web page5 into one parallel data set, i.e., Europarl (Koehn, 2005), DGT - translation memories generated by the Directorate-General for Translation (Steinberger et al., 2014), MultiUN corpus (Eisele and Chen, 2010), EMEA, KDE4, OpenOffice (Tiedemann, 2009), OpenSubtitles2012 (Tiedemann, 2012). Similarly, we"
C16-1010,P02-1040,0,0.103798,"inberger et al., 2014), MultiUN corpus (Eisele and Chen, 2010), EMEA, KDE4, OpenOffice (Tiedemann, 2009), OpenSubtitles2012 (Tiedemann, 2012). Similarly, we concatenate parallel corpora for identifying relevant sentences containing WordNet entries, which are then translated into the targeted languages. Table 2 shows the number of parallel sentences used for the ten language pairs. 4.4 Translation Evaluation Metrics The automatic translation evaluation is based on the correspondence between the SMT output and reference translation (gold standard). For the automatic evaluation we used the BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014) and chrF (Popovi´c, 2015) metrics. BLEU (Bilingual Evaluation Understudy) is calculated for individual translated segments (n-grams) by comparing them with a data set of reference translations.6 The calculated scores, between 0 and 100 (perfect translation), are averaged over the whole evaluation data set to reach an estimate of the translation’s overall quality. Considering the short length of the terms in WordNet, while we report scores based on the unigram overlap (BLEU-1), this is in most cases only precision, so in addition we also report other metrics"
C16-1010,W15-3049,0,0.0522913,"Missing"
C16-1010,2016.gwc-1.43,0,0.0636712,"Missing"
C16-1010,W15-3031,0,0.0649935,"Missing"
C16-1010,E12-1015,0,0.17713,"ZA++ toolkit (Och and Ney, 2003). The Kenlm toolkit (Heafield, 2011) was used to build a 5-gram language model. 4.3 Parallel Resources for SMT training and Word-Sense-Disambiguation To ensure a broad lexical and domain coverage of our SMT system we merged the existing parallel corpora for each language pair from the OPUS web page5 into one parallel data set, i.e., Europarl (Koehn, 2005), DGT - translation memories generated by the Directorate-General for Translation (Steinberger et al., 2014), MultiUN corpus (Eisele and Chen, 2010), EMEA, KDE4, OpenOffice (Tiedemann, 2009), OpenSubtitles2012 (Tiedemann, 2012). Similarly, we concatenate parallel corpora for identifying relevant sentences containing WordNet entries, which are then translated into the targeted languages. Table 2 shows the number of parallel sentences used for the ten language pairs. 4.4 Translation Evaluation Metrics The automatic translation evaluation is based on the correspondence between the SMT output and reference translation (gold standard). For the automatic evaluation we used the BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014) and chrF (Popovi´c, 2015) metrics. BLEU (Bilingual Evaluation Understudy) is calcu"
C16-1010,C04-1192,0,0.101426,"Missing"
D13-1179,P07-2045,0,0.00343338,"Missing"
D13-1179,2005.mtsummit-papers.11,0,0.05364,"Missing"
D13-1179,P10-1116,0,0.0732658,"Missing"
D13-1179,D09-1092,0,0.153583,"Missing"
D13-1179,palmer-etal-1998-rapid,0,0.0865569,"citly defined topics, but then computes latent relations between these. Thus, the method combines the benefits of both explicit and latent topic modelling approaches. We show that on a crosslingual mate retrieval task, our model significantly outperforms LDA, LSI, and ESA, as well as a baseline that translates every word in a document into the target language. 1 Introduction Cross-lingual document matching is the task of, given a query document in some source language, estimating the similarity to a document in some target language. This task has important applications in machine translation (Palmer et al., 1998; Tam et al., 2007), word sense disambiguation (Li et al., 2010) and ontology alignment (Spiliopoulos et al., 2007). An approach that has become quite popular in recent years for cross-lingual document matching is Explicit Semantics Analysis (ESA, Gabrilovich and Markovitch (2007)) and its cross-lingual extension A key choice in Explicit Semantic Analysis is the document space that will act as the topic space. The standard choice is to regard all articles from a background document collection – Wikipedia articles are a typical choice – as the topic space. However, it is crucial to ensure that"
ehrmann-etal-2014-representing,C08-2017,0,\N,Missing
ehrmann-etal-2014-representing,van-assem-etal-2006-conversion,0,\N,Missing
ehrmann-etal-2014-representing,mccrae-etal-2012-collaborative,1,\N,Missing
ehrmann-etal-2014-representing,Q14-1019,1,\N,Missing
ehrmann-etal-2014-representing,E12-1059,0,\N,Missing
ehrmann-etal-2014-representing,P13-1133,0,\N,Missing
ehrmann-etal-2014-representing,P14-1044,1,\N,Missing
ehrmann-etal-2014-representing,S13-2040,1,\N,Missing
ehrmann-etal-2014-representing,P14-1089,1,\N,Missing
ehrmann-etal-2014-representing,P14-1122,1,\N,Missing
ehrmann-etal-2014-representing,francopoulo-etal-2006-lexical,0,\N,Missing
L16-1386,2016.gwc-1.9,1,0.821562,"gy, since they are used as domain specific ontologies. Additionally, other supporting ontologies have been added, such as GeoNames for the named entities; PROTON as an upper ontology; SKOS as a mapper between ontologies and terminological lexicons; Dublin Core as a metadata ontology. Also, for the purposes of search, Web Interface Querying EUCases Linking Platform was designed. For its Web Interface, the EUCases Linking Platform relies on a customized version of the GraphDB Workbench27 , developed by Ontotext AD. 5.2. Wordnet Interlingual Index (ILI) A recent development (Vossen et al., 2016; Bond et al., 2016) has been the adoption of LLOD technology by the wordnet community, with a new plan that uses LLOD as the basic mechanism for the creation of links between wordnets in different languages. This Collaborative InterLingual Index enables wordnets to share and link their resources for concepts lexicalized in any of the group’s languages. This was supported directly by a workshop at the 2016 Global WordNet Conference and will lead to the adoption of LLOD technology by a new community. In addition, the open multilingual wordnet (Bond et al., 2014) provides all open wordnets for download using the le"
L16-1386,calzolari-etal-2012-lre,0,0.0712075,"Missing"
L16-1386,ehrmann-etal-2014-representing,1,0.804428,"lable by attempting to download it and discarding all resources that are no longer available. We have attempted to notify the authors of resources that no longer meet the criteria for inclusion in the cloud. However, our experience has been that this did not motivate many authors to update their resources. 2.5. Vocabularies The Linguistic Linked Open Data Cloud has grown significantly in the last few years and most notably, unlike the non-linguistic LOD Cloud, is not centered around one nucleus but instead has used many different vocabularies and datasets to link to. Among these are BabelNet (Ehrmann et al., 2014), LexInfo (Cimiano et al., 2011), and Lexvo (de Melo, 2015). In addition, a number of new vocabularies have emerged including the OntoLex model,13 the NLP Interchange format NIF (Hellmann et al., 2013), the WordNet Interlingual Index (Sect. 5.2.), and the FrameBase schema (Rouces et al., 2015a) (Sect. 5.3.). These vocabularies have increased the power of linked data to represent the complete spectrum of language resources and show that new resources can be created that use the power of linked data to link across different types of languages resources, such as terminologies and dictionaries (Si"
L16-1386,federmann-etal-2012-meta,0,0.0601026,"Missing"
L16-1386,W15-4205,1,0.920405,"not necessarily created for this purpose, e.g., large collections of texts such as news articles, terminological or encyclopedic and general-purpose knowledge bases such as DBpedia (Bizer et al., 2009), or metadata collections. 2.2. Infrastructure and Metadata The OWLG provides guidelines to data publishers on how to include their resources in the LLOD cloud.6 The cloud diagram is currently generated from metadata maintained at DataHub7 and hence contains only resources described in DataHub. An alternative metadata repository specialized for linguistic resources is under development: Linghub (McCrae et al., 2015a).8 It aims to provide a search engine and index for linguistic resources and attempts to harmonize metadata from a number of different sources, including Metashare (Federmann et al., 2012), CLARIN VLO (Van Uytvanck et al., 2012), DataHub and LRE Map (Calzolari et al., 2012). It will soon replace DataHub in the generation of the cloud diagram. LingHub, 4 http://lod2.eu/ http://qtleap.eu/ 6 http://wiki.okfn.org/Working_Groups/ Linguistics/How_to_contribute 7 http://datahub.io 8 http://linghub.org 5 Datasets Links 28 53 103 126 128 41 78 167 203 209 February 2012 September 2013 November 2014 Ma"
L16-1386,W15-4201,0,0.02663,"Missing"
L16-1386,W15-4207,1,0.813122,"4), LexInfo (Cimiano et al., 2011), and Lexvo (de Melo, 2015). In addition, a number of new vocabularies have emerged including the OntoLex model,13 the NLP Interchange format NIF (Hellmann et al., 2013), the WordNet Interlingual Index (Sect. 5.2.), and the FrameBase schema (Rouces et al., 2015a) (Sect. 5.3.). These vocabularies have increased the power of linked data to represent the complete spectrum of language resources and show that new resources can be created that use the power of linked data to link across different types of languages resources, such as terminologies and dictionaries (Siemoneit et al., 2015) and corpora and dictionaries (McGovern et al., 2015). 3. OWLG members have been very active in promoting the development and adoption of linguistic linked data, which had an effect not only in the growth of the LLOD cloud but in the development of representation models, guidelines, and best practices. These activities have been developed in the context of a number of W3C groups and projects, as it is detailed in the rest of this section. 13 12 http://lodvader.aksw.org/ Other Community Group Efforts http://cimiano.github.io/ontolex/ specification.html 2437 3.1. OntoLex 8. LLOD aware services 1"
L16-1386,van-uytvanck-etal-2012-semantic,0,0.0695217,"Missing"
L18-1149,P15-1069,1,0.827864,"ord-alignment and machine translation approaches, and compared the results with the proposed semantics transfer approach, focusing on the semantic coherence of the generated translations between Spanish, French and German. Sajous et al. (2010) enriched Wiktionary by relying on similarity measures based on random walks through the graphs extracted from its lexical networks. In their final step they engaged users in collaborative editing in order to validate the resource. A different approach for translation and disambiguation of domainspecific expressions stored in knowledge bases was shown in Arcan et al. (2015), where the authors identified relevant in-domain parallel sentences and used them to train a small but domain-aware SMT system. Ordan et al. (2017) demonstrated an approach for bilingual dictionary creation using different translation directions within a loop. In contrast, de Melo and Weikum (2012) did not match concepts with SMT, but showed a machine learning approach which determines the best translation for English WordNet synsets by taking bilingual dictionaries, structural information of WordNet and corpus frequency information into account. Similarly, the multilingual disambiguation of"
L18-1149,C16-1010,1,0.797131,"idge the gap between language-specific information and the language-independent semantic content (Gracia et al., 2012). Since manual multilingual translation and evaluation of knowledge bases is a very time-consuming and expensive process, we apply SMT to automatically translate domain-specific expressions and demonstrate its validity by translating the IATE entries. While an SMT system can only return the most frequent or dominant translation when given a term by itself, it has been showed that SMT provides strong word sense disambiguation when the word is given in the context of a sentence (Arcan et al., 2016a; Arcan et al., 2016b). As a motivating example, we consider the word vessel, which appears several times in the IATE repository, whereby the most frequent translation into German is Schiff, with the meaning of ‘a craft designed for water transportation’, e.g., as given by Google Translate.3 To overcome the issue of obtaining translations for vessel in other languages, and also in different domains (in the sense of blood vessel, for instance), we aim to identify (several) parallel sentences, which hold the terminological entries in the targeted domain, and use their context to translate them"
L18-1149,P11-2031,0,0.0290034,"luation data set to reach an estimate of the translation’s overall quality. METEOR (Denkowski and Lavie, 2014) is based on the harmonic mean of precision and recall, whereby recall is weighted higher than precision. Along with exact word (or phrase) matching it uses additional features, i.e., stemming, paraphrasing and synonymy matching. chrF3 (Popovi´c, 2015) is a character n-gram metric, which has shown very good correlations with human judgements, especially when translating from English into morphologically rich languages (Stanojevi´c et al., 2015). The approximate randomization approach (Clark et al., 2011) is used to test whether differences among system performances are statistically significant. 5. Results In this section, we present the evaluation of the translated IATE entries into several languages not initially included in this resource, and how existing IATE terms have been exploited for our purposes in the parallel corpora used in this work.7 In addition to this, we illustrate the enhancing of IATE RDF resource with additional contextual information 7 We randomly selected 2,000 terms, although not all target terms are represented in each language for evaluation. 933 # of Terms Bulgarian"
L18-1149,declerck-etal-2006-multilingual,0,0.117658,"Missing"
L18-1149,W14-3348,0,0.0177264,"3M 163M 938k 1M 687k 561k 1M 1M 640k 826k 1M 180k 626k 1M 934k 421k 389k 218k 976k 1M 1M 1M 543k 631k 687k 1M 1M 1M 1M 1M 1M 1M 2M 1M 271k 1M 3M 1M 833k 653k 380k 1M 1M 1M 1M 1M 1M 1M Table 3: Statistics on parallel data for translation model training and word-sense disambiguation. data set of reference translations. Considering the shortness of the entries in IATE, we report scores based on the unigram overlap (BLEU-1). Those scores, between 0 and 100 (perfect translation), are then averaged over the whole evaluation data set to reach an estimate of the translation’s overall quality. METEOR (Denkowski and Lavie, 2014) is based on the harmonic mean of precision and recall, whereby recall is weighted higher than precision. Along with exact word (or phrase) matching it uses additional features, i.e., stemming, paraphrasing and synonymy matching. chrF3 (Popovi´c, 2015) is a character n-gram metric, which has shown very good correlations with human judgements, especially when translating from English into morphologically rich languages (Stanojevi´c et al., 2015). The approximate randomization approach (Clark et al., 2011) is used to test whether differences among system performances are statistically significan"
L18-1149,eisele-chen-2010-multiun,0,0.0159357,"nts, necessary for generating translation models, were built with the GIZA++ toolkit (Och and Ney, 2003). The KenLM toolkit (Heafield, 2011) was used to build a 5-gram language model. 4.3. Parallel Resources for SMT training and Multilingual Word Sense Disambiguation To ensure a broad lexical and domain coverage of our SMT system, we merged the existing parallel corpora for each language pair from the OPUS web page6 into one parallel data set, i.e., Europarl (Koehn, 2005), DGT translation memories generated by the Directorate-General for Translation (Steinberger et al., 2014), MultiUN corpus (Eisele and Chen, 2010), EMEA, KDE4, OpenOffice (Tiedemann, 2009), OpenSubtitles2012 (Tiedemann, 2012). Similarly, we concatenated parallel corpora to identify relevant sentences containing IATE entries, which are then translated into the targeted languages. Table 3 shows the amount of parallel sentences used for the different language pairs. 4.4. Translation Evaluation Metrics The automatic translation evaluation is based on the correspondence between the SMT output and reference translation (gold standard). BLEU (Papineni et al., 2002) is calculated for individual translated segments (n-grams) by comparing them wi"
L18-1149,W11-2123,0,0.0166436,"the best translation of a string, given by a log-linear model combining a set of features. The translation that maximizes the score of the log-linear model is obtained by searching all possible translation candidates. The decoder, which is a search procedure, provides the most probable translation based on a statistical translation model learned from the training data. For our task, we use the statistical translation toolkit Moses (Koehn et al., 2007), where word alignments, necessary for generating translation models, were built with the GIZA++ toolkit (Och and Ney, 2003). The KenLM toolkit (Heafield, 2011) was used to build a 5-gram language model. 4.3. Parallel Resources for SMT training and Multilingual Word Sense Disambiguation To ensure a broad lexical and domain coverage of our SMT system, we merged the existing parallel corpora for each language pair from the OPUS web page6 into one parallel data set, i.e., Europarl (Koehn, 2005), DGT translation memories generated by the Directorate-General for Translation (Steinberger et al., 2014), MultiUN corpus (Eisele and Chen, 2010), EMEA, KDE4, OpenOffice (Tiedemann, 2009), OpenSubtitles2012 (Tiedemann, 2012). Similarly, we concatenated parallel c"
L18-1149,N03-1017,0,0.00891204,"uation techniques. 4.1. IATE - Inter-Active Terminology for Europe IATE is the terminology database of the EU with its objective of supporting the EU translators and creating a terminology resource to ensure standardisation throughout all institutions. It incorporates the various terminology databases into one database containing approximately one million multilingual entries in English (Table 2).5 Recent domains that have been extensively covered include the financial crisis, environment, fisheries and migration. 4.2. Statistical Machine Translation Our approach is based on phrase-based SMT (Koehn et al., 2003), where we wish to find the best translation of a string, given by a log-linear model combining a set of features. The translation that maximizes the score of the log-linear model is obtained by searching all possible translation candidates. The decoder, which is a search procedure, provides the most probable translation based on a statistical translation model learned from the training data. For our task, we use the statistical translation toolkit Moses (Koehn et al., 2007), where word alignments, necessary for generating translation models, were built with the GIZA++ toolkit (Och and Ney, 20"
L18-1149,P07-2045,0,0.0132526,"s, environment, fisheries and migration. 4.2. Statistical Machine Translation Our approach is based on phrase-based SMT (Koehn et al., 2003), where we wish to find the best translation of a string, given by a log-linear model combining a set of features. The translation that maximizes the score of the log-linear model is obtained by searching all possible translation candidates. The decoder, which is a search procedure, provides the most probable translation based on a statistical translation model learned from the training data. For our task, we use the statistical translation toolkit Moses (Koehn et al., 2007), where word alignments, necessary for generating translation models, were built with the GIZA++ toolkit (Och and Ney, 2003). The KenLM toolkit (Heafield, 2011) was used to build a 5-gram language model. 4.3. Parallel Resources for SMT training and Multilingual Word Sense Disambiguation To ensure a broad lexical and domain coverage of our SMT system, we merged the existing parallel corpora for each language pair from the OPUS web page6 into one parallel data set, i.e., Europarl (Koehn, 2005), DGT translation memories generated by the Directorate-General for Translation (Steinberger et al., 201"
L18-1149,2005.mtsummit-papers.11,0,0.0830539,"learned from the training data. For our task, we use the statistical translation toolkit Moses (Koehn et al., 2007), where word alignments, necessary for generating translation models, were built with the GIZA++ toolkit (Och and Ney, 2003). The KenLM toolkit (Heafield, 2011) was used to build a 5-gram language model. 4.3. Parallel Resources for SMT training and Multilingual Word Sense Disambiguation To ensure a broad lexical and domain coverage of our SMT system, we merged the existing parallel corpora for each language pair from the OPUS web page6 into one parallel data set, i.e., Europarl (Koehn, 2005), DGT translation memories generated by the Directorate-General for Translation (Steinberger et al., 2014), MultiUN corpus (Eisele and Chen, 2010), EMEA, KDE4, OpenOffice (Tiedemann, 2009), OpenSubtitles2012 (Tiedemann, 2012). Similarly, we concatenated parallel corpora to identify relevant sentences containing IATE entries, which are then translated into the targeted languages. Table 3 shows the amount of parallel sentences used for the different language pairs. 4.4. Translation Evaluation Metrics The automatic translation evaluation is based on the correspondence between the SMT output and r"
L18-1149,W11-1013,1,0.819266,"Missing"
L18-1149,J03-1002,0,0.00757023,"n et al., 2003), where we wish to find the best translation of a string, given by a log-linear model combining a set of features. The translation that maximizes the score of the log-linear model is obtained by searching all possible translation candidates. The decoder, which is a search procedure, provides the most probable translation based on a statistical translation model learned from the training data. For our task, we use the statistical translation toolkit Moses (Koehn et al., 2007), where word alignments, necessary for generating translation models, were built with the GIZA++ toolkit (Och and Ney, 2003). The KenLM toolkit (Heafield, 2011) was used to build a 5-gram language model. 4.3. Parallel Resources for SMT training and Multilingual Word Sense Disambiguation To ensure a broad lexical and domain coverage of our SMT system, we merged the existing parallel corpora for each language pair from the OPUS web page6 into one parallel data set, i.e., Europarl (Koehn, 2005), DGT translation memories generated by the Directorate-General for Translation (Steinberger et al., 2014), MultiUN corpus (Eisele and Chen, 2010), EMEA, KDE4, OpenOffice (Tiedemann, 2009), OpenSubtitles2012 (Tiedemann, 2012). S"
L18-1149,P02-1040,0,0.108804,"Missing"
L18-1149,W15-3049,0,0.0629611,"Missing"
L18-1149,W15-3031,0,0.0609754,"Missing"
L18-1149,E12-1015,0,0.0499086,"(Och and Ney, 2003). The KenLM toolkit (Heafield, 2011) was used to build a 5-gram language model. 4.3. Parallel Resources for SMT training and Multilingual Word Sense Disambiguation To ensure a broad lexical and domain coverage of our SMT system, we merged the existing parallel corpora for each language pair from the OPUS web page6 into one parallel data set, i.e., Europarl (Koehn, 2005), DGT translation memories generated by the Directorate-General for Translation (Steinberger et al., 2014), MultiUN corpus (Eisele and Chen, 2010), EMEA, KDE4, OpenOffice (Tiedemann, 2009), OpenSubtitles2012 (Tiedemann, 2012). Similarly, we concatenated parallel corpora to identify relevant sentences containing IATE entries, which are then translated into the targeted languages. Table 3 shows the amount of parallel sentences used for the different language pairs. 4.4. Translation Evaluation Metrics The automatic translation evaluation is based on the correspondence between the SMT output and reference translation (gold standard). BLEU (Papineni et al., 2002) is calculated for individual translated segments (n-grams) by comparing them with a 5 6 Based on IATE TBX file - IATE export 16032017.tbx http://opus.nlpl.eu/"
L18-1192,W17-5223,1,0.825919,"ing n rating scale annotations results in comaparisons. n(n−1) 2 1200 Dimension Spearman Correlation (Regression Models) F1 (Comparison Models) Valence Arousal Dominance Surprise 0.72 0.64 0.53 0.42 0.72 0.69 0.71 0.63 Average 0.58 0.69 7. Table 5: Cross validation results for rating scale regression models and comparison classification models. 5. Predictive Model As further verification of the utility of the data, we built two supervised models, one each from the 5-point rating scale and pairwise comparison annotations. For the rating scale data, regressions were built using the approach in (Andryushechkin et al., 2017). This model consists of an ensemble of two supervised models: an SVR (Support Vector Machine Regression) with n-gram and several custom features (see (Andryushechkin et al., 2017)) and a BiLSTM (Bidirectional Long-Short Term Memory) model utilising 100 dimensional Glove word embeddings trained on tweets (Pennington et al., 2014). For the comparison data, an SVM (Support Vector Machine) was built using the same Glove word embeddings as features. The cross validation results shown in Table 5 indicate that supervised modelling can be effective for predicting emotions using this data. 6. Conclusi"
L18-1192,E17-2092,0,0.0433944,"ular with dimensional annotations. Existing text corpora with dimensional emotion annotations include Affective Norms for English Texts (Bradley and Lang, 2007), a collection of 120 generic texts with VAD annotations; a collection of 2,895 Facebook posts annotated by just two annotators with valence and arousal dimensions (Preotiuc-Pietro et al., 2016). Yu et.al. (2016) presented a collection of 2009 Chinese sentences from various online texts, again annotated with valence and arousal only. Subsequent to our annotation efforts, several further annotated data sets have been published: EMOBANK (Buechel and Hahn, 2017), a collection of ten thousand texts from diverse sources, but not including tweets, and data for the upcoming “Affect in Tweets” task for SemEval 20181 which presents tweets annotated for valence, arousal and dominance in English, Spanish and Arabic. In addition, two recent data sets annotated with emotion intensity in Ekman emotion categories have also been released: data for the WASSA emotion intensity detection competition (Mohammad and Bravo-Marquez, 2017), 1,500 to 2,000 tweets for each of the four Ekman emotions joy, anger, sadness and fear; and further data from SemEval 2018. Several a"
L18-1192,P17-2074,0,0.368721,"ordinal scale, such as the SAM manikins (Bradley and Lang, 1994). It has been argued that human estimations of relative values are more consistent than when assigning an absolute value (Metallinou and Narayanan, 2013; Yannakakis et al., 2017). To address this, Martinez et.al. (2014) suggest that ranked annotations not be treated as absolute values, and instead treated as ordinal, and used, for example, to train ranking estimators. Another approach is to perform relative annotations directly, such as best/worst scaling, where the highest and lowest ranked tweets are chosen from a set of four (Kiritchenko and Mohammad, 2017). Pairwise tweet comparisons are another option, however we are not aware of this approach being used previously in the emotion annotation literature as it requires a large number of annotations to acquire a reasonable ranking. In this work, we present a collection of 2,019 tweets annotated following the four dimensional emotion representation scheme of Fontaine et.al. (2007). We further assess the relative merits of annotations on a ranking scale vs. comparisons, providing annotations using both a 5 point rank1197 1 http://alt.qcri.org/semeval2018 ing scale and pairwise comparisons2 . An init"
L18-1192,W17-5205,0,0.147475,"annotated with valence and arousal only. Subsequent to our annotation efforts, several further annotated data sets have been published: EMOBANK (Buechel and Hahn, 2017), a collection of ten thousand texts from diverse sources, but not including tweets, and data for the upcoming “Affect in Tweets” task for SemEval 20181 which presents tweets annotated for valence, arousal and dominance in English, Spanish and Arabic. In addition, two recent data sets annotated with emotion intensity in Ekman emotion categories have also been released: data for the WASSA emotion intensity detection competition (Mohammad and Bravo-Marquez, 2017), 1,500 to 2,000 tweets for each of the four Ekman emotions joy, anger, sadness and fear; and further data from SemEval 2018. Several approaches to annotating emotion expressed in text on a continuous scale have been used. Probably the most common utilises an ordinal scale, such as the SAM manikins (Bradley and Lang, 1994). It has been argued that human estimations of relative values are more consistent than when assigning an absolute value (Metallinou and Narayanan, 2013; Yannakakis et al., 2017). To address this, Martinez et.al. (2014) suggest that ranked annotations not be treated as absolu"
L18-1192,passonneau-2004-computing,0,0.163964,"rics operate on a similar scale (i.e.: values for conceptually similar annotation differences should be the same). In this work, we do not attempt to empirically evaluate these disagreement metrics beyond comparison of agreement values on the presented data sets. 4.1. Annotation Difference Metrics Categorical Annotations (Multiple Categories Allowed) There are several metrics that have been applied to categorical annotations with multiple categories allowed. The Jacccard set similarity metric (Jaccard, 1912) is the ratio between the sizes of the intersection and union of the sets. Passonneau (Passonneau, 2004) observed that if one annotator is inclined to provide, in general, more labels than another annotator, you should consider any extra labels from the prolific annotator as less indicative of disagreement, proposing a simple difference metric that attempts to capture this idea (see below). Passonneau later proposed a combination of the two metrics (Passonneau, 2006), capturing the granularity of the Jaccard metric and the motivating principle of his previous proposal. He named this metric MASI (Measuring Agreement on Set-valued Items). In the formulae below, A and B refer to two annotations of"
L18-1192,passonneau-2006-measuring,0,0.0544999,"ral metrics that have been applied to categorical annotations with multiple categories allowed. The Jacccard set similarity metric (Jaccard, 1912) is the ratio between the sizes of the intersection and union of the sets. Passonneau (Passonneau, 2004) observed that if one annotator is inclined to provide, in general, more labels than another annotator, you should consider any extra labels from the prolific annotator as less indicative of disagreement, proposing a simple difference metric that attempts to capture this idea (see below). Passonneau later proposed a combination of the two metrics (Passonneau, 2006), capturing the granularity of the Jaccard metric and the motivating principle of his previous proposal. He named this metric MASI (Measuring Agreement on Set-valued Items). In the formulae below, A and B refer to two annotations of a data element (tweet in our case), with each a set of annotated categories. Jacc(A, B) = 1 − |A ∩ B| |A ∪ B|  0    0.3 Pass(A, B) =  0.6    1 A=B A ⊂ B or B ⊂ A A ∩ B 6= ∅ A∩B =∅ Masi(A, B) = 1 − Jacc(A, B) × Pass(A, B) Another scenario, where the above metrics could be seen as overly pessimistic, is as follows: in cases where an anno1199 Metric Wood Masi"
L18-1192,D14-1162,0,0.0824505,"Missing"
L18-1192,W16-0404,0,0.0606471,"tion of positive/negative sentiment, while more nuanced emotion representation models have received relatively little attention. In particular, there has been a lack of quality annotated resources for model building and evaluation in that space (Mohammad, 2016) and in particular with dimensional annotations. Existing text corpora with dimensional emotion annotations include Affective Norms for English Texts (Bradley and Lang, 2007), a collection of 120 generic texts with VAD annotations; a collection of 2,895 Facebook posts annotated by just two annotators with valence and arousal dimensions (Preotiuc-Pietro et al., 2016). Yu et.al. (2016) presented a collection of 2009 Chinese sentences from various online texts, again annotated with valence and arousal only. Subsequent to our annotation efforts, several further annotated data sets have been published: EMOBANK (Buechel and Hahn, 2017), a collection of ten thousand texts from diverse sources, but not including tweets, and data for the upcoming “Affect in Tweets” task for SemEval 20181 which presents tweets annotated for valence, arousal and dominance in English, Spanish and Arabic. In addition, two recent data sets annotated with emotion intensity in Ekman emo"
L18-1192,N16-1066,0,0.0670196,"Missing"
L18-1324,S16-1168,1,0.928904,"Missing"
L18-1324,S16-1205,0,0.0176911,"most important goals of such research and several methods going back to (Hearst, 1992) have been proposed for this task. A recent such system to use this is the TAXI system (Panchenko et 2059 al., 2016), which combined simple string substring metrics with Hearst-like patterns learned from text, which are then constructed into a taxonomy using Tarjan’s algorithm (Tarjan, 1972). A different approach, by (Tan et al., 2016), used the endocentricity of a term, that is if a term contains another term, e.g., ‘fish’ in ‘goldfish’, whether this indicates a hypernymlike relationship. The QASSIT system (Cleuziou and Moreno, 2016) used the genetic algorithm in order to learn taxonomic relations, however performance across domains was poor. BabelNet (Navigli and Ponzetto, 2012), a wide coverage dictionary, has been used both as a source of information about taxonomic relations (Maitra and Das, 2016) and also itself was constructed using automatic taxonomy learning (Navigli et al., 2011). However, the focus of this has been mostly on single words as would be found in a dictionary and less on the kind of multi-word terminology that can be used to describe specialist domains as in this paper. Finally, there has been some w"
L18-1324,P14-1113,0,0.0231681,"rformance across domains was poor. BabelNet (Navigli and Ponzetto, 2012), a wide coverage dictionary, has been used both as a source of information about taxonomic relations (Maitra and Das, 2016) and also itself was constructed using automatic taxonomy learning (Navigli et al., 2011). However, the focus of this has been mostly on single words as would be found in a dictionary and less on the kind of multi-word terminology that can be used to describe specialist domains as in this paper. Finally, there has been some work on the use of word embeddings to predict hypernym relations, such as in (Fu et al., 2014), where a linear project function from a word embedding to its hypernym was constructed. The possibility of combining this with syntactic patterns for hypernym discovery has also been investigated (Shwartz et al., 2016). 3. Methodology Our methodology is based on the use of multiple features that can be extracted from the labels or an associated corpus. We then learn to combine these using a supervised learning approach. In this section we will present the methodology for the features we used. 3.1. Data While, the TexEval task has recently given a baseline, by which performance on this task ca"
L18-1324,C92-2082,0,0.587432,"ection of texts and secondly, in the taxonomy learning step these terms are grouped into a hierarchical structure. The first step is well explored and recent strong results have been shown on this task (Astrakhantsev, 2016; Buitelaar et al., 2013) and as such we shall not focus on it in the course of this article. The second task is however much less well-explored and this is the focus of this article, and so we assume that the terms have already been identified by an approach such as those outlines above. The task of taxonomy extraction is closely related to tasks such as hypernym detection (Hearst, 1992) or ontology learning (Buitelaar et al., 2005), in which a structured representation of concepts should be learned. However, the task of taxonomy extraction does not have the formal nature 1 http://www.acm.org/publications/ class-2012 that either of these tasks in that the terms only need to be loosely associated. For examples, taxonomies frequently place terms under broader concepts that do not match the strict requirements of ontological subsumption (Gangemi et al., 2002) that would be required from an ontology, e.g., grouping “Kalman Filter” is under “Filtering” in the IEEE taxonomy, where"
L18-1324,S16-1204,0,0.0127014,"earned from text, which are then constructed into a taxonomy using Tarjan’s algorithm (Tarjan, 1972). A different approach, by (Tan et al., 2016), used the endocentricity of a term, that is if a term contains another term, e.g., ‘fish’ in ‘goldfish’, whether this indicates a hypernymlike relationship. The QASSIT system (Cleuziou and Moreno, 2016) used the genetic algorithm in order to learn taxonomic relations, however performance across domains was poor. BabelNet (Navigli and Ponzetto, 2012), a wide coverage dictionary, has been used both as a source of information about taxonomic relations (Maitra and Das, 2016) and also itself was constructed using automatic taxonomy learning (Navigli et al., 2011). However, the focus of this has been mostly on single words as would be found in a dictionary and less on the kind of multi-word terminology that can be used to describe specialist domains as in this paper. Finally, there has been some work on the use of word embeddings to predict hypernym relations, such as in (Fu et al., 2014), where a linear project function from a word embedding to its hypernym was constructed. The possibility of combining this with syntactic patterns for hypernym discovery has also b"
L18-1324,S16-1206,0,0.0322349,"Missing"
L18-1324,P16-1226,0,0.0441135,"Missing"
L18-1383,P06-4018,0,0.0626103,"olve real-world problems. However, it is frequently the case that these components are developed independently and thus far integration of these services is far from trivial. The installation of these services can act as a significant barrier to entry for NLP developers and even once developed these pipelines can be opaque and brittle. These issues are of course endemic to software development and until recently could only be solved by integrating all components within a single development model, for example, such as integrating all NLP tools using the Python language as has been done by NLTK(Bird, 2006). An alternative model has arisen in the form of Web services that provide integration between multiple components through clear and defined protocols such as REST. However, Web services have generally not been adopted by researchers or industry, in part due to the fact that the remote nature of the computation can lead to issues with the availability of services (as external services are often down) and the speed of these services (as sending requests to servers creates significant bottlenecks). In this paper, we propose a new platform called Teanga1 , which aims to achieve the best of both w"
L18-1383,P10-4004,0,0.0340285,"rchitecture, multiple frameworks, toolkits, and suites have been created, and each of them 2 ‘teanga’ [""tj aNg@] means ‘language’ in Irish 2410 https://json-ld.org/ • Use URIs to identify all types of data items, for example, if we have a dataset of papers, we would use a unique URI for each paper. uses a different approach to creating interoperability among all their services, and, by that, reduce the amount of manual work needed to process data. Among these are the LAPPS Grid (Ide et al., 2015) and its Galaxy front-end (Ide et al., 2016), GernEdiT: A Graphical Tool for GermaNet Development (Henrich and Hinrichs, 2010), Language Grid: An Infrastructure for Intercultural Collaboration (Ishida, 2006), and Unstructured Information Management Architecture (UIMA) (Ferrucci and Lally, 2004). Some problem with the applications of other platforms is that some of them only run on a desktop machine or rely on a platform-specific program, e.g. Eclipse plugins. For example, in the case of UIMA, it’s only a middleware architecture to be taken into account while developing a new NLP tool. For example, it doesn’t provide the user with an interface to process data. UIMA also is like GATE when it comes to the complexity of"
L18-1383,L16-1073,0,0.0291147,"eads to the encoding of the data. 3. Related Work In the domain of NLP architecture, multiple frameworks, toolkits, and suites have been created, and each of them 2 ‘teanga’ [""tj aNg@] means ‘language’ in Irish 2410 https://json-ld.org/ • Use URIs to identify all types of data items, for example, if we have a dataset of papers, we would use a unique URI for each paper. uses a different approach to creating interoperability among all their services, and, by that, reduce the amount of manual work needed to process data. Among these are the LAPPS Grid (Ide et al., 2015) and its Galaxy front-end (Ide et al., 2016), GernEdiT: A Graphical Tool for GermaNet Development (Henrich and Hinrichs, 2010), Language Grid: An Infrastructure for Intercultural Collaboration (Ishida, 2006), and Unstructured Information Management Architecture (UIMA) (Ferrucci and Lally, 2004). Some problem with the applications of other platforms is that some of them only run on a desktop machine or rely on a platform-specific program, e.g. Eclipse plugins. For example, in the case of UIMA, it’s only a middleware architecture to be taken into account while developing a new NLP tool. For example, it doesn’t provide the user with an int"
mccrae-etal-2012-collaborative,kemps-snijders-etal-2008-isocat,0,\N,Missing
mccrae-etal-2012-collaborative,W10-0719,0,\N,Missing
mccrae-etal-2012-collaborative,D08-1027,0,\N,Missing
mccrae-etal-2012-collaborative,W07-1501,0,\N,Missing
mccrae-etal-2012-collaborative,W09-1904,0,\N,Missing
mccrae-etal-2012-collaborative,francopoulo-etal-2006-lexical,0,\N,Missing
mccrae-etal-2012-collaborative,zesch-etal-2008-extracting,0,\N,Missing
mccrae-etal-2012-collaborative,van-assem-etal-2006-conversion,0,\N,Missing
S14-2016,D13-1179,1,0.850781,") = cos(Φ(di ), Φ(dj )) = ||Φ(di ) |Φ(dj )|| sim(di , dj ) = cos(XT di , XT dj ) = T dT i XX dj T ||X di |XT dj || The key challenge with topic modelling is choosing a good background document collection B = {b1 , ..., bN }. A simple minimal criterion for a good background document collection is that each document in this collection should be maximally similar to itself and less similar to any other document:  ∀i 6= j 1 = sim(bj , bj ) &gt; sim(bi , bj ) ≥ 0 −(AT A)−1 AT BC0 C0  A 0 B C  =I (1) The inverse C0 is approximated by the Jacobi Preconditioner, J, of CT C: As shown in McCrae et al. (2013), this property is satisfied by the following projection: ΦONETA (d) = (XT X)−1 XT d C0 &apos; JCT  ||c1 ||−2  =  0 And hence the similarity between two documents can be calculated as: sim(di , dj ) = cos(ΦONETA (di ), ΦONETA (dj )) 3 (AT A)−1 AT 0 4 Approximations .. 0 . ||cN2 ||−2  (2)  T C Normalization A key factor in the effectiveness of topic-based methods is the appropriate normalization of the elements of the document matrix X. This is even more relevant for orthonormal topics as the matrix inversion procedure can be very sensitive to small changes in the matrix. In this context, we c"
S16-1110,S15-1010,1,0.835823,"r sentence. We examine several methods that can be used to learn these alignments including word embeddings (Mikolov et al., 2013; Pennington et al., 2014) and models based on deep learning that have been suggested for machine translation (Bahdanau et al., 2014; Cho et al., 2014). In addition, we look into recent models for sentence and document similarity that can leverage the large amount of loosely aligned text in particular those based on Explicit Semantic Analysis (Gabrilovich and Markovitch, 2007) and recent extensions aimed at generating orthogonal representations (McCrae et al., 2013; Aggarwal et al., 2015). While these novel techniques alone can achieve high performance on the task, we note that simple metrics such as the number of overlapping terms can produce reasonable performance. For added robustness we combine features based on simple metrics with novel methods explored in this work as a multi-feature regression problem, which we solve by means of an M5 Decision tree (Wang and Witten, 1996; Quinlan, 1992). The rest of the paper is structured as follows: we present our system in Section 2. We then present both our internal evaluation results and the official Task 1 results in Section 3 and"
S16-1110,S14-2010,0,0.0136638,"ard and soft alignments, and the ESA similarity, we combine all of our features into a single vector and thus transform the problem into that of a traditional regression task. We experimented with various classifiers using the Weka toolkit (Hall et al., 2009) and found that in nearly all experiments, the strongest performance was obtained using the M5 Decision Tree method (Wang and Witten, 1996; Quinlan, 1992) and so we adopted this for all our experiments. 3 Evaluation 3.1 Internal Evaluation 2.4 ESA Similarity We conducted a series of evaluations using data from previous SemEval challenges (Agirre et al., 2014) as a baseline as shown in Table 1. These results present the following configurations using 10-fold cross-validation: Gabrilovich and Markovitch (2007) introduced the ESA model that represents the semantics of a word Sultan Only (-DF) Using baseline features, which are also used in all experiments, and Sultan et 0 αij = αij df (ai )df (bj ) 715 al.’s (2014a) aligner. Without accounting for term document frequency (see Section 2.3.3) Sultan Only As above only with document frequency included as a feature Sultan + Jacana Including the Jacana aligner (Section 2.2) Sultan + WordSim Including the"
S16-1110,P14-1023,0,0.0203315,"t uses Conditional Random Field (CRF) model to globally decode the best alignment. It uses features based on WordNet and part-of-speech tags. 2.3 1 https://github.com/arunjeyapal/ GreedyStringTiling 713 Soft Alignment 2.3.1 WordSim Semantic relatedness measures can be directly used to compute the soft alignments between the sentences. In this approach, we compare the pretrained neural word embeddings to compute the relatedness between words across both the sentences, thus producing the soft alignment matrix. We use cosine similarity for this purpose. We use the neural embeddings5 developed by Baroni et al. (2014). 2 Source and Target Length The length (in tokens) of each of the two strings Hard alignment https://github.com/ma-sultan/ monolingual-word-aligner 3 For example in the pair “Being against nukes does not mean not wanting to use nukes” and “Being against using nukes means not wanting to use nukes” the systems differed in the alignment of the word “use” 4 https://github.com/chetannaik/jacana 5 Best predict vectors on http://clic.cimec. unitn.it/composes/semantic-vectors.html Dataset Method Sultan Only (-DF) Sultan Only Sultan + Jacana Sultan + WordSim Sultan + WordSim + WordPairs All aligners A"
S16-1110,D14-1179,0,0.020235,"Missing"
S16-1110,S15-2046,0,0.038739,"Missing"
S16-1110,D13-1179,1,0.847973,"ther word in the other sentence. We examine several methods that can be used to learn these alignments including word embeddings (Mikolov et al., 2013; Pennington et al., 2014) and models based on deep learning that have been suggested for machine translation (Bahdanau et al., 2014; Cho et al., 2014). In addition, we look into recent models for sentence and document similarity that can leverage the large amount of loosely aligned text in particular those based on Explicit Semantic Analysis (Gabrilovich and Markovitch, 2007) and recent extensions aimed at generating orthogonal representations (McCrae et al., 2013; Aggarwal et al., 2015). While these novel techniques alone can achieve high performance on the task, we note that simple metrics such as the number of overlapping terms can produce reasonable performance. For added robustness we combine features based on simple metrics with novel methods explored in this work as a multi-feature regression problem, which we solve by means of an M5 Decision tree (Wang and Witten, 1996; Quinlan, 1992). The rest of the paper is structured as follows: we present our system in Section 2. We then present both our internal evaluation results and the official Task 1"
S16-1110,D14-1162,0,0.0793427,"es correspond to each other. This is quite successful in many cases where many words have the same lemma, however when synonymous and semantically similar terms are used, it is much harder to construct alignment. For this reason, we propose the use of soft alignments, where instead of producing a hard linking between individual words in the sentence, we instead produce a score indicating how likely one word in a sentence is to be aligned to another word in the other sentence. We examine several methods that can be used to learn these alignments including word embeddings (Mikolov et al., 2013; Pennington et al., 2014) and models based on deep learning that have been suggested for machine translation (Bahdanau et al., 2014; Cho et al., 2014). In addition, we look into recent models for sentence and document similarity that can leverage the large amount of loosely aligned text in particular those based on Explicit Semantic Analysis (Gabrilovich and Markovitch, 2007) and recent extensions aimed at generating orthogonal representations (McCrae et al., 2013; Aggarwal et al., 2015). While these novel techniques alone can achieve high performance on the task, we note that simple metrics such as the number of over"
S16-1110,Q14-1018,0,0.0688326,"computing text similarity, and also evaluate different methods to produce it. The main features used by our system are based on alignment and Explicit Semantic Analysis. Our system was above the median scores for 4 out of the 5 datasets at SemEval 2016 STS Task 1. 1 Introduction Semantic textual similarity is the task of deciding if two sentences express a similar or identical meaning and requires a deep understanding of a sentence and its meaning in order to achieve high performance. Recent successful approaches to this problem have been based on the idea of creating monolingual alignments (Sultan et al., 2014a) indicating which words in each of the two sentences correspond to each other. This is quite successful in many cases where many words have the same lemma, however when synonymous and semantically similar terms are used, it is much harder to construct alignment. For this reason, we propose the use of soft alignments, where instead of producing a hard linking between individual words in the sentence, we instead produce a score indicating how likely one word in a sentence is to be aligned to another word in the other sentence. We examine several methods that can be used to learn these alignmen"
S16-1110,S14-2039,0,0.0173864,"computing text similarity, and also evaluate different methods to produce it. The main features used by our system are based on alignment and Explicit Semantic Analysis. Our system was above the median scores for 4 out of the 5 datasets at SemEval 2016 STS Task 1. 1 Introduction Semantic textual similarity is the task of deciding if two sentences express a similar or identical meaning and requires a deep understanding of a sentence and its meaning in order to achieve high performance. Recent successful approaches to this problem have been based on the idea of creating monolingual alignments (Sultan et al., 2014a) indicating which words in each of the two sentences correspond to each other. This is quite successful in many cases where many words have the same lemma, however when synonymous and semantically similar terms are used, it is much harder to construct alignment. For this reason, we propose the use of soft alignments, where instead of producing a hard linking between individual words in the sentence, we instead produce a score indicating how likely one word in a sentence is to be aligned to another word in the other sentence. We examine several methods that can be used to learn these alignmen"
S16-1110,P13-2123,0,0.0376981,"Missing"
W11-1013,E09-1010,0,0.0122886,"uch as bilingual lexica. Instead, in this paper we look at how we may gain an adequate translation using statistical machine translation approaches that also utilise the semantic information beyond the label or term describing the concept, that is relations among the concepts in the ontology, as well as the attributes or properties that describe concepts, as will be explained in more detail in section 2. Current work in machine translation has shown that word sense disambiguation can play an important role by using the surrounding words as context to disambiguate terms (Carpuat and Wu, 2007) (Apidianaki, 2009). Such techniques have 116 Proceedings of SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 116–125, c ACL HLT 2011, Portland, Oregon, USA, June 2011. 2011 Association for Computational Linguistics been extrapolated to the translation of taxonomies and ontologies, in which the “context” of a taxonomy or ontology label corresponds to the ontology structure that surrounds the label in question. This structure, which is made up of the lexical information provided by labels and the semantic information provided by the ontology structure, defines the sense"
W11-1013,D07-1007,0,0.0334307,"her similar resources such as bilingual lexica. Instead, in this paper we look at how we may gain an adequate translation using statistical machine translation approaches that also utilise the semantic information beyond the label or term describing the concept, that is relations among the concepts in the ontology, as well as the attributes or properties that describe concepts, as will be explained in more detail in section 2. Current work in machine translation has shown that word sense disambiguation can play an important role by using the surrounding words as context to disambiguate terms (Carpuat and Wu, 2007) (Apidianaki, 2009). Such techniques have 116 Proceedings of SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 116–125, c ACL HLT 2011, Portland, Oregon, USA, June 2011. 2011 Association for Computational Linguistics been extrapolated to the translation of taxonomies and ontologies, in which the “context” of a taxonomy or ontology label corresponds to the ontology structure that surrounds the label in question. This structure, which is made up of the lexical information provided by labels and the semantic information provided by the ontology structure,"
W11-1013,P07-2045,0,0.00517699,"than WER and, as expected, performs better. 5 Approaches for taxonomy and ontology translation 5.1 Domain adaptation It is generally the case that many ontologies and taxonomies focus on only a very specific domain, thus it seems likely that adaptation of translation systems by use of an in-domain corpus may improve translation quality. This is particularly valid in the case of ontologies which frequently contain “subject” annotations6 for not only the whole data structure but often individual elements. To demonstrate this we tried to translate the IFRS 2009 taxonomy using the Moses Decoder (Koehn et al., 2007), which we trained on the EuroParl corpus (Koehn, 2005), translating from Spanish to English. As the IFRS taxonomy is on the topic of finance and accounting, we 6 For example from the Dublin Core vocabulary: see http: //dublincore.org/ WER∗ METEOR NIST BLEU Baseline 0.135 0.324 1.229 0.090 With domain adaptation 0.138 0.335 1.278 0.116 English Dutch German Spanish P (syn|s) 0.147 0.137 0.125 0.126 P (syn|p) 0.012 0.011 0.007 0.012 P (syn|n) 0.001 0.001 0.001 0.001 Table 4: Results of domain-adapted translation. ∗ Lower WER scores are better Table 5: Probability of syntactic relationship given"
W11-1013,2005.mtsummit-papers.11,0,0.0231548,"taxonomy and ontology translation 5.1 Domain adaptation It is generally the case that many ontologies and taxonomies focus on only a very specific domain, thus it seems likely that adaptation of translation systems by use of an in-domain corpus may improve translation quality. This is particularly valid in the case of ontologies which frequently contain “subject” annotations6 for not only the whole data structure but often individual elements. To demonstrate this we tried to translate the IFRS 2009 taxonomy using the Moses Decoder (Koehn et al., 2007), which we trained on the EuroParl corpus (Koehn, 2005), translating from Spanish to English. As the IFRS taxonomy is on the topic of finance and accounting, we 6 For example from the Dublin Core vocabulary: see http: //dublincore.org/ WER∗ METEOR NIST BLEU Baseline 0.135 0.324 1.229 0.090 With domain adaptation 0.138 0.335 1.278 0.116 English Dutch German Spanish P (syn|s) 0.147 0.137 0.125 0.126 P (syn|p) 0.012 0.011 0.007 0.012 P (syn|n) 0.001 0.001 0.001 0.001 Table 4: Results of domain-adapted translation. ∗ Lower WER scores are better Table 5: Probability of syntactic relationship given a semantic relationship in IFRS labels chose all terms"
W11-1013,J10-4005,0,0.0200211,".108 0.036 0.134 0.122 0.209 0.214 0.303 0.169 0.183 0.062 0.266 0.164 0.177 0.111 0.251 0.194 0.151 0.067 0.210 0.204 0.143 0.129 0.221 0.120 Table 3: Correlation between manual evaluation results and automatic evaluation scores label length of 2.45 tokens and the translations generated had an average length of 2.16 tokens. We then created a data set by mixing the translations from the web translation services with a number of translations from the source ontologies, to act as a control. We then gave these translations to 3 evaluators, who scored them for adequacy and fluency as described in Koehn (2010). Finally, we calculated the Pearson correlation coefficient between the automatic scores and the manual scores obtained. These are presented in table 3 and figure 1. As we can see from these results, one metric, namely METEOR, seems to perform best in evaluating the quality of the translations. In fact this is not surprising as there is a clear mathematical deficiency that both NIST and BLEU have for evaluating translations for very short labels like the ones we have here. To illustrate this, we recall the formulation of BLEU as given in (Papineni et al., 2002): B LEU = BP · exp( N ∑ wn log p"
W11-1013,P10-1023,0,0.049565,"Missing"
W11-1013,P02-1040,0,0.0801228,"eriving a corpus from Wikipedia, for example it is possible to provide some hierarchical links by the use of the category that a page belongs to, such as has been performed by the DBpedia project (Auer et al., 2007). 4 Evaluation metrics for taxonomy and ontology translation Given the linguistic differences in taxonomy and ontology labels, it seems necessary to investigate the effectiveness of various metrics for the evaluation of translation quality. There are a number of metrics that are widely used for evaluating translation. Here we will focus on some of the most widely used, namely BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005) and WER (McCowan et al., 2004). However, it is not clear which of these methods correlate best with human evaluation, particularly for the ontologies with short labels. To evaluate this we collected a mixture of ontologies with short labels on the topics of human diseases, agriculture, geometry and project management, producing 437 labels. These were translated with web translation services from English to Spanish, in particular Google Translate3 , Yahoo! BabelFish4 and SDL FreeTranslation5 . Having obtained translations for each lab"
W11-1013,J99-4008,0,0.230196,"ilar) ontology structures to be compared. Figure 2: Two approaches to translate ontology labels. From a technical point of view, we consider the translation task as a word sense disambiguation task. We identify two methods for comparing ontology structures, which are illustrated in Figure 2. The first method relies on a multilingual resource, i.e., a multilingual ontology or taxonomy. The ontology represented on the left-hand side of the figure consists of several monolingual conceptualizations related to each other by means of an interlingual index, as is the case in the EuroWordNet lexicon (Vossen, 1999). For example, if the original label is chair for seat in English, several translations for it are obtained in Spanish such as: silla (for seat), c´atedra (for university position), presidente (for person leading a meeting). Each of these correspond to a sense in the English WordNet, and hence each translation selects a hierachical structure with English labels. The next step is to compare the input structure of the original ontology containing chair against the three different structures in English representing the several senses of chair and obtain the corresponding label in Spanish. The sec"
W11-1013,C08-1125,0,0.204106,"Missing"
W11-1013,W05-0909,0,\N,Missing
W13-5203,E12-1059,0,0.0126561,"rase table of the Moses system (Koehn et al., 2007) trained on Europarl data (Koehn, 2005). We used the system primarily in an ‘off-the-shelf’ manner in order to focus on the effect of adding the linked data translations. Moses uses a log-linear model as the baseline for its translations, where translations are generated by a decoder and evaluated according to the following model: Translations mined from linguistic linked data For finding translations from linguistic linked data, we focus on the lemonUby (Eckle-Kohler et al., 2013) resource, which is a linked data version of the UBY resource (Gurevych et al., 2012). This resource contains lemon versions of a number of resources in particular: p(t|f ) = exp( X φi (t, f )) i Where t is the candidate translation sentence, f is the input foreign text and φi are scoring functions. In the phrase-based model the translation is derived compositionally by considering phrases and their translations stored in the so called phrase table of the Moses system. The main challenge in integrating these translations derived from linked data lies in the fact that they lack a probability score. For each translation pair (a, b) derived from the linked data, we distinguish tw"
W13-5203,J10-4005,0,0.0249879,"s been a massive explosion in the amount and quality of data available as linked data on the web. This data frequently describes entities in multiple languages and as such can be used as a source of translations. In particular, the web contains much data about named entities, such as locations, films, people and so forth, and often these named entities have translations in many languages. In this paper, we address the question of what can be achieved by using the large amounts of data available as multilingual Linked Open Data (LOD). As state-of-theart statistical machine translation systems (Koehn, 2010) are typically trained on outdated or out-ofdomain parallel corpora such as on the transcripts of the European Parliament (Koehn, 2005), we expect to increase the coverage of domain-specific terminology. In addition, there has recently been a move towards the publication of of language resources using linked data principles (Chiarcos et al., 2011), which can be expected to lead to a significant increase the availability of information relevant to NLP on the Web. In particular, the representation of legacy resources such as Wiktionary and 2 Mining translations from the linked open data cloud Ob"
W13-5203,P98-1013,0,0.0313697,"e, f is the input foreign text and φi are scoring functions. In the phrase-based model the translation is derived compositionally by considering phrases and their translations stored in the so called phrase table of the Moses system. The main challenge in integrating these translations derived from linked data lies in the fact that they lack a probability score. For each translation pair (a, b) derived from the linked data, we distinguish two cases: If the translation is already in the phrase table, we add a new feature that is set to 1.0 to indicate that the translation was found • FrameNet (Baker et al., 1998) • OmegaWiki 3 • VerbNet (Schuler, 2005) • Wiktionary 4 • WordNet (Fellbaum, 2010) 1 We use the dump of the 3.5 version The dump was downloaded on June 21st 2013 3 http://omegawiki.org 4 http://www.wiktionary.org 2 9 Resource BTC DBpedia FreeBase All English Labels 398,902,866 7,332,616 41,261,806 447,497,288 German Labels 144,226 590,381 1,654,254 2,338,861 Translations 51,756 540,134 259,923 665,910 Table 1: The number of labels and translations found in the linked data cloud by resource &lt;OW_eng_LexicalEntry_0#CanonicalForm&gt; lemon:writtenRep &quot;rain&quot;@eng. &lt;OW_eng_LexicalEntry_0&gt; lemon:canonica"
W13-5203,P03-1021,0,0.00589599,"fluent in English, and had a Cohen’s Kappa Agreement of 0.56. In addition, we calculated BLEU (Papineni et al., 2002) scores. The results are presented in Table 3. Table 3: The comparative evaluation of the translations with and without linked data from the Linked Data Cloud. If the translation was not in the phrase table, we add a new entry with probability 1.0 for all scores and the feature for linked data set to 1.0. For all other translations, the feature indicating provenance from the Linked Data Cloud is set to 0.0. The weights for the loglinear model are learned using the MERT system (Och, 2003). As such we do not use the linked data itself to choose between different translation candidates but rely on the methods built into the machine translations system, in particular the language model. 5 Results We extracted the baseline phrase table, reordering and language model from version 7 of the EuroParl corpus translating from English to German. In order to evaluate the impact of Linked Data translations on translation quality, we rely on the News Commentary 2011 corpus provided as part of the WMT-12 translation task (Callison-Burch et al., 2012). We found that 25,688 translations from t"
W13-5203,P02-1040,0,0.0880865,"translation. For each translation, we performed a manual evaluation with two evaluators. They were both presented with 50 translations, one with linked data and one without linked data and asked to choose the best one (“no opinion” was also allowed). The translations were presented in a random order and there was no indication which system they came from so this experiment was performed blind. The evaluators were a native English speaker, who is fluent in German, and a native German speaker, who is fluent in English, and had a Cohen’s Kappa Agreement of 0.56. In addition, we calculated BLEU (Papineni et al., 2002) scores. The results are presented in Table 3. Table 3: The comparative evaluation of the translations with and without linked data from the Linked Data Cloud. If the translation was not in the phrase table, we add a new entry with probability 1.0 for all scores and the feature for linked data set to 1.0. For all other translations, the feature indicating provenance from the Linked Data Cloud is set to 0.0. The weights for the loglinear model are learned using the MERT system (Och, 2003). As such we do not use the linked data itself to choose between different translation candidates but rely o"
W13-5203,W12-3102,0,0.0675323,"Missing"
W13-5203,C98-1013,0,\N,Missing
W13-5203,P07-2045,0,\N,Missing
W13-5203,2005.mtsummit-papers.11,0,\N,Missing
W13-5501,I08-1051,0,0.0753902,"esent typologically relevant phenomena, along with examples for their illustration and annotations (glosses) and translations applied to these examples (structurally comparable to corpus data), or word lists (structurally comparable to lexical-semantic resources). RDF as a generic representation formalism is thus particularly appealing for this class of resources. Finally, for linguistic corpora (Fig. 1, corpora), the potential of the Linked Data paradigm for modeling, processing and querying of corpora is immense, and RDF conversions of semantically annotated corpora have been proposed early [3]. RDF provides a graph-based data model as required for the interoperable representation of arbitrary kinds of annotation [2, 15], and this flexibility makes it a promising candidate for a general means of representation for corpora with complex and heterogeneous annotations. RDF does not only establish interoperability between annotations within a corpus, but also between corpora and other linguistic resources [4]. In comparison to other types of linguistic resources, corpora are currently underrepresented in the LLOD cloud, but the development of schemes for corpora and/or NLP annotations re"
W13-5501,chiarcos-2012-ontologies,1,0.897695,"to establish conceptual interoperability between language resources. If resourcespecific annotations or abbreviations are expanded into references to repositories of linguistic terminology and/or metadata categories, linguistic annotations, grammatical features and metadata specifications become more easily comparable. Important repositories developed by different communities include GOLD [9] and ISOcat [20, 19], yet, only recently these terminology repositories were put in relation with each other using Linked Data principles and with linguistic resources, e.g., within the OLiA architecture [5]. Linguistic databases are a particularly heterogeneous group of linguistic resources; they contain complex and manifold types of information, e.g., feature structures that represent typologically relevant phenomena, along with examples for their illustration and annotations (glosses) and translations applied to these examples (structurally comparable to corpus data), or word lists (structurally comparable to lexical-semantic resources). RDF as a generic representation formalism is thus particularly appealing for this class of resources. Finally, for linguistic corpora (Fig. 1, corpora), the p"
W13-5501,W07-1501,0,0.0927928,"s refer to ISOcat URIs. Ecosystem RDF as a data exchange framework is maintained by an interdisciplinary, large and active community, and it comes with a developed infrastructure that provides APIs, database implementations, technical support and validators for various RDF-based languages, e.g., reasoners for OWL. For developers of linguistic resources, this ecosystem can provide technological support or off-the-shelf implementations for common problems, e.g., the development of a database that is capable of supporting flexible, graph-based data structures as necessary for multi-layer corpora [15]. Beyond this, another advantage warrants a mention: The distributed approach of the Linked Data paradigm facilitates the distributed development of a web of resources and collaboration between researchers that provide and use this data and that employ a shared set of technologies. One consequence is the emergence of interdisciplinary efforts to create large and interconnected sets of resources in linguistics and beyond. LDL-2013 aims to provide a forum to discuss and to facilitate such on-going developments. LLOD: Building the Cloud Recent years have seen not only a number of approaches to pr"
W13-5501,wright-2004-global,0,0.0894745,"able URIs, it is possible to combine information from physically separated repositories in a single query at runtime. Information from different resources in the cloud can then be integrated freely. Dynamic Import If cross-references between linguistic resources are represented by resolvable URIs instead of system-defined ID references or static copies of parts from another resource, it is not only possible to resolve them at runtime, but also to have access to the most recent version of a resource. For community-maintained terminology repositories like the ISO TC37/SC4 Data Category Registry [20, 19, ISOcat], for example, new categories, definitions or examples can be introduced occasionally, and this information is available immediately to anyone whose resources refer to ISOcat URIs. Ecosystem RDF as a data exchange framework is maintained by an interdisciplinary, large and active community, and it comes with a developed infrastructure that provides APIs, database implementations, technical support and validators for various RDF-based languages, e.g., reasoners for OWL. For developers of linguistic resources, this ecosystem can provide technological support or off-the-shelf implementations for c"
W13-5507,ide-etal-2000-xces,0,0.0754649,"11,812 constituents (including terminal nodes) from the orthographically corrected chat messages (resulting in a total average of 17.75 constituent nodes per chat message). 3 From internal representations to RDF 3.1 Internal representation We developed FiESTA (an acronym for “format for extensive spatiotemporal annotations”), which takes into account various approaches, among them, the annotation graph approach (Bird and Liberman, 2001), the NITE object model (Evert et al., 2003), the speech transcription facilities of the TEI P5 specification (TEI Consortium, 2008), and the (X)CES standard (Ide et al., 2000). There were shortcomings in all these approaches that made it very difficult to express complex multimodal data structures. These shortcomings can also be found in theories and models that are more established in the Linked Data community, such as POWLA (Chiarcos, 2012) or LAF (Ide et al., 2003). One of the most pressing problems is the restriction to a single, flat stream or sequence of primary data (called “text” in some approaches), or a single, flat timeline. In several data collections we need to support multiple timelines, especially in cases where multiple novel recording and tracking"
W13-5507,W03-0804,0,0.0333824,"e spatiotemporal annotations”), which takes into account various approaches, among them, the annotation graph approach (Bird and Liberman, 2001), the NITE object model (Evert et al., 2003), the speech transcription facilities of the TEI P5 specification (TEI Consortium, 2008), and the (X)CES standard (Ide et al., 2000). There were shortcomings in all these approaches that made it very difficult to express complex multimodal data structures. These shortcomings can also be found in theories and models that are more established in the Linked Data community, such as POWLA (Chiarcos, 2012) or LAF (Ide et al., 2003). One of the most pressing problems is the restriction to a single, flat stream or sequence of primary data (called “text” in some approaches), or a single, flat timeline. In several data collections we need to support multiple timelines, especially in cases where multiple novel recording and tracking devices are used whose temporal synchronisation is nontrivial (because of irregular tracking intervals, computational delay, etc.). However, when working in a project with a limited duration, researchers are under time pressure, as a consequence, it can become necessary to perform analyses of dat"
W13-5507,P03-1054,0,0.00355911,"mat internal RDF data structure Turtle custom RDF RDF Views RDF Views Views RDF/XML FiESTA / MExiCo library POSEIdON library Rails view generation Figure 3: Architecture of the corpus management web application, grouped into scopes of responsibility of the respective libraries (FiESTA and POSEIdON). 1. A transformation of the written messages into orthographically and syntactically correct utterances. This was necessary for the parser (see below) to perform with an adequate accuracy. 2. Utterances were segmented into sentences and then parsed with the Stanford Parser (Klein and Manning, 2002; Klein and Manning, 2003), using the German version trained on the Negra corpus (Rafferty and Manning, 2008). 3. Syntactic and semantic properties of sentences were annotated, among them elaborateness (e.g., fragments and full sentences), speech acts (e.g., greetings, instructions, corrections, feedback) and localisation strategies – for instance, whether positions were described in relation to present objects (“to the right of the circle”), by describing absolute locations of the board itself (“into the bottomleft corner”), or by using metaphors (such as points of the compass, floors of buildings for rows: “south of"
W13-5507,W08-1006,0,0.0158476,"XML FiESTA / MExiCo library POSEIdON library Rails view generation Figure 3: Architecture of the corpus management web application, grouped into scopes of responsibility of the respective libraries (FiESTA and POSEIdON). 1. A transformation of the written messages into orthographically and syntactically correct utterances. This was necessary for the parser (see below) to perform with an adequate accuracy. 2. Utterances were segmented into sentences and then parsed with the Stanford Parser (Klein and Manning, 2002; Klein and Manning, 2003), using the German version trained on the Negra corpus (Rafferty and Manning, 2008). 3. Syntactic and semantic properties of sentences were annotated, among them elaborateness (e.g., fragments and full sentences), speech acts (e.g., greetings, instructions, corrections, feedback) and localisation strategies – for instance, whether positions were described in relation to present objects (“to the right of the circle”), by describing absolute locations of the board itself (“into the bottomleft corner”), or by using metaphors (such as points of the compass, floors of buildings for rows: “south of the circle”). 4. The parse trees were further annotated with basic tree measures (d"
W14-4718,Q13-1023,0,0.0222605,"Missing"
W14-4724,W06-1805,0,0.510785,"Missing"
W14-4724,W13-2102,1,0.879649,"Missing"
W14-4724,P88-1004,0,0.702557,"Missing"
W14-4724,peters-peters-2000-treatment,0,0.0389614,"round, heavy, wooden, inlaid magnifying glass • ‘round’ represents the Formal role (giving indications of shape and dimensionality) • ‘heavy’ and ‘wooden’ related to the Constitutive role and indicate the relation between the object and its parts (e. g. by specifying weight, material, parts and components) • ‘inlaid’ is the Agentive role of the lexical item, denoting the factors that have been involved in the generation of the objects, such as creator, artifact, natural kind, and causal chain • ‘magnifying’ describes the Telic role of ‘glass’, since it shows its purpose and function Finally, Peters and Peters (2000) provide one of the few other practical reports on modelling adjectives with ontologies, in the context of the SIMPLE lexica. This work is primarily focussed on the categorization of by means of intensional and extensional properties, rather than due to their logical modelling. 6 Conclusion In this paper we have proposed an approach to model the semantics of adjectives in the context of the lexicon-ontology interface with a focus on the ontology-lexicon model lemon. We have argued that the semantics of adjectives, in particular gradable and privative adjectives, is beyond what can be expressed"
W14-4724,J91-4003,0,0.422441,"e gradable, i.e. whether a comparative or superlative statement with these adjectives makes sense. For example, adjectives such as ‘big’ or ‘tall’ can express relationships such as ‘X is bigger than Y ’. However it is not possible to say that one individual is ‘more former’. Most gradable adjectives are subsective (e. g.‘a big mouse’ is not ‘a big animal’ (Morzycki, 2013a)). Finally, we consider operator or property-modifying adjectives. They can be understood along the lines of privative adjectives but differ in that they represent operators that modify some property in the qualia structure (Pustejovsky, 1991) of the class. For instance, we may express the adjective ‘former’ in lambda calculus as a function that takes a class C as input and returns the class of entities that were a member of C to some prior time point t (Partee, 2003): λC[λx∃tC(x, t) ∩ t < now] Such adjectives have not only a difference in semantic meaning but can also frequently have syntactic impact, for example in adjective ordering restrictions, as they may be reordered with only semantic impact (Teodorescu, 2006), e.g., 4. (a) A big red car. (b) ? A red big car. 5. (a) A famous former actor. (b) A former famous actor. Finally,"
W15-4205,calzolari-etal-2012-lre,0,0.098711,"ar}@insight-centre.org Abstract statistical taggers, statistical parsers, and statistical machine translation systems) or they require lexico-semantic resources as background knowledge to perform some task (e.g. word sense disambiguation). As the number of language resources available keeps growing, the task of discovering and finding resources that are pertinent to a particular task becomes increasingly difficult. While there are a number of repositories that collect and index metadata of language resources, such as META-SHARE (Federmann et al., 2012), CLARIN (Broeder et al., 2010), LRE-Map (Calzolari et al., 2012), Datahub.io1 and OLAC (Simons and Bird, 2003), they do not provide a complete solution to the discovery problem for two reasons. First, integrated search over all these different repositories is not possible, as they use different data models, different vocabularies and expose different interfaces and APIs. Second, these repositories must strike a balance between quality and coverage, either opting for coverage at the expense of quality of metadata, or vice versa. When collecting metadata from multiple resources, we understand that there are two principal challenges: property harmonization an"
W15-4205,choukri-etal-2012-using,0,0.0195543,"x XML schema is provided to describe metadata of resources (Gavrilidou et al., 2012). At the same time, considerable effort has been devoted to ensuring data quality (Piperidis, 2012). In contrast, CLARIN does not provide a single schema, but a set of ‘profiles’ that are described in a schema language called the CMDI Component Specification Language (Broeder et al., 2012). Each institute describing resources using CMDI can instantiate the vocabulary to suit their particular needs. Similarly, an attempt has been made to catalogue language resources by assigning them a single unique identifier (Choukri et al., 2012). Other more decentralized approaches are found in initiatives such as the LRE-Map (Calzolari et al., 2012) which provides a repository for researchers who want to submit the resources accompanying papers submitted to conferences. Most fields in LRE-Map consist of a text field with some prespecified options to select and a thorough analysis of the results has been conducted (Mariani et al., 2014). Similarly, the Open Linguistics Working Group (Chiarcos et al., 2012) has been collecting language resources published as linked data in a Related Work Interoperability of metadata is an important pr"
W15-4205,de-marneffe-etal-2006-generating,0,0.0158775,"Missing"
W15-4205,federmann-etal-2012-meta,0,0.277944,"Missing"
W15-4205,broeder-etal-2010-data,0,0.0785251,"Missing"
W15-4205,mariani-etal-2014-facing,0,0.0288608,"Missing"
W15-4205,piperidis-2012-meta,0,0.0544122,"oaches have been pursued to collect metadata of resources. Large consortium-led projects and initiatives such as the CLARIN projects and METANET have attempted to create metadata standards for representing linguistic data. Interoperability of the data stemming from these two repositories is however severely limited due to incompatibilities in their data models. META-SHARE favors a qualitative approach in which a relatively complex XML schema is provided to describe metadata of resources (Gavrilidou et al., 2012). At the same time, considerable effort has been devoted to ensuring data quality (Piperidis, 2012). In contrast, CLARIN does not provide a single schema, but a set of ‘profiles’ that are described in a schema language called the CMDI Component Specification Language (Broeder et al., 2012). Each institute describing resources using CMDI can instantiate the vocabulary to suit their particular needs. Similarly, an attempt has been made to catalogue language resources by assigning them a single unique identifier (Choukri et al., 2012). Other more decentralized approaches are found in initiatives such as the LRE-Map (Calzolari et al., 2012) which provides a repository for researchers who want t"
W15-4205,Q14-1019,1,\N,Missing
W15-4205,gavrilidou-etal-2012-meta,0,\N,Missing
W15-4207,ehrmann-etal-2014-representing,1,0.796049,"degree of linking of the Linguistic Linked Data cloud. In this paper we describe the results of a small project attempting to link four datasets of different types (two terminologies, one lexico-conceptual resource and one corpus). As terminological resources, we have considered the Glossary of the European Migration Network (EMN)1 as well as the Interactive Terminology for Europe (IATE) 2 . They are both represented using the lemon model (McCrae et al., 2012). As lexico-conceptual resource we rely on BabelNet (Navigli and Ponzetto, 2012), which has been previously migrated into Linked Data (Ehrmann et al., 2014). As corpus we use the Manually Annotated Subcorpus (MASC) of the American National Corpus (Ide et al., 2008), which contains disambiguated links to BabelNet. We describe how the datasets have been migrated to RDF and describe our methodology for linking the datasets at the lexical entry level and present a sampled evaluation of the quality of the induced links. We first use a simple technique based on strict matching of the canonical form of lexical entries in different resources. By this we then link the EMN to both IATE and BabelNet. MASC has been previously linked to BabelNet and we includ"
W15-4207,W07-1501,0,0.0905595,"Missing"
W15-4207,ide-etal-2008-masc,0,0.0185752,"attempting to link four datasets of different types (two terminologies, one lexico-conceptual resource and one corpus). As terminological resources, we have considered the Glossary of the European Migration Network (EMN)1 as well as the Interactive Terminology for Europe (IATE) 2 . They are both represented using the lemon model (McCrae et al., 2012). As lexico-conceptual resource we rely on BabelNet (Navigli and Ponzetto, 2012), which has been previously migrated into Linked Data (Ehrmann et al., 2014). As corpus we use the Manually Annotated Subcorpus (MASC) of the American National Corpus (Ide et al., 2008), which contains disambiguated links to BabelNet. We describe how the datasets have been migrated to RDF and describe our methodology for linking the datasets at the lexical entry level and present a sampled evaluation of the quality of the induced links. We first use a simple technique based on strict matching of the canonical form of lexical entries in different resources. By this we then link the EMN to both IATE and BabelNet. MASC has been previously linked to BabelNet and we included these links into our version of MASC. The paper is structured as follows: in the next Section 2 we briefly"
W15-4207,Q14-1019,0,0.0727669,"Missing"
W18-0910,D17-1316,0,0.023835,"Missing"
W18-0910,P16-1018,0,0.0363228,"Missing"
W18-0910,E06-1042,0,0.0359598,"enses associated with the source concept. The system exploits a small set of metaphoric expressions as a seed to detect metaphors in a semi-supervised manner. In a follow-up work, Shutova and Sun (2013) investigated the use of hierarchical graph factorization clustering to derive a network of concepts in order to learn metaphorical associations in an unsupervised way which then was used as features to identify metaphors. We consider the work introduced by Shutova et al. (2010) as a baseline for our proposed approach, thus we are going to explain its reimplementation details in subsection 3.3. Birke and Sarkar (2006) introduced TroFi, which is considered the first statistical system to identify the metaphorical senses of verbs in a semi-supervised way. The authors adapted a statistical similarity-based word sense disambiguation approach to cluster literal and non-literal senses. A predefined set of seed sentences is utilised to compute the similarity between a given sentence and the seed sentences. 3 3.1 Hypothesis Our hypothesis in this work is that a given candidate should have common characteristics and semantic features with some positive examples of metaphors. However, simply calculating the similari"
W18-0910,W13-0907,0,0.021453,"weighted cosine similarity function is used to automatically select the important vector dimensions for the metaphor detection task. The authors experimented with different pretrained word representations, namely skip-gram model and an attribute-based model. Two different datasets, which were referred to as the TSV dataset 2 82 the WordNet lexicographer name of the words first sense the state-of-the-art semi-supervised system used as our baseline system. baum, 1998) have been employed to develop supervised systems to detect metaphors (K¨oper and Schulte im Walde, 2017; Tsvetkov et al., 2013; Hovy et al., 2013; Turney et al., 2011). Shutova et al. (2010) was among the earliest approaches to computational modelling of metaphor, avoiding task-specific hand-crafted knowledge and huge annotated resources. They introduced a semi-supervised approach to identify verb-noun metaphors using corpus-driven distributional clustering. Their strategy is based on clustering abstract nouns based on their contextual features in order to capture the metaphorical senses associated with the source concept. The system exploits a small set of metaphoric expressions as a seed to detect metaphors in a semi-supervised manne"
W18-0910,P06-4020,0,0.0117563,"The cosine similarity between the candidates “break promise” and “break glass” and the top 10 metaphoric seeds in the seed set using a pre-trained Word2Vec word embedding model on Google News dataset. 2009) and consists of 62 metaphoric verb-noun pairs (more details are given in section 4). Spectral clustering (Meila and Shi, 2001) is used to cluster the abstract concepts (nouns) and the concrete concepts (verbs) then an association (mapping) is drawn between the two clusters using the seed set. The candidate extraction component employs the Robust Accurate Statistical Parsing (RASP) parser (Briscoe et al., 2006) to extract verb-subject and verb-direct object grammar relations. After that, the linked clusters (through the seed set) is used to identify potential metaphoric candidates. The filtering component is finally used to filter out these candidates based on a selectional preferences strength (SPS) measure (Resnik, 1993). The verbs exhibiting weak selectional preferences are considered to have lower metaphorical potential. An SPS threshold was set experimentally to be 1.32, thus, the candidates which verbs have an SPS value below this threshold are discarded. Dts = d(vt , vs ) ∀vs ∈ S This gives a"
W18-0910,W17-1903,0,0.025657,"Missing"
W18-0910,E17-2084,0,0.055548,"l., 2016). In this work, a word or an expression is a metaphor if it has at least one basic/literal sense (more concrete, physical) and a secondary metaphoric sense (abstract, 1 These examples could be found in the United Nations Parallel Corpus (Ziemski et al., 2016). 81 Proceedings of the Workshop on Figurative Language Processing, pages 81–90 c New Orleans, Louisiana, June 6, 2018. 2018 Association for Computational Linguistics (Tsvetkov et al., 2013) and the MOH dataset (Mohammad et al., 2016), were used to train the system and optimise its parameters as well as to assess its performance. Bulat et al. (2017) is a recent approach that investigated whether property-based semantic word representation can provide better concept generalisation for detecting metaphors than dense linguistic representation. The authors proposed property-based vectors through cross-modal mapping between dense linguistic representations and a property-norm semantic space. The authors built a count-based distributional vector and employed a skip-gram model trained on Wikipedia articles as their dense linguistic representations. The property-norm semantic space is obtained from the property-norm dataset (McRae et al., 2005)."
W18-0910,P14-5010,0,0.00246872,"n the clusters or not. And if the candidate’s noun appeared in a noun cluster but this cluster was not mapped to the cluster where the verb occurs the candidate will be discarded. 4 System Architecture As described in Figure 1 below, our system consists of three main components: a parser, a seed set of metaphoric expressions and a pre-trained word embedding model. Parser: Since our aim is to identify metaphors on the phrase-level, the Stanford parser is used to extract the grammar relations in a given sentence. We used the recurrent neural network (RNN) parser in the Stanford CoreNLP toolkit (Manning et al., 2014) to extract dependencies focusing on verb-subject and verb-direct object grammar relations. Seed Set: We used the seed set of Shutova et al. (2010) to act as our set of existing known metaphoric expressions (positive examples). The seed set consists of 62 verb-subject and verb-direct object phrases where the verb is used metaphorically3 . These seeds are extracted originally from a subset of the BNC corpus which contains 761 sentences. These sentences were annotated for grammatical relations to extract the specified grammar relations which are then filtered and manually annotated for metaphori"
W18-0910,N13-1118,0,0.0178746,"Shutova et al. (2010) was among the earliest approaches to computational modelling of metaphor, avoiding task-specific hand-crafted knowledge and huge annotated resources. They introduced a semi-supervised approach to identify verb-noun metaphors using corpus-driven distributional clustering. Their strategy is based on clustering abstract nouns based on their contextual features in order to capture the metaphorical senses associated with the source concept. The system exploits a small set of metaphoric expressions as a seed to detect metaphors in a semi-supervised manner. In a follow-up work, Shutova and Sun (2013) investigated the use of hierarchical graph factorization clustering to derive a network of concepts in order to learn metaphorical associations in an unsupervised way which then was used as features to identify metaphors. We consider the work introduced by Shutova et al. (2010) as a baseline for our proposed approach, thus we are going to explain its reimplementation details in subsection 3.3. Birke and Sarkar (2006) introduced TroFi, which is considered the first statistical system to identify the metaphorical senses of verbs in a semi-supervised way. The authors adapted a statistical simila"
W18-0910,C10-1113,0,0.304198,"sed to automatically select the important vector dimensions for the metaphor detection task. The authors experimented with different pretrained word representations, namely skip-gram model and an attribute-based model. Two different datasets, which were referred to as the TSV dataset 2 82 the WordNet lexicographer name of the words first sense the state-of-the-art semi-supervised system used as our baseline system. baum, 1998) have been employed to develop supervised systems to detect metaphors (K¨oper and Schulte im Walde, 2017; Tsvetkov et al., 2013; Hovy et al., 2013; Turney et al., 2011). Shutova et al. (2010) was among the earliest approaches to computational modelling of metaphor, avoiding task-specific hand-crafted knowledge and huge annotated resources. They introduced a semi-supervised approach to identify verb-noun metaphors using corpus-driven distributional clustering. Their strategy is based on clustering abstract nouns based on their contextual features in order to capture the metaphorical senses associated with the source concept. The system exploits a small set of metaphoric expressions as a seed to detect metaphors in a semi-supervised manner. In a follow-up work, Shutova and Sun (2013"
W18-0910,S16-2003,0,0.0591525,"Missing"
W18-0910,W13-0906,0,0.115478,"taphoric expressions such as “...eradicate poverty”, “...root out the causes of poverty”, or “...the roots of poverty are...”1 (Lakoff and Johnson, 1980; Veale et al., 2016). In this work, a word or an expression is a metaphor if it has at least one basic/literal sense (more concrete, physical) and a secondary metaphoric sense (abstract, 1 These examples could be found in the United Nations Parallel Corpus (Ziemski et al., 2016). 81 Proceedings of the Workshop on Figurative Language Processing, pages 81–90 c New Orleans, Louisiana, June 6, 2018. 2018 Association for Computational Linguistics (Tsvetkov et al., 2013) and the MOH dataset (Mohammad et al., 2016), were used to train the system and optimise its parameters as well as to assess its performance. Bulat et al. (2017) is a recent approach that investigated whether property-based semantic word representation can provide better concept generalisation for detecting metaphors than dense linguistic representation. The authors proposed property-based vectors through cross-modal mapping between dense linguistic representations and a property-norm semantic space. The authors built a count-based distributional vector and employed a skip-gram model trained o"
W18-0910,D14-1162,0,0.0825187,"7371 question 0.8462 question 0.9424 promise Cand. N glass Table 2: The cosine distance between the verbs and nouns of the candidates “break promise” and “break glass” verses the verbs and the nouns of the top 10 metaphoric seeds in the seed set using a pre-trained Word2Vec word embedding model on Google News dataset. metaphors in the seed set are “mend marriage, break agreement, cast doubt, and stir excitement”. Word Embedding Model: This work utilises distributional vector representation of word meaning to calculate semantic similarity between a candidate and a seed set. Word2Vec and GloVe (Pennington et al., 2014) are two widely used word embeddings algorithms to construct embeddings vectors based on the distributional hypothesis (Firth, 1957) but using different machine learning techniques. In this work, we investigated the effect of using different pre-trained models and similarity measures as shown in detail in the next section. This is one of the limitations of this system; a candidate is either in the clusters or not. And if the candidate’s noun appeared in a noun cluster but this cluster was not mapped to the cluster where the verb occurs the candidate will be discarded. 4 System Architecture As"
W18-0910,D11-1063,0,0.0287244,"milarity function is used to automatically select the important vector dimensions for the metaphor detection task. The authors experimented with different pretrained word representations, namely skip-gram model and an attribute-based model. Two different datasets, which were referred to as the TSV dataset 2 82 the WordNet lexicographer name of the words first sense the state-of-the-art semi-supervised system used as our baseline system. baum, 1998) have been employed to develop supervised systems to detect metaphors (K¨oper and Schulte im Walde, 2017; Tsvetkov et al., 2013; Hovy et al., 2013; Turney et al., 2011). Shutova et al. (2010) was among the earliest approaches to computational modelling of metaphor, avoiding task-specific hand-crafted knowledge and huge annotated resources. They introduced a semi-supervised approach to identify verb-noun metaphors using corpus-driven distributional clustering. Their strategy is based on clustering abstract nouns based on their contextual features in order to capture the metaphorical senses associated with the source concept. The system exploits a small set of metaphoric expressions as a seed to detect metaphors in a semi-supervised manner. In a follow-up work"
W18-0910,D17-1162,0,0.0467174,"ammatical relations. In this paper, we are interested in phrase-level linguistic metaphor detection, focusing on verbnoun phrases (grammatical relations) by employing semantic representation of word meaning. Therefore, due to space limitation, we will discuss the most relevant research in this regard in this section. An extensive literature review is presented in (Zhou et al., 2007; Shutova, 2015). Some recent work on metaphor detection has been looking into the utilization of semantic representations through word embeddings representations to design supervised systems for metaphor detection (Rei et al., 2017; Bulat et al., 2017; Shutova et al., 2016). Our approach also utilises such representations but in a semi-supervised manner to avoid the need for large training corpora. Rei et al. (2017) introduced a neural network architecture to detect adjective-noun and verb-noun metaphoric constructions. Their system comprises three main components which are: word gating, vector representation mapping and a weighted similarity function. The word gating is used to model the association between the properties of the source and target domains which is done via a non-linear transformation of the word embeddi"
W18-0910,J15-4002,0,0.018927,"identification” which is concerned with recognising (detecting) the metaphoric expressions in the input text. Metaphor detection could be done on the word-level (token-level) or on the phrase-level by extracting grammatical relations. In this paper, we are interested in phrase-level linguistic metaphor detection, focusing on verbnoun phrases (grammatical relations) by employing semantic representation of word meaning. Therefore, due to space limitation, we will discuss the most relevant research in this regard in this section. An extensive literature review is presented in (Zhou et al., 2007; Shutova, 2015). Some recent work on metaphor detection has been looking into the utilization of semantic representations through word embeddings representations to design supervised systems for metaphor detection (Rei et al., 2017; Bulat et al., 2017; Shutova et al., 2016). Our approach also utilises such representations but in a semi-supervised manner to avoid the need for large training corpora. Rei et al. (2017) introduced a neural network architecture to detect adjective-noun and verb-noun metaphoric constructions. Their system comprises three main components which are: word gating, vector representatio"
W18-0910,L16-1561,0,0.0163589,"be transferred to another concept’s sense such as “poverty” by exploiting the properties of the first concept. This then can be expressed in our everyday language in terms of linguistic metaphoric expressions such as “...eradicate poverty”, “...root out the causes of poverty”, or “...the roots of poverty are...”1 (Lakoff and Johnson, 1980; Veale et al., 2016). In this work, a word or an expression is a metaphor if it has at least one basic/literal sense (more concrete, physical) and a secondary metaphoric sense (abstract, 1 These examples could be found in the United Nations Parallel Corpus (Ziemski et al., 2016). 81 Proceedings of the Workshop on Figurative Language Processing, pages 81–90 c New Orleans, Louisiana, June 6, 2018. 2018 Association for Computational Linguistics (Tsvetkov et al., 2013) and the MOH dataset (Mohammad et al., 2016), were used to train the system and optimise its parameters as well as to assess its performance. Bulat et al. (2017) is a recent approach that investigated whether property-based semantic word representation can provide better concept generalisation for detecting metaphors than dense linguistic representation. The authors proposed property-based vectors through c"
W18-0910,N16-1020,0,0.254651,"emantic space. The authors built a count-based distributional vector and employed a skip-gram model trained on Wikipedia articles as their dense linguistic representations. The property-norm semantic space is obtained from the property-norm dataset (McRae et al., 2005). The TSV dataset is used to train and test a support vector machine (SVM) classifier to classify adjective-noun pairs using the introduced cognitively salient properties as features. An interesting approach, which employed multi-model embeddings of visual and linguistic features to detect metaphoricity in text, is introduced by Shutova et al. (2016). The proposed approach obtained linguistic word embeddings using a log-linear skip-gram model trained on Wikipedia text and obtained visual embeddings using a deep convolutional neural network trained on image data. This was done for both the words and phrases of adjective-noun and verb-noun pairs individually. Then, the cosine similarity function has been employed to measure the distance between the phrase vector and the corresponding vectors of its constituent words. Metaphor classification is done based on an optimised threshold output of the cosine similarity function. The authors used th"
W18-4921,W07-2441,0,0.0451838,"e size of the corpus should allow for at least 3,500 MWE annotations 5. The language must be of sufficiently high quality There were several corpora considered for selection, including the DiMSUM corpus (Schneider et al., 2016), the UP/TAP corpus,3 Wikidata parallel text (Vrandeˇci´c and Krötzsch, 2014) and the Universal Dependencies (UD) treebanks.4 Three corpora from the UD treebanks for English were ultimately selected as a source of data, as they alone fulfilled the criteria mentioned above: text was selected from the English-EWT corpus (Silveira et al., 2014),5 the LinES parallel corpus (Ahrenberg, 2007) and the Parallel Universal Dependencies (PUD) treebank (Zeman et al., 2017).6 The files were extracted in CoNLL-U format and converted to FoLiA XML format (see section 3) for annotating. The training, development and testing datasets for each treebank were concatenated, and then split into files of 201 sentences for annotation. 3 Annotation During the data preparation period, annotators were trained in the use of the FoLiA Linguistic Annotation Tool (FLAT). FLAT is an open-source web-based environment,7 using the XML-based FoLiA format. In order to aid annotators in annotating only verbal MWE"
W18-4921,J17-4005,0,0.162863,"Missing"
W18-4921,W15-0905,0,0.0600086,"Missing"
W18-4921,N15-1177,1,0.831467,"ting multiword expressions, and were all native speakers of English. Four dialects of English were represented: Irish English, British English, American English and Canadian English. 3 Documentation for UP/TAP: https://www.l2f.inesc-id.pt/~thomas/metashare/report-UP-TAP.pdf Documentation for UD: http://universaldependencies.org 5 Originally sourced from the English Web Treebank (Bies et al., 2012) 6 Though not a part of the task dataset, we have also fully annotated the Reviews portion of the UD English-EWT corpus by adding VMWE types to the existing VMWEs in STREUSLE (Schneider et al., 2014; Schneider and Smith, 2015, https: //github.com/nert-gu/streusle/); they were previously uncategorized. STREUSLE as of version 4.1 comprises 3812 sentences and 871 VMWE instances (121 IAV, 12 LVC.cause, 123 LVC.full, 310 VID, 206 VPC.full, 99 VPC.semi). 4 7 http://flat.readthedocs.io/en/latest/ 194 Figure 1: Screenshot of the FLAT Platform 3.1 Categories of VMWE Seven categories of VMWE were used in the English annotation task: Verbal Idioms (VID), Verb-Particle Constructions (VPC.full and VPC.semi),8 Light-Verb Constructions (LVC.full and LVC.cause),8 MultiVerb Constructions (MVC) and Inherently Adpositional Verbs (IA"
W18-4921,schneider-etal-2014-comprehensive,1,0.86405,"th or interest in annotating multiword expressions, and were all native speakers of English. Four dialects of English were represented: Irish English, British English, American English and Canadian English. 3 Documentation for UP/TAP: https://www.l2f.inesc-id.pt/~thomas/metashare/report-UP-TAP.pdf Documentation for UD: http://universaldependencies.org 5 Originally sourced from the English Web Treebank (Bies et al., 2012) 6 Though not a part of the task dataset, we have also fully annotated the Reviews portion of the UD English-EWT corpus by adding VMWE types to the existing VMWEs in STREUSLE (Schneider et al., 2014; Schneider and Smith, 2015, https: //github.com/nert-gu/streusle/); they were previously uncategorized. STREUSLE as of version 4.1 comprises 3812 sentences and 871 VMWE instances (121 IAV, 12 LVC.cause, 123 LVC.full, 310 VID, 206 VPC.full, 99 VPC.semi). 4 7 http://flat.readthedocs.io/en/latest/ 194 Figure 1: Screenshot of the FLAT Platform 3.1 Categories of VMWE Seven categories of VMWE were used in the English annotation task: Verbal Idioms (VID), Verb-Particle Constructions (VPC.full and VPC.semi),8 Light-Verb Constructions (LVC.full and LVC.cause),8 MultiVerb Constructions (MVC) and Inhere"
W18-4921,silveira-etal-2014-gold,0,0.0775921,"Missing"
W19-5116,D18-2029,0,0.0316412,"s a neologism based on whether p(Neologism|uv) > p(¬Neologism|uv) where: p(Neologism|uv) ∝ p(u|Neologism)p(v|Neologism)p(Neologism) Baseline Models We used three pretrained models for computing a single representation of adjective-nouns: A natural approach for determining whether an adjective-noun pair is compositional would be to compare the frequency with which the adjectivenoun occurs in comparison to the adjective and noun’s total frequency. This can be achieved by means of Probabilistic Mutual Information as follows:  P M I(uv) = p(uv) log p(uv) p(u)p(v) USE Universal sentence encoders (Cer et al., 2018) were introduced to provide a way to make embeddings of whole sentences. As such, they directly model semantic compositionality and we apply them by considering our term as a sentence and generating an 512dimensional embedding of the term.  ELMo ELMo is a pretrained language model that provides a deep contextual representation of a sentence. We used the ‘small’ model which generates a representation of 1,024 dimensions. Where p(uv) represents the probability of the adjective-noun pair, uv, occurring in our corpus, i.e., the total frequency divided by the length of the corpus, and p(u) and p(v"
W19-5116,P16-4003,0,0.0304629,"r, while the task has received some attention, most approaches so far have significant weaknesses, even though it is a major area of work for publishers in lexicography (O’Donovan and O’Neill, 2008). Some semi-automated approaches have relied on the extraction of features and the use of classifiers such as SVMs (Falk et al., 2014) or on language-specific features (Breen, 2010). Of close relationship to this task is automatic term recognition, where new terms are recognized based on their occurrence in a corpus. In these works, a number of metric for assessing ‘termhood’ (Spasi´c et al., 2013; Cram and Daille, 2016) have been introduced and these are often developed to work in specific domains (Buitelaar et al., 2013). It has been shown that combinations of many metrics can effectively learn terms (Astrakhantsev, 2014). However, previous work (McCrae et al., 2017) as well as the results in this paper show that these metrics perform poorly at identifying semantic compositionality. The semantics of adjectives have been studied not only from a logical perspective but as in terms of vector space models and word embeddings and in the context of analysis of semantic compositionality (Mitchell and Lapata, 2008)"
W19-5116,falk-etal-2014-non,0,0.0397228,"Missing"
W19-5116,D10-1115,0,0.0507156,"previous work (McCrae et al., 2017) as well as the results in this paper show that these metrics perform poorly at identifying semantic compositionality. The semantics of adjectives have been studied not only from a logical perspective but as in terms of vector space models and word embeddings and in the context of analysis of semantic compositionality (Mitchell and Lapata, 2008). Most works start from Mitchell and Lapata in representing the compositional vector of an adjective-noun pair with the following equation Further, it has been suggested that adjectives themselves should be matrices (Baroni and Zamparelli, 2010), such that p = Au v However, learning a matrix to represent each word can be quite difficult. This has been further extended to an approach where each word has a matrix to give a general approach to semantic compositionality (Socher et al., 2012). Moreover, it was shown that simpler models such as bidirectional LSTMs produce better results (Tai et al., 2015). This has lead to the development of pretrained models (Devlin et al., 2018; Peters et al., 2018), which can be trained on truly massive corpora and then still be effectively applied to tasks with relatively little training data. 3 3.1 Me"
W19-5116,W13-0104,0,0.0329589,"a positive set. Developing a negative set is much harder, as we would need to ask an expert lexicographer to manually evaluate a large number of adjective-noun combinations and verify that they were not neologisms that could be put into a dictionary. As such, we rely on a weakly supervised dataset that was constructed from Wikipedia. In particular, we randomly chose from Wikipedia articles a list p = αu + βv Where p is the vector of compound, u and v are vectors for the individual words and α,β are learned weights. This has been extended by replacing the scalar values, α and β with matrices (Boleda et al., 2013): p = Au + Bv 136 of unique adjective-noun pairs, which again were identified by part-of-speech tagging with NLTK, and then filtered out all those pairs, which are already in Wordnet. As this negative set is still likely to contain some true neologisms, we performed a quick manual analysis of 100 of these terms showed that 5 of them were certainly worthy of inclusion in a dictionary (e.g., ‘special education’, ‘safe position’) as they have meanings that are not deducible from the two words that compose the phrase. In contrast, most of the examples in the set were clearly compositional, e.g., ‘"
W19-5116,E12-1060,0,0.0329893,"ures, however the combination with linguistic features still further improves the results, suggesting the strength of a hybrid approach. 1 Introduction In the context of the construction of lexical resources, such as WordNet (Miller, 1995; Fellbaum, 2012), a key task is the identifications of terms that would be of relevance for inclusion in the resource and this task is called ‘neologism detection.’ Detection of single word neologisms can be principally accomplished by means of frequency statistics (McCrae et al., 2017) and even new senses of words can be identified by means of topic models (Lau et al., 2012). However, this task is much harder when we consider multiword expressions as a multiword expression may consist of two or more words that are already in the dictionary but whose combination may give extra meaning that could not be understood from just the words that compose this multiword expression. For example a ‘common viper’ is not merely a viper that is ‘common’, but in fact refers to Vipera berus a specific species of snake. In contrast, a ‘dangerous viper’ is simply a viper that is also dangerous and as such most lexicographers would prefer not to include the term in their resources. 1"
W19-5116,N18-1202,0,0.320354,"ex in terms of their semantic compositionality (McCrae et al., 2014) and this can be broadly broken down into three categories, intersective, subsective and privative adjectives (Partee, 2003; Bouillon and Viegas, 1999; Morzycki, 2015). We use WordNet as the principle background knowledge and thus rely on the judgement of the WordNet lexicographers in order to deduce if a particular adjectivenoun combination is a neologism. Our approach for detecting whether adjectivenoun pairs are likely to be neological is based on the recent breakthroughs regarding pretrained language models, such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018), which have shown to be effective for solving a wide variety of tasks (Radford et al., 2018). For this particular problem of neologism detection, it is clear that there is significant value in the use of these pretrained models as they easily create a vector that represents the adjective-noun combination and this can be compared with a word-based model such as GloVe (Pennington et al., 2014), to deduce if an adjective-noun pair is compositional or neological. The paper is structured as follows, first in Section 2 we will describe some of the related work in the"
W19-5116,W02-0109,0,0.0613558,"Devlin et al., 2018; Peters et al., 2018), which can be trained on truly massive corpora and then still be effectively applied to tasks with relatively little training data. 3 3.1 Methodology Data Preparation In order to develop a classifier to determine if a particular adjective-noun pair is a neologism. We first need to develop a set of pairs that we know to be neological and a set that we can assume is likely not to be. For the development of the positive set, we simply took all the two-word expressions within Princeton WordNet 3.1, and deduced the likely part-of-speech tagging using NLTK (Loper and Bird, 2002) and selected only those that were tagged as “JJ NN” or “JJ NNS”. This yielded as set of 11,474 terms that we could use as a positive set. Developing a negative set is much harder, as we would need to ask an expert lexicographer to manually evaluate a large number of adjective-noun combinations and verify that they were not neologisms that could be put into a dictionary. As such, we rely on a weakly supervised dataset that was constructed from Wikipedia. In particular, we randomly chose from Wikipedia articles a list p = αu + βv Where p is the vector of compound, u and v are vectors for the in"
W19-5116,W14-4724,1,0.896893,"Missing"
W19-5116,D12-1110,0,0.0668901,"vector space models and word embeddings and in the context of analysis of semantic compositionality (Mitchell and Lapata, 2008). Most works start from Mitchell and Lapata in representing the compositional vector of an adjective-noun pair with the following equation Further, it has been suggested that adjectives themselves should be matrices (Baroni and Zamparelli, 2010), such that p = Au v However, learning a matrix to represent each word can be quite difficult. This has been further extended to an approach where each word has a matrix to give a general approach to semantic compositionality (Socher et al., 2012). Moreover, it was shown that simpler models such as bidirectional LSTMs produce better results (Tai et al., 2015). This has lead to the development of pretrained models (Devlin et al., 2018; Peters et al., 2018), which can be trained on truly massive corpora and then still be effectively applied to tasks with relatively little training data. 3 3.1 Methodology Data Preparation In order to develop a classifier to determine if a particular adjective-noun pair is a neologism. We first need to develop a set of pairs that we know to be neological and a set that we can assume is likely not to be. Fo"
W19-5116,P15-1150,0,0.0302607,"pata, 2008). Most works start from Mitchell and Lapata in representing the compositional vector of an adjective-noun pair with the following equation Further, it has been suggested that adjectives themselves should be matrices (Baroni and Zamparelli, 2010), such that p = Au v However, learning a matrix to represent each word can be quite difficult. This has been further extended to an approach where each word has a matrix to give a general approach to semantic compositionality (Socher et al., 2012). Moreover, it was shown that simpler models such as bidirectional LSTMs produce better results (Tai et al., 2015). This has lead to the development of pretrained models (Devlin et al., 2018; Peters et al., 2018), which can be trained on truly massive corpora and then still be effectively applied to tasks with relatively little training data. 3 3.1 Methodology Data Preparation In order to develop a classifier to determine if a particular adjective-noun pair is a neologism. We first need to develop a set of pairs that we know to be neological and a set that we can assume is likely not to be. For the development of the positive set, we simply took all the two-word expressions within Princeton WordNet 3.1, a"
W19-5116,C00-2137,0,0.273187,"BERT BERT BERT BERT n/a 50 100 200 300 0.830 0.862 0.882 0.854 0.848 0.808 0.839 0.895 0.872 0.828 0.866 0.894 0.866 0.830 0.879 0.835 0.866† 0.880† 0.850† 0.853† BERT (No Freq) 100 0.846 0.834 0.863 0.848† Model Table 2: Result for the detection of neological adjective-noun terms using our models. ∗ and † denote a statistically significant improvement over the Na¨ıve Bayes baseline at p = 0.05, 0.01 respectively. evaluated all these settings on the 3 pretrained language models, USE, ELMo and BERT and the results are presented in Table 2. Statistical significance was calculated at two levels (Yeh, 2000). The strongest result in accuracy, precision and F-Measure is the BERT model with GloVe vectors of dimensionality 100, although the USE and ELMo methods present a similar result with GloVe dimensionality of 100 or 200, suggesting that the use of pretrained models in general is helpful for the identification of neological adjective-noun phrases. The difference in performance between the choice of models was however not statistically significant. Furthermore, we also observe that the larger GloVe vectors are not helpful and observations of the test set accuracy as well as preliminary experiment"
W19-6809,I08-2113,0,0.0302845,"e Dravidian language family to exploit the similar syntax and semantic structures by phonetic transcription of the corpora into Latin script along with image feature to improve the translation quality. 3 3.1 Background Dravidian Languages Dravidian languages have individual writing scripts and have been assigned a unique block in the Unicode computing industry standard. The similarity of these languages is that they are all written from left to right, consist of sequences of simple or complex characters and follow an alpha-syllabic writing system in which the individual symbols are syllables (Bhanuprasad and Svenson, 2008). The languages also have different sets of vowels and consonants. Vowels and consonants are atomic but when they are combined with each other they form consonant ligatures. Dravidian languages such as Tamil do not represent differences between aspirated and unaspirated stops, while other Dravidian languages such as Kannada Dublin, Aug. 19-23, 2019 |p. 57 and Malayalam have a large number of loan words from Indo-Aryan languages and support a large number of compound characters resulting from the combination of two consonants symbols (Kumar et al., 2015). The attention model calculates ct as th"
W19-6809,D17-1105,0,0.0119647,"odal NMT (MNMT) was introduced by Specia et al. (2016b) to generate image descriptions for a target language, given an image and/or a description in the source language. In previous works on MNMT, the researchers utilized visual context by involving both NMT and Image Description Generation (IDG) features that explicitly uses an encoder-decoder (Cho et al., 2014). However, the encoder-decoder architecture encodes the source sentence into a fixed-length vector. To overcome this drawback (Bahdanau et al., 2015) introduced attention mechanism to focus on parts of the source sentence. The work by Calixto and Liu (2017), carried out different experiments to incorporate visual features into NMT by projecting an image feature vector as words into the source sentence, using the image to initialize the encoder hidden state, and using image features to initialize the decoder hidden state. In Calixto et al. (2017), the author incorporated features through a separate encoder and doublyattentive attention of the decoder to depend on the LoResMT 2019 image feature. This allowed them to predict the next word and showed that the image feature improved the translation quality. Although all these approaches have demonstr"
W19-6809,P17-1175,0,0.0396914,"Missing"
W19-6809,2018.gwc-1.10,1,0.442141,"Missing"
W19-6809,2003.mtsummit-papers.9,0,0.059995,"Missing"
W19-6809,W18-3405,0,0.064437,"e aligned with the parallel sentences for training. The largest existing dataset containing captions, images, and translations for English, German, French and Czech is the WMT shared task Multi30K dataset which is derived from the Flickr30k dataset (Plummer et al., 2015; Plummer et al., 2017). Typically this data is manually created with the help of bilingual annotators (Elliott et al., 2016), however, for many languages, such resources are not available. In those cases, machine translation can be a useful tool for the quick expansion to new languages by producing candidate translation (Dutta Chowdhury et al., 2018). In order to reduce the amount of time, we pose translation as a post-editing task. We automatically translated the English sentences from the WMT corpus using a pre-trained general domain Statistical Machine Translation (SMT) and Neural Machine Translation (NMT). Multilingual NMT models (Firat et al., 2016) have been shown to increase the translation quality for under-resourced languages. Closely related Dravidian languages such as Tamil (ISO-639-1: ta), Kannada (ISO-639-1: kn), and Malayalam (ISO-639-1: ml) exhibit a large overlap in their vocabulary and strong syntactic and lexical similar"
W19-6809,N16-1101,0,0.0247052,"is manually created with the help of bilingual annotators (Elliott et al., 2016), however, for many languages, such resources are not available. In those cases, machine translation can be a useful tool for the quick expansion to new languages by producing candidate translation (Dutta Chowdhury et al., 2018). In order to reduce the amount of time, we pose translation as a post-editing task. We automatically translated the English sentences from the WMT corpus using a pre-trained general domain Statistical Machine Translation (SMT) and Neural Machine Translation (NMT). Multilingual NMT models (Firat et al., 2016) have been shown to increase the translation quality for under-resourced languages. Closely related Dravidian languages such as Tamil (ISO-639-1: ta), Kannada (ISO-639-1: kn), and Malayalam (ISO-639-1: ml) exhibit a large overlap in their vocabulary and strong syntactic and lexical similarities. Dravidian languages are a family of languages spoken primarily in the southern part of India and spread over South Asia and are considered as under-resourced languages. However, the scripts used to write these languages are different and they differ in their morphology. Recently Chakravarthi et al. (20"
W19-6809,Q17-1024,0,0.075519,"Missing"
W19-6809,P17-4012,0,0.0113752,"data have been widely used to improve the performance of NMT and MNMT. To produce a target side description of an image, we create a general domain SMT and NMT for English-Tamil, English-Kannada, and EnglishMalayalam pairs. We collected the general domain parallel corpora for the Dravidian languages from the OPUS website (Tiedemann and Nygaard, 2004) and (Chakravarthi et al., 2018). The corpus statistics are shown in Table 1. The corpus is tokenized and standardized to lowercase. The general domain SMT was created with Moses (Koehn et al., 2007) while the NMT system was trained with OpenNMT (Klein et al., 2017). After tokenization, we fed the parallel corpora to Moses and OpenNMT. Preprocessed files are then used to train the models. We used the default OpenNMT parameters for training, i.e. 2 layers LSTM with 500 hidden units for both, the encoder and decoder. The SMT and NMT system results on general Dublin, Aug. 19-23, 2019 |p. 59 Table 3: Results are expressed in BLEU score: Baseline is Multimodal NMT, MMNMT is trained on native script, and MMNMT-T is trained utilizing phonetic transcription. Lang pair En-Ta En-Ml En-Kn Ta-En Ml-En Kn-En Figure 2: Example of sentence and image with candidate tran"
W19-6809,P07-2045,0,0.00476175,"ription pairs for each image. Synthetic data or back-transliterated data have been widely used to improve the performance of NMT and MNMT. To produce a target side description of an image, we create a general domain SMT and NMT for English-Tamil, English-Kannada, and EnglishMalayalam pairs. We collected the general domain parallel corpora for the Dravidian languages from the OPUS website (Tiedemann and Nygaard, 2004) and (Chakravarthi et al., 2018). The corpus statistics are shown in Table 1. The corpus is tokenized and standardized to lowercase. The general domain SMT was created with Moses (Koehn et al., 2007) while the NMT system was trained with OpenNMT (Klein et al., 2017). After tokenization, we fed the parallel corpora to Moses and OpenNMT. Preprocessed files are then used to train the models. We used the default OpenNMT parameters for training, i.e. 2 layers LSTM with 500 hidden units for both, the encoder and decoder. The SMT and NMT system results on general Dublin, Aug. 19-23, 2019 |p. 59 Table 3: Results are expressed in BLEU score: Baseline is Multimodal NMT, MMNMT is trained on native script, and MMNMT-T is trained utilizing phonetic transcription. Lang pair En-Ta En-Ml En-Kn Ta-En Ml-E"
W19-6809,W15-5404,0,0.115764,"Missing"
W19-6809,P16-1162,0,0.00778975,"slation for the training set of MMDravi. For our tasks, all descriptions in English were converted to lowercase and tokenized, while we LoResMT 2019 Baseline 50.2 35.6 44.5 45.2 34.3 50.0 BLEUscore MMNMT MMNMT-T 51.0 52.3 36.0 36.5 45.1 45.9 47.4 48.9 36.2 37.6 50.2 50.8 did not have to bother about the case correction for Dravidian languages (as they do not have cases). We tokenized the Dravidian language using the OpenNMT tokenizer with segment alphabet options for Tamil, Kannada, and Malayalam. For the sub-word level representation, we chose the 10,000 most frequent units to train the BPE (Sennrich et al., 2016) model. We used this model for the sub-word level segmentation for the training, development, and evaluation set. We trained the MMNMT model to translate from English into Dravidian languages as well as from Dravidian languages into English. Visual features were extracted from publicly available pre-trained CNN’s. Specifically, we extract spatial image features using the VGG-19 network (Simonyan and Zisserman, 2014). In our experiment, we pass all the images in our dataset through the pre-trained VGG19 layered network to extract global information and use them in a separate visual attention me"
W19-6809,W16-2346,0,0.059033,"Missing"
W19-6809,tiedemann-nygaard-2004-opus,0,0.103665,"ed by Multi30K dataset (Elliott et al., 2016). The first one is an English description for each image and its German translation. The second is a corpus of five independently collected English and German description pairs for each image. Synthetic data or back-transliterated data have been widely used to improve the performance of NMT and MNMT. To produce a target side description of an image, we create a general domain SMT and NMT for English-Tamil, English-Kannada, and EnglishMalayalam pairs. We collected the general domain parallel corpora for the Dravidian languages from the OPUS website (Tiedemann and Nygaard, 2004) and (Chakravarthi et al., 2018). The corpus statistics are shown in Table 1. The corpus is tokenized and standardized to lowercase. The general domain SMT was created with Moses (Koehn et al., 2007) while the NMT system was trained with OpenNMT (Klein et al., 2017). After tokenization, we fed the parallel corpora to Moses and OpenNMT. Preprocessed files are then used to train the models. We used the default OpenNMT parameters for training, i.e. 2 layers LSTM with 500 hidden units for both, the encoder and decoder. The SMT and NMT system results on general Dublin, Aug. 19-23, 2019 |p. 59 Table"
W19-6907,C94-2167,0,0.669268,"rms as well as the length (in words) of the term, |w|: 3.4.4 Topic Modelling Finally, the use of topic models has been suggested based on the success of Latent Dirichlet Allocation (Blei et al., 2003) in the form of the Novel Topic Model (NTM) (Li et al., 2013), although we did not in fact use this metric, as our previous experiments have shown it to perform poorly. NTM requires a probability distribution of a word being labelled to one of K topics, p(wi = w|zi = k), the score is then calculated as NTM(w) = tf (w) X v∈w ComboBasic(w) = |w|tf (w)+ α|Tsuper (w) |+ β|Tsub (w)| Similarly, cValue (Ananiadou, 1994) uses the subterm frequency as well: cValue(w) = log2 (|w |+ 0.1)× ! P 0 t0 ∈Tsub (w) tf (t ) tf (w) − |Tsub (w)| The domain coherence measures the correlation, using probabilistic mutual information, of the term with other words in the corpus and then uses this to predict a score, in particular we use the PostRankDC method (Buitelaar et al., 2013). tf (w) tfref (w) max P (wi = w|zi = k) k 3.4.5 Multi-metric scoring Once all the scores for all candidate terms have been calculated, a ranking of the top terms is necessary. In general, these terms produce very different scores and as such, method"
W19-6907,W15-4301,0,0.116885,"Saffron system (Bordea et al., 2014; Pereira et al., 2019). The main requirements for this are the development of a partof-speech tagger, a lemmatizer and a large background corpus and we will detail in this paper how we constructed these models for Irish. In particular, the largest challenge was the construction of a part-of-speech tagger and we base our work on two main systems that have been developed based on annotated corpora. Firstly, we look at the system of U´ı Dhonnchadha and van Genabith (2006), which was developed on a general language domain and secondly we refer to the system of Lynn et al. (2015), which was developed specifically for tweets. We then looked at an alternative approach using the terminology database, Tearma1 , to provide an annotation over the Irish Wikipedia, ‘An Vicip´eid’2 . For both the systems trained on part-of-speech corpora and those on the terminology database, we compare them for the challenge of recognizing terms. We show how we incorporate into our term recognition system morphology information extracted from Pota Focal (Mˇechura, 2018). To analyse this we developed a small gold standard dataset of Wikipedia articles and compared the two methods on this datas"
W19-6907,ui-dhonnchadha-van-genabith-2006-part,0,0.132309,"Missing"
W19-6910,P05-1071,0,0.11153,"losses, dating from about the middle of the eighth century, are quite an early example of text which demonstrates such spacing. The practice would not become the standard in European texts until about the thirteenth century. Tolmachev et al. (2018) present a toolkit for developing morphological analysers for scriptio continua languages, which utilises RNN and linear neural net models. Turning towards modern natural languages further comparisons can be made. Tokenization solutions which have been developed for languages including Finnish (Haverinen et al., 2013; Lankinen et al., 2016), Arabic (Habash and Rambow, 2005) and Vietnamese (Hông Phuong et al., 2008) may provide a basis for developing an Old Irish tokenizer. In the case of Vietnamese, Hông Phuong et al. explain that the language uses an alphabetic script, but that spacing is used not only to separate words, but also the syllables which make up words. Furthermore, syllables, taken in isolation, are typically words themselves. When combined with other syllables, words of complex meaning are created. As such, the problem faced by Vietnamese in terms of word segmentation is comparable to that of Old Irish where compound verbs are formed by combining t"
W19-6910,C92-4173,0,0.0560731,"ally tokenizing Irish compound verbs in that a tokenizer must not split these apart when encountered. A more challenging problem is presented, however, in the way Old Irish deals with pronouns which form the objects of these compound verbs. These are infixed between the preverbal particle and the verbal root, effectively splitting what might ideally be considered a single token and requiring that another token be placed within it. To exemplify this issue, where the verb mentioned above, dobeir, “he gives”, appears with the first singular infixed pronoun, -m, it becomes dombeir, “he gives me”. Webster and Kit (1992) make the point that the “simplicity of recognising words in English [results] from the existence of space marks as explicit delimiters”. It is, perhaps based on this Proceedings of the Celtic Language Technology Workshop 2019 Dublin, 19–23 Aug., 2019 |p. 71 same notion that Hông Phuong et al. (2008) claim “a tokenizer which simply replaces blanks with word boundaries ... is already quite accurate” for alphabetic scripts. Unfortunately, for the reasons outlined above, such an approach is not necessarily feasible with Old Irish texts. Before tokenization can be carried out decisions must be mad"
W19-7101,C16-1010,1,0.937123,"m, 1998) was built from scratch. The taxonomies of the languages, synsets, relations among synset are built first in the merge approach. Popular wordnets like EuroWordNet (Vossen, 1997) and IndoWordNet (Bhattacharyya, 2010) are developed by the expand approach whereby the synsets are built in correspondence with the existing wordnet synsets by translation. For the Tamil language, Rajendran et al. (2002) proposed a design template for the Tamil wordnet. To evaluate and improve the wordnets for the targeted under-resourced Dravidian languages, Chakravarthi et al. (2018) followed the approach of Arcan et al. (2016), which uses the existing translations of wordnets in other languages to identify contextual information for wordnet senses from a large set of generic parallel corpora. They use this contextual information to improve the translation quality of WordNet senses. They showed that their approach can help overcome the drawbacks of simple translations of words without context. Chakravarthi et al. (2018) removed the codemixing based on the script of the parallel corpus to reduce the noise in translation. The authors used the SMT to create bilingual MT for three Dravidian languages. In our work, we us"
W19-7101,W05-0909,0,0.097147,"n in different scripts, they must be converted to some common representation before training the MNMT to take advantage of closely related language resources. A phonetic transcription is an approach where a word in one script is transformed into a different script by maintaining phonetic correspondence. Phonetic transcribing to Latin script and International Phonetic Alphabet (IPA) was studied by (Chakravarthi et al., 2019) and showed that Latin script outperforms IPA for the MNMT Dravidian languages. The improvements in results were shown in terms of the BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) and chrF (Popovi´c, 2015) metric. To evaluate the similarity of the corpus the authors used cosine similarity and shown that transcribing to Latin script retain more similarity. We used Indic-trans library by Bhat et al. (2015), which bring all the languages into a single representation by phoneme matching algorithm. The same library can also back2 3 http://opus.nlpl.eu/ https://github.com/libindic/indic-trans MomenT-2019 transliterate from English (Latin script) to Indian languages. 3.5 Code-Mixing Code-mixing is a phenomenon which occurs commonly in most multilingual societies where the spe"
W19-7101,I08-2113,0,0.0393287,"orpora from three languages. In the second one removed code-mixing, phonetically transcribed the corpora and then compiled the multilingual corpora by concatenating the parallel corpora from three languages. These two experiments are contribution to this work compared to the previous works. 3 3.1 Experiment Setup Dravidian Languages For our study, we perform experiments on Tamil (ISO 639-1: ta), Telugu (ISO 639-1: te) and Kannada (ISO 639-1: kn). The targeted languages for this work differ in their orthographies due to historical reasons and whether they adopted the Sanskrit tradition or not (Bhanuprasad and Svenson, 2008). Each of these has been assigned a unique block in Unicode, and thus from an MNMT perspective are completely distinct. 3.2 Multilingual Neural Machine Translation Johnson et al. (2017) and Ha et al. (2016) extended the architecture of Bahdanau et al. (2015) to use a universal model to handle multiple source and target languages with a special tag in the encoder to determine which target language to translate. The idea is to use the unified vocabulary and training corpus without modification in the architecture to take advantage of the shared embedding. The goal of this approach is to improve"
W19-7101,2018.gwc-1.10,1,0.486668,"and Slavic share words from a common root (cognates), which are highly semantically and phonologically similar. In the scope of the wordnet creation for underresourced languages, combining parallel corpus from closely related languages, phonetic transcription of the corpus and creating multilingual neural machine translation has been shown to improve the results in this paper. The evaluation results ob1 http://globalwordnet.org/ Dublin, Aug. 19-23, 2019 |p. 1 tained from MNMT with transliterated corpus are better than the results of Statistical Machine Translation (SMT) from the recent work (Chakravarthi et al., 2018). 2 Related Work The Princeton WordNet (Miller, 1995; Fellbaum, 1998) was built from scratch. The taxonomies of the languages, synsets, relations among synset are built first in the merge approach. Popular wordnets like EuroWordNet (Vossen, 1997) and IndoWordNet (Bhattacharyya, 2010) are developed by the expand approach whereby the synsets are built in correspondence with the existing wordnet synsets by translation. For the Tamil language, Rajendran et al. (2002) proposed a design template for the Tamil wordnet. To evaluate and improve the wordnets for the targeted under-resourced Dravidian la"
W19-7101,O09-5003,0,0.0184181,"created by voluntary annotators or align automatically. The technical documents translation such as KDE, GNOME, and Ubuntu translations have code-mixing data since some of the technical terms may not be known to voluntary annotators for translation. But the code-mixing from OpenSubtitle are due to bilingual and historical reasons of Indian speakers (Chanda et al., 2016; Parshad et al., 2016). Different combinations of languages may occur while code-mixing for example GermanItalian and French-Italian in Switzerland, HindiTelugu in state of Telangana, India, TaiwaneseMandarin Chinese in Taiwan (Chan et al., 2009). Since the Internet era, English become the international language of the younger generation. Hence, English words are frequently embedded in Indians’ speech. For our work, only intra-sentential code-mixing was taken into account. In this case, Dravidian languages as the primary language, and English as secondary languages. We removed the English words considering only the English as a foreign word based on the script. Statistics of the removal of code-mixing is shown in Table 2. 3.6 WordNet creation Using contextual information to improve the translation quality of wordnet senses was shown t"
W19-7101,W16-5814,0,0.0295609,"societies where the speaker or writer alternate between two or more languages in a sentence (Ayeomoni, 2006; Ranjan et al., 2016; Yoder et al., 2017; Parshad et al., 2016). Since most of our corpus came from publicly available parallel corpus are created by voluntary annotators or align automatically. The technical documents translation such as KDE, GNOME, and Ubuntu translations have code-mixing data since some of the technical terms may not be known to voluntary annotators for translation. But the code-mixing from OpenSubtitle are due to bilingual and historical reasons of Indian speakers (Chanda et al., 2016; Parshad et al., 2016). Different combinations of languages may occur while code-mixing for example GermanItalian and French-Italian in Switzerland, HindiTelugu in state of Telangana, India, TaiwaneseMandarin Chinese in Taiwan (Chan et al., 2009). Since the Internet era, English become the international language of the younger generation. Hence, English words are frequently embedded in Indians’ speech. For our work, only intra-sentential code-mixing was taken into account. In this case, Dravidian languages as the primary language, and English as secondary languages. We removed the English wor"
W19-7101,P17-1176,0,0.0191449,"bilingual MT for three Dravidian languages. In our work, we use MNMT system and we transliterate the closely related language corpus into a single script to take advantage of MNMT systems. Neural Machine Translation achieved rapid development in recent years, however, conventional NMT (Bahdanau et al., 2015) creates a separate machine translation system for each pair of languages. Creating individual machine translation system for many languages is resource consuming, considering there are around 7000 languages in the world. Recent work on NMT, specifically on lowresource (Zoph et al., 2016; Chen et al., 2017) or zero-resource machine translation (Johnson et al., 2017; Firat et al., 2016) uses third languages as pivots and showed that translation quality is significantly improved. Ha et al. (2016) proposed an approach to extend the Bahdanau et al. (2015) architecture to multilingual translation by sharing the MomenT-2019 entire model. The approach of shared vocabulary across multiple languages resulted in a shared embedding space. Although the results were promising, the result of the experiments was reported in highly resourced languages such as English, German, and French but many under-resourced"
W19-7101,D16-1026,0,0.0629126,"Missing"
W19-7101,Q17-1024,0,0.067806,"Missing"
W19-7101,P17-4012,0,0.0462817,"the common semantics across languages and reduce the number of translation systems needed. The sentence of different languages are distinguished through languages codes. 3.3 Data We used datasets from Chakravarthi et al. (2018) in our experiment. The authors collected three Dravidian languages ↔ English pairs from OPUS2 web-page (Tiedemann and Nygaard, 2004). Corpus statistics are shown in Table 1. More descriptions about the three datasets can be found in Chakravarthi et al. (2018). We transliterated this corpus using Indic-trans library3 . All the sentences are first tokenized with OpenNMT (Klein et al., 2017) tokenizer and then segmented into subword symbols using Byte Pair Encoding (BPE) (Sennrich et al., 2016). We learn the BPE merge operations across all the languages. Following Ha et al. (2016), we indicate the language by prepending two tokens to indicate the desired source and target language. An example of a sentence in English to be translated into Tamil would be: src__en tgt_ta I like ice-cream 3.4 Transliteration As the Indian languages under our study are written in different scripts, they must be converted to some common representation before training the MNMT to take advantage of clos"
W19-7101,P02-1040,0,0.110524,"uages under our study are written in different scripts, they must be converted to some common representation before training the MNMT to take advantage of closely related language resources. A phonetic transcription is an approach where a word in one script is transformed into a different script by maintaining phonetic correspondence. Phonetic transcribing to Latin script and International Phonetic Alphabet (IPA) was studied by (Chakravarthi et al., 2019) and showed that Latin script outperforms IPA for the MNMT Dravidian languages. The improvements in results were shown in terms of the BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) and chrF (Popovi´c, 2015) metric. To evaluate the similarity of the corpus the authors used cosine similarity and shown that transcribing to Latin script retain more similarity. We used Indic-trans library by Bhat et al. (2015), which bring all the languages into a single representation by phoneme matching algorithm. The same library can also back2 3 http://opus.nlpl.eu/ https://github.com/libindic/indic-trans MomenT-2019 transliterate from English (Latin script) to Indian languages. 3.5 Code-Mixing Code-mixing is a phenomenon which occurs commonly in most m"
W19-7101,W16-4806,0,0.0565569,"Missing"
W19-7101,tiedemann-nygaard-2004-opus,0,0.0459346,"fication in the architecture to take advantage of the shared embedding. The goal of this approach is to improve the transDublin, Aug. 19-23, 2019 |p. 2 lation quality for individual languages pairs, for which parallel corpus data is scarce by letting the NMT to learn the common semantics across languages and reduce the number of translation systems needed. The sentence of different languages are distinguished through languages codes. 3.3 Data We used datasets from Chakravarthi et al. (2018) in our experiment. The authors collected three Dravidian languages ↔ English pairs from OPUS2 web-page (Tiedemann and Nygaard, 2004). Corpus statistics are shown in Table 1. More descriptions about the three datasets can be found in Chakravarthi et al. (2018). We transliterated this corpus using Indic-trans library3 . All the sentences are first tokenized with OpenNMT (Klein et al., 2017) tokenizer and then segmented into subword symbols using Byte Pair Encoding (BPE) (Sennrich et al., 2016). We learn the BPE merge operations across all the languages. Following Ha et al. (2016), we indicate the language by prepending two tokens to indicate the desired source and target language. An example of a sentence in English to be tr"
W19-7101,W17-2911,0,0.06079,"Missing"
W19-7101,D16-1163,0,0.0322041,"d the SMT to create bilingual MT for three Dravidian languages. In our work, we use MNMT system and we transliterate the closely related language corpus into a single script to take advantage of MNMT systems. Neural Machine Translation achieved rapid development in recent years, however, conventional NMT (Bahdanau et al., 2015) creates a separate machine translation system for each pair of languages. Creating individual machine translation system for many languages is resource consuming, considering there are around 7000 languages in the world. Recent work on NMT, specifically on lowresource (Zoph et al., 2016; Chen et al., 2017) or zero-resource machine translation (Johnson et al., 2017; Firat et al., 2016) uses third languages as pivots and showed that translation quality is significantly improved. Ha et al. (2016) proposed an approach to extend the Bahdanau et al. (2015) architecture to multilingual translation by sharing the MomenT-2019 entire model. The approach of shared vocabulary across multiple languages resulted in a shared embedding space. Although the results were promising, the result of the experiments was reported in highly resourced languages such as English, German, and French but"
