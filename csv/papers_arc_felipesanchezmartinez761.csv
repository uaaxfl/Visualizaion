2021.emnlp-main.669,Rethinking Data Augmentation for Low-Resource Neural Machine Translation: A Multi-Task Learning Approach,2021,-1,-1,4,0.758937,9987,victor sanchezcartagena,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"In the context of neural machine translation, data augmentation (DA) techniques may be used for generating additional training samples when the available parallel data are scarce. Many DA approaches aim at expanding the support of the empirical data distribution by generating new sentence pairs that contain infrequent words, thus making it closer to the true data distribution of parallel sentences. In this paper, we propose to follow a completely different approach and present a multi-task DA approach in which we generate new sentence pairs with transformations, such as reversing the order of the target sentence, which produce unfluent target sentences. During training, these augmented sentences are used as auxiliary tasks in a multi-task framework with the aim of providing new contexts where the target prefix is not informative enough to predict the next word. This strengthens the encoder and forces the decoder to pay more attention to the source representations of the encoder. Experiments carried out on six low-resource translation tasks show consistent improvements over the baseline and over DA methods aiming at extending the support of the empirical data distribution. The systems trained with our approach rely more on the source tokens, are more robust against domain shift and suffer less hallucinations."
2020.wmt-1.107,Bicleaner at {WMT} 2020: {U}niversitat d{'}Alacant-Prompsit{'}s submission to the parallel corpus filtering shared task,2020,-1,-1,4,0.811321,5040,miquel esplagomis,Proceedings of the Fifth Conference on Machine Translation,0,"This paper describes the joint submission of Universitat d{'}Alacant and Prompsit Language Engineering to the WMT 2020 shared task on parallel corpus filtering. Our submission, based on the free/open-source tool Bicleaner, enhances it with Extremely Randomised Trees and lexical similarity features that account for the frequency of the words in the parallel sentences to determine if two sentences are parallel. To train this classifier we used the clean corpora provided for the task and synthetic noisy parallel sentences. In addition we re-score the output of Bicleaner using character-level language models and n-gram saturation."
2020.eamt-1.8,A multi-source approach for {B}reton{--}{F}rench hybrid machine translation,2020,-1,-1,3,0.783317,9987,victor sanchezcartagena,Proceedings of the 22nd Annual Conference of the European Association for Machine Translation,0,"Corpus-based approaches to machine translation (MT) have difficulties when the amount of parallel corpora to use for training is scarce, especially if the languages involved in the translation are highly inflected. This problem can be addressed from different perspectives, including data augmentation, transfer learning, and the use of additional resources, such as those used in rule-based MT. This paper focuses on the hybridisation of rule-based MT and neural MT for the Breton{--}French under-resourced language pair in an attempt to study to what extent the rule-based MT resources help improve the translation quality of the neural MT system for this particular under-resourced language pair. We combine both translation approaches in a multi-source neural MT architecture and find out that, even though the rule-based system has a low performance according to automatic evaluation metrics, using it leads to improved translation quality."
2020.eamt-1.32,An {E}nglish-{S}wahili parallel corpus and its use for neural machine translation in the news domain,2020,-1,-1,1,1,9988,felipe sanchezmartinez,Proceedings of the 22nd Annual Conference of the European Association for Machine Translation,0,"This paper describes our approach to create a neural machine translation system to translate between English and Swahili (both directions) in the news domain, as well as the process we followed to crawl the necessary parallel corpora from the Internet. We report the results of a pilot human evaluation performed by the news media organisations participating in the H2020 EU-funded project GoURMET."
2020.coling-main.349,Understanding the effects of word-level linguistic annotations in under-resourced neural machine translation,2020,-1,-1,3,0.783317,9987,victor sanchezcartagena,Proceedings of the 28th International Conference on Computational Linguistics,0,"This paper studies the effects of word-level linguistic annotations in under-resourced neural machine translation, for which there is incomplete evidence in the literature. The study covers eight language pairs, different training corpus sizes, two architectures, and three types of annotation: dummy tags (with no linguistic information at all), part-of-speech tags, and morpho-syntactic description tags, which consist of part of speech and morphological features. These linguistic annotations are interleaved in the input or output streams as a single tag placed before each word. In order to measure the performance under each scenario, we use automatic evaluation metrics and perform automatic error classification. Our experiments show that, in general, source-language annotations are helpful and morpho-syntactic descriptions outperform part of speech for some language pairs. On the contrary, when words are annotated in the target language, part-of-speech tags systematically outperform morpho-syntactic description tags in terms of automatic evaluation metrics, even though the use of morpho-syntactic description tags improves the grammaticality of the output. We provide a detailed analysis of the reasons behind this result."
W19-6723,Global Under-Resourced Media Translation ({G}o{URMET}),2019,-1,-1,6,0,5031,alexandra birch,"Proceedings of Machine Translation Summit XVII: Translator, Project and User Tracks",0,None
W19-6625,Improving Translations by Combining Fuzzy-Match Repair with Automatic Post-Editing,2019,0,0,2,1,5074,john ortega,Proceedings of Machine Translation Summit XVII: Research Track,0,None
W19-5339,The {U}niversitat d{'}Alacant Submissions to the {E}nglish-to-{K}azakh News Translation Task at {WMT} 2019,2019,-1,-1,3,0.783317,9987,victor sanchezcartagena,"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)",0,"This paper describes the two submissions of Universitat d{'}Alacant to the English-to-Kazakh news translation task at WMT 2019. Our submissions take advantage of monolingual data and parallel data from other language pairs by means of iterative backtranslation, pivot backtranslation and transfer learning. They also use linguistic information in two ways: morphological segmentation of Kazakh text, and integration of the output of a rule-based machine translation system. Our systems were ranked second in terms of chrF++ despite being built from an ensemble of only 2 independent training runs."
W18-6464,{UA}lacant machine translation quality estimation at {WMT} 2018: a simple approach using phrase tables and feed-forward neural networks,2018,9,0,1,1,9988,felipe sanchezmartinez,Proceedings of the Third Conference on Machine Translation: Shared Task Papers,0,"We describe the Universitat d{'}Alacant submissions to the word- and sentence-level machine translation (MT) quality estimation (QE) shared task at WMT 2018. Our approach to word-level MT QE builds on previous work to mark the words in the machine-translated sentence as \textit{OK} or \textit{BAD}, and is extended to determine if a word or sequence of words need to be inserted in the gap after each word. Our sentence-level submission simply uses the edit operations predicted by the word-level approach to approximate TER. The method presented ranked first in the sub-task of identifying insertions in gaps for three out of the six datasets, and second in the rest of them."
W16-2383,{UA}lacant word-level and phrase-level machine translation quality estimation systems at {WMT} 2016,2016,6,2,2,0.892623,5040,miquel esplagomis,"Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",0,"This paper describes the Universitat dxe2x80x99Alacant submissions (labeled as UAlacant) to the machine translation quality estimation (MTQE) shared task at WMT 2016, where we have participated in the word-level and phrase-level MTQE subtasks. Our systems use external sources of bilingual information as a black box to spot sub-segment correspondences between the source segment and the translation hypothesis. For our submissions, two sources of bilingual information have been used: machine translation (Lucy LT KWIK Translator and Google Translate) and the bilingual concordancer Reverso Context. Building upon the word-level approach implemented for WMT 2015, a method for phrase-based MTQE is proposed which builds on the probabilities obtained for word-level MTQE. For each sub-task we have submitted two systems: one using the features produced exclusively based on online sources of bilingual information, and one combining them with the baseline features provided by the organisers of the task."
2016.amta-researchers.3,Fuzzy-match repair using black-box machine translation systems: what can be expected?,2016,-1,-1,2,1,5074,john ortega,Conferences of the Association for Machine Translation in the Americas: MT Researchers' Track,0,"Computer-aided translation (CAT) tools often use a translation memory (TM) as the key resource to assist translators. A TM contains translation units (TU) which are made up of source and target language segments; translators use the target segments in the TU suggested by the CAT tool by converting them into the desired translation. Proposals from TMs could be made more useful by using techniques such as fuzzy-match repair (FMR) which modify words in the target segment corresponding to mismatches identified in the source segment. Modifications in the target segment are done by translating the mismatched source sub-segments using an external source of bilingual information (SBI) and applying the translations to the corresponding positions in the target segment. Several combinations of translated sub-segments can be applied to the target segment which can produce multiple repair candidates. We provide a formal algorithmic description of a method that is capable of using any SBI to generate all possible fuzzy-match repairs and perform an oracle evaluation on three different language pairs to ascertain the potential of the method to improve translation productivity. Using DGT-TM translation memories and the machine system Apertium as the single source to build repair operators in three different language pairs, we show that the best repaired fuzzy matches are consistently closer to reference translations than either machine-translated segments or unrepaired fuzzy matches."
W15-4903,Using on-line available sources of bilingual information for word-level machine translation quality estimation,2015,20,8,2,1,5040,miquel esplagomis,Proceedings of the 18th Annual Conference of the {E}uropean Association for Machine Translation,0,"This paper explores the use of external sources of bilingual information available on-line for word-level machine translation quality estimation (MTQE). These sources of bilingual information are used as a black box to spot sub-segment correspondences between a source-language (SL) sentence S to be translated and a given translation hypothesis T in the target-language (TL). This is done by segmenting both S and T into overlapping sub-segments of variable length and translating them into the TL and the SL, respectively, using the available bilingual sources of information on the fly. A collection of features is then obtained from the resulting sub-segment translations, which is used by a binary classifier to determine which target words in T need to be post-edited. Experiments are conducted based on the data sets published for the word-level MTQE task in the 2014 edition of the Workshop on Statistical Machine Translation (WMT 2014). The sources of bilingual information used are: machine translation (Apertium and Google Translate) and the bilingual concordancer Reverso Context. The results obtained confirm that, using less information and fewer features, our approach obtains results comparable to those of state-of-the-art approaches, and even outperform them in some data sets. c xc2xa9 2015 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND."
W15-4904,A general framework for minimizing translation effort: towards a principled combination of translation technologies in computer-aided translation,2015,-1,-1,2,0,5037,mikel forcada,Proceedings of the 18th Annual Conference of the {E}uropean Association for Machine Translation,0,None
W15-4919,Unsupervised training of maximum-entropy models for lexical selection in rule-based machine translation,2015,21,2,2,0.539042,1394,francis tyers,Proceedings of the 18th Annual Conference of the {E}uropean Association for Machine Translation,0,"This article presents a method of training maximum-entropy models to perform lexical selection in a rule-based machine translation system. The training method described is unsupervised; that is, it does not require any annotated corpus. The method uses source-language monolingual corpora, the machine translation (MT) system in which the models are integrated, and a statistical target-language model. Using the MT system, the sentences in the sourcelanguage corpus are translated in all possible ways according to the different translation equivalents in the bilingual dictionary of the system. These translations are then scored on the target-language model and the scores are normalised to provide fractional counts for training source-language maximum-entropy lexical-selection models. We show that these models can perform equally well, or better, than using the target-language model directly for lexical selection, at a substantially reduced computational cost."
W15-3036,{UA}lacant word-level machine translation quality estimation system at {WMT} 2015,2015,8,9,2,1,5040,miquel esplagomis,Proceedings of the Tenth Workshop on Statistical Machine Translation,0,"This paper describes the Universitat dxe2x80x99Alacant submissions (labelled as UAlacant) for the machine translation quality estimation (MTQE) shared task in WMT 2015, where we participated in the wordlevel MTQE sub-task. The method we used to produce our submissions uses external sources of bilingual information as a black box to spot sub-segment correspondences between a source segmentS and the translation hypothesisT produced by a machine translation system. This is done by segmenting bothS andT into overlapping subsegments of variable length and translating them in both translation directions, using the available sources of bilingual information on the fly. For our submissions, two sources of bilingual information were used: machine translation (Apertium and Google Translate) and the bilingual concordancer Reverso Context. After obtaining the subsegment correspondences, a collection of features is extracted from them, which are then used by a binary classifer to obtain the final xe2x80x9cGOODxe2x80x9d or xe2x80x9cBADxe2x80x9d word-level quality labels. We prepared two submissions for this yearxe2x80x99s edition of WMT 2015: one using the features produced by our system, and one combining them with the baseline features published by the organisers of the task, which were ranked third and first for the sub-task, respectively."
2015.eamt-1.4,Using on-line available sources of bilingual information for word-level machine translation quality estimation,2015,20,8,2,1,5040,miquel esplagomis,Proceedings of the 18th Annual Conference of the European Association for Machine Translation,0,"This paper explores the use of external sources of bilingual information available on-line for word-level machine translation quality estimation (MTQE). These sources of bilingual information are used as a black box to spot sub-segment correspondences between a source-language (SL) sentence S to be translated and a given translation hypothesis T in the target-language (TL). This is done by segmenting both S and T into overlapping sub-segments of variable length and translating them into the TL and the SL, respectively, using the available bilingual sources of information on the fly. A collection of features is then obtained from the resulting sub-segment translations, which is used by a binary classifier to determine which target words in T need to be post-edited. Experiments are conducted based on the data sets published for the word-level MTQE task in the 2014 edition of the Workshop on Statistical Machine Translation (WMT 2014). The sources of bilingual information used are: machine translation (Apertium and Google Translate) and the bilingual concordancer Reverso Context. The results obtained confirm that, using less information and fewer features, our approach obtains results comparable to those of state-of-the-art approaches, and even outperform them in some data sets. c xc2xa9 2015 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND."
2015.eamt-1.5,A general framework for minimizing translation effort: towards a principled combination of translation technologies in computer-aided translation,2015,-1,-1,2,0,5037,mikel forcada,Proceedings of the 18th Annual Conference of the European Association for Machine Translation,0,None
2015.eamt-1.20,Unsupervised training of maximum-entropy models for lexical selection i in rule-based machine translation,2015,21,2,2,0.539042,1394,francis tyers,Proceedings of the 18th Annual Conference of the European Association for Machine Translation,0,"This article presents a method of training maximum-entropy models to perform lexical selection in a rule-based machine translation system. The training method described is unsupervised; that is, it does not require any annotated corpus. The method uses source-language monolingual corpora, the machine translation (MT) system in which the models are integrated, and a statistical target-language model. Using the MT system, the sentences in the sourcelanguage corpus are translated in all possible ways according to the different translation equivalents in the bilingual dictionary of the system. These translations are then scored on the target-language model and the scores are normalised to provide fractional counts for training source-language maximum-entropy lexical-selection models. We show that these models can perform equally well, or better, than using the target-language model directly for lexical selection, at a substantially reduced computational cost."
W14-3319,{A}bu-{M}a{T}ran at {WMT} 2014 Translation Task: Two-step Data Selection and {RBMT}-Style Synthetic Rules,2014,20,4,7,0,8609,raphael rubino,Proceedings of the Ninth Workshop on Statistical Machine Translation,0,"This paper presents the machine translation systems submitted by the AbuMaTran project to the WMT 2014 translation task. The language pair concerned is Englishxe2x80x90French with a focus on French as the target language. The French to English translation direction is also considered, based on the word alignment computed in the other direction. Large language and translation models are built using all the datasets provided by the shared task organisers, as well as the monolingual data from LDC. To build the translation models, we apply a two-step data selection method based on bilingual crossentropy difference and vocabulary saturation, considering each parallel corpus individually. Synthetic translation rules are extracted from the development sets and used to train another translation model. We then interpolate the translation models, minimising the perplexity on the development sets, to obtain our final SMT system. Our submission for the English to French translation task was ranked second amongst nine teams and a total of twenty submissions."
W14-3320,The {UA}-Prompsit hybrid machine translation system for the 2014 Workshop on Statistical Machine Translation,2014,19,6,3,0.984848,9987,victor sanchezcartagena,Proceedings of the Ninth Workshop on Statistical Machine Translation,0,"This paper describes the system jointly developed by members of the Departament de Llenguatges i Sistemes Inform` atics at Universitat dxe2x80x99Alacant and the Prompsit Language Engineering company for the shared translation task of the 2014 Workshop on Statistical Machine Translation. We present a phrase-based statistical machine translation system whose phrase table is enriched with information obtained from dictionaries and shallowtransfer rules like those used in rule-based machine translation. The novelty of our approach lies in the fact that the transfer rules used were not written by humans, but automatically inferred from a parallel corpus."
2014.eamt-1.4,An efficient method to assist non-expert users in extending dictionaries by assigning stems and inflectional paradigms to unknknown words,2014,18,4,3,1,5040,miquel esplagomis,Proceedings of the 17th Annual conference of the European Association for Machine Translation,0,A method is presented to assist users with no background in linguistics in adding the unknown words in a text to monolingual dictionaries such as those used in rulebased machine translation systems. Adding a word to these dictionaries requires identifying its stem and the inflection paradigm to be used in order to generate all its word forms. Our method is based on a previous interactive approach in which non-expert users were asked to validate whether some tentative word forms were correct forms of the new word; these validations were then used to determine the most appropriate stem and paradigm. The previous approach was based on a set of intuitive heuristics designed both to obtain an estimate of the eligibility of each candidate stem/paradigm combination and to determine the word form to be validated at each step. Our new approach however uses formal models for both tasks (a hidden Markov model to estimate eligibility and a decision tree to select the word form) and achieves significantly better results.
2014.amta-researchers.4,Using any machine translation source for fuzzy-match repair in a computer-aided translation setting,2014,13,1,2,1,5074,john ortega,Proceedings of the 11th Conference of the Association for Machine Translation in the Americas: MT Researchers Track,0,"When a computer-assisted translation (CAT) tool does not find an exact match for the source segment to translate in its translation memory (TM), translators must use fuzzy matches that come from translation units in the translation memory that do not completely match the source segment. We explore the use of a fuzzy-match repair technique called patching to repair translation proposals from a TM in a CAT environment using any available machine translation system, or any external bilingual source, regardless of its internals. Patching attempts to aid CAT tool users by repairing fuzzy matches and proposing improved translations. Our results show that patching improves the quality of translation proposals and reduces the amount of edit operations to perform, especially when a specific set of restrictions is applied."
S12-1065,{UA}lacant: Using Online Machine Translation for Cross-Lingual Textual Entailment,2012,17,8,2,1,5040,miquel esplagomis,"*{SEM} 2012: The First Joint Conference on Lexical and Computational Semantics {--} Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation ({S}em{E}val 2012)",0,"This paper describes a new method for cross-lingual textual entailment (CLTE) detection based on machine translation (MT). We use sub-segment translations from different MT systems available online as a source of cross-lingual knowledge. In this work we describe and evaluate different features derived from these sub-segment translations, which are used by a support vector machine classifier to detect CLTEs. We presented this system to the SemEval 2012 task 8 obtaining an accuracy up to 59.8% on the English-Spanish test set, the second best performing approach in the contest."
2012.eamt-1.54,Flexible finite-state lexical selection for rule-based machine translation,2012,21,13,2,0.539042,1394,francis tyers,Proceedings of the 16th Annual conference of the European Association for Machine Translation,0,"In this paper we describe a module (rule formalism, rule compiler and rule processor) designed to provide flexible support for lexical selection in rule-based machine translation. The motivation and implementation for the system is outlined and an efficient algorithm to compute the best coverage of lexical-selection rules over an ambiguous input sentence is described. We provide a demonstration of the module by learning rules for it on a typical training corpus and evaluating against other possible lexicalselection strategies. The inclusion of the module, along with rules learnt from the parallel corpus provides a small, but consistent and statistically-significant improvement over either using the highest-scoring translation according to a target-language model or using the most frequent aligned translation in the parallel corpus which is also found in the systemxe2x80x99s bilingual dictionaries."
W11-2157,The {U}niversitat d{'}Alacant hybrid machine translation system for {WMT} 2011,2011,19,5,2,0.984848,9987,victor sanchezcartagena,Proceedings of the Sixth Workshop on Statistical Machine Translation,0,"This paper describes the machine translation (MT) system developed by the Transducens Research Group, from Universitat d'Alacant, Spain, for the WMT 2011 shared translation task. We submitted a hybrid system for the Spanish--English language pair consisting of a phrase-based statistical MT system whose phrase table was enriched with bilingual phrase pairs matching transfer rules and dictionary entries from the Apertium shallow-transfer rule-based MT platform. Our hybrid system outperforms, in terms of BLEU, GTM and METEOR, a standard phrase-based statistical MT system trained on the same corpus, and received the second best BLEU score in the automatic evaluation."
R11-1013,Enriching a statistical machine translation system trained on small parallel corpora with rule-based bilingual phrases,2011,15,0,2,0.984848,9987,victor sanchezcartagena,Proceedings of the International Conference Recent Advances in Natural Language Processing 2011,0,"In this paper, we present a new hybridisation approach consisting of enriching the phrase table of a phrase-based statistical machine translation system with bilingual phrase pairs matching structural transfer rules and dictionary entries from a shallowtransfer rule-based machine translation system. We have tested this approach on different small parallel corpora scenarios, where pure statistical machine translation systems suffer from data sparseness. The results obtained show an improvement in translation quality, specially when translating out-of-domain texts that are well covered by the shallow-transfer rule-based machine translation system we have used."
2011.mtsummit-papers.18,Using machine translation in computer-aided translation to suggest the target-side words to change,2011,20,9,2,0.895522,5040,miquel esplagomis,Proceedings of Machine Translation Summit XIII: Papers,0,Work supported by Spanish government through TIN2009-14009-C02-01 project. M.L. Forcadaxe2x80x99s sabbatical stay at Dublin City University was supported by Science Foundation Ireland (SFI) through ETS Walton Award 07/W.1/I1802 and by Universitat dxe2x80x99Alacant (Spain).
2011.mtsummit-papers.64,Integrating shallow-transfer rules into phrase-based statistical machine translation,2011,18,8,2,0.984848,9987,victor sanchezcartagena,Proceedings of Machine Translation Summit XIII: Papers,0,Work funded by the Spanish Ministry of Science and Innovation through project TIN2009-14009-C02-01 and by Generalitat Valenciana through grant ACIF/2010/174 (VALid programme).
2011.eamt-1.13,Using word alignments to assist computer-aided translation users by marking which target-side words to change or keep unedited,2011,17,10,2,0,23594,miquel espla,Proceedings of the 15th Annual conference of the European Association for Machine Translation,0,"This paper explores a new method to improve computer-aided translation (CAT) systems based on translation memory (TM) by using pre-computed word alignments between the source and target segments in the translation units (TUs) of the userxe2x80x99s TM. When a new segment is to be translated by the CAT user, our approach uses the word alignments in the matching TUs to mark the words that should be changed or kept unedited to transform the proposed translation into an adequate translation. In this paper, we evaluate different sets of alignments obtained by using GIZA. Experiments conducted in the translation of Spanish texts into English show that this approach is able to predict which target words have to be changed or kept unedited with an accuracy above 94% for fuzzy-match scores greater or equal to 60%. In an appendix we evaluate our approach when new TUs (not seen during the computation of the word-alignment models) are used."
2011.eamt-1.15,Choosing the best machine translation system to translate a sentence by using only source-language information,2011,33,7,1,1,9988,felipe sanchezmartinez,Proceedings of the 15th Annual conference of the European Association for Machine Translation,0,Work funded by the European Association for Machine Translation through its 2010 sponsorship of activities program and by the Spanish Ministry of Science and Innovation through project TIN2009-14009-C02-01.
2009.freeopmt-1.11,A trigram part-of-speech tagger for the Apertium free/open-source machine translation platform,2009,15,1,2,0,833,zaid sheikh,Proceedings of the First International Workshop on Free/Open-Source Rule-Based Machine Translation,0,"This paper describes the implementation of a second-order hidden Markov model (HMM) based part-of-speech tagger for the Apertium free/opensource rule-based machine translation platform. We describe the part-ofspeech (PoS) tagging approach in Apertium and how it is parametrised through a tagger definition file that defines: (1) the set of tags to be used and (2) constrain rules that can be used to forbid certain PoS tag sequences, thus refining the HMM parameters and increasing its tagging accuracy. The paper also reviews the Baum-Welch algorithm used to estimate the HMM parameters and compares the tagging accuracy achieved with that achieved by the original, first-order HMM-based PoS tagger in Apertium."
2009.eamt-1.20,Marker-Based Filtering of Bilingual Phrase Pairs for {SMT},2009,32,8,1,1,9988,felipe sanchezmartinez,Proceedings of the 13th Annual conference of the European Association for Machine Translation,0,Spanish Ministry of Education and Science (project TIN2006-15071-C03-01). Science Foundation Ireland through grants 05/IN/1732 and 06/RF/CMS064.
2007.tmi-papers.22,Automatic induction of shallow-transfer rules for open-source machine translation,2007,16,9,1,1,9988,felipe sanchezmartinez,Proceedings of the 11th Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages: Papers,0,Spanish Government through projects TIC2003-08681-C02-01 and TIN2006-15071-C03-01. Spanish Government and the European Social Fund through research grant BES-2004-4711.
2005.mtsummit-osmtw.4,An Open-Source Shallow-Transfer Machine Translation Toolbox: Consequences of Its Release and Availability,2005,8,14,9,0,8510,carme armentanooller,Workshop on open-source machine translation,0,"By the time Machine Translation Summit X is held in September 2005, our group will have released an open-source machine translation toolbox as part of a large government-funded project involving four universities and three linguistic technology companies from Spain. The machine translation toolbox, which will most likely be released under a GPL-like license includes (a) the open-source engine itself, a modular shallow-transfer machine translation engine suitable for related languages and largely based upon that of systems we have already developed, such as interNOSTRUM for Spanish{---}Catalan and Traductor Universia for Spanish{---}Portuguese, (b) extensive documentation (including document type declarations) specifying the XML format of all linguistic (dictionaries, rules) and document format management files, (c) compilers converting these data into the high-speed (tens of thousands of words a second) format used by the engine, and (d) pilot linguistic data for Spanish{---}Catalan and Spanish{---}Galician and format management specifications for the HTML, RTF and plain text formats. After describing very briefly this toolbox, this paper aims at exploring possible consequences of the availability of this architecture, including the community-driven development of machine translation systems for languages lacking this kind of linguistic technology."
2005.eamt-1.12,An open-source shallow-transfer machine translation engine for the {R}omance languages of {S}pain,2005,7,44,6,0,51220,antonio corbibellot,Proceedings of the 10th EAMT Conference: Practical applications of machine translation,0,"We present the current status of development of an open-source shallow-transfer machine translation engine for the Romance languages of Spain (the main ones being Spanish, Catalan and Galician) as part of a larger government-funded project which includes non-Romance languages such as Basque and involving both universities and linguistic technology companies. The machine translation architecture uses finite-state transducers for lexical processing, hidden Markov models for part-of-speech tagging, and finite-state based chunking for structural transfer, and is largely based upon that of systems already developed by the Transducens group at the Universitat d'Alacant, such as interNOSTRUM (Spanishxe2x80x94Catalan) and Traductor Universia (Spanishxe2x80x94Portuguese). The possible scope of the project, however, is wider, since it will be possible to use the resulting machine translation system with new pairs of languages; to that end, the project also aims at proposing standard formats to encode the linguistic data needed. This paper briefly describes the machine translation engine, the formats it uses for linguistic data, and the compilers that convert these data into an efficient format used by the engine."
2004.tmi-1.15,Cooperative unsupervised training of the part-of-speech taggers in a bidirectional machine translation system,2004,8,5,1,1,9988,felipe sanchezmartinez,Proceedings of the 10th Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages,0,"When building a machine translation system, the embedded part-of-speech (PoS) tagger deserves special attention, since PoS ambiguities are one of the main sources of mistranslations, specially when related languages are involved. The standard statistical approach for PoS tagging are hidden Markov models (HMM) properly trained by collecting statistics from source-language texts. In the case of bidirectional machine translation systems, this kind of training is often individually performed on each PoS tagger without taking into account the other language, that is, the corresponding target language. But target-language information may help to improve performance. In this paper, a new method is proposed which trains both PoS taggers simultaneously using mutual interaction: at every iteration, the parameters of the HMM corresponding to one of the languages are refined by using the statistical data supplied by the current HMM for the other language. Both models bootstrap by learning cooperatively in an unsupervised manner and require only monolingual texts; no aligned texts are needed. Preliminary results are promising and surpass those of traditional unsupervised approaches."
