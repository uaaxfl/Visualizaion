2020.acl-main.697,P16-1068,0,0.0234579,"word choices than a writer who uses a more limited vocabulary. Attempts to capture topicality (Beigman Klebanov et al., 2016b) or development 7800 (Beigman Klebanov and Flor, 2013b; Somasundaran et al., 2016) through properties of vocabulary distribution without human annotation of topicality and development exemplify such approaches. 3.2.2 Model Interpretability Recent research has shown that more sophisticated machine learning models might perform better than simple regression-based models when it comes to predictive accuracy (Chen and He, 2013; Cummins et al., 2016; Taghipour and Ng, 2016; Alikaniotis et al., 2016; Dong et al., 2017; Dasgupta et al., 2018; Jin et al., 2018). However, unlike linear regression where stakeholders can understand how much each feature used in the model contributed to the predicted score, many of the more complex models are essentially “black boxes” and do not really lend themselves to post-hoc interpretability (Lipton, 2016). Although interpretability is an active area of research in the machine learning literature (Ribeiro et al., 2016; Koh and Liang, 2017; Doshi-Velez and Kim, 2017), it currently lags behind the research on machine learning methods. For this reason, some"
2020.acl-main.697,E17-4010,0,0.0158167,"dialect may not work for another. This consideration does not appear in Page (1966), yet it is an active line of subsequent work. While most of the research we cited so far has been on English, various aspects of writing evaluation, e.g., annotation, detection of various types of errors, and building AWE systems, have been researched for a variety of languages: Song et al. (2016), Rao et al. (2017), Shiue et al. (2017) worked with data in Chinese, 15 Writing Pal does not provide specific grammar and spelling feedback. 7798 Lorenzen et al. (2019) in Danish, Berggren et al. (2019) in Norwegian, Amorim and Veloso (2017) in Portuguese, Stymne et al. (2017) in Swedish, Berkling (2018) and Weiss and Meurers (2019) in German, Mezher and Omar (2016) in Arabic, Kakkonen et al. (2005) in Finnish, Loraksa and Peachavanish (2007) in Thai, Lemaire and Dessus (2001) in French, and Ishioka and Kameda (2006) in Japanese. The list is by no means exhaustive; see Flor and Cahill (2020) for a recent review. these true characteristics.” Page seems to do away rather quickly with trying to measure the actual thing – the set of all and only “true characteristics of essays”, or trins. Why is that? He explains: Notwithstanding the"
2020.acl-main.697,W17-5110,0,0.0886908,"Missing"
2020.acl-main.697,Q13-1028,0,0.0292127,"phor use (proportion of metaphorically used words in an essay) correlates with essay quality; Littlemore et al. (2014) likewise found that more skilled writers use metaphor more often. Song et al. (2016) observed a positive correlation between use of parallelism – syntactically similar and semantically related constructors, often used for emphasis or to enhance memorability – in student essays. Some pioneering work has been done on comparing writing that is recognized as outstanding (through receiving prestigious prizes) vs writing that is “merely” good in the domain of scientific journalism (Louis and Nenkova, 2013). Once various indicators of originality can be successfully measured, additional work may be necessary to incorporate these measurements into scoring ecosystems since such indicators may only occur infrequently. One way to achieve this would be to compute a “macro” feature that aggregates multiple such indicators, another would be to direct such essays to a human rater for review. 2.2.2 Gaming Won’t this grading system be easy to con? Can’t the shrewd student just put in the proxies which will get a good grade? (Page, 1966) Certainly, students can and do employ gaming strategies to discover a"
2020.acl-main.697,W19-4401,1,0.835194,"nt testtakers at test time. The educational measurement community has long been studying fairness in automated scoring (Williamson et al., 2012; Ramineni and Williamson, 2013; AERA, 2014) and recent progress made by the NLP community towards enhancing the usual accuracy-based evaluations with some of these psychometric analyses – from computing indicators of potential biases in automatic scores across various demographic sub-groups to computing new metrics that incorporate measurement theory to produce more reliable indicators of system performance – is quite promising (Madnani et al., 2017b; Loukina et al., 2019). 3.4 Pervasiveness of Technology Page’s gedankenexperiment on the potential of automated essay evaluation in a classroom context no doubt appeared audacious in 1966 but nothing back then could have prepared his readers to the pervasiveness of technology we are experiencing today. Today you can very literally carry your AWE system in your pocket; you can even carry several. You can use them (almost) at any time and at any place – not only in classrooms, but at home, at work, and even while texting with a friend. This is perhaps the biggest issue that Page’s vision did not address: the possibil"
2020.acl-main.697,W13-1722,1,0.818042,"Missing"
2020.acl-main.697,W16-0524,1,0.853421,"ent We are talking awfully casually about grading subject matter like history. Isn’t this a wholly different sort of problem? Aren’t we supposed to see that what the students are saying makes sense, above and beyond their using commas in the right places? (Page, 1966) Indeed, work has been done over the last decade on automated evaluation of written responses for their content and not their general writing quality (Sukkarieh and Bolge, 2008; Mohler et al., 2011; Ziai et al., 2012; Basu et al., 2013; Madnani et al., 2013; Ramachandran et al., 2015; Burrows et al., 2015; Sakaguchi et al., 2015; Madnani et al., 2016; Padó, 2016; Madnani et al., 2017a; Riordan et al., 2017; Kumar et al., 2017; Horbach et al., 2018; Riordan et al., 2019). Scoring for content focuses primarily on what students know, have learned, or can do in a specific subject area such as Computer Science, Biology, or Music, with the fluency of the response being secondary. For example, some spelling or grammar errors are acceptable as long as the desired specific information (e.g., scientific principles, trends in a graph, or details from a reading passage) is included in the response. Note that most current content scoring systems ascer"
2020.acl-main.697,W17-5052,1,0.919248,"y about grading subject matter like history. Isn’t this a wholly different sort of problem? Aren’t we supposed to see that what the students are saying makes sense, above and beyond their using commas in the right places? (Page, 1966) Indeed, work has been done over the last decade on automated evaluation of written responses for their content and not their general writing quality (Sukkarieh and Bolge, 2008; Mohler et al., 2011; Ziai et al., 2012; Basu et al., 2013; Madnani et al., 2013; Ramachandran et al., 2015; Burrows et al., 2015; Sakaguchi et al., 2015; Madnani et al., 2016; Padó, 2016; Madnani et al., 2017a; Riordan et al., 2017; Kumar et al., 2017; Horbach et al., 2018; Riordan et al., 2019). Scoring for content focuses primarily on what students know, have learned, or can do in a specific subject area such as Computer Science, Biology, or Music, with the fluency of the response being secondary. For example, some spelling or grammar errors are acceptable as long as the desired specific information (e.g., scientific principles, trends in a graph, or details from a reading passage) is included in the response. Note that most current content scoring systems ascertain the “correctness&quot; of a respon"
2020.acl-main.697,W17-1605,1,0.884257,"Missing"
2020.acl-main.697,W19-4446,0,0.0293242,"with a new annual ACM conference on Fairness, Accountability, and Transparency having been inaugurated in 201817 and relevant research appearing at many impactful publication venues, such as Science (Caliskan et al., 2017), NIPS (Pleiss et al., 2017; Kim et al., 2018), ICML (Kearns et al., 2018), ACL (Hovy and Spruit, 2016; Sun et al., 2019; Sap et al., 2019), KDD (Speicher et al., 2018), AAAI (Zhang and Bareinboim, 2018), and others (Dwork et al., 2012; Hajian and Domingo-Ferrer, 2013). There is also recent work that examines fairness and ethical considerations when using AI in an education (Mayfield et al., 2019; Gardner et al., 2019). In the context of assessment, fairness considerations dictate that the test reflects the same construct(s) for the entire test taking population, that 17 https://facctconference.org/ scores from the test have the same meaning for all the test taking population, and that a fair test does not offer undue advantages (or disadvantages) to some individuals because of their characteristics – such as those associated with race, ethnicity, gender, age, socioeconomic status, or linguistic or cultural background – or the test characteristics itself, e.g., the different prompts s"
2020.acl-main.697,P11-1076,0,0.0122347,"ndardized testing (§3.2). This is one of the reasons standardized tests are often not scored solely by an AWE system but also by a human rater. 2.2.3 Content We are talking awfully casually about grading subject matter like history. Isn’t this a wholly different sort of problem? Aren’t we supposed to see that what the students are saying makes sense, above and beyond their using commas in the right places? (Page, 1966) Indeed, work has been done over the last decade on automated evaluation of written responses for their content and not their general writing quality (Sukkarieh and Bolge, 2008; Mohler et al., 2011; Ziai et al., 2012; Basu et al., 2013; Madnani et al., 2013; Ramachandran et al., 2015; Burrows et al., 2015; Sakaguchi et al., 2015; Madnani et al., 2016; Padó, 2016; Madnani et al., 2017a; Riordan et al., 2017; Kumar et al., 2017; Horbach et al., 2018; Riordan et al., 2019). Scoring for content focuses primarily on what students know, have learned, or can do in a specific subject area such as Computer Science, Biology, or Music, with the fluency of the response being secondary. For example, some spelling or grammar errors are acceptable as long as the desired specific information (e.g., sci"
2020.acl-main.697,C16-1206,0,0.0117805,"ully casually about grading subject matter like history. Isn’t this a wholly different sort of problem? Aren’t we supposed to see that what the students are saying makes sense, above and beyond their using commas in the right places? (Page, 1966) Indeed, work has been done over the last decade on automated evaluation of written responses for their content and not their general writing quality (Sukkarieh and Bolge, 2008; Mohler et al., 2011; Ziai et al., 2012; Basu et al., 2013; Madnani et al., 2013; Ramachandran et al., 2015; Burrows et al., 2015; Sakaguchi et al., 2015; Madnani et al., 2016; Padó, 2016; Madnani et al., 2017a; Riordan et al., 2017; Kumar et al., 2017; Horbach et al., 2018; Riordan et al., 2019). Scoring for content focuses primarily on what students know, have learned, or can do in a specific subject area such as Computer Science, Biology, or Music, with the fluency of the response being secondary. For example, some spelling or grammar errors are acceptable as long as the desired specific information (e.g., scientific principles, trends in a graph, or details from a reading passage) is included in the response. Note that most current content scoring systems ascertain the “co"
2020.acl-main.697,P13-1026,0,0.0240454,"rmation relevant to that particular aspect of essay quality. This would be a mix between what Page called a prox and a trin, in that a particular, intrinsically interesting, aspect of an essay can be identified reliably by humans, and an automated system can learn how to approximate that particular construct. Such approaches have been developed for organization (well-organized) (Burstein et al., 2003), coherence (well-focused, conveys ideas fluently) (Burstein et al., 2010; Somasundaran et al., 2014), grammaticality (facility with conventions) (Heilman et al., 2014), thesis clarity (clarity) (Persing and Ng, 2013) as well as aspects of scoring rubrics that are more task-specific, e.g., argumentation (clear position, with compelling reasons) (Stab and Gurevych, 2014; Ghosh et al., 2016; Beigman Klebanov et al., 2017; Stab and Gurevych, 2017; Carlile et al., 2018), use of evidence in the context of source-based writing (Rahimi et al., 2017). Finally, for some rubric items, it is not clear exactly how to reliably translate the relevant aspect of the writing construct into annotations guidelines, and so proxes might be employed. For example, consider Page’s argument for capturing “diction” (appropriate wor"
2020.acl-main.697,W15-0612,0,0.0253389,"not scored solely by an AWE system but also by a human rater. 2.2.3 Content We are talking awfully casually about grading subject matter like history. Isn’t this a wholly different sort of problem? Aren’t we supposed to see that what the students are saying makes sense, above and beyond their using commas in the right places? (Page, 1966) Indeed, work has been done over the last decade on automated evaluation of written responses for their content and not their general writing quality (Sukkarieh and Bolge, 2008; Mohler et al., 2011; Ziai et al., 2012; Basu et al., 2013; Madnani et al., 2013; Ramachandran et al., 2015; Burrows et al., 2015; Sakaguchi et al., 2015; Madnani et al., 2016; Padó, 2016; Madnani et al., 2017a; Riordan et al., 2017; Kumar et al., 2017; Horbach et al., 2018; Riordan et al., 2019). Scoring for content focuses primarily on what students know, have learned, or can do in a specific subject area such as Computer Science, Biology, or Music, with the fluency of the response being secondary. For example, some spelling or grammar errors are acceptable as long as the desired specific information (e.g., scientific principles, trends in a graph, or details from a reading passage) is included i"
2020.acl-main.697,I17-4001,0,0.0281998,"sessing writing in multiple languages In order to advance the work on understanding and assessing writing quality, there is clearly a need for a multi-lingual perspective, since methods developed for one language or dialect may not work for another. This consideration does not appear in Page (1966), yet it is an active line of subsequent work. While most of the research we cited so far has been on English, various aspects of writing evaluation, e.g., annotation, detection of various types of errors, and building AWE systems, have been researched for a variety of languages: Song et al. (2016), Rao et al. (2017), Shiue et al. (2017) worked with data in Chinese, 15 Writing Pal does not provide specific grammar and spelling feedback. 7798 Lorenzen et al. (2019) in Danish, Berggren et al. (2019) in Norwegian, Amorim and Veloso (2017) in Portuguese, Stymne et al. (2017) in Swedish, Berkling (2018) and Weiss and Meurers (2019) in German, Mezher and Omar (2016) in Arabic, Kakkonen et al. (2005) in Finnish, Loraksa and Peachavanish (2007) in Thai, Lemaire and Dessus (2001) in French, and Ishioka and Kameda (2006) in Japanese. The list is by no means exhaustive; see Flor and Cahill (2020) for a recent review"
2020.acl-main.697,N18-1012,0,0.0226197,"4 typically go beyond grammar 5 https://mentormywriting.org/ http://www.adaptiveliteracy.com/writing-pal 7 http://www.ets.org/criterion 8 https://www.grammarly.com/ 9 https://writeandimprove.com/ 10 https://www.gingersoftware.com/essay-checker 11 https://www.turnitin.com/products/ revision-assistant 12 http://www.vantagelearning.com/products/ my-access-school-edition/ 13 https://www.pearsonmylabandmastering.com 14 http://wtl.pearsonkt.com 6 and spelling.15 Such tools provide feedback on discourse structure (Criterion), topic development and coherence (Writing Mentor), tone (Writing Assistant, Rao and Tetreault (2018)), thesis relevance (Writing Pal), sentence “spicing” through suggestions of synonyms and idioms (Ginger’s Sentence Rephraser), and style & argumentationrelated feedback (Revision Assistant). Can we then put a green check-mark against Page’s agenda for automated feedback, which “may magnify and disseminate the best human capacities to criticize, evaluate, and correct”? Alas, not yet; research on effectiveness of automated feedback on writing is inconclusive (Englert et al., 2007; Shermis et al., 2008; Grimes and Warschauer, 2010; Choi, 2010; Roscoe and McNamara, 2013; Wilson and Czik, 2016; Wi"
2020.acl-main.697,D15-1050,0,0.0194271,"o the human and which part is due to the machine – perhaps the machine only corrected misspellings, or suggested improvements for the human to vet, or maybe the human only contributed the very first ideation, and the machine has done the rest. Perhaps all the human writer contributed was the thesis (‘I think school should start at 8 rather than 7’) and then clicked ‘submit’ to get back an essay making a cogent and convincing case in support of the thesis. Mining large textual databases for arguments and evaluating them are feasible today as recently demonstrated by IBM’s Debater technology18 (Rinott et al., 2015; Levy et al., 2017; Gretz et al., 2019); introduce some figuration to 18 https://www.research.ibm.com/ artificial-intelligence/project-debater/ make it more appealing (Veale et al., 2017; Veale, 2018) and storify it (Riegl and Veale, 2018; Radford et al., 2019), et voilà! This type of use is essentially a machine’s augmentation of human ability, and is hinted at, for example, in a customer testimonial for Grammarly: “Grammarly allows me to get those communications out and feel confident that I’m putting my best foot forward. Grammarly is like a little superpower, especially when I need to be"
2020.acl-main.697,W19-4411,0,0.0129014,"lem? Aren’t we supposed to see that what the students are saying makes sense, above and beyond their using commas in the right places? (Page, 1966) Indeed, work has been done over the last decade on automated evaluation of written responses for their content and not their general writing quality (Sukkarieh and Bolge, 2008; Mohler et al., 2011; Ziai et al., 2012; Basu et al., 2013; Madnani et al., 2013; Ramachandran et al., 2015; Burrows et al., 2015; Sakaguchi et al., 2015; Madnani et al., 2016; Padó, 2016; Madnani et al., 2017a; Riordan et al., 2017; Kumar et al., 2017; Horbach et al., 2018; Riordan et al., 2019). Scoring for content focuses primarily on what students know, have learned, or can do in a specific subject area such as Computer Science, Biology, or Music, with the fluency of the response being secondary. For example, some spelling or grammar errors are acceptable as long as the desired specific information (e.g., scientific principles, trends in a graph, or details from a reading passage) is included in the response. Note that most current content scoring systems ascertain the “correctness&quot; of a response based on its similarity to other responses that humans have deemed to be correct or,"
2020.acl-main.697,W17-5017,0,0.012585,"matter like history. Isn’t this a wholly different sort of problem? Aren’t we supposed to see that what the students are saying makes sense, above and beyond their using commas in the right places? (Page, 1966) Indeed, work has been done over the last decade on automated evaluation of written responses for their content and not their general writing quality (Sukkarieh and Bolge, 2008; Mohler et al., 2011; Ziai et al., 2012; Basu et al., 2013; Madnani et al., 2013; Ramachandran et al., 2015; Burrows et al., 2015; Sakaguchi et al., 2015; Madnani et al., 2016; Padó, 2016; Madnani et al., 2017a; Riordan et al., 2017; Kumar et al., 2017; Horbach et al., 2018; Riordan et al., 2019). Scoring for content focuses primarily on what students know, have learned, or can do in a specific subject area such as Computer Science, Biology, or Music, with the fluency of the response being secondary. For example, some spelling or grammar errors are acceptable as long as the desired specific information (e.g., scientific principles, trends in a graph, or details from a reading passage) is included in the response. Note that most current content scoring systems ascertain the “correctness&quot; of a response based on its similar"
2020.acl-main.697,W10-1004,0,0.0270171,"sts, developers of a scoring engine often need to provide a construct validity argument in order to show that what the system is measuring is actually aligned with the “writing construct” – the actual set of writing skills that the test is supposed to measure. 16 https://www.ets.org/gre/revised_general/ prepare/analytical_writing/issue/scoring_guide Some of the items in a human-oriented scoring rubrics are amenable to reasonably direct implementation, often with the help of human-annotated gold standard data such as misspellings (Flor, 2012; Flor and Futagi, 2013) and specific grammar errors (Rozovskaya and Roth, 2010; Leacock et al., 2014). It might be the case that the system would miss some grammar errors and declare an error where there is none, but a grammar assessment system can be built for identifying specific, observable instances of errors that a human reader focused on Mechanics would likely pick upon. For other items in a rubric, one might need to drill down, articulate a reliable guideline for humans to assess that particular aspect of the essay, annotate a substantial enough number of essays using the guidelines to make machine learning possible, and then find automatically measurable propert"
2020.acl-main.697,N15-1111,1,0.825608,"human rater. 2.2.3 Content We are talking awfully casually about grading subject matter like history. Isn’t this a wholly different sort of problem? Aren’t we supposed to see that what the students are saying makes sense, above and beyond their using commas in the right places? (Page, 1966) Indeed, work has been done over the last decade on automated evaluation of written responses for their content and not their general writing quality (Sukkarieh and Bolge, 2008; Mohler et al., 2011; Ziai et al., 2012; Basu et al., 2013; Madnani et al., 2013; Ramachandran et al., 2015; Burrows et al., 2015; Sakaguchi et al., 2015; Madnani et al., 2016; Padó, 2016; Madnani et al., 2017a; Riordan et al., 2017; Kumar et al., 2017; Horbach et al., 2018; Riordan et al., 2019). Scoring for content focuses primarily on what students know, have learned, or can do in a specific subject area such as Computer Science, Biology, or Music, with the fluency of the response being secondary. For example, some spelling or grammar errors are acceptable as long as the desired specific information (e.g., scientific principles, trends in a graph, or details from a reading passage) is included in the response. Note that most current content"
2020.acl-main.697,P19-1163,0,0.0129424,"use some variant of least squares linear regression as the machine learning model to predict test taker scores. 3.3 Increased Attention to Fairness It would probably not be an overstatement to say that fairness in AI is quickly becoming its own sub-field, with a new annual ACM conference on Fairness, Accountability, and Transparency having been inaugurated in 201817 and relevant research appearing at many impactful publication venues, such as Science (Caliskan et al., 2017), NIPS (Pleiss et al., 2017; Kim et al., 2018), ICML (Kearns et al., 2018), ACL (Hovy and Spruit, 2016; Sun et al., 2019; Sap et al., 2019), KDD (Speicher et al., 2018), AAAI (Zhang and Bareinboim, 2018), and others (Dwork et al., 2012; Hajian and Domingo-Ferrer, 2013). There is also recent work that examines fairness and ethical considerations when using AI in an education (Mayfield et al., 2019; Gardner et al., 2019). In the context of assessment, fairness considerations dictate that the test reflects the same construct(s) for the entire test taking population, that 17 https://facctconference.org/ scores from the test have the same meaning for all the test taking population, and that a fair test does not offer undue advantages"
2020.acl-main.697,P17-2064,0,0.0220375,"multiple languages In order to advance the work on understanding and assessing writing quality, there is clearly a need for a multi-lingual perspective, since methods developed for one language or dialect may not work for another. This consideration does not appear in Page (1966), yet it is an active line of subsequent work. While most of the research we cited so far has been on English, various aspects of writing evaluation, e.g., annotation, detection of various types of errors, and building AWE systems, have been researched for a variety of languages: Song et al. (2016), Rao et al. (2017), Shiue et al. (2017) worked with data in Chinese, 15 Writing Pal does not provide specific grammar and spelling feedback. 7798 Lorenzen et al. (2019) in Danish, Berggren et al. (2019) in Norwegian, Amorim and Veloso (2017) in Portuguese, Stymne et al. (2017) in Swedish, Berkling (2018) and Weiss and Meurers (2019) in German, Mezher and Omar (2016) in Arabic, Kakkonen et al. (2005) in Finnish, Loraksa and Peachavanish (2007) in Thai, Lemaire and Dessus (2001) in French, and Ishioka and Kameda (2006) in Japanese. The list is by no means exhaustive; see Flor and Cahill (2020) for a recent review. these true characte"
2020.acl-main.697,C14-1090,0,0.0296825,"es to make machine learning possible, and then find automatically measurable properties of essays that would provide information relevant to that particular aspect of essay quality. This would be a mix between what Page called a prox and a trin, in that a particular, intrinsically interesting, aspect of an essay can be identified reliably by humans, and an automated system can learn how to approximate that particular construct. Such approaches have been developed for organization (well-organized) (Burstein et al., 2003), coherence (well-focused, conveys ideas fluently) (Burstein et al., 2010; Somasundaran et al., 2014), grammaticality (facility with conventions) (Heilman et al., 2014), thesis clarity (clarity) (Persing and Ng, 2013) as well as aspects of scoring rubrics that are more task-specific, e.g., argumentation (clear position, with compelling reasons) (Stab and Gurevych, 2014; Ghosh et al., 2016; Beigman Klebanov et al., 2017; Stab and Gurevych, 2017; Carlile et al., 2018), use of evidence in the context of source-based writing (Rahimi et al., 2017). Finally, for some rubric items, it is not clear exactly how to reliably translate the relevant aspect of the writing construct into annotations guideli"
2020.acl-main.697,Q18-1007,0,0.0177326,"of a response based on its similarity to other responses that humans have deemed to be correct or, at least, high-scoring; they do not employ explicit fact-checking or reasoning for this purpose. Concerns about specific content extends to other cases where the scoring system needs to pay 7797 attention to details of genre and task – not all essays are five-paragraph persuasive essays; the specific task might require assessing whether the student has appropriately used specific source materials (Beigman Klebanov et al., 2014; Rahimi et al., 2017; Zhang and Litman, 2018) or assessing narrative (Somasundaran et al., 2018) or reflective (Beigman Klebanov et al., 2016a; Luo and Litman, 2016), rather than persuasive, writing. 2.2.4 Feedback Page emphasized the importance of feedback, and considered the following to be “the sort of feedback that can almost be programmed right now” (original italics): John [. . . ], please correct the following misspellings: believe, receive. Note the ie/ei problem. You overuse the words interesting, good, nice; then was repeated six times. Check trite expressions. All of your sentences are of the subject-verb variety and all are declarative. Reconstruct. Check subject-verb agreeme"
2020.acl-main.697,C16-1148,0,0.0259396,"for some rubric items, it is not clear exactly how to reliably translate the relevant aspect of the writing construct into annotations guidelines, and so proxes might be employed. For example, consider Page’s argument for capturing “diction” (appropriate word choice) through word frequency – a writer who can use many different words, including rarer and often semantically nuanced ones, is likelier to make precise word choices than a writer who uses a more limited vocabulary. Attempts to capture topicality (Beigman Klebanov et al., 2016b) or development 7800 (Beigman Klebanov and Flor, 2013b; Somasundaran et al., 2016) through properties of vocabulary distribution without human annotation of topicality and development exemplify such approaches. 3.2.2 Model Interpretability Recent research has shown that more sophisticated machine learning models might perform better than simple regression-based models when it comes to predictive accuracy (Chen and He, 2013; Cummins et al., 2016; Taghipour and Ng, 2016; Alikaniotis et al., 2016; Dong et al., 2017; Dasgupta et al., 2018; Jin et al., 2018). However, unlike linear regression where stakeholders can understand how much each feature used in the model contributed t"
2020.acl-main.697,C16-1076,0,0.0995089,"system; finding such aspects, as well as measuring them, is still work in progress. While no current operational scoring system we are aware of is specifically looking for originality, research into aspects of writing that are often considered original is taking place. For example, using data from different tests, Beigman Klebanov and Flor (2013a) and Beigman Klebanov et al. (2018) found that the extent of metaphor use (proportion of metaphorically used words in an essay) correlates with essay quality; Littlemore et al. (2014) likewise found that more skilled writers use metaphor more often. Song et al. (2016) observed a positive correlation between use of parallelism – syntactically similar and semantically related constructors, often used for emphasis or to enhance memorability – in student essays. Some pioneering work has been done on comparing writing that is recognized as outstanding (through receiving prestigious prizes) vs writing that is “merely” good in the domain of scientific journalism (Louis and Nenkova, 2013). Once various indicators of originality can be successfully measured, additional work may be necessary to incorporate these measurements into scoring ecosystems since such indica"
2020.acl-main.697,E17-1092,0,0.0159877,"s, and an automated system can learn how to approximate that particular construct. Such approaches have been developed for organization (well-organized) (Burstein et al., 2003), coherence (well-focused, conveys ideas fluently) (Burstein et al., 2010; Somasundaran et al., 2014), grammaticality (facility with conventions) (Heilman et al., 2014), thesis clarity (clarity) (Persing and Ng, 2013) as well as aspects of scoring rubrics that are more task-specific, e.g., argumentation (clear position, with compelling reasons) (Stab and Gurevych, 2014; Ghosh et al., 2016; Beigman Klebanov et al., 2017; Stab and Gurevych, 2017; Carlile et al., 2018), use of evidence in the context of source-based writing (Rahimi et al., 2017). Finally, for some rubric items, it is not clear exactly how to reliably translate the relevant aspect of the writing construct into annotations guidelines, and so proxes might be employed. For example, consider Page’s argument for capturing “diction” (appropriate word choice) through word frequency – a writer who can use many different words, including rarer and often semantically nuanced ones, is likelier to make precise word choices than a writer who uses a more limited vocabulary. Attempts"
2020.acl-main.697,W17-0306,0,0.0211933,"onsideration does not appear in Page (1966), yet it is an active line of subsequent work. While most of the research we cited so far has been on English, various aspects of writing evaluation, e.g., annotation, detection of various types of errors, and building AWE systems, have been researched for a variety of languages: Song et al. (2016), Rao et al. (2017), Shiue et al. (2017) worked with data in Chinese, 15 Writing Pal does not provide specific grammar and spelling feedback. 7798 Lorenzen et al. (2019) in Danish, Berggren et al. (2019) in Norwegian, Amorim and Veloso (2017) in Portuguese, Stymne et al. (2017) in Swedish, Berkling (2018) and Weiss and Meurers (2019) in German, Mezher and Omar (2016) in Arabic, Kakkonen et al. (2005) in Finnish, Loraksa and Peachavanish (2007) in Thai, Lemaire and Dessus (2001) in French, and Ishioka and Kameda (2006) in Japanese. The list is by no means exhaustive; see Flor and Cahill (2020) for a recent review. these true characteristics.” Page seems to do away rather quickly with trying to measure the actual thing – the set of all and only “true characteristics of essays”, or trins. Why is that? He explains: Notwithstanding the wonders of the computer, we have to"
2020.acl-main.697,P19-1159,0,0.0232578,"in, 2006) – still use some variant of least squares linear regression as the machine learning model to predict test taker scores. 3.3 Increased Attention to Fairness It would probably not be an overstatement to say that fairness in AI is quickly becoming its own sub-field, with a new annual ACM conference on Fairness, Accountability, and Transparency having been inaugurated in 201817 and relevant research appearing at many impactful publication venues, such as Science (Caliskan et al., 2017), NIPS (Pleiss et al., 2017; Kim et al., 2018), ICML (Kearns et al., 2018), ACL (Hovy and Spruit, 2016; Sun et al., 2019; Sap et al., 2019), KDD (Speicher et al., 2018), AAAI (Zhang and Bareinboim, 2018), and others (Dwork et al., 2012; Hajian and Domingo-Ferrer, 2013). There is also recent work that examines fairness and ethical considerations when using AI in an education (Mayfield et al., 2019; Gardner et al., 2019). In the context of assessment, fairness considerations dictate that the test reflects the same construct(s) for the entire test taking population, that 17 https://facctconference.org/ scores from the test have the same meaning for all the test taking population, and that a fair test does not offe"
2020.acl-main.697,D16-1193,0,0.0327825,"ikelier to make precise word choices than a writer who uses a more limited vocabulary. Attempts to capture topicality (Beigman Klebanov et al., 2016b) or development 7800 (Beigman Klebanov and Flor, 2013b; Somasundaran et al., 2016) through properties of vocabulary distribution without human annotation of topicality and development exemplify such approaches. 3.2.2 Model Interpretability Recent research has shown that more sophisticated machine learning models might perform better than simple regression-based models when it comes to predictive accuracy (Chen and He, 2013; Cummins et al., 2016; Taghipour and Ng, 2016; Alikaniotis et al., 2016; Dong et al., 2017; Dasgupta et al., 2018; Jin et al., 2018). However, unlike linear regression where stakeholders can understand how much each feature used in the model contributed to the predicted score, many of the more complex models are essentially “black boxes” and do not really lend themselves to post-hoc interpretability (Lipton, 2016). Although interpretability is an active area of research in the machine learning literature (Ribeiro et al., 2016; Koh and Liang, 2017; Doshi-Velez and Kim, 2017), it currently lags behind the research on machine learning metho"
2020.acl-main.697,W19-4440,0,0.0135048,"s an active line of subsequent work. While most of the research we cited so far has been on English, various aspects of writing evaluation, e.g., annotation, detection of various types of errors, and building AWE systems, have been researched for a variety of languages: Song et al. (2016), Rao et al. (2017), Shiue et al. (2017) worked with data in Chinese, 15 Writing Pal does not provide specific grammar and spelling feedback. 7798 Lorenzen et al. (2019) in Danish, Berggren et al. (2019) in Norwegian, Amorim and Veloso (2017) in Portuguese, Stymne et al. (2017) in Swedish, Berkling (2018) and Weiss and Meurers (2019) in German, Mezher and Omar (2016) in Arabic, Kakkonen et al. (2005) in Finnish, Loraksa and Peachavanish (2007) in Thai, Lemaire and Dessus (2001) in French, and Ishioka and Kameda (2006) in Japanese. The list is by no means exhaustive; see Flor and Cahill (2020) for a recent review. these true characteristics.” Page seems to do away rather quickly with trying to measure the actual thing – the set of all and only “true characteristics of essays”, or trins. Why is that? He explains: Notwithstanding the wonders of the computer, we have to develop a strategy in order to tell the computer what to"
2020.acl-main.697,N18-3008,1,0.829014,"aphs over and over, varying sentence structure, replacing words with more sophisticated variants, re-using words from the prompt, using general academic words, plagiarizing from other responses or from material found on the Internet, inserting unnecessary shell language – linguistic scaffolding for organizing claims and arguments, and automated generation of essays (Powers et al., 2001; Bejar et al., 2013, 2014; Higgins and Heilman, 2014; Sobel et al., 2014). Such strategies are generally handled by building in filters or flags for aberrant responses (Higgins et al., 2006; Zhang et al., 2016; Yoon et al., 2018; Cahill et al., 2018). However, developers of AWE systems can never anticipate all possible strategies and may have to react quickly as new ones are discovered in use, by developing new AWE methods to identify them. This cat-andmouse game is particularly rampant in the context of standardized testing (§3.2). This is one of the reasons standardized tests are often not scored solely by an AWE system but also by a human rater. 2.2.3 Content We are talking awfully casually about grading subject matter like history. Isn’t this a wholly different sort of problem? Aren’t we supposed to see that what"
2020.acl-main.697,W18-0549,0,0.0127282,"ent scoring systems ascertain the “correctness&quot; of a response based on its similarity to other responses that humans have deemed to be correct or, at least, high-scoring; they do not employ explicit fact-checking or reasoning for this purpose. Concerns about specific content extends to other cases where the scoring system needs to pay 7797 attention to details of genre and task – not all essays are five-paragraph persuasive essays; the specific task might require assessing whether the student has appropriately used specific source materials (Beigman Klebanov et al., 2014; Rahimi et al., 2017; Zhang and Litman, 2018) or assessing narrative (Somasundaran et al., 2018) or reflective (Beigman Klebanov et al., 2016a; Luo and Litman, 2016), rather than persuasive, writing. 2.2.4 Feedback Page emphasized the importance of feedback, and considered the following to be “the sort of feedback that can almost be programmed right now” (original italics): John [. . . ], please correct the following misspellings: believe, receive. Note the ie/ei problem. You overuse the words interesting, good, nice; then was repeated six times. Check trite expressions. All of your sentences are of the subject-verb variety and all are d"
2020.acl-main.697,W12-2022,0,0.0294318,".2). This is one of the reasons standardized tests are often not scored solely by an AWE system but also by a human rater. 2.2.3 Content We are talking awfully casually about grading subject matter like history. Isn’t this a wholly different sort of problem? Aren’t we supposed to see that what the students are saying makes sense, above and beyond their using commas in the right places? (Page, 1966) Indeed, work has been done over the last decade on automated evaluation of written responses for their content and not their general writing quality (Sukkarieh and Bolge, 2008; Mohler et al., 2011; Ziai et al., 2012; Basu et al., 2013; Madnani et al., 2013; Ramachandran et al., 2015; Burrows et al., 2015; Sakaguchi et al., 2015; Madnani et al., 2016; Padó, 2016; Madnani et al., 2017a; Riordan et al., 2017; Kumar et al., 2017; Horbach et al., 2018; Riordan et al., 2019). Scoring for content focuses primarily on what students know, have learned, or can do in a specific subject area such as Computer Science, Biology, or Music, with the fluency of the response being secondary. For example, some spelling or grammar errors are acceptable as long as the desired specific information (e.g., scientific principles,"
2020.bea-1.14,D19-1291,0,0.0199318,"s: Precision Recall 0.851 0.946 0.611 0.338 0.799 1.00 0 0 0.852 0.940 0.591 0.349 F1 0.896 0.436 0.889 0 0.894 0.439 Table 2: Performance of baseline features. “Structure"" corresponds to the dr_pn feature set, “Content” corresponds to the 1-3gr ppos feature set, both from Beigman Klebanov et al. (2017). shown to be effective in various NLP applications. Particularly, we focus on BERT (Devlin et al., 2018), a bi-directional transformer (Vaswani et al., 2017) based architecture that has produced excellent performance on argumentation tasks such as argument component and relation identiﬁcation (Chakrabarty et al., 2019) and argument clustering (Reimers et al., 2019). The BERT model is initially trained over a 3.3 billion word English corpus on two tasks: (1) given a sentence containing multiple masked words predict the identity of a particular masked word, and (2) given two sentences, predict whether they are adjacent. The BERT model exploits a multi-head attention operation to compute context-sensitive representations for each token in a sentence. During its training, a special token “[CLS]” is added to the beginning of each training utterance. During evaluation, the learned representation for this “[CLS]”"
2020.bea-1.14,W15-0608,0,0.534076,"Missing"
2020.bea-1.14,P16-2089,1,0.943005,"Missing"
2020.bea-1.14,P16-1150,0,0.0581491,"Missing"
2020.bea-1.14,P16-1107,0,0.33601,"Missing"
2020.bea-1.14,P15-1053,0,0.300138,"Missing"
2020.bea-1.14,N16-1164,0,0.150358,"nd the skill of using relevant examples to craft a support for one’s point of view (Walton, 1996). In recent times, the surge in AI-informed scoring systems has made it possible to assess writing skills using automated systems. Recent research suggests the possibility of argumentationaware automated essay scoring systems (Stab and Gurevych, 2017b). Most of the current work on computational analysis of argumentative writing in educational context focuses on automatically identifying the argument structures (e.g., argument components and their relations) in the essays (Stab and Gurevych, 2017a; Persing and Ng, 2016; Nguyen 145 th Proceedings of the 15 Workshop on Innovative Use of NLP for Building Educational Applications, pages 145–150 c July 10, 2020. 2020 Association for Computational Linguistics Dear Editor, (3) Their claims are badly writtin [sic] and have no good arguments. They need to support their claims with SOLID evidence and only claim arguments that can be undecicive [sic]. Advertising aimed at children under 12 should be allowed for several reasons. First, one family in my neighbourhood sits down and watches TV together almost every evening. The whole family learns a lot, which shows that"
2020.bea-1.14,P19-1054,0,0.0278743,".00 0 0 0.852 0.940 0.591 0.349 F1 0.896 0.436 0.889 0 0.894 0.439 Table 2: Performance of baseline features. “Structure"" corresponds to the dr_pn feature set, “Content” corresponds to the 1-3gr ppos feature set, both from Beigman Klebanov et al. (2017). shown to be effective in various NLP applications. Particularly, we focus on BERT (Devlin et al., 2018), a bi-directional transformer (Vaswani et al., 2017) based architecture that has produced excellent performance on argumentation tasks such as argument component and relation identiﬁcation (Chakrabarty et al., 2019) and argument clustering (Reimers et al., 2019). The BERT model is initially trained over a 3.3 billion word English corpus on two tasks: (1) given a sentence containing multiple masked words predict the identity of a particular masked word, and (2) given two sentences, predict whether they are adjacent. The BERT model exploits a multi-head attention operation to compute context-sensitive representations for each token in a sentence. During its training, a special token “[CLS]” is added to the beginning of each training utterance. During evaluation, the learned representation for this “[CLS]” token is processed by an additional layer with"
2020.bea-1.14,J17-3005,0,0.0762938,"n argumentative essay, it does share basic local sequential structures with the more mature writers. 1 Introduction Argument and logic are essential in academic writing as they enhance the critical thinking capacities of students. Argumentation requires systematic reasoning and the skill of using relevant examples to craft a support for one’s point of view (Walton, 1996). In recent times, the surge in AI-informed scoring systems has made it possible to assess writing skills using automated systems. Recent research suggests the possibility of argumentationaware automated essay scoring systems (Stab and Gurevych, 2017b). Most of the current work on computational analysis of argumentative writing in educational context focuses on automatically identifying the argument structures (e.g., argument components and their relations) in the essays (Stab and Gurevych, 2017a; Persing and Ng, 2016; Nguyen 145 th Proceedings of the 15 Workshop on Innovative Use of NLP for Building Educational Applications, pages 145–150 c July 10, 2020. 2020 Association for Computational Linguistics Dear Editor, (3) Their claims are badly writtin [sic] and have no good arguments. They need to support their claims with SOLID evidence an"
2020.bea-1.14,E17-1092,0,0.0178475,"n argumentative essay, it does share basic local sequential structures with the more mature writers. 1 Introduction Argument and logic are essential in academic writing as they enhance the critical thinking capacities of students. Argumentation requires systematic reasoning and the skill of using relevant examples to craft a support for one’s point of view (Walton, 1996). In recent times, the surge in AI-informed scoring systems has made it possible to assess writing skills using automated systems. Recent research suggests the possibility of argumentationaware automated essay scoring systems (Stab and Gurevych, 2017b). Most of the current work on computational analysis of argumentative writing in educational context focuses on automatically identifying the argument structures (e.g., argument components and their relations) in the essays (Stab and Gurevych, 2017a; Persing and Ng, 2016; Nguyen 145 th Proceedings of the 15 Workshop on Innovative Use of NLP for Building Educational Applications, pages 145–150 c July 10, 2020. 2020 Association for Computational Linguistics Dear Editor, (3) Their claims are badly writtin [sic] and have no good arguments. They need to support their claims with SOLID evidence an"
2020.figlang-1.3,2020.figlang-1.28,0,0.497097,"aining the contextualized embeddings of a sentence, a linear layer is applied followed by softmax on each token to predict whether it is metaphorical or not. The authors spell-correct the TOEFL data, which improves performance. Chen et al. (2020) present two multi-task settings: In the first, metaphor detection on out-of-domain data is treated as an auxiliary task; in the second, idiom detection on in-domain data is the auxiliary task. Performance on TOEFL is helped by the first multi-task setting; performance on VUA is helped by the second. UoB team: Bi-LSTM + GloVe embeddings + concreteness Alnafesah et al. (2020) explore ways of using concreteness information in a neural metaphor detection context. GloVe embeddings are used as features to an SVM classifier to learn concreteness values, training it using human labels of concreteness. Then, for metaphor detection, every input word is represented as a 304-dimensional vector – 300 dimensions are GloVe pre-trained embeddings, plus probabilities for the four concreteness classes. These representations of words are given as input to a Bi-LSTM which outputs a sequence of labels. Results suggest that explicit concreteness information helps improve metaphor det"
2020.figlang-1.3,W13-0902,1,0.88651,"s the same as the one used in the first shared task on metaphor detection (Leong et al., 2018), where the reader is referred for further details. 3.1.2 TOEFL corpus This data labeled for metaphor was sampled from the publicly available ETS Corpus of Non-Native Written English1 and was first introduced by (Beigman Klebanov et al., 2018). The annotated data comprises essay responses to eight persuasive/argumentative prompts, for three native languages of the writer (Japanese, Italian, Arabic), and for two proficiency levels – medium and high. The data was annotated using the protocol in Beigman Klebanov and Flor (2013), that emphasized argumentation-relevant metaphors: “Argumentation-relevant metaphors are, briefly, those that help the author advance her argument. For example, if you are arguing against some action because it would drain resources, drain 1 is a metaphor that helps you advance your argument, because it presents the expenditure in a very negative way, suggesting that resources would disappear very quickly and without control.” Beigman Klebanov and Flor (2013) https://catalog.ldc.upenn.edu/LDC2014T06 #texts #sents VUA Train 90 12,123 Test 27 4,081 TOEFL Train 180 2,741 Test 60 968 Table 1: Num"
2020.figlang-1.3,N18-2014,1,0.563417,"hysical and familiar social experiences to a multitude of other subjects and contexts (Lakoff and Johnson, 2008); it is a fundamental way to structure our understanding of the world even without our conscious realization of its presence as we speak and write. It highlights the unknown using the known, explains the complex using the simple, and helps us to emphasize the relevant aspects of meaning resulting in effective communication. Metaphor has been studied in the context of political communication, marketing, mental health, teaching, assessment of English proficiency, among others (Beigman Klebanov et al., 2018; Gutierrez et al., 2017; Littlemore et al., 2013; Thibodeau and Boroditsky, 2011; Kaviani and Hamedi, 2011; Kathpalia and Carmel, 2011; Landau et al., 2009; Beigman Klebanov et al., 2008; Zaltman and Zaltman, 2008; Littlemore and Low, 2006; Cameron, 2003; Lakoff, 2010; Billow et al., 1997; Bosman, 1987); see chapter 7 in Veale et al. (2016) for a recent review. We report on the second shared task on automatic metaphor detection, following up on the first shared task held in 2018 (Leong et al., 2018). We present the shared task and provide a brief description of each of the participating syste"
2020.figlang-1.3,P16-2017,1,0.853359,"an popular topic, which manifests itself in both a variety of approaches and in an increasing variety of data to which the methods are applied. In terms of methods, approaches based on feature-engineering in a supervised machine learning paradigm explored features based on concreteness and imageability, semantic classification using WordNet, FrameNet, VerbNet, SUMO ontology, property norms, and distributional semantic models, syntactic dependency patterns, sensorial and vision-based features (Bulat et al., 2017; K¨oper and im Walde, 2017; Gutierrez et al., 2016; Shutova et al., 2016; Beigman Klebanov et al., 2016; Tekiroglu et al., 2015; Tsvetkov et al., 2014; Beigman Klebanov et al., 2014; Dunn, 2013; Neuman et al., 2013; Mohler et al., 2013; Hovy et al., 2013; Tsvetkov et al., 2013; Turney et al., 2011; Shutova et al., 2010; Gedigian et al., 2006); see Shutova et al. (2017) and Veale et al. (2016) for reviews of supervised as well as semi-supervised and unsupervised approaches. Recently, deep learning methods have been explored for token-level metaphor detection (Mao et al., 2019; Dankers et al., 2019; Gao et al., 2018; Wu et al., 2018; Rei et al., 2017; Gutierrez et al., 2017; Do Dinh and Gurevych,"
2020.figlang-1.3,W14-2302,1,0.89369,"in an increasing variety of data to which the methods are applied. In terms of methods, approaches based on feature-engineering in a supervised machine learning paradigm explored features based on concreteness and imageability, semantic classification using WordNet, FrameNet, VerbNet, SUMO ontology, property norms, and distributional semantic models, syntactic dependency patterns, sensorial and vision-based features (Bulat et al., 2017; K¨oper and im Walde, 2017; Gutierrez et al., 2016; Shutova et al., 2016; Beigman Klebanov et al., 2016; Tekiroglu et al., 2015; Tsvetkov et al., 2014; Beigman Klebanov et al., 2014; Dunn, 2013; Neuman et al., 2013; Mohler et al., 2013; Hovy et al., 2013; Tsvetkov et al., 2013; Turney et al., 2011; Shutova et al., 2010; Gedigian et al., 2006); see Shutova et al. (2017) and Veale et al. (2016) for reviews of supervised as well as semi-supervised and unsupervised approaches. Recently, deep learning methods have been explored for token-level metaphor detection (Mao et al., 2019; Dankers et al., 2019; Gao et al., 2018; Wu et al., 2018; Rei et al., 2017; Gutierrez et al., 2017; Do Dinh and Gurevych, 2016). In terms of data, researchers used specially constructed or selected s"
2020.figlang-1.3,E06-1042,0,0.102231,"pproaches. Recently, deep learning methods have been explored for token-level metaphor detection (Mao et al., 2019; Dankers et al., 2019; Gao et al., 2018; Wu et al., 2018; Rei et al., 2017; Gutierrez et al., 2017; Do Dinh and Gurevych, 2016). In terms of data, researchers used specially constructed or selected sets, such as adjective noun pairs (Gutierrez et al., 2016; Tsvetkov et al., 2014), WordNet synsets and glosses (Mohammad et al., 2016), annotated lexical items (from a range of word classes) in sentences sampled from cor¨ pora (Ozbal et al., 2016; Jang et al., 2015; Hovy et al., 2013; Birke and Sarkar, 2006), all the way to annotation of all words in running text for metaphoricity (Beigman Klebanov et al., 2018; Steen et al., 2010); Veale et al. (2016) review various annotated datasets. 3 Task Description The goal of this shared task is to detect, at the word level, all content word metaphors in a given text. We are using two datasets – VUA and TOEFL, to be described shortly. There are two tracks for each dataset, for a total of four tracks: VUA All POS, VUA Verbs, TOEFL All POS, and TOEFL Verbs. The AllPOS track is concerned with the detection of all content words, i.e., nouns, verbs, adverbs an"
2020.figlang-1.3,2020.figlang-1.32,1,0.836236,"ursive neural network architecture with long-term short-term memory (LSTM BiRNN) and implements a flat sequenceto-sequence neural network with one hidden layer using TensorFlow and Keras in Python. The system uses fastText word embeddings from different corpora, including learner corpus and BNC data. Finally, Baseline 3: BERT is constructed by finetuning the BERT model (Devlin et al., 2018) in a standard token classification task: After obtaining the contextualized embeddings of a sentence, we apply a linear layer followed by softmax on each token to predict whether it is metaphorical or not. Chen et al. (2020) gives more details about the architecture of this baseline. For Verbs tracks, we tune the system on All POS data and test on Verbs, 6 Baseline 1 is “all-16” in Beigman Klebanov et al. (2018) as this produced better results during preliminary experimentation than training on Verbs only. 4.2 System Descriptions illiniMet: RoBERTa embedding + Linguistic features + Ensemble Gong et al. (2020) used RoBERTa to obtain a contextualized embedding of a word and concatenate it with features extracted from linguistic resources (e.g. WordNet, VerbNet) as well as other features (e.g. POS, topicality, concr"
2020.figlang-1.3,2020.figlang-1.31,0,0.22856,"ed, the 2018 best performing system would have earned the rank of 11 in the 2020 All POS track, suggesting that the field has generally moved to more effective models than those proposed for the 2018 competitions. The best results posted for the 2020 shared task are the new state-of-the-art for both VUA7 and TOEFL corpora. 7 While a number of recent systems were evaluted on VUA data (Le et al., 2020; Dankers et al., 2019; Mao et al., 2019; Gao et al., 2018), their results are not directly comparable to the shared task, since they evaluated on all parts of speech, including function words. See Dankers et al. (2020) for a discussion. 1 2 3 4 5 6 7 8 9 10 11 12 1 2 3 4 5 6 7 8 9 10 11 12 DeepMet zhengchang illiniMet Go Figure! Duke Data Science Baseline 3: BERT Zenith umd bilstm iiegn PolyU-LLT Baseline 2: bot.zen Baseline 1: UL + + WN + CCDB Verbs DeepMet zhengchang illiniMet Go Figure! Duke Data Science Baseline 3: BERT Zenith umd bilstm PolyU-LLT Baseline 2: bot.zen Baseline 1: UL + + WN + CCDB iiegn Table 5: TOEFL Dataset: Performance and ranking of the best system per team and baselines, for All POS track (top panel) and for Verbs track (bottom panel). 5.3 Performance across genres: VUA Table 6 shows"
2020.figlang-1.3,D19-1227,0,0.187949,"lat et al., 2017; K¨oper and im Walde, 2017; Gutierrez et al., 2016; Shutova et al., 2016; Beigman Klebanov et al., 2016; Tekiroglu et al., 2015; Tsvetkov et al., 2014; Beigman Klebanov et al., 2014; Dunn, 2013; Neuman et al., 2013; Mohler et al., 2013; Hovy et al., 2013; Tsvetkov et al., 2013; Turney et al., 2011; Shutova et al., 2010; Gedigian et al., 2006); see Shutova et al. (2017) and Veale et al. (2016) for reviews of supervised as well as semi-supervised and unsupervised approaches. Recently, deep learning methods have been explored for token-level metaphor detection (Mao et al., 2019; Dankers et al., 2019; Gao et al., 2018; Wu et al., 2018; Rei et al., 2017; Gutierrez et al., 2017; Do Dinh and Gurevych, 2016). In terms of data, researchers used specially constructed or selected sets, such as adjective noun pairs (Gutierrez et al., 2016; Tsvetkov et al., 2014), WordNet synsets and glosses (Mohammad et al., 2016), annotated lexical items (from a range of word classes) in sentences sampled from cor¨ pora (Ozbal et al., 2016; Jang et al., 2015; Hovy et al., 2013; Birke and Sarkar, 2006), all the way to annotation of all words in running text for metaphoricity (Beigman Klebanov et al., 2018; Steen"
2020.figlang-1.3,W16-1104,0,0.0878594,"banov et al., 2016; Tekiroglu et al., 2015; Tsvetkov et al., 2014; Beigman Klebanov et al., 2014; Dunn, 2013; Neuman et al., 2013; Mohler et al., 2013; Hovy et al., 2013; Tsvetkov et al., 2013; Turney et al., 2011; Shutova et al., 2010; Gedigian et al., 2006); see Shutova et al. (2017) and Veale et al. (2016) for reviews of supervised as well as semi-supervised and unsupervised approaches. Recently, deep learning methods have been explored for token-level metaphor detection (Mao et al., 2019; Dankers et al., 2019; Gao et al., 2018; Wu et al., 2018; Rei et al., 2017; Gutierrez et al., 2017; Do Dinh and Gurevych, 2016). In terms of data, researchers used specially constructed or selected sets, such as adjective noun pairs (Gutierrez et al., 2016; Tsvetkov et al., 2014), WordNet synsets and glosses (Mohammad et al., 2016), annotated lexical items (from a range of word classes) in sentences sampled from cor¨ pora (Ozbal et al., 2016; Jang et al., 2015; Hovy et al., 2013; Birke and Sarkar, 2006), all the way to annotation of all words in running text for metaphoricity (Beigman Klebanov et al., 2018; Steen et al., 2010); Veale et al. (2016) review various annotated datasets. 3 Task Description The goal of this"
2020.figlang-1.3,W13-0901,0,0.0555814,"y of data to which the methods are applied. In terms of methods, approaches based on feature-engineering in a supervised machine learning paradigm explored features based on concreteness and imageability, semantic classification using WordNet, FrameNet, VerbNet, SUMO ontology, property norms, and distributional semantic models, syntactic dependency patterns, sensorial and vision-based features (Bulat et al., 2017; K¨oper and im Walde, 2017; Gutierrez et al., 2016; Shutova et al., 2016; Beigman Klebanov et al., 2016; Tekiroglu et al., 2015; Tsvetkov et al., 2014; Beigman Klebanov et al., 2014; Dunn, 2013; Neuman et al., 2013; Mohler et al., 2013; Hovy et al., 2013; Tsvetkov et al., 2013; Turney et al., 2011; Shutova et al., 2010; Gedigian et al., 2006); see Shutova et al. (2017) and Veale et al. (2016) for reviews of supervised as well as semi-supervised and unsupervised approaches. Recently, deep learning methods have been explored for token-level metaphor detection (Mao et al., 2019; Dankers et al., 2019; Gao et al., 2018; Wu et al., 2018; Rei et al., 2017; Gutierrez et al., 2017; Do Dinh and Gurevych, 2016). In terms of data, researchers used specially constructed or selected sets, such as"
2020.figlang-1.3,D18-1060,0,0.863004,"er and im Walde, 2017; Gutierrez et al., 2016; Shutova et al., 2016; Beigman Klebanov et al., 2016; Tekiroglu et al., 2015; Tsvetkov et al., 2014; Beigman Klebanov et al., 2014; Dunn, 2013; Neuman et al., 2013; Mohler et al., 2013; Hovy et al., 2013; Tsvetkov et al., 2013; Turney et al., 2011; Shutova et al., 2010; Gedigian et al., 2006); see Shutova et al. (2017) and Veale et al. (2016) for reviews of supervised as well as semi-supervised and unsupervised approaches. Recently, deep learning methods have been explored for token-level metaphor detection (Mao et al., 2019; Dankers et al., 2019; Gao et al., 2018; Wu et al., 2018; Rei et al., 2017; Gutierrez et al., 2017; Do Dinh and Gurevych, 2016). In terms of data, researchers used specially constructed or selected sets, such as adjective noun pairs (Gutierrez et al., 2016; Tsvetkov et al., 2014), WordNet synsets and glosses (Mohammad et al., 2016), annotated lexical items (from a range of word classes) in sentences sampled from cor¨ pora (Ozbal et al., 2016; Jang et al., 2015; Hovy et al., 2013; Birke and Sarkar, 2006), all the way to annotation of all words in running text for metaphoricity (Beigman Klebanov et al., 2018; Steen et al., 2010); Vea"
2020.figlang-1.3,W06-3506,0,0.0468488,"aradigm explored features based on concreteness and imageability, semantic classification using WordNet, FrameNet, VerbNet, SUMO ontology, property norms, and distributional semantic models, syntactic dependency patterns, sensorial and vision-based features (Bulat et al., 2017; K¨oper and im Walde, 2017; Gutierrez et al., 2016; Shutova et al., 2016; Beigman Klebanov et al., 2016; Tekiroglu et al., 2015; Tsvetkov et al., 2014; Beigman Klebanov et al., 2014; Dunn, 2013; Neuman et al., 2013; Mohler et al., 2013; Hovy et al., 2013; Tsvetkov et al., 2013; Turney et al., 2011; Shutova et al., 2010; Gedigian et al., 2006); see Shutova et al. (2017) and Veale et al. (2016) for reviews of supervised as well as semi-supervised and unsupervised approaches. Recently, deep learning methods have been explored for token-level metaphor detection (Mao et al., 2019; Dankers et al., 2019; Gao et al., 2018; Wu et al., 2018; Rei et al., 2017; Gutierrez et al., 2017; Do Dinh and Gurevych, 2016). In terms of data, researchers used specially constructed or selected sets, such as adjective noun pairs (Gutierrez et al., 2016; Tsvetkov et al., 2014), WordNet synsets and glosses (Mohammad et al., 2016), annotated lexical items (fr"
2020.figlang-1.3,2020.figlang-1.33,0,0.161581,"with POS tags to generate features for a Bi-LSTM for token-level metaphor classification. For the testing phase, the authros used an ensemble strategy, training four copies of the Bi-LSTM with different initializations and averaging their predictions. To increase the likelihood of prediction of a metaphor label, a token is declared a metaphor if: (1) its predicted probability is higher than the threshold, or (2) if its probability is three orders of magnitude higher than the median predicted probability for that word in the evaluation set. chasingkangaroos: RNN + BiLSTM + Attention + Ensemble Brooks and Youssef (2020) use an ensemble of RNN models with Bi-LSTMs and bidirectional attention mechanisms. Each word was represented by an 11-gram and appeared at the center of the 11-gram; each word in the 11-gram was represented by a 1,324 dimensional word embedding (concatenation of ELMo and GloVe embeddings). The authors experimented with ensembles of models that implement somewhat different architecture (in terms of attention) and models trained on all POS and on a specific POS. Go Figure!: BERT + multi-task + spell correction + idioms + domain adaptation Chen et al. (2020) baseline system (also one of the sha"
2020.figlang-1.3,2020.figlang-1.21,0,0.560606,") in a standard token classification task: After obtaining the contextualized embeddings of a sentence, we apply a linear layer followed by softmax on each token to predict whether it is metaphorical or not. Chen et al. (2020) gives more details about the architecture of this baseline. For Verbs tracks, we tune the system on All POS data and test on Verbs, 6 Baseline 1 is “all-16” in Beigman Klebanov et al. (2018) as this produced better results during preliminary experimentation than training on Verbs only. 4.2 System Descriptions illiniMet: RoBERTa embedding + Linguistic features + Ensemble Gong et al. (2020) used RoBERTa to obtain a contextualized embedding of a word and concatenate it with features extracted from linguistic resources (e.g. WordNet, VerbNet) as well as other features (e.g. POS, topicality, concreteness) previously used in the first shared task (Leong et al., 2018) before feeding them into a fully-connected Feedforward network to generate predictions. During inference, an ensemble of three independently trained models using different train/development splits is proposed to yield a final prediction based on majority vote. Using just RoBERTa without linguistic features in an ensembl"
2020.figlang-1.3,E17-2084,0,0.397707,"d in the shared task. 2 Related Work Over the last decade, automated detection of metaphor has become an popular topic, which manifests itself in both a variety of approaches and in an increasing variety of data to which the methods are applied. In terms of methods, approaches based on feature-engineering in a supervised machine learning paradigm explored features based on concreteness and imageability, semantic classification using WordNet, FrameNet, VerbNet, SUMO ontology, property norms, and distributional semantic models, syntactic dependency patterns, sensorial and vision-based features (Bulat et al., 2017; K¨oper and im Walde, 2017; Gutierrez et al., 2016; Shutova et al., 2016; Beigman Klebanov et al., 2016; Tekiroglu et al., 2015; Tsvetkov et al., 2014; Beigman Klebanov et al., 2014; Dunn, 2013; Neuman et al., 2013; Mohler et al., 2013; Hovy et al., 2013; Tsvetkov et al., 2013; Turney et al., 2011; Shutova et al., 2010; Gedigian et al., 2006); see Shutova et al. (2017) and Veale et al. (2016) for reviews of supervised as well as semi-supervised and unsupervised approaches. Recently, deep learning methods have been explored for token-level metaphor detection (Mao et al., 2019; Dankers et al.,"
2020.figlang-1.3,D17-1316,0,0.681734,"cial experiences to a multitude of other subjects and contexts (Lakoff and Johnson, 2008); it is a fundamental way to structure our understanding of the world even without our conscious realization of its presence as we speak and write. It highlights the unknown using the known, explains the complex using the simple, and helps us to emphasize the relevant aspects of meaning resulting in effective communication. Metaphor has been studied in the context of political communication, marketing, mental health, teaching, assessment of English proficiency, among others (Beigman Klebanov et al., 2018; Gutierrez et al., 2017; Littlemore et al., 2013; Thibodeau and Boroditsky, 2011; Kaviani and Hamedi, 2011; Kathpalia and Carmel, 2011; Landau et al., 2009; Beigman Klebanov et al., 2008; Zaltman and Zaltman, 2008; Littlemore and Low, 2006; Cameron, 2003; Lakoff, 2010; Billow et al., 1997; Bosman, 1987); see chapter 7 in Veale et al. (2016) for a recent review. We report on the second shared task on automatic metaphor detection, following up on the first shared task held in 2018 (Leong et al., 2018). We present the shared task and provide a brief description of each of the participating systems, a comparative evalua"
2020.figlang-1.3,P16-1018,0,0.382976,"ast decade, automated detection of metaphor has become an popular topic, which manifests itself in both a variety of approaches and in an increasing variety of data to which the methods are applied. In terms of methods, approaches based on feature-engineering in a supervised machine learning paradigm explored features based on concreteness and imageability, semantic classification using WordNet, FrameNet, VerbNet, SUMO ontology, property norms, and distributional semantic models, syntactic dependency patterns, sensorial and vision-based features (Bulat et al., 2017; K¨oper and im Walde, 2017; Gutierrez et al., 2016; Shutova et al., 2016; Beigman Klebanov et al., 2016; Tekiroglu et al., 2015; Tsvetkov et al., 2014; Beigman Klebanov et al., 2014; Dunn, 2013; Neuman et al., 2013; Mohler et al., 2013; Hovy et al., 2013; Tsvetkov et al., 2013; Turney et al., 2011; Shutova et al., 2010; Gedigian et al., 2006); see Shutova et al. (2017) and Veale et al. (2016) for reviews of supervised as well as semi-supervised and unsupervised approaches. Recently, deep learning methods have been explored for token-level metaphor detection (Mao et al., 2019; Dankers et al., 2019; Gao et al., 2018; Wu et al., 2018; Rei et al."
2020.figlang-1.3,W13-0907,0,0.0854507,"f methods, approaches based on feature-engineering in a supervised machine learning paradigm explored features based on concreteness and imageability, semantic classification using WordNet, FrameNet, VerbNet, SUMO ontology, property norms, and distributional semantic models, syntactic dependency patterns, sensorial and vision-based features (Bulat et al., 2017; K¨oper and im Walde, 2017; Gutierrez et al., 2016; Shutova et al., 2016; Beigman Klebanov et al., 2016; Tekiroglu et al., 2015; Tsvetkov et al., 2014; Beigman Klebanov et al., 2014; Dunn, 2013; Neuman et al., 2013; Mohler et al., 2013; Hovy et al., 2013; Tsvetkov et al., 2013; Turney et al., 2011; Shutova et al., 2010; Gedigian et al., 2006); see Shutova et al. (2017) and Veale et al. (2016) for reviews of supervised as well as semi-supervised and unsupervised approaches. Recently, deep learning methods have been explored for token-level metaphor detection (Mao et al., 2019; Dankers et al., 2019; Gao et al., 2018; Wu et al., 2018; Rei et al., 2017; Gutierrez et al., 2017; Do Dinh and Gurevych, 2016). In terms of data, researchers used specially constructed or selected sets, such as adjective noun pairs (Gutierrez et al., 2016; Tsvetkov et al"
2020.figlang-1.3,W18-0907,1,0.682245,"marketing, mental health, teaching, assessment of English proficiency, among others (Beigman Klebanov et al., 2018; Gutierrez et al., 2017; Littlemore et al., 2013; Thibodeau and Boroditsky, 2011; Kaviani and Hamedi, 2011; Kathpalia and Carmel, 2011; Landau et al., 2009; Beigman Klebanov et al., 2008; Zaltman and Zaltman, 2008; Littlemore and Low, 2006; Cameron, 2003; Lakoff, 2010; Billow et al., 1997; Bosman, 1987); see chapter 7 in Veale et al. (2016) for a recent review. We report on the second shared task on automatic metaphor detection, following up on the first shared task held in 2018 (Leong et al., 2018). We present the shared task and provide a brief description of each of the participating systems, a comparative evaluation of the systems, and our observations about trends in designs and performance of the systems that participated in the shared task. 2 Related Work Over the last decade, automated detection of metaphor has become an popular topic, which manifests itself in both a variety of approaches and in an increasing variety of data to which the methods are applied. In terms of methods, approaches based on feature-engineering in a supervised machine learning paradigm explored features b"
2020.figlang-1.3,W15-4650,0,0.060284,"as semi-supervised and unsupervised approaches. Recently, deep learning methods have been explored for token-level metaphor detection (Mao et al., 2019; Dankers et al., 2019; Gao et al., 2018; Wu et al., 2018; Rei et al., 2017; Gutierrez et al., 2017; Do Dinh and Gurevych, 2016). In terms of data, researchers used specially constructed or selected sets, such as adjective noun pairs (Gutierrez et al., 2016; Tsvetkov et al., 2014), WordNet synsets and glosses (Mohammad et al., 2016), annotated lexical items (from a range of word classes) in sentences sampled from cor¨ pora (Ozbal et al., 2016; Jang et al., 2015; Hovy et al., 2013; Birke and Sarkar, 2006), all the way to annotation of all words in running text for metaphoricity (Beigman Klebanov et al., 2018; Steen et al., 2010); Veale et al. (2016) review various annotated datasets. 3 Task Description The goal of this shared task is to detect, at the word level, all content word metaphors in a given text. We are using two datasets – VUA and TOEFL, to be described shortly. There are two tracks for each dataset, for a total of four tracks: VUA All POS, VUA Verbs, TOEFL All POS, and TOEFL Verbs. The AllPOS track is concerned with the detection of all c"
2020.figlang-1.3,W17-1903,0,0.311783,"Missing"
2020.figlang-1.3,2020.figlang-1.18,0,0.686586,"beddings. The authors also experimented with a spell-corrected version of TOEFL data and found it further improves the performance of the Bi-LSTM system. atr2112: Residual Bi-LSTM + Embeddings + CRF + POS + WN Rivera et al. (2020) proposed a deep architecture that takes as inputs ELMo embeddings that represent words and lemmas, along with POS labels and WordNet synsets. The inputs are processed by a residual Bi-LSTM, then by a number of additional layers, with a final CRF sequence labeling step to generate predictions. Zenith: Character embeddings + Similarity Networks + Bi-LSTM + Transformer Kumar and Sharma (2020) added lexical and orthographic information via character embeddings in addition to GloVe and ELMo embeddings for an enriched input representation. The authors also constructed a similarity metric between the literal and contextual representations of a word as another input component. A Bi-LSTM network and Transformer network are trained independently and combined in an ensemble. Eventually, adding both character-based information and similarity network are the most helpful, as evidenced by results obtained using cross-validation on the training datasets. rowanhm: Static and contextual embeddi"
2020.figlang-1.3,2020.figlang-1.26,0,0.442631,"d the metaphoricity role of each word token in a shorter sequence within a given sentence. Features belonging to five different categories are provided as inputs to the network i.e. global text context, local text context, query word, general POS, finegrained POS. The features are then mapped onto embeddings before going into Transformer stacks and ensemble for inference. An ablation experiment was also performed with the observation that fine-grained POS and global text features are the most helpful for detecting metaphors. umd bilstm: Bi-LSTM + Embeddings + Unigram Lemmas + Spell Correction Kuo and Carpuat (2020) explored the effectiveness of additional features by augmenting the basic contextual metaphor detection system developed by Gao et al. (2018) with one-hot unigram lemma features in addition to GloVe and ELMo embeddings. The authors also experimented with a spell-corrected version of TOEFL data and found it further improves the performance of the Bi-LSTM system. atr2112: Residual Bi-LSTM + Embeddings + CRF + POS + WN Rivera et al. (2020) proposed a deep architecture that takes as inputs ELMo embeddings that represent words and lemmas, along with POS labels and WordNet synsets. The inputs are p"
2020.figlang-1.3,2020.figlang-1.34,0,0.35861,"emle and Onysko, 2018) that served as one of the baseline in the current shared task (see section 4.1). Analyzing the training data, the authors observed that essays written by more proficient users had significantly more metaphors, and that essays responding to some of the prompts had significantly more metaphors than other prompts; however, using proficiency and prompt metadata explicitly in the classifier did not improve performance. The authors also experimented with combining VUA and TOEFL data. Duke Data Science: BERT, XNET language models + POS tags as features for a Bi-LSTM classifier Liu et al. (2020) use pre-trained BERT and XLNet language models to create contextualized embeddings, which are combined with POS tags to generate features for a Bi-LSTM for token-level metaphor classification. For the testing phase, the authros used an ensemble strategy, training four copies of the Bi-LSTM with different initializations and averaging their predictions. To increase the likelihood of prediction of a metaphor label, a token is declared a metaphor if: (1) its predicted probability is higher than the threshold, or (2) if its probability is three orders of magnitude higher than the median predicted"
2020.figlang-1.3,P19-1378,0,0.604447,"based features (Bulat et al., 2017; K¨oper and im Walde, 2017; Gutierrez et al., 2016; Shutova et al., 2016; Beigman Klebanov et al., 2016; Tekiroglu et al., 2015; Tsvetkov et al., 2014; Beigman Klebanov et al., 2014; Dunn, 2013; Neuman et al., 2013; Mohler et al., 2013; Hovy et al., 2013; Tsvetkov et al., 2013; Turney et al., 2011; Shutova et al., 2010; Gedigian et al., 2006); see Shutova et al. (2017) and Veale et al. (2016) for reviews of supervised as well as semi-supervised and unsupervised approaches. Recently, deep learning methods have been explored for token-level metaphor detection (Mao et al., 2019; Dankers et al., 2019; Gao et al., 2018; Wu et al., 2018; Rei et al., 2017; Gutierrez et al., 2017; Do Dinh and Gurevych, 2016). In terms of data, researchers used specially constructed or selected sets, such as adjective noun pairs (Gutierrez et al., 2016; Tsvetkov et al., 2014), WordNet synsets and glosses (Mohammad et al., 2016), annotated lexical items (from a range of word classes) in sentences sampled from cor¨ pora (Ozbal et al., 2016; Jang et al., 2015; Hovy et al., 2013; Birke and Sarkar, 2006), all the way to annotation of all words in running text for metaphoricity (Beigman Klebano"
2020.figlang-1.3,2020.figlang-1.30,0,0.256794,"via character embeddings in addition to GloVe and ELMo embeddings for an enriched input representation. The authors also constructed a similarity metric between the literal and contextual representations of a word as another input component. A Bi-LSTM network and Transformer network are trained independently and combined in an ensemble. Eventually, adding both character-based information and similarity network are the most helpful, as evidenced by results obtained using cross-validation on the training datasets. rowanhm: Static and contextual embeddings + concreteness + Multi-layer Perceptron Maudslay et al. (2020) created a system that combines the concreteness of a word, its static embedding and its contextual embedding before providing them as inputs into a deep Multi-layer Perceptron network which predicts word metaphoricity. Specifically, the concreteness value of a word is formulated as a linear interpolation between two reference vectors (concrete and abstract) which were randomly initialized and learned from data. iiegn: LSTM BiRNN + metadata; combine TOEFL and VUA data Stemle and Onysko (2020) used an LSTM BiRNN classifier to study the relationship between the metadata in the TOEFL corpus (prof"
2020.figlang-1.3,S16-2003,0,0.170927,"2011; Shutova et al., 2010; Gedigian et al., 2006); see Shutova et al. (2017) and Veale et al. (2016) for reviews of supervised as well as semi-supervised and unsupervised approaches. Recently, deep learning methods have been explored for token-level metaphor detection (Mao et al., 2019; Dankers et al., 2019; Gao et al., 2018; Wu et al., 2018; Rei et al., 2017; Gutierrez et al., 2017; Do Dinh and Gurevych, 2016). In terms of data, researchers used specially constructed or selected sets, such as adjective noun pairs (Gutierrez et al., 2016; Tsvetkov et al., 2014), WordNet synsets and glosses (Mohammad et al., 2016), annotated lexical items (from a range of word classes) in sentences sampled from cor¨ pora (Ozbal et al., 2016; Jang et al., 2015; Hovy et al., 2013; Birke and Sarkar, 2006), all the way to annotation of all words in running text for metaphoricity (Beigman Klebanov et al., 2018; Steen et al., 2010); Veale et al. (2016) review various annotated datasets. 3 Task Description The goal of this shared task is to detect, at the word level, all content word metaphors in a given text. We are using two datasets – VUA and TOEFL, to be described shortly. There are two tracks for each dataset, for a tota"
2020.figlang-1.3,W13-0904,0,0.0986461,"e applied. In terms of methods, approaches based on feature-engineering in a supervised machine learning paradigm explored features based on concreteness and imageability, semantic classification using WordNet, FrameNet, VerbNet, SUMO ontology, property norms, and distributional semantic models, syntactic dependency patterns, sensorial and vision-based features (Bulat et al., 2017; K¨oper and im Walde, 2017; Gutierrez et al., 2016; Shutova et al., 2016; Beigman Klebanov et al., 2016; Tekiroglu et al., 2015; Tsvetkov et al., 2014; Beigman Klebanov et al., 2014; Dunn, 2013; Neuman et al., 2013; Mohler et al., 2013; Hovy et al., 2013; Tsvetkov et al., 2013; Turney et al., 2011; Shutova et al., 2010; Gedigian et al., 2006); see Shutova et al. (2017) and Veale et al. (2016) for reviews of supervised as well as semi-supervised and unsupervised approaches. Recently, deep learning methods have been explored for token-level metaphor detection (Mao et al., 2019; Dankers et al., 2019; Gao et al., 2018; Wu et al., 2018; Rei et al., 2017; Gutierrez et al., 2017; Do Dinh and Gurevych, 2016). In terms of data, researchers used specially constructed or selected sets, such as adjective noun pairs (Gutierrez et al., 2"
2020.figlang-1.3,L16-1600,0,0.0155088,"f supervised as well as semi-supervised and unsupervised approaches. Recently, deep learning methods have been explored for token-level metaphor detection (Mao et al., 2019; Dankers et al., 2019; Gao et al., 2018; Wu et al., 2018; Rei et al., 2017; Gutierrez et al., 2017; Do Dinh and Gurevych, 2016). In terms of data, researchers used specially constructed or selected sets, such as adjective noun pairs (Gutierrez et al., 2016; Tsvetkov et al., 2014), WordNet synsets and glosses (Mohammad et al., 2016), annotated lexical items (from a range of word classes) in sentences sampled from cor¨ pora (Ozbal et al., 2016; Jang et al., 2015; Hovy et al., 2013; Birke and Sarkar, 2006), all the way to annotation of all words in running text for metaphoricity (Beigman Klebanov et al., 2018; Steen et al., 2010); Veale et al. (2016) review various annotated datasets. 3 Task Description The goal of this shared task is to detect, at the word level, all content word metaphors in a given text. We are using two datasets – VUA and TOEFL, to be described shortly. There are two tracks for each dataset, for a total of four tracks: VUA All POS, VUA Verbs, TOEFL All POS, and TOEFL Verbs. The AllPOS track is concerned with the"
2020.figlang-1.3,D17-1162,0,0.712342,"al., 2016; Shutova et al., 2016; Beigman Klebanov et al., 2016; Tekiroglu et al., 2015; Tsvetkov et al., 2014; Beigman Klebanov et al., 2014; Dunn, 2013; Neuman et al., 2013; Mohler et al., 2013; Hovy et al., 2013; Tsvetkov et al., 2013; Turney et al., 2011; Shutova et al., 2010; Gedigian et al., 2006); see Shutova et al. (2017) and Veale et al. (2016) for reviews of supervised as well as semi-supervised and unsupervised approaches. Recently, deep learning methods have been explored for token-level metaphor detection (Mao et al., 2019; Dankers et al., 2019; Gao et al., 2018; Wu et al., 2018; Rei et al., 2017; Gutierrez et al., 2017; Do Dinh and Gurevych, 2016). In terms of data, researchers used specially constructed or selected sets, such as adjective noun pairs (Gutierrez et al., 2016; Tsvetkov et al., 2014), WordNet synsets and glosses (Mohammad et al., 2016), annotated lexical items (from a range of word classes) in sentences sampled from cor¨ pora (Ozbal et al., 2016; Jang et al., 2015; Hovy et al., 2013; Birke and Sarkar, 2006), all the way to annotation of all words in running text for metaphoricity (Beigman Klebanov et al., 2018; Steen et al., 2010); Veale et al. (2016) review various ann"
2020.figlang-1.3,2020.figlang-1.27,0,0.171076,"fine-grained POS and global text features are the most helpful for detecting metaphors. umd bilstm: Bi-LSTM + Embeddings + Unigram Lemmas + Spell Correction Kuo and Carpuat (2020) explored the effectiveness of additional features by augmenting the basic contextual metaphor detection system developed by Gao et al. (2018) with one-hot unigram lemma features in addition to GloVe and ELMo embeddings. The authors also experimented with a spell-corrected version of TOEFL data and found it further improves the performance of the Bi-LSTM system. atr2112: Residual Bi-LSTM + Embeddings + CRF + POS + WN Rivera et al. (2020) proposed a deep architecture that takes as inputs ELMo embeddings that represent words and lemmas, along with POS labels and WordNet synsets. The inputs are processed by a residual Bi-LSTM, then by a number of additional layers, with a final CRF sequence labeling step to generate predictions. Zenith: Character embeddings + Similarity Networks + Bi-LSTM + Transformer Kumar and Sharma (2020) added lexical and orthographic information via character embeddings in addition to GloVe and ELMo embeddings for an enriched input representation. The authors also constructed a similarity metric between th"
2020.figlang-1.3,N16-1020,0,0.169821,"tection of metaphor has become an popular topic, which manifests itself in both a variety of approaches and in an increasing variety of data to which the methods are applied. In terms of methods, approaches based on feature-engineering in a supervised machine learning paradigm explored features based on concreteness and imageability, semantic classification using WordNet, FrameNet, VerbNet, SUMO ontology, property norms, and distributional semantic models, syntactic dependency patterns, sensorial and vision-based features (Bulat et al., 2017; K¨oper and im Walde, 2017; Gutierrez et al., 2016; Shutova et al., 2016; Beigman Klebanov et al., 2016; Tekiroglu et al., 2015; Tsvetkov et al., 2014; Beigman Klebanov et al., 2014; Dunn, 2013; Neuman et al., 2013; Mohler et al., 2013; Hovy et al., 2013; Tsvetkov et al., 2013; Turney et al., 2011; Shutova et al., 2010; Gedigian et al., 2006); see Shutova et al. (2017) and Veale et al. (2016) for reviews of supervised as well as semi-supervised and unsupervised approaches. Recently, deep learning methods have been explored for token-level metaphor detection (Mao et al., 2019; Dankers et al., 2019; Gao et al., 2018; Wu et al., 2018; Rei et al., 2017; Gutierrez et a"
2020.figlang-1.3,J17-1003,0,0.115096,"Missing"
2020.figlang-1.3,C10-1113,0,0.0661215,"sed machine learning paradigm explored features based on concreteness and imageability, semantic classification using WordNet, FrameNet, VerbNet, SUMO ontology, property norms, and distributional semantic models, syntactic dependency patterns, sensorial and vision-based features (Bulat et al., 2017; K¨oper and im Walde, 2017; Gutierrez et al., 2016; Shutova et al., 2016; Beigman Klebanov et al., 2016; Tekiroglu et al., 2015; Tsvetkov et al., 2014; Beigman Klebanov et al., 2014; Dunn, 2013; Neuman et al., 2013; Mohler et al., 2013; Hovy et al., 2013; Tsvetkov et al., 2013; Turney et al., 2011; Shutova et al., 2010; Gedigian et al., 2006); see Shutova et al. (2017) and Veale et al. (2016) for reviews of supervised as well as semi-supervised and unsupervised approaches. Recently, deep learning methods have been explored for token-level metaphor detection (Mao et al., 2019; Dankers et al., 2019; Gao et al., 2018; Wu et al., 2018; Rei et al., 2017; Gutierrez et al., 2017; Do Dinh and Gurevych, 2016). In terms of data, researchers used specially constructed or selected sets, such as adjective noun pairs (Gutierrez et al., 2016; Tsvetkov et al., 2014), WordNet synsets and glosses (Mohammad et al., 2016), ann"
2020.figlang-1.3,W18-0918,1,0.824734,"features, features based on WordNet, VerbNet, and those derived from a distributional semantic model, POS-based, concreteness and difference in concreteness, as well as topic models. We adopted three informed baselines from prior work. As Baseline 1: UL + WordNet + CCDB, we use the best system from Beigman Klebanov et al. (2016). The features are: lemmatized unigrams, generalized WordNet semantic classes, and difference in concreteness ratings between verbs/adjectives and nouns (UL + WN + CCDB).6 Baseline 2: bot.zen is one of the top-ranked systems in the first metaphor shared task in 2018 by Stemle and Onysko (2018) that uses a bi-directional recursive neural network architecture with long-term short-term memory (LSTM BiRNN) and implements a flat sequenceto-sequence neural network with one hidden layer using TensorFlow and Keras in Python. The system uses fastText word embeddings from different corpora, including learner corpus and BNC data. Finally, Baseline 3: BERT is constructed by finetuning the BERT model (Devlin et al., 2018) in a standard token classification task: After obtaining the contextualized embeddings of a sentence, we apply a linear layer followed by softmax on each token to predict whet"
2020.figlang-1.3,2020.figlang-1.35,1,0.818359,"the training datasets. rowanhm: Static and contextual embeddings + concreteness + Multi-layer Perceptron Maudslay et al. (2020) created a system that combines the concreteness of a word, its static embedding and its contextual embedding before providing them as inputs into a deep Multi-layer Perceptron network which predicts word metaphoricity. Specifically, the concreteness value of a word is formulated as a linear interpolation between two reference vectors (concrete and abstract) which were randomly initialized and learned from data. iiegn: LSTM BiRNN + metadata; combine TOEFL and VUA data Stemle and Onysko (2020) used an LSTM BiRNN classifier to study the relationship between the metadata in the TOEFL corpus (proficiency, L1 of the author, and the prompt to which the essay is responding) and classifier performance. The system is an extension of the authors’ system for the 2018 shared task (Stemle and Onysko, 2018) that served as one of the baseline in the current shared task (see section 4.1). Analyzing the training data, the authors observed that essays written by more proficient users had significantly more metaphors, and that essays responding to some of the prompts had significantly more metaphors"
2020.figlang-1.3,2020.figlang-1.4,0,0.682733,"om linguistic resources (e.g. WordNet, VerbNet) as well as other features (e.g. POS, topicality, concreteness) previously used in the first shared task (Leong et al., 2018) before feeding them into a fully-connected Feedforward network to generate predictions. During inference, an ensemble of three independently trained models using different train/development splits is proposed to yield a final prediction based on majority vote. Using just RoBERTa without linguistic features in an ensemble also generates competitive performance. DeepMet: Global and local text information + Transformer stacks Su et al. (2020) proposed a reading comprehension paradigm for metaphor detection, where the system seeks to understand the metaphoricity role of each word token in a shorter sequence within a given sentence. Features belonging to five different categories are provided as inputs to the network i.e. global text context, local text context, query word, general POS, finegrained POS. The features are then mapped onto embeddings before going into Transformer stacks and ensemble for inference. An ablation experiment was also performed with the observation that fine-grained POS and global text features are the most"
2020.figlang-1.3,W15-1404,0,0.205309,"h manifests itself in both a variety of approaches and in an increasing variety of data to which the methods are applied. In terms of methods, approaches based on feature-engineering in a supervised machine learning paradigm explored features based on concreteness and imageability, semantic classification using WordNet, FrameNet, VerbNet, SUMO ontology, property norms, and distributional semantic models, syntactic dependency patterns, sensorial and vision-based features (Bulat et al., 2017; K¨oper and im Walde, 2017; Gutierrez et al., 2016; Shutova et al., 2016; Beigman Klebanov et al., 2016; Tekiroglu et al., 2015; Tsvetkov et al., 2014; Beigman Klebanov et al., 2014; Dunn, 2013; Neuman et al., 2013; Mohler et al., 2013; Hovy et al., 2013; Tsvetkov et al., 2013; Turney et al., 2011; Shutova et al., 2010; Gedigian et al., 2006); see Shutova et al. (2017) and Veale et al. (2016) for reviews of supervised as well as semi-supervised and unsupervised approaches. Recently, deep learning methods have been explored for token-level metaphor detection (Mao et al., 2019; Dankers et al., 2019; Gao et al., 2018; Wu et al., 2018; Rei et al., 2017; Gutierrez et al., 2017; Do Dinh and Gurevych, 2016). In terms of data"
2020.figlang-1.3,P14-1024,0,0.483204,"th a variety of approaches and in an increasing variety of data to which the methods are applied. In terms of methods, approaches based on feature-engineering in a supervised machine learning paradigm explored features based on concreteness and imageability, semantic classification using WordNet, FrameNet, VerbNet, SUMO ontology, property norms, and distributional semantic models, syntactic dependency patterns, sensorial and vision-based features (Bulat et al., 2017; K¨oper and im Walde, 2017; Gutierrez et al., 2016; Shutova et al., 2016; Beigman Klebanov et al., 2016; Tekiroglu et al., 2015; Tsvetkov et al., 2014; Beigman Klebanov et al., 2014; Dunn, 2013; Neuman et al., 2013; Mohler et al., 2013; Hovy et al., 2013; Tsvetkov et al., 2013; Turney et al., 2011; Shutova et al., 2010; Gedigian et al., 2006); see Shutova et al. (2017) and Veale et al. (2016) for reviews of supervised as well as semi-supervised and unsupervised approaches. Recently, deep learning methods have been explored for token-level metaphor detection (Mao et al., 2019; Dankers et al., 2019; Gao et al., 2018; Wu et al., 2018; Rei et al., 2017; Gutierrez et al., 2017; Do Dinh and Gurevych, 2016). In terms of data, researchers used spec"
2020.figlang-1.3,W13-0906,0,0.164837,"es based on feature-engineering in a supervised machine learning paradigm explored features based on concreteness and imageability, semantic classification using WordNet, FrameNet, VerbNet, SUMO ontology, property norms, and distributional semantic models, syntactic dependency patterns, sensorial and vision-based features (Bulat et al., 2017; K¨oper and im Walde, 2017; Gutierrez et al., 2016; Shutova et al., 2016; Beigman Klebanov et al., 2016; Tekiroglu et al., 2015; Tsvetkov et al., 2014; Beigman Klebanov et al., 2014; Dunn, 2013; Neuman et al., 2013; Mohler et al., 2013; Hovy et al., 2013; Tsvetkov et al., 2013; Turney et al., 2011; Shutova et al., 2010; Gedigian et al., 2006); see Shutova et al. (2017) and Veale et al. (2016) for reviews of supervised as well as semi-supervised and unsupervised approaches. Recently, deep learning methods have been explored for token-level metaphor detection (Mao et al., 2019; Dankers et al., 2019; Gao et al., 2018; Wu et al., 2018; Rei et al., 2017; Gutierrez et al., 2017; Do Dinh and Gurevych, 2016). In terms of data, researchers used specially constructed or selected sets, such as adjective noun pairs (Gutierrez et al., 2016; Tsvetkov et al., 2014), WordNet synse"
2020.figlang-1.3,D11-1063,0,0.288304,"ineering in a supervised machine learning paradigm explored features based on concreteness and imageability, semantic classification using WordNet, FrameNet, VerbNet, SUMO ontology, property norms, and distributional semantic models, syntactic dependency patterns, sensorial and vision-based features (Bulat et al., 2017; K¨oper and im Walde, 2017; Gutierrez et al., 2016; Shutova et al., 2016; Beigman Klebanov et al., 2016; Tekiroglu et al., 2015; Tsvetkov et al., 2014; Beigman Klebanov et al., 2014; Dunn, 2013; Neuman et al., 2013; Mohler et al., 2013; Hovy et al., 2013; Tsvetkov et al., 2013; Turney et al., 2011; Shutova et al., 2010; Gedigian et al., 2006); see Shutova et al. (2017) and Veale et al. (2016) for reviews of supervised as well as semi-supervised and unsupervised approaches. Recently, deep learning methods have been explored for token-level metaphor detection (Mao et al., 2019; Dankers et al., 2019; Gao et al., 2018; Wu et al., 2018; Rei et al., 2017; Gutierrez et al., 2017; Do Dinh and Gurevych, 2016). In terms of data, researchers used specially constructed or selected sets, such as adjective noun pairs (Gutierrez et al., 2016; Tsvetkov et al., 2014), WordNet synsets and glosses (Moham"
2020.figlang-1.3,2020.figlang-1.16,0,0.440132,"ormation helps improve metaphor detection, relative to a baseline that uses GloVe embeddings only. zhengchang: ALBERT + BiLSTM Li et al. (2020) use a sequence labeling model based on ALBERT-LSTM-Softmax. Embeddings produced by BERT serve as input to BiLSTM, as well as to the final softmax layer. The authors report on experiments with inputs to BERT (single-sentence vs pairs; variants using BERT tokenization), spellcorrection of the TOEFL data, and CRF vs softmax at the classification layer. PolyU-LLT: Sensorimotor and embodiment features + embeddings + n-grams + logistic regression classifier Wan et al. (2020) use sensorimotor and embodiment features. They use the Lancaster Sensorimotor norms (Lynott et al., 2019) that include measures of sensorimotor strength for about 40K English words across six perceptual modalities (e.g., touch, hearing, smell), and five action effectors (mouth/throat, hand/arm, etc), and embodiment norms from Sidhu et al. (2014). The authors also use word, lemma, and POS n-grams; word2vec and GloVe word embeddings, as well as cosine distance measurements using the embeddings. The different features are combined using logistic regression and other classifiers. 5 Results and Di"
2020.figlang-1.32,W13-0902,1,0.850327,"Missing"
2020.figlang-1.32,N18-2014,1,0.910795,"er subjects and contexts (Lakoff and Johnson, 2008); it is commonly used to help us understand the world in a structured way, and oftentimes in an unconscious manner while we speak and write. It sheds light on the unknown using the known, explains the complex using the simple, and helps us to emphasize the relevant aspects of meaning resulting in effective communication. There is a large body of work in the literature that discusses how metaphor has been used in the context of political communication, marketing, mental health, teaching, assessment of English proficiency, among others (Beigman Klebanov et al., 2018; Gutierrez et al., 2017; Littlemore et al., 2013; Thibodeau and Boroditsky, 2011; Kaviani and Hamedi, 2011; Kathpalia and Carmel, 2011; Landau et al., 2009; Beigman Klebanov et al., 2008; Zaltman and Zaltman, 2008; Littlemore and Low, 2006; Cameron, 2003; Lakoff, 2010; Billow et al., 1997; Bosman, 1987); see chapter 7 in Veale et al. (2016) for a recent review. 1 https://sites.google.com/view/figlang2020/home 235 Proceedings of the Second Workshop on Figurative Language Processing, pages 235–243 c July 9, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 The"
2020.figlang-1.32,P16-2017,1,0.736628,"such as RoBERTa (Liu et al., 2019), ERNIE (Sun et al., 2019), XLNet (Yang et al., 2019); we use the most basic model (bert-baseuncased). We fine-tune the BERT model as a standard token classification task, that is, after obtaining the contextualized embeddings of a sentence, we apply a linear layer followed by softmax on each token to predict whether it is metaphorical or not. Fig 1 shows the architecture of the baseline model. We tune the hyperparameters based on cross-validation on training data. The fold partitions for the VUA corpus are the same as the ones used for experiments in Beigman Klebanov et al. (2016). For the TOEFL corpus, we obtained the folds information from the shared task organizers directly. We select batch size in {16, 32, 64}, number of training epochs in {2, 3, 4, 5}, and use a fixed learning rate of 3 × 10−5 . We also apply the learning rate scheduler known as slanted triangular (Howard and Ruder, 2018). Due to the imbalanced class distribution in our data (see Table 2), the positive class is up-weighted by a factor of 3. The same setting applies to experiments described in all the following sections. TOEFL corpus This data labeled for metaphor was sampled from the publicly avai"
2020.figlang-1.32,D19-1227,0,0.122573,"Missing"
2020.figlang-1.32,W18-0905,1,0.771022,"Missing"
2020.figlang-1.32,2020.figlang-1.3,1,0.725565,"that the system trained on VUA data considered metaphorical were not considered so in a system that was exposed to TOEFL data Results Tables 4 and 5 show performance of the various systems on AllPOS and Verbs-only tasks, respectively, for both VUA and TOEFL data. Since it is clear that spelling correction is useful for improving performance on TOEFL data, we used the spell corrected version of the data for all the systems from experiments 2 and 3 on TOEFL data. Our best-performing systems reported here are also benchmarked against other participating systems in the shared task summary report (Leong et al., 2020). We obtained a ranking of 2nd and 4th in the VUA and TOEFL tasks, respectively. Since VUA data contains well-edited BNC text, we did not run spelling correction on VUA data. For the Verbs tracks, we experimented with both (a) training on AllPOS data and evaluating on the 3 Discussion https://catalog.ldc.upenn.edu/LDC2014T06 239 BNC find oneself other than long time great deal once again ups and downs once more much less come through old woman bear in mind cup of tea day-to-day ask the question let alone need-to-know common law close one’s eyes blue-eyed change one’s mind TOEFL 11 long time ne"
2020.figlang-1.32,W19-4407,1,0.831151,"lti-task system Experiment 1: Spell correction system Proper automatic detection of lexically-anchored phenomena in text often depends on availability of correct spelling in the text. The contribution of spelling correction to other tasks has been documented previously, especially for English texts produced by non-native learners of English (Rozovskaya and Roth, 2016; Granger and Wynne, 1999). Essays written by TOEFL testtakers are known to contain a considerable amount of spelling errors (Flor et al., 2015). To alleviate this, we used a state-of-the-art automatic spelling corrections system (Flor et al., 2019) to correct spelling in the TOEFL dataset. Specifically, for the training partition of the TOEFL dataset, the 5.1 Experiment 2: Learning from out-of-domain data Since both the VUA and the TOEFL corpora are annotated for metaphors, using one to help the other during learning could potentially provide additional relevant training data. However, since the data is from different types of texts and dif237 in psycholinguistic literature (Gibbs and O’Brien, 1990; Nunberg et al., 1994; Glucksberg, 2001). The main idea here is that one or more of the words participating in idiomatic expressions are oft"
2020.figlang-1.32,W18-0907,1,0.706766,"ed using the MIP-VU procedure with a strong inter-annotator reliability of κ > 0.8. It is based on the MIP procedure (Pragglejaz, 2007), extending it to handle metaphoricity through reference (such as marking did as a metaphor in As the weather broke up, so did their friendship) and allow for explicit coding of difficult cases where a group of annotators could not arrive at a consensus. Note that we only considered words marked as metaphors decided as such by the shared task organizers. The VUA dataset and annotations is the same as the one used in the first shared task on metaphor detection (Leong et al., 2018). 2.2 We build our baseline system based on BERT (Devlin et al., 2018). BERT (Bidirectional Encoder Representations from Transformers) is a transformer (Vaswani et al., 2017) model that is pretrained on a large quantity of texts, and obtained state-of-the-art performance on many NLP benchmarks (Wang et al., 2018; Zellers et al., 2018; Rajpurkar et al., 2016). Since its introduction, there have been many improvements over the original BERT model, such as RoBERTa (Liu et al., 2019), ERNIE (Sun et al., 2019), XLNet (Yang et al., 2019); we use the most basic model (bert-baseuncased). We fine-tune"
2020.figlang-1.32,2021.ccl-1.108,0,0.152927,"Missing"
2020.figlang-1.32,D17-1316,0,0.11303,"s (Lakoff and Johnson, 2008); it is commonly used to help us understand the world in a structured way, and oftentimes in an unconscious manner while we speak and write. It sheds light on the unknown using the known, explains the complex using the simple, and helps us to emphasize the relevant aspects of meaning resulting in effective communication. There is a large body of work in the literature that discusses how metaphor has been used in the context of political communication, marketing, mental health, teaching, assessment of English proficiency, among others (Beigman Klebanov et al., 2018; Gutierrez et al., 2017; Littlemore et al., 2013; Thibodeau and Boroditsky, 2011; Kaviani and Hamedi, 2011; Kathpalia and Carmel, 2011; Landau et al., 2009; Beigman Klebanov et al., 2008; Zaltman and Zaltman, 2008; Littlemore and Low, 2006; Cameron, 2003; Lakoff, 2010; Billow et al., 1997; Bosman, 1987); see chapter 7 in Veale et al. (2016) for a recent review. 1 https://sites.google.com/view/figlang2020/home 235 Proceedings of the Second Workshop on Figurative Language Processing, pages 235–243 c July 9, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 The second auxiliary task i"
2020.figlang-1.32,P19-1378,0,0.360599,"Missing"
2020.figlang-1.32,P18-1031,0,0.0284072,"x on each token to predict whether it is metaphorical or not. Fig 1 shows the architecture of the baseline model. We tune the hyperparameters based on cross-validation on training data. The fold partitions for the VUA corpus are the same as the ones used for experiments in Beigman Klebanov et al. (2016). For the TOEFL corpus, we obtained the folds information from the shared task organizers directly. We select batch size in {16, 32, 64}, number of training epochs in {2, 3, 4, 5}, and use a fixed learning rate of 3 × 10−5 . We also apply the learning rate scheduler known as slanted triangular (Howard and Ruder, 2018). Due to the imbalanced class distribution in our data (see Table 2), the positive class is up-weighted by a factor of 3. The same setting applies to experiments described in all the following sections. TOEFL corpus This data labeled for metaphor was sampled from the publicly available ETS Corpus of NonNative Written English2 (Blanchard et al., 2013) and was first introduced by (Beigman Klebanov et al., 2018). The annotated data comprises essay responses to eight persuaisve/argumentative prompts, for three native languages of the writer (Japanese, Italian, Arabic), and for two proficiency leve"
2020.figlang-1.32,Y18-1025,0,0.0519703,"Missing"
2020.figlang-1.32,D16-1264,0,0.0155199,"could not arrive at a consensus. Note that we only considered words marked as metaphors decided as such by the shared task organizers. The VUA dataset and annotations is the same as the one used in the first shared task on metaphor detection (Leong et al., 2018). 2.2 We build our baseline system based on BERT (Devlin et al., 2018). BERT (Bidirectional Encoder Representations from Transformers) is a transformer (Vaswani et al., 2017) model that is pretrained on a large quantity of texts, and obtained state-of-the-art performance on many NLP benchmarks (Wang et al., 2018; Zellers et al., 2018; Rajpurkar et al., 2016). Since its introduction, there have been many improvements over the original BERT model, such as RoBERTa (Liu et al., 2019), ERNIE (Sun et al., 2019), XLNet (Yang et al., 2019); we use the most basic model (bert-baseuncased). We fine-tune the BERT model as a standard token classification task, that is, after obtaining the contextualized embeddings of a sentence, we apply a linear layer followed by softmax on each token to predict whether it is metaphorical or not. Fig 1 shows the architecture of the baseline model. We tune the hyperparameters based on cross-validation on training data. The fo"
2020.figlang-1.32,P16-1208,0,0.0178634,"by a factor of 0.1. The hyperparameters are selected in the same way as described in section 3. Figure 1: Baseline system architecture. The output is a pair of probabilities – the first for class 0 (nonmetaphor) and the second for class 1 (metaphor). 4 Multi-task system Experiment 1: Spell correction system Proper automatic detection of lexically-anchored phenomena in text often depends on availability of correct spelling in the text. The contribution of spelling correction to other tasks has been documented previously, especially for English texts produced by non-native learners of English (Rozovskaya and Roth, 2016; Granger and Wynne, 1999). Essays written by TOEFL testtakers are known to contain a considerable amount of spelling errors (Flor et al., 2015). To alleviate this, we used a state-of-the-art automatic spelling corrections system (Flor et al., 2019) to correct spelling in the TOEFL dataset. Specifically, for the training partition of the TOEFL dataset, the 5.1 Experiment 2: Learning from out-of-domain data Since both the VUA and the TOEFL corpora are annotated for metaphors, using one to help the other during learning could potentially provide additional relevant training data. However, since"
2020.figlang-1.32,W18-5446,0,0.0606397,"Missing"
2020.figlang-1.32,D18-1009,0,0.0177003,"a group of annotators could not arrive at a consensus. Note that we only considered words marked as metaphors decided as such by the shared task organizers. The VUA dataset and annotations is the same as the one used in the first shared task on metaphor detection (Leong et al., 2018). 2.2 We build our baseline system based on BERT (Devlin et al., 2018). BERT (Bidirectional Encoder Representations from Transformers) is a transformer (Vaswani et al., 2017) model that is pretrained on a large quantity of texts, and obtained state-of-the-art performance on many NLP benchmarks (Wang et al., 2018; Zellers et al., 2018; Rajpurkar et al., 2016). Since its introduction, there have been many improvements over the original BERT model, such as RoBERTa (Liu et al., 2019), ERNIE (Sun et al., 2019), XLNet (Yang et al., 2019); we use the most basic model (bert-baseuncased). We fine-tune the BERT model as a standard token classification task, that is, after obtaining the contextualized embeddings of a sentence, we apply a linear layer followed by softmax on each token to predict whether it is metaphorical or not. Fig 1 shows the architecture of the baseline model. We tune the hyperparameters based on cross-validation"
C18-2025,P13-1113,1,0.833458,"feedback. Claims Sources Topic Development Flow of Ideas Transition Terms Long Sentences Headers Use of Anaphora Grammar, Usage, & Mechanic Errors Claim Verbs Word Choice Contractions Convincing Arguing expressions from a lexicon (Burstein et al., 1998) that contains discourse cue terms and relations (e.g., contrast, parallel, summary) and arguing expressions classified by stance (for/against) & type (hedges vs. boosters). Rule-based system that detects in-text formal citations consistent with MLA, APA and Chicago styles. Well-developed Detection of topics and their related word sets (Beigman Klebanov and Flor, 2013; Burstein et al., 2016a) Coherent Leverages terms in a document generated for the main topic (as identified by Topic Development above) and their related word sets. Identified using the same lexicon as in Claims above. Sentences with 1 independent clause & 1+ dependent clauses, identified using a syntactic chunker (Abney, 1996; Burstein and Chodorow, 1999) Rule-based system using regular expressions to identify title & section headers. Pronouns identified using a part-of-speech tagger (Ratnaparkhi, 1996). Well-Edited 9 automatically-detected grammar error feature types, 12 automatically-detec"
C18-2025,W99-0411,1,0.597332,"essions classified by stance (for/against) & type (hedges vs. boosters). Rule-based system that detects in-text formal citations consistent with MLA, APA and Chicago styles. Well-developed Detection of topics and their related word sets (Beigman Klebanov and Flor, 2013; Burstein et al., 2016a) Coherent Leverages terms in a document generated for the main topic (as identified by Topic Development above) and their related word sets. Identified using the same lexicon as in Claims above. Sentences with 1 independent clause & 1+ dependent clauses, identified using a syntactic chunker (Abney, 1996; Burstein and Chodorow, 1999) Rule-based system using regular expressions to identify title & section headers. Pronouns identified using a part-of-speech tagger (Ratnaparkhi, 1996). Well-Edited 9 automatically-detected grammar error feature types, 12 automatically-detected mechanics error feature types, and 10 automatically-detected word usage error feature types (Attali and Burstein, 2006). Verbs denoting claims from the lexicon used in Claims above. Rule-based system that detects words and expressions related to a set of 13 ‘unnecessary’ words and terms, e.g. very, literally, a total of. Identified using a part-of-speec"
C18-2025,P98-1032,1,0.719671,"plete College America, 2012). We describe Writing Mentor, an NLP-based solution to the literacy challenge that is designed to help struggling writers in 2- and 4-year colleges improve their writing at a self-regulated pace. Writing Mentor is a Google Docs add-on1 that provides automated instructional feedback focused on four key writing skills: credibility of claims, topic development, coherence, and editing. Writing mentor builds on a large body of research in the area of automated writing evaluation (AWE) which has so far primarily been used for scoring standardized assessments (Page, 1966; Burstein et al., 1998; Attali and Burstein, 2006; Zechner et al., 2009; Bernstein et al., 2010). Burstein et al. (2017) examined relationships between NLPderived linguistic features extracted from college writing samples and broader success indicators (such as, SAT and ACT composite and subject scores). Their findings suggested that AWE can also be useful for generating automated feedback that can help students with their writing. Writing Mentor has been developed to provide one-stop-shopping for writers looking for help with academic writing. Apps such as Grammarly and LanguageTool, cater to individual users but"
C18-2025,W17-5011,1,0.809701,"Missing"
C18-2025,W96-0213,0,0.470675,"cago styles. Well-developed Detection of topics and their related word sets (Beigman Klebanov and Flor, 2013; Burstein et al., 2016a) Coherent Leverages terms in a document generated for the main topic (as identified by Topic Development above) and their related word sets. Identified using the same lexicon as in Claims above. Sentences with 1 independent clause & 1+ dependent clauses, identified using a syntactic chunker (Abney, 1996; Burstein and Chodorow, 1999) Rule-based system using regular expressions to identify title & section headers. Pronouns identified using a part-of-speech tagger (Ratnaparkhi, 1996). Well-Edited 9 automatically-detected grammar error feature types, 12 automatically-detected mechanics error feature types, and 10 automatically-detected word usage error feature types (Attali and Burstein, 2006). Verbs denoting claims from the lexicon used in Claims above. Rule-based system that detects words and expressions related to a set of 13 ‘unnecessary’ words and terms, e.g. very, literally, a total of. Identified using a part-of-speech tagger (Ratnaparkhi, 1996). Table 1: Inventory of features provided by the NLP backend, grouped by Writing Mentor feedback types. 2 Description Writi"
J09-4005,J08-4004,0,0.362144,"ses.11 5.2 Plausibility of the Model Beyond the separation into easy and hard instances, our model prescribes certain annotator behavior for each type. In our work on metaphor, we observed that certain metaphor markups were retracted by their authors, when asked after 4–8 weeks to revisit the annotations (Beigman Klebanov, Beigman, and Diermeier 2008). These were apparently hard cases, with people resolving their doubts inconsistently on the two occasions; coin-ﬂipping is a reasonable ﬁrst-cut model for such cases. The model also accommodates category over-use bias (Di Eugenio and Glass 2004; Artstein and Poesio 2008; Reidsma and Carletta 2008), as P(Xij = bj |li = H) may vary across annotators. Still, this model is clearly a simpliﬁcation. For example, it is possible that there is more than one degree of hardness, and annotator behavior changes accordingly. Another extension is modeling imperfect annotators, allowed to commit random errors on easy cases; this extension would be needed if a large number of annotators is used. Such extensions, as well as methods for estimating these more complex models, should clearly be put on the community’s research agenda. The main contribution of the simple model is i"
J09-4005,P09-1032,1,0.826091,"Missing"
J09-4005,W08-1202,1,0.875837,"Missing"
J09-4005,J96-2004,0,0.130752,"l of the agreed subset of annotated data, which helps promote cautious benchmarking. 1. Introduction By and large, the reason a computational linguist engages in an annotation project is to build a reliable data set for the eventual testing, and possibly training, of an algorithm performing the task. Hence, the crucial question regarding the annotated data set is whether it is good for benchmarking. For classiﬁcation tasks, the current practice is to infer this information from the value of an inter-annotator agreement coefﬁcient such as the κ statistic (Cohen 1960; Siegel and Castellan 1988; Carletta 1996). If agreement is high, the whole of the data set is good for training and testing; the remaining disagreements are typically adjudicated by an expert (Snyder and Palmer 2004; Palmer, Kingsbury, and Gildea 2005; Girju, Badulescu, and Moldovan 2006) or through discussion (Litman, Hirschberg, and Swerts 2006), or, in case of more than two annotators, the majority label is chosen (Vieira and Poesio 2000).1 There are some studies where cases of disagreement were removed from test data (Markert and Nissim 2002; Dagan, Glickman, and Magnini 2006). If agreement is low, the whole data set is discarded"
J09-4005,J02-3001,0,0.00960087,"wever, of exactly how and how well the value of κ reﬂects the quality of the data for benchmarking purposes. We develop a model of annotation generation that allows estimation of the level of noise in a specially constructed gold standard. A gold standard with a noise ﬁgure supports cautious benchmarking, ∗ Kellogg School of Management, Northwestern University, Evanston, IL, beata@northwestern.edu. ∗∗ Kellogg School of Management, Northwestern University, Evanston, IL, e-beigman@northwestern.edu. 1 In many studies, the procedure for handling disagreements is not clearly speciﬁed. For example, Gildea and Jurafsky (2002) mention a “consistency check”; in Lapata (2002), two annotators attained κ = 0.78 on 200 test instances, but it is not clear how cases of disagreements were settled. Submission received: 30 June 2008; revised submission received: 3 December 2008; accepted for publication: 26 January 2009. © 2009 Association for Computational Linguistics Computational Linguistics Volume 35, Number 4 by requiring that the performance of an algorithm be better than baseline by more than that which can be attributed to noise. Articulating an annotation generation model also allows us to shed light on the informat"
J09-4005,J06-1005,0,0.0211092,"Missing"
J09-4005,J02-3004,0,0.00992533,"quality of the data for benchmarking purposes. We develop a model of annotation generation that allows estimation of the level of noise in a specially constructed gold standard. A gold standard with a noise ﬁgure supports cautious benchmarking, ∗ Kellogg School of Management, Northwestern University, Evanston, IL, beata@northwestern.edu. ∗∗ Kellogg School of Management, Northwestern University, Evanston, IL, e-beigman@northwestern.edu. 1 In many studies, the procedure for handling disagreements is not clearly speciﬁed. For example, Gildea and Jurafsky (2002) mention a “consistency check”; in Lapata (2002), two annotators attained κ = 0.78 on 200 test instances, but it is not clear how cases of disagreements were settled. Submission received: 30 June 2008; revised submission received: 3 December 2008; accepted for publication: 26 January 2009. © 2009 Association for Computational Linguistics Computational Linguistics Volume 35, Number 4 by requiring that the performance of an algorithm be better than baseline by more than that which can be attributed to noise. Articulating an annotation generation model also allows us to shed light on the information κ can contribute to benchmarking. 2. Annotat"
J09-4005,J06-3004,0,0.052658,"Missing"
J09-4005,W02-1027,0,0.0190992,"ter-annotator agreement coefﬁcient such as the κ statistic (Cohen 1960; Siegel and Castellan 1988; Carletta 1996). If agreement is high, the whole of the data set is good for training and testing; the remaining disagreements are typically adjudicated by an expert (Snyder and Palmer 2004; Palmer, Kingsbury, and Gildea 2005; Girju, Badulescu, and Moldovan 2006) or through discussion (Litman, Hirschberg, and Swerts 2006), or, in case of more than two annotators, the majority label is chosen (Vieira and Poesio 2000).1 There are some studies where cases of disagreement were removed from test data (Markert and Nissim 2002; Dagan, Glickman, and Magnini 2006). If agreement is low, the whole data set is discarded as unreliable. The threshold of acceptability seems to have stabilized around κ = 0.67 (Carletta 1996; Di Eugenio and Glass 2004). There is little understanding, however, of exactly how and how well the value of κ reﬂects the quality of the data for benchmarking purposes. We develop a model of annotation generation that allows estimation of the level of noise in a specially constructed gold standard. A gold standard with a noise ﬁgure supports cautious benchmarking, ∗ Kellogg School of Management, Northw"
J09-4005,J05-1004,0,0.0634223,"Missing"
J09-4005,J98-2001,0,0.0606162,"Missing"
J09-4005,W08-1203,0,0.09594,"Missing"
J09-4005,W04-0811,0,0.0191173,"nnotation project is to build a reliable data set for the eventual testing, and possibly training, of an algorithm performing the task. Hence, the crucial question regarding the annotated data set is whether it is good for benchmarking. For classiﬁcation tasks, the current practice is to infer this information from the value of an inter-annotator agreement coefﬁcient such as the κ statistic (Cohen 1960; Siegel and Castellan 1988; Carletta 1996). If agreement is high, the whole of the data set is good for training and testing; the remaining disagreements are typically adjudicated by an expert (Snyder and Palmer 2004; Palmer, Kingsbury, and Gildea 2005; Girju, Badulescu, and Moldovan 2006) or through discussion (Litman, Hirschberg, and Swerts 2006), or, in case of more than two annotators, the majority label is chosen (Vieira and Poesio 2000).1 There are some studies where cases of disagreement were removed from test data (Markert and Nissim 2002; Dagan, Glickman, and Magnini 2006). If agreement is low, the whole data set is discarded as unreliable. The threshold of acceptability seems to have stabilized around κ = 0.67 (Carletta 1996; Di Eugenio and Glass 2004). There is little understanding, however, of"
J09-4005,J00-4003,0,0.0542595,"arking. For classiﬁcation tasks, the current practice is to infer this information from the value of an inter-annotator agreement coefﬁcient such as the κ statistic (Cohen 1960; Siegel and Castellan 1988; Carletta 1996). If agreement is high, the whole of the data set is good for training and testing; the remaining disagreements are typically adjudicated by an expert (Snyder and Palmer 2004; Palmer, Kingsbury, and Gildea 2005; Girju, Badulescu, and Moldovan 2006) or through discussion (Litman, Hirschberg, and Swerts 2006), or, in case of more than two annotators, the majority label is chosen (Vieira and Poesio 2000).1 There are some studies where cases of disagreement were removed from test data (Markert and Nissim 2002; Dagan, Glickman, and Magnini 2006). If agreement is low, the whole data set is discarded as unreliable. The threshold of acceptability seems to have stabilized around κ = 0.67 (Carletta 1996; Di Eugenio and Glass 2004). There is little understanding, however, of exactly how and how well the value of κ reﬂects the quality of the data for benchmarking purposes. We develop a model of annotation generation that allows estimation of the level of noise in a specially constructed gold standard."
J09-4005,J08-3001,0,\N,Missing
J09-4005,W07-1401,0,\N,Missing
J09-4005,W10-1106,0,\N,Missing
J09-4005,P07-1096,0,\N,Missing
N06-2004,W97-0703,0,0.109531,"lly does better than competing WordNet-based measures (section 5). We discuss future directions in section 6. In this paper, we (1) propose a new dataset for testing the degree of relatedness between pairs of words; (2) propose a new WordNet-based measure of relatedness, and evaluate it on the new dataset. 1 Introduction Estimating the degree of semantic relatedness between words in a text is deemed important in numerous applications: word-sense disambiguation (Banerjee and Pedersen, 2003), story segmentation (Stokes et al., 2004), error correction (Hirst and Budanitsky, 2005), summarization (Barzilay and Elhadad, 1997; Gurevych and Strube, 2004). Furthermore, Budanitsky and Hirst (2006) noted that various applications tend to pick the same measures of relatedness, which suggests a certain commonality in what is required from such a measure by the different applications. It thus seems worthwhile to develop such measures intrinsically, before putting them to application-based utility tests. The most popular, by-now-standard testbed is Rubenstein and Goodenough’s (1965) list of 65 noun pairs, ranked by similarity of meaning. A 30-pair subset (henceforth, MC) passed a number of replications (Miller and Charles"
N06-2004,N06-2004,1,0.0512178,"pare scores emerging from the different subgroups. We know from Beigman Klebanov and Shamir’s (2006) analysis that it is not the case that the 20-subject group clusters into subgroups that systematically produced different patterns of answers. This leads us to expect relative lack of sensitivity to the exact splits into subgroups. To validate this reasoning, we performed 100 random choices of two 9-subject4 groups, calculated the scores induced by the two groups, and computed 3 Two subjects were revealed as outliers and their data was removed (Beigman Klebanov and Shamir, 2006). 4 See Beigman Klebanov (2006) for details. 14 Pearson correlation between the two lists. Thus, for every BS text, we have a distribution of 100 coefficients, which is approximately normal. Estimations of µ and σ of these distributions are µ = .69 − .82 (av. 0.75), σ = .02 − .03 for the different BS texts. To summarize: although the homogeneity is lower than for MC data, we observe good average intergroup correlations with little deviation across the 100 splits. We now turn to discussion of a relatedness measure, which we will evaluate using the data. 4 Gic: WordNet-based Measure Measures using WordNet taxonomy are state-o"
N06-2004,C04-1110,0,0.225843,"ng WordNet-based measures (section 5). We discuss future directions in section 6. In this paper, we (1) propose a new dataset for testing the degree of relatedness between pairs of words; (2) propose a new WordNet-based measure of relatedness, and evaluate it on the new dataset. 1 Introduction Estimating the degree of semantic relatedness between words in a text is deemed important in numerous applications: word-sense disambiguation (Banerjee and Pedersen, 2003), story segmentation (Stokes et al., 2004), error correction (Hirst and Budanitsky, 2005), summarization (Barzilay and Elhadad, 1997; Gurevych and Strube, 2004). Furthermore, Budanitsky and Hirst (2006) noted that various applications tend to pick the same measures of relatedness, which suggests a certain commonality in what is required from such a measure by the different applications. It thus seems worthwhile to develop such measures intrinsically, before putting them to application-based utility tests. The most popular, by-now-standard testbed is Rubenstein and Goodenough’s (1965) list of 65 noun pairs, ranked by similarity of meaning. A 30-pair subset (henceforth, MC) passed a number of replications (Miller and Charles, 1991; Resnik, 1995), and i"
N06-2004,I05-1067,0,0.0979585,"oring relations they could find. The rendering of relatedness between two concepts is not tied to any specific lexical relation, but rather to common-sense knowledge, which has to do with “knowledge of kinds, of associations, of typical situations, and even typical utterances”.2 The phenomenon is thus clearly construed as much broader than degree-of-synonymy. Beigman Klebanov and Shamir (2006) provide reliability estimation of the experimental data using ness” (Hirst and Budanitsky, 2005); “To our knowledge, no datasets are available for validating the results of semantic relatedness metric” (Gurevych, 2005). 2 according to Hirst (2000), cited in the guidelines 13 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 13–16, c New York, June 2006. 2006 Association for Computational Linguistics statistical analysis and a validation experiment, identifying reliably anchored items with their strong anchors, and reliably un-anchored items. Such analysis provides high-validity data for classification; however, much of the data regarding intermediate degrees of relatedness is left out. 3 Relatedness Scores Our idea is to induce scores for pairs of anchor"
N06-2004,O97-1002,0,0.203415,"y their glosses, but also glosses of items standing in various WordNet relations with A and B. For example, it compares the gloss of A’s meronym to that of B’s hyponym. We use the default configuration of the measure in WordNet::Similarity-0.12 package (Pedersen et al., 2004), and, with a single exception, the measure performed below Gic; see BP in table 1. As mentioned before, taxonomy-based similarity measures cannot fully handle BS data. Table 2 uses nominal-only subsets of BS data and the MC nominal similarity dataset to show that (a) state-of-the-art WordNet-based similarity measure JC8 (Jiang and Conrath, 1997; Budanitsky and Hirst, 2006) does very poorly on the relatedness data, suggesting that nominal similarity and relatedness are rather different things; (b) Gic does better on average, and is more robust; (c) Gic yields on MC to gain performance on BS, whereas BP is no more inclined tosingle word which is relatively rarely used in glosses; (b) the multitude of low-IC items in many of the overlaps that tend to downplay the impact of the few higher-IC members of the overlap. 7 To speed the processing up, we use first 5 WordNet senses of each item for results reported here. 8 See formula in append"
N06-2004,N04-3012,0,0.0804132,"ws suit. AApair son – man son – family son – mother Human 2 13 16 Gic 0.355 0.375 0.370 JC 22.3 16.9 20.1 Related Work Table 3: Relatendess vs. similarity We compare Gic to another WordNet-based measure that can handle cross-POS comparisons, proposed by Banerjee and Pedersen (2003). To compare word senses A and B, the algorithm compares not only their glosses, but also glosses of items standing in various WordNet relations with A and B. For example, it compares the gloss of A’s meronym to that of B’s hyponym. We use the default configuration of the measure in WordNet::Similarity-0.12 package (Pedersen et al., 2004), and, with a single exception, the measure performed below Gic; see BP in table 1. As mentioned before, taxonomy-based similarity measures cannot fully handle BS data. Table 2 uses nominal-only subsets of BS data and the MC nominal similarity dataset to show that (a) state-of-the-art WordNet-based similarity measure JC8 (Jiang and Conrath, 1997; Budanitsky and Hirst, 2006) does very poorly on the relatedness data, suggesting that nominal similarity and relatedness are rather different things; (b) Gic does better on average, and is more robust; (c) Gic yields on MC to gain performance on BS, w"
N06-2004,J06-1003,0,\N,Missing
N10-1067,P09-1032,1,0.884232,"Missing"
N10-1067,D09-1030,0,0.0815222,"can be used to zero in on unreliable instances, facilitating generation of cleaner gold standards for benchmarking. 1 Introduction Traditionally, studies in computational linguistics use few trained annotators. Lately this might be changing, as inexpensive annotators are available in large numbers through projects like Amazon Mechanical Turk or through online games where annotations are produced as a by-product (Poesio et al., 2008; von Ahn, 2006), and, at least for certain tasks, the quality of multiple non-expert annotations is close to that of a small number of experts (Snow et al., 2008; Callison-Burch, 2009). Apart from the reduced costs, mass annotation is a promising way to get detailed information about the dataset, such as the level of difficulty of the difference instances. Such information is important both from the linguistic and from the machine learning perspective, as the existence of a group of instances difficult enough to look like they have been labeled by random guesses can in the worst case induce the machine learner training on the dataset to misclassify a constant proportion of easy, noncontroversial instances, as well as produce incorrect comparative results in a benchmarking s"
N10-1067,poesio-etal-2008-anawiki,0,0.0298419,"Beigman Klebanov, 2009). In this paper, we provide quantitative empirical evidence for the existence of this type of noise in a recently benchmarked dataset. The proposed methodology can be used to zero in on unreliable instances, facilitating generation of cleaner gold standards for benchmarking. 1 Introduction Traditionally, studies in computational linguistics use few trained annotators. Lately this might be changing, as inexpensive annotators are available in large numbers through projects like Amazon Mechanical Turk or through online games where annotations are produced as a by-product (Poesio et al., 2008; von Ahn, 2006), and, at least for certain tasks, the quality of multiple non-expert annotations is close to that of a small number of experts (Snow et al., 2008; Callison-Burch, 2009). Apart from the reduced costs, mass annotation is a promising way to get detailed information about the dataset, such as the level of difficulty of the difference instances. Such information is important both from the linguistic and from the machine learning perspective, as the existence of a group of instances difficult enough to look like they have been labeled by random guesses can in the worst case induce t"
N10-1067,J08-3001,0,0.026942,"Missing"
N10-1067,D08-1027,0,0.0345283,"Missing"
N10-1067,J09-4005,1,\N,Missing
N10-1067,W07-1401,0,\N,Missing
N18-1195,N15-3020,0,0.0136824,"b) same passages as used for model training but read by a different narrator; (c) different passages (test partition) read by a different narrator. 6.2 Baseline: text complexity We used text complexity as our baseline, following the practice in the reading assessment community. While we do not expect either of the narrators to experience any reading comprehension difficulties, one might reasonably assume that a skilled narrator would slow down on fragments which are harder for the listener to comprehend. We used TextEvaluator,4 a state-of-the-art measure of comprehension complexity of a text (Napolitano et al., 2015; Sheehan et al., 2014, 2013; Nelson et al., 2012).5 TextEvaluator extracts a range of linguistic features and uses them to compute a complexity score on the scale of 100– 2000. TextEvaluator computes three complexity scores based on the models optimized for literary, informational and mixed texts. We used the literary metric. The average complexity score for passages in the training set was 613.1, with a large variation across passages: min=240, max=1,019, 4 https://textevaluator.ets.org/ TextEvaluator appears in the Nelson et al. (2012) benchmark as SourceRater. 5 SD=154.75. In other words,"
N18-1195,W13-1506,0,0.0682504,"Missing"
N18-2014,P16-2089,0,0.0138479,"anov et al. (2015) reported an evaluation of a metaphor detection system on students’ writing; however, their corpus was not released for public use. Our contributions are as follows: (1) We release metaphor annotations of 240 argumentative essays written by non-native speakers of English. This is the first publicly available metaphor annotated data in this genre we are aware of. (2) We evaluate state-ofart (SoA) feature sets on the new data. (3) We show that use of argumentation-relevant metaphor is a significant predictor of a holistic score of essay quality, above and beyond essay length. (Ghosh et al., 2016; Persing and Ng, 2015; Stab and Gurevych, 2014; Song et al., 2014; Somasundaran et al., 2014; Gurevich and Deane, 2007). Use of metaphor is another aspect of language use that goes beyond grammar and mechanics; recent research suggests that use of metaphor differs with proficiency (Beigman Klebanov et al., 2013), including in non-native writing (Littlemore et al., 2013). On top of serving as a new dataset for metaphor detection experiments, our corpus supports investigation of the relationship between metaphor use and English proficiency. Most of the recent work on supervised metaphor identif"
N18-2014,N07-2013,0,0.033519,"pus was not released for public use. Our contributions are as follows: (1) We release metaphor annotations of 240 argumentative essays written by non-native speakers of English. This is the first publicly available metaphor annotated data in this genre we are aware of. (2) We evaluate state-ofart (SoA) feature sets on the new data. (3) We show that use of argumentation-relevant metaphor is a significant predictor of a holistic score of essay quality, above and beyond essay length. (Ghosh et al., 2016; Persing and Ng, 2015; Stab and Gurevych, 2014; Song et al., 2014; Somasundaran et al., 2014; Gurevich and Deane, 2007). Use of metaphor is another aspect of language use that goes beyond grammar and mechanics; recent research suggests that use of metaphor differs with proficiency (Beigman Klebanov et al., 2013), including in non-native writing (Littlemore et al., 2013). On top of serving as a new dataset for metaphor detection experiments, our corpus supports investigation of the relationship between metaphor use and English proficiency. Most of the recent work on supervised metaphor identification in running text has been done on the VU Amsterdam Metaphor Corpus (VUA) (Steen et al., 2010), a large-scale reso"
N18-2014,W13-0902,1,0.745814,"ublicly available metaphor annotated data in this genre we are aware of. (2) We evaluate state-ofart (SoA) feature sets on the new data. (3) We show that use of argumentation-relevant metaphor is a significant predictor of a holistic score of essay quality, above and beyond essay length. (Ghosh et al., 2016; Persing and Ng, 2015; Stab and Gurevych, 2014; Song et al., 2014; Somasundaran et al., 2014; Gurevich and Deane, 2007). Use of metaphor is another aspect of language use that goes beyond grammar and mechanics; recent research suggests that use of metaphor differs with proficiency (Beigman Klebanov et al., 2013), including in non-native writing (Littlemore et al., 2013). On top of serving as a new dataset for metaphor detection experiments, our corpus supports investigation of the relationship between metaphor use and English proficiency. Most of the recent work on supervised metaphor identification in running text has been done on the VU Amsterdam Metaphor Corpus (VUA) (Steen et al., 2010), a large-scale resource containing excerpts from the BNC in four genres (news, academic, fiction, and conversation) annotated for metaphor at the word level (Beigman Klebanov et al., 2016; Haagsma and Bjerva, 2016"
N18-2014,W15-1402,1,0.785867,"dentification in running text has been done on the VU Amsterdam Metaphor Corpus (VUA) (Steen et al., 2010), a large-scale resource containing excerpts from the BNC in four genres (news, academic, fiction, and conversation) annotated for metaphor at the word level (Beigman Klebanov et al., 2016; Haagsma and Bjerva, 2016; Rai et al., 2016; Do Dinh and Gurevych, 2016; Dunn, 2014). Recently, researchers also reported experiments ¨ on a corpus of proverbs (Ozbal et al., 2016), a corpus of posts to an online breast cancer support group (Jang et al., 2016, 2015), and on argumentative essays (Beigman Klebanov et al., 2015); in these studies, feature sets originally developed for the VUA corpus served as baselines. We follow the same methodology. 2 3 We present a corpus of 240 argumentative essays written by non-native speakers of English annotated for metaphor. The corpus is made publicly available. We provide benchmark performance of state-of-the-art systems on this new corpus, and explore the relationship between writing proficiency and metaphor use. 1 Introduction Related Work Research in automated assessment of students’ writing, both native and non-native, is increasingly moving beyond traditional models t"
N18-2014,W16-1102,0,0.0240988,"n Klebanov et al., 2013), including in non-native writing (Littlemore et al., 2013). On top of serving as a new dataset for metaphor detection experiments, our corpus supports investigation of the relationship between metaphor use and English proficiency. Most of the recent work on supervised metaphor identification in running text has been done on the VU Amsterdam Metaphor Corpus (VUA) (Steen et al., 2010), a large-scale resource containing excerpts from the BNC in four genres (news, academic, fiction, and conversation) annotated for metaphor at the word level (Beigman Klebanov et al., 2016; Haagsma and Bjerva, 2016; Rai et al., 2016; Do Dinh and Gurevych, 2016; Dunn, 2014). Recently, researchers also reported experiments ¨ on a corpus of proverbs (Ozbal et al., 2016), a corpus of posts to an online breast cancer support group (Jang et al., 2016, 2015), and on argumentative essays (Beigman Klebanov et al., 2015); in these studies, feature sets originally developed for the VUA corpus served as baselines. We follow the same methodology. 2 3 We present a corpus of 240 argumentative essays written by non-native speakers of English annotated for metaphor. The corpus is made publicly available. We provide benc"
N18-2014,P16-2017,1,0.500359,"ith proficiency (Beigman Klebanov et al., 2013), including in non-native writing (Littlemore et al., 2013). On top of serving as a new dataset for metaphor detection experiments, our corpus supports investigation of the relationship between metaphor use and English proficiency. Most of the recent work on supervised metaphor identification in running text has been done on the VU Amsterdam Metaphor Corpus (VUA) (Steen et al., 2010), a large-scale resource containing excerpts from the BNC in four genres (news, academic, fiction, and conversation) annotated for metaphor at the word level (Beigman Klebanov et al., 2016; Haagsma and Bjerva, 2016; Rai et al., 2016; Do Dinh and Gurevych, 2016; Dunn, 2014). Recently, researchers also reported experiments ¨ on a corpus of proverbs (Ozbal et al., 2016), a corpus of posts to an online breast cancer support group (Jang et al., 2016, 2015), and on argumentative essays (Beigman Klebanov et al., 2015); in these studies, feature sets originally developed for the VUA corpus served as baselines. We follow the same methodology. 2 3 We present a corpus of 240 argumentative essays written by non-native speakers of English annotated for metaphor. The corpus is made publicly"
N18-2014,P16-1021,0,0.019341,"h proficiency. Most of the recent work on supervised metaphor identification in running text has been done on the VU Amsterdam Metaphor Corpus (VUA) (Steen et al., 2010), a large-scale resource containing excerpts from the BNC in four genres (news, academic, fiction, and conversation) annotated for metaphor at the word level (Beigman Klebanov et al., 2016; Haagsma and Bjerva, 2016; Rai et al., 2016; Do Dinh and Gurevych, 2016; Dunn, 2014). Recently, researchers also reported experiments ¨ on a corpus of proverbs (Ozbal et al., 2016), a corpus of posts to an online breast cancer support group (Jang et al., 2016, 2015), and on argumentative essays (Beigman Klebanov et al., 2015); in these studies, feature sets originally developed for the VUA corpus served as baselines. We follow the same methodology. 2 3 We present a corpus of 240 argumentative essays written by non-native speakers of English annotated for metaphor. The corpus is made publicly available. We provide benchmark performance of state-of-the-art systems on this new corpus, and explore the relationship between writing proficiency and metaphor use. 1 Introduction Related Work Research in automated assessment of students’ writing, both nativ"
N18-2014,W15-4650,0,0.184571,"Missing"
N18-2014,C14-1142,0,0.0265386,"of a metaphor detection system on students’ writing; however, their corpus was not released for public use. Our contributions are as follows: (1) We release metaphor annotations of 240 argumentative essays written by non-native speakers of English. This is the first publicly available metaphor annotated data in this genre we are aware of. (2) We evaluate state-ofart (SoA) feature sets on the new data. (3) We show that use of argumentation-relevant metaphor is a significant predictor of a holistic score of essay quality, above and beyond essay length. (Ghosh et al., 2016; Persing and Ng, 2015; Stab and Gurevych, 2014; Song et al., 2014; Somasundaran et al., 2014; Gurevich and Deane, 2007). Use of metaphor is another aspect of language use that goes beyond grammar and mechanics; recent research suggests that use of metaphor differs with proficiency (Beigman Klebanov et al., 2013), including in non-native writing (Littlemore et al., 2013). On top of serving as a new dataset for metaphor detection experiments, our corpus supports investigation of the relationship between metaphor use and English proficiency. Most of the recent work on supervised metaphor identification in running text has been done on the VU"
N18-2014,D16-1220,0,0.0235099,"corpus supports investigation of the relationship between metaphor use and English proficiency. Most of the recent work on supervised metaphor identification in running text has been done on the VU Amsterdam Metaphor Corpus (VUA) (Steen et al., 2010), a large-scale resource containing excerpts from the BNC in four genres (news, academic, fiction, and conversation) annotated for metaphor at the word level (Beigman Klebanov et al., 2016; Haagsma and Bjerva, 2016; Rai et al., 2016; Do Dinh and Gurevych, 2016; Dunn, 2014). Recently, researchers also reported experiments ¨ on a corpus of proverbs (Ozbal et al., 2016), a corpus of posts to an online breast cancer support group (Jang et al., 2016, 2015), and on argumentative essays (Beigman Klebanov et al., 2015); in these studies, feature sets originally developed for the VUA corpus served as baselines. We follow the same methodology. 2 3 We present a corpus of 240 argumentative essays written by non-native speakers of English annotated for metaphor. The corpus is made publicly available. We provide benchmark performance of state-of-the-art systems on this new corpus, and explore the relationship between writing proficiency and metaphor use. 1 Introduction"
N18-2014,P15-1053,0,0.0187593,"eported an evaluation of a metaphor detection system on students’ writing; however, their corpus was not released for public use. Our contributions are as follows: (1) We release metaphor annotations of 240 argumentative essays written by non-native speakers of English. This is the first publicly available metaphor annotated data in this genre we are aware of. (2) We evaluate state-ofart (SoA) feature sets on the new data. (3) We show that use of argumentation-relevant metaphor is a significant predictor of a holistic score of essay quality, above and beyond essay length. (Ghosh et al., 2016; Persing and Ng, 2015; Stab and Gurevych, 2014; Song et al., 2014; Somasundaran et al., 2014; Gurevich and Deane, 2007). Use of metaphor is another aspect of language use that goes beyond grammar and mechanics; recent research suggests that use of metaphor differs with proficiency (Beigman Klebanov et al., 2013), including in non-native writing (Littlemore et al., 2013). On top of serving as a new dataset for metaphor detection experiments, our corpus supports investigation of the relationship between metaphor use and English proficiency. Most of the recent work on supervised metaphor identification in running tex"
N18-2014,W16-1103,0,0.0839435,"Missing"
N18-2014,C14-1090,0,0.0260309,"writing; however, their corpus was not released for public use. Our contributions are as follows: (1) We release metaphor annotations of 240 argumentative essays written by non-native speakers of English. This is the first publicly available metaphor annotated data in this genre we are aware of. (2) We evaluate state-ofart (SoA) feature sets on the new data. (3) We show that use of argumentation-relevant metaphor is a significant predictor of a holistic score of essay quality, above and beyond essay length. (Ghosh et al., 2016; Persing and Ng, 2015; Stab and Gurevych, 2014; Song et al., 2014; Somasundaran et al., 2014; Gurevich and Deane, 2007). Use of metaphor is another aspect of language use that goes beyond grammar and mechanics; recent research suggests that use of metaphor differs with proficiency (Beigman Klebanov et al., 2013), including in non-native writing (Littlemore et al., 2013). On top of serving as a new dataset for metaphor detection experiments, our corpus supports investigation of the relationship between metaphor use and English proficiency. Most of the recent work on supervised metaphor identification in running text has been done on the VU Amsterdam Metaphor Corpus (VUA) (Steen et al."
N18-2014,W14-2110,1,0.836502,"ystem on students’ writing; however, their corpus was not released for public use. Our contributions are as follows: (1) We release metaphor annotations of 240 argumentative essays written by non-native speakers of English. This is the first publicly available metaphor annotated data in this genre we are aware of. (2) We evaluate state-ofart (SoA) feature sets on the new data. (3) We show that use of argumentation-relevant metaphor is a significant predictor of a holistic score of essay quality, above and beyond essay length. (Ghosh et al., 2016; Persing and Ng, 2015; Stab and Gurevych, 2014; Song et al., 2014; Somasundaran et al., 2014; Gurevich and Deane, 2007). Use of metaphor is another aspect of language use that goes beyond grammar and mechanics; recent research suggests that use of metaphor differs with proficiency (Beigman Klebanov et al., 2013), including in non-native writing (Littlemore et al., 2013). On top of serving as a new dataset for metaphor detection experiments, our corpus supports investigation of the relationship between metaphor use and English proficiency. Most of the recent work on supervised metaphor identification in running text has been done on the VU Amsterdam Metaphor"
P05-2010,W97-0703,0,0.0692202,"nanchored ones? When asked to recall the content of a text, would people remember prolific anchors of this text? Such experiments will further our understanding of the nature of text-reader interaction and help improve applications like text generation and summarization. Second, it can serve as a minimal test data for computational models of lexical cohesion: any good model should at least get the core part right. Much of the existing applied research on lexical cohesion uses WordNet-based (Miller, 1990) lexical chains to identify the cohesive texture for a larger text processing application (Barzilay and Elhadad, 1997; Stokes et al., 2004; Moldovan and Novischi, 2002; Al-Halimi and Kazman, 1998). We can now subject these putative chains to a direct test; in fact, this is the immediate future research direction. In addition, analysis techniques discussed in the paper – separating interpretation disagreement from difference in consistency, using statistical hypothesis testing to find reliable parts of the annotations and validating them experimentally – may be applied to data resulting from other kinds of exploratory experiments to gain insights about the phenomena at hand. Acknowledgment I would like to tha"
P05-2010,J96-2004,0,0.0300445,"t was timed for 2-4 annotators). 4 Analysis of Experimental Data Most of the existing research in computational linguistics that uses human annotators is within the framework of classification, where an annotator decides, for every test item, on an appropriate tag out of the pre-specified set of tags (Poesio and Vieira, 1998; Webber and Byron, 2004; Hearst, 1997; Marcus et al., 1993). Although our task is not that of classification, we start from a classification sub-task, and use agreement figures to guide subsequent analysis. We use the by now standard statistic (Di Eugenio and Glass, 2004; Carletta, 1996; Marcu et al., 1999; Webber and Byron, 2004) to quantify the degree of above-chance agreement between multiple annotators, and the  statistic for analysis of sources of unreliability (Krippendorff, 1980). The formulas for the two statistics are given in appendix A. 4.1 Classification Sub-Task Classifying items into anchored/unanchored can be viewed as a sub-task of our experiment: before writing any particular item as an anchor, the annotator asked himself whether the concept at hand is easy to accommodate at all. Getting reliable data on this task is therefore a pre-condition for asking any"
P05-2010,J97-1003,0,0.0706948,"metimes the same form is used in a somewhat different sense and may get anchored separately from the previous use of this form. This issue needs further experimental investigation.  utes on average (each annotator was timed on two texts; every text was timed for 2-4 annotators). 4 Analysis of Experimental Data Most of the existing research in computational linguistics that uses human annotators is within the framework of classification, where an annotator decides, for every test item, on an appropriate tag out of the pre-specified set of tags (Poesio and Vieira, 1998; Webber and Byron, 2004; Hearst, 1997; Marcus et al., 1993). Although our task is not that of classification, we start from a classification sub-task, and use agreement figures to guide subsequent analysis. We use the by now standard statistic (Di Eugenio and Glass, 2004; Carletta, 1996; Marcu et al., 1999; Webber and Byron, 2004) to quantify the degree of above-chance agreement between multiple annotators, and the  statistic for analysis of sources of unreliability (Krippendorff, 1980). The formulas for the two statistics are given in appendix A. 4.1 Classification Sub-Task Classifying items into anchored/unanchored can be view"
P05-2010,W99-0307,0,0.0235609,"2-4 annotators). 4 Analysis of Experimental Data Most of the existing research in computational linguistics that uses human annotators is within the framework of classification, where an annotator decides, for every test item, on an appropriate tag out of the pre-specified set of tags (Poesio and Vieira, 1998; Webber and Byron, 2004; Hearst, 1997; Marcus et al., 1993). Although our task is not that of classification, we start from a classification sub-task, and use agreement figures to guide subsequent analysis. We use the by now standard statistic (Di Eugenio and Glass, 2004; Carletta, 1996; Marcu et al., 1999; Webber and Byron, 2004) to quantify the degree of above-chance agreement between multiple annotators, and the  statistic for analysis of sources of unreliability (Krippendorff, 1980). The formulas for the two statistics are given in appendix A. 4.1 Classification Sub-Task Classifying items into anchored/unanchored can be viewed as a sub-task of our experiment: before writing any particular item as an anchor, the annotator asked himself whether the concept at hand is easy to accommodate at all. Getting reliable data on this task is therefore a pre-condition for asking any questions about the"
P05-2010,J93-2004,0,0.0258166,"me form is used in a somewhat different sense and may get anchored separately from the previous use of this form. This issue needs further experimental investigation.  utes on average (each annotator was timed on two texts; every text was timed for 2-4 annotators). 4 Analysis of Experimental Data Most of the existing research in computational linguistics that uses human annotators is within the framework of classification, where an annotator decides, for every test item, on an appropriate tag out of the pre-specified set of tags (Poesio and Vieira, 1998; Webber and Byron, 2004; Hearst, 1997; Marcus et al., 1993). Although our task is not that of classification, we start from a classification sub-task, and use agreement figures to guide subsequent analysis. We use the by now standard statistic (Di Eugenio and Glass, 2004; Carletta, 1996; Marcu et al., 1999; Webber and Byron, 2004) to quantify the degree of above-chance agreement between multiple annotators, and the  statistic for analysis of sources of unreliability (Krippendorff, 1980). The formulas for the two statistics are given in appendix A. 4.1 Classification Sub-Task Classifying items into anchored/unanchored can be viewed as a sub-task of ou"
P05-2010,C02-1167,0,0.0237531,"of a text, would people remember prolific anchors of this text? Such experiments will further our understanding of the nature of text-reader interaction and help improve applications like text generation and summarization. Second, it can serve as a minimal test data for computational models of lexical cohesion: any good model should at least get the core part right. Much of the existing applied research on lexical cohesion uses WordNet-based (Miller, 1990) lexical chains to identify the cohesive texture for a larger text processing application (Barzilay and Elhadad, 1997; Stokes et al., 2004; Moldovan and Novischi, 2002; Al-Halimi and Kazman, 1998). We can now subject these putative chains to a direct test; in fact, this is the immediate future research direction. In addition, analysis techniques discussed in the paper – separating interpretation disagreement from difference in consistency, using statistical hypothesis testing to find reliable parts of the annotations and validating them experimentally – may be applied to data resulting from other kinds of exploratory experiments to gain insights about the phenomena at hand. Acknowledgment I would like to thank Prof. Eli Shamir for guidance and numerous disc"
P05-2010,W04-2607,0,0.134268,"s are an answer (section 2), describe an experiment on 22 readers using this question (section 3), and analyze the experimental data (section 4). Cohesive ties between items in a text draw on the resources of a language to build up the text’s unity (Halliday and Hasan, 1976). Lexical cohesive ties draw on the lexicon, i.e. word meanings. Sometimes the relation between the members of a tie is easy to identify, like near-synonymy (disease/illness), complementarity (boy/girl), whole-topart (box/lid), but the bulk of lexical cohesive texture is created by relations that are difficult to classify (Morris and Hirst, 2004). Halliday and Hasan (1976) exemplify those with pairs like dig/garden, ill/doctor, laugh/joke, which are reminiscent of the idea of scripts (Schank and Abelson, 1977) or schemata (Rumelhart, 1984): certain things are expected in certain situations, the paradigm example being menu, tables, waiters and food in a restaurant. However, texts sometimes start with descriptions of situations where many possible scripts could apply. Consider a text starting with Mother died today.1 What are the generated expectations? A description of an accident that led to the death, or of a long illness? A story ab"
P05-2010,J98-2001,0,0.0840978,"luently readable by people dates back at least to Halliday and Hasan’s (1976) seminal work on textual cohesion. They identified a number of cohesive constructions: repetition (using the same words, or via repeated reference, substitution and ellipsis), conjunction and lexical cohesion. Some of those structures - for example, cohesion achieved through repeated reference - have been subjected to reader based tests, often while trying to produce gold standard data for testing computational models, a task requiring sufficient inter-annotator agreement (Hirschman et al., 1998; Mitkov et al., 2000; Poesio and Vieira, 1998). Experimental investigation of lexical cohesion is an emerging enterprise (Morris and Hirst, 2005) to which the current study contributes. We present our version of the question to the reader to which lexical cohesion patterns are an answer (section 2), describe an experiment on 22 readers using this question (section 3), and analyze the experimental data (section 4). Cohesive ties between items in a text draw on the resources of a language to build up the text’s unity (Halliday and Hasan, 1976). Lexical cohesive ties draw on the lexicon, i.e. word meanings. Sometimes the relation between the"
P09-1032,W03-1022,0,0.0129904,"Missing"
P09-1032,W04-3240,0,0.0130368,"instances are posing as easy. 281 0 λe sary can not only choose the placement of the hard cases, but also their labels. In worst case, the 0-1 loss model would sustain a constant rate of error due to malicious noise, whereas annotation noise is tolerated quite well in large datasets. λh 3 Voted Perceptron Freund and Schapire (1999) describe the voted perceptron. This algorithm and its many variants are widely used in the computational linguistics community (Collins, 2002a; Collins and Duffy, 2002; Collins, 2002b; Collins and Roark, 2004; Henderson and Titov, 2005; Viola and Narasimhan, 2005; Cohen et al., 2004; Carreras et al., 2005; Shen and Joshi, 2005; Ciaramita and Johnson, 2003). In this section, we show that the voted perceptron can be vulnerable to annotation noise. The algorithm is shown below. Figure 1: The adversarial case for 0-1 loss. Squares correspond to easy instances, circles – to hard ones. Filled squares and circles are labeled −1, empty ones are labeled +1. Algorithm 1 Voted Perceptron √ Training Input: a labeled training set (x1 , y1 ), . . . , (xN , yN ) Output: a list of perceptrons w1 , . . . , wN N (0, γN ). This implies that a value as low as −2σ cannot be ruled out with hi"
P09-1032,P02-1034,0,0.0394756,"ial. When there is just one annotator, no distinction between easy vs hard instances can be made; in this sense, all hard instances are posing as easy. 281 0 λe sary can not only choose the placement of the hard cases, but also their labels. In worst case, the 0-1 loss model would sustain a constant rate of error due to malicious noise, whereas annotation noise is tolerated quite well in large datasets. λh 3 Voted Perceptron Freund and Schapire (1999) describe the voted perceptron. This algorithm and its many variants are widely used in the computational linguistics community (Collins, 2002a; Collins and Duffy, 2002; Collins, 2002b; Collins and Roark, 2004; Henderson and Titov, 2005; Viola and Narasimhan, 2005; Cohen et al., 2004; Carreras et al., 2005; Shen and Joshi, 2005; Ciaramita and Johnson, 2003). In this section, we show that the voted perceptron can be vulnerable to annotation noise. The algorithm is shown below. Figure 1: The adversarial case for 0-1 loss. Squares correspond to easy instances, circles – to hard ones. Filled squares and circles are labeled −1, empty ones are labeled +1. Algorithm 1 Voted Perceptron √ Training Input: a labeled training set (x1 , y1 ), . . . , (xN , yN ) Output: a"
P09-1032,P04-1015,0,0.0107477,"distinction between easy vs hard instances can be made; in this sense, all hard instances are posing as easy. 281 0 λe sary can not only choose the placement of the hard cases, but also their labels. In worst case, the 0-1 loss model would sustain a constant rate of error due to malicious noise, whereas annotation noise is tolerated quite well in large datasets. λh 3 Voted Perceptron Freund and Schapire (1999) describe the voted perceptron. This algorithm and its many variants are widely used in the computational linguistics community (Collins, 2002a; Collins and Duffy, 2002; Collins, 2002b; Collins and Roark, 2004; Henderson and Titov, 2005; Viola and Narasimhan, 2005; Cohen et al., 2004; Carreras et al., 2005; Shen and Joshi, 2005; Ciaramita and Johnson, 2003). In this section, we show that the voted perceptron can be vulnerable to annotation noise. The algorithm is shown below. Figure 1: The adversarial case for 0-1 loss. Squares correspond to easy instances, circles – to hard ones. Filled squares and circles are labeled −1, empty ones are labeled +1. Algorithm 1 Voted Perceptron √ Training Input: a labeled training set (x1 , y1 ), . . . , (xN , yN ) Output: a list of perceptrons w1 , . . . , wN N (0"
P09-1032,W02-1001,0,0.0941018,"annotated material. When there is just one annotator, no distinction between easy vs hard instances can be made; in this sense, all hard instances are posing as easy. 281 0 λe sary can not only choose the placement of the hard cases, but also their labels. In worst case, the 0-1 loss model would sustain a constant rate of error due to malicious noise, whereas annotation noise is tolerated quite well in large datasets. λh 3 Voted Perceptron Freund and Schapire (1999) describe the voted perceptron. This algorithm and its many variants are widely used in the computational linguistics community (Collins, 2002a; Collins and Duffy, 2002; Collins, 2002b; Collins and Roark, 2004; Henderson and Titov, 2005; Viola and Narasimhan, 2005; Cohen et al., 2004; Carreras et al., 2005; Shen and Joshi, 2005; Ciaramita and Johnson, 2003). In this section, we show that the voted perceptron can be vulnerable to annotation noise. The algorithm is shown below. Figure 1: The adversarial case for 0-1 loss. Squares correspond to easy instances, circles – to hard ones. Filled squares and circles are labeled −1, empty ones are labeled +1. Algorithm 1 Voted Perceptron √ Training Input: a labeled training set (x1 , y1 ), ."
P09-1032,W08-1202,1,0.781356,"e models. For example, it could be that an annotator’s bias is exercised on each and every instance, making his preferred category likelier for any instance than in another person’s annotations. Another possibility, recently explored by Beigman Klebanov and Beigman (2009), is that some items are really quite clear-cut for an annotator with any bias, belonging squarely within one particular category. However, some instances – termed hard cases therein – are harder to decide upon, and this is where various preferences and biases come into play. In a metaphor annotation study reported by Beigman Klebanov et al. (2008), certain markups received overwhelming annotator support when people were asked to validate annotations after a certain time delay. Other instances saw opinions split; moreover, Beigman Klebanov et al. (2008) observed cases where people retracted their own earlier annotations. To start accounting for such annotator behavior, Beigman Klebanov and Beigman (2009) proposed a model where instances are either easy, and then all annotators agree on them, or hard, and then each annotator flips his or her own coin to deIt is usually assumed that the kind of noise existing in annotated data is random c"
P09-1032,D08-1027,0,0.0541395,"Missing"
P09-1032,P05-1023,0,0.0141018,"vs hard instances can be made; in this sense, all hard instances are posing as easy. 281 0 λe sary can not only choose the placement of the hard cases, but also their labels. In worst case, the 0-1 loss model would sustain a constant rate of error due to malicious noise, whereas annotation noise is tolerated quite well in large datasets. λh 3 Voted Perceptron Freund and Schapire (1999) describe the voted perceptron. This algorithm and its many variants are widely used in the computational linguistics community (Collins, 2002a; Collins and Duffy, 2002; Collins, 2002b; Collins and Roark, 2004; Henderson and Titov, 2005; Viola and Narasimhan, 2005; Cohen et al., 2004; Carreras et al., 2005; Shen and Joshi, 2005; Ciaramita and Johnson, 2003). In this section, we show that the voted perceptron can be vulnerable to annotation noise. The algorithm is shown below. Figure 1: The adversarial case for 0-1 loss. Squares correspond to easy instances, circles – to hard ones. Filled squares and circles are labeled −1, empty ones are labeled +1. Algorithm 1 Voted Perceptron √ Training Input: a labeled training set (x1 , y1 ), . . . , (xN , yN ) Output: a list of perceptrons w1 , . . . , wN N (0, γN ). This implies that a"
P09-1032,poesio-etal-2008-anawiki,0,0.0498249,"econd annotator is expected to detect about half the hard cases, as they would surface as disagreements between the annotators. Subsequently, a machine learner can be told to ignore those cases during training, reducing the risk of hard case bias. While this is certainly a daunting task, it is possible that for annotation studies that do not require expert annotators and extensive annotator training, the newly available access to a large pool of inexpensive annotators, such as the Amazon Mechanical Turk scheme (Snow et al., 2008),4 or embedding the task in an online game played by volunteers (Poesio et al., 2008; von Ahn, 2006) could provide some solutions. Reidsma and op den Akker (2008) suggest a different option. When non-overlapping parts of the dataset are annotated by different annotators, each classifier can be trained to reflect the opinion (albeit biased) of a specific annotator, using different parts of the datasets. Such “subjective machines” can be applied to a new set of data; an item that causes disagreement between classifiers is then extrapolated to be a case of potential disagreement between the humans they replicate, i.e. Lemma 5 There exists √ c > 0 such that with a high probabilit"
P09-1032,W08-1203,0,0.135686,"Missing"
P09-1032,H05-1102,0,0.0194088,"Missing"
P09-1032,J08-3001,0,\N,Missing
P09-1032,J09-4005,1,\N,Missing
P09-1032,P02-1062,0,\N,Missing
P10-1072,W08-1202,1,0.821889,"be used to select documents (Hardie et al., 2007; Gedigian et al., 2006). Another approach is metaphor “harvesting” – hypothesizing that metaphors of interest would occur in close proximity to lexical items representing the target domain of the metaphor, such as the 4 word window around the lemma Europe used in Reining and L¨onneker-Rodman (2007). 5.3 Data annotation A further challenge is producing reliable annotations. Pragglejaz (2007) propose a methodology for testing metaphoricity of a word in discourse and report κ=0.56-0.70 agreement for a group of six highly expert annotators. Beigman Klebanov et al. (2008) report κ=0.66 for detecting paragraphs containing metaphors from the source domains LOVE and VEHICLE with multiple non-expert annotators, though other source domains that often feature highly conventionalized metaphors (like structure or foundation from BUILDLING domain) or are more abstract and difficult to delimit (such as AUTHORITY) present a more challenging annotation task. 5.4 Measuring metaphors A fully empirical basis for the kind of model presented in this paper would also involve defining a metric on metaphors that would allow measuring the frame chosen by the given version of the m"
P10-1072,W06-3506,0,0.296772,"ttled and acknowledged by both sides and the debate moves to evaluation of the relative importance of those features. 5.1 Metaphors in NLP Metaphors received increasing attention from computational linguistics community in the last two decades. The tasks that have been addressed are explication of the reasoning behind the metaphor (Barnden et al., 2002; Narayanan, 1999; Hobbs, 1992); detection of conventional metaphors between two specific domains (Mason, 2004); classification of words, phrases or sentences as metaphoric or non-metaphoric (Krishnakumaran and Zhu, 2007; Birke and Sarkar, 2006; Gedigian et al., 2006; Fass, 1991). We are not aware of research on automatic methods specifically geared to recognition of extended metaphors. Indeed, most computational work cited above concentrates on the detection of a local incongruity due to a violation of selectional restrictions when the verb or one of its arguments is used metaphorically (as in Protesters derailed the conference). Extended metaphors are expected to be difficult for such approaches, since many of the clauses are completely situated in the source domain and hence no local incongruities exist (see examples on the first page of this article)."
P10-1072,E06-1042,0,0.0412565,"where the “facts” are settled and acknowledged by both sides and the debate moves to evaluation of the relative importance of those features. 5.1 Metaphors in NLP Metaphors received increasing attention from computational linguistics community in the last two decades. The tasks that have been addressed are explication of the reasoning behind the metaphor (Barnden et al., 2002; Narayanan, 1999; Hobbs, 1992); detection of conventional metaphors between two specific domains (Mason, 2004); classification of words, phrases or sentences as metaphoric or non-metaphoric (Krishnakumaran and Zhu, 2007; Birke and Sarkar, 2006; Gedigian et al., 2006; Fass, 1991). We are not aware of research on automatic methods specifically geared to recognition of extended metaphors. Indeed, most computational work cited above concentrates on the detection of a local incongruity due to a violation of selectional restrictions when the verb or one of its arguments is used metaphorically (as in Protesters derailed the conference). Extended metaphors are expected to be difficult for such approaches, since many of the clauses are completely situated in the source domain and hence no local incongruities exist (see examples on the first"
P10-1072,N09-1057,0,0.0337245,"re compared to pupils that barely make the grade with Britain as a ‘drop-out’ who gave up even trying (Musolff, 2000). The fact that European policy is being communicated and negotiated via a metaphor is not surprising; after all, “there is always someone willing to help us think by providing us with a metaphor 2 699 Capitalization in the original, Bolinger (1980, p. 146). not only in chronological terms (Gruhl et al., 2004; Allan, 2002), but in strategic terms, i.e. in terms that reflect agendas of the actors, such as political agendas in legislatures (Quinn et al., 2006) or activist forums (Greene and Resnik, 2009), research agendas in group meetings (Morgan et al., 2001), or social agendas in speed-dates (Jurafsky et al., 2009). Game theoretical models are well suited for modeling dynamics that emerge under multiple, possibly conflicting constraints, as we exemplify in this article. quence of actions is an equilibrium. The resulting model is thereby able to rationalize the observed behavior as a naturally emerging dynamics between agents maximizing certain utility functions. In economics, game-theoretic models are used to explain price change, organization of production, and market failures (Mas-Colell"
P10-1072,J91-1003,0,0.424182,"by both sides and the debate moves to evaluation of the relative importance of those features. 5.1 Metaphors in NLP Metaphors received increasing attention from computational linguistics community in the last two decades. The tasks that have been addressed are explication of the reasoning behind the metaphor (Barnden et al., 2002; Narayanan, 1999; Hobbs, 1992); detection of conventional metaphors between two specific domains (Mason, 2004); classification of words, phrases or sentences as metaphoric or non-metaphoric (Krishnakumaran and Zhu, 2007; Birke and Sarkar, 2006; Gedigian et al., 2006; Fass, 1991). We are not aware of research on automatic methods specifically geared to recognition of extended metaphors. Indeed, most computational work cited above concentrates on the detection of a local incongruity due to a violation of selectional restrictions when the verb or one of its arguments is used metaphorically (as in Protesters derailed the conference). Extended metaphors are expected to be difficult for such approaches, since many of the clauses are completely situated in the source domain and hence no local incongruities exist (see examples on the first page of this article). As another e"
P10-1072,N09-1072,0,0.337803,"The fact that European policy is being communicated and negotiated via a metaphor is not surprising; after all, “there is always someone willing to help us think by providing us with a metaphor 2 699 Capitalization in the original, Bolinger (1980, p. 146). not only in chronological terms (Gruhl et al., 2004; Allan, 2002), but in strategic terms, i.e. in terms that reflect agendas of the actors, such as political agendas in legislatures (Quinn et al., 2006) or activist forums (Greene and Resnik, 2009), research agendas in group meetings (Morgan et al., 2001), or social agendas in speed-dates (Jurafsky et al., 2009). Game theoretical models are well suited for modeling dynamics that emerge under multiple, possibly conflicting constraints, as we exemplify in this article. quence of actions is an equilibrium. The resulting model is thereby able to rationalize the observed behavior as a naturally emerging dynamics between agents maximizing certain utility functions. In economics, game-theoretic models are used to explain price change, organization of production, and market failures (Mas-Colell et al., 1995; von Neumann and Morgenstern, 1944); in biology — the operation of natural selection processes (Axelro"
P10-1072,W07-0103,0,0.065581,"r in which product to a stage where the “facts” are settled and acknowledged by both sides and the debate moves to evaluation of the relative importance of those features. 5.1 Metaphors in NLP Metaphors received increasing attention from computational linguistics community in the last two decades. The tasks that have been addressed are explication of the reasoning behind the metaphor (Barnden et al., 2002; Narayanan, 1999; Hobbs, 1992); detection of conventional metaphors between two specific domains (Mason, 2004); classification of words, phrases or sentences as metaphoric or non-metaphoric (Krishnakumaran and Zhu, 2007; Birke and Sarkar, 2006; Gedigian et al., 2006; Fass, 1991). We are not aware of research on automatic methods specifically geared to recognition of extended metaphors. Indeed, most computational work cited above concentrates on the detection of a local incongruity due to a violation of selectional restrictions when the verb or one of its arguments is used metaphorically (as in Protesters derailed the conference). Extended metaphors are expected to be difficult for such approaches, since many of the clauses are completely situated in the source domain and hence no local incongruities exist (s"
P10-1072,D09-1035,0,0.018388,"re of success for a given version of the frame is its ability to sway the public in the evaluative direction envisioned by the author by providing sufficient educational benefit, so-to-speak, that is, convincingly rendering a good portion of a complex reality in accessible terms. Once a frame is found that provides extensive education benefit, such as the E UROPEAN INTE GRATION AS TRAIN JOURNEY above, a politi4.3 Social dynamics This article contributes to the growing literature on modeling social linguistic behavior, like debates (Somasundaran and Wiebe, 2009), dating (Jurafsky et al., 2009; Ranganath et al., 2009), collaborative authoring and editing in wikis (Leuf and Cunningham, 2001) such as Wikipedia (Vuong et al., 2008; Kittur et al., 2007; Vi´egas et al., 2004). The latter literature in particular sees the social activity as an unfolding process, for example, detecting the onset and resolution of a controversy over the content of a Wikipedia article through tracking article talk7 and deletion-and-reversion patterns. Somewhat similarly to the metaphor debate discussed in this article, Vi´egas et al. (2004) note first-mover advantage in Wikipedia authoring, that is, the first version gives the tone"
P10-1072,W07-0102,0,0.390281,"Missing"
P10-1072,J04-1002,0,0.0950805,"he focus of the debate changes from the initial stage of elucidating which features are better in which product to a stage where the “facts” are settled and acknowledged by both sides and the debate moves to evaluation of the relative importance of those features. 5.1 Metaphors in NLP Metaphors received increasing attention from computational linguistics community in the last two decades. The tasks that have been addressed are explication of the reasoning behind the metaphor (Barnden et al., 2002; Narayanan, 1999; Hobbs, 1992); detection of conventional metaphors between two specific domains (Mason, 2004); classification of words, phrases or sentences as metaphoric or non-metaphoric (Krishnakumaran and Zhu, 2007; Birke and Sarkar, 2006; Gedigian et al., 2006; Fass, 1991). We are not aware of research on automatic methods specifically geared to recognition of extended metaphors. Indeed, most computational work cited above concentrates on the detection of a local incongruity due to a violation of selectional restrictions when the verb or one of its arguments is used metaphorically (as in Protesters derailed the conference). Extended metaphors are expected to be difficult for such approaches, sin"
P10-1072,H01-1051,0,0.0224482,"as a ‘drop-out’ who gave up even trying (Musolff, 2000). The fact that European policy is being communicated and negotiated via a metaphor is not surprising; after all, “there is always someone willing to help us think by providing us with a metaphor 2 699 Capitalization in the original, Bolinger (1980, p. 146). not only in chronological terms (Gruhl et al., 2004; Allan, 2002), but in strategic terms, i.e. in terms that reflect agendas of the actors, such as political agendas in legislatures (Quinn et al., 2006) or activist forums (Greene and Resnik, 2009), research agendas in group meetings (Morgan et al., 2001), or social agendas in speed-dates (Jurafsky et al., 2009). Game theoretical models are well suited for modeling dynamics that emerge under multiple, possibly conflicting constraints, as we exemplify in this article. quence of actions is an equilibrium. The resulting model is thereby able to rationalize the observed behavior as a naturally emerging dynamics between agents maximizing certain utility functions. In economics, game-theoretic models are used to explain price change, organization of production, and market failures (Mas-Colell et al., 1995; von Neumann and Morgenstern, 1944); in biol"
P10-1072,P04-1052,0,0.0117198,"teresting connections have been pointed out between game theory and machine learning: Freund and Schapire (1996) present both online learning and boosting as a repeated zero-sum game; Shalev-Shwartz and Singer (2006) show similarly that loss minimization in online learning is akin to an equilibrium path in a repeated game. While game theoretic models are not much utilized in computational linguistics, they are quite attractive to tackle some of the problems computational linguists are interested in. For example, generation of referring expressions (Paraboni et al., 2007; Gardent et al., 2004; Siddharthan and Copestake, 2004; Dale and Reiter, 1995) can be rendered as a communication game with utility functions that reflect pressures to use shorter expressions while avoiding excessive ambiguity (Clark and Parikh, 2007), with corpora annotated for entity mentions informing the design of a model. Generally, computational linguistics research produces algorithms to detect entities of various kinds, be it topics, named entities, metaphors, moves in a multi-party conversations, or syntactic constructions in large corpora; such primary data can be used to trace developments 3.1 Player utility For a given issue under dis"
P10-1072,P09-1026,0,0.146202,"rstanding of the former (Gentner and Gentner, 1983). The measure of success for a given version of the frame is its ability to sway the public in the evaluative direction envisioned by the author by providing sufficient educational benefit, so-to-speak, that is, convincingly rendering a good portion of a complex reality in accessible terms. Once a frame is found that provides extensive education benefit, such as the E UROPEAN INTE GRATION AS TRAIN JOURNEY above, a politi4.3 Social dynamics This article contributes to the growing literature on modeling social linguistic behavior, like debates (Somasundaran and Wiebe, 2009), dating (Jurafsky et al., 2009; Ranganath et al., 2009), collaborative authoring and editing in wikis (Leuf and Cunningham, 2001) such as Wikipedia (Vuong et al., 2008; Kittur et al., 2007; Vi´egas et al., 2004). The latter literature in particular sees the social activity as an unfolding process, for example, detecting the onset and resolution of a controversy over the content of a Wikipedia article through tracking article talk7 and deletion-and-reversion patterns. Somewhat similarly to the metaphor debate discussed in this article, Vi´egas et al. (2004) note first-mover advantage in Wikipe"
P10-1072,J07-2004,0,0.0291089,"Missing"
P10-1072,C02-2021,0,\N,Missing
P10-2047,P07-1114,0,0.0287563,"a debate on a specific document, we can consider writings from pro- and con- perspective, in, for example, the death penalty controversy over a course of a period of time. Relaxing the issue specificity somewhat, 1 2 Vocabulary Selection A line of inquiry going back at least to Zipf strives to characterize word frequency distributions in texts and corpora; see Baayen (2001) for a survey. One of the findings in this literature is that a multinomial (called “urn model” by Baayen) is not a good model for word frequency distributions. Among the many proposed remedies (Baayen, 2001; Jansche, 2003; Baroni and Evert, 2007; Bhat and Sproat, 2009), we would like to draw attention to the following insight articulated Google English Dictionary, Dictionary.com 253 Proceedings of the ACL 2010 Conference Short Papers, pages 253–257, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics by President Bush in 2003. We use data from 278 legislators, with 669 speeches in all. We take only one speech per speaker per year; since many serve multiple years, each speaker is represented with 1 to 5 speeches. We perform 10-fold cross-validation splitting by speakers, so that all speeches by the same"
P10-2047,P09-1013,0,0.0192677,"ocument, we can consider writings from pro- and con- perspective, in, for example, the death penalty controversy over a course of a period of time. Relaxing the issue specificity somewhat, 1 2 Vocabulary Selection A line of inquiry going back at least to Zipf strives to characterize word frequency distributions in texts and corpora; see Baayen (2001) for a survey. One of the findings in this literature is that a multinomial (called “urn model” by Baayen) is not a good model for word frequency distributions. Among the many proposed remedies (Baayen, 2001; Jansche, 2003; Baroni and Evert, 2007; Bhat and Sproat, 2009), we would like to draw attention to the following insight articulated Google English Dictionary, Dictionary.com 253 Proceedings of the ACL 2010 Conference Short Papers, pages 253–257, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics by President Bush in 2003. We use data from 278 legislators, with 669 speeches in all. We take only one speech per speaker per year; since many serve multiple years, each speaker is represented with 1 to 5 speeches. We perform 10-fold cross-validation splitting by speakers, so that all speeches by the same speaker are assigned to"
P10-2047,N09-1057,0,0.0731115,"Missing"
P10-2047,J97-1003,0,0.378921,"Missing"
P10-2047,P03-1037,0,0.0184716,"requirement of a debate on a specific document, we can consider writings from pro- and con- perspective, in, for example, the death penalty controversy over a course of a period of time. Relaxing the issue specificity somewhat, 1 2 Vocabulary Selection A line of inquiry going back at least to Zipf strives to characterize word frequency distributions in texts and corpora; see Baayen (2001) for a survey. One of the findings in this literature is that a multinomial (called “urn model” by Baayen) is not a good model for word frequency distributions. Among the many proposed remedies (Baayen, 2001; Jansche, 2003; Baroni and Evert, 2007; Bhat and Sproat, 2009), we would like to draw attention to the following insight articulated Google English Dictionary, Dictionary.com 253 Proceedings of the ACL 2010 Conference Short Papers, pages 253–257, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics by President Bush in 2003. We use data from 278 legislators, with 669 speeches in all. We take only one speech per speaker per year; since many serve multiple years, each speaker is represented with 1 to 5 speeches. We perform 10-fold cross-validation splitting by speakers, so that a"
P10-2047,P06-1133,0,0.103707,"bortion debates, using infanticide as a synonym for abortion is a pro-life stigma. Note that this does not mean the rest of the features are not informative for classification, only that they are redundant with respect to a small percentage of top weight features. 6 When N best features are eliminated, performance goes down significantly with even smaller N for PBA and BL datasets. Thus, top features are not only effective, they are also crucial for accurate classification, as their discrimination capacity is not replicated by any of the other vocabulary words. This finding is consistent with Lin and Hauptmann (2006) study of perspective vs topic classification: While topical differences between two corpora are manifested in difference in distributions of great many words, they observed little perspective-based variation in distributions of most words, apart from certain words that are preferentially used by adherents of one or the other perspective on the given topic. Consolidation of perspective We explore feature redundancy in perspective classification.We first investigate retention of only N best features, then elimination thereof. As a proxy of feature quality, we use the weight assigned to the feat"
P10-2047,W06-2915,0,0.543099,"Missing"
P10-2047,W02-1011,0,0.0113234,"perspectives on Middle Eastern affairs in 2003-2009 from http://www.bitterlemons-international.org/. The writers and interviewees on this site are usually former diplomats or government officials, academics, journalists, media and political analysts.4 The specific issues cover a broad spectrum, including public life, politics, wars and conflicts, education, trade relations in and between countries like Lebanon, Jordan, Iraq, Egypt, Yemen, Morocco, Saudi Arabia, as well as their relations with the US and members of the European Union. NB - COUNT and SVM - NORMF for perspective classification; Pang et al. (2002) consider most and Yu et al. (2008) all of the above for related tasks of movie review and political party classification. We use SVMlight (Joachims, 1999) for SVM and WEKA toolkit (Witten and Frank, 2005; Hall et al., 2009) for both version of Naive Bayes. Parameter optimization for all SVM models is performed using grid search on the training data separately for each partition into train and test data.6 3.1 Table 2 summarizes the cross-validation results for the four datasets discussed above. Notably, the SVM - BOOL model is either the best or not significantly different from the best perfor"
P13-1113,J10-4006,0,0.0411996,"text; we opted for all pairs of content word types occurring in a text, irrespective of the distance between them. We consider word types, not tokens; no lemmatization is performed. The third decision is how to represent the co-occurrence profiles; we use a histogram where each bin represents the proportion of word pairs in the given interval of PMI values. The rest of the section gives more detail about these decisions. To obtain comprehensive information about typical co-occurrence behavior of words of English, we build a first-order co-occurrence word-space model (Turney and Pantel, 2010; Baroni and Lenci, 2010). The model was generated from a corpus of texts of about 2.5 billion words, counting co-occurrence in a paragraph,2 using no distance coefficients (Bullinaria and Levy, 2007). About 2 billion words come from the Gigaword 2003 corpus (Graff and Cieri, 2003). Additional 500 million words come from an in-house corpus containing popular science and fiction texts. Occurrence counts of 2.1 million word types and of 1,279 million word type pairs are efficiently compressed using the TrendStream technology (Flor, 2013), resulting in a database file of 4.7GB. TrendStream is a trie-based architecture fo"
P13-1113,W97-0703,0,0.0280334,"e of words with related meanings in a text. Lexically cohesive words are traced through the text, forming lexical chains or graphs, and these representations are used in a variety of applications, such as segmentation, keyword extraction, summarization, sentiment analysis, temporal indexing, hypelink generation, error correction (Guinaudeau et al., 2012; Marathe and Hirst, 2010; Ercan and Cicekli, 2007; Devitt and Ahmad, 2007; Hirst and Budanitsky, 2005; Inkpen and D´esilets, 2005; Gurevych and Strube, 2004; Stokes et al., 2004; Silber and McCoy, 2002; Green, 1998; Al-Halimi and Kazman, 1998; Barzilay and Elhadad, 1997). To our knowledge, lexical cohesion has not so far been used for automated scoring of essays. Our results suggest that this direction is promising, as merely the proportion of highly associated word pairs is already contributing a clear signal regarding essay quality; it is possible that additional information can be derived from richer representations common in the lexical cohesion literature. Aspects related to the distribution of words in essays have been studied in relation to essay scoring. One line of work focuses on assessing coherence of essays. Foltz et al. (1998) use Latent 11 We al"
P13-1113,J08-1001,0,0.025992,". The results were similar to those of the blind test presented here, with erater+HAT significantly improving upon e-rater alone, using Wilcoxon test, W=374, n=29, p&lt;0.05. Semantic Analysis to model the smoothness of transitions between adjacent segments of an essay. Higgins et al. (2004) compare sentences from certain discourse segments in an essay to determine their semantic similarity, such as comparing thesis statements to conclusions or thesis statements to essay prompts. Additional approaches include evaluation of coherence based on repeated reference to entities (Burstein et al., 2010; Barzilay and Lapata, 2008; Miltsakaki and Kukich, 2004). Our approach is different in that it does not measure the flow of the text, that is, the sequencing and repetition of the words, but rather assesses the choice of vocabulary as a whole. Topic models have been proposed as a technique for capturing clusters of related words that tend to occur in the same documents in a given collection. A text is modeled as being composed of a small number of topics, and words in the text are generated conditioned on the selected topics (Gruber et al., 2007; Blei et al., 2003). Since (a) topics encapsulate clusters of highly assoc"
P13-1113,N10-1099,0,0.067663,"Missing"
P13-1113,J90-1003,0,0.511165,"r storage, retrieval, and updating of very large word n-gram datasets. We store pairwise word associations as bigrams; since associations are unordered, only one of the orders in actually stored in the database. There is an extensive literature on the use of word-association measures for NLP, especially for detection of collocations (Pecina, 2010; Evert, 2008; Futagi et al., 2008). The use of pointwise mutual information with word-space models is noted in (Zhang et al., 2012; Baroni and Lenci, 2010; Mitchell and Lapata, 2008; Turney, 2001). Point-wise mutual information is defined as follows (Church and Hanks, 1990): 2 In all texts, we use human-marked paragraphs, indicated either by a new line or by an xml markup. P M I(x, y) = log2 P (x, y) P (x)P (y) (1) Differently from Church and Hanks (1990), we disregard word order when computing P (x, y). All probabilities are estimated using frequencies. We define WAPT – a word association profile of a text T – as the distribution of PMI(x, y) for all pairs of content3 word types (x, y) ∈T. All pairs of word types for which the associations database returned a null value (the pair has never been observed in the same paragraph) are excluded from the calculation."
P13-1113,P07-1124,0,0.0278092,"ociation profile. Thus, following Halliday and Hasan (1976), Hoey (1991), and Morris and Hirst (1991), the notion of lexical cohesion has been used to capture repetitions of words and occurrence of words with related meanings in a text. Lexically cohesive words are traced through the text, forming lexical chains or graphs, and these representations are used in a variety of applications, such as segmentation, keyword extraction, summarization, sentiment analysis, temporal indexing, hypelink generation, error correction (Guinaudeau et al., 2012; Marathe and Hirst, 2010; Ercan and Cicekli, 2007; Devitt and Ahmad, 2007; Hirst and Budanitsky, 2005; Inkpen and D´esilets, 2005; Gurevych and Strube, 2004; Stokes et al., 2004; Silber and McCoy, 2002; Green, 1998; Al-Halimi and Kazman, 1998; Barzilay and Elhadad, 1997). To our knowledge, lexical cohesion has not so far been used for automated scoring of essays. Our results suggest that this direction is promising, as merely the proportion of highly associated word pairs is already contributing a clear signal regarding essay quality; it is possible that additional information can be derived from richer representations common in the lexical cohesion literature. Asp"
P13-1113,D08-1035,0,0.0809107,"Missing"
P13-1113,D08-1094,0,0.0437901,"Missing"
P13-1113,C04-1110,0,0.0109903,"rris and Hirst (1991), the notion of lexical cohesion has been used to capture repetitions of words and occurrence of words with related meanings in a text. Lexically cohesive words are traced through the text, forming lexical chains or graphs, and these representations are used in a variety of applications, such as segmentation, keyword extraction, summarization, sentiment analysis, temporal indexing, hypelink generation, error correction (Guinaudeau et al., 2012; Marathe and Hirst, 2010; Ercan and Cicekli, 2007; Devitt and Ahmad, 2007; Hirst and Budanitsky, 2005; Inkpen and D´esilets, 2005; Gurevych and Strube, 2004; Stokes et al., 2004; Silber and McCoy, 2002; Green, 1998; Al-Halimi and Kazman, 1998; Barzilay and Elhadad, 1997). To our knowledge, lexical cohesion has not so far been used for automated scoring of essays. Our results suggest that this direction is promising, as merely the proportion of highly associated word pairs is already contributing a clear signal regarding essay quality; it is possible that additional information can be derived from richer representations common in the lexical cohesion literature. Aspects related to the distribution of words in essays have been studied in relation t"
P13-1113,J97-1003,0,0.0615345,"ra will be used for illustration purposes. The paper is organized as follows. Section 2 presents our methodology for building word association profiles for texts. Section 3 illustrates the profiles for three corpora from different genres. Section 4.2 presents our study of the relationship between writing quality and patterns of word associations, with section 4.5 showing the results of adding a feature based on word association profile to a state-of-art essay scoring system. Related work is reviewed is section 5. 1 Note that the classical approach to topical segmentation of texts, TextTiling (Hearst, 1997), uses only word repetitions. The cited approaches use topic models that are in turn estimated using word co-occurrence. 1148 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1148–1158, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics 2 Methodology In order to describe the word association profile of a text, three decisions need to be made. The first decision is how to quantify the extent of cooccurrence between two words; we will use pointwise mutual information (PMI) estimated from a large and diverse corpus of t"
P13-1113,N04-1024,0,0.0197472,"erence of essays. Foltz et al. (1998) use Latent 11 We also performed a cross-validation test on setA p1p6, where we estimated a regression model on setA-pi and evaluate it on setA-pj, for all i, j ∈ {1, .., 6}, i 6= j, and compared the performance with that of e-rater alone on setApj, yielding 30 different train-test combinations. The results were similar to those of the blind test presented here, with erater+HAT significantly improving upon e-rater alone, using Wilcoxon test, W=374, n=29, p&lt;0.05. Semantic Analysis to model the smoothness of transitions between adjacent segments of an essay. Higgins et al. (2004) compare sentences from certain discourse segments in an essay to determine their semantic similarity, such as comparing thesis statements to conclusions or thesis statements to essay prompts. Additional approaches include evaluation of coherence based on repeated reference to entities (Burstein et al., 2010; Barzilay and Lapata, 2008; Miltsakaki and Kukich, 2004). Our approach is different in that it does not measure the flow of the text, that is, the sequencing and repetition of the words, but rather assesses the choice of vocabulary as a whole. Topic models have been proposed as a technique"
P13-1113,H05-1007,0,0.0633302,"Missing"
P13-1113,P98-2127,0,0.025112,"or automated scoring of essays. 1 Introduction The vast majority of contemporary research that investigates statistical properties of language deals with characterizing words by extracting information about their behavior from large corpora. Thus, co-occurrence of words in n-word windows, syntactic structures, sentences, paragraphs, and even whole documents is captured in vector-space models built from text corpora (Turney and Pantel, 2010; Basili and Pennacchiotti, 2010; Erk and Pad´o, 2008; Mitchell and Lapata, 2008; Bullinaria and Levy, 2007; Jones and Mewhort, 2007; Pado and Lapata, 2007; Lin, 1998; Landauer and Dumais, 1997; Lund and Burgess, 1996; Salton et al., 1975). However, little is known about typical profiles of texts in terms of co-occurrence behavior of their words. Some information can be inferred from the success of statistical techniques in predicting certain structures in text. For example, the From the applied perspective, our interest is in quantifying differences between well-written and poorly written essays, for the purposes of automated scoring of essays. We therefore concentrate on essay data for the main experiments reported in this paper, although some additional"
P13-1113,P08-1028,0,0.148096,"s of mildly associated pairs of words. Finally, we use word association profiles to improve a system for automated scoring of essays. 1 Introduction The vast majority of contemporary research that investigates statistical properties of language deals with characterizing words by extracting information about their behavior from large corpora. Thus, co-occurrence of words in n-word windows, syntactic structures, sentences, paragraphs, and even whole documents is captured in vector-space models built from text corpora (Turney and Pantel, 2010; Basili and Pennacchiotti, 2010; Erk and Pad´o, 2008; Mitchell and Lapata, 2008; Bullinaria and Levy, 2007; Jones and Mewhort, 2007; Pado and Lapata, 2007; Lin, 1998; Landauer and Dumais, 1997; Lund and Burgess, 1996; Salton et al., 1975). However, little is known about typical profiles of texts in terms of co-occurrence behavior of their words. Some information can be inferred from the success of statistical techniques in predicting certain structures in text. For example, the From the applied perspective, our interest is in quantifying differences between well-written and poorly written essays, for the purposes of automated scoring of essays. We therefore concentrate o"
P13-1113,J07-2002,0,0.0204622,"s to improve a system for automated scoring of essays. 1 Introduction The vast majority of contemporary research that investigates statistical properties of language deals with characterizing words by extracting information about their behavior from large corpora. Thus, co-occurrence of words in n-word windows, syntactic structures, sentences, paragraphs, and even whole documents is captured in vector-space models built from text corpora (Turney and Pantel, 2010; Basili and Pennacchiotti, 2010; Erk and Pad´o, 2008; Mitchell and Lapata, 2008; Bullinaria and Levy, 2007; Jones and Mewhort, 2007; Pado and Lapata, 2007; Lin, 1998; Landauer and Dumais, 1997; Lund and Burgess, 1996; Salton et al., 1975). However, little is known about typical profiles of texts in terms of co-occurrence behavior of their words. Some information can be inferred from the success of statistical techniques in predicting certain structures in text. For example, the From the applied perspective, our interest is in quantifying differences between well-written and poorly written essays, for the purposes of automated scoring of essays. We therefore concentrate on essay data for the main experiments reported in this paper, although some"
P13-1113,N12-1064,0,0.0277649,"Missing"
P13-1113,J02-4004,0,0.00947446,"esion has been used to capture repetitions of words and occurrence of words with related meanings in a text. Lexically cohesive words are traced through the text, forming lexical chains or graphs, and these representations are used in a variety of applications, such as segmentation, keyword extraction, summarization, sentiment analysis, temporal indexing, hypelink generation, error correction (Guinaudeau et al., 2012; Marathe and Hirst, 2010; Ercan and Cicekli, 2007; Devitt and Ahmad, 2007; Hirst and Budanitsky, 2005; Inkpen and D´esilets, 2005; Gurevych and Strube, 2004; Stokes et al., 2004; Silber and McCoy, 2002; Green, 1998; Al-Halimi and Kazman, 1998; Barzilay and Elhadad, 1997). To our knowledge, lexical cohesion has not so far been used for automated scoring of essays. Our results suggest that this direction is promising, as merely the proportion of highly associated word pairs is already contributing a clear signal regarding essay quality; it is possible that additional information can be derived from richer representations common in the lexical cohesion literature. Aspects related to the distribution of words in essays have been studied in relation to essay scoring. One line of work focuses on"
P13-1113,C98-2122,0,\N,Missing
P14-2041,P13-1000,0,0.185265,"Missing"
P14-2041,W04-1013,0,0.00447918,"tance – the content that is unique to the lecture is more important than the content that is shared by the lecture and the reading. The following two models capitalize on evidence of use of information in better and worse essays. For estimating these models, we sample, for each prompt, a development set of 750 essays responding to the prompt (that is, addressing a given pair of lecture and reading stimuli). Out of these, we take, for each prompt, all essays at score points 2 In the future, we intend to explore more complex realization functions, allowing paraphrase, skip n-grams (as in ROUGE (Lin, 2004)), and other approximate matches, such as misspellings and inflectional variants. 3 Prob, Position, and LectVsRead models normalize by nR and nL to enable comparison of essays responding to different lecture + reading stimuli (prompts). 248 4 and 5 (EGood) and all essays at score points 1 and 2 (EBad). These data do not overlap with the experimental data described in section 4. In both definitions below, e is an essay. We therefore evaluate each content importance model for different granularities of the content unit x: n-grams for n = 1, 2, 3, 4. Table 2 shows the correlations with essay scor"
P14-2041,N10-1122,0,0.0324406,"er, such as examples, discourse markers, evaluations, introduction and conclusion, etc. Our approach allows focusing on a particular aspect of content quality, namely, selection of appropriate materials from the source. 5 such as that fish, of fish, farming is, “, fish” 250 the other two being fishing and used. the reading sometimes interpret the same facts in a positive or negative light (for example, the fact that chemicals are used in fish farms is negative if compared to wild fish, but not so if compared to other farm-raised foods like poultry). Relationships between aspect and sentiment (Brody and Elhadad, 2010; Lazaridou et al., 2013) are also relevant, since aspects of the same fact are emphasized with different evaluations (the quantity vs the variety of species that go into fish meal for farmed fish). We hypothesize that units participating in sentiment and aspect contrasts are of higher importance; this is a direction for future work. Our results are related to the findings of Gurevich and Deane (2007) who studied the difference between the reading and the lecture in their impact on essay scores for this test. Using data from a single prompt, they showed that the difference between the essay’s"
P14-2041,N04-1019,0,0.0630845,"to summarize the points made in the lecture, explaining how they cast doubt on points made in the reading. The quality of the information selected from the lecture is emphasized in excerpts from the scoring rubric for this test (below); essays are scored on a 1-5 scale: 2 Design of Experiment In evaluations of summarization algorithms, it is common practice to derive the gold standard content importance scores from human summaries, as done, for example, in the pyramid method, where the importance of a content element corresponds to the number of reference human summaries that make use of it (Nenkova and Passonneau, 2004). Selection of the appropriate content plays a crucial role in attaining a high score for the essays we consider here, as suggested by the quotes from the scoring rubric in §1, as well as by a corpus study by Plakans and Gebril (2013). We therefore observe that high-scoring essays can be thought Score 5 successfully selects the important information from the lecture and coherently and accurately presents this information in relation to the relevant information presented in the reading. 1 http://www.corestandards.org/ ELA-Literacy/CCRA/W. 247 Proceedings of the 52nd Annual Meeting of the Associ"
P14-2041,W13-1721,0,0.0194633,"ssay scores for this test. Using data from a single prompt, they showed that the difference between the essay’s average cosine similarity to the reading and its average cosine similarity to the lecture is predictive of the score for non-native speakers of English, thus using a model similar to LectVsRead, although they took all lecture, reading, and essay words into account, in contrast to our model that looks only at n-grams that appear in the lecture. Our study shows that the effectiveness of lecture-reading contrast models for essay scoring generalizes to a large set of prompts. Similarly, Evanini et al. (2013) found that overlap with material that is unique to the lecture (not shared with the reading) was predictive of scores in a spoken source-based question answering task. 8 Conclusion In this paper, we addressed the task of automatically assigning importance scores to parts of a lecture that is to be summarized as part of an English language proficiency test. We investigated the optimal units of information to which importance should be assigned, as well as a variety of importance scoring models, drawing on the news summarization and essay scoring literature. We found that bigrams and trigrams w"
P14-2041,D10-1007,0,0.0432653,"Missing"
P14-2041,N07-2013,0,0.0318622,"le, the fact that chemicals are used in fish farms is negative if compared to wild fish, but not so if compared to other farm-raised foods like poultry). Relationships between aspect and sentiment (Brody and Elhadad, 2010; Lazaridou et al., 2013) are also relevant, since aspects of the same fact are emphasized with different evaluations (the quantity vs the variety of species that go into fish meal for farmed fish). We hypothesize that units participating in sentiment and aspect contrasts are of higher importance; this is a direction for future work. Our results are related to the findings of Gurevich and Deane (2007) who studied the difference between the reading and the lecture in their impact on essay scores for this test. Using data from a single prompt, they showed that the difference between the essay’s average cosine similarity to the reading and its average cosine similarity to the lecture is predictive of the score for non-native speakers of English, thus using a model similar to LectVsRead, although they took all lecture, reading, and essay words into account, in contrast to our model that looks only at n-grams that appear in the lecture. Our study shows that the effectiveness of lecture-reading"
P14-2041,E14-1075,0,0.122239,"tion, and LectVsRead models normalize by nR and nL to enable comparison of essays responding to different lecture + reading stimuli (prompts). 248 4 and 5 (EGood) and all essays at score points 1 and 2 (EBad). These data do not overlap with the experimental data described in section 4. In both definitions below, e is an essay. We therefore evaluate each content importance model for different granularities of the content unit x: n-grams for n = 1, 2, 3, 4. Table 2 shows the correlations with essay scores. Good: w(x) = |{e∈EGood|x∈e}| . An x is more im|EGood| portant if more good essays use it. Hong and Nenkova (2014) showed that a variant of this measure used on pairs of articles and their abstracts from the New York Times effectively identified words that typically go into summaries, across topics. In contrast, our measurements are prompt-specific. GoodVsBad: w(x) = |{e∈EBad|x∈e}| . |EBad| |{e∈EGood|x∈e}| |EGood| Content Importance Model Na¨ıve Prob Position LectVsRead Good GoodVsBad − An x is more important if good essays use it more than bad essays. To our knowledge, this measure has not been used in the summarization literature, probably because a large sample of human summaries of varying quality is"
P14-2041,P13-1160,0,0.0172766,"course markers, evaluations, introduction and conclusion, etc. Our approach allows focusing on a particular aspect of content quality, namely, selection of appropriate materials from the source. 5 such as that fish, of fish, farming is, “, fish” 250 the other two being fishing and used. the reading sometimes interpret the same facts in a positive or negative light (for example, the fact that chemicals are used in fish farms is negative if compared to wild fish, but not so if compared to other farm-raised foods like poultry). Relationships between aspect and sentiment (Brody and Elhadad, 2010; Lazaridou et al., 2013) are also relevant, since aspects of the same fact are emphasized with different evaluations (the quantity vs the variety of species that go into fish meal for farmed fish). We hypothesize that units participating in sentiment and aspect contrasts are of higher importance; this is a direction for future work. Our results are related to the findings of Gurevich and Deane (2007) who studied the difference between the reading and the lecture in their impact on essay scores for this test. Using data from a single prompt, they showed that the difference between the essay’s average cosine similarity"
P14-2064,J08-4004,0,0.0182651,"at investigating their impact on the learning outcomes in some detail can provide insight about the task at hand. The main contribution of this paper is therefore in providing additional empirical evidence in support of the argument put forward in the literature regarding the need to pay attention to problematic, disagreeable instances in annotated data – both from the linguistic and from the machine learning perspectives. Related Work Reliability of annotation is a concern widely discussed in the computational linguistics literature (Bayerl and Paul, 2011; Beigman Klebanov and Beigman, 2009; Artstein and Poesio, 2008; Craggs and McGee Wood, 2005; Di Eugenio and Glass, 2004; Carletta, 1996). Ensuring high reliability is not always feasible, however; the advent of crowdsourcing brought about interest in algorithms for recovering from noisy annotations: Snow et al. (2008), Passonneau and Carpenter (2013) and Raykar et al. (2010) discuss methods for improving over annotator majority vote when estimating the ground truth from multiple noisy annotations. A situation where learning from a small number of carefully chosen examples leads to a better performance in classifiers is discussed in the active learning li"
P14-2064,J11-4004,0,0.0166683,"fer, 2011; Reidsma and Carletta, 2008); yet we also show that investigating their impact on the learning outcomes in some detail can provide insight about the task at hand. The main contribution of this paper is therefore in providing additional empirical evidence in support of the argument put forward in the literature regarding the need to pay attention to problematic, disagreeable instances in annotated data – both from the linguistic and from the machine learning perspectives. Related Work Reliability of annotation is a concern widely discussed in the computational linguistics literature (Bayerl and Paul, 2011; Beigman Klebanov and Beigman, 2009; Artstein and Poesio, 2008; Craggs and McGee Wood, 2005; Di Eugenio and Glass, 2004; Carletta, 1996). Ensuring high reliability is not always feasible, however; the advent of crowdsourcing brought about interest in algorithms for recovering from noisy annotations: Snow et al. (2008), Passonneau and Carpenter (2013) and Raykar et al. (2010) discuss methods for improving over annotator majority vote when estimating the ground truth from multiple noisy annotations. A situation where learning from a small number of carefully chosen examples leads to a better pe"
P14-2064,P09-1032,1,0.94607,"Missing"
P14-2064,J09-4005,1,0.827546,", 2008); yet we also show that investigating their impact on the learning outcomes in some detail can provide insight about the task at hand. The main contribution of this paper is therefore in providing additional empirical evidence in support of the argument put forward in the literature regarding the need to pay attention to problematic, disagreeable instances in annotated data – both from the linguistic and from the machine learning perspectives. Related Work Reliability of annotation is a concern widely discussed in the computational linguistics literature (Bayerl and Paul, 2011; Beigman Klebanov and Beigman, 2009; Artstein and Poesio, 2008; Craggs and McGee Wood, 2005; Di Eugenio and Glass, 2004; Carletta, 1996). Ensuring high reliability is not always feasible, however; the advent of crowdsourcing brought about interest in algorithms for recovering from noisy annotations: Snow et al. (2008), Passonneau and Carpenter (2013) and Raykar et al. (2010) discuss methods for improving over annotator majority vote when estimating the ground truth from multiple noisy annotations. A situation where learning from a small number of carefully chosen examples leads to a better performance in classifiers is discusse"
P14-2064,N06-2004,1,0.811831,"e of the family of classifiers analyzed in Beigman and Beigman Klebanov (2009), where they are theoretically shown to be vulnerable to hard case bias in the worst case. To represent the instances, we use two features that capture semantic relatedness between words. One feature uses Latent Semantic Analysis (Deerwester et al., 1990) trained on the Wall Street Journal articles to quantify the distributional similarity of two words, the other uses an algorithm based on WordNet (Miller, 1990) to calculate semantic relatedness, combining information from both the hierarchy and the glosses (Beigman Klebanov, 2006). For each word, we calculate LSA (WordNet) relatedness score for this word with each preceding word in the text, and report the highest pairwise score as the LSA (WordNet) feature value for the given word. The values of the features can be thought of as quantifying the strength of the evidence for semantic non-newness that could be obtained via a distributional or a dictionary-based method. 1 The weight corresponds to the number of people who marked the item as 1, for hard cases. We take a weighted sample of 100 hard cases. 392 5 Discussion 5.1 5.2 Figure 1 plots the hard instances in the two"
P14-2064,J96-2004,0,0.00994888,"sight about the task at hand. The main contribution of this paper is therefore in providing additional empirical evidence in support of the argument put forward in the literature regarding the need to pay attention to problematic, disagreeable instances in annotated data – both from the linguistic and from the machine learning perspectives. Related Work Reliability of annotation is a concern widely discussed in the computational linguistics literature (Bayerl and Paul, 2011; Beigman Klebanov and Beigman, 2009; Artstein and Poesio, 2008; Craggs and McGee Wood, 2005; Di Eugenio and Glass, 2004; Carletta, 1996). Ensuring high reliability is not always feasible, however; the advent of crowdsourcing brought about interest in algorithms for recovering from noisy annotations: Snow et al. (2008), Passonneau and Carpenter (2013) and Raykar et al. (2010) discuss methods for improving over annotator majority vote when estimating the ground truth from multiple noisy annotations. A situation where learning from a small number of carefully chosen examples leads to a better performance in classifiers is discussed in the active learning literature (Schohn and Cohn, 2000; Cebron and Berthold, 2009; Nguyen and Sme"
P14-2064,W13-2323,0,0.0225711,"Missing"
P14-2064,H05-1002,0,0.0218128,"ral fellow at Northwestern University, Evanston, IL and the second author was a visiting assistant professor at Washington University, St. Louis, MO. 390 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 390–396, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics # 1s tionship between semantic non-novelty (conceptualized as semantic association with some preceding word in the text), the information structure in terms of given and new information, and the cognitive status of discourse entities (Postolache et al., 2005; Birner and Ward, 1998; Gundel et al., 1993; Prince, 1981). If an annotator identified an associative tie from the target word back to some other word in the text, the target word is thereby classified as semantically old (class 1, or positive); if no ties were identified, it is classified as new (class 0, or negative). 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 For the project, annotations were collected for 10 texts of various genres, where annotators were asked, for every first appearance of a word in a text, to point out previous words in the text that are semantically or associ"
P14-2064,W10-1808,0,0.0296917,"Missing"
P14-2064,P11-1005,0,0.107516,".org Abstract leads to unstable benchmarking results, as different gold standards might provide conflicting annotations for such items. Reidsma and Carletta (2008) demonstrated by simulation that systematic disagreements between annotators negatively impact generalization ability of classifiers built using data from different annotators. Oosten et al. (2011) showed that judgments of readability of the same texts by different groups of experts are sufficiently systematically different to hamper cross-expert generalization of readability classifiers trained on annotations from different groups. Rehbein and Ruppenhofer (2011) discuss the negative impact of systematic simulated annotation inconsistencies on active learning performance on a word-sense disambiguation task. In this paper, we address the task of classifying words in a text as semantically new or old. Using multiple annotators, we empirically identify instances that show substantial disagreement between annotators. We then discuss those both from the linguistic perspective, identifying some characteristics of such cases, and from the perspective of machine learning, showing that the presence of difficult cases in the training data misleads the machine l"
P14-2064,J08-3001,0,0.0856477,"ied. We addressed this issue in the context of a subjective semantic task. In this setting, we showed that the presence of difficult instances in training data misleads a machine learner into misclassifying clear-cut, easy cases. We also showed that considering machine learning outcomes with and without the difficult cases, it is possible to identify specific weaknesses of the problem representation. Our results align with the literature suggesting that difficult cases in training data can be disruptive (Beigman and Beigman Klebanov, 2009; Schwartz et al., 2011; Rehbein and Ruppenhofer, 2011; Reidsma and Carletta, 2008); yet we also show that investigating their impact on the learning outcomes in some detail can provide insight about the task at hand. The main contribution of this paper is therefore in providing additional empirical evidence in support of the argument put forward in the literature regarding the need to pay attention to problematic, disagreeable instances in annotated data – both from the linguistic and from the machine learning perspectives. Related Work Reliability of annotation is a concern widely discussed in the computational linguistics literature (Bayerl and Paul, 2011; Beigman Klebano"
P14-2064,P11-1067,0,0.12137,"ses of the problem representation. 1 Introduction The problem of cases that are difficult for annotation received recent attention from both the theoretical and the applied perspectives. Such items might receive contradictory labels, without a clear way of settling the disagreement. Beigman and Beigman Klebanov (2009) showed theoretically that hard cases – items with unreliable annotations – can lead to unfair benchmarking results when found in test data, and, in worst case, to a degradation in a machi74ne learner’s performance on easy, uncontroversial instances if found in the training data. Schwartz et al. (2011) provided an empirical demonstration that the presence of such difficult cases in dependency parsing evaluations 2 Data The task considered here is that of classifying first occurrences of words in a text as semantically old or new. One of goals of the project is to investigate the relationship between various kinds of non-novelty in text, and, in particular, the rela1 The work presented in this paper was done when the first author was a post-doctoral fellow at Northwestern University, Evanston, IL and the second author was a visiting assistant professor at Washington University, St. Louis, MO"
P14-2064,D08-1027,0,0.0269523,"Missing"
P14-2064,J05-3001,0,\N,Missing
P14-2064,Q14-1025,0,\N,Missing
P16-2017,W14-2302,1,0.859433,"Missing"
P16-2017,W15-1402,1,0.882725,"icsi.berkeley.edu, ekaterina.shutova@cl.cam.ac.uk Abstract (Tsvetkov et al., 2014; Heintz et al., 2013; Turney et al., 2011; Birke and Sarkar, 2007; Gedigan et al., 2006), rather than using naturally occurring continuous text, as done here. Beigman Klebanov et al. (2014) and Beigman Klebanov et al. (2015) are the exceptions, used as a baseline in the current paper. Features that have been used so far in supervised metaphor classification address concreteness and abstractness, topic models, orthographic unigrams, sensorial features, semantic classifications using WordNet, among others (Beigman Klebanov et al., 2015; Tekiroglu et al., 2015; Tsvetkov et al., 2014; Dunn, 2014; Heintz et al., 2013; Turney et al., 2011). Of the feature sets presented in this paper, all but WordNet features are novel. We investigate the effectiveness of semantic generalizations/classifications for capturing the regularities of the behavior of verbs in terms of their metaphoricity. Starting from orthographic word unigrams, we experiment with various ways of defining semantic classes for verbs (grammatical, resource-based, distributional) and measure the effectiveness of these classes for classifying all verbs in a running text"
P16-2017,W07-0104,0,0.0704928,"Missing"
P16-2017,P07-1115,0,0.0145023,"rated verb clusters as semantic classes. We clustered VerbNet verbs using a spectral clustering algorithm and lexico-syntactic features. We selected the verbs that occur more than 150 times in the British National Corpus, 1,610 in total, and clustered them into 150 clusters (Corpus). We used verb subcategorization frames (SCF) and the verb’s nominal arguments as features for clustering, as they have proved successful in previous verb classification experiments (Shutova et al., 2010). We extracted our features from the Gigaword corpus (Graff et al., 2003) using the SCF classification system of Preiss et al. (2007) to identify verb SCFs and the RASP parser (Briscoe et al., 2006) to extract the verb’s nominal arguments. Spectral clustering partitions the data relying on a similarity matrix that records similarities between all pairs of data points. We use JensenShannon divergence (dJS ) to measure similarity between feature vectors for two verbs, vi and vj , and construct a similarity matrix Sij : Sij = exp(−dJS (vi , vj )) (1) The matrix S encodes a similarity graph G over our verbs. The clustering problem can then be defined as identifying the optimal partition, or cut, of the graph into clusters. We u"
P16-2017,P06-4020,0,0.00877201,"rbs using a spectral clustering algorithm and lexico-syntactic features. We selected the verbs that occur more than 150 times in the British National Corpus, 1,610 in total, and clustered them into 150 clusters (Corpus). We used verb subcategorization frames (SCF) and the verb’s nominal arguments as features for clustering, as they have proved successful in previous verb classification experiments (Shutova et al., 2010). We extracted our features from the Gigaword corpus (Graff et al., 2003) using the SCF classification system of Preiss et al. (2007) to identify verb SCFs and the RASP parser (Briscoe et al., 2006) to extract the verb’s nominal arguments. Spectral clustering partitions the data relying on a similarity matrix that records similarities between all pairs of data points. We use JensenShannon divergence (dJS ) to measure similarity between feature vectors for two verbs, vi and vj , and construct a similarity matrix Sij : Sij = exp(−dJS (vi , vj )) (1) The matrix S encodes a similarity graph G over our verbs. The clustering problem can then be defined as identifying the optimal partition, or cut, of the graph into clusters. We use the multiway normalized cut (MNCut) algorithm of Meila and Shi"
P16-2017,C10-1113,1,0.908946,"h thematic role (VN-Role). Finally, VerbNet provides annotations of the re3.3 Corpus-based We also experimented with automaticallygenerated verb clusters as semantic classes. We clustered VerbNet verbs using a spectral clustering algorithm and lexico-syntactic features. We selected the verbs that occur more than 150 times in the British National Corpus, 1,610 in total, and clustered them into 150 clusters (Corpus). We used verb subcategorization frames (SCF) and the verb’s nominal arguments as features for clustering, as they have proved successful in previous verb classification experiments (Shutova et al., 2010). We extracted our features from the Gigaword corpus (Graff et al., 2003) using the SCF classification system of Preiss et al. (2007) to identify verb SCFs and the RASP parser (Briscoe et al., 2006) to extract the verb’s nominal arguments. Spectral clustering partitions the data relying on a similarity matrix that records similarities between all pairs of data points. We use JensenShannon divergence (dJS ) to measure similarity between feature vectors for two verbs, vi and vj , and construct a similarity matrix Sij : Sij = exp(−dJS (vi , vj )) (1) The matrix S encodes a similarity graph G over"
P16-2017,W14-2304,0,0.0331739,"t al., 2014; Heintz et al., 2013; Turney et al., 2011; Birke and Sarkar, 2007; Gedigan et al., 2006), rather than using naturally occurring continuous text, as done here. Beigman Klebanov et al. (2014) and Beigman Klebanov et al. (2015) are the exceptions, used as a baseline in the current paper. Features that have been used so far in supervised metaphor classification address concreteness and abstractness, topic models, orthographic unigrams, sensorial features, semantic classifications using WordNet, among others (Beigman Klebanov et al., 2015; Tekiroglu et al., 2015; Tsvetkov et al., 2014; Dunn, 2014; Heintz et al., 2013; Turney et al., 2011). Of the feature sets presented in this paper, all but WordNet features are novel. We investigate the effectiveness of semantic generalizations/classifications for capturing the regularities of the behavior of verbs in terms of their metaphoricity. Starting from orthographic word unigrams, we experiment with various ways of defining semantic classes for verbs (grammatical, resource-based, distributional) and measure the effectiveness of these classes for classifying all verbs in a running text as metaphor or non metaphor. 1 Introduction According to t"
P16-2017,W15-1404,0,0.602277,"erina.shutova@cl.cam.ac.uk Abstract (Tsvetkov et al., 2014; Heintz et al., 2013; Turney et al., 2011; Birke and Sarkar, 2007; Gedigan et al., 2006), rather than using naturally occurring continuous text, as done here. Beigman Klebanov et al. (2014) and Beigman Klebanov et al. (2015) are the exceptions, used as a baseline in the current paper. Features that have been used so far in supervised metaphor classification address concreteness and abstractness, topic models, orthographic unigrams, sensorial features, semantic classifications using WordNet, among others (Beigman Klebanov et al., 2015; Tekiroglu et al., 2015; Tsvetkov et al., 2014; Dunn, 2014; Heintz et al., 2013; Turney et al., 2011). Of the feature sets presented in this paper, all but WordNet features are novel. We investigate the effectiveness of semantic generalizations/classifications for capturing the regularities of the behavior of verbs in terms of their metaphoricity. Starting from orthographic word unigrams, we experiment with various ways of defining semantic classes for verbs (grammatical, resource-based, distributional) and measure the effectiveness of these classes for classifying all verbs in a running text as metaphor or non meta"
P16-2017,W06-3506,0,0.313909,"Missing"
P16-2017,P14-1024,0,0.473829,"uk Abstract (Tsvetkov et al., 2014; Heintz et al., 2013; Turney et al., 2011; Birke and Sarkar, 2007; Gedigan et al., 2006), rather than using naturally occurring continuous text, as done here. Beigman Klebanov et al. (2014) and Beigman Klebanov et al. (2015) are the exceptions, used as a baseline in the current paper. Features that have been used so far in supervised metaphor classification address concreteness and abstractness, topic models, orthographic unigrams, sensorial features, semantic classifications using WordNet, among others (Beigman Klebanov et al., 2015; Tekiroglu et al., 2015; Tsvetkov et al., 2014; Dunn, 2014; Heintz et al., 2013; Turney et al., 2011). Of the feature sets presented in this paper, all but WordNet features are novel. We investigate the effectiveness of semantic generalizations/classifications for capturing the regularities of the behavior of verbs in terms of their metaphoricity. Starting from orthographic word unigrams, we experiment with various ways of defining semantic classes for verbs (grammatical, resource-based, distributional) and measure the effectiveness of these classes for classifying all verbs in a running text as metaphor or non metaphor. 1 Introduction Ac"
P16-2017,W13-0908,0,0.126343,"Heintz et al., 2013; Turney et al., 2011; Birke and Sarkar, 2007; Gedigan et al., 2006), rather than using naturally occurring continuous text, as done here. Beigman Klebanov et al. (2014) and Beigman Klebanov et al. (2015) are the exceptions, used as a baseline in the current paper. Features that have been used so far in supervised metaphor classification address concreteness and abstractness, topic models, orthographic unigrams, sensorial features, semantic classifications using WordNet, among others (Beigman Klebanov et al., 2015; Tekiroglu et al., 2015; Tsvetkov et al., 2014; Dunn, 2014; Heintz et al., 2013; Turney et al., 2011). Of the feature sets presented in this paper, all but WordNet features are novel. We investigate the effectiveness of semantic generalizations/classifications for capturing the regularities of the behavior of verbs in terms of their metaphoricity. Starting from orthographic word unigrams, we experiment with various ways of defining semantic classes for verbs (grammatical, resource-based, distributional) and measure the effectiveness of these classes for classifying all verbs in a running text as metaphor or non metaphor. 1 Introduction According to the Conceptual Metapho"
P16-2017,D11-1063,0,\N,Missing
P17-2038,P16-1150,0,0.0165917,"relevant evidence (Council of Chief State School Officers & National Governors Association, 2010). The increase in argumentative writing tasks, in both instructional and assessment contexts, results in a high demand for automated feedback on and scoring of arguments. Automated analysis of argumentative writing has mostly concentrated on argument structure – namely, presence of claims and premises, and relationships between them (Ghosh et al., 2016; Nguyen and Litman, 2016; Persing and Ng, 2016; Ong et al., 2014; Stab and Gurevych, 2014). Addressing the content of arguments in on-line debates, Habernal and Gurevych (2016) ranked arguments on the same topic by convincingness; they showed that convincingness can be automatically predicted, to an extent, in a cross-topics fashion, as 244 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 244–249 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2038 3 will have seen essays responding to each of the prompts it could encounter at deployment time is often unwarranted. Further, not only should a system be able to handle responses to a"
P17-2038,P16-1107,0,0.120032,"on & Related Work Argumentation is an important skill in higher education and the workplace; students are expected to show sound reasoning and use relevant evidence (Council of Chief State School Officers & National Governors Association, 2010). The increase in argumentative writing tasks, in both instructional and assessment contexts, results in a high demand for automated feedback on and scoring of arguments. Automated analysis of argumentative writing has mostly concentrated on argument structure – namely, presence of claims and premises, and relationships between them (Ghosh et al., 2016; Nguyen and Litman, 2016; Persing and Ng, 2016; Ong et al., 2014; Stab and Gurevych, 2014). Addressing the content of arguments in on-line debates, Habernal and Gurevych (2016) ranked arguments on the same topic by convincingness; they showed that convincingness can be automatically predicted, to an extent, in a cross-topics fashion, as 244 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 244–249 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2038 3 will have seen essays respondi"
P17-2038,W14-2104,0,0.0297665,"skill in higher education and the workplace; students are expected to show sound reasoning and use relevant evidence (Council of Chief State School Officers & National Governors Association, 2010). The increase in argumentative writing tasks, in both instructional and assessment contexts, results in a high demand for automated feedback on and scoring of arguments. Automated analysis of argumentative writing has mostly concentrated on argument structure – namely, presence of claims and premises, and relationships between them (Ghosh et al., 2016; Nguyen and Litman, 2016; Persing and Ng, 2016; Ong et al., 2014; Stab and Gurevych, 2014). Addressing the content of arguments in on-line debates, Habernal and Gurevych (2016) ranked arguments on the same topic by convincingness; they showed that convincingness can be automatically predicted, to an extent, in a cross-topics fashion, as 244 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 244–249 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2038 3 will have seen essays responding to each of the prompts it could encou"
P17-2038,N16-1164,0,0.102482,"tation is an important skill in higher education and the workplace; students are expected to show sound reasoning and use relevant evidence (Council of Chief State School Officers & National Governors Association, 2010). The increase in argumentative writing tasks, in both instructional and assessment contexts, results in a high demand for automated feedback on and scoring of arguments. Automated analysis of argumentative writing has mostly concentrated on argument structure – namely, presence of claims and premises, and relationships between them (Ghosh et al., 2016; Nguyen and Litman, 2016; Persing and Ng, 2016; Ong et al., 2014; Stab and Gurevych, 2014). Addressing the content of arguments in on-line debates, Habernal and Gurevych (2016) ranked arguments on the same topic by convincingness; they showed that convincingness can be automatically predicted, to an extent, in a cross-topics fashion, as 244 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 244–249 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2038 3 will have seen essays responding to each of the prom"
P17-2038,W14-2110,1,0.802338,"fective prediction was achieved. Song et al. (2014) developed an annotation protocol for analyzing argument critiques in students’ essays, drawing on the theory of argumentation schemes (Walton et al., 2008; Walton, 1996). According to this theory, different types of arguments invite specific types of critiques. For example, an argument from authority made in the prompt – According to X, Y is the case – avails critiques along the lines of whether X has the necessary knowledge and is an unbiased source of information about Y. Analyzing prompts used in an assessment of argument critique skills, Song et al. (2014) identified a number of common schemes, such as arguments from policy, sample, example, and used the argumentation schemes theory to specify what critiques would count as “good” for arguments from the given scheme. Once a prompt is associated with a specific set of argumentation schemes, it follows that those critiques that count as good under one of the schemes used in the prompt would be considered as good critiques in essays responding to that prompt. The goal of the annotation was to identify all sentences in an essay that participate in making a good critique, according to the above defin"
P17-2038,D14-1006,0,0.0867774,"ducation and the workplace; students are expected to show sound reasoning and use relevant evidence (Council of Chief State School Officers & National Governors Association, 2010). The increase in argumentative writing tasks, in both instructional and assessment contexts, results in a high demand for automated feedback on and scoring of arguments. Automated analysis of argumentative writing has mostly concentrated on argument structure – namely, presence of claims and premises, and relationships between them (Ghosh et al., 2016; Nguyen and Litman, 2016; Persing and Ng, 2016; Ong et al., 2014; Stab and Gurevych, 2014). Addressing the content of arguments in on-line debates, Habernal and Gurevych (2016) ranked arguments on the same topic by convincingness; they showed that convincingness can be automatically predicted, to an extent, in a cross-topics fashion, as 244 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 244–249 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2038 3 will have seen essays responding to each of the prompts it could encounter at deployment time is"
P17-2038,P16-2089,0,0.262721,"uments. 1 Introduction & Related Work Argumentation is an important skill in higher education and the workplace; students are expected to show sound reasoning and use relevant evidence (Council of Chief State School Officers & National Governors Association, 2010). The increase in argumentative writing tasks, in both instructional and assessment contexts, results in a high demand for automated feedback on and scoring of arguments. Automated analysis of argumentative writing has mostly concentrated on argument structure – namely, presence of claims and premises, and relationships between them (Ghosh et al., 2016; Nguyen and Litman, 2016; Persing and Ng, 2016; Ong et al., 2014; Stab and Gurevych, 2014). Addressing the content of arguments in on-line debates, Habernal and Gurevych (2016) ranked arguments on the same topic by convincingness; they showed that convincingness can be automatically predicted, to an extent, in a cross-topics fashion, as 244 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 244–249 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2038 3 will"
P17-2038,W15-4631,0,0.0632652,"Missing"
P19-3024,W18-0530,1,0.893066,"Missing"
Q13-1009,E06-1027,0,0.0893898,"er of machine learning algorithms: SVM tion 7.2. Linear and RBF, Na¨ıve Bayes, Logistic Regression The task of assigning sentiment profiles to words (using WEKA (Hall et al., 2009)), and c5.0 Decision in a sentiment lexicon has been addressed in the liteTrees (Quinlan, 1993).5 rature. SentiWordNet assigns profiles to all words in WordNet based on a propagation algorithm from a 5.1 Data small seed set manually annotated by a small numWe generated the data for training and testing the ber of judges (Baccianella et al., 2010; Cerini et al., machine learning systems as follows. We used our 2007). Andreevskaia and Bergler (2006) use graph 5 propagation algorithms on WordNet to assign cenavailable from http://rulequest.com/ 800 −1.0 −0.5 0.0 −1.0 0.5 −0.5 1.0 0.0 0.5 1.0 800 102 pool of 100,000 essays to sample a second, nonoverlapping set of 5,000 essays, so that no essay used for lexicon development appears in this set. From these essays, we randomly sampled 550 sentences, and submitted them to sentiment polarity annotation by two experienced research assistants; 50 double-annotated sentenced showed κ=0.8. TEST set contains the 43 agreed double-annotated sentences, and additional 238 sampled from the 500 single-anno"
Q13-1009,baccianella-etal-2010-sentiwordnet,0,0.0987014,"of the observed trends, we experiment with with WordNet-based expansion as descibed in seca number of machine learning algorithms: SVM tion 7.2. Linear and RBF, Na¨ıve Bayes, Logistic Regression The task of assigning sentiment profiles to words (using WEKA (Hall et al., 2009)), and c5.0 Decision in a sentiment lexicon has been addressed in the liteTrees (Quinlan, 1993).5 rature. SentiWordNet assigns profiles to all words in WordNet based on a propagation algorithm from a 5.1 Data small seed set manually annotated by a small numWe generated the data for training and testing the ber of judges (Baccianella et al., 2010; Cerini et al., machine learning systems as follows. We used our 2007). Andreevskaia and Bergler (2006) use graph 5 propagation algorithms on WordNet to assign cenavailable from http://rulequest.com/ 800 −1.0 −0.5 0.0 −1.0 0.5 −0.5 1.0 0.0 0.5 1.0 800 102 pool of 100,000 essays to sample a second, nonoverlapping set of 5,000 essays, so that no essay used for lexicon development appears in this set. From these essays, we randomly sampled 550 sentences, and submitted them to sentiment polarity annotation by two experienced research assistants; 50 double-annotated sentenced showed κ=0.8. TEST se"
Q13-1009,P05-1074,0,0.0214597,"itive and negative sentiment words manually selected from a full list of word types in these data, and (2) words marked in a small-scale annotation of a sample of sentences from these data for all positive and negative words. A more detailed descrip100 tion of the construction of seed lexicon can be found in Beigman Klebanov et al (2012). The seed lexicon contains 749 single words, 406 positive and 343 negative. 2.2 Expanded Lexicon We used a pivot-based lexical and phrasal paraphrase generation system (Madnani and Dorr, 2013). The paraphraser implements the pivot-based method as described by Bannard and Callison-Burch (2005) with several additional filtering mechanisms to increase the precision of the extracted pairs. The pivot-based method utilizes the inherent monolingual semantic knowledge from bilingual corpora: We first identify phrasal correspondences between English and a given foreign language F , then map from English to English by following translation units from English to the other language and back. For example, if the two English phrases e1 and e2 both correspond to the same foreign phrase f , then they may be considered to be paraphrases of each other with the following probability: p(e1|e2) ≈ p(e1"
Q13-1009,W11-1716,0,0.0265561,"Missing"
Q13-1009,P12-1105,0,0.211161,"Missing"
Q13-1009,E06-1025,0,0.307381,"Missing"
Q13-1009,N09-1002,0,0.0368913,"Missing"
Q13-1009,P97-1023,0,0.0619707,"n-enrichment procedure for a sentiment clas400 400 400 400Qiu et400 400 2010; Velikovich et al., 2010; al., 2009; Mo- sification400 task. hammad et al., 2009; Esuli and Sebastiani, 2006; profiles 200 and Hovy, 2004; Andreevskaia 200 200 for sentence-level 200 200 Kim and Bergler, 5 Using 200 200 sentiment polarity classification 2006; Hu and Liu, 2004; Kanayama and Nasukawa, 2006; Strapparava and Valitutti, 2004; Kamps et al., 0 0 0 0 To evaluate 0 the usefulness of the lexicons, we 0 use 2004; Takamura et al., 2005;0 Turney and Littman, them to generate features for machine learning sys2003; Hatzivassiloglou and McKeown, 1997). The tems, and compare performance on 3-way sentenceparaphrase-based expansion method is in the dislevel sentiment polarity classification. To ensure rotributional similarity camp; we also experimented bustness of the observed trends, we experiment with with WordNet-based expansion as descibed in seca number of machine learning algorithms: SVM tion 7.2. Linear and RBF, Na¨ıve Bayes, Logistic Regression The task of assigning sentiment profiles to words (using WEKA (Hall et al., 2009)), and c5.0 Decision in a sentiment lexicon has been addressed in the liteTrees (Quinlan, 1993).5 rature. SentiW"
Q13-1009,E09-1046,0,0.021996,"available for research and education only1 and under GNU GPL license that disallows their incorporation into proprietary materials,2 respectively. 1 2 http://www.wjh.harvard.edu/ inquirer/j1 1/manual/ http://www.gnu.org/copyleft/gpl.html Those wishing to integrate sentiment analysis into products, along with those studying subjectivity in languages other than English, or for specific domains such as finance, or for particular genres such as MySpace comments, reported construction of lexicons (Taboada et al., 2011; Loughran and McDonald, 2011; Thelwall et al., 2010; Rao and Ravichandran, 2009; Jijkoun and Hofmann, 2009; Pitel and Grefenstette, 2008; Mihalcea et al., 2007). In this paper, we address the step of expanding a small-scale, manually-built subjectivity lexicon (a seed lexicon, typically for a domain or language in question) into a much larger but noisier lexicon using an automatic procedure. We present a novel expansion method using a state-of-the-art paraphrasing system. The expansion yields a 4-fold increase in lexicon size; yet, the expansion alone is insufficient in order to improve performance on sentence-level sentiment polarity classification. In this paper we test the following hypothesis."
Q13-1009,kamps-etal-2004-using,0,0.236109,"Missing"
Q13-1009,W06-1642,0,0.25578,"Missing"
Q13-1009,C04-1200,0,0.674053,"Missing"
Q13-1009,2005.mtsummit-papers.11,0,0.00782496,"y are all used in computing the probability: p(e1|e2) ≈ Seed abuse accuse anxiety conflict X f0 Expansion exploitation reproach disquiet crisis p(e1|f 0 )p(f 0 |e2) Seed costly dangerous improve invaluable Expansion onerous unsafe reinforce precious Table 1: Examples of paraphraser expansions. Some examples of expansions generated by the paraphraser are shown in Table 1. More details about this kind of approach can be found in Bannard and Callison-Burch (2005). We use the FrenchEnglish parallel corpus (approximately 1.2 million sentences) from the corpus of European parliamentary proceedings (Koehn, 2005) as the data on which pivoting is performed to extract the paraphrases. However, the base paraphrase system is susceptible to large amounts of noise due to the imperfect bilingual word alignments. Therefore, we implement additional heuristics in order to minimize the number of noisy paraphrase pairs (Madnani and Dorr, 2013). For example, one such heuristic filters out pairs where a function word may have been inferred as a paraphrase of a content word. For the lexicon expansion experiment reported here, we use the top 15 single-word paraphrases for every word from the seed lexicon, excluding m"
Q13-1009,P98-2127,0,0.147367,"Missing"
Q13-1009,P07-1123,0,0.0284025,"GPL license that disallows their incorporation into proprietary materials,2 respectively. 1 2 http://www.wjh.harvard.edu/ inquirer/j1 1/manual/ http://www.gnu.org/copyleft/gpl.html Those wishing to integrate sentiment analysis into products, along with those studying subjectivity in languages other than English, or for specific domains such as finance, or for particular genres such as MySpace comments, reported construction of lexicons (Taboada et al., 2011; Loughran and McDonald, 2011; Thelwall et al., 2010; Rao and Ravichandran, 2009; Jijkoun and Hofmann, 2009; Pitel and Grefenstette, 2008; Mihalcea et al., 2007). In this paper, we address the step of expanding a small-scale, manually-built subjectivity lexicon (a seed lexicon, typically for a domain or language in question) into a much larger but noisier lexicon using an automatic procedure. We present a novel expansion method using a state-of-the-art paraphrasing system. The expansion yields a 4-fold increase in lexicon size; yet, the expansion alone is insufficient in order to improve performance on sentence-level sentiment polarity classification. In this paper we test the following hypothesis. We suggest that the effectiveness of the expansion is"
Q13-1009,D09-1063,0,0.38205,"Missing"
Q13-1009,pitel-grefenstette-2008-semi,0,0.0206582,"education only1 and under GNU GPL license that disallows their incorporation into proprietary materials,2 respectively. 1 2 http://www.wjh.harvard.edu/ inquirer/j1 1/manual/ http://www.gnu.org/copyleft/gpl.html Those wishing to integrate sentiment analysis into products, along with those studying subjectivity in languages other than English, or for specific domains such as finance, or for particular genres such as MySpace comments, reported construction of lexicons (Taboada et al., 2011; Loughran and McDonald, 2011; Thelwall et al., 2010; Rao and Ravichandran, 2009; Jijkoun and Hofmann, 2009; Pitel and Grefenstette, 2008; Mihalcea et al., 2007). In this paper, we address the step of expanding a small-scale, manually-built subjectivity lexicon (a seed lexicon, typically for a domain or language in question) into a much larger but noisier lexicon using an automatic procedure. We present a novel expansion method using a state-of-the-art paraphrasing system. The expansion yields a 4-fold increase in lexicon size; yet, the expansion alone is insufficient in order to improve performance on sentence-level sentiment polarity classification. In this paper we test the following hypothesis. We suggest that the effective"
Q13-1009,E09-1077,0,0.0517249,"ebe and Riloff, 2005) – are available for research and education only1 and under GNU GPL license that disallows their incorporation into proprietary materials,2 respectively. 1 2 http://www.wjh.harvard.edu/ inquirer/j1 1/manual/ http://www.gnu.org/copyleft/gpl.html Those wishing to integrate sentiment analysis into products, along with those studying subjectivity in languages other than English, or for specific domains such as finance, or for particular genres such as MySpace comments, reported construction of lexicons (Taboada et al., 2011; Loughran and McDonald, 2011; Thelwall et al., 2010; Rao and Ravichandran, 2009; Jijkoun and Hofmann, 2009; Pitel and Grefenstette, 2008; Mihalcea et al., 2007). In this paper, we address the step of expanding a small-scale, manually-built subjectivity lexicon (a seed lexicon, typically for a domain or language in question) into a much larger but noisier lexicon using an automatic procedure. We present a novel expansion method using a state-of-the-art paraphrasing system. The expansion yields a 4-fold increase in lexicon size; yet, the expansion alone is insufficient in order to improve performance on sentence-level sentiment polarity classification. In this paper we tes"
Q13-1009,strapparava-valitutti-2004-wordnet,0,0.195547,"Missing"
Q13-1009,W08-1207,0,0.0328751,"Missing"
Q13-1009,P05-1017,0,0.0938726,"xicon, showing the effectiveness of this ture thereof (Cruz et al., 2011; Baccianella et al., lexicon-enrichment procedure for a sentiment clas400 400 400 400Qiu et400 400 2010; Velikovich et al., 2010; al., 2009; Mo- sification400 task. hammad et al., 2009; Esuli and Sebastiani, 2006; profiles 200 and Hovy, 2004; Andreevskaia 200 200 for sentence-level 200 200 Kim and Bergler, 5 Using 200 200 sentiment polarity classification 2006; Hu and Liu, 2004; Kanayama and Nasukawa, 2006; Strapparava and Valitutti, 2004; Kamps et al., 0 0 0 0 To evaluate 0 the usefulness of the lexicons, we 0 use 2004; Takamura et al., 2005;0 Turney and Littman, them to generate features for machine learning sys2003; Hatzivassiloglou and McKeown, 1997). The tems, and compare performance on 3-way sentenceparaphrase-based expansion method is in the dislevel sentiment polarity classification. To ensure rotributional similarity camp; we also experimented bustness of the observed trends, we experiment with with WordNet-based expansion as descibed in seca number of machine learning algorithms: SVM tion 7.2. Linear and RBF, Na¨ıve Bayes, Logistic Regression The task of assigning sentiment profiles to words (using WEKA (Hall et al., 200"
Q13-1009,N03-1033,0,0.0138253,"Missing"
Q13-1009,N10-1119,0,0.0992446,"Missing"
Q13-1009,P06-1134,0,0.0744594,"Missing"
Q13-1009,J11-2001,0,\N,Missing
Q13-1009,C98-2122,0,\N,Missing
W08-1202,J96-2004,0,0.501946,"Missing"
W09-2001,W08-1202,1,0.836013,"Missing"
W09-2001,E06-1042,0,0.174157,"stopped,” she said. Traditionally, metaphor detectors use the observation that a metaphorically used item creates a local incongruity because there is a violation of a selectional restriction, such as providing a non-vehicle object to the verb derail in Protesters derailed the conference. Current state of art in metaphor detection therefore tends to be “localistic” – the distributional profile of the target word in its immediate grammatical or collocational context in a background corpus or a database like WordNet is used to determine metaphoricity (Mason, 2004; Krishnakumaran and Zhu, 2007; Birke and Sarkar, 2006; Gedigian et al., 2006; Fass, 1991). However, some theories of metaphor postulate certain features of metaphors that connect it to the surrounding text beyond the small grammatical or proximal locality. For example, for Kittay (1987) metaphor is a discourse phenomenon; although the minimal metaphoric unit is a clause, often much larger chunks of text constitute a metaphor. Consider, for example, the TRAIN metaphor in the In the example above, the quotation is not in itself a metaphor, as there is no indication that something other than the actual train is being discussed (and so no local inco"
W09-2001,U07-1005,0,0.0153419,"words that (a) contained only letters; and (b) appeared at least 6 times in the collection. All documents were indexed using this 21,046 word vocabulary. We will designate all the indexed words in document i as Di . To identify the main discourse topics in the EUI+M corpus, we submitted the indexed documents to an unsupervised clustering method Latent Dirichlet Allocation (Blei et al., 2003) (henceforth, LDA).6 The designation of the clusters as topics is supported by findings reported in Blei et al. (2003) that the clusters contain information relevant for topic discrimination. Additionally, Chanen and Patrick (2007) show that LDA achieves significant correlations with humans on a topic characterization task, where humans produced not just a topic classification but also identified phrases they believed were indicative of each class. Using the default settings of LDA implementation,7 we analyzed the corpus into 100 topics. Table 1 exemplifies some of the emergent topics. 1.3 Topical words in a text LDA is a generative model of text. According to its outlook, every text is about a small (typically 5-7) number of topics, and each indexed word in the text belongs to one of these topics. However, in many case"
W09-2001,J91-1003,0,0.543018,"tectors use the observation that a metaphorically used item creates a local incongruity because there is a violation of a selectional restriction, such as providing a non-vehicle object to the verb derail in Protesters derailed the conference. Current state of art in metaphor detection therefore tends to be “localistic” – the distributional profile of the target word in its immediate grammatical or collocational context in a background corpus or a database like WordNet is used to determine metaphoricity (Mason, 2004; Krishnakumaran and Zhu, 2007; Birke and Sarkar, 2006; Gedigian et al., 2006; Fass, 1991). However, some theories of metaphor postulate certain features of metaphors that connect it to the surrounding text beyond the small grammatical or proximal locality. For example, for Kittay (1987) metaphor is a discourse phenomenon; although the minimal metaphoric unit is a clause, often much larger chunks of text constitute a metaphor. Consider, for example, the TRAIN metaphor in the In the example above, the quotation is not in itself a metaphor, as there is no indication that something other than the actual train is being discussed (and so no local incongruities exist). Only when situated"
W09-2001,W06-3506,0,0.0612217,"ditionally, metaphor detectors use the observation that a metaphorically used item creates a local incongruity because there is a violation of a selectional restriction, such as providing a non-vehicle object to the verb derail in Protesters derailed the conference. Current state of art in metaphor detection therefore tends to be “localistic” – the distributional profile of the target word in its immediate grammatical or collocational context in a background corpus or a database like WordNet is used to determine metaphoricity (Mason, 2004; Krishnakumaran and Zhu, 2007; Birke and Sarkar, 2006; Gedigian et al., 2006; Fass, 1991). However, some theories of metaphor postulate certain features of metaphors that connect it to the surrounding text beyond the small grammatical or proximal locality. For example, for Kittay (1987) metaphor is a discourse phenomenon; although the minimal metaphoric unit is a clause, often much larger chunks of text constitute a metaphor. Consider, for example, the TRAIN metaphor in the In the example above, the quotation is not in itself a metaphor, as there is no indication that something other than the actual train is being discussed (and so no local incongruities exist). Only"
W09-2001,W07-0103,0,0.0851282,"ctorates. But the train can be stopped,” she said. Traditionally, metaphor detectors use the observation that a metaphorically used item creates a local incongruity because there is a violation of a selectional restriction, such as providing a non-vehicle object to the verb derail in Protesters derailed the conference. Current state of art in metaphor detection therefore tends to be “localistic” – the distributional profile of the target word in its immediate grammatical or collocational context in a background corpus or a database like WordNet is used to determine metaphoricity (Mason, 2004; Krishnakumaran and Zhu, 2007; Birke and Sarkar, 2006; Gedigian et al., 2006; Fass, 1991). However, some theories of metaphor postulate certain features of metaphors that connect it to the surrounding text beyond the small grammatical or proximal locality. For example, for Kittay (1987) metaphor is a discourse phenomenon; although the minimal metaphoric unit is a clause, often much larger chunks of text constitute a metaphor. Consider, for example, the TRAIN metaphor in the In the example above, the quotation is not in itself a metaphor, as there is no indication that something other than the actual train is being discuss"
W09-2001,J04-1002,0,0.147944,"rstood by electorates. But the train can be stopped,” she said. Traditionally, metaphor detectors use the observation that a metaphorically used item creates a local incongruity because there is a violation of a selectional restriction, such as providing a non-vehicle object to the verb derail in Protesters derailed the conference. Current state of art in metaphor detection therefore tends to be “localistic” – the distributional profile of the target word in its immediate grammatical or collocational context in a background corpus or a database like WordNet is used to determine metaphoricity (Mason, 2004; Krishnakumaran and Zhu, 2007; Birke and Sarkar, 2006; Gedigian et al., 2006; Fass, 1991). However, some theories of metaphor postulate certain features of metaphors that connect it to the surrounding text beyond the small grammatical or proximal locality. For example, for Kittay (1987) metaphor is a discourse phenomenon; although the minimal metaphoric unit is a clause, often much larger chunks of text constitute a metaphor. Consider, for example, the TRAIN metaphor in the In the example above, the quotation is not in itself a metaphor, as there is no indication that something other than the"
W09-2001,W07-0102,0,0.0241226,"Missing"
W12-2007,P08-1004,0,0.0364752,"Missing"
W12-2007,Y06-1004,0,0.0274959,"s, as well as organization and content (Attali and Burstein, 2006). For assessment of content, the focus is traditionally on topical appropriateness of the vocabulary (Attali and Burstein, 2006; Landauer et al., 2003; Louis and Higgins, 2010; Chen et al., 2010; De and Kopparapu, 2011; Higgins et al., 2006; Ishioka and Kameda, 2006; Kakkonen et al., 2005; Kakkonen and Sutinen, 2004; Lemaire and Dessus, 2001; Ros´e et al., 2003; Larkey, 1998), although recently other aspects, such as detection of sentiment or figurative language, have started to attract attention (Beigman Klebanov et al., 2012; Chang et al., 2006). The nature of factual information used in an essay has not so far been addressed, to our knowledge; yet a misleading premise, insufficient factual basis, or an example that flies in the face of the reader’s knowledge clearly detract from an essay’s quality. This paper presents a study on assessing the use of factual knowledge in argumentative essays on general topics written for a graduate school entrance exam. We propose a definition of fact, and an operationalization thereof. We find that the proposed measure has positive medium-strength correlation with essay grade, which remains signific"
W12-2007,D09-1028,0,0.0269648,"Missing"
W12-2007,de-marneffe-etal-2006-generating,0,0.0301696,"Missing"
W12-2007,P05-1045,0,0.0513228,"Missing"
W12-2007,M95-1001,0,0.269421,"Missing"
W12-2007,P06-1030,0,0.0313056,"simple, relatively robust measure vs a more sophisticated measure with better construct validity. Implications for development of automated essay scoring systems are discussed. 1 Introduction Automated scoring of essays deals with various aspects of writing, such as grammar, usage, mechanics, as well as organization and content (Attali and Burstein, 2006). For assessment of content, the focus is traditionally on topical appropriateness of the vocabulary (Attali and Burstein, 2006; Landauer et al., 2003; Louis and Higgins, 2010; Chen et al., 2010; De and Kopparapu, 2011; Higgins et al., 2006; Ishioka and Kameda, 2006; Kakkonen et al., 2005; Kakkonen and Sutinen, 2004; Lemaire and Dessus, 2001; Ros´e et al., 2003; Larkey, 1998), although recently other aspects, such as detection of sentiment or figurative language, have started to attract attention (Beigman Klebanov et al., 2012; Chang et al., 2006). The nature of factual information used in an essay has not so far been addressed, to our knowledge; yet a misleading premise, insufficient factual basis, or an example that flies in the face of the reader’s knowledge clearly detract from an essay’s quality. This paper presents a study on assessing the use of f"
W12-2007,W05-0206,0,0.0324422,"measure vs a more sophisticated measure with better construct validity. Implications for development of automated essay scoring systems are discussed. 1 Introduction Automated scoring of essays deals with various aspects of writing, such as grammar, usage, mechanics, as well as organization and content (Attali and Burstein, 2006). For assessment of content, the focus is traditionally on topical appropriateness of the vocabulary (Attali and Burstein, 2006; Landauer et al., 2003; Louis and Higgins, 2010; Chen et al., 2010; De and Kopparapu, 2011; Higgins et al., 2006; Ishioka and Kameda, 2006; Kakkonen et al., 2005; Kakkonen and Sutinen, 2004; Lemaire and Dessus, 2001; Ros´e et al., 2003; Larkey, 1998), although recently other aspects, such as detection of sentiment or figurative language, have started to attract attention (Beigman Klebanov et al., 2012; Chang et al., 2006). The nature of factual information used in an essay has not so far been addressed, to our knowledge; yet a misleading premise, insufficient factual basis, or an example that flies in the face of the reader’s knowledge clearly detract from an essay’s quality. This paper presents a study on assessing the use of factual knowledge in arg"
W12-2007,P03-1054,0,0.00730639,"Missing"
W12-2007,W10-1013,1,0.843814,"f essay length was factored out. The correlations did not differ substantionally between a simple, relatively robust measure vs a more sophisticated measure with better construct validity. Implications for development of automated essay scoring systems are discussed. 1 Introduction Automated scoring of essays deals with various aspects of writing, such as grammar, usage, mechanics, as well as organization and content (Attali and Burstein, 2006). For assessment of content, the focus is traditionally on topical appropriateness of the vocabulary (Attali and Burstein, 2006; Landauer et al., 2003; Louis and Higgins, 2010; Chen et al., 2010; De and Kopparapu, 2011; Higgins et al., 2006; Ishioka and Kameda, 2006; Kakkonen et al., 2005; Kakkonen and Sutinen, 2004; Lemaire and Dessus, 2001; Ros´e et al., 2003; Larkey, 1998), although recently other aspects, such as detection of sentiment or figurative language, have started to attract attention (Beigman Klebanov et al., 2012; Chang et al., 2006). The nature of factual information used in an essay has not so far been addressed, to our knowledge; yet a misleading premise, insufficient factual basis, or an example that flies in the face of the reader’s knowledge cle"
W12-2007,X93-1015,0,0.345266,"Missing"
W12-2007,P02-1006,0,0.106482,"Missing"
W12-2007,W03-0210,0,0.0943607,"Missing"
W12-2007,J93-3001,0,\N,Missing
W13-0902,W08-1202,1,0.844033,"Missing"
W13-0902,W06-3506,0,0.196458,"Missing"
W13-0902,W07-0102,0,0.218779,"Missing"
W13-0902,shutova-teufel-2010-metaphor,0,0.314491,"Missing"
W13-0902,J13-2003,0,0.16176,"Missing"
W13-1504,J10-4006,0,0.0250107,"e dog barked and wagged its tail: dog barked wagged tail dog 7.02 7.64 5.57 barked 9.18 5.95 wagged 9.45 tail Green ideas sleep furiously: green ideas sleep furiously green 0.44 1.47 2.05 ideas 1.01 0.94 sleep 2.18 furiously Table 1. Word association matrices (PMI values) for two illustrative examples. 10 Percentage of pairs of word tokens There exists an extensive literature on the use of word-association measures for NLP, especially for detection of collocations (Pecina, 2010; Evert, 2008). The use of pointwise mutual information (PMI) with word-space models is noted in (Zhang et al., 2012; Baroni and Lenci, 2010; Mitchell and Lapata, 2008; Turney, 2001). We begin with PMI, and provide a modified measure in later sections. To obtain comprehensive information about cooccurrence behavior of words in English, we build a first-order co-occurrence word-space model (Turney and Pantel, 2010; Baroni and Lenci, 2010). The model was generated from a corpus of texts of about 2.5 billion word tokens, counting non-directed co-occurrence in a paragraph, using no distance coefficients (Bullinaria and Levy, 2007). About 2 billion word tokens come from the Gigaword 2003 corpus (Graff and Cieri, 2003). Additional 500 m"
W13-1504,P13-1113,1,0.839785,"-bin histrogram spanning all obtained PMI values, the lowest bin contains pairs with PMI≤–5, the highest bin contains pairs with PMI>4.83, while the rest of the bins contain word pairs (a,b) with -5<PMI(a,b)≤4.83. Figure 1 presents WAP histograms for two real text samples, one for grade level 3 (age 8-9) and one for grade level 11 (age 16-17). We observe that the shape of distribution is normal-like. The distribution of GL3 text is shifted to the right – it contains more highly associated word-pairs than the text of GL11. In a separate study we investigated the properties of WAP distribution (Beigman-Klebanov and Flor, 2013). The normal-like shape turns out to be stable across a variety of texts. The dog barked and wagged its tail: dog barked wagged tail dog 7.02 7.64 5.57 barked 9.18 5.95 wagged 9.45 tail Green ideas sleep furiously: green ideas sleep furiously green 0.44 1.47 2.05 ideas 1.01 0.94 sleep 2.18 furiously Table 1. Word association matrices (PMI values) for two illustrative examples. 10 Percentage of pairs of word tokens There exists an extensive literature on the use of word-association measures for NLP, especially for detection of collocations (Pecina, 2010; Evert, 2008). The use of pointwise mutua"
W13-1504,J90-1003,0,0.517908,"xtGL11 -2 -1 0 TextGL3 1 2 3 4 5 PMI Figure 1. Word Association Profiles for two sample texts, showing 60-bin histograms with smoothed lines instead of bars. The last bin of the histogram contains all pairs with PMI>4.83, hence the uptick at PMI=5. 2.2 Lexical Tightness 2.3 Datasets In this section we consider how to derive a single measure to represent each text for further analyses. Given the stable normal-like shape of WAP, we use average (mean) value per text for further investigations. We experimented with several association measures. Point-wise mutual information is defined as follows (Church and Hanks, 1990): Our data consists of two sets of passages. The first set consists of 1012 passages (636K words) – reading materials that were used in various tests in state and national assessment frameworks in the USA. Part of this set is taken from Sheehan et al. (2007) (from testing programs and US state departments of education), and part was taken from the Standardized State Test Passages set of the Race To The Top (RTT) competition (Nelson et al., 2012). A distinguishing feature of this dataset is that the exact grade level specification was available for each text. Table 2 provides the breakdown by g"
W13-1504,N04-1025,0,0.0351301,"ree height, noun-phrase count, verbphrase count and average count of subordinated clauses. They use machine learning to train classifiers for direct prediction of grade level. Vajjala and Meurers (2012) also use machine learning, with a wide variety of features, including classic features, parse features, and features motivated from studies on second language acquisition, such as Lexical Density and Type-Token Ratio. Word frequency and its derivations, such as proportion of rare words, are utilized in many models of complexity (Graesser et al., 2011; Sheehan et al, 2010; Stenner et al., 2006; Collins-Thompson and Callan, 2004). Inspired by psycholinguistic research, two systems have explicitly set to measure textual cohesion for estimations of readability and complexity: Coh-Metrix (Graesser et al., 2011) and SourceRater (Sheehan et al., 2010). One notion of cohesion involved in those two systems is lexical cohesion – the amount of lexically/semantically related words in a text. Some amount of local lexical cohesion can be measured via stem overlap of adjacent sentences, with averaging of such metric per text (McNamara et al., 2010). However, Sheehan et al. (submitted) demonstrated that such measure is not well cor"
W13-1504,P08-1028,0,0.016944,"its tail: dog barked wagged tail dog 7.02 7.64 5.57 barked 9.18 5.95 wagged 9.45 tail Green ideas sleep furiously: green ideas sleep furiously green 0.44 1.47 2.05 ideas 1.01 0.94 sleep 2.18 furiously Table 1. Word association matrices (PMI values) for two illustrative examples. 10 Percentage of pairs of word tokens There exists an extensive literature on the use of word-association measures for NLP, especially for detection of collocations (Pecina, 2010; Evert, 2008). The use of pointwise mutual information (PMI) with word-space models is noted in (Zhang et al., 2012; Baroni and Lenci, 2010; Mitchell and Lapata, 2008; Turney, 2001). We begin with PMI, and provide a modified measure in later sections. To obtain comprehensive information about cooccurrence behavior of words in English, we build a first-order co-occurrence word-space model (Turney and Pantel, 2010; Baroni and Lenci, 2010). The model was generated from a corpus of texts of about 2.5 billion word tokens, counting non-directed co-occurrence in a paragraph, using no distance coefficients (Bullinaria and Levy, 2007). About 2 billion word tokens come from the Gigaword 2003 corpus (Graff and Cieri, 2003). Additional 500 million word tokens come fro"
W13-1504,W06-1605,0,0.0520687,"Missing"
W13-1504,W12-2019,0,0.199598,"tes with grade level in a collection of expertly rated reading materials. Lexical tightness captures aspects of prose complexity that are not covered by classic readability indexes, especially for literary texts. We also present initial findings on the utility of this measure for automated estimation of complexity for poetry. 1 Introduction Adequate estimation of text complexity has a long and rich history. Various readability metrics have been designed in the last 100 years (DuBay, 2004). Recent work on computational estimation of text complexity for school- and college-level texts includes (Vajjala and Meurers 2012; Graesser et al., 2011; Sheehan et al., 2010; Petersen and Ostendorf, 2009; Heilman et al., 2006). Several commercial systems were recently evaluated in the Race To The Top competition (Nelson et al., 2012) in relation to the US Common Core State Standards for instruction (CCSSI, 2010). A variety of factors influence text complexity, including vocabulary, sentence structure, academic orientation, narrativity, cohesion, etc. (Hiebert, 2011) and corresponding features are utilized in automated systems of complexity evaluation (Vajjala and Meurers, 2012; Graesser et al., 2011; Sheehan et al., 20"
W13-3304,P13-1113,1,0.855024,"Missing"
W13-3304,W12-3156,0,0.0853761,"Missing"
W13-3304,D11-1084,0,0.064617,"target language, Carpuat and Simard (2012) found that when MT systems produce different target language translations, they are stylistically, syntactically, or semantically inadequate in most cases Introduction While most current approaches to machine translation concentrate on single sentences, there is emerging interest in phenomena that go beyond a single sentence and pertain to the whole text being translated. For example, Wong and Kit (2012) demonstrated that repetition of content words is a predictor of translation quality, with poorer translations failing to repeat words appropriately. Gong et al. (2011) and Tiedemann (2010) present caching of translations from earlier sections of a document to facilitate the translation of its later sections. 1 italics in the original 27 Proceedings of the Workshop on Discourse in Machine Translation (DiscoMT), pages 27–32, c Sofia, Bulgaria, August 9, 2013. 2013 Association for Computational Linguistics Annotated Corpus.4 The selected articles had baseball annotated as their sole topic, and ranged from 250 to 750 words in length. We expect these articles to contain a large group of words that reflects vocabulary that is commonly used in discussing baseball"
W13-3304,W10-2602,0,0.0577196,"t and Simard (2012) found that when MT systems produce different target language translations, they are stylistically, syntactically, or semantically inadequate in most cases Introduction While most current approaches to machine translation concentrate on single sentences, there is emerging interest in phenomena that go beyond a single sentence and pertain to the whole text being translated. For example, Wong and Kit (2012) demonstrated that repetition of content words is a predictor of translation quality, with poorer translations failing to repeat words appropriately. Gong et al. (2011) and Tiedemann (2010) present caching of translations from earlier sections of a document to facilitate the translation of its later sections. 1 italics in the original 27 Proceedings of the Workshop on Discourse in Machine Translation (DiscoMT), pages 27–32, c Sofia, Bulgaria, August 9, 2013. 2013 Association for Computational Linguistics Annotated Corpus.4 The selected articles had baseball annotated as their sole topic, and ranged from 250 to 750 words in length. We expect these articles to contain a large group of words that reflects vocabulary that is commonly used in discussing baseball and no other systemat"
W13-3304,N12-1046,0,0.0907398,"loss – through domestication or foreignization – it can help establish that association loss has occurred in at least one or both of the translation processes involved, since the original native English version provides a natural benchmark against which the resulting back-translations can be measured. To make the phenomenon of association loss more concrete, consider the following sentence: (see upper panel of Table 5 therein), that is, diversifying the signifiers appropriately is a challenging task. For recent work on biasing SMT systems towards consistent translations of repeated words, see Ture et al. (2012) and Xiao et al. (2011). Moving beyond single signifieds, or concepts, Berman faults translations for “the destruction of underlying networks of signification”, whereby groups of related words are translated without preserving the relatedness in the target language. While these might be unavoidable in any translation, we show below that machine translation specifically indeed suffers from such a loss (section 3) and that machine translation suffers from it more than the human translations (section 4). 2 Methodology We define WAPT – a word association profile of a text T – as the distribution o"
W13-3304,D12-1097,0,0.354016,"d as an important reality in his work by the use of three signifiers. The translation that does not respect this multiplicity renders the “visage” of an unrecognizable work. There is a loss, then, since the translation contains fewer signifiers than the original.”1 While Berman’s remarks refer to literary translation, recent work demonstrates its relevance for machine translation, showing that MT systems tend to under-use linguistic devices that are commonly used for repeated reference, such as superordinates or meronyms, although the pattern with synonyms and near-synonyms was not clear cut (Wong and Kit, 2012). Studying a complementary phenomenon of translation of same-lemma lexical items in the source document into a target language, Carpuat and Simard (2012) found that when MT systems produce different target language translations, they are stylistically, syntactically, or semantically inadequate in most cases Introduction While most current approaches to machine translation concentrate on single sentences, there is emerging interest in phenomena that go beyond a single sentence and pertain to the whole text being translated. For example, Wong and Kit (2012) demonstrated that repetition of conten"
W13-3304,2011.mtsummit-papers.13,0,0.0619989,"ation or foreignization – it can help establish that association loss has occurred in at least one or both of the translation processes involved, since the original native English version provides a natural benchmark against which the resulting back-translations can be measured. To make the phenomenon of association loss more concrete, consider the following sentence: (see upper panel of Table 5 therein), that is, diversifying the signifiers appropriately is a challenging task. For recent work on biasing SMT systems towards consistent translations of repeated words, see Ture et al. (2012) and Xiao et al. (2011). Moving beyond single signifieds, or concepts, Berman faults translations for “the destruction of underlying networks of signification”, whereby groups of related words are translated without preserving the relatedness in the target language. While these might be unavoidable in any translation, we show below that machine translation specifically indeed suffers from such a loss (section 3) and that machine translation suffers from it more than the human translations (section 4). 2 Methodology We define WAPT – a word association profile of a text T – as the distribution of PMI(x, y) for all pai"
W14-2110,P98-1032,0,0.0308434,"Missing"
W14-2110,P11-1099,0,0.044237,"usage, style, essay organization, and vocabulary) examined by the baseline model. Furthermore, we have implemented an automated system for predicting the human annotations. This system focused only on predicting whether or not a sentence raises any critical questions (i.e., generic vs. nongeneric). In the future, we plan to test whether features based on automated annotations make contributions to essay scoring models that are similar to the contributions of manual annotations. We also plan to work on detecting specific critical questions and adding additional features, such as features from Feng and Hirst (2011). .379 Prompt A combined 7 Table 4: Performance of the NLP Model The model trained on data from both prompts performed relatively well compared to the other models. For the testing data for prompt B, the combined model outperformed the model trained on just data from prompt B. However, the prompt-specific model for prompt A slightly outperformed the combined model on the testing data for prompt A. Although the performance of models trained with data from one prompt and tested with data from another prompt did not perform as well, there is evidence of some generalization across prompts. The mod"
W14-2110,N12-1003,1,0.759163,"includes multiple points that match more than one critical question. Thirdly, for a highlighted unit, the annotator will choose a topic, a category, and a second topic, if applicable. Only one category label can be assigned to each selected text unit. “Generic” information will not be selected or assigned an annotation label. Generic information includes restatements of the text in the prompt, general statements that do not address any specific questions, rhetoric attacks, and irrelevant information. Note that this notion of generic information is related to “shell language,” as described by Madnani et al (2012). However, our definition here focuses more closely on sentences that do not raise critical questions. Surface errors (e.g., grammar and spelling) can be Application of the Annotation Approach This section focuses on applying the annotation approach and the following research question: Can this scheme-based annotation approach be applied consistently by raters to a corpus of argumentative essays? 4.1 Annotation Rules 72 ignored if they do not prevent people from understanding the meaning of the essay. Here is an example of annotated text. Four annotators with linguistics backgrounds who were n"
W14-2110,C98-1032,0,\N,Missing
W14-2110,N12-1000,0,\N,Missing
W14-2110,W08-0908,0,\N,Missing
W14-2302,W13-0908,0,0.252489,"Missing"
W14-2302,W13-0902,1,0.841799,"vision, reading books is not as important as it once was. People can learn as much by watching television as they can by reading books.” (Set B). Multiple essays on the same topic is a unique feature of this dataset, allowing the examination of the effect of topic on performance, by comparing performance in within-topic and across-topic settings. The essays were annotated using a protocol that prefers a reader’s intuition over a formal definition, and emphasizes the connection between metaphor and the arguments that are put forward by the writer. The protocol is presented in detail in Beigman Klebanov and Flor (2013). All essays were doubly annotated. The reliability is κ = 0.58 for Set A and κ = 0.56 for Set B. We merge the two annotations (union), following the observation in a previous study Beigman Klebanov et al. (2008) that attention slips play a large role in accounting for observed disagreements. We will report results for 10-fold crossvalidation on each of sets A and B, as well as • Part-of-Speech (P): We use Stanford POS tagger 3.3.0 and the full Penn Treebank tagset for content words (tags starting with A, N, V, and J), removing the auxiliaries have, be, do. • Concreteness (C): We use Brysbaert"
W14-2302,W08-1202,1,0.817648,"t, allowing the examination of the effect of topic on performance, by comparing performance in within-topic and across-topic settings. The essays were annotated using a protocol that prefers a reader’s intuition over a formal definition, and emphasizes the connection between metaphor and the arguments that are put forward by the writer. The protocol is presented in detail in Beigman Klebanov and Flor (2013). All essays were doubly annotated. The reliability is κ = 0.58 for Set A and κ = 0.56 for Set B. We merge the two annotations (union), following the observation in a previous study Beigman Klebanov et al. (2008) that attention slips play a large role in accounting for observed disagreements. We will report results for 10-fold crossvalidation on each of sets A and B, as well as • Part-of-Speech (P): We use Stanford POS tagger 3.3.0 and the full Penn Treebank tagset for content words (tags starting with A, N, V, and J), removing the auxiliaries have, be, do. • Concreteness (C): We use Brysbaert et al. (2013) database of concreteness ratings for about 40,000 English words. The mean ratings, ranging 1-5, are binned in 0.25 increments; each bin is used as a binary feature. • Topic models (T): We use Laten"
W14-2302,W13-0907,0,0.133107,"Missing"
W14-2302,W09-2002,0,0.151224,"Missing"
W14-2302,H05-1053,0,0.0514231,"Missing"
W14-2302,P06-4018,0,0.01572,"gger 3.3.0 and the full Penn Treebank tagset for content words (tags starting with A, N, V, and J), removing the auxiliaries have, be, do. • Concreteness (C): We use Brysbaert et al. (2013) database of concreteness ratings for about 40,000 English words. The mean ratings, ranging 1-5, are binned in 0.25 increments; each bin is used as a binary feature. • Topic models (T): We use Latent Dirichlet Allocation (Blei et al., 2003) to derive a 100-topic model from the NYT corpus years 2003–2007 (Sandhaus, 2008) to represent common topics of public discussion. The NYT data was lemmatized using NLTK (Bird, 2006). We used the gensim toolkit ˇ uˇrek and Sojka, 2010) for building the (Reh˚ models, with default parameters. The score assigned to an instance w on a topic t is log PP(w|t) (w) where P (w) were estimated from the Gigaword corpus (Parker et al., 2009). These features are based on the hypothesis that certain topics are likelier to be used as source domains for metaphors than others. 4 Results For each dataset, we present the results for the unigram model (baseline) and the results for the full model containing all the features. For crossvalidation results, all words from the same text were alwa"
W14-2302,Q13-1031,0,0.0662045,"Missing"
W14-2302,W13-0904,0,0.320055,"Missing"
W14-2302,W13-0901,0,0.444432,"Missing"
W14-2302,J91-1003,0,0.383731,"Missing"
W14-2302,H92-1045,0,0.123337,"Missing"
W14-2302,W06-3506,0,0.211309,"Missing"
W14-2302,N13-1118,0,0.0731795,"Missing"
W14-2302,shutova-teufel-2010-metaphor,0,0.369067,"Missing"
W14-2302,C10-1113,0,0.473573,"Missing"
W14-2302,N03-1033,0,0.0439816,"Missing"
W14-2302,D11-1063,0,0.239015,"Missing"
W14-4705,J10-4006,0,0.0273311,"One part is the English Gigaword 2003 corpus (Graff and Cieri, 2003), with 1.7 billion tokens. The second part is an ETS in-house corpus containing texts from the genres of fiction and popular science (Sheehan et al., 2006), with about 430 million tokens. 2.2 Types of distributional information From this combined corpus we have built two specific lexical resources. One resource is a bigram repository, which stores counts for sequences of two words. The other resource is a first-order co-occurrence word-space model (Turney and Pantel, 2010), also known as a Distributional Semantic Model (DSM) (Baroni and Lenci, 2010). In our implementation of DSM, we counted non-directed cooccurrence of tokens in a paragraph, using no distance coefficients (Bullinaria and Levy, 2007). Counts for 2.1 million word-form types, and the sparse matrix of their co-occurrences, are efficiently compressed using the TrendStream toolkit (Flor, 2013), resulting in a database file of 4.7GB. The same toolkit supports both n-grams and DSM repositories, and allows fast retrieval of word probabilities and statistical associations for pairs of words. 1 It also supports retrieval of co-occurrence vectors. When generating these two resources"
W14-4705,W13-3304,1,0.842918,"s described above were not generated for the COGALEX-4 shared task. Rather, those are general-purpose large-scale lexical resources that we have used in previous research, for a variety of NLP tasks. This is an important aspect, as our intention was to find out how well those general resources would perform on this novel task. Our bigrams repository is actually part of a 5gram language model that is used for context-aware spelling correction. The algorithms for that application are described by Flor (2012). The DSM has been used for spelling correction (Flor, 2012), for essay scoring (Beigman Klebanov and Flor, 2013a), for readability estimation (Flor and Beigman Klebanov, in press; Flor et al., 2013), as well as for a study on quality of machine translation (Beigman Klebanov and Flor, 2013b). 2.3 Measures of association For the shared task, we used three measures of word association. Pointwise Mutual Information (Church & Hanks, 1990): PMI a , b=log 2 P a , b P a  P b Normalized Pointwise Mutual Information (Bouma, 2009): NPMI  a , b=log 2 1 P a , b / −log 2 P a , b P  a P b  The TrendStream toolkit provides compression and storage for large-scale n-gram models, and for large-scale"
W14-4705,J90-1003,0,0.465225,"Missing"
W14-4705,W13-1504,1,0.792321,"l-purpose large-scale lexical resources that we have used in previous research, for a variety of NLP tasks. This is an important aspect, as our intention was to find out how well those general resources would perform on this novel task. Our bigrams repository is actually part of a 5gram language model that is used for context-aware spelling correction. The algorithms for that application are described by Flor (2012). The DSM has been used for spelling correction (Flor, 2012), for essay scoring (Beigman Klebanov and Flor, 2013a), for readability estimation (Flor and Beigman Klebanov, in press; Flor et al., 2013), as well as for a study on quality of machine translation (Beigman Klebanov and Flor, 2013b). 2.3 Measures of association For the shared task, we used three measures of word association. Pointwise Mutual Information (Church & Hanks, 1990): PMI a , b=log 2 P a , b P a  P b Normalized Pointwise Mutual Information (Bouma, 2009): NPMI  a , b=log 2 1 P a , b / −log 2 P a , b P  a P b  The TrendStream toolkit provides compression and storage for large-scale n-gram models, and for large-scale co-occurrence matrices. In all cases, actual counts are stored and values for statisti"
W14-4705,W14-1810,0,0.0656075,"Missing"
W14-4705,rapp-2014-corpus,0,0.0214057,"ll experiments. Experiments with additional filtering are described in section 4.2. In any single experimental run we consistently use the same measure of association (no mixing of different formulae). 37 (SBS), and 2) product (multiplication) of ranks (MR). Sum of best scores is simply the sum of best association scores that a candidate has with each of the five cues (families). To produce a final ranked list of candidate targets, candidates are sorted by their aggregate sum value (better candidates have higher values). Multiplication of ranks has been proposed as an aggregation procedure by Rapp (2014, 2008). In this procedure, all candidates are sorted by their association scores with each of the five cues (families) separately, and five rank values are registered for each candidate. The five rank values are then multiplied to produce the final aggregate score. All candidates are then sorted by the aggregate score, and in such ranking better candidates have lower aggregate scores. Multiplication of ranks is computationally more intensive than sum of scores – for a given set of candidate words from five cues, multiplication of ranks requires six calls for sorting, while aggregation via sum"
W14-4705,W08-1914,0,0.0555009,"Missing"
W14-4705,P13-1113,1,\N,Missing
W15-1402,W13-0902,1,0.934308,". The use of supervised machine learning techniques for metaphor identification has increased manyfold in the recent years (see section 10, Related Work, for a review and references), partially due to the availability of largescale annotated resources for training and evaluating the algorithms, such as the VU Amsterdam corpus (Steen et al., 2010), datasets built as part of a U.S. government-funded initiative to advance the state-of-art in metaphor identification and interpretation (Mohler et al., 2013; Strzalkowski et al., 2013), and recent annotation efforts with other kinds of data (Beigman Klebanov and Flor, 2013; Jang et al., 2014). Some of these data are publicly available (Steen et al., 2010), allowing for benchmarking and for measuring incremental improvements, which is the approach taken in this paper. We start with a baseline set of features and training regime from Beigman Klebanov et al. (2014), and investigate the impact of re-weighting of training examples and of a suite of features related to concreteness of the target concept, as well as to the difference in concreteness within certain types of dependency relations. The usage of concreteness features was previously discussed in the literat"
W15-1402,W14-2302,1,0.879611,"such as the VU Amsterdam corpus (Steen et al., 2010), datasets built as part of a U.S. government-funded initiative to advance the state-of-art in metaphor identification and interpretation (Mohler et al., 2013; Strzalkowski et al., 2013), and recent annotation efforts with other kinds of data (Beigman Klebanov and Flor, 2013; Jang et al., 2014). Some of these data are publicly available (Steen et al., 2010), allowing for benchmarking and for measuring incremental improvements, which is the approach taken in this paper. We start with a baseline set of features and training regime from Beigman Klebanov et al. (2014), and investigate the impact of re-weighting of training examples and of a suite of features related to concreteness of the target concept, as well as to the difference in concreteness within certain types of dependency relations. The usage of concreteness features was previously discussed in the literature; to our knowledge, these features have not yet been evaluated for their impact in a comprehensive system for word-level metaphor detection, apart from the concreteness features as used in Beigman Klebanov et al. (2014), which we use as a baseline. 2 Data 2.1 VU Amsterdam Data We use the VU"
W15-1402,W09-2002,0,0.0303158,"hor has grown dramatically over the last few years, and there exists a plurality of approaches to the task. Shutova and Sun (2013) and Shutova et al. (2013) explored unsupervised clustering-based approaches. Features used in supervised learning approaches include selectional preferences violation, outlier detection, semantic analysis using topical signatures and ontologies, as well as n-gram features, among others (Tsvetkov et al., 2014; Schulder and Hovy, 2014; Beigman Klebanov et al., 2014; Mohler et al., 2013; Dunn, 2013; Tsvetkov et al., 2013; Hovy et al., 2013; Strzalkowski et al., 2013; Bethard et al., 2009; Pasanek and Sculley, 2008). A number of previous studies used features capturing concreteness of concepts and difference in concreteness between concepts standing in AdjN and VN dependency relations. The approach proposed by Turney et al. (2011) derives concreteness information using a small seed set of concrete and abstract terms and a corpus-based method for inferring the values for the remaining words. This infor18 mation was used to build a feature for detection of metaphorical AdjN phrases; the methodology was extended in Assaf et al. (2013) and again in Neuman et al. (2013) to provide"
W15-1402,de-marneffe-etal-2006-generating,0,0.0071153,"Missing"
W15-1402,W13-0901,0,0.0724513,"ctual noun is low. 10 Related Work The field of automated identification of metaphor has grown dramatically over the last few years, and there exists a plurality of approaches to the task. Shutova and Sun (2013) and Shutova et al. (2013) explored unsupervised clustering-based approaches. Features used in supervised learning approaches include selectional preferences violation, outlier detection, semantic analysis using topical signatures and ontologies, as well as n-gram features, among others (Tsvetkov et al., 2014; Schulder and Hovy, 2014; Beigman Klebanov et al., 2014; Mohler et al., 2013; Dunn, 2013; Tsvetkov et al., 2013; Hovy et al., 2013; Strzalkowski et al., 2013; Bethard et al., 2009; Pasanek and Sculley, 2008). A number of previous studies used features capturing concreteness of concepts and difference in concreteness between concepts standing in AdjN and VN dependency relations. The approach proposed by Turney et al. (2011) derives concreteness information using a small seed set of concrete and abstract terms and a corpus-based method for inferring the values for the remaining words. This infor18 mation was used to build a feature for detection of metaphorical AdjN phrases; the me"
W15-1402,W14-2304,0,0.202829,"roposed improvements are effective overall, as shown in section 8 (Results), it is clear that the main driver of the improvement is the reweighting of examples, while the contribution of the other changes is very small (observe the small difference between the second column in Table 7 and the OptiWeight column in Table 3). The small improvement is perhaps not surprising, since the baseline model itself already contains a version of the concreteness features. Given the relevant literature that has put forward concreteness and difference in concreteness as important predictors of metaphoricity (Dunn, 2014; Tsvetkov et al., 2014; Gandy et al., 2013; Assaf et al., 2013; Turney et al., 2011), it is instructive to evaluate the overall contribution of the concreteness features over the UPT baseline (no concreteness features), across the different weighting regimes. Table 9 provides this information. The improvement afforded by the concreteness and Data A-B B-A Set A Set B Av. Acad. Conv. Fiction News Av. UPT+ CBins unweighted (Baseline) P R F .713 .351 .470 .567 .491 .527 .701 .478 .566 .760 .592 .665 .685 .478 .557 .631 .351 .419 .503 .241 .317 .551 .291 .378 .640 .464 .536 .581 .337 .413 UPT+ CUp"
W15-1402,P14-2118,0,0.0577917,"ight have been influenced by the metaphorical use. For example, if a person considered a concept like “dark thoughts” when assigning a concreteness value to dark, the concept is quite abstract, so perhaps the word dark is given a relatively abstract rating. This is, of course, circular, because the perceived abstractness of “dark thoughts” came about precisely because a concrete term dark is accommodated, metaphorically, into an abstract domain of thinking. Another possibility is that it is not concreteness but some other property of adjectives that is relevant for metaphoricity. According to Hill and Korhonen (2014), the property of interest for adjectives is subjectivity, rather than concreteness. A feature capturing subjectivity of an adjective is a possible avenue for future work. In addition, they provide evidence that a potentially better way to quantify the concreteness of an adjective is to use mean concreteness of the nouns it modifies – as if concreteness for adjectives were a reflected property, based on its companion nouns. A large discrepancy between thusly calculated concreteness and the concreteness of the actual noun corresponds to non-literal meanings, especially for cases where the predi"
W15-1402,W13-0907,0,0.157059,"The field of automated identification of metaphor has grown dramatically over the last few years, and there exists a plurality of approaches to the task. Shutova and Sun (2013) and Shutova et al. (2013) explored unsupervised clustering-based approaches. Features used in supervised learning approaches include selectional preferences violation, outlier detection, semantic analysis using topical signatures and ontologies, as well as n-gram features, among others (Tsvetkov et al., 2014; Schulder and Hovy, 2014; Beigman Klebanov et al., 2014; Mohler et al., 2013; Dunn, 2013; Tsvetkov et al., 2013; Hovy et al., 2013; Strzalkowski et al., 2013; Bethard et al., 2009; Pasanek and Sculley, 2008). A number of previous studies used features capturing concreteness of concepts and difference in concreteness between concepts standing in AdjN and VN dependency relations. The approach proposed by Turney et al. (2011) derives concreteness information using a small seed set of concrete and abstract terms and a corpus-based method for inferring the values for the remaining words. This infor18 mation was used to build a feature for detection of metaphorical AdjN phrases; the methodology was extended in Assaf et al. (20"
W15-1402,W14-2301,0,0.0159793,"achine learning techniques for metaphor identification has increased manyfold in the recent years (see section 10, Related Work, for a review and references), partially due to the availability of largescale annotated resources for training and evaluating the algorithms, such as the VU Amsterdam corpus (Steen et al., 2010), datasets built as part of a U.S. government-funded initiative to advance the state-of-art in metaphor identification and interpretation (Mohler et al., 2013; Strzalkowski et al., 2013), and recent annotation efforts with other kinds of data (Beigman Klebanov and Flor, 2013; Jang et al., 2014). Some of these data are publicly available (Steen et al., 2010), allowing for benchmarking and for measuring incremental improvements, which is the approach taken in this paper. We start with a baseline set of features and training regime from Beigman Klebanov et al. (2014), and investigate the impact of re-weighting of training examples and of a suite of features related to concreteness of the target concept, as well as to the difference in concreteness within certain types of dependency relations. The usage of concreteness features was previously discussed in the literature; to our knowledg"
W15-1402,W13-0904,0,0.141552,"improving on previous work on the task of supervised word-level detection of linguistic metaphor in running text. The use of supervised machine learning techniques for metaphor identification has increased manyfold in the recent years (see section 10, Related Work, for a review and references), partially due to the availability of largescale annotated resources for training and evaluating the algorithms, such as the VU Amsterdam corpus (Steen et al., 2010), datasets built as part of a U.S. government-funded initiative to advance the state-of-art in metaphor identification and interpretation (Mohler et al., 2013; Strzalkowski et al., 2013), and recent annotation efforts with other kinds of data (Beigman Klebanov and Flor, 2013; Jang et al., 2014). Some of these data are publicly available (Steen et al., 2010), allowing for benchmarking and for measuring incremental improvements, which is the approach taken in this paper. We start with a baseline set of features and training regime from Beigman Klebanov et al. (2014), and investigate the impact of re-weighting of training examples and of a suite of features related to concreteness of the target concept, as well as to the difference in concreteness wit"
W15-1402,P14-1045,0,0.214841,"Mean concreteness values from Brysbaert et al. (2013) set of concreteness norms, represented using 0.25-wide bins that span the 1-5 range of possible values; • log PP(w|t) (w) values for each of 100 topics generated by Latent Dirichlet Allocation (Blei et al., 2003) from the NYT corpus (Sandhaus, 2008). 5 Experiment 1: Re-weighting of Examples Given that the category distribution is generally heavily skewed towards the non-metaphor category (see Table 1), we experimented with cost-sensitive machine learning techniques to try to correct for the imbalanced class distribution (Yang et al., 2014; Muller et al., 2014). The first technique uses AutoWeight (as implemented in the auto flag in scikitlearn toolkit), where we assign weights that are inversely proportional to the class frequencies.2 Table 2 shows the results. The effect of auto-weighting on the VUA data is quite dramatic: A 14-point drop in precision is offset by a 32-point increase in recall, on average, along with a 10-point average increase in F1 score. The precision-recall balance for VUA data changed from P=0.58, R=0.34 to P=0.44, R=0.66, nearly doubling 2 The re-weighting of examples was only applied to training data; the test data is unwei"
W15-1402,W14-2303,0,0.213307,"edicted concreteness of the adjective is high while the concreteness of the actual noun is low. 10 Related Work The field of automated identification of metaphor has grown dramatically over the last few years, and there exists a plurality of approaches to the task. Shutova and Sun (2013) and Shutova et al. (2013) explored unsupervised clustering-based approaches. Features used in supervised learning approaches include selectional preferences violation, outlier detection, semantic analysis using topical signatures and ontologies, as well as n-gram features, among others (Tsvetkov et al., 2014; Schulder and Hovy, 2014; Beigman Klebanov et al., 2014; Mohler et al., 2013; Dunn, 2013; Tsvetkov et al., 2013; Hovy et al., 2013; Strzalkowski et al., 2013; Bethard et al., 2009; Pasanek and Sculley, 2008). A number of previous studies used features capturing concreteness of concepts and difference in concreteness between concepts standing in AdjN and VN dependency relations. The approach proposed by Turney et al. (2011) derives concreteness information using a small seed set of concrete and abstract terms and a corpus-based method for inferring the values for the remaining words. This infor18 mation was used to bu"
W15-1402,N13-1118,0,0.0316172,"ss of an adjective is to use mean concreteness of the nouns it modifies – as if concreteness for adjectives were a reflected property, based on its companion nouns. A large discrepancy between thusly calculated concreteness and the concreteness of the actual noun corresponds to non-literal meanings, especially for cases where the predicted concreteness of the adjective is high while the concreteness of the actual noun is low. 10 Related Work The field of automated identification of metaphor has grown dramatically over the last few years, and there exists a plurality of approaches to the task. Shutova and Sun (2013) and Shutova et al. (2013) explored unsupervised clustering-based approaches. Features used in supervised learning approaches include selectional preferences violation, outlier detection, semantic analysis using topical signatures and ontologies, as well as n-gram features, among others (Tsvetkov et al., 2014; Schulder and Hovy, 2014; Beigman Klebanov et al., 2014; Mohler et al., 2013; Dunn, 2013; Tsvetkov et al., 2013; Hovy et al., 2013; Strzalkowski et al., 2013; Bethard et al., 2009; Pasanek and Sculley, 2008). A number of previous studies used features capturing concreteness of concepts an"
W15-1402,J13-2003,0,0.0135514,"e mean concreteness of the nouns it modifies – as if concreteness for adjectives were a reflected property, based on its companion nouns. A large discrepancy between thusly calculated concreteness and the concreteness of the actual noun corresponds to non-literal meanings, especially for cases where the predicted concreteness of the adjective is high while the concreteness of the actual noun is low. 10 Related Work The field of automated identification of metaphor has grown dramatically over the last few years, and there exists a plurality of approaches to the task. Shutova and Sun (2013) and Shutova et al. (2013) explored unsupervised clustering-based approaches. Features used in supervised learning approaches include selectional preferences violation, outlier detection, semantic analysis using topical signatures and ontologies, as well as n-gram features, among others (Tsvetkov et al., 2014; Schulder and Hovy, 2014; Beigman Klebanov et al., 2014; Mohler et al., 2013; Dunn, 2013; Tsvetkov et al., 2013; Hovy et al., 2013; Strzalkowski et al., 2013; Bethard et al., 2009; Pasanek and Sculley, 2008). A number of previous studies used features capturing concreteness of concepts and difference in concretene"
W15-1402,W13-0909,0,0.390278,"s work on the task of supervised word-level detection of linguistic metaphor in running text. The use of supervised machine learning techniques for metaphor identification has increased manyfold in the recent years (see section 10, Related Work, for a review and references), partially due to the availability of largescale annotated resources for training and evaluating the algorithms, such as the VU Amsterdam corpus (Steen et al., 2010), datasets built as part of a U.S. government-funded initiative to advance the state-of-art in metaphor identification and interpretation (Mohler et al., 2013; Strzalkowski et al., 2013), and recent annotation efforts with other kinds of data (Beigman Klebanov and Flor, 2013; Jang et al., 2014). Some of these data are publicly available (Steen et al., 2010), allowing for benchmarking and for measuring incremental improvements, which is the approach taken in this paper. We start with a baseline set of features and training regime from Beigman Klebanov et al. (2014), and investigate the impact of re-weighting of training examples and of a suite of features related to concreteness of the target concept, as well as to the difference in concreteness within certain types of depende"
W15-1402,N03-1033,0,0.0930307,"dentification of metaphors. We investigate the effect of reweighting of examples, as well as the effectiveness of features related to the notion of concreteness. Experimental Set-Up In this study, each content-word token in a text is an instance that is classified as either a metaphor or not a metaphor. We use the logistic regression classifier as implemented in the SKLL package (Blanchard et al., 2013), which is based on scikitlearn (Pedregosa et al., 2011), with F1 optimization (“metaphor” class). Performance will be evaluated 12 • Part-of-speech tags generated by Stanford POS tagger 3.3.0 (Toutanova et al., 2003); • Mean concreteness values from Brysbaert et al. (2013) set of concreteness norms, represented using 0.25-wide bins that span the 1-5 range of possible values; • log PP(w|t) (w) values for each of 100 topics generated by Latent Dirichlet Allocation (Blei et al., 2003) from the NYT corpus (Sandhaus, 2008). 5 Experiment 1: Re-weighting of Examples Given that the category distribution is generally heavily skewed towards the non-metaphor category (see Table 1), we experimented with cost-sensitive machine learning techniques to try to correct for the imbalanced class distribution (Yang et al., 20"
W15-1402,W13-0906,0,0.0795702,"s low. 10 Related Work The field of automated identification of metaphor has grown dramatically over the last few years, and there exists a plurality of approaches to the task. Shutova and Sun (2013) and Shutova et al. (2013) explored unsupervised clustering-based approaches. Features used in supervised learning approaches include selectional preferences violation, outlier detection, semantic analysis using topical signatures and ontologies, as well as n-gram features, among others (Tsvetkov et al., 2014; Schulder and Hovy, 2014; Beigman Klebanov et al., 2014; Mohler et al., 2013; Dunn, 2013; Tsvetkov et al., 2013; Hovy et al., 2013; Strzalkowski et al., 2013; Bethard et al., 2009; Pasanek and Sculley, 2008). A number of previous studies used features capturing concreteness of concepts and difference in concreteness between concepts standing in AdjN and VN dependency relations. The approach proposed by Turney et al. (2011) derives concreteness information using a small seed set of concrete and abstract terms and a corpus-based method for inferring the values for the remaining words. This infor18 mation was used to build a feature for detection of metaphorical AdjN phrases; the methodology was extended"
W15-1402,P14-1024,0,0.596821,"ovements are effective overall, as shown in section 8 (Results), it is clear that the main driver of the improvement is the reweighting of examples, while the contribution of the other changes is very small (observe the small difference between the second column in Table 7 and the OptiWeight column in Table 3). The small improvement is perhaps not surprising, since the baseline model itself already contains a version of the concreteness features. Given the relevant literature that has put forward concreteness and difference in concreteness as important predictors of metaphoricity (Dunn, 2014; Tsvetkov et al., 2014; Gandy et al., 2013; Assaf et al., 2013; Turney et al., 2011), it is instructive to evaluate the overall contribution of the concreteness features over the UPT baseline (no concreteness features), across the different weighting regimes. Table 9 provides this information. The improvement afforded by the concreteness and Data A-B B-A Set A Set B Av. Acad. Conv. Fiction News Av. UPT+ CBins unweighted (Baseline) P R F .713 .351 .470 .567 .491 .527 .701 .478 .566 .760 .592 .665 .685 .478 .557 .631 .351 .419 .503 .241 .317 .551 .291 .378 .640 .464 .536 .581 .337 .413 UPT+ CUpDown+ DCUpDown opti-wei"
W15-1402,D11-1063,0,0.660337,"present results of experiments trying to incorporate contextual information about the difference in concreteness between the adjective and its head noun (AdjN) and between the verb and its direct object (VN). The intuition behind this approach is that a metaphor is often used to describe an abstract concept in more familiar, physical terms. A concrete adjective modifying an abstract noun is likely to be used metaphorically (as in soft revolution or dark thought); similarly, a concrete verb with an abstract direct object is likely to be a metaphor (as in pour consolation or drive innovation). Turney et al. (2011) introduced a method for acquiring estimates of concreteness of words automatically, and measuring difference in concreteness in AdjN and VN constructions. They reported improved metaphor classification accuracies on constructed sets of AdjN and VN pairs. We implemented a difference-in-concreteness feature using the values from Brysbaert et al. (2013) database. We parsed texts using Stanford Dependency Parser (de Marneffe et al., 2006), and identified all instances of amod, dobj, and rcmod relations that connect an adjective to a noun (amod), a verb to its direct object (dobj), and a verb in a"
W16-0507,P14-2041,1,0.826611,"regation. Louis and Higgins (2010) and Higgins et al. (2006) address the task of detecting off-topic essays without on-topic training materials. Persing and Ng (2014) reported a study where essays were scored on an analytic rubric of adherence to the prompt; while this is a promising way to evaluate text-topicality models intrinsically, the reliability of the annotations was low (r=0.234). Content scoring was also studied for essays written in response to an extensive reading or listening prompt – quality of content is then related to integrating information from the source materials (Beigman Klebanov et al., 2014; Kakkonen et al., 2005; Lemaire and Dessus, 2001). A related direction of research implicitly treats topicality as a part of a more generalized notion of “good content,” namely, words that are used by good writers. The approach to estimating the quality of content is to compare the content of the current text to sets of training texts that represent various score points (Attali and Burstein, 2006; Kakkonen et al., 2005; Xie et al., 2012). In this approach, there is no differentiation between content that is topical and other words that might be used for other reasons, such as discourse marker"
W16-0507,N04-1024,0,0.0604274,"h. We develop text topicality indices and evaluate them in the context of automated scoring of essays. Specifically, we investigate the relationship between the extent to which the essay engages the topic provided in the essay question (prompt) and the quality of the essay as quantified by a human-provided holistic score. In the existing literature, topicality has been addressed as a control flag to identify off-topic essays or spoken responses (Yoon and Xie, 2014; Louis and Higgins, 2010; Higgins et al., 2006) or as an element in the overall coherence of the essay (Somasundaran et al., 2014; Higgins et al., 2004; Foltz et al., 1998). Persing and Ng (2014) annotated essays for promptadherence, and found that achieving inter-rater reliability was very challenging, reporting Pearson r = 0.243 between two raters. We address the relationship between a continuous topicality score and the holistic quality of an essay. Generally, one can think of the topicality of a given word w on a given topic T as the extent to which w occurs more often in texts addressing T than in otherwise comparable texts addressing a different topic. We consider three models of word topicality from the literature: the significancetes"
W16-0507,W05-0206,0,0.0343023,"gins (2010) and Higgins et al. (2006) address the task of detecting off-topic essays without on-topic training materials. Persing and Ng (2014) reported a study where essays were scored on an analytic rubric of adherence to the prompt; while this is a promising way to evaluate text-topicality models intrinsically, the reliability of the annotations was low (r=0.234). Content scoring was also studied for essays written in response to an extensive reading or listening prompt – quality of content is then related to integrating information from the source materials (Beigman Klebanov et al., 2014; Kakkonen et al., 2005; Lemaire and Dessus, 2001). A related direction of research implicitly treats topicality as a part of a more generalized notion of “good content,” namely, words that are used by good writers. The approach to estimating the quality of content is to compare the content of the current text to sets of training texts that represent various score points (Attali and Burstein, 2006; Kakkonen et al., 2005; Xie et al., 2012). In this approach, there is no differentiation between content that is topical and other words that might be used for other reasons, such as discourse markers used for organization"
W16-0507,C00-1072,0,0.852767,"Ng (2014) annotated essays for promptadherence, and found that achieving inter-rater reliability was very challenging, reporting Pearson r = 0.243 between two raters. We address the relationship between a continuous topicality score and the holistic quality of an essay. Generally, one can think of the topicality of a given word w on a given topic T as the extent to which w occurs more often in texts addressing T than in otherwise comparable texts addressing a different topic. We consider three models of word topicality from the literature: the significancetest approach as in topic signatures (Lin and Hovy, 2000), the score-product approach as described in the essay scoring literature (Higgins et al., 2006), and a simple cutoff-based approach relying on difference in probabilities. Given a definition of word topicality, the question arises how to quantify the topicality of the whole text. Specifically, is topicality a property of the vocabulary of a text (of word types) or a property of both the vocabulary and the unfolding discourse (of word tokens)? Thus, do the sentences “I hate restaurants, abhor restaurants, loath restaurants, and love restaurants” and “I hate restaurants, abhor waiters, loath me"
W16-0507,W10-1013,0,0.506396,"of a text is far from settled, and little is known about the interaction of topicality and other properties of text, such as length. We develop text topicality indices and evaluate them in the context of automated scoring of essays. Specifically, we investigate the relationship between the extent to which the essay engages the topic provided in the essay question (prompt) and the quality of the essay as quantified by a human-provided holistic score. In the existing literature, topicality has been addressed as a control flag to identify off-topic essays or spoken responses (Yoon and Xie, 2014; Louis and Higgins, 2010; Higgins et al., 2006) or as an element in the overall coherence of the essay (Somasundaran et al., 2014; Higgins et al., 2004; Foltz et al., 1998). Persing and Ng (2014) annotated essays for promptadherence, and found that achieving inter-rater reliability was very challenging, reporting Pearson r = 0.243 between two raters. We address the relationship between a continuous topicality score and the holistic quality of an essay. Generally, one can think of the topicality of a given word w on a given topic T as the extent to which w occurs more often in texts addressing T than in otherwise comp"
W16-0507,N12-1003,0,0.0313087,"on of research implicitly treats topicality as a part of a more generalized notion of “good content,” namely, words that are used by good writers. The approach to estimating the quality of content is to compare the content of the current text to sets of training texts that represent various score points (Attali and Burstein, 2006; Kakkonen et al., 2005; Xie et al., 2012). In this approach, there is no differentiation between content that is topical and other words that might be used for other reasons, such as discourse markers used for organizational purposes or spurious, shell-like elements (Madnani et al., 2012); an essay that is dissimilar from high-scoring essays on all or some of these accounts is likely to be viewed as having “bad content.” An essay rife with misspellings would like70 wise be seen as having “bad content”, because the model high-scoring essays are generally not prone to misspellings. In contrast, our topicality lists are estimated based on a random sample of essays, including low scoring essays; this allows introduction of common misspellings of words frequently used to address the given topic into the topical lists. For example, one of the topical lists includes more than a dozen"
W16-0507,D11-1024,0,0.0309708,"ellings would like70 wise be seen as having “bad content”, because the model high-scoring essays are generally not prone to misspellings. In contrast, our topicality lists are estimated based on a random sample of essays, including low scoring essays; this allows introduction of common misspellings of words frequently used to address the given topic into the topical lists. For example, one of the topical lists includes more than a dozen misspellings of the word contemporaries. There is a large body of work using topic models to capture different topics typically addressed in a corpus of text (Mimno et al., 2011; Newman et al., 2011; Gruber et al., 2007; Blei et al., 2003). In this general framework, each text can address a few different topics and the number and identity of topics for the corpus is typically unknown. In our setting, we assume that each essay is on a single topic, and that topic is known in advance.10 However, many of these topics are very open-ended, so they might exhibit non-trivial sub-topical structures. For example, a topic about cultural role models might be dealt with by discussing politicians, musicians, sportsmen – each of these could yield a specific sub-topic. In fact, Per"
W16-0507,P14-1144,0,0.218098,"valuate them in the context of automated scoring of essays. Specifically, we investigate the relationship between the extent to which the essay engages the topic provided in the essay question (prompt) and the quality of the essay as quantified by a human-provided holistic score. In the existing literature, topicality has been addressed as a control flag to identify off-topic essays or spoken responses (Yoon and Xie, 2014; Louis and Higgins, 2010; Higgins et al., 2006) or as an element in the overall coherence of the essay (Somasundaran et al., 2014; Higgins et al., 2004; Foltz et al., 1998). Persing and Ng (2014) annotated essays for promptadherence, and found that achieving inter-rater reliability was very challenging, reporting Pearson r = 0.243 between two raters. We address the relationship between a continuous topicality score and the holistic quality of an essay. Generally, one can think of the topicality of a given word w on a given topic T as the extent to which w occurs more often in texts addressing T than in otherwise comparable texts addressing a different topic. We consider three models of word topicality from the literature: the significancetest approach as in topic signatures (Lin and H"
W16-0507,C14-1090,0,0.0259708,"ties of text, such as length. We develop text topicality indices and evaluate them in the context of automated scoring of essays. Specifically, we investigate the relationship between the extent to which the essay engages the topic provided in the essay question (prompt) and the quality of the essay as quantified by a human-provided holistic score. In the existing literature, topicality has been addressed as a control flag to identify off-topic essays or spoken responses (Yoon and Xie, 2014; Louis and Higgins, 2010; Higgins et al., 2006) or as an element in the overall coherence of the essay (Somasundaran et al., 2014; Higgins et al., 2004; Foltz et al., 1998). Persing and Ng (2014) annotated essays for promptadherence, and found that achieving inter-rater reliability was very challenging, reporting Pearson r = 0.243 between two raters. We address the relationship between a continuous topicality score and the holistic quality of an essay. Generally, one can think of the topicality of a given word w on a given topic T as the extent to which w occurs more often in texts addressing T than in otherwise comparable texts addressing a different topic. We consider three models of word topicality from the literatur"
W16-0507,C12-1158,0,0.0191503,"ions (82,500 essays, 76 prompts for each dataset). Their sheer sizes and the variety of topics (prompts) allow for a thorough evaluation of the proposed measures. However, the proprietary nature of these data does not allow for easy replication of the results, or benchmarking; we therefore use a third, publicly available dataset containing 12,100 essays written for the TOEFL test by non-native speakers of English seeking college entrance in the United States, as well as for other purposes. The dataset was originally built for the task of native language identification (Blanchard et al., 2013; Tetreault et al., 2012); however, the distribution provides coarse-grained holistic scores as well says that in both sentences, half the content words are topical, whereas the type-based model says that only 1 out of 5 different content words is topical in the first sentence, and 4 out of 8 in the second. 64 Part. Dev Train Test Set 1 76 × 500 51 × 500 76 × 250 Set 2 76 × 500 51 × 500 76 × 250 TOEFL 8 × 500 8 × 760 (Av) 8 × 253 (Av) Table 1: Sizes of the data partitions for each dataset. In the N × M notation, N = # prompts, M = # essays per prompt. In TOEFL train and test sets, we show average numbers of essays per"
W16-0507,N12-1011,0,0.0607406,"Missing"
W16-0507,W14-1814,0,0.0193086,"asure this property of a text is far from settled, and little is known about the interaction of topicality and other properties of text, such as length. We develop text topicality indices and evaluate them in the context of automated scoring of essays. Specifically, we investigate the relationship between the extent to which the essay engages the topic provided in the essay question (prompt) and the quality of the essay as quantified by a human-provided holistic score. In the existing literature, topicality has been addressed as a control flag to identify off-topic essays or spoken responses (Yoon and Xie, 2014; Louis and Higgins, 2010; Higgins et al., 2006) or as an element in the overall coherence of the essay (Somasundaran et al., 2014; Higgins et al., 2004; Foltz et al., 1998). Persing and Ng (2014) annotated essays for promptadherence, and found that achieving inter-rater reliability was very challenging, reporting Pearson r = 0.243 between two raters. We address the relationship between a continuous topicality score and the holistic quality of an essay. Generally, one can think of the topicality of a given word w on a given topic T as the extent to which w occurs more often in texts addressing"
W16-0522,P10-2047,1,0.844893,"that words of cognitive involvement and insight are used more in UV writing. In recent years, NLP techniques have increasingly been applied to studying a variety of social and psychological phenomena. In particular, NLP research 203 has been used to detect language that reflects certain traits of the authors’ disposition or thinking, such as detection of deception, sentiment and affect, flirtation, ideological orientation, depression, and suicidal tendencies (Mihalcea and Strapparava, 2009; Abouelenien et al., 2014; Hu and Liu, 2004; Ranganath et al., 2009; Neviarouskaya et al., 2010; Beigman Klebanov et al., 2010; Greene and Resnik, 2009; Pedersen, 2015; Resnik et al., 2013; Mulholland and Quinn, 2013). Such studies have a tremendous potential to help measure, understand, and ultimately enhance personal and societal well being. We believe that the line of inquiry initiated by the current study that is focused on motivation in college likewise promises important potential benefits. 6 Conclusion & Future Work We presented the first experiments, to our knowledge, on using NLP to measure the extent to which a writing sample contains expression of the writer’s utility value from studying a STEM subject. St"
W16-0522,W16-0507,1,0.837157,"t of expressions that qualify or enhance a claim (based on Aull and Lancaster (2014)). The features use log token count for each category. • A RG D EV: Words that could serve to develop an argument, such as plausibly, just as, not enough, specifically, for instance, unfortunately, doubtless, for sure, supposing, what if. 3 This is similar to Lin and Hovy (2000) topic signatures approach (or, rather, genre-topic signatures here), without the transformation that supports significance thresholds. This simpler approach was found to be effective in our work on topicality for essay scoring (Beigman Klebanov et al., 2016). • H EDGE B OOST: Hedging and boosting expressions, such as: perhaps, probably, to some extent, not entirely true, less likely, roughly (hedges); naturally, can never, inevitably, only way, vital that (boosters). In addition, in order to connect the biology content to the writer’s own life, the writer might need to provide a personal mini-narrative – background with details about the events in his or her life that motivate the particular UV statement. Since heavier reliance on verbs is a hallmark of narrativity, we define the following features (using log frequency per 1,000 words): • C OM V"
W16-0522,W98-0303,1,0.377496,"ypical genre-topic vocabulary is done on training and development data. • GENRE VOC: Log type proportion of genretopic words. 3.4 Argumentative and Narrative Elements While summaries of technical biology material are likely to be written in an expository, informational style, one might expect the UV elements to be more argumentative, as the writer needs to put forward a claim regarding the relationship between their own or other people’s lives and biology knowledge, along with necessary qualifications. We therefore defined lists of expressions that could serve to develop an argument (based on Burstein et al. (1998)) and a list of expressions that qualify or enhance a claim (based on Aull and Lancaster (2014)). The features use log token count for each category. • A RG D EV: Words that could serve to develop an argument, such as plausibly, just as, not enough, specifically, for instance, unfortunately, doubtless, for sure, supposing, what if. 3 This is similar to Lin and Hovy (2000) topic signatures approach (or, rather, genre-topic signatures here), without the transformation that supports significance thresholds. This simpler approach was found to be effective in our work on topicality for essay scorin"
W16-0522,N09-1057,0,0.0119824,"involvement and insight are used more in UV writing. In recent years, NLP techniques have increasingly been applied to studying a variety of social and psychological phenomena. In particular, NLP research 203 has been used to detect language that reflects certain traits of the authors’ disposition or thinking, such as detection of deception, sentiment and affect, flirtation, ideological orientation, depression, and suicidal tendencies (Mihalcea and Strapparava, 2009; Abouelenien et al., 2014; Hu and Liu, 2004; Ranganath et al., 2009; Neviarouskaya et al., 2010; Beigman Klebanov et al., 2010; Greene and Resnik, 2009; Pedersen, 2015; Resnik et al., 2013; Mulholland and Quinn, 2013). Such studies have a tremendous potential to help measure, understand, and ultimately enhance personal and societal well being. We believe that the line of inquiry initiated by the current study that is focused on motivation in college likewise promises important potential benefits. 6 Conclusion & Future Work We presented the first experiments, to our knowledge, on using NLP to measure the extent to which a writing sample contains expression of the writer’s utility value from studying a STEM subject. Studies in social psycholog"
W16-0522,C00-1072,0,0.0373388,"m regarding the relationship between their own or other people’s lives and biology knowledge, along with necessary qualifications. We therefore defined lists of expressions that could serve to develop an argument (based on Burstein et al. (1998)) and a list of expressions that qualify or enhance a claim (based on Aull and Lancaster (2014)). The features use log token count for each category. • A RG D EV: Words that could serve to develop an argument, such as plausibly, just as, not enough, specifically, for instance, unfortunately, doubtless, for sure, supposing, what if. 3 This is similar to Lin and Hovy (2000) topic signatures approach (or, rather, genre-topic signatures here), without the transformation that supports significance thresholds. This simpler approach was found to be effective in our work on topicality for essay scoring (Beigman Klebanov et al., 2016). • H EDGE B OOST: Hedging and boosting expressions, such as: perhaps, probably, to some extent, not entirely true, less likely, roughly (hedges); naturally, can never, inevitably, only way, vital that (boosters). In addition, in order to connect the biology content to the writer’s own life, the writer might need to provide a personal mini"
W16-0522,P09-2078,0,0.0389315,"other humans, as well as words describing social processes were used in significantly higher proportions in UV writing. They also found that words of cognitive involvement and insight are used more in UV writing. In recent years, NLP techniques have increasingly been applied to studying a variety of social and psychological phenomena. In particular, NLP research 203 has been used to detect language that reflects certain traits of the authors’ disposition or thinking, such as detection of deception, sentiment and affect, flirtation, ideological orientation, depression, and suicidal tendencies (Mihalcea and Strapparava, 2009; Abouelenien et al., 2014; Hu and Liu, 2004; Ranganath et al., 2009; Neviarouskaya et al., 2010; Beigman Klebanov et al., 2010; Greene and Resnik, 2009; Pedersen, 2015; Resnik et al., 2013; Mulholland and Quinn, 2013). Such studies have a tremendous potential to help measure, understand, and ultimately enhance personal and societal well being. We believe that the line of inquiry initiated by the current study that is focused on motivation in college likewise promises important potential benefits. 6 Conclusion & Future Work We presented the first experiments, to our knowledge, on using NLP to"
W16-0522,I13-1079,1,0.81122,"t years, NLP techniques have increasingly been applied to studying a variety of social and psychological phenomena. In particular, NLP research 203 has been used to detect language that reflects certain traits of the authors’ disposition or thinking, such as detection of deception, sentiment and affect, flirtation, ideological orientation, depression, and suicidal tendencies (Mihalcea and Strapparava, 2009; Abouelenien et al., 2014; Hu and Liu, 2004; Ranganath et al., 2009; Neviarouskaya et al., 2010; Beigman Klebanov et al., 2010; Greene and Resnik, 2009; Pedersen, 2015; Resnik et al., 2013; Mulholland and Quinn, 2013). Such studies have a tremendous potential to help measure, understand, and ultimately enhance personal and societal well being. We believe that the line of inquiry initiated by the current study that is focused on motivation in college likewise promises important potential benefits. 6 Conclusion & Future Work We presented the first experiments, to our knowledge, on using NLP to measure the extent to which a writing sample contains expression of the writer’s utility value from studying a STEM subject. Studies in social psychology have shown that a writing intervention where a STEM student is a"
W16-0522,C10-1091,0,0.0230557,"ions in UV writing. They also found that words of cognitive involvement and insight are used more in UV writing. In recent years, NLP techniques have increasingly been applied to studying a variety of social and psychological phenomena. In particular, NLP research 203 has been used to detect language that reflects certain traits of the authors’ disposition or thinking, such as detection of deception, sentiment and affect, flirtation, ideological orientation, depression, and suicidal tendencies (Mihalcea and Strapparava, 2009; Abouelenien et al., 2014; Hu and Liu, 2004; Ranganath et al., 2009; Neviarouskaya et al., 2010; Beigman Klebanov et al., 2010; Greene and Resnik, 2009; Pedersen, 2015; Resnik et al., 2013; Mulholland and Quinn, 2013). Such studies have a tremendous potential to help measure, understand, and ultimately enhance personal and societal well being. We believe that the line of inquiry initiated by the current study that is focused on motivation in college likewise promises important potential benefits. 6 Conclusion & Future Work We presented the first experiments, to our knowledge, on using NLP to measure the extent to which a writing sample contains expression of the writer’s utility value f"
W16-0522,W15-1206,0,0.0296269,"are used more in UV writing. In recent years, NLP techniques have increasingly been applied to studying a variety of social and psychological phenomena. In particular, NLP research 203 has been used to detect language that reflects certain traits of the authors’ disposition or thinking, such as detection of deception, sentiment and affect, flirtation, ideological orientation, depression, and suicidal tendencies (Mihalcea and Strapparava, 2009; Abouelenien et al., 2014; Hu and Liu, 2004; Ranganath et al., 2009; Neviarouskaya et al., 2010; Beigman Klebanov et al., 2010; Greene and Resnik, 2009; Pedersen, 2015; Resnik et al., 2013; Mulholland and Quinn, 2013). Such studies have a tremendous potential to help measure, understand, and ultimately enhance personal and societal well being. We believe that the line of inquiry initiated by the current study that is focused on motivation in college likewise promises important potential benefits. 6 Conclusion & Future Work We presented the first experiments, to our knowledge, on using NLP to measure the extent to which a writing sample contains expression of the writer’s utility value from studying a STEM subject. Studies in social psychology have shown tha"
W16-0522,D09-1035,0,0.0152225,"ificantly higher proportions in UV writing. They also found that words of cognitive involvement and insight are used more in UV writing. In recent years, NLP techniques have increasingly been applied to studying a variety of social and psychological phenomena. In particular, NLP research 203 has been used to detect language that reflects certain traits of the authors’ disposition or thinking, such as detection of deception, sentiment and affect, flirtation, ideological orientation, depression, and suicidal tendencies (Mihalcea and Strapparava, 2009; Abouelenien et al., 2014; Hu and Liu, 2004; Ranganath et al., 2009; Neviarouskaya et al., 2010; Beigman Klebanov et al., 2010; Greene and Resnik, 2009; Pedersen, 2015; Resnik et al., 2013; Mulholland and Quinn, 2013). Such studies have a tremendous potential to help measure, understand, and ultimately enhance personal and societal well being. We believe that the line of inquiry initiated by the current study that is focused on motivation in college likewise promises important potential benefits. 6 Conclusion & Future Work We presented the first experiments, to our knowledge, on using NLP to measure the extent to which a writing sample contains expression of"
W16-0522,D13-1133,0,0.0270868,"UV writing. In recent years, NLP techniques have increasingly been applied to studying a variety of social and psychological phenomena. In particular, NLP research 203 has been used to detect language that reflects certain traits of the authors’ disposition or thinking, such as detection of deception, sentiment and affect, flirtation, ideological orientation, depression, and suicidal tendencies (Mihalcea and Strapparava, 2009; Abouelenien et al., 2014; Hu and Liu, 2004; Ranganath et al., 2009; Neviarouskaya et al., 2010; Beigman Klebanov et al., 2010; Greene and Resnik, 2009; Pedersen, 2015; Resnik et al., 2013; Mulholland and Quinn, 2013). Such studies have a tremendous potential to help measure, understand, and ultimately enhance personal and societal well being. We believe that the line of inquiry initiated by the current study that is focused on motivation in college likewise promises important potential benefits. 6 Conclusion & Future Work We presented the first experiments, to our knowledge, on using NLP to measure the extent to which a writing sample contains expression of the writer’s utility value from studying a STEM subject. Studies in social psychology have shown that a writing intervent"
W16-2808,W14-2109,0,0.0192301,"n mining in various genres. Moens et al. (2007) identify argumentative sentences in newspapers, parliamentary records, court reports and online discussions. Mochales-Palau and Moens (2009) identify argumentation structures including claims and premises in court cases. Other approaches focus on online comments and recognize argument components (Habernal and Gurevych, 2015), justifications (Biran and Rambow, 2011) or different types of claims (Kwon et al., 2007). Work in the context of the IBM Debater project deals with identifying claims and evidence in Wikipedia articles (Rinott et al., 2015; Aharoni et al., 2014). Peldszus and Stede (2015) identify argumentation structures in microtexts (similar to essays). They rely on several base classifiers and minimum spanning trees to recognize argumentative tree structures. Stab and Gurevych (2016) extract argument structures from essays by recognizing argument components and jointly modeling their types and relations between them. Both approaches focus on the structure and neglect the content of arguments. Persing and Ng (2015) annotate argument strength, which is related to content, yet what it is that makes an argument strong has not been made explicit in th"
W16-2808,W14-2104,0,0.322564,"mentative writing, is receiving increased attention, along with a demand for argumentation-aware Automated Writing Evaluation (AWE) systems. However, current AWE systems typically do not consider argumentation (Lim and Kahng, 2012), and employ features that address grammar, mechanics, discourse structure, syntactic and lexical richness (Burstein et al., 2013). Developments in Computational Argumentation (CA) could bridge this gap. Recently, progress has been made towards a more detailed understanding of argumentation in essays (Song et al., 2014; Stab and Gurevych, 2014; Persing and Ng, 2015; Ong et al., 2014). An important distinction emerging from the relevant work is that between argumentative structure and argumentative content. Facility with the argumentation structure underlies the contrast between (1) and (2) below: In (1), claims are made without support; relationships between claims are not explicit; there is intervening irrelevant material. In (2), the argumentative structure is clear – there is a critical claim supported by a specific reason. Yet, 1 (2) “The mayor’s policy of switching to a new trash collector service is flawed because he failed to consider the ugly color of the trucks u"
W16-2808,D15-1110,0,0.0488105,"res. Moens et al. (2007) identify argumentative sentences in newspapers, parliamentary records, court reports and online discussions. Mochales-Palau and Moens (2009) identify argumentation structures including claims and premises in court cases. Other approaches focus on online comments and recognize argument components (Habernal and Gurevych, 2015), justifications (Biran and Rambow, 2011) or different types of claims (Kwon et al., 2007). Work in the context of the IBM Debater project deals with identifying claims and evidence in Wikipedia articles (Rinott et al., 2015; Aharoni et al., 2014). Peldszus and Stede (2015) identify argumentation structures in microtexts (similar to essays). They rely on several base classifiers and minimum spanning trees to recognize argumentative tree structures. Stab and Gurevych (2016) extract argument structures from essays by recognizing argument components and jointly modeling their types and relations between them. Both approaches focus on the structure and neglect the content of arguments. Persing and Ng (2015) annotate argument strength, which is related to content, yet what it is that makes an argument strong has not been made explicit in the rubric and the annotation"
W16-2808,P15-1053,0,0.378947,"ore specifically, argumentative writing, is receiving increased attention, along with a demand for argumentation-aware Automated Writing Evaluation (AWE) systems. However, current AWE systems typically do not consider argumentation (Lim and Kahng, 2012), and employ features that address grammar, mechanics, discourse structure, syntactic and lexical richness (Burstein et al., 2013). Developments in Computational Argumentation (CA) could bridge this gap. Recently, progress has been made towards a more detailed understanding of argumentation in essays (Song et al., 2014; Stab and Gurevych, 2014; Persing and Ng, 2015; Ong et al., 2014). An important distinction emerging from the relevant work is that between argumentative structure and argumentative content. Facility with the argumentation structure underlies the contrast between (1) and (2) below: In (1), claims are made without support; relationships between claims are not explicit; there is intervening irrelevant material. In (2), the argumentative structure is clear – there is a critical claim supported by a specific reason. Yet, 1 (2) “The mayor’s policy of switching to a new trash collector service is flawed because he failed to consider the ugly co"
W16-2808,D15-1050,0,0.0564987,"cuses on argumentation mining in various genres. Moens et al. (2007) identify argumentative sentences in newspapers, parliamentary records, court reports and online discussions. Mochales-Palau and Moens (2009) identify argumentation structures including claims and premises in court cases. Other approaches focus on online comments and recognize argument components (Habernal and Gurevych, 2015), justifications (Biran and Rambow, 2011) or different types of claims (Kwon et al., 2007). Work in the context of the IBM Debater project deals with identifying claims and evidence in Wikipedia articles (Rinott et al., 2015; Aharoni et al., 2014). Peldszus and Stede (2015) identify argumentation structures in microtexts (similar to essays). They rely on several base classifiers and minimum spanning trees to recognize argumentative tree structures. Stab and Gurevych (2016) extract argument structures from essays by recognizing argument components and jointly modeling their types and relations between them. Both approaches focus on the structure and neglect the content of arguments. Persing and Ng (2015) annotate argument strength, which is related to content, yet what it is that makes an argument strong has not b"
W16-2808,W14-2110,1,0.927507,"ndards for Education,1 argumentation, and, more specifically, argumentative writing, is receiving increased attention, along with a demand for argumentation-aware Automated Writing Evaluation (AWE) systems. However, current AWE systems typically do not consider argumentation (Lim and Kahng, 2012), and employ features that address grammar, mechanics, discourse structure, syntactic and lexical richness (Burstein et al., 2013). Developments in Computational Argumentation (CA) could bridge this gap. Recently, progress has been made towards a more detailed understanding of argumentation in essays (Song et al., 2014; Stab and Gurevych, 2014; Persing and Ng, 2015; Ong et al., 2014). An important distinction emerging from the relevant work is that between argumentative structure and argumentative content. Facility with the argumentation structure underlies the contrast between (1) and (2) below: In (1), claims are made without support; relationships between claims are not explicit; there is intervening irrelevant material. In (2), the argumentative structure is clear – there is a critical claim supported by a specific reason. Yet, 1 (2) “The mayor’s policy of switching to a new trash collector service is f"
W16-2808,D15-1255,1,0.837525,"and content are conceptually distinct, they might in reality go together. We therefore evaluate the ability of the structurebased system to deal with content-based annotations of argumentation. 2 Related Work 3 Existing work in CA focuses on argumentation mining in various genres. Moens et al. (2007) identify argumentative sentences in newspapers, parliamentary records, court reports and online discussions. Mochales-Palau and Moens (2009) identify argumentation structures including claims and premises in court cases. Other approaches focus on online comments and recognize argument components (Habernal and Gurevych, 2015), justifications (Biran and Rambow, 2011) or different types of claims (Kwon et al., 2007). Work in the context of the IBM Debater project deals with identifying claims and evidence in Wikipedia articles (Rinott et al., 2015; Aharoni et al., 2014). Peldszus and Stede (2015) identify argumentation structures in microtexts (similar to essays). They rely on several base classifiers and minimum spanning trees to recognize argumentative tree structures. Stab and Gurevych (2016) extract argument structures from essays by recognizing argument components and jointly modeling their types and relations"
W16-2808,C14-1142,1,0.829606,"n,1 argumentation, and, more specifically, argumentative writing, is receiving increased attention, along with a demand for argumentation-aware Automated Writing Evaluation (AWE) systems. However, current AWE systems typically do not consider argumentation (Lim and Kahng, 2012), and employ features that address grammar, mechanics, discourse structure, syntactic and lexical richness (Burstein et al., 2013). Developments in Computational Argumentation (CA) could bridge this gap. Recently, progress has been made towards a more detailed understanding of argumentation in essays (Song et al., 2014; Stab and Gurevych, 2014; Persing and Ng, 2015; Ong et al., 2014). An important distinction emerging from the relevant work is that between argumentative structure and argumentative content. Facility with the argumentation structure underlies the contrast between (1) and (2) below: In (1), claims are made without support; relationships between claims are not explicit; there is intervening irrelevant material. In (2), the argumentative structure is clear – there is a critical claim supported by a specific reason. Yet, 1 (2) “The mayor’s policy of switching to a new trash collector service is flawed because he failed t"
W16-2808,J17-3005,1,\N,Missing
W17-5003,E09-1027,0,0.0250012,"f two studies: In the first study we looked at variation in text complexity across passages selected from HP1. In the second study we investigate how text complexity estimates relate to WCPM of children reading selected passages from the book. Our findings are then discussed and implications for research on continuous tracking of fluency are drawn. 2 comprehension, including special formulas and models designed for special populations, such as young children (Spache, 1953), learners of English as a second language (Beinborn et al., 2014; Heilman et al., 2007), adults with mental disabilities (Feng et al., 2009), among others. While comprehension-based complexity estimation of relatively short reading passages has been the subject of extensive research for many decades, there is little research on estimating the complexity of long, book-level texts. In early work on readability, Fowler (1978) estimated readability of a novel using the mean of readability estimates of fifteen randomly selected 100-word passages from the novel. Milone (2012) generates book-level complexity estimates by combining complexity estimates for the text in the book with a measure based on the length of the book, following the"
W17-5003,N15-3020,0,0.0371726,"Missing"
W17-5003,N07-1058,0,0.0351768,"and oral reading fluency. We then present the results of two studies: In the first study we looked at variation in text complexity across passages selected from HP1. In the second study we investigate how text complexity estimates relate to WCPM of children reading selected passages from the book. Our findings are then discussed and implications for research on continuous tracking of fluency are drawn. 2 comprehension, including special formulas and models designed for special populations, such as young children (Spache, 1953), learners of English as a second language (Beinborn et al., 2014; Heilman et al., 2007), adults with mental disabilities (Feng et al., 2009), among others. While comprehension-based complexity estimation of relatively short reading passages has been the subject of extensive research for many decades, there is little research on estimating the complexity of long, book-level texts. In early work on readability, Fowler (1978) estimated readability of a novel using the mean of readability estimates of fifteen randomly selected 100-word passages from the novel. Milone (2012) generates book-level complexity estimates by combining complexity estimates for the text in the book with a me"
W17-5003,P05-1065,0,0.0627981,"sch-Kincaid measure, Spache measure, and average sentence length did not significantly correlate with performance. On the other hand, they found that percentage of high frequency words was significantly Text Complexity Estimation: While for Dale and Chall (1949) the notion of text readability involved “the extent to which they [readers] understand it [the text], read it at an optimal speed, and find it interesting”,3 most classical (Flesch, 1948; Gunning, 1952; Kincaid et al., 1975; McLaughlin, 1969) and modern (Sheehan et al., 2014; Flor and Beigman Klebanov, 2014; Vajjala and Meurers, 2012; Schwarm and Ostendorf, 2005) measures of text readability/complexity focus on reading 3 Quoted from DuBay (2004). 23 at each turn. For each chunk, after 250 words, we either extended or reduced the chunk to the end of a paragraph, thus ensuring that each passage had a natural break point. The whole book consists of 79,508 words spread across 17 chapters. We created 318 consecutive passages, with a mean length of 250.0 words (SD=16.9). The shortest passage contained 177 words and the longest passage contained 309 words. Half of the passages (II and III quartiles) fell within 242-259 words range. We used TextEvaluator,4 a"
W17-5003,W13-1506,0,0.0498369,"Missing"
W17-5003,W12-2019,0,0.0163453,"researchers found that Flesch-Kincaid measure, Spache measure, and average sentence length did not significantly correlate with performance. On the other hand, they found that percentage of high frequency words was significantly Text Complexity Estimation: While for Dale and Chall (1949) the notion of text readability involved “the extent to which they [readers] understand it [the text], read it at an optimal speed, and find it interesting”,3 most classical (Flesch, 1948; Gunning, 1952; Kincaid et al., 1975; McLaughlin, 1969) and modern (Sheehan et al., 2014; Flor and Beigman Klebanov, 2014; Vajjala and Meurers, 2012; Schwarm and Ostendorf, 2005) measures of text readability/complexity focus on reading 3 Quoted from DuBay (2004). 23 at each turn. For each chunk, after 250 words, we either extended or reduced the chunk to the end of a paragraph, thus ensuring that each passage had a natural break point. The whole book consists of 79,508 words spread across 17 chapters. We created 318 consecutive passages, with a mean length of 250.0 words (SD=16.9). The shortest passage contained 177 words and the longest passage contained 309 words. Half of the passages (II and III quartiles) fell within 242-259 words ran"
W17-5003,W09-2102,1,0.753431,"velopmental trajectory, by continuously tracking the child’s reading fluency throughout his reading turns. Oral reading fluency is not only an important indicator of reading skill in itself (Hudson et al., 2008; Fuchs et al., 2001), for students in early elementary grades it is also strongly correlated (r around 0.7) with reading comprehension (Roberts et al., 2005; Good et al., 2001). The standard measure of oral reading fluency is words correct per minute (henceforth, WCPM) (Wayman et al., 2007), combining aspects of speed and accuracy of oral reading.2 Several studies (Balogh et al., 2007; Zechner et al., 2009) showed that WCPM can be accurately computed automatically using an automated speech recognizer (ASR) and a string matching algorithm; this approach has already been incorporated into many commercial and research systems for automated oral fluency assessment such as VersaReader (Balogh et al., 2012) or Project LISTEN (Mostow, 2012) (see also Eskenazi (2009) for a review). Previous studies on reading fluency indicate that WCPM may vary across different texts (Ardoin et al., 2005; Compton et al., 2004). It seems reasonable to assume that variation in text complexity/readability might be one of t"
W17-5011,P16-2017,1,0.791391,"of main topics and related words (Beigman Klebanov et al , 2013; Burstein et al, 2016) Aggregate measure generated related to word frequency (Attali & Burstein (2006) Aggregate measure generated related to average word length for all words in a text (Attali & Burstein, 2006) Detection of morphologically complex inflectional (variants1) and derivational (variants2) word forms using an algorithm that first over-generates variants using rules and then filters using co-occurrence statistics computed over Gigaword. (Madnani et al, 2016) Detection of metaphor (Beigman Klebanov et al (2015); Beigman Klebanov et al (2016) Count measures based on VADER4 sentiment lexicon entries. Aggregate feature composed of a number of text-based vocabulary-related measures (e.g., morphological complexity, relatedness of words in a text). This work is not yet published. Aggregate measure related to collocation and preposition use (described in Burstein et al, 2013). Table 1: The 26 Features, Subconstructs & Methods 4 https://github.com/cjhutto/vaderSentiment 103 to address, and its alignment with the writing assessment construct Before modeling the interactions between the 61 AWE features and other measures, an analysis was c"
W17-5011,P16-4014,1,0.824652,"te value generated based on sentence-type factors (Burstein et al, 2013) Detection of main topics and related words (Beigman Klebanov et al , 2013; Burstein et al, 2016) Aggregate measure generated related to word frequency (Attali & Burstein (2006) Aggregate measure generated related to average word length for all words in a text (Attali & Burstein, 2006) Detection of morphologically complex inflectional (variants1) and derivational (variants2) word forms using an algorithm that first over-generates variants using rules and then filters using co-occurrence statistics computed over Gigaword. (Madnani et al, 2016) Detection of metaphor (Beigman Klebanov et al (2015); Beigman Klebanov et al (2016) Count measures based on VADER4 sentiment lexicon entries. Aggregate feature composed of a number of text-based vocabulary-related measures (e.g., morphological complexity, relatedness of words in a text). This work is not yet published. Aggregate measure related to collocation and preposition use (described in Burstein et al, 2013). Table 1: The 26 Features, Subconstructs & Methods 4 https://github.com/cjhutto/vaderSentiment 103 to address, and its alignment with the writing assessment construct Before modelin"
W17-5011,C14-1090,1,0.876939,"logg English conventions nsqm English conventions nsqu English conventions statives narrativity PR1, PR2 personal reflection complexnp phrasal complexity svf sentence variety topicdev wordln_2 topic development vocabulary sophistication vocabulary sophistication variants1, variants2 vocabulary usage metaphor vocabulary usage sentiment vocabulary usage vocab_richness vocabulary usage colprep vocabulary usage nwf_median coherence NLP-Based Feature / Resource Description Detection of sentences containing argumentation (Beigman Klebanov et al, 2017) Aggregate discourse coherence quality measure (Somasundaran et al, 2014) Latent semantic analysis values computed for long-distance sentence pairs (Somasundaran et al, 2014) Three measures related to topic distribution in a text (Beigman Klebanov et al, 2013; Burstein et al, 2016) Noun phrase collocations identified using a rank-ratio based collocation detection algorithm trained on the Google Web1T n-gram corpus (Futagi et al, 2008) Aggregate value based on length of essay-based discourse element (Attali & Burstein, 2006) derived from a discourse structure detection method that identifies essaybased discourse elements (e.g., thesis statement) (Burstein et al, 200"
W17-5011,P14-2029,0,\N,Missing
W18-0501,P14-2064,1,0.919766,"performance. To monitor rater performance, testing programs sometimes use previously scored responses that are intermixed with the operational responses. These responses are selected from operational responses to represent exemplar cases of each score level and the scores are further reviewed by multiple raters to ensure their accuracy. In this paper we are examining the effect of using such “exemplar” responses for scoring model training and evaluation in the context of automated speech scoring. In particular, we aim to address the following research questions: In a series of papers, Beigman Klebanov and Beigman (2014; 2009; 2009) studied annotation noise in linguistic data, namely, a situation where some of the data is easy to judge, with clear-cut annotation/classification, whereas some of the data is harder to judge, yielding disagreements among raters. They show that in a binary classification task, the presence of annotation noise (hard to judge cases) in the evaluation data could skew benchmarking, especially in cases of small discrepancies between competing models. They also show that the presence of hard cases in the training data could compromise system performance on easyto-judge test cases, a ph"
W18-0501,W17-4609,1,0.89323,"Missing"
W18-0501,J08-3001,0,0.0316515,"Missing"
W18-0501,W15-0602,1,0.757219,"g on the question type and system used. This model achieved substantially higher performance on the EXEM - 5.2 Size of the training set To further evaluate whether training on a larger number of EXEMPLAR responses may have lead to better performance on the MAIN corpus, we re-trained the models using all responses pooled across the different question types. Such an approach has been previously used in other studies in situations where all types of questions are scored based on the same or similar rubrics and the scoring models do not include any questionspecific features (Higgins et al., 2011; Loukina et al., 2015). A substantial increase in the size of the training set to some extent compensates for loss of information about question-specific patterns. The models were evaluated by question type, as in the rest of this paper. To obtain the learning curves for different training sets, we trained all models using training sets of varying sizes from 1000 responses to the full training partition of a given corpus. For each N other than where N is the length of full corpus we trained models 5 times using 5 randomly sampled training sets. Figure 1 shows the learning curves for different combinations of traini"
W18-0501,P11-1067,0,0.275616,"Missing"
W18-0501,W17-1605,1,0.464752,"Missing"
W18-0501,N15-1152,0,0.384794,"orpus of such test taker responses with scores assigned by trained human raters, considered to be the “gold standard” for both training and evaluation of the automated scoring system (Page, 1966; Attali and Burstein, 2006; Bernstein et al., 2010; Williamson et al., 2012). Human raters follow certain agreed-upon scoring guidelines (“rubrics”) that define the characteristics of a 1 Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 1–12 c New Orleans, Louisiana, June 5, 2018. 2018 Association for Computational Linguistics 2008; Mart´ınez Alonso et al., 2015; Plank et al., 2014). performance. To monitor rater performance, testing programs sometimes use previously scored responses that are intermixed with the operational responses. These responses are selected from operational responses to represent exemplar cases of each score level and the scores are further reviewed by multiple raters to ensure their accuracy. In this paper we are examining the effect of using such “exemplar” responses for scoring model training and evaluation in the context of automated speech scoring. In particular, we aim to address the following research questions: In a ser"
W18-0501,E14-1078,0,0.0271219,"Missing"
W18-0501,W15-0615,0,0.059364,"Missing"
W18-0905,W12-2006,0,0.0126444,"s, prepositions and a set of other common function words (see appendix for the full list), as well as possessive “’s”, and punctuation like commas and hyphens. An idiom should be matched even if such elements are missing in the text. For example, with inflectional expansion and with marking of optional elements, the idiom “give the royal treatment” becomes “{give, given, gave, giving, gives} [the,a,an] {royal, royals} {treatment, treatments}”. The need for optional elements stems from the notion that writers, especially EFL writers, often omit articles and prepositions, or use erroneous ones (Dale et al., 2012). Note that the algorithm has two separate skip strategies. On the one hand, there are optional elements in the idiom searchspecification, such as determiners or pronouns. This means that not all components of an idiom have to be matched in order to spot a potential idiom-instance. On the other hand, the algorithm can skip over tokens in the text, to allow for intervening material. The combination of these two approaches allows to find instances of lexically underspecified idioms. For example, the idiom “change one’s mind ” is expanded to “{changes, changing, change, changed} [my, your, his, h"
W18-0905,W13-0902,1,0.890999,"ish experience when encountering metaphors1 in British university lectures, including non-understanding (failure to interpret) and misunderstanding (incorrect interpretation). A complementary line of research focuses on the EFL students’ use of metaphors in language production. Littlemore et al. (2014) analyzed the use of metaphors in 200 exam essays written by EFL students, at different levels of English proficiency. They found that metaphor use increases with proficiency level, and even suggested that descriptors for metaphor use could be integrated in the rating scales for writing. Beigman Klebanov and Flor (2013) investigated the use of metaphors in 116 argumentative essays and found moderateto-strong correlation between the percentage of metaphorically used words in an essay and the writing quality score. Notably, both studies used a small number of essays and conducted an exhaustive manual analysis of metaphoric expressions. 3 Idiom identification Syntactic and lexical flexibility are two of the issues dealt with at length in the linguistic and psycholinguistic literature on idioms (Glucksberg, 2001; Nunberg et al., 1994). Idioms can vary from being fully syntactically flexible to not at all. Althou"
W18-0905,W09-2903,0,0.0248907,"ays/usually idiomatic or literal (Shutova et al., 2010; Gedigian et al., 2006; Widdows and Dorow, 2005), and token classification, where each occurrence of a phrase, in a specific context, can be idiomatic or literal (Peng et al., 2014; Li and Sporleder, 2009; Sporleder and Li, 2009; Fazly et al., 2009; Katz and Giesbrecht, 2006). Early work on idiom detection involved small sets of expressions (Fazly and Stevenson, 2006), and focused on specific types of syntactic constructions (such as verb + complement, e.g. “stir excitement”,“play with fire”) (Shutova et al., 2010; Li and Sporleder, 2009; Diab and Bhutada, 2009; Diab and Krishna, 2009). More recent research on detection of non-compositional word combinations has shown a proliferation of approaches, but much work still focuses on acontextual classification (Hashimoto and Tsuruoka, 2016; Cordeiro et al., 2016; Ramisch et al., 2016; Yazdani et al., 2015; Salehi et al., 2014; Salehi and Cook, 2013; Kiela and Clark, 2013; Reddy et al., 2011). Recent work on detection of idiom instances in context (Gharbieh et al., 2016; Salton et al., 2016; Peng et al., 2014) focused only on Verb+Noun constructions, using the same dataset (Cook et al., 2008). A notable e"
W18-0905,W09-0213,0,0.0197712,"literal (Shutova et al., 2010; Gedigian et al., 2006; Widdows and Dorow, 2005), and token classification, where each occurrence of a phrase, in a specific context, can be idiomatic or literal (Peng et al., 2014; Li and Sporleder, 2009; Sporleder and Li, 2009; Fazly et al., 2009; Katz and Giesbrecht, 2006). Early work on idiom detection involved small sets of expressions (Fazly and Stevenson, 2006), and focused on specific types of syntactic constructions (such as verb + complement, e.g. “stir excitement”,“play with fire”) (Shutova et al., 2010; Li and Sporleder, 2009; Diab and Bhutada, 2009; Diab and Krishna, 2009). More recent research on detection of non-compositional word combinations has shown a proliferation of approaches, but much work still focuses on acontextual classification (Hashimoto and Tsuruoka, 2016; Cordeiro et al., 2016; Ramisch et al., 2016; Yazdani et al., 2015; Salehi et al., 2014; Salehi and Cook, 2013; Kiela and Clark, 2013; Reddy et al., 2011). Recent work on detection of idiom instances in context (Gharbieh et al., 2016; Salton et al., 2016; Peng et al., 2014) focused only on Verb+Noun constructions, using the same dataset (Cook et al., 2008). A notable exception is the work of F"
W18-0905,J09-1005,0,0.0866498,"of the idiom, posing an additional challenge for automatic detection. 3.1 Automated detection of idioms In computational linguistics, idiom detection systems fall into one of two paradigms (Muzny and Zettlemoyer, 2013): type classification, where a decision is made whether an expression (out of any context) is always/usually idiomatic or literal (Shutova et al., 2010; Gedigian et al., 2006; Widdows and Dorow, 2005), and token classification, where each occurrence of a phrase, in a specific context, can be idiomatic or literal (Peng et al., 2014; Li and Sporleder, 2009; Sporleder and Li, 2009; Fazly et al., 2009; Katz and Giesbrecht, 2006). Early work on idiom detection involved small sets of expressions (Fazly and Stevenson, 2006), and focused on specific types of syntactic constructions (such as verb + complement, e.g. “stir excitement”,“play with fire”) (Shutova et al., 2010; Li and Sporleder, 2009; Diab and Bhutada, 2009; Diab and Krishna, 2009). More recent research on detection of non-compositional word combinations has shown a proliferation of approaches, but much work still focuses on acontextual classification (Hashimoto and Tsuruoka, 2016; Cordeiro et al., 2016; Ramisch et al., 2016; Yazdan"
W18-0905,E06-1043,0,0.0425784,"tional linguistics, idiom detection systems fall into one of two paradigms (Muzny and Zettlemoyer, 2013): type classification, where a decision is made whether an expression (out of any context) is always/usually idiomatic or literal (Shutova et al., 2010; Gedigian et al., 2006; Widdows and Dorow, 2005), and token classification, where each occurrence of a phrase, in a specific context, can be idiomatic or literal (Peng et al., 2014; Li and Sporleder, 2009; Sporleder and Li, 2009; Fazly et al., 2009; Katz and Giesbrecht, 2006). Early work on idiom detection involved small sets of expressions (Fazly and Stevenson, 2006), and focused on specific types of syntactic constructions (such as verb + complement, e.g. “stir excitement”,“play with fire”) (Shutova et al., 2010; Li and Sporleder, 2009; Diab and Bhutada, 2009; Diab and Krishna, 2009). More recent research on detection of non-compositional word combinations has shown a proliferation of approaches, but much work still focuses on acontextual classification (Hashimoto and Tsuruoka, 2016; Cordeiro et al., 2016; Ramisch et al., 2016; Yazdani et al., 2015; Salehi et al., 2014; Salehi and Cook, 2013; Kiela and Clark, 2013; Reddy et al., 2011). Recent work on det"
W18-0905,W06-3506,0,0.151521,"tic expressions: idioms that are not fully lexically specified. Such idioms, e.g. “be the apple of one’s eye”, include slots that must be filled in context, thus involving modification and discontinuity of the lexical components of the idiom, posing an additional challenge for automatic detection. 3.1 Automated detection of idioms In computational linguistics, idiom detection systems fall into one of two paradigms (Muzny and Zettlemoyer, 2013): type classification, where a decision is made whether an expression (out of any context) is always/usually idiomatic or literal (Shutova et al., 2010; Gedigian et al., 2006; Widdows and Dorow, 2005), and token classification, where each occurrence of a phrase, in a specific context, can be idiomatic or literal (Peng et al., 2014; Li and Sporleder, 2009; Sporleder and Li, 2009; Fazly et al., 2009; Katz and Giesbrecht, 2006). Early work on idiom detection involved small sets of expressions (Fazly and Stevenson, 2006), and focused on specific types of syntactic constructions (such as verb + complement, e.g. “stir excitement”,“play with fire”) (Shutova et al., 2010; Li and Sporleder, 2009; Diab and Bhutada, 2009; Diab and Krishna, 2009). More recent research on dete"
W18-0905,W09-3211,0,0.0122601,"ion and discontinuity of the lexical components of the idiom, posing an additional challenge for automatic detection. 3.1 Automated detection of idioms In computational linguistics, idiom detection systems fall into one of two paradigms (Muzny and Zettlemoyer, 2013): type classification, where a decision is made whether an expression (out of any context) is always/usually idiomatic or literal (Shutova et al., 2010; Gedigian et al., 2006; Widdows and Dorow, 2005), and token classification, where each occurrence of a phrase, in a specific context, can be idiomatic or literal (Peng et al., 2014; Li and Sporleder, 2009; Sporleder and Li, 2009; Fazly et al., 2009; Katz and Giesbrecht, 2006). Early work on idiom detection involved small sets of expressions (Fazly and Stevenson, 2006), and focused on specific types of syntactic constructions (such as verb + complement, e.g. “stir excitement”,“play with fire”) (Shutova et al., 2010; Li and Sporleder, 2009; Diab and Bhutada, 2009; Diab and Krishna, 2009). More recent research on detection of non-compositional word combinations has shown a proliferation of approaches, but much work still focuses on acontextual classification (Hashimoto and Tsuruoka, 2016; Cordeir"
W18-0905,W16-1817,0,0.0177784,"tactic constructions (such as verb + complement, e.g. “stir excitement”,“play with fire”) (Shutova et al., 2010; Li and Sporleder, 2009; Diab and Bhutada, 2009; Diab and Krishna, 2009). More recent research on detection of non-compositional word combinations has shown a proliferation of approaches, but much work still focuses on acontextual classification (Hashimoto and Tsuruoka, 2016; Cordeiro et al., 2016; Ramisch et al., 2016; Yazdani et al., 2015; Salehi et al., 2014; Salehi and Cook, 2013; Kiela and Clark, 2013; Reddy et al., 2011). Recent work on detection of idiom instances in context (Gharbieh et al., 2016; Salton et al., 2016; Peng et al., 2014) focused only on Verb+Noun constructions, using the same dataset (Cook et al., 2008). A notable exception is the work of Feldman and Peng (2013), which is not limited by the type of syntactic construction. 4 4.1 A collection of idioms For our collection, we use Wiktionary as a resource. Wiktionary has a facility for contributors to tag definitions as idiomatic. The English Wiktionary was used in some previous computational work on idioms (Salehi et al., 2014), as it has rather broad coverage for idioms (although it is far from being complete (Muzny and"
W18-0905,P16-1020,0,0.0180398,"et al., 2014; Li and Sporleder, 2009; Sporleder and Li, 2009; Fazly et al., 2009; Katz and Giesbrecht, 2006). Early work on idiom detection involved small sets of expressions (Fazly and Stevenson, 2006), and focused on specific types of syntactic constructions (such as verb + complement, e.g. “stir excitement”,“play with fire”) (Shutova et al., 2010; Li and Sporleder, 2009; Diab and Bhutada, 2009; Diab and Krishna, 2009). More recent research on detection of non-compositional word combinations has shown a proliferation of approaches, but much work still focuses on acontextual classification (Hashimoto and Tsuruoka, 2016; Cordeiro et al., 2016; Ramisch et al., 2016; Yazdani et al., 2015; Salehi et al., 2014; Salehi and Cook, 2013; Kiela and Clark, 2013; Reddy et al., 2011). Recent work on detection of idiom instances in context (Gharbieh et al., 2016; Salton et al., 2016; Peng et al., 2014) focused only on Verb+Noun constructions, using the same dataset (Cook et al., 2008). A notable exception is the work of Feldman and Peng (2013), which is not limited by the type of syntactic construction. 4 4.1 A collection of idioms For our collection, we use Wiktionary as a resource. Wiktionary has a facility for contrib"
W18-0905,W06-1203,0,0.151712,"an additional challenge for automatic detection. 3.1 Automated detection of idioms In computational linguistics, idiom detection systems fall into one of two paradigms (Muzny and Zettlemoyer, 2013): type classification, where a decision is made whether an expression (out of any context) is always/usually idiomatic or literal (Shutova et al., 2010; Gedigian et al., 2006; Widdows and Dorow, 2005), and token classification, where each occurrence of a phrase, in a specific context, can be idiomatic or literal (Peng et al., 2014; Li and Sporleder, 2009; Sporleder and Li, 2009; Fazly et al., 2009; Katz and Giesbrecht, 2006). Early work on idiom detection involved small sets of expressions (Fazly and Stevenson, 2006), and focused on specific types of syntactic constructions (such as verb + complement, e.g. “stir excitement”,“play with fire”) (Shutova et al., 2010; Li and Sporleder, 2009; Diab and Bhutada, 2009; Diab and Krishna, 2009). More recent research on detection of non-compositional word combinations has shown a proliferation of approaches, but much work still focuses on acontextual classification (Hashimoto and Tsuruoka, 2016; Cordeiro et al., 2016; Ramisch et al., 2016; Yazdani et al., 2015; Salehi et al"
W18-0905,D13-1145,0,0.0876827,"esults with the first part of this approach - detecting candidate-idiom expressions in student essays. In this respect, it is important to mention one very common sub-type of idiomatic expressions: idioms that are not fully lexically specified. Such idioms, e.g. “be the apple of one’s eye”, include slots that must be filled in context, thus involving modification and discontinuity of the lexical components of the idiom, posing an additional challenge for automatic detection. 3.1 Automated detection of idioms In computational linguistics, idiom detection systems fall into one of two paradigms (Muzny and Zettlemoyer, 2013): type classification, where a decision is made whether an expression (out of any context) is always/usually idiomatic or literal (Shutova et al., 2010; Gedigian et al., 2006; Widdows and Dorow, 2005), and token classification, where each occurrence of a phrase, in a specific context, can be idiomatic or literal (Peng et al., 2014; Li and Sporleder, 2009; Sporleder and Li, 2009; Fazly et al., 2009; Katz and Giesbrecht, 2006). Early work on idiom detection involved small sets of expressions (Fazly and Stevenson, 2006), and focused on specific types of syntactic constructions (such as verb + com"
W18-0905,D13-1147,0,0.209681,"involved small sets of expressions (Fazly and Stevenson, 2006), and focused on specific types of syntactic constructions (such as verb + complement, e.g. “stir excitement”,“play with fire”) (Shutova et al., 2010; Li and Sporleder, 2009; Diab and Bhutada, 2009; Diab and Krishna, 2009). More recent research on detection of non-compositional word combinations has shown a proliferation of approaches, but much work still focuses on acontextual classification (Hashimoto and Tsuruoka, 2016; Cordeiro et al., 2016; Ramisch et al., 2016; Yazdani et al., 2015; Salehi et al., 2014; Salehi and Cook, 2013; Kiela and Clark, 2013; Reddy et al., 2011). Recent work on detection of idiom instances in context (Gharbieh et al., 2016; Salton et al., 2016; Peng et al., 2014) focused only on Verb+Noun constructions, using the same dataset (Cook et al., 2008). A notable exception is the work of Feldman and Peng (2013), which is not limited by the type of syntactic construction. 4 4.1 A collection of idioms For our collection, we use Wiktionary as a resource. Wiktionary has a facility for contributors to tag definitions as idiomatic. The English Wiktionary was used in some previous computational work on idioms (Salehi et al., 2"
W18-0905,D14-1216,0,0.0134614,"involving modification and discontinuity of the lexical components of the idiom, posing an additional challenge for automatic detection. 3.1 Automated detection of idioms In computational linguistics, idiom detection systems fall into one of two paradigms (Muzny and Zettlemoyer, 2013): type classification, where a decision is made whether an expression (out of any context) is always/usually idiomatic or literal (Shutova et al., 2010; Gedigian et al., 2006; Widdows and Dorow, 2005), and token classification, where each occurrence of a phrase, in a specific context, can be idiomatic or literal (Peng et al., 2014; Li and Sporleder, 2009; Sporleder and Li, 2009; Fazly et al., 2009; Katz and Giesbrecht, 2006). Early work on idiom detection involved small sets of expressions (Fazly and Stevenson, 2006), and focused on specific types of syntactic constructions (such as verb + complement, e.g. “stir excitement”,“play with fire”) (Shutova et al., 2010; Li and Sporleder, 2009; Diab and Bhutada, 2009; Diab and Krishna, 2009). More recent research on detection of non-compositional word combinations has shown a proliferation of approaches, but much work still focuses on acontextual classification (Hashimoto and"
W18-0905,W05-1006,0,0.0592302,"that are not fully lexically specified. Such idioms, e.g. “be the apple of one’s eye”, include slots that must be filled in context, thus involving modification and discontinuity of the lexical components of the idiom, posing an additional challenge for automatic detection. 3.1 Automated detection of idioms In computational linguistics, idiom detection systems fall into one of two paradigms (Muzny and Zettlemoyer, 2013): type classification, where a decision is made whether an expression (out of any context) is always/usually idiomatic or literal (Shutova et al., 2010; Gedigian et al., 2006; Widdows and Dorow, 2005), and token classification, where each occurrence of a phrase, in a specific context, can be idiomatic or literal (Peng et al., 2014; Li and Sporleder, 2009; Sporleder and Li, 2009; Fazly et al., 2009; Katz and Giesbrecht, 2006). Early work on idiom detection involved small sets of expressions (Fazly and Stevenson, 2006), and focused on specific types of syntactic constructions (such as verb + complement, e.g. “stir excitement”,“play with fire”) (Shutova et al., 2010; Li and Sporleder, 2009; Diab and Bhutada, 2009; Diab and Krishna, 2009). More recent research on detection of non-compositional"
W18-0905,P16-2026,0,0.07925,"Li, 2009; Fazly et al., 2009; Katz and Giesbrecht, 2006). Early work on idiom detection involved small sets of expressions (Fazly and Stevenson, 2006), and focused on specific types of syntactic constructions (such as verb + complement, e.g. “stir excitement”,“play with fire”) (Shutova et al., 2010; Li and Sporleder, 2009; Diab and Bhutada, 2009; Diab and Krishna, 2009). More recent research on detection of non-compositional word combinations has shown a proliferation of approaches, but much work still focuses on acontextual classification (Hashimoto and Tsuruoka, 2016; Cordeiro et al., 2016; Ramisch et al., 2016; Yazdani et al., 2015; Salehi et al., 2014; Salehi and Cook, 2013; Kiela and Clark, 2013; Reddy et al., 2011). Recent work on detection of idiom instances in context (Gharbieh et al., 2016; Salton et al., 2016; Peng et al., 2014) focused only on Verb+Noun constructions, using the same dataset (Cook et al., 2008). A notable exception is the work of Feldman and Peng (2013), which is not limited by the type of syntactic construction. 4 4.1 A collection of idioms For our collection, we use Wiktionary as a resource. Wiktionary has a facility for contributors to tag definitions as idiomatic. The En"
W18-0905,D15-1201,0,0.0606328,", 2009; Katz and Giesbrecht, 2006). Early work on idiom detection involved small sets of expressions (Fazly and Stevenson, 2006), and focused on specific types of syntactic constructions (such as verb + complement, e.g. “stir excitement”,“play with fire”) (Shutova et al., 2010; Li and Sporleder, 2009; Diab and Bhutada, 2009; Diab and Krishna, 2009). More recent research on detection of non-compositional word combinations has shown a proliferation of approaches, but much work still focuses on acontextual classification (Hashimoto and Tsuruoka, 2016; Cordeiro et al., 2016; Ramisch et al., 2016; Yazdani et al., 2015; Salehi et al., 2014; Salehi and Cook, 2013; Kiela and Clark, 2013; Reddy et al., 2011). Recent work on detection of idiom instances in context (Gharbieh et al., 2016; Salton et al., 2016; Peng et al., 2014) focused only on Verb+Noun constructions, using the same dataset (Cook et al., 2008). A notable exception is the work of Feldman and Peng (2013), which is not limited by the type of syntactic construction. 4 4.1 A collection of idioms For our collection, we use Wiktionary as a resource. Wiktionary has a facility for contributors to tag definitions as idiomatic. The English Wiktionary was u"
W18-0905,I11-1024,0,0.0719073,"expressions (Fazly and Stevenson, 2006), and focused on specific types of syntactic constructions (such as verb + complement, e.g. “stir excitement”,“play with fire”) (Shutova et al., 2010; Li and Sporleder, 2009; Diab and Bhutada, 2009; Diab and Krishna, 2009). More recent research on detection of non-compositional word combinations has shown a proliferation of approaches, but much work still focuses on acontextual classification (Hashimoto and Tsuruoka, 2016; Cordeiro et al., 2016; Ramisch et al., 2016; Yazdani et al., 2015; Salehi et al., 2014; Salehi and Cook, 2013; Kiela and Clark, 2013; Reddy et al., 2011). Recent work on detection of idiom instances in context (Gharbieh et al., 2016; Salton et al., 2016; Peng et al., 2014) focused only on Verb+Noun constructions, using the same dataset (Cook et al., 2008). A notable exception is the work of Feldman and Peng (2013), which is not limited by the type of syntactic construction. 4 4.1 A collection of idioms For our collection, we use Wiktionary as a resource. Wiktionary has a facility for contributors to tag definitions as idiomatic. The English Wiktionary was used in some previous computational work on idioms (Salehi et al., 2014), as it has rathe"
W18-0905,S13-1039,0,0.017605,"ork on idiom detection involved small sets of expressions (Fazly and Stevenson, 2006), and focused on specific types of syntactic constructions (such as verb + complement, e.g. “stir excitement”,“play with fire”) (Shutova et al., 2010; Li and Sporleder, 2009; Diab and Bhutada, 2009; Diab and Krishna, 2009). More recent research on detection of non-compositional word combinations has shown a proliferation of approaches, but much work still focuses on acontextual classification (Hashimoto and Tsuruoka, 2016; Cordeiro et al., 2016; Ramisch et al., 2016; Yazdani et al., 2015; Salehi et al., 2014; Salehi and Cook, 2013; Kiela and Clark, 2013; Reddy et al., 2011). Recent work on detection of idiom instances in context (Gharbieh et al., 2016; Salton et al., 2016; Peng et al., 2014) focused only on Verb+Noun constructions, using the same dataset (Cook et al., 2008). A notable exception is the work of Feldman and Peng (2013), which is not limited by the type of syntactic construction. 4 4.1 A collection of idioms For our collection, we use Wiktionary as a resource. Wiktionary has a facility for contributors to tag definitions as idiomatic. The English Wiktionary was used in some previous computational work on i"
W18-0905,D14-1189,0,0.0133136,"recht, 2006). Early work on idiom detection involved small sets of expressions (Fazly and Stevenson, 2006), and focused on specific types of syntactic constructions (such as verb + complement, e.g. “stir excitement”,“play with fire”) (Shutova et al., 2010; Li and Sporleder, 2009; Diab and Bhutada, 2009; Diab and Krishna, 2009). More recent research on detection of non-compositional word combinations has shown a proliferation of approaches, but much work still focuses on acontextual classification (Hashimoto and Tsuruoka, 2016; Cordeiro et al., 2016; Ramisch et al., 2016; Yazdani et al., 2015; Salehi et al., 2014; Salehi and Cook, 2013; Kiela and Clark, 2013; Reddy et al., 2011). Recent work on detection of idiom instances in context (Gharbieh et al., 2016; Salton et al., 2016; Peng et al., 2014) focused only on Verb+Noun constructions, using the same dataset (Cook et al., 2008). A notable exception is the work of Feldman and Peng (2013), which is not limited by the type of syntactic construction. 4 4.1 A collection of idioms For our collection, we use Wiktionary as a resource. Wiktionary has a facility for contributors to tag definitions as idiomatic. The English Wiktionary was used in some previous"
W18-0905,P16-1019,0,0.0812891,"uch as verb + complement, e.g. “stir excitement”,“play with fire”) (Shutova et al., 2010; Li and Sporleder, 2009; Diab and Bhutada, 2009; Diab and Krishna, 2009). More recent research on detection of non-compositional word combinations has shown a proliferation of approaches, but much work still focuses on acontextual classification (Hashimoto and Tsuruoka, 2016; Cordeiro et al., 2016; Ramisch et al., 2016; Yazdani et al., 2015; Salehi et al., 2014; Salehi and Cook, 2013; Kiela and Clark, 2013; Reddy et al., 2011). Recent work on detection of idiom instances in context (Gharbieh et al., 2016; Salton et al., 2016; Peng et al., 2014) focused only on Verb+Noun constructions, using the same dataset (Cook et al., 2008). A notable exception is the work of Feldman and Peng (2013), which is not limited by the type of syntactic construction. 4 4.1 A collection of idioms For our collection, we use Wiktionary as a resource. Wiktionary has a facility for contributors to tag definitions as idiomatic. The English Wiktionary was used in some previous computational work on idioms (Salehi et al., 2014), as it has rather broad coverage for idioms (although it is far from being complete (Muzny and Zettlemoyer, 2013))."
W18-0905,C10-1113,0,0.0540016,"mon sub-type of idiomatic expressions: idioms that are not fully lexically specified. Such idioms, e.g. “be the apple of one’s eye”, include slots that must be filled in context, thus involving modification and discontinuity of the lexical components of the idiom, posing an additional challenge for automatic detection. 3.1 Automated detection of idioms In computational linguistics, idiom detection systems fall into one of two paradigms (Muzny and Zettlemoyer, 2013): type classification, where a decision is made whether an expression (out of any context) is always/usually idiomatic or literal (Shutova et al., 2010; Gedigian et al., 2006; Widdows and Dorow, 2005), and token classification, where each occurrence of a phrase, in a specific context, can be idiomatic or literal (Peng et al., 2014; Li and Sporleder, 2009; Sporleder and Li, 2009; Fazly et al., 2009; Katz and Giesbrecht, 2006). Early work on idiom detection involved small sets of expressions (Fazly and Stevenson, 2006), and focused on specific types of syntactic constructions (such as verb + complement, e.g. “stir excitement”,“play with fire”) (Shutova et al., 2010; Li and Sporleder, 2009; Diab and Bhutada, 2009; Diab and Krishna, 2009). More"
W18-0905,E09-1086,0,0.0153416,"the lexical components of the idiom, posing an additional challenge for automatic detection. 3.1 Automated detection of idioms In computational linguistics, idiom detection systems fall into one of two paradigms (Muzny and Zettlemoyer, 2013): type classification, where a decision is made whether an expression (out of any context) is always/usually idiomatic or literal (Shutova et al., 2010; Gedigian et al., 2006; Widdows and Dorow, 2005), and token classification, where each occurrence of a phrase, in a specific context, can be idiomatic or literal (Peng et al., 2014; Li and Sporleder, 2009; Sporleder and Li, 2009; Fazly et al., 2009; Katz and Giesbrecht, 2006). Early work on idiom detection involved small sets of expressions (Fazly and Stevenson, 2006), and focused on specific types of syntactic constructions (such as verb + complement, e.g. “stir excitement”,“play with fire”) (Shutova et al., 2010; Li and Sporleder, 2009; Diab and Bhutada, 2009; Diab and Krishna, 2009). More recent research on detection of non-compositional word combinations has shown a proliferation of approaches, but much work still focuses on acontextual classification (Hashimoto and Tsuruoka, 2016; Cordeiro et al., 2016; Ramisch"
W18-0907,N18-2014,1,0.408162,"her subjects and contexts (Lakoff and Johnson, 2008); it is a fundamental way to structure our understanding of the world even without our conscious realization of its presence as we speak and write. It highlights the unknown using the known, explains the complex using the simple, and helps us to emphasize the relevant aspects of meaning resulting in effective communication. Consider the following examples of metaphor use in Table 1. Metaphor has been studied in the context of political communication, marketing, mental health, teaching, assessment of English proficiency, among others (Beigman Klebanov et al., 2018; Gutierrez et al., 2017; Littlemore et al., 2013; Thibodeau and Boroditsky, 2011; Kaviani and Hamedi, 2011; Kathpalia and Carmel, 2011; Landau et al., 2009; Beigman Klebanov et al., 2008; Zaltman and Zaltman, 2008; Littlemore and Low, 2006; Cameron, 2003; Lakoff, 2010; Billow et al., 1997; Bosman, 1987); see chapter 7 in Veale et al. (2016) for a recent review. Table 1: Metaphorical sentences (M) characterized by metaphors in bold and their literal interpretations (I) In this paper, we report on the first shared task on automatic metaphor detection. By making available an easily accessible co"
W18-0907,P16-2017,1,0.639013,"gly popular topic, which manifests itself in both a variety of approaches and in an increasing variety of data to which the methods are applied. In terms of methods, approaches based on feature-engineering in a supervised machine learning paradigm explored features based on concreteness and imageability, semantic classification using WordNet, FrameNet, VerbNet, SUMO ontology, property norms, and distributional semantic models, syntactic dependency patterns, sensorial and vision-based features (Bulat et al., 2017; K¨oper and im Walde, 2017; Gutierrez et al., 2016; Shutova et al., 2016; Beigman Klebanov et al., 2016; Tekiroglu et al., 2015; Tsvetkov et al., 2014; Beigman Klebanov et al., 2014; Dunn, 2013; Neuman et al., 2013; Mohler et al., 2013; Hovy et al., 2013; Tsvetkov et al., 2013; Turney et al., 2011; Shutova et al., 2010; Gedigian et al., 2006); see Shutova et al. (2017) and Veale et al. (2016) for reviews of supervised as well as semi-supervised and unsupervised approaches. Recently, deep learning methods have been explored for token-level metaphor detection (Rei et al., 2017; Gutierrez et al., 2017; Do Dinh and Gurevych, 2016). As discussed later in the paper later, the fact that all but one of"
W18-0907,W14-2302,1,0.856765,"in an increasing variety of data to which the methods are applied. In terms of methods, approaches based on feature-engineering in a supervised machine learning paradigm explored features based on concreteness and imageability, semantic classification using WordNet, FrameNet, VerbNet, SUMO ontology, property norms, and distributional semantic models, syntactic dependency patterns, sensorial and vision-based features (Bulat et al., 2017; K¨oper and im Walde, 2017; Gutierrez et al., 2016; Shutova et al., 2016; Beigman Klebanov et al., 2016; Tekiroglu et al., 2015; Tsvetkov et al., 2014; Beigman Klebanov et al., 2014; Dunn, 2013; Neuman et al., 2013; Mohler et al., 2013; Hovy et al., 2013; Tsvetkov et al., 2013; Turney et al., 2011; Shutova et al., 2010; Gedigian et al., 2006); see Shutova et al. (2017) and Veale et al. (2016) for reviews of supervised as well as semi-supervised and unsupervised approaches. Recently, deep learning methods have been explored for token-level metaphor detection (Rei et al., 2017; Gutierrez et al., 2017; Do Dinh and Gurevych, 2016). As discussed later in the paper later, the fact that all but one of the participating teams for the shared task experimented with neural network"
W18-0907,P16-1018,1,0.852524,"utomated detection of metaphor has become an increasingly popular topic, which manifests itself in both a variety of approaches and in an increasing variety of data to which the methods are applied. In terms of methods, approaches based on feature-engineering in a supervised machine learning paradigm explored features based on concreteness and imageability, semantic classification using WordNet, FrameNet, VerbNet, SUMO ontology, property norms, and distributional semantic models, syntactic dependency patterns, sensorial and vision-based features (Bulat et al., 2017; K¨oper and im Walde, 2017; Gutierrez et al., 2016; Shutova et al., 2016; Beigman Klebanov et al., 2016; Tekiroglu et al., 2015; Tsvetkov et al., 2014; Beigman Klebanov et al., 2014; Dunn, 2013; Neuman et al., 2013; Mohler et al., 2013; Hovy et al., 2013; Tsvetkov et al., 2013; Turney et al., 2011; Shutova et al., 2010; Gedigian et al., 2006); see Shutova et al. (2017) and Veale et al. (2016) for reviews of supervised as well as semi-supervised and unsupervised approaches. Recently, deep learning methods have been explored for token-level metaphor detection (Rei et al., 2017; Gutierrez et al., 2017; Do Dinh and Gurevych, 2016). As discussed l"
W18-0907,P04-3031,0,0.145739,"fiers We make available to shared task participants a number of features from prior published work on metaphor detection, including unigram features, features based on WordNet, VerbNet, and those derived from a distributional semantic model, POS-based, concreteness and difference in concreteness, as well as topic models. As baselines, we train two logistic regression classifiers for each track (Verbs and All-POS), with instance weights inversely proportional to class frequencies. Lemmatized unigrams (UL) is a simple yet fairly strong baseline (Baseline 1). This feature is produced using NLTK (Bird and Loper, 2004) to generate the lemma of each word according to its tagged POS. As Baseline 2, we use the best system from Beigman Klebanov et al. (2016). The features are: lemmatized unigrams, generalized WordNet semantic classes, and difference in concreteness ratings between verbs/adjectives and nouns (UL + WordNet + CCDB).5 4.1 nsu ai (Mosolova et al., 2018) used linguistic features based on unigrams, lemmas, POS tags, topical LDAs, concreteness, WordNet, VerbNet and verb clusters and trained a Conditional Random Field (CRF) model with gradient descent using the L-BFGS method to generate predictions. OCO"
W18-0907,W13-0907,0,0.0910459,"f methods, approaches based on feature-engineering in a supervised machine learning paradigm explored features based on concreteness and imageability, semantic classification using WordNet, FrameNet, VerbNet, SUMO ontology, property norms, and distributional semantic models, syntactic dependency patterns, sensorial and vision-based features (Bulat et al., 2017; K¨oper and im Walde, 2017; Gutierrez et al., 2016; Shutova et al., 2016; Beigman Klebanov et al., 2016; Tekiroglu et al., 2015; Tsvetkov et al., 2014; Beigman Klebanov et al., 2014; Dunn, 2013; Neuman et al., 2013; Mohler et al., 2013; Hovy et al., 2013; Tsvetkov et al., 2013; Turney et al., 2011; Shutova et al., 2010; Gedigian et al., 2006); see Shutova et al. (2017) and Veale et al. (2016) for reviews of supervised as well as semi-supervised and unsupervised approaches. Recently, deep learning methods have been explored for token-level metaphor detection (Rei et al., 2017; Gutierrez et al., 2017; Do Dinh and Gurevych, 2016). As discussed later in the paper later, the fact that all but one of the participating teams for the shared task experimented with neural network architectures testifies to the increasing popularity of this modeling app"
W18-0907,E06-1042,0,0.299486,"per later, the fact that all but one of the participating teams for the shared task experimented with neural network architectures testifies to the increasing popularity of this modeling approach. In terms of data used for evaluating metaphor detection systems, researchers used specially constructed or selected sets, such as adjective noun pairs (Gutierrez et al., 2016; Tsvetkov et al., 2014), WordNet synsets and glosses (Mohammad et al., 2016), annotated lexical items (from a range of word classes) in sentences sampled from cor¨ pora (Ozbal et al., 2016; Jang et al., 2015; Hovy et al., 2013; Birke and Sarkar, 2006), all the way to annotation of all words in running text for metaphoricity (Beigman Klebanov et al., 2018; Steen et al., 2010); Veale et al. (2016) review additional annotated datasets. By far the largest annotated dataset is the VU Amsterdam Metaphor Corpus; it has also been used for evaluating many of the cited supervised learning-based systems. Due to its size, availability, reliability of annotation, 3 Task Description The goal of this shared task is to detect, at the word level, all metaphors in a given text. Specifically, there are two tracks, namely, All Part-Of-Speech (POS) and Verbs."
W18-0907,W15-4650,0,0.0706848,"h, 2016). As discussed later in the paper later, the fact that all but one of the participating teams for the shared task experimented with neural network architectures testifies to the increasing popularity of this modeling approach. In terms of data used for evaluating metaphor detection systems, researchers used specially constructed or selected sets, such as adjective noun pairs (Gutierrez et al., 2016; Tsvetkov et al., 2014), WordNet synsets and glosses (Mohammad et al., 2016), annotated lexical items (from a range of word classes) in sentences sampled from cor¨ pora (Ozbal et al., 2016; Jang et al., 2015; Hovy et al., 2013; Birke and Sarkar, 2006), all the way to annotation of all words in running text for metaphoricity (Beigman Klebanov et al., 2018; Steen et al., 2010); Veale et al. (2016) review additional annotated datasets. By far the largest annotated dataset is the VU Amsterdam Metaphor Corpus; it has also been used for evaluating many of the cited supervised learning-based systems. Due to its size, availability, reliability of annotation, 3 Task Description The goal of this shared task is to detect, at the word level, all metaphors in a given text. Specifically, there are two tracks,"
W18-0907,W18-0911,0,0.541678,"generate the lemma of each word according to its tagged POS. As Baseline 2, we use the best system from Beigman Klebanov et al. (2016). The features are: lemmatized unigrams, generalized WordNet semantic classes, and difference in concreteness ratings between verbs/adjectives and nouns (UL + WordNet + CCDB).5 4.1 nsu ai (Mosolova et al., 2018) used linguistic features based on unigrams, lemmas, POS tags, topical LDAs, concreteness, WordNet, VerbNet and verb clusters and trained a Conditional Random Field (CRF) model with gradient descent using the L-BFGS method to generate predictions. OCOTA (Bizzoni and Ghanimifard, 2018) experimented with a deep neural network composed of a Bi-LSTM preceded and followed by fully connected layers, as well as a simpler model that has a sequence of fully connected neural networks. The authors also experiment with word embeddings trained on various data, with explicit features based on concreteness, and with preprocessing that addresses variability in sentence length. The authors observe that a model that combines Bi-LSTM with the explicit features and sentence-length manipulation shows the best performance. The authors also show that an ensemble of the two types of neural models"
W18-0907,E17-2084,1,0.829013,"ed task. 2 Related Work Over the last decade, automated detection of metaphor has become an increasingly popular topic, which manifests itself in both a variety of approaches and in an increasing variety of data to which the methods are applied. In terms of methods, approaches based on feature-engineering in a supervised machine learning paradigm explored features based on concreteness and imageability, semantic classification using WordNet, FrameNet, VerbNet, SUMO ontology, property norms, and distributional semantic models, syntactic dependency patterns, sensorial and vision-based features (Bulat et al., 2017; K¨oper and im Walde, 2017; Gutierrez et al., 2016; Shutova et al., 2016; Beigman Klebanov et al., 2016; Tekiroglu et al., 2015; Tsvetkov et al., 2014; Beigman Klebanov et al., 2014; Dunn, 2013; Neuman et al., 2013; Mohler et al., 2013; Hovy et al., 2013; Tsvetkov et al., 2013; Turney et al., 2011; Shutova et al., 2010; Gedigian et al., 2006); see Shutova et al. (2017) and Veale et al. (2016) for reviews of supervised as well as semi-supervised and unsupervised approaches. Recently, deep learning methods have been explored for token-level metaphor detection (Rei et al., 2017; Gutierrez et al."
W18-0907,W17-1903,0,0.571942,"Missing"
W18-0907,W16-1104,0,0.143776,"d im Walde, 2017; Gutierrez et al., 2016; Shutova et al., 2016; Beigman Klebanov et al., 2016; Tekiroglu et al., 2015; Tsvetkov et al., 2014; Beigman Klebanov et al., 2014; Dunn, 2013; Neuman et al., 2013; Mohler et al., 2013; Hovy et al., 2013; Tsvetkov et al., 2013; Turney et al., 2011; Shutova et al., 2010; Gedigian et al., 2006); see Shutova et al. (2017) and Veale et al. (2016) for reviews of supervised as well as semi-supervised and unsupervised approaches. Recently, deep learning methods have been explored for token-level metaphor detection (Rei et al., 2017; Gutierrez et al., 2017; Do Dinh and Gurevych, 2016). As discussed later in the paper later, the fact that all but one of the participating teams for the shared task experimented with neural network architectures testifies to the increasing popularity of this modeling approach. In terms of data used for evaluating metaphor detection systems, researchers used specially constructed or selected sets, such as adjective noun pairs (Gutierrez et al., 2016; Tsvetkov et al., 2014), WordNet synsets and glosses (Mohammad et al., 2016), annotated lexical items (from a range of word classes) in sentences sampled from cor¨ pora (Ozbal et al., 2016; Jang et"
W18-0907,W13-0901,0,0.0396711,"y of data to which the methods are applied. In terms of methods, approaches based on feature-engineering in a supervised machine learning paradigm explored features based on concreteness and imageability, semantic classification using WordNet, FrameNet, VerbNet, SUMO ontology, property norms, and distributional semantic models, syntactic dependency patterns, sensorial and vision-based features (Bulat et al., 2017; K¨oper and im Walde, 2017; Gutierrez et al., 2016; Shutova et al., 2016; Beigman Klebanov et al., 2016; Tekiroglu et al., 2015; Tsvetkov et al., 2014; Beigman Klebanov et al., 2014; Dunn, 2013; Neuman et al., 2013; Mohler et al., 2013; Hovy et al., 2013; Tsvetkov et al., 2013; Turney et al., 2011; Shutova et al., 2010; Gedigian et al., 2006); see Shutova et al. (2017) and Veale et al. (2016) for reviews of supervised as well as semi-supervised and unsupervised approaches. Recently, deep learning methods have been explored for token-level metaphor detection (Rei et al., 2017; Gutierrez et al., 2017; Do Dinh and Gurevych, 2016). As discussed later in the paper later, the fact that all but one of the participating teams for the shared task experimented with neural network architecture"
W18-0907,W06-3506,0,0.0195224,"aradigm explored features based on concreteness and imageability, semantic classification using WordNet, FrameNet, VerbNet, SUMO ontology, property norms, and distributional semantic models, syntactic dependency patterns, sensorial and vision-based features (Bulat et al., 2017; K¨oper and im Walde, 2017; Gutierrez et al., 2016; Shutova et al., 2016; Beigman Klebanov et al., 2016; Tekiroglu et al., 2015; Tsvetkov et al., 2014; Beigman Klebanov et al., 2014; Dunn, 2013; Neuman et al., 2013; Mohler et al., 2013; Hovy et al., 2013; Tsvetkov et al., 2013; Turney et al., 2011; Shutova et al., 2010; Gedigian et al., 2006); see Shutova et al. (2017) and Veale et al. (2016) for reviews of supervised as well as semi-supervised and unsupervised approaches. Recently, deep learning methods have been explored for token-level metaphor detection (Rei et al., 2017; Gutierrez et al., 2017; Do Dinh and Gurevych, 2016). As discussed later in the paper later, the fact that all but one of the participating teams for the shared task experimented with neural network architectures testifies to the increasing popularity of this modeling approach. In terms of data used for evaluating metaphor detection systems, researchers used s"
W18-0907,S16-2003,1,0.487324,"ep learning methods have been explored for token-level metaphor detection (Rei et al., 2017; Gutierrez et al., 2017; Do Dinh and Gurevych, 2016). As discussed later in the paper later, the fact that all but one of the participating teams for the shared task experimented with neural network architectures testifies to the increasing popularity of this modeling approach. In terms of data used for evaluating metaphor detection systems, researchers used specially constructed or selected sets, such as adjective noun pairs (Gutierrez et al., 2016; Tsvetkov et al., 2014), WordNet synsets and glosses (Mohammad et al., 2016), annotated lexical items (from a range of word classes) in sentences sampled from cor¨ pora (Ozbal et al., 2016; Jang et al., 2015; Hovy et al., 2013; Birke and Sarkar, 2006), all the way to annotation of all words in running text for metaphoricity (Beigman Klebanov et al., 2018; Steen et al., 2010); Veale et al. (2016) review additional annotated datasets. By far the largest annotated dataset is the VU Amsterdam Metaphor Corpus; it has also been used for evaluating many of the cited supervised learning-based systems. Due to its size, availability, reliability of annotation, 3 Task Descriptio"
W18-0907,W13-0904,0,0.0694103,"e applied. In terms of methods, approaches based on feature-engineering in a supervised machine learning paradigm explored features based on concreteness and imageability, semantic classification using WordNet, FrameNet, VerbNet, SUMO ontology, property norms, and distributional semantic models, syntactic dependency patterns, sensorial and vision-based features (Bulat et al., 2017; K¨oper and im Walde, 2017; Gutierrez et al., 2016; Shutova et al., 2016; Beigman Klebanov et al., 2016; Tekiroglu et al., 2015; Tsvetkov et al., 2014; Beigman Klebanov et al., 2014; Dunn, 2013; Neuman et al., 2013; Mohler et al., 2013; Hovy et al., 2013; Tsvetkov et al., 2013; Turney et al., 2011; Shutova et al., 2010; Gedigian et al., 2006); see Shutova et al. (2017) and Veale et al. (2016) for reviews of supervised as well as semi-supervised and unsupervised approaches. Recently, deep learning methods have been explored for token-level metaphor detection (Rei et al., 2017; Gutierrez et al., 2017; Do Dinh and Gurevych, 2016). As discussed later in the paper later, the fact that all but one of the participating teams for the shared task experimented with neural network architectures testifies to the increasing popularity o"
W18-0907,W18-0915,0,0.0237534,"o logistic regression classifiers for each track (Verbs and All-POS), with instance weights inversely proportional to class frequencies. Lemmatized unigrams (UL) is a simple yet fairly strong baseline (Baseline 1). This feature is produced using NLTK (Bird and Loper, 2004) to generate the lemma of each word according to its tagged POS. As Baseline 2, we use the best system from Beigman Klebanov et al. (2016). The features are: lemmatized unigrams, generalized WordNet semantic classes, and difference in concreteness ratings between verbs/adjectives and nouns (UL + WordNet + CCDB).5 4.1 nsu ai (Mosolova et al., 2018) used linguistic features based on unigrams, lemmas, POS tags, topical LDAs, concreteness, WordNet, VerbNet and verb clusters and trained a Conditional Random Field (CRF) model with gradient descent using the L-BFGS method to generate predictions. OCOTA (Bizzoni and Ghanimifard, 2018) experimented with a deep neural network composed of a Bi-LSTM preceded and followed by fully connected layers, as well as a simpler model that has a sequence of fully connected neural networks. The authors also experiment with word embeddings trained on various data, with explicit features based on concreteness,"
W18-0907,W18-0918,0,0.0701424,"authors also experiment with word embeddings trained on various data, with explicit features based on concreteness, and with preprocessing that addresses variability in sentence length. The authors observe that a model that combines Bi-LSTM with the explicit features and sentence-length manipulation shows the best performance. The authors also show that an ensemble of the two types of neural models works even better, due to a substantial increase in recall over single models. System Descriptions The best-performing system from each participant is described below, in alphabetic order. bot.zen (Stemle and Onysko, 2018) used word embeddings from different standard corpora representing different levels of language mastery, encoding each word in a sentence into multiple vector-based embeddings which are then fed into an LSTM RNN network architecture. Specifically, the backpropagation step was performed using weightings computed based on the logarithmic function of the inverse of the count of the metaphors and non-metaphors. Their implementation is hosted on Github6 under the Apache License Version 2.0. Samsung RD PL (Skurniak et al., 2018) explored the use of several orthogonal resources in a cascading manner"
W18-0907,W18-0914,0,0.198339,"ithub6 under the Apache License Version 2.0. Samsung RD PL (Skurniak et al., 2018) explored the use of several orthogonal resources in a cascading manner to predict metaphoricity. For a given word in a sentence, they extracted three feature sets: concreteness score from the Brysbaert database, intermediate hidden vector representing the word in a neural translation framework, and generated logits of a CRF sequence tagging model trained using word embeddings and contextual information. Trained on the VUA data, the CRF model alone outperforms that of a GRU taking all three features. DeepReader (Swarnkar and Singh, 2018) The authors present a neural network architecture that concatenates hidden states of forward and backward LSTMs, with feature selection and classification. The authors also show that reweighting examples and adding linguistic features (WordNet, POS, concreteness) helps improve performance further. 5 6 THU NGN (Wu et al., 2018) created word embeddings using a pre-trained word2vec model and added features such as embedding clusterings and POS tags before using CNN and Baseline 2 is “all-16” in Beigman Klebanov et al. (2018). https://github.com/bot-zen/naacl flp st 59 two introduce explicit ling"
W18-0907,W15-1404,0,0.705695,"h manifests itself in both a variety of approaches and in an increasing variety of data to which the methods are applied. In terms of methods, approaches based on feature-engineering in a supervised machine learning paradigm explored features based on concreteness and imageability, semantic classification using WordNet, FrameNet, VerbNet, SUMO ontology, property norms, and distributional semantic models, syntactic dependency patterns, sensorial and vision-based features (Bulat et al., 2017; K¨oper and im Walde, 2017; Gutierrez et al., 2016; Shutova et al., 2016; Beigman Klebanov et al., 2016; Tekiroglu et al., 2015; Tsvetkov et al., 2014; Beigman Klebanov et al., 2014; Dunn, 2013; Neuman et al., 2013; Mohler et al., 2013; Hovy et al., 2013; Tsvetkov et al., 2013; Turney et al., 2011; Shutova et al., 2010; Gedigian et al., 2006); see Shutova et al. (2017) and Veale et al. (2016) for reviews of supervised as well as semi-supervised and unsupervised approaches. Recently, deep learning methods have been explored for token-level metaphor detection (Rei et al., 2017; Gutierrez et al., 2017; Do Dinh and Gurevych, 2016). As discussed later in the paper later, the fact that all but one of the participating teams"
W18-0907,L16-1600,0,0.0452044,"Do Dinh and Gurevych, 2016). As discussed later in the paper later, the fact that all but one of the participating teams for the shared task experimented with neural network architectures testifies to the increasing popularity of this modeling approach. In terms of data used for evaluating metaphor detection systems, researchers used specially constructed or selected sets, such as adjective noun pairs (Gutierrez et al., 2016; Tsvetkov et al., 2014), WordNet synsets and glosses (Mohammad et al., 2016), annotated lexical items (from a range of word classes) in sentences sampled from cor¨ pora (Ozbal et al., 2016; Jang et al., 2015; Hovy et al., 2013; Birke and Sarkar, 2006), all the way to annotation of all words in running text for metaphoricity (Beigman Klebanov et al., 2018; Steen et al., 2010); Veale et al. (2016) review additional annotated datasets. By far the largest annotated dataset is the VU Amsterdam Metaphor Corpus; it has also been used for evaluating many of the cited supervised learning-based systems. Due to its size, availability, reliability of annotation, 3 Task Description The goal of this shared task is to detect, at the word level, all metaphors in a given text. Specifically, the"
W18-0907,W18-0908,0,0.0535889,"s can refer to the Testing phase In this phase, instances for evaluation are released.2 Each participating system generated predictions for the test instances, for up to N models.3 Predictions are submitted to CodaLab4 1 https://github.com/EducationalTestingService/metaphor /tree/master/NAACL-FLP-shared-task 2 In principle, participants could have access to the test data by independently obtaining the VUA corpus. The shared task was based on a presumption of fair play by participants. 3 We set N =12. 4 https://competitions.codalab.org/competitions/17805 58 teams’ papers for more details. MAP (Pramanick et al., 2018) used a hybrid architecture of Bi-directional LSTM and Conditional Random Fields (CRF) for metaphor detection, relying on features such as token, lemma and POS, and using word2vec embeddings trained on English Wikipedia. Specifically, the authors considered contextual information within a sentence for generating predictions. Baseline Classifiers We make available to shared task participants a number of features from prior published work on metaphor detection, including unigram features, features based on WordNet, VerbNet, and those derived from a distributional semantic model, POS-based, concr"
W18-0907,P14-1024,0,0.524498,"th a variety of approaches and in an increasing variety of data to which the methods are applied. In terms of methods, approaches based on feature-engineering in a supervised machine learning paradigm explored features based on concreteness and imageability, semantic classification using WordNet, FrameNet, VerbNet, SUMO ontology, property norms, and distributional semantic models, syntactic dependency patterns, sensorial and vision-based features (Bulat et al., 2017; K¨oper and im Walde, 2017; Gutierrez et al., 2016; Shutova et al., 2016; Beigman Klebanov et al., 2016; Tekiroglu et al., 2015; Tsvetkov et al., 2014; Beigman Klebanov et al., 2014; Dunn, 2013; Neuman et al., 2013; Mohler et al., 2013; Hovy et al., 2013; Tsvetkov et al., 2013; Turney et al., 2011; Shutova et al., 2010; Gedigian et al., 2006); see Shutova et al. (2017) and Veale et al. (2016) for reviews of supervised as well as semi-supervised and unsupervised approaches. Recently, deep learning methods have been explored for token-level metaphor detection (Rei et al., 2017; Gutierrez et al., 2017; Do Dinh and Gurevych, 2016). As discussed later in the paper later, the fact that all but one of the participating teams for the shared task ex"
W18-0907,D17-1162,1,0.780118,"based features (Bulat et al., 2017; K¨oper and im Walde, 2017; Gutierrez et al., 2016; Shutova et al., 2016; Beigman Klebanov et al., 2016; Tekiroglu et al., 2015; Tsvetkov et al., 2014; Beigman Klebanov et al., 2014; Dunn, 2013; Neuman et al., 2013; Mohler et al., 2013; Hovy et al., 2013; Tsvetkov et al., 2013; Turney et al., 2011; Shutova et al., 2010; Gedigian et al., 2006); see Shutova et al. (2017) and Veale et al. (2016) for reviews of supervised as well as semi-supervised and unsupervised approaches. Recently, deep learning methods have been explored for token-level metaphor detection (Rei et al., 2017; Gutierrez et al., 2017; Do Dinh and Gurevych, 2016). As discussed later in the paper later, the fact that all but one of the participating teams for the shared task experimented with neural network architectures testifies to the increasing popularity of this modeling approach. In terms of data used for evaluating metaphor detection systems, researchers used specially constructed or selected sets, such as adjective noun pairs (Gutierrez et al., 2016; Tsvetkov et al., 2014), WordNet synsets and glosses (Mohammad et al., 2016), annotated lexical items (from a range of word classes) in sentences"
W18-0907,W13-0906,0,0.123869,"es based on feature-engineering in a supervised machine learning paradigm explored features based on concreteness and imageability, semantic classification using WordNet, FrameNet, VerbNet, SUMO ontology, property norms, and distributional semantic models, syntactic dependency patterns, sensorial and vision-based features (Bulat et al., 2017; K¨oper and im Walde, 2017; Gutierrez et al., 2016; Shutova et al., 2016; Beigman Klebanov et al., 2016; Tekiroglu et al., 2015; Tsvetkov et al., 2014; Beigman Klebanov et al., 2014; Dunn, 2013; Neuman et al., 2013; Mohler et al., 2013; Hovy et al., 2013; Tsvetkov et al., 2013; Turney et al., 2011; Shutova et al., 2010; Gedigian et al., 2006); see Shutova et al. (2017) and Veale et al. (2016) for reviews of supervised as well as semi-supervised and unsupervised approaches. Recently, deep learning methods have been explored for token-level metaphor detection (Rei et al., 2017; Gutierrez et al., 2017; Do Dinh and Gurevych, 2016). As discussed later in the paper later, the fact that all but one of the participating teams for the shared task experimented with neural network architectures testifies to the increasing popularity of this modeling approach. In terms of data"
W18-0907,N16-1020,1,0.838762,"taphor has become an increasingly popular topic, which manifests itself in both a variety of approaches and in an increasing variety of data to which the methods are applied. In terms of methods, approaches based on feature-engineering in a supervised machine learning paradigm explored features based on concreteness and imageability, semantic classification using WordNet, FrameNet, VerbNet, SUMO ontology, property norms, and distributional semantic models, syntactic dependency patterns, sensorial and vision-based features (Bulat et al., 2017; K¨oper and im Walde, 2017; Gutierrez et al., 2016; Shutova et al., 2016; Beigman Klebanov et al., 2016; Tekiroglu et al., 2015; Tsvetkov et al., 2014; Beigman Klebanov et al., 2014; Dunn, 2013; Neuman et al., 2013; Mohler et al., 2013; Hovy et al., 2013; Tsvetkov et al., 2013; Turney et al., 2011; Shutova et al., 2010; Gedigian et al., 2006); see Shutova et al. (2017) and Veale et al. (2016) for reviews of supervised as well as semi-supervised and unsupervised approaches. Recently, deep learning methods have been explored for token-level metaphor detection (Rei et al., 2017; Gutierrez et al., 2017; Do Dinh and Gurevych, 2016). As discussed later in the paper late"
W18-0907,D11-1063,0,0.370862,"ineering in a supervised machine learning paradigm explored features based on concreteness and imageability, semantic classification using WordNet, FrameNet, VerbNet, SUMO ontology, property norms, and distributional semantic models, syntactic dependency patterns, sensorial and vision-based features (Bulat et al., 2017; K¨oper and im Walde, 2017; Gutierrez et al., 2016; Shutova et al., 2016; Beigman Klebanov et al., 2016; Tekiroglu et al., 2015; Tsvetkov et al., 2014; Beigman Klebanov et al., 2014; Dunn, 2013; Neuman et al., 2013; Mohler et al., 2013; Hovy et al., 2013; Tsvetkov et al., 2013; Turney et al., 2011; Shutova et al., 2010; Gedigian et al., 2006); see Shutova et al. (2017) and Veale et al. (2016) for reviews of supervised as well as semi-supervised and unsupervised approaches. Recently, deep learning methods have been explored for token-level metaphor detection (Rei et al., 2017; Gutierrez et al., 2017; Do Dinh and Gurevych, 2016). As discussed later in the paper later, the fact that all but one of the participating teams for the shared task experimented with neural network architectures testifies to the increasing popularity of this modeling approach. In terms of data used for evaluating"
W18-0907,J17-1003,1,0.902706,"Missing"
W18-0907,C10-1113,1,0.812699,"sed machine learning paradigm explored features based on concreteness and imageability, semantic classification using WordNet, FrameNet, VerbNet, SUMO ontology, property norms, and distributional semantic models, syntactic dependency patterns, sensorial and vision-based features (Bulat et al., 2017; K¨oper and im Walde, 2017; Gutierrez et al., 2016; Shutova et al., 2016; Beigman Klebanov et al., 2016; Tekiroglu et al., 2015; Tsvetkov et al., 2014; Beigman Klebanov et al., 2014; Dunn, 2013; Neuman et al., 2013; Mohler et al., 2013; Hovy et al., 2013; Tsvetkov et al., 2013; Turney et al., 2011; Shutova et al., 2010; Gedigian et al., 2006); see Shutova et al. (2017) and Veale et al. (2016) for reviews of supervised as well as semi-supervised and unsupervised approaches. Recently, deep learning methods have been explored for token-level metaphor detection (Rei et al., 2017; Gutierrez et al., 2017; Do Dinh and Gurevych, 2016). As discussed later in the paper later, the fact that all but one of the participating teams for the shared task experimented with neural network architectures testifies to the increasing popularity of this modeling approach. In terms of data used for evaluating metaphor detection sys"
