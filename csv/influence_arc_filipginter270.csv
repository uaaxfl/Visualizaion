2020.iwpt-1.17,S15-2153,0,0.132715,"Missing"
2020.iwpt-1.17,K17-3001,1,0.866065,"Missing"
2020.lrec-1.470,P19-1309,0,0.0701269,"guages. Further, typically only a small fraction of sentences can be assumed to have a translation counterpart in such data. These methods allow us to utilize also text sources inaccessible to methods that assume a prior document-level alignment. Bitext mining from incomparable corpora relies on crosslingual sentence embeddings constructed such that sentences from different languages are embedded into a single vector space, allowing their subsequent comparison using, for instance, the cosine similarity measure. Here we apply the LASER embeddings (Language Agnostic SEntence Representations) of Artetxe and Schwenk (2019b). This approach is based on an encoder-decoder architecture, with a shared sentence encoder, and a set of language-specific decoders. The encoder and decoders are trained using existing parallel data and the shared encoder can subsequently data set Web crawl ¨ crawl FISKMO YLE News Archive YLE News RSS Total gold 500K 85K 140K 25K 750K size silver mono (fi/sv) 2M 54M/90M 130K 4.3M/1.3M 300K 13.9M/4.5M 100K 1.4M/1.1M 2.5M 74M/97M Table 1: The number perfect or near-perfect (Gold) and partial translations or highly related (Silver) translation pairs extracted from different data collections. S"
2020.lrec-1.470,Q19-1038,0,0.101553,"guages. Further, typically only a small fraction of sentences can be assumed to have a translation counterpart in such data. These methods allow us to utilize also text sources inaccessible to methods that assume a prior document-level alignment. Bitext mining from incomparable corpora relies on crosslingual sentence embeddings constructed such that sentences from different languages are embedded into a single vector space, allowing their subsequent comparison using, for instance, the cosine similarity measure. Here we apply the LASER embeddings (Language Agnostic SEntence Representations) of Artetxe and Schwenk (2019b). This approach is based on an encoder-decoder architecture, with a shared sentence encoder, and a set of language-specific decoders. The encoder and decoders are trained using existing parallel data and the shared encoder can subsequently data set Web crawl ¨ crawl FISKMO YLE News Archive YLE News RSS Total gold 500K 85K 140K 25K 750K size silver mono (fi/sv) 2M 54M/90M 130K 4.3M/1.3M 300K 13.9M/4.5M 100K 1.4M/1.1M 2.5M 74M/97M Table 1: The number perfect or near-perfect (Gold) and partial translations or highly related (Silver) translation pairs extracted from different data collections. S"
2020.lrec-1.470,W19-6146,1,0.800955,"will return an aligned corpus in TMX format that can directly be useful in translation workflows and that we apply data pre-procesing and cleaning pipelines that help further data curation. We accept a variety of formats including websites, PDFs, Microsoft Word document to name a few. We also ask the general public to suggest sources like translated websites or to send translated documents through our public translation tool (see Figure 2). This is connected to a data crunching backend that runs data conversion, text extraction and alignment jobs. More details about that system are given in (Aulamo and Tiedemann, 2019). One common concern of potential data providers is related to privacy issues as discussed earlier. Some data is too sensitive to be published for general use. However, such data may still be used in model development and internal research. We offer different levels of privacy according to the choice of the provider. The data can be public, used internally in training public MT and language models or it can be used for training customized MT only for the provider itself. The main principles are illustrated in Figure 1. 2.2. Sentence-level Bitext Mining Recently, several methods have been propo"
2020.lrec-1.470,W18-6317,0,0.0116371,"ata is too sensitive to be published for general use. However, such data may still be used in model development and internal research. We offer different levels of privacy according to the choice of the provider. The data can be public, used internally in training public MT and language models or it can be used for training customized MT only for the provider itself. The main principles are illustrated in Figure 1. 2.2. Sentence-level Bitext Mining Recently, several methods have been proposed for bitext mining from incomparable, monolingual corpora (see for example (Espana-Bonet et al., 2017; Guo et al., 2018; Schwenk, 2018)). Such corpora, most typically resulting from web crawls, contain no metadata information that would allow direct linking of sentences, or at least documents across the languages. Further, typically only a small fraction of sentences can be assumed to have a translation counterpart in such data. These methods allow us to utilize also text sources inaccessible to methods that assume a prior document-level alignment. Bitext mining from incomparable corpora relies on crosslingual sentence embeddings constructed such that sentences from different languages are embedded into a sing"
2020.lrec-1.470,P18-4020,0,0.0267722,"Missing"
2020.lrec-1.470,2016.amta-researchers.9,0,0.0132079,"benefit is that the confidentiality and security of the MT system can be absolutely guaranteed. The risk posed by outages or discontinuation of external services is also eliminated. The deployment of the plugin is also simple, and can be performed by the end user. ¨ MT is also free to use, unlike most online MT FISKMO services intended for professional translation. From a technical point of view, the offline implementation ¨ plugin offers a promising basis for future of the FISKMO development of interactive MT capabilities that require low latency, such as interactive translation prediction (Knowles and Koehn, 2016). In an online MT system, these features are constrained by the latency of the two-way communication between the server and the client. Of course, an offline system is constrained by the available hardware, but the hardware constraint is not fixed like network latency, as it can be alleviated by optimization and design. A major motivation for creating the plugin was to generate interest in the data collection part of the project by offering potential partners a clear way to benefit from the ¨ project. This goal has been achieved, since the FISKMO MT plugin for SDL Trados Studio has already gen"
2020.lrec-1.470,P07-2045,0,0.00724838,"this, we apply the data we have collected in OPUS and the ¨ project and train state-of-the-art neural machine FISKMO translation (NMT) models using the popular transformer model (Vaswani et al., 2017) as implemented in MarianNMT (Junczys-Dowmunt et al., 2018). The training data comprises roughly 33 million training examples with about 1 billion tokens (counting both languages together). The data is derived from a wide mix of sources ranging from legislative texts to translated movie subtitles, software localization and general web content. We pre-process the data with the common Moses tools (Koehn et al., 2007) applying Unicode character normalization, corpus cleaning tools and the Moses tokenizer for Finnish and Swedish. Furthermore, we perform BPE-based subword segmentation using the Subword NMT package9 (Sennrich et al., 2016) with BPE models trained separately for each language and setting the number of merging operations to 32,000. For training the MT models we apply standard settings of a 6-layer transformer model (in both, encoder and decoder) with 8 self attention heads per layer, tied embed6 http://opus.nlpl.eu http://opus.nlpl.eu/bin/opuscqp.pl 8 https://version.helsinki.fi/Helsinki-NLP/fi"
2020.lrec-1.470,W15-2124,1,0.766044,"is computationally demanding, the highly optimized FAISS library of Johnson et al. (2019) is typically applied to carry the comparison out efficiently. Recently, this method was applied to Wikipedia, obtaining 135M parallel sentences for 85 languages (Schwenk et al., 2019a) and to CommonCrawl web crawl data, obtaining 3.5B parallel sentences for 38 languages (Schwenk et al., 2019b). Here, we use the LASER+FAISS method to extract Finnish-Swedish parallel data from several monolingual corpus pairs, whose sizes are summarized in Table 1. For web crawl data, we use the Finnish Internet Parsebank (Luotolahti et al., 2015), a large-scale dedicated Finnish web crawl corpus, and the Swedish section of the CoNLL-17 Shared Task raw data (Ginter et al., 2017), based on CommonCrawl. The news data are sourced from the Finnish national broadcast organization YLE 2011–2018 archive available through the Language Bank of Finland (YLE news archive) (Yleisradio, 2019b; Yleisradio, 2019a), and from the RSS feed of the Finnish and Swedish language YLE news (YLE News RSS), gathered during 2018–2019. Finally, we crawled web pages of various government organizations (FISKMO¨ crawl). For each corpus pair, we manually evaluated a"
2020.lrec-1.470,P02-1040,0,0.106958,"ems that support the translation between Finnish and Swedish, namely Google Translate15 and Presidency MT16 . The latter has been released in connection with the EU presidency of Finland and it has been heavily optimized for the translation from Finnish into English and Swedish. It is developed by Tilde, a Latvian language service provider, in collaboration with the Prime Minister’s Office of Finland using large data sets collected from European resources and data provided by the Finnish authorities. Both on-line systems have been accessed on October 29, 2019 and the results in terms of BLEU (Papineni et al., 2002) and chr-F2 (Popovi´c, 2015) scores are shown in Table 3. From the results, we can see that our system fairs quite well in comparison to both on-line translation engines. Note that it is not optimised for the data set in any way and trained without any fine-tuning for any specific domain. The advantage over Google Translate is astonishing showing that uncommon language pairs are still not well supported by general-purpose engines. In comparison to the Presidency MT engine, our system performs on a similar scale, slightly better for the translation into Finnish but worse in the other directions"
2020.lrec-1.470,W15-3049,0,0.0666906,"Missing"
2020.lrec-1.470,W18-6319,0,0.0340736,"Missing"
2020.lrec-1.470,P18-2037,0,0.0202527,"ve to be published for general use. However, such data may still be used in model development and internal research. We offer different levels of privacy according to the choice of the provider. The data can be public, used internally in training public MT and language models or it can be used for training customized MT only for the provider itself. The main principles are illustrated in Figure 1. 2.2. Sentence-level Bitext Mining Recently, several methods have been proposed for bitext mining from incomparable, monolingual corpora (see for example (Espana-Bonet et al., 2017; Guo et al., 2018; Schwenk, 2018)). Such corpora, most typically resulting from web crawls, contain no metadata information that would allow direct linking of sentences, or at least documents across the languages. Further, typically only a small fraction of sentences can be assumed to have a translation counterpart in such data. These methods allow us to utilize also text sources inaccessible to methods that assume a prior document-level alignment. Bitext mining from incomparable corpora relies on crosslingual sentence embeddings constructed such that sentences from different languages are embedded into a single vector space,"
2020.lrec-1.470,P16-1162,0,0.0120622,"MarianNMT (Junczys-Dowmunt et al., 2018). The training data comprises roughly 33 million training examples with about 1 billion tokens (counting both languages together). The data is derived from a wide mix of sources ranging from legislative texts to translated movie subtitles, software localization and general web content. We pre-process the data with the common Moses tools (Koehn et al., 2007) applying Unicode character normalization, corpus cleaning tools and the Moses tokenizer for Finnish and Swedish. Furthermore, we perform BPE-based subword segmentation using the Subword NMT package9 (Sennrich et al., 2016) with BPE models trained separately for each language and setting the number of merging operations to 32,000. For training the MT models we apply standard settings of a 6-layer transformer model (in both, encoder and decoder) with 8 self attention heads per layer, tied embed6 http://opus.nlpl.eu http://opus.nlpl.eu/bin/opuscqp.pl 8 https://version.helsinki.fi/Helsinki-NLP/fiskmo 9 https://github.com/rsennrich/subword-nmt 7 4 size 199K 322K 473K https://github.com/danielvarga/hunalign https://github.com/Helsinki-NLP/uplug 3811 ¨ translation interface. Figure 2: The public FISKMO dings and a sha"
2020.lrec-1.470,K17-3009,0,0.0645588,"Missing"
2020.lrec-1.497,de-marneffe-etal-2006-generating,1,\N,Missing
2020.lrec-1.497,zeman-2008-reusable,1,\N,Missing
2020.lrec-1.497,de-marneffe-etal-2014-universal,1,\N,Missing
2020.lrec-1.497,W08-1301,1,\N,Missing
2020.lrec-1.497,petrov-etal-2012-universal,0,\N,Missing
2020.lrec-1.497,P13-1051,1,\N,Missing
2020.lrec-1.497,P15-2111,0,\N,Missing
2020.lrec-1.497,L16-1376,1,\N,Missing
2020.lrec-1.497,L16-1262,1,\N,Missing
2020.lrec-1.497,W18-6012,1,\N,Missing
2021.motra-1.11,N19-1131,0,0.0528733,"Missing"
2021.nodalida-main.1,W16-2501,1,0.885859,"Missing"
2021.nodalida-main.1,2020.acl-main.747,0,0.122644,"Missing"
2021.nodalida-main.1,N19-1423,0,0.0391996,"er open licenses from https://github.com/ turkunlp/wikibert . 1 Introduction Transfer learning using language models pretrained on large unannotated corpora has allowed for substantial recent advances at a broad range of natural language processing (NLP) tasks. By contrast to earlier distributional semantics approaches such as random indexing (Kanerva et al., 2000) and context-independent neural approaches such as word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014), models such as ULMFiT (Howard and Ruder, 2018), ELMo (Peters et al., 2018), GPT (Radford et al., 2018) and BERT (Devlin et al., 2019) create contextualized representations of meaning, capable of providing both contextualized word embeddings as well as embeddings for text segments longer than words. Recent pre-trained neural language models have been rapidly advancing the state of the art in a range of natural language understanding and NLP tasks (Wang et al., 2018, 2019; Strakov´a et al., 2019; Kondratyuk and Straka, 2019). The transformer architecture (Vaswani et al., 2017) and the BERT language model of Devlin et al. (2019) have been particularly influential, with transformer-based models in general and BERT in particular"
2021.nodalida-main.1,2020.findings-emnlp.387,1,0.690183,"at all. We introduce a fully automated pipeline for creating languagespecific BERT models from Wikipedia data and apply this pipeline to create 42 new such models. 2 Related work Considerable recent effort by various groups has focused on introducing dedicated BERT models covering single languages or a small number of (often closely related) languages. Dedicated monolingual models include e.g. BERTje1 (de Vries et al., 2019) for Dutch, CamemBERT2 (Martin et al., 2020) for French, FinBERT3 (Virtanen et al., 2019) for Finnish, RuBERT4 (Kuratov and Arkhipov, 2019) for Russian, and Romanian BERT (Dumitrescu et al., 2020); more focused multilingual models include e.g. the bilingual Finnish-English model of Chang et al. (2020) and the trilingual Finnish-Estonian-English and Croatian-Slovenian-English models of Ulˇcar and ˇ Robnik-Sikonja (2020). Many of these studies have demonstrated the newly introduced models to allow for substantial improvements over mBERT in various languagespecific downstream task evaluations, thus supporting the continued value of creating monolingual and focused multilingual models. However, these efforts still cover only a fairly limited number of languages, and do not offer a straight"
2021.nodalida-main.1,P18-1031,0,0.0201775,", but decreases for others. All of the resources introduced in this work are available under open licenses from https://github.com/ turkunlp/wikibert . 1 Introduction Transfer learning using language models pretrained on large unannotated corpora has allowed for substantial recent advances at a broad range of natural language processing (NLP) tasks. By contrast to earlier distributional semantics approaches such as random indexing (Kanerva et al., 2000) and context-independent neural approaches such as word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014), models such as ULMFiT (Howard and Ruder, 2018), ELMo (Peters et al., 2018), GPT (Radford et al., 2018) and BERT (Devlin et al., 2019) create contextualized representations of meaning, capable of providing both contextualized word embeddings as well as embeddings for text segments longer than words. Recent pre-trained neural language models have been rapidly advancing the state of the art in a range of natural language understanding and NLP tasks (Wang et al., 2018, 2019; Strakov´a et al., 2019; Kondratyuk and Straka, 2019). The transformer architecture (Vaswani et al., 2017) and the BERT language model of Devlin et al. (2019) have been pa"
2021.nodalida-main.1,D19-1279,0,0.0571478,"dent neural approaches such as word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014), models such as ULMFiT (Howard and Ruder, 2018), ELMo (Peters et al., 2018), GPT (Radford et al., 2018) and BERT (Devlin et al., 2019) create contextualized representations of meaning, capable of providing both contextualized word embeddings as well as embeddings for text segments longer than words. Recent pre-trained neural language models have been rapidly advancing the state of the art in a range of natural language understanding and NLP tasks (Wang et al., 2018, 2019; Strakov´a et al., 2019; Kondratyuk and Straka, 2019). The transformer architecture (Vaswani et al., 2017) and the BERT language model of Devlin et al. (2019) have been particularly influential, with transformer-based models in general and BERT in particular fuelling a broad range of advances and serving as the basis of many recent studies of neural language models (e.g. Lan et al., 2019; Liu et al., 2019; Sanh et al., 2019). As is the case for most studies on new deep neural language models, the original study introducing BERT addressed only English. The authors later released a Chinese model as well as a multilingual model, mBERT, trained on t"
2021.nodalida-main.1,D18-2012,0,0.0739757,"Missing"
2021.nodalida-main.1,2020.lrec-1.497,1,0.89205,"Missing"
2021.nodalida-main.1,D14-1162,0,0.0887301,"tantially improved performance for some languages, but decreases for others. All of the resources introduced in this work are available under open licenses from https://github.com/ turkunlp/wikibert . 1 Introduction Transfer learning using language models pretrained on large unannotated corpora has allowed for substantial recent advances at a broad range of natural language processing (NLP) tasks. By contrast to earlier distributional semantics approaches such as random indexing (Kanerva et al., 2000) and context-independent neural approaches such as word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014), models such as ULMFiT (Howard and Ruder, 2018), ELMo (Peters et al., 2018), GPT (Radford et al., 2018) and BERT (Devlin et al., 2019) create contextualized representations of meaning, capable of providing both contextualized word embeddings as well as embeddings for text segments longer than words. Recent pre-trained neural language models have been rapidly advancing the state of the art in a range of natural language understanding and NLP tasks (Wang et al., 2018, 2019; Strakov´a et al., 2019; Kondratyuk and Straka, 2019). The transformer architecture (Vaswani et al., 2017) and the BERT lan"
2021.nodalida-main.1,N18-1202,0,0.0507358,"of the resources introduced in this work are available under open licenses from https://github.com/ turkunlp/wikibert . 1 Introduction Transfer learning using language models pretrained on large unannotated corpora has allowed for substantial recent advances at a broad range of natural language processing (NLP) tasks. By contrast to earlier distributional semantics approaches such as random indexing (Kanerva et al., 2000) and context-independent neural approaches such as word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014), models such as ULMFiT (Howard and Ruder, 2018), ELMo (Peters et al., 2018), GPT (Radford et al., 2018) and BERT (Devlin et al., 2019) create contextualized representations of meaning, capable of providing both contextualized word embeddings as well as embeddings for text segments longer than words. Recent pre-trained neural language models have been rapidly advancing the state of the art in a range of natural language understanding and NLP tasks (Wang et al., 2018, 2019; Strakov´a et al., 2019; Kondratyuk and Straka, 2019). The transformer architecture (Vaswani et al., 2017) and the BERT language model of Devlin et al. (2019) have been particularly influential, with"
2021.nodalida-main.1,P19-1493,0,0.0379076,"Missing"
2021.nodalida-main.1,2021.ccl-1.108,0,0.0615209,"Missing"
2021.nodalida-main.1,L16-1680,0,0.0605072,"Missing"
2021.nodalida-main.1,P19-1527,0,0.027327,"Missing"
2021.nodalida-main.1,L16-1262,1,0.844251,"Missing"
2021.nodalida-main.1,W18-5446,0,0.0671215,"Missing"
2021.nodalida-main.14,P19-4007,0,0.0332717,"Missing"
2021.nodalida-main.14,N19-1423,0,0.127688,"under an open license from https://github.com/TurkuNLP/turku-one A degree of language independence has long been a central goal in NER research. One notable example are the CoNLL shared tasks on Language-Independent Named Entity Recognition in 2002 and 2003 (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003). The Spanish, Dutch, English and German datasets introduced in these shared tasks were all annotated for the same types of entity mentions – persons, organizations, locations, and miscellaneous – and the datasets still remain key benchmarks for evaluating NER methods today (e.g. (Devlin et al., 2019)). Nevertheless, until recently most NER methods aimed for language independence only in that they supported training on corpora of more than one language, resulting in multiple separate monolingual models. In recent years, advances in deep learning have made it possible to create multilingual language models that achieve competitive levels of performance when trained and applied on texts representing more than one language (e.g. Kondratyuk and Straka (2019)). One notable model is the multilingual version of the influential BERT model (Devlin et al., 2019), mBERT, trained on more than 100 lang"
2021.nodalida-main.14,N06-2015,0,0.153575,"ted on the basis of the Universal Dependencies (Nivre et al., 2016) representation of the manually annotated Turku Dependency Treebank (TDT) (Haverinen et al., 2014; Pyysalo et al., 2015), a multi-domain corpus spanning ten different genres. The Turku NER annotation follows the types and annotation guidelines of the FiNER corpus. An evaluation by Luoma et al. (2020) demonstrated the compatibility of the two Finnish NER corpora by showing that models trained on the simple concatenation of the two corpora outperformed ones trained on either resource in isolation. 2.3 OntoNotes corpus OntoNotes (Hovy et al., 2006; Weischedel et al., 2013) is a large, multilingual (English, Chinese, and Arabic), multi-genre corpus annotated with several layers covering text structure as well as shallow semantics. In this work, we focus exclusively on the OntoNotes English language NER annotation and refer to this part of the data simply as OntoNotes for brevity. Specifically, we use the NER annotations of the OntoNotes v5.0 release (Weischedel et al., 2013), cast into CoNLL-like format by Pradhan et al. (2013).2 Sections of the corpus lacking NER annotation (such as the Old and New Testament texts) are excluded. The On"
2021.nodalida-main.14,K18-2013,1,0.795596,"nslated OntoNotes English terms tagged with the relevant types.4 Numeric types To annotate OntoNotes numeric types (CARDINAL, ORDINAL, etc.) in the Turku NER corpus section of the data, we mapped the manual part-of-speech and feature annotation of the source corpus (TDT) to initial annotations that were then manually revised to identify the more specific types such as PERCENT, QUANTITY and MONEY based on context. For the FiNER texts, annotation for these types followed a similar process with the exception that automatic part-of-speech and feature annotation created by the Turku neural parser (Kanerva et al., 2018) was used as a starting point as no manual syntactic annotation was available for the texts. Fine-grained tokenization The FiNER annotation guidelines specify that annotated name men4 The accuracy of this initial dictionary-based tagging step was not evaluated separately. Language Finnish Finnish Finnish English English English Model FinBERT mBERT biBERT BERT mBERT biBERT Train data Finnish Combined (Fi+En) Combined (Fi+En) English Combined (Fi+En) Combined (Fi+En) Development data Finnish Finnish Finnish English English English Test data Finnish Finnish Finnish English English English Table 4"
2021.nodalida-main.14,D19-1279,0,0.0207932,"ons – persons, organizations, locations, and miscellaneous – and the datasets still remain key benchmarks for evaluating NER methods today (e.g. (Devlin et al., 2019)). Nevertheless, until recently most NER methods aimed for language independence only in that they supported training on corpora of more than one language, resulting in multiple separate monolingual models. In recent years, advances in deep learning have made it possible to create multilingual language models that achieve competitive levels of performance when trained and applied on texts representing more than one language (e.g. Kondratyuk and Straka (2019)). One notable model is the multilingual version of the influential BERT model (Devlin et al., 2019), mBERT, trained on more than 100 languages. mBERT performs well on zero-shot cross-lingual transfer experiments, including NER experiments (Wu and Dredze, 2019). Moon et al. (2019) propose an mBERT-based model trained simultaneously on multiple languages. Training and validating on the OntoNotes v5.0 corpus (see Section 2.3) and the CoNLL datasets, they show that multilingual models outperform models trained on one single language and have cross-lingual zero-shot ability. The zeroshot cross-lin"
2021.nodalida-main.14,2020.acl-main.519,0,0.0904599,"Missing"
2021.nodalida-main.14,2020.lrec-1.567,1,0.859839,"Missing"
2021.nodalida-main.14,2020.coling-main.78,1,0.85986,"ised and corrected in a full, manual annotation pass. All manual revisions of the data were performed by a single annotator familiar with the corpora as well as the FiNER and OntoNotes guidelines. While the single-annotator setting regrettably precludes us from reporting inter-annotator agreement, our monolingual and cross-lingual results below suggest that the consistency of the annotation has not decreased from that of the source corpora. 4 Methods We next present the applied NER method and detail the experimental setup. 4.1 NER method We use the BERT-based named entity tagger introduced by Luoma and Pyysalo (2020). In brief, the method is based on adding a simple timedistributed dense layer on top of BERT to predict IOB2 named entity tags in a locally greedy manner. The model is both trained and applied with examples consisting of sentences catenated with their context sentences, resulting in multiple predictions for each token (appearing in both “focus” and context sentences). These predictions are then summarized using majority voting. For brevity, we refer to Luoma and Pyysalo (2020) for further details.7 Here, we do not use the wrapping of data in documentwise manner as in (Luoma and Pyysalo, 2020)"
2021.nodalida-main.14,P19-1493,0,0.0194198,"ned on more than 100 languages. mBERT performs well on zero-shot cross-lingual transfer experiments, including NER experiments (Wu and Dredze, 2019). Moon et al. (2019) propose an mBERT-based model trained simultaneously on multiple languages. Training and validating on the OntoNotes v5.0 corpus (see Section 2.3) and the CoNLL datasets, they show that multilingual models outperform models trained on one single language and have cross-lingual zero-shot ability. The zeroshot cross-lingual transfer ability of mBERT also spikes interest in the study of multilingual representations, both on mBERT (Pires et al., 2019; K et al., 2020), and on multilingual encoders in general (Ravishankar et al., 2019; Zhao et al., 2020; Choenni and Shutova, 2020). Corpus OntoNotes FiNER Turku NER Language English Finnish Finnish Tokens 2.0M 290K 200K Entities 162K 29K 11K Domain(s) News, magazines, conversation Technology news, Wikipedia News, magazines, blogs, Wikipedia, speech, fiction, etc. Table 1: Corpus features and statistics. OntoNotes token count only includes sections of the corpus annotated for name mentions. Entity counts include also non-name types such as DATE. In this paper, we aim to assess and realize the"
2021.nodalida-main.14,W13-3516,0,0.0425749,"Missing"
2021.nodalida-main.14,W15-1821,1,0.852822,"Missing"
2021.nodalida-main.14,W19-6205,0,0.0862737,"Missing"
2021.nodalida-main.14,E12-2021,1,0.796146,"Missing"
2021.nodalida-main.14,W02-2024,0,0.03001,"tated for mentions of entity names of interest. While extensive corpora with fine-grained NER annotation have long been available for high-resource languages such as English, NER for many lesser-resourced languages has been limited by smaller, lower-coverage corpora with comparatively coarse annotation. 1 The corpus is available under an open license from https://github.com/TurkuNLP/turku-one A degree of language independence has long been a central goal in NER research. One notable example are the CoNLL shared tasks on Language-Independent Named Entity Recognition in 2002 and 2003 (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003). The Spanish, Dutch, English and German datasets introduced in these shared tasks were all annotated for the same types of entity mentions – persons, organizations, locations, and miscellaneous – and the datasets still remain key benchmarks for evaluating NER methods today (e.g. (Devlin et al., 2019)). Nevertheless, until recently most NER methods aimed for language independence only in that they supported training on corpora of more than one language, resulting in multiple separate monolingual models. In recent years, advances in deep learning have made"
2021.nodalida-main.14,D19-1077,0,0.0151595,"orted training on corpora of more than one language, resulting in multiple separate monolingual models. In recent years, advances in deep learning have made it possible to create multilingual language models that achieve competitive levels of performance when trained and applied on texts representing more than one language (e.g. Kondratyuk and Straka (2019)). One notable model is the multilingual version of the influential BERT model (Devlin et al., 2019), mBERT, trained on more than 100 languages. mBERT performs well on zero-shot cross-lingual transfer experiments, including NER experiments (Wu and Dredze, 2019). Moon et al. (2019) propose an mBERT-based model trained simultaneously on multiple languages. Training and validating on the OntoNotes v5.0 corpus (see Section 2.3) and the CoNLL datasets, they show that multilingual models outperform models trained on one single language and have cross-lingual zero-shot ability. The zeroshot cross-lingual transfer ability of mBERT also spikes interest in the study of multilingual representations, both on mBERT (Pires et al., 2019; K et al., 2020), and on multilingual encoders in general (Ravishankar et al., 2019; Zhao et al., 2020; Choenni and Shutova, 2020"
2021.nodalida-main.29,I05-5002,0,0.0730355,"nish, sufficiently large for model training. The number of manually annotated examples makes the released dataset one of the largest, if not the largest manually annotated paraphrase corpus for any language. And thirdly, we report the experiences, tools, and baseline results on this new dataset, hopefully allowing other language NLP communities to assess the potential of developing a similar corpus for other languages. 2 Related Work Statistics of the different paraphrase corpora most relevant to our work are summarized in Table 1. For English, the Microsoft Research Paraphrase Corpus (MRPC) (Dolan and Brockett, 2005) is extracted from an online news collection by applying heuristics to recognize candidate document pairs and candidate sentences from the documents. Paraphrase candidates are subsequently filtered using a classifier, before the final manual binary annotation (paraphrase or not). In the Twitter URL Corpus (TUC) (Lan et al., 2017), paraphrase candidates are identified by recognizing Corpus English MRPC TUC ParaSCI PARADE QQP Finnish Opusparcus TaPaCo Data source Size autom. Size manual Labels Online news News tweets Scientific papers flashcards (computer sci.) Quora — — 350K — 404K 5,801 52K —"
2021.nodalida-main.29,J13-3001,0,0.0137248,"at least in their given context, if not in all contexts. Additionally, we establish a manual candidate selection method and demonstrate its feasibility in high quality paraphrase selection in terms of both cost and quality. 1 Introduction The powerful language models that have recently become available in NLP have also resulted in a distinct shift towards more meaning-oriented tasks for model fine-tuning and evaluation. The most typical example is entailment detection, with the paraphrase task raising in interest recently. Paraphrases, texts that express the same meaning with differing words (Bhagat and Hovy, 2013), are — already by their very definition — a suitable target to induce and evaluate models’ ability to represent meaning. Paraphrase detection and generation has numerous direct applications in NLP (Madnani and Dorr, 2010), among others in question answering (Soni and Roberts, 2019), plagiarism detection (Altheneyan and Menai, 2019), and machine translation (Mehdizadeh Seraj et al., 2015). Research in paraphrase naturally depends on the availability of datasets for the task. We will review these in more detail in Section 2, nevertheless, barring few exceptions, paraphrase corpora are typically"
2021.nodalida-main.29,L18-1218,0,0.0768893,"labels, the labeling is not originally designed for paraphrasing and the dataset providers warn about labeling not guaranteed to be perfect. Another common approach for automatic paraphrase identification is through language pivoting using multilingual parallel datasets. Here sentence alignments are used to recognize whether two different surface realizations share an identical or near-identical translation, assuming that the identical translation likely implies a paraphrase. There are two different multilingual paraphrase datasets automatically extracted using language pivoting, Opusparcus (Creutz, 2018) and TaPaCo (Scherrer, 1 data.quora.com/First-Quora-Dataset- Release-Question-Pairs 2020), both including a Finnish subsection. Opusparcus consists of candidate paraphrases automatically extracted from the alternative translations of movie and TV show subtitles after automatic sentence alignment. While the candidate paraphrases are automatically extracted, a small subset of a few thousand paraphrase pairs for each language is manually annotated. TaPaCo contains candidate paraphrases automatically extracted from the Tatoeba dataset2 , which is a multilingual crowdsourced database of sentences"
2021.nodalida-main.29,2021.eacl-main.33,0,0.0326169,"papers flashcards (computer sci.) Quora — — 350K — 404K 5,801 52K — 10K — 0/1 0/1 1-5 0-3 0/1 OpenSubtitles Tatoeba crowdsourcing 480K* 12K 3,703 — 1-4 — Table 1: Summary of available paraphrase corpora of naturally occurring sentential paraphrases. The corpora sizes include the total amount of pairs in the corpus (i.e. also those labeled as non-paraphrases), thus the actual number of good paraphrases depend on the class distribution of each corpus. *The highest quality cutpoint estimated by the authors. shared URLs in news related tweets. All candidates are manually binary-labeled. ParaSCI (Dong et al., 2021) is created by collecting paraphrase candidates from ACL and arXiv papers using heuristics based on term definitions, citation information as well as sentence embedding similarity. The extracted candidates are automatically filtered, but no manually annotated data is available. PARADE (He et al., 2020) is created by collecting online user-generated flashcards for computer science related concepts. All definitions for the same term are first clustered, and paraphrase candidates are extracted only among a cluster to reduce noise in candidate selection. All extracted candidates are manually annot"
2021.nodalida-main.29,2020.emnlp-main.611,0,0.0296089,"the corpus (i.e. also those labeled as non-paraphrases), thus the actual number of good paraphrases depend on the class distribution of each corpus. *The highest quality cutpoint estimated by the authors. shared URLs in news related tweets. All candidates are manually binary-labeled. ParaSCI (Dong et al., 2021) is created by collecting paraphrase candidates from ACL and arXiv papers using heuristics based on term definitions, citation information as well as sentence embedding similarity. The extracted candidates are automatically filtered, but no manually annotated data is available. PARADE (He et al., 2020) is created by collecting online user-generated flashcards for computer science related concepts. All definitions for the same term are first clustered, and paraphrase candidates are extracted only among a cluster to reduce noise in candidate selection. All extracted candidates are manually annotated using a scheme with four labels. Quora Question Pairs (QQP)1 contains question headings from the forum with binary labels into duplicate-or-not questions. The QQP dataset is larger than other datasets, however, although including human-produced labels, the labeling is not originally designed for p"
2021.nodalida-main.29,D17-1126,0,0.0303387,"Missing"
2021.nodalida-main.29,J10-3003,0,0.0358498,"y. 1 Introduction The powerful language models that have recently become available in NLP have also resulted in a distinct shift towards more meaning-oriented tasks for model fine-tuning and evaluation. The most typical example is entailment detection, with the paraphrase task raising in interest recently. Paraphrases, texts that express the same meaning with differing words (Bhagat and Hovy, 2013), are — already by their very definition — a suitable target to induce and evaluate models’ ability to represent meaning. Paraphrase detection and generation has numerous direct applications in NLP (Madnani and Dorr, 2010), among others in question answering (Soni and Roberts, 2019), plagiarism detection (Altheneyan and Menai, 2019), and machine translation (Mehdizadeh Seraj et al., 2015). Research in paraphrase naturally depends on the availability of datasets for the task. We will review these in more detail in Section 2, nevertheless, barring few exceptions, paraphrase corpora are typically large and gathered automatically using one of several possible heuristics. Typically a comparatively small section of the corpus is manually classified to serve as a test set for method development. The heuristics used to"
2021.nodalida-main.29,D15-1163,0,0.0271892,"l fine-tuning and evaluation. The most typical example is entailment detection, with the paraphrase task raising in interest recently. Paraphrases, texts that express the same meaning with differing words (Bhagat and Hovy, 2013), are — already by their very definition — a suitable target to induce and evaluate models’ ability to represent meaning. Paraphrase detection and generation has numerous direct applications in NLP (Madnani and Dorr, 2010), among others in question answering (Soni and Roberts, 2019), plagiarism detection (Altheneyan and Menai, 2019), and machine translation (Mehdizadeh Seraj et al., 2015). Research in paraphrase naturally depends on the availability of datasets for the task. We will review these in more detail in Section 2, nevertheless, barring few exceptions, paraphrase corpora are typically large and gathered automatically using one of several possible heuristics. Typically a comparatively small section of the corpus is manually classified to serve as a test set for method development. The heuristics used to gather and filter the corpora naturally introduce a bias to the corpora which, as we will show later in this paper, demonstrates itself as a tendency towards short exam"
2021.nodalida-main.29,2020.lrec-1.848,0,0.0560438,"Missing"
2021.nodalida-main.29,W19-5003,0,0.0185634,"ntly become available in NLP have also resulted in a distinct shift towards more meaning-oriented tasks for model fine-tuning and evaluation. The most typical example is entailment detection, with the paraphrase task raising in interest recently. Paraphrases, texts that express the same meaning with differing words (Bhagat and Hovy, 2013), are — already by their very definition — a suitable target to induce and evaluate models’ ability to represent meaning. Paraphrase detection and generation has numerous direct applications in NLP (Madnani and Dorr, 2010), among others in question answering (Soni and Roberts, 2019), plagiarism detection (Altheneyan and Menai, 2019), and machine translation (Mehdizadeh Seraj et al., 2015). Research in paraphrase naturally depends on the availability of datasets for the task. We will review these in more detail in Section 2, nevertheless, barring few exceptions, paraphrase corpora are typically large and gathered automatically using one of several possible heuristics. Typically a comparatively small section of the corpus is manually classified to serve as a test set for method development. The heuristics used to gather and filter the corpora naturally introduce a bias to"
2021.nodalida-main.29,tiedemann-2012-parallel,0,0.0177813,"es OpenSubtitles3 distributes an extensive collection of user generated subtitles for different movies and TV episodes. These subtitles are available in multiple languages, but surprisingly often the same movie or episode have versions in a single language, originating from different sources. This gives an opportunity to exploit the natural variation produced by independent translators, and by comparing two different subtitles for a single movie or episode, there is a high likelihood of finding naturally occurring paraphrases. From the database dump of OpenSubtitles2018 obtained through OPUS (Tiedemann, 2012), we selected all movies and TV episodes with at least two Finnish subtitle versions. In case more versions are available, the two most lexically differing are selected for paraphrase extraction. We measure lexical similarity by TF-IDF weighted document vectors. Specifically, we create TFIDF vectors with TfidfVectorizer from the sklearn package. We limit the number of features to 200K, apply sublinear scaling, use character 4-grams created out of text inside word boundaries, and otherwise use the default settings. To filter out subtitle pairs with low density of interesting paraphrase candidat"
D19-5728,W09-1402,1,0.83615,"Missing"
D19-5728,K18-2013,1,0.891217,"of the input sentences. CRAFT SA also adopts the format and evaluation tools of the CoNLL tasks, and its representation matches the universal representation of these tasks in part. The CRAFT task is differentiated from the many corpus resources applied in the CoNLL tasks specifically in focusing on biomedical domain texts, and CRAFT is unique among syntactically annotated biomedical corpora in that its texts are drawn from full-text articles, rather than only article titles and abstracts. We participated in the CRAFT SA task using an approach that builds primarily on the Turku neural parser (Kanerva et al., 2018), a native dependency parsing system that previously ranked among the best systems in the CoNLL 2018 task. As the parser is fully retrainable, designed to accept the format used for the CRAFT data, and agnostic to the details of the representation, it was possible to train it for the CRAFT task with little modification. Additionally, as the parser has not been deWe present the approach taken by the TurkuNLP group in the CRAFT Structural Annotation task, a shared task on dependency parsing. Our approach builds primarily on the Turku neural parser, a native dependency parser that ranked among th"
D19-5728,Q17-1010,0,0.0120724,"er was ranked second on LAS and MLAS, and first on BLEX on the CoNLL-2018 Shared Task, making it highly competitive. 4.2 of which produce a lemma by removing and possibly substituting characters from the word prefix and suffix. As in tagging, an averaged perceptron then disambiguates among the candidates. The dependency parser is a transition-based parser with a feed-forward neural network serving as the classifier that decides on the next transition taken by the parser. 4.3 For inducing new sets of word vectors, we used the word2vec6 (Mikolov et al., 2013) and FastText7 (Joulin et al., 2016; Bojanowski et al., 2017) tools. In brief, these tools generate a vector representation for each token based on the similarity of the contexts in which they appear in a large corpus of unannotated text. Word vectors were induced on texts extracted from PubMed abstracts and PMC Open Access publications (Section 3.3) using both the skip-gram and continuous bagof-words (CBOW) models implemented in both tools. Model parameters were primarily kept at their default values, but we performed a series of experiments with different values of the window parameter, which has been found to be particularly impactful in previous wor"
D19-5728,W04-3111,0,0.188427,"Missing"
D19-5728,W06-2920,0,0.0137916,"h Stanford dependencies conversion was the workhorse of biomedical dependency parsing for nearly a decade. Also the treebanks available for training the parsers in the biomedical domain have traditionally been constituency-based, for instance the Penn BioIE (Kulick et al., 2004) and especially the GENIA treebank (Tateisi et al., 2005). The BioInfer corpus (Pyysalo et al., 2007) was the first domain corpus to adopt Stanford Dependencies as the native annotation scheme, coinciding with a generally growing interest in dependency parsing and its applications. The CoNLL 2006 and 2007 shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007) addressed multilingual dependency parsing, and while data was provided for different languages in the same format, the underlying representation (e.g. dependency types) was not standardized in these tasks. These tasks also included only prediction of syntactic trees, whereas tokenization and part-of-speech tags were given for the participants. In recent years, there has been an increased interest in native dependency parsing, reflected in efforts such as Universal Dependencies (UD) (Nivre et al., 2016) and the CoNLL 2017 and 2018 shared tasks on multilingual parsing using"
D19-5728,P05-1022,0,0.019375,"November 4, 2019. 2019 Association for Computational Linguistics veloped or previously applied to biomedical English, we consider a number of modifications and adaptions to improve on its performance, finding in particular that the strong baseline performance of the parser can be further improved through initialization with in-domain word vectors. 2 Documents Sentences Tokens Train 67 21 731 561 032 Test 30 9 099 232 619 Table 1: CRAFT Structural Annotation statistics Background Documents Sentences Tokens Biomedical domain models have been available for a number of constituency parsers (e.g. Charniak and Johnson (2005), McClosky and Charniak (2008)) and have been widely applied in domain information extraction efforts, frequently in conjunction with heuristic conversions into dependency representations such as Stanford dependencies (De Marneffe and Manning, 2008). There have also been native dependency parsers available for the domain, such as Pro3Gres (Schneider and Rinaldi, 2004) and, later, GDep (Miyao et al., 2008), nevertheless the abovementioned McClosky-Charniak parser with Stanford dependencies conversion was the workhorse of biomedical dependency parsing for nearly a decade. Also the treebanks avai"
D19-5728,W16-2922,1,0.924707,"ddings by Ginter et al. (2017) trained on general English extracted from Wikipedia and Internet crawls. These embeddings are trained using the word2vec (Mikolov et al., 2013) tool with lowercased data, skip-gram algorithm, window size of 10 and 100 dimensions. The vectors were originally provided for the CoNLL 2017 and 2018 multilingual parsing shared task, and thus used by many of the participating systems in their English parsing models. We also considered a number of word vectors induced specifically on biomedical text for domain tasks, including those created by Pyysalo et al. (2013)1 and Chiu et al. (2016)2 . 4.1 Turku Parser Our primary parser used in all experiments is the Turku Neural Parser Pipeline4 (Kanerva et al., 2018), a full parser pipeline meant for end-to-end analysis from raw text into UD. The pipeline includes sentence and word segmentation, part-ofspeech and morphological tagging, syntactic parsing, and lemmatization. The segmentation component in the Turku pipeline is built using UDPipe (Straka and Strakov´a, 2017), where the token and sentence boundaries are jointly predicted using a singlelayer bidirectional GRU network. Universal (UPOS) and language-specific (XPOS) part-ofspe"
D19-5728,P14-5010,0,0.00250899,"available in this particular hybrid SD/UD CoNLL-U representation. We expand on this issue below in Section 6.2. 3.3 Unlabelled data 3.2 4 Methods To induce new word vectors (Section 4.3) and conduct co-training experiments (Section 5.2), we used unlabelled texts from PubMed titles and abstracts and PubMed Central (PMC) full texts. The data was drawn from the PubMed 2017 baseline distribution and a 2017 download of the PMC Open Access subset.3 The texts were segmented into sentences using the GENIA sentence splitter and then tokenized using the PTBTokenizer included in Stanford CoreNLP tools (Manning et al., 2014) and the tokenized sentences shuffled randomly. The resulting dataset consists of 12.5 billion tokens in 500 million sentences. As the text of the full-text articles of the CRAFT corpus contains characters outside of the basic ASCII character set, we created word vectors on the original extracted texts instead of first applying a mapping to ASCII characters as was done in a number of similar previous efforts (e.g. (Pyysalo et al., 2013)). Word vectors We considered a number of previously released word vectors for initializing the parser. As a baseline we use the English word embeddings by Gint"
D19-5728,W08-1301,0,0.0773769,"Missing"
D19-5728,P08-2026,0,0.0465163,"iation for Computational Linguistics veloped or previously applied to biomedical English, we consider a number of modifications and adaptions to improve on its performance, finding in particular that the strong baseline performance of the parser can be further improved through initialization with in-domain word vectors. 2 Documents Sentences Tokens Train 67 21 731 561 032 Test 30 9 099 232 619 Table 1: CRAFT Structural Annotation statistics Background Documents Sentences Tokens Biomedical domain models have been available for a number of constituency parsers (e.g. Charniak and Johnson (2005), McClosky and Charniak (2008)) and have been widely applied in domain information extraction efforts, frequently in conjunction with heuristic conversions into dependency representations such as Stanford dependencies (De Marneffe and Manning, 2008). There have also been native dependency parsers available for the domain, such as Pro3Gres (Schneider and Rinaldi, 2004) and, later, GDep (Miyao et al., 2008), nevertheless the abovementioned McClosky-Charniak parser with Stanford dependencies conversion was the workhorse of biomedical dependency parsing for nearly a decade. Also the treebanks available for training the parsers"
D19-5728,K17-3002,0,0.0189026,"The segmentation component in the Turku pipeline is built using UDPipe (Straka and Strakov´a, 2017), where the token and sentence boundaries are jointly predicted using a singlelayer bidirectional GRU network. Universal (UPOS) and language-specific (XPOS) part-ofspeech tags, as well as morphological features 3 We used 2017 data as we had a plain text version readily available from previous work. 4 https://turkunlp.org/ Turku-neural-parser-pipeline/ 1 http://bio.nlplab.org/ 2 https://github.com/cambridgeltl/ BioNLP-2016 208 (FEATS) are predicted with a modified version of the one published by Dozat et al. (2017), a time-distributed classifier over tokens in a sentence embedded using bidirectional LSTM network. The tagger has two separate classification layers, one for universal part-of-speech and one originally used for language-specific part-ofspeech tags. The bidirectional encoding is shared between both classifiers. In the modified version (Kanerva et al., 2018), the second classifier is used to jointly predict the language-specific POS tags together with morphological features by simply concatenating the two input columns into one. The syntactic analysis is based on a graph-based parser by Dozat"
D19-5728,N06-1020,0,0.048667,"Missing"
D19-5728,W10-1905,1,0.795518,"st work focusing on the analysis of English news texts (Marcus et al., 1994). Syntactic analyses are required also by many methods for the analysis of biomedical text; for example, information extraction methods commonly rely on the shortest path over syntactic dependencies to identify how entities mentioned in text are related (Airola et al., 2008; Bj¨orne et al., 2009; Liu et al., 2013; Luo et al., 2016). The performance of parsers is known to be domaindependent: to create high-quality analyses of e.g. biomedical texts, the tools should be trained on annotated corpora reflecting the domain (Miwa et al., 2010). Syntactically annotated corpora of domain texts are thus required for much of biomedical NLP. These resources should also preferably follow the relevant standards in the representation of 206 Proceedings of the 5th Workshop on BioNLP Open Shared Tasks, pages 206–215 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics veloped or previously applied to biomedical English, we consider a number of modifications and adaptions to improve on its performance, finding in particular that the strong baseline performance of the parser can be further improved through initi"
D19-5728,K18-2001,1,0.896613,"Missing"
D19-5728,P08-1006,0,0.0454783,"9 232 619 Table 1: CRAFT Structural Annotation statistics Background Documents Sentences Tokens Biomedical domain models have been available for a number of constituency parsers (e.g. Charniak and Johnson (2005), McClosky and Charniak (2008)) and have been widely applied in domain information extraction efforts, frequently in conjunction with heuristic conversions into dependency representations such as Stanford dependencies (De Marneffe and Manning, 2008). There have also been native dependency parsers available for the domain, such as Pro3Gres (Schneider and Rinaldi, 2004) and, later, GDep (Miyao et al., 2008), nevertheless the abovementioned McClosky-Charniak parser with Stanford dependencies conversion was the workhorse of biomedical dependency parsing for nearly a decade. Also the treebanks available for training the parsers in the biomedical domain have traditionally been constituency-based, for instance the Penn BioIE (Kulick et al., 2004) and especially the GENIA treebank (Tateisi et al., 2005). The BioInfer corpus (Pyysalo et al., 2007) was the first domain corpus to adopt Stanford Dependencies as the native annotation scheme, coinciding with a generally growing interest in dependency parsin"
D19-5728,L16-1262,1,0.785909,"Missing"
D19-5728,W07-1004,1,0.640338,"e and Manning, 2008). There have also been native dependency parsers available for the domain, such as Pro3Gres (Schneider and Rinaldi, 2004) and, later, GDep (Miyao et al., 2008), nevertheless the abovementioned McClosky-Charniak parser with Stanford dependencies conversion was the workhorse of biomedical dependency parsing for nearly a decade. Also the treebanks available for training the parsers in the biomedical domain have traditionally been constituency-based, for instance the Penn BioIE (Kulick et al., 2004) and especially the GENIA treebank (Tateisi et al., 2005). The BioInfer corpus (Pyysalo et al., 2007) was the first domain corpus to adopt Stanford Dependencies as the native annotation scheme, coinciding with a generally growing interest in dependency parsing and its applications. The CoNLL 2006 and 2007 shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007) addressed multilingual dependency parsing, and while data was provided for different languages in the same format, the underlying representation (e.g. dependency types) was not standardized in these tasks. These tasks also included only prediction of syntactic trees, whereas tokenization and part-of-speech tags were given for the pa"
D19-5728,W04-1505,0,0.0292052,"Missing"
D19-5728,K17-3009,0,0.0541379,"Missing"
de-marneffe-etal-2014-universal,de-marneffe-etal-2006-generating,1,\N,Missing
de-marneffe-etal-2014-universal,W09-2307,1,\N,Missing
de-marneffe-etal-2014-universal,J03-4003,0,\N,Missing
de-marneffe-etal-2014-universal,W08-1301,1,\N,Missing
de-marneffe-etal-2014-universal,P03-1054,1,\N,Missing
de-marneffe-etal-2014-universal,P06-1033,1,\N,Missing
de-marneffe-etal-2014-universal,W13-3721,1,\N,Missing
de-marneffe-etal-2014-universal,P08-1109,1,\N,Missing
de-marneffe-etal-2014-universal,P06-1055,0,\N,Missing
de-marneffe-etal-2014-universal,C12-1147,0,\N,Missing
de-marneffe-etal-2014-universal,petrov-etal-2012-universal,0,\N,Missing
de-marneffe-etal-2014-universal,J05-1004,0,\N,Missing
de-marneffe-etal-2014-universal,P05-1013,1,\N,Missing
de-marneffe-etal-2014-universal,N13-1070,0,\N,Missing
de-marneffe-etal-2014-universal,W13-2308,0,\N,Missing
de-marneffe-etal-2014-universal,P13-2103,0,\N,Missing
de-marneffe-etal-2014-universal,P13-2017,1,\N,Missing
K17-3001,K17-3023,0,0.0375672,"Missing"
K17-3001,P16-1231,1,0.301678,"M Table 1: The supporting data overview: the number of words (M = million; K = thousand) for each language. http://commoncrawl.org/ Except for Ancient Greek, which was gathered from the Perseus Digital Library. 3 http://github.com/CLD2Owners/cld2 4 http://unicode.org/reports/tr15/ 3 verted to Unicode character NO-BREAK SPACE (U+00A0).5 The dimensionality of the word embeddings was chosen to be 100 after thorough discussion – more dimensions may yield better results and are commonly used, but even with just 100, the uncompressed word embeddings for the 45 languages take 135 GiB. Also note that Andor et al. (2016) achieved state-of-the-art results with 64 dimensions. The word embeddings were precomputed using word2vec (Mikolov et al., 2013) with the following options: word2vec -min-count 10 -size 100 -window 10 -negative 5 -iter 2 -threads 16 -cbow 0 -binary 0. The precomputed word embeddings are available on-line (Ginter et al., 2017). 2.3 this shared task, i.e., not included in any previous UD release. The PUD treebank consists of 1000 sentences currently in 18 languages (15 K to 27 K words, depending on the language), which were randomly picked from on-line newswire and Wikipedia;7 usually only a fe"
K17-3001,W06-2920,0,0.0145655,"categorization of the different approaches of the participating systems. Introduction Ten years ago, two CoNLL shared tasks were a major milestone for parsing research in general and dependency parsing in particular. For the first time dependency treebanks in more than ten languages were available for learning parsers. Many of them were used in follow-up work, evaluating parsers on multiple languages became standard, and multiple state-of-the-art, open-source parsers became available, facilitating production of dependency structures to be used in downstream applications. While the two tasks (Buchholz and Marsi, 2006; Nivre et al., 2007) were extremely important in setting the scene for the following years, there were also limitations that complicated application of their results: (1) gold-standard to1 Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, pages 1–19, c 2017 Association for Computational Linguistics Vancouver, Canada, August 3-4, 2017. kenization and part-of-speech tags in the test data moved the tasks away from real-world scenarios, and (2) incompatible annotation schemes made cross-linguistic comparison impossible. CoNLL 2017 has picked"
K17-3001,K17-3017,0,0.147208,"emains with participants, and since open sourcing the software underlying a paper is still the exception rather than the rule. To ensure both, TIRA supplies participants with a virtual machine, offering a range of commonly used operating systems in order not to limit the choice of technology stacks and development environments. Once deployed and tested, the virtual machines are archived to preserve the software within. Many participants agreed to share their code so that we decided to collect the respective projects in a kind of open source proceedings at GitHub.14 4.3 by Straka and Strakov´a (2017) as one of the competing systems. Straka and Strakov´a (2017) describe both these versions in more detail. The baseline models were released together with the UD 2.0 training data, one model for each treebank. Because only training and development data were available during baseline model training, we put aside a part of the training data for hyperparameter tuning, and evaluated the baseline model performance on development data. We called this data split baseline model split. The baseline models, the baseline model split, and also UD 2.0 training data with morphology predicted by 10-fold jack"
K17-3001,K17-3005,0,0.0752704,"Missing"
K17-3001,K17-3026,0,0.0310687,"E 90.88 82.31 82.46 LyS-FASTPARSE 90.88 82.31 79.14 NAIST SATO 90.88 82.31 82.46 Orange – Deski˜n 90.88 38.81 15.38 UALING 90.88 82.31 82.46 UParse 90.88 82.31 82.46 naistCL 90.88 82.31 82.46 Table 5: Universal POS tags, features and lemmas (ordered by UPOS F1 scores). duce suboptimal results when deployed on a machine different from the one where it was trained. Several teams used the library and may have been affected; for the Uppsala team (de Lhoneux et al., 2017) the issue led to official LAS = 65.11 (23rd place) instead of 69.66 (9th place). In the second case, the ParisNLP system (De La Clergerie et al., 2017) used a wrong method of recognizing the input language, which was not supported in the test data (but unfortunately it was possible to get along with it in development and trial data). Simply crashing could mean that the task moderator would show the team their diagnostic output and they would fix the bug; however, the parser was robust enough to switch to a languageagnostic mode and produced results that were not great, but also not so bad to alert the moderator and make him investigate. Thus the official LAS of the system is 60.02 (27th place) while without the bug it could have been 70.35 ("
K17-3001,K17-3021,0,0.0954088,"emains with participants, and since open sourcing the software underlying a paper is still the exception rather than the rule. To ensure both, TIRA supplies participants with a virtual machine, offering a range of commonly used operating systems in order not to limit the choice of technology stacks and development environments. Once deployed and tested, the virtual machines are archived to preserve the software within. Many participants agreed to share their code so that we decided to collect the respective projects in a kind of open source proceedings at GitHub.14 4.3 by Straka and Strakov´a (2017) as one of the competing systems. Straka and Strakov´a (2017) describe both these versions in more detail. The baseline models were released together with the UD 2.0 training data, one model for each treebank. Because only training and development data were available during baseline model training, we put aside a part of the training data for hyperparameter tuning, and evaluated the baseline model performance on development data. We called this data split baseline model split. The baseline models, the baseline model split, and also UD 2.0 training data with morphology predicted by 10-fold jack"
K17-3001,K17-3022,1,0.891655,"Missing"
K17-3001,K17-3025,0,0.0327614,"Missing"
K17-3001,K17-3024,0,0.050508,"Missing"
K17-3001,K17-3027,0,0.0537913,"Missing"
K17-3001,K17-3014,0,0.0756362,"Missing"
K17-3001,K17-3015,0,0.0745209,"Missing"
K17-3001,K17-3007,0,0.0511894,"Missing"
K17-3001,L16-1262,1,0.869327,"Missing"
K17-3001,W14-6111,0,0.0253686,"Missing"
K17-3001,W17-0411,1,0.831758,"ossible when the system run completed; before that, even the task moderator would not see whether the system was really producing output and not just sitting in an endless loop. Especially given the scale of operations this year, this turned out to be a major obstacle for some participants; TIRA needs to be improved by offering more finegrained process monitoring tools, both for organizers and participants. Content-word Labeled Attachment Score (CLAS) has been proposed as an alternative parsing metric that is tailored to the UD annotation style and more suitable for cross-language comparison (Nivre and Fang, 2017). It differs from LAS in that it only considers relations between content words. Attachment of function words is disregarded because it corresponds to morphological features in other languages (and morphology is not evaluated in this shared task). Furthermore, languages with many function words (e.g., English) have longer sentences than morphologically rich languages (e.g., Finnish), hence a single error in Finnish costs the parser significantly more than an error in English. CLAS also disregards attachment of punctuation. As CLAS is still experimental, we have designated full LAS as our main"
K17-3001,K17-3003,0,0.0845341,"Missing"
K17-3001,W17-0412,1,0.869806,"Missing"
K17-3001,L16-1680,1,0.0475333,"Missing"
K17-3001,K17-3009,1,0.104147,"Missing"
K17-3001,tiedemann-2012-parallel,0,0.0126153,"oses (so that follow-up research is not obstructed). We deliberately did not place upper bounds on data sizes (in contrast to e.g. Nivre et al. (2007)), despite the fact that processing large amounts of data may be difficult for some teams. Our primary objective was to determine the capability of current parsers with the data that is currently available. In practice, the task was formally closed, i.e., we listed the approved data resources so that all participants were aware of their options. However, the selection was rather broad, ranging from Wikipedia dumps over the OPUS parallel corpora (Tiedemann, 2012) to morphological transducers. Some of the resources were proposed by the participating teams. 2.2 Supporting Data To enable the induction of custom embeddings and the use of semi-supervised methods in general, the participants were provided with supporting resources primarily consisting of large text corpora for (nearly) all of the languages in the task, as well as embeddings pre-trained on these corpora. 1 Outside CoNLL, there were several other parsing tasks in the meantime, which naturally also explored previously unadressed aspects—for example SANCL (Petrov and McDonald, 2012) or SPMRL (S"
K17-3001,K17-3016,0,0.0605417,"Missing"
K17-3001,K17-3020,0,0.0375614,"Missing"
K17-3001,K17-3013,0,0.0456211,"Missing"
K17-3001,D07-1096,1,\N,Missing
K17-3001,K17-3002,1,\N,Missing
K17-3001,K17-3019,0,\N,Missing
K17-3001,K17-3012,1,\N,Missing
K17-3001,K17-3006,0,\N,Missing
K17-3001,K17-3010,0,\N,Missing
K17-3001,K17-3018,0,\N,Missing
K17-3001,K17-3028,1,\N,Missing
K17-3001,K17-3011,0,\N,Missing
K17-3012,L16-1680,0,0.120494,"Missing"
K17-3012,P14-5003,0,0.0713368,"Missing"
K17-3012,D14-1082,0,\N,Missing
K17-3012,P13-2017,0,\N,Missing
K18-2001,K18-2015,0,0.053009,"Missing"
K18-2001,Q17-1010,0,0.211935,"Missing"
K18-2001,K18-2010,0,0.0386566,"Missing"
K18-2001,K18-2017,0,0.075361,"Missing"
K18-2001,W06-2920,0,0.453112,"Missing"
K18-2001,K18-2025,0,0.0365994,"Missing"
K18-2001,K18-2005,0,0.120251,"Missing"
K18-2001,K18-2013,1,0.806044,"Missing"
K18-2001,K18-2026,0,0.0321915,"Missing"
K18-2001,K18-2012,0,0.0235436,"above are all intrinsic measures: they evaluate the grammatical analysis task per se, with the hope that better scores correspond to output that is more useful for downstream NLP applications. Nevertheless, such correlations are not automatically granted. We thus seek to complement our task with an extrinsic evaluation, where the output of parsing systems is exploited by applications like biological event extraction, opinion analysis and negation scope resolution. This optional track involves English only. It is organized in collaboration with the EPE initiative;7 for details see Fares et al. (2018). Syntactic Word Alignment The higher segmentation level is based on the notion of syntactic word. Some languages contain multi-word tokens (MWT) that are regarded as contractions of multiple syntactic words. For example, the German token zum is a contraction of the preposition zu “to” and the article dem “the”. Syntactic words constitute independent nodes in dependency trees. As shown by the example, it is not required that the MWT is a pure concatenation of the participating words; the simple token alignment thus does not work when MWTs 4 TIRA: The System Submission Platform Similarly to our"
K18-2001,K18-2003,0,0.040574,"Missing"
K18-2001,K18-2006,0,0.0774162,"Missing"
K18-2001,K18-2014,0,0.0664725,"Missing"
K18-2001,K18-2008,0,0.0697052,"Missing"
K18-2001,L16-1262,1,0.910778,"Missing"
K18-2001,W17-0411,1,0.849881,"and in the system output before comparing them. In the end-to-end evaluation of our task, LAS is re-defined as the harmonic mean (F1 ) of precision P and recall R, where P = #correctRelations #systemNodes (1) R= #correctRelations #goldNodes (2) LAS = 2P R P +R (3) Note that attachment of all nodes including punctuation is evaluated. LAS is computed separately for each of the 82 test files and a macro-average of all these scores is used to rank the systems. 3.2 MLAS: Morphology-Aware Labeled Attachment Score MLAS aims at cross-linguistic comparability of the scores. It is an extension of CLAS (Nivre and Fang, 2017), which was tested experimentally in the 2017 task. CLAS focuses on dependencies between content words and disregards attachment of function words; in MLAS, function words are not ignored, but they are treated as features of content words. In addition, part-of-speech tags and morphological features are evaluated, too. 3.3 BLEX: Bilexical Dependency Score BLEX is similar to MLAS in that it focuses on relations between content words. Instead of morphological features, it incorporates lemmatization in the evaluation. It is thus closer to semantic content and evaluates two aspects of UD annota5 ar"
K18-2001,K18-2022,0,0.0296323,"Missing"
K18-2001,K18-2011,1,0.844373,"Missing"
K18-2001,W17-0412,1,0.901947,"Missing"
K18-2001,L16-1680,1,0.90044,"Missing"
K18-2001,K17-3009,1,0.858784,"Missing"
K18-2001,tiedemann-2012-parallel,0,0.0674866,"at follow-up research is not obstructed). We deliberately did not place upper bounds on data sizes (in contrast to e.g. Nivre et al. (2007)), despite the fact that processing large amounts of data may be difficult for some teams. Our primary objective was to determine the capability of current parsers provided with large amounts of freely available data. In practice, the task was formally closed, i.e., we listed the approved data resources so that all participants were aware of their options. However, the selection was rather broad, ranging from Wikipedia dumps over the OPUS parallel corpora (Tiedemann, 2012) to morphological transducers. Some of the resources were proposed by the participating teams. We provided dependency-annotated training and test data, and also large quantities of crawled raw texts. Other language resources are available from third-party servers and we only referred to the respective download sites. 2.1 Training Data: UD 2.2 Training and development data came from the Universal Dependencies (UD) 2.2 collection (Nivre et al., 2018). This year, the official UD release immediately followed the test phase of the shared task. The training and development data were available to the"
K18-2001,K18-2016,0,0.0988933,"Missing"
K18-2001,K18-2019,0,0.110064,"Missing"
K18-2001,K18-2007,0,0.0602044,"Missing"
K18-2001,K18-2004,0,0.103154,"Missing"
K18-2013,K17-2002,0,0.0731186,"Missing"
K18-2013,K17-2001,0,0.351337,"f treebanks and languages. The 2017 task (Zeman et al., 2017) focused primarily on the evaluation of the syntactic trees produced by the participating systems, whereas the 2018 task (Zeman et al., 2018) adds further two metrics which also measure the accuracy of morphological tagging and lemmatization. In this paper, we present the TurkuNLP system submission to the CoNLL 2018 UD Shared Task. The system is an end-toend parsing pipeline, with components for segmentation, morphological tagging, parsing, and lemmatization. The tagger and parser are based on the 2017 winning system by Dozat et al. (2017), while the lemmatizer is a novel approach utilizing the OpenNMT neural machine translation system for sequence-to-sequence learning. Our pipeline LAS The proportion of words which have the correct head word with the correct dependency relation. MLAS Similar to LAS, with the additional requirement that a subset of the morphology features is correctly predicted and the functional dependents of the word are correctly attached. MLAS is only calculated on content-bearing words, and strives to level the field w.r.t. morphological richness of languages. 1 https://dumps.wikimedia.org https://svn.code"
K18-2013,P81-1022,0,0.696953,"Missing"
K18-2013,K17-3002,0,0.361739,"a large set of treebanks and languages. The 2017 task (Zeman et al., 2017) focused primarily on the evaluation of the syntactic trees produced by the participating systems, whereas the 2018 task (Zeman et al., 2018) adds further two metrics which also measure the accuracy of morphological tagging and lemmatization. In this paper, we present the TurkuNLP system submission to the CoNLL 2018 UD Shared Task. The system is an end-toend parsing pipeline, with components for segmentation, morphological tagging, parsing, and lemmatization. The tagger and parser are based on the 2017 winning system by Dozat et al. (2017), while the lemmatizer is a novel approach utilizing the OpenNMT neural machine translation system for sequence-to-sequence learning. Our pipeline LAS The proportion of words which have the correct head word with the correct dependency relation. MLAS Similar to LAS, with the additional requirement that a subset of the morphology features is correctly predicted and the functional dependents of the word are correctly attached. MLAS is only calculated on content-bearing words, and strives to level the field w.r.t. morphological richness of languages. 1 https://dumps.wikimedia.org https://svn.code"
K18-2013,tiedemann-2012-parallel,0,0.294131,"orphological tagging and parsing. 1 2 Task overview CoNLL 2018 UD Shared Task is a follow-up to the 2017 shared task of developing systems predicting syntactic dependencies on raw texts across a number of typologically different languages. In addition to the 82 UD treebanks for 57 languages, which formed the primary training data, the participating teams were allowed to use also additional resources such as Wikipedia dumps1 , raw web crawl data and word embeddings (Ginter et al., 2017), morphological transducers provided by Apertium2 and Giellatekno3 , and the OPUS parallel corpus collection (Tiedemann, 2012). In addition to the 2017 primary metric (LAS), the systems were additionally evaluated also on metrics which include lemmatization and morphology prediction. In brief, the three primary metrics of the task are as follows (see Zeman et al. (2018) for detailed definitions): Introduction The 2017 and 2018 CoNLL UD Shared tasks aim at an evaluation of end-to-end parsing systems on a large set of treebanks and languages. The 2017 task (Zeman et al., 2017) focused primarily on the evaluation of the syntactic trees produced by the participating systems, whereas the 2018 task (Zeman et al., 2018) add"
K18-2013,W11-2123,0,0.0267418,"Missing"
K18-2013,P18-4020,0,0.025348,"Missing"
K18-2013,K17-3001,1,0.903814,"Missing"
K18-2013,K17-2003,0,0.045518,"Missing"
K18-2013,K18-2001,1,0.804081,"Missing"
K18-2013,P17-4012,0,0.184601,"we rely for most but not all languages on the tokenization and sentence splitting provided by the UDPipe baseline (Straka et al., 2016). Tagging and parsing is carried out using the parser of Dozat et al. (2017), the winning entry of the 2017 shared task. Using a simple data manipulation technique, we also obtain the morphological feature predictions from the same tagger which was originally used to produce only universal partof-speech (UPOS) and language-specific part-ofspeech (XPOS) predictions. Finally, the lemmatization is carried out using the OpenNMT neural machine translation toolkit (Klein et al., 2017), casting lemmatization as a machine translation problem. All these components are wrapped into one parsing pipeline, making it possible to run all four steps with one simple command and gain state-of-the-art or very close to state-of-the-art results for each step. In the following, we describe each of these four steps in more detail, while more detailed description of the pipeline itself is given in Section 6. 3.2 Pre-trained embeddings Where available, we used the pre-trained embeddings from the 2017 shared task (Ginter et al., 2017). Embeddings for Afrikaans, Breton, Buryat, Faroese, Gothic"
K18-2013,D15-1166,0,0.0171543,"translation toolkit and translation model implementations. Our current implementation is based on the Python version of the OpenNMT: Open-Source Toolkit for Neural Machine Translation (Klein et al., 2017). We use a deep attentional encoder-decoder network with 2 layered bidirectional LSTM encoder for reading the sequence of input characters + morphological tags and producing a sequence of encoded vectors. Our decoder is a 2 layered unidirectional LSTM with input feeding attention for generating the sequence of output characters based on the encoded representations. In input feeding attention (Luong et al., 2015) the previous attention weights are given as input in the next time step to inform the model about past alignment decisions and prevent the model to repeat the same output multiple times. We use beam search with beam size 5 during decoding. As the lemmatizer does not see the actual sentence where a word appears, morphological tags are used in the input sequence to inform the system about the word’s morpho-syntactic context. The tagger is naturally able to see the full sentence context and in most cases it should produce enough information for the lemmatizer to give it a possibility to lemmatiz"
L16-1262,W13-2308,0,0.0114723,"g diverse tag sets to a common standard. The morphological layer also builds on Interset (Zeman, 2008), which started as a tool for conversion between morphosyntactic tag sets of multiple languages. It dates back to 2006 when it was used in the first experiments with cross-lingual delexicalized parser adaptation (Zeman and Resnik, 2008). The Stanford dependencies, used in the syntactic layer, were developed for English in 2005 and eventually emerged as the de facto standard for dependency analysis of English. They have since been adapted to a number of different languages (Chang et al., 2009; Bosco et al., 2013; Haverinen et al., 2013; Seraji et al., 2013; Lipenkova and Souˇcek, 2014). These resources have featured in other attempts at universal standards. The Google Universal Dependency Treebank (UDT) project (McDonald et al., 2013) was the first attempt to combine the Stanford dependencies and the Google universal part-of-speech tags into a universal annotation scheme: treebanks were released for 6 languages in 2013 (English, French, German, Spanish, Swedish and Korean) and for 11 languages in 2014 (Brazilian Portuguese, English, Finnish, French, German, Italian, Indonesian, Japanese, Korean, Span"
L16-1262,W06-2920,0,0.443151,"clauses as an important subtype of adnominal clauses. By design, we can always map back to the core label set by stripping the specific relations that appear after the colon. For a complete list of currently used languagespecific relations, we refer to the UD website. 2 Complete guidelines for the enhanced representations have not been worked out yet, and only one treebank (Finnish) uses them so far, but see Schuster and Manning (2016) for a concrete proposal for English. 3.4. Format and Tools The data is encoded in the CoNLL-U format, which is an evolution of the widely used CoNLL-X format (Buchholz and Marsi, 2006), where each word/token is represented in tab-separated columns on one line and sentence boundaries are marked by blank lines. The 10 columns on a word/token line are used to specify a unique id (integer for words, ranges for multiword tokens), word form, lemma, universal part-of-speech tag, optional language-specific part-ofspeech tag, morphological features, head, dependency relation, additional dependencies in the enhanced representation and miscellaneous information. The format is illustrated in Figure 3, with the French sentence from Figure 2. To support work on treebanks in this format,"
L16-1262,W09-2307,1,0.329071,"standard for mapping diverse tag sets to a common standard. The morphological layer also builds on Interset (Zeman, 2008), which started as a tool for conversion between morphosyntactic tag sets of multiple languages. It dates back to 2006 when it was used in the first experiments with cross-lingual delexicalized parser adaptation (Zeman and Resnik, 2008). The Stanford dependencies, used in the syntactic layer, were developed for English in 2005 and eventually emerged as the de facto standard for dependency analysis of English. They have since been adapted to a number of different languages (Chang et al., 2009; Bosco et al., 2013; Haverinen et al., 2013; Seraji et al., 2013; Lipenkova and Souˇcek, 2014). These resources have featured in other attempts at universal standards. The Google Universal Dependency Treebank (UDT) project (McDonald et al., 2013) was the first attempt to combine the Stanford dependencies and the Google universal part-of-speech tags into a universal annotation scheme: treebanks were released for 6 languages in 2013 (English, French, German, Spanish, Swedish and Korean) and for 11 languages in 2014 (Brazilian Portuguese, English, Finnish, French, German, Italian, Indonesian, Ja"
L16-1262,P11-1061,1,0.573621,"UNCT Definite=Def Gender=Fem Number=Plur Definite=Def Gender=Masc Definite=Def Gender=Masc Number=Plur Number=Plur Person=3 Number=Plur Number=Plur Number=Sing Number=Sing Tense=Pres Figure 2: UD annotation for a French sentence. (Translation: However, girls love chocolate desserts.) 2. History 3. UD comprises two layers of annotation with diverse origins. The Google universal tag set used in the morphological layer grew out of the cross-linguistic error analysis based on the CoNLL-X shared task data by McDonald and Nivre (2007). It was initially used for unsupervised partof-speech tagging by Das and Petrov (2011), and has been adopted as a widely used standard for mapping diverse tag sets to a common standard. The morphological layer also builds on Interset (Zeman, 2008), which started as a tool for conversion between morphosyntactic tag sets of multiple languages. It dates back to 2006 when it was used in the first experiments with cross-lingual delexicalized parser adaptation (Zeman and Resnik, 2008). The Stanford dependencies, used in the syntactic layer, were developed for English in 2005 and eventually emerged as the de facto standard for dependency analysis of English. They have since been adapt"
L16-1262,W08-1301,1,0.58058,"Missing"
L16-1262,de-marneffe-etal-2006-generating,1,0.213978,"Missing"
L16-1262,de-marneffe-etal-2014-universal,1,0.831309,"Missing"
L16-1262,E14-4028,0,0.0377832,"Missing"
L16-1262,N15-3011,1,0.696846,"Missing"
L16-1262,D07-1013,1,0.230402,"hocolat . le fille adorer le dessert a` le chocolat . DET NOUN VERB DET NOUN ADP DET NOUN PUNCT Definite=Def Gender=Fem Number=Plur Definite=Def Gender=Masc Definite=Def Gender=Masc Number=Plur Number=Plur Person=3 Number=Plur Number=Plur Number=Sing Number=Sing Tense=Pres Figure 2: UD annotation for a French sentence. (Translation: However, girls love chocolate desserts.) 2. History 3. UD comprises two layers of annotation with diverse origins. The Google universal tag set used in the morphological layer grew out of the cross-linguistic error analysis based on the CoNLL-X shared task data by McDonald and Nivre (2007). It was initially used for unsupervised partof-speech tagging by Das and Petrov (2011), and has been adopted as a widely used standard for mapping diverse tag sets to a common standard. The morphological layer also builds on Interset (Zeman, 2008), which started as a tool for conversion between morphosyntactic tag sets of multiple languages. It dates back to 2006 when it was used in the first experiments with cross-lingual delexicalized parser adaptation (Zeman and Resnik, 2008). The Stanford dependencies, used in the syntactic layer, were developed for English in 2005 and eventually emerged"
L16-1262,P13-2017,1,0.877648,"Missing"
L16-1262,W15-2127,0,0.0113361,"n English, we also obtain parallel representations between prepositional phrases and subordinate clauses, which are in practice often introduced by a preposition, as in (5). nmod case nsubj (5) a. Sue 1662 det left after the rehearsal advcl nsubj b. Sue Language mark nsubj left after we did The choice to make content words the backbone of the syntactic representations may seem to be at odds with the strong tendency in modern syntactic theory to give priority to functional heads, a tendency that is found in both constituency-based and dependency-based approaches to syntax (Brug´e et al., 2012; Osborne and Maxwell, 2015). We believe, however, that this conflict is more apparent than real. The UD view is that we need to recognize both lexical and functional heads, but in order to maximize parallelism across languages, only lexical heads are inferable from the topology of our tree structures. Functional heads are instead represented as specifying features of content words, using dedicated relation labels, features which can alternatively be specified through morphological processes. In the dependency grammar tradition, this is very close to the view of Tesni`ere (1959), according to whom dependencies hold betwe"
L16-1262,petrov-etal-2012-universal,1,0.717175,"exist to build consistent resources for many languages, and the UD project is a merger of some of the initiatives. It combines the (universal) Stanford dependencies (de Marneffe et al., 2006; de Marneffe and Manning, 2008; de Marneffe et al., 2014), the universal sv: en nsubj katt conj jagar r˚attor conj och m¨oss cc conj nsubj ? da: en dobj kat jager rotter og mus conj det en: a nsubj cat dobj chases cc rats and mice Figure 1: Divergent annotation of parallel structures Google dependency scheme (Universal Dependency Treebanks) (McDonald et al., 2013), the Google universal partof-speech tags (Petrov et al., 2012), and the Interset interlingua for morphosyntactic tag sets (Zeman, 2008) used in the HamleDT treebanks (a project that transforms existing treebanks under a common annotation scheme, Zeman et al. 2012). UD is thus based on common usage and existing de facto standards, and is intended to replace all the previous versions by a single coherent standard.1 The general philosophy is to provide a universal inventory of categories and guidelines to facilitate consistent annotation of similar constructions across languages, while allowing languagespecific extensions when necessary. In this paper, we p"
L16-1262,rosa-etal-2014-hamledt,1,0.832586,"Missing"
L16-1262,L16-1376,1,0.208211,"fferent languages. For instance, while the universal UD scheme has a single relation acl for adnominal clauses, several languages make use of the subtype acl:relcl to distinguish relative clauses as an important subtype of adnominal clauses. By design, we can always map back to the core label set by stripping the specific relations that appear after the colon. For a complete list of currently used languagespecific relations, we refer to the UD website. 2 Complete guidelines for the enhanced representations have not been worked out yet, and only one treebank (Finnish) uses them so far, but see Schuster and Manning (2016) for a concrete proposal for English. 3.4. Format and Tools The data is encoded in the CoNLL-U format, which is an evolution of the widely used CoNLL-X format (Buchholz and Marsi, 2006), where each word/token is represented in tab-separated columns on one line and sentence boundaries are marked by blank lines. The 10 columns on a word/token line are used to specify a unique id (integer for words, ranges for multiword tokens), word form, lemma, universal part-of-speech tag, optional language-specific part-ofspeech tag, morphological features, head, dependency relation, additional dependencies i"
L16-1262,E12-2021,1,0.581828,"Missing"
L16-1262,stepanek-pajas-2010-querying,0,0.067769,"Missing"
L16-1262,P13-2103,1,0.625022,"hese resources have featured in other attempts at universal standards. The Google Universal Dependency Treebank (UDT) project (McDonald et al., 2013) was the first attempt to combine the Stanford dependencies and the Google universal part-of-speech tags into a universal annotation scheme: treebanks were released for 6 languages in 2013 (English, French, German, Spanish, Swedish and Korean) and for 11 languages in 2014 (Brazilian Portuguese, English, Finnish, French, German, Italian, Indonesian, Japanese, Korean, Spanish and Swedish). The first proposal for incorporating morphology was made by Tsarfaty (2013). The second version of HamleDT (Rosa et al., 2014) provided Stanford/Google annotation for 30 languages by automatically harmonizing treebanks with different native annotations. These efforts were followed by the development of the universal Stanford dependencies (USD), revising Stanford Dependencies for cross-linguistic annotations in light of the Google scheme (de Marneffe et al., 2014). UD is the result of merging all these initiatives into a single coherent framework, based on the universal Stanford dependencies, an extended version of the Google universal tag set, a revised subset of the"
L16-1262,I08-3008,1,0.20904,"the morphological layer grew out of the cross-linguistic error analysis based on the CoNLL-X shared task data by McDonald and Nivre (2007). It was initially used for unsupervised partof-speech tagging by Das and Petrov (2011), and has been adopted as a widely used standard for mapping diverse tag sets to a common standard. The morphological layer also builds on Interset (Zeman, 2008), which started as a tool for conversion between morphosyntactic tag sets of multiple languages. It dates back to 2006 when it was used in the first experiments with cross-lingual delexicalized parser adaptation (Zeman and Resnik, 2008). The Stanford dependencies, used in the syntactic layer, were developed for English in 2005 and eventually emerged as the de facto standard for dependency analysis of English. They have since been adapted to a number of different languages (Chang et al., 2009; Bosco et al., 2013; Haverinen et al., 2013; Seraji et al., 2013; Lipenkova and Souˇcek, 2014). These resources have featured in other attempts at universal standards. The Google Universal Dependency Treebank (UDT) project (McDonald et al., 2013) was the first attempt to combine the Stanford dependencies and the Google universal part-of-"
L16-1262,zeman-etal-2012-hamledt,1,0.729155,"Missing"
L16-1262,zeman-2008-reusable,1,0.897534,"erger of some of the initiatives. It combines the (universal) Stanford dependencies (de Marneffe et al., 2006; de Marneffe and Manning, 2008; de Marneffe et al., 2014), the universal sv: en nsubj katt conj jagar r˚attor conj och m¨oss cc conj nsubj ? da: en dobj kat jager rotter og mus conj det en: a nsubj cat dobj chases cc rats and mice Figure 1: Divergent annotation of parallel structures Google dependency scheme (Universal Dependency Treebanks) (McDonald et al., 2013), the Google universal partof-speech tags (Petrov et al., 2012), and the Interset interlingua for morphosyntactic tag sets (Zeman, 2008) used in the HamleDT treebanks (a project that transforms existing treebanks under a common annotation scheme, Zeman et al. 2012). UD is thus based on common usage and existing de facto standards, and is intended to replace all the previous versions by a single coherent standard.1 The general philosophy is to provide a universal inventory of categories and guidelines to facilitate consistent annotation of similar constructions across languages, while allowing languagespecific extensions when necessary. In this paper, we present version 1 of the universal guidelines and explain the underlying d"
L16-1374,de-marneffe-etal-2014-universal,1,0.91623,"Missing"
L16-1374,de-marneffe-etal-2006-generating,0,0.111941,"Missing"
L16-1374,W08-1301,0,0.127764,"Missing"
L16-1374,foth-etal-2014-size,0,0.0446328,"Missing"
L16-1374,nivre-etal-2006-maltparser,1,0.750147,"ttp://stp.lingfil.uu.se/∼mojgan/UPDT.html 3 The treebank data is extracted from the open source, validated Uppsala Persian Corpus (UPC), which is currently the largest freely available corpus of Persian. The corpus consists of 2,704,028 tokens and annotated with part-of-speech tags and morphological features. For a comprehensive description of the corpus pertaining to the tokenization and morphological annotation see Seraji (2015, Chapter 3) but annotated with special labels at the syntactic level instead. The syntactic annotation of the UPDT has been done semi-automatically using MaltParser (Nivre et al., 2006) in a bootstrapping scenario. All sentences have been manually validated. When converting the UPDT to the Persian UD, all words containing unsegmented clitics (pronominal and copula clitics) annotated with complex labels in the UPDT, were separated from the clitics and received distinct labels in the Persian UD. Figure 1 illustrates the differences between the two treebanks for a Persian sentence. In this example, the direct object (dobj) in the UPDT consists of the word mother and the possessive pronominal clitic her/his (colored in pink), marked with the label dobj/pc. This label in the UPDT"
L16-1374,petrov-etal-2012-universal,0,0.137394,"Missing"
L16-1374,W15-2133,1,0.849148,"e been separated from the clitics and appear with distinct labels in the Persian UD. The treebank has its original syntactic annotation scheme based on Stanford Typed Dependencies. In this paper, we present the approaches taken in the development of the Persian UD. Keywords: Universal Dependencies, Persian, Treebank 1. Introduction In the past decade, the development of numerous dependency parsers for different languages has frequently been benefited by the use of syntactically annotated resources, or treebanks (B¨ohmov´a et al., 2003; Haverinen et al., 2010; Kromann, 2003; Foth et al., 2014; Seraji et al., 2015; Vincze et al., 2010). However, treebanks only exist for a small number of languages, and considering the number of 7,000+ languages in the world,1 a large number of languages still lack treebanks. Due to the diverse typologies and grammatical structures that exist across languages, treebanks are created with different annotation schemes. These annotation variations can further be explained by different linguistic theories and the syntactic annotations that treebank developers select based on their own preferences (Nivre, 2015). These dissimilarities in annotation schemes often have an impact"
L16-1374,zeman-2008-reusable,0,0.112212,"n that having a common standard and cross-linguistically valid annotation scheme would favor parsing research. Recently, there have been a number of initiatives for developing data sets with cross-linguistically consistent annotation scheme for morphological and syntactic structures. These efforts have resulted in the emergence of the Stanford Typed Dependencies Representation (de Marneffe et al., 2006; de Marneffe and Manning, 2008), the Google Universal Part-of-Speech Tagset (Petrov et al., 2012), and Interset interlingua for morphosyntactic features used in the HamleDT treebank collection (Zeman, 2008; Zeman et al., 2012). The most recent effort is the Universal Dependencies (UD), which more or less combine all the earlier efforts in this regard. In 1 http://www.bbc.co.uk/languages/guide/languages.shtml this paper, we present how we adapt the Universal Dependencies to Persian by converting the Uppsala Persian Dependency Treebank (UPDT) (Seraji, 2015) to the Persian Universal Dependencies (Persian UD). First, we briefly describe the Universal Dependencies and then we present the morphosyntactic annotations used in the extended version of the Persian UD. 2. Universal Dependencies Universal D"
L16-1374,zeman-etal-2012-hamledt,0,0.0364471,"Missing"
L18-1290,K17-3002,0,0.0273794,"aux advmod det cop nsubj advmod nmod nsubj But not always do those three agree and not always are their decisions equal CCONJ PART ADV AUX DET NUM VERB CCONJ PART ADV AUX PRON NOUN ADJ Figure 3: An example of a matched sentence (before conversion). 5.1. Input Data Our methodology can in principle be applied to any UD treebank. The dataset and experiments presented in this paper are based on Czech, English and Finnish treebanks from UD 2.1 (Nivre et al., 2017). In addition, large web corpora of the three languages (Zeman et al., 2017) (Ginter et al., 2017) were parsed by two parsers (Stanford (Dozat et al., 2017) and Baseline UDPipe (Zeman et al., 2017) entries in the CoNLL17 Shared Task on Multilingual Parsing from Raw Text to Universal Dependencies) and used as an additional source of trees to get more candidate material. After double parsing, only trees with identical analysis were kept to ensure the quality of the automatic parses. An informal manual inspection of these trees confirmed that the quality is sufficient. All input data are in the CoNLL-U format.3 For English, there are four UD treebanks: “Original”, LinES, ParTUT, PUD. For Czech, we used the training parts of “Original”, CAC and FicTr"
L18-1290,W17-0406,1,0.349262,"et al., 2016). The annotation style of UD does not mark ellipsis explicitly when it does not have to: most types are solved by simply promoting one orphaned dependent to the position of its missing parent. Admittedly, there are treebanks that overtly annotate a wider range of elliptical structures. Our main reason for working with UD is practical: substantial data is available in this annotation style for several dozens of languages, and state-of-the art parsers have been trained and tested on UD. The one exception where UD explicitly marks ellipsis are certain types of gapping and stripping (Droganova and Zeman, 2017), where multiple orphaned dependents of a missing predicate have to be connected using a special relation called orphan (Figure 1). In the present work we investigate how frequent are the orphan relations in data, how well can existing parsers learn to recognize them, and how can we extend the data to provide more training material and improve parsing accuracy. 2. Data For the purpose of the experiments we use the system outputs from the CoNLL 2017 Shared Task (Zeman et al., 2017), that are now available as a corpus. We chose 12 teams whose systems surpassed baseline results (Zeman et al., 201"
L18-1290,Q16-1025,0,0.0212181,"English English web Finnish web Related Work The idea of artificial generation or modification of corpora is not new and it has been occasionally applied to various areas of language learning, whenever the studied phenomenon is underrepresented in existing resources. To name just a few: In (van der Plas et al., 2009), creation of an artificial treebank from an existing text treebank helps to overcome domain differences. (Khoshnavataher et al., 2015) artificialy modify text to look like obfuscated plagiarism; the resulting corpus is used to train a plagiarismdetecting system for Persian. And (Gulordava and Merlo, 2016) generate word-order permutations to study the imInitial 1.7M / 102K 23M / 2M 408K / 24K 883K / 89K 31M / 4.3M Processed 13K / 498 37K / 2369 6.8K / 284 6.4K / 422 31K / 2442 Manual NA NA 3.7K / 183 3.6K / 238 13K / 1000 Table 3: The size of the data. Initial: the size of the input data, tokens/sentences; Processed: the size of the data after the application of the conversion pipeline, tokens/sentences; Manual: the size of the data after manual correction, tokens/sentences 1851 pact of word order on parsing accuracy in twelve different languages. 8. Conclusion We have presented experiments tha"
L18-1290,L16-1262,1,0.886738,"Missing"
L18-1290,W17-0412,0,0.0295645,"Missing"
L18-1290,W17-0416,0,0.0210676,"s contain a “nsubj” (subject) and an “advmod” (adverbial modifier). After transformation the sentence would lose an adjective and its dependent. The new structure is shown in Figure 4. It should be mentioned that patterns do not require a particular word order, only particular dependents. Thus the sentence in Figure 5 is a match as well. The methodology requires manual efforts. After application of the script, the data have to be checked and corrected: • After artificial omission sentences must remain grammatically correct (Figure 10, Figure 11); Creating Artificial Treebanks Recent research (Schuster et al., 2017; Droganova and Zeman, 2017) provides a detailed overview of elliptical constructions within the UD framework and presents typical patterns that can be used for detection of elliptic constructions. This information allows us to develop a script that transforms non-elliptic UD style trees to elliptic trees. Figure 2 shows a subtree pattern that matches sentences where gapping (Johnson, 2009) could potentially occur (but 1846 • The patterns are designed to match as many instances as possible, so the erroneous instances have to be filtered out or manually corrected. All sentences at Figures 3, 5"
L18-1290,W17-7604,0,0.0123989,"pronoun, we create an elliptical sentence where two actors perform presumably the same action but with different patients (Figure 6, Figure 7). Another example would be a rule for copular constructions. If the main clause contains a copula, the sentence must be converted into Type 2 structure (Figure 8, Results We provide artificial ellipsis treebanks for three languages, Czech, English and Finnish, using our processing pipeline explained in previous sections. Furthermore, the data for English and Finnish is manually checked and fixed to be grammatical and naturally-sound using UD Annotatrix (Tyers et al., 2017) annotation tool. This further ensures the 1850 conj cc obj xcomp advmod obj det obl amod Tunsin itseni onnelliseksi mutta samaan aikaan tunsin my¨os syv¨aa¨ kaipausta I felt myself happy but at the same time I felt also deep yearning VERB PRON ADJ CCONJ PRON NOUN VERB ADV ADJ NOUN Figure 16: An example of a Finnish sentence automatically identified in the parsed Finnish web corpus. conj cc orphan xcomp advmod obj det amod Tunsin itseni onnelliseksi mutta samaan aikaan my¨os syv¨aa¨ kaipausta I felt myself happy but at the same time also deep yearning VERB PRON ADJ CCONJ PRON NOUN ADV ADJ NOUN"
L18-1290,N09-2032,0,0.0229589,"Missing"
L18-1290,K17-3001,1,0.889728,"Missing"
N15-3011,P13-4010,0,0.0733617,"Missing"
N15-3011,levy-andrew-2006-tregex,0,0.118323,"Missing"
P15-4016,H94-1020,0,0.545178,"DF serializations: JSON-LD, N-Triples and N-Quads, Notation3, RDF/XML, TriG, TriX, and Turtle. With the exception of named graphs for serializations that do not support them, conversion between these representations is guaranteed to preserve all information. In addition to the general, reversible format translation services provided by the OA Adapter, we provide scripts for offline conversion of various annotation file formats into the OA JSON-LD format to allow existing datasets to be imported into OA stores. The following are currently supported: Penn Treebank format (including PTB II PAS) (Marcus et al., 1994), a number of variants of CoNLL formats, including CoNLL-U,5 Knowtator XML (Ogren, 2006), and the standoff format used by the BRAT annotation tool (Stenetorp et al., 2012). We also provide supporting tools for importing files with OA JSON-LD data to a store and exporting to files over the RESTful OA API. Validation OA JSON-LD data can be validated on three levels: 1) whether the data is syntactically wellformed JSON, 2) whether it conforms to the JSON-LD specification, and 3) whether the abstract information content fulfills the OA data model. The first two can be accomplished using any one of"
P15-4016,N06-4006,0,0.0351016,"ith the exception of named graphs for serializations that do not support them, conversion between these representations is guaranteed to preserve all information. In addition to the general, reversible format translation services provided by the OA Adapter, we provide scripts for offline conversion of various annotation file formats into the OA JSON-LD format to allow existing datasets to be imported into OA stores. The following are currently supported: Penn Treebank format (including PTB II PAS) (Marcus et al., 1994), a number of variants of CoNLL formats, including CoNLL-U,5 Knowtator XML (Ogren, 2006), and the standoff format used by the BRAT annotation tool (Stenetorp et al., 2012). We also provide supporting tools for importing files with OA JSON-LD data to a store and exporting to files over the RESTful OA API. Validation OA JSON-LD data can be validated on three levels: 1) whether the data is syntactically wellformed JSON, 2) whether it conforms to the JSON-LD specification, and 3) whether the abstract information content fulfills the OA data model. The first two can be accomplished using any one of the available libraries that implement the full JSON-LD syntax and API specifications.6"
P15-4016,W12-3610,0,0.0188254,"format (Sporny et al., 2014) and is the recommended serialization of OA. Every JSON-LD document is both a JSON document and a representation of RDF data. Figure 2 shows an example of a simple annotation using the OA JSON-LD representation.2 ""@id"": ""@type"": ""target"": ""body"": Action Read annotation Read all annotations Update annotation Delete annotation Create annotation Table 1: HTTP verbs, resources, and actions. Read-only services support only the two GET requests. is an RDF-based graph representation compatible with linguistic annotation formalisms such as LAF/GrAF (Ide and Suderman, 2007; Verspoor and Livingston, 2012). At its most basic level, the OA model differentiates between three key components: annotation, body, and target, where the annotation expresses that the body is related to the target of the annotation (Figure 1). The body can carry arbitrarily complex embedded data. { Resource Annotation Collection Annotation Annotation Collection 3.1 OA Store The OA Store is a reference implementation of persistent, server-side annotation storage that allows clients to create, read, update and delete annotations using the API. The store uses MongoDB, which is well suited to the task as it is a documentorien"
P15-4016,wright-2014-restful,0,0.0306169,"o all four databases. They query a PostgreSQL back-end for text and annotations, which are formatted as OA JSON-LD using the standard Python json module. 4.4 5 Related work Our approach builds directly on the OA data model (Bradshaw et al., 2013), which harmonizes the earlier Open Annotation Collaboration (Haslhofer et al., 2011) and Annotation Ontology Initiative (Ciccarese et al., 2011) efforts and is currently developed further under the auspices of the W3C Web Annotation WG.8 Approaches building on RESTful architectures and JSON-LD are also being pursued by the Linguistic Data Consortium (Wright, 2014) and the Language Application Grid (Ide et al., 2014), among others. A number of annotation stores following similar protocols have also been released recently, including Lorestore (Hunter and Gerber, 2012), PubAnnotation (Kim and Wang, 2012), the Annotator.js store9 , and NYU annotations10 . 6 Conclusions and future work We have proposed to share annotations using a minimal RESTful interface for Open Annotation data in JSON-LD. We introduced reference implementations of a server, client, validation and conversion tools, and demonstrated the integration of several independently developed annot"
P15-4016,W07-1501,0,\N,Missing
P15-4016,W12-2425,0,\N,Missing
P15-4016,ide-etal-2014-language,0,\N,Missing
P15-4016,E12-2021,1,\N,Missing
Q13-1034,C00-2143,1,0.361351,"bank covered by morphological analyzer. Clusters: number of tokens and types in unlabeled corpus. treebank annotation we have to rely on a heuristic mapping between the two. Word clusters are derived from the so-called Huge German Corpus.7 Hungarian For training and test we use the Szeged Dependency Treebank (Farkas et al., 2012). We use a finite-state morphological analyzer constructed from the morphdb.hu lexical resource (Tr´on et al., 2006), and word clusters come from the Hungarian National Corpus (V´aradi, 2002). Russian Parsers are trained and tested on data from the SynTagRus Treebank (Boguslavsky et al., 2000; Boguslavsky et al., 2002). The morphological analyzer is a module of the ETAP-3 linguistic processor (Apresian et al., 2003) with a dictionary comprising more than 130,000 lexemes (Iomdin and Sizov, 2008). Word clusters have been produced on the basis of an unlabeled corpus of Russian compiled by the Russian Language Institute of the Russian Academy of Sciences and tokenized by the ETAP-3 analyzer. 4 Joint Morphology and Syntax We start by exploring different ways of integrating morphology and syntax in a data-driven setting, that is, where our only knowledge source is the annotated training"
Q13-1034,boguslavsky-etal-2002-development,1,0.676986,"cal analyzer. Clusters: number of tokens and types in unlabeled corpus. treebank annotation we have to rely on a heuristic mapping between the two. Word clusters are derived from the so-called Huge German Corpus.7 Hungarian For training and test we use the Szeged Dependency Treebank (Farkas et al., 2012). We use a finite-state morphological analyzer constructed from the morphdb.hu lexical resource (Tr´on et al., 2006), and word clusters come from the Hungarian National Corpus (V´aradi, 2002). Russian Parsers are trained and tested on data from the SynTagRus Treebank (Boguslavsky et al., 2000; Boguslavsky et al., 2002). The morphological analyzer is a module of the ETAP-3 linguistic processor (Apresian et al., 2003) with a dictionary comprising more than 130,000 lexemes (Iomdin and Sizov, 2008). Word clusters have been produced on the basis of an unlabeled corpus of Russian compiled by the Russian Language Institute of the Russian Academy of Sciences and tokenized by the ETAP-3 analyzer. 4 Joint Morphology and Syntax We start by exploring different ways of integrating morphology and syntax in a data-driven setting, that is, where our only knowledge source is the annotated training corpus. At both learning a"
Q13-1034,E12-1009,1,0.573095,"ther support for this choice, at least for the languages considered in this paper. Note also that the choice is not motivated by efficiency concerns, since increasing the values of kp and km has only a marginal effect on running time, as explained in Section 2.4. Finally, the choice not to consider k-best lemmas is dictated by the fact that our lemmatizer only provides a 1-best analysis. For the first three models, we use the same feature representations as Bohnet and Nivre (2012),9 consisting of their adaptation of the features used by Zhang and Nivre (2011), the graph completion features of Bohnet and Kuhn (2012), and the special features over k-best tags introduced specifically for joint tagging and parsing by Bohnet and Nivre (2012). For the J OINT model, we simply add features over the k-best morphological descriptions analogous to the features over k-best tags.10 Experimental results for these four models can be found in Table 2. From the P IPELINE results, we see that the 1-best accuracy of the preprocessing tagger ranges from 95.0 (Finnish) to 99.2 (Czech) for POS, and from 89.4 (Finnish) to 96.5 (Hungarian) for MOR. The lemmatizer does a good job for four of the languages (93.9–97.9) but has re"
Q13-1034,D12-1133,1,0.0763756,"s, which tend to assume that all morphological disambiguation has been performed before syntactic analysis begins. However, as argued by Lee et al. (2011), in morphologically rich languages there is often considerable interaction between morphology and syntax, such that neither can be disambiguated without the other. Lee et al. (2011) go on to show that a discriminative model for joint morphological disambiguation and dependency parsing gives consistent improvements in morphological and syntactic accuracy, compared to a pipeline model, for Ancient Greek, Czech, Hungarian and Latin. Similarly, Bohnet and Nivre (2012) propose a model for 1 See https://sites.google.com/site/spmrl2013/home/sharedtask. 415 Transactions of the Association for Computational Linguistics, 1 (2013) 415–428. Action Editor: Brian Roark. c Submitted 7/2013; Revised 9/2013; Published 10/2013. 2013 Association for Computational Linguistics. joint part-of-speech tagging and dependency parsing and report improved accuracy for Czech and German (but also for Chinese and English), although in this case the joint model is limited to basic part-ofspeech tags and does not involve the full complex of morphological features. An integrated approa"
Q13-1034,C10-1011,1,0.684406,"nguages. For Czech, the best previous UAS on the standard train-test split of the PDT is 87.32, reported by Koo et al. (2010) with a parser using non-projective head automata and dual decomposition, while the best LAS is 78.82 LAS from Nilsson et al. (2006), using a greedy arc-eager transitionbased system with pseudo-projective parsing. Our best results are 1.7 percentage points better for UAS (89.0) and almost 5 percentage points better for LAS (83.7).14 For Finnish, the only previous results are from Haverinen et al. (2013), who achieve 81.01 LAS and 84.97 UAS with the graph-based parser of Bohnet (2010). We get substantial improvements with 83.1 LAS and 86.6 UAS. We also improve slightly over their best POS score, obtained with the HunPos tagger (Hal´acsy et al., 2007) together with the OMorFi analyzer (95.7 vs. 95.4). For German, the best previous results on the same train-test split are from Seeker and Kuhn (2012), using the graphbased parser of Bohnet (2010) in a pipeline architecture. With the same evaluation setup as in this paper, they achieve 91.50 LAS and 93.48 UAS – 13 L EX S OFT averages 0.132 ms per sentence on an Intel i73930K processor with 6 cores, against 0.112 ms for P IPELIN"
Q13-1034,J92-4003,0,0.242366,"striking for German, where the soft lexical constraints are clearly beneficial (especially for the MOR score) despite not being quite compatible with the morphological descriptions in the training set. In terms of statistical signifance, L EX S OFT outperforms the J OINT model with respect to the PMD score for all languages (p < 0.01). It is also significantly better than L EX H ARD for all languages except Finnish (p < 0.01). 6 Word Clusters Finally, we add word cluster features to the best model for each language (L EX H ARD for Finnish, L EX S OFT for the others).11 We use Brown clusters (Brown et al., 1992), with 800 clusters for all languages, and we use the same feature representation as Bohnet and Nivre (2012). The results in Table 2 show small but consistent improvements in almost all metrics for all languages, confirming the benefit of cluster features for morphologically rich languages. It is worth noting that we see the biggest improvement for Finnish, the language with the smallest training set and therefore most likely to 11 The best model was selected according to results on the dev set (cross-validation on the training set for Finnish). suffer from sparse data, where the syntactic acc"
Q13-1034,W06-2920,0,0.719913,"re languages, it has also been observed that typological differences between languages lead to new challenges. In particular, it has been found over and over again that languages exhibiting rich morphological structure, often together with a relatively free word order, usually obtain lower parsing accuracy, especially in comparison to English. One striking demonstration of this tendency can be found in the CoNLL shared tasks on multilingual dependency parsing, organized in 2006 and 2007, where richly inflected languages clustered at the lower end of the scale with respect to parsing accuracy (Buchholz and Marsi, 2006; Nivre et al., 2007). These and similar observations have led to an increased interest in the special challenges posed by parsing morphologically rich languages, as evidenced most clearly by a new series of workshops devoted to this topic (Tsarfaty et al., 2010), as well as a special issue in Computational Linguistics (Tsarfaty et al., 2013) and a shared task on parsing morphologically rich languages.1 One hypothesized explanation for the lower parsing accuracy observed for richly inflected languages is the strict separation of morphological and syntactic analysis assumed in many parsing fram"
Q13-1034,D07-1022,0,0.0694561,"ish), although in this case the joint model is limited to basic part-ofspeech tags and does not involve the full complex of morphological features. An integrated approach to morphological and syntactic analysis can also be found in grammar-based dependency parsers, such as the ETAP-3 linguistic processor (Apresian et al., 2003), where morphological disambiguation is mostly carried out together with syntactic analysis. Finally, it is worth noting that joint models of morphology and syntax have been more popular in constituency-based statistical parsing (Cowan and Collins, 2005; Tsarfaty, 2006; Cohen and Smith, 2007; Goldberg and Tsarfaty, 2008). Another hypothesis from the literature is that the high type-token ratio resulting from large morphological paradigms leads to data sparseness when estimating the parameters of a statistical parsing model (Tsarfaty et al., 2010; Tsarfaty et al., 2013). In particular, for many words in the language, only a subset of its morphological forms will be observed at training time. This suggests that using rule-based morphological analyzers or other lexical resources may be a viable strategy to improve coverage and performance. Thus, Goldberg and Elhadad (2013) show that"
Q13-1034,D11-1114,0,0.0374383,"Missing"
Q13-1034,P04-1015,0,0.261816,"put sentence x with weight vector w. The symbols h.c, h.s and h.f denote, respectively, the configuration, score and feature vector of a hypothesis h; Γc denotes the MS-parse defined by c. to 0.0, make N iterations over the training data and update the weight vector for every sentence x where the transition sequence C0,m corresponding to the gold parse is different from the highest scoring tran4 More precisely, we use the ∗ sition sequence C0,m 0. passive-aggressive update of Crammer et al. (2006). We also use the early update strategy found beneficial for parsing in several previous studies (Collins and Roark, 2004; Zhang and Clark, 2008; Huang and Sagae, 2010). This means that, at learning time, we terminate the beam search as soon as the hypothesis corresponding to the gold parse is pruned from the beam and then update with respect to the partial transition sequences constructed up to that point. Finally, we use the standard technique of averaging over all weight vectors seen in training, as originally proposed by Collins (2002). 4 Note that there may be more than one transition sequence corresponding to the gold parse, in which case we pick the canonical transition sequence that processes all left-de"
Q13-1034,W02-1001,0,0.162454,"inear for natural language data sets, due to the sparsity of non-projective dependencies (Nivre, 2009). The running time is also linear in |D |+ |P × M |, which means that joint prediction only gives a linear increase in running time, often quite marginal because |D |> |P × M |. This assumes that the lemma is predicted deterministically given a tag and a morphological description, an assumption that is enforced in all our experiments. 2.5 Learning In order to learn a weight vector w from a training set of sentences with gold parses, we use a variant of the structured perceptron, introduced by Collins (2002) and first used for transition-based parsing by Zhang and Clark (2008). We initialize all weights 3 While there exist exact dynamic programming algorithms for projective transition systems (Huang and Sagae, 2010; Kuhlmann et al., 2011) and even for restricted non-projective systems (Cohen et al., 2011), parsing is intractable for systems like ours that permit arbitrary non-projective trees. PARSE(x, w) 1 h0 .c ← cs (x) 2 h0 .s ← 0.0 3 h0 .f ← {0.0}dim(w) 4 B EAM ← [h0 ] 5 while ∃h ∈ B EAM : h.c 6∈ Ct 6 T MP ← [ ] 7 foreach h ∈ B EAM 8 foreach t ∈ T : P ERMISSIBLE(h.c, t) 9 h.f ← h.f + f(x, h.c"
Q13-1034,H05-1100,0,0.0303135,"and German (but also for Chinese and English), although in this case the joint model is limited to basic part-ofspeech tags and does not involve the full complex of morphological features. An integrated approach to morphological and syntactic analysis can also be found in grammar-based dependency parsers, such as the ETAP-3 linguistic processor (Apresian et al., 2003), where morphological disambiguation is mostly carried out together with syntactic analysis. Finally, it is worth noting that joint models of morphology and syntax have been more popular in constituency-based statistical parsing (Cowan and Collins, 2005; Tsarfaty, 2006; Cohen and Smith, 2007; Goldberg and Tsarfaty, 2008). Another hypothesis from the literature is that the high type-token ratio resulting from large morphological paradigms leads to data sparseness when estimating the parameters of a statistical parsing model (Tsarfaty et al., 2010; Tsarfaty et al., 2013). In particular, for many words in the language, only a subset of its morphological forms will be observed at training time. This suggests that using rule-based morphological analyzers or other lexical resources may be a viable strategy to improve coverage and performance. Thus"
Q13-1034,E12-1007,1,0.93185,"22 1105 33 151,971 71,263 200,249,814 538,138 14 454 78 97,905 35,039 195,897,041 639,446 Table 1: Statistics about data sets and resources used in the experiments. Treebank: number of tokens in data sets; number of labels in label sets. Morphology: number of word forms and lemmas in treebank covered by morphological analyzer. Clusters: number of tokens and types in unlabeled corpus. treebank annotation we have to rely on a heuristic mapping between the two. Word clusters are derived from the so-called Huge German Corpus.7 Hungarian For training and test we use the Szeged Dependency Treebank (Farkas et al., 2012). We use a finite-state morphological analyzer constructed from the morphdb.hu lexical resource (Tr´on et al., 2006), and word clusters come from the Hungarian National Corpus (V´aradi, 2002). Russian Parsers are trained and tested on data from the SynTagRus Treebank (Boguslavsky et al., 2000; Boguslavsky et al., 2002). The morphological analyzer is a module of the ETAP-3 linguistic processor (Apresian et al., 2003) with a dictionary comprising more than 130,000 lexemes (Iomdin and Sizov, 2008). Word clusters have been produced on the basis of an unlabeled corpus of Russian compiled by the Rus"
Q13-1034,W09-1205,0,0.0234606,"binary), and w is a weight vector of the same dimensionality, where each component wi is the real-valued weight of the feature fi (x, c, t). The choice of features to include in f (x, c, t) is discussed separately for each instantiation of the model in Sections 4–6. 2 Hatori et al. (2011) previously made the same modification to the arc-standard system (Nivre, 2004), without the S WAP transition. Similarly, Titov and Henderson (2007) added a word parameter to the S HIFT transition to get a joint model of word strings and dependency trees. A similar model was considered but finally not used by Gesmundo et al. (2009). 418 2.4 Decoding Exact decoding for transition-based parsing is hard in general.3 Early transition-based parsers mostly relied on greedy, deterministic decoding, which makes for very efficient parsing (Yamada and Matsumoto, 2003; Nivre, 2003), but research has shown that accuracy can be improved by using beam search instead (Zhang and Clark, 2008; Zhang and Nivre, 2012). While still not exact, beam search decoders explore a larger part of the search space than greedy parsers, which is likely to be especially important for joint models, where the search space is larger than for plain dependen"
Q13-1034,J13-1007,0,0.0248062,"Tsarfaty, 2006; Cohen and Smith, 2007; Goldberg and Tsarfaty, 2008). Another hypothesis from the literature is that the high type-token ratio resulting from large morphological paradigms leads to data sparseness when estimating the parameters of a statistical parsing model (Tsarfaty et al., 2010; Tsarfaty et al., 2013). In particular, for many words in the language, only a subset of its morphological forms will be observed at training time. This suggests that using rule-based morphological analyzers or other lexical resources may be a viable strategy to improve coverage and performance. Thus, Goldberg and Elhadad (2013) show that integrating an external wide-coverage lexicon with a treebank-trained PCFG parser improves parsing accuracy for Modern Hebrew, which is in line with earlier studies of part-of-speech tagging for morphologically rich languages (Hajiˇc, 2000). The sparsity of lexical features can also be tackled by the use of distributional word clusters as pioneered by Koo et al. (2008). In this paper, we present a transition-based model that jointly predicts complex morphological representations and dependency relations, generalizing the approach of Bohnet and Nivre (2012) to include the full range"
Q13-1034,P08-1043,0,0.0558272,"case the joint model is limited to basic part-ofspeech tags and does not involve the full complex of morphological features. An integrated approach to morphological and syntactic analysis can also be found in grammar-based dependency parsers, such as the ETAP-3 linguistic processor (Apresian et al., 2003), where morphological disambiguation is mostly carried out together with syntactic analysis. Finally, it is worth noting that joint models of morphology and syntax have been more popular in constituency-based statistical parsing (Cowan and Collins, 2005; Tsarfaty, 2006; Cohen and Smith, 2007; Goldberg and Tsarfaty, 2008). Another hypothesis from the literature is that the high type-token ratio resulting from large morphological paradigms leads to data sparseness when estimating the parameters of a statistical parsing model (Tsarfaty et al., 2010; Tsarfaty et al., 2013). In particular, for many words in the language, only a subset of its morphological forms will be observed at training time. This suggests that using rule-based morphological analyzers or other lexical resources may be a viable strategy to improve coverage and performance. Thus, Goldberg and Elhadad (2013) show that integrating an external wide-"
Q13-1034,P98-1080,1,0.597993,"Missing"
Q13-1034,A00-2013,0,0.0703501,"Missing"
Q13-1034,P07-2053,0,0.0543962,"Missing"
Q13-1034,I11-1136,0,0.0554181,"hang and Clark (2008), we assume that the score is given by a linear model whose feature representations decompose in the same way: s(x, C0,m ) = f (x, C0,m ) · w m X = f (x, ci , ti ) · w (2) i=0 Here, f (x, c, t) is a high-dimensional feature vector, where each component fi (x, c, t) is a nonnegative numerical feature (usually binary), and w is a weight vector of the same dimensionality, where each component wi is the real-valued weight of the feature fi (x, c, t). The choice of features to include in f (x, c, t) is discussed separately for each instantiation of the model in Sections 4–6. 2 Hatori et al. (2011) previously made the same modification to the arc-standard system (Nivre, 2004), without the S WAP transition. Similarly, Titov and Henderson (2007) added a word parameter to the S HIFT transition to get a joint model of word strings and dependency trees. A similar model was considered but finally not used by Gesmundo et al. (2009). 418 2.4 Decoding Exact decoding for transition-based parsing is hard in general.3 Early transition-based parsers mostly relied on greedy, deterministic decoding, which makes for very efficient parsing (Yamada and Matsumoto, 2003; Nivre, 2003), but research has show"
Q13-1034,P10-1110,0,0.18704,"near increase in running time, often quite marginal because |D |> |P × M |. This assumes that the lemma is predicted deterministically given a tag and a morphological description, an assumption that is enforced in all our experiments. 2.5 Learning In order to learn a weight vector w from a training set of sentences with gold parses, we use a variant of the structured perceptron, introduced by Collins (2002) and first used for transition-based parsing by Zhang and Clark (2008). We initialize all weights 3 While there exist exact dynamic programming algorithms for projective transition systems (Huang and Sagae, 2010; Kuhlmann et al., 2011) and even for restricted non-projective systems (Cohen et al., 2011), parsing is intractable for systems like ours that permit arbitrary non-projective trees. PARSE(x, w) 1 h0 .c ← cs (x) 2 h0 .s ← 0.0 3 h0 .f ← {0.0}dim(w) 4 B EAM ← [h0 ] 5 while ∃h ∈ B EAM : h.c 6∈ Ct 6 T MP ← [ ] 7 foreach h ∈ B EAM 8 foreach t ∈ T : P ERMISSIBLE(h.c, t) 9 h.f ← h.f + f(x, h.c, t) 10 h.s ← h.s + f(x, h.c, t) · w 11 h.c ← t(h.c) 12 T MP ← I NSERT(h, T MP) 13 B EAM ← P RUNE(T MP) 14 h∗ ← T OP(B EAM) 15 return Γh∗c Figure 2: Beam search algorithm for finding the best MSparse for input s"
Q13-1034,P08-1068,0,0.0257504,"rphological forms will be observed at training time. This suggests that using rule-based morphological analyzers or other lexical resources may be a viable strategy to improve coverage and performance. Thus, Goldberg and Elhadad (2013) show that integrating an external wide-coverage lexicon with a treebank-trained PCFG parser improves parsing accuracy for Modern Hebrew, which is in line with earlier studies of part-of-speech tagging for morphologically rich languages (Hajiˇc, 2000). The sparsity of lexical features can also be tackled by the use of distributional word clusters as pioneered by Koo et al. (2008). In this paper, we present a transition-based model that jointly predicts complex morphological representations and dependency relations, generalizing the approach of Bohnet and Nivre (2012) to include the full range of morphological information. We start by investigating different ways of integrating morphological features into the model, go on to examine the effect of using rule-based morphological analyzers to derive hard or soft constraints on the morphological analysis, and finally add word cluster features to combat lexical sparsity. We evaluate our methods on data from Czech, Finnish,"
Q13-1034,D10-1125,0,0.0274644,"Missing"
Q13-1034,P11-1068,0,0.0182537,"Missing"
Q13-1034,P11-1089,0,0.0278511,"al., 2010), as well as a special issue in Computational Linguistics (Tsarfaty et al., 2013) and a shared task on parsing morphologically rich languages.1 One hypothesized explanation for the lower parsing accuracy observed for richly inflected languages is the strict separation of morphological and syntactic analysis assumed in many parsing frameworks (Tsarfaty et al., 2010; Tsarfaty et al., 2013). This is true in particular for data-driven dependency parsers, which tend to assume that all morphological disambiguation has been performed before syntactic analysis begins. However, as argued by Lee et al. (2011), in morphologically rich languages there is often considerable interaction between morphology and syntax, such that neither can be disambiguated without the other. Lee et al. (2011) go on to show that a discriminative model for joint morphological disambiguation and dependency parsing gives consistent improvements in morphological and syntactic accuracy, compared to a pipeline model, for Ancient Greek, Czech, Hungarian and Latin. Similarly, Bohnet and Nivre (2012) propose a model for 1 See https://sites.google.com/site/spmrl2013/home/sharedtask. 415 Transactions of the Association for Computa"
Q13-1034,P06-1033,1,0.867756,"Missing"
Q13-1034,W09-3811,1,0.599747,"ng time, we terminate the beam search as soon as the hypothesis corresponding to the gold parse is pruned from the beam and then update with respect to the partial transition sequences constructed up to that point. Finally, we use the standard technique of averaging over all weight vectors seen in training, as originally proposed by Collins (2002). 4 Note that there may be more than one transition sequence corresponding to the gold parse, in which case we pick the canonical transition sequence that processes all left-dependents before right-dependents and applies the lazy swapping strategy of Nivre et al. (2009). 419 3 Data Sets and Resources Throughout the paper, we experiment with data from five languages: Czech, Finnish, German, Hungarian, and Russian. For each language, we use a morphologically and syntactically annotated corpus (treebank), divided into a training set, a development set and a test set. In addition, we use a lexicon generated by a rule-based morphological analyzer, and distributional word clusters derived from a large unlabeled corpus. Below we describe the specific resources used for each language. Table 1 provides descriptive statistics about the resources. Czech For training an"
Q13-1034,W03-3017,1,0.552345,"ections 4–6. 2 Hatori et al. (2011) previously made the same modification to the arc-standard system (Nivre, 2004), without the S WAP transition. Similarly, Titov and Henderson (2007) added a word parameter to the S HIFT transition to get a joint model of word strings and dependency trees. A similar model was considered but finally not used by Gesmundo et al. (2009). 418 2.4 Decoding Exact decoding for transition-based parsing is hard in general.3 Early transition-based parsers mostly relied on greedy, deterministic decoding, which makes for very efficient parsing (Yamada and Matsumoto, 2003; Nivre, 2003), but research has shown that accuracy can be improved by using beam search instead (Zhang and Clark, 2008; Zhang and Nivre, 2012). While still not exact, beam search decoders explore a larger part of the search space than greedy parsers, which is likely to be especially important for joint models, where the search space is larger than for plain dependency parsing without morphology (even more so with the S WAP transition for nonprojectivity). Figure 2 outlines the beam search algorithm used for decoding with our model. Different instantiations of the model will require slightly different impl"
Q13-1034,W04-0308,1,0.586101,"e representations decompose in the same way: s(x, C0,m ) = f (x, C0,m ) · w m X = f (x, ci , ti ) · w (2) i=0 Here, f (x, c, t) is a high-dimensional feature vector, where each component fi (x, c, t) is a nonnegative numerical feature (usually binary), and w is a weight vector of the same dimensionality, where each component wi is the real-valued weight of the feature fi (x, c, t). The choice of features to include in f (x, c, t) is discussed separately for each instantiation of the model in Sections 4–6. 2 Hatori et al. (2011) previously made the same modification to the arc-standard system (Nivre, 2004), without the S WAP transition. Similarly, Titov and Henderson (2007) added a word parameter to the S HIFT transition to get a joint model of word strings and dependency trees. A similar model was considered but finally not used by Gesmundo et al. (2009). 418 2.4 Decoding Exact decoding for transition-based parsing is hard in general.3 Early transition-based parsers mostly relied on greedy, deterministic decoding, which makes for very efficient parsing (Yamada and Matsumoto, 2003; Nivre, 2003), but research has shown that accuracy can be improved by using beam search instead (Zhang and Clark,"
Q13-1034,P09-1040,1,0.939947,"Γ = (A, π, µ, λ, δ) is an MS-parse for x. We take the initial configuration for a sentence x = w1 , . . . , wn to be cs (x) = ([0], [1, . . . , n], (∅, ⊥, ⊥, ⊥, ⊥)), where ⊥ is the function that is undefined for all arguments, and we take the set Ct of terminal configurations to be the set of all configurations of the form c = ([0], [ ], Γ) (for any Γ). The MS-parse defined for x by c = (Σ, B, (A, π, µ, λ, δ)) is Γc = (A, π, µ, λ, δ), and the MS-parse defined for x by a complete transition sequence C0,m is Γtm (cm ) . The set T of transitions is shown in Figure 1. It is based on the system of Nivre (2009), where a dependency tree is built by repeated applications of the L EFT-A RCd and R IGHT-A RCd transitions, which add an arc (with some label d ∈ D) between the two topmost nodes on the stack (with the leftmost or rightmost node as the dependent, respectively). The S HIFT transition is used to move nodes from the buffer to the stack, and the S WAP transition is used to permute nodes in order to allow non-projective dependencies. Bohnet and Nivre (2012) modified this system by replacing the simple S HIFT transition by S HIFTp , which not only moves a node from the buffer to the stack but also"
Q13-1034,W11-4644,0,0.0793275,"Missing"
Q13-1034,W09-3829,0,0.0396525,"Missing"
Q13-1034,schmid-etal-2004-smor,0,0.0195468,"Missing"
Q13-1034,seeker-kuhn-2012-making,0,0.0289485,"Missing"
Q13-1034,C10-2129,1,0.896137,"Missing"
Q13-1034,spoustova-spousta-2012-high,0,0.0221884,"Missing"
Q13-1034,W07-2218,0,0.0373151,") = f (x, C0,m ) · w m X = f (x, ci , ti ) · w (2) i=0 Here, f (x, c, t) is a high-dimensional feature vector, where each component fi (x, c, t) is a nonnegative numerical feature (usually binary), and w is a weight vector of the same dimensionality, where each component wi is the real-valued weight of the feature fi (x, c, t). The choice of features to include in f (x, c, t) is discussed separately for each instantiation of the model in Sections 4–6. 2 Hatori et al. (2011) previously made the same modification to the arc-standard system (Nivre, 2004), without the S WAP transition. Similarly, Titov and Henderson (2007) added a word parameter to the S HIFT transition to get a joint model of word strings and dependency trees. A similar model was considered but finally not used by Gesmundo et al. (2009). 418 2.4 Decoding Exact decoding for transition-based parsing is hard in general.3 Early transition-based parsers mostly relied on greedy, deterministic decoding, which makes for very efficient parsing (Yamada and Matsumoto, 2003; Nivre, 2003), but research has shown that accuracy can be improved by using beam search instead (Zhang and Clark, 2008; Zhang and Nivre, 2012). While still not exact, beam search deco"
Q13-1034,tron-etal-2006-morphdb,0,0.119292,"Missing"
Q13-1034,W10-1401,0,0.0166567,"Missing"
Q13-1034,J13-1003,1,0.829911,"Missing"
Q13-1034,P06-3009,0,0.0901789,"Chinese and English), although in this case the joint model is limited to basic part-ofspeech tags and does not involve the full complex of morphological features. An integrated approach to morphological and syntactic analysis can also be found in grammar-based dependency parsers, such as the ETAP-3 linguistic processor (Apresian et al., 2003), where morphological disambiguation is mostly carried out together with syntactic analysis. Finally, it is worth noting that joint models of morphology and syntax have been more popular in constituency-based statistical parsing (Cowan and Collins, 2005; Tsarfaty, 2006; Cohen and Smith, 2007; Goldberg and Tsarfaty, 2008). Another hypothesis from the literature is that the high type-token ratio resulting from large morphological paradigms leads to data sparseness when estimating the parameters of a statistical parsing model (Tsarfaty et al., 2010; Tsarfaty et al., 2013). In particular, for many words in the language, only a subset of its morphological forms will be observed at training time. This suggests that using rule-based morphological analyzers or other lexical resources may be a viable strategy to improve coverage and performance. Thus, Goldberg and E"
Q13-1034,varadi-2002-hungarian,0,0.105288,"Missing"
Q13-1034,W03-3023,0,0.0799723,"tantiation of the model in Sections 4–6. 2 Hatori et al. (2011) previously made the same modification to the arc-standard system (Nivre, 2004), without the S WAP transition. Similarly, Titov and Henderson (2007) added a word parameter to the S HIFT transition to get a joint model of word strings and dependency trees. A similar model was considered but finally not used by Gesmundo et al. (2009). 418 2.4 Decoding Exact decoding for transition-based parsing is hard in general.3 Early transition-based parsers mostly relied on greedy, deterministic decoding, which makes for very efficient parsing (Yamada and Matsumoto, 2003; Nivre, 2003), but research has shown that accuracy can be improved by using beam search instead (Zhang and Clark, 2008; Zhang and Nivre, 2012). While still not exact, beam search decoders explore a larger part of the search space than greedy parsers, which is likely to be especially important for joint models, where the search space is larger than for plain dependency parsing without morphology (even more so with the S WAP transition for nonprojectivity). Figure 2 outlines the beam search algorithm used for decoding with our model. Different instantiations of the model will require slightly"
Q13-1034,D08-1059,0,0.738873,"so that a node moved from the buffer to the stack is assigned not only a tag p but also a morphological description m and a lemma l. In this way, we get a joint model for the prediction of part-ofspeech tags, morphological features, lemmas, and dependency trees. 2.3 Scoring In transition-based parsing, we score parses in an indirect fashion by scoring transition sequences. In general, we assume that the score function s factors by configuration-transition pairs: s(x, C0,m ) = m X s(x, ci , ti ) (1) i=0 Moreover, when using structured learning, as first proposed for transition-based parsing by Zhang and Clark (2008), we assume that the score is given by a linear model whose feature representations decompose in the same way: s(x, C0,m ) = f (x, C0,m ) · w m X = f (x, ci , ti ) · w (2) i=0 Here, f (x, c, t) is a high-dimensional feature vector, where each component fi (x, c, t) is a nonnegative numerical feature (usually binary), and w is a weight vector of the same dimensionality, where each component wi is the real-valued weight of the feature fi (x, c, t). The choice of features to include in f (x, c, t) is discussed separately for each instantiation of the model in Sections 4–6. 2 Hatori et al. (2011)"
Q13-1034,P11-2033,1,0.457721,"ection 7, we present an empirical analysis that gives further support for this choice, at least for the languages considered in this paper. Note also that the choice is not motivated by efficiency concerns, since increasing the values of kp and km has only a marginal effect on running time, as explained in Section 2.4. Finally, the choice not to consider k-best lemmas is dictated by the fact that our lemmatizer only provides a 1-best analysis. For the first three models, we use the same feature representations as Bohnet and Nivre (2012),9 consisting of their adaptation of the features used by Zhang and Nivre (2011), the graph completion features of Bohnet and Kuhn (2012), and the special features over k-best tags introduced specifically for joint tagging and parsing by Bohnet and Nivre (2012). For the J OINT model, we simply add features over the k-best morphological descriptions analogous to the features over k-best tags.10 Experimental results for these four models can be found in Table 2. From the P IPELINE results, we see that the 1-best accuracy of the preprocessing tagger ranges from 95.0 (Finnish) to 99.2 (Czech) for POS, and from 89.4 (Finnish) to 96.5 (Hungarian) for MOR. The lemmatizer does a"
Q13-1034,C12-2136,1,0.0524599,"out the S WAP transition. Similarly, Titov and Henderson (2007) added a word parameter to the S HIFT transition to get a joint model of word strings and dependency trees. A similar model was considered but finally not used by Gesmundo et al. (2009). 418 2.4 Decoding Exact decoding for transition-based parsing is hard in general.3 Early transition-based parsers mostly relied on greedy, deterministic decoding, which makes for very efficient parsing (Yamada and Matsumoto, 2003; Nivre, 2003), but research has shown that accuracy can be improved by using beam search instead (Zhang and Clark, 2008; Zhang and Nivre, 2012). While still not exact, beam search decoders explore a larger part of the search space than greedy parsers, which is likely to be especially important for joint models, where the search space is larger than for plain dependency parsing without morphology (even more so with the S WAP transition for nonprojectivity). Figure 2 outlines the beam search algorithm used for decoding with our model. Different instantiations of the model will require slightly different implementations of the permissibility condition invoked in line 8, which can be used to filter out labels that are improbable or incom"
Q13-1034,C98-1077,0,\N,Missing
Q13-1034,W09-1201,1,\N,Missing
Q13-1034,D07-1096,1,\N,Missing
S14-2121,D12-1133,0,0.0386276,"n In the SemEval-2014 Task 8 on semantic parsing, the objective is to extract for each sentence a rich set of typed semantic dependencies in three different formats: DM, PAS and PCEDT. These formats differ substantially both in the assignment of semantic heads as well as in the lexicon of semantic dependency types. In the open track of the shared task, participants were encouraged to use all resources and tools also beyond the provided training data. To improve the comparability of the systems, the organizers provided ready-to-use dependency parses produced using the state-of-theart parser of Bohnet and Nivre (2012). In this paper we describe our entry in the open track of the shared task. Our system is a pipeline of three support vector machine classifiers trained separately for detecting semantic dependencies, assigning their roles, and selecting the top nodes of semantic graphs. In this, we loosely follow the architecture of e.g. the TEES (Bj¨orne et al., 2012) and EventMine (Miwa et al., 2012) systems, which were found to be effective in the structurally 2 Detecting Semantic Dependencies The first step of our semantic parsing pipeline is to detect semantic dependencies, i.e. governordependent pairs w"
S14-2121,S13-1035,0,0.0426492,"Missing"
S14-2121,W14-1501,1,0.702993,"Missing"
S14-2121,W09-1201,0,\N,Missing
S14-2121,W09-1208,0,\N,Missing
S14-2143,N13-1090,0,0.0187348,"odel. The default parameter (C2 = 1.0) gave the best performing system and thus was used throughout the work. Finally, for the NER task, we submitted two models. The first model was trained with the original training data and duplicates of sentences with at least one entity mention. The second model was trained by using the combination of the first model’s training data and development set. 2.1 3 Task B: Normalization with Compositional Vector Representations Our normalization approach is based on continuous distributed word vector representations, namely the state-of-the-art method word2vec (Mikolov et al., 2013a). Our word2vec model was trained on a subset of abstracts and full articles from the PubMed and PubMed Central resources. This data was used as it was readily available to us from the EVEX resource (Van Landeghem et al., 2013). Before training, all nonalphanumeric characters were removed and all tokens were lower-cased. Even though a set of unannotated clinical reports was provided in the task to support unsupervised learning methods, our experiments on the development set showed better performance with the model trained with PubMed articles. This might be due to the size of the corpora, as"
S15-2161,D12-1133,0,0.170319,"e data from the Penn Chinese Treebank (Xue et al., 2005). For English and Czech also out-of-domain test data is provided in order to test the generalization ability of the systems. The semantic parsing task includes three different tracks. In the closed track the systems must be trained using only the official training data, whereas in the open track all additional sources of information are allowed. Together with the training data the organizers provided also syntactic dependency parses produced in the Stanford Dependencies scheme (De Marneffe and Manning, 2008) with the dependency parser of Bohnet and Nivre (2012). In addition to the closed and open tracks, also a gold track is included, where gold standard dependency parses are given for both training and test data. This paper describes our system used to take part in the open and gold tracks of the shared task. The system is a sequence classifier built on top of an existing dependency parser. The main idea behind the implementation is to turn the task of predicting all arguments for a single predicate to a sequence classification problem, but still process each predicate independently. Predicting one predicate at a time feels very natural when workin"
S15-2161,D09-1060,0,0.0265707,"by just preserving the real semantic relations and leaving out the empty NOTARG relations. As will be explained later, syntactic parses are a major source of features. For English, the syntactic parses are obtained from the companion and gold data provided by the organizers. Since for Czech and Chinese no companion data was available, the Czech syntactic representation is obtained using the MaltParser (Nivre et al., 2007) trained on the training section of the Prague Dependency Treebank (Hajiˇc et al., 2000) and the Chinese analysis is acquired using DuDuPlus, a graph-based dependency parser (Chen et al., 2009) with a model trained on the training section of the Chinese Treebank (Xue et al., 2005). 3.2 Transition System Since the structure of the input trees is completely flat (i.e. all words are attached to the sentence root, which is the predicate under inspection) the transition system of the parser can be simplified substantially. For every token other than the root, only the relation type must be predicted (using the NOTARG relation for tokens which are not arguments of this particular predicate). Thus, the transition system is modified to keep the root token always in the parsing stack, and on"
S15-2161,W02-1001,0,0.0206164,"cates and their arguments do not affect the decision), but when assigning arguments for one predicate, keep a global view of arguments already predicted for this particular predicate. The system is built on top of the open-source Turku transition-based dependency parser1 to obtain the full functionality of such a parser and to be able to freely modify it to fit to the needs of our approach. The Turku Dependency Parser is an implementation of the parser of Bohnet and Kuhn (2012), with full functionality of that parser, including e.g. online learning implemented with the generalized perceptron (Collins, 2002), beam search and graph-based completion features, and the full feature representation taken from the Bohnet and Nivre (2012). 3.1 Data Processing Before training the parser, the data is processed to meet the requirements of the standard, off-the-shelf dependency parsers. As the arguments are predicted 1 https://github.com/jmnybl/ Turku-Dependency-Parser 966 separately for each predicate, semantic graphs can be subtracted into several smaller units where each subgraph preserves the semantic arguments of one particular predicate. This means that each sentence is turned into as many pseudotrees"
S15-2161,S14-2080,0,0.164783,"y parsing task (Oepen et al., 2014) relied on the methods developed in the context of syntactic parsing, and existing state-of-the-art dependency parsers were widely used. Systems using dependency parsers are mainly based on graph-to-tree transformations (Koller, 2014; Schluter et al., 2014), parsers 965 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 965–969, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics able to produce directed acyclic graphs (Ribeyre et al., 2014; Kuhlmann, 2014), or a combination of these two (Du et al., 2014). The winner system of the 2014 open track is based on the graph-based dependency parser able to produce full non-projective graphs (Martins and Almeida, 2014). The system we used to participate in the same task last year was a pipeline of three different support vector machine classifiers trained separately for dependency detection, role assignment and top node prediction, where each governor–dependent pair was classified individually without any global view of the semantic structures (Kanerva et al., 2014a). A similar approach with the exception of using a structured support vector machine a"
S15-2161,W13-5609,1,0.892419,"Missing"
S15-2161,W14-1501,1,0.89656,"Missing"
S15-2161,S14-2121,1,0.901547,"ected acyclic graphs (Ribeyre et al., 2014; Kuhlmann, 2014), or a combination of these two (Du et al., 2014). The winner system of the 2014 open track is based on the graph-based dependency parser able to produce full non-projective graphs (Martins and Almeida, 2014). The system we used to participate in the same task last year was a pipeline of three different support vector machine classifiers trained separately for dependency detection, role assignment and top node prediction, where each governor–dependent pair was classified individually without any global view of the semantic structures (Kanerva et al., 2014a). A similar approach with the exception of using a structured support vector machine and therefore gaining a bit more of a global view to the problem was introduced by Jeffrey et al. (2014). 3 System Architecture The main approach is based on the recent progress on syntactic dependency parsing, yet taking a completely different approach than the mainstream graph-to-tree transformation methods and DAG parsers discussed in Section 2. Our main focus is to process each predicate independently (i.e. other predicates and their arguments do not affect the decision), but when assigning arguments for"
S15-2161,S14-2081,0,0.0201715,"., 2013), we did not want to merely follow the main methods from last year. Our system also requires syntactic analyses of the data, which is why we participated only on the tasks which allow their use (open and gold tracks). The system will be described in detail in Section 3. 2 Related Work The main approaches in the 2014 semantic dependency parsing task (Oepen et al., 2014) relied on the methods developed in the context of syntactic parsing, and existing state-of-the-art dependency parsers were widely used. Systems using dependency parsers are mainly based on graph-to-tree transformations (Koller, 2014; Schluter et al., 2014), parsers 965 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 965–969, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics able to produce directed acyclic graphs (Ribeyre et al., 2014; Kuhlmann, 2014), or a combination of these two (Du et al., 2014). The winner system of the 2014 open track is based on the graph-based dependency parser able to produce full non-projective graphs (Martins and Almeida, 2014). The system we used to participate in the same task last year was a pipeline of three differ"
S15-2161,S14-2068,0,0.402424,"e main approaches in the 2014 semantic dependency parsing task (Oepen et al., 2014) relied on the methods developed in the context of syntactic parsing, and existing state-of-the-art dependency parsers were widely used. Systems using dependency parsers are mainly based on graph-to-tree transformations (Koller, 2014; Schluter et al., 2014), parsers 965 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 965–969, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics able to produce directed acyclic graphs (Ribeyre et al., 2014; Kuhlmann, 2014), or a combination of these two (Du et al., 2014). The winner system of the 2014 open track is based on the graph-based dependency parser able to produce full non-projective graphs (Martins and Almeida, 2014). The system we used to participate in the same task last year was a pipeline of three different support vector machine classifiers trained separately for dependency detection, role assignment and top node prediction, where each governor–dependent pair was classified individually without any global view of the semantic structures (Kanerva et al., 2014a). A similar approach with the excepti"
S15-2161,W08-1301,0,0.0420046,"Missing"
S15-2161,S14-2082,0,0.107775,"sers were widely used. Systems using dependency parsers are mainly based on graph-to-tree transformations (Koller, 2014; Schluter et al., 2014), parsers 965 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 965–969, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics able to produce directed acyclic graphs (Ribeyre et al., 2014; Kuhlmann, 2014), or a combination of these two (Du et al., 2014). The winner system of the 2014 open track is based on the graph-based dependency parser able to produce full non-projective graphs (Martins and Almeida, 2014). The system we used to participate in the same task last year was a pipeline of three different support vector machine classifiers trained separately for dependency detection, role assignment and top node prediction, where each governor–dependent pair was classified individually without any global view of the semantic structures (Kanerva et al., 2014a). A similar approach with the exception of using a structured support vector machine and therefore gaining a bit more of a global view to the problem was introduced by Jeffrey et al. (2014). 3 System Architecture The main approach is based on th"
S15-2161,S14-2008,0,0.119616,"dently. Predicting one predicate at a time feels very natural when working with data annotated in PropBank style (Palmer et al., 2005), and since our main objective is to develop an SRL system optimized for Finnish PropBank (Haverinen et al., 2013), we did not want to merely follow the main methods from last year. Our system also requires syntactic analyses of the data, which is why we participated only on the tasks which allow their use (open and gold tracks). The system will be described in detail in Section 3. 2 Related Work The main approaches in the 2014 semantic dependency parsing task (Oepen et al., 2014) relied on the methods developed in the context of syntactic parsing, and existing state-of-the-art dependency parsers were widely used. Systems using dependency parsers are mainly based on graph-to-tree transformations (Koller, 2014; Schluter et al., 2014), parsers 965 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 965–969, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics able to produce directed acyclic graphs (Ribeyre et al., 2014; Kuhlmann, 2014), or a combination of these two (Du et al., 2014). The winner system"
S15-2161,J05-1004,0,0.0929244,"ks, also a gold track is included, where gold standard dependency parses are given for both training and test data. This paper describes our system used to take part in the open and gold tracks of the shared task. The system is a sequence classifier built on top of an existing dependency parser. The main idea behind the implementation is to turn the task of predicting all arguments for a single predicate to a sequence classification problem, but still process each predicate independently. Predicting one predicate at a time feels very natural when working with data annotated in PropBank style (Palmer et al., 2005), and since our main objective is to develop an SRL system optimized for Finnish PropBank (Haverinen et al., 2013), we did not want to merely follow the main methods from last year. Our system also requires syntactic analyses of the data, which is why we participated only on the tasks which allow their use (open and gold tracks). The system will be described in detail in Section 3. 2 Related Work The main approaches in the 2014 semantic dependency parsing task (Oepen et al., 2014) relied on the methods developed in the context of syntactic parsing, and existing state-of-the-art dependency pars"
S15-2161,S14-2034,0,0.0264024,"id not want to merely follow the main methods from last year. Our system also requires syntactic analyses of the data, which is why we participated only on the tasks which allow their use (open and gold tracks). The system will be described in detail in Section 3. 2 Related Work The main approaches in the 2014 semantic dependency parsing task (Oepen et al., 2014) relied on the methods developed in the context of syntactic parsing, and existing state-of-the-art dependency parsers were widely used. Systems using dependency parsers are mainly based on graph-to-tree transformations (Koller, 2014; Schluter et al., 2014), parsers 965 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 965–969, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics able to produce directed acyclic graphs (Ribeyre et al., 2014; Kuhlmann, 2014), or a combination of these two (Du et al., 2014). The winner system of the 2014 open track is based on the graph-based dependency parser able to produce full non-projective graphs (Martins and Almeida, 2014). The system we used to participate in the same task last year was a pipeline of three different support vector machi"
S15-2161,E12-1009,0,\N,Missing
S15-2161,S14-2027,0,\N,Missing
S15-2161,S14-2012,0,\N,Missing
W04-1203,P99-1065,0,0.0256684,"nteraction subgraphs can be partially overlapping, because a single link can be part of more than one interaction subgraph. Figure 1 shows an example of an annotated text fragment. 4 Evaluation criteria We evaluated the performance of the LG parser according to the following three quantitative criteria: • Number of dependencies recovered • Number of fully correct linkages • Number of interaction subgraphs recovered The number of recovered dependencies gives an estimate of the probability that a dependency will be correctly identified by the LG parser (this criterion is also employed by, e.g., Collins et al. (1999)). The number of fully correct linkages, i.e. linkages where all annotated dependencies are recovered, measures the fraction of sentences that are parsed without error. However, a fully correct linkage is not necessary to extract protein-protein interactions from a sentence; to estimate how many interactions can potentially be recovered, we measure the number of interaction subgraphs for which all dependencies were recovered. For each criterion, we measure the performance for the first linkage returned by the parser. However, the first linkage as ordered by the heuristics of the LG parser was"
W06-3605,H05-1003,0,0.029408,"d. Further, each of the various problems in natural language processing is typically approached with a different class of models, ranging from n-gram statistics to complex regressors and classifiers such as the support vector machines. These different approaches need to be combined in order to find the globally optimal solution. Therefore, in our study we aim to develop a search strategy that allows to combine a wider range of target functions. An alternative approach is that of propagating n best solutions through the pipeline system, where each step re-ranks the solutions by local criteria (Ji et al., 2005). Incorporating a wide range of features representing information from all levels of analysis into a single master classifier is other commonly used method (Kambhatla, 2004; Zelenko et al., 2004). In this paper, we assume the possibility of generating the structurally correct solutions incrementally, through a sequence of partially completed solutions. We then derive a probabilistic search algorithm that attempts to identify the globally best solution, without fully completing all structurally correct solutions. Further, we do not impose strong restrictions, such as the Viterbi assumption, on"
W06-3605,P04-3022,0,0.0258244,"egressors and classifiers such as the support vector machines. These different approaches need to be combined in order to find the globally optimal solution. Therefore, in our study we aim to develop a search strategy that allows to combine a wider range of target functions. An alternative approach is that of propagating n best solutions through the pipeline system, where each step re-ranks the solutions by local criteria (Ji et al., 2005). Incorporating a wide range of features representing information from all levels of analysis into a single master classifier is other commonly used method (Kambhatla, 2004; Zelenko et al., 2004). In this paper, we assume the possibility of generating the structurally correct solutions incrementally, through a sequence of partially completed solutions. We then derive a probabilistic search algorithm that attempts to identify the globally best solution, without fully completing all structurally correct solutions. Further, we do not impose strong restrictions, such as the Viterbi assumption, on the target functions. To a certain extent, this approach is related to the problem of cost-sensitive learning, where obtaining a feature value is associated with a cost and"
W06-3605,C90-2040,0,0.0578486,"set of candidate parses and typically also implements a target function that scores the parses based on their structural and lexical features. Each parse that is compatible with both the POS tagger and the parser is structurally correct. The best solution may be defined, for instance, as such a solution that maximizes the sum of the scores of the POS- and parser-centric target functions. In practice, the set of structurally correct solutions may be computed, for example, through the intersection or composition of finite-state automata as in the formalism of finite-state intersection grammars (Koskenniemi, 1990). Finding the best so33 Workshop on Computationally Hard Problemsand Joint Inference in Speech and Language Processing, pages 33–40, c New York City, New York, June 2006. 2006 Association for Computational Linguistics lution may be implemented as a best-path search through Viterbi decoding, given a target function that satisfies the Viterbi condition. Most of the recent approaches to NLP tasks like parse re-ranking make, however, use of featurebased representations and machine-learning induced target functions, which do not allow efficient search strategies that are guaranteed to find the glob"
W07-1004,I05-1006,0,0.310811,"Missing"
W07-1004,de-marneffe-etal-2006-generating,0,0.168812,"Missing"
W07-1004,levy-andrew-2006-tregex,0,0.0261922,"ctic dependencies between the words. The scheme defines a hierarchy of 48 grammatical relations, or dependency types. The most generic relation, dependent, can be specialized as auxiliary, argument, or modifier, which again have several subtypes (de Marneffe et al., 2006). The Stanford conversion transforms phrase structure parses into the Stanford scheme. First, the semantic head of each constituent is identified using head rules similar to those of Collins (1999) and untyped dependencies are then extracted and labeled with the most specific grammatical relations possible using Tregex rules (Levy and Andrew, 2006). The system additionally provides a set of collapsing rules, suggested to be beneficial for IE applications (de Marneffe et al., 2006; Clegg and Shepherd, 2007). These rules collapse some dependencies by incorporating certain parts of speech (mostly Spx CC Pv CC Ss Js A/AN A/AN Cs E MVs Dsu Mp Pv Vimentin and actin were also up-regulated , whereas an isoform of myosin heavy chain was down-regulated . advcl> <nsubjpass conj> <auxpass cc> <advmod <mark <nsubjpass pobj> <nmod <nmod <det prep> <auxpass Vimentin and actin were also up-regulated , whereas an isoform of myosin heavy chain was down-r"
W07-1004,1993.iwpt-1.22,0,0.290886,"for dependency comparable to that of PTB for constituency, no widely adopted standard currently exists. In this paper, we present a step towards unifying the diverse syntax schemes in use in IE systems and corpora such as the GENIA Treebank1 and the recently introduced BioInfer corpus (Pyysalo et al., 2007). Clegg and Shepherd (2007) have recently proposed to use the Stanford dependency scheme (de Marneffe et al., 2006) as a common, application-oriented syntax representation. To assess this choice, we develop a set of conversion rules for transforming the Link Grammar (LG) dependency scheme (Sleator and Temperley, 1993) to 1 http://www-tsujii.is.s.u-tokyo.ac.jp/ ∼genia 25 BioNLP 2007: Biological, translational, and clinical language processing, pages 25–32, c Prague, June 2007. 2007 Association for Computational Linguistics the Stanford scheme and then create a version of the BioInfer corpus in the Stanford scheme by applying the conversion rules and manually correcting the errors. By making the BioInfer corpus available in the Stanford scheme, we also increase the value of the corpus for biomedical IE. The transformation has the further benefit of allowing Link Grammar output to be normalized into a more ap"
W07-1004,A00-2018,0,\N,Missing
W07-1004,J03-4003,0,\N,Missing
W08-0601,H05-1091,0,0.0342895,"ach to PPI extraction. Finally, we discuss the effects that different evaluation strategies, choice of corpus and applied metrics have on measured performance, and conclude. P2 &lt;nsubj &lt;cop &lt;det &lt;nn &lt;nn P1 is a P2 binding protein &lt;xsubj xcomp&gt; &lt;aux &lt;nsubj P1 fails to 2 dobj&gt; bind P2 Figure 1: Stanford dependency parses (“collapsed” representation) where the shortest path, shown in bold, excludes important words. We next present our graph representation, formalize the notion of graph kernels, and present our learning method of choice, the sparse RLS. 2.1 et al., 2003) and shortest path kernels (Bunescu and Mooney, 2005) have been proposed and successfully used for relation extraction. However, these methods lack the expressive power to consider representations derived from general, possibly cyclic, dependency graph structures, such as those generated by the Stanford tools. The subsequence kernel approach does not consider parses at all, and the shortest path approach is limited to representing only a single path in the full dependency graph, which excludes relevant words even in many simple cases (Figure 1). Tree kernels can represent more complex structures, but are still restricted to tree representations."
W08-0601,I05-1006,0,0.085227,", 2008), provides an opportunity for building PPI extraction systems automatically using machine learning. A major challenge is how to supply the learner with the contextual and syntactic information needed to distinguish between interactions and non-interactions. To address the ambiguity and variability of the natural language expressions used to state PPI, several recent studies have focused on the development, adaptation and application of NLP tools for the biomedical domain. Many high-quality domain-specific tools are now freely available, including full parsers such as that introduced by Charniak and Lease (2005). Additionally, a number of conversions from phrase structure parses to dependency structures that make the relationships between words more directly accessible have been introduced. These include conversions into representations such as the Stanford dependency scheme (de Marneffe et al., 2006) that are explicitly designed for information extraction purposes. However, specialized feature representations and kernels are required to make learning from such structures possible. Approaches such as subsequence kernels (Bunescu and Mooney, 2006), tree kernels (Zelenko 1 BioNLP 2008: Current Trends i"
W08-0601,de-marneffe-etal-2006-generating,0,0.0576609,"Missing"
W08-0601,E06-1051,0,0.542388,"nteract are positive examples and other co-occuring  pairs negative. Thus, from each sentence, n2 examples are generated, where n is the number of occurrences of protein names in the sentence. Finally, we form the graph representation described earlier for each candidate interaction. We evaluate the method with 10-fold documentlevel cross-validation on all of the corpora. This guarantees the maximal use of the available data, and also allows comparison to relevant earlier work. In particular, on the AImed corpus we apply the exact same 10-fold split that was used by Bunescu et al. (2006) and Giuliano et al. (2006). Performance is measured according to the following criteria: interactions are considered untyped, undirected pairwise relations between specific protein mentions, that is, if the same protein name occurs multiple 1 Available at http://mars.cs.utu.fi/PPICorpora. 5 times in a sentence, the correct interactions must be extracted for each occurrence. Further, we do not consider self-interactions as candidates and remove them from the corpora prior to evaluation. The majority of PPI extraction system evaluations use the balanced F-score measure for quantifying the performance of the systems. This"
W08-0601,W07-1004,1,0.928632,"ic literature is a task of significant interest in the BioNLP field. The most commonly addressed problem has been the extraction of binary interactions, where the system identifies which protein pairs in a sentence have a biologically relevant relationship between them. Proposed solutions include both hand-crafted rule-based systems and machine learning approaches (see e.g. (Bunescu et al., 2005)). A wide range of results have been reported for the systems, but as we will show, differences in The public availability of large annotated PPIcorpora such as AImed (Bunescu et al., 2005), BioInfer (Pyysalo et al., 2007a) and GENIA (Kim et al., 2008), provides an opportunity for building PPI extraction systems automatically using machine learning. A major challenge is how to supply the learner with the contextual and syntactic information needed to distinguish between interactions and non-interactions. To address the ambiguity and variability of the natural language expressions used to state PPI, several recent studies have focused on the development, adaptation and application of NLP tools for the biomedical domain. Many high-quality domain-specific tools are now freely available, including full parsers suc"
W09-1402,W09-4605,1,0.795318,"Missing"
W09-1402,P05-1022,0,0.0377311,"of which may actually be associated with the same event. Similar assumptions are made in the trigger detection phase, where the classifications of individual tokens are independent. A common way to relax independence assumptions is to use N -best re-ranking where N mostlikely candidates are re-ranked using global features that model data dependencies that could not be modelled in the candidate generation step. The best candidate with respect to this re-ranked order is then the final prediction of the system. N -best re-ranking has been successfully applied for example in statistical parsing (Charniak and Johnson, 2005). We generated the ten most likely candidate graphs, as determined by the confidence scores of the individual edges given by the multi-class SVM. A perfect reranking of these ten candidates would lead to 11.5 percentage point improvement in the overall system F-score on the development set. While we were unable to produce a re-ranker sufficiently accurate to improve the system performance in the time given, the large potential gain warrants further research. 15 In trigger word detection, we experimented with a structural SVM incorporating Hidden Markov Model type of sequential dependencies (Al"
W09-1402,C08-1053,0,0.0324557,"hen classified as theme, cause, or a negative denoting the absence of an edge between the two nodes in the given direction. It should be noted that even though event nodes often require multiple outgoing edges corresponding to multiple event arguments, all edges are predicted independently and are not affected by positive or negative classifications of other edges. The feature set makes extensive use of syntactic dependencies, in line with many recent studies in biomedical information extraction (see, e.g. (Kim et al., 2008b; Miwa et al., 2008; Airola et al., 2008; Van Landeghem et al., 2008; Katrenko and Adriaans, 2008)). The central concept in generating features of potential event argument edges is the shortest undirected path of syntactic dependencies in the Stanford scheme parse of the sentence which we assume to accurately capture the relationship expressed by the edge. In Figure 3, we show that the distances among event and named entity nodes in terms of shortest dependency path length are considerably shorter than in terms of their linear order in the sentence. The end points of the path are the syntactic head tokens of the two named entities or event triggers. The head tokens are identified using a s"
W09-1402,W09-1401,0,0.659422,"For each event, its class, trigger expression in the text, and arguments need to be extracted. The task follows the recent movement in BioNLP towards the extraction of semantically typed, complex events the arguments of which can also be other events. This results in a nested structure that captures the underlying biological statements more accurately compared to the prevailing approach of merely detecting binary interactions of pairs of biological entities. Introduction In this paper, we present the best-performing system in the primary task of the BioNLP’09 Shared Task on Event Extraction (Kim et al., 2009).1 The purpose of this shared task was to competitively evaluate information extraction systems targeting complex events in the biomedical domain. Such an evaluation helps to establish the relative merits of competing approaches, allowing direct comparability of results in a controlled setting. The shared task was 1 http://www-tsujii.is.s.u-tokyo.ac.jp/ GENIA/SharedTask 10 Our system is characterized by heavy reliance on efficient, state-of-the-art machine learning techniques and a wide array of features derived from a full dependency analysis of each sentence. The system is a pipeline of thre"
W09-1402,P08-2026,0,0.183383,"ed in the system are trained as follows. First we optimize the regularization parameter C by training on the shared task training set and testing on the shared task development set. We then re-train the final classifier on the union of the training and development sets, using the best value of C in the previous step. The same protocol is followed for the λ and β parameters in trigger detection. 3.2 Dependency parses Both trigger detection and edge prediction rely on a wide array of features derived from full dependency parses of the sentence. We use the McCloskyCharniak domain-adapted parser (McClosky and Charniak, 2008) which is among the best performing parsers trained on the GENIA Treebank corpus. The native constituency output of the parser is transformed to the “collapsed” form of the Stanford dependency scheme (de Marneffe and Manning, 2008) using the Stanford parser tools.4 The parses were provided by the shared task organizers. 4 Results and discussion The final evaluation of the system was performed by the shared task organizers using a test set whose an4 http://nlp.stanford.edu/software/ 16 notation was at no point available to the task participants. By the main criterion of Task 1, approximate span"
W09-1402,W08-1301,0,\N,Missing
W09-4605,W09-1402,1,0.866311,"Missing"
W09-4605,doddington-etal-2004-automatic,0,0.0506133,"Missing"
W09-4605,E06-1051,0,0.0690424,"Missing"
W09-4605,C08-1053,0,0.190084,"Missing"
W09-4605,I05-1006,0,0.0207855,"Missing"
W09-4605,de-marneffe-etal-2006-generating,0,0.0775516,"Missing"
W09-4605,P08-1006,0,0.0407415,"Missing"
W09-4605,W07-1004,1,0.847509,"IE and has served as the basis for real-world applications for e.g. assisted database curation (Alex et al., 2008), its limitations, such as the restriction to events between entity pairs commonly referred to as binary interactions in the domain literature, are increasingly recognized by the biomedical NLP community. In this paper, we argue for an alternate model and present the first machine-learning approach to the extraction of structured, complex events and relationships among bioentities. To overcome the limitations of the pairwise approach to biomedical IE, two recent corpora, BioInfer (Pyysalo et al., 2007a) and the GENIA Event corpus (Kim et al., 2008a) annotate events and static relationships using a more expressive formalism that differs from the prevailing approach in several key aspects: First, type, direction and the trigger statement in the text stating the relationship (often a verb) are annotated. Second, events can have more than two participants whose roles are specified, allowing the accurate representation of statements such as proteins A, B and C form a complex. Finally, events can also act as arguments of other events, enabling the annotation of nested events such as A causes B t"
W09-4605,M95-1002,0,0.0448994,"ng approach in several key aspects: First, type, direction and the trigger statement in the text stating the relationship (often a verb) are annotated. Second, events can have more than two participants whose roles are specified, allowing the accurate representation of statements such as proteins A, B and C form a complex. Finally, events can also act as arguments of other events, enabling the annotation of nested events such as A causes B to bind C (Figure 1A). These representations largely resemble event extraction as formulated in (later) Message Understanding Conferences (MUC) (see, e.g., Sundheim (1995)) and in the Automatic Content Extraction (ACE) program (see, e.g., Doddington (2004)). BioInfer also annotates static relations (e.g. substructure) and both BioInfer and GENIA annotate non-biological relationships (e.g. coreferences) with specialized mechanisms. In this paper, we use the term complex relationship to encompass both event and generic relationship annotation. Learning to Extract Biological Event and Relation Graphs causes xcomp&gt; &lt;nsubj &lt;nsubj dobj&gt; &lt;aux Profilin causes actin NN VBZ NN Protein CAUSE Protein &lt;agent to TO bind VBZ BIND &lt;participant cofilin . NN . Protein . } } part"
W09-4605,W08-2121,0,0.070167,"Missing"
W09-4611,C90-3030,0,0.0378865,"pplications, we refer to the review by de Marneffe and Manning (2008). While numerous corpora and parsers exist for English and many other languages, resources for Finnish are scarce. For instance, there is no publicly available syntactically annotated corpus suitable for statistical parser induction. The only publicly available full parser is Connexor Machinese Syntax,1 a closed-source commercial dependency parser for the general language. Other tools include FinTWOL and FinCG,2 a morphological analyzer and a Constraint Grammar parser that resolves morphological ambiguity (Koskenniemi, 1983; Karlsson, 1990). The rule-based parser of Laippala et al. (2009) used in this work was developed for the clinical domain, and builds full constituency analyses on top of the morpholexical analyses provided by FinTWOL and FinCG. 3 ICU Finnish in the Stanford dependency scheme ICU Finnish differs from standard Finnish in many ways (for details, see the discussion by Laippala et al. (2009)). Some of the most distinguishing features present in ICU Finnish, as well as many clinical sublanguages, are frequent misspellings, abbreviations and technical terms, telegraphic sentences, syntactic structures that would no"
W09-4611,de-marneffe-etal-2006-generating,0,0.0732138,"Missing"
W09-4611,H91-1060,0,0.302648,"Missing"
W09-4611,J08-4003,0,0.250411,"h UMLS Specialist lexicon terms to improve its applicability to medical texts and Pyysalo et al. (2006) incorporate into LG a domain-adapted part-of-speech tagger. The different ways to represent natural language syntax can be broadly distinguished into two categories. A constituency analysis divides the sentence into nested phrases, whereas a dependency analysis consists of a set of labelled dependencies between pairs of words. In this work, we focus on dependency parsing because of its benefits in applications and parser evaluation (see for example Lin (1998), Clegg and Shepherd (2007), and Nivre (2008b)), as well as its applicability to languages with a relatively free word order, such Katri Haverinen, Filip Ginter, Veronika Laippala and Tapio Salakoski Y¨ovuoro Potilas levoton, valittaa kipua. Annettu 100mg [l¨aa¨ ke] hieman rauhottui. HENGITS: Hapettuu hyvin repiraattorissa. Putkesta hiukan nest. illalla. Diureesi: riitt¨av¨aa¨ . Hemodyn: annettu 50 mg/h [l¨aa¨ ke], heikohko vaste vaihdettu [l¨aa¨ ke]. OMAISET: vaimo soittanut jutellut l¨aa¨ k¨arin kanssa. Nightshift Patient restless, complains of pain. Given 100mg [drugname] a little calmed down. BREATING: Oxidates well in repirator. A"
W09-4611,W04-1505,0,0.0276513,"nal structures are semantically similar, it would be desirable to represent them in a similar manner also in the dependency structure. Therefore, we introduce a new dependency type, nommod (nominal modifier), to represent inflectional structures. This same type can also be used in sentences with actual pre- and postpositions. Only one additional type is needed for prepositional structures, a type named adpos (adposition). For an illustration of the usage of these two types, see Figure 3. The structure given to prepositional phrases is similar to that used in the scheme of the Pro3Gres parser (Schneider et al., 2004). A third modification to the SD scheme is required by the nature of the ICU language: sentence boundaries are often not clearly marked, or they lack punctuation altogether (see Figure 4). We split the text into separate sentences only when there is explicit punctuation that marks the sentence boundary. Recovering sentence boundaries that have no explicit surface marking is left to the parser, as recognizing them would be difficult for standard sentence splitters that lack syntax information. We have thus introduced a new dependency type, sdep, to connect these isolated sentences that are not"
W10-1819,W04-2412,0,0.0404405,"Missing"
W10-1819,J93-2004,0,0.0386402,"Missing"
W10-1819,W05-0620,0,0.0419225,"Missing"
W10-1819,J05-1004,0,0.633867,"ing trends, and others (see the extensive review by Friedman and Johnson (2006)). While some of these applications, such as document retrieval and trend mining, can rely solely on word-frequency-based methods, others, such as information extraction and summarization require a detailed linguistic analysis capturing some of the sentence semantics. Among the most important steps in this direction is an analysis of verbs and their argument structures. In this work, we focus on the Finnish language in the clinical domain, analyzing its verbs and their argument structures using the PropBank scheme (Palmer et al., 2005). The choice of this 137 Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 137–141, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics kest¨aa¨ .0: “tolerate” Arg0: the one who tolerates Arg1: what is being tolerated kest¨aa¨ .1: “last” Arg1: the thing that lasts Arg2: how long it lasts <xsubj <nsubj xcomp> Potilas saanut ottaa mehua ja leipää . Patient allowed to_have juice and bread . Figure 1: The PropBank framesets for kest¨aa¨ (translated to English from the original frames file) correspond to two different uses of the verb. Pitk¨a y"
W10-1819,W08-1301,0,0.124115,"Missing"
W10-1819,palmer-etal-2008-pilot,0,0.0301017,"Missing"
W10-1819,W08-2121,0,0.0374597,"Missing"
W10-1819,W03-1707,0,0.0656707,"Missing"
W10-1819,W09-4611,1,0.763101,"unct> dobj:Arg1> <advmod juonut.0 vähän drank little mehua . juice . Figure 4: The PropBank annotation scheme on top of the treebank syntactic annotation. The verb juonut (drank) is marked with its frameset, in this case the frameset number 0. This frameset specifies that Arg0 marks the agent doing the drinking and Arg1 the liquid being consumed. The ArgM-tmp label specifies that Aamulla is a temporal modifier. The example can be translated as In the morning patient drank a little juice. Clinical Finnish and the clinical Finnish treebank This study is based on the clinical Finnish treebank of Haverinen et al. (2009), which consists of 2,081 sentences with 15,335 tokens and 13,457 dependencies. The text of the treebank comprises eight complete patient reports from an intensive care unit in a Finnish hospital. An intensive care patient report describes the condition of the patient and its development in time. The clinical Finnish in these reports has many characteristics typical of clinical languages, including frequent misspellings, abbreviations, domain terms, telegraphic style and non-standard syntactic structures (see Figure 2 for an illustration). For a detailed analysis, we refer the reader to the st"
W10-1904,W09-1402,1,0.418613,"Missing"
W10-1904,de-marneffe-etal-2006-generating,0,0.0630604,"Missing"
W10-1904,W09-1313,1,0.821387,"terms to ones involving the associated genes/proteins. The first challenge, gene/protein name normalization, is a wellstudied task in biomedical NLP for which a number of systems with promising performance have been proposed (Morgan and Hirschman, 2007). The second we believe to be novel. In the following, we propose a method for resolving this task. We base the decision on how to map events referencing broadly defined terms to ones referencing associated gene/protein names in part on a recently introduced dataset of “static relations” (Pyysalo et al., 2009) between named entities and terms (Ohta et al., 2009b). This dataset was created based on approximately 10,000 cases where GGP NEs, as annotated in the GENIA GGP corpus (Ohta et al., 2009a), were embedded in terms, as annotated in the GENIA term corpus (Ohta et al., 2002). For each such case, the relation between the NE and the term was annotated using a set of introduced relation types whose granularity was defined with reference to MeSH terms (see Table 2, Ohta et al., 2009b). From this data, we extracted prefix and suffix strings that, when affixed to a GGP name, produced a term with a predictable relation (within the dataset) to the GGP. Th"
W10-1904,W03-1018,1,0.827599,"achieves results close to the best published on the standard GENETAG dataset and was reported to have the best performance in a recent study comparing publicly available taggers (Kabiljo et al., 2009). Titles and abstracts of all 17.8M citations in the 2009 distribution of PubMed are processed through the BANNER system. Titles and abstracts of PubMed citations in which at least one named entity was identified, and 29 which therefore contain a possible target for event extraction, are subsequently split into sentences using a maximum-entropy based sentence splitter trained on the GENIA corpus (Kazama and Tsujii, 2003) with limited rule-based post-processing for some common errors. All sentences containing at least one named entity are then parsed with the domain-adapted McClosky-Charniak parser (McClosky and Charniak, 2008; McClosky, 2009), which has achieved the currently best published performance on the GENIA Treebank (Tateisi et al., 2005). The constituency parse trees are then transformed to the collapsed-ccprocessed variant of the Stanford Dependency scheme using the conversion tool2 introduced by de Marneffe et al. (2006). Finally, events are extracted using the Turku Event Extraction System of Bj¨o"
W10-1904,W09-1301,1,0.800599,"apping from the events involving automatically extracted terms to ones involving the associated genes/proteins. The first challenge, gene/protein name normalization, is a wellstudied task in biomedical NLP for which a number of systems with promising performance have been proposed (Morgan and Hirschman, 2007). The second we believe to be novel. In the following, we propose a method for resolving this task. We base the decision on how to map events referencing broadly defined terms to ones referencing associated gene/protein names in part on a recently introduced dataset of “static relations” (Pyysalo et al., 2009) between named entities and terms (Ohta et al., 2009b). This dataset was created based on approximately 10,000 cases where GGP NEs, as annotated in the GENIA GGP corpus (Ohta et al., 2009a), were embedded in terms, as annotated in the GENIA term corpus (Ohta et al., 2002). For each such case, the relation between the NE and the term was annotated using a set of introduced relation types whose granularity was defined with reference to MeSH terms (see Table 2, Ohta et al., 2009b). From this data, we extracted prefix and suffix strings that, when affixed to a GGP name, produced a term with a pred"
W10-1904,W09-1401,1,0.830255,"Missing"
W10-1904,I05-2038,1,0.208821,"stracts of PubMed citations in which at least one named entity was identified, and 29 which therefore contain a possible target for event extraction, are subsequently split into sentences using a maximum-entropy based sentence splitter trained on the GENIA corpus (Kazama and Tsujii, 2003) with limited rule-based post-processing for some common errors. All sentences containing at least one named entity are then parsed with the domain-adapted McClosky-Charniak parser (McClosky and Charniak, 2008; McClosky, 2009), which has achieved the currently best published performance on the GENIA Treebank (Tateisi et al., 2005). The constituency parse trees are then transformed to the collapsed-ccprocessed variant of the Stanford Dependency scheme using the conversion tool2 introduced by de Marneffe et al. (2006). Finally, events are extracted using the Turku Event Extraction System of Bj¨orne et al. which achieved the best performance in the BioNLP’09 Shared Task and remains fully competitive with even the most recent advances (Miwa et al., 2010). We use a recent publicly available revision of the event extraction system that performs also extraction of Shared Task subtask 2 and 3 information, providing additional"
W10-1904,P08-2026,0,0.00479175,"es and abstracts of all 17.8M citations in the 2009 distribution of PubMed are processed through the BANNER system. Titles and abstracts of PubMed citations in which at least one named entity was identified, and 29 which therefore contain a possible target for event extraction, are subsequently split into sentences using a maximum-entropy based sentence splitter trained on the GENIA corpus (Kazama and Tsujii, 2003) with limited rule-based post-processing for some common errors. All sentences containing at least one named entity are then parsed with the domain-adapted McClosky-Charniak parser (McClosky and Charniak, 2008; McClosky, 2009), which has achieved the currently best published performance on the GENIA Treebank (Tateisi et al., 2005). The constituency parse trees are then transformed to the collapsed-ccprocessed variant of the Stanford Dependency scheme using the conversion tool2 introduced by de Marneffe et al. (2006). Finally, events are extracted using the Turku Event Extraction System of Bj¨orne et al. which achieved the best performance in the BioNLP’09 Shared Task and remains fully competitive with even the most recent advances (Miwa et al., 2010). We use a recent publicly available revision of"
W11-0204,W09-1402,1,0.87622,"Missing"
W11-0204,W10-1904,1,0.917946,"Missing"
W11-0204,W09-1401,0,0.274957,") and publicly available gene database records (Section 2.2). 2.1 Text mining predictions Bj¨orne et al. (2010) have applied to all PubMed abstracts an event extraction pipeline comprising of the BANNER named entity recognizer (Leaman and Gonzalez, 2008) and the Turku Event Extraction System (Bj¨orne et al., 2009). The resulting dataset contains 36.5M occurrences of gene / gene product (GGP) entities and 19.2M occurrences of events pertaining to these entities. The file format and information scheme of the resource correspond to the definition of the BioNLP’09 Shared Task on Event Extraction (Kim et al., 2009). Events are defined as typed relations between arguments that are either entity occurrences or, recursively, other events. There are nine possible event types: Localization, Binding, Gene expression, Transcription, Protein catabolism, Phosphorylation, Regulation, Positive regulation, and Negative regulation. Further, arguments are assigned a role: Theme or Cause for the core arguments and AtLoc, ToLoc, Site, and CSite for auxiliary arguments that define additional information such as cellular location of the event. In addition, each event occurrence may be marked as negative and/or speculativ"
W11-0204,C10-1096,0,0.0422396,"Missing"
W11-0204,P06-4005,0,\N,Missing
W12-2410,W11-1828,1,0.799541,"Missing"
W12-2410,W09-1402,1,0.89609,"Missing"
W12-2410,W10-1904,1,0.854996,"Missing"
W12-2410,P05-1022,0,0.0136407,"et VBZ is agent> VBN mediated IN by . . NN CKI EPI detection TEES 2.3 <Theme <Site Entity Serine Theme> Cause> Phosphorylation phosphorylation Protein of T-bet Protein Catalysis is mediated by CKI . is mediated by CKI . <Protein-Component Entity Serine E Protein phosphorylation REL of T-bet tion, protein/gene names are detected and sentences are parsed. TEES handles all these preprocessing steps via a pipeline of tool wrappers for the GENIA Sentence Splitter (Kazama and Tsujii, 2003), the BANNER named entity recognizer (Leaman and Gonzalez, 2008), the McClosky-Charniak-Johnson (McCCJ) parser (Charniak and Johnson, 2005; McClosky, 2010) and the Stanford tools (de Marneffe et al., 2006). For a detailed description of TEES we refer to Bj¨orne and Salakoski (2011) and for the computational requirements of PubMed-scale event extraction to Bj¨orne et al. (2010). EPI conversion to ST format and database import Figure 1: Event and relation extraction. Article text is split into sentences (A), where gene/protein entities are detected and normalized to their Entrez Gene IDs (B). Each sentence with at least one entity is then parsed (C). EPI events and REL relations are extracted from the parsed sentences (D) and foll"
W12-2410,W03-1018,0,0.0217666,"ed by CKI . McCJ-parser + Stanford Conversion <nsubjpass <nn NN Serine prep_of> NN phosphorylation D IN of REL detection <auxpass NN T-bet VBZ is agent> VBN mediated IN by . . NN CKI EPI detection TEES 2.3 <Theme <Site Entity Serine Theme> Cause> Phosphorylation phosphorylation Protein of T-bet Protein Catalysis is mediated by CKI . is mediated by CKI . <Protein-Component Entity Serine E Protein phosphorylation REL of T-bet tion, protein/gene names are detected and sentences are parsed. TEES handles all these preprocessing steps via a pipeline of tool wrappers for the GENIA Sentence Splitter (Kazama and Tsujii, 2003), the BANNER named entity recognizer (Leaman and Gonzalez, 2008), the McClosky-Charniak-Johnson (McCCJ) parser (Charniak and Johnson, 2005; McClosky, 2010) and the Stanford tools (de Marneffe et al., 2006). For a detailed description of TEES we refer to Bj¨orne and Salakoski (2011) and for the computational requirements of PubMed-scale event extraction to Bj¨orne et al. (2010). EPI conversion to ST format and database import Figure 1: Event and relation extraction. Article text is split into sentences (A), where gene/protein entities are detected and normalized to their Entrez Gene IDs (B). Ea"
W12-2410,W09-1401,1,0.800261,"by existing PubMed-scale event extraction efforts. The methods and data introduced in this study are freely available from bionlp.utu.fi. 1 Introduction Biomedical domain information extraction has in recent years seen a shift from focus on the extraction of simple pairwise relations (Pyysalo et al., 2008; Tikk et al., 2010) towards the extraction of events, represented as structured associations of arbitrary numbers of participants in specific roles (Ananiadou et al., 2010). Domain event extraction has been popularized in particular by the BioNLP Shared Task (ST) challenges in 2009 and 2011 (Kim et al., 2009; Kim et al., 2011). While the BioNLP ST’09 emphasized protein interactions and regulatory relationships, the expressive event formalism can also be applied to the extraction of statements regarding the properties of individual proteins. Accordingly, the EPI (Epigenetics and Post-Translational Modifications) subchallenge of the BioNLP ST’11 provided corpora and competitive evaluations for the detection of epigenetics and post-translational modification (PTM) events, while the REL (Entity Relations) subchallenge covers structural and complex membership relations of proteins (Ohta et al., 2011b;"
W12-2410,W11-1801,1,0.780089,"ein Catalysis is mediated by CKI . is mediated by CKI . <Protein-Component Entity Serine E Protein phosphorylation REL of T-bet tion, protein/gene names are detected and sentences are parsed. TEES handles all these preprocessing steps via a pipeline of tool wrappers for the GENIA Sentence Splitter (Kazama and Tsujii, 2003), the BANNER named entity recognizer (Leaman and Gonzalez, 2008), the McClosky-Charniak-Johnson (McCCJ) parser (Charniak and Johnson, 2005; McClosky, 2010) and the Stanford tools (de Marneffe et al., 2006). For a detailed description of TEES we refer to Bj¨orne and Salakoski (2011) and for the computational requirements of PubMed-scale event extraction to Bj¨orne et al. (2010). EPI conversion to ST format and database import Figure 1: Event and relation extraction. Article text is split into sentences (A), where gene/protein entities are detected and normalized to their Entrez Gene IDs (B). Each sentence with at least one entity is then parsed (C). EPI events and REL relations are extracted from the parsed sentences (D) and following conversion to the BioNLP ST format are imported into a database (E). (Adapted from Bj¨orne and Salakoski (2011)). The extraction of events"
W12-2410,de-marneffe-etal-2006-generating,0,0.0781296,"Missing"
W12-2410,N10-1004,0,0.0126786,"d IN by . . NN CKI EPI detection TEES 2.3 <Theme <Site Entity Serine Theme> Cause> Phosphorylation phosphorylation Protein of T-bet Protein Catalysis is mediated by CKI . is mediated by CKI . <Protein-Component Entity Serine E Protein phosphorylation REL of T-bet tion, protein/gene names are detected and sentences are parsed. TEES handles all these preprocessing steps via a pipeline of tool wrappers for the GENIA Sentence Splitter (Kazama and Tsujii, 2003), the BANNER named entity recognizer (Leaman and Gonzalez, 2008), the McClosky-Charniak-Johnson (McCCJ) parser (Charniak and Johnson, 2005; McClosky, 2010) and the Stanford tools (de Marneffe et al., 2006). For a detailed description of TEES we refer to Bj¨orne and Salakoski (2011) and for the computational requirements of PubMed-scale event extraction to Bj¨orne et al. (2010). EPI conversion to ST format and database import Figure 1: Event and relation extraction. Article text is split into sentences (A), where gene/protein entities are detected and normalized to their Entrez Gene IDs (B). Each sentence with at least one entity is then parsed (C). EPI events and REL relations are extracted from the parsed sentences (D) and following conversion"
W12-2410,W10-1903,1,0.868585,"Missing"
W12-2410,W11-1803,1,0.805324,"ein Catalysis is mediated by CKI . is mediated by CKI . <Protein-Component Entity Serine E Protein phosphorylation REL of T-bet tion, protein/gene names are detected and sentences are parsed. TEES handles all these preprocessing steps via a pipeline of tool wrappers for the GENIA Sentence Splitter (Kazama and Tsujii, 2003), the BANNER named entity recognizer (Leaman and Gonzalez, 2008), the McClosky-Charniak-Johnson (McCCJ) parser (Charniak and Johnson, 2005; McClosky, 2010) and the Stanford tools (de Marneffe et al., 2006). For a detailed description of TEES we refer to Bj¨orne and Salakoski (2011) and for the computational requirements of PubMed-scale event extraction to Bj¨orne et al. (2010). EPI conversion to ST format and database import Figure 1: Event and relation extraction. Article text is split into sentences (A), where gene/protein entities are detected and normalized to their Entrez Gene IDs (B). Each sentence with at least one entity is then parsed (C). EPI events and REL relations are extracted from the parsed sentences (D) and following conversion to the BioNLP ST format are imported into a database (E). (Adapted from Bj¨orne and Salakoski (2011)). The extraction of events"
W12-2410,W09-1301,1,0.907084,"Missing"
W12-2410,W11-1812,1,0.693859,"ein Catalysis is mediated by CKI . is mediated by CKI . <Protein-Component Entity Serine E Protein phosphorylation REL of T-bet tion, protein/gene names are detected and sentences are parsed. TEES handles all these preprocessing steps via a pipeline of tool wrappers for the GENIA Sentence Splitter (Kazama and Tsujii, 2003), the BANNER named entity recognizer (Leaman and Gonzalez, 2008), the McClosky-Charniak-Johnson (McCCJ) parser (Charniak and Johnson, 2005; McClosky, 2010) and the Stanford tools (de Marneffe et al., 2006). For a detailed description of TEES we refer to Bj¨orne and Salakoski (2011) and for the computational requirements of PubMed-scale event extraction to Bj¨orne et al. (2010). EPI conversion to ST format and database import Figure 1: Event and relation extraction. Article text is split into sentences (A), where gene/protein entities are detected and normalized to their Entrez Gene IDs (B). Each sentence with at least one entity is then parsed (C). EPI events and REL relations are extracted from the parsed sentences (D) and following conversion to the BioNLP ST format are imported into a database (E). (Adapted from Bj¨orne and Salakoski (2011)). The extraction of events"
W12-2410,W11-1816,1,0.886081,"Missing"
W12-2410,W10-1921,1,0.898429,"Missing"
W13-1908,W09-1402,1,0.898294,"Missing"
W13-1908,W10-1904,1,0.906348,"Missing"
W13-1908,W13-2004,1,0.864971,"Missing"
W13-1908,W13-2001,0,0.0223085,"Missing"
W13-1908,E12-2021,0,0.0276754,"Missing"
W13-1908,W11-0204,1,0.861919,"Missing"
W13-1908,W09-1401,0,0.504712,"entity recognition’) is a central task and a prerequisite for any follow-up work in BioNLP. Linking these mentions to their respective gene database identifiers, ‘gene normalization’, is a crucial step to allow for integration of textual information with authoritative databases and experimental results. Other BC tasks are engaged in finding functional and physical relations between gene products, including Gene Ontology annotations and protein-protein interactions. Focusing more specifically on the molecular interactions between genes and proteins, the BioNLP Shared Task on Event Extraction (Kim et al., 2009; Kim et al., 2011b; Nedellec and others, 2013) covers a number of detailed molecular event types, including binding and transcription, regulatory control and post-translational modifications. Additionally, separate tracks involve specific applications of event extraction, including infectious diseases, bacterial biotopes and cancer genetics. Performance of the participants in each of these challenges is measured using numeric metrics such as precision, recall, F-measure, slot error rate, MAP and TAP scores. While such rigurous evaluations allow for a meaningful comparison between different sy"
W13-1908,W11-1801,0,0.0571276,"Missing"
W13-1908,W11-1802,0,0.037052,"Missing"
W13-1908,W13-2002,0,\N,Missing
W13-1908,W11-1828,0,\N,Missing
W13-2004,W11-0204,1,0.634815,"Missing"
W13-2004,W13-2003,1,\N,Missing
W13-2004,W11-1828,1,\N,Missing
W13-2004,W11-1805,0,\N,Missing
W13-3728,P92-1003,0,0.306285,"o that achieved by the commonly used Stanford tools. 2 Related work The general problem of coordination scope ambiguity is a widely studied, difficult problem. It is frequently tackled by utilizing lexical parallelism and selectional preferences, as for instance in the works of Kawahara and Kurohashi (2011) and Resnik (1999). In the domain of requirements engineering, Chantree et al. (2005) disambiguate coordinations using heuristics based on the distributions of the words appearing in them. Goldberg (1999) has presented an unsupervised model for a limited range of coordination phenomena, and Agarwal and Boggess (1992) introduce a simplified algorithm for recognizing the correct conjuncts for coordinations. Kawahara and Kurohashi (2007) and van Noord (2007) have incorporated disambiguation methods into parsers of Japanese and Dutch, respectively. In dependency representations, there are multiple ways to treat coordination structures, and the chosen treatment also affects coordination scope ambiguities. The Stanford Dependencies scheme (de Marneffe and Manning, 2008) used in this work considers the first coordinated element the head of the coordination, and uses an additional layer of dependencies to represe"
W13-3728,C10-1011,0,0.0301952,"Missing"
W13-3728,P04-1082,0,0.0289912,"ask is comparatively simple. Separately for the governor and dependent, we generate their token features (the same features as in conjunct propagation) and the set of types of dependencies they govern. As with conjunct propagation, explicit feature pair generation as well as normalization of feature vectors to unit length are employed. 4.3 External subject assignment Unlike in the two previous tasks, we find that assignment of external subjects is best approached by a simple rule-based method, since only clausal complements governed by an xcomp dependency have an external subject. As noted by Campbell (2004), in some highly restricted linguistic problems rule-based approaches are sufficient. The rule assigning external subjects only needs to account for whether the external subject dependency type is the regular subject type xsubj or the copular subject type xsubj-cop. Further, chains of clausal complements with external subjects must be correctly addressed, so that each open clausal complement correctly receives an external subject. 4.4 Combining predictions As mentioned earlier in Section 3.2, the three tasks are not independent of each other: First, if the predicted syntactic function of the r"
W13-3728,P99-1081,0,0.0603513,"erior to each of these baselines. In particular, the method demonstrates performance clearly superior to that achieved by the commonly used Stanford tools. 2 Related work The general problem of coordination scope ambiguity is a widely studied, difficult problem. It is frequently tackled by utilizing lexical parallelism and selectional preferences, as for instance in the works of Kawahara and Kurohashi (2011) and Resnik (1999). In the domain of requirements engineering, Chantree et al. (2005) disambiguate coordinations using heuristics based on the distributions of the words appearing in them. Goldberg (1999) has presented an unsupervised model for a limited range of coordination phenomena, and Agarwal and Boggess (1992) introduce a simplified algorithm for recognizing the correct conjuncts for coordinations. Kawahara and Kurohashi (2007) and van Noord (2007) have incorporated disambiguation methods into parsers of Japanese and Dutch, respectively. In dependency representations, there are multiple ways to treat coordination structures, and the chosen treatment also affects coordination scope ambiguities. The Stanford Dependencies scheme (de Marneffe and Manning, 2008) used in this work considers t"
W13-3728,W09-1201,0,0.102402,"Missing"
W13-3728,P07-2053,0,0.123146,"Missing"
W13-3728,W10-1819,1,0.845146,", which is defined in multiple variants. The basic variant requires sentence structures to be trees, and the other variants can then be used to add further dependencies on top of the tree structure, making the resulting structures graphs rather than trees. Phenomena that are further analyzed in the non-basic variants of SD include relative clauses, open clausal complements, coordinations and prepositional phrases. 252 The dependencies present in non-basic variants of SD can be useful for applications that build on top of the syntactic analysis. For instance, the clinical domain pilot study of Haverinen et al. (2010) has shown that these dependencies can be used in annotating argument structures of verbs using the popular PropBank scheme (Palmer et al., 2005). Also, Yuret et al. (2012) have used the propagated and collapsed variant of the SD scheme to retrieve as semantically meaningful dependencies as possible in the context of textual entailments. The nonbasic variants of SD are also extensively applied in information extraction, as seen for example in the BioNLP shared tasks on event extraction, where a number of top-ranking systems relied on SD analyses (Kim et al., 2011). In this work, we are concern"
W13-3728,D07-1032,0,0.0211954,"ty is a widely studied, difficult problem. It is frequently tackled by utilizing lexical parallelism and selectional preferences, as for instance in the works of Kawahara and Kurohashi (2011) and Resnik (1999). In the domain of requirements engineering, Chantree et al. (2005) disambiguate coordinations using heuristics based on the distributions of the words appearing in them. Goldberg (1999) has presented an unsupervised model for a limited range of coordination phenomena, and Agarwal and Boggess (1992) introduce a simplified algorithm for recognizing the correct conjuncts for coordinations. Kawahara and Kurohashi (2007) and van Noord (2007) have incorporated disambiguation methods into parsers of Japanese and Dutch, respectively. In dependency representations, there are multiple ways to treat coordination structures, and the chosen treatment also affects coordination scope ambiguities. The Stanford Dependencies scheme (de Marneffe and Manning, 2008) used in this work considers the first coordinated element the head of the coordination, and uses an additional layer of dependencies to represent the propagation of conjunct dependencies (see Figures 1 and 2). As a point of comparison, for instance the Link Gramm"
W13-3728,I11-1051,0,0.0133445,"3), pages 252–261, c 2013 Charles University in Prague, Matfyzpress, Prague, Czech Republic Prague, August 27–30, 2013. several baseline methods, and conclude that the proposed method achieves a performance clearly superior to each of these baselines. In particular, the method demonstrates performance clearly superior to that achieved by the commonly used Stanford tools. 2 Related work The general problem of coordination scope ambiguity is a widely studied, difficult problem. It is frequently tackled by utilizing lexical parallelism and selectional preferences, as for instance in the works of Kawahara and Kurohashi (2011) and Resnik (1999). In the domain of requirements engineering, Chantree et al. (2005) disambiguate coordinations using heuristics based on the distributions of the words appearing in them. Goldberg (1999) has presented an unsupervised model for a limited range of coordination phenomena, and Agarwal and Boggess (1992) introduce a simplified algorithm for recognizing the correct conjuncts for coordinations. Kawahara and Kurohashi (2007) and van Noord (2007) have incorporated disambiguation methods into parsers of Japanese and Dutch, respectively. In dependency representations, there are multiple"
W13-3728,W11-1801,0,0.0546911,"Missing"
W13-3728,W03-2401,0,0.0365747,"represent the propagation of conjunct dependencies (see Figures 1 and 2). As a point of comparison, for instance the Link Grammar scheme (Sleator and Temperley, 1993) makes the coordinating conjunction the head word of the coordination, thus partially resolving the scope ambiguities using tree structures only as will be shown in greater detail in Section 5.2. The Stanford tools1 are able to produce output with the additional dependencies of the SD scheme present, but according to de Marneffe and Manning (2008), this part of the tools performs imperfectly. While the English resource PARC 700 (King et al., 2003), annotated in the LFG-formalism (Bresnan, 2001), contains dependencies similar to those considered in this work, 1 http://nlp.stanford.edu/software/ lex-parser.shtml 253 punct&gt; &lt;nsubj Hän He conj&gt; cc&gt; nommod&gt; nommod&gt; soitti minulle ja kertoi tilanteesta . called me and told about_situation . Figure 1: The basic variant of the Stanford Dependencies scheme on a Finnish sentence. The example can be translated as He called me and told me about the situation. to our knowledge the Turku Dependency Treebank (Haverinen et al., 2011) is the only existing manually annotated resource that contains conju"
W13-3728,E06-1011,0,0.0322825,"kertoi tilanteesta . called me and told about_situation . Figure 1: The basic variant of the Stanford Dependencies scheme on a Finnish sentence. The example can be translated as He called me and told me about the situation. to our knowledge the Turku Dependency Treebank (Haverinen et al., 2011) is the only existing manually annotated resource that contains conjunct propagation as described in the SD scheme. In addition to the post-processing approach implemented in the Stanford tools, also methods to directly parse dependency graphs involving tokens with multiple governors have been studied. McDonald and Pereira (2006) introduce a modification of the Maximum Spanning Tree algorithm to infer secondary dependencies in the Danish Dependency Treebank, and Sagae and Tsujii (2008) present a modification of the Shift-Reduce algorithm, which can parse directed acyclic graphs. 3 3.1 Data Turku Dependency Treebank As both the training and testing data of this study, we use the Turku Dependency Treebank (TDT) (Haverinen et al., 2011), which is a publicly available treebank for Finnish. TDT contains 15,126 sentences (204,399 tokens) from ten different genres or text sources, including for instance Wikipedia, EU-text an"
W13-3728,J05-1004,0,0.0293667,"further dependencies on top of the tree structure, making the resulting structures graphs rather than trees. Phenomena that are further analyzed in the non-basic variants of SD include relative clauses, open clausal complements, coordinations and prepositional phrases. 252 The dependencies present in non-basic variants of SD can be useful for applications that build on top of the syntactic analysis. For instance, the clinical domain pilot study of Haverinen et al. (2010) has shown that these dependencies can be used in annotating argument structures of verbs using the popular PropBank scheme (Palmer et al., 2005). Also, Yuret et al. (2012) have used the propagated and collapsed variant of the SD scheme to retrieve as semantically meaningful dependencies as possible in the context of textual entailments. The nonbasic variants of SD are also extensively applied in information extraction, as seen for example in the BioNLP shared tasks on event extraction, where a number of top-ranking systems relied on SD analyses (Kim et al., 2011). In this work, we are concerned with three phenomena represented in the non-basic variants of SD. Most importantly, we consider the dependencies that are the result of conjun"
W13-3728,C08-1095,0,0.0212903,"e translated as He called me and told me about the situation. to our knowledge the Turku Dependency Treebank (Haverinen et al., 2011) is the only existing manually annotated resource that contains conjunct propagation as described in the SD scheme. In addition to the post-processing approach implemented in the Stanford tools, also methods to directly parse dependency graphs involving tokens with multiple governors have been studied. McDonald and Pereira (2006) introduce a modification of the Maximum Spanning Tree algorithm to infer secondary dependencies in the Danish Dependency Treebank, and Sagae and Tsujii (2008) present a modification of the Shift-Reduce algorithm, which can parse directed acyclic graphs. 3 3.1 Data Turku Dependency Treebank As both the training and testing data of this study, we use the Turku Dependency Treebank (TDT) (Haverinen et al., 2011), which is a publicly available treebank for Finnish. TDT contains 15,126 sentences (204,399 tokens) from ten different genres or text sources, including for instance Wikipedia, EU-text and amateur fiction. TDT has been annotated using the SD scheme, which was originally developed to be used with the English language. Thus it has been slightly m"
W13-3728,C12-1147,0,0.0529933,"tructure, making the resulting structures graphs rather than trees. Phenomena that are further analyzed in the non-basic variants of SD include relative clauses, open clausal complements, coordinations and prepositional phrases. 252 The dependencies present in non-basic variants of SD can be useful for applications that build on top of the syntactic analysis. For instance, the clinical domain pilot study of Haverinen et al. (2010) has shown that these dependencies can be used in annotating argument structures of verbs using the popular PropBank scheme (Palmer et al., 2005). Also, Yuret et al. (2012) have used the propagated and collapsed variant of the SD scheme to retrieve as semantically meaningful dependencies as possible in the context of textual entailments. The nonbasic variants of SD are also extensively applied in information extraction, as seen for example in the BioNLP shared tasks on event extraction, where a number of top-ranking systems relied on SD analyses (Kim et al., 2011). In this work, we are concerned with three phenomena represented in the non-basic variants of SD. Most importantly, we consider the dependencies that are the result of conjunct propagation. They resolv"
W13-3728,1993.iwpt-1.22,0,0.402265,"ord (2007) have incorporated disambiguation methods into parsers of Japanese and Dutch, respectively. In dependency representations, there are multiple ways to treat coordination structures, and the chosen treatment also affects coordination scope ambiguities. The Stanford Dependencies scheme (de Marneffe and Manning, 2008) used in this work considers the first coordinated element the head of the coordination, and uses an additional layer of dependencies to represent the propagation of conjunct dependencies (see Figures 1 and 2). As a point of comparison, for instance the Link Grammar scheme (Sleator and Temperley, 1993) makes the coordinating conjunction the head word of the coordination, thus partially resolving the scope ambiguities using tree structures only as will be shown in greater detail in Section 5.2. The Stanford tools1 are able to produce output with the additional dependencies of the SD scheme present, but according to de Marneffe and Manning (2008), this part of the tools performs imperfectly. While the English resource PARC 700 (King et al., 2003), annotated in the LFG-formalism (Bresnan, 2001), contains dependencies similar to those considered in this work, 1 http://nlp.stanford.edu/software/"
W13-3728,W07-2201,0,0.0796595,"Missing"
W13-5609,W10-1811,0,0.0262924,"Missing"
W13-5609,P98-1046,0,0.236343,"RL) is one of the fundamental tasks of natural language processing. In a sense, it continues from where syntactic parsing ends: it identifies the events and participants, such as agents and patients, present in a sentence, and therefore it is an essential step in automatically processing the sentence semantics. SRL can be applied in, for example, text generation, text understanding, machine translation and fact retrieval (Palmer et al., 2005). There have been several different efforts to capture and annotate semantic roles, the best-known projects being FrameNet (Baker et al., 1998), VerbNet (Dang et al., 1998) and PropBank (Palmer et al., 2005), all built for the English language. Out of the three resources, FrameNet is the most fine-grained one, defining roles for specific classes of verbs, such as Cook and Food for verbs relating to cooking. PropBank, in contrast, uses very generic labels, and is the only one of the three intended for corpus annotation rather than as a lexical resource. VerbNet, in turn, is between FrameNet and PropBank in granularity, and somewhat like PropBank, has close ties to syntactic structure. For a more thorough comparison of the three schemes, see the overview by Palmer"
W13-5609,W11-4519,0,0.488288,"n rather than as a lexical resource. VerbNet, in turn, is between FrameNet and PropBank in granularity, and somewhat like PropBank, has close ties to syntactic structure. For a more thorough comparison of the three schemes, see the overview by Palmer et al. (2010). The PropBank scheme in particular has become popular for semantic role labeling resources: after the initial effort on English, PropBanks for different languages have emerged, including, among others, PropBanks for Chinese (Xue and Palmer, 2009), Arabic (Zaghouani et al., 2010), Hindi (Palmer et al., 2009) and Brazilian Portuguese (Duran and Aluísio, 2011). As a PropBank is intended for corpus annotation purposes, and as the annotation scheme is closely tied to syntax, PropBanks are annotated on top of existing treebanks. For Finnish, a freely available general language treebank has recently become available (Haverinen et al., 2010b, 2011), but no corpus annotated for semantic roles exists in the general domain. Haverinen et al. (2010a) have previously made available a small-scale PropBank of clinical Finnish, and thus shown that in principle, the PropBank scheme is suitable for Finnish and combinable with the Stanford Dependency (SD) scheme (d"
W13-5609,W10-1819,1,0.927136,"me in particular has become popular for semantic role labeling resources: after the initial effort on English, PropBanks for different languages have emerged, including, among others, PropBanks for Chinese (Xue and Palmer, 2009), Arabic (Zaghouani et al., 2010), Hindi (Palmer et al., 2009) and Brazilian Portuguese (Duran and Aluísio, 2011). As a PropBank is intended for corpus annotation purposes, and as the annotation scheme is closely tied to syntax, PropBanks are annotated on top of existing treebanks. For Finnish, a freely available general language treebank has recently become available (Haverinen et al., 2010b, 2011), but no corpus annotated for semantic roles exists in the general domain. Haverinen et al. (2010a) have previously made available a small-scale PropBank of clinical Finnish, and thus shown that in principle, the PropBank scheme is suitable for Finnish and combinable with the Stanford Dependency (SD) scheme (de Marneffe and Manning, 2008a,b), the annotation scheme of both the clinical treebank and the general language treebank of Haverinen et al. In this work, we present the first results of a project that aims to create a general language PropBank for Finnish, built on top of the exis"
W13-5609,J93-2004,0,0.0415251,"dencies with an associated PropBank argument are marked in bold. Note how one of the arguments (Arg0) of the latter verb in the sentence is associated with a second-layer dependency. The example sentence can be translated as The judges disqualified the competitor due to deceit and ordered a punishment. 4 Dependency-based PropBanking The PropBank annotation of this work is built on top of the dependency syntax annotation of TDT, including both the first and second annotation layer. This is in contrast to the English PropBank, which has been built on top of the constituency-based Penn Treebank (Marcus et al., 1993). In the Finnish PropBank, each argument of a verb is associated with a dependency (be it first or second layer) in the underlying treebank, which means that the subtree of the dependent word, as defined by the dependencies of the first annotation layer, acts as the argument. For an illustration of the dependency-based PropBank annotation, see Figure 5. In contrast to the original PropBank (Palmer et al., 2005) where in theory any constituent could be an argument, we make use of a heuristic: in most cases, the arguments of a verb will be its direct dependents. However, unlike the clinical lang"
W13-5609,W08-1301,0,0.339099,"Missing"
W13-5609,W04-2705,0,0.285219,"Missing"
W13-5609,J05-1004,0,0.915622,"19th Nordic Conference of Computational Linguistics (NODALIDA 2013); Linköping Electronic Conference Proceedings #85 [page 41 of 474] 1 Introduction Semantic role labeling (SRL) is one of the fundamental tasks of natural language processing. In a sense, it continues from where syntactic parsing ends: it identifies the events and participants, such as agents and patients, present in a sentence, and therefore it is an essential step in automatically processing the sentence semantics. SRL can be applied in, for example, text generation, text understanding, machine translation and fact retrieval (Palmer et al., 2005). There have been several different efforts to capture and annotate semantic roles, the best-known projects being FrameNet (Baker et al., 1998), VerbNet (Dang et al., 1998) and PropBank (Palmer et al., 2005), all built for the English language. Out of the three resources, FrameNet is the most fine-grained one, defining roles for specific classes of verbs, such as Cook and Food for verbs relating to cooking. PropBank, in contrast, uses very generic labels, and is the only one of the three intended for corpus annotation rather than as a lexical resource. VerbNet, in turn, is between FrameNet and"
W13-5609,W12-1506,0,0.0553242,"Missing"
W13-5609,W10-1836,0,0.0218227,"generic labels, and is the only one of the three intended for corpus annotation rather than as a lexical resource. VerbNet, in turn, is between FrameNet and PropBank in granularity, and somewhat like PropBank, has close ties to syntactic structure. For a more thorough comparison of the three schemes, see the overview by Palmer et al. (2010). The PropBank scheme in particular has become popular for semantic role labeling resources: after the initial effort on English, PropBanks for different languages have emerged, including, among others, PropBanks for Chinese (Xue and Palmer, 2009), Arabic (Zaghouani et al., 2010), Hindi (Palmer et al., 2009) and Brazilian Portuguese (Duran and Aluísio, 2011). As a PropBank is intended for corpus annotation purposes, and as the annotation scheme is closely tied to syntax, PropBanks are annotated on top of existing treebanks. For Finnish, a freely available general language treebank has recently become available (Haverinen et al., 2010b, 2011), but no corpus annotated for semantic roles exists in the general domain. Haverinen et al. (2010a) have previously made available a small-scale PropBank of clinical Finnish, and thus shown that in principle, the PropBank scheme is"
W13-5609,C98-1046,0,\N,Missing
W13-5609,P98-1013,0,\N,Missing
W13-5609,C98-1013,0,\N,Missing
W13-5609,duran-aluisio-2012-propbank,0,\N,Missing
W13-5626,C10-1011,0,0.0894175,"Missing"
W13-5626,W08-1301,0,0.14397,"Missing"
W13-5626,2005.mtsummit-papers.11,0,0.0102569,"c Conference Proceedings #85 [page 291 of 474] 1 Introduction In this paper, we describe the methods and resources used to build the FinnTreeBank-3 (FTB-3) parsebank, a 76.4 million token corpus of Finnish with automatically produced morphological and dependency syntax analyses. The corpus is a resource developed within the FIN-CLARIN consortium, the Finnish member of the CLARIN infrastructure project1 and aims at supporting research and language technology development requiring large-scale parsed corpora. Further, as the underlying texts consist of the multilingual parallel corpora EuroParl (Koehn, 2005) and JRCAcquis (Steinberger et al., 2006), corresponding parsebanks can be constructed for a number of other languages into which these two corpora have been translated as well. The larger context of the FTB-3 parsebank is described by Voutilainen et al. (2012b); our involvement in its development was through a public request for quotation issued by FIN-CLARIN, seeking the development of a sufficiently accurate Finnish syntactic parser and its application to the EuroParl and JRC-Acquis corpora. Our starting point for the development was thus untypical as the corpus text, morphological tagset,"
W13-5626,steinberger-etal-2006-jrc,0,0.0442941,"Missing"
W14-1118,W10-1108,1,0.899237,"Missing"
W14-1118,N13-1090,0,0.107499,"ediate context (BoW architecture). The vector space representation is subsequently extracted from the learned weights within the neural network. One of the main practical advantages of the word2vec method lies in its scalability, allowing quick training on large amounts of text, setting it apart from the majority of other methods of distributional semantics. Additionally, the word2vec method has been shown to produce representations that surpass in quality traditional methods such as Latent Semantic Analysis, especially on tasks measuring the preservation of important linguistic regularities (Mikolov et al., 2013b). Weight 1 2 0 1 0 1 J21.1 Figure 2: Weighting applied to ICD-code index vectors when training WSMs based on ICD-10 codes (RI-ICD). 4.2 this similarity is encoded in the resulting WSM. As a example: for a clinical note labelled with the code J21.1, we add the following index vectors to the context vectors of all its constituting words: iv(J) × 0.125, iv(J2) × 0.25, iv(J21) × 0.5 and iv(J21.1) × 1.0. The underlying hypothesis for building a WSM in this way is that it may capture relations between words in a way that better reflects the clinical domain, compared to the other domain-independent"
W14-1118,W13-3210,1,0.852867,"Missing"
W14-1501,W08-1301,0,0.159588,"Missing"
W14-1501,W04-2705,0,0.0444667,"be used as-is to perform argument identification. We are especially interested in this scheme as it is designed to capture semantically contentful relations (de Marneffe and Manning, 2008) and would thus appear to be the ideal choice as the underlying syntactic representation for SRL. 2 &lt;nsubj:A0 He punct&gt; &lt;nsubj:A0 conj&gt; cc&gt; dobj:A1&gt; &lt;advmod:AM-TMP ate.01 lunch and then dobj:A1&gt; washed.01 dishes . Figure 1: Extended Stanford Dependencies scheme combined with PropBank annotation. sense combinations. The English CoNLL data is derived from the PropBank and NomBank corpora (Palmer et al., 2005; Meyers et al., 2004) and it has a total of 54 different argument roles. In addition to the same 22 roles as Finnish, English also has discontinuous variants for each role. The English data has 958,024 training tokens with 178,988 occurrences of 15,880 unique predicate-sense combinations. All Finnish results are reported on the test subset of the Finnish PropBank, and have no previously published baseline to compare with. The results we report for English are produced on the official test section of the CoNLL’09 data and are thus directly comparable to the official results reported in the Shared Task. In the test"
W14-1501,D09-1003,0,0.0609423,"Missing"
W14-1501,N13-1090,0,0.0640165,"ic role labeling for Finnish and English. Specifically, we show that the vectors can be circularly shifted to encode syntactic information and subsequently averaged to produce representations of predicate senses and arguments. Further, we show that it is possible to effectively learn a linear transformation between the vector representations of predicates and their arguments, within the same vector space. 1 Introduction Recently, there has been much progress in the development of highly scalable methods for inducing vector space representations of language. In particular, the word2vec method (Mikolov et al., 2013b) is capable of training on billions of tokens in a matter of hours, producing high quality representations. An exciting property exhibited by the vector spaces induced using word2vec is that they preserve a number of linguistic regularities, lending themselves to simple algebraic operations with the vectors (Mikolov et al., 2013c) and linear mapping between different spaces (Mikolov et al., 2013a). These can be seen as post-hoc operations manipulating the vector space with the significant advantage of not requiring a new task-specific representation to be induced, as is customary. In this pa"
W14-1501,P07-2053,0,0.0247386,"Missing"
W14-1501,W13-3728,1,0.824304,"phase, we follow the Shared Task setting whereby morphological and syntactic analysis is predicted as well, i.e., no gold standard data enters the system other than the tokenization and the information of which tokens constitute predicates. We produce the Finnish morphological and syntactic analyses for the test set with the parsing pipeline of Haverinen et al. (2013b), composed of a morphological analyzer and tagger (Hal´acsy et al., 2007; Pirinen, 2008; Lind´en et al., 2009), dependency parser (Bohnet, 2010) and a machinelearning based component for predicting the extended SD dependencies (Nyblom et al., 2013). While the English data is provided with automatically produced dependency parses, we are specifically interested in the SD scheme and therefore we re-parse the corpus with the Stanford parser2 taking a union of the base and collapsed dependency outputs to match the Finnish data. The vector space models used throughout this paper are induced using the word2vec software (skip-gram architecture with default parameters). For Finnish, the model is trained on 1.5 billion tokens of Finnish Internet texts gathered from the Common Crawl dataset.3 The data was sentenceData and Task Setting Throughout"
W14-1501,W13-5609,1,0.851667,"Missing"
W14-1501,J05-1004,0,0.196642,"ncies (SD) scheme can be used as-is to perform argument identification. We are especially interested in this scheme as it is designed to capture semantically contentful relations (de Marneffe and Manning, 2008) and would thus appear to be the ideal choice as the underlying syntactic representation for SRL. 2 &lt;nsubj:A0 He punct&gt; &lt;nsubj:A0 conj&gt; cc&gt; dobj:A1&gt; &lt;advmod:AM-TMP ate.01 lunch and then dobj:A1&gt; washed.01 dishes . Figure 1: Extended Stanford Dependencies scheme combined with PropBank annotation. sense combinations. The English CoNLL data is derived from the PropBank and NomBank corpora (Palmer et al., 2005; Meyers et al., 2004) and it has a total of 54 different argument roles. In addition to the same 22 roles as Finnish, English also has discontinuous variants for each role. The English data has 958,024 training tokens with 178,988 occurrences of 15,880 unique predicate-sense combinations. All Finnish results are reported on the test subset of the Finnish PropBank, and have no previously published baseline to compare with. The results we report for English are produced on the official test section of the CoNLL’09 data and are thus directly comparable to the official results reported in the Sha"
W14-1501,E12-2021,0,0.0710468,"Missing"
W14-1501,E12-1003,0,0.0196112,"to build task specific vector space representations, post-hoc methods to manipulate the vector spaces without retraining are rare. Current SRL systems utilize supervised machine learning approaches, and typically a large set of features. For instance, the winning system in the CoNLL’09 shared task (SRL-only) introduces a heavy feature engineering system, which has about 1000 potential feature templates from which the system discovers the best set to be used (Zhao et al., 2009b). Word similarities are usually introduced to SRL as a part of unsupervised or semi-supervised methods. For example, Titov and Klementiev (2012) present an unsupervised clustering method applying word representation techniques, and Deschacht and Moens (2009) used vector similarities to automatically expand the small training set to build semi-supervised SRL system. Additionally, Turian et al. (2010) have shown that word representations can be included among the features to improve the performance of named entity recognition and chunking systems. 8 Conclusions We set out to test two post-hoc vector space manipulation techniques in the context of semantic role labeling. We found that the circular shift operation can indeed be applied al"
W14-1501,P10-1040,0,0.0600059,"system in the CoNLL’09 shared task (SRL-only) introduces a heavy feature engineering system, which has about 1000 potential feature templates from which the system discovers the best set to be used (Zhao et al., 2009b). Word similarities are usually introduced to SRL as a part of unsupervised or semi-supervised methods. For example, Titov and Klementiev (2012) present an unsupervised clustering method applying word representation techniques, and Deschacht and Moens (2009) used vector similarities to automatically expand the small training set to build semi-supervised SRL system. Additionally, Turian et al. (2010) have shown that word representations can be included among the features to improve the performance of named entity recognition and chunking systems. 8 Conclusions We set out to test two post-hoc vector space manipulation techniques in the context of semantic role labeling. We found that the circular shift operation can indeed be applied also to other vector representations as a way to encode syntactic information. Importantly, the circular shift is applied to a pre-existing vector space representation, rather than during its induction, and is therefore taskindependent. Further, we find that s"
W14-1501,W09-1209,0,0.0198866,"top of the SD scheme, while the English PropBank and NomBank corpora were not. 7 Related work While different methods have been studied to build task specific vector space representations, post-hoc methods to manipulate the vector spaces without retraining are rare. Current SRL systems utilize supervised machine learning approaches, and typically a large set of features. For instance, the winning system in the CoNLL’09 shared task (SRL-only) introduces a heavy feature engineering system, which has about 1000 potential feature templates from which the system discovers the best set to be used (Zhao et al., 2009b). Word similarities are usually introduced to SRL as a part of unsupervised or semi-supervised methods. For example, Titov and Klementiev (2012) present an unsupervised clustering method applying word representation techniques, and Deschacht and Moens (2009) used vector similarities to automatically expand the small training set to build semi-supervised SRL system. Additionally, Turian et al. (2010) have shown that word representations can be included among the features to improve the performance of named entity recognition and chunking systems. 8 Conclusions We set out to test two post-hoc"
W14-1501,W09-1208,0,0.0761987,"top of the SD scheme, while the English PropBank and NomBank corpora were not. 7 Related work While different methods have been studied to build task specific vector space representations, post-hoc methods to manipulate the vector spaces without retraining are rare. Current SRL systems utilize supervised machine learning approaches, and typically a large set of features. For instance, the winning system in the CoNLL’09 shared task (SRL-only) introduces a heavy feature engineering system, which has about 1000 potential feature templates from which the system discovers the best set to be used (Zhao et al., 2009b). Word similarities are usually introduced to SRL as a part of unsupervised or semi-supervised methods. For example, Titov and Klementiev (2012) present an unsupervised clustering method applying word representation techniques, and Deschacht and Moens (2009) used vector similarities to automatically expand the small training set to build semi-supervised SRL system. Additionally, Turian et al. (2010) have shown that word representations can be included among the features to improve the performance of named entity recognition and chunking systems. 8 Conclusions We set out to test two post-hoc"
W14-1501,W09-1201,0,\N,Missing
W15-1815,P14-2048,0,0.0417197,"Missing"
W15-1815,P13-1157,0,0.0297284,"similar studies include Ilisei et al. (2010) presenting a languageindependent system based on average sentence length or lexical richness, Popescu (2011) using solely character 5-grams (ignoring sentence boundaries) to detect English translations, and Avner, Ordan and Wintner (2014) concentrating on morphological properties in Hebrew. Previous studies on classifying machine translated texts mostly rely on different combinations of lexical and grammatical features as well. Aharoni, Koppel, and Goldberg (2014) use a set of function words, POS tags and a mix of the two to classify texts, whereas Arase and Zhou (2013) concentrate on indicators based on sentenceinternal coherence, also called the phrase salad phenomenon (Lopez, 2008). Despite their relative infrequency, some previous work also concentrate on classifying informality. Unlike those concerning translation classification, these concentrate on lexical rather than morphological features. Lahiri, Mitra, and Lu (2011) explore the Formality Score, a frequency list based on the differences of word classes in a corpus. Mosquera and Moreda (2011) define the most relevant features of informality to be the frequencies of spelling mistakes, interjections,"
W15-1815,S13-1035,0,0.049393,"nalyses. The current version consists of 3.2 billion tokens and 241 million sentences. This article has two main objectives. The first aim is to develop classification methods in order to detect informality, machine translations and human translations from the Parsebank. This would facilitate the use of the Parsebank, as searches or applications could be targeted only at certain parts of the corpus. In the classification, the features used include syntactic n-grams, little subtrees of dependency syntax analyses developed for Finnish by Kanerva et al. (2014), originally produced for English by Goldberg and Orwant (2013). Secondly, the study points research directions for the analysis of the linguistic characteristics of the text classes. The automatic classification based on the data-driven combination of lexical, syntactic and morphological features offers a new approach to the linguistic study of these texts and their characteristics, as traditional linguistic studies often concentrate on the analysis of a limited number of preselected features. The study consists of three sets of classification experiments and their analyses. In the first, texts are classified according to the level of formality to standa"
W15-1815,J03-3001,0,0.0493192,"ation performs well for the indomain experiments, delexicalized methods with morpho-syntactic features prove to be more tolerant to variation caused by genre or source language. In addition, the results show that the features used in the classification provide interesting pointers for further, more detailed studies on the linguistic characteristics of these texts. 1 Introduction With its growing size and coverage, the Internet has become an attractive source of material for linguistic resources, used both for linguistics and natural language processing (NLP) applications (Baroni et al., 2009; Kilgarriff and Grefenstette, 2003). However, automatically collected, very large corpora covering all the text that can be found are very heterogeneous, which may complicate their usage. In linguistics, the origin of the corpus texts is of primary importance (Biber et al., 1998; Sinclair, 1996), and also in many NLP applications, such as automatic syntactic analysis, linguistic variation across different domains affects the results significantly (Laippala et al., 2014). This paper presents the first results on the linguistic variation in the Finnish-language Internet by analyzing informality, machine translations and human tra"
W15-1815,W11-4523,0,0.0303982,"Koppel, and Goldberg (2014) use a set of function words, POS tags and a mix of the two to classify texts, whereas Arase and Zhou (2013) concentrate on indicators based on sentenceinternal coherence, also called the phrase salad phenomenon (Lopez, 2008). Despite their relative infrequency, some previous work also concentrate on classifying informality. Unlike those concerning translation classification, these concentrate on lexical rather than morphological features. Lahiri, Mitra, and Lu (2011) explore the Formality Score, a frequency list based on the differences of word classes in a corpus. Mosquera and Moreda (2011) define the most relevant features of informality to be the frequencies of spelling mistakes, interjections, and emoticons. These same individual features have also been studied as signs of informality in many linguistic studies (Lehti and Laippala, 2014). 3 3.1 Data Finnish Internet Parsebank and Syntactic N-grams The current version of the Finnish Internet Parsebank consists of 3.2 billion tokens and 241 million sentences. It is produced by crawling the Finnish web with the Spiderling web crawler2 . Being designed for collecting text corpora, it can be targeted to crawl only pages in a speci"
W15-1815,R11-1091,0,0.255473,"l of translated texts. Baker (1993) was the first to define potential translation universals: features that all translated texts hypothetically share. The existence of translationese has also been tested by studies applying machine learning. Baroni and Bernardini (2006) use monolingual corpora to experiment with for instance lemmas and POS tags, providing evidence that an algorithm can perform better than humans in recognizing human translated texts. Other similar studies include Ilisei et al. (2010) presenting a languageindependent system based on average sentence length or lexical richness, Popescu (2011) using solely character 5-grams (ignoring sentence boundaries) to detect English translations, and Avner, Ordan and Wintner (2014) concentrating on morphological properties in Hebrew. Previous studies on classifying machine translated texts mostly rely on different combinations of lexical and grammatical features as well. Aharoni, Koppel, and Goldberg (2014) use a set of function words, POS tags and a mix of the two to classify texts, whereas Arase and Zhou (2013) concentrate on indicators based on sentenceinternal coherence, also called the phrase salad phenomenon (Lopez, 2008). Despite their"
W15-1818,P05-1036,0,0.147395,"tles, removing or abridging parts which are less critical for the understandability of the programme. 2 Sentence Compression The goal of automatic sentence compression is to create a shorter version of the input sentence, in a way preserving its meaning. Sentence compression is most often extractive, formed by dropping words from a sentence that are not needed for the sentence to be grammatical and do not importantly contribute to the meaning of the sentence. Many sentence compression methods are based on supervised learning using parallel corpora as training material (Knight and Marcu, 2002; Turner and Charniak, 2005; McDonald, 2006; Cohn and Lapata, 2009). Some methods don’t require parallel corpora, but are either based on rules (Gagnon and Da Sylva, 2005) or use language models or statistics gathered from non-parallel sources (Chiori and Furui, 2004; Filippova and Strube, 2008; Clarke and Lapata, 2006). While some systems prune the sentence based on the linear order of the words, others prune the parse trees or modified parse trees. Language models are commonly used to ensure grammatical output. 3 Data and its pre-processing We draw our data from subtitles of the Finnish national broadcasting corporati"
W15-1818,P06-2019,0,0.0251061,"ctive, formed by dropping words from a sentence that are not needed for the sentence to be grammatical and do not importantly contribute to the meaning of the sentence. Many sentence compression methods are based on supervised learning using parallel corpora as training material (Knight and Marcu, 2002; Turner and Charniak, 2005; McDonald, 2006; Cohn and Lapata, 2009). Some methods don’t require parallel corpora, but are either based on rules (Gagnon and Da Sylva, 2005) or use language models or statistics gathered from non-parallel sources (Chiori and Furui, 2004; Filippova and Strube, 2008; Clarke and Lapata, 2006). While some systems prune the sentence based on the linear order of the words, others prune the parse trees or modified parse trees. Language models are commonly used to ensure grammatical output. 3 Data and its pre-processing We draw our data from subtitles of the Finnish national broadcasting corporation television programs provided to us by Lingsoft Inc. From Lingsoft, we have obtained the texts both before and after the compression step of the subtitling process, extracted from the internal processing pipeline. As illustrated in Figure 1, each programme consists of the subtitle texts and"
W15-1818,W08-1301,0,0.0963247,"Missing"
W15-1818,W08-1105,0,0.133946,"pression is most often extractive, formed by dropping words from a sentence that are not needed for the sentence to be grammatical and do not importantly contribute to the meaning of the sentence. Many sentence compression methods are based on supervised learning using parallel corpora as training material (Knight and Marcu, 2002; Turner and Charniak, 2005; McDonald, 2006; Cohn and Lapata, 2009). Some methods don’t require parallel corpora, but are either based on rules (Gagnon and Da Sylva, 2005) or use language models or statistics gathered from non-parallel sources (Chiori and Furui, 2004; Filippova and Strube, 2008; Clarke and Lapata, 2006). While some systems prune the sentence based on the linear order of the words, others prune the parse trees or modified parse trees. Language models are commonly used to ensure grammatical output. 3 Data and its pre-processing We draw our data from subtitles of the Finnish national broadcasting corporation television programs provided to us by Lingsoft Inc. From Lingsoft, we have obtained the texts both before and after the compression step of the subtitling process, extracted from the internal processing pipeline. As illustrated in Figure 1, each programme consists"
W15-1818,S14-2121,1,0.893403,"Missing"
W15-1818,E06-1038,0,0.154909,"parts which are less critical for the understandability of the programme. 2 Sentence Compression The goal of automatic sentence compression is to create a shorter version of the input sentence, in a way preserving its meaning. Sentence compression is most often extractive, formed by dropping words from a sentence that are not needed for the sentence to be grammatical and do not importantly contribute to the meaning of the sentence. Many sentence compression methods are based on supervised learning using parallel corpora as training material (Knight and Marcu, 2002; Turner and Charniak, 2005; McDonald, 2006; Cohn and Lapata, 2009). Some methods don’t require parallel corpora, but are either based on rules (Gagnon and Da Sylva, 2005) or use language models or statistics gathered from non-parallel sources (Chiori and Furui, 2004; Filippova and Strube, 2008; Clarke and Lapata, 2006). While some systems prune the sentence based on the linear order of the words, others prune the parse trees or modified parse trees. Language models are commonly used to ensure grammatical output. 3 Data and its pre-processing We draw our data from subtitles of the Finnish national broadcasting corporation television pr"
W15-1821,C12-1015,0,0.076097,"Missing"
W15-1821,C10-1011,0,0.215062,"atures to the Marmot tagger. We initially apply a hard constraint approach, where the output of the tagger is used to select one of these readings (the reading with the highest overlap of tags and a priority for readings matching the main POS), effectively disambiguating OMorFi output. For words not recognized by OMorFi, the reading produced by Marmot is used as-is, and the wordform itself is used in place of the lemma. This has so far been the strategy taken when learning to parse Finnish (Bohnet et al., 2013). The tagged text is then parsed with the Mate tools graph-based dependency parser (Bohnet, 2010).7 As baseline, we consider the most recent Finnish dependency parser trained and evaluated on the original distribution of TDT. Note that the test sets differ: the baseline is evaluated on a test set matching the data it was trained on, which differs from the new test set in several aspects such as the treatment of named entities. The results are thus broadly comparable, but not directly so. 6 http://turkunlp.github.io/ Finnish-dep-parser/ 7 https://code.google.com/p/mate-tools Proceedings of the 20th Nordic Conference of Computational Linguistics (NODALIDA 2015) 169 Baseline (Haverinen et al"
W15-1821,Q13-1034,1,0.905517,"Missing"
W15-1821,W13-2308,0,0.0432522,"nd resources are available under open licenses from http://bionlp.utu.fi/ud-finnish.html. 1 UD builds on the Google Universal part-ofspeech (POS) tagset (Petrov et al., 2012), the Interset interlingua of morphosyntactic features (Zeman, 2008), and Stanford Dependencies (de Marneffe et al., 2006; Tsarfaty, 2013; de Marneffe et al., 2014). In addition to the abstract annotation scheme, UD defines also a treebank storage format, CoNLL-U. A first version of UD treebank data, building on the Google Universal Dependency Treebanks (McDonald et al., 2013) and many other previously released resources (Bosco et al., 2013; Haverinen et al., 2013b), was recently released1 (Nivre et al., 2015). In this paper, we present the adaptation of the UD guidelines to Finnish and the creation of the UD Finnish treebank by conversion of the previously introduced Turku Dependency Treebank (TDT) (Haverinen et al., 2013b). We also provide a first set of experiments comparing the parsing scores of language-specific treebank annotation to that of a UD treebank, providing an evaluation of both the conversion quality and the feasibility of UD annotation as a parsing target. In a related but separate effort within the UD initiativ"
W15-1821,de-marneffe-etal-2014-universal,1,0.919781,"Missing"
W15-1821,de-marneffe-etal-2006-generating,0,0.146088,"Missing"
W15-1821,E12-1007,0,0.0564683,"Missing"
W15-1821,W13-5609,1,0.649864,"ilable under open licenses from http://bionlp.utu.fi/ud-finnish.html. 1 UD builds on the Google Universal part-ofspeech (POS) tagset (Petrov et al., 2012), the Interset interlingua of morphosyntactic features (Zeman, 2008), and Stanford Dependencies (de Marneffe et al., 2006; Tsarfaty, 2013; de Marneffe et al., 2014). In addition to the abstract annotation scheme, UD defines also a treebank storage format, CoNLL-U. A first version of UD treebank data, building on the Google Universal Dependency Treebanks (McDonald et al., 2013) and many other previously released resources (Bosco et al., 2013; Haverinen et al., 2013b), was recently released1 (Nivre et al., 2015). In this paper, we present the adaptation of the UD guidelines to Finnish and the creation of the UD Finnish treebank by conversion of the previously introduced Turku Dependency Treebank (TDT) (Haverinen et al., 2013b). We also provide a first set of experiments comparing the parsing scores of language-specific treebank annotation to that of a UD treebank, providing an evaluation of both the conversion quality and the feasibility of UD annotation as a parsing target. In a related but separate effort within the UD initiative, the FinnTreeBank 12 ("
W15-1821,W14-4606,0,0.0564409,"Missing"
W15-1821,P13-2017,0,0.0766526,"Missing"
W15-1821,D13-1032,0,0.0980004,"Missing"
W15-1821,silveira-etal-2014-gold,0,0.0495424,"Missing"
W15-1821,simi-etal-2014-less,0,0.0409146,"Missing"
W15-1821,E12-2021,1,0.893639,"Missing"
W15-1821,P13-2103,0,0.023125,"f-the-art parser trained on a languagespecific annotation schema to performance on the corresponding UD annotation. The results show improvement compared to the source annotation, indicating that the conversion is accurate and supporting the feasibility of UD as a parsing target. The introduced tools and resources are available under open licenses from http://bionlp.utu.fi/ud-finnish.html. 1 UD builds on the Google Universal part-ofspeech (POS) tagset (Petrov et al., 2012), the Interset interlingua of morphosyntactic features (Zeman, 2008), and Stanford Dependencies (de Marneffe et al., 2006; Tsarfaty, 2013; de Marneffe et al., 2014). In addition to the abstract annotation scheme, UD defines also a treebank storage format, CoNLL-U. A first version of UD treebank data, building on the Google Universal Dependency Treebanks (McDonald et al., 2013) and many other previously released resources (Bosco et al., 2013; Haverinen et al., 2013b), was recently released1 (Nivre et al., 2015). In this paper, we present the adaptation of the UD guidelines to Finnish and the creation of the UD Finnish treebank by conversion of the previously introduced Turku Dependency Treebank (TDT) (Haverinen et al., 2013b). W"
W15-1821,zeman-2008-reusable,0,0.308489,"y present parsing experiments comparing the performance of a stateof-the-art parser trained on a languagespecific annotation schema to performance on the corresponding UD annotation. The results show improvement compared to the source annotation, indicating that the conversion is accurate and supporting the feasibility of UD as a parsing target. The introduced tools and resources are available under open licenses from http://bionlp.utu.fi/ud-finnish.html. 1 UD builds on the Google Universal part-ofspeech (POS) tagset (Petrov et al., 2012), the Interset interlingua of morphosyntactic features (Zeman, 2008), and Stanford Dependencies (de Marneffe et al., 2006; Tsarfaty, 2013; de Marneffe et al., 2014). In addition to the abstract annotation scheme, UD defines also a treebank storage format, CoNLL-U. A first version of UD treebank data, building on the Google Universal Dependency Treebanks (McDonald et al., 2013) and many other previously released resources (Bosco et al., 2013; Haverinen et al., 2013b), was recently released1 (Nivre et al., 2015). In this paper, we present the adaptation of the UD guidelines to Finnish and the creation of the UD Finnish treebank by conversion of the previously in"
W15-1821,petrov-etal-2012-universal,0,\N,Missing
W15-2124,de-marneffe-etal-2014-universal,1,0.859804,"Missing"
W15-2124,S13-1035,0,0.0159772,"les and by using the dependency syntax information to analyze the language of the web corpus. We conclude with a discussion of the requirements of extending from this case study on Finnish to create consistently annotated web-scale parsebanks for a large number of languages. 1 Introduction The enormous potential of the web as a source of material for linguistic research in a wide range of areas is well established (Kilgarriff and Grefenstette, 2003), with many new opportunities created by web-scale resources ranging from simple N -grams (Brants and Franz, 2006) to syntactically analyzed text (Goldberg and Orwant, 2013). Yet, while the use of multilingual web data to support linguistic research is well recognized (Way 1 http://universaldependencies.github. io/docs/ 211 Proceedings of the Third International Conference on Dependency Linguistics (Depling 2015), pages 211–220, Uppsala, Sweden, August 24–26 2015. corpora, which range in size from 24,000 tokens (Irish) (Lynn et al., 2014) to over 1,5 million tokens (Czech) (Bejˇcek et al., 2012). interface, thus building a large-scale corpus and pairing it with the tools necessary for its efficient use. Using real-world examples, we show how the large web corpus"
W15-2124,C12-1015,0,0.0537307,"Missing"
W15-2124,J03-3001,0,0.698978,"innish web-scale parsebank. We further integrate this data into an online dependency search system and demonstrate its applicability by showing linguistically motivated search examples and by using the dependency syntax information to analyze the language of the web corpus. We conclude with a discussion of the requirements of extending from this case study on Finnish to create consistently annotated web-scale parsebanks for a large number of languages. 1 Introduction The enormous potential of the web as a source of material for linguistic research in a wide range of areas is well established (Kilgarriff and Grefenstette, 2003), with many new opportunities created by web-scale resources ranging from simple N -grams (Brants and Franz, 2006) to syntactically analyzed text (Goldberg and Orwant, 2013). Yet, while the use of multilingual web data to support linguistic research is well recognized (Way 1 http://universaldependencies.github. io/docs/ 211 Proceedings of the Third International Conference on Dependency Linguistics (Depling 2015), pages 211–220, Uppsala, Sweden, August 24–26 2015. corpora, which range in size from 24,000 tokens (Irish) (Lynn et al., 2014) to over 1,5 million tokens (Czech) (Bejˇcek et al., 201"
W15-2124,Q13-1034,1,0.915741,"Missing"
W15-2124,C10-1011,0,0.0765138,"Missing"
W15-2124,N15-3011,1,0.802082,"h the basic this data (primarily) by deterministic conversion, the results are thus not fully comparable with results for the UD Finnish corpus. 6 Note that results are for the original SD annotation of the TDT corpus. While the UD Finnish treebank is created from 214 Figure 3: A screenshot of the online query interface, showing a simple query for transitive verbs. Item All tokens Lemma count Sentence count Unique token count Unique sentence count Tokens without duplicates Number 3,662,727,698 28,585,422 275,690,022 39,688,642 178,547,962 2,554,094,599 and the extended layers of the analysis (Luotolahti et al., 2015). This detailed corpus search enables fast and easy retrieval of material for many linguistic questions that otherwise would require manual work to address. The query system allows search for any arbitrary subtree structure, including arbitrarily nested negations. For instance, one can search for verbs which have their subject in the partitive case, unless that subject has a numeral modifier, and unless the verb is governed by the clausal complement relation. In addition to the constraints on the syntactic structure, any combination of normal and negated constraints on the morphology of the wo"
W15-2124,W14-4606,0,0.0200094,"ide range of areas is well established (Kilgarriff and Grefenstette, 2003), with many new opportunities created by web-scale resources ranging from simple N -grams (Brants and Franz, 2006) to syntactically analyzed text (Goldberg and Orwant, 2013). Yet, while the use of multilingual web data to support linguistic research is well recognized (Way 1 http://universaldependencies.github. io/docs/ 211 Proceedings of the Third International Conference on Dependency Linguistics (Depling 2015), pages 211–220, Uppsala, Sweden, August 24–26 2015. corpora, which range in size from 24,000 tokens (Irish) (Lynn et al., 2014) to over 1,5 million tokens (Czech) (Bejˇcek et al., 2012). interface, thus building a large-scale corpus and pairing it with the tools necessary for its efficient use. Using real-world examples, we show how the large web corpus with the syntactic annotation can be used for gathering data on rare phenomena in linguistic research. For linguistic research web corpora, containing broad scope of text, are well suited for the search of rare linguistics constructs as well as those which do not often appear on official text, such as the use of colloquial terms and structures. Other motivations beyond"
W15-2124,W15-1821,1,0.872646,"Missing"
W15-2124,D13-1032,0,0.0617608,"Missing"
W15-2124,E12-2021,1,0.855308,"Missing"
W15-2124,J03-3004,0,0.046451,"Missing"
W15-2124,W13-3728,1,0.905182,"Missing"
W15-2124,petrov-etal-2012-universal,0,0.057271,"Missing"
W15-2124,W11-4644,0,0.0406162,"Missing"
W15-2124,prasad-etal-2008-penn,0,0.0160259,"ubstantially increase the size of any language-specific corpus over that created here, we expect the total computational cost of scaling from one language to ten to be simply an order of magnitude greater than that here. Thus, we estimate that the total computational cost of creating the first set of UD web parsebanks to be on the order of 100,000 CPU core hours. While this is a non-trivial cost, it is well within our resources. be used to retrieve material to study phenomena crossing the limits of individual sentences, such as semantic relations between text elements and discourse structure (Prasad et al., 2008; Laippala et al., 2015). As the search tool allows the restriction of the query to certain sentence elements, it can be delimited to sentence-initial elements, such as sentence-initial, individual conjunctions that instead of co-ordinating sentence-internal clauses or phrases refer to previous text elements and express relations between sentences and the discourse structure. This can provide useful information both on the frequency of different conjunctions used in this position and on discourse structure more in general. The distribution of the most frequently used conjunctions in this funct"
W15-3021,de-marneffe-etal-2014-universal,1,0.838757,"Missing"
W15-3021,N13-1073,0,0.03498,"ing data which results in significant improvements without any language-specific optimization. In the following, we will first present our systems and the results achieved with our models before discussing the translation produced in more detail. The latter analyses pinpoint issues and problems that provide valuable insights for future development. 2 Basic Setup and Data Sets All our translation systems are based on Moses (Koehn et al., 2007) and standard components for training and tuning the models. We apply KenLM for language modeling (Heafield et al., 2013), fast align for word alignment (Dyer et al., 2013) and MERT for parameter tuning (Och, 2003). All our models use lowercased training data and the results that we report refer to lowercased output of our models. All language models are of order five and use the standard modified Kneser-Ney smoothing implemented in KenLM. All phrase tables are pruned based on significance testing (Johnson et al., 2007) and reducing translation options to at most 30 per phrase type. The maximum phrase length is seven. 177 Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 177–183, c Lisboa, Portugal, 17-18 September 2015. 2015 Associatio"
W15-3021,E12-1068,0,0.0734406,"hological pre-processing of Finnish. 1 Figure 1: A sentence illustrating the inflective and compounding nature of Finnish in contrast to English. (ADE, INE: adessive, inessive cases, PASS: passive, PL: plural) Introduction The basic goal of our submissions is to establish some straightforward baselines for the translation between Finnish and English using standard technology such as phrase-based and factored statistical machine translation, in preparation for a more focused future effort in combination with the stateof-the-art techniques in SMT for morphologically complex languages (see e.g. (Fraser et al., 2012)). The translation between Finnish and English (in both directions) is a new task in this year’s workshop adding a new exciting challenge to the established setup. The main difficulty in this task is to manage the rich morphology of Finnish which has several implications on training and expected results with standard SMT models (see the illustration in Figure 1). Moreover, the monolingual and parallel training data is substantially smaller which makes the task even tougher compared with other languages pairs in the competition. In our contribution, we focus on Finnish-English emphasizing the n"
W15-3021,P13-2121,0,0.0509981,"Missing"
W15-3021,D07-1103,0,0.0424679,"2 Basic Setup and Data Sets All our translation systems are based on Moses (Koehn et al., 2007) and standard components for training and tuning the models. We apply KenLM for language modeling (Heafield et al., 2013), fast align for word alignment (Dyer et al., 2013) and MERT for parameter tuning (Och, 2003). All our models use lowercased training data and the results that we report refer to lowercased output of our models. All language models are of order five and use the standard modified Kneser-Ney smoothing implemented in KenLM. All phrase tables are pruned based on significance testing (Johnson et al., 2007) and reducing translation options to at most 30 per phrase type. The maximum phrase length is seven. 177 Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 177–183, c Lisboa, Portugal, 17-18 September 2015. 2015 Association for Computational Linguistics. For processing Finnish, we use the Finnish dependency parser pipeline1 developed at the University of Turku (Haverinen et al., 2014). This pipeline integrates all pre-processing steps that are necessary for data-driven dependency parsing including tokenization, morphological analyses and part-ofspeech tagging, and prod"
W15-3021,niemi-linden-2012-representing,0,0.06502,"Missing"
W15-3021,P03-1021,0,0.0419687,"without any language-specific optimization. In the following, we will first present our systems and the results achieved with our models before discussing the translation produced in more detail. The latter analyses pinpoint issues and problems that provide valuable insights for future development. 2 Basic Setup and Data Sets All our translation systems are based on Moses (Koehn et al., 2007) and standard components for training and tuning the models. We apply KenLM for language modeling (Heafield et al., 2013), fast align for word alignment (Dyer et al., 2013) and MERT for parameter tuning (Och, 2003). All our models use lowercased training data and the results that we report refer to lowercased output of our models. All language models are of order five and use the standard modified Kneser-Ney smoothing implemented in KenLM. All phrase tables are pruned based on significance testing (Johnson et al., 2007) and reducing translation options to at most 30 per phrase type. The maximum phrase length is seven. 177 Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 177–183, c Lisboa, Portugal, 17-18 September 2015. 2015 Association for Computational Linguistics. For proce"
W15-3021,W15-1821,1,0.885434,"Missing"
W15-3021,skadins-etal-2014-billions,1,0.864782,"Missing"
W15-3021,tiedemann-2012-parallel,1,0.881187,"Missing"
W15-3021,P07-2045,0,\N,Missing
W16-2326,E14-1061,1,0.903294,"s. The underlying idea of what we call re-inflection models in our submission is that we reduce all Finnish training data to an underspecified representation, where words are reduced to their lemmas and noun and adjective compounds are split into their component parts. Then, we train models and translate from English into this underspecified representation of Finnish and in a post-processing step we then merge compounds and predict morphological features for Finnish. This approach has been successfully applied to Russian and Arabic (Toutanova et al., 2008) and to German (Fraser et al. (2012), Cap et al. (2014)). Note however, that for example Fraser et al. (2012) relied on German prepositions to predict case-markers on underspecified German SMT output. In contrast to many other languages, Finnish only has a limited number of stand-alone pre- and postpositions. Instead, the prepositional meaning is encoded by case-marking. We thus adapt an approach by Tiedemann et al. (2015b) and introduce place-holder prepositions in the Finnish training data, which are likely to correspond to the prepositions used on the English side and thus improve word alignment quality. Place-holder Prepositions: In contrast t"
W16-2326,W15-2501,1,0.829554,"ents (system (c) in Table 2). In combination with the surfaceoriented translation model this also leads to a slight improvement over the non-factored model (without back-translated news), which is also evi394 3.4 ilar to Tiedemann et al. (2015b), we introduce place-holder prepositions at the beginning of noun phrases bearing the corresponding case-marker in order to support word alignment. Gappy Language Models Tiedemann (2015) introduces the use of language models over selected words in the framework of document-level SMT using Docent applied to the pronoun-aware translation task of DiscoMT (Hardmeier et al., 2015). We extended this idea by developing a general framework for what we call gappy language models that refer to monolingual or bilingual n-gram language models over selected words and their alignments. We can use different factors attached to the source and target language tokens to filter for word sequences that we would like to consider. Given word alignments are used to establish the link between source and target tokens. Gappy language models may cross sentence-boundaries but may also stop at those borders. Regular expressions can be used to make the selection more flexible. Multi-word alig"
W16-2326,N12-1047,0,0.0925095,"ta sets. We use the English models for sentence boundary detection and tokenisation provided by OpenNLP,2 which is compatible with the Penn Treebank style of tokenisation. This is important for the subsequent tagging and parsing steps, which we trained on the Universal Dependencies treebank for English using MarMoT and mate-tools. MT Tools: Most of our systems are based on Moses (Koehn et al., 2007) and common components for training and tuning models. We apply KenLM (Heafield et al., 2013) and SRILM (Stolcke, 2002) for estimating language model parameters and MERT (Och, 2003) and batch-MIRA (Cherry and Foster, 2012) for parameter tuning. Most of our models are based on lowercased training data. All language models use order five with modified Kneser-Ney smoothing if not stated otherwise. All MT systems apply the phrase-based paradigm, some of them with factored representations and generation models if necessary. For word alignment we experiment with different tools. We apply standard tools like GIZA++ 1 2 (Och and Ney, 2003) and fast align (Dyer et al., 2013) but also the recently proposed Bayesian ¨ word aligner efmaral (Ostling, 2015). Efmaral is an efficient implementation of a Markov-Chain aligner us"
W16-2326,N13-1138,0,0.0239033,"a to their heads and train the models on this representation. As we are using the words and lemmas as features for the CRFs, the reduction of compounds to their heads reduces data sparsity and allows the model to better generalise over all occurrences. For the translation output we remove all compound modifiers before case prediction. Morphological Generation The predicted casemarkers are then fed into the morphological generation automaton (Pirinen, 2015) in order to get fully inflected forms. In cases where this generation failed, we used a supervised machine learning approach as a backoff (Durrett and DeNero, 2013). Compound Processing In a final step, we merge compounds using a POS-matching strategy (Stymne et al., 2008). We merge the marked compound modifiers with the following word if it is a noun or adjective, and add hyphens for modifiers in coordinated compounds. Compounding forms of modifiers are restored based on corpus frequencies. Like Stymne et al. (2008) and Cap et al. (2014), we also merge compounds in every iteration of the tuning process before the translations are scored against the reference. All re-inflection systems are constrained systems. We used Europarl and Wikipedia as parallel r"
W16-2326,P13-2121,0,0.0411259,"Missing"
W16-2326,N13-1073,0,0.0338579,"e apply KenLM (Heafield et al., 2013) and SRILM (Stolcke, 2002) for estimating language model parameters and MERT (Och, 2003) and batch-MIRA (Cherry and Foster, 2012) for parameter tuning. Most of our models are based on lowercased training data. All language models use order five with modified Kneser-Ney smoothing if not stated otherwise. All MT systems apply the phrase-based paradigm, some of them with factored representations and generation models if necessary. For word alignment we experiment with different tools. We apply standard tools like GIZA++ 1 2 (Och and Ney, 2003) and fast align (Dyer et al., 2013) but also the recently proposed Bayesian ¨ word aligner efmaral (Ostling, 2015). Efmaral is an efficient implementation of a Markov-Chain aligner using Gibbs sampling with a Bayesian extension of the IBM alignment models. It is both fast and accurate and works as a straightforward plug-in replacement for standard tools in the SMT training pipeline. The aligner is faster than fast align but more accurate in terms of alignment error rate in various benchmark tests. The advantage of using Gibbs sampling rather than the Expectation-Maximisation algorithm (as do both fast align and GIZA++) is that"
W16-2326,W11-2123,0,0.0837537,"d compound modifiers with the following word if it is a noun or adjective, and add hyphens for modifiers in coordinated compounds. Compounding forms of modifiers are restored based on corpus frequencies. Like Stymne et al. (2008) and Cap et al. (2014), we also merge compounds in every iteration of the tuning process before the translations are scored against the reference. All re-inflection systems are constrained systems. We used Europarl and Wikipedia as parallel resources and all of the Finnish data available from WMT to train five-gram language models with SRILM (Stolcke, 2002) and KENLM (Heafield, 2011). No particular cleaning or preprocessing of the data has happened. This makes the re-inflection systems differ from all other systems in this paper. Otherwise, we trained a conventional phrase-based Moses system with default settings, tuned weights using batch-MIRA with ”safe-hope” (Cherry and Foster, 2012) and used an underspecified representation of the tuning reference set to derive BLEU scores. The final result of our system is listed in Table 4. • nouns and their alignments (sentence-internal only and even document-wide) • verbs and their alignments (sentence-internal only and even docum"
W16-2326,E12-1068,1,0.904335,"with the other systems. The underlying idea of what we call re-inflection models in our submission is that we reduce all Finnish training data to an underspecified representation, where words are reduced to their lemmas and noun and adjective compounds are split into their component parts. Then, we train models and translate from English into this underspecified representation of Finnish and in a post-processing step we then merge compounds and predict morphological features for Finnish. This approach has been successfully applied to Russian and Arabic (Toutanova et al., 2008) and to German (Fraser et al. (2012), Cap et al. (2014)). Note however, that for example Fraser et al. (2012) relied on German prepositions to predict case-markers on underspecified German SMT output. In contrast to many other languages, Finnish only has a limited number of stand-alone pre- and postpositions. Instead, the prepositional meaning is encoded by case-marking. We thus adapt an approach by Tiedemann et al. (2015b) and introduce place-holder prepositions in the Finnish training data, which are likely to correspond to the prepositions used on the English side and thus improve word alignment quality. Place-holder Preposit"
W16-2326,P13-4033,1,0.860325,"an fast align but more accurate in terms of alignment error rate in various benchmark tests. The advantage of using Gibbs sampling rather than the Expectation-Maximisation algorithm (as do both fast align and GIZA++) is that inference remains quadratic with respect to sentence length even when word order and fertility models are added, which enables the efficient use of higher-order models. This is the first time that the performance of this tool is reported in the setting of statistical machine translation. Besides Moses, we also apply another phrasebased machine translation decoder, Docent (Hardmeier et al., 2013), which implements a stochastic local search decoder that is able to incorporate features with long-distance dependencies even across sentence boundaries. Docent emphasises document-level decoding but includes standard local features that make the decoder comparable with standard phrase-based SMT. The decoding algorithm applies randomly selected statechange operations to complete translation hypotheses (covering the whole document) that may be accepted by a strict hill-climbing procedure or a simulated annealing schedule. The main motivation for using Docent in our setup is to introduce non-lo"
W16-2326,W15-3021,1,0.892607,"Helsinki Fabienne Cap Uppsala University Jenna Kanerva and Filip Ginter University of Turku Sara Stymne Uppsala University ¨ Robert Ostling University of Helsinki Marion Di Marco University of Stuttgart Abstract strained systems apply all the data provided by WMT and also the English Giga-Word corpus that is distributed by the LDC. Our best systems include additional parallel data sets coming from OPUS (Tiedemann, 2012) and syntactically analysed monolingual data from the Finnish Internet Parsebank (Luotolahti et al., 2015). Additional to the parallel data we used in our submission last year (Tiedemann et al., 2015a), we include the new version of the OpenSubtitle corpus (Lison and Tiedemann, 2016) with its 18.6 million aligned translation units in English and Finnish. Furthermore, we make use of alternative subtitle translations that have been aligned monolingually in the same collection (Tiedemann, 2016). Expanding the parallel corpus with alternative translations extends the subtitle corpus by roughly 350,000 translation units with about 6.8 million tokens (counting both languages together). The contribution is quite small compared to the original corpus with its 107 million Finnish tokens and 167 mi"
W16-2326,L16-1147,1,0.802036,"y of Turku Sara Stymne Uppsala University ¨ Robert Ostling University of Helsinki Marion Di Marco University of Stuttgart Abstract strained systems apply all the data provided by WMT and also the English Giga-Word corpus that is distributed by the LDC. Our best systems include additional parallel data sets coming from OPUS (Tiedemann, 2012) and syntactically analysed monolingual data from the Finnish Internet Parsebank (Luotolahti et al., 2015). Additional to the parallel data we used in our submission last year (Tiedemann et al., 2015a), we include the new version of the OpenSubtitle corpus (Lison and Tiedemann, 2016) with its 18.6 million aligned translation units in English and Finnish. Furthermore, we make use of alternative subtitle translations that have been aligned monolingually in the same collection (Tiedemann, 2016). Expanding the parallel corpus with alternative translations extends the subtitle corpus by roughly 350,000 translation units with about 6.8 million tokens (counting both languages together). The contribution is quite small compared to the original corpus with its 107 million Finnish tokens and 167 million English tokens, but, nevertheless, it contributes to the overall collection esp"
W16-2326,tiedemann-2012-parallel,1,0.855367,"Missing"
W16-2326,W15-2124,1,0.739948,"a, Better Models and Alternative Alignment and Translation Tools J¨org Tiedemann University of Helsinki Fabienne Cap Uppsala University Jenna Kanerva and Filip Ginter University of Turku Sara Stymne Uppsala University ¨ Robert Ostling University of Helsinki Marion Di Marco University of Stuttgart Abstract strained systems apply all the data provided by WMT and also the English Giga-Word corpus that is distributed by the LDC. Our best systems include additional parallel data sets coming from OPUS (Tiedemann, 2012) and syntactically analysed monolingual data from the Finnish Internet Parsebank (Luotolahti et al., 2015). Additional to the parallel data we used in our submission last year (Tiedemann et al., 2015a), we include the new version of the OpenSubtitle corpus (Lison and Tiedemann, 2016) with its 18.6 million aligned translation units in English and Finnish. Furthermore, we make use of alternative subtitle translations that have been aligned monolingually in the same collection (Tiedemann, 2016). Expanding the parallel corpus with alternative translations extends the subtitle corpus by roughly 350,000 translation units with about 6.8 million tokens (counting both languages together). The contribution"
W16-2326,W15-2515,1,0.854248,". Using this type of lexicalisation helps to find construction-like mappings between the two languages which seems to be beneficial for the system according to the scores in our experiments (system (c) in Table 2). In combination with the surfaceoriented translation model this also leads to a slight improvement over the non-factored model (without back-translated news), which is also evi394 3.4 ilar to Tiedemann et al. (2015b), we introduce place-holder prepositions at the beginning of noun phrases bearing the corresponding case-marker in order to support word alignment. Gappy Language Models Tiedemann (2015) introduces the use of language models over selected words in the framework of document-level SMT using Docent applied to the pronoun-aware translation task of DiscoMT (Hardmeier et al., 2015). We extended this idea by developing a general framework for what we call gappy language models that refer to monolingual or bilingual n-gram language models over selected words and their alignments. We can use different factors attached to the source and target language tokens to filter for word sequences that we would like to consider. Given word alignments are used to establish the link between source"
W16-2326,D13-1032,0,0.0777915,"Missing"
W16-2326,L16-1559,1,0.839226,"Sara Stymne Uppsala University ¨ Robert Ostling University of Helsinki Marion Di Marco University of Stuttgart Abstract strained systems apply all the data provided by WMT and also the English Giga-Word corpus that is distributed by the LDC. Our best systems include additional parallel data sets coming from OPUS (Tiedemann, 2012) and syntactically analysed monolingual data from the Finnish Internet Parsebank (Luotolahti et al., 2015). Additional to the parallel data we used in our submission last year (Tiedemann et al., 2015a), we include the new version of the OpenSubtitle corpus (Lison and Tiedemann, 2016) with its 18.6 million aligned translation units in English and Finnish. Furthermore, we make use of alternative subtitle translations that have been aligned monolingually in the same collection (Tiedemann, 2016). Expanding the parallel corpus with alternative translations extends the subtitle corpus by roughly 350,000 translation units with about 6.8 million tokens (counting both languages together). The contribution is quite small compared to the original corpus with its 107 million Finnish tokens and 167 million English tokens, but, nevertheless, it contributes to the overall collection esp"
W16-2326,J03-1002,0,0.0224522,"s for training and tuning models. We apply KenLM (Heafield et al., 2013) and SRILM (Stolcke, 2002) for estimating language model parameters and MERT (Och, 2003) and batch-MIRA (Cherry and Foster, 2012) for parameter tuning. Most of our models are based on lowercased training data. All language models use order five with modified Kneser-Ney smoothing if not stated otherwise. All MT systems apply the phrase-based paradigm, some of them with factored representations and generation models if necessary. For word alignment we experiment with different tools. We apply standard tools like GIZA++ 1 2 (Och and Ney, 2003) and fast align (Dyer et al., 2013) but also the recently proposed Bayesian ¨ word aligner efmaral (Ostling, 2015). Efmaral is an efficient implementation of a Markov-Chain aligner using Gibbs sampling with a Bayesian extension of the IBM alignment models. It is both fast and accurate and works as a straightforward plug-in replacement for standard tools in the SMT training pipeline. The aligner is faster than fast align but more accurate in terms of alignment error rate in various benchmark tests. The advantage of using Gibbs sampling rather than the Expectation-Maximisation algorithm (as do b"
W16-2326,P03-1021,0,0.0559308,"Missing"
W16-2326,P08-1059,0,0.0341143,"are, therefore, not directly comparable with the other systems. The underlying idea of what we call re-inflection models in our submission is that we reduce all Finnish training data to an underspecified representation, where words are reduced to their lemmas and noun and adjective compounds are split into their component parts. Then, we train models and translate from English into this underspecified representation of Finnish and in a post-processing step we then merge compounds and predict morphological features for Finnish. This approach has been successfully applied to Russian and Arabic (Toutanova et al., 2008) and to German (Fraser et al. (2012), Cap et al. (2014)). Note however, that for example Fraser et al. (2012) relied on German prepositions to predict case-markers on underspecified German SMT output. In contrast to many other languages, Finnish only has a limited number of stand-alone pre- and postpositions. Instead, the prepositional meaning is encoded by case-marking. We thus adapt an approach by Tiedemann et al. (2015b) and introduce place-holder prepositions in the Finnish training data, which are likely to correspond to the prepositions used on the English side and thus improve word alig"
W16-2326,W15-1844,0,0.0376448,"older Prepositions: In contrast to Tiedemann et al. (2015b), we do not apply factored models (with both, lemmatised and surface forms) here but strip the case-markers from those words and only keep the underspecified representation. Moreover, we apply the approach in the opposite translation direction, which requires a generation component. The place-holder prepositions will not only lead to improved word alignments, but we will also use them to predict case-markers after translation. Overall, we follow the processing pipeline of (Cap et al., 2014): we use a rule-based morphological analyser (Pirinen, 2015) to split compounds (using the Finnish parsing pipeline to disambiguate multiple analyses) and lemmatise all Finnish training data. Compound modifiers are reduced to their lemmas and marked with a symbol that distinguishes them from other words. SimBLEU 14.10 5.45 10.89 14.17 14.70 Table 2: Lower-cased BLEU scores for factored SMT models on development test data (newstest 2015). System (a) is the same as the constrained model in Table 1. System (b) uses a factored model that translates surface words to target lemmas and morphosyntactic features separately. System (c) keeps closed-class words i"
W16-2326,W15-1821,1,0.867757,"Missing"
W16-2326,W08-0317,1,0.79278,"the CRFs, the reduction of compounds to their heads reduces data sparsity and allows the model to better generalise over all occurrences. For the translation output we remove all compound modifiers before case prediction. Morphological Generation The predicted casemarkers are then fed into the morphological generation automaton (Pirinen, 2015) in order to get fully inflected forms. In cases where this generation failed, we used a supervised machine learning approach as a backoff (Durrett and DeNero, 2013). Compound Processing In a final step, we merge compounds using a POS-matching strategy (Stymne et al., 2008). We merge the marked compound modifiers with the following word if it is a noun or adjective, and add hyphens for modifiers in coordinated compounds. Compounding forms of modifiers are restored based on corpus frequencies. Like Stymne et al. (2008) and Cap et al. (2014), we also merge compounds in every iteration of the tuning process before the translations are scored against the reference. All re-inflection systems are constrained systems. We used Europarl and Wikipedia as parallel resources and all of the Finnish data available from WMT to train five-gram language models with SRILM (Stolck"
W16-2326,P07-2045,0,\N,Missing
W16-2353,W15-2508,0,0.348754,"of-speech tags and combination of lemmas and part-of-speech tags. In addition we have separate embeddings for source language pronouns aligned with the unknown target pronoun. Context windows are then sequences of indices for these different token-level embeddings, except the aligned source language prohelp predict the pronoun. Other teams relied more on the context, for example UU-Tiedemann (Tiedemann, 2015) used a linear SVM with features from the context of the pronoun. IDIAP (Luong et al., 2015) went on to use a naive-bayes classifier with features from contextual noun-phrases. WHATELLES (Callin et al., 2015) used a neural network approach with features from preceding noun-phrases. It is to be noted that the last year’s task was won by a language model baseline, provided by the organizers. Our system fits the second category of systems, those relying on the context to predict the pronoun. None of the systems participating in the shared task seem to be using explicit sequence classification approaches. 3 3.1 Network Architecture Our system is a deep recurrent neural network model with learned token-level embeddings, two layers of Gated Recurrent Units (GRUs), a dense network layer with rectified li"
W16-2353,W15-2501,0,0.219645,"translation directions, so in total four different source-target pairs must be considered. In the target language side selected set of pronouns are substituted with replace, and the task is then to predict the missing pronoun. Furthermore, the target side language is not given as running text, but instead in lemma plus part-of-speech tag format. This is to mimic the representation which many standard machine translation systems produce and to complicate the matter of standard ? 2 Related work This shared task is a spiritual successor to an earlier cross-lingual pronoun prediction shared task (Hardmeier et al., 2015). The systems submitted to the earlier task provide us with a good view of the recent related work on the problem. The earlier task received altogether six system description papers. The organizers identify two main approaches used by the participants. Teams UEDIN (Wetzel et al., 2015) and MALTA (Pham and van der Plas, 2015) explicitly tried to resolve anaphoras in the text and using the information to Both authors contributed equally to this work. 596 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 596–601, c Berlin, Germany, August 11-12, 2016."
W16-2353,2005.mtsummit-papers.11,0,0.0748555,"reached performance very close to the maximum within three training epochs. All networks were evaluated on the development set after each training epoch, and the model with the highest macro recall was selected for evaluation. The practical, time-wise, predictive performance of our system is reasonable and doesn’t require the use of a GPU. Predicting a test set for an individual language pair takes on a 6-core Intel Training the system Only the training data provided by the shared task organizers is used to train our system. The data is based on three different datasets, the Europarl dataset (Koehn, 2005), news commentary corpora (IWSLT15, NCv9), and the TED corpus2 . We used the whole TED corpus only as development data, and thus our submitted systems are trained on the union of Europarl and news commentary texts, which are randomly shuffled on document level. The total size of training data for each source– target pair is approximately 2.4M sentences, having 590K–760K training examples depending on 1 Dense layer with tanh activation was also tested, but ReLU turned out to give better results. 2 http://www.ted.com 3 598 www.csc.fi de-en en-de fr-en en-fr Target POS 15 15 34 39 Target Word 170"
W16-2353,W15-2513,0,0.0520558,"embedding matrices; embeddings for source language words, separate embeddings for the target language lemmas, part-of-speech tags and combination of lemmas and part-of-speech tags. In addition we have separate embeddings for source language pronouns aligned with the unknown target pronoun. Context windows are then sequences of indices for these different token-level embeddings, except the aligned source language prohelp predict the pronoun. Other teams relied more on the context, for example UU-Tiedemann (Tiedemann, 2015) used a linear SVM with features from the context of the pronoun. IDIAP (Luong et al., 2015) went on to use a naive-bayes classifier with features from contextual noun-phrases. WHATELLES (Callin et al., 2015) used a neural network approach with features from preceding noun-phrases. It is to be noted that the last year’s task was won by a language model baseline, provided by the organizers. Our system fits the second category of systems, those relying on the context to predict the pronoun. None of the systems participating in the shared task seem to be using explicit sequence classification approaches. 3 3.1 Network Architecture Our system is a deep recurrent neural network model with"
W16-2353,W15-2514,0,0.0368659,"Missing"
W16-2353,W15-2515,0,0.152999,"ersa. Starting from the input of the network, our system has five sets of 90-dimensional embedding matrices; embeddings for source language words, separate embeddings for the target language lemmas, part-of-speech tags and combination of lemmas and part-of-speech tags. In addition we have separate embeddings for source language pronouns aligned with the unknown target pronoun. Context windows are then sequences of indices for these different token-level embeddings, except the aligned source language prohelp predict the pronoun. Other teams relied more on the context, for example UU-Tiedemann (Tiedemann, 2015) used a linear SVM with features from the context of the pronoun. IDIAP (Luong et al., 2015) went on to use a naive-bayes classifier with features from contextual noun-phrases. WHATELLES (Callin et al., 2015) used a neural network approach with features from preceding noun-phrases. It is to be noted that the last year’s task was won by a language model baseline, provided by the organizers. Our system fits the second category of systems, those relying on the context to predict the pronoun. None of the systems participating in the shared task seem to be using explicit sequence classification app"
W16-2353,W15-2516,0,0.0221217,"text, but instead in lemma plus part-of-speech tag format. This is to mimic the representation which many standard machine translation systems produce and to complicate the matter of standard ? 2 Related work This shared task is a spiritual successor to an earlier cross-lingual pronoun prediction shared task (Hardmeier et al., 2015). The systems submitted to the earlier task provide us with a good view of the recent related work on the problem. The earlier task received altogether six system description papers. The organizers identify two main approaches used by the participants. Teams UEDIN (Wetzel et al., 2015) and MALTA (Pham and van der Plas, 2015) explicitly tried to resolve anaphoras in the text and using the information to Both authors contributed equally to this work. 596 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 596–601, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics Figure 2: The architecture of our recurrent neural network system. On the target side the context is read in the left and right direction starting from the replace token, and the replace token itself is not included in the context window."
W16-2353,W16-2345,0,\N,Missing
W16-2913,W09-1401,0,0.108114,"Missing"
W16-2913,P05-1022,0,0.0554004,"aracter encoding from UTF-8 to ASCII as many of the legacy language processing tools are incapable of handling non-ASCII characters. Additionally, all excess meta data is removed, leaving titles, abstracts and full-text contents for further processing. These documents are subsequently split into sentences using GENIA sentence splitter (Sætre et al., 2007) as most linguistic analyses are done on the sentence level. GENIA sentence splitter is trained on biomedical text (GENIA corpus) and has state-of-the-art performance on this domain. The whole data is parsed with the BLLIP constituent parser (Charniak and Johnson, 2005), using a model adapted for the biomedical domain (McClosky, 2010), as provided in the TEES processing pipeline. The distributed tokenization and POS tagging are also produced with the parser pipeline. We chose to use this tool as the performance of the TEES software has been previously evaluated on a large-scale together with this parsing pipeline (Van Landeghem et al., 2013b) and it should be a reliable choice for biomedical relation extraction. Since dependency parsing has become the prevalent approach in modeling syntactic relations, we also provide conversions to the collapsed Stanford de"
W16-2913,de-marneffe-etal-2006-generating,0,0.135531,"Missing"
W16-2913,N10-1004,0,0.0318647,"tools are incapable of handling non-ASCII characters. Additionally, all excess meta data is removed, leaving titles, abstracts and full-text contents for further processing. These documents are subsequently split into sentences using GENIA sentence splitter (Sætre et al., 2007) as most linguistic analyses are done on the sentence level. GENIA sentence splitter is trained on biomedical text (GENIA corpus) and has state-of-the-art performance on this domain. The whole data is parsed with the BLLIP constituent parser (Charniak and Johnson, 2005), using a model adapted for the biomedical domain (McClosky, 2010), as provided in the TEES processing pipeline. The distributed tokenization and POS tagging are also produced with the parser pipeline. We chose to use this tool as the performance of the TEES software has been previously evaluated on a large-scale together with this parsing pipeline (Van Landeghem et al., 2013b) and it should be a reliable choice for biomedical relation extraction. Since dependency parsing has become the prevalent approach in modeling syntactic relations, we also provide conversions to the collapsed Stanford dependency scheme (De Marneffe et al., 2006). The pipeline is run in"
W16-2913,N06-2024,0,0.0106302,"able for these entity types (Leaman et al., 2015; Leaman and Gonzalez, 2008), we have decided to use a single tool, NERsuite3 , for all types. NERsuite is based on conditional random field classifiers as implemented in the CRFsuite software (Okazaki, 2007). Having a single tool for this processing step instead of using the various state-of-the-art tools is critical for the maintainability of the processing pipeline. NERsuite was selected as several biological models are readily available for this software (Kaewphan et al., 2016; Pyysalo and Ananiadou, 2014) and as it supports label weighting (Minkov et al., 2006) unlike many other NER tools. For cell line names we use a publicly available state-of-the-art model (Kaewphan et al., 2016), whereas for the other entity types we train our own models with manually annotated data from GENETAG (Tanabe et al., 2005), CHEMDNER (Krallinger et al., 2015), SPECIES (Pafilis et al., 2013) and NCBI disease (Doˇgan et al., 2014) corpora for GGPs, chemicals, organisms and diseases, respectively. All these corpora are comprised of biomedical articles and should thus reflect well the text types seen in PubMed. All used corpora provide the data divided to training, develop"
W16-2913,W13-1908,1,0.907063,"Missing"
W16-2913,S13-2056,0,0.0778119,"Missing"
W16-2913,W13-2003,1,\N,Missing
W16-3009,N10-1004,0,0.0234799,"o run a basic preprocessing pipeline of tokenization, POS tagging, and parsing, as well as to remove cross-sentence relations. Like our approach, TEES targets the extraction of associations between entities that occur in the same sentence. To support this functionality, it can detect and eliminate relations that cross sentence boundaries in its input. We use this feature of TEES as an initial preprocessing step to remove such relations from the data. To obtain tokens, POS tags and parse graphs, TEES uses the BLLIP parser (Charniak and Johnson, 2005) with the biomedical domain model created by McClosky (2010). The phrase structure trees produced by the parser are further processed with the Stanford conversion tool (de Marneffe et al., 2006) to create dependency graphs. The Stanford system can produce several variants of the Stanford Dependencies (SD) representation. Here, we use the collapsed variant, which is designed to be useful for information extraction and language understanding tasks (de Marneffe and Manning, 2008). Our approach builds on the shortest dependency path between each pair of entities. However, while dependency parse graphs connect words to others in the same sentence, a number"
W16-3009,W13-2003,1,0.577396,"Missing"
W16-3009,W11-1809,0,0.0535061,"Missing"
W16-3009,W13-2001,1,0.868609,"Missing"
W16-3009,H05-1091,0,0.721626,"e there are various ways of converting the shared task annotations into examples for classification, the numbers we report here may differ from those reported by other participating teams. 3.2 Shortest Dependency Path The syntactic structure connecting two entities e1 and e2 in various forms of syntactic analysis is known to contain most of the words relevant to characterizing the relationship R(e1 , e2 ), while excluding less relevant and uninformative words. This observation has served as the basis for many successful relation extraction approaches in both general and biomedical domain NLP (Bunescu and Mooney, 2005; Airola et al., 2008; Nguyen et al., 2009; Chowdhury et al., 2011). The TEES system also heavily relies on the shortest dependency path for defining and ex1 Official evaluation results on the test data are of course comparable to those of other systems: any cross-sentence relations in the test data count against our submission as false negatives. 74 3.3 tracting features (Bj¨orne et al., 2012; Bj¨orne and Salakoski, 2013). Recently, this idea was applied in an LSTM-based relation extraction system by Xu et al. (2015). Since the dependency parse is directed (i.e. the path from e1 to e2 differs"
W16-3009,D09-1143,0,0.0186528,"ed task annotations into examples for classification, the numbers we report here may differ from those reported by other participating teams. 3.2 Shortest Dependency Path The syntactic structure connecting two entities e1 and e2 in various forms of syntactic analysis is known to contain most of the words relevant to characterizing the relationship R(e1 , e2 ), while excluding less relevant and uninformative words. This observation has served as the basis for many successful relation extraction approaches in both general and biomedical domain NLP (Bunescu and Mooney, 2005; Airola et al., 2008; Nguyen et al., 2009; Chowdhury et al., 2011). The TEES system also heavily relies on the shortest dependency path for defining and ex1 Official evaluation results on the test data are of course comparable to those of other systems: any cross-sentence relations in the test data count against our submission as false negatives. 74 3.3 tracting features (Bj¨orne et al., 2012; Bj¨orne and Salakoski, 2013). Recently, this idea was applied in an LSTM-based relation extraction system by Xu et al. (2015). Since the dependency parse is directed (i.e. the path from e1 to e2 differs from that from e2 to e1 ), they separate"
W16-3009,P05-1022,0,0.0164901,"eloped by members of the TurkuNLP group (Bj¨orne and Salakoski, 2013), to run a basic preprocessing pipeline of tokenization, POS tagging, and parsing, as well as to remove cross-sentence relations. Like our approach, TEES targets the extraction of associations between entities that occur in the same sentence. To support this functionality, it can detect and eliminate relations that cross sentence boundaries in its input. We use this feature of TEES as an initial preprocessing step to remove such relations from the data. To obtain tokens, POS tags and parse graphs, TEES uses the BLLIP parser (Charniak and Johnson, 2005) with the biomedical domain model created by McClosky (2010). The phrase structure trees produced by the parser are further processed with the Stanford conversion tool (de Marneffe et al., 2006) to create dependency graphs. The Stanford system can produce several variants of the Stanford Dependencies (SD) representation. Here, we use the collapsed variant, which is designed to be useful for information extraction and language understanding tasks (de Marneffe and Manning, 2008). Our approach builds on the shortest dependency path between each pair of entities. However, while dependency parse gr"
W16-3009,W11-0216,0,0.0192759,"nto examples for classification, the numbers we report here may differ from those reported by other participating teams. 3.2 Shortest Dependency Path The syntactic structure connecting two entities e1 and e2 in various forms of syntactic analysis is known to contain most of the words relevant to characterizing the relationship R(e1 , e2 ), while excluding less relevant and uninformative words. This observation has served as the basis for many successful relation extraction approaches in both general and biomedical domain NLP (Bunescu and Mooney, 2005; Airola et al., 2008; Nguyen et al., 2009; Chowdhury et al., 2011). The TEES system also heavily relies on the shortest dependency path for defining and ex1 Official evaluation results on the test data are of course comparable to those of other systems: any cross-sentence relations in the test data count against our submission as false negatives. 74 3.3 tracting features (Bj¨orne et al., 2012; Bj¨orne and Salakoski, 2013). Recently, this idea was applied in an LSTM-based relation extraction system by Xu et al. (2015). Since the dependency parse is directed (i.e. the path from e1 to e2 differs from that from e2 to e1 ), they separate the shortest dependency p"
W16-3009,W11-1815,0,0.399828,"Missing"
W16-3009,W08-1301,0,0.0397729,"Missing"
W16-3009,de-marneffe-etal-2006-generating,0,0.0640478,"Missing"
W16-3009,D15-1206,0,0.077098,"ion extraction approaches in both general and biomedical domain NLP (Bunescu and Mooney, 2005; Airola et al., 2008; Nguyen et al., 2009; Chowdhury et al., 2011). The TEES system also heavily relies on the shortest dependency path for defining and ex1 Official evaluation results on the test data are of course comparable to those of other systems: any cross-sentence relations in the test data count against our submission as false negatives. 74 3.3 tracting features (Bj¨orne et al., 2012; Bj¨orne and Salakoski, 2013). Recently, this idea was applied in an LSTM-based relation extraction system by Xu et al. (2015). Since the dependency parse is directed (i.e. the path from e1 to e2 differs from that from e2 to e1 ), they separate the shortest dependency path into two sub-paths, each from an entity to the common ancestor of the two entities, generate features along the two sub-paths, and feed them into different LSTM networks, to process the information in a direction sensitive manner. To avoid doubling the number of LSTM chains (and hence the number of weights), we convert the dependency parse to an undirected graph, find the shortest path between the two entities (BACTERIA and H ABITAT/G EOGRAPHICAL),"
W17-0218,J03-3001,0,0.285008,"Missing"
W17-0218,W15-2124,1,0.90854,"Missing"
W17-0218,sharoff-etal-2010-web,0,0.0431372,"Missing"
W17-0218,P09-1076,0,0.0818757,"Missing"
W17-0233,N15-3011,1,0.922247,"genitive case, one can query: (L=cat |L=dog) & !Case=Gen. Introduction Huge text collections crawled from the Internet have become a popular resource for natural language processing and linguistic studies. Recently, massive corpora with automatically analyzed full syntactic structure became available for 45 languages (Ginter et al., 2017). To efficiently find and access specific types of sentences or syntactic structures from corpora with billions of tokens, powerful search systems are needed for both text and tree based queries, and combinations thereof. The SETS dependency tree search tool (Luotolahti et al., 2015) is developed for efficient and scalable tree search in dependency treebanks and parsebanks, supporting complex searches on words, lemmas, detailed morphological analyses and dependency graphs. It is implemented using efficient data indexing and turning search expressions into compiled code. In this paper we present the Turku dep search tool, an improved version of SETS. In addition to changing the name, our main contributions are: 1) major speed-up and reduction of stored index size by changing backend from SQLite to Solr1 2 http://www.lmdb.tech/doc/ 3 http://universaldependencies.org/ 1 http"
W17-0233,E12-2021,0,0.128414,"Missing"
W17-0510,W17-0249,1,0.73727,"study Filip Ginter Turku NLP Group Department of FT University of Turku filip.ginter@utu.fi of nineteenth-century US newspapers by Ryan Cordell, David A. Smith and their research group (Cordell, 2015; Smith et al., 2015). However, in contrast to the US press, the nineteenth- and early twentieth-century Finnish newspapers were typically printed in the Fraktur typeface, which (together with other possible sources of noise) poses unusual difficulties for Optical Character Recognition (Kettunen, 2016). To solve this problem, we have developed a novel text reuse detection solution based on BLAST (Vesanto et al., 2017) that is accurate and resistant to OCR mistakes and other noise, making the text circulation and virality of newspaper publicity in Finland a feasible research question. 2 Detecting Text Reuse In the nineteenth century, contemporaries saw newspapers as reflections of modern culture. Many phenomena were amplified by the increasing power of the press, including urbanization, consumerism, and business life. The changes in transport technology led to more efficient distribution of information. Before 1880s, there was no copyright agreement to regulate the free copying of texts, which became a dist"
W17-2310,P16-1072,0,0.0202084,"ys traverse the SDP by starting the path from the BACTERIA entity mention to the H ABITAT/G EOGRAPHICAL, regardless of the order of their occurrence in the sentence. Evaluation against the development set showed that this approach leads to better generalization in comparison with simply traversing the path from the first occurring entity mention to the second (with/without considering the direction of the edges). Table 1: BB3-event data statistics. which connects the two candidate entities in the syntactic parse graph. Many successful relation extraction systems have been built utilizing SDP (Cai et al., 2016; Mehryary et al., 2016; Xu et al., 2015; Bj¨orne and Salakoski, 2013; Bj¨orne et al., 2012; Bunescu and Mooney, 2005) since it is known to contain most of the relevant words for expressing the relation between the two entities while excluding less relevant and uninformative words. Since this approach focuses on a single sentence parse graph at a time, it is unable to detect plausible cross-sentence relations, i.e, the cases in which the two candidate entities belong to different sentences. As discussed by Kim et al. (2011), detecting such relations is a major challenge for relation extraction"
W17-2310,P05-1022,0,0.0688858,"coded entity-type vectors are concatenated. The first entity-type vector represents the type of the first occurring entity in the sentence (BACTERIA, H ABITAT or G EOGRAPH ICAL), and the other is used for the second one. The resulting vector is then forwarded into a fully connected hidden layer and finally, the hidden layer connects to a single-node binary classification layer. Preprocessing For preprocessing, we use the preprocessing pipeline of the TEES system (Bj¨orne and Salakoski, 2013) which automates tokenization, part-of-speech tagging and sentence parsing. TEES runs the BLLIP parser (Charniak and Johnson, 2005) with the biomedical domain model created by McClosky (2010). The resulting phrase structure trees are then converted to dependency graphs (nonCollapsed variant of Stanford Dependency) using the Stanford conversion tool (version 2.0.1) (de Marneffe et al., 2006). 2.3.2 For the word features, we use a vector space model with 200-dimensional word embeddings pre-trained by Pyysalo et al. (2013). These are fine-tuned during the training while the POS-tag and dependency type embeddings are learned from scratch after being randomly initialized. Relation extraction system architecture Based on experi"
W17-2310,de-marneffe-etal-2006-generating,0,0.1383,"Missing"
W17-2310,P16-1096,0,0.0271373,"on 3.2). Both of these expansion methods have similar intentions as the preprocessing steps utilized by the winning system in BB3 (BOUN) by Tiftikci et al. (2016), but our system uses more relaxed criteria for finding the full forms and should thus result in better recall at the expense of precision. to NCBI Taxonomy and OntoBiotope identifiers respectively. This task is commonly known as named entity normalization or entity linking and various approaches ranging from Levenshtein edit distances to recurrent neural networks have been suggested as the plausible solutions (Tiftikci et al., 2016; Limsopatham and Collier, 2016). Our categorization method is based on the common approach of TFIDF weighted sparse vector space representations (Salton and Buckley, 1988; Leaman et al., 2013; Hakala, 2015), i.e. the problem is seen as an information retrieval task where each concept name in the ontology is considered a document and the IDF weights are based on these names. Consequently, each concept name and each entity mention is represented with a TFIDF weighted vector and the concept with the highest cosine similarity is assigned for a given entity. Whereas these representations are commonly formed in a bag-of-words fas"
W17-2310,W16-3002,0,0.0945246,"Missing"
W17-2310,N10-1004,0,0.0163206,"represents the type of the first occurring entity in the sentence (BACTERIA, H ABITAT or G EOGRAPH ICAL), and the other is used for the second one. The resulting vector is then forwarded into a fully connected hidden layer and finally, the hidden layer connects to a single-node binary classification layer. Preprocessing For preprocessing, we use the preprocessing pipeline of the TEES system (Bj¨orne and Salakoski, 2013) which automates tokenization, part-of-speech tagging and sentence parsing. TEES runs the BLLIP parser (Charniak and Johnson, 2005) with the biomedical domain model created by McClosky (2010). The resulting phrase structure trees are then converted to dependency graphs (nonCollapsed variant of Stanford Dependency) using the Stanford conversion tool (version 2.0.1) (de Marneffe et al., 2006). 2.3.2 For the word features, we use a vector space model with 200-dimensional word embeddings pre-trained by Pyysalo et al. (2013). These are fine-tuned during the training while the POS-tag and dependency type embeddings are learned from scratch after being randomly initialized. Relation extraction system architecture Based on experiments on the development set, we have set the dimensionality"
W17-2310,P05-1045,0,0.0711618,"couver, Canada, August 4, 2017. 2017 Association for Computational Linguistics 2 Method 2.1 strings under the semantic type geographical area from UMLS database (version 2016AA) (Bodenreider, 2004). All dictionaries prepared in this step are directly provided to NERsuite through the dictionary-tagging module without any normalization. The tagging provides additional features describing whether the tokens are present in some semantic categories, such as bacteria names or geographical places. For G EOGRAPHICAL model, we also add token-level tagging results for location from Stanford NER (SNER) (Finkel et al., 2005) as binary values to NERsuite; 1 and 0 for location and non-location, respectively. Although utilizing dictionary features is beneficial for NER, strict string matching tends to lead to low coverage, an issue which is also common in the categorization task. To remedy this problem, we also generate fuzzy matching features based on our categorization system (see Section 2.2) by measuring the maximum similarity of each token against the NCBI Taxonomy and OntoBiotope ontologies for BACTERIA and H ABITAT respectively. Thus, instead of a binary feature denoting whether a token is present in the onto"
W17-2310,W16-3009,1,0.839766,"P by starting the path from the BACTERIA entity mention to the H ABITAT/G EOGRAPHICAL, regardless of the order of their occurrence in the sentence. Evaluation against the development set showed that this approach leads to better generalization in comparison with simply traversing the path from the first occurring entity mention to the second (with/without considering the direction of the edges). Table 1: BB3-event data statistics. which connects the two candidate entities in the syntactic parse graph. Many successful relation extraction systems have been built utilizing SDP (Cai et al., 2016; Mehryary et al., 2016; Xu et al., 2015; Bj¨orne and Salakoski, 2013; Bj¨orne et al., 2012; Bunescu and Mooney, 2005) since it is known to contain most of the relevant words for expressing the relation between the two entities while excluding less relevant and uninformative words. Since this approach focuses on a single sentence parse graph at a time, it is unable to detect plausible cross-sentence relations, i.e, the cases in which the two candidate entities belong to different sentences. As discussed by Kim et al. (2011), detecting such relations is a major challenge for relation extraction systems. We simply exc"
W17-2310,W16-3008,0,0.171275,"able 6 shows, using previous BBST data for training the NER leads to 3pp increase in F-score of (BACTERIA,G EOGRAPHICAL) relations on the development set and about 11pp for the test set, probably due to the drastically increased performance for G EOGRAPHICAL entity detection. Unfortunately, since there are much less (BACTERIA,G EOGRAPHICAL) relations than (BACTERIA,H ABITAT) relations in the data, our approach increases the overall F-score only by 1pp for the test set. Table 7 compares the performance of our endto-end system with the winning team in the BB3-event+ner task (LIMSI, developed by Grouin (2016)). As it can be seen in the table, our system outperforms the winning team by 19pp in Fscore, achieving the new state-of-the-art score for the task. Even if we solely rely on BB3 data for the NER system, the improvement is 18pp in F-score. We emphasize that no other data than BB3 is used for training/optimization of our relation extraction system in any way. Teams LIMSI Our system F-score 0.192 0.381 Recall 0.191 0.292 Precision 0.193 0.548 SER 1.558 0.891 Table 7: Official evaluation results for BB3event+ner test data of our system compared to LIMSI, the winning team in the Shared Task. 4 Con"
W17-2310,W12-4304,0,0.0316335,"designed for parsing PubMed documents (Pyysalo et al., 2013) 1 . Next we split documents into sentences using the Genia Sentence Splitter (Sætre et al., 2007) and the sentences are subsequently tokenized and part-ofspeech tagged using the tokenization and POStagging modules in NERsuite 2 , respectively. To detect the entity mentions we use NERsuite, a named entity recognition toolkit, as it is relatively easy to train on new corpora, yet supports adding novel user-defined features. In biomedical NER, NERsuite has been a versatile tool achieving excellent performance for various entity types (Ohta et al., 2012; Kaewphan et al., 2014, 2016), however, it is not capable of dealing with overlapping entities. Therefore, we only use the longest spans of overlapping annotated entities as our training data, ignoring embedded entities which are substrings of the longest spans. In biomedical NER, domain knowledge such as controlled vocabularies has been crucial for achieving high performance. In this work we prepare 3 dictionaries, specific for each entity type. For BACTERIA, we compile a dictionary of names exclusively from the NCBI Taxonomy database3 by including all names under bacteria superkingdom (NCBI"
W17-2310,S15-2064,1,0.738953,"axed criteria for finding the full forms and should thus result in better recall at the expense of precision. to NCBI Taxonomy and OntoBiotope identifiers respectively. This task is commonly known as named entity normalization or entity linking and various approaches ranging from Levenshtein edit distances to recurrent neural networks have been suggested as the plausible solutions (Tiftikci et al., 2016; Limsopatham and Collier, 2016). Our categorization method is based on the common approach of TFIDF weighted sparse vector space representations (Salton and Buckley, 1988; Leaman et al., 2013; Hakala, 2015), i.e. the problem is seen as an information retrieval task where each concept name in the ontology is considered a document and the IDF weights are based on these names. Consequently, each concept name and each entity mention is represented with a TFIDF weighted vector and the concept with the highest cosine similarity is assigned for a given entity. Whereas these representations are commonly formed in a bag-of-words fashion, in our experiments using character-level ngrams resulted in better outcome. In the final system we use ngrams of length 1, 2 and 3 characters. These ngram lengths produc"
W17-2310,W16-2913,1,0.834597,"-cat+ner, we use our own implementation by calculating the F-score using exact string matching criteria as our main scoring metric. In this study, we consider BB3event+ner as our primary subtask and thus all hyper-parameters in model selection are optimized against F-score instead of SER. Named entity detection Detecting the BB3 H ABITAT, BACTERIA and G E OGRAPHICAL mentions is a standard named entity recognition task, evaluated based on the correctness of the type and character offsets of the discovered text spans. In our NER pipeline, all documents are preprocessed following the approach of Hakala et al. (2016). In brief, we first convert all documents and annotation files from UTF8 to ASCII encoding using a modified version of publicly available tool designed for parsing PubMed documents (Pyysalo et al., 2013) 1 . Next we split documents into sentences using the Genia Sentence Splitter (Sætre et al., 2007) and the sentences are subsequently tokenized and part-ofspeech tagged using the tokenization and POStagging modules in NERsuite 2 , respectively. To detect the entity mentions we use NERsuite, a named entity recognition toolkit, as it is relatively easy to train on new corpora, yet supports addin"
W17-2310,S14-2143,1,0.86257,"nary features is beneficial for NER, strict string matching tends to lead to low coverage, an issue which is also common in the categorization task. To remedy this problem, we also generate fuzzy matching features based on our categorization system (see Section 2.2) by measuring the maximum similarity of each token against the NCBI Taxonomy and OntoBiotope ontologies for BACTERIA and H ABITAT respectively. Thus, instead of a binary feature denoting whether a token is present in the ontology or not, a similarity score ranging from 0 to 1 is assigned for each token. This approach is similar to (Kaewphan et al., 2014), but instead of using word embedding similarities, our fuzzy matching relies on character ngrams. We do not use these features for the G EO GRAPHICAL entities, which are not categorized by our system. In the official BB3 evaluation, NER is jointly evaluated with either categorization or event extraction system. In BB3-cat+ner task, SER (Slot Error Rate) is used as the main scoring metric, whereas in BB3-event+ner, participating teams are ranked based on F-score of extracted relations. Due to the lack of an official evaluation on NER for all entities in BB3-event+ner and for G EOGRAPHICAL in B"
W17-2310,W16-3007,0,0.199971,"Missing"
W17-2310,D15-1206,0,0.0261839,"from the BACTERIA entity mention to the H ABITAT/G EOGRAPHICAL, regardless of the order of their occurrence in the sentence. Evaluation against the development set showed that this approach leads to better generalization in comparison with simply traversing the path from the first occurring entity mention to the second (with/without considering the direction of the edges). Table 1: BB3-event data statistics. which connects the two candidate entities in the syntactic parse graph. Many successful relation extraction systems have been built utilizing SDP (Cai et al., 2016; Mehryary et al., 2016; Xu et al., 2015; Bj¨orne and Salakoski, 2013; Bj¨orne et al., 2012; Bunescu and Mooney, 2005) since it is known to contain most of the relevant words for expressing the relation between the two entities while excluding less relevant and uninformative words. Since this approach focuses on a single sentence parse graph at a time, it is unable to detect plausible cross-sentence relations, i.e, the cases in which the two candidate entities belong to different sentences. As discussed by Kim et al. (2011), detecting such relations is a major challenge for relation extraction systems. We simply exclude any cross-se"
W17-2310,H05-1091,0,\N,Missing
W17-2310,W13-2024,0,\N,Missing
W17-4808,P14-2050,0,0.0607628,"o word alignments, we use a union of skip-gram contexts on the source side and the target side. Therefore, in this mixed context method, for every source word, word2vec is used to predict the target sentence pronouns, the source sentence context words, and the target sentence context lemmas. 1 As the training data includes word-level alignments between the source and target language, we are able to identify the source language counterpart for the missing pronoun. The word embeddings are trained using 64 Figure 2: GRU architecture word2vec2 and word2vecf3 softwares by Mikolov et al. (2013) and Levy and Goldberg (2014) respectively, the latter supporting arbitrary contexts for word2vec style embedding learning. All embeddings are trained using the full training data, i.e. also sentences without training examples for the pronoun prediction task and no other data is used. All word embeddings use 90-dimensional vectors, and are trained using the skip-gram architecture with negative sampling and 10 training iterations. 2.2 portional to the frequencies of the classes, so that misclassifying a rare class is a more serious error than misclassifying a common class. This scheme produces outputs with a higher emphasi"
W17-4808,W17-4801,0,0.0415306,"Missing"
W17-4808,W16-2353,1,0.508611,"ace token, and the task is then to predict the missing pronoun. Furthermore, the target side language is not given as running text, but instead in lemma plus part-of-speech tag format, which makes even harder to model the target language. An example of an English-French sentence pair is given in Figure 1. In this paper we describe the pronoun prediction system of the Turku NLP Group. Our system extends the last year’s deep recurrent neural networks based system with word-level embeddings, two layers of Gated Recurrent Units (GRUs) and a softmax layer on top of it to make the final prediction (Luotolahti et al., 2016). This year 2 System Architecture As in the previous year, our system is a deep neural network model reading context from both source and target side sentences around the focus pronoun. The most important change are the tokenlevel embeddings, which are now pre-trained before training the full system. The system architecture itself is improved relative to the last year system by filtering from the data aligned pronouns that are too long, as these are alignment errors rather than actual pronouns. We also increase the size of the last dense neural network layer from 320 to 720 units, to address a"
W17-6511,N09-1003,0,0.0283734,"Missing"
W17-6511,Q13-1034,1,0.898996,"Missing"
W17-6511,P14-2050,0,0.0388234,"distributional semantics approaches (Bengio et al., 2003; Collobert et al., 2011). The methods introduced by Mikolov et al. (2013a) and implemented in their popular word2vec tool have been proven both effective and a good foundation for further exploration. In addition to representing word contexts as sliding windows of words in linear sequence, recent work has included efforts of building the word vectors using dependency-based approaches (Levy and Gold2.2 Dependency-based word embeddings Observing that the SGNS model is not inherently restricted to working with contexts consisting of words, Levy and Goldberg (2014) extended the model to work with arbitrary contexts, focusing 1 https://code.google.com/p/word2vec/ 83 Proceedings of the Fourth International Conference on Dependency Linguistics (Depling 2017), pages 83-91, Pisa, Italy, September 18-20 2017 nsubj cop det amod in particular on dependency-based contexts consisting of combinations of a neigbouring word in the dependency graph and its dependency relation to the target word (e.g. scientist/nsubj). Compared to embeddings based on linear contexts of words, they showed dependency-based embeddings to emphasize functional over topical similarity and t"
W17-6511,W13-3512,0,0.122728,"Missing"
W17-6511,W15-2124,1,0.871923,"Missing"
W17-6511,P12-1015,0,0.0206098,"Missing"
W17-6511,W16-2501,1,0.822797,"for training embeddings. Of the 64 treebanks in the release, 9 do not fulfill these criteria (French-ParTUT, GalicianTreeGal, Irish, Kazakh, Latin, Slovenian-SST, Ukrainian and Uyghur do not have development data, Gothic does not have raw data) and are not included in the evaluation. Models are trained on the training section of a treebank and tested on the development section.7 Word vectors are frequently evaluated by assessing how well their distance correlates with human judgments of word similarity. Although these intrinsic evaluations have known issues (see e.g. Batchkarov et al. (2016), Chiu et al. (2016), Faruqui et al. (2016)) and we agree with the criticism that they are frequently poor indicators of the merits of representations, we include this common form of intrinsic evaluation here for reference purposes. We provide results using a comprehensive collection of English datasets annotated for word similarity and relatedness. Specifically, we used the evaluation service introduced by Faruqui and Dyer (2014) to evaluate on the 13 datasets available on the service3 at the time of this writing. The datasets are summarized below in Table 3. 3.4 4 Results We next informally illustrate the chara"
W17-6511,P14-5004,0,0.0136124,"evaluated by assessing how well their distance correlates with human judgments of word similarity. Although these intrinsic evaluations have known issues (see e.g. Batchkarov et al. (2016), Chiu et al. (2016), Faruqui et al. (2016)) and we agree with the criticism that they are frequently poor indicators of the merits of representations, we include this common form of intrinsic evaluation here for reference purposes. We provide results using a comprehensive collection of English datasets annotated for word similarity and relatedness. Specifically, we used the evaluation service introduced by Faruqui and Dyer (2014) to evaluate on the 13 datasets available on the service3 at the time of this writing. The datasets are summarized below in Table 3. 3.4 4 Results We next informally illustrate the characteristics of the English word vectors using nearest neighbours and give the intrinsic evaluation results for these vectors before presenting the results of our primary multilingual parsing experiments. Extrinsic evaluation Our primary evaluation is based on dependency parsing, where we evaluate parsing accuracy using different pre-trained word embeddings during parser training. We use the UDPipe pipeline4 for"
W17-6511,D13-1032,0,0.0575446,"Missing"
W17-6511,W16-2506,0,0.0122627,"ngs. Of the 64 treebanks in the release, 9 do not fulfill these criteria (French-ParTUT, GalicianTreeGal, Irish, Kazakh, Latin, Slovenian-SST, Ukrainian and Uyghur do not have development data, Gothic does not have raw data) and are not included in the evaluation. Models are trained on the training section of a treebank and tested on the development section.7 Word vectors are frequently evaluated by assessing how well their distance correlates with human judgments of word similarity. Although these intrinsic evaluations have known issues (see e.g. Batchkarov et al. (2016), Chiu et al. (2016), Faruqui et al. (2016)) and we agree with the criticism that they are frequently poor indicators of the merits of representations, we include this common form of intrinsic evaluation here for reference purposes. We provide results using a comprehensive collection of English datasets annotated for word similarity and relatedness. Specifically, we used the evaluation service introduced by Faruqui and Dyer (2014) to evaluate on the 13 datasets available on the service3 at the time of this writing. The datasets are summarized below in Table 3. 3.4 4 Results We next informally illustrate the characteristics of the Engli"
W17-6511,L16-1262,1,0.854686,"Missing"
W17-6511,D16-1235,0,0.024257,"Missing"
W17-6511,W15-1821,1,0.8326,"Missing"
W17-6511,L16-1680,0,0.0267194,"Missing"
W17-6511,P14-5003,0,0.0637101,"Missing"
W17-6511,D14-1034,0,\N,Missing
W17-6511,W16-2502,0,\N,Missing
W17-6514,W11-3405,0,0.61958,"), van Halteren (2000), Dickinson & Meurers (2003a)), most approaches to assess the consistency of dependency annotations are based on heuristic patterns (i.a., De Smedt et al. (2016) who focus on multi-word 1 http://universaldependencies.org expressions in the UD v1 corpora (Nivre et al., 2016)). There exists a variety of querying tools allowing to search dependency treebanks, given such heuristic patterns (i.a., SETS (Luotolahti et al., 2015); Grew (Bonfante et al., 2011); PML ˇ ep´anek and Pajas, 2010); ICARUS TreeQuery (Stˇ (G¨artner et al., 2013)). Statistical methods, such as the one of Ambati et al. (2011), are supplemented with hand-written rules. While approaches based on heuristic patterns work extremely well to look for given constructions (e.g., clefts) or check that specific guidelines are taken into account (e.g., auxiliary dependencies should not form a chain in UD), such approaches are limited to finding what has been defined a priori. In this paper, we adapt the method proposed by Boyd et al. (2008) to flag potential dependency annotation inconsistencies, and evaluate it on three of the UD v2 corpora (English, French and Finnish). The original Boyd et al. method finds pairs of words i"
W17-6514,W11-0108,0,0.0180377,"rucial importance. While there has been a fair amount of work to automatically detect part-of-speech inconsistent annotations (i.a., Eskin (2000), van Halteren (2000), Dickinson & Meurers (2003a)), most approaches to assess the consistency of dependency annotations are based on heuristic patterns (i.a., De Smedt et al. (2016) who focus on multi-word 1 http://universaldependencies.org expressions in the UD v1 corpora (Nivre et al., 2016)). There exists a variety of querying tools allowing to search dependency treebanks, given such heuristic patterns (i.a., SETS (Luotolahti et al., 2015); Grew (Bonfante et al., 2011); PML ˇ ep´anek and Pajas, 2010); ICARUS TreeQuery (Stˇ (G¨artner et al., 2013)). Statistical methods, such as the one of Ambati et al. (2011), are supplemented with hand-written rules. While approaches based on heuristic patterns work extremely well to look for given constructions (e.g., clefts) or check that specific guidelines are taken into account (e.g., auxiliary dependencies should not form a chain in UD), such approaches are limited to finding what has been defined a priori. In this paper, we adapt the method proposed by Boyd et al. (2008) to flag potential dependency annotation incons"
W17-6514,E03-1068,0,0.538508,"linked differently. Boyd et al. also experimented with a “dependency context heuristic” requiring the governors of the dependency pairs to have the same incoming dependency relation. They also considered the case of pairs of words which are linked by a dependency relation in some instances and not linked by any relation in other instances, but required for those cases that the internal context between the two words be exactly the same. advmod cop nummod (1) a. Here ’s two examples : nsubj cop Boyd et al. (2008) extend, to dependency representation, the concept of variation nuclei developed by Dickinson and Meurers (2003b; 2005) for identifying inconsistent annotations in phrasestructure trees. Variation nuclei are elements which occur multiple times in a corpus with varying annotation. For phrase-structure trees, a variation nucleus is any n-gram for which bracketing or labeling varies, with one shared word of context on each side of the n-gram. Figure 1, from Boyd et al. (2008), shows an example of a 5-gram, its biggest jolt last month, which receives two different analyses in the Penn TreeBank. For dependency representation, the basic elements are dependencies, i.e. pairs of words linked by a labeled depen"
W17-6514,P05-1040,0,0.66456,"Missing"
W17-6514,A00-2020,0,0.132573,"are followed, and crucially that similar phenomena do receive a consistent analysis within and across corpora. Given the recent success of the Universal Dependencies (UD) project1 which aims at building cross-linguistically consistent treebanks for many languages and the rapid creation of 74 corpora for 51 languages supposedly following the UD scheme, investigating the quality of the dependency annotations and improving their consistency is, more than ever, of crucial importance. While there has been a fair amount of work to automatically detect part-of-speech inconsistent annotations (i.a., Eskin (2000), van Halteren (2000), Dickinson & Meurers (2003a)), most approaches to assess the consistency of dependency annotations are based on heuristic patterns (i.a., De Smedt et al. (2016) who focus on multi-word 1 http://universaldependencies.org expressions in the UD v1 corpora (Nivre et al., 2016)). There exists a variety of querying tools allowing to search dependency treebanks, given such heuristic patterns (i.a., SETS (Luotolahti et al., 2015); Grew (Bonfante et al., 2011); PML ˇ ep´anek and Pajas, 2010); ICARUS TreeQuery (Stˇ (G¨artner et al., 2013)). Statistical methods, such as the one of A"
W17-6514,W04-1905,0,0.0982428,"Missing"
W17-6514,P13-4010,0,0.0655167,"Missing"
W17-6514,N15-3011,1,0.839952,"istency is, more than ever, of crucial importance. While there has been a fair amount of work to automatically detect part-of-speech inconsistent annotations (i.a., Eskin (2000), van Halteren (2000), Dickinson & Meurers (2003a)), most approaches to assess the consistency of dependency annotations are based on heuristic patterns (i.a., De Smedt et al. (2016) who focus on multi-word 1 http://universaldependencies.org expressions in the UD v1 corpora (Nivre et al., 2016)). There exists a variety of querying tools allowing to search dependency treebanks, given such heuristic patterns (i.a., SETS (Luotolahti et al., 2015); Grew (Bonfante et al., 2011); PML ˇ ep´anek and Pajas, 2010); ICARUS TreeQuery (Stˇ (G¨artner et al., 2013)). Statistical methods, such as the one of Ambati et al. (2011), are supplemented with hand-written rules. While approaches based on heuristic patterns work extremely well to look for given constructions (e.g., clefts) or check that specific guidelines are taken into account (e.g., auxiliary dependencies should not form a chain in UD), such approaches are limited to finding what has been defined a priori. In this paper, we adapt the method proposed by Boyd et al. (2008) to flag potentia"
W17-6514,nivre-etal-2006-talbanken05,0,0.0580503,"Missing"
W17-6514,L16-1262,1,0.901229,"Missing"
W17-6514,stepanek-pajas-2010-querying,0,0.0586947,"Missing"
W17-6514,L16-1680,0,0.0410951,"Missing"
W17-6514,E14-2015,0,0.0383523,"Missing"
W17-6514,W00-1907,0,\N,Missing
W18-5611,D15-1167,0,0.0342895,"4 Proceedings of the 9th International Workshop on Health Text Mining and Information Analysis (LOUHI 2018), pages 94–100 c Brussels, Belgium, October 31, 2018. 2018 Association for Computational Linguistics are here seen as separate from the section headings used by the clinicians when writing, thus the section headings are considered as input to the classifier along with the free text. As classifiers they use two variations of Bayesian networks. Deep learning methods based on artificial neural networks (ANNs) are currently representing state of the art in many NLP tasks (Zhang et al., 2015; Tang et al., 2015), including text classification, relation extraction and translation. In the presented experiment/prototype system we use the popular long short-term memory (LSTM) recurrent neural network architecture (Hochreiter and Schmidhuber, 1997; Gers et al., 2000) for conducting the text classification. In the data set used here there are 676 unique headings to choose from by the classifier. nursing narratives according to the current care classification standard. A central component is a text classification model based on a long shortterm memory (LSTM) recurrent neural network architecture (Hochreiter"
W18-5611,P82-1020,0,0.78685,"Missing"
W18-6006,K17-3002,0,0.0357898,"ctions present in the treebanks (Droganova and Zeman, 2017), while also covering different modern languages and providing variation. Decisions are based on the work by Droganova and Zeman (2017) who collected statistics on elliptical constructions that are explicitly marked with orphan relation within the UD treebanks. Relatively high number of elliptical constructions within chosen treebanks is the property of the treebanks rather than the languages. 2.2 In our experiments the main parser used in final experiments as well as labeling the crawl data, is the neural graph-based Stanford parser (Dozat et al., 2017), the winning and state-of-the-art system from the CoNLL-17 Shared Task (Zeman et al., 2017). The secondary parser for labeling the crawl data is UDPipe, a neural transition-based parser, as these parses are already provided together with the crawl data. Both of these parsers include their own part-of-speech tagger, which is trained together (but not jointly) with the dependency parser in all our experiments. In the final self-training web crawl datasets we then keep only deduplicated sentences with identical partof-speech and dependency analyses. All results reported in this paper are measure"
W18-6006,W17-0406,1,0.364789,"2.0. This UD release was used in the CoNLL-17 Shared Task on Multilingual Parsing from Raw Text to Universal Dependencies (Zeman et al., 2017), giving us a point of comparison to the state-of-the-art. For UD Russian-SynTagRus, we use UD release 2.1, which has a considerably improved annotation of elliptic sentences. For English, which has only a few elliptical sentences in the original treebank, we also utilize in testing a set of elliptical sentences gathered by Schuster et al. (2018). This selection of data strives to maximize the amount of elliptical constructions present in the treebanks (Droganova and Zeman, 2017), while also covering different modern languages and providing variation. Decisions are based on the work by Droganova and Zeman (2017) who collected statistics on elliptical constructions that are explicitly marked with orphan relation within the UD treebanks. Relatively high number of elliptical constructions within chosen treebanks is the property of the treebanks rather than the languages. 2.2 In our experiments the main parser used in final experiments as well as labeling the crawl data, is the neural graph-based Stanford parser (Dozat et al., 2017), the winning and state-of-the-art syste"
W18-6006,L18-1290,1,0.849997,"long coordinated item lists where the ratio drops much lower than average. We of course take into account that a relation type can naturally occur more than once in a sentence, and that it is not ideal to force the ratio close to 1.0. However, as the sampling method tries to mimic the distribution from the original treebank, it should to pick the correct variance while discarding the extremes. Artificial treebanks on elliptical constructions For specifically experimenting on elliptical constructions, we additionally include data from the semi-automatically constructed artificial treebanks by Droganova et al. (2018). These treebanks simulate gapping by removing words in particular coordination constructions, providing data for experimenting with the otherwise very rare construction. For English and Finnish the given datasets are manually curated for grammaticality and fluency, whereas for Czech the quality relies on the rules developed for the process. For Russian and Slovak, which are not part of the original artificial treebank release, we create automatically constructed artificial datasets by running the pipeline developed for the Czech language. Size of the artificial data is shown in Table 1. Czech"
W18-6006,Q17-1031,0,0.0655154,"Department of Future Technologies {droganova,zeman}@ufal.mff.cuni.cz {figint,jmnybl}@utu.fi Abstract of its missing parent, and connecting all remaining core arguments to that promoted one with the orphan relation (see Figure 1). Therefore the dependency parser must learn to predict relations between words that should not usually be connected. Gapping has been studied extensively in theoretical works (Johnson, 2009, 2014; Lakoff and Ross, 1970; Sag, 1976). However, it received almost no attention in NLP works, neither concerned with parsing nor with corpora creation. Among the recent papers, Kummerfeld and Klein (2017) proposed a one-endpoint-crossing graph parser able to recover a range of null elements and trace types, and Schuster (Schuster et al., 2018) proposed two methods to recover elided predicates in sentences with gapping. The aforementioned lack of corpora that would pay attention to gapping, as well as natural relative rarity of gapping, leads to its underrepresentation in training corpora: they do not provide enough examples for the parser to learn gapping. Therefore we investigate methods of enriching the training data with new material from large raw corpora. The present work consist of two p"
W18-6006,P14-1043,0,0.019268,"self-training and tri-training techniques. In selftraining, the labeled training data (L) is iteratively enriched with unlabeled data (U ) automatically labeled with the same learning system (L = L+Ul ), whereas in tri-training (Zhou and Li, 2005) there are three different learning systems, A, B and C, and the labeled data for the system A is enriched with instances from U on which the two other systems agree, therefore La = L + (Ub ∩ Uc ). Different variations of these methods have been successfully applied in dependency parsing, for example (McClosky et al., 2006; Søgaard and Rishøj, 2010; Li et al., 2014; Weiss et al., 2015). In this work we use two parsers (A and B) to process the unlabeled crawl data, and then the sentences where these two parsers fully agree are used to enrich the training data for the system A, i.e. La = L + (Ua ∩ Ub ). Therefore the method can be seen as a form of expanded self-training or limited tri-training. A similar technique is successfully used for example by Sagae and Tsujii (2007) in parser domain adaptation and Bj¨orkelund et al. (2014) in general parsing. lection (Nivre et al., 2016). We experiment with the following treebanks: UD Czech, UD English, UD Russian"
W18-6006,N06-1020,0,0.148383,"e apply a method that stands between the standard self-training and tri-training techniques. In selftraining, the labeled training data (L) is iteratively enriched with unlabeled data (U ) automatically labeled with the same learning system (L = L+Ul ), whereas in tri-training (Zhou and Li, 2005) there are three different learning systems, A, B and C, and the labeled data for the system A is enriched with instances from U on which the two other systems agree, therefore La = L + (Ub ∩ Uc ). Different variations of these methods have been successfully applied in dependency parsing, for example (McClosky et al., 2006; Søgaard and Rishøj, 2010; Li et al., 2014; Weiss et al., 2015). In this work we use two parsers (A and B) to process the unlabeled crawl data, and then the sentences where these two parsers fully agree are used to enrich the training data for the system A, i.e. La = L + (Ua ∩ Ub ). Therefore the method can be seen as a form of expanded self-training or limited tri-training. A similar technique is successfully used for example by Sagae and Tsujii (2007) in parser domain adaptation and Bj¨orkelund et al. (2014) in general parsing. lection (Nivre et al., 2016). We experiment with the following"
W18-6006,L16-1262,1,0.858484,"Missing"
W18-6006,D07-1111,0,0.0178248,"agree, therefore La = L + (Ub ∩ Uc ). Different variations of these methods have been successfully applied in dependency parsing, for example (McClosky et al., 2006; Søgaard and Rishøj, 2010; Li et al., 2014; Weiss et al., 2015). In this work we use two parsers (A and B) to process the unlabeled crawl data, and then the sentences where these two parsers fully agree are used to enrich the training data for the system A, i.e. La = L + (Ua ∩ Ub ). Therefore the method can be seen as a form of expanded self-training or limited tri-training. A similar technique is successfully used for example by Sagae and Tsujii (2007) in parser domain adaptation and Bj¨orkelund et al. (2014) in general parsing. lection (Nivre et al., 2016). We experiment with the following treebanks: UD Czech, UD English, UD Russian-SynTagRus, and UD Finnish, UD Slovak. With the exception of UD RussianSynTagRus, all our experiments are based on UD release 2.0. This UD release was used in the CoNLL-17 Shared Task on Multilingual Parsing from Raw Text to Universal Dependencies (Zeman et al., 2017), giving us a point of comparison to the state-of-the-art. For UD Russian-SynTagRus, we use UD release 2.1, which has a considerably improved annot"
W18-6006,N18-1105,0,0.176443,"Missing"
W18-6006,C10-1120,0,0.0266349,"tands between the standard self-training and tri-training techniques. In selftraining, the labeled training data (L) is iteratively enriched with unlabeled data (U ) automatically labeled with the same learning system (L = L+Ul ), whereas in tri-training (Zhou and Li, 2005) there are three different learning systems, A, B and C, and the labeled data for the system A is enriched with instances from U on which the two other systems agree, therefore La = L + (Ub ∩ Uc ). Different variations of these methods have been successfully applied in dependency parsing, for example (McClosky et al., 2006; Søgaard and Rishøj, 2010; Li et al., 2014; Weiss et al., 2015). In this work we use two parsers (A and B) to process the unlabeled crawl data, and then the sentences where these two parsers fully agree are used to enrich the training data for the system A, i.e. La = L + (Ua ∩ Ub ). Therefore the method can be seen as a form of expanded self-training or limited tri-training. A similar technique is successfully used for example by Sagae and Tsujii (2007) in parser domain adaptation and Bj¨orkelund et al. (2014) in general parsing. lection (Nivre et al., 2016). We experiment with the following treebanks: UD Czech, UD En"
W18-6006,K17-3009,0,0.0267855,"Missing"
W18-6006,P15-1032,0,0.0171688,"d tri-training techniques. In selftraining, the labeled training data (L) is iteratively enriched with unlabeled data (U ) automatically labeled with the same learning system (L = L+Ul ), whereas in tri-training (Zhou and Li, 2005) there are three different learning systems, A, B and C, and the labeled data for the system A is enriched with instances from U on which the two other systems agree, therefore La = L + (Ub ∩ Uc ). Different variations of these methods have been successfully applied in dependency parsing, for example (McClosky et al., 2006; Søgaard and Rishøj, 2010; Li et al., 2014; Weiss et al., 2015). In this work we use two parsers (A and B) to process the unlabeled crawl data, and then the sentences where these two parsers fully agree are used to enrich the training data for the system A, i.e. La = L + (Ua ∩ Ub ). Therefore the method can be seen as a form of expanded self-training or limited tri-training. A similar technique is successfully used for example by Sagae and Tsujii (2007) in parser domain adaptation and Bj¨orkelund et al. (2014) in general parsing. lection (Nivre et al., 2016). We experiment with the following treebanks: UD Czech, UD English, UD Russian-SynTagRus, and UD Fi"
W18-6012,W07-1427,0,0.0164425,"Missing"
W18-6012,C04-1204,0,0.0109514,"a Montemagni Sebastian Schuster? Maria Simi• ∗ Uppsala University, Department of Linguistics and Philology † University of Pavia, Department of Linguistics ‡ University of Turku, Department of Future Technologies  Institute for Computational Linguistics «A. Zampolli» – CNR, Italy ? Stanford University, Department of Linguistics • University of Pisa, Department of Computer Science Abstract and as input to manual validation. Further, enhanced UD graphs are in many respects very similar to semantic dependency representations that encode predicate-argument structures (e.g., Böhmová et al. 2003; Miyao and Tsujii 2004; Oepen and Lønning 2006). While the latter exist only for a small number of languages and are typically either produced by complex hand-written grammars or by manual annotation, basic UD treebanks currently exist for more than 60 languages. Hence, automatic methods capable of predicting enhanced dependencies from UD treebanks, have the potential to drastically increase the availability of semantic dependency treebanks. In this paper, we evaluate a rule-based system developed for English and a data-driven system trained on de-lexicalized Finnish data, for predicting enhanced dependencies on a"
W18-6012,W13-3728,1,0.552289,"ll Swe Ita LSI RBE RBE 660 112 162 0.85 0.85 0.76 0.78 19 15 0 65 2 35 Table 1: Evaluation of predicted enhanced dependencies for Italian and Swedish (RBE = rule-based English system, DDF = data-driven Finnish system, LSI = language-specific Italian system). 3.2 The Data-Driven Finnish System lines but does not yet handle null nodes. It provides an interesting point of comparison for the cross-lingual systems but cannot really be evaluated on the same conditions since it has been developed using data from the Italian treebank. This data-driven approach is adapted from the supervised method of Nyblom et al. (2013) originally developed for Finnish. First, patterns identify candidate relations, which are subsequently classified with a linear SVM, trained on gold standard annotation. The original method does not predict null nodes, and therefore we only discuss added subject relations and coordination below. Added subject relations For any infinitive verb attached to a higher predicate with an xcomp relation, the system adds a subject relation to a core or (dative) oblique dependent of the governing verb. In contrast to the other systems, this system uses external language-specific resources that specify"
W18-6012,oepen-lonning-2006-discriminant,0,0.0444464,"Schuster? Maria Simi• ∗ Uppsala University, Department of Linguistics and Philology † University of Pavia, Department of Linguistics ‡ University of Turku, Department of Future Technologies  Institute for Computational Linguistics «A. Zampolli» – CNR, Italy ? Stanford University, Department of Linguistics • University of Pisa, Department of Computer Science Abstract and as input to manual validation. Further, enhanced UD graphs are in many respects very similar to semantic dependency representations that encode predicate-argument structures (e.g., Böhmová et al. 2003; Miyao and Tsujii 2004; Oepen and Lønning 2006). While the latter exist only for a small number of languages and are typically either produced by complex hand-written grammars or by manual annotation, basic UD treebanks currently exist for more than 60 languages. Hence, automatic methods capable of predicting enhanced dependencies from UD treebanks, have the potential to drastically increase the availability of semantic dependency treebanks. In this paper, we evaluate a rule-based system developed for English and a data-driven system trained on de-lexicalized Finnish data, for predicting enhanced dependencies on a sample of 1,000 sentences"
W18-6012,D17-1009,0,0.0314015,"pe language-specific system. 1 Introduction Universal Dependencies (UD) is a framework for cross-linguistically consistent treebank annotation (Nivre et al., 2016). Its syntactic annotation layer exists in two versions: a basic representation, where words are connected by syntactic relations into a dependency tree, and an enhanced representation, which is a richer graph structure that adds external subject relations, shared dependents in coordination, and predicate-argument relations in elliptical constructions, among other things. Despite the usefulness of enhanced representations (see e.g., Reddy et al. 2017; Schuster et al. 2017), most UD treebanks still contain only basic dependencies1 and therefore cannot be used to train or evaluate systems that output enhanced UD graphs. In this paper, we explore cross-lingual methods for predicting enhanced dependencies given a basic dependencies treebank. If these predictions are accurate enough, they can be used as a first approximation of enhanced representations for the nearly 100 UD treebanks that lack them, 2 Basic and Enhanced Dependencies Basic dependencies are strict surface syntax trees that connect content words with argument and modifier relatio"
W18-6012,L16-1376,1,0.763597,"e been developed, such as the one by Candito et al. (2017) for French, but this is the first attempt to predict enhanced dependencies in a language-independent way. sion of the basic one, this does not hold in general (as shown by the treatment of ellipsis below). The current UD guidelines define five enhancements: 1. 2. 3. 4. 5. Added subject relations in control and raising Null nodes for elided predicates (gapping) Shared heads and dependents in coordination Co-reference in relative clause constructions Modifier relations typed by case markers 3.1 The system is an adaptation of the work by Schuster and Manning (2016), developed for English. It relies on Semgrex (Chambers et al., 2007) patterns to find dependency structures that should be enhanced and applies heuristics-based processing steps corresponding to the five types of enhancement described in Section 2. We briefly discuss the three steps that are relevant to our study. The last two enhancements can in most cases be predicted deterministically from the basic representation and are mainly a practical convenience. We therefore limit our attention to the first three types, illustrated in Figure 1 (a–c). Added subject relations Basic dependencies do no"
W18-6012,N18-1105,1,0.752943,"Missing"
W18-6012,W13-2308,1,0.82118,"enhanced errors attributed to the systems. Feature representation To enable transfer from models trained on Finnish to other languages, we remove lexical and morphological features except universal POS tags and morphological categories that we expect to generalize well: Number, Mood, Tense, VerbForm, Voice. Languagespecific dependency type features are generalized to universal types (e.g., from nmod:tmod to nmod). 3.3 Evaluation The Language-Specific Italian System The language-specific Italian system builds on the rule-based enhancer developed for the Italian Stanford Dependencies Treebank (Bosco et al., 2013, 2014). It has been adapted to predict enhanced dependencies according to the UD guide104 nsubj xcomp obj Om du ... låter pengarna stå kvar till 1971 års slut . nsubj (1) nsubj “If you ... let the money remain [in the account] until the end of 1971.” xcomp nsubj obl E le autorità di Zagabria hanno proibito ai giornalisti di andare a Petrinja ... nsubj nsubj (2) “And the Zagreb authorities have forbidden journalists to go to Petrinja ...” conj conj amod För fysiska personer , dödsbon och familjestiftelser slopas rätten att göra avdrag ... (3) amod “For natural persons, estates and family found"
W19-6125,P16-1154,0,0.0206171,"pop6 We use label weighting to account for the imbalanced distribution, which we optimize against the validation set to 0.85:1 for the positive class (other optimal hyperparameters are C1=35.0, C2=0.5, as well as defaults). We use CRFsuite (Okazaki, 2007) with label weighting by Sampo Pyysalo: https://github.com/spyysalo/crfsuite ular baseline dataset. After that, we describe the training of the generation model on our ice hockey corpus and use automatic evaluation metrics to compare against existing references. 4.1 Model Architecture We use a pointer-generation network (Vinyals et al., 2015; Gu et al., 2016; See et al., 2017), where the neural attention mechanism in the encoder-decoder model is adapted to jointly model a probability distribution over words from the known vocabulary, a distribution over words from the input sequence to copy and a probability that controls the copying mechanism. A separate coverage attention vector, a sum of past attention distributions, is maintained to inform the model of its past attention decisions. Such a coverage model is shown to prevent text repetition in generated output (Tu et al., 2016; See et al., 2017). The model is implemented using the OpenNMT-py li"
W19-6125,W04-1013,0,0.0522025,"Missing"
W19-6125,P16-1008,0,0.0138748,"hitecture We use a pointer-generation network (Vinyals et al., 2015; Gu et al., 2016; See et al., 2017), where the neural attention mechanism in the encoder-decoder model is adapted to jointly model a probability distribution over words from the known vocabulary, a distribution over words from the input sequence to copy and a probability that controls the copying mechanism. A separate coverage attention vector, a sum of past attention distributions, is maintained to inform the model of its past attention decisions. Such a coverage model is shown to prevent text repetition in generated output (Tu et al., 2016; See et al., 2017). The model is implemented using the OpenNMT-py library (Klein et al., 2017). The encoder has two bidirectional LSTM layers with 500 hidden units, together with 500-dimensional word embeddings. The decoder has two unidirectional LSTM layers with 500 hidden units. Both encoder and decoder apply a dropout of 0.3 between LSTM layers. 4.2 Baseline Experiments on the E2E Dataset To demonstrate the performance of our generation model architecture, we report results on a known dataset with published baselines, namely the E2E NLG Challenge (Duˇsek et al., 2018) on end-to-end natural"
W19-6125,H05-1042,0,\N,Missing
W19-6125,P02-1040,0,\N,Missing
W19-6125,W07-0734,0,\N,Missing
W19-6125,P17-4012,0,\N,Missing
W19-6125,P17-1099,0,\N,Missing
W19-6125,D17-1239,0,\N,Missing
W19-6125,W17-3528,0,\N,Missing
W19-6125,D18-1356,0,\N,Missing
W19-6125,K18-2013,1,\N,Missing
W19-6125,W18-6557,0,\N,Missing
W19-6125,D18-1422,0,\N,Missing
W19-6204,W19-4825,0,0.185099,"nt attractors, i.e. an intervening subordinate clause with opposite number of the subject. BERT is also shown to perform well on the agreement task even if tokens are randomly substituted from the same part-of-speech category, making the input semantically meaningless while preserving the syntactic structure. Similarly, Ettinger (2019) evaluates the BERT model on several English psycholinguistic datasets, where the model is shown generally being able to distinguish a good completion from a bad one, while still failing in some more complex categories, for example being insensitive to negation. Lin et al. (2019) uses a diagnostic classifier to study to which extent syntactic or positional information can be predicted from the English BERT embeddings, and how this information is carried through the different layers. The multilingual BERT model is studied in the context of zero-shot cross-lingual transfer, where it is shown to perform competitively to other transfer models. (Pires et al., 2019; Wu and Dredze, 2019) Text generation with BERT is introduced by Wang and Cho (2019), who demonstrate several different algorithms to generate language with a BERT model. They demonstrate that BERT even though no"
W19-6204,L16-1262,1,0.877294,"Missing"
W19-6204,P19-1452,0,0.069369,"Missing"
W19-6204,D19-1077,0,0.0291987,"s, where the model is shown generally being able to distinguish a good completion from a bad one, while still failing in some more complex categories, for example being insensitive to negation. Lin et al. (2019) uses a diagnostic classifier to study to which extent syntactic or positional information can be predicted from the English BERT embeddings, and how this information is carried through the different layers. The multilingual BERT model is studied in the context of zero-shot cross-lingual transfer, where it is shown to perform competitively to other transfer models. (Pires et al., 2019; Wu and Dredze, 2019) Text generation with BERT is introduced by Wang and Cho (2019), who demonstrate several different algorithms to generate language with a BERT model. They demonstrate that BERT even though not being trained on an explicit language generation objective, is capable of generating coherent, varied language. Language English German Danish Finnish Nor. (Bokm˚al) Nor. (Nynorsk) Swedish BERT mono multi mono multi multi multi multi multi multi Test acc. 86.03 87.82 97.27 95.29 89.96 93.20 93.67 94.44 93.00 Baseline 54.93 54.44 69.61 69.19 53.25 50.54 56.19 53.18 62.09 Table 1: Diagnostic classifier res"
