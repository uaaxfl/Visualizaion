2009.jeptalnrecital-demonstration.2,J08-4005,0,0.0548956,"Missing"
2009.jeptalnrecital-demonstration.2,2008.jeptalnrecital-long.23,1,0.755463,"Missing"
2009.jeptalnrecital-demonstration.2,N03-1024,0,0.0677999,"Missing"
2010.jeptalnrecital-court.18,P05-1074,0,0.148687,"Missing"
2010.jeptalnrecital-court.18,N03-1003,0,0.0954374,"Missing"
2010.jeptalnrecital-court.18,2010.jeptalnrecital-recital.5,1,0.496316,"Missing"
2010.jeptalnrecital-court.18,D08-1021,0,0.100887,"Missing"
2010.jeptalnrecital-court.18,C08-1013,0,0.149237,"Missing"
2010.jeptalnrecital-court.18,P99-1044,0,0.228551,"Missing"
2010.jeptalnrecital-court.18,N03-1016,0,0.0475047,"Missing"
2010.jeptalnrecital-court.18,2008.jeptalnrecital-long.23,1,0.853768,"Missing"
2010.jeptalnrecital-court.18,J03-1002,0,0.0123418,"Missing"
2010.jeptalnrecital-court.18,N03-1024,0,0.563199,"Missing"
2010.jeptalnrecital-recital.5,P01-1008,0,0.12745,"Missing"
2010.jeptalnrecital-recital.5,W08-0906,0,0.0271564,"Missing"
2010.jeptalnrecital-recital.5,2010.jeptalnrecital-court.18,1,0.496316,"Missing"
2010.jeptalnrecital-recital.5,J08-4005,0,0.0406393,"Missing"
2010.jeptalnrecital-recital.5,W09-3102,0,0.306035,"Missing"
2010.jeptalnrecital-recital.5,C04-1051,0,0.115791,"Missing"
2010.jeptalnrecital-recital.5,P08-4006,0,0.118509,"Missing"
2010.jeptalnrecital-recital.5,W03-1608,0,0.273003,"Missing"
2010.jeptalnrecital-recital.5,P98-1116,0,0.130832,"Missing"
2010.jeptalnrecital-recital.5,max-wisniewski-2010-mining,0,0.0938746,"Missing"
2010.jeptalnrecital-recital.5,N03-1024,0,0.492963,"Missing"
2010.jeptalnrecital-recital.5,E09-1082,0,0.0470594,"Missing"
2010.jeptalnrecital-recital.5,W09-0441,0,0.0724597,"Missing"
2010.jeptalnrecital-recital.5,takezawa-etal-2002-toward,0,0.0641449,"Missing"
2011.jeptalnrecital-long.33,2010.jeptalnrecital-recital.5,1,0.860909,"Missing"
2011.jeptalnrecital-long.33,A94-1019,0,0.0542167,"Missing"
2011.jeptalnrecital-long.33,W09-3102,0,0.0650934,"Missing"
2011.jeptalnrecital-long.33,I05-5002,0,0.087889,"Missing"
2011.jeptalnrecital-long.33,P08-4006,0,0.0470072,"Missing"
2011.jeptalnrecital-long.33,J10-3003,0,0.0590089,"Missing"
2011.jeptalnrecital-long.33,2008.jeptalnrecital-long.23,1,0.825827,"Missing"
2011.jeptalnrecital-long.33,D10-1064,1,0.89007,"Missing"
2011.jeptalnrecital-long.33,max-wisniewski-2010-mining,1,0.850881,"Missing"
2011.jeptalnrecital-long.33,2010.jeptalnrecital-long.13,1,0.853233,"Missing"
2011.jeptalnrecital-long.33,N10-1056,0,0.0334747,"Missing"
2011.jeptalnrecital-long.33,zesch-etal-2008-extracting,0,0.0211202,"Missing"
2011.jeptalnrecital-long.38,P05-1074,0,0.0841195,"Missing"
2011.jeptalnrecital-long.38,N03-1003,0,0.0657265,"Missing"
2011.jeptalnrecital-long.38,P01-1008,0,0.146473,"Missing"
2011.jeptalnrecital-long.38,2010.jeptalnrecital-recital.5,1,0.812219,"Missing"
2011.jeptalnrecital-long.38,2010.jeptalnrecital-court.18,1,0.783574,"Missing"
2011.jeptalnrecital-long.38,D08-1021,0,0.0432575,"Missing"
2011.jeptalnrecital-long.38,C08-1013,0,0.0271679,"Missing"
2011.jeptalnrecital-long.38,C10-1027,1,0.871956,"Missing"
2011.jeptalnrecital-long.38,W09-3102,0,0.0252497,"Missing"
2011.jeptalnrecital-long.38,I05-5002,0,0.054168,"Missing"
2011.jeptalnrecital-long.38,P08-4006,0,0.0400888,"Missing"
2011.jeptalnrecital-long.38,W03-1608,0,0.0648712,"Missing"
2011.jeptalnrecital-long.38,P99-1044,0,0.11808,"Missing"
2011.jeptalnrecital-long.38,N06-1058,0,0.0538278,"Missing"
2011.jeptalnrecital-long.38,P07-2045,0,0.00674383,"Missing"
2011.jeptalnrecital-long.38,P98-1116,0,0.0309369,"Missing"
2011.jeptalnrecital-long.38,2008.jeptalnrecital-long.23,1,0.87252,"Missing"
2011.jeptalnrecital-long.38,W09-2503,1,0.885153,"Missing"
2011.jeptalnrecital-long.38,J03-1002,0,0.00695544,"Missing"
2011.jeptalnrecital-long.38,N03-1024,0,0.0817453,"Missing"
2011.jeptalnrecital-long.38,W04-3219,0,0.0613814,"Missing"
2011.jeptalnrecital-long.38,W09-0441,0,0.0401231,"Missing"
2011.jeptalnrecital-long.38,2010.amta-papers.18,0,0.0870546,"Missing"
2020.gebnlp-1.12,P17-1183,0,0.0187964,"c parallel gender corpus has many of such spelling confusions. 4 Joint Gender Reinflection Model In this section, we discuss the motivation behind our model architecture as well as the integration of the linguistic features. We also describe the training settings and the model’s hyperparameters for reproducibility. 4.1 Motivation Sequence-to-sequence models have achieved significant results in grammatical error correction (GEC) (Chollampatt and Ng, 2018; Junczys-Dowmunt et al., 2018; Grundkiewicz et al., 2019) and morphological reinflection tasks (Faruqui et al., 2016; Kann and Schütze, 2016; Aharoni and Goldberg, 2017). Many of these problems are modeled on the word-level, however, such models usually require large amounts of training data to achieve good results. Character-level sequence-to-sequence models can be superior in mitigating the lack of training data and in dealing with subtle morphological reinflection. Further, pre-trained distributed word representations have also shown to be helpful if integrated properly within character-level sequence-to-sequence models (Watson et al., 2018). We formulate the gender reinflection problem as a user-aware grammatical error correction (UGEC) task at the charac"
2020.gebnlp-1.12,P11-2062,1,0.880036,"rantee that we map from feminine to masculine in every context. For example,  the noun word éÒêÓ mhm~ ‘mission/assignment’ is only feminine and has no meaningful masculine form, as opposed to the adjective  mhm~ ‘important [feminine singular]’ discussed above. éÒêÓ These facts pose major challenges to deep learning models attempting to learn from limited supervised or even large unsupervised data. In this work, we make use of morphological analyzers that indicate all the possible gender information of the words in terms of their functional (grammatical) and form-based (affixational) values (Alkuhlani and Habash, 2011). 141 Orthographic Ambiguity and Noise Arabic uses diacritics to specify short vowels and consonantal doubling. These diacritics are optional and generally unwritten, leaving readers to decipher words using  J» knt can be diacritized as kuntu ‘I contextual and templatic morphology clues. For example, the verb I was’, kunta ‘You [masculine] were’, or kunti ‘You [feminine] were’. This is a challenge for identifying the words that need to change for a first-person target gender. In addition to the issue of orthographic ambiguity, unedited MSA text is reported to be quite noisy with spelling erro"
2020.gebnlp-1.12,Q17-1010,0,0.0115858,"c Features and Word Embeddings We explore adding word-level morphological features as well as pre-trained distributed word representations to the character embeddings. We use the CALIMAStar Arabic morphological analyzer (Taji et al., 2018) to obtain word-level functional gender features (Alkuhlani and Habash, 2011).5 We represent the morphological features for word wj as a four-dimension one-hot vector µwj ∈ R4 . Each element of this one-hot vector represents whether the word wj is masculine or feminine as well as if the analysis was obtained with or without spelling backoff. We use FastText (Bojanowski et al., 2017) to learn distributed word representations and we denote the FastText word embedding for word wj as ρwj ∈ RF . Similarly to Watson et al. (2018), we added the word-level features to the character embeddings only on the encoder side. Each character embedding exi is then enriched with ρwj and µwj to create a single vector [exi ; µwj ; ρwj ] ∈ RE+4+F which we feed to the encoder, where wj is the word containing character xi . Inference At inference time, we use greedy decoding to find the most likely sequence:6 yˆ1:m = argmax P (ˆ y |x1:n , g) = argmax yˆ∈Vy yˆ∈Vy Y P (yˆt |ˆ y1:t−1 , x1:n , g) y"
2020.gebnlp-1.12,N19-3002,0,0.013635,"ude and present future work in Section 8. 2 Related Work Many NLP systems have the ability to embed and amplify societal (gender, racial, religious, etc.) biases across a variety of core tasks such as coreference resolution (Rudinger et al., 2018; Zhao et al., 2018a), machine translation (Rabinovich et al., 2017; Vanmassenhove et al., 2018; Font and Costa-jussà, 2019; Moryossef et al., 2019; Stanovsky et al., 2019; Stafanoviˇcs et al., 2020; Gonen and Webster, 2020), named entity recognition (Mehrabi et al., 2019), dialogue systems (Dinan et al., 2019), and language modeling (Lu et al., 2018; Bordia and Bowman, 2019). For the case of gender bias, various research efforts have shown that this could be caused by either human-generated training datasets (Font and Costa-jussà, 2019; Habash et al., 2019), pre-trained word embeddings (Bolukbasi et al., 2016; Zhao et al., 2017; Caliskan et al., 2017; Manzini et al., 2019), or language models (Kurita et al., 2019; Zhao et al., 2019). To mitigate this problem, several researchers 2 3 Arabic transliteration is in the HSB scheme (Habash et al., 2007). https://github.com/CAMeL-Lab/gender-reinflection 140 proposed approaches in which they focus mainly on debiasing wor"
2020.gebnlp-1.12,P17-1074,0,0.0126789,"on metric (Papineni et al., 2002), however, we believe that BLEU is not a suitable metric for our task due to the high similarity between the input and output sentences. We use SacreBLEU (Post, 2018) to compute the BLEU scores. Additionally, we use the MaxMatch (M2 ) scorer (Dahlmeier and Ng, 2012) to compute the word-level edits between the input and reinflected output. We report the precision, recall, and F0.5 scores calculated against the gold edits, which were also created by the M2 scorer. We are aware that there are other tools to consider for word-level edit calculation such as ERRANT (Bryant et al., 2017), but we did not use them as they require additional dependencies to work for Arabic. Input Gender Identification Our sequence-to-sequence model does not explicitly identify the gender of the input sentence; however, we consider any attempted change (or lack thereof) to the input as a signal for the implicit gender identification: if our model reinflects the source sentence, then we consider the gender of this sentence to be the opposite of the given target gender. But if the model does not reinflect the source sentence, then we consider the gender of this sentence to be the same as the target"
2020.gebnlp-1.12,D14-1179,0,0.027343,"Missing"
2020.gebnlp-1.12,N12-1067,0,0.14311,"tely. In this paper, we compare to their results using the publicly available Arabic parallel gender corpus they built – a parallel corpus of first-person-singular Arabic sentences that are gender-annotated and reinflected. However, our work is different from theirs in that we jointly learn reinflection for both masculine and feminine genders together. We also model identification implicitly with reinflection in a single architecture. Furthermore, we formulate the problem as a user-aware grammatical error correction task (UGEC). As such, we use as our primary metric the MaxMatch (M2 ) scorer (Dahlmeier and Ng, 2012), which is far more meaningful than the BLEU (Papineni et al., 2002) metric used by Habash et al. (2019) for this task. 3 Arabic Linguistic Background Modern Standard Arabic (MSA) NLP systems and more specifically those using deep learning, face several challenges when it comes to gender expression including morphological richness, orthographic ambiguity and noise. Morphological Richness and Complexity Arabic has a rich morphological system that inflects for gender, number, person, case, state, aspect, mood and voice, in addition to numerous attachable clitics (prepositions, particles, pronoun"
2020.gebnlp-1.12,2020.emnlp-main.23,0,0.0175873,"no-gender expressions. We are not aware of any sociolinguistics published research that discusses such alternatives for Arabic, although there are growing grassroots efforts, e.g., the Ebdal Project.1 1 Introduction The recent advances in machine learning have propelled the field of Natural Language Processing (NLP) forward at a great pace and raised expectation about the quality of results and especially their impact in a social context, including not only race (Merullo et al., 2019) and politics (Fan et al., 2019), but also gender identities (Font and Costa-jussà, 2019; Dinan et al., 2019; Dinan et al., 2020). Human-generated data, reflective of the gender discrimination and sexist stereotypes perpetrated through language and speaker’s lexical choices, is considered the primary source of these biases (Maass and Arcuri, 1996; Menegatti and Rubini, 2017). However, Habash et al. (2019) pointed out that NLP gender biases do not just exist in human-generated training data, and models built from it; but also stem from gender-blind (i.e., gender-unaware) systems designed to generate a single text output without considering any target gender information. Such systems propagate the biases of the models the"
2020.gebnlp-1.12,D19-1664,0,0.0610199,"Missing"
2020.gebnlp-1.12,N16-1077,0,0.0267881,"d Tiedemann, 2016) they use to build the Arabic parallel gender corpus has many of such spelling confusions. 4 Joint Gender Reinflection Model In this section, we discuss the motivation behind our model architecture as well as the integration of the linguistic features. We also describe the training settings and the model’s hyperparameters for reproducibility. 4.1 Motivation Sequence-to-sequence models have achieved significant results in grammatical error correction (GEC) (Chollampatt and Ng, 2018; Junczys-Dowmunt et al., 2018; Grundkiewicz et al., 2019) and morphological reinflection tasks (Faruqui et al., 2016; Kann and Schütze, 2016; Aharoni and Goldberg, 2017). Many of these problems are modeled on the word-level, however, such models usually require large amounts of training data to achieve good results. Character-level sequence-to-sequence models can be superior in mitigating the lack of training data and in dealing with subtle morphological reinflection. Further, pre-trained distributed word representations have also shown to be helpful if integrated properly within character-level sequence-to-sequence models (Watson et al., 2018). We formulate the gender reinflection problem as a user-aware g"
2020.gebnlp-1.12,W19-3821,0,0.0842555,"other alternatives such as non-binary gender or no-gender expressions. We are not aware of any sociolinguistics published research that discusses such alternatives for Arabic, although there are growing grassroots efforts, e.g., the Ebdal Project.1 1 Introduction The recent advances in machine learning have propelled the field of Natural Language Processing (NLP) forward at a great pace and raised expectation about the quality of results and especially their impact in a social context, including not only race (Merullo et al., 2019) and politics (Fan et al., 2019), but also gender identities (Font and Costa-jussà, 2019; Dinan et al., 2019; Dinan et al., 2020). Human-generated data, reflective of the gender discrimination and sexist stereotypes perpetrated through language and speaker’s lexical choices, is considered the primary source of these biases (Maass and Arcuri, 1996; Menegatti and Rubini, 2017). However, Habash et al. (2019) pointed out that NLP gender biases do not just exist in human-generated training data, and models built from it; but also stem from gender-blind (i.e., gender-unaware) systems designed to generate a single text output without considering any target gender information. Such syste"
2020.gebnlp-1.12,W19-3621,0,0.02337,"ve shown that this could be caused by either human-generated training datasets (Font and Costa-jussà, 2019; Habash et al., 2019), pre-trained word embeddings (Bolukbasi et al., 2016; Zhao et al., 2017; Caliskan et al., 2017; Manzini et al., 2019), or language models (Kurita et al., 2019; Zhao et al., 2019). To mitigate this problem, several researchers 2 3 Arabic transliteration is in the HSB scheme (Habash et al., 2007). https://github.com/CAMeL-Lab/gender-reinflection 140 proposed approaches in which they focus mainly on debiasing word embeddings (Bolukbasi et al., 2016; Zhao et al., 2018b; Gonen and Goldberg, 2019) or using counterfactual data augmentation techniques (Lu et al., 2018; Zhao et al., 2018a; Zmigrod et al., 2019; Hall Maudslay et al., 2019). Most of the solutions were mainly proposed to reduce gender bias in English and may not work as well when it comes to morphologically rich languages. Nevertheless, there have been recent studies that explored the gender bias problem in languages other than English. Zhao et al. (2020) studied gender bias which is exhibited by multilingual embeddings in four languages (English, German, French, and Spanish) and demonstrated that such bias can impact cross-"
2020.gebnlp-1.12,2020.findings-emnlp.180,0,0.0173518,"architecture. Then, we present the experimental setup in Section 5 and discuss the results in Section 6. An error analysis is given in Section 7. We conclude and present future work in Section 8. 2 Related Work Many NLP systems have the ability to embed and amplify societal (gender, racial, religious, etc.) biases across a variety of core tasks such as coreference resolution (Rudinger et al., 2018; Zhao et al., 2018a), machine translation (Rabinovich et al., 2017; Vanmassenhove et al., 2018; Font and Costa-jussà, 2019; Moryossef et al., 2019; Stanovsky et al., 2019; Stafanoviˇcs et al., 2020; Gonen and Webster, 2020), named entity recognition (Mehrabi et al., 2019), dialogue systems (Dinan et al., 2019), and language modeling (Lu et al., 2018; Bordia and Bowman, 2019). For the case of gender bias, various research efforts have shown that this could be caused by either human-generated training datasets (Font and Costa-jussà, 2019; Habash et al., 2019), pre-trained word embeddings (Bolukbasi et al., 2016; Zhao et al., 2017; Caliskan et al., 2017; Manzini et al., 2019), or language models (Kurita et al., 2019; Zhao et al., 2019). To mitigate this problem, several researchers 2 3 Arabic transliteration is in"
2020.gebnlp-1.12,W19-4427,0,0.0189068,"Ya, and Ta-Marbuta, since the OpenSubtitles 2018 corpus (Lison and Tiedemann, 2016) they use to build the Arabic parallel gender corpus has many of such spelling confusions. 4 Joint Gender Reinflection Model In this section, we discuss the motivation behind our model architecture as well as the integration of the linguistic features. We also describe the training settings and the model’s hyperparameters for reproducibility. 4.1 Motivation Sequence-to-sequence models have achieved significant results in grammatical error correction (GEC) (Chollampatt and Ng, 2018; Junczys-Dowmunt et al., 2018; Grundkiewicz et al., 2019) and morphological reinflection tasks (Faruqui et al., 2016; Kann and Schütze, 2016; Aharoni and Goldberg, 2017). Many of these problems are modeled on the word-level, however, such models usually require large amounts of training data to achieve good results. Character-level sequence-to-sequence models can be superior in mitigating the lack of training data and in dealing with subtle morphological reinflection. Further, pre-trained distributed word representations have also shown to be helpful if integrated properly within character-level sequence-to-sequence models (Watson et al., 2018). We"
2020.gebnlp-1.12,W19-3822,1,0.0533142,"eld of Natural Language Processing (NLP) forward at a great pace and raised expectation about the quality of results and especially their impact in a social context, including not only race (Merullo et al., 2019) and politics (Fan et al., 2019), but also gender identities (Font and Costa-jussà, 2019; Dinan et al., 2019; Dinan et al., 2020). Human-generated data, reflective of the gender discrimination and sexist stereotypes perpetrated through language and speaker’s lexical choices, is considered the primary source of these biases (Maass and Arcuri, 1996; Menegatti and Rubini, 2017). However, Habash et al. (2019) pointed out that NLP gender biases do not just exist in human-generated training data, and models built from it; but also stem from gender-blind (i.e., gender-unaware) systems designed to generate a single text output without considering any target gender information. Such systems propagate the biases of the models they use. One example is the I-ama-doctor/I-am-a-nurse problem in machine translation (MT) systems targeting many morphologically 1 https://www.facebook.com/EbdalProject/ This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licen"
2020.gebnlp-1.12,D19-1530,0,0.0179616,"rd embeddings (Bolukbasi et al., 2016; Zhao et al., 2017; Caliskan et al., 2017; Manzini et al., 2019), or language models (Kurita et al., 2019; Zhao et al., 2019). To mitigate this problem, several researchers 2 3 Arabic transliteration is in the HSB scheme (Habash et al., 2007). https://github.com/CAMeL-Lab/gender-reinflection 140 proposed approaches in which they focus mainly on debiasing word embeddings (Bolukbasi et al., 2016; Zhao et al., 2018b; Gonen and Goldberg, 2019) or using counterfactual data augmentation techniques (Lu et al., 2018; Zhao et al., 2018a; Zmigrod et al., 2019; Hall Maudslay et al., 2019). Most of the solutions were mainly proposed to reduce gender bias in English and may not work as well when it comes to morphologically rich languages. Nevertheless, there have been recent studies that explored the gender bias problem in languages other than English. Zhao et al. (2020) studied gender bias which is exhibited by multilingual embeddings in four languages (English, German, French, and Spanish) and demonstrated that such bias can impact cross-lingual transfer learning tasks. Zmigrod et al. (2019) used a counterfactual data augmentation approach and developed a generative model to c"
2020.gebnlp-1.12,N18-1055,0,0.0260361,"ly normalized space for Alif, Ya, and Ta-Marbuta, since the OpenSubtitles 2018 corpus (Lison and Tiedemann, 2016) they use to build the Arabic parallel gender corpus has many of such spelling confusions. 4 Joint Gender Reinflection Model In this section, we discuss the motivation behind our model architecture as well as the integration of the linguistic features. We also describe the training settings and the model’s hyperparameters for reproducibility. 4.1 Motivation Sequence-to-sequence models have achieved significant results in grammatical error correction (GEC) (Chollampatt and Ng, 2018; Junczys-Dowmunt et al., 2018; Grundkiewicz et al., 2019) and morphological reinflection tasks (Faruqui et al., 2016; Kann and Schütze, 2016; Aharoni and Goldberg, 2017). Many of these problems are modeled on the word-level, however, such models usually require large amounts of training data to achieve good results. Character-level sequence-to-sequence models can be superior in mitigating the lack of training data and in dealing with subtle morphological reinflection. Further, pre-trained distributed word representations have also shown to be helpful if integrated properly within character-level sequence-to-sequence model"
2020.gebnlp-1.12,P16-2090,0,0.0285595,"y use to build the Arabic parallel gender corpus has many of such spelling confusions. 4 Joint Gender Reinflection Model In this section, we discuss the motivation behind our model architecture as well as the integration of the linguistic features. We also describe the training settings and the model’s hyperparameters for reproducibility. 4.1 Motivation Sequence-to-sequence models have achieved significant results in grammatical error correction (GEC) (Chollampatt and Ng, 2018; Junczys-Dowmunt et al., 2018; Grundkiewicz et al., 2019) and morphological reinflection tasks (Faruqui et al., 2016; Kann and Schütze, 2016; Aharoni and Goldberg, 2017). Many of these problems are modeled on the word-level, however, such models usually require large amounts of training data to achieve good results. Character-level sequence-to-sequence models can be superior in mitigating the lack of training data and in dealing with subtle morphological reinflection. Further, pre-trained distributed word representations have also shown to be helpful if integrated properly within character-level sequence-to-sequence models (Watson et al., 2018). We formulate the gender reinflection problem as a user-aware grammatical error correct"
2020.gebnlp-1.12,W19-3823,0,0.0175934,"d Costa-jussà, 2019; Moryossef et al., 2019; Stanovsky et al., 2019; Stafanoviˇcs et al., 2020; Gonen and Webster, 2020), named entity recognition (Mehrabi et al., 2019), dialogue systems (Dinan et al., 2019), and language modeling (Lu et al., 2018; Bordia and Bowman, 2019). For the case of gender bias, various research efforts have shown that this could be caused by either human-generated training datasets (Font and Costa-jussà, 2019; Habash et al., 2019), pre-trained word embeddings (Bolukbasi et al., 2016; Zhao et al., 2017; Caliskan et al., 2017; Manzini et al., 2019), or language models (Kurita et al., 2019; Zhao et al., 2019). To mitigate this problem, several researchers 2 3 Arabic transliteration is in the HSB scheme (Habash et al., 2007). https://github.com/CAMeL-Lab/gender-reinflection 140 proposed approaches in which they focus mainly on debiasing word embeddings (Bolukbasi et al., 2016; Zhao et al., 2018b; Gonen and Goldberg, 2019) or using counterfactual data augmentation techniques (Lu et al., 2018; Zhao et al., 2018a; Zmigrod et al., 2019; Hall Maudslay et al., 2019). Most of the solutions were mainly proposed to reduce gender bias in English and may not work as well when it comes to m"
2020.gebnlp-1.12,L16-1147,0,0.0167557,"noise result in a high degree of morphological confusability and model sparsity. For  instance, a common spelling error of writing the Ta-Marbuta ( è ~) as Ha ( è h) results in interpreting the ( è h) as a possessive pronoun clitic attached to a masculine noun: éJ . KA¿ kAtb~ ‘writer [feminine]’. éJ.KA¿ kAtbh ‘his writer [masculine]’, vs Normalizing the text may solve some issues related to noise and ambiguity. In this paper, we follow Habash et al. (2019)’s decision to evaluate within an orthographically normalized space for Alif, Ya, and Ta-Marbuta, since the OpenSubtitles 2018 corpus (Lison and Tiedemann, 2016) they use to build the Arabic parallel gender corpus has many of such spelling confusions. 4 Joint Gender Reinflection Model In this section, we discuss the motivation behind our model architecture as well as the integration of the linguistic features. We also describe the training settings and the model’s hyperparameters for reproducibility. 4.1 Motivation Sequence-to-sequence models have achieved significant results in grammatical error correction (GEC) (Chollampatt and Ng, 2018; Junczys-Dowmunt et al., 2018; Grundkiewicz et al., 2019) and morphological reinflection tasks (Faruqui et al., 20"
2020.gebnlp-1.12,D15-1166,0,0.0845227,"Missing"
2020.gebnlp-1.12,N19-1062,0,0.0210389,", 2017; Vanmassenhove et al., 2018; Font and Costa-jussà, 2019; Moryossef et al., 2019; Stanovsky et al., 2019; Stafanoviˇcs et al., 2020; Gonen and Webster, 2020), named entity recognition (Mehrabi et al., 2019), dialogue systems (Dinan et al., 2019), and language modeling (Lu et al., 2018; Bordia and Bowman, 2019). For the case of gender bias, various research efforts have shown that this could be caused by either human-generated training datasets (Font and Costa-jussà, 2019; Habash et al., 2019), pre-trained word embeddings (Bolukbasi et al., 2016; Zhao et al., 2017; Caliskan et al., 2017; Manzini et al., 2019), or language models (Kurita et al., 2019; Zhao et al., 2019). To mitigate this problem, several researchers 2 3 Arabic transliteration is in the HSB scheme (Habash et al., 2007). https://github.com/CAMeL-Lab/gender-reinflection 140 proposed approaches in which they focus mainly on debiasing word embeddings (Bolukbasi et al., 2016; Zhao et al., 2018b; Gonen and Goldberg, 2019) or using counterfactual data augmentation techniques (Lu et al., 2018; Zhao et al., 2018a; Zmigrod et al., 2019; Hall Maudslay et al., 2019). Most of the solutions were mainly proposed to reduce gender bias in English an"
2020.gebnlp-1.12,D19-1666,0,0.0496671,"Missing"
2020.gebnlp-1.12,W19-3807,0,0.0221462,"our model for joint gender reinflection and describes the encoder-decoder architecture. Then, we present the experimental setup in Section 5 and discuss the results in Section 6. An error analysis is given in Section 7. We conclude and present future work in Section 8. 2 Related Work Many NLP systems have the ability to embed and amplify societal (gender, racial, religious, etc.) biases across a variety of core tasks such as coreference resolution (Rudinger et al., 2018; Zhao et al., 2018a), machine translation (Rabinovich et al., 2017; Vanmassenhove et al., 2018; Font and Costa-jussà, 2019; Moryossef et al., 2019; Stanovsky et al., 2019; Stafanoviˇcs et al., 2020; Gonen and Webster, 2020), named entity recognition (Mehrabi et al., 2019), dialogue systems (Dinan et al., 2019), and language modeling (Lu et al., 2018; Bordia and Bowman, 2019). For the case of gender bias, various research efforts have shown that this could be caused by either human-generated training datasets (Font and Costa-jussà, 2019; Habash et al., 2019), pre-trained word embeddings (Bolukbasi et al., 2016; Zhao et al., 2017; Caliskan et al., 2017; Manzini et al., 2019), or language models (Kurita et al., 2019; Zhao et al., 2019). To"
2020.gebnlp-1.12,P02-1040,0,0.107682,"vailable Arabic parallel gender corpus they built – a parallel corpus of first-person-singular Arabic sentences that are gender-annotated and reinflected. However, our work is different from theirs in that we jointly learn reinflection for both masculine and feminine genders together. We also model identification implicitly with reinflection in a single architecture. Furthermore, we formulate the problem as a user-aware grammatical error correction task (UGEC). As such, we use as our primary metric the MaxMatch (M2 ) scorer (Dahlmeier and Ng, 2012), which is far more meaningful than the BLEU (Papineni et al., 2002) metric used by Habash et al. (2019) for this task. 3 Arabic Linguistic Background Modern Standard Arabic (MSA) NLP systems and more specifically those using deep learning, face several challenges when it comes to gender expression including morphological richness, orthographic ambiguity and noise. Morphological Richness and Complexity Arabic has a rich morphological system that inflects for gender, number, person, case, state, aspect, mood and voice, in addition to numerous attachable clitics (prepositions, particles, pronouns) (Habash, 2010). This results in a large number of forms for any"
2020.gebnlp-1.12,W18-6319,0,0.0119102,". (2019). After merging the corpora we ended up with 17,132 sentence pairs for training (T RAIN), 2,448 for development (D EV), and 4,896 for testing (T EST). All of our systems are trained to take a source sentence and a target gender as input to produce a gender-reinflected target sentence as described in section 4.2. 5.2 Metrics Gender Reinflection We follow Habash et al. (2019) and use BLEU as an evaluation metric (Papineni et al., 2002), however, we believe that BLEU is not a suitable metric for our task due to the high similarity between the input and output sentences. We use SacreBLEU (Post, 2018) to compute the BLEU scores. Additionally, we use the MaxMatch (M2 ) scorer (Dahlmeier and Ng, 2012) to compute the word-level edits between the input and reinflected output. We report the precision, recall, and F0.5 scores calculated against the gold edits, which were also created by the M2 scorer. We are aware that there are other tools to consider for word-level edit calculation such as ERRANT (Bryant et al., 2017), but we did not use them as they require additional dependencies to work for Arabic. Input Gender Identification Our sequence-to-sequence model does not explicitly identify the g"
2020.gebnlp-1.12,E17-1101,0,0.0254015,"some Arabic linguistic facts related to grammatical gender. Section 4 introduces our model for joint gender reinflection and describes the encoder-decoder architecture. Then, we present the experimental setup in Section 5 and discuss the results in Section 6. An error analysis is given in Section 7. We conclude and present future work in Section 8. 2 Related Work Many NLP systems have the ability to embed and amplify societal (gender, racial, religious, etc.) biases across a variety of core tasks such as coreference resolution (Rudinger et al., 2018; Zhao et al., 2018a), machine translation (Rabinovich et al., 2017; Vanmassenhove et al., 2018; Font and Costa-jussà, 2019; Moryossef et al., 2019; Stanovsky et al., 2019; Stafanoviˇcs et al., 2020; Gonen and Webster, 2020), named entity recognition (Mehrabi et al., 2019), dialogue systems (Dinan et al., 2019), and language modeling (Lu et al., 2018; Bordia and Bowman, 2019). For the case of gender bias, various research efforts have shown that this could be caused by either human-generated training datasets (Font and Costa-jussà, 2019; Habash et al., 2019), pre-trained word embeddings (Bolukbasi et al., 2016; Zhao et al., 2017; Caliskan et al., 2017; Manzin"
2020.gebnlp-1.12,N18-2002,0,0.0458667,"Missing"
2020.gebnlp-1.12,2020.wmt-1.73,0,0.0349747,"Missing"
2020.gebnlp-1.12,P19-1164,0,0.0147204,"der reinflection and describes the encoder-decoder architecture. Then, we present the experimental setup in Section 5 and discuss the results in Section 6. An error analysis is given in Section 7. We conclude and present future work in Section 8. 2 Related Work Many NLP systems have the ability to embed and amplify societal (gender, racial, religious, etc.) biases across a variety of core tasks such as coreference resolution (Rudinger et al., 2018; Zhao et al., 2018a), machine translation (Rabinovich et al., 2017; Vanmassenhove et al., 2018; Font and Costa-jussà, 2019; Moryossef et al., 2019; Stanovsky et al., 2019; Stafanoviˇcs et al., 2020; Gonen and Webster, 2020), named entity recognition (Mehrabi et al., 2019), dialogue systems (Dinan et al., 2019), and language modeling (Lu et al., 2018; Bordia and Bowman, 2019). For the case of gender bias, various research efforts have shown that this could be caused by either human-generated training datasets (Font and Costa-jussà, 2019; Habash et al., 2019), pre-trained word embeddings (Bolukbasi et al., 2016; Zhao et al., 2017; Caliskan et al., 2017; Manzini et al., 2019), or language models (Kurita et al., 2019; Zhao et al., 2019). To mitigate this problem,"
2020.gebnlp-1.12,W18-5816,1,0.835155,"the decoder (d) hidden state ht , the context vector ct , and the embedding of the predicted symbol from the previous (d) time step eyˆt−1 to create vector zt = [ht ; ct ; eyˆt−1 ; eg ] ∈ RH+2H+E+J . We finally project zt to a vector of size |Vy |followed by a softmax layer to model the distribution over the target vocabulary PVy (ˆ yt ) = softmax (Wb zt + bb ). 143 Linguistic Features and Word Embeddings We explore adding word-level morphological features as well as pre-trained distributed word representations to the character embeddings. We use the CALIMAStar Arabic morphological analyzer (Taji et al., 2018) to obtain word-level functional gender features (Alkuhlani and Habash, 2011).5 We represent the morphological features for word wj as a four-dimension one-hot vector µwj ∈ R4 . Each element of this one-hot vector represents whether the word wj is masculine or feminine as well as if the analysis was obtained with or without spelling backoff. We use FastText (Bojanowski et al., 2017) to learn distributed word representations and we denote the FastText word embedding for word wj as ρwj ∈ RF . Similarly to Watson et al. (2018), we added the word-level features to the character embeddings only on"
2020.gebnlp-1.12,D18-1334,0,0.0216965,"acts related to grammatical gender. Section 4 introduces our model for joint gender reinflection and describes the encoder-decoder architecture. Then, we present the experimental setup in Section 5 and discuss the results in Section 6. An error analysis is given in Section 7. We conclude and present future work in Section 8. 2 Related Work Many NLP systems have the ability to embed and amplify societal (gender, racial, religious, etc.) biases across a variety of core tasks such as coreference resolution (Rudinger et al., 2018; Zhao et al., 2018a), machine translation (Rabinovich et al., 2017; Vanmassenhove et al., 2018; Font and Costa-jussà, 2019; Moryossef et al., 2019; Stanovsky et al., 2019; Stafanoviˇcs et al., 2020; Gonen and Webster, 2020), named entity recognition (Mehrabi et al., 2019), dialogue systems (Dinan et al., 2019), and language modeling (Lu et al., 2018; Bordia and Bowman, 2019). For the case of gender bias, various research efforts have shown that this could be caused by either human-generated training datasets (Font and Costa-jussà, 2019; Habash et al., 2019), pre-trained word embeddings (Bolukbasi et al., 2016; Zhao et al., 2017; Caliskan et al., 2017; Manzini et al., 2019), or language"
2020.gebnlp-1.12,D18-1097,1,0.931313,"rundkiewicz et al., 2019) and morphological reinflection tasks (Faruqui et al., 2016; Kann and Schütze, 2016; Aharoni and Goldberg, 2017). Many of these problems are modeled on the word-level, however, such models usually require large amounts of training data to achieve good results. Character-level sequence-to-sequence models can be superior in mitigating the lack of training data and in dealing with subtle morphological reinflection. Further, pre-trained distributed word representations have also shown to be helpful if integrated properly within character-level sequence-to-sequence models (Watson et al., 2018). We formulate the gender reinflection problem as a user-aware grammatical error correction (UGEC) task at the character-level. We also explore leveraging linguistic knowledge on the word-level as well as pre-trained word embeddings to enhance the performance of the model. 4.2 Model Architecture Given an input sequence x1:n ∈ Vx containing k words w1:k ∈ Vw , a gender-reinflected output sequence y1:m ∈ Vy , and a target gender g ∈ {F, M }, the goal is to model an auto-regressive distribution which is defined over the target vocabulary:4 PVy (y1:m |x1:n , g) = m Y P (yt |y1:t−1 , x1:n , g; θ);"
2020.gebnlp-1.12,D17-1323,0,0.0292491,"a), machine translation (Rabinovich et al., 2017; Vanmassenhove et al., 2018; Font and Costa-jussà, 2019; Moryossef et al., 2019; Stanovsky et al., 2019; Stafanoviˇcs et al., 2020; Gonen and Webster, 2020), named entity recognition (Mehrabi et al., 2019), dialogue systems (Dinan et al., 2019), and language modeling (Lu et al., 2018; Bordia and Bowman, 2019). For the case of gender bias, various research efforts have shown that this could be caused by either human-generated training datasets (Font and Costa-jussà, 2019; Habash et al., 2019), pre-trained word embeddings (Bolukbasi et al., 2016; Zhao et al., 2017; Caliskan et al., 2017; Manzini et al., 2019), or language models (Kurita et al., 2019; Zhao et al., 2019). To mitigate this problem, several researchers 2 3 Arabic transliteration is in the HSB scheme (Habash et al., 2007). https://github.com/CAMeL-Lab/gender-reinflection 140 proposed approaches in which they focus mainly on debiasing word embeddings (Bolukbasi et al., 2016; Zhao et al., 2018b; Gonen and Goldberg, 2019) or using counterfactual data augmentation techniques (Lu et al., 2018; Zhao et al., 2018a; Zmigrod et al., 2019; Hall Maudslay et al., 2019). Most of the solutions were mainl"
2020.gebnlp-1.12,N18-2003,0,0.0510292,"ome related work. In Section 3, we present some Arabic linguistic facts related to grammatical gender. Section 4 introduces our model for joint gender reinflection and describes the encoder-decoder architecture. Then, we present the experimental setup in Section 5 and discuss the results in Section 6. An error analysis is given in Section 7. We conclude and present future work in Section 8. 2 Related Work Many NLP systems have the ability to embed and amplify societal (gender, racial, religious, etc.) biases across a variety of core tasks such as coreference resolution (Rudinger et al., 2018; Zhao et al., 2018a), machine translation (Rabinovich et al., 2017; Vanmassenhove et al., 2018; Font and Costa-jussà, 2019; Moryossef et al., 2019; Stanovsky et al., 2019; Stafanoviˇcs et al., 2020; Gonen and Webster, 2020), named entity recognition (Mehrabi et al., 2019), dialogue systems (Dinan et al., 2019), and language modeling (Lu et al., 2018; Bordia and Bowman, 2019). For the case of gender bias, various research efforts have shown that this could be caused by either human-generated training datasets (Font and Costa-jussà, 2019; Habash et al., 2019), pre-trained word embeddings (Bolukbasi et al., 2016;"
2020.gebnlp-1.12,D18-1521,0,0.0642243,"ome related work. In Section 3, we present some Arabic linguistic facts related to grammatical gender. Section 4 introduces our model for joint gender reinflection and describes the encoder-decoder architecture. Then, we present the experimental setup in Section 5 and discuss the results in Section 6. An error analysis is given in Section 7. We conclude and present future work in Section 8. 2 Related Work Many NLP systems have the ability to embed and amplify societal (gender, racial, religious, etc.) biases across a variety of core tasks such as coreference resolution (Rudinger et al., 2018; Zhao et al., 2018a), machine translation (Rabinovich et al., 2017; Vanmassenhove et al., 2018; Font and Costa-jussà, 2019; Moryossef et al., 2019; Stanovsky et al., 2019; Stafanoviˇcs et al., 2020; Gonen and Webster, 2020), named entity recognition (Mehrabi et al., 2019), dialogue systems (Dinan et al., 2019), and language modeling (Lu et al., 2018; Bordia and Bowman, 2019). For the case of gender bias, various research efforts have shown that this could be caused by either human-generated training datasets (Font and Costa-jussà, 2019; Habash et al., 2019), pre-trained word embeddings (Bolukbasi et al., 2016;"
2020.gebnlp-1.12,N19-1064,0,0.0142373,"Moryossef et al., 2019; Stanovsky et al., 2019; Stafanoviˇcs et al., 2020; Gonen and Webster, 2020), named entity recognition (Mehrabi et al., 2019), dialogue systems (Dinan et al., 2019), and language modeling (Lu et al., 2018; Bordia and Bowman, 2019). For the case of gender bias, various research efforts have shown that this could be caused by either human-generated training datasets (Font and Costa-jussà, 2019; Habash et al., 2019), pre-trained word embeddings (Bolukbasi et al., 2016; Zhao et al., 2017; Caliskan et al., 2017; Manzini et al., 2019), or language models (Kurita et al., 2019; Zhao et al., 2019). To mitigate this problem, several researchers 2 3 Arabic transliteration is in the HSB scheme (Habash et al., 2007). https://github.com/CAMeL-Lab/gender-reinflection 140 proposed approaches in which they focus mainly on debiasing word embeddings (Bolukbasi et al., 2016; Zhao et al., 2018b; Gonen and Goldberg, 2019) or using counterfactual data augmentation techniques (Lu et al., 2018; Zhao et al., 2018a; Zmigrod et al., 2019; Hall Maudslay et al., 2019). Most of the solutions were mainly proposed to reduce gender bias in English and may not work as well when it comes to morphologically rich"
2020.gebnlp-1.12,P19-1161,0,0.017333,"al., 2019), pre-trained word embeddings (Bolukbasi et al., 2016; Zhao et al., 2017; Caliskan et al., 2017; Manzini et al., 2019), or language models (Kurita et al., 2019; Zhao et al., 2019). To mitigate this problem, several researchers 2 3 Arabic transliteration is in the HSB scheme (Habash et al., 2007). https://github.com/CAMeL-Lab/gender-reinflection 140 proposed approaches in which they focus mainly on debiasing word embeddings (Bolukbasi et al., 2016; Zhao et al., 2018b; Gonen and Goldberg, 2019) or using counterfactual data augmentation techniques (Lu et al., 2018; Zhao et al., 2018a; Zmigrod et al., 2019; Hall Maudslay et al., 2019). Most of the solutions were mainly proposed to reduce gender bias in English and may not work as well when it comes to morphologically rich languages. Nevertheless, there have been recent studies that explored the gender bias problem in languages other than English. Zhao et al. (2020) studied gender bias which is exhibited by multilingual embeddings in four languages (English, German, French, and Spanish) and demonstrated that such bias can impact cross-lingual transfer learning tasks. Zmigrod et al. (2019) used a counterfactual data augmentation approach and deve"
2020.lrec-1.508,L18-1577,0,0.0183933,".com/ 4130 2. Related Work Automatic DA processing has been attracting a considerable amount of research in NLP (Shoufan and Al-Ameri, 2015), facilitated by the newly developed monolingual and multilingual dialectal corpora. Several mono-dialectal corpora covering different Arabic dialects at different granularity levels (region, country and city levels) were built and made available (McNeil and Faiza, 2011; Zaidan and Callison-Burch, 2011; Zbib et al., 2012; Cotterell and Callison-Burch, 2014; Salama et al., 2014; Jeblee et al., 2014; Al-Badrashiny and Diab, 2016; Zaghouani and Charfi, 2018; Abdul-Mageed et al., 2018). As for dialect-to-dialect parallel corpora, Bouamor et al. (2018) presented the MADAR Corpus, a large-scale collection of parallel sentences covering the dialects of 25 Arab cities alongside the English, French and MSA parallel texts. This resource was a commissioned translation of the Basic Traveling Expression Corpus (BTEC) (Takezawa et al., 2007) sentences from English and French to the different dialects. It includes two corpora. The first corpus (Corpus-26) consists of 2,000 sentences translated into 25 Arab city dialects in parallel. The second corpus (Corpus6) has 10,000 additional se"
2020.lrec-1.508,C16-1115,0,0.0123356,"pelling conventionalization”. 2 http://resources.camel-lab.com/ 4130 2. Related Work Automatic DA processing has been attracting a considerable amount of research in NLP (Shoufan and Al-Ameri, 2015), facilitated by the newly developed monolingual and multilingual dialectal corpora. Several mono-dialectal corpora covering different Arabic dialects at different granularity levels (region, country and city levels) were built and made available (McNeil and Faiza, 2011; Zaidan and Callison-Burch, 2011; Zbib et al., 2012; Cotterell and Callison-Burch, 2014; Salama et al., 2014; Jeblee et al., 2014; Al-Badrashiny and Diab, 2016; Zaghouani and Charfi, 2018; Abdul-Mageed et al., 2018). As for dialect-to-dialect parallel corpora, Bouamor et al. (2018) presented the MADAR Corpus, a large-scale collection of parallel sentences covering the dialects of 25 Arab cities alongside the English, French and MSA parallel texts. This resource was a commissioned translation of the Basic Traveling Expression Corpus (BTEC) (Takezawa et al., 2007) sentences from English and French to the different dialects. It includes two corpora. The first corpus (Corpus-26) consists of 2,000 sentences translated into 25 Arab city dialects in parall"
2020.lrec-1.508,W14-1604,1,0.86826,"ts have been introduced to modernize and extend Arabic orthography and develop orthographic conventions for Arabic dialects. Habash et al. (2012) introduced the concept of Conventional Orthography for Dialectal Arabic (CODA), the very first effort to present a set of guidelines and exception lists for Egyptian Arabic orthography. Although the first CODA was developed for Egyptian Arabic, it was designed with extensibility in mind. As Egyptian CODA began to be integrated into several Egyptian Arabic resources (Maamouri et al., 2014; Diab et al., 2014; Pasha et al., 2014; Eskander et al., 2013; Al-Badrashiny et al., 2014), other efforts began to extend CODA’s coverage into new dialects. 2014 saw the creation of two additional guidelines, Tunisian CODA (Zribi et al., 2014) and Palestinian CODA (Jarrar et al., 2014). Using a variant of CODA adopted for speech recognition, Ali et al. (2014) demonstrated reduced out of vocabulary (OOV) and perplexity for texts rendered in CODA. More dialects have followed since then, with the creation of Algerian CODA (Saadane and Habash, 2015), Moroccan CODA and Yemeni CODA(AlShargi et al., 2016), and Gulf CODA (Khalifa et al., 2018). More recently, CODA has garnered the interest"
2020.lrec-1.508,2014.iwslt-papers.1,0,0.00968304,"d exception lists for Egyptian Arabic orthography. Although the first CODA was developed for Egyptian Arabic, it was designed with extensibility in mind. As Egyptian CODA began to be integrated into several Egyptian Arabic resources (Maamouri et al., 2014; Diab et al., 2014; Pasha et al., 2014; Eskander et al., 2013; Al-Badrashiny et al., 2014), other efforts began to extend CODA’s coverage into new dialects. 2014 saw the creation of two additional guidelines, Tunisian CODA (Zribi et al., 2014) and Palestinian CODA (Jarrar et al., 2014). Using a variant of CODA adopted for speech recognition, Ali et al. (2014) demonstrated reduced out of vocabulary (OOV) and perplexity for texts rendered in CODA. More dialects have followed since then, with the creation of Algerian CODA (Saadane and Habash, 2015), Moroccan CODA and Yemeni CODA(AlShargi et al., 2016), and Gulf CODA (Khalifa et al., 2018). More recently, CODA has garnered the interest of literacy, pedagogy, and heritage specialists as a convenient orthographic standard, such as a website that teaches Palestinian Arabic,3 amongst others. These efforts were unified in overall principles, namely in how to spell open class words. But during creation of t"
2020.lrec-1.508,L18-1535,1,0.896504,"ional Orthography for Dialectal Arabic (CODA) (Habash et al., 2012; Jarrar et al., 2014; Zribi et al., 2014; Saadane and Habash, 2015; Khalifa et al., 2016; Habash et al., 2018). The majority of resources involving CODA annotation consider it a side task to efforts like morphological disambiguation, diacritization and lemmatization, as opposed to being the main target task (CODA for CODA). In this paper, we explore and report on the task of CODA annotation, i.e., spelling correction into the CODA convention.1 We work with a unique corpus of parallel multiple Arabic dialects, the MADAR Corpus (Bouamor et al., 2018), focusing on five cities: Beirut, Cairo, Doha, Rabat and Tunis. Our contributions are threefold. First, we created a parallel CODA version of a parallel multi-dialectal corpus, a unique resource, first of its kind. Second, we describe and follow a bootstrapping technique for CODA creation, and we report on its speed and initial accuracy under different pre-existing resource settings. Finally, we quantify the degrees of similarity across the dialects we work on using the annotated data in both Raw and CODA spaces. As expected CODA reduces the overall vocabulary within dialects and increases th"
2020.lrec-1.508,W19-4622,1,0.759668,"ext normalization for Arabic dialects. We present results on a bootstrapping technique we use to speed up the CODA annotation, as well as on the degree of similarity across the dialects before and after CODA annotation. Keywords: Dialects, Corpora, Spelling Correction, Conventional Orthography for Dialectal Arabic 1. Introduction While the standard form of any language is the variety most likely to receive attention from natural language processing (NLP) researchers and developers, more research is on the rise to address the needs of non-standard varieties and dialects (Zampieri et al., 2019; Bouamor et al., 2019). The Arabic language, spoken by over 400 million people, is in fact a collective of multiple variants, among which Modern Standard Arabic (MSA) is considered the official primarily written variety of education and culture, even though it is not the native language of any speakers. The other variants are known collectively as Dialectal Arabic (DA), but often classified regionally (as Egyptian, North African, Levantine, Gulf, Yemeni) or sub-regionally (i.e, Tunisian, Moroccan, Lebanese, and Qatari). Arabic dialects are the true native languages historically connected to Classical Arabic and man"
2020.lrec-1.508,cotterell-callison-burch-2014-multi,0,0.013415,"rpose, we find the term “spelling correction” in a NLP context clearer than “spelling conventionalization”. 2 http://resources.camel-lab.com/ 4130 2. Related Work Automatic DA processing has been attracting a considerable amount of research in NLP (Shoufan and Al-Ameri, 2015), facilitated by the newly developed monolingual and multilingual dialectal corpora. Several mono-dialectal corpora covering different Arabic dialects at different granularity levels (region, country and city levels) were built and made available (McNeil and Faiza, 2011; Zaidan and Callison-Burch, 2011; Zbib et al., 2012; Cotterell and Callison-Burch, 2014; Salama et al., 2014; Jeblee et al., 2014; Al-Badrashiny and Diab, 2016; Zaghouani and Charfi, 2018; Abdul-Mageed et al., 2018). As for dialect-to-dialect parallel corpora, Bouamor et al. (2018) presented the MADAR Corpus, a large-scale collection of parallel sentences covering the dialects of 25 Arab cities alongside the English, French and MSA parallel texts. This resource was a commissioned translation of the Basic Traveling Expression Corpus (BTEC) (Takezawa et al., 2007) sentences from English and French to the different dialects. It includes two corpora. The first corpus (Corpus-26) con"
2020.lrec-1.508,diab-etal-2014-tharwa,1,0.838048,"and applications. To alleviate this bottleneck, several efforts have been introduced to modernize and extend Arabic orthography and develop orthographic conventions for Arabic dialects. Habash et al. (2012) introduced the concept of Conventional Orthography for Dialectal Arabic (CODA), the very first effort to present a set of guidelines and exception lists for Egyptian Arabic orthography. Although the first CODA was developed for Egyptian Arabic, it was designed with extensibility in mind. As Egyptian CODA began to be integrated into several Egyptian Arabic resources (Maamouri et al., 2014; Diab et al., 2014; Pasha et al., 2014; Eskander et al., 2013; Al-Badrashiny et al., 2014), other efforts began to extend CODA’s coverage into new dialects. 2014 saw the creation of two additional guidelines, Tunisian CODA (Zribi et al., 2014) and Palestinian CODA (Jarrar et al., 2014). Using a variant of CODA adopted for speech recognition, Ali et al. (2014) demonstrated reduced out of vocabulary (OOV) and perplexity for texts rendered in CODA. More dialects have followed since then, with the creation of Algerian CODA (Saadane and Habash, 2015), Moroccan CODA and Yemeni CODA(AlShargi et al., 2016), and Gulf CO"
2020.lrec-1.508,N13-1066,1,0.929863,"ttleneck, several efforts have been introduced to modernize and extend Arabic orthography and develop orthographic conventions for Arabic dialects. Habash et al. (2012) introduced the concept of Conventional Orthography for Dialectal Arabic (CODA), the very first effort to present a set of guidelines and exception lists for Egyptian Arabic orthography. Although the first CODA was developed for Egyptian Arabic, it was designed with extensibility in mind. As Egyptian CODA began to be integrated into several Egyptian Arabic resources (Maamouri et al., 2014; Diab et al., 2014; Pasha et al., 2014; Eskander et al., 2013; Al-Badrashiny et al., 2014), other efforts began to extend CODA’s coverage into new dialects. 2014 saw the creation of two additional guidelines, Tunisian CODA (Zribi et al., 2014) and Palestinian CODA (Jarrar et al., 2014). Using a variant of CODA adopted for speech recognition, Ali et al. (2014) demonstrated reduced out of vocabulary (OOV) and perplexity for texts rendered in CODA. More dialects have followed since then, with the creation of Algerian CODA (Saadane and Habash, 2015), Moroccan CODA and Yemeni CODA(AlShargi et al., 2016), and Gulf CODA (Khalifa et al., 2018). More recently, C"
2020.lrec-1.508,habash-etal-2012-conventional,1,0.893253,"he degree of sparsity in the data. Such noise can be handled using modeling techniques that normalize and cluster variants if DA is the input to the system, e.g. in machine translation from dialects to other languages. However, when the dialect is the target output, as in speech recognition systems (Ali, 2018), or machine translation into the dialects (Erdmann et al., 2017), evaluation and thus optimization may struggle. A number of efforts in Arabic NLP have argued for the creation of a common convention for Arabic dialect spelling, named Conventional Orthography for Dialectal Arabic (CODA) (Habash et al., 2012; Jarrar et al., 2014; Zribi et al., 2014; Saadane and Habash, 2015; Khalifa et al., 2016; Habash et al., 2018). The majority of resources involving CODA annotation consider it a side task to efforts like morphological disambiguation, diacritization and lemmatization, as opposed to being the main target task (CODA for CODA). In this paper, we explore and report on the task of CODA annotation, i.e., spelling correction into the CODA convention.1 We work with a unique corpus of parallel multiple Arabic dialects, the MADAR Corpus (Bouamor et al., 2018), focusing on five cities: Beirut, Cairo, Doh"
2020.lrec-1.508,L18-1574,1,0.897446,"as Dialectal Arabic (DA), but often classified regionally (as Egyptian, North African, Levantine, Gulf, Yemeni) or sub-regionally (i.e, Tunisian, Moroccan, Lebanese, and Qatari). Arabic dialects are the true native languages historically connected to Classical Arabic and many other regional languages. These dialects are primarily spoken, though their dominance on social media is on the rise. Lacking official recognition, they do not have standard orthographies. As a result, dialectal text tends to have a lot of variety and noise (from a computational linguistics point of view). For instance, Habash et al. (2018) reported 27 different spellings for the Egyptian Arabic utterance /mabiPulha:S/ “he does not say it”, that vary in terms of etymological or phonetic spelling decisions. This high degree of noise is a major challenge for NLP system development as it increases the degree of sparsity in the data. Such noise can be handled using modeling techniques that normalize and cluster variants if DA is the input to the system, e.g. in machine translation from dialects to other languages. However, when the dialect is the target output, as in speech recognition systems (Ali, 2018), or machine translation int"
2020.lrec-1.508,W14-3603,1,0.959932,"in the data. Such noise can be handled using modeling techniques that normalize and cluster variants if DA is the input to the system, e.g. in machine translation from dialects to other languages. However, when the dialect is the target output, as in speech recognition systems (Ali, 2018), or machine translation into the dialects (Erdmann et al., 2017), evaluation and thus optimization may struggle. A number of efforts in Arabic NLP have argued for the creation of a common convention for Arabic dialect spelling, named Conventional Orthography for Dialectal Arabic (CODA) (Habash et al., 2012; Jarrar et al., 2014; Zribi et al., 2014; Saadane and Habash, 2015; Khalifa et al., 2016; Habash et al., 2018). The majority of resources involving CODA annotation consider it a side task to efforts like morphological disambiguation, diacritization and lemmatization, as opposed to being the main target task (CODA for CODA). In this paper, we explore and report on the task of CODA annotation, i.e., spelling correction into the CODA convention.1 We work with a unique corpus of parallel multiple Arabic dialects, the MADAR Corpus (Bouamor et al., 2018), focusing on five cities: Beirut, Cairo, Doha, Rabat and Tunis. O"
2020.lrec-1.508,W14-3627,1,0.840234,"ntext clearer than “spelling conventionalization”. 2 http://resources.camel-lab.com/ 4130 2. Related Work Automatic DA processing has been attracting a considerable amount of research in NLP (Shoufan and Al-Ameri, 2015), facilitated by the newly developed monolingual and multilingual dialectal corpora. Several mono-dialectal corpora covering different Arabic dialects at different granularity levels (region, country and city levels) were built and made available (McNeil and Faiza, 2011; Zaidan and Callison-Burch, 2011; Zbib et al., 2012; Cotterell and Callison-Burch, 2014; Salama et al., 2014; Jeblee et al., 2014; Al-Badrashiny and Diab, 2016; Zaghouani and Charfi, 2018; Abdul-Mageed et al., 2018). As for dialect-to-dialect parallel corpora, Bouamor et al. (2018) presented the MADAR Corpus, a large-scale collection of parallel sentences covering the dialects of 25 Arab cities alongside the English, French and MSA parallel texts. This resource was a commissioned translation of the Basic Traveling Expression Corpus (BTEC) (Takezawa et al., 2007) sentences from English and French to the different dialects. It includes two corpora. The first corpus (Corpus-26) consists of 2,000 sentences translated into 2"
2020.lrec-1.508,L16-1679,1,0.896251,"hat normalize and cluster variants if DA is the input to the system, e.g. in machine translation from dialects to other languages. However, when the dialect is the target output, as in speech recognition systems (Ali, 2018), or machine translation into the dialects (Erdmann et al., 2017), evaluation and thus optimization may struggle. A number of efforts in Arabic NLP have argued for the creation of a common convention for Arabic dialect spelling, named Conventional Orthography for Dialectal Arabic (CODA) (Habash et al., 2012; Jarrar et al., 2014; Zribi et al., 2014; Saadane and Habash, 2015; Khalifa et al., 2016; Habash et al., 2018). The majority of resources involving CODA annotation consider it a side task to efforts like morphological disambiguation, diacritization and lemmatization, as opposed to being the main target task (CODA for CODA). In this paper, we explore and report on the task of CODA annotation, i.e., spelling correction into the CODA convention.1 We work with a unique corpus of parallel multiple Arabic dialects, the MADAR Corpus (Bouamor et al., 2018), focusing on five cities: Beirut, Cairo, Doha, Rabat and Tunis. Our contributions are threefold. First, we created a parallel CODA ve"
2020.lrec-1.508,L18-1607,1,0.958153,"sha et al., 2014; Eskander et al., 2013; Al-Badrashiny et al., 2014), other efforts began to extend CODA’s coverage into new dialects. 2014 saw the creation of two additional guidelines, Tunisian CODA (Zribi et al., 2014) and Palestinian CODA (Jarrar et al., 2014). Using a variant of CODA adopted for speech recognition, Ali et al. (2014) demonstrated reduced out of vocabulary (OOV) and perplexity for texts rendered in CODA. More dialects have followed since then, with the creation of Algerian CODA (Saadane and Habash, 2015), Moroccan CODA and Yemeni CODA(AlShargi et al., 2016), and Gulf CODA (Khalifa et al., 2018). More recently, CODA has garnered the interest of literacy, pedagogy, and heritage specialists as a convenient orthographic standard, such as a website that teaches Palestinian Arabic,3 amongst others. These efforts were unified in overall principles, namely in how to spell open class words. But during creation of these CODA extensions, each dialect tended to curate its own list of exceptional spellings for closed class words. With the growing number of dialects being incorporated, Habash et al. (2018) presented a more Unified Guidelines and Resources for Arabic Dialect Orthography — dubbed C"
2020.lrec-1.508,maamouri-etal-2014-developing,1,0.845875,"ing efficient NLP tools and applications. To alleviate this bottleneck, several efforts have been introduced to modernize and extend Arabic orthography and develop orthographic conventions for Arabic dialects. Habash et al. (2012) introduced the concept of Conventional Orthography for Dialectal Arabic (CODA), the very first effort to present a set of guidelines and exception lists for Egyptian Arabic orthography. Although the first CODA was developed for Egyptian Arabic, it was designed with extensibility in mind. As Egyptian CODA began to be integrated into several Egyptian Arabic resources (Maamouri et al., 2014; Diab et al., 2014; Pasha et al., 2014; Eskander et al., 2013; Al-Badrashiny et al., 2014), other efforts began to extend CODA’s coverage into new dialects. 2014 saw the creation of two additional guidelines, Tunisian CODA (Zribi et al., 2014) and Palestinian CODA (Jarrar et al., 2014). Using a variant of CODA adopted for speech recognition, Ali et al. (2014) demonstrated reduced out of vocabulary (OOV) and perplexity for texts rendered in CODA. More dialects have followed since then, with the creation of Algerian CODA (Saadane and Habash, 2015), Moroccan CODA and Yemeni CODA(AlShargi et al.,"
2020.lrec-1.508,W14-3605,1,0.878926,"losed class spelling in more detail and unifying the CODA creation process. CODA* has since been used to represent over two dozen Arabic dialects. It is worth noting that in recent years, the problem of spell checking and spelling error correction for Arabic has been investigated in a number of research effort (Attia et al., 2016; Watson et al., 2018). The QALB (Qatar Arabic Language Bank) project (Zaghouani et al., 2014) aimed at building an annotated corpus of manually corrected MSA text for building automatic correction tools, and it was used in two shared tasks on MSA spelling correction (Mohit et al., 2014; Rozovskaya et al., 2015). 3. 3.1. CODA: Conventional Orthography for Dialectal Arabic The Orthography of Arabic and its Dialects As mentioned in the introduction, Arabic is a family of variants, among which MSA is the official standard language. However, MSA is not the native language of any speakers of Arabic. In unscripted situations where spoken MSA would typically be required (such as talk shows on TV), speakers usually resort to repeated code-switching between their dialects and MSA (Abu-Melhim, 1991; Bassiouney, 2009). Arabic dialects vary phonologically, lexically, and morphologically"
2020.lrec-1.508,pasha-etal-2014-madamira,1,0.846361,"To alleviate this bottleneck, several efforts have been introduced to modernize and extend Arabic orthography and develop orthographic conventions for Arabic dialects. Habash et al. (2012) introduced the concept of Conventional Orthography for Dialectal Arabic (CODA), the very first effort to present a set of guidelines and exception lists for Egyptian Arabic orthography. Although the first CODA was developed for Egyptian Arabic, it was designed with extensibility in mind. As Egyptian CODA began to be integrated into several Egyptian Arabic resources (Maamouri et al., 2014; Diab et al., 2014; Pasha et al., 2014; Eskander et al., 2013; Al-Badrashiny et al., 2014), other efforts began to extend CODA’s coverage into new dialects. 2014 saw the creation of two additional guidelines, Tunisian CODA (Zribi et al., 2014) and Palestinian CODA (Jarrar et al., 2014). Using a variant of CODA adopted for speech recognition, Ali et al. (2014) demonstrated reduced out of vocabulary (OOV) and perplexity for texts rendered in CODA. More dialects have followed since then, with the creation of Algerian CODA (Saadane and Habash, 2015), Moroccan CODA and Yemeni CODA(AlShargi et al., 2016), and Gulf CODA (Khalifa et al.,"
2020.lrec-1.508,W15-3204,1,0.771221,"in more detail and unifying the CODA creation process. CODA* has since been used to represent over two dozen Arabic dialects. It is worth noting that in recent years, the problem of spell checking and spelling error correction for Arabic has been investigated in a number of research effort (Attia et al., 2016; Watson et al., 2018). The QALB (Qatar Arabic Language Bank) project (Zaghouani et al., 2014) aimed at building an annotated corpus of manually corrected MSA text for building automatic correction tools, and it was used in two shared tasks on MSA spelling correction (Mohit et al., 2014; Rozovskaya et al., 2015). 3. 3.1. CODA: Conventional Orthography for Dialectal Arabic The Orthography of Arabic and its Dialects As mentioned in the introduction, Arabic is a family of variants, among which MSA is the official standard language. However, MSA is not the native language of any speakers of Arabic. In unscripted situations where spoken MSA would typically be required (such as talk shows on TV), speakers usually resort to repeated code-switching between their dialects and MSA (Abu-Melhim, 1991; Bassiouney, 2009). Arabic dialects vary phonologically, lexically, and morphologically from MSA and from each ot"
2020.lrec-1.508,W15-3208,1,0.943213,"sing modeling techniques that normalize and cluster variants if DA is the input to the system, e.g. in machine translation from dialects to other languages. However, when the dialect is the target output, as in speech recognition systems (Ali, 2018), or machine translation into the dialects (Erdmann et al., 2017), evaluation and thus optimization may struggle. A number of efforts in Arabic NLP have argued for the creation of a common convention for Arabic dialect spelling, named Conventional Orthography for Dialectal Arabic (CODA) (Habash et al., 2012; Jarrar et al., 2014; Zribi et al., 2014; Saadane and Habash, 2015; Khalifa et al., 2016; Habash et al., 2018). The majority of resources involving CODA annotation consider it a side task to efforts like morphological disambiguation, diacritization and lemmatization, as opposed to being the main target task (CODA for CODA). In this paper, we explore and report on the task of CODA annotation, i.e., spelling correction into the CODA convention.1 We work with a unique corpus of parallel multiple Arabic dialects, the MADAR Corpus (Bouamor et al., 2018), focusing on five cities: Beirut, Cairo, Doha, Rabat and Tunis. Our contributions are threefold. First, we crea"
2020.lrec-1.508,salama-etal-2014-youdacc,1,0.839767,"rrection” in a NLP context clearer than “spelling conventionalization”. 2 http://resources.camel-lab.com/ 4130 2. Related Work Automatic DA processing has been attracting a considerable amount of research in NLP (Shoufan and Al-Ameri, 2015), facilitated by the newly developed monolingual and multilingual dialectal corpora. Several mono-dialectal corpora covering different Arabic dialects at different granularity levels (region, country and city levels) were built and made available (McNeil and Faiza, 2011; Zaidan and Callison-Burch, 2011; Zbib et al., 2012; Cotterell and Callison-Burch, 2014; Salama et al., 2014; Jeblee et al., 2014; Al-Badrashiny and Diab, 2016; Zaghouani and Charfi, 2018; Abdul-Mageed et al., 2018). As for dialect-to-dialect parallel corpora, Bouamor et al. (2018) presented the MADAR Corpus, a large-scale collection of parallel sentences covering the dialects of 25 Arab cities alongside the English, French and MSA parallel texts. This resource was a commissioned translation of the Basic Traveling Expression Corpus (BTEC) (Takezawa et al., 2007) sentences from English and French to the different dialects. It includes two corpora. The first corpus (Corpus-26) consists of 2,000 senten"
2020.lrec-1.508,W15-3205,0,0.0127336,"that the term “spelling correction” evokes a claim of an “official standard,” we observe that there are no authorities interested in creating such a standard in the Arab world. And given the growing number of NLP papers and tools working with CODA, it is slowly becoming the de facto standard, at least for NLP. Finally, for the sake of clarity of purpose, we find the term “spelling correction” in a NLP context clearer than “spelling conventionalization”. 2 http://resources.camel-lab.com/ 4130 2. Related Work Automatic DA processing has been attracting a considerable amount of research in NLP (Shoufan and Al-Ameri, 2015), facilitated by the newly developed monolingual and multilingual dialectal corpora. Several mono-dialectal corpora covering different Arabic dialects at different granularity levels (region, country and city levels) were built and made available (McNeil and Faiza, 2011; Zaidan and Callison-Burch, 2011; Zbib et al., 2012; Cotterell and Callison-Burch, 2014; Salama et al., 2014; Jeblee et al., 2014; Al-Badrashiny and Diab, 2016; Zaghouani and Charfi, 2018; Abdul-Mageed et al., 2018). As for dialect-to-dialect parallel corpora, Bouamor et al. (2018) presented the MADAR Corpus, a large-scale coll"
2020.lrec-1.508,D18-1097,1,0.800849,"exceptional spellings for closed class words. With the growing number of dialects being incorporated, Habash et al. (2018) presented a more Unified Guidelines and Resources for Arabic Dialect Orthography — dubbed CODA* (CODA-Star) as in for any dialect — specifying closed class spelling in more detail and unifying the CODA creation process. CODA* has since been used to represent over two dozen Arabic dialects. It is worth noting that in recent years, the problem of spell checking and spelling error correction for Arabic has been investigated in a number of research effort (Attia et al., 2016; Watson et al., 2018). The QALB (Qatar Arabic Language Bank) project (Zaghouani et al., 2014) aimed at building an annotated corpus of manually corrected MSA text for building automatic correction tools, and it was used in two shared tasks on MSA spelling correction (Mohit et al., 2014; Rozovskaya et al., 2015). 3. 3.1. CODA: Conventional Orthography for Dialectal Arabic The Orthography of Arabic and its Dialects As mentioned in the introduction, Arabic is a family of variants, among which MSA is the official standard language. However, MSA is not the native language of any speakers of Arabic. In unscripted situat"
2020.lrec-1.508,L18-1111,0,0.0169579,"2 http://resources.camel-lab.com/ 4130 2. Related Work Automatic DA processing has been attracting a considerable amount of research in NLP (Shoufan and Al-Ameri, 2015), facilitated by the newly developed monolingual and multilingual dialectal corpora. Several mono-dialectal corpora covering different Arabic dialects at different granularity levels (region, country and city levels) were built and made available (McNeil and Faiza, 2011; Zaidan and Callison-Burch, 2011; Zbib et al., 2012; Cotterell and Callison-Burch, 2014; Salama et al., 2014; Jeblee et al., 2014; Al-Badrashiny and Diab, 2016; Zaghouani and Charfi, 2018; Abdul-Mageed et al., 2018). As for dialect-to-dialect parallel corpora, Bouamor et al. (2018) presented the MADAR Corpus, a large-scale collection of parallel sentences covering the dialects of 25 Arab cities alongside the English, French and MSA parallel texts. This resource was a commissioned translation of the Basic Traveling Expression Corpus (BTEC) (Takezawa et al., 2007) sentences from English and French to the different dialects. It includes two corpora. The first corpus (Corpus-26) consists of 2,000 sentences translated into 25 Arab city dialects in parallel. The second corpus (Corpu"
2020.lrec-1.508,zaghouani-etal-2014-large,1,0.862129,"of dialects being incorporated, Habash et al. (2018) presented a more Unified Guidelines and Resources for Arabic Dialect Orthography — dubbed CODA* (CODA-Star) as in for any dialect — specifying closed class spelling in more detail and unifying the CODA creation process. CODA* has since been used to represent over two dozen Arabic dialects. It is worth noting that in recent years, the problem of spell checking and spelling error correction for Arabic has been investigated in a number of research effort (Attia et al., 2016; Watson et al., 2018). The QALB (Qatar Arabic Language Bank) project (Zaghouani et al., 2014) aimed at building an annotated corpus of manually corrected MSA text for building automatic correction tools, and it was used in two shared tasks on MSA spelling correction (Mohit et al., 2014; Rozovskaya et al., 2015). 3. 3.1. CODA: Conventional Orthography for Dialectal Arabic The Orthography of Arabic and its Dialects As mentioned in the introduction, Arabic is a family of variants, among which MSA is the official standard language. However, MSA is not the native language of any speakers of Arabic. In unscripted situations where spoken MSA would typically be required (such as talk shows on"
2020.lrec-1.508,P11-2007,0,0.149465,"east for NLP. Finally, for the sake of clarity of purpose, we find the term “spelling correction” in a NLP context clearer than “spelling conventionalization”. 2 http://resources.camel-lab.com/ 4130 2. Related Work Automatic DA processing has been attracting a considerable amount of research in NLP (Shoufan and Al-Ameri, 2015), facilitated by the newly developed monolingual and multilingual dialectal corpora. Several mono-dialectal corpora covering different Arabic dialects at different granularity levels (region, country and city levels) were built and made available (McNeil and Faiza, 2011; Zaidan and Callison-Burch, 2011; Zbib et al., 2012; Cotterell and Callison-Burch, 2014; Salama et al., 2014; Jeblee et al., 2014; Al-Badrashiny and Diab, 2016; Zaghouani and Charfi, 2018; Abdul-Mageed et al., 2018). As for dialect-to-dialect parallel corpora, Bouamor et al. (2018) presented the MADAR Corpus, a large-scale collection of parallel sentences covering the dialects of 25 Arab cities alongside the English, French and MSA parallel texts. This resource was a commissioned translation of the Basic Traveling Expression Corpus (BTEC) (Takezawa et al., 2007) sentences from English and French to the different dialects. It"
2020.lrec-1.508,N12-1006,0,0.0286398,"ke of clarity of purpose, we find the term “spelling correction” in a NLP context clearer than “spelling conventionalization”. 2 http://resources.camel-lab.com/ 4130 2. Related Work Automatic DA processing has been attracting a considerable amount of research in NLP (Shoufan and Al-Ameri, 2015), facilitated by the newly developed monolingual and multilingual dialectal corpora. Several mono-dialectal corpora covering different Arabic dialects at different granularity levels (region, country and city levels) were built and made available (McNeil and Faiza, 2011; Zaidan and Callison-Burch, 2011; Zbib et al., 2012; Cotterell and Callison-Burch, 2014; Salama et al., 2014; Jeblee et al., 2014; Al-Badrashiny and Diab, 2016; Zaghouani and Charfi, 2018; Abdul-Mageed et al., 2018). As for dialect-to-dialect parallel corpora, Bouamor et al. (2018) presented the MADAR Corpus, a large-scale collection of parallel sentences covering the dialects of 25 Arab cities alongside the English, French and MSA parallel texts. This resource was a commissioned translation of the Basic Traveling Expression Corpus (BTEC) (Takezawa et al., 2007) sentences from English and French to the different dialects. It includes two corpo"
2020.lrec-1.508,zribi-etal-2014-conventional,1,0.933241,"ise can be handled using modeling techniques that normalize and cluster variants if DA is the input to the system, e.g. in machine translation from dialects to other languages. However, when the dialect is the target output, as in speech recognition systems (Ali, 2018), or machine translation into the dialects (Erdmann et al., 2017), evaluation and thus optimization may struggle. A number of efforts in Arabic NLP have argued for the creation of a common convention for Arabic dialect spelling, named Conventional Orthography for Dialectal Arabic (CODA) (Habash et al., 2012; Jarrar et al., 2014; Zribi et al., 2014; Saadane and Habash, 2015; Khalifa et al., 2016; Habash et al., 2018). The majority of resources involving CODA annotation consider it a side task to efforts like morphological disambiguation, diacritization and lemmatization, as opposed to being the main target task (CODA for CODA). In this paper, we explore and report on the task of CODA annotation, i.e., spelling correction into the CODA convention.1 We work with a unique corpus of parallel multiple Arabic dialects, the MADAR Corpus (Bouamor et al., 2018), focusing on five cities: Beirut, Cairo, Doha, Rabat and Tunis. Our contributions are"
2020.wanlp-1.9,2020.wanlp-1.26,0,0.0901457,"Missing"
2020.wanlp-1.9,2020.wanlp-1.22,0,0.0733884,"Missing"
2020.wanlp-1.9,bouamor-etal-2014-multidialectal,1,0.809064,"received more attention relatively recently (Harrell, 1962; Cowell, 1964; Badawi, 1973; Brustad, 2000; Holes, 2004). A majority of DA computational efforts have targeted creating resources for country or regionally specific dialects (Gadalla et al., 1997; Diab et al., 2010; Al-Sabbagh and Girju, 2012; Sadat et al., 2014; Sma¨ıli et al., 2014; Jarrar et al., 2016; Khalifa et al., 2016; Al-Twairesh et al., 2018; El-Haj, 2020). The expansion into multi-dialectal data sets and models to identify them was initially done at the regional level (Zaidan and Callison-Burch, 2011; Elfardy et al., 2014; Bouamor et al., 2014; Meftouh et al., 2015). A number of Arabic dialect identification shared tasks were organized as part of the VarDial workshop. These focused on regional varieties such as Egyptian, Gulf, Levantine, and North African based on speech broadcast transcriptions (Malmasi et al., 2016) but also acoustic features (Zampieri et al., 2017) and phonetic features (Zampieri et al., 2018) extracted from raw audio. Althobaiti (2020) presents a recent survey of computational work on Arabic dialects. An early effort for creating finer grained parallel dialectal corpus and lexicon was done under the Multi Arabi"
2020.wanlp-1.9,W19-4622,1,0.341438,"translation. Their data was also used for dialectal identification at the city level (Salameh et al., 2018; Obeid et al., 2019) of 25 Arab cities. One issue with the MADAR data in the context of identification is that it was commissioned and not naturally occurring. Concurrently, larger Twitter-based datasets covering 10-21 countries were also introduced (Mubarak and Darwish, 2014; Abdul-Mageed et al., 2018; Zaghouani and Charfi, 2018). Researchers are also starting to introduce DA datasets labeled for socio-pragmatics, e.g., (Abbes et al., 2020; Mubarak et al., 2020). The MADAR shared task (Bouamor et al., 2019) comprised two subtasks, one focusing on 21 Arab countries exploiting Twitter data manually labeled at the user level, and another on 25 Arab cities mentioned above. During the same time as NADI, Abdul-Mageed et al. (2020) describe data and models at country, province, and city levels. The NADI shared task follows these pioneering works by availing data to the (Arabic) NLP community, and encouraging work on Arabic dialects. Similar to the MADAR shared task, we include a country-level dialect identification task (Subtask 1), and a sub-country dialect identification task (Subtask 2). However, ou"
2020.wanlp-1.9,2020.lrec-1.165,0,0.0905249,"2 Related Work As we explained in Section 1, Arabic could be viewed as comprised of 3 main types: CA, MSA, and DA. While CA and MSA have been studied and taught extensively, DA has only received more attention relatively recently (Harrell, 1962; Cowell, 1964; Badawi, 1973; Brustad, 2000; Holes, 2004). A majority of DA computational efforts have targeted creating resources for country or regionally specific dialects (Gadalla et al., 1997; Diab et al., 2010; Al-Sabbagh and Girju, 2012; Sadat et al., 2014; Sma¨ıli et al., 2014; Jarrar et al., 2016; Khalifa et al., 2016; Al-Twairesh et al., 2018; El-Haj, 2020). The expansion into multi-dialectal data sets and models to identify them was initially done at the regional level (Zaidan and Callison-Burch, 2011; Elfardy et al., 2014; Bouamor et al., 2014; Meftouh et al., 2015). A number of Arabic dialect identification shared tasks were organized as part of the VarDial workshop. These focused on regional varieties such as Egyptian, Gulf, Levantine, and North African based on speech broadcast transcriptions (Malmasi et al., 2016) but also acoustic features (Zampieri et al., 2017) and phonetic features (Zampieri et al., 2018) extracted from raw audio. Alth"
2020.wanlp-1.9,2020.wanlp-1.27,0,0.141485,"Missing"
2020.wanlp-1.9,W14-3911,0,0.018515,"tensively, DA has only received more attention relatively recently (Harrell, 1962; Cowell, 1964; Badawi, 1973; Brustad, 2000; Holes, 2004). A majority of DA computational efforts have targeted creating resources for country or regionally specific dialects (Gadalla et al., 1997; Diab et al., 2010; Al-Sabbagh and Girju, 2012; Sadat et al., 2014; Sma¨ıli et al., 2014; Jarrar et al., 2016; Khalifa et al., 2016; Al-Twairesh et al., 2018; El-Haj, 2020). The expansion into multi-dialectal data sets and models to identify them was initially done at the regional level (Zaidan and Callison-Burch, 2011; Elfardy et al., 2014; Bouamor et al., 2014; Meftouh et al., 2015). A number of Arabic dialect identification shared tasks were organized as part of the VarDial workshop. These focused on regional varieties such as Egyptian, Gulf, Levantine, and North African based on speech broadcast transcriptions (Malmasi et al., 2016) but also acoustic features (Zampieri et al., 2017) and phonetic features (Zampieri et al., 2018) extracted from raw audio. Althobaiti (2020) presents a recent survey of computational work on Arabic dialects. An early effort for creating finer grained parallel dialectal corpus and lexicon was done"
2020.wanlp-1.9,2020.wanlp-1.28,0,0.0931316,"Missing"
2021.finnlp-1.1,P00-1038,0,0.477115,"Missing"
2021.finnlp-1.1,N19-1423,0,0.0597414,"Missing"
2021.finnlp-1.1,2020.fnp-1.1,0,0.0676488,"Missing"
2021.finnlp-1.1,E09-1029,0,0.109017,"Missing"
2021.finnlp-1.1,W04-1013,0,0.155347,"Missing"
2021.finnlp-1.1,P04-1077,0,0.418891,"Missing"
2021.finnlp-1.1,2020.fnp-1.34,0,0.0608031,"Missing"
2021.finnlp-1.1,P04-3020,0,0.393467,"Missing"
2021.finnlp-1.1,W04-3252,0,0.50865,"Missing"
2021.finnlp-1.1,2020.fnp-1.18,0,0.0522836,"Missing"
2021.finnlp-1.1,W13-1103,0,0.0658954,"Missing"
2021.wanlp-1.10,N16-3003,0,0.0268095,"data is known a priori to be CA, then we select the CAMeLBERT-CA model; if the task data is known to be MSA, we select the CAMeLBERT-MSA model; otherwise, we use the CAMeLBERT-Mix model (for dialects, i.e.). We report on this model in Table 4 and 5 as CAMeLBERT-Star. It is noteworthy that this model is not the same as Comparison with Existing Models Table 5 compares our work with other existing models. We do not use models that require morphological pre-tokenization to allow direct comparison, and also because existing tokenization systems are mostly focused on MSA or EGY (Pasha et al., 2014; Abdelali et al., 2016; Obeid et al., 2020). We are aware that design decisions such as vocabulary size and number of training steps are not the same across these eight existing pre-trained models, which might be a contributing factor to their varying performances. We plan to investigate the effects of such decisions in future work. Task Performance Complementarity The best model on average is AraBERTv02 (X3 ); it wins or ties for a win in six out of 12 subtasks (four MSA and two DA). Our CAMeLBERT-Star is second overall on average, and it wins or ties for a win in four out of 12 subtasks (three DA, one CA). Intere"
2021.wanlp-1.10,2020.wanlp-1.9,1,0.906045,"tant than the pre-training data size. We exploit this insight in defining an optimized system selection model for the studied tasks. 1 Introduction Pre-trained language models such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019b) have shown significant success in a wide range of natural language processing (NLP) tasks in various languages. Arabic has benefited from extensive efforts in building dedicated pre-trained language models, achieving state-of-the-art results in a number of NLP tasks, across both Modern Standard Arabic (MSA) and Dialectal Arabic (DA) (Antoun et al., 2020; Abdul-Mageed et al., 2020a). However, it is hard to compare these models to understand what contributes to their performances because of their different design decisions and hyperparameters, such as data size, language variant, tokenization, vocabulary size, number of training steps, and so forth. Practically, one may empirically choose the best performing pre-trained model by fine-tuning it on a particular task; however, it is still unclear why a particular model is performing better than another and what design choices are contributing to its performance. To answer this question, we pre-trained various language mode"
2021.wanlp-1.10,W19-4621,0,0.0128008,"sets using the accuracy score. 4.3 Sentiment Analysis Dataset We used a combination of sentiment analysis datasets to fine-tune our models. The datasets are: (1) the Arabic Speech-Act and Sentiment Corpus of Tweets (ArSAS) (Elmadany et al., 2018); (2) the Arabic Sentiment Tweets Dataset (ASTD) (Nabil et al., 2015); (3) SemEval-2017 task 4-A benchmark dataset (Rosenthal et al., 2017); and (4) the Multi-Topic Corpus for Target-based Sentiment Analysis in Arabic Levantine Tweets (ArSenTD-Lev) (Baly et al., 2019). We combined and preprocessed the datasets in a similar way to what was done by Abu Farha and Magdy (2019) and Obeid et al. (2020). That is, we removed diacritics, URLs, and Twitter usernames from all the tweets. Experimental Setup Our models were finetuned on ArSenTD-Lev and the train splits from SemEval, ASTD, and ArSAS (23,327 tweets) on a single GPU for 3 epochs with a learning rate of 3e-5, batch size of 32, and a maximum sequence length of 128. After the fine-tuning, we used the best checkpoint based on a single dev set from SemEval, ASTD, and ArSAS to report results on the test sets. We used the F1P N score which was defined in the SemEval-2017 task 4-A; F1P N is the macro F1 score over the"
2021.wanlp-1.10,L18-1576,0,0.0321346,"Missing"
2021.wanlp-1.10,2020.osact-1.2,0,0.2166,"ng data is more important than the pre-training data size. We exploit this insight in defining an optimized system selection model for the studied tasks. 1 Introduction Pre-trained language models such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019b) have shown significant success in a wide range of natural language processing (NLP) tasks in various languages. Arabic has benefited from extensive efforts in building dedicated pre-trained language models, achieving state-of-the-art results in a number of NLP tasks, across both Modern Standard Arabic (MSA) and Dialectal Arabic (DA) (Antoun et al., 2020; Abdul-Mageed et al., 2020a). However, it is hard to compare these models to understand what contributes to their performances because of their different design decisions and hyperparameters, such as data size, language variant, tokenization, vocabulary size, number of training steps, and so forth. Practically, one may empirically choose the best performing pre-trained model by fine-tuning it on a particular task; however, it is still unclear why a particular model is performing better than another and what design choices are contributing to its performance. To answer this question, we pre-tr"
2021.wanlp-1.10,bouamor-etal-2014-multidialectal,1,0.880214,"Missing"
2021.wanlp-1.10,W19-4622,1,0.848151,"28. After the fine-tuning, we used the best checkpoint based on a single dev set from SemEval, ASTD, and ArSAS to report results on the test sets. We used the F1P N score which was defined in the SemEval-2017 task 4-A; F1P N is the macro F1 score over the positive and negative classes only while neglecting the neutral class. 4.4 Dialect Identification Dataset We fine-tuned our models on four different dialect identification datasets: (1) MADAR Corpus 26 which includes 26 labels; (2) MADAR Corpus 6 which includes six labels; (3) MADAR Twitter Corpus (Bouamor et al., 2018; Salameh et al., 2018; Bouamor et al., 2019) which includes 21 labels; and (4) NADI Country-level (AbdulMageed et al., 2020b) which includes 21 labels. The datasets were preprocessed by removing diacritics, URLs, and Twitter usernames while maintaining the same train, dev, and test splits for each dataset. Moreover, we collated the tweets belonging to a particular user in the MADAR Twitter Corpus in groups of 5 before feeding them to the model. We refer to this preprocessed version as MADARTwitter-5 to avoid confusion with the publicly available original MADAR Twitter Corpus. Experimental Setup Our models were finetuned for 10 epochs wi"
2021.wanlp-1.10,2020.acl-main.493,0,0.0111152,"itched pre-training using Wikipedia, Gigaword, and the OSCAR corpus. Most recently, Abdul-Mageed et al. (2020a) developed two models, ARBERT and MARBERT, pre-trained on a large collection of datasets in MSA and DA. They reported new stateof-the-art results on the majority of the datasets in their fine-tuning benchmark. Moreover, there have been various studies explaining why pre-trained language models perform well on downstream tasks either in monolingual (Hewitt and Manning, 2019; Jawahar et al., 2019; Liu et al., 2019a; Tenney et al., 2019a,b) or multilingual settings (Wu and Dredze, 2019; Chi et al., 2020; Kulmizev et al., 2020; Vuli´c et al., 2020). Most of these efforts leveraged probing techniques to explore the linguistic knowledge that is captured by pre-trained language models such as morphosyntactic and semantic knowledge. More recently, there have been additional efforts investigating the effects of pre-training data size and tokenization on the performance of pre-trained language models. Zhang et al. (2020) showed that pre-training RoBERTa requires 10M to 100M words to learn representations that reliably encode most syntactic and semantic features. However, a much larger quantity of d"
2021.wanlp-1.10,cotterell-callison-burch-2014-multi,0,0.0688368,"Missing"
2021.wanlp-1.10,N19-1423,0,0.173218,"three. We also examine the importance of pre-training data size by building additional models that are pre-trained on a scaled-down set of the MSA variant. We compare our different models to each other, as well as to eight publicly available models by fine-tuning them on five NLP tasks spanning 12 datasets. Our results suggest that the variant proximity of pre-training data to fine-tuning data is more important than the pre-training data size. We exploit this insight in defining an optimized system selection model for the studied tasks. 1 Introduction Pre-trained language models such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019b) have shown significant success in a wide range of natural language processing (NLP) tasks in various languages. Arabic has benefited from extensive efforts in building dedicated pre-trained language models, achieving state-of-the-art results in a number of NLP tasks, across both Modern Standard Arabic (MSA) and Dialectal Arabic (DA) (Antoun et al., 2020; Abdul-Mageed et al., 2020a). However, it is hard to compare these models to understand what contributes to their performances because of their different design decisions and hyperparameters, such as data size,"
2021.wanlp-1.10,2020.acl-main.375,0,0.0145807,"g using Wikipedia, Gigaword, and the OSCAR corpus. Most recently, Abdul-Mageed et al. (2020a) developed two models, ARBERT and MARBERT, pre-trained on a large collection of datasets in MSA and DA. They reported new stateof-the-art results on the majority of the datasets in their fine-tuning benchmark. Moreover, there have been various studies explaining why pre-trained language models perform well on downstream tasks either in monolingual (Hewitt and Manning, 2019; Jawahar et al., 2019; Liu et al., 2019a; Tenney et al., 2019a,b) or multilingual settings (Wu and Dredze, 2019; Chi et al., 2020; Kulmizev et al., 2020; Vuli´c et al., 2020). Most of these efforts leveraged probing techniques to explore the linguistic knowledge that is captured by pre-trained language models such as morphosyntactic and semantic knowledge. More recently, there have been additional efforts investigating the effects of pre-training data size and tokenization on the performance of pre-trained language models. Zhang et al. (2020) showed that pre-training RoBERTa requires 10M to 100M words to learn representations that reliably encode most syntactic and semantic features. However, a much larger quantity of data is needed for the m"
2021.wanlp-1.10,2020.lrec-1.165,0,0.0537853,"Missing"
2021.wanlp-1.10,N19-1419,0,0.0123424,"eral English-Arabic bilingual models dubbed GigaBERTs, where they studied the effectiveness of cross-lingual transfer learning and code-switched pre-training using Wikipedia, Gigaword, and the OSCAR corpus. Most recently, Abdul-Mageed et al. (2020a) developed two models, ARBERT and MARBERT, pre-trained on a large collection of datasets in MSA and DA. They reported new stateof-the-art results on the majority of the datasets in their fine-tuning benchmark. Moreover, there have been various studies explaining why pre-trained language models perform well on downstream tasks either in monolingual (Hewitt and Manning, 2019; Jawahar et al., 2019; Liu et al., 2019a; Tenney et al., 2019a,b) or multilingual settings (Wu and Dredze, 2019; Chi et al., 2020; Kulmizev et al., 2020; Vuli´c et al., 2020). Most of these efforts leveraged probing techniques to explore the linguistic knowledge that is captured by pre-trained language models such as morphosyntactic and semantic knowledge. More recently, there have been additional efforts investigating the effects of pre-training data size and tokenization on the performance of pre-trained language models. Zhang et al. (2020) showed that pre-training RoBERTa requires 10M to 1"
2021.wanlp-1.10,P19-1356,0,0.0138436,"ual models dubbed GigaBERTs, where they studied the effectiveness of cross-lingual transfer learning and code-switched pre-training using Wikipedia, Gigaword, and the OSCAR corpus. Most recently, Abdul-Mageed et al. (2020a) developed two models, ARBERT and MARBERT, pre-trained on a large collection of datasets in MSA and DA. They reported new stateof-the-art results on the majority of the datasets in their fine-tuning benchmark. Moreover, there have been various studies explaining why pre-trained language models perform well on downstream tasks either in monolingual (Hewitt and Manning, 2019; Jawahar et al., 2019; Liu et al., 2019a; Tenney et al., 2019a,b) or multilingual settings (Wu and Dredze, 2019; Chi et al., 2020; Kulmizev et al., 2020; Vuli´c et al., 2020). Most of these efforts leveraged probing techniques to explore the linguistic knowledge that is captured by pre-trained language models such as morphosyntactic and semantic knowledge. More recently, there have been additional efforts investigating the effects of pre-training data size and tokenization on the performance of pre-trained language models. Zhang et al. (2020) showed that pre-training RoBERTa requires 10M to 100M words to learn rep"
2021.wanlp-1.10,2020.emnlp-main.382,0,0.084089,"Missing"
2021.wanlp-1.10,N19-1112,0,0.148226,"nce of pre-training data size by building additional models that are pre-trained on a scaled-down set of the MSA variant. We compare our different models to each other, as well as to eight publicly available models by fine-tuning them on five NLP tasks spanning 12 datasets. Our results suggest that the variant proximity of pre-training data to fine-tuning data is more important than the pre-training data size. We exploit this insight in defining an optimized system selection model for the studied tasks. 1 Introduction Pre-trained language models such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019b) have shown significant success in a wide range of natural language processing (NLP) tasks in various languages. Arabic has benefited from extensive efforts in building dedicated pre-trained language models, achieving state-of-the-art results in a number of NLP tasks, across both Modern Standard Arabic (MSA) and Dialectal Arabic (DA) (Antoun et al., 2020; Abdul-Mageed et al., 2020a). However, it is hard to compare these models to understand what contributes to their performances because of their different design decisions and hyperparameters, such as data size, language variant, tokenization"
2021.wanlp-1.10,Y15-1004,0,0.0604818,"Missing"
2021.wanlp-1.10,2020.emnlp-main.632,0,0.0875708,"Missing"
2021.wanlp-1.10,D15-1299,0,0.0732032,"Missing"
2021.wanlp-1.10,2020.acl-main.156,0,0.0825257,"Missing"
2021.wanlp-1.10,pasha-etal-2014-madamira,1,0.837299,"Missing"
2021.wanlp-1.10,S17-2088,0,0.065031,"Missing"
2021.wanlp-1.10,2020.semeval-1.271,0,0.417822,"Missing"
2021.wanlp-1.10,salama-etal-2014-youdacc,1,0.882439,"Missing"
2021.wanlp-1.10,C18-1113,1,0.815036,"m sequence length of 128. After the fine-tuning, we used the best checkpoint based on a single dev set from SemEval, ASTD, and ArSAS to report results on the test sets. We used the F1P N score which was defined in the SemEval-2017 task 4-A; F1P N is the macro F1 score over the positive and negative classes only while neglecting the neutral class. 4.4 Dialect Identification Dataset We fine-tuned our models on four different dialect identification datasets: (1) MADAR Corpus 26 which includes 26 labels; (2) MADAR Corpus 6 which includes six labels; (3) MADAR Twitter Corpus (Bouamor et al., 2018; Salameh et al., 2018; Bouamor et al., 2019) which includes 21 labels; and (4) NADI Country-level (AbdulMageed et al., 2020b) which includes 21 labels. The datasets were preprocessed by removing diacritics, URLs, and Twitter usernames while maintaining the same train, dev, and test splits for each dataset. Moreover, we collated the tweets belonging to a particular user in the MADAR Twitter Corpus in groups of 5 before feeding them to the model. We refer to this preprocessed version as MADARTwitter-5 to avoid confusion with the publicly available original MADAR Twitter Corpus. Experimental Setup Our models were fin"
2021.wanlp-1.10,P19-1452,0,0.014568,"tudied the effectiveness of cross-lingual transfer learning and code-switched pre-training using Wikipedia, Gigaword, and the OSCAR corpus. Most recently, Abdul-Mageed et al. (2020a) developed two models, ARBERT and MARBERT, pre-trained on a large collection of datasets in MSA and DA. They reported new stateof-the-art results on the majority of the datasets in their fine-tuning benchmark. Moreover, there have been various studies explaining why pre-trained language models perform well on downstream tasks either in monolingual (Hewitt and Manning, 2019; Jawahar et al., 2019; Liu et al., 2019a; Tenney et al., 2019a,b) or multilingual settings (Wu and Dredze, 2019; Chi et al., 2020; Kulmizev et al., 2020; Vuli´c et al., 2020). Most of these efforts leveraged probing techniques to explore the linguistic knowledge that is captured by pre-trained language models such as morphosyntactic and semantic knowledge. More recently, there have been additional efforts investigating the effects of pre-training data size and tokenization on the performance of pre-trained language models. Zhang et al. (2020) showed that pre-training RoBERTa requires 10M to 100M words to learn representations that reliably encode most s"
2021.wanlp-1.10,2020.emnlp-main.586,0,0.0697447,"Missing"
2021.wanlp-1.10,D19-1077,0,0.0209708,"learning and code-switched pre-training using Wikipedia, Gigaword, and the OSCAR corpus. Most recently, Abdul-Mageed et al. (2020a) developed two models, ARBERT and MARBERT, pre-trained on a large collection of datasets in MSA and DA. They reported new stateof-the-art results on the majority of the datasets in their fine-tuning benchmark. Moreover, there have been various studies explaining why pre-trained language models perform well on downstream tasks either in monolingual (Hewitt and Manning, 2019; Jawahar et al., 2019; Liu et al., 2019a; Tenney et al., 2019a,b) or multilingual settings (Wu and Dredze, 2019; Chi et al., 2020; Kulmizev et al., 2020; Vuli´c et al., 2020). Most of these efforts leveraged probing techniques to explore the linguistic knowledge that is captured by pre-trained language models such as morphosyntactic and semantic knowledge. More recently, there have been additional efforts investigating the effects of pre-training data size and tokenization on the performance of pre-trained language models. Zhang et al. (2020) showed that pre-training RoBERTa requires 10M to 100M words to learn representations that reliably encode most syntactic and semantic features. However, a much la"
2021.wanlp-1.10,L18-1111,0,0.0536388,"Missing"
2021.wanlp-1.10,P11-2007,0,0.0981301,"Missing"
2021.wanlp-1.10,W19-4619,0,0.0350489,"Missing"
2021.wanlp-1.28,bouamor-etal-2014-multidialectal,1,0.782333,"A has received more attention only in recent years. One major challenge with studying DA has been rarity of resources. For this reason, most pioneering DA works focused on creating resources, usually for only a small number of regions or countries (Gadalla et al., 1997; Diab et al., 2010; AlSabbagh and Girju, 2012; Sadat et al., 2014; Sma¨ıli et al., 2014; Jarrar et al., 2016; Khalifa et al., 2016; Al-Twairesh et al., 2018; El-Haj, 2020). A number of works introducing multi-dialectal data sets and regional level detection models followed (Zaidan and Callison-Burch, 2011; Elfardy et al., 2014; Bouamor et al., 2014; Meftouh et al., 2015). The Multi Arabic Dialects Application and Resources (MADAR) project (Bouamor et al., 2018) introduced finer-grained dialectal data and a lexicon. The MADAR data were used for dialect identification at the city level (Salameh et al., 2018; Obeid et al., 2019) of 25 Arab cities. An issue with the MADAR data, in the context of DA identification, is that it was commissioned and not naturally occurring. Several larger datasets covering 10-21 countries were also introduced (Mubarak and Darwish, 2014; Abdul-Mageed et al., 2018; Zaghouani and Charfi, 2018). These datasets come"
2021.wanlp-1.28,W19-4622,1,0.537072,"classification level next. 3.1 Country-level Classification • Subtask 1.1: Country-level MSA. The goal of Subtask 1.1 is to identify country level MSA from short written sentences (tweets). NADI 2021 Subtask 1.1 is novel since no previous works focused on teasing apart MSA by country of origin. • Subtask 1.2: Country-level DA. Subtask 1.2 is similar to Subtask 1.1, but focuses on identifying country level dialect from tweets. Subtask 1.2 is similar to previous works that have also taken country as their target (Mubarak and Darwish, 2014; Abdul-Mageed et al., 2018; Zaghouani and Charfi, 2018; Bouamor et al., 2019; Abdul-Mageed et al., 2020b). We provided labeled data to NADI 2021 participants with specific training (TRAIN) and development (DEV) splits. Each of the 21 labels corresponding to the 21 countries is represented in both TRAIN and DEV. Teams could score their models through an online system (codalab) on the DEV set before the deadline. We released our TEST set of unlabeled tweets shortly before the system submission deadline. We then invited participants to submit their predictions to the online scoring system housing the gold TEST set labels. Table 2 shows the distribution of the TRAIN, DEV,"
2021.wanlp-1.28,2020.lrec-1.165,0,0.0729724,"Arabic has three main categories: CA, MSA, and DA. While CA and MSA have been studied extensively (Harrell, 1962; Cowell, 1964; Badawi, 1973; Brustad, 2000; Holes, 2004), DA has received more attention only in recent years. One major challenge with studying DA has been rarity of resources. For this reason, most pioneering DA works focused on creating resources, usually for only a small number of regions or countries (Gadalla et al., 1997; Diab et al., 2010; AlSabbagh and Girju, 2012; Sadat et al., 2014; Sma¨ıli et al., 2014; Jarrar et al., 2016; Khalifa et al., 2016; Al-Twairesh et al., 2018; El-Haj, 2020). A number of works introducing multi-dialectal data sets and regional level detection models followed (Zaidan and Callison-Burch, 2011; Elfardy et al., 2014; Bouamor et al., 2014; Meftouh et al., 2015). The Multi Arabic Dialects Application and Resources (MADAR) project (Bouamor et al., 2018) introduced finer-grained dialectal data and a lexicon. The MADAR data were used for dialect identification at the city level (Salameh et al., 2018; Obeid et al., 2019) of 25 Arab cities. An issue with the MADAR data, in the context of DA identification, is that it was commissioned and not naturally occur"
2021.wanlp-1.28,W14-3911,0,0.0207609,"2000; Holes, 2004), DA has received more attention only in recent years. One major challenge with studying DA has been rarity of resources. For this reason, most pioneering DA works focused on creating resources, usually for only a small number of regions or countries (Gadalla et al., 1997; Diab et al., 2010; AlSabbagh and Girju, 2012; Sadat et al., 2014; Sma¨ıli et al., 2014; Jarrar et al., 2016; Khalifa et al., 2016; Al-Twairesh et al., 2018; El-Haj, 2020). A number of works introducing multi-dialectal data sets and regional level detection models followed (Zaidan and Callison-Burch, 2011; Elfardy et al., 2014; Bouamor et al., 2014; Meftouh et al., 2015). The Multi Arabic Dialects Application and Resources (MADAR) project (Bouamor et al., 2018) introduced finer-grained dialectal data and a lexicon. The MADAR data were used for dialect identification at the city level (Salameh et al., 2018; Obeid et al., 2019) of 25 Arab cities. An issue with the MADAR data, in the context of DA identification, is that it was commissioned and not naturally occurring. Several larger datasets covering 10-21 countries were also introduced (Mubarak and Darwish, 2014; Abdul-Mageed et al., 2018; Zaghouani and Charfi, 2018"
2021.wanlp-1.28,2021.wanlp-1.32,0,0.0786186,"Missing"
2021.wanlp-1.28,L16-1679,1,0.731098,"5. 2 Related Work As we explained in Section 1, Arabic has three main categories: CA, MSA, and DA. While CA and MSA have been studied extensively (Harrell, 1962; Cowell, 1964; Badawi, 1973; Brustad, 2000; Holes, 2004), DA has received more attention only in recent years. One major challenge with studying DA has been rarity of resources. For this reason, most pioneering DA works focused on creating resources, usually for only a small number of regions or countries (Gadalla et al., 1997; Diab et al., 2010; AlSabbagh and Girju, 2012; Sadat et al., 2014; Sma¨ıli et al., 2014; Jarrar et al., 2016; Khalifa et al., 2016; Al-Twairesh et al., 2018; El-Haj, 2020). A number of works introducing multi-dialectal data sets and regional level detection models followed (Zaidan and Callison-Burch, 2011; Elfardy et al., 2014; Bouamor et al., 2014; Meftouh et al., 2015). The Multi Arabic Dialects Application and Resources (MADAR) project (Bouamor et al., 2018) introduced finer-grained dialectal data and a lexicon. The MADAR data were used for dialect identification at the city level (Salameh et al., 2018; Obeid et al., 2019) of 25 Arab cities. An issue with the MADAR data, in the context of DA identification, is that it"
2021.wanlp-1.28,N19-4002,1,0.803112,"2010; AlSabbagh and Girju, 2012; Sadat et al., 2014; Sma¨ıli et al., 2014; Jarrar et al., 2016; Khalifa et al., 2016; Al-Twairesh et al., 2018; El-Haj, 2020). A number of works introducing multi-dialectal data sets and regional level detection models followed (Zaidan and Callison-Burch, 2011; Elfardy et al., 2014; Bouamor et al., 2014; Meftouh et al., 2015). The Multi Arabic Dialects Application and Resources (MADAR) project (Bouamor et al., 2018) introduced finer-grained dialectal data and a lexicon. The MADAR data were used for dialect identification at the city level (Salameh et al., 2018; Obeid et al., 2019) of 25 Arab cities. An issue with the MADAR data, in the context of DA identification, is that it was commissioned and not naturally occurring. Several larger datasets covering 10-21 countries were also introduced (Mubarak and Darwish, 2014; Abdul-Mageed et al., 2018; Zaghouani and Charfi, 2018). These datasets come from the Twitter domain, and hence are naturally-occurring. Several works have also focused on sociopragmatics meaning exploiting dialectal data. These include sentiment analysis (Abdul-Mageed et al., 2014), emotion (Alhuzali et al., 2018), age and gender (Abbes et al., 2020), offe"
2021.wanlp-1.28,W14-5904,0,0.0248499,", and a high-level description of submitted systems in Section 5. 2 Related Work As we explained in Section 1, Arabic has three main categories: CA, MSA, and DA. While CA and MSA have been studied extensively (Harrell, 1962; Cowell, 1964; Badawi, 1973; Brustad, 2000; Holes, 2004), DA has received more attention only in recent years. One major challenge with studying DA has been rarity of resources. For this reason, most pioneering DA works focused on creating resources, usually for only a small number of regions or countries (Gadalla et al., 1997; Diab et al., 2010; AlSabbagh and Girju, 2012; Sadat et al., 2014; Sma¨ıli et al., 2014; Jarrar et al., 2016; Khalifa et al., 2016; Al-Twairesh et al., 2018; El-Haj, 2020). A number of works introducing multi-dialectal data sets and regional level detection models followed (Zaidan and Callison-Burch, 2011; Elfardy et al., 2014; Bouamor et al., 2014; Meftouh et al., 2015). The Multi Arabic Dialects Application and Resources (MADAR) project (Bouamor et al., 2018) introduced finer-grained dialectal data and a lexicon. The MADAR data were used for dialect identification at the city level (Salameh et al., 2018; Obeid et al., 2019) of 25 Arab cities. An issue wit"
2021.wanlp-1.28,C18-1113,1,0.850076,"., 1997; Diab et al., 2010; AlSabbagh and Girju, 2012; Sadat et al., 2014; Sma¨ıli et al., 2014; Jarrar et al., 2016; Khalifa et al., 2016; Al-Twairesh et al., 2018; El-Haj, 2020). A number of works introducing multi-dialectal data sets and regional level detection models followed (Zaidan and Callison-Burch, 2011; Elfardy et al., 2014; Bouamor et al., 2014; Meftouh et al., 2015). The Multi Arabic Dialects Application and Resources (MADAR) project (Bouamor et al., 2018) introduced finer-grained dialectal data and a lexicon. The MADAR data were used for dialect identification at the city level (Salameh et al., 2018; Obeid et al., 2019) of 25 Arab cities. An issue with the MADAR data, in the context of DA identification, is that it was commissioned and not naturally occurring. Several larger datasets covering 10-21 countries were also introduced (Mubarak and Darwish, 2014; Abdul-Mageed et al., 2018; Zaghouani and Charfi, 2018). These datasets come from the Twitter domain, and hence are naturally-occurring. Several works have also focused on sociopragmatics meaning exploiting dialectal data. These include sentiment analysis (Abdul-Mageed et al., 2014), emotion (Alhuzali et al., 2018), age and gender (Abbe"
2021.wanlp-1.28,2021.wanlp-1.35,0,0.0644144,"Missing"
2021.wanlp-1.28,L18-1111,0,0.109857,"2011; Elfardy et al., 2014; Bouamor et al., 2014; Meftouh et al., 2015). The Multi Arabic Dialects Application and Resources (MADAR) project (Bouamor et al., 2018) introduced finer-grained dialectal data and a lexicon. The MADAR data were used for dialect identification at the city level (Salameh et al., 2018; Obeid et al., 2019) of 25 Arab cities. An issue with the MADAR data, in the context of DA identification, is that it was commissioned and not naturally occurring. Several larger datasets covering 10-21 countries were also introduced (Mubarak and Darwish, 2014; Abdul-Mageed et al., 2018; Zaghouani and Charfi, 2018). These datasets come from the Twitter domain, and hence are naturally-occurring. Several works have also focused on sociopragmatics meaning exploiting dialectal data. These include sentiment analysis (Abdul-Mageed et al., 2014), emotion (Alhuzali et al., 2018), age and gender (Abbes et al., 2020), offensive language (Mubarak et al., 2020), and sarcasm (Abu Farha and Magdy, 2020). Concurrent with our work, (Abdul-Mageed et al., 2020c) also describe data and models at country, province, and city levels. 1 The dataset is accessible via our GitHub at: https: //github.com/UBC-NLP/nadi. 245 The fir"
2021.wanlp-1.28,P11-2007,0,0.0259273,"ell, 1964; Badawi, 1973; Brustad, 2000; Holes, 2004), DA has received more attention only in recent years. One major challenge with studying DA has been rarity of resources. For this reason, most pioneering DA works focused on creating resources, usually for only a small number of regions or countries (Gadalla et al., 1997; Diab et al., 2010; AlSabbagh and Girju, 2012; Sadat et al., 2014; Sma¨ıli et al., 2014; Jarrar et al., 2016; Khalifa et al., 2016; Al-Twairesh et al., 2018; El-Haj, 2020). A number of works introducing multi-dialectal data sets and regional level detection models followed (Zaidan and Callison-Burch, 2011; Elfardy et al., 2014; Bouamor et al., 2014; Meftouh et al., 2015). The Multi Arabic Dialects Application and Resources (MADAR) project (Bouamor et al., 2018) introduced finer-grained dialectal data and a lexicon. The MADAR data were used for dialect identification at the city level (Salameh et al., 2018; Obeid et al., 2019) of 25 Arab cities. An issue with the MADAR data, in the context of DA identification, is that it was commissioned and not naturally occurring. Several larger datasets covering 10-21 countries were also introduced (Mubarak and Darwish, 2014; Abdul-Mageed et al., 2018; Zagh"
2021.wanlp-1.28,W17-1201,0,0.0998386,"Missing"
2021.wanlp-1.28,W18-3901,0,0.0455917,"Missing"
bouamor-etal-2012-contrastive,I05-5002,0,\N,Missing
bouamor-etal-2012-contrastive,W04-3219,0,\N,Missing
bouamor-etal-2012-contrastive,J10-3003,0,\N,Missing
bouamor-etal-2012-contrastive,C04-1077,0,\N,Missing
bouamor-etal-2012-contrastive,W03-1004,0,\N,Missing
bouamor-etal-2012-contrastive,P02-1040,0,\N,Missing
bouamor-etal-2012-contrastive,P01-1008,0,\N,Missing
bouamor-etal-2012-contrastive,C10-1149,0,\N,Missing
bouamor-etal-2012-contrastive,P08-4006,0,\N,Missing
bouamor-etal-2012-contrastive,W07-0734,0,\N,Missing
bouamor-etal-2012-contrastive,W09-0621,0,\N,Missing
bouamor-etal-2012-contrastive,P11-1020,0,\N,Missing
bouamor-etal-2012-contrastive,E09-1082,0,\N,Missing
bouamor-etal-2012-contrastive,J08-4005,0,\N,Missing
bouamor-etal-2012-contrastive,shimohata-etal-2004-building,0,\N,Missing
bouamor-etal-2012-contrastive,max-wisniewski-2010-mining,1,\N,Missing
bouamor-etal-2014-multidialectal,N12-1006,0,\N,Missing
bouamor-etal-2014-multidialectal,2006.amta-papers.21,0,\N,Missing
bouamor-etal-2014-multidialectal,E06-1047,1,\N,Missing
bouamor-etal-2014-multidialectal,P13-2001,0,\N,Missing
bouamor-etal-2014-multidialectal,J14-1006,0,\N,Missing
bouamor-etal-2014-multidialectal,P11-1122,0,\N,Missing
bouamor-etal-2014-multidialectal,N13-1036,1,\N,Missing
bouamor-etal-2014-multidialectal,habash-etal-2012-conventional,1,\N,Missing
bouamor-etal-2014-multidialectal,N13-1044,1,\N,Missing
bouamor-etal-2014-multidialectal,pasha-etal-2014-madamira,1,\N,Missing
bouamor-etal-2014-multidialectal,I13-1048,0,\N,Missing
bouamor-etal-2014-multidialectal,zribi-etal-2014-conventional,1,\N,Missing
bouamor-etal-2014-multidialectal,P13-2081,0,\N,Missing
bouamor-etal-2014-multidialectal,al-sabbagh-girju-2010-mining,0,\N,Missing
bouamor-etal-2014-multidialectal,W12-2301,1,\N,Missing
bouamor-etal-2014-multidialectal,N07-5003,1,\N,Missing
C16-1132,D14-1026,1,0.876215,"ation (Liu and Gildea, 2005; Giménez and Màrquez, 2007; Liu et al., 2010; Chen and Kuhn, 2011); (iii) morphology (Tantug et al., 2008); (iv) semantics (Dahlmeier et al., 2011; Lo et al., 2012) and (v) discourse (Guzmán et al., 2014b; Joty et al., 2014). Generally, these metrics have been focused on translation into English. However, there has been little attention into their direct applicability to languages with rich morphology. Our work focuses on automatic evaluation of translation into morphologically rich languages, Arabic more specifically. In that sense, our work is related to AL-BLEU (Bouamor et al., 2014) which is an adaptation of BLEU that gives partial credits for stem and morphological matchings of hypothesis and reference words. Here, in addition to using lexical information captured by n-gram metrics, we show that using morpho-syntactic representations can significantly improve the correlation with human judgments. Furthermore, we use a neural-network, which uses non-linearities to improve modeling. Over the past few years, neural network models have dramatically improved the state-of-the-art of different NLP applications (Goldberg, 2015). For instance, in SMT we have observed an increase"
C16-1132,E06-1032,0,0.161511,"Missing"
C16-1132,W12-3102,0,0.031103,"xplore this relationship more in depth in Section. 5.4. 5.3 Combination of Representations Given the complimentary information embedded in the different representations, it is natural to combine them to obtain a stronger metric. To combine different embedding representations, we simply concatenate the different embedding representations before feeding them to the network. Below, we present 4 Note that the Kendall’s τ results for M ETEOR are in the range of the results for translation from English into other two morphologically rich languages (German: 18.0 and Czech 16.0) reported in WMT 2012 (Callison-Burch et al., 2012) 1403 Kendall’s τ Combinations C. Embeddings and N-gram based metrics Lexical 14 5 METRICS+TOKEN 15 5 METRICS+NORM 16 5 METRICS+LEMMA Morpho-syntactic 17 5 METRICS+B UCKWALTER POS 18 5 METRICS+K ULICK POS 19 5 METRICS+S TANFORD POS 20 5 METRICS+CAT I BPOS result prev. best delta 23.62 24.17 23.51 24.35 23.22 21.17 ( -0.73) (+0.95) (+2.34) 29.81 23.58 21.79 18.93 25.49 18.12 18.12 18.12 (+4.32) (+5.46) (+3.67) (+0.81) 25.12 25.42 24.90 25.34 24.35 23.22 24.35 25.42 (+0.77) (+2.20) (+0.55) (-0.08) * 31.87 30.69 30.69 25.49 25.49 25.49 (+6.38) (+5.19) (+5.19) 25.56 25.42 25.45 28.35 25.12 25.42 2"
C16-1132,W11-2105,0,0.021929,"Section 2. We describe our approach in detail in Section 3; and we evaluate in Section 4. We present a discussion of our findings in Section 5. 2 Related Work Despite its well-known shortcomings (Callison-Burch et al., 2006), BLEU continues to be the de-facto MT evaluation metric. Several studies have attempted to improve upon it by taking into account different aspects of linguistic structures including: (i) synonym dictionaries or paraphrase tables (Denkowski and Lavie, 2011; Snover et al., 2010); (ii) syntactic information (Liu and Gildea, 2005; Giménez and Màrquez, 2007; Liu et al., 2010; Chen and Kuhn, 2011); (iii) morphology (Tantug et al., 2008); (iv) semantics (Dahlmeier et al., 2011; Lo et al., 2012) and (v) discourse (Guzmán et al., 2014b; Joty et al., 2014). Generally, these metrics have been focused on translation into English. However, there has been little attention into their direct applicability to languages with rich morphology. Our work focuses on automatic evaluation of translation into morphologically rich languages, Arabic more specifically. In that sense, our work is related to AL-BLEU (Bouamor et al., 2014) which is an adaptation of BLEU that gives partial credits for stem and m"
C16-1132,E03-1009,0,0.0349933,"ly on an syntactic neural parser (Socher et al., 2013), which increases the complexity of the evaluation setup, and is not readily available for every language. Here, we instead use morpho-syntactic representations which capture both syntactic and morphological aspects of language. In our experiments, these simple representations are powerful enough to provide state-of-the-art performance. In this work, we use neural network models to improve MT evaluation into Arabic using representations that capture morphology. Morphological structure has been shown to improve the quality of word clusters (Clark, 2003), word vector representations (Cotterell and Schütze, 2015) and neural language models (Botha and Blunsom, 2014). The novelty of our work resides in the way we integrate lexical and morpho-syntactic distributed representations into a neural-network. We demonstrate that combining several sources of complementary information is useful to capture sentence similarity in a translation evaluation scenario. And arguably, capture complex phenomena like morphological agreement. 3 Approach We use a pairwise approach to translation evaluation (Guzmán et al., 2014a) using neural networks. We use neural ne"
C16-1132,N15-1140,0,0.137724,"for tuning system parameters, it is crucial that the MT metrics correctly handle morphology. Most recently, deep learning models have been used more heavily in different parts of the natural language processing (NLP) community, including MT and MT evaluation. One of the main advantages of such models is the use of distributed word representations (embeddings). It has been shown that word embeddings are able to capture to certain semantic and syntactic aspects of words (Mikolov et al., 2013). Further refinements allow the inclusion of morphological information into distributed representations (Cotterell and Schütze, 2015). Word embeddings have been shown to help with modeling textual similarity well in the context of MT evaluation for MT into English (Guzmán et al., 2015), and community Question Answering (Guzmán et al., 2016). Nonetheless little exploration has been done on the use of embeddings for MT into MRL. In this paper, we investigate how embeddings obtained from different levels of lexical and morphosyntactic linguistic analysis can improve MT evaluation into a MRL. Specifically we report on Arabic, a language with complex and rich morphology paired with a high degree of ambiguity (Habash, 2010). Our"
C16-1132,W11-2106,0,0.0261248,"Section 4. We present a discussion of our findings in Section 5. 2 Related Work Despite its well-known shortcomings (Callison-Burch et al., 2006), BLEU continues to be the de-facto MT evaluation metric. Several studies have attempted to improve upon it by taking into account different aspects of linguistic structures including: (i) synonym dictionaries or paraphrase tables (Denkowski and Lavie, 2011; Snover et al., 2010); (ii) syntactic information (Liu and Gildea, 2005; Giménez and Màrquez, 2007; Liu et al., 2010; Chen and Kuhn, 2011); (iii) morphology (Tantug et al., 2008); (iv) semantics (Dahlmeier et al., 2011; Lo et al., 2012) and (v) discourse (Guzmán et al., 2014b; Joty et al., 2014). Generally, these metrics have been focused on translation into English. However, there has been little attention into their direct applicability to languages with rich morphology. Our work focuses on automatic evaluation of translation into morphologically rich languages, Arabic more specifically. In that sense, our work is related to AL-BLEU (Bouamor et al., 2014) which is an adaptation of BLEU that gives partial credits for stem and morphological matchings of hypothesis and reference words. Here, in addition to u"
C16-1132,W11-2107,0,0.514566,"s: from handling a complex and rich vocabulary, to designing adequate MT metrics that take morphology into account. While the first problem has widely explored (e.g. by using morphological analysis tools to reduce sparsity), the evaluation part has only been partly addressed. This is problematic since traditional MT metrics struggle to distinguish between (i) incorrect lexical choices; (ii) valid alternative lexical or syntactic variations; and (iii) differences in morphological inflection that are the result of incorrect case assignment or morphological agreement. While metrics like M ETEOR (Denkowski and Lavie, 2011) have made it possible to distinguish between (i) and (ii) by using paraphrases, (iii) is still an open problem. As a result, progress in SMT for MRL is hindered by the lack of adequate evaluation metrics. Since SMT metrics are used not only for evaluation but also for tuning system parameters, it is crucial that the MT metrics correctly handle morphology. Most recently, deep learning models have been used more heavily in different parts of the natural language processing (NLP) community, including MT and MT evaluation. One of the main advantages of such models is the use of distributed word r"
C16-1132,P14-1129,0,0.033517,"w that using morpho-syntactic representations can significantly improve the correlation with human judgments. Furthermore, we use a neural-network, which uses non-linearities to improve modeling. Over the past few years, neural network models have dramatically improved the state-of-the-art of different NLP applications (Goldberg, 2015). For instance, in SMT we have observed an increased use of neural nets for language modeling (Bengio et al., 2003; Mikolov et al., 2010), for improving answer ranking in community Question Answering (Guzmán et al., 2016), for improving the translation modeling (Devlin et al., 2014; Bahdanau et al., 2014; Cho et al., 2014) and for machine translation evaluation (Guzmán et al., 2015; Gupta et al., 2015). Our work is related to Guzmán et al. (2015), in several levels of lexical, syntactic and semantic are combined in a compact fashion using a pairwise neural framework. There are several differences between that work and ours: (i) we do not use syntactic embedding representations, (ii) we include additional pairwise features, namely the pairwise cosine similarity between embeddings; and (ii) we focus on an MRL language. While use of syntactic representations has proven a u"
C16-1132,W07-0738,0,0.0350979,"man judges. Next, we present related work in Section 2. We describe our approach in detail in Section 3; and we evaluate in Section 4. We present a discussion of our findings in Section 5. 2 Related Work Despite its well-known shortcomings (Callison-Burch et al., 2006), BLEU continues to be the de-facto MT evaluation metric. Several studies have attempted to improve upon it by taking into account different aspects of linguistic structures including: (i) synonym dictionaries or paraphrase tables (Denkowski and Lavie, 2011; Snover et al., 2010); (ii) syntactic information (Liu and Gildea, 2005; Giménez and Màrquez, 2007; Liu et al., 2010; Chen and Kuhn, 2011); (iii) morphology (Tantug et al., 2008); (iv) semantics (Dahlmeier et al., 2011; Lo et al., 2012) and (v) discourse (Guzmán et al., 2014b; Joty et al., 2014). Generally, these metrics have been focused on translation into English. However, there has been little attention into their direct applicability to languages with rich morphology. Our work focuses on automatic evaluation of translation into morphologically rich languages, Arabic more specifically. In that sense, our work is related to AL-BLEU (Bouamor et al., 2014) which is an adaptation of BLEU t"
C16-1132,D15-1124,0,0.0437833,"Missing"
C16-1132,D14-1027,1,0.917619,"on 5. 2 Related Work Despite its well-known shortcomings (Callison-Burch et al., 2006), BLEU continues to be the de-facto MT evaluation metric. Several studies have attempted to improve upon it by taking into account different aspects of linguistic structures including: (i) synonym dictionaries or paraphrase tables (Denkowski and Lavie, 2011; Snover et al., 2010); (ii) syntactic information (Liu and Gildea, 2005; Giménez and Màrquez, 2007; Liu et al., 2010; Chen and Kuhn, 2011); (iii) morphology (Tantug et al., 2008); (iv) semantics (Dahlmeier et al., 2011; Lo et al., 2012) and (v) discourse (Guzmán et al., 2014b; Joty et al., 2014). Generally, these metrics have been focused on translation into English. However, there has been little attention into their direct applicability to languages with rich morphology. Our work focuses on automatic evaluation of translation into morphologically rich languages, Arabic more specifically. In that sense, our work is related to AL-BLEU (Bouamor et al., 2014) which is an adaptation of BLEU that gives partial credits for stem and morphological matchings of hypothesis and reference words. Here, in addition to using lexical information captured by n-gram metrics, we s"
C16-1132,P14-1065,1,0.914549,"on 5. 2 Related Work Despite its well-known shortcomings (Callison-Burch et al., 2006), BLEU continues to be the de-facto MT evaluation metric. Several studies have attempted to improve upon it by taking into account different aspects of linguistic structures including: (i) synonym dictionaries or paraphrase tables (Denkowski and Lavie, 2011; Snover et al., 2010); (ii) syntactic information (Liu and Gildea, 2005; Giménez and Màrquez, 2007; Liu et al., 2010; Chen and Kuhn, 2011); (iii) morphology (Tantug et al., 2008); (iv) semantics (Dahlmeier et al., 2011; Lo et al., 2012) and (v) discourse (Guzmán et al., 2014b; Joty et al., 2014). Generally, these metrics have been focused on translation into English. However, there has been little attention into their direct applicability to languages with rich morphology. Our work focuses on automatic evaluation of translation into morphologically rich languages, Arabic more specifically. In that sense, our work is related to AL-BLEU (Bouamor et al., 2014) which is an adaptation of BLEU that gives partial credits for stem and morphological matchings of hypothesis and reference words. Here, in addition to using lexical information captured by n-gram metrics, we s"
C16-1132,P15-1078,1,0.842391,"ifferent parts of the natural language processing (NLP) community, including MT and MT evaluation. One of the main advantages of such models is the use of distributed word representations (embeddings). It has been shown that word embeddings are able to capture to certain semantic and syntactic aspects of words (Mikolov et al., 2013). Further refinements allow the inclusion of morphological information into distributed representations (Cotterell and Schütze, 2015). Word embeddings have been shown to help with modeling textual similarity well in the context of MT evaluation for MT into English (Guzmán et al., 2015), and community Question Answering (Guzmán et al., 2016). Nonetheless little exploration has been done on the use of embeddings for MT into MRL. In this paper, we investigate how embeddings obtained from different levels of lexical and morphosyntactic linguistic analysis can improve MT evaluation into a MRL. Specifically we report on Arabic, a language with complex and rich morphology paired with a high degree of ambiguity (Habash, 2010). Our results show that using a pairwise neural-network over different representations produces results This work is licensed under a Creative Commons Attribut"
C16-1132,P16-2075,1,0.862944,"community, including MT and MT evaluation. One of the main advantages of such models is the use of distributed word representations (embeddings). It has been shown that word embeddings are able to capture to certain semantic and syntactic aspects of words (Mikolov et al., 2013). Further refinements allow the inclusion of morphological information into distributed representations (Cotterell and Schütze, 2015). Word embeddings have been shown to help with modeling textual similarity well in the context of MT evaluation for MT into English (Guzmán et al., 2015), and community Question Answering (Guzmán et al., 2016). Nonetheless little exploration has been done on the use of embeddings for MT into MRL. In this paper, we investigate how embeddings obtained from different levels of lexical and morphosyntactic linguistic analysis can improve MT evaluation into a MRL. Specifically we report on Arabic, a language with complex and rich morphology paired with a high degree of ambiguity (Habash, 2010). Our results show that using a pairwise neural-network over different representations produces results This work is licensed under a Creative Commons Attribution 4.0 International License. //creativecommons.org/lic"
C16-1132,N06-2013,1,0.667533,"eatures representing different levels lexical and morphosyntactic information. As a baseline, we also used several MT metrics that are based on n-gram matches. Lexical units A distinguishing characteristic of Arabic morphology is the presence of concatenative morphemes, where words are formed via concatenations of stems, affixes and clitics. To allow our system to model how morphemes interact at a finer level, we split the morphemes. We used MADAMIRA (Pasha et al., 2014), the state-of-the-art morphological analyzer and disambiguator, to perform morphological tokenization following ATB scheme (Habash and Sadat, 2006). We extracted two forms of lexical features: NORM and TOKEN, which are tokens with and without Alef/Yaa normalization, respectively. We also extract the LEMMA feature; a morphological abstraction that represents words related by inflectional morphology. Morpho-Syntactic units We extracted part-of-speech (POS) tags according to different POS tagsets including: (i) CAT I BPOS(Habash et al., 2009), (ii) K ULICK POS1 (Kulick et al., 2006), (iii) B UCKWALTER POS (Buckwalter, 2004) and (iv) S TANFORD POS tagsets. These tagsets differ in their richness and complexity they capture. CAT I BPOS is the"
C16-1132,W14-3352,1,0.852249,"espite its well-known shortcomings (Callison-Burch et al., 2006), BLEU continues to be the de-facto MT evaluation metric. Several studies have attempted to improve upon it by taking into account different aspects of linguistic structures including: (i) synonym dictionaries or paraphrase tables (Denkowski and Lavie, 2011; Snover et al., 2010); (ii) syntactic information (Liu and Gildea, 2005; Giménez and Màrquez, 2007; Liu et al., 2010; Chen and Kuhn, 2011); (iii) morphology (Tantug et al., 2008); (iv) semantics (Dahlmeier et al., 2011; Lo et al., 2012) and (v) discourse (Guzmán et al., 2014b; Joty et al., 2014). Generally, these metrics have been focused on translation into English. However, there has been little attention into their direct applicability to languages with rich morphology. Our work focuses on automatic evaluation of translation into morphologically rich languages, Arabic more specifically. In that sense, our work is related to AL-BLEU (Bouamor et al., 2014) which is an adaptation of BLEU that gives partial credits for stem and morphological matchings of hypothesis and reference words. Here, in addition to using lexical information captured by n-gram metrics, we show that using morpho"
C16-1132,W05-0904,0,0.0469328,"the preferences of human judges. Next, we present related work in Section 2. We describe our approach in detail in Section 3; and we evaluate in Section 4. We present a discussion of our findings in Section 5. 2 Related Work Despite its well-known shortcomings (Callison-Burch et al., 2006), BLEU continues to be the de-facto MT evaluation metric. Several studies have attempted to improve upon it by taking into account different aspects of linguistic structures including: (i) synonym dictionaries or paraphrase tables (Denkowski and Lavie, 2011; Snover et al., 2010); (ii) syntactic information (Liu and Gildea, 2005; Giménez and Màrquez, 2007; Liu et al., 2010; Chen and Kuhn, 2011); (iii) morphology (Tantug et al., 2008); (iv) semantics (Dahlmeier et al., 2011; Lo et al., 2012) and (v) discourse (Guzmán et al., 2014b; Joty et al., 2014). Generally, these metrics have been focused on translation into English. However, there has been little attention into their direct applicability to languages with rich morphology. Our work focuses on automatic evaluation of translation into morphologically rich languages, Arabic more specifically. In that sense, our work is related to AL-BLEU (Bouamor et al., 2014) which"
C16-1132,W10-1754,0,0.0287716,"t related work in Section 2. We describe our approach in detail in Section 3; and we evaluate in Section 4. We present a discussion of our findings in Section 5. 2 Related Work Despite its well-known shortcomings (Callison-Burch et al., 2006), BLEU continues to be the de-facto MT evaluation metric. Several studies have attempted to improve upon it by taking into account different aspects of linguistic structures including: (i) synonym dictionaries or paraphrase tables (Denkowski and Lavie, 2011; Snover et al., 2010); (ii) syntactic information (Liu and Gildea, 2005; Giménez and Màrquez, 2007; Liu et al., 2010; Chen and Kuhn, 2011); (iii) morphology (Tantug et al., 2008); (iv) semantics (Dahlmeier et al., 2011; Lo et al., 2012) and (v) discourse (Guzmán et al., 2014b; Joty et al., 2014). Generally, these metrics have been focused on translation into English. However, there has been little attention into their direct applicability to languages with rich morphology. Our work focuses on automatic evaluation of translation into morphologically rich languages, Arabic more specifically. In that sense, our work is related to AL-BLEU (Bouamor et al., 2014) which is an adaptation of BLEU that gives partial"
C16-1132,W12-3129,0,0.0215102,"discussion of our findings in Section 5. 2 Related Work Despite its well-known shortcomings (Callison-Burch et al., 2006), BLEU continues to be the de-facto MT evaluation metric. Several studies have attempted to improve upon it by taking into account different aspects of linguistic structures including: (i) synonym dictionaries or paraphrase tables (Denkowski and Lavie, 2011; Snover et al., 2010); (ii) syntactic information (Liu and Gildea, 2005; Giménez and Màrquez, 2007; Liu et al., 2010; Chen and Kuhn, 2011); (iii) morphology (Tantug et al., 2008); (iv) semantics (Dahlmeier et al., 2011; Lo et al., 2012) and (v) discourse (Guzmán et al., 2014b; Joty et al., 2014). Generally, these metrics have been focused on translation into English. However, there has been little attention into their direct applicability to languages with rich morphology. Our work focuses on automatic evaluation of translation into morphologically rich languages, Arabic more specifically. In that sense, our work is related to AL-BLEU (Bouamor et al., 2014) which is an adaptation of BLEU that gives partial credits for stem and morphological matchings of hypothesis and reference words. Here, in addition to using lexical infor"
C16-1132,P14-5010,0,0.00285382,"xtracted part-of-speech (POS) tags according to different POS tagsets including: (i) CAT I BPOS(Habash et al., 2009), (ii) K ULICK POS1 (Kulick et al., 2006), (iii) B UCKWALTER POS (Buckwalter, 2004) and (iv) S TANFORD POS tagsets. These tagsets differ in their richness and complexity they capture. CAT I BPOS is the simplest with only 6 base tags2 , B UCKWALTER POS is the richest with 485 base tags, and K ULICK POS and S TANFORD POS come inbetween with 43 and 32 base tags, respectively. These tags were extracted using MADAMIRA, except for the Stanford tags, for which we used Stanford CoreNLP (Manning et al., 2014). Table 1 illustrates an example sentence with its lexical and morpho-syntactic features.3 ˇ bldhm − ςAd AlmSrywn Alðyn AxtTfwA Alý Sentence: TOKEN NORM Ñë+ +hm bld YÊK. ˇ Alý Ñë+ YÊK. ú Í@ Aly úÍ@ ˇ +hm bld YÊK . Ñë + LEMMA úÍ@ Jk@ @ñ®¢ AxtTfwA Jk@ @ñ®¢ AxtTfwA  ¢ Jk@ QåÖÏ@ XA« Jk@ áK YË@ ÑëYÊK. úÍ@ @ñ®¢ àñK àñK QåÖÏ@ áK YË@ XA« Alðyn AlmSrywn ςAd QåÖÏ@ áK YË@ àñK XA« Alðyn  ø Y Ë@ AlmSrywn ø Q åÓ XA« +hum balad Ailaý Aix.taTaf Al∼aðiy CAT I BPOS +NOM NOM PRT VRB-PASS NOM NOM VRB K ULICK POS +PRP$ NN IN VBN WP DT+NNS VBD S TANFORD POS PRP$ NN IN VBN WP DTNNS VBD +POSS _PRON _3MP NO"
C16-1132,C12-1121,1,0.84926,"¨ ¨ ¬   È Ð à è ð ø Â b t θ j H x d ð r z s š S D T Dˇ ς γ f q k l m n h w y, and 2 . .  w, ˇ @ A, ¯ ð' ˆ the additional symbols: Z ’, @ Â, @ A, Zø' yˆ , è ~, ø ý. Diacritics are represented as:  a,  u,  i,  .,  ã,  u˜ ,  ˜ı, and  ∼. 1401 obtain sentence-level representations for each of the translations and the references, we used additive composition (Mitchell and Lapata, 2010) with dropping unknown words. N-gram MT metrics We used the different n-gram based metrics to serve as a benchmark, and as additional features that capture lexical similarity. We used: BLEU+1 (Nakov et al., 2012),NIST (Doddington, 2002); M ETEOR (Denkowski and Lavie, 2011), 1-TER (Snover et al., 2006), and AL-BLEU (Bouamor et al., 2014), to compute scores at the sentence-level. For consistency with previous work, we report scores over words, and not over morphemes. 4 Experimental Setup In this section, we describe the experimental settings we used through our study. First, we introduce our evaluation criteria, then we elaborate on the dataset and various settings we used for our experiments. 4.1 Performance evaluation Automatic evaluation metrics are evaluated based on their correlation with human-per"
C16-1132,pasha-etal-2014-madamira,1,0.889937,"Missing"
C16-1132,2006.amta-papers.25,0,0.0619799,"nd 2 . .  w, ˇ @ A, ¯ ð' ˆ the additional symbols: Z ’, @ Â, @ A, Zø' yˆ , è ~, ø ý. Diacritics are represented as:  a,  u,  i,  .,  ã,  u˜ ,  ˜ı, and  ∼. 1401 obtain sentence-level representations for each of the translations and the references, we used additive composition (Mitchell and Lapata, 2010) with dropping unknown words. N-gram MT metrics We used the different n-gram based metrics to serve as a benchmark, and as additional features that capture lexical similarity. We used: BLEU+1 (Nakov et al., 2012),NIST (Doddington, 2002); M ETEOR (Denkowski and Lavie, 2011), 1-TER (Snover et al., 2006), and AL-BLEU (Bouamor et al., 2014), to compute scores at the sentence-level. For consistency with previous work, we report scores over words, and not over morphemes. 4 Experimental Setup In this section, we describe the experimental settings we used through our study. First, we introduce our evaluation criteria, then we elaborate on the dataset and various settings we used for our experiments. 4.1 Performance evaluation Automatic evaluation metrics are evaluated based on their correlation with human-performed evaluations (Soricut and Brill, 2004). In this work, we use Kendall’s τ , a coeffic"
C16-1132,P13-1045,0,0.0302945,"mán et al., 2015; Gupta et al., 2015). Our work is related to Guzmán et al. (2015), in several levels of lexical, syntactic and semantic are combined in a compact fashion using a pairwise neural framework. There are several differences between that work and ours: (i) we do not use syntactic embedding representations, (ii) we include additional pairwise features, namely the pairwise cosine similarity between embeddings; and (ii) we focus on an MRL language. While use of syntactic representations has proven a useful component to evaluate English, it relies heavily on an syntactic neural parser (Socher et al., 2013), which increases the complexity of the evaluation setup, and is not readily available for every language. Here, we instead use morpho-syntactic representations which capture both syntactic and morphological aspects of language. In our experiments, these simple representations are powerful enough to provide state-of-the-art performance. In this work, we use neural network models to improve MT evaluation into Arabic using representations that capture morphology. Morphological structure has been shown to improve the quality of word clusters (Clark, 2003), word vector representations (Cotterell a"
C16-1132,P04-1078,0,0.0560723,"002); M ETEOR (Denkowski and Lavie, 2011), 1-TER (Snover et al., 2006), and AL-BLEU (Bouamor et al., 2014), to compute scores at the sentence-level. For consistency with previous work, we report scores over words, and not over morphemes. 4 Experimental Setup In this section, we describe the experimental settings we used through our study. First, we introduce our evaluation criteria, then we elaborate on the dataset and various settings we used for our experiments. 4.1 Performance evaluation Automatic evaluation metrics are evaluated based on their correlation with human-performed evaluations (Soricut and Brill, 2004). In this work, we use Kendall’s τ , a coefficient that measures the agreement between rankings produced by human judgments and rankings produced by an automatic metric, at the sentence-level. We use the WMT’12 (workshop of machine translation) definition of Kendall’s τ that ignores ties, and is calculated as follows: [τ = (# concordant pairs − # discordant pairs) /total pairs], where the # concordant pairs is the number of times the human judgment and the automatic metric agree in the ranking of any two translations that belong to the same source sentence. The # discordant pairs is the opposi"
C18-1113,L18-1577,0,0.112743,"Missing"
C18-1113,C16-1115,0,0.035577,"and more advanced deep learning methods were submitted. High-order character n-grams extracted from speech or phonetic transcripts and i-vectors (a low-dimensional representation of audio recordings) were shown to be the most successful and efficient features (Butnaru and Ionescu, 2018), while deep learning approaches (Belinkov and Glass, 2016) did not perform well. Recently, there are more efforts to collect and annotate datasets for dialect identification. AbdulMageed et al. (2018) present a large dataset from Twitter domain covering dialects from 29 major Arab cities in 10 Arab countries. Al-Badrashiny and Diab (2016) present a system that detects points of codeswitching in sentences between MSA and dialectal Arabic. Most, if not all of the approaches, proposed in the literature have been exploring DID at the regional or country level. To the best of our knowledge, this is the first fine-grained DID system covering the dialects of 25 cities from several countries, including cities in the same country in the Arab World. Moreover, this is the first study pinpointing Arabic DID, discussing the difference between regional and city-level identification and redrawing the geographical map for Arabic DID. Furtherm"
C18-1113,W16-4819,0,0.171986,"main regions: Egyptian, Gulf, Levantine and North African, in addition to MSA. The dataset used in these tasks is different from the dataset we use in this work in its genre, size and the dialects covered. Several systems implementing a range of traditional supervised learning and more advanced deep learning methods were submitted. High-order character n-grams extracted from speech or phonetic transcripts and i-vectors (a low-dimensional representation of audio recordings) were shown to be the most successful and efficient features (Butnaru and Ionescu, 2018), while deep learning approaches (Belinkov and Glass, 2016) did not perform well. Recently, there are more efforts to collect and annotate datasets for dialect identification. AbdulMageed et al. (2018) present a large dataset from Twitter domain covering dialects from 29 major Arab cities in 10 Arab countries. Al-Badrashiny and Diab (2016) present a system that detects points of codeswitching in sentences between MSA and dialectal Arabic. Most, if not all of the approaches, proposed in the literature have been exploring DID at the regional or country level. To the best of our knowledge, this is the first fine-grained DID system covering the dialects o"
C18-1113,W14-3612,1,0.881669,"ptian and I.JºK AK An example of phonological differences is in the pronunciation of dialectal words whose MSA cognate  has the letter Qaf (  q). It is often observed that in Tunisian Arabic, this consonant appears as /q/ (similar to MSA), while in Egyptian and Levantine Arabic it is /P/ (glottal stop) and in Gulf Arabic it is /G/ (Haeri, 1991; Habash, 2010). It should be also noted that while MSA has an established standard orthography, the dialects do not. Often people write words reflecting the phonology or the history (etymology) of these words. DA is sometimes written in Roman script (Bies et al., 2014). In the context of NLP, a set of conventional orthography guidelines (CODA) has been proposed, but only for specific dialects (Habash et al., 2018). Despite these differences, distinguishing between dialects is a very challenging task because: (i) dialects use the same writing script (not in a conventionalized way) and share part of the vocabulary; and (ii) Arabic speakers usually resort to repeated code-switching between their dialect and MSA (AbuMelhim, 1991; Bassiouney, 2009), creating sentences with different levels/percentages of dialectness. More discussion on the similarity between dia"
C18-1113,W18-0519,0,0.0312111,"ranscripts along with acoustic features for dialects of four main regions: Egyptian, Gulf, Levantine and North African, in addition to MSA. The dataset used in these tasks is different from the dataset we use in this work in its genre, size and the dialects covered. Several systems implementing a range of traditional supervised learning and more advanced deep learning methods were submitted. High-order character n-grams extracted from speech or phonetic transcripts and i-vectors (a low-dimensional representation of audio recordings) were shown to be the most successful and efficient features (Butnaru and Ionescu, 2018), while deep learning approaches (Belinkov and Glass, 2016) did not perform well. Recently, there are more efforts to collect and annotate datasets for dialect identification. AbdulMageed et al. (2018) present a large dataset from Twitter domain covering dialects from 29 major Arab cities in 10 Arab countries. Al-Badrashiny and Diab (2016) present a system that detects points of codeswitching in sentences between MSA and dialectal Arabic. Most, if not all of the approaches, proposed in the literature have been exploring DID at the regional or country level. To the best of our knowledge, this i"
C18-1113,D14-1154,0,0.340509,"edicated to discriminating between language varieties (Malmasi et al., 2016; Zampieri et al., 2017). This is not surprising considering the importance of automatic DID for several NLP tasks, where prior knowledge about the dialect of an input text can be helpful, such as machine translation (Salloum et al., 2014), sentiment analysis (Al-Twairesh et al., 2016), or author profiling (Sadat et al., 2014). For Arabic DID, previous work typically targeted coarse-grained five dialect classes plus Standard Arabic at most (6-way classification) (Zaidan and Callison-Burch, 2014; Elfardy and Diab, 2013; Darwish et al., 2014). In this paper, we tackle a finer-grained dialect classification task, covering 25 cities from across the Arab World (from Rabat to Muscat), in addition to Standard Arabic. Table 1 shows the break up we follow in choosing these cities. The table relates the typical five-way regional break up of Arabic dialects (Habash, 2010) to a more refined ten-way sub-region division, and even further into 25 cities. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 1332 Proceedings of the 27th International C"
C18-1113,elfardy-diab-2012-simplified,0,0.0406909,"ntation and exploring different datasets has attracted increasing attention in recent years. Shoufan and Alameri (2015) and Al-Ayyoub et al. (2017) present a survey on NLP and deep learning methods for processing Arabic dialectal data with an overview on Arabic DID of text and speech. Biadsy and Hirschberg (2009) presented a system that identifies dialectal words in speech and their dialect of origin (on four regional Arabic dialects) from acoustic signals. In the same context, Bougrine et al. (2017) propose a hierarchical classification approach for spoken Arabic Algerian DID, using prosody. Diab and Elfardy (2012) presented a set of guidelines for token-level identification of dialectness. They later proposed a supervised approach for identifying whether a given sentence is prevalently MSA or Egyptian (Elfardy and Diab, 2013) using the Arabic online commentary dataset (AOC) (Zaidan and Callison-Burch, 2011). Their system (Elfardy and Diab, 2012) combines a token-level DID approach with other features to train a Naive-Bayes classifier. Similarly, Tillmann et al. (2014) use a linear SVM 1333 classifier to label the AOC dataset. Also, El-Haj et al. (2018) used grammatical, stylistic and Subtractive Bivale"
C18-1113,L18-1573,0,0.038802,"for spoken Arabic Algerian DID, using prosody. Diab and Elfardy (2012) presented a set of guidelines for token-level identification of dialectness. They later proposed a supervised approach for identifying whether a given sentence is prevalently MSA or Egyptian (Elfardy and Diab, 2013) using the Arabic online commentary dataset (AOC) (Zaidan and Callison-Burch, 2011). Their system (Elfardy and Diab, 2012) combines a token-level DID approach with other features to train a Naive-Bayes classifier. Similarly, Tillmann et al. (2014) use a linear SVM 1333 classifier to label the AOC dataset. Also, El-Haj et al. (2018) used grammatical, stylistic and Subtractive Bivalency Profiling features for dialect identification on the AOC dataset. Sadat et al. (2014) presented a bi-gram character-level model to identify the dialect of sentences in the social media context among dialects of 18 Arab countries. More recently, discriminating between Arabic Dialects has been the goal of a dedicated shared task (Zampieri et al., 2017; Malmasi et al., 2016), encouraging researchers to submit systems to recognize the dialect of speech transcripts along with acoustic features for dialects of four main regions: Egyptian, Gulf,"
C18-1113,C12-2029,0,0.027042,"identifies dialectal words in speech and their dialect of origin (on four regional Arabic dialects) from acoustic signals. In the same context, Bougrine et al. (2017) propose a hierarchical classification approach for spoken Arabic Algerian DID, using prosody. Diab and Elfardy (2012) presented a set of guidelines for token-level identification of dialectness. They later proposed a supervised approach for identifying whether a given sentence is prevalently MSA or Egyptian (Elfardy and Diab, 2013) using the Arabic online commentary dataset (AOC) (Zaidan and Callison-Burch, 2011). Their system (Elfardy and Diab, 2012) combines a token-level DID approach with other features to train a Naive-Bayes classifier. Similarly, Tillmann et al. (2014) use a linear SVM 1333 classifier to label the AOC dataset. Also, El-Haj et al. (2018) used grammatical, stylistic and Subtractive Bivalency Profiling features for dialect identification on the AOC dataset. Sadat et al. (2014) presented a bi-gram character-level model to identify the dialect of sentences in the social media context among dialects of 18 Arab countries. More recently, discriminating between Arabic Dialects has been the goal of a dedicated shared task (Zamp"
C18-1113,P13-2081,0,0.520396,"luation campaigns were dedicated to discriminating between language varieties (Malmasi et al., 2016; Zampieri et al., 2017). This is not surprising considering the importance of automatic DID for several NLP tasks, where prior knowledge about the dialect of an input text can be helpful, such as machine translation (Salloum et al., 2014), sentiment analysis (Al-Twairesh et al., 2016), or author profiling (Sadat et al., 2014). For Arabic DID, previous work typically targeted coarse-grained five dialect classes plus Standard Arabic at most (6-way classification) (Zaidan and Callison-Burch, 2014; Elfardy and Diab, 2013; Darwish et al., 2014). In this paper, we tackle a finer-grained dialect classification task, covering 25 cities from across the Arab World (from Rabat to Muscat), in addition to Standard Arabic. Table 1 shows the break up we follow in choosing these cities. The table relates the typical five-way regional break up of Arabic dialects (Habash, 2010) to a more refined ten-way sub-region division, and even further into 25 cities. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 1332 Proceedings of t"
C18-1113,P06-1086,1,0.779032,"l translations. An example of a 28-way parallel sentence (25 cities plus MSA, English and French) extracted from C ORPUS -26 is given in Figure 1. Data pre-processing and splitting In our experiments, we only tokenize the sentences in both C ORPUS -6 and C ORPUS -26 using punctuation marks. Morphological analysis has been shown to improve the performance of DID systems for a small number of dialects (Darwish et al., 2014). However, the number and sophistication of morphological analysis and segmentation tools for DA are very limited (Pasha et al., 2014), cover only a small number of dialects (Habash and Rambow, 2006; Habash et al., 2012b; Khalifa et al., 2017) and unavailable for most of the others. We split each corpus into Train, Development (Dev) and Test sets. The splits are balanced for each dialect and the distribution of each split is given in Table 2. We use TRAIN, DEVELOPMENT and TEST terms with C ORPUS -6 and C ORPUS -26 to refer to the training, development and test sets of the specified corpus. We use the term MODEL to refer to the trained system. Pairwise similarity between dialects In order to get a Train Dev Test Classes sense of the complexity of our task, we explore the deC ORPUS -6 9000"
C18-1113,habash-etal-2012-conventional,1,0.950925,"ic (which may include Sudan), North African Arabic (vaguely covering Morocco, Algeria, Tunisia, Libya and Mauritania), and Yemeni Arabic (Habash, 2010). However, within each of these regional groups, there is significant variation down to the village, town, and city levels. Arabic dialects differ from one another and from MSA on all levels of linguistic representation, from phonology and morphology to lexicon and syntax (Watson, 2007).1 The number of lexical differences is  @ ÂwD¯ « γrf¯h, Lybian P@X dAr and Tunisian ˇ h ‘room’ corresponds to MSA é¯Q significant i.e., Egyptian éð  K. byt (Habash et al., 2012a).2 Morphological differences are also quite common. One example is the I future marker particle which appears as +  sa+ or ¬ñ sawfa in MSA, + hHa+ or hP raH in Levantine  . bAš in Tunisian. This together with the variation in the templatic morphology make the AK   forms of some verbs rather different: e.g., ’I will write’ is I . J» A sa Âaktubu in MSA, I.J» Ag HaÂaktub dialects and 1 Comparative studies of several Arabic dialects suggest that the syntactic differences between the dialects are minor (Benmamoun, 2012). 2 Arabic transliteration is presented in the Habash-Soudi-Buckwalte"
C18-1113,W12-2301,1,0.925705,"ic (which may include Sudan), North African Arabic (vaguely covering Morocco, Algeria, Tunisia, Libya and Mauritania), and Yemeni Arabic (Habash, 2010). However, within each of these regional groups, there is significant variation down to the village, town, and city levels. Arabic dialects differ from one another and from MSA on all levels of linguistic representation, from phonology and morphology to lexicon and syntax (Watson, 2007).1 The number of lexical differences is  @ ÂwD¯ « γrf¯h, Lybian P@X dAr and Tunisian ˇ h ‘room’ corresponds to MSA é¯Q significant i.e., Egyptian éð  K. byt (Habash et al., 2012a).2 Morphological differences are also quite common. One example is the I future marker particle which appears as +  sa+ or ¬ñ sawfa in MSA, + hHa+ or hP raH in Levantine  . bAš in Tunisian. This together with the variation in the templatic morphology make the AK   forms of some verbs rather different: e.g., ’I will write’ is I . J» A sa Âaktubu in MSA, I.J» Ag HaÂaktub dialects and 1 Comparative studies of several Arabic dialects suggest that the syntactic differences between the dialects are minor (Benmamoun, 2012). 2 Arabic transliteration is presented in the Habash-Soudi-Buckwalte"
C18-1113,W11-2123,0,0.0254713,"g data. 1336 0.65 0.60 Token Dissimilarity 0.55 0.50 0.45 0.40 0.35 0.30 MO S BA G BA S BE N TR I SA N DO H JE D RIY TU N SF X AL G RA B FE S KH A MS A MU S AL X CA I AS W SA L AM M JE R BE I DA M AL E 0.25 Dialects Figure 2: Pairwise similarity between dialects in Corpus-26 word or character counts or representation for text classification) (Manning et al., 2008). Baseline We follow the approach described in Zaidan and Callison-Burch (2014) for dialect identification and adapt it to build our baseline model. For each dialect, we train a 5-gram character level language model (LM) using KenLM (Heafield, 2011) with default parameters and Kneser–Ney smoothing. Then, we use the LM to assign to each sentence S, the dialect Di that maximizes its conditional probability score argmaxi P (Di |S). Character-based LMs leverage subword information and are generally good for capturing particular peculiarities that are specific to certain dialects such as the use of certain cli tics, affixes or internal base word structure. For example, the word prefixes K X @ Âdy, JK. bt and ËAë hAl  X @ Âdyš btxdm hAlfyzA ? ‘How long is this depicted by the character n-gram LM in ? @Q ®ËAë ÐYjJK.  visa good for?’ are"
C18-1113,W16-4801,0,0.0610176,"Missing"
C18-1113,pasha-etal-2014-madamira,1,0.919885,"Missing"
C18-1113,W14-5904,0,0.54531,"g the dialect of a particular segment of speech or text of any size (i.e., word, sentence, or document). This task has attracted increasing attention in recent years. For instance, several evaluation campaigns were dedicated to discriminating between language varieties (Malmasi et al., 2016; Zampieri et al., 2017). This is not surprising considering the importance of automatic DID for several NLP tasks, where prior knowledge about the dialect of an input text can be helpful, such as machine translation (Salloum et al., 2014), sentiment analysis (Al-Twairesh et al., 2016), or author profiling (Sadat et al., 2014). For Arabic DID, previous work typically targeted coarse-grained five dialect classes plus Standard Arabic at most (6-way classification) (Zaidan and Callison-Burch, 2014; Elfardy and Diab, 2013; Darwish et al., 2014). In this paper, we tackle a finer-grained dialect classification task, covering 25 cities from across the Arab World (from Rabat to Muscat), in addition to Standard Arabic. Table 1 shows the break up we follow in choosing these cities. The table relates the typical five-way regional break up of Arabic dialects (Habash, 2010) to a more refined ten-way sub-region division, and eve"
C18-1113,P14-2125,1,0.87917,". Ë@ 1 Introduction Dialect identification (DID) is the task of automatically identifying the dialect of a particular segment of speech or text of any size (i.e., word, sentence, or document). This task has attracted increasing attention in recent years. For instance, several evaluation campaigns were dedicated to discriminating between language varieties (Malmasi et al., 2016; Zampieri et al., 2017). This is not surprising considering the importance of automatic DID for several NLP tasks, where prior knowledge about the dialect of an input text can be helpful, such as machine translation (Salloum et al., 2014), sentiment analysis (Al-Twairesh et al., 2016), or author profiling (Sadat et al., 2014). For Arabic DID, previous work typically targeted coarse-grained five dialect classes plus Standard Arabic at most (6-way classification) (Zaidan and Callison-Burch, 2014; Elfardy and Diab, 2013; Darwish et al., 2014). In this paper, we tackle a finer-grained dialect classification task, covering 25 cities from across the Arab World (from Rabat to Muscat), in addition to Standard Arabic. Table 1 shows the break up we follow in choosing these cities. The table relates the typical five-way regional break up"
C18-1113,W15-3205,0,0.0261751,"d analysis and discussion on dialect confusability, optimal classification and tweet dialect classification. Finally, we conclude and give our future directions in Section 6. 2 Related Work Working on DID is more challenging than just recognizing a specific language (Etman and Beex, 2015). Since Arabic dialects use the same script and share part of the vocabulary, it is quite arduous to distinguish between them. Hence, developing an automatic identification system working at different levels of representation and exploring different datasets has attracted increasing attention in recent years. Shoufan and Alameri (2015) and Al-Ayyoub et al. (2017) present a survey on NLP and deep learning methods for processing Arabic dialectal data with an overview on Arabic DID of text and speech. Biadsy and Hirschberg (2009) presented a system that identifies dialectal words in speech and their dialect of origin (on four regional Arabic dialects) from acoustic signals. In the same context, Bougrine et al. (2017) propose a hierarchical classification approach for spoken Arabic Algerian DID, using prosody. Diab and Elfardy (2012) presented a set of guidelines for token-level identification of dialectness. They later propose"
C18-1113,W14-5313,0,0.0251541,"the same context, Bougrine et al. (2017) propose a hierarchical classification approach for spoken Arabic Algerian DID, using prosody. Diab and Elfardy (2012) presented a set of guidelines for token-level identification of dialectness. They later proposed a supervised approach for identifying whether a given sentence is prevalently MSA or Egyptian (Elfardy and Diab, 2013) using the Arabic online commentary dataset (AOC) (Zaidan and Callison-Burch, 2011). Their system (Elfardy and Diab, 2012) combines a token-level DID approach with other features to train a Naive-Bayes classifier. Similarly, Tillmann et al. (2014) use a linear SVM 1333 classifier to label the AOC dataset. Also, El-Haj et al. (2018) used grammatical, stylistic and Subtractive Bivalency Profiling features for dialect identification on the AOC dataset. Sadat et al. (2014) presented a bi-gram character-level model to identify the dialect of sentences in the social media context among dialects of 18 Arab countries. More recently, discriminating between Arabic Dialects has been the goal of a dedicated shared task (Zampieri et al., 2017; Malmasi et al., 2016), encouraging researchers to submit systems to recognize the dialect of speech transc"
C18-1113,P11-1122,0,0.0544994,"sy and Hirschberg (2009) presented a system that identifies dialectal words in speech and their dialect of origin (on four regional Arabic dialects) from acoustic signals. In the same context, Bougrine et al. (2017) propose a hierarchical classification approach for spoken Arabic Algerian DID, using prosody. Diab and Elfardy (2012) presented a set of guidelines for token-level identification of dialectness. They later proposed a supervised approach for identifying whether a given sentence is prevalently MSA or Egyptian (Elfardy and Diab, 2013) using the Arabic online commentary dataset (AOC) (Zaidan and Callison-Burch, 2011). Their system (Elfardy and Diab, 2012) combines a token-level DID approach with other features to train a Naive-Bayes classifier. Similarly, Tillmann et al. (2014) use a linear SVM 1333 classifier to label the AOC dataset. Also, El-Haj et al. (2018) used grammatical, stylistic and Subtractive Bivalency Profiling features for dialect identification on the AOC dataset. Sadat et al. (2014) presented a bi-gram character-level model to identify the dialect of sentences in the social media context among dialects of 18 Arab countries. More recently, discriminating between Arabic Dialects has been th"
C18-1113,W17-1201,0,0.118119,"Missing"
D12-1066,P05-1074,0,0.234533,"f a pair are extracted from the other sentence, and the intersection of the sets for both directions is kept. Edit rate on word sequences (T ERp ) The T ERp tool (Snover et al., 2010) can be used to compute an optimal set of word and phrase edits that can transform one sentence into another one.9 Edit types are parameterized by one or more weights which were optimized towards F-measure by hill climbing with 100 random restarts using the held-out data set consisting of 125 sentence pairs for each corpus type. Translational equivalence (P IVOT) We exploited the paraphrase probability defined by Bannard and Callison-Burch (2005) on bilingual parallel corpora. We used the Europarl corpus10 of parliamentary debates in English and French, consisting of approximately 1.7 million parallel sentences, using each language as source and pivot in turn. G IZA ++ 9 Note that contrarily to what T ERp allows, we did not used the possibility of using word or phrase equivalents as those are only made available for English. This type of knowledge is however captured in part by the FASTR and P IVOT systems. 10 http://statmt.org/europarl Phrase pair features – edit distance between paraphrases, stem identity, bag-of-tokens similarity,"
D12-1066,W03-1004,0,0.0363564,"ally, there are substantially more positive paraphrase examples for French (19,427) than for English (12,593). 4 Related work Over the years, paraphrase acquisition and generation have attracted a wealth of research works that are too many to adequatly summarize here: (Madnani and Dorr, 2010) presents a complete and upto-date review of the main approaches. Sentential paraphrase collection has been tackled from specific resources increasing the probability of sentences being paraphrases (Dolan et al., 2004; Bernhard and Gurevych, 2008; Wubben et al., 2009), from comparable monolingual corpora (Barzilay and Elhadad, 2003; Fung and Cheung, 2004; Nelken and Shieber, 2006), and even at web scale (Pasc¸a and Dienes, 2005; Bhagat and Ravichandran, 2008). Various techniques have been proposed for paraphrase acquisition from related sentence pairs (Barzilay and McKeown, 2001; Pang et al., 2003) and from bilingual parallel corpora (Bannard and Callison-Burch, 2005; Kok and Brockett, 2010). The issue of corpus construction for developing and evaluating paraphrase acquisition techniques are addressed in (Cohn et al., 2008; Callison-Burch et al., 2008). To the best of our knowledge, this is the first time that a study i"
D12-1066,P01-1008,0,0.118027,"summarize here: (Madnani and Dorr, 2010) presents a complete and upto-date review of the main approaches. Sentential paraphrase collection has been tackled from specific resources increasing the probability of sentences being paraphrases (Dolan et al., 2004; Bernhard and Gurevych, 2008; Wubben et al., 2009), from comparable monolingual corpora (Barzilay and Elhadad, 2003; Fung and Cheung, 2004; Nelken and Shieber, 2006), and even at web scale (Pasc¸a and Dienes, 2005; Bhagat and Ravichandran, 2008). Various techniques have been proposed for paraphrase acquisition from related sentence pairs (Barzilay and McKeown, 2001; Pang et al., 2003) and from bilingual parallel corpora (Bannard and Callison-Burch, 2005; Kok and Brockett, 2010). The issue of corpus construction for developing and evaluating paraphrase acquisition techniques are addressed in (Cohn et al., 2008; Callison-Burch et al., 2008). To the best of our knowledge, this is the first time that a study in paraphrase acquisition is conducted on several corpus types and for 2 languages. Faruqui and Pad´o (2011) study the acquisition of entailment pairs (premise and hypothesis), with experiments in 3 languages and various domains of newspaper corpora for"
D12-1066,W08-0906,0,0.0143987,"type contains more than half of the total number of examples for the two languages. Finally, there are substantially more positive paraphrase examples for French (19,427) than for English (12,593). 4 Related work Over the years, paraphrase acquisition and generation have attracted a wealth of research works that are too many to adequatly summarize here: (Madnani and Dorr, 2010) presents a complete and upto-date review of the main approaches. Sentential paraphrase collection has been tackled from specific resources increasing the probability of sentences being paraphrases (Dolan et al., 2004; Bernhard and Gurevych, 2008; Wubben et al., 2009), from comparable monolingual corpora (Barzilay and Elhadad, 2003; Fung and Cheung, 2004; Nelken and Shieber, 2006), and even at web scale (Pasc¸a and Dienes, 2005; Bhagat and Ravichandran, 2008). Various techniques have been proposed for paraphrase acquisition from related sentence pairs (Barzilay and McKeown, 2001; Pang et al., 2003) and from bilingual parallel corpora (Bannard and Callison-Burch, 2005; Kok and Brockett, 2010). The issue of corpus construction for developing and evaluating paraphrase acquisition techniques are addressed in (Cohn et al., 2008; Callison-B"
D12-1066,P08-1077,0,0.0228721,"Over the years, paraphrase acquisition and generation have attracted a wealth of research works that are too many to adequatly summarize here: (Madnani and Dorr, 2010) presents a complete and upto-date review of the main approaches. Sentential paraphrase collection has been tackled from specific resources increasing the probability of sentences being paraphrases (Dolan et al., 2004; Bernhard and Gurevych, 2008; Wubben et al., 2009), from comparable monolingual corpora (Barzilay and Elhadad, 2003; Fung and Cheung, 2004; Nelken and Shieber, 2006), and even at web scale (Pasc¸a and Dienes, 2005; Bhagat and Ravichandran, 2008). Various techniques have been proposed for paraphrase acquisition from related sentence pairs (Barzilay and McKeown, 2001; Pang et al., 2003) and from bilingual parallel corpora (Bannard and Callison-Burch, 2005; Kok and Brockett, 2010). The issue of corpus construction for developing and evaluating paraphrase acquisition techniques are addressed in (Cohn et al., 2008; Callison-Burch et al., 2008). To the best of our knowledge, this is the first time that a study in paraphrase acquisition is conducted on several corpus types and for 2 languages. Faruqui and Pad´o (2011) study the acquisition"
D12-1066,E12-1073,1,0.841294,"e pair candidates that include possible reference paraphrases will not penalize precision while not increasing recall. All performance values reported in the following sections will be obtained using 10-fold crossvalidation and averaging the results on each sub-test. All data sets of cross-validation contain 500 sentence pairs per corpus type, and 125 pairs are kept for development. 3.2 A framework for sub-sentential paraphrase identification We now describe the systems that will be tested on the various corpora described in section 2 using the methodology described in section 3.1. Following (Bouamor et al., 2012), a combination system is used to automatically weight paraphrase pair candidates produced by individual systems using a set of features aiming at recognizing paraphrases, as illustrated on Figure 3. Four individual systems have been used and are described below: the reasons for considering those systems include their free avail725 Statistical learning of word alignments (G IZA) The G IZA ++ tool (Och and Ney, 2004) computes statistical word alignment models of increasing complexity from parallel corpora. It was run on each monolingual corpus of sentence pairs in both directions, symmetrized a"
D12-1066,C08-1013,0,0.0746642,"using such types of paraphrases into applications would however often be too strongly context-dependent. 724 TEXT SPEECH SCENE 70 EVENT 60 60 50 50 40 40 30 30 20 20 10 10 0 COSINE*100 BLEU 1-TER METEOR 0 COSINE*100 BLEU 1-TER METEOR Figure 2: Sentence pair average similarities for all corpora for English (left) and French (right) using the cosine of token vectors, BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and METEOR (Lavie and Agarwal, 2007). 3 3.1 Bilingual experiments across corpus types Evaluation of paraphrase acquisition We followed the PARAMETRIC methodology described in (Callison-Burch et al., 2008) for assessing the performance of systems on the task of subsentential paraphrase acquisition. In this methodology, a set of paraphrase candidates extracted from a sentence pair is compared with a set of reference paraphrases, obtained through human annotation, by computing usual measures of precision (P ) and recall (R). The first value corresponds to the proportion of paraphrase candidates, denoted H, produced by a system and that are correct relative to the reference set containing sure and possible paraphrases, denoted Rall . Recall is obtained by measuring the proportion of the reference"
D12-1066,P11-1020,0,0.141813,"escribed in (Tiedemann, 2007), based on time frames and developed for bilingual subtitles, we then filtered out sentence pairs below a minimal edit distance threshold, and manually removed obvious errors made by the algorithm. a boy rides a bike on a dirt road . So he uses the photo booths to remind people what he looks like . e.g. So he uses the photo booths to remind people what he looks like. ↔ He uses those machines to remind the living of his face. a boy is riding on a bicycle fast . Pigeons have numerical abilities just like primates S CENE We used the Multiple Video Description Corpus (Chen and Dolan, 2011) obtained from multiple descriptions of short videos. Similarly to what we did for T EXT, we selected sentence pairs from clusters by minimal edit distance above a threshold. An important fact is that for English we were able to use what is described as “verified” descriptions. There were, however, far fewer descriptions available for French, and none had the “verified” status. We decided to use this corpus nonetheless, but with the knowledge that this source for French is of a substantially lower quality (this corpus type will therefore appear as “(S CENE)” in all tables to reflect this). Pig"
D12-1066,J08-4005,0,0.137572,"d alignment matrices on Figure 1. A corpus for each type has been collected for 2 languages, English and French, and comprises 625 sentence pairs per language. We now briefly describe how each corpus was built. 721 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 721–731, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics It is anticipated that the annual total foreign trade volume will exceed US$9 billion . T EXT For English, we used the MTC corpus1 (described in (Cohn et al., 2008)) consisting of sets of news article translations from Chinese, and for French the CESTA corpus2 consisting of sets of news article translations from English. For each sentence cluster, we selected sentence pairs with minimal edit distance above an empirically-selected threshold, covering all clusters first and then selecting from already used clusters to reach the target number of sentence pairs. It is estimated that the total annual volume of import and export will exceed 9 billion US dollars . He uses those machines to remind the living of his face . e.g. It is estimated that the total annu"
D12-1066,C04-1051,0,0.0997121,"act that this corpus type contains more than half of the total number of examples for the two languages. Finally, there are substantially more positive paraphrase examples for French (19,427) than for English (12,593). 4 Related work Over the years, paraphrase acquisition and generation have attracted a wealth of research works that are too many to adequatly summarize here: (Madnani and Dorr, 2010) presents a complete and upto-date review of the main approaches. Sentential paraphrase collection has been tackled from specific resources increasing the probability of sentences being paraphrases (Dolan et al., 2004; Bernhard and Gurevych, 2008; Wubben et al., 2009), from comparable monolingual corpora (Barzilay and Elhadad, 2003; Fung and Cheung, 2004; Nelken and Shieber, 2006), and even at web scale (Pasc¸a and Dienes, 2005; Bhagat and Ravichandran, 2008). Various techniques have been proposed for paraphrase acquisition from related sentence pairs (Barzilay and McKeown, 2001; Pang et al., 2003) and from bilingual parallel corpora (Bannard and Callison-Burch, 2005; Kok and Brockett, 2010). The issue of corpus construction for developing and evaluating paraphrase acquisition techniques are addressed in ("
D12-1066,W11-0111,0,0.0659731,"Missing"
D12-1066,C04-1151,0,0.0143876,"y more positive paraphrase examples for French (19,427) than for English (12,593). 4 Related work Over the years, paraphrase acquisition and generation have attracted a wealth of research works that are too many to adequatly summarize here: (Madnani and Dorr, 2010) presents a complete and upto-date review of the main approaches. Sentential paraphrase collection has been tackled from specific resources increasing the probability of sentences being paraphrases (Dolan et al., 2004; Bernhard and Gurevych, 2008; Wubben et al., 2009), from comparable monolingual corpora (Barzilay and Elhadad, 2003; Fung and Cheung, 2004; Nelken and Shieber, 2006), and even at web scale (Pasc¸a and Dienes, 2005; Bhagat and Ravichandran, 2008). Various techniques have been proposed for paraphrase acquisition from related sentence pairs (Barzilay and McKeown, 2001; Pang et al., 2003) and from bilingual parallel corpora (Bannard and Callison-Burch, 2005; Kok and Brockett, 2010). The issue of corpus construction for developing and evaluating paraphrase acquisition techniques are addressed in (Cohn et al., 2008; Callison-Burch et al., 2008). To the best of our knowledge, this is the first time that a study in paraphrase acquisitio"
D12-1066,P08-4006,0,0.0272049,"dates differed from more than one day. We repeated the same selection procedure as for T EXT and S CENE to have a maximal cluster coverage and select more similar pairs first. e.g. Pigeons Have an Understanding of Numbers on Par With Primates ↔ Pigeons Have Numerical Abilities Just Like Primates Table 1 provides various statistics for these corpora. The first observation is that T EXT contains significantly larger sentences than the other types, more than twice as long as those of S PEECH. Annotation was performed following the guidelines proposed by Cohn et al. (2008)5 using the YAWAT tool (Germann, 2008), except that alignments where not initially obtained automatically so as not to bias our annotators’ work (there were two annotators per language). The main guidelines that they had to follow were that sure and possible paraphrases must be distinguished, smaller alignments were to be prefered but any-to-any alignments may be used, and sentences should be aligned as much as possible. Henceforth, we will only consider for all reported statistics and experiments those paraphrases that are not identity pairs (e.g. (a nice day ↔ a nice day)), as they are 4 http://news.google.com See http://staffww"
D12-1066,I11-1090,0,0.0115185,"to achieve. Laslty, the evaluation of automatically generated paraphrases has recently received some attention (Liu et al., 2010; Chen and Dolan, 2011; Metzler et al., 2011) although it remains a difficult issue. Application-driven paraphrase generation provides indirect means of evaluating paraphrase generation (Zhao et al., 2009). For instance, the field of Statistical Machine Translation has produced works showing both the usefulness of human-produced (Schroeder et al., 2009; Resnik et al., 2010) and automatically produced paraphrases (Madnani et al., 2008; Marton et al., 2009; Max, 2010; He et al., 2011) for improving translation performance. 5 Discussion and future work This work has addressed the issue of sub-sentential paraphrase acquisition from text pairs. Analoguously to bilingual parallel corpora, which are still to date the most reliable resources for automatic acquisition of sub-sentential translations, monolingual parallel corpora are generally regarded as very appropriate for paraphrase acquisition. However, their low availability makes searching for less parallel corpora a necessity. In this study, we have attempted to identify corpora of various degrees of semantic textual simila"
D12-1066,P99-1044,0,0.00825571,"ividual systems have been used and are described below: the reasons for considering those systems include their free avail725 Statistical learning of word alignments (G IZA) The G IZA ++ tool (Och and Ney, 2004) computes statistical word alignment models of increasing complexity from parallel corpora. It was run on each monolingual corpus of sentence pairs in both directions, symmetrized alignments were kept and classical phrase extraction heuristics were applied (Koehn et al., 2003), without growing phrases with unaligned tokens. Linguistic knowledge on term variation (FASTR) The FASTR tool (Jacquemin, 1999) spots term variants in large corpora, where variants are described through metarules expressing how the morphosyntactic structure of a term variant can be derived from a given term by means of regular expressions on morphosyntactic categories. Paradigmatic variation can also be expressed with constraints between words, imposing that they be of the same morphological or semantic family using existing resources available in our two languages. Variants for all phrases from one sentence of a pair are extracted from the other sentence, and the intersection of the sets for both directions is kept."
D12-1066,N03-1017,0,0.00378411,"ates produced by individual systems using a set of features aiming at recognizing paraphrases, as illustrated on Figure 3. Four individual systems have been used and are described below: the reasons for considering those systems include their free avail725 Statistical learning of word alignments (G IZA) The G IZA ++ tool (Och and Ney, 2004) computes statistical word alignment models of increasing complexity from parallel corpora. It was run on each monolingual corpus of sentence pairs in both directions, symmetrized alignments were kept and classical phrase extraction heuristics were applied (Koehn et al., 2003), without growing phrases with unaligned tokens. Linguistic knowledge on term variation (FASTR) The FASTR tool (Jacquemin, 1999) spots term variants in large corpora, where variants are described through metarules expressing how the morphosyntactic structure of a term variant can be derived from a given term by means of regular expressions on morphosyntactic categories. Paradigmatic variation can also be expressed with constraints between words, imposing that they be of the same morphological or semantic family using existing resources available in our two languages. Variants for all phrases f"
D12-1066,P07-2045,0,0.00433743,"milarity of token context vectors for each phrase of a paraphrase (derived from counts in the large English-French parallel corpus from WMT’11 (http://www.statmt.org/ wmt11/translation-task.html) (approx. 30 million parallel sentences) System features – combination of the individual systems that proposed the paraphrase pair Table 3: Features used by our classifiers. Discretized intervals based on median values are used for real values, and binarized values are used for combinations. was used for word alignment and phrase translation probabilities were estimated from them by the M OSES system (Koehn et al., 2007). For each phrase of a sentence pair, we built its set of paraphrases, and extracted its paraphrase from the other sentence with highest probability. We repeated this process in both directions, and finally kept for each phrase its paraphrase pair from any direction with highest probability. Automatic validation of candidate paraphrases Taking the union of all paraphrase pair candidates from all the above systems for each sentence pair, we perform a Maximum Entropy two-class classification11 , which allows us to include features that were not necessarily exploited or straightforward to exploit"
D12-1066,N10-1017,0,0.0142172,"paraphrase collection has been tackled from specific resources increasing the probability of sentences being paraphrases (Dolan et al., 2004; Bernhard and Gurevych, 2008; Wubben et al., 2009), from comparable monolingual corpora (Barzilay and Elhadad, 2003; Fung and Cheung, 2004; Nelken and Shieber, 2006), and even at web scale (Pasc¸a and Dienes, 2005; Bhagat and Ravichandran, 2008). Various techniques have been proposed for paraphrase acquisition from related sentence pairs (Barzilay and McKeown, 2001; Pang et al., 2003) and from bilingual parallel corpora (Bannard and Callison-Burch, 2005; Kok and Brockett, 2010). The issue of corpus construction for developing and evaluating paraphrase acquisition techniques are addressed in (Cohn et al., 2008; Callison-Burch et al., 2008). To the best of our knowledge, this is the first time that a study in paraphrase acquisition is conducted on several corpus types and for 2 languages. Faruqui and Pad´o (2011) study the acquisition of entailment pairs (premise and hypothesis), with experiments in 3 languages and various domains of newspaper corpora for one language. Although their work is not directly comparable to ours, they report that robustness across domains i"
D12-1066,W07-0734,0,0.0712034,"Missing"
D12-1066,D10-1090,0,0.012901,"al., 2008; Callison-Burch et al., 2008). To the best of our knowledge, this is the first time that a study in paraphrase acquisition is conducted on several corpus types and for 2 languages. Faruqui and Pad´o (2011) study the acquisition of entailment pairs (premise and hypothesis), with experiments in 3 languages and various domains of newspaper corpora for one language. Although their work is not directly comparable to ours, they report that robustness across domains is difficult to achieve. Laslty, the evaluation of automatically generated paraphrases has recently received some attention (Liu et al., 2010; Chen and Dolan, 2011; Metzler et al., 2011) although it remains a difficult issue. Application-driven paraphrase generation provides indirect means of evaluating paraphrase generation (Zhao et al., 2009). For instance, the field of Statistical Machine Translation has produced works showing both the usefulness of human-produced (Schroeder et al., 2009; Resnik et al., 2010) and automatically produced paraphrases (Madnani et al., 2008; Marton et al., 2009; Max, 2010; He et al., 2011) for improving translation performance. 5 Discussion and future work This work has addressed the issue of sub-sen"
D12-1066,J10-3003,0,0.11718,"escribes a study on the impact of the original signal (text, speech, visual scene, event) of a text pair on the task of both manual and automatic sub-sentential paraphrase acquisition. A corpus of 2,500 annotated sentences in English and French is described, and performance on this corpus is reported for an efficient system combination exploiting a large set of features for paraphrase recognition. A detailed quantified typology of subsentential paraphrases found in our corpus types is given. 1 Introduction Sub-sentential paraphrases can be acquired from text pairs expressing the same meaning (Madnani and Dorr, 2010). If the semantic similarity of a text pair has a direct impact on the quality of the acquired paraphrases, it has, to our knowledge, never been shown what impact the type of original signal has on paraphrase acquisition. In this work, we consider four types of corpora, which we think are representative of the main types of original semantic signals: text pairs (roughly, sentences) originating a) from independent translations of a text (T EXT), b) from independent translations of a speech (S PEECH), c) from independent descriptions of a visual scene (S CENE), and d) from independent descriptio"
D12-1066,2008.amta-papers.13,0,0.0116239,"hey report that robustness across domains is difficult to achieve. Laslty, the evaluation of automatically generated paraphrases has recently received some attention (Liu et al., 2010; Chen and Dolan, 2011; Metzler et al., 2011) although it remains a difficult issue. Application-driven paraphrase generation provides indirect means of evaluating paraphrase generation (Zhao et al., 2009). For instance, the field of Statistical Machine Translation has produced works showing both the usefulness of human-produced (Schroeder et al., 2009; Resnik et al., 2010) and automatically produced paraphrases (Madnani et al., 2008; Marton et al., 2009; Max, 2010; He et al., 2011) for improving translation performance. 5 Discussion and future work This work has addressed the issue of sub-sentential paraphrase acquisition from text pairs. Analoguously to bilingual parallel corpora, which are still to date the most reliable resources for automatic acquisition of sub-sentential translations, monolingual parallel corpora are generally regarded as very appropriate for paraphrase acquisition. However, their low availability makes searching for less parallel corpora a necessity. In this study, we have attempted to identify cor"
D12-1066,D09-1040,0,0.0290815,"ness across domains is difficult to achieve. Laslty, the evaluation of automatically generated paraphrases has recently received some attention (Liu et al., 2010; Chen and Dolan, 2011; Metzler et al., 2011) although it remains a difficult issue. Application-driven paraphrase generation provides indirect means of evaluating paraphrase generation (Zhao et al., 2009). For instance, the field of Statistical Machine Translation has produced works showing both the usefulness of human-produced (Schroeder et al., 2009; Resnik et al., 2010) and automatically produced paraphrases (Madnani et al., 2008; Marton et al., 2009; Max, 2010; He et al., 2011) for improving translation performance. 5 Discussion and future work This work has addressed the issue of sub-sentential paraphrase acquisition from text pairs. Analoguously to bilingual parallel corpora, which are still to date the most reliable resources for automatic acquisition of sub-sentential translations, monolingual parallel corpora are generally regarded as very appropriate for paraphrase acquisition. However, their low availability makes searching for less parallel corpora a necessity. In this study, we have attempted to identify corpora of various degre"
D12-1066,D10-1064,1,0.833483,"s difficult to achieve. Laslty, the evaluation of automatically generated paraphrases has recently received some attention (Liu et al., 2010; Chen and Dolan, 2011; Metzler et al., 2011) although it remains a difficult issue. Application-driven paraphrase generation provides indirect means of evaluating paraphrase generation (Zhao et al., 2009). For instance, the field of Statistical Machine Translation has produced works showing both the usefulness of human-produced (Schroeder et al., 2009; Resnik et al., 2010) and automatically produced paraphrases (Madnani et al., 2008; Marton et al., 2009; Max, 2010; He et al., 2011) for improving translation performance. 5 Discussion and future work This work has addressed the issue of sub-sentential paraphrase acquisition from text pairs. Analoguously to bilingual parallel corpora, which are still to date the most reliable resources for automatic acquisition of sub-sentential translations, monolingual parallel corpora are generally regarded as very appropriate for paraphrase acquisition. However, their low availability makes searching for less parallel corpora a necessity. In this study, we have attempted to identify corpora of various degrees of seman"
D12-1066,P11-2096,0,0.0136934,". To the best of our knowledge, this is the first time that a study in paraphrase acquisition is conducted on several corpus types and for 2 languages. Faruqui and Pad´o (2011) study the acquisition of entailment pairs (premise and hypothesis), with experiments in 3 languages and various domains of newspaper corpora for one language. Although their work is not directly comparable to ours, they report that robustness across domains is difficult to achieve. Laslty, the evaluation of automatically generated paraphrases has recently received some attention (Liu et al., 2010; Chen and Dolan, 2011; Metzler et al., 2011) although it remains a difficult issue. Application-driven paraphrase generation provides indirect means of evaluating paraphrase generation (Zhao et al., 2009). For instance, the field of Statistical Machine Translation has produced works showing both the usefulness of human-produced (Schroeder et al., 2009; Resnik et al., 2010) and automatically produced paraphrases (Madnani et al., 2008; Marton et al., 2009; Max, 2010; He et al., 2011) for improving translation performance. 5 Discussion and future work This work has addressed the issue of sub-sentential paraphrase acquisition from text pair"
D12-1066,E06-1021,0,0.0169469,"ase examples for French (19,427) than for English (12,593). 4 Related work Over the years, paraphrase acquisition and generation have attracted a wealth of research works that are too many to adequatly summarize here: (Madnani and Dorr, 2010) presents a complete and upto-date review of the main approaches. Sentential paraphrase collection has been tackled from specific resources increasing the probability of sentences being paraphrases (Dolan et al., 2004; Bernhard and Gurevych, 2008; Wubben et al., 2009), from comparable monolingual corpora (Barzilay and Elhadad, 2003; Fung and Cheung, 2004; Nelken and Shieber, 2006), and even at web scale (Pasc¸a and Dienes, 2005; Bhagat and Ravichandran, 2008). Various techniques have been proposed for paraphrase acquisition from related sentence pairs (Barzilay and McKeown, 2001; Pang et al., 2003) and from bilingual parallel corpora (Bannard and Callison-Burch, 2005; Kok and Brockett, 2010). The issue of corpus construction for developing and evaluating paraphrase acquisition techniques are addressed in (Cohn et al., 2008; Callison-Burch et al., 2008). To the best of our knowledge, this is the first time that a study in paraphrase acquisition is conducted on several c"
D12-1066,J04-4002,0,0.00874287,"araphrase identification We now describe the systems that will be tested on the various corpora described in section 2 using the methodology described in section 3.1. Following (Bouamor et al., 2012), a combination system is used to automatically weight paraphrase pair candidates produced by individual systems using a set of features aiming at recognizing paraphrases, as illustrated on Figure 3. Four individual systems have been used and are described below: the reasons for considering those systems include their free avail725 Statistical learning of word alignments (G IZA) The G IZA ++ tool (Och and Ney, 2004) computes statistical word alignment models of increasing complexity from parallel corpora. It was run on each monolingual corpus of sentence pairs in both directions, symmetrized alignments were kept and classical phrase extraction heuristics were applied (Koehn et al., 2003), without growing phrases with unaligned tokens. Linguistic knowledge on term variation (FASTR) The FASTR tool (Jacquemin, 1999) spots term variants in large corpora, where variants are described through metarules expressing how the morphosyntactic structure of a term variant can be derived from a given term by means of r"
D12-1066,N03-1024,0,0.0385101,"d Dorr, 2010) presents a complete and upto-date review of the main approaches. Sentential paraphrase collection has been tackled from specific resources increasing the probability of sentences being paraphrases (Dolan et al., 2004; Bernhard and Gurevych, 2008; Wubben et al., 2009), from comparable monolingual corpora (Barzilay and Elhadad, 2003; Fung and Cheung, 2004; Nelken and Shieber, 2006), and even at web scale (Pasc¸a and Dienes, 2005; Bhagat and Ravichandran, 2008). Various techniques have been proposed for paraphrase acquisition from related sentence pairs (Barzilay and McKeown, 2001; Pang et al., 2003) and from bilingual parallel corpora (Bannard and Callison-Burch, 2005; Kok and Brockett, 2010). The issue of corpus construction for developing and evaluating paraphrase acquisition techniques are addressed in (Cohn et al., 2008; Callison-Burch et al., 2008). To the best of our knowledge, this is the first time that a study in paraphrase acquisition is conducted on several corpus types and for 2 languages. Faruqui and Pad´o (2011) study the acquisition of entailment pairs (premise and hypothesis), with experiments in 3 languages and various domains of newspaper corpora for one language. Altho"
D12-1066,P02-1040,0,0.0840745,"Missing"
D12-1066,I05-1011,0,0.0215987,"Missing"
D12-1066,D10-1013,0,0.0152712,"ge. Although their work is not directly comparable to ours, they report that robustness across domains is difficult to achieve. Laslty, the evaluation of automatically generated paraphrases has recently received some attention (Liu et al., 2010; Chen and Dolan, 2011; Metzler et al., 2011) although it remains a difficult issue. Application-driven paraphrase generation provides indirect means of evaluating paraphrase generation (Zhao et al., 2009). For instance, the field of Statistical Machine Translation has produced works showing both the usefulness of human-produced (Schroeder et al., 2009; Resnik et al., 2010) and automatically produced paraphrases (Madnani et al., 2008; Marton et al., 2009; Max, 2010; He et al., 2011) for improving translation performance. 5 Discussion and future work This work has addressed the issue of sub-sentential paraphrase acquisition from text pairs. Analoguously to bilingual parallel corpora, which are still to date the most reliable resources for automatic acquisition of sub-sentential translations, monolingual parallel corpora are generally regarded as very appropriate for paraphrase acquisition. However, their low availability makes searching for less parallel corpora"
D12-1066,E09-1082,0,0.0134125,"r corpora for one language. Although their work is not directly comparable to ours, they report that robustness across domains is difficult to achieve. Laslty, the evaluation of automatically generated paraphrases has recently received some attention (Liu et al., 2010; Chen and Dolan, 2011; Metzler et al., 2011) although it remains a difficult issue. Application-driven paraphrase generation provides indirect means of evaluating paraphrase generation (Zhao et al., 2009). For instance, the field of Statistical Machine Translation has produced works showing both the usefulness of human-produced (Schroeder et al., 2009; Resnik et al., 2010) and automatically produced paraphrases (Madnani et al., 2008; Marton et al., 2009; Max, 2010; He et al., 2011) for improving translation performance. 5 Discussion and future work This work has addressed the issue of sub-sentential paraphrase acquisition from text pairs. Analoguously to bilingual parallel corpora, which are still to date the most reliable resources for automatic acquisition of sub-sentential translations, monolingual parallel corpora are generally regarded as very appropriate for paraphrase acquisition. However, their low availability makes searching for"
D12-1066,2006.amta-papers.25,0,0.0293574,"Missing"
D12-1066,W09-0621,0,0.0487281,"Missing"
D12-1066,P09-1094,0,0.0240288,"Pad´o (2011) study the acquisition of entailment pairs (premise and hypothesis), with experiments in 3 languages and various domains of newspaper corpora for one language. Although their work is not directly comparable to ours, they report that robustness across domains is difficult to achieve. Laslty, the evaluation of automatically generated paraphrases has recently received some attention (Liu et al., 2010; Chen and Dolan, 2011; Metzler et al., 2011) although it remains a difficult issue. Application-driven paraphrase generation provides indirect means of evaluating paraphrase generation (Zhao et al., 2009). For instance, the field of Statistical Machine Translation has produced works showing both the usefulness of human-produced (Schroeder et al., 2009; Resnik et al., 2010) and automatically produced paraphrases (Madnani et al., 2008; Marton et al., 2009; Max, 2010; He et al., 2011) for improving translation performance. 5 Discussion and future work This work has addressed the issue of sub-sentential paraphrase acquisition from text pairs. Analoguously to bilingual parallel corpora, which are still to date the most reliable resources for automatic acquisition of sub-sentential translations, mon"
D12-1066,S12-1051,0,\N,Missing
D12-1066,2010.amta-workshop.3,0,\N,Missing
D14-1026,E06-1032,0,0.372467,"Missing"
D14-1026,W11-2103,0,0.0501888,"-scale gold-standard data. 3 NIST MEDAR WIKI # of Documents 100 4 7 # of Sentences 1056 509 327 Table 1: Statistics on the datasets. We use six state-of-the-art English-to-Arabic MT systems. These include four research-oriented phrase-based systems with various morphological and syntactic features and different Arabic tokenization schemes and also two commercial offthe-shelf systems. 3.2 In order conduct a manual evaluation of the six MT systems, we formulated it as a ranking problem. We adapt the framework used in the WMT 2011 shared task for evaluating MT metrics on European language pairs (Callison-Burch et al., 2011) for Arabic MT. We gather human ranking judgments by asking ten annotators (each native speaker of Arabic with English as a second language) to assess the quality of the English-Arabic systems, by ranking sentences relative to each other, from the best to the worst (ties are allowed). We use the Appraise toolkit (Federmann, 2012) designed for manual MT evaluation. The tool displays to the annotator, the source sentence and translations produced by various MT systems. The annotators received initial training on the tool and the task with ten sentences. They were presented with a brief guideline"
D14-1026,W11-2105,0,0.151955,"nslations. We only use the first reference in this study. (3) a small dataset of Wikipedia articles (WIKI) to extend our corpus and metric evaluation to topics beyond the commonly-used news topics. This sub-corpus consists of our in-house Arabic translations of seven English Wikipedia articles. The articles are: Earl Francis Lloyd, Western Europe, Citizenship, Marcus Garvey, Middle Age translation, Acadian, NBA. The English articles which do not exist in the Arabic Wikipedia were manually translated by a bilingual linguist. Table 1 gives an overview of these sub-corpora characteristics. tion, Chen and Kuhn (2011) proposed AMBER, a modified version of BLEU incorporating recall, extra penalties, and light linguistic knowledge about English morphology. Liu et al. (2010) propose TESLA-M, a variant of a metric based on n-gram matching that utilizes light-weight linguistic analysis including lemmatization, POS tagging, and WordNet synonym relations. This metric was then extended to TESLA-B to model phrase synonyms by exploiting bilingual phrase tables (Dahlmeier et al., 2011). Tantug et al. (2008) presented BLEU+, a tool that implements various extension to BLEU computation to allow for a better evaluation"
D14-1026,W05-0904,0,0.0825879,"e 300 remaining sentences (100 from each corpus) are kept for testing. The development and test sets are composed of equal portions of sentences from the three sub-corpora (NIST, MEDAR, WIKI). As baselines, we measured the correlation of BLEU and METEOR with human judgments collected for each sentence. We did not observe a strong correlation with the Arabic-tuned METEOR. We conducted our experiments on the standard METEOR which was a stronger baseline than its Arabic version. In order to avoid the zero ngram counts and artificially low BLEU scores, we use a smoothed version of BLEU. We follow Liu and Gildea (2005) to add a small value to both the matched n-grams and the total number of n-grams (epsilon value of 10−3 ). In order to reach an optimal ordering of partial matches, we conducted a set of experiments in which we compared different orders between the morphological and lexical matchings to settle with the final order which was presented in Figure 1. Table 4 shows a comparison of the average correlation with human judgments for BLEU, ME6 Conclusion We presented AL-BLEU, our adaptation of BLEU for the evaluation of machine translation into Arabic. The metric uses morphological, syntactic and lexic"
D14-1026,W10-1754,0,0.0836725,"beyond the commonly-used news topics. This sub-corpus consists of our in-house Arabic translations of seven English Wikipedia articles. The articles are: Earl Francis Lloyd, Western Europe, Citizenship, Marcus Garvey, Middle Age translation, Acadian, NBA. The English articles which do not exist in the Arabic Wikipedia were manually translated by a bilingual linguist. Table 1 gives an overview of these sub-corpora characteristics. tion, Chen and Kuhn (2011) proposed AMBER, a modified version of BLEU incorporating recall, extra penalties, and light linguistic knowledge about English morphology. Liu et al. (2010) propose TESLA-M, a variant of a metric based on n-gram matching that utilizes light-weight linguistic analysis including lemmatization, POS tagging, and WordNet synonym relations. This metric was then extended to TESLA-B to model phrase synonyms by exploiting bilingual phrase tables (Dahlmeier et al., 2011). Tantug et al. (2008) presented BLEU+, a tool that implements various extension to BLEU computation to allow for a better evaluation of the translation performance for Turkish. To the best of our knowledge the only human judgment dataset for Arabic MT is the small corpus which was used to"
D14-1026,W11-2106,0,0.128828,"Arabic Wikipedia were manually translated by a bilingual linguist. Table 1 gives an overview of these sub-corpora characteristics. tion, Chen and Kuhn (2011) proposed AMBER, a modified version of BLEU incorporating recall, extra penalties, and light linguistic knowledge about English morphology. Liu et al. (2010) propose TESLA-M, a variant of a metric based on n-gram matching that utilizes light-weight linguistic analysis including lemmatization, POS tagging, and WordNet synonym relations. This metric was then extended to TESLA-B to model phrase synonyms by exploiting bilingual phrase tables (Dahlmeier et al., 2011). Tantug et al. (2008) presented BLEU+, a tool that implements various extension to BLEU computation to allow for a better evaluation of the translation performance for Turkish. To the best of our knowledge the only human judgment dataset for Arabic MT is the small corpus which was used to tune parameters of the METEOR metric for Arabic (Denkowski and Lavie, 2011). Due to the shortage of Arabic human judgment dataset, studies on the performance of evaluation metrics have been constrained and limited. A relevant effort in this area is the upper-bound estimation of BLEU and METEOR scores for Ara"
D14-1026,W13-2202,0,0.013344,"linguistic resources. Popovi´c and Ney (2009) showed that n-gram based evaluation metrics calculated on POS sequences correlate well with human judgments, and recently designed and evaluated MPF, a BLEU-style metric based on morphemes and POS tags (Popovi´c, 2011). In the same direcEvaluation of Machine Translation (MT) continues to be a challenging research problem. There is an ongoing effort in finding simple and scalable metrics with rich linguistic analysis. A wide range of metrics have been proposed and evaluated mostly for European target languages (CallisonBurch et al., 2011; Mach´acˇ ek and Bojar, 2013). These metrics are usually evaluated based on their correlation with human judgments on a set of MT output. While there has been growing interest in building systems for translating into Arabic, the evaluation of Arabic MT is still an under-studied problem. Standard MT metrics such as BLEU (Papineni et al., 2002) or TER (Snover et al., 2006) have been widely used for evaluating Arabic MT (El Kholy and Habash, 2012). These metrics use strict word and phrase matching between the MT output and reference translations. For morphologically rich target languages such as Arabic, such criteria are too"
D14-1026,W11-2107,0,0.256144,"he research community. 1 2 Introduction Related Work Several studies on MT evaluation have pointed out the inadequacy of the standard n-gram based evaluation metrics for various languages (CallisonBurch et al., 2006). For morphologically complex languages and those without word delimiters, several studies have attempted to improve upon them and suggest more reliable metrics that correlate better with human judgments (Denoual and Lepage, 2005; Homola et al., 2009). A common approach to the problem of morphologically complex words is to integrate some linguistic knowledge in the metric. METEOR (Denkowski and Lavie, 2011), TERPlus (Snover et al., 2010) incorporate limited linguistic resources. Popovi´c and Ney (2009) showed that n-gram based evaluation metrics calculated on POS sequences correlate well with human judgments, and recently designed and evaluated MPF, a BLEU-style metric based on morphemes and POS tags (Popovi´c, 2011). In the same direcEvaluation of Machine Translation (MT) continues to be a challenging research problem. There is an ongoing effort in finding simple and scalable metrics with rich linguistic analysis. A wide range of metrics have been proposed and evaluated mostly for European targ"
D14-1026,maegaard-etal-2010-cooperation,0,0.0712016,"Missing"
D14-1026,I05-2014,0,0.0558301,"luate BLEU, METEOR and AL-BLEU on our human judgments corpus and show that AL-BLEU has the highest correlation with human judgments. We are releasing the dataset and software to the research community. 1 2 Introduction Related Work Several studies on MT evaluation have pointed out the inadequacy of the standard n-gram based evaluation metrics for various languages (CallisonBurch et al., 2006). For morphologically complex languages and those without word delimiters, several studies have attempted to improve upon them and suggest more reliable metrics that correlate better with human judgments (Denoual and Lepage, 2005; Homola et al., 2009). A common approach to the problem of morphologically complex words is to integrate some linguistic knowledge in the metric. METEOR (Denkowski and Lavie, 2011), TERPlus (Snover et al., 2010) incorporate limited linguistic resources. Popovi´c and Ney (2009) showed that n-gram based evaluation metrics calculated on POS sequences correlate well with human judgments, and recently designed and evaluated MPF, a BLEU-style metric based on morphemes and POS tags (Popovi´c, 2011). In the same direcEvaluation of Machine Translation (MT) continues to be a challenging research proble"
D14-1026,P02-1040,0,0.0891493,"ation (MT) continues to be a challenging research problem. There is an ongoing effort in finding simple and scalable metrics with rich linguistic analysis. A wide range of metrics have been proposed and evaluated mostly for European target languages (CallisonBurch et al., 2011; Mach´acˇ ek and Bojar, 2013). These metrics are usually evaluated based on their correlation with human judgments on a set of MT output. While there has been growing interest in building systems for translating into Arabic, the evaluation of Arabic MT is still an under-studied problem. Standard MT metrics such as BLEU (Papineni et al., 2002) or TER (Snover et al., 2006) have been widely used for evaluating Arabic MT (El Kholy and Habash, 2012). These metrics use strict word and phrase matching between the MT output and reference translations. For morphologically rich target languages such as Arabic, such criteria are too simplistic and inadequate. In this paper, we present: (a) the first human judgment dataset for Arabic MT (b) the Arabic Language 1 The dataset and the software are available at: http://nlp.qatar.cmu.edu/resources/ AL-BLEU 207 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ("
D14-1026,2011.mtsummit-papers.24,0,0.0251778,"Missing"
D14-1026,W09-0402,0,0.0482224,"Missing"
D14-1026,W11-2110,0,0.0353723,"Missing"
D14-1026,2006.amta-papers.25,0,0.0922648,"allenging research problem. There is an ongoing effort in finding simple and scalable metrics with rich linguistic analysis. A wide range of metrics have been proposed and evaluated mostly for European target languages (CallisonBurch et al., 2011; Mach´acˇ ek and Bojar, 2013). These metrics are usually evaluated based on their correlation with human judgments on a set of MT output. While there has been growing interest in building systems for translating into Arabic, the evaluation of Arabic MT is still an under-studied problem. Standard MT metrics such as BLEU (Papineni et al., 2002) or TER (Snover et al., 2006) have been widely used for evaluating Arabic MT (El Kholy and Habash, 2012). These metrics use strict word and phrase matching between the MT output and reference translations. For morphologically rich target languages such as Arabic, such criteria are too simplistic and inadequate. In this paper, we present: (a) the first human judgment dataset for Arabic MT (b) the Arabic Language 1 The dataset and the software are available at: http://nlp.qatar.cmu.edu/resources/ AL-BLEU 207 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 207–213, c Octo"
D14-1026,W09-0403,0,0.0456348,"Missing"
D14-1026,P04-1078,0,0.266026,"Missing"
D14-1026,tantug-etal-2008-bleu,1,\N,Missing
E12-1073,P05-1074,0,0.569991,"ms, increases the likelihood of finding very close contexts for sub-sentential units. Barzilay and Lee (2003) proposed a multi-sequence alignment algorithm that takes structurally similar sentences and builds a compact lattice representation that encodes local variations. The work by Bhagat and Ravichandran (2008) describes an application of a similar technique on a very large scale. The hypothesis that two words or phrases are interchangeable if they share a common translation into one or more other languages has also been extensively studied in works on subsentential paraphrase acquisition. Bannard and Callison-Burch (2005) described a pivoting approach that can exploit bilingual parallel corpora in several languages. The same technique has been applied to the acquisition of local paraphrasing patterns in Zhao et al. (2008). The work of Callison-Burch (2008) has shown how the monolingual context of a sentence to paraphrase can be used to improve the quality of the acquired paraphrases. Another approach consists in modelling local paraphrasing identification rules. The work of Jacquemin (1999) on the identification of term variants, which exploits rewriting morphosyntactic rules and descriptions of morphological"
E12-1073,N03-1003,0,0.129166,"phrases, occur in similar contexts then they may be interchangeable has been extensively tested. The distributional hypothesis, attributed to Zellig Harris, was for example applied to syntactic dependency paths in the work of Lin and Pantel (2001). Their results take the form of equivalence patterns with two arguments such as {X asks for Y, X requests Y, X’s request for Y, X wants Y, Y is requested by X, . . .}. Using comparable corpora, where the same information probably exists under various linguistic forms, increases the likelihood of finding very close contexts for sub-sentential units. Barzilay and Lee (2003) proposed a multi-sequence alignment algorithm that takes structurally similar sentences and builds a compact lattice representation that encodes local variations. The work by Bhagat and Ravichandran (2008) describes an application of a similar technique on a very large scale. The hypothesis that two words or phrases are interchangeable if they share a common translation into one or more other languages has also been extensively studied in works on subsentential paraphrase acquisition. Bannard and Callison-Burch (2005) described a pivoting approach that can exploit bilingual parallel corpora i"
E12-1073,P01-1008,0,0.780616,"local paraphrasing identification rules. The work of Jacquemin (1999) on the identification of term variants, which exploits rewriting morphosyntactic rules and descriptions of morphological and semantic lexical families, can be extended to extract the various forms corresponding to input patterns from large monolingual corpora. When parallel monolingual corpora aligned at the sentence level are available (e.g. multiple translations into the same language), the task of sub-sentential paraphrase acquisition can be cast as one of word alignment between two aligned sentences (Cohn et al., 2008). Barzilay and McKeown (2001) applied the distributionality hypothesis on such parallel sentences, and Pang et al. (2003) proposed an algorithm to align sentences by recursive fusion of their common syntactic constituants. Finally, they has been a recent interest in automatic evaluation of paraphrases (Callison-Burch et al., 2008; Liu et al., 2010; Chen and Dolan, 2011; Metzler et al., 2011). 3 Experimental setting We used the main aspects of the methodology described by Cohn et al. (2008) for constructing evaluation corpora and assessing the performance of techniques on the task of sub-sentential paraphrase acquisition."
E12-1073,P08-1077,0,0.0788221,"ndency paths in the work of Lin and Pantel (2001). Their results take the form of equivalence patterns with two arguments such as {X asks for Y, X requests Y, X’s request for Y, X wants Y, Y is requested by X, . . .}. Using comparable corpora, where the same information probably exists under various linguistic forms, increases the likelihood of finding very close contexts for sub-sentential units. Barzilay and Lee (2003) proposed a multi-sequence alignment algorithm that takes structurally similar sentences and builds a compact lattice representation that encodes local variations. The work by Bhagat and Ravichandran (2008) describes an application of a similar technique on a very large scale. The hypothesis that two words or phrases are interchangeable if they share a common translation into one or more other languages has also been extensively studied in works on subsentential paraphrase acquisition. Bannard and Callison-Burch (2005) described a pivoting approach that can exploit bilingual parallel corpora in several languages. The same technique has been applied to the acquisition of local paraphrasing patterns in Zhao et al. (2008). The work of Callison-Burch (2008) has shown how the monolingual context of a"
E12-1073,C08-1013,0,0.212209,"Missing"
E12-1073,D08-1021,0,0.0523473,"des local variations. The work by Bhagat and Ravichandran (2008) describes an application of a similar technique on a very large scale. The hypothesis that two words or phrases are interchangeable if they share a common translation into one or more other languages has also been extensively studied in works on subsentential paraphrase acquisition. Bannard and Callison-Burch (2005) described a pivoting approach that can exploit bilingual parallel corpora in several languages. The same technique has been applied to the acquisition of local paraphrasing patterns in Zhao et al. (2008). The work of Callison-Burch (2008) has shown how the monolingual context of a sentence to paraphrase can be used to improve the quality of the acquired paraphrases. Another approach consists in modelling local paraphrasing identification rules. The work of Jacquemin (1999) on the identification of term variants, which exploits rewriting morphosyntactic rules and descriptions of morphological and semantic lexical families, can be extended to extract the various forms corresponding to input patterns from large monolingual corpora. When parallel monolingual corpora aligned at the sentence level are available (e.g. multiple transl"
E12-1073,candito-etal-2010-statistical,0,0.022921,"Missing"
E12-1073,P11-1020,0,0.265691,"Missing"
E12-1073,J08-4005,0,0.252742,"nsists in modelling local paraphrasing identification rules. The work of Jacquemin (1999) on the identification of term variants, which exploits rewriting morphosyntactic rules and descriptions of morphological and semantic lexical families, can be extended to extract the various forms corresponding to input patterns from large monolingual corpora. When parallel monolingual corpora aligned at the sentence level are available (e.g. multiple translations into the same language), the task of sub-sentential paraphrase acquisition can be cast as one of word alignment between two aligned sentences (Cohn et al., 2008). Barzilay and McKeown (2001) applied the distributionality hypothesis on such parallel sentences, and Pang et al. (2003) proposed an algorithm to align sentences by recursive fusion of their common syntactic constituants. Finally, they has been a recent interest in automatic evaluation of paraphrases (Callison-Burch et al., 2008; Liu et al., 2010; Chen and Dolan, 2011; Metzler et al., 2011). 3 Experimental setting We used the main aspects of the methodology described by Cohn et al. (2008) for constructing evaluation corpora and assessing the performance of techniques on the task of sub-senten"
E12-1073,C04-1051,0,0.507008,"Missing"
E12-1073,P08-4006,0,0.16624,"indicates the proportion of tokens from the sentence pairs that could be manually aligned by a native-speaker annotator.2 Obviously, the more common tokens two sentences from a pair contain, the fewer subsentential paraphrases may be extracted from that pair. However, high lexical overlap increases the probability that two sentences be indeed paraphrases, and in turn the probability that some of their phrases be paraphrases. Furthermore, the tated corpora using them we considered all alignments as being correct. 2 The same annotator hand-aligned the 5*100=500 paraphrase pairs using the YAWAT (Germann, 2008) manual alignment tool. presence of common token may serve as useful clues to guide paraphrase extraction. For our experiments, we chose to use parallel monolingual corpora obtained by single language translation, the most direct resource type for acquiring sub-sentential paraphrase pairs. This allows us to define acceptable references for the task and resort to the most consensual evaluation technique for paraphrase acquisition to date. Using such corpora, we expect to be able to extract precise paraphrases (see Table 1), which will be natural candidates for further validation, which will be"
E12-1073,P99-1044,0,0.614531,"more other languages has also been extensively studied in works on subsentential paraphrase acquisition. Bannard and Callison-Burch (2005) described a pivoting approach that can exploit bilingual parallel corpora in several languages. The same technique has been applied to the acquisition of local paraphrasing patterns in Zhao et al. (2008). The work of Callison-Burch (2008) has shown how the monolingual context of a sentence to paraphrase can be used to improve the quality of the acquired paraphrases. Another approach consists in modelling local paraphrasing identification rules. The work of Jacquemin (1999) on the identification of term variants, which exploits rewriting morphosyntactic rules and descriptions of morphological and semantic lexical families, can be extended to extract the various forms corresponding to input patterns from large monolingual corpora. When parallel monolingual corpora aligned at the sentence level are available (e.g. multiple translations into the same language), the task of sub-sentential paraphrase acquisition can be cast as one of word alignment between two aligned sentences (Cohn et al., 2008). Barzilay and McKeown (2001) applied the distributionality hypothesis"
E12-1073,P03-1054,0,0.00429764,"sider all phrases from 5 http://statmt.org/europarl the first sentence and search for variants in the other sentence, then do the reverse process and finally take the intersection of the two sets. 4.4 Syntactic similarity (Synt) The algorithm introduced by Pang et al. (2003) takes two sentences as input and merges them by top-down syntactic fusion guided by compatible syntactic substructure. A lexical blocking mechanism prevents constituents from fusionning when there is evidence of the presence of a word in another constituent of one of the sentence. We use the Berkeley Probabilistic parser (Klein and Manning, 2003) to obtain syntactic trees for English and its adapted version for French (Candito et al., 2010). Because this process is highly sensitive to syntactic parse errors, we use in our implementation k-best parses and retain the most compact fusion from any pair of candidate parses. 4.5 Edit rate on word sequences (TERp ) TERp (Translation Edit Rate Plus) (Snover et al., 2010) is a score designed for the evaluation of Machine Translation output. Its typical use takes a system hypothesis to compute an optimal set of word edits that can transform it into some existing reference translation. Edit type"
E12-1073,P07-2045,0,0.00309514,"ability between two phrases based on their translation probability through all possible pivot phrases as: X Ppara (p1 , p2 ) = Pt (piv|p1 )Pt (p2 |piv) piv where Pt denotes translation probabilies. We used the Europarl corpus5 of parliamentary debates in English and French, consisting of approximately 1.7 million parallel sentences : this allowed us to use the same resource to build paraphrases for English, using French as the pivot language, and for French, using English as the pivot language. The GIZA++ tool was used for word alignment and the M OSES Statistical Machine Translation toolkit (Koehn et al., 2007) was used to compute phrase translation probabilities from these word alignments. For each sentential paraphrase pair, we applied the following algorithm: for each phrase, we build the entire set of paraphrases using the previous definition. We then extract its best paraphrase as the one exactly appearing in the other sentence with maximum paraphrase probability, using a minimal threshold value of 10−4 . 4.3 Linguistic knowledge on term variation (Fastr) The FASTR tool (Jacquemin, 1999) was designed to spot term/phrase variants in large corpora. Variants are described through metarules express"
E12-1073,D10-1090,0,0.20541,"Missing"
E12-1073,J10-3003,0,0.0362016,"quivalent meaning at the phrasal level (including single words). For instance, the phrases six months and half a year form a paraphrase pair applicable in many different contexts, as they would appropriately denote the same concept. Although one can envisage to manually build high-coverage lists of synonyms, enumerating meaning equivalences at the level of phrases is too daunting a task for humans. Because this type of knowledge can however greatly benefit many NLP applications, automatic acquisition of such paraphrases has attracted a lot of attention (Androutsopoulos and Malakasiotis, 2010; Madnani and Dorr, 2010), and significant research efforts have been devoted to this objective (Callison-Burch, 2007; Bhagat, 2009; Madnani, 2010). Central to acquiring paraphrases is the need of assessing the quality of the candidate paraphrases produced by a given technique. Most works to date have resorted to human evaluation of paraphrases on the levels of grammaticality and meaning equivalence. Human evaluation is however often criticized as being both costly and non reproducible, and the situation is even more complicated by the inherent complexity of the task that can produce low inter-judge agreement. Taskbas"
E12-1073,P11-2096,0,0.341476,"Missing"
E12-1073,J04-4002,0,0.0481864,"ork. In this work, we consider the scenario where sentential paraphrases are available and words and phrases from one sentence can be aligned to words and phrases from the other sentence to form atomic paraphrase pairs. We now describe several techniques that perform the task of sub-sentential unit alignment. We have selected and implemented five techniques which we believe are representative of the type of knowledge that these techniques use, and have reused existing tools, initially developed for other tasks, when possible. 4.1 Statistical learning of word alignments (Giza) The GIZA++ tool (Och and Ney, 2004) computes statistical word alignment models of increasing complexity from parallel corpora. While originally developed in the bilingual context of Statistical Machine Translation, nothing prevents building such models on monolingual corpora. However, in order to build reliable models, it is necessary to use enough training material including minimal redundancy of words. To this end, we provided GIZA++ with all possible sentence pairs from our mutiply-translated corpus to improve the quality of its word alignments (note that 4 719 http://www.elda.org/article125.html we used symmetrized alignmen"
E12-1073,N03-1024,0,0.142135,"ariants, which exploits rewriting morphosyntactic rules and descriptions of morphological and semantic lexical families, can be extended to extract the various forms corresponding to input patterns from large monolingual corpora. When parallel monolingual corpora aligned at the sentence level are available (e.g. multiple translations into the same language), the task of sub-sentential paraphrase acquisition can be cast as one of word alignment between two aligned sentences (Cohn et al., 2008). Barzilay and McKeown (2001) applied the distributionality hypothesis on such parallel sentences, and Pang et al. (2003) proposed an algorithm to align sentences by recursive fusion of their common syntactic constituants. Finally, they has been a recent interest in automatic evaluation of paraphrases (Callison-Burch et al., 2008; Liu et al., 2010; Chen and Dolan, 2011; Metzler et al., 2011). 3 Experimental setting We used the main aspects of the methodology described by Cohn et al. (2008) for constructing evaluation corpora and assessing the performance of techniques on the task of sub-sentential paraphrase acquisition. Pairs of related sentences are hand-aligned to define a set of reference atomic paraphrase p"
E12-1073,P08-1089,0,0.0629322,"lattice representation that encodes local variations. The work by Bhagat and Ravichandran (2008) describes an application of a similar technique on a very large scale. The hypothesis that two words or phrases are interchangeable if they share a common translation into one or more other languages has also been extensively studied in works on subsentential paraphrase acquisition. Bannard and Callison-Burch (2005) described a pivoting approach that can exploit bilingual parallel corpora in several languages. The same technique has been applied to the acquisition of local paraphrasing patterns in Zhao et al. (2008). The work of Callison-Burch (2008) has shown how the monolingual context of a sentence to paraphrase can be used to improve the quality of the acquired paraphrases. Another approach consists in modelling local paraphrasing identification rules. The work of Jacquemin (1999) on the identification of term variants, which exploits rewriting morphosyntactic rules and descriptions of morphological and semantic lexical families, can be extended to extract the various forms corresponding to input patterns from large monolingual corpora. When parallel monolingual corpora aligned at the sentence level"
F12-2015,P05-1074,0,0.100015,"Missing"
F12-2015,N03-1003,0,0.0998693,"Missing"
F12-2015,P01-1008,0,0.128195,"Missing"
F12-2015,P08-1077,0,0.0466614,"Missing"
F12-2015,P11-2069,1,0.832338,"Missing"
F12-2015,I05-5001,0,0.0442445,"Missing"
F12-2015,candito-etal-2010-statistical,0,0.0313508,"Missing"
F12-2015,J08-4005,0,0.0391223,"Missing"
F12-2015,C08-1018,0,0.0238144,"Missing"
F12-2015,W09-3102,0,0.0252956,"Missing"
F12-2015,D11-1108,0,0.0281563,"Missing"
F12-2015,N10-1017,0,0.024775,"Missing"
F12-2015,J10-3003,0,0.0338433,"Missing"
F12-2015,2008.amta-papers.13,0,0.0544819,"Missing"
F12-2015,C04-1166,1,0.861184,"Missing"
F12-2015,max-wisniewski-2010-mining,1,0.905676,"Missing"
F12-2015,W08-1911,1,0.894545,"Missing"
F12-2015,P10-2001,0,0.0606923,"Missing"
F12-2015,N03-1024,0,0.0273338,"Missing"
F12-2015,I05-1011,0,0.070267,"Missing"
F12-2015,N07-1051,0,0.0193161,"Missing"
F12-2015,W04-3219,0,0.0732191,"Missing"
F12-2015,D10-1013,0,0.0613904,"Missing"
F12-2015,E09-1082,0,0.0252746,"Missing"
F12-2015,N10-2012,0,0.0230885,"Missing"
F12-2015,P09-1094,0,0.0541476,"Missing"
F12-2015,C10-1149,0,0.0375349,"Missing"
F12-2015,C10-1152,0,0.0607389,"Missing"
F12-2020,P05-1074,0,0.104377,"Missing"
F12-2020,W03-1004,0,0.0177066,"Missing"
F12-2020,P01-1008,0,0.134909,"Missing"
F12-2020,W08-0906,0,0.0568488,"Missing"
F12-2020,P08-1077,0,0.04689,"Missing"
F12-2020,C08-1013,0,0.0337731,"Missing"
F12-2020,W11-2504,0,0.0299908,"Missing"
F12-2020,P11-1020,0,0.0337687,"Missing"
F12-2020,J08-4005,0,0.0286401,"Missing"
F12-2020,D10-1113,0,0.0284105,"Missing"
F12-2020,C04-1051,0,0.0305195,"Missing"
F12-2020,P10-2017,0,0.0473081,"Missing"
F12-2020,W11-0111,0,0.0510101,"Missing"
F12-2020,C04-1151,0,0.0202999,"Missing"
F12-2020,P08-4006,0,0.0272382,"Missing"
F12-2020,P99-1044,0,0.0159824,"Missing"
F12-2020,P07-2045,0,0.00680795,"Missing"
F12-2020,N03-1017,0,0.00672789,"Missing"
F12-2020,N10-1017,0,0.0209541,"Missing"
F12-2020,P10-4008,0,0.0247424,"Missing"
F12-2020,W07-0734,0,0.0414566,"Missing"
F12-2020,D10-1090,0,0.0286066,"Missing"
F12-2020,J10-3003,0,0.0436927,"Missing"
F12-2020,2008.amta-papers.13,0,0.0208433,"Missing"
F12-2020,D09-1040,0,0.0235767,"Missing"
F12-2020,D10-1064,1,0.874667,"Missing"
F12-2020,P11-2096,0,0.0210605,"Missing"
F12-2020,E06-1021,0,0.0267946,"Missing"
F12-2020,J04-4002,0,0.0126002,"Missing"
F12-2020,N03-1024,0,0.0196593,"Missing"
F12-2020,P02-1040,0,0.085289,"Missing"
F12-2020,I05-1011,0,0.0668427,"Missing"
F12-2020,D10-1013,0,0.0442446,"Missing"
F12-2020,E09-1082,0,0.0227366,"Missing"
F12-2020,2006.amta-papers.25,0,0.0530812,"Missing"
F12-2020,W09-0621,0,0.0209422,"Missing"
F12-2020,P09-1094,0,0.0370838,"Missing"
I13-1031,radev-etal-2004-mead,0,0.0423336,"indicator of its grammaticality. In our experiments, we used the SRILM toolkit (Stolcke, 2002) to build 5-gram language model using the LDC Arabic Gigaword corpus. We then, apply this model to obtain log-likelihood and perplexity scores for each sentence. 5.1 MT-based scores: We extract a set of features from the generated MT output. These include the absolute number and the ratios of out of vocabulary terms and the ratio of Arabic detokenization that is performed on the Arabic MT output. The MEAD Summarization system In our experiments, the summary for each document is generated using MEAD (Radev et al., 2004), a state-of-the-art single- and multidocument summarization system. MEAD has been widely used both as a platform for developing summarization systems and as a baseline system for testing novel summarizers. It is a Morphosyntactic features: We use features to model the difference of sequences of POS tags 273 6.2 centroid-based extractive summarizer which selects the most important sentences from a sequence of sentences based on a linear combination of three parameters: the sentence length, the centroid score and the position score (Radev et al., 2001). MEAD also employs a cosine reranker to el"
I13-1031,2006.amta-papers.25,0,0.0239053,"uality estimation system (b) predicts if a translated sentence has high or low translation quality and assigns a quality score to each sentence. 3. We summarize the English document using our MT-aware summarization system (c), which incorporates the translation quality score (output of (b)) in its sentence selection process. 4.1 In order to train the binary classifier, we need gold standard data with English source sentences labeled as having high or low translation quality when translated into Arabic. For this labeling, we estimate translation quality by the Translation Edit Rate TER metric (Snover et al., 2006).2 We deliberately use two different metrics for gold standard labeling (TER) and the final MT evaluation (using BLEU (Papineni et al., 2002)) to reduce the bias that a metric can introduce to the framework. In this task, we use a parallel corpus that is composed of a set of documents. We automatically translate each document and label its sentences based on the following procedure: 4. We produce the final Arabic translation summary by matching the English summarized sentences with the corresponding Arabic translations (d). 5. We automatically evaluate the quality of our MT-aware summarization"
I13-1031,P10-1063,0,0.0173967,"source language text for an MT system can degrade the translation quality dramatically. Thus, extractive summarization like our framework is more suitable for MT summarization. In retrospect our annotated Arabic-English summaries is a unique bilingual resource as most other Arabic-English summarization corpora (e.g. DUC) are abstractive summaries. There has been a body of recent work on the reference-free prediction of translation quality both as confidence estimation metrics and also direct prediction of human judgment scores (Bojar et al., 2013; Specia, 2012) or the range of the BLEU score (Soricut and Echihabi, 2010; Mohit and Hwa, 2007). These works mostly use supervised learning frameworks with a rich set of source and target language features. Our binary classification of MT quality is closer to the classification system of Mohit and Hwa (2007) to estimate translation difficulty of phrases. However, there are several modifications such as the method of labeling, the focus on sentence level prediction and finally the use of a different metric for both the labeling and final evaluation (which reduces the metric bias). For learning features, we cumulatively explore and optimize most of the reported featu"
I13-1031,W06-3114,0,0.0132807,"h a quality superior to translating the entire document. Figure 1 illustrates an overview of our framework composed of the following major components: (a) a standard 271 (a) MT System Arabic Sentences English Sentences English Document (b) Quality Estimation SentEN1, SentAR1 : Score1 SentEN2, SentAR2 : Score2 SentEN3, SentAR3 : Score3 . . . . SentENn, SentARn : Scoren English Summary (c) MT-aware Summarizer Arabic Summary (d) Sentence Matcher English Sentences Figure 1: An overview of our MT-aware summarization system been developed in evaluation programs like NIST, and workshops such as WMT (Koehn and Monz, 2006). Since such human judgments do not exist for English to Arabic translations, we adapt the framework of Mohit and Hwa (2007) for predicting the translation quality. This framework uses only reference translations and the automatic MT evaluation scores to create labeled data for training a classifier. The binary classifier reads in a source language sentence, with its automatically obtained translation and predicts if the target sentence has high or low translation quality. We describe details of this framework in the following section. SMT system; (b) our reference-free MT quality estimation s"
I13-1031,P13-4014,0,0.0321452,"Missing"
I13-1031,W07-0734,0,0.0292012,"experimented with different configurations of the MT and the summarization system with the goal of achieving a balanced performance in both dimensions. We reached the sweet spot of performance in both dimensions in our MT-aware summarization system in which we achieved major (over 4 points BLEU score) improvements while maintaining an acceptable summarization quality. In the following we discuss the performance of the MT and summarization systems. 7.1 BLEU MT evaluation Table 1 presents MT quality for the baseline system and different summarization frameworks measured by BLEU, TER and METEOR (Lavie and Agarwal, 2007) scores.5 The remaining MT experiments are conducted on summarized documents. These include summaries provided by: (a) a length-based baseline system that simply chooses the subset of sentences with the shortest length (Length); (b) the state of the art MEAD summarizer (MEAD); (c) our MT quality estimation classifier (Classifier); (d) a linear interpolation of informativeness and MT quality scores in the spirit of Wan et al. (2010) (Interpol)6 ; (e) our MT-aware summarizer 5 Our English to Arabic baseline system shows a performance in the ballpark of the reported score for the state of the art"
I13-1031,N03-1020,0,0.122062,"the perfor7.3 Model-free summarization evaluation In addition to the reference-based summarization evaluation described above, we conducted modelfree experiments evaluating the summary quality for both languages. Recently, Louis and Nenkova (2013) proposed SIMetrix, a framework that does not require gold standard summaries for measuring the summarization quality. The framework is based on the idea that higher similarity with the source document would be indicative of high quality summary. SIMetrix is a suite of model-free similarity metrics for comparing a generated sum7 A study conducted by Lin and Hovy (2003) shows that automatic evaluation using unigram and bigram cooccurrences between summary pairs have the highest correlation with human evaluations and have high recall and precision in significance test with manual evaluation results. 8 http://duc.nist.gov/duc2007/tasks. html. 276 8 mary with the source document for which it was produced. That includes cosine similarity, distributional similarity and also use of topic signature words. SIMetrix is shown to produce summary scores that correlate accurately with human assessments.9 We used SIMetrix to evaluate the quality of the summaries generated"
I13-1031,P10-1094,0,0.273718,"the underlying SMT system and also its training data. There are many overlaps between the features used in confidence estimation and the MT quality prediction. However, the two frameworks use different learning methods. Confidence estimation systems usually do not have gold standard data and are mostly a linear interpolation of a large group of scores. In contrast, MT quality predictors such as our framework usually use supervised learning and rely on gold standard data. Text summarization has been successfully paired with different NLP applications such as MT in cross-language summarization. Wan et al. (2010) and Boudin et al. (2011) proposed crosslanguage summarization frameworks in which for each sentence, in a source language text, an MT quality and informativeness scores are combined to produce summary in a target language (Chinese and French, respectively). In the latter, sentences are first translated, ranked and then summaries are generated. Differently, in Wan et al. (2010), each sentence of the source document is ranked based on an a posteriori combination of both scores. The selected summarized sentences are then translated to the target language using Google Translate. In contrast, we g"
I13-1031,W04-1013,0,0.0355672,"Missing"
I13-1031,W06-3110,0,0.0219471,"rized to make a more cohesive document. Thus, for tasks in which complete translation is not mandatory, MT can be effective if the system can provide a 1 The bilingually summarized corpora could be found at: http://nlp.qatar.cmu.edu/resources/SuMT 270 International Joint Conference on Natural Language Processing, pages 270–278, Nagoya, Japan, 14-18 October 2013. ments of its own performance. The confidence measure is a score for N-grams (substrings of the hypothesis) which are generated by an MT system. Confidence estimation is performed at the word level (Blatz et al., 2003) or phrase level (Zens and Ney, 2006). The measure is based on feature values extracted from the underlying SMT system and also its training data. There are many overlaps between the features used in confidence estimation and the MT quality prediction. However, the two frameworks use different learning methods. Confidence estimation systems usually do not have gold standard data and are mostly a linear interpolation of a large group of scores. In contrast, MT quality predictors such as our framework usually use supervised learning and rely on gold standard data. Text summarization has been successfully paired with different NLP a"
I13-1031,J13-2002,0,0.0156541,"nt with five summarizers: Length, MEAD, Classifier, Interpol, SuMT. As expected, the MEAD summarizer shows the best summarization performance. Also, the length-based baseline system generates poor quality summaries (about 22 score ROUGE-1 reduction from MEAD). This is not surprising since the baseline only uses the length of the sentence regardless its content. Furthermore, the perfor7.3 Model-free summarization evaluation In addition to the reference-based summarization evaluation described above, we conducted modelfree experiments evaluating the summary quality for both languages. Recently, Louis and Nenkova (2013) proposed SIMetrix, a framework that does not require gold standard summaries for measuring the summarization quality. The framework is based on the idea that higher similarity with the source document would be indicative of high quality summary. SIMetrix is a suite of model-free similarity metrics for comparing a generated sum7 A study conducted by Lin and Hovy (2003) shows that automatic evaluation using unigram and bigram cooccurrences between summary pairs have the highest correlation with human evaluations and have high recall and precision in significance test with manual evaluation resu"
I13-1031,W07-0737,1,0.92321,"MT system can degrade the translation quality dramatically. Thus, extractive summarization like our framework is more suitable for MT summarization. In retrospect our annotated Arabic-English summaries is a unique bilingual resource as most other Arabic-English summarization corpora (e.g. DUC) are abstractive summaries. There has been a body of recent work on the reference-free prediction of translation quality both as confidence estimation metrics and also direct prediction of human judgment scores (Bojar et al., 2013; Specia, 2012) or the range of the BLEU score (Soricut and Echihabi, 2010; Mohit and Hwa, 2007). These works mostly use supervised learning frameworks with a rich set of source and target language features. Our binary classification of MT quality is closer to the classification system of Mohit and Hwa (2007) to estimate translation difficulty of phrases. However, there are several modifications such as the method of labeling, the focus on sentence level prediction and finally the use of a different metric for both the labeling and final evaluation (which reduces the metric bias). For learning features, we cumulatively explore and optimize most of the reported features, and add document-"
I13-1031,P03-1021,0,0.0173447,"Missing"
I13-1031,P02-1040,0,0.0867087,"ence. 3. We summarize the English document using our MT-aware summarization system (c), which incorporates the translation quality score (output of (b)) in its sentence selection process. 4.1 In order to train the binary classifier, we need gold standard data with English source sentences labeled as having high or low translation quality when translated into Arabic. For this labeling, we estimate translation quality by the Translation Edit Rate TER metric (Snover et al., 2006).2 We deliberately use two different metrics for gold standard labeling (TER) and the final MT evaluation (using BLEU (Papineni et al., 2002)) to reduce the bias that a metric can introduce to the framework. In this task, we use a parallel corpus that is composed of a set of documents. We automatically translate each document and label its sentences based on the following procedure: 4. We produce the final Arabic translation summary by matching the English summarized sentences with the corresponding Arabic translations (d). 5. We automatically evaluate the quality of our MT-aware summarization system using MT and summarization metrics. Our contributions are mainly related to the second and third components which will be discussed i"
I13-1031,C04-1046,0,\N,Missing
I13-1031,W13-2201,0,\N,Missing
L16-1175,W15-3206,0,0.0323242,"Missing"
L16-1175,bouamor-etal-2014-multidialectal,1,0.891026,"Missing"
L16-1175,diab-etal-2014-tharwa,1,0.89137,"Missing"
L16-1175,W14-3911,0,0.0305357,"Missing"
L16-1175,P06-1086,1,0.825282,"Missing"
L16-1175,W12-2301,1,0.906938,"Missing"
L16-1175,N13-1044,1,0.840128,"Missing"
L16-1175,D15-1254,0,0.0303772,"Missing"
L16-1175,L16-1679,1,0.805401,"Missing"
L16-1175,2006.bcs-1.4,0,0.122097,"Missing"
L16-1175,pasha-etal-2014-madamira,1,0.877273,"Missing"
L16-1175,W14-5904,0,0.0392074,"Missing"
L16-1175,P13-2001,0,0.0372105,"Missing"
L16-1175,salama-etal-2014-youdacc,1,0.889209,"Missing"
L16-1175,W11-2602,1,0.849481,"Missing"
L16-1175,W15-3205,0,0.0276656,"Missing"
L16-1175,P11-2007,0,0.0817413,"Missing"
L16-1175,N12-1006,0,0.0453082,"Missing"
L16-1295,avramidis-etal-2014-taraxu,0,0.0610864,"A portion of the corpus contains an analysis of the type of errors made by the MT system. Elming (2006) created a 265K-word English-Danish MT manually corrected corpus by a human professional translator. The full corpus covers the chemical patents domain. Simard et al. (2007) created a 500K-word corpus of manually edited FrenchEnglish and English-French MT from the Canadian Job Bank website. The corpus is a collection of blocks composed of the source language texts, the machine translation output of a rule-based MT system and the final post-edited version done by a human translator. Moreover, Avramidis et al. (2014) built a corpus of human-annotated machine translations which was evaluated by professional human translators for the following three language pairs: GermanEnglish, English-German and Spanish-German. Fishel et al. (2012) created a corpus of automatically produced translations with detailed manual translation error analysis of 576 sentences for four language pairs: EnglishCzech;French-German;German-English;English-Serbian. Popescu-belis et al. (2002) produced a small corpus of 50 texts translated by students and corrected by their professors and all translation errors are annotated with their c"
L16-1295,D14-1026,1,0.764023,"was evaluated by professional human translators for the following three language pairs: GermanEnglish, English-German and Spanish-German. Fishel et al. (2012) created a corpus of automatically produced translations with detailed manual translation error analysis of 576 sentences for four language pairs: EnglishCzech;French-German;German-English;English-Serbian. Popescu-belis et al. (2002) produced a small corpus of 50 texts translated by students and corrected by their professors and all translation errors are annotated with their corrections in this corpus. For Arabic, we cite the effort of Bouamor et al. (2014) who created a medium scale human judgment corpus of Arabic machine translation using the output of six MT systems and a total of 1892 sentences and 22K rankings. Our corpus is a part of the Qatar Arabic Language Bank (QALB) project, a large scale manually annotated annotation project (Zaghouani et al., 2014b; Zaghouani et al., 1869 2015). The project goal was to create an error corrected 2M-words corpus for online user comments on news websites, native speaker essays, non-native speaker essays and machine translation output. The 100K-word machine translation portion was selected from various"
L16-1295,W12-5611,0,0.0270103,"Missing"
L16-1295,2006.eamt-1.27,0,0.166044,"pair. Keywords: Post-Editing, Guidelines, Annotation 1. 2. Introduction In recent years, machine translation (MT) became widely used by translation companies to reduce their costs and improve their speed. Therefore, the demand for quick and accurate machine translations is growing. Machine translation (MT) systems often produce incorrect output with many grammatical and lexical choice errors. Correcting machine-produced translation errors, or MT Post-Editing (PE) can be done automatically or manually. Successful automatic post-editing approaches using manually corrected MT output were used by Elming (2006) and Simard et al. (2007). The availability of annotated resources is required for such approaches. When it comes to the Arabic language, to the best of our knowledge, there is no manually post-edited MT corpora available to build such systems. Therefore, there is a clear need to build such valuable resources for the Arabic language. In this paper, we present our guidelines and annotation procedure to create a human corrected MT corpus for the Modern Standard Arabic (MSA). The creation of any manually annotated corpus usually presents many challenges. In order to address these challenges, we c"
L16-1295,fishel-etal-2012-terra,0,0.104056,"ers the chemical patents domain. Simard et al. (2007) created a 500K-word corpus of manually edited FrenchEnglish and English-French MT from the Canadian Job Bank website. The corpus is a collection of blocks composed of the source language texts, the machine translation output of a rule-based MT system and the final post-edited version done by a human translator. Moreover, Avramidis et al. (2014) built a corpus of human-annotated machine translations which was evaluated by professional human translators for the following three language pairs: GermanEnglish, English-German and Spanish-German. Fishel et al. (2012) created a corpus of automatically produced translations with detailed manual translation error analysis of 576 sentences for four language pairs: EnglishCzech;French-German;German-English;English-Serbian. Popescu-belis et al. (2002) produced a small corpus of 50 texts translated by students and corrected by their professors and all translation errors are annotated with their corrections in this corpus. For Arabic, we cite the effort of Bouamor et al. (2014) who created a medium scale human judgment corpus of Arabic machine translation using the output of six MT systems and a total of 1892 sen"
L16-1295,W14-3605,1,0.85988,"one. 7. Conclusions We have presented in detail the methodology used to create a 100K-word English to Arabic MT manually post-edited corpus, including the development of the guidelines as well as the annotation procedure and the quality control procedure using frequent inter-annotator measures. The created guidelines will be made publicly available and we look forward to distribute the post-edited corpus in a planned shared task on automatic error correction and getting feedback from the community on its usefulness as it was in the previous shared tasks we organized for the L1 and L2 corpus (Mohit et al., 2014; Rozovskaya et al., 2015). We believe that this corpus will be valuable to advance research efforts in the machine translation area since manually annotated data is often needed by the MT community. We believe that our methodology for guideline development and annotation consistency checking can be applied in other projects and other languages as well. In the future, we plan to increase the size of the corpus and also to add other corpus domains. Acknowledgements We would like to thank the anonymous reviewers for their valuable comments and suggestions. We also thank all our dedicated annotat"
L16-1295,I13-2001,1,0.511808,"e was closely monitored during the initial period, before allowing the annotator to join the official post-editing production phase. Moreover, a dedicated online discussion group was frequently used by the annotation team to keep track of the MT post-editing questions and issues raised during the annotation process. This mechanism, proved to help the annotators and the lead annotator to have a better communication. The annotation itself is done using the QAWI annotation tool, an in house built web annotation framework designed originally for the manual correction of errors in L1 and L2 texts (Obeid et al., 2013). This framework includes two major components: The annotation management interface which is used to assist the lead annotator in the general work-flow process, it allows the annotation manager easily upload and organize files and projects, manage users, assign files in a batch or individually, export annotation tasks and monitor the current annotation progress by processing real time annotation progress statistics. Moreover inter-annotator agreement (IAA), evaluation metrics such as the Word Error Rate (WER) are integrated with the management interface to allow the scores to be computed and t"
L16-1295,pasha-etal-2014-madamira,1,0.825321,"Word Edit: to correct/modify a word. 2. Word Move: to move words to the right location in the sentence. 3. Add Word: insert missing words in the text. 4. Delete: delete unnecessary words. 5. Merge and Split: to merge or split words. All post-editing action history previously mentioned are recorded in a database and can be exported to an XML file. Figure 2 shows an example of how the annotation actions are stored in the XML annotation export file. Finally, and in order to increase the post-editing speed and prior to the first human pass, an automatic post-editing pass is done through MADAMIRA (Pasha et al., 2014), a tool that automatically corrects common spelling errors using a prediction model based on the words in-context. MADAMIRA uses a morphological analyzer to produce, for each input word, a list of analyses specifying every possible morphological interpretation of that word, covering all morphological features of the word. Most of the errors automatically corrected are related to Ya/AlifMaqsura, Ha/Ta-Marbuta and Hamzated Alif forms, which are common spelling errors in Arabic.6 6. 6.1. Evaluation Inter-Annotator Agreement We use Word error Rate (WER) as a proxy of the Inter annotator agreement"
L16-1295,W15-3204,1,0.858567,"We have presented in detail the methodology used to create a 100K-word English to Arabic MT manually post-edited corpus, including the development of the guidelines as well as the annotation procedure and the quality control procedure using frequent inter-annotator measures. The created guidelines will be made publicly available and we look forward to distribute the post-edited corpus in a planned shared task on automatic error correction and getting feedback from the community on its usefulness as it was in the previous shared tasks we organized for the L1 and L2 corpus (Mohit et al., 2014; Rozovskaya et al., 2015). We believe that this corpus will be valuable to advance research efforts in the machine translation area since manually annotated data is often needed by the MT community. We believe that our methodology for guideline development and annotation consistency checking can be applied in other projects and other languages as well. In the future, we plan to increase the size of the corpus and also to add other corpus domains. Acknowledgements We would like to thank the anonymous reviewers for their valuable comments and suggestions. We also thank all our dedicated annotators: Noor Alzeer, Hoda Fat"
L16-1295,N07-1064,0,0.122569,"st-Editing, Guidelines, Annotation 1. 2. Introduction In recent years, machine translation (MT) became widely used by translation companies to reduce their costs and improve their speed. Therefore, the demand for quick and accurate machine translations is growing. Machine translation (MT) systems often produce incorrect output with many grammatical and lexical choice errors. Correcting machine-produced translation errors, or MT Post-Editing (PE) can be done automatically or manually. Successful automatic post-editing approaches using manually corrected MT output were used by Elming (2006) and Simard et al. (2007). The availability of annotated resources is required for such approaches. When it comes to the Arabic language, to the best of our knowledge, there is no manually post-edited MT corpora available to build such systems. Therefore, there is a clear need to build such valuable resources for the Arabic language. In this paper, we present our guidelines and annotation procedure to create a human corrected MT corpus for the Modern Standard Arabic (MSA). The creation of any manually annotated corpus usually presents many challenges. In order to address these challenges, we created comprehensive and"
L16-1295,2006.amta-papers.25,0,0.111195,"Missing"
L16-1295,wisniewski-etal-2014-corpus,0,0.0305494,"ed to check the annotation quality. To the best of our knowledge, this is the first published machine translation manual post-editing annotation effort for Arabic of this scale. In the next sections, we review related work (Section 2), describe our corpus and the development of the guidelines (Sections 3-4), and present our annotation procedure (Section 5), than we present the annotation evaluation in Section 6, finally we conclude our work in Section 7. Related Work Large scale manually corrected MT corpora are not yet widely available due to the high cost related to building such resources. Wisniewski et al. (2014) created a corpus of machine translation errors extracted from several translation students taking part in a master program in specialized translations. The texts are translated from English to French. A portion of the corpus contains an analysis of the type of errors made by the MT system. Elming (2006) created a 265K-word English-Danish MT manually corrected corpus by a human professional translator. The full corpus covers the chemical patents domain. Simard et al. (2007) created a 500K-word corpus of manually edited FrenchEnglish and English-French MT from the Canadian Job Bank website. The"
L16-1295,zaghouani-etal-2014-large,1,0.889004,"glishCzech;French-German;German-English;English-Serbian. Popescu-belis et al. (2002) produced a small corpus of 50 texts translated by students and corrected by their professors and all translation errors are annotated with their corrections in this corpus. For Arabic, we cite the effort of Bouamor et al. (2014) who created a medium scale human judgment corpus of Arabic machine translation using the output of six MT systems and a total of 1892 sentences and 22K rankings. Our corpus is a part of the Qatar Arabic Language Bank (QALB) project, a large scale manually annotated annotation project (Zaghouani et al., 2014b; Zaghouani et al., 1869 2015). The project goal was to create an error corrected 2M-words corpus for online user comments on news websites, native speaker essays, non-native speaker essays and machine translation output. The 100K-word machine translation portion was selected from various Wikinews English articles translated to Arabic automatically using the Google Translate tool.1 3. Corpus Description We collected a 100K-word corpus of English news articles taken from the collaborative journalism Wikinews website.2 Since Wikinews is a free-content news source, we avoided any copyrights comp"
L16-1295,W15-1614,1,0.623101,"Missing"
L16-1577,D15-1274,0,0.145302,"entence. There are three types of diacritics: vowel, nunation, and shadda (gemination). The lack of diacritics leads usually to considerable lexical and morphological ambiguity as shown in the example in Table 1.1 Full diacritization has been shown to improve state-of-the-art Arabic automatic systems such as speech recognition (ASR) systems (Kirchhoff and Vergyri, 2005) and statistical machine translation (SMT) (Diab et al., 2007). Hence, diacritization has been receiving increased attention in several Arabic NLP applications (Zitouni et al., 2006; Shahrour et al., 2015; Abandah et al., 2015; Belinkov and Glass, 2015). Building models to assign diacritics to each letter in a word requires a large amount of annotated training corpora covering different topics and domains to overcome the sparseness problem. The currently available MSA diacritized corpora are generally limited to newswire stories (those distributed by the LDC), religious texts such as the Holy Quran, or educational texts. 1 We use the Buckwalter transliteration encoding system to represent Arabic in Romanized script (Buckwalter, 2002) Undiacritized Diacritized Buckwalter Y«ð Y « ð /waEad/ Y«ð Y«ð Y«ð Y«ð  ð Y« ð Y« Y « ð Y « ð English he p"
L16-1577,W15-3209,1,0.859315,"th the diacritics for a variety of Arabic texts covering more than 10 genres. The target size of the annotated corpus is 2 million words. The present work was mainly motivated by the lack of equivalent multi-genres large scale annotated corpus. The creation of manually annotated corpus usually presents many challenges and issues. In order to address those challenges, we created comprehensive and simplified annotation guidelines that were used by a team consisting of five annotators and an annotation manager. The guidelines were defined after an initial pilot annotation experiment described in Bouamor et al. (2015). In order to ensure a high annotation agreement between the annotators, multiple training sessions were held and a regular inter-annotator agreement (IAA) measures were performed to check annotation quality. To the best of our knowledge, this is the first Arabic diacritized multi-genres corpus. The remainder of this paper is organized as follows. We review related work in section 2. Afterwards, we discuss the challenges posed by the complexity of the Arabic diacritization process in section 3. Then, we describe our corpus and the development of the guidelines in sections 4 and 5. 3637 We pres"
L16-1577,2007.mtsummit-papers.20,1,0.926265,"nced, except the last letter diacritization, and b) case and mood diacritization, which exists above or below the last letter in each word, indicating its grammatical function in the sentence. There are three types of diacritics: vowel, nunation, and shadda (gemination). The lack of diacritics leads usually to considerable lexical and morphological ambiguity as shown in the example in Table 1.1 Full diacritization has been shown to improve state-of-the-art Arabic automatic systems such as speech recognition (ASR) systems (Kirchhoff and Vergyri, 2005) and statistical machine translation (SMT) (Diab et al., 2007). Hence, diacritization has been receiving increased attention in several Arabic NLP applications (Zitouni et al., 2006; Shahrour et al., 2015; Abandah et al., 2015; Belinkov and Glass, 2015). Building models to assign diacritics to each letter in a word requires a large amount of annotated training corpora covering different topics and domains to overcome the sparseness problem. The currently available MSA diacritized corpora are generally limited to newswire stories (those distributed by the LDC), religious texts such as the Holy Quran, or educational texts. 1 We use the Buckwalter translite"
L16-1577,palmer-etal-2008-pilot,1,0.881222,"utomatic diacritization system for Arabic using rule-based, statistical and hybrid methods. We refer to the recent literature review in Abandah et al. (2015) for a general overview of these methods and tools. The most relevant resource to our work is the Penn Arabic Treebank (PATB), a large corpus annotated by the Linguistic Data Consortium (Maamouri et al., 2010). Most of the LDC Treebank corpora are also manually diacritized, but they cover mainly news and weblog text genres. The PATB served later to build the first Arabic Proposition Bank (APB) using the fully specified diacritized lemmas (Diab et al., 2008; Zaghouani et al., 2010). The Tashkeela classical Arabic vocalized corpus (Zerrouki, 2011) is another notable dataset covering six million words. Tashkeela was compiled from various web sources covering Islamic religious heritage (mainly classical Arabic books). Moreover, Dukes and Habash (2010), created the Quranic Arabic Corpus, a fully diacritized annotated linguistic resource which we used later on to build the first Quranic Arabic Proposition Bank Zaghouani et al. (2012). The Qatar Arabic Language Bank (Zaghouani et al., 2014; Zaghouani et al., 2015; Zaghouani et al., 2016) is another re"
L16-1577,dukes-habash-2010-morphological,0,0.0333421,"corpus annotated by the Linguistic Data Consortium (Maamouri et al., 2010). Most of the LDC Treebank corpora are also manually diacritized, but they cover mainly news and weblog text genres. The PATB served later to build the first Arabic Proposition Bank (APB) using the fully specified diacritized lemmas (Diab et al., 2008; Zaghouani et al., 2010). The Tashkeela classical Arabic vocalized corpus (Zerrouki, 2011) is another notable dataset covering six million words. Tashkeela was compiled from various web sources covering Islamic religious heritage (mainly classical Arabic books). Moreover, Dukes and Habash (2010), created the Quranic Arabic Corpus, a fully diacritized annotated linguistic resource which we used later on to build the first Quranic Arabic Proposition Bank Zaghouani et al. (2012). The Qatar Arabic Language Bank (Zaghouani et al., 2014; Zaghouani et al., 2015; Zaghouani et al., 2016) is another relevant work that aims to build a large corpus of manually corrected Arabic text for building automatic correction tools for three Arabic text genres: native, non-native and machine translation post-edited text. Recently, in Bouamor et al. (2015), we conducted various annotation experiments to fin"
L16-1577,maamouri-etal-2008-enhancing,0,0.0233017,"about the text, the author and the source. In order to use the CCA corpus, a normalization effort was done to produce a consistent XML mark-up format to be used by our annotation tool. 5. Development of the Guidelines We provided the annotators with detailed guidelines, describing our diacritization scheme and specifying when and where to add the diacritics. We describe the annotation procedure and explained how to deal with borderline cases. We also include several annotated examples to illustrate the specified rules. Our guidelines are mostly inspired from the LDC POS annotation guidelines (Maamouri et al., 2008). Since, the LDC guidelines are mainly designed for the POS annotation and not specifically for the diacritization per se, we created a simplified version and added some specific diacritization rules to make the annotation process consistent. Below we provide some examples of diacritization exceptions and specific rules. The Shadda: The shadda mark should be added in all cases specified in the guidelines except the following in the definite artilce, where it should not be added to the letter ÊË@ /Allymwn/ ’lemon’ È /l/ of the definite article (e.g. àñÒJ  ÊË@ /A˜llymwn/). Moreover, the shadda"
L16-1577,maamouri-etal-2010-speech,1,0.874215,"and present our future work in section 8. 2. Related Work Since, our paper is mainly about the creation and evaluation of a large annotated corpus, we will focus mostly on this aspect in the previous works. There have been numerous approaches to build an automatic diacritization system for Arabic using rule-based, statistical and hybrid methods. We refer to the recent literature review in Abandah et al. (2015) for a general overview of these methods and tools. The most relevant resource to our work is the Penn Arabic Treebank (PATB), a large corpus annotated by the Linguistic Data Consortium (Maamouri et al., 2010). Most of the LDC Treebank corpora are also manually diacritized, but they cover mainly news and weblog text genres. The PATB served later to build the first Arabic Proposition Bank (APB) using the fully specified diacritized lemmas (Diab et al., 2008; Zaghouani et al., 2010). The Tashkeela classical Arabic vocalized corpus (Zerrouki, 2011) is another notable dataset covering six million words. Tashkeela was compiled from various web sources covering Islamic religious heritage (mainly classical Arabic books). Moreover, Dukes and Habash (2010), created the Quranic Arabic Corpus, a fully diacrit"
L16-1577,W14-3605,1,0.918057,"Missing"
L16-1577,pasha-etal-2014-madamira,1,0.898534,"Missing"
L16-1577,W15-3204,1,0.903933,"Missing"
L16-1577,D15-1152,0,0.0742449,"indicating its grammatical function in the sentence. There are three types of diacritics: vowel, nunation, and shadda (gemination). The lack of diacritics leads usually to considerable lexical and morphological ambiguity as shown in the example in Table 1.1 Full diacritization has been shown to improve state-of-the-art Arabic automatic systems such as speech recognition (ASR) systems (Kirchhoff and Vergyri, 2005) and statistical machine translation (SMT) (Diab et al., 2007). Hence, diacritization has been receiving increased attention in several Arabic NLP applications (Zitouni et al., 2006; Shahrour et al., 2015; Abandah et al., 2015; Belinkov and Glass, 2015). Building models to assign diacritics to each letter in a word requires a large amount of annotated training corpora covering different topics and domains to overcome the sparseness problem. The currently available MSA diacritized corpora are generally limited to newswire stories (those distributed by the LDC), religious texts such as the Holy Quran, or educational texts. 1 We use the Buckwalter transliteration encoding system to represent Arabic in Romanized script (Buckwalter, 2002) Undiacritized Diacritized Buckwalter Y«ð Y « ð /waEad/ Y«ð Y"
L16-1577,2006.amta-papers.25,0,0.0982236,"Missing"
L16-1577,W10-1836,1,0.806692,"tion system for Arabic using rule-based, statistical and hybrid methods. We refer to the recent literature review in Abandah et al. (2015) for a general overview of these methods and tools. The most relevant resource to our work is the Penn Arabic Treebank (PATB), a large corpus annotated by the Linguistic Data Consortium (Maamouri et al., 2010). Most of the LDC Treebank corpora are also manually diacritized, but they cover mainly news and weblog text genres. The PATB served later to build the first Arabic Proposition Bank (APB) using the fully specified diacritized lemmas (Diab et al., 2008; Zaghouani et al., 2010). The Tashkeela classical Arabic vocalized corpus (Zerrouki, 2011) is another notable dataset covering six million words. Tashkeela was compiled from various web sources covering Islamic religious heritage (mainly classical Arabic books). Moreover, Dukes and Habash (2010), created the Quranic Arabic Corpus, a fully diacritized annotated linguistic resource which we used later on to build the first Quranic Arabic Proposition Bank Zaghouani et al. (2012). The Qatar Arabic Language Bank (Zaghouani et al., 2014; Zaghouani et al., 2015; Zaghouani et al., 2016) is another relevant work that aims to"
L16-1577,W12-2511,1,0.785024,"genres. The PATB served later to build the first Arabic Proposition Bank (APB) using the fully specified diacritized lemmas (Diab et al., 2008; Zaghouani et al., 2010). The Tashkeela classical Arabic vocalized corpus (Zerrouki, 2011) is another notable dataset covering six million words. Tashkeela was compiled from various web sources covering Islamic religious heritage (mainly classical Arabic books). Moreover, Dukes and Habash (2010), created the Quranic Arabic Corpus, a fully diacritized annotated linguistic resource which we used later on to build the first Quranic Arabic Proposition Bank Zaghouani et al. (2012). The Qatar Arabic Language Bank (Zaghouani et al., 2014; Zaghouani et al., 2015; Zaghouani et al., 2016) is another relevant work that aims to build a large corpus of manually corrected Arabic text for building automatic correction tools for three Arabic text genres: native, non-native and machine translation post-edited text. Recently, in Bouamor et al. (2015), we conducted various annotation experiments to find the most suitable and efficient annotation procedure in creating a large scale diacritized corpus. 3. Arabic Diacritics Arabic script consists of two classes of symbols: letters and"
L16-1577,zaghouani-etal-2014-large,1,0.532947,"roposition Bank (APB) using the fully specified diacritized lemmas (Diab et al., 2008; Zaghouani et al., 2010). The Tashkeela classical Arabic vocalized corpus (Zerrouki, 2011) is another notable dataset covering six million words. Tashkeela was compiled from various web sources covering Islamic religious heritage (mainly classical Arabic books). Moreover, Dukes and Habash (2010), created the Quranic Arabic Corpus, a fully diacritized annotated linguistic resource which we used later on to build the first Quranic Arabic Proposition Bank Zaghouani et al. (2012). The Qatar Arabic Language Bank (Zaghouani et al., 2014; Zaghouani et al., 2015; Zaghouani et al., 2016) is another relevant work that aims to build a large corpus of manually corrected Arabic text for building automatic correction tools for three Arabic text genres: native, non-native and machine translation post-edited text. Recently, in Bouamor et al. (2015), we conducted various annotation experiments to find the most suitable and efficient annotation procedure in creating a large scale diacritized corpus. 3. Arabic Diacritics Arabic script consists of two classes of symbols: letters and diacritics. Letters comprise long vowels such as A, y, w"
L16-1577,W15-1614,1,0.736149,"ing the fully specified diacritized lemmas (Diab et al., 2008; Zaghouani et al., 2010). The Tashkeela classical Arabic vocalized corpus (Zerrouki, 2011) is another notable dataset covering six million words. Tashkeela was compiled from various web sources covering Islamic religious heritage (mainly classical Arabic books). Moreover, Dukes and Habash (2010), created the Quranic Arabic Corpus, a fully diacritized annotated linguistic resource which we used later on to build the first Quranic Arabic Proposition Bank Zaghouani et al. (2012). The Qatar Arabic Language Bank (Zaghouani et al., 2014; Zaghouani et al., 2015; Zaghouani et al., 2016) is another relevant work that aims to build a large corpus of manually corrected Arabic text for building automatic correction tools for three Arabic text genres: native, non-native and machine translation post-edited text. Recently, in Bouamor et al. (2015), we conducted various annotation experiments to find the most suitable and efficient annotation procedure in creating a large scale diacritized corpus. 3. Arabic Diacritics Arabic script consists of two classes of symbols: letters and diacritics. Letters comprise long vowels such as A, y, w as well as consonants."
L16-1577,L16-1295,1,0.768521,"diacritized lemmas (Diab et al., 2008; Zaghouani et al., 2010). The Tashkeela classical Arabic vocalized corpus (Zerrouki, 2011) is another notable dataset covering six million words. Tashkeela was compiled from various web sources covering Islamic religious heritage (mainly classical Arabic books). Moreover, Dukes and Habash (2010), created the Quranic Arabic Corpus, a fully diacritized annotated linguistic resource which we used later on to build the first Quranic Arabic Proposition Bank Zaghouani et al. (2012). The Qatar Arabic Language Bank (Zaghouani et al., 2014; Zaghouani et al., 2015; Zaghouani et al., 2016) is another relevant work that aims to build a large corpus of manually corrected Arabic text for building automatic correction tools for three Arabic text genres: native, non-native and machine translation post-edited text. Recently, in Bouamor et al. (2015), we conducted various annotation experiments to find the most suitable and efficient annotation procedure in creating a large scale diacritized corpus. 3. Arabic Diacritics Arabic script consists of two classes of symbols: letters and diacritics. Letters comprise long vowels such as A, y, w as well as consonants. Diacritics on the other h"
L16-1577,P06-1073,0,0.603177,"t letter in each word, indicating its grammatical function in the sentence. There are three types of diacritics: vowel, nunation, and shadda (gemination). The lack of diacritics leads usually to considerable lexical and morphological ambiguity as shown in the example in Table 1.1 Full diacritization has been shown to improve state-of-the-art Arabic automatic systems such as speech recognition (ASR) systems (Kirchhoff and Vergyri, 2005) and statistical machine translation (SMT) (Diab et al., 2007). Hence, diacritization has been receiving increased attention in several Arabic NLP applications (Zitouni et al., 2006; Shahrour et al., 2015; Abandah et al., 2015; Belinkov and Glass, 2015). Building models to assign diacritics to each letter in a word requires a large amount of annotated training corpora covering different topics and domains to overcome the sparseness problem. The currently available MSA diacritized corpora are generally limited to newswire stories (those distributed by the LDC), religious texts such as the Holy Quran, or educational texts. 1 We use the Buckwalter transliteration encoding system to represent Arabic in Romanized script (Buckwalter, 2002) Undiacritized Diacritized Buckwalter"
L18-1415,W15-3206,0,0.310868,"tactic annotation and morphological tokenization for Arabic. In general, many of these existing tools are not designed to handle the peculiarities of dialectal Arabic. They neither provide facilities for managing thousands of documents nor permit the distribution of tasks to tens of annotators, including managing inter-annotator agreement (IAA) tasks. Our interface borrows ideas from three other existing annotation tools: DIWAN, QAWI, and MANDIAC. Here we describe each of these tools and how they have influenced the design of our system. DIWAN is an annotation tool for Arabic dialectal texts (Al-Shargi and Rambow, 2015). It provides annotators with a set of tools for reducing duplicate effort including the use of morphological analyzers to pre-compute analyses, and the ability to apply analyses to multiple occurrences simultaneously. However, it requires installation on a Windows machine and the user interface is not very friendly to newcomers. QAWI (the QALB Annotation Web Interface) was used for token-based text editing to create raw and and text corrected parallel data for automatic text correction tasks (Obeid et al., 2013; Zaghouani et al., 2014; Zaghouani et al., 2015; Zaghouani et al., 2016). It suppo"
L18-1415,L16-1646,0,0.0160278,"al. (2012), Stymne (2011), Llitjós and Carbonell (2004), and Dickinson and Ledbetter (2012). For Arabic, there are several existing annotation tools, however, they are designed to handle specific NLP tasks and are not easy to adapt to our project. Examples include tools for semantic annotation such as the work of Saleh and Al-Khalifa (2009) and El-ghobashy et al. (2014), 2616 3 ¯⌦ P@Y” madAriy means ‘my orbit’ in Arabic. and the work on dialect annotation by Benajiba and Diab (2010) and Diab et al. (2010). Attia et al. (2009) built a morphological annotation tool. Recently, Al-Twairesh et al. (2016) introduced MADAD, a general-purpose online collaborative annotation tool for readability assessments project in Arabic. In the COLABA initiative (Diab et al., 2010), the authors built tools and resources to process Arabic social media data such as blogs, discussion forums, and chats. Javed et al. (2018) presented an online interface for joint syntactic annotation and morphological tokenization for Arabic. In general, many of these existing tools are not designed to handle the peculiarities of dialectal Arabic. They neither provide facilities for managing thousands of documents nor permit the"
L18-1415,aziz-etal-2012-pet,0,0.0188875,"Missing"
L18-1415,dickinson-ledbetter-2012-annotating,0,0.0264408,"of our annotation framework. In Section 4. and Section 5., we discuss the annotation and management interfaces, respectively. We finally describe a user study of working with MADARi in Section 6. 2. Related Work Several annotation tools and interfaces were proposed for many languages and to achieve various annotation tasks. Some are general purpose annotation tools, such as BRAT (Stenetorp et al., 2012) and WebAnno (Yimam et al., 2013). Task-specific annotation tools for post-editing and error correction include the work of Aziz et al. (2012), Stymne (2011), Llitjós and Carbonell (2004), and Dickinson and Ledbetter (2012). For Arabic, there are several existing annotation tools, however, they are designed to handle specific NLP tasks and are not easy to adapt to our project. Examples include tools for semantic annotation such as the work of Saleh and Al-Khalifa (2009) and El-ghobashy et al. (2014), 2616 3 ¯⌦ P@Y” madAriy means ‘my orbit’ in Arabic. and the work on dialect annotation by Benajiba and Diab (2010) and Diab et al. (2010). Attia et al. (2009) built a morphological annotation tool. Recently, Al-Twairesh et al. (2016) introduced MADAD, a general-purpose online collaborative annotation tool for readabi"
L18-1415,L18-1574,1,0.916519,"the word i.J⌦ mÃ '@AÎÒK. AK⌦  wyAbwhAAlxlyj1 involves two spelling errors2 (a word merge and character replacement) which can be corrected as i.J⌦ mÃ '@ AÎÒK. Ag.  wjAbwhA Alxlyj ‘and they brought it to the Gulf’. Furthermore, the first of the two corrected words includes two clitics that when segmented produce the form: AÎ+ @ÒK. Ag. +  w+ jAbwA +hA ‘and+ they-brought +it’. 1 Transliterations are in the Habash-Soudi-Buckwalter scheme (Habash et al., 2007). 2 Since Arabic dialects do not have a standard orthography, spelling correction here means to conventionalize as per the CODA standard (Habash et al., 2018). Previous work on Arabic morphology annotation interfaces focused either on the problem of manual annotations for POS tagging (Maamouri et al., 2014), or diacritization (Obeid et al., 2016), or spelling correction (Obeid et al., 2013). In this paper we present a tool that allows doing all of these tasks together, eliminating the possibility of error propagation from one annotation level to another. Our tool is named MADARi3 after the project under which it was created: Multi-Arabic Dialect Annotations and Resources (Bouamor et al., 2018). The remainder of this paper is structured as follows:"
L18-1415,L18-1345,1,0.571093,"on such as the work of Saleh and Al-Khalifa (2009) and El-ghobashy et al. (2014), 2616 3 ¯⌦ P@Y” madAriy means ‘my orbit’ in Arabic. and the work on dialect annotation by Benajiba and Diab (2010) and Diab et al. (2010). Attia et al. (2009) built a morphological annotation tool. Recently, Al-Twairesh et al. (2016) introduced MADAD, a general-purpose online collaborative annotation tool for readability assessments project in Arabic. In the COLABA initiative (Diab et al., 2010), the authors built tools and resources to process Arabic social media data such as blogs, discussion forums, and chats. Javed et al. (2018) presented an online interface for joint syntactic annotation and morphological tokenization for Arabic. In general, many of these existing tools are not designed to handle the peculiarities of dialectal Arabic. They neither provide facilities for managing thousands of documents nor permit the distribution of tasks to tens of annotators, including managing inter-annotator agreement (IAA) tasks. Our interface borrows ideas from three other existing annotation tools: DIWAN, QAWI, and MANDIAC. Here we describe each of these tools and how they have influenced the design of our system. DIWAN is an"
L18-1415,L18-1607,1,0.939775,"al., 2018). MANDIAC utilized the token-based editor used in QAWI to perform text diacritization tasks (Obeid et al., 2016). More importantly, it introduced a flexible hybrid data storage system that allows for adding new features to the annotation front-end with little to no modifications to the back-end. MADARi utilizes this design to provide the same utility. 3. MADARi Design The MADARi interface is designed to be used by human annotators to create a morphologically annotated corpus of Arabic text. The text we work with comes from social media and is highly dialectal (Bouamor et al., 2018; Khalifa et al., 2018) and has numerous spelling errors. The annotators will carefully correct the spelling of the words and also annotate their morphology. The in-context morphology annotation includes tokenization, POS tagging, lemmatization and English glossing. 3.1. Desiderata In order to manage and process the annotation of the large scale dialectal Arabic corpus, we needed to create a tool to streamline the annotation process. The desiderata for developing the MADARi annotation tool include the following: • The tool must have very minimal requirements on the annotators. • The tool must allow off-site data man"
L18-1415,font-llitjos-carbonell-2004-translation,0,0.0249819,"iscuss the design and architecture of our annotation framework. In Section 4. and Section 5., we discuss the annotation and management interfaces, respectively. We finally describe a user study of working with MADARi in Section 6. 2. Related Work Several annotation tools and interfaces were proposed for many languages and to achieve various annotation tasks. Some are general purpose annotation tools, such as BRAT (Stenetorp et al., 2012) and WebAnno (Yimam et al., 2013). Task-specific annotation tools for post-editing and error correction include the work of Aziz et al. (2012), Stymne (2011), Llitjós and Carbonell (2004), and Dickinson and Ledbetter (2012). For Arabic, there are several existing annotation tools, however, they are designed to handle specific NLP tasks and are not easy to adapt to our project. Examples include tools for semantic annotation such as the work of Saleh and Al-Khalifa (2009) and El-ghobashy et al. (2014), 2616 3 ¯⌦ P@Y” madAriy means ‘my orbit’ in Arabic. and the work on dialect annotation by Benajiba and Diab (2010) and Diab et al. (2010). Attia et al. (2009) built a morphological annotation tool. Recently, Al-Twairesh et al. (2016) introduced MADAD, a general-purpose online colla"
L18-1415,maamouri-etal-2014-developing,1,0.844143,"mÃ '@ AÎÒK. Ag.  wjAbwhA Alxlyj ‘and they brought it to the Gulf’. Furthermore, the first of the two corrected words includes two clitics that when segmented produce the form: AÎ+ @ÒK. Ag. +  w+ jAbwA +hA ‘and+ they-brought +it’. 1 Transliterations are in the Habash-Soudi-Buckwalter scheme (Habash et al., 2007). 2 Since Arabic dialects do not have a standard orthography, spelling correction here means to conventionalize as per the CODA standard (Habash et al., 2018). Previous work on Arabic morphology annotation interfaces focused either on the problem of manual annotations for POS tagging (Maamouri et al., 2014), or diacritization (Obeid et al., 2016), or spelling correction (Obeid et al., 2013). In this paper we present a tool that allows doing all of these tasks together, eliminating the possibility of error propagation from one annotation level to another. Our tool is named MADARi3 after the project under which it was created: Multi-Arabic Dialect Annotations and Resources (Bouamor et al., 2018). The remainder of this paper is structured as follows: we present work related to this effort in Section 2. In Section 3., we discuss the design and architecture of our annotation framework. In Section 4."
L18-1415,I13-2001,1,0.898118,"st of the two corrected words includes two clitics that when segmented produce the form: AÎ+ @ÒK. Ag. +  w+ jAbwA +hA ‘and+ they-brought +it’. 1 Transliterations are in the Habash-Soudi-Buckwalter scheme (Habash et al., 2007). 2 Since Arabic dialects do not have a standard orthography, spelling correction here means to conventionalize as per the CODA standard (Habash et al., 2018). Previous work on Arabic morphology annotation interfaces focused either on the problem of manual annotations for POS tagging (Maamouri et al., 2014), or diacritization (Obeid et al., 2016), or spelling correction (Obeid et al., 2013). In this paper we present a tool that allows doing all of these tasks together, eliminating the possibility of error propagation from one annotation level to another. Our tool is named MADARi3 after the project under which it was created: Multi-Arabic Dialect Annotations and Resources (Bouamor et al., 2018). The remainder of this paper is structured as follows: we present work related to this effort in Section 2. In Section 3., we discuss the design and architecture of our annotation framework. In Section 4. and Section 5., we discuss the annotation and management interfaces, respectively. We"
L18-1415,pasha-etal-2014-madamira,1,0.866692,"al., 2016). In particular, we utilized the client-server architecture, as well as the flexible hybrid SQL/JSON storage system used by MANDIAC. This allows us to easily extend our annotation interface with minor changes, if any, to the back-end. Our system stores documents one sentence per row, unlike MANDIAC which stores one document per row. This modification allows the annotation interface to handle larger file sizes without affecting its performance by only overwriting the JSON of the modified sentences and not that of the entire document. Like, DIWAN and MANDIAC, we also utilize MADAMIRA (Pasha et al., 2014), a morphological analyzer and disambiguator for Arabic to pre-compute analyses. 4. Annotation Interface The annotation interface (illustrated in Figures 1 to 4) is where annotators perform the annotation tasks assigned to them. Here we describe the different components and utilities provided this interface. 4.1. Task Overview When starting an annotation session, annotators are first shown the “Task Overview” screen (Figure 1). Here annotators can see information on the size of the current task and their progress so far (Figure 1a). The sentence list can be filtered to contain sentences matchi"
L18-1415,E12-2021,0,0.168509,"Missing"
L18-1415,P11-4010,0,0.0125903,"ection 3., we discuss the design and architecture of our annotation framework. In Section 4. and Section 5., we discuss the annotation and management interfaces, respectively. We finally describe a user study of working with MADARi in Section 6. 2. Related Work Several annotation tools and interfaces were proposed for many languages and to achieve various annotation tasks. Some are general purpose annotation tools, such as BRAT (Stenetorp et al., 2012) and WebAnno (Yimam et al., 2013). Task-specific annotation tools for post-editing and error correction include the work of Aziz et al. (2012), Stymne (2011), Llitjós and Carbonell (2004), and Dickinson and Ledbetter (2012). For Arabic, there are several existing annotation tools, however, they are designed to handle specific NLP tasks and are not easy to adapt to our project. Examples include tools for semantic annotation such as the work of Saleh and Al-Khalifa (2009) and El-ghobashy et al. (2014), 2616 3 ¯⌦ P@Y” madAriy means ‘my orbit’ in Arabic. and the work on dialect annotation by Benajiba and Diab (2010) and Diab et al. (2010). Attia et al. (2009) built a morphological annotation tool. Recently, Al-Twairesh et al. (2016) introduced MADAD,"
L18-1415,P13-4001,0,0.0513893,"Missing"
L18-1415,zaghouani-etal-2014-large,1,0.850041,"WAN is an annotation tool for Arabic dialectal texts (Al-Shargi and Rambow, 2015). It provides annotators with a set of tools for reducing duplicate effort including the use of morphological analyzers to pre-compute analyses, and the ability to apply analyses to multiple occurrences simultaneously. However, it requires installation on a Windows machine and the user interface is not very friendly to newcomers. QAWI (the QALB Annotation Web Interface) was used for token-based text editing to create raw and and text corrected parallel data for automatic text correction tasks (Obeid et al., 2013; Zaghouani et al., 2014; Zaghouani et al., 2015; Zaghouani et al., 2016). It supported the exact recording of all modifications performed by the annotator which previous tools did not. We utilize this token-based editing system for minor text corrections that transform text of a given dialect into the appropriate CODA orthography (Habash et al., 2018). MANDIAC utilized the token-based editor used in QAWI to perform text diacritization tasks (Obeid et al., 2016). More importantly, it introduced a flexible hybrid data storage system that allows for adding new features to the annotation front-end with little to no modi"
L18-1415,W15-1614,1,0.860721,"l for Arabic dialectal texts (Al-Shargi and Rambow, 2015). It provides annotators with a set of tools for reducing duplicate effort including the use of morphological analyzers to pre-compute analyses, and the ability to apply analyses to multiple occurrences simultaneously. However, it requires installation on a Windows machine and the user interface is not very friendly to newcomers. QAWI (the QALB Annotation Web Interface) was used for token-based text editing to create raw and and text corrected parallel data for automatic text correction tasks (Obeid et al., 2013; Zaghouani et al., 2014; Zaghouani et al., 2015; Zaghouani et al., 2016). It supported the exact recording of all modifications performed by the annotator which previous tools did not. We utilize this token-based editing system for minor text corrections that transform text of a given dialect into the appropriate CODA orthography (Habash et al., 2018). MANDIAC utilized the token-based editor used in QAWI to perform text diacritization tasks (Obeid et al., 2016). More importantly, it introduced a flexible hybrid data storage system that allows for adding new features to the annotation front-end with little to no modifications to the back-en"
L18-1415,L16-1295,1,0.845639,"exts (Al-Shargi and Rambow, 2015). It provides annotators with a set of tools for reducing duplicate effort including the use of morphological analyzers to pre-compute analyses, and the ability to apply analyses to multiple occurrences simultaneously. However, it requires installation on a Windows machine and the user interface is not very friendly to newcomers. QAWI (the QALB Annotation Web Interface) was used for token-based text editing to create raw and and text corrected parallel data for automatic text correction tasks (Obeid et al., 2013; Zaghouani et al., 2014; Zaghouani et al., 2015; Zaghouani et al., 2016). It supported the exact recording of all modifications performed by the annotator which previous tools did not. We utilize this token-based editing system for minor text corrections that transform text of a given dialect into the appropriate CODA orthography (Habash et al., 2018). MANDIAC utilized the token-based editor used in QAWI to perform text diacritization tasks (Obeid et al., 2016). More importantly, it introduced a flexible hybrid data storage system that allows for adding new features to the annotation front-end with little to no modifications to the back-end. MADARi utilizes this d"
L18-1535,W14-1604,1,0.907171,"t. • Think of more than one translation into his/her dialect and carefully specify the city. • Use external informants to get more information for cities in his/her area if it is not his original city. • Enter the CODA and CAPHI versions of each entry, using the guidelines provided. Very recently, automatic DA processing has attracted a considerable amount of research in NLP (Shoufan and Alameri, 2015), facilitated by the newly developed monolingual and multilingual dialectal corpora and lexicons. Several mono-dialectal corpora covering different Arabic dialects were built and made available. Al-Badrashiny et al. (2014) compiled a large dialect-identified corpus of DA from several Egyptian sources, but with a large presence of MSA. In a related effort, McNeil and Faiza (2011) built a four-million-word corpus of Tunisian Spoken Arabic. Various other research work resulted in multidialectal non parallel corpora at different scales (Zaidan and Callison-Burch, 2011; Zbib et al., 2012; Cotterell and Callison-Burch, 2014; Salama et al., 2014; Jeblee et al., 2014; Al-Shargi et al., 2016; Zaghouani and Charfi, 2018). 10 The latest version of the lexicon is available for browsing online at http://nlp.qatar.cmu.edu/ma"
L18-1535,L16-1207,1,0.824624,"lectal corpora and lexicons. Several mono-dialectal corpora covering different Arabic dialects were built and made available. Al-Badrashiny et al. (2014) compiled a large dialect-identified corpus of DA from several Egyptian sources, but with a large presence of MSA. In a related effort, McNeil and Faiza (2011) built a four-million-word corpus of Tunisian Spoken Arabic. Various other research work resulted in multidialectal non parallel corpora at different scales (Zaidan and Callison-Burch, 2011; Zbib et al., 2012; Cotterell and Callison-Burch, 2014; Salama et al., 2014; Jeblee et al., 2014; Al-Shargi et al., 2016; Zaghouani and Charfi, 2018). 10 The latest version of the lexicon is available for browsing online at http://nlp.qatar.cmu.edu/madar/. 3393 As for dialect-to-dialect parallel corpora, Bouamor et al. (2014) presented the first small-scale 7-way parallel corpus covering several dialects in addition to MSA, and English, all translated from Egyptian sentences. The fact that Egyptian was chosen as a starting point affected the quality of the translation. The sentences produced were biased by the use of some Egyptian expressions that might be accepted in other dialects, but a native would not prod"
L18-1535,bouamor-etal-2014-multidialectal,1,0.953924,"cities has 12,000 sentences that are five-way parallel translations, and that could be used to build several Dialectal Arabic NLP applications such as machine translation. An example of a 28-way parallel sentences extracted from C ORPUS -25 is given in Figure 1.5 Translators, identified from each of the 25 cities specifically, were asked to read a set of sentences provided in English or French, and translate them into their dialects. The translators are all native speakers of the dialects of the cities they hail from. We did not choose MSA as a starting point to avoid biasing the translation (Bouamor et al., 2014). 6 4 The English, French and MSA versions we use are those provided in the IWSLT evaluation campaign (Eck and Hori, 2005). 5 The MADAR Corpus is available for browsing online at http://nlp.qatar.cmu.edu/madar/. 6 The translation was handled by Ramitechs (http://www. ramitechs.com/), a company that creates and annotates several types of corpora and lexicons using expert linguists. 3388 English French MSA Beirut Cairo Doha Rabat Tunis Aleppo Alexandria Algiers Amman Aswan Baghdad Basra Benghazi Damascus Fes Jeddah Jerusalem Khartoum Mosul Muscat This room is too small. Cette chambre est trop pe"
L18-1535,cotterell-callison-burch-2014-multi,0,0.122726,"ri, 2015), facilitated by the newly developed monolingual and multilingual dialectal corpora and lexicons. Several mono-dialectal corpora covering different Arabic dialects were built and made available. Al-Badrashiny et al. (2014) compiled a large dialect-identified corpus of DA from several Egyptian sources, but with a large presence of MSA. In a related effort, McNeil and Faiza (2011) built a four-million-word corpus of Tunisian Spoken Arabic. Various other research work resulted in multidialectal non parallel corpora at different scales (Zaidan and Callison-Burch, 2011; Zbib et al., 2012; Cotterell and Callison-Burch, 2014; Salama et al., 2014; Jeblee et al., 2014; Al-Shargi et al., 2016; Zaghouani and Charfi, 2018). 10 The latest version of the lexicon is available for browsing online at http://nlp.qatar.cmu.edu/madar/. 3393 As for dialect-to-dialect parallel corpora, Bouamor et al. (2014) presented the first small-scale 7-way parallel corpus covering several dialects in addition to MSA, and English, all translated from Egyptian sentences. The fact that Egyptian was chosen as a starting point affected the quality of the translation. The sentences produced were biased by the use of some Egyptian expressions tha"
L18-1535,W14-3629,0,0.0380058,"these differences. Phonology An example of phonological differences is in the pronunciation of dialectal words whose MSA cog nate has the letter Qaf (  q).2 It is often observed that in Tunisian Arabic, this consonant appears as /q/ (similar to MSA), while in Egyptian and Levantine Arabic it is /P/ (glottal stop) and in Gulf Arabic it is /g/ (Haeri, 1991; Habash, 2010). Orthography While MSA has a standard orthography, the dialects do not. Often people write words reflecting the phonology or the etymology of these words. DA is sometimes written in the so-called Arabizi Romanization script (Darwish, 2014). In the context of NLP, a set of conventional orthography guidelines (CODA) has been proposed for a number of dialects (Habash et al., 2012a; Jar2 Arabic transliteration is presented in the Habash-SoudiBuckwalter scheme (Habash et al., 2007): (in alphabetical orˇ der) AbtθjHxdðrzsšSDTDςγfqklmnhwy and the additional sym   ˇ ¯ ˆ ð', yˆ ø , ¯ bols: ’ Z, Â @, A @, A @, w h è, ý ø. 3387 Region Maghreb Sub-region Morocco Algeria Tunisia Cities Rabat Algiers Tunis (RAB) (ALG) (TUN) Fes Sfax (FES) (SFX) Libya Tripoli (TRI) Benghazi (BEN) Nile Basin Egypt/Sudan Cairo (CAI) Alexandria (ALX) Aswan (AS"
L18-1535,diab-etal-2014-tharwa,1,0.957896,"itten language of informal communication online in the Arab World: in emails, blogs, discussion forums, chats, SMS, etc. There has been a rising interest in research on computational models of Arabic dialects in the last decade (Meftouh et al., 2015). There have been several efforts on creating different resources to allow building models for several Natural Language Processing (NLP) applications. However, these efforts have been disjoint from each other, and most of them have focused on a small number of dialects that represent vast regions of the Arab World (Zaidan and Callison-Burch, 2011; Diab et al., 2014; Sajjad et al., 2013). In this paper, we present two resources we created as part of the Multi Arabic Dialect Applications and Resources (MADAR) project.1 The goal of MADAR is to create, for a large number of dialects, a unified framework with common annotation guidelines and decisions, and targeting applications of Dialect Identification (DID) and Machine Translation (MT). The first resource is a large parallel corpus of 25 Arabic city dialects, in addition to the pre-existing parallel set for English, French and Modern Standard Arabic (MSA). The second resource is a 25-way lexicon of 1,045"
L18-1535,habash-etal-2012-conventional,1,0.901479,"the letter Qaf (  q).2 It is often observed that in Tunisian Arabic, this consonant appears as /q/ (similar to MSA), while in Egyptian and Levantine Arabic it is /P/ (glottal stop) and in Gulf Arabic it is /g/ (Haeri, 1991; Habash, 2010). Orthography While MSA has a standard orthography, the dialects do not. Often people write words reflecting the phonology or the etymology of these words. DA is sometimes written in the so-called Arabizi Romanization script (Darwish, 2014). In the context of NLP, a set of conventional orthography guidelines (CODA) has been proposed for a number of dialects (Habash et al., 2012a; Jar2 Arabic transliteration is presented in the Habash-SoudiBuckwalter scheme (Habash et al., 2007): (in alphabetical orˇ der) AbtθjHxdðrzsšSDTDςγfqklmnhwy and the additional sym   ˇ ¯ ˆ ð', yˆ ø , ¯ bols: ’ Z, Â @, A @, A @, w h è, ý ø. 3387 Region Maghreb Sub-region Morocco Algeria Tunisia Cities Rabat Algiers Tunis (RAB) (ALG) (TUN) Fes Sfax (FES) (SFX) Libya Tripoli (TRI) Benghazi (BEN) Nile Basin Egypt/Sudan Cairo (CAI) Alexandria (ALX) Aswan (ASW) Khartoum (KHA) Levant South Levant North Levant Jerusalem Beirut (JER) (BEI) Amman Damascus (AMM) (DAM) Salt Aleppo (SAL) (ALE) Gulf Iraq"
L18-1535,W12-2301,1,0.897253,"the letter Qaf (  q).2 It is often observed that in Tunisian Arabic, this consonant appears as /q/ (similar to MSA), while in Egyptian and Levantine Arabic it is /P/ (glottal stop) and in Gulf Arabic it is /g/ (Haeri, 1991; Habash, 2010). Orthography While MSA has a standard orthography, the dialects do not. Often people write words reflecting the phonology or the etymology of these words. DA is sometimes written in the so-called Arabizi Romanization script (Darwish, 2014). In the context of NLP, a set of conventional orthography guidelines (CODA) has been proposed for a number of dialects (Habash et al., 2012a; Jar2 Arabic transliteration is presented in the Habash-SoudiBuckwalter scheme (Habash et al., 2007): (in alphabetical orˇ der) AbtθjHxdðrzsšSDTDςγfqklmnhwy and the additional sym   ˇ ¯ ˆ ð', yˆ ø , ¯ bols: ’ Z, Â @, A @, A @, w h è, ý ø. 3387 Region Maghreb Sub-region Morocco Algeria Tunisia Cities Rabat Algiers Tunis (RAB) (ALG) (TUN) Fes Sfax (FES) (SFX) Libya Tripoli (TRI) Benghazi (BEN) Nile Basin Egypt/Sudan Cairo (CAI) Alexandria (ALX) Aswan (ASW) Khartoum (KHA) Levant South Levant North Levant Jerusalem Beirut (JER) (BEI) Amman Damascus (AMM) (DAM) Salt Aleppo (SAL) (ALE) Gulf Iraq"
L18-1535,L18-1574,1,0.896503,"I) Benghazi (BEN) Nile Basin Egypt/Sudan Cairo (CAI) Alexandria (ALX) Aswan (ASW) Khartoum (KHA) Levant South Levant North Levant Jerusalem Beirut (JER) (BEI) Amman Damascus (AMM) (DAM) Salt Aleppo (SAL) (ALE) Gulf Iraq Gulf Mosul Doha (MOS) (DOH) Baghdad Muscat (BAG) (MUS) Basra Riyadh (BAS) (RIY) Jeddah (JED) Yemen Yemen Sana’a (SAN) Table 1: Different region, sub-region, and city dialects considered in building the MADAR resources. rar et al., 2014; Zribi et al., 2014; Saadane and Habash, 2015; Turki et al., 2016; Khalifa et al., 2016), and has been recently unified under the CODA∗ effort (Habash et al., 2018). Morphology Morphological differences are quite common. One example is the future marker particle which appears as +  s+ or ¬ñ swf in MSA, + hH+ or hP  . bAš rH in Levantine dialects, +ë h+ in Egyptian and AK in Tunisian. This together with variation in the templatic morphology make the forms of some verbs rather different: J» A sÂktb (MSA), IJ» Ag HÂktub e.g., ’I will write’ is I . .   . bAš (Palestinian), I . Jºë hktb (Egyptian) and I.JºK AK nktb (Tunisian). Syntax Comparative studies of several Arabic dialects suggest that the syntactic differences between the dialects are relat"
L18-1535,W14-3603,1,0.936447,"Missing"
L18-1535,W14-3627,1,0.896251,"and multilingual dialectal corpora and lexicons. Several mono-dialectal corpora covering different Arabic dialects were built and made available. Al-Badrashiny et al. (2014) compiled a large dialect-identified corpus of DA from several Egyptian sources, but with a large presence of MSA. In a related effort, McNeil and Faiza (2011) built a four-million-word corpus of Tunisian Spoken Arabic. Various other research work resulted in multidialectal non parallel corpora at different scales (Zaidan and Callison-Burch, 2011; Zbib et al., 2012; Cotterell and Callison-Burch, 2014; Salama et al., 2014; Jeblee et al., 2014; Al-Shargi et al., 2016; Zaghouani and Charfi, 2018). 10 The latest version of the lexicon is available for browsing online at http://nlp.qatar.cmu.edu/madar/. 3393 As for dialect-to-dialect parallel corpora, Bouamor et al. (2014) presented the first small-scale 7-way parallel corpus covering several dialects in addition to MSA, and English, all translated from Egyptian sentences. The fact that Egyptian was chosen as a starting point affected the quality of the translation. The sentences produced were biased by the use of some Egyptian expressions that might be accepted in other dialects, but"
L18-1535,L16-1679,1,0.93096,"Rabat Algiers Tunis (RAB) (ALG) (TUN) Fes Sfax (FES) (SFX) Libya Tripoli (TRI) Benghazi (BEN) Nile Basin Egypt/Sudan Cairo (CAI) Alexandria (ALX) Aswan (ASW) Khartoum (KHA) Levant South Levant North Levant Jerusalem Beirut (JER) (BEI) Amman Damascus (AMM) (DAM) Salt Aleppo (SAL) (ALE) Gulf Iraq Gulf Mosul Doha (MOS) (DOH) Baghdad Muscat (BAG) (MUS) Basra Riyadh (BAS) (RIY) Jeddah (JED) Yemen Yemen Sana’a (SAN) Table 1: Different region, sub-region, and city dialects considered in building the MADAR resources. rar et al., 2014; Zribi et al., 2014; Saadane and Habash, 2015; Turki et al., 2016; Khalifa et al., 2016), and has been recently unified under the CODA∗ effort (Habash et al., 2018). Morphology Morphological differences are quite common. One example is the future marker particle which appears as +  s+ or ¬ñ swf in MSA, + hH+ or hP  . bAš rH in Levantine dialects, +ë h+ in Egyptian and AK in Tunisian. This together with variation in the templatic morphology make the forms of some verbs rather different: J» A sÂktb (MSA), IJ» Ag HÂktub e.g., ’I will write’ is I . .   . bAš (Palestinian), I . Jºë hktb (Egyptian) and I.JºK AK nktb (Tunisian). Syntax Comparative studies of several Arabic di"
L18-1535,Y15-1004,0,0.20582,"h of their coverage and the fine location granularity. The focus on cities, as opposed to regions in studying Arabic dialects, opens new avenues to many areas of research from dialectology to dialect identification and machine translation. Keywords: Arabic Dialects, Parallel Corpus, Lexicon 1. Introduction 2. Dialectal Arabic (DA) is emerging nowadays as the primary written language of informal communication online in the Arab World: in emails, blogs, discussion forums, chats, SMS, etc. There has been a rising interest in research on computational models of Arabic dialects in the last decade (Meftouh et al., 2015). There have been several efforts on creating different resources to allow building models for several Natural Language Processing (NLP) applications. However, these efforts have been disjoint from each other, and most of them have focused on a small number of dialects that represent vast regions of the Arab World (Zaidan and Callison-Burch, 2011; Diab et al., 2014; Sajjad et al., 2013). In this paper, we present two resources we created as part of the Multi Arabic Dialect Applications and Resources (MADAR) project.1 The goal of MADAR is to create, for a large number of dialects, a unified fra"
L18-1535,pasha-etal-2014-madamira,1,0.875164,"les from the BTEC parallel corpus. Tuples are then clustered based on their semantic similarity, such that each cluster represents a concept. The automatic process is followed by manual validation and fixing of errors resulting from the automatic process. 4.2.1. Automatic Extraction of Concept Keys Data Preprocessing Since the concept triplet words are represented in terms of lemmas, we pre-process the parallel data to map it into the lemma space. For English, we use the Stanford POS tagger (Toutanova et al., 2003) and for French, we use Treetagger (Schmid, 1994). For Arabic, we use MADAMIRA (Pasha et al., 2014) to tokenize words into the D3 scheme, which separates all clitics from the basewords. Arabic tokenization is required as the clitics attached to basewords in Arabic, are typically represented as separate words in English and French. The most common examples are the proclitic definite article + È@ Al+ ‘the’, and the enclitic possessive pronouns, such as è+ +h ‘his’. The goal here is to harmonize the forms of the three languages to encourage better word alignment and concept extraction. Triplet Extraction Our trilingual concept extraction approach focuses on collecting frequently used triplets."
L18-1535,W15-3208,1,0.95203,"hreb Sub-region Morocco Algeria Tunisia Cities Rabat Algiers Tunis (RAB) (ALG) (TUN) Fes Sfax (FES) (SFX) Libya Tripoli (TRI) Benghazi (BEN) Nile Basin Egypt/Sudan Cairo (CAI) Alexandria (ALX) Aswan (ASW) Khartoum (KHA) Levant South Levant North Levant Jerusalem Beirut (JER) (BEI) Amman Damascus (AMM) (DAM) Salt Aleppo (SAL) (ALE) Gulf Iraq Gulf Mosul Doha (MOS) (DOH) Baghdad Muscat (BAG) (MUS) Basra Riyadh (BAS) (RIY) Jeddah (JED) Yemen Yemen Sana’a (SAN) Table 1: Different region, sub-region, and city dialects considered in building the MADAR resources. rar et al., 2014; Zribi et al., 2014; Saadane and Habash, 2015; Turki et al., 2016; Khalifa et al., 2016), and has been recently unified under the CODA∗ effort (Habash et al., 2018). Morphology Morphological differences are quite common. One example is the future marker particle which appears as +  s+ or ¬ñ swf in MSA, + hH+ or hP  . bAš rH in Levantine dialects, +ë h+ in Egyptian and AK in Tunisian. This together with variation in the templatic morphology make the forms of some verbs rather different: J» A sÂktb (MSA), IJ» Ag HÂktub e.g., ’I will write’ is I . .   . bAš (Palestinian), I . Jºë hktb (Egyptian) and I.JºK AK nktb (Tunisian). Synt"
L18-1535,P13-2001,0,0.0207968,"nformal communication online in the Arab World: in emails, blogs, discussion forums, chats, SMS, etc. There has been a rising interest in research on computational models of Arabic dialects in the last decade (Meftouh et al., 2015). There have been several efforts on creating different resources to allow building models for several Natural Language Processing (NLP) applications. However, these efforts have been disjoint from each other, and most of them have focused on a small number of dialects that represent vast regions of the Arab World (Zaidan and Callison-Burch, 2011; Diab et al., 2014; Sajjad et al., 2013). In this paper, we present two resources we created as part of the Multi Arabic Dialect Applications and Resources (MADAR) project.1 The goal of MADAR is to create, for a large number of dialects, a unified framework with common annotation guidelines and decisions, and targeting applications of Dialect Identification (DID) and Machine Translation (MT). The first resource is a large parallel corpus of 25 Arabic city dialects, in addition to the pre-existing parallel set for English, French and Modern Standard Arabic (MSA). The second resource is a 25-way lexicon of 1,045 entries in each city’s"
L18-1535,salama-etal-2014-youdacc,1,0.898896,"developed monolingual and multilingual dialectal corpora and lexicons. Several mono-dialectal corpora covering different Arabic dialects were built and made available. Al-Badrashiny et al. (2014) compiled a large dialect-identified corpus of DA from several Egyptian sources, but with a large presence of MSA. In a related effort, McNeil and Faiza (2011) built a four-million-word corpus of Tunisian Spoken Arabic. Various other research work resulted in multidialectal non parallel corpora at different scales (Zaidan and Callison-Burch, 2011; Zbib et al., 2012; Cotterell and Callison-Burch, 2014; Salama et al., 2014; Jeblee et al., 2014; Al-Shargi et al., 2016; Zaghouani and Charfi, 2018). 10 The latest version of the lexicon is available for browsing online at http://nlp.qatar.cmu.edu/madar/. 3393 As for dialect-to-dialect parallel corpora, Bouamor et al. (2014) presented the first small-scale 7-way parallel corpus covering several dialects in addition to MSA, and English, all translated from Egyptian sentences. The fact that Egyptian was chosen as a starting point affected the quality of the translation. The sentences produced were biased by the use of some Egyptian expressions that might be accepted i"
L18-1535,W15-3205,0,0.305387,"ponsible for. • Delete all entries that are NOT relevant to the cities he/she is responsible for. • Apply the necessary changes for some entries that may need some minor fixes. • Add new words that are not on the AUTO list. • Think of more than one translation into his/her dialect and carefully specify the city. • Use external informants to get more information for cities in his/her area if it is not his original city. • Enter the CODA and CAPHI versions of each entry, using the guidelines provided. Very recently, automatic DA processing has attracted a considerable amount of research in NLP (Shoufan and Alameri, 2015), facilitated by the newly developed monolingual and multilingual dialectal corpora and lexicons. Several mono-dialectal corpora covering different Arabic dialects were built and made available. Al-Badrashiny et al. (2014) compiled a large dialect-identified corpus of DA from several Egyptian sources, but with a large presence of MSA. In a related effort, McNeil and Faiza (2011) built a four-million-word corpus of Tunisian Spoken Arabic. Various other research work resulted in multidialectal non parallel corpora at different scales (Zaidan and Callison-Burch, 2011; Zbib et al., 2012; Cotterell"
L18-1535,N03-1033,0,0.0111141,"t key identification relies on an automatic process that extracts (English, French, Arabic) related tuples from the BTEC parallel corpus. Tuples are then clustered based on their semantic similarity, such that each cluster represents a concept. The automatic process is followed by manual validation and fixing of errors resulting from the automatic process. 4.2.1. Automatic Extraction of Concept Keys Data Preprocessing Since the concept triplet words are represented in terms of lemmas, we pre-process the parallel data to map it into the lemma space. For English, we use the Stanford POS tagger (Toutanova et al., 2003) and for French, we use Treetagger (Schmid, 1994). For Arabic, we use MADAMIRA (Pasha et al., 2014) to tokenize words into the D3 scheme, which separates all clitics from the basewords. Arabic tokenization is required as the clitics attached to basewords in Arabic, are typically represented as separate words in English and French. The most common examples are the proclitic definite article + È@ Al+ ‘the’, and the enclitic possessive pronouns, such as è+ +h ‘his’. The goal here is to harmonize the forms of the three languages to encourage better word alignment and concept extraction. Triplet Ex"
L18-1535,L18-1111,1,0.771723,"ons. Several mono-dialectal corpora covering different Arabic dialects were built and made available. Al-Badrashiny et al. (2014) compiled a large dialect-identified corpus of DA from several Egyptian sources, but with a large presence of MSA. In a related effort, McNeil and Faiza (2011) built a four-million-word corpus of Tunisian Spoken Arabic. Various other research work resulted in multidialectal non parallel corpora at different scales (Zaidan and Callison-Burch, 2011; Zbib et al., 2012; Cotterell and Callison-Burch, 2014; Salama et al., 2014; Jeblee et al., 2014; Al-Shargi et al., 2016; Zaghouani and Charfi, 2018). 10 The latest version of the lexicon is available for browsing online at http://nlp.qatar.cmu.edu/madar/. 3393 As for dialect-to-dialect parallel corpora, Bouamor et al. (2014) presented the first small-scale 7-way parallel corpus covering several dialects in addition to MSA, and English, all translated from Egyptian sentences. The fact that Egyptian was chosen as a starting point affected the quality of the translation. The sentences produced were biased by the use of some Egyptian expressions that might be accepted in other dialects, but a native would not produce naturally. The same conce"
L18-1535,P11-2007,0,0.621852,"erging nowadays as the primary written language of informal communication online in the Arab World: in emails, blogs, discussion forums, chats, SMS, etc. There has been a rising interest in research on computational models of Arabic dialects in the last decade (Meftouh et al., 2015). There have been several efforts on creating different resources to allow building models for several Natural Language Processing (NLP) applications. However, these efforts have been disjoint from each other, and most of them have focused on a small number of dialects that represent vast regions of the Arab World (Zaidan and Callison-Burch, 2011; Diab et al., 2014; Sajjad et al., 2013). In this paper, we present two resources we created as part of the Multi Arabic Dialect Applications and Resources (MADAR) project.1 The goal of MADAR is to create, for a large number of dialects, a unified framework with common annotation guidelines and decisions, and targeting applications of Dialect Identification (DID) and Machine Translation (MT). The first resource is a large parallel corpus of 25 Arabic city dialects, in addition to the pre-existing parallel set for English, French and Modern Standard Arabic (MSA). The second resource is a 25-wa"
L18-1535,N12-1006,0,0.0438526,"(Shoufan and Alameri, 2015), facilitated by the newly developed monolingual and multilingual dialectal corpora and lexicons. Several mono-dialectal corpora covering different Arabic dialects were built and made available. Al-Badrashiny et al. (2014) compiled a large dialect-identified corpus of DA from several Egyptian sources, but with a large presence of MSA. In a related effort, McNeil and Faiza (2011) built a four-million-word corpus of Tunisian Spoken Arabic. Various other research work resulted in multidialectal non parallel corpora at different scales (Zaidan and Callison-Burch, 2011; Zbib et al., 2012; Cotterell and Callison-Burch, 2014; Salama et al., 2014; Jeblee et al., 2014; Al-Shargi et al., 2016; Zaghouani and Charfi, 2018). 10 The latest version of the lexicon is available for browsing online at http://nlp.qatar.cmu.edu/madar/. 3393 As for dialect-to-dialect parallel corpora, Bouamor et al. (2014) presented the first small-scale 7-way parallel corpus covering several dialects in addition to MSA, and English, all translated from Egyptian sentences. The fact that Egyptian was chosen as a starting point affected the quality of the translation. The sentences produced were biased by the"
L18-1535,zribi-etal-2014-conventional,1,0.960805,"ý ø. 3387 Region Maghreb Sub-region Morocco Algeria Tunisia Cities Rabat Algiers Tunis (RAB) (ALG) (TUN) Fes Sfax (FES) (SFX) Libya Tripoli (TRI) Benghazi (BEN) Nile Basin Egypt/Sudan Cairo (CAI) Alexandria (ALX) Aswan (ASW) Khartoum (KHA) Levant South Levant North Levant Jerusalem Beirut (JER) (BEI) Amman Damascus (AMM) (DAM) Salt Aleppo (SAL) (ALE) Gulf Iraq Gulf Mosul Doha (MOS) (DOH) Baghdad Muscat (BAG) (MUS) Basra Riyadh (BAS) (RIY) Jeddah (JED) Yemen Yemen Sana’a (SAN) Table 1: Different region, sub-region, and city dialects considered in building the MADAR resources. rar et al., 2014; Zribi et al., 2014; Saadane and Habash, 2015; Turki et al., 2016; Khalifa et al., 2016), and has been recently unified under the CODA∗ effort (Habash et al., 2018). Morphology Morphological differences are quite common. One example is the future marker particle which appears as +  s+ or ¬ñ swf in MSA, + hH+ or hP  . bAš rH in Levantine dialects, +ë h+ in Egyptian and AK in Tunisian. This together with variation in the templatic morphology make the forms of some verbs rather different: J» A sÂktb (MSA), IJ» Ag HÂktub e.g., ’I will write’ is I . .   . bAš (Palestinian), I . Jºë hktb (Egyptian) and I.JºK"
L18-1574,C16-2044,1,0.881168,"Missing"
L18-1574,2014.iwslt-papers.1,0,0.119633,"Missing"
L18-1574,P11-2062,1,0.860318,"f MSA patterns will retain the spelling choice of the MSA pattern if the difference in pronunciation can be expressed using diacritics (for vowel change or absence), or if the pronunciation is a shortened form of the MSA pattern vowels. Alif Maqsura The MSA rules for spelling the AlifMaqsura ( ø ý), which are sometimes based on roots and sometimes on patterns, apply in CODA*. 6.4.2. The Ta-Marbuta  The Ta-Marbuta ( è ¯h) is a secondary letter of the Arabic alphabet used to represent a particular suffix morpheme that is often (but not exclusively) associated with the femininesingular feature (Alkuhlani and Habash, 2011). This morpheme has a number of allomorphs with differing pronunciations. Most notably, it appears as a vowel at the end of nominals, and changes to a ∼ /t/ when followed by possessive pronominal enclitics. The Ta-Marbuta should be writ ten as è ¯h in word-final positions, regardless of its pronunciation, and following general CODA rules in non-word-final positions. See Table 3 for example cases. 6.3.3. Clitic Spelling The general rule on phonological clitic spelling is that clitics that are mapped into single letters (with possible diacritics) will be spelled attached to the word, and will n"
L18-1574,W14-3612,1,0.893572,"1 which it can be confused by speakers of dialects which do not have the phoneme [p]. Thus, pQ is included in CAPHI as /p./ as it is useful in describing the dialectal differences between Iraqi and other dialects. The complete CAPHI inventory is listed in Figure 2. 6. CODA* General Rules and Specifications While the goals of the CODA* guidelines is to precisely define the CODA choices, it is unavoidable that different versions of the guidelines will need to be presented differently for specific annotators on specific tasks for specific dialects: e.g., conversion form Arabizi to Arabic script (Bies et al., 2014), or lexicon construction (Diab et al., 2014). In this paper, we summarize and highlight specific contributions of the effort; but the full set of CODA* guidelines is described on its online page (See Section 8.). We start with a description of the technical terminology we use; then we discuss the various rules and how to use them. The border between the general rules and the specification rules is broadly drawn along the lines that general rules do not refer to any specific lexical items (morphemes or words) and pertain to the meta-mechanics of CODA; while the specification rules are lexicall"
L18-1574,L18-1535,1,0.789785,"presented in previous work; (c) the introduction of the concept of a multidialectal Seed Lexicon that is used to allow users of CODA* to have access to previous decisions when identifying spellings for new words in new dialects; and finally, (d) a set of online pages that give users easy public access to all of these resources. The CODA* guidelines and their connected resources are being used by three large Arabic dialect processing projects in three universities: The Multi-Arabic Dialect Applications and Resources at Carnegie Mellon University Qatar and New York University Abu Dhabi (NYUAD) (Bouamor et al., 2018), The Gulf Arabic Annotated Corpus (NYUAD) (Khalifa et al., 2018), and The Columbia Arabic Dialect Annotation project (Columbia University and NYUAD). The CODA* effort is large and ongoing; the goal of this paper is to introduce the effort and some of its important contributions on how to conceptualize and address the question of orthographic decisions in dialectal Arabic computational processing. The rest of the paper is structured as follows. We present common challenges to Arabic processing in Section 2. This is followed by related work in Section 3. We introduce CODA* in Section 4., and di"
L18-1574,diab-etal-2014-tharwa,1,0.959854,"sals such as the Asaakir system (‘Asaakir, 1950) and Akl’s system (Arkadiusz, 2006), neither of which are broadly used today. Various DA dictionaries used Arabic, Latin or mixed script orthographies (Badawi and Hinds, 1986). In the context of NLP, the Linguistic Data Consortium (LDC) guidelines for transcribing Levantine Arabic (Maamouri et al., 2004) and the COLABA project at Columbia University (Diab et al., 2010) were precursors to the work of Habash et al. (2012). After the CODA-Egyptian guidelines were created and used for the creation of Egyptian Arabic resources (Maamouri et al., 2014; Diab et al., 2014; Pasha et al., 2014; Eskander et al., 2013; Al-Badrashiny et al., 2014), two additional sets of guidelines were created for CODATunisian (Zribi et al., 2014) and CODA-Palestinian (Jarrar et al., 2014). These were part of projects involving morphology annotation (Palestinian) or speech recognition (Tunisian). A variant on CODA was proposed for speech recognition by Ali et al. (2014) and was shown to reduce OOV and perplexity. Since then, four more dialects followed: CODA-Algerian (Saadane and Habash, 2015), CODA-Gulf (Khalifa et al., 2016), CODA-Moroccan and CODA-Yemeni (Al-Shargi et al., 2016"
L18-1574,N13-1066,1,0.885238,"Missing"
L18-1574,habash-etal-2012-conventional,1,0.57243,"vary from MSA and from each other in terms of phonology, morphology, lexicon and syntax (Watson, 2007), using MSA orthographic standards cannot fully address the needs of the dialects. As an example of the degree of variety in dialectal spelling, Figure 1. presents the 27 actually attested spellings of one Egyptian Arabic word online. The large number of possibilities results from independent decisions such as whether the proclitic /ma/ should be written attached or separated (+Ó m+1 or AÓ mA), or whether to write the stem in a way  w), ˆ or etymology (  q). that reflects its phonology (ð' Habash et al. (2012) introduced the concept of Conventional Orthography for Dialectal Arabic (CODA); and they proposed a set of guidelines and exception lists for Egyptian Arabic. Their conventions were used in the Linguistic Data Consortium for annotating Egyptian Arabic (Maamouri et al., 2014). Since then, a number of additional efforts followed suit for other dialects (Zribi et al., 2014; Saadane and Habash, 2015; Jarrar et al., 2016; Khalifa et al., 2016). While the original CODA guidelines aimed at being easy to adjust to new dialects and contained some 1 Arabic script transliteration is presented in the Hab"
L18-1574,W14-3603,1,0.933108,"Missing"
L18-1574,L16-1679,1,0.936174,"should be written attached or separated (+Ó m+1 or AÓ mA), or whether to write the stem in a way  w), ˆ or etymology (  q). that reflects its phonology (ð' Habash et al. (2012) introduced the concept of Conventional Orthography for Dialectal Arabic (CODA); and they proposed a set of guidelines and exception lists for Egyptian Arabic. Their conventions were used in the Linguistic Data Consortium for annotating Egyptian Arabic (Maamouri et al., 2014). Since then, a number of additional efforts followed suit for other dialects (Zribi et al., 2014; Saadane and Habash, 2015; Jarrar et al., 2016; Khalifa et al., 2016). While the original CODA guidelines aimed at being easy to adjust to new dialects and contained some 1 Arabic script transliteration is presented in the Habash-SoudiBuckwalter transliteration scheme (Habash et al., 2007):   P     ¨ ¨ ¬   È Ð à è ð ø @ H. H H h. h p X XP ˇ ς γ f q k l m n hw y Â b t θ j H x dðr z s š S D T D ˇ ¯  and the additional symbols: ’ Z, Â @, A @, A @, wˆ ð', yˆ Zø', ¯h è, ý ø. Phonological forms are presented in IPA or in the CAPHI scheme, which is discussed in Section 5. Frequency ≈ 26,000 ≈ 13,000 ≤ 10,000 ≤ 1,000 ≤ 100 ≤ 10 Figure 1: 27 encountered ways"
L18-1574,L18-1607,1,0.954641,"f a multidialectal Seed Lexicon that is used to allow users of CODA* to have access to previous decisions when identifying spellings for new words in new dialects; and finally, (d) a set of online pages that give users easy public access to all of these resources. The CODA* guidelines and their connected resources are being used by three large Arabic dialect processing projects in three universities: The Multi-Arabic Dialect Applications and Resources at Carnegie Mellon University Qatar and New York University Abu Dhabi (NYUAD) (Bouamor et al., 2018), The Gulf Arabic Annotated Corpus (NYUAD) (Khalifa et al., 2018), and The Columbia Arabic Dialect Annotation project (Columbia University and NYUAD). The CODA* effort is large and ongoing; the goal of this paper is to introduce the effort and some of its important contributions on how to conceptualize and address the question of orthographic decisions in dialectal Arabic computational processing. The rest of the paper is structured as follows. We present common challenges to Arabic processing in Section 2. This is followed by related work in Section 3. We introduce CODA* in Section 4., and discuss its components in Section 5. (CAPHI), Section 6. (General R"
L18-1574,maamouri-etal-2014-developing,1,0.931749,"actually attested spellings of one Egyptian Arabic word online. The large number of possibilities results from independent decisions such as whether the proclitic /ma/ should be written attached or separated (+Ó m+1 or AÓ mA), or whether to write the stem in a way  w), ˆ or etymology (  q). that reflects its phonology (ð' Habash et al. (2012) introduced the concept of Conventional Orthography for Dialectal Arabic (CODA); and they proposed a set of guidelines and exception lists for Egyptian Arabic. Their conventions were used in the Linguistic Data Consortium for annotating Egyptian Arabic (Maamouri et al., 2014). Since then, a number of additional efforts followed suit for other dialects (Zribi et al., 2014; Saadane and Habash, 2015; Jarrar et al., 2016; Khalifa et al., 2016). While the original CODA guidelines aimed at being easy to adjust to new dialects and contained some 1 Arabic script transliteration is presented in the Habash-SoudiBuckwalter transliteration scheme (Habash et al., 2007):   P     ¨ ¨ ¬   È Ð à è ð ø @ H. H H h. h p X XP ˇ ς γ f q k l m n hw y Â b t θ j H x dðr z s š S D T D ˇ ¯  and the additional symbols: ’ Z, Â @, A @, A @, wˆ ð', yˆ Zø', ¯h è, ý ø. Phonological fo"
L18-1574,Y15-1004,0,0.179652,"Missing"
L18-1574,pasha-etal-2014-madamira,1,0.908097,"Missing"
L18-1574,W15-3208,1,0.95202,"t decisions such as whether the proclitic /ma/ should be written attached or separated (+Ó m+1 or AÓ mA), or whether to write the stem in a way  w), ˆ or etymology (  q). that reflects its phonology (ð' Habash et al. (2012) introduced the concept of Conventional Orthography for Dialectal Arabic (CODA); and they proposed a set of guidelines and exception lists for Egyptian Arabic. Their conventions were used in the Linguistic Data Consortium for annotating Egyptian Arabic (Maamouri et al., 2014). Since then, a number of additional efforts followed suit for other dialects (Zribi et al., 2014; Saadane and Habash, 2015; Jarrar et al., 2016; Khalifa et al., 2016). While the original CODA guidelines aimed at being easy to adjust to new dialects and contained some 1 Arabic script transliteration is presented in the Habash-SoudiBuckwalter transliteration scheme (Habash et al., 2007):   P     ¨ ¨ ¬   È Ð à è ð ø @ H. H H h. h p X XP ˇ ς γ f q k l m n hw y Â b t θ j H x dðr z s š S D T D ˇ ¯  and the additional symbols: ’ Z, Â @, A @, A @, wˆ ð', yˆ Zø', ¯h è, ý ø. Phonological forms are presented in IPA or in the CAPHI scheme, which is discussed in Section 5. Frequency ≈ 26,000 ≈ 13,000 ≤ 10,000 ≤ 1,"
L18-1574,zaghouani-etal-2014-large,1,0.923929,"Missing"
L18-1574,N18-1087,1,0.880283,"Missing"
L18-1574,zribi-etal-2014-conventional,1,0.961603,"ults from independent decisions such as whether the proclitic /ma/ should be written attached or separated (+Ó m+1 or AÓ mA), or whether to write the stem in a way  w), ˆ or etymology (  q). that reflects its phonology (ð' Habash et al. (2012) introduced the concept of Conventional Orthography for Dialectal Arabic (CODA); and they proposed a set of guidelines and exception lists for Egyptian Arabic. Their conventions were used in the Linguistic Data Consortium for annotating Egyptian Arabic (Maamouri et al., 2014). Since then, a number of additional efforts followed suit for other dialects (Zribi et al., 2014; Saadane and Habash, 2015; Jarrar et al., 2016; Khalifa et al., 2016). While the original CODA guidelines aimed at being easy to adjust to new dialects and contained some 1 Arabic script transliteration is presented in the Habash-SoudiBuckwalter transliteration scheme (Habash et al., 2007):   P     ¨ ¨ ¬   È Ð à è ð ø @ H. H H h. h p X XP ˇ ς γ f q k l m n hw y Â b t θ j H x dðr z s š S D T D ˇ ¯  and the additional symbols: ’ Z, Â @, A @, A @, wˆ ð', yˆ Zø', ¯h è, ý ø. Phonological forms are presented in IPA or in the CAPHI scheme, which is discussed in Section 5. Frequency ≈ 26,"
N13-1046,D07-1074,0,0.105208,"Missing"
N13-1046,N06-1060,0,0.0229809,"sawa, 2007) and have been already used in different works for NEs recognition (Nothman et al., 2013) and disambiguation (Cucerzan, 2007). We improve the Arabic-English Wikipedia title lexicon of Mohit et al. (2012) and build a Wikipedia exclusive lexicon with 4K bilingual entities. In order to test the domain effects, our lexicon includes only NEs which are not present in the parallel corpus. The statistics given in Table 1 demonstrate different nature of the labeled datasets. The two datasets were labeled semi-automatically using the transliteration similarity measure (Frscore ) proposed by Freeman et al. (2006), a variant of edit distance measuring the similarity between an English word and its Arabic transliteration. In our experiments, English tokens having an Frscore &gt; 0.6 are considered as transliteration, others having Frscore &lt; 0.5 as translation. These thresholds were determined after tuning with a held out development set. For tokens having Frscore between 0.5 and 0.6, the decision is not obvious. To label these instances (around 5K unique tokens), we manually transliterate them using Microsoft Maren tool.3 We again compute the Frscore between the obtained transliteration, in its Buckwalter"
N13-1046,W98-1005,0,0.223308,"nditions is expected to reduce the domain gap and improve the translation quality. English Input: The British comedy troupe Monty Python. Baseline MT: Q.Ë@ éK YJ ÓñºË@ é¯Q ®Ë@ .ù ª¯@ UNK éJ KA¢ Alfrqp Alkwmydyp AlbryTAnyp UNK AfEY MT+Classifier: JK AK. úæKñÓ éJ KA¢ Q.Ë@ éK YJ ÓñºË@ é¯Q ®Ë@ . àñ Alfrqp Alkwmydyp AlbryTAnyp mwnty bAyvwn. 4 Related work A number of efforts have been made to undertake the NE translation problem for different language pairs. Among them some use sequence of phonetic-based probabilistic models to convert names written in Arabic into the English script (Glover-Stalls and Knight, 1998) for transliteration of names and technical terms that occurs in Arabic texts and originate in English. Others rely on spellingbased model that directly maps an English letter sequence into an Arabic one (Al-Onaizan and Knight, 2002a). In a related work, Al-Onaizan and Knight (2002b) describe a combination of a phonetic-based model and a spellingbased one to build a transliteration model to generate Arabic to English name translations. In the same direction, Hassan et al. (2007) extracted NE translation pairs from both comparable and parallel corpora and evaluate their quality in a NE translat"
N13-1046,P08-1045,0,0.178336,"Missing"
N13-1046,P07-2045,0,0.00674443,"e offer the transliterated form as an option to the decoder aiming to improve the decoding process. For that a human annotator selected the transliterations from the suggested list that is provided by the automatic transliterator (Maren) without any knowledge of the reference transliterations. Table 4 shows the impact of adding the classifier to the SMT pipeline with a modest improvement. Moreover, a bilingual annotator examined the automatically tagged NEs in the MT test set and labeled them with the translation vs. transliteration 4 The baseline MT system is the M OSES phrase-based decoder (Koehn et al., 2007) trained on a standard English-Arabic parallel corpus. The 18 million parallel corpus consists of the non-UN parts of the NIST corpus distributed by the Linguistic Data Consortium. We perform the standard preprocessing and tokenization on the English side. We also use MADA+TOKAN (Habash et al., 2009) to preprocess and tokenize the Arabic side of the corpus. We use the standard setting of G IZA ++ and the grow-diagonal-final heuristic of M OSES to get the word alignments. We use a set of 500 sentences to tune the decoder parameters using the MERT (Och, 2003). We use El Kholy and Habash (2010) d"
N13-1046,2011.iwslt-papers.3,0,0.164606,"es and technical terms that occurs in Arabic texts and originate in English. Others rely on spellingbased model that directly maps an English letter sequence into an Arabic one (Al-Onaizan and Knight, 2002a). In a related work, Al-Onaizan and Knight (2002b) describe a combination of a phonetic-based model and a spellingbased one to build a transliteration model to generate Arabic to English name translations. In the same direction, Hassan et al. (2007) extracted NE translation pairs from both comparable and parallel corpora and evaluate their quality in a NE translation system. More recently, Ling et al. (2011) propose a Web-based method that translates Chinese NEs into English. Our work is similar in its general objectives and framework to the work presented by Hermjakob et al. (2008), which describes an approach for identifying NEs that should be transliterated from Arabic into English during translation. Their method seeks to find a corresponding English word for each Arabic word in a parallel corpus, and tag the Arabic words as either NEs or non-NEs based on a matching algorithm. In contrast, we tackle this problem in the reverse direction (translating/transliterating English NEs into Arabic). W"
N13-1046,maegaard-etal-2010-cooperation,0,0.0664051,"Missing"
N13-1046,E12-1017,1,0.8978,"Missing"
N13-1046,P03-1021,0,0.0165298,"Missing"
N13-1046,P02-1040,0,0.0849224,"Missing"
N13-1046,W09-1119,0,0.0420229,"of the news related NEs and a smaller set of diverse-topic NEs extracted from Wikipedia titles. We evaluate the two classifiers in both news and the diverse domains to observe the effects of noise and domain change. 2.1 Preparing the labeled data Our classifier requires a set of NEs with token-level gold labels. We compile such data from two resources: We heuristically extract and label parallel NEs from a large word aligned parallel corpus and we use a lexicon of bilingual NEs collected from Arabic and Wikipedia titles. Starting with a word aligned parallel corpus, we use the UIUC NE tagger (Ratinov and Roth, 2009) to tag the English sentences with four classes of NEs: Person (PER), Location (LOC), Organization (ORG) and Miscellaneous (MISC). Furthermore, we use the word alignments to project and collect the span of the associated Arabic named-entities. To reduce the noisy nature of word alignments, we designed a procedure to clean up the noisy Arabic NE spans by POS verification, and heuristically filtering impossible items (e.g. verbs). This results in a bilingual lexicon of about 57K named-entity pairs. The distribution of NEs categories is reported in Table 1. To train and evaluate the Cdiverse clas"
N13-1046,P02-1051,0,\N,Missing
N16-1125,N16-3004,1,0.864659,"Missing"
N16-1125,W07-0718,0,0.0456341,"res given by the evaluators. 1 Introduction Human evaluation has been the preferred method for tracking the progress of MT systems. In the past, the prevalent criterion was to judge the quality of a translation in terms of fluency and adequacy, on an absolute scale (White et al., 1994). However, different evaluators focused on different aspects of the translations, which increased the subjectivity of their judgments. As a result, evaluations suffered from low inter- and intra-annotator agreements (Turian et al., 2003; Snover et al., 2006). This caused a shift towards a ranking-based approach (Callison-Burch et al., 2007). Unfortunately, the disagreement between evaluators is still a challenge that cannot be easily resolved due to the non-transparent thought-process that evaluators follow to make a judgment. The eye-mind hypothesis (Just and Carpenter, 1980; Potter, 1983) states that when completing a task, people cognitively process objects that are in front of their eyes (i.e. where they fixate their gaze).1 Based on this assumption, it has been possible to study reading behavior and patterns (Rayner, 1998; Garrod, 2006; Hansen and Ji, 2010). The overall difficulty of a sentence and its syntactic complexity"
N16-1125,W12-3102,0,0.0857719,"Missing"
N16-1125,P11-1105,1,0.882545,"and distance between the start and end words. For subsequent words n, n + 1, this would mean a forward jump of distance equal to 1. All jumps with distance greater than 4 were sorted into a 5+ bucket. Additionally, we separate the features for reference and translation jumps. We also count the total number of jumps. Total jump distance We additionally aggregate jump distances2 to count the total distance covered while evaluating a sentence. We have reference distance and translation distance features. Again, the 2 Jump count and distance features have also shown to be useful in SMT decoders (Durrani et al., 2011). 1083 idea is that for a well-formed sentence, gaze distance should be less, compared to a poorly-formed one. Inter-region jumps While reading a translation, evaluators can jump between the translation and a reference to compare them. Intuitively, more jumps of this type could signify that the translation is harder to evaluate. Here we count the number of transitions between reference and translation. Dwell time The amount of time a person fixates on a region is a crucial marker for processing difficulty in sentence comprehension (Clifton et al., 2007) and moderately correlates with the quali"
N16-1125,W13-2305,0,0.0177286,"was performed by 6 different evaluators, resulting in 720 evaluations. The annotators were presented with a translationreference pair at a time. The two evaluation tasks corresponding to the same reference were presented at two different times with at least 40 other tasks in-between. This was done to prevent any possible spurious effects that may arise from remembering the content of a first translation, when evaluating the second translation of the same sentence. During each evaluation task, the evaluators were asked to assess the quality of a translation by providing a score between 0–100 (Graham et al., 2013). The observed inter-annotator agreement (Cohen’s kappa) among our annotators was 0.321. This is slightly higher than the overall inter-annotator agreement of 0.284 reported in WMT’12 for the Spanish-English.3 For reading patterns we use the EyeTribe eye-tracker at 3 For a rough comparison only. Note that these two numbers are not exactly comparable given that they are calculated on different subsets of the same data. Still, there is a fair agreement between the our evaluators and the expected wins from WMT’12 (avg. pairwise kappa of 0.381) 1084 Evaluation In our evaluation, we used eye-tracki"
N16-1125,W15-3059,1,0.875061,"Missing"
N16-1125,P02-1040,0,0.0970473,"n evaluation metric So far, we’ve shown that the individual sets of features based on reading patterns can help to predict translation quality, and that this goes beyond simple fluency. One question that remains to be answered is whether these features could be used as a whole to evaluate the quality of a translation semi-automatically. That is, whether we can use the gaze information, and other lexical information to anticipate the score that an evaluator will assign to a translation. Here, we present evaluation results combining several of these gaze features, and compare them against BLEU (Papineni et al., 2002), which uses lexical information and is designed to measure not only fluency but also adequacy. In Table 2, we present results in the following way: in (I) we present the best non-lexicalized feature combinations that improve the predictive power of the model. In (II) we re-introduce the results of lexicalized jumps feature. In (III) we present results of BLEU and the combination of eye-tracking features with it. Finally in (IV) we present the humanto-human agreement measured in average Kendall’s tau and in max human-to-human Kendall’s tau. Combinations of translation jumps In section I we pre"
N16-1125,2006.amta-papers.25,0,0.0363431,"patterns can be used to build semi-automatic metrics that anticipate the scores given by the evaluators. 1 Introduction Human evaluation has been the preferred method for tracking the progress of MT systems. In the past, the prevalent criterion was to judge the quality of a translation in terms of fluency and adequacy, on an absolute scale (White et al., 1994). However, different evaluators focused on different aspects of the translations, which increased the subjectivity of their judgments. As a result, evaluations suffered from low inter- and intra-annotator agreements (Turian et al., 2003; Snover et al., 2006). This caused a shift towards a ranking-based approach (Callison-Burch et al., 2007). Unfortunately, the disagreement between evaluators is still a challenge that cannot be easily resolved due to the non-transparent thought-process that evaluators follow to make a judgment. The eye-mind hypothesis (Just and Carpenter, 1980; Potter, 1983) states that when completing a task, people cognitively process objects that are in front of their eyes (i.e. where they fixate their gaze).1 Based on this assumption, it has been possible to study reading behavior and patterns (Rayner, 1998; Garrod, 2006; Hans"
N16-1125,stymne-etal-2012-eye,0,0.322545,"enomena remain to be explored in future work. Human performance On average, evaluators agreements with each other are fair (τ = 0.33) and below the best combination (CB3 ), while the maximum agreement of any two evaluators is relatively higher (τ = 0.53). This tells us that on average the semi-automatic approach to evaluation that we propose here is already competitive to predictions done by another (average) human. However, there is still room for improvement with respect to the mostagreeing pair of evaluators. 5 Related Work Eye-tracking devices have been used previously in the MT research. Stymne et al. (2012) used eye-tracking to identify and classify MT errors. 1086 SYS Feature Sets τ I. Combination of translation jumps EyeTrabj Backward jumps CTJ1 Backward jumps, total jumps CTJ2 Backward jumps, total jumps, distance 0.22 0.25 0.27 II. Eye-tracking: Best Lexicalized EyeLexall Lexicalized gaze jumps 0.22 III. Combinations with BLEU Bbleu BLEU CB1 Bbleu + EyeTrabj CB2 Bbleu + CTJ2 CB3 Bbleu + EyeLexall 0.34 0.38 0.39 0.42 IV. Human performance Avg Avg. human-to-human agreement Max Max. human-to-human agreement 0.33 0.53 Table 2: Result of combining several jump and lexicalized features with BLEU."
N16-1125,2003.mtsummit-papers.51,0,0.111887,"ts show that reading patterns can be used to build semi-automatic metrics that anticipate the scores given by the evaluators. 1 Introduction Human evaluation has been the preferred method for tracking the progress of MT systems. In the past, the prevalent criterion was to judge the quality of a translation in terms of fluency and adequacy, on an absolute scale (White et al., 1994). However, different evaluators focused on different aspects of the translations, which increased the subjectivity of their judgments. As a result, evaluations suffered from low inter- and intra-annotator agreements (Turian et al., 2003; Snover et al., 2006). This caused a shift towards a ranking-based approach (Callison-Burch et al., 2007). Unfortunately, the disagreement between evaluators is still a challenge that cannot be easily resolved due to the non-transparent thought-process that evaluators follow to make a judgment. The eye-mind hypothesis (Just and Carpenter, 1980; Potter, 1983) states that when completing a task, people cognitively process objects that are in front of their eyes (i.e. where they fixate their gaze).1 Based on this assumption, it has been possible to study reading behavior and patterns (Rayner, 19"
N16-1125,1994.amta-1.25,0,0.757488,"Missing"
N19-4002,W15-3206,0,0.0282561,"tandard Arabic (MSA). ADIDA displays the results with either a point map or a heat map overlaid on top of a geographical map of the Arab World. 2 3 Related Work 3.1 Arabic Dialect Processing While automatic processing of DA is relatively recent compared to MSA, it has attracted a considerable amount of research in NLP (Shoufan and Al-Ameri, 2015). Most of it focuses on (i) collecting datasets from various sources and at different levels (Zaidan and Callison-Burch, 2011; Khalifa et al., 2016; Abdul-Mageed et al., 2018; Bouamor et al., 2018), (ii) creating processing tools (Habash et al., 2013; Al-Shargi and Rambow, 2015; Obeid et al., 2018) (iii) developing DA to English maArabic and its Dialects Although MSA is the official language across the Arab World, it is not the native language of any speakers of Arabic. Dialectal Arabic (DA), on the other hand, is the daily informal spoken variety. 1 https://adida.abudhabi.nyu.edu/  The Arabic word èYK Y« /ςadida/ means ‘numerous’. 2 https://camel.abudhabi.nyu.edu/madar/ 6 Proceedings of NAACL-HLT 2019: Demonstrations, pages 6–11 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics Region Maghreb Sub-region Morocco Algeria"
N19-4002,W16-4819,0,0.0196803,"ence, developing an automatic identification system working at different levels of representation and exploring different datasets has attracted increasing attention in recent years. For instance, DID has been the goal of a dedicated shared task (Malmasi et al., 2016; Zampieri et al., 2017, 2018), encouraging researchers to submit systems to recognize the dialect of speech transcripts for dialects of four main regions: Egyptian, Gulf, Levantine and North African, and MSA. Several systems implementing a range of traditional supervised learning (Tillmann et al., 2014) and deep learning methods (Belinkov and Glass, 2016; Michon et al., 2018) were proposed. In the literature, a number of studies have been exploring DID using several datasets, ranging from user-generated content (i.e., blogs, social media posts) (Sadat et al., 2014), speech transcripts (Biadsy et al., 2009; Bougrine et al., 2017), and other corpora (Elfardy and Diab, 2012, 2013; Zaidan and Callison-Burch, 2014; Salameh et al., 2018; Dinu et al., 2018; Goldman et al., 2018). Shoufan and Al-Ameri (2015) and Al-Ayyoub et al. (2017) present a survey on NLP and deep learning methods for processing Arabic dialectal data with an overview on Arabic DI"
N19-4002,W09-0807,1,0.731291,", 2016; Zampieri et al., 2017, 2018), encouraging researchers to submit systems to recognize the dialect of speech transcripts for dialects of four main regions: Egyptian, Gulf, Levantine and North African, and MSA. Several systems implementing a range of traditional supervised learning (Tillmann et al., 2014) and deep learning methods (Belinkov and Glass, 2016; Michon et al., 2018) were proposed. In the literature, a number of studies have been exploring DID using several datasets, ranging from user-generated content (i.e., blogs, social media posts) (Sadat et al., 2014), speech transcripts (Biadsy et al., 2009; Bougrine et al., 2017), and other corpora (Elfardy and Diab, 2012, 2013; Zaidan and Callison-Burch, 2014; Salameh et al., 2018; Dinu et al., 2018; Goldman et al., 2018). Shoufan and Al-Ameri (2015) and Al-Ayyoub et al. (2017) present a survey on NLP and deep learning methods for processing Arabic dialectal data with an overview on Arabic DID of text and speech. While most of the proposed approaches targeted regional or country level DID, Salameh et al. (2018) introduced a fine-grained DID system covering the dialects of 25 cities from several countries across the Arab world (from Rabat to Mu"
N19-4002,W14-5313,0,0.0297096,"it is quite arduous to distinguish between them. Hence, developing an automatic identification system working at different levels of representation and exploring different datasets has attracted increasing attention in recent years. For instance, DID has been the goal of a dedicated shared task (Malmasi et al., 2016; Zampieri et al., 2017, 2018), encouraging researchers to submit systems to recognize the dialect of speech transcripts for dialects of four main regions: Egyptian, Gulf, Levantine and North African, and MSA. Several systems implementing a range of traditional supervised learning (Tillmann et al., 2014) and deep learning methods (Belinkov and Glass, 2016; Michon et al., 2018) were proposed. In the literature, a number of studies have been exploring DID using several datasets, ranging from user-generated content (i.e., blogs, social media posts) (Sadat et al., 2014), speech transcripts (Biadsy et al., 2009; Bougrine et al., 2017), and other corpora (Elfardy and Diab, 2012, 2013; Zaidan and Callison-Burch, 2014; Salameh et al., 2018; Dinu et al., 2018; Goldman et al., 2018). Shoufan and Al-Ameri (2015) and Al-Ayyoub et al. (2017) present a survey on NLP and deep learning methods for processing"
N19-4002,J14-1006,0,0.1236,"Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics Region Maghreb Sub-region Morocco Algeria Tunisia Libya Cities Rabat Algiers Tunis Tripoli Fes Sfax Benghazi Nile Basin Egypt/Sudan Cairo Alexandria Aswan Khartoum Levant Gulf South Levant North Levant Iraq Gulf Jerusalem Beirut Mosul Doha Amman Damascus Baghdad Muscat Salt Aleppo Basra Riyadh Jeddah Yemen Yemen Sana’a Table 1: Different city dialects covered in ADIDA and the regions they belong to. 3.3 chine translation systems (Zbib et al., 2012; Sajjad et al., 2013), (iv) or performing dialect identification (Zaidan and Callison-Burch, 2014; Huang, 2015; Salameh et al., 2018). 3.2 Visualization Map visualizations are used in multiple fields of study including linguistics, socio-linguistics, and political science to display geographical relations of non-geographic data. Geographical visualizations may include point maps to display individual data points, choropleths and Voronoi tessalation maps that cluster data points by region, and heat maps and surface maps that interpolate data over some geographical area. In the general context of visualization of language data, one example is the Visualizing Medieval Places project (Wrisley"
N19-4002,P11-2007,0,0.409839,"18). The dialect identification system produces a vector of probabilities indicating the likelihood an input sentence is from 25 cities (Table 1) and Modern Standard Arabic (MSA). ADIDA displays the results with either a point map or a heat map overlaid on top of a geographical map of the Arab World. 2 3 Related Work 3.1 Arabic Dialect Processing While automatic processing of DA is relatively recent compared to MSA, it has attracted a considerable amount of research in NLP (Shoufan and Al-Ameri, 2015). Most of it focuses on (i) collecting datasets from various sources and at different levels (Zaidan and Callison-Burch, 2011; Khalifa et al., 2016; Abdul-Mageed et al., 2018; Bouamor et al., 2018), (ii) creating processing tools (Habash et al., 2013; Al-Shargi and Rambow, 2015; Obeid et al., 2018) (iii) developing DA to English maArabic and its Dialects Although MSA is the official language across the Arab World, it is not the native language of any speakers of Arabic. Dialectal Arabic (DA), on the other hand, is the daily informal spoken variety. 1 https://adida.abudhabi.nyu.edu/  The Arabic word èYK Y« /ςadida/ means ‘numerous’. 2 https://camel.abudhabi.nyu.edu/madar/ 6 Proceedings of NAACL-HLT 2019: Demonstrati"
N19-4002,W17-1201,0,0.190799,"Missing"
N19-4002,W18-3901,0,0.102306,"Missing"
N19-4002,N12-1006,0,0.0279204,"du/madar/ 6 Proceedings of NAACL-HLT 2019: Demonstrations, pages 6–11 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics Region Maghreb Sub-region Morocco Algeria Tunisia Libya Cities Rabat Algiers Tunis Tripoli Fes Sfax Benghazi Nile Basin Egypt/Sudan Cairo Alexandria Aswan Khartoum Levant Gulf South Levant North Levant Iraq Gulf Jerusalem Beirut Mosul Doha Amman Damascus Baghdad Muscat Salt Aleppo Basra Riyadh Jeddah Yemen Yemen Sana’a Table 1: Different city dialects covered in ADIDA and the regions they belong to. 3.3 chine translation systems (Zbib et al., 2012; Sajjad et al., 2013), (iv) or performing dialect identification (Zaidan and Callison-Burch, 2014; Huang, 2015; Salameh et al., 2018). 3.2 Visualization Map visualizations are used in multiple fields of study including linguistics, socio-linguistics, and political science to display geographical relations of non-geographic data. Geographical visualizations may include point maps to display individual data points, choropleths and Voronoi tessalation maps that cluster data points by region, and heat maps and surface maps that interpolate data over some geographical area. In the general context"
N19-4002,D15-1254,0,0.0167688,". 2019 Association for Computational Linguistics Region Maghreb Sub-region Morocco Algeria Tunisia Libya Cities Rabat Algiers Tunis Tripoli Fes Sfax Benghazi Nile Basin Egypt/Sudan Cairo Alexandria Aswan Khartoum Levant Gulf South Levant North Levant Iraq Gulf Jerusalem Beirut Mosul Doha Amman Damascus Baghdad Muscat Salt Aleppo Basra Riyadh Jeddah Yemen Yemen Sana’a Table 1: Different city dialects covered in ADIDA and the regions they belong to. 3.3 chine translation systems (Zbib et al., 2012; Sajjad et al., 2013), (iv) or performing dialect identification (Zaidan and Callison-Burch, 2014; Huang, 2015; Salameh et al., 2018). 3.2 Visualization Map visualizations are used in multiple fields of study including linguistics, socio-linguistics, and political science to display geographical relations of non-geographic data. Geographical visualizations may include point maps to display individual data points, choropleths and Voronoi tessalation maps that cluster data points by region, and heat maps and surface maps that interpolate data over some geographical area. In the general context of visualization of language data, one example is the Visualizing Medieval Places project (Wrisley, 2017, 2019)"
N19-4002,L16-1679,1,0.732632,"ystem produces a vector of probabilities indicating the likelihood an input sentence is from 25 cities (Table 1) and Modern Standard Arabic (MSA). ADIDA displays the results with either a point map or a heat map overlaid on top of a geographical map of the Arab World. 2 3 Related Work 3.1 Arabic Dialect Processing While automatic processing of DA is relatively recent compared to MSA, it has attracted a considerable amount of research in NLP (Shoufan and Al-Ameri, 2015). Most of it focuses on (i) collecting datasets from various sources and at different levels (Zaidan and Callison-Burch, 2011; Khalifa et al., 2016; Abdul-Mageed et al., 2018; Bouamor et al., 2018), (ii) creating processing tools (Habash et al., 2013; Al-Shargi and Rambow, 2015; Obeid et al., 2018) (iii) developing DA to English maArabic and its Dialects Although MSA is the official language across the Arab World, it is not the native language of any speakers of Arabic. Dialectal Arabic (DA), on the other hand, is the daily informal spoken variety. 1 https://adida.abudhabi.nyu.edu/  The Arabic word èYK Y« /ςadida/ means ‘numerous’. 2 https://camel.abudhabi.nyu.edu/madar/ 6 Proceedings of NAACL-HLT 2019: Demonstrations, pages 6–11 c Minn"
N19-4002,W16-4801,0,0.0485913,"Missing"
N19-4002,W18-3914,0,0.0141618,"tic identification system working at different levels of representation and exploring different datasets has attracted increasing attention in recent years. For instance, DID has been the goal of a dedicated shared task (Malmasi et al., 2016; Zampieri et al., 2017, 2018), encouraging researchers to submit systems to recognize the dialect of speech transcripts for dialects of four main regions: Egyptian, Gulf, Levantine and North African, and MSA. Several systems implementing a range of traditional supervised learning (Tillmann et al., 2014) and deep learning methods (Belinkov and Glass, 2016; Michon et al., 2018) were proposed. In the literature, a number of studies have been exploring DID using several datasets, ranging from user-generated content (i.e., blogs, social media posts) (Sadat et al., 2014), speech transcripts (Biadsy et al., 2009; Bougrine et al., 2017), and other corpora (Elfardy and Diab, 2012, 2013; Zaidan and Callison-Burch, 2014; Salameh et al., 2018; Dinu et al., 2018; Goldman et al., 2018). Shoufan and Al-Ameri (2015) and Al-Ayyoub et al. (2017) present a survey on NLP and deep learning methods for processing Arabic dialectal data with an overview on Arabic DID of text and speech."
N19-4002,L18-1415,1,0.849481,"displays the results with either a point map or a heat map overlaid on top of a geographical map of the Arab World. 2 3 Related Work 3.1 Arabic Dialect Processing While automatic processing of DA is relatively recent compared to MSA, it has attracted a considerable amount of research in NLP (Shoufan and Al-Ameri, 2015). Most of it focuses on (i) collecting datasets from various sources and at different levels (Zaidan and Callison-Burch, 2011; Khalifa et al., 2016; Abdul-Mageed et al., 2018; Bouamor et al., 2018), (ii) creating processing tools (Habash et al., 2013; Al-Shargi and Rambow, 2015; Obeid et al., 2018) (iii) developing DA to English maArabic and its Dialects Although MSA is the official language across the Arab World, it is not the native language of any speakers of Arabic. Dialectal Arabic (DA), on the other hand, is the daily informal spoken variety. 1 https://adida.abudhabi.nyu.edu/  The Arabic word èYK Y« /ςadida/ means ‘numerous’. 2 https://camel.abudhabi.nyu.edu/madar/ 6 Proceedings of NAACL-HLT 2019: Demonstrations, pages 6–11 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics Region Maghreb Sub-region Morocco Algeria Tunisia Libya Cities"
N19-4002,W14-5904,0,0.23213,"f a dedicated shared task (Malmasi et al., 2016; Zampieri et al., 2017, 2018), encouraging researchers to submit systems to recognize the dialect of speech transcripts for dialects of four main regions: Egyptian, Gulf, Levantine and North African, and MSA. Several systems implementing a range of traditional supervised learning (Tillmann et al., 2014) and deep learning methods (Belinkov and Glass, 2016; Michon et al., 2018) were proposed. In the literature, a number of studies have been exploring DID using several datasets, ranging from user-generated content (i.e., blogs, social media posts) (Sadat et al., 2014), speech transcripts (Biadsy et al., 2009; Bougrine et al., 2017), and other corpora (Elfardy and Diab, 2012, 2013; Zaidan and Callison-Burch, 2014; Salameh et al., 2018; Dinu et al., 2018; Goldman et al., 2018). Shoufan and Al-Ameri (2015) and Al-Ayyoub et al. (2017) present a survey on NLP and deep learning methods for processing Arabic dialectal data with an overview on Arabic DID of text and speech. While most of the proposed approaches targeted regional or country level DID, Salameh et al. (2018) introduced a fine-grained DID system covering the dialects of 25 cities from several countrie"
N19-4002,P13-2001,0,0.0233764,"ings of NAACL-HLT 2019: Demonstrations, pages 6–11 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics Region Maghreb Sub-region Morocco Algeria Tunisia Libya Cities Rabat Algiers Tunis Tripoli Fes Sfax Benghazi Nile Basin Egypt/Sudan Cairo Alexandria Aswan Khartoum Levant Gulf South Levant North Levant Iraq Gulf Jerusalem Beirut Mosul Doha Amman Damascus Baghdad Muscat Salt Aleppo Basra Riyadh Jeddah Yemen Yemen Sana’a Table 1: Different city dialects covered in ADIDA and the regions they belong to. 3.3 chine translation systems (Zbib et al., 2012; Sajjad et al., 2013), (iv) or performing dialect identification (Zaidan and Callison-Burch, 2014; Huang, 2015; Salameh et al., 2018). 3.2 Visualization Map visualizations are used in multiple fields of study including linguistics, socio-linguistics, and political science to display geographical relations of non-geographic data. Geographical visualizations may include point maps to display individual data points, choropleths and Voronoi tessalation maps that cluster data points by region, and heat maps and surface maps that interpolate data over some geographical area. In the general context of visualization of la"
N19-4002,C18-1113,1,0.873594,"ar Habash New York University Abu Dhabi, UAE † Carnegie Mellon University in Qatar, Qatar {oobeid,nizar.habash}@nyu.edu {msalameh,hbouamor}@cmu.edu Abstract DA is nowadays emerging as the primary language of communication – not just spoken, but also written, particularly in social media. Arabic dialects are often classified in terms of geographical regions, such as Levantine Arabic, Gulf Arabic and Egyptian Arabic (Habash, 2010). However, within each of these regional groups, there is significant variation down to the village, town, and city levels. The demo we present is based on the work of Salameh et al. (2018), who utilize the MADAR Project parallel corpus of 25 Arab cities plus MSA (Table 1) (Bouamor et al., 2018).2 Arabic dialects differ in various ways from MSA and from each other. These include phonological, morphological, lexical, and syntactic differences (Haeri, 1991; Holes, 2004; Watson, 2007; Bassiouney, 2009). Despite these differences, distinguishing between Arabic dialects in written form is an arduous task because: (i) dialects use the same writing script and share part of the vocabulary; and (ii) Arabic speakers usually resort to repeated code-switching between their dialect and MSA ("
P11-2069,P05-1074,0,0.0676412,"oses. We show that the tunable TER-PLUS metric from Machine Translation evaluation can achieve good performance on this task and that it can effectively exploit information coming from complementary sources. 1 Introduction The acquisition of subsentential paraphrases has attracted a lot of attention recently (Madnani and Dorr, 2010). Techniques are usually developed for extracting paraphrase candidates from specific types of corpora, including monolingual parallel corpora (Barzilay and McKeown, 2001), monolingual comparable corpora (Del´eger and Zweigenbaum, 2009), bilingual parallel corpora (Bannard and Callison-Burch, 2005), and edit histories of multi-authored text (Max and Wisniewski, 2010). These approaches face two main issues, which correspond to the typical measures of precision, or how appropriate the extracted paraphrases are, and of recall, or how many of the paraphrases present in a given corpus can be found effectively. To start with, both measures are often hard to compute in practice, as 1) the definition of what makes an acceptable paraphrase pair is still a research question, and 2) it is often impractical to extract a complete set of acceptable paraphrases 395 from most resources. Second, as rega"
P11-2069,C08-1013,0,0.0203037,"ties. We are finally also in the process of conducting a careful study of the characteristics of the paraphrase pairs that each technique can extract with high confidence, so that we can improve our hybridation experiments by considering confidence values at the paraphrase level using Machine Learning. This way, we may be able to use an edit rate computation algorithm such as TER-PLUS as a more efficient system combiner for paraphrase extraction methods than what was proposed here. A potential application of this would be an alternative proposal to the paraphrase evaluation metric PARAMETRIC (Callison-Burch et al., 2008), where individual techniques, outputing word alignments or not, could be evaluated from the ability of the informated edit rate technique to use correct equivalence units. 4 Indeed, measuring the precision on the union yields a poor performance of 23.96, but with the highest achievable value of 50.56 for recall. Similarly, the maximum value for precision with a good recall can be obtained by taking the intersection of the results of TER Ppara and G IZA ++, which yields a value of 60.39. 399 Acknowledgments This work was partly funded by a grant from LIMSI. The authors wish to thank the anonym"
P11-2069,candito-etal-2010-statistical,0,0.0892695,"Missing"
P11-2069,J08-4005,0,0.747644,"et al., 2004) often contain unrelated segments that should not be aligned to form a subsentential paraphrase pair. Using bilingual corpora to acquire paraphrases indirectly by pivoting through other languages is faced, in particular, with the issue of phrase polysemy, both in the source and in the pivot languages. It has previously been noted that highly parallel monolingual corpora, typically obtained via multiple translation into the same language, constitute the most appropriate type of corpus for extracting high quality paraphrases, in spite of their rareness (Barzilay and McKeown, 2001; Cohn et al., 2008; Bouamor et al., 2010). We build on this claim here to propose an original approach for the task of subsentential alignment based on the computation of a minimum edit rate between two sentential paraphrases. More precisely, we concentrate on the alignment of atomic paraphrase pairs (Cohn et al., 2008), where the words from both paraphrases are aligned as a whole to the words of the other paraphrase, as opposed to composite paraphrase pairs obtained by joining together adjacent paraphrase pairs or possibly adding unaligned words. Figure 1 provides examples of atomic paraphrase pairs derived fr"
P11-2069,W09-3102,0,0.426347,"Missing"
P11-2069,C04-1051,0,0.0546249,"measures are often hard to compute in practice, as 1) the definition of what makes an acceptable paraphrase pair is still a research question, and 2) it is often impractical to extract a complete set of acceptable paraphrases 395 from most resources. Second, as regards the precision of paraphrase acquisition techniques in particular, it is notable that most works on paraphrase acquisition are not based on direct observation of larger paraphrase pairs. Even monolingual corpora obtained by pairing very closely related texts such as news headlines on the same topic and from the same time frame (Dolan et al., 2004) often contain unrelated segments that should not be aligned to form a subsentential paraphrase pair. Using bilingual corpora to acquire paraphrases indirectly by pivoting through other languages is faced, in particular, with the issue of phrase polysemy, both in the source and in the pivot languages. It has previously been noted that highly parallel monolingual corpora, typically obtained via multiple translation into the same language, constitute the most appropriate type of corpus for extracting high quality paraphrases, in spite of their rareness (Barzilay and McKeown, 2001; Cohn et al., 2"
P11-2069,P08-4006,0,0.0199569,"11.92 18.47 17.10 6.94 21.02 20.28 3.41 18.94 16.44 13.57 19.30 16.35 f1 16.25 9.30 4.21 18.77 4.31 19.26 19.52 16.52 19.14 18.95 11.91 20.92 21.33 6.15 18.47 17.59 18.58 17.96 21.02 Figure 2: Results on the test set on French and English for the individual techniques and TER P hybrid systems. Column headers of the form “→ c” indicate that TER P was tuned on criterion c. figures reveal that the French corpus tends to contain more literal translations, possibly due to the original languages of the sentences, which are closer to the target language than Chinese is to English. We used the YAWAT (Germann, 2008) interactive alignment tool and measure inter-annotator agreement over a subset and found it to be similar to the value reported by Cohn et al. (2008) for English. Results for all individual techniques in the two languages are given on Figure 2. We first note that all techniques fared better on the French corpus than on the English corpus. This can certainly be explained by the fact that the former results from more literal translations, which are consequently easier to word-align. TERMT (i.e. TER tuned for Machine Translation evaluation) performs significantly worse on all metrics for both la"
P11-2069,P99-1044,0,0.0302307,"models it is necessary to use enough training material including minimal redundancy of words. To this end, we will be using monolingual corpora made up of multiply-translated sentences, allowing us to provide GIZA++ with all possible sentence pairs to improve the quality of its word alignments (note that following common practice we used symetrized alignments from the alignments in both directions). This constitutes an advantage for this technique that the following techniques working on each sentence pair independently do not have. Symbolic expression of linguistic variation The FASTR tool (Jacquemin, 1999) was designed to spot term variants in large corpora. Variants are described through metarules expressing how the morphosyntactic structure of a term variant can be derived from a given term by means of regular expressions on word categories. Paradigmatic variation can also be expressed by defining constraints between words to force them to belong to the same morphological or semantic family, both constraints relying on preexisting repertoires available for English and French. To compute candidate paraphrase pairs using FASTR, we first consider all the phrases from the first sentence and searc"
P11-2069,J10-3003,0,0.064709,", we present a novel way of tackling the monolingual alignment problem on pairs of sentential paraphrases by means of edit rate computation. In order to inform the edit rate, information in the form of subsentential paraphrases is provided by a range of techniques built for different purposes. We show that the tunable TER-PLUS metric from Machine Translation evaluation can achieve good performance on this task and that it can effectively exploit information coming from complementary sources. 1 Introduction The acquisition of subsentential paraphrases has attracted a lot of attention recently (Madnani and Dorr, 2010). Techniques are usually developed for extracting paraphrase candidates from specific types of corpora, including monolingual parallel corpora (Barzilay and McKeown, 2001), monolingual comparable corpora (Del´eger and Zweigenbaum, 2009), bilingual parallel corpora (Bannard and Callison-Burch, 2005), and edit histories of multi-authored text (Max and Wisniewski, 2010). These approaches face two main issues, which correspond to the typical measures of precision, or how appropriate the extracted paraphrases are, and of recall, or how many of the paraphrases present in a given corpus can be found"
P11-2069,max-wisniewski-2010-mining,1,0.799494,"tion can achieve good performance on this task and that it can effectively exploit information coming from complementary sources. 1 Introduction The acquisition of subsentential paraphrases has attracted a lot of attention recently (Madnani and Dorr, 2010). Techniques are usually developed for extracting paraphrase candidates from specific types of corpora, including monolingual parallel corpora (Barzilay and McKeown, 2001), monolingual comparable corpora (Del´eger and Zweigenbaum, 2009), bilingual parallel corpora (Bannard and Callison-Burch, 2005), and edit histories of multi-authored text (Max and Wisniewski, 2010). These approaches face two main issues, which correspond to the typical measures of precision, or how appropriate the extracted paraphrases are, and of recall, or how many of the paraphrases present in a given corpus can be found effectively. To start with, both measures are often hard to compute in practice, as 1) the definition of what makes an acceptable paraphrase pair is still a research question, and 2) it is often impractical to extract a complete set of acceptable paraphrases 395 from most resources. Second, as regards the precision of paraphrase acquisition techniques in particular,"
P11-2069,J04-4002,0,0.0177494,"acquisition, which we will only briefly introduce (see (Bouamor et al., 2010) for more details). As explained previously, we want to evaluate whether and how their candidate paraphrase pairs can be used to improve paraphrase acquisition on sentential paraphrases using TER- PLUS. We selected these three techniques for the complementarity of types of information that they use: statistical word alignment without a priori linguistic knowledge, symbolic expression of linguistic variation exploiting a priori linguistic knowledge, and syntactic similarity. Statistical Word Alignment The GIZA++ tool (Och and Ney, 2004) computes statistical word alignment models of increasing complexity from parallel corpora. While originally developped in the bilingual context of Machine Translation, nothing prevents building such models on monolingual corpora. However, in order to build reliable models it is necessary to use enough training material including minimal redundancy of words. To this end, we will be using monolingual corpora made up of multiply-translated sentences, allowing us to provide GIZA++ with all possible sentence pairs to improve the quality of its word alignments (note that following common practice w"
P11-2069,N03-1024,0,0.520245,"Missing"
P11-2069,N07-1051,0,0.0463616,"Missing"
P11-2069,P08-1089,0,0.0201834,"t at the subsentential level from sentential paraphrases and the possibility of informing this search with paraphrase candidates coming from other techniques. Our experiments have shown that in some circumstances some techniques have a good complementarity and manage to improve results significantly. We are currently studying hard-to-align subsentential paraphrases from the type of corpora we used in order to get a better understanding of the types of knowledge required to improve automatic acquisition of these units. Our future work also includes the acquisition of paraphrase patterns (e.g. (Zhao et al., 2008)) to generalize the acquired equivalence units to more contexts, which could be both used in applications and to attempt improving further paraphrase acquisition techniques. Integrating the use of patterns within an edit rate computation technique will however raise new difficulties. We are finally also in the process of conducting a careful study of the characteristics of the paraphrase pairs that each technique can extract with high confidence, so that we can improve our hybridation experiments by considering confidence values at the paraphrase level using Machine Learning. This way, we may"
P11-2069,P01-1008,0,\N,Missing
S14-2028,S13-2053,0,0.0942666,"cessors, which are all the other features. Then, we rank features according to the f-score and we only process the best feature and prune the rest. We pass all the current pruned features as successors to the next level of the tree. The process iterates until all partial solutions in the tree are processed or terminated. The selected features are the following: Task A X X X X X X Task B X X X X X Table 1: Feature summary for each task. 4 Sentiment lexicons : we used the Bing Liu Lexicon (Hu and Liu, 2004), the MPQA Subjectivity Lexicon (Wilson et al., 2005), and NRC Hashtag Sentiment Lexicon (Mohammad et al., 2013). We count the number of words in each class, resulting in three features: (a) positive words count, (b) negative words count and (c) neutral words count. Modeling Kernel Functions Initially we experimented with both logistic regression and the Support Vector Machine (SVM) (Fan et al., 2008), using the Stochastic Gradient Descent (SGD) algorithm for parameter optimization. In our development experiments, SVM outperformed and became our single classifier. We used the LIBSVM package (Chang and Lin, 2011) to train and test our classifier. Negative presence: presence of negative words in a term/tw"
S14-2028,S13-2052,0,0.0179901,"becoming a very popular communication tool. An analysis of this platform reveals a large amount of community messages expressing their opinions and sentiments on different topics and aspects of life. This makes Twitter a valuable source of subjective and opinionated text that could be used in several NLP research works on sentiment analysis. Many approaches for detecting subjectivity and determining polarity of opinions in Twitter have been proposed (Pang and Lee, 2008; Davidov et al., 2010; Pak and Paroubek, 2010; Tang et al., 2014). For instance, the Twitter sentiment analysis shared task (Nakov et al., 2013) is an interesting testbed to develop and evaluate sentiment analysis systems on social media text. Participants are asked to implement 3 Feature Extraction Out of a wide variety of features, we selected the most effective features using the best-first branch and bound method (Neapolitan, 2014), a search tree technique for solving optimization problems. We used this technique to determine which punctuation marks to keep in the preprocessing step and This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organis"
S14-2028,baccianella-etal-2010-sentiwordnet,0,0.0166115,"d for best F-score on the development set. In order to avoid the model overfitting the data, we select the optimal parameter value only if there are smooth gaps between the near neighbors of the corresponded F-score. Otherwise, the search will continue to the second optimal value. Textual tokens: the target term/tweet is segmented into tokens based on space. Token identity features are created and assigned the value of 1. Overall polarity score: we determine the polarity scores of words in a target term/tweet using the Sentiment140 Lexicon (Mohammad et al., 2013) and the SentiWordNet lexicon (Baccianella et al., 2010). The overall score is computed by adding up all word scores. In machine learning, the difference between the number of training samples, m, and the number of features, n, is crucial in the selection process of SVM kernel functions. The Gaussian kernel is suggested when m is slightly larger than n. Otherwise, the linear kernel is recommended. In TaskB, the n : m ratio was 1 : 3 indicating a large difference between the two numbers. Whereas in Task-A, a ratio of 5 : 2 indicated a small difference between the two numbers. We selected the theoretical types, after conducting an experimental verifi"
S14-2028,S10-1097,0,0.020062,"nd share their opinion through social media. For instance, microblogging websites such as Twitter1 are becoming a very popular communication tool. An analysis of this platform reveals a large amount of community messages expressing their opinions and sentiments on different topics and aspects of life. This makes Twitter a valuable source of subjective and opinionated text that could be used in several NLP research works on sentiment analysis. Many approaches for detecting subjectivity and determining polarity of opinions in Twitter have been proposed (Pang and Lee, 2008; Davidov et al., 2010; Pak and Paroubek, 2010; Tang et al., 2014). For instance, the Twitter sentiment analysis shared task (Nakov et al., 2013) is an interesting testbed to develop and evaluate sentiment analysis systems on social media text. Participants are asked to implement 3 Feature Extraction Out of a wide variety of features, we selected the most effective features using the best-first branch and bound method (Neapolitan, 2014), a search tree technique for solving optimization problems. We used this technique to determine which punctuation marks to keep in the preprocessing step and This work is licensed under a Creative Commons"
S14-2028,S14-2009,0,0.025601,"Missing"
S14-2028,C10-2028,0,0.0422081,"increasingly express and share their opinion through social media. For instance, microblogging websites such as Twitter1 are becoming a very popular communication tool. An analysis of this platform reveals a large amount of community messages expressing their opinions and sentiments on different topics and aspects of life. This makes Twitter a valuable source of subjective and opinionated text that could be used in several NLP research works on sentiment analysis. Many approaches for detecting subjectivity and determining polarity of opinions in Twitter have been proposed (Pang and Lee, 2008; Davidov et al., 2010; Pak and Paroubek, 2010; Tang et al., 2014). For instance, the Twitter sentiment analysis shared task (Nakov et al., 2013) is an interesting testbed to develop and evaluate sentiment analysis systems on social media text. Participants are asked to implement 3 Feature Extraction Out of a wide variety of features, we selected the most effective features using the best-first branch and bound method (Neapolitan, 2014), a search tree technique for solving optimization problems. We used this technique to determine which punctuation marks to keep in the preprocessing step and This work is licensed u"
S14-2028,P14-1146,0,0.0190631,"hrough social media. For instance, microblogging websites such as Twitter1 are becoming a very popular communication tool. An analysis of this platform reveals a large amount of community messages expressing their opinions and sentiments on different topics and aspects of life. This makes Twitter a valuable source of subjective and opinionated text that could be used in several NLP research works on sentiment analysis. Many approaches for detecting subjectivity and determining polarity of opinions in Twitter have been proposed (Pang and Lee, 2008; Davidov et al., 2010; Pak and Paroubek, 2010; Tang et al., 2014). For instance, the Twitter sentiment analysis shared task (Nakov et al., 2013) is an interesting testbed to develop and evaluate sentiment analysis systems on social media text. Participants are asked to implement 3 Feature Extraction Out of a wide variety of features, we selected the most effective features using the best-first branch and bound method (Neapolitan, 2014), a search tree technique for solving optimization problems. We used this technique to determine which punctuation marks to keep in the preprocessing step and This work is licensed under a Creative Commons Attribution 4.0 Inte"
S14-2028,H05-1044,0,0.0425685,"eps: we process the current feature by generating its successors, which are all the other features. Then, we rank features according to the f-score and we only process the best feature and prune the rest. We pass all the current pruned features as successors to the next level of the tree. The process iterates until all partial solutions in the tree are processed or terminated. The selected features are the following: Task A X X X X X X Task B X X X X X Table 1: Feature summary for each task. 4 Sentiment lexicons : we used the Bing Liu Lexicon (Hu and Liu, 2004), the MPQA Subjectivity Lexicon (Wilson et al., 2005), and NRC Hashtag Sentiment Lexicon (Mohammad et al., 2013). We count the number of words in each class, resulting in three features: (a) positive words count, (b) negative words count and (c) neutral words count. Modeling Kernel Functions Initially we experimented with both logistic regression and the Support Vector Machine (SVM) (Fan et al., 2008), using the Stochastic Gradient Descent (SGD) algorithm for parameter optimization. In our development experiments, SVM outperformed and became our single classifier. We used the LIBSVM package (Chang and Lin, 2011) to train and test our classifier."
S14-2029,baccianella-etal-2010-sentiwordnet,0,0.0486368,"tures described in the previous section. We use a linear kernel and tune its parameter C separately for the two tasks. Task-A system was bound tight to the development set with C=0.18 whereas in Task-B the system was given freedom by setting C=0.55. These values were optimized during the development using a bruteforce mechanism. • Hybrid Lexicon: We combine the Sentiment140 lexicon (Mohammad et al., 2013) with the Bing Liu’s bag of positive and negative words (Hu and Liu, 2004) to create a dictionary in which each token is assigned a sentiment. • Token weight: we use the SentiWordNet lexicon (Baccianella et al., 2010) to define this feature. SentiWordNet contains positive, negative and objective scores between 0 and 1 for all senses in WordNet. Based on this sense level annotation, we first map each token to its weight in this lexicon and then the sum of all these weights was used as the tweet weight. LiveJournal 2014 SMS 2013 Twitter 2013 Twitter 2014 Sarcasm Weighted average Task-A 83.89 88.08 89.85 83.45 78.07 87.11 Task-B 65.63 62.95 65.11 65.53 40.52 64.52 Table 3: F1 measures and final results of the system for Task-A and Task-B for all the data sets including the weighted average of the sets. Furthe"
S14-2029,S13-2053,0,0.038578,"mber of words mapped to ”positive” from the MPQA Subjectivity lexicon (Wilson et al., 2005). Classifier We use a Support Vector Machine (SVM) classifier (Chang and Lin, 2011) to which we provide the rich set of features described in the previous section. We use a linear kernel and tune its parameter C separately for the two tasks. Task-A system was bound tight to the development set with C=0.18 whereas in Task-B the system was given freedom by setting C=0.55. These values were optimized during the development using a bruteforce mechanism. • Hybrid Lexicon: We combine the Sentiment140 lexicon (Mohammad et al., 2013) with the Bing Liu’s bag of positive and negative words (Hu and Liu, 2004) to create a dictionary in which each token is assigned a sentiment. • Token weight: we use the SentiWordNet lexicon (Baccianella et al., 2010) to define this feature. SentiWordNet contains positive, negative and objective scores between 0 and 1 for all senses in WordNet. Based on this sense level annotation, we first map each token to its weight in this lexicon and then the sum of all these weights was used as the tweet weight. LiveJournal 2014 SMS 2013 Twitter 2013 Twitter 2014 Sarcasm Weighted average Task-A 83.89 88."
S14-2029,S13-2052,0,0.13605,"Missing"
S14-2029,N13-1039,0,0.172815,"Missing"
S14-2029,pak-paroubek-2010-twitter,0,0.14681,"Missing"
S14-2029,S14-2009,0,0.0922738,"Missing"
S14-2029,P14-1146,0,0.042767,"Missing"
S14-2029,H05-1044,0,0.0789758,"Missing"
salama-etal-2014-youdacc,N12-1006,0,\N,Missing
salama-etal-2014-youdacc,P11-2007,0,\N,Missing
salama-etal-2014-youdacc,P13-2001,0,\N,Missing
salama-etal-2014-youdacc,P11-1122,0,\N,Missing
salama-etal-2014-youdacc,N13-1036,0,\N,Missing
salama-etal-2014-youdacc,N13-1044,0,\N,Missing
salama-etal-2014-youdacc,C12-1114,0,\N,Missing
W11-1602,P05-1074,0,0.562204,"ote that using the web may not always be appropriate, or that at least it should be used in a different way than what we propose in this article, in particular in cases where the desired properties of the rewritten text are better described in controlled corpora. 11 The use of automatic targeted paraphrasing as an authoring aid has been illustrated by the work of Max and Zock (2008), in which writers are presented with potential paraphrases of sub-sentential fragments that they wish to reword. The automatic paraphrasing technique used is a contextual variant of bilingual translation pivoting (Bannard and Callison-Burch, 2005). It has also been proposed to externalize various text editing tasks, including proofreading, by having crowdsourcing functions on text directly from word processors (Bernstein et al., 2010). Text improvements may also be more specifically targeted for automatic applications. In the work by Resnik et al. (2010), rephrasings for specific phrases are acquired through crowdsourcing. Difficult-to-translate phrases in the source text are first identified, and monolingual contributors are asked to provide rephrasings in context. Collected rephrasings can then be used as input for a Machine Translat"
W11-1602,P01-1008,0,0.06346,"generation The acquisition of paraphrases, and in particular of sub-sentential paraphrases and paraphrase patterns, has attracted a lot of works with the advent of data-intensive Natural Language Processing (Madnani and Dorr, 2010). The techniques proposed have a strong relationship to the type of text corpus used 3 This verse from Apollinaire’s Nuit Rh´enane [which seems almost without rhythmic structure → whose cesura is as if hidden]. . . 12 for acquisition, mainly: • pairs of sentential paraphrases (monolingual parallel corpora) allow for a good precision but evidently a low recall (e.g. (Barzilay and McKeown, 2001; Pang et al., 2003; Cohn et al., 2008; Bouamor et al., 2011)) • pairs of bilingual sentences (bilingual parallel corpora) allow for a comparatively better recall (e.g. (Bannard and Callison-Burch, 2005; Kok and Brockett, 2010)) • pairs of related sentences (monolingual comparable corpora) allow for even higher recall but possibly lower precision (e.g. (Barzilay and Lee, 2003; Li et al., 2005; Bhagat and Ravichandran, 2008; Del´eger and Zweigenbaum, 2009) Although the precision of such techniques can in some cases be formulated with regards to a predefined reference set (Cohn et al., 2008), it"
W11-1602,P08-1077,0,0.0155111,"is as if hidden]. . . 12 for acquisition, mainly: • pairs of sentential paraphrases (monolingual parallel corpora) allow for a good precision but evidently a low recall (e.g. (Barzilay and McKeown, 2001; Pang et al., 2003; Cohn et al., 2008; Bouamor et al., 2011)) • pairs of bilingual sentences (bilingual parallel corpora) allow for a comparatively better recall (e.g. (Bannard and Callison-Burch, 2005; Kok and Brockett, 2010)) • pairs of related sentences (monolingual comparable corpora) allow for even higher recall but possibly lower precision (e.g. (Barzilay and Lee, 2003; Li et al., 2005; Bhagat and Ravichandran, 2008; Del´eger and Zweigenbaum, 2009) Although the precision of such techniques can in some cases be formulated with regards to a predefined reference set (Cohn et al., 2008), it should more generally be assessed in the specific context of some use of the paraphrase pair. This refers to the problem of substituability in context (e.g. (Connor and Roth, 2007; Zhao et al., 2007)), which is a well studied field at the lexical level and the object of evaluation campains (McCarthy and Navigli, 2009). Contextual phrase substitution poses the additional challenge that phrases are rarer than words, so that"
W11-1602,P11-2069,1,0.819058,"b-sentential paraphrases and paraphrase patterns, has attracted a lot of works with the advent of data-intensive Natural Language Processing (Madnani and Dorr, 2010). The techniques proposed have a strong relationship to the type of text corpus used 3 This verse from Apollinaire’s Nuit Rh´enane [which seems almost without rhythmic structure → whose cesura is as if hidden]. . . 12 for acquisition, mainly: • pairs of sentential paraphrases (monolingual parallel corpora) allow for a good precision but evidently a low recall (e.g. (Barzilay and McKeown, 2001; Pang et al., 2003; Cohn et al., 2008; Bouamor et al., 2011)) • pairs of bilingual sentences (bilingual parallel corpora) allow for a comparatively better recall (e.g. (Bannard and Callison-Burch, 2005; Kok and Brockett, 2010)) • pairs of related sentences (monolingual comparable corpora) allow for even higher recall but possibly lower precision (e.g. (Barzilay and Lee, 2003; Li et al., 2005; Bhagat and Ravichandran, 2008; Del´eger and Zweigenbaum, 2009) Although the precision of such techniques can in some cases be formulated with regards to a predefined reference set (Cohn et al., 2008), it should more generally be assessed in the specific context of"
W11-1602,I05-5001,0,0.656547,"sed corpus would contain enough information for appropriate modeling of the substituability in context decision. It is therefore tempting to consider using the Web as the largest available information source, in spite of several of its known limitations, including that data can be of varying quality. It has however been shown that a large range of NLP applications can be improved by exploiting n-gram counts from the Web (using Web document counts as a proxy) (Lapata and Keller, 2005). Paraphrase identification has been addressed previously, both using features computed from an offline corpus (Brockett and Dolan, 2005) and features computed from Web queries (Zhao et al., 2007). However, to our knowledge previous work exploiting information from the Web was limited to the identification of lexical paraphrases. Although the probability of finding phrase occurrences significantly increases by considering the Web, some phrases are still very rare or not present in search engine indexes. As in (Brockett and Dolan, 2005), we tackle our paraphrase identification task as one of monolingual classification. More precisely, considering an original phrase p within the context of sentence s, we seek to determine whether"
W11-1602,D08-1021,0,0.0262373,"l., 2008), it should more generally be assessed in the specific context of some use of the paraphrase pair. This refers to the problem of substituability in context (e.g. (Connor and Roth, 2007; Zhao et al., 2007)), which is a well studied field at the lexical level and the object of evaluation campains (McCarthy and Navigli, 2009). Contextual phrase substitution poses the additional challenge that phrases are rarer than words, so that building contextual and grammatical models to ensure that the generated rephrasings are both semantically compatible and grammatical is more complicated (e.g. (Callison-Burch, 2008)). The present work does not aim to present any original technique for paraphrase acquisition, but rather focusses on the task of sub-sentential paraphrase validation in context. We thus resort to some existing repertoire of phrasal paraphrase pairs. As explained in section 2, we use the W I C O PAC O corpus as a source of sub-sentential paraphrases: the phrase after rewriting can thus be used as a potential paraphrase in context.4 To obtain other candidates of various quality, we used two knowledge sources. The first uses automatic pivot translation (Bannard and Callison-Burch, 2005), where a"
W11-1602,candito-etal-2010-statistical,0,0.0325447,"Missing"
W11-1602,C96-2183,0,0.014623,"o-text realization problem. However, such needs apply sometimes to cases where a new text should be derived from some existing texts, an instance of text-to-text generation. The general idea is not anymore to produce a text from data, but to transform a text so as to ensure that it has desirable properties appropriate for some intended application (Zhao et al., 2009). For example, one may want a text to be shorter (Cohn and Lapata, 2008), tailored to some reader profile (Zhu et al., 2010), compliant with some specific norms (Max, 2004), or more adapted for subsequent machine processing tasks (Chandrasekar et al., 1996). The generation process must produce a text having a meaning which is compatible with the definition of the task at hand (e.g. strict paraphrasing for document normalization, relaxed paraGabriel Illouz LIMSI-CNRS Univ. Paris Sud gabrieli@limsi.fr Anne Vilnat LIMSI-CNRS Univ. Paris Sud anne@limsi.fr phrasing for text simplification), while ensuring that it remains grammatically correct. Its complexity, compared with concept-to-text generation, mostly stems from the fact that the semantic relationship between the original text and the new one is more difficult to control, as the mapping from on"
W11-1602,C08-1018,0,0.0219686,"history of Wikipedia. 1 Introduction There are many instances where it is reasonable to expect machines to produce text automatically. Traditionally, this was tackled as a concept-to-text realization problem. However, such needs apply sometimes to cases where a new text should be derived from some existing texts, an instance of text-to-text generation. The general idea is not anymore to produce a text from data, but to transform a text so as to ensure that it has desirable properties appropriate for some intended application (Zhao et al., 2009). For example, one may want a text to be shorter (Cohn and Lapata, 2008), tailored to some reader profile (Zhu et al., 2010), compliant with some specific norms (Max, 2004), or more adapted for subsequent machine processing tasks (Chandrasekar et al., 1996). The generation process must produce a text having a meaning which is compatible with the definition of the task at hand (e.g. strict paraphrasing for document normalization, relaxed paraGabriel Illouz LIMSI-CNRS Univ. Paris Sud gabrieli@limsi.fr Anne Vilnat LIMSI-CNRS Univ. Paris Sud anne@limsi.fr phrasing for text simplification), while ensuring that it remains grammatically correct. Its complexity, compared"
W11-1602,J08-4005,0,0.0146725,"in particular of sub-sentential paraphrases and paraphrase patterns, has attracted a lot of works with the advent of data-intensive Natural Language Processing (Madnani and Dorr, 2010). The techniques proposed have a strong relationship to the type of text corpus used 3 This verse from Apollinaire’s Nuit Rh´enane [which seems almost without rhythmic structure → whose cesura is as if hidden]. . . 12 for acquisition, mainly: • pairs of sentential paraphrases (monolingual parallel corpora) allow for a good precision but evidently a low recall (e.g. (Barzilay and McKeown, 2001; Pang et al., 2003; Cohn et al., 2008; Bouamor et al., 2011)) • pairs of bilingual sentences (bilingual parallel corpora) allow for a comparatively better recall (e.g. (Bannard and Callison-Burch, 2005; Kok and Brockett, 2010)) • pairs of related sentences (monolingual comparable corpora) allow for even higher recall but possibly lower precision (e.g. (Barzilay and Lee, 2003; Li et al., 2005; Bhagat and Ravichandran, 2008; Del´eger and Zweigenbaum, 2009) Although the precision of such techniques can in some cases be formulated with regards to a predefined reference set (Cohn et al., 2008), it should more generally be assessed in"
W11-1602,W09-3102,0,0.0384282,"Missing"
W11-1602,N10-1017,0,0.0119854,"). The techniques proposed have a strong relationship to the type of text corpus used 3 This verse from Apollinaire’s Nuit Rh´enane [which seems almost without rhythmic structure → whose cesura is as if hidden]. . . 12 for acquisition, mainly: • pairs of sentential paraphrases (monolingual parallel corpora) allow for a good precision but evidently a low recall (e.g. (Barzilay and McKeown, 2001; Pang et al., 2003; Cohn et al., 2008; Bouamor et al., 2011)) • pairs of bilingual sentences (bilingual parallel corpora) allow for a comparatively better recall (e.g. (Bannard and Callison-Burch, 2005; Kok and Brockett, 2010)) • pairs of related sentences (monolingual comparable corpora) allow for even higher recall but possibly lower precision (e.g. (Barzilay and Lee, 2003; Li et al., 2005; Bhagat and Ravichandran, 2008; Del´eger and Zweigenbaum, 2009) Although the precision of such techniques can in some cases be formulated with regards to a predefined reference set (Cohn et al., 2008), it should more generally be assessed in the specific context of some use of the paraphrase pair. This refers to the problem of substituability in context (e.g. (Connor and Roth, 2007; Zhao et al., 2007)), which is a well studied"
W11-1602,I05-5007,0,0.0221954,"re → whose cesura is as if hidden]. . . 12 for acquisition, mainly: • pairs of sentential paraphrases (monolingual parallel corpora) allow for a good precision but evidently a low recall (e.g. (Barzilay and McKeown, 2001; Pang et al., 2003; Cohn et al., 2008; Bouamor et al., 2011)) • pairs of bilingual sentences (bilingual parallel corpora) allow for a comparatively better recall (e.g. (Bannard and Callison-Burch, 2005; Kok and Brockett, 2010)) • pairs of related sentences (monolingual comparable corpora) allow for even higher recall but possibly lower precision (e.g. (Barzilay and Lee, 2003; Li et al., 2005; Bhagat and Ravichandran, 2008; Del´eger and Zweigenbaum, 2009) Although the precision of such techniques can in some cases be formulated with regards to a predefined reference set (Cohn et al., 2008), it should more generally be assessed in the specific context of some use of the paraphrase pair. This refers to the problem of substituability in context (e.g. (Connor and Roth, 2007; Zhao et al., 2007)), which is a well studied field at the lexical level and the object of evaluation campains (McCarthy and Navigli, 2009). Contextual phrase substitution poses the additional challenge that phrase"
W11-1602,J10-3003,0,0.0705506,"iel Illouz LIMSI-CNRS Univ. Paris Sud gabrieli@limsi.fr Anne Vilnat LIMSI-CNRS Univ. Paris Sud anne@limsi.fr phrasing for text simplification), while ensuring that it remains grammatically correct. Its complexity, compared with concept-to-text generation, mostly stems from the fact that the semantic relationship between the original text and the new one is more difficult to control, as the mapping from one text to another is very dependent on the rewriting context. The wide variety of techniques for acquiring phrasal paraphrases, which can subsequently be used by text paraphrasing techniques (Madnani and Dorr, 2010), the inherent polysemy of such linguistic units and the pragmatic constraints on their uses make it impossible to ensure that potential paraphrase pairs will be substitutable in any context, an observation which was already made at a lexical level (Zhao et al., 2007). Hence, automatic contextual validation of candidate rewritings is a fundamental issue for text paraphrasing with phrasal units. In this article, we tackle the problem of what we call targeted paraphrasing, defined as the rewriting of a subpart of a sentence, as in e.g. (Resnik et al., 2010) where it is applied to making parts of"
W11-1602,2008.amta-papers.13,0,0.0182599,"original wording. The task of rewriting complete sentences has also been addressed in various works (e.g. (Barzilay and Lee, 2003; Quirk et al., 2004; Zhao et al., 2010)). It poses, however, numerous other challenges, in particular regarding how it could be correctly evaluated. Human judgments of whole sentence transformations are complex and intra- and inter-judge coherence is difficult to attain with hypotheses of comparable quality. Using sentential paraphrases to support a given task (e.g. providing alternative reference translations for optimizing Statistical Machine Translation systems (Madnani et al., 2008)) 2 It is to be noted that, in the scenario presented in (Resnik et al., 2010), monolingual contributors cannot predict how useful their rewritings will be to the underlying Machine Translation engine used. can be seen as a proxy for extrinsic evaluation of the quality of paraphrases, but it is not clear from published results that improvements on the task are clearly correlated with the quality of the produced paraphrases. Lastly, automatic metrics have been proposed for evaluating the grammaticality of sentences (e.g. (Mutton et al., 2007)). Automatic evaluation of sentential paraphrases has"
W11-1602,max-wisniewski-2010-mining,1,0.840188,"metrics have been proposed for evaluating the grammaticality of sentences (e.g. (Mutton et al., 2007)). Automatic evaluation of sentential paraphrases has not produced any consensual results so far, as they do not integrate task-specific considerations and can be strongly biased towards some paraphrasing techniques. In this work, we tackle the comparatively more modest task of sub-sentential paraphrasing applied to text revision. In order to use an unbiased task, we use a corpus of naturally-occurring rewritings from an authoring memory of Wikipedia articles. We use the W I C O PAC O corpus (Max and Wisniewski, 2010), a collection of local rephrasings from the edit history of Wikipedia which contains instances of lexical, syntactical and semantic rephrasings (Dutrey et al., 2011), the latter type being illustrated by the following example: Ce vers de Nuit rh´enane d’Apollinaire [qui paraˆıt presque sans structure rythmique → dont la c´esure est comme masqu´ee]. . . 3 The appropriateness of this corpus for our work is twofold: first, the fact that it contains naturallyoccurring rewritings provides us with an interesting source of text spans in context which have been rewritten. Moreover, for those instance"
W11-1602,W08-1911,1,0.937786,"et al., 2011). This study also reports that there is an important variety of rephrasing phenomena, as illustrated by the difficulty of reaching a good identification coverage using a rule-based term variant identification engine. 1 Note that using the web may not always be appropriate, or that at least it should be used in a different way than what we propose in this article, in particular in cases where the desired properties of the rewritten text are better described in controlled corpora. 11 The use of automatic targeted paraphrasing as an authoring aid has been illustrated by the work of Max and Zock (2008), in which writers are presented with potential paraphrases of sub-sentential fragments that they wish to reword. The automatic paraphrasing technique used is a contextual variant of bilingual translation pivoting (Bannard and Callison-Burch, 2005). It has also been proposed to externalize various text editing tasks, including proofreading, by having crowdsourcing functions on text directly from word processors (Bernstein et al., 2010). Text improvements may also be more specifically targeted for automatic applications. In the work by Resnik et al. (2010), rephrasings for specific phrases are"
W11-1602,C04-1166,1,0.795727,"ce text automatically. Traditionally, this was tackled as a concept-to-text realization problem. However, such needs apply sometimes to cases where a new text should be derived from some existing texts, an instance of text-to-text generation. The general idea is not anymore to produce a text from data, but to transform a text so as to ensure that it has desirable properties appropriate for some intended application (Zhao et al., 2009). For example, one may want a text to be shorter (Cohn and Lapata, 2008), tailored to some reader profile (Zhu et al., 2010), compliant with some specific norms (Max, 2004), or more adapted for subsequent machine processing tasks (Chandrasekar et al., 1996). The generation process must produce a text having a meaning which is compatible with the definition of the task at hand (e.g. strict paraphrasing for document normalization, relaxed paraGabriel Illouz LIMSI-CNRS Univ. Paris Sud gabrieli@limsi.fr Anne Vilnat LIMSI-CNRS Univ. Paris Sud anne@limsi.fr phrasing for text simplification), while ensuring that it remains grammatically correct. Its complexity, compared with concept-to-text generation, mostly stems from the fact that the semantic relationship between t"
W11-1602,P07-1044,0,0.0127222,"optimizing Statistical Machine Translation systems (Madnani et al., 2008)) 2 It is to be noted that, in the scenario presented in (Resnik et al., 2010), monolingual contributors cannot predict how useful their rewritings will be to the underlying Machine Translation engine used. can be seen as a proxy for extrinsic evaluation of the quality of paraphrases, but it is not clear from published results that improvements on the task are clearly correlated with the quality of the produced paraphrases. Lastly, automatic metrics have been proposed for evaluating the grammaticality of sentences (e.g. (Mutton et al., 2007)). Automatic evaluation of sentential paraphrases has not produced any consensual results so far, as they do not integrate task-specific considerations and can be strongly biased towards some paraphrasing techniques. In this work, we tackle the comparatively more modest task of sub-sentential paraphrasing applied to text revision. In order to use an unbiased task, we use a corpus of naturally-occurring rewritings from an authoring memory of Wikipedia articles. We use the W I C O PAC O corpus (Max and Wisniewski, 2010), a collection of local rephrasings from the edit history of Wikipedia which"
W11-1602,P10-2001,0,0.0266855,"does not take the original wording into account. We therefore used a ratio of the language model score of the paraphrased sentence with the language model score of the original 6 http://research.microsoft.com/en-us/ collaboration/focus/cs/web-ngram.aspx 7 Note that in order to query on French text, we had to remove all diacritics for the service to behave correctly, independently of encodings: careful examination of ranked hypotheses showed that this trick allowed us to obtain results coherent with expectations. 14 sentence, after normalization by sentence length of the language model scores (Onishi et al., 2010): hLM ratio = LM (para) lm(para)1/length(para) = LM (orig) lm(orig)1/length(orig) (2) Contextless thematic model scores Cooccurring words are used in distributional semantics to account for common meanings of words. We build vector representations of cooccurrences for both the original phrase p and its paraphrase p0 . Our contextless thematic model is built in the following fashion: we query a search engine to retrieve the top N document snippets for phrase p. We then count frequencies for all content words in these snippets, and keep the set W of words appearing more than a fraction of N . We"
W11-1602,N03-1024,0,0.123923,"f paraphrases, and in particular of sub-sentential paraphrases and paraphrase patterns, has attracted a lot of works with the advent of data-intensive Natural Language Processing (Madnani and Dorr, 2010). The techniques proposed have a strong relationship to the type of text corpus used 3 This verse from Apollinaire’s Nuit Rh´enane [which seems almost without rhythmic structure → whose cesura is as if hidden]. . . 12 for acquisition, mainly: • pairs of sentential paraphrases (monolingual parallel corpora) allow for a good precision but evidently a low recall (e.g. (Barzilay and McKeown, 2001; Pang et al., 2003; Cohn et al., 2008; Bouamor et al., 2011)) • pairs of bilingual sentences (bilingual parallel corpora) allow for a comparatively better recall (e.g. (Bannard and Callison-Burch, 2005; Kok and Brockett, 2010)) • pairs of related sentences (monolingual comparable corpora) allow for even higher recall but possibly lower precision (e.g. (Barzilay and Lee, 2003; Li et al., 2005; Bhagat and Ravichandran, 2008; Del´eger and Zweigenbaum, 2009) Although the precision of such techniques can in some cases be formulated with regards to a predefined reference set (Cohn et al., 2008), it should more genera"
W11-1602,N07-1051,0,0.00991518,"and right boundary of the sub-sentential paraphrase is higher than 10. Syntactic dependency baseline When rewriting a subpart of a sentence, the fact that syntactic dependencies between the rewritten phrase and its context are the same than those of the original phrase and the same context can provide some information about the grammatical and semantic substituability of the two phrases (Zhao et al., 2007; Max and Zock, 2008). We thus build syntactic dependencies for both the original and rewritten sentence, using the French version (Candito et al., 2010) of the Berkeley probabilistic parser (Petrov and Klein, 2007), and consider the subset of dependencies for the two sentences that exist between a word inside the phrase under focus and a word outside it (Deporig and Deppara ). Our C ONT D EP baseline considers a sentence as a paraphrase iff Deppara = Deporig . 5.3 Evaluation results We used the models described in Section 4 to build a SVM classifier using the LIBSVM package (Chang and Lin, 2001). Accuracy results are reported on Figure 5. P OSSIBLE S URE S URER W EB LM 62.79 68.37 56.79 B OUND LM 54.88 36.27 51.41 C ONT D EP 48.53 51.90 42.69 C LASSIFIER 57.67 70.69 62.85 Figure 5: Accuracy results for"
W11-1602,W04-3219,0,0.02426,"expression to produce more confident translations for better estimated source units (Schroeder et al., 2009).2 For instance, the phrase in bold in the sentence The number of people known to have died has now reached 358 can be rewritten as 1) who died, 2) identified to have died and 3) known to have passed away. All such rephrasings are grammatically correct, the first one being significantly shorter, and they all convey a meaning which is reasonably close to the original wording. The task of rewriting complete sentences has also been addressed in various works (e.g. (Barzilay and Lee, 2003; Quirk et al., 2004; Zhao et al., 2010)). It poses, however, numerous other challenges, in particular regarding how it could be correctly evaluated. Human judgments of whole sentence transformations are complex and intra- and inter-judge coherence is difficult to attain with hypotheses of comparable quality. Using sentential paraphrases to support a given task (e.g. providing alternative reference translations for optimizing Statistical Machine Translation systems (Madnani et al., 2008)) 2 It is to be noted that, in the scenario presented in (Resnik et al., 2010), monolingual contributors cannot predict how usef"
W11-1602,D10-1013,0,0.334238,"ed by text paraphrasing techniques (Madnani and Dorr, 2010), the inherent polysemy of such linguistic units and the pragmatic constraints on their uses make it impossible to ensure that potential paraphrase pairs will be substitutable in any context, an observation which was already made at a lexical level (Zhao et al., 2007). Hence, automatic contextual validation of candidate rewritings is a fundamental issue for text paraphrasing with phrasal units. In this article, we tackle the problem of what we call targeted paraphrasing, defined as the rewriting of a subpart of a sentence, as in e.g. (Resnik et al., 2010) where it is applied to making parts of sentences easier to translate automatically. While this problem is simpler than full sentence rewriting, its study is justified as it should be handled correctly for the more complex task to be successful. Moreover, being simpler, it offers evaluation scenarios which make the performance on the task easier to assess. Our particular experiments here aim to assist a Wikipedia contributor in revising a text to improve its quality. For this, we use a collection of phrases that have been rewritten in Wikipedia, and test the substitutability of paraphrases com"
W11-1602,E09-1082,0,0.0196018,"word processors (Bernstein et al., 2010). Text improvements may also be more specifically targeted for automatic applications. In the work by Resnik et al. (2010), rephrasings for specific phrases are acquired through crowdsourcing. Difficult-to-translate phrases in the source text are first identified, and monolingual contributors are asked to provide rephrasings in context. Collected rephrasings can then be used as input for a Machine Translation system, which can positively exploit the increased variety in expression to produce more confident translations for better estimated source units (Schroeder et al., 2009).2 For instance, the phrase in bold in the sentence The number of people known to have died has now reached 358 can be rewritten as 1) who died, 2) identified to have died and 3) known to have passed away. All such rephrasings are grammatically correct, the first one being significantly shorter, and they all convey a meaning which is reasonably close to the original wording. The task of rewriting complete sentences has also been addressed in various works (e.g. (Barzilay and Lee, 2003; Quirk et al., 2004; Zhao et al., 2010)). It poses, however, numerous other challenges, in particular regardin"
W11-1602,N10-2012,0,0.0235665,"ses proposed by sets of players using a compact word-lattice view. Note that in its standard definition, the game attributes higher scores to paraphrase candidates that are highly rated and rarer. hedit = TER(Lemorig , Lempara ) (1) Note that this model is not derived from information from the Web, in contrast to all the models described next. Language model score The likelihood of a sentence can be a good indicator of its grammaticality (Mutton, 2006). Language model probabilities can now be obtained from Web counts. In our experiments, we used the Microsoft Web N-gram Service6 for research (Wang et al., 2010) to obtain log likelihood scores for text units.7 However, this score is certainly not sufficient as it does not take the original wording into account. We therefore used a ratio of the language model score of the paraphrased sentence with the language model score of the original 6 http://research.microsoft.com/en-us/ collaboration/focus/cs/web-ngram.aspx 7 Note that in order to query on French text, we had to remove all diacritics for the service to behave correctly, independently of encodings: careful examination of ranked hypotheses showed that this trick allowed us to obtain results cohere"
W11-1602,P09-1094,0,0.0165748,"taken from a rewriting memory automatically extracted from the edit history of Wikipedia. 1 Introduction There are many instances where it is reasonable to expect machines to produce text automatically. Traditionally, this was tackled as a concept-to-text realization problem. However, such needs apply sometimes to cases where a new text should be derived from some existing texts, an instance of text-to-text generation. The general idea is not anymore to produce a text from data, but to transform a text so as to ensure that it has desirable properties appropriate for some intended application (Zhao et al., 2009). For example, one may want a text to be shorter (Cohn and Lapata, 2008), tailored to some reader profile (Zhu et al., 2010), compliant with some specific norms (Max, 2004), or more adapted for subsequent machine processing tasks (Chandrasekar et al., 1996). The generation process must produce a text having a meaning which is compatible with the definition of the task at hand (e.g. strict paraphrasing for document normalization, relaxed paraGabriel Illouz LIMSI-CNRS Univ. Paris Sud gabrieli@limsi.fr Anne Vilnat LIMSI-CNRS Univ. Paris Sud anne@limsi.fr phrasing for text simplification), while e"
W11-1602,C10-1149,0,0.0124392,"ce more confident translations for better estimated source units (Schroeder et al., 2009).2 For instance, the phrase in bold in the sentence The number of people known to have died has now reached 358 can be rewritten as 1) who died, 2) identified to have died and 3) known to have passed away. All such rephrasings are grammatically correct, the first one being significantly shorter, and they all convey a meaning which is reasonably close to the original wording. The task of rewriting complete sentences has also been addressed in various works (e.g. (Barzilay and Lee, 2003; Quirk et al., 2004; Zhao et al., 2010)). It poses, however, numerous other challenges, in particular regarding how it could be correctly evaluated. Human judgments of whole sentence transformations are complex and intra- and inter-judge coherence is difficult to attain with hypotheses of comparable quality. Using sentential paraphrases to support a given task (e.g. providing alternative reference translations for optimizing Statistical Machine Translation systems (Madnani et al., 2008)) 2 It is to be noted that, in the scenario presented in (Resnik et al., 2010), monolingual contributors cannot predict how useful their rewritings"
W11-1602,C10-1152,0,0.0258946,"nces where it is reasonable to expect machines to produce text automatically. Traditionally, this was tackled as a concept-to-text realization problem. However, such needs apply sometimes to cases where a new text should be derived from some existing texts, an instance of text-to-text generation. The general idea is not anymore to produce a text from data, but to transform a text so as to ensure that it has desirable properties appropriate for some intended application (Zhao et al., 2009). For example, one may want a text to be shorter (Cohn and Lapata, 2008), tailored to some reader profile (Zhu et al., 2010), compliant with some specific norms (Max, 2004), or more adapted for subsequent machine processing tasks (Chandrasekar et al., 1996). The generation process must produce a text having a meaning which is compatible with the definition of the task at hand (e.g. strict paraphrasing for document normalization, relaxed paraGabriel Illouz LIMSI-CNRS Univ. Paris Sud gabrieli@limsi.fr Anne Vilnat LIMSI-CNRS Univ. Paris Sud anne@limsi.fr phrasing for text simplification), while ensuring that it remains grammatically correct. Its complexity, compared with concept-to-text generation, mostly stems from t"
W11-1602,N03-1003,0,\N,Missing
W11-1602,2010.amta-workshop.3,0,\N,Missing
W14-3618,D12-1052,0,0.0398906,"ata and selected the optimal subset to train their system. Alkanhal et al. (2012) presented a stochastic approach for spelling correction of Arabic text. They used a context-based system to automatically correct misspelled words. First of all, a list is generated with possible alternatives for each misspelled word using the Damerau-Levenshtein edit distance, then the right alternative for each misspelled word is selected stochastically using a lattice search, and an n-gram method. Shaalan et al. (2012) trained a Noisy Channel Model on word-based unigrams to detect and correct spelling errors. Dahlmeier and Ng (2012a) built specialized decoders for English grammatical error correction. More recently, (Pasha et al., 2014) created MADAMIRA, a system for morphological analysis and disambiguation of Arabic, this system can be used to improve the accuracy of spelling checking system especially with Hamza spelling correction. In contrast to the approaches described above, we use a machine translation (MT) based method to train an error correction system. To the best of our knowledge, this is the first error correction system for Arabic using an MT approach. 3 omission errors, which makes a good base for other"
W14-3618,P10-4002,0,0.0297961,"Missing"
W14-3618,P08-2015,0,0.0602574,"ts written by humans (e.g., non-native speakers), or machines (e.g., 2 Related Work Automatic error detection and correction include automatic spelling checking, grammar checking and postediting. Numerous approaches (both supervised and unsupervised) have been explored to improve the fluency of the text and reduce the percentage of outof-vocabulary words using NLP tools, resources, and heuristics, e.g., morphological analyzers, language models, and edit-distance measure (Kukich, 1992; Oflazer, 1996; Zribi and Ben Ahmed, 2003; Shaalan et al., 2003; Haddad and Yaseen, 2007; Hassan et al., 2008; Habash, 2008; Shaalan et al., 2010). There has been a lot of work on error correction for English (e.g., (Golding and Roth, 1999)). Other approaches learn models of correction by training on paired examples of errors and their corrections, which is the main goal of this work. For Arabic, this issue was studied in various directions and in different research work. In 2003, Shaalan et al. (2003) presented work on the specification and classification of spelling errors in Arabic. Later on, Haddad and Yaseen (2007) presented a hybrid approach using morphological features and rules to fine 137 Proceedings of t"
W14-3618,I08-2131,0,0.42987,"curring errors in texts written by humans (e.g., non-native speakers), or machines (e.g., 2 Related Work Automatic error detection and correction include automatic spelling checking, grammar checking and postediting. Numerous approaches (both supervised and unsupervised) have been explored to improve the fluency of the text and reduce the percentage of outof-vocabulary words using NLP tools, resources, and heuristics, e.g., morphological analyzers, language models, and edit-distance measure (Kukich, 1992; Oflazer, 1996; Zribi and Ben Ahmed, 2003; Shaalan et al., 2003; Haddad and Yaseen, 2007; Hassan et al., 2008; Habash, 2008; Shaalan et al., 2010). There has been a lot of work on error correction for English (e.g., (Golding and Roth, 1999)). Other approaches learn models of correction by training on paired examples of errors and their corrections, which is the main goal of this work. For Arabic, this issue was studied in various directions and in different research work. In 2003, Shaalan et al. (2003) presented work on the specification and classification of spelling errors in Arabic. Later on, Haddad and Yaseen (2007) presented a hybrid approach using morphological features and rules to fine 137 Pr"
W14-3618,P13-2121,0,0.0348173,"Missing"
W14-3618,P07-2045,0,0.00793191,"correct the spelling errors and 99K tokens were inserted (mostly punctuation marks). Furthermore, there is a total of 6,7K non necessary tokens deleted and 10.6K attached tokens split and 18.2 tokens merged. Finally, there are only 427 tokens moved in the sentence and 1563 multiple correction action. We experiment with different configurations and reach the sweet spot of performance when combining the different modules. Table 2: Clitics handled by the rule-based module. instance of that character (e.g. !!!!!!! would be replaced with !). Statistical Phrase-based Model We use the Moses toolkit (Koehn et al., 2007) to create a statistical phrase-based machine translation model built on the best pre-processed data, as described above. We treat this last step as a translation problem, where the source language is pre-processed incorrect Arabic text, and the reference is correct Arabic. Feature 14 extraction, rule-based correction, and character de-duplication are applied to both the train and dev sets. All but the last 1,000 sentences of the train data are used at the training set for the phrasebased model, the last 1,000 sentences of the train data are used as a tuning set, and the dev set is used for te"
W14-3618,C12-2011,0,0.527137,"Missing"
W14-3618,W14-3605,1,0.794478,"Missing"
W14-3618,J03-1002,0,0.0057042,"where the source language is pre-processed incorrect Arabic text, and the reference is correct Arabic. Feature 14 extraction, rule-based correction, and character de-duplication are applied to both the train and dev sets. All but the last 1,000 sentences of the train data are used at the training set for the phrasebased model, the last 1,000 sentences of the train data are used as a tuning set, and the dev set is used for testing and evaluation. We use fast align, the aligner included with the cdec decoder (Dyer et al., 2010) as the word aligner with grow-diag as the symmetrization heuristic (Och and Ney, 2003), and build a 5-gram language model from the correct Arabic training data with KenLM (Heafield et al., 2013). The system is evaluated with BLEU (Papineni et al., 2002) and then scored for precision, recall, and F1 measure against the dev set reference. We tested several different reordering window sizes since this is not a standard translation task, so we may want shorter distance reordering. Although 7 is the default size, we tested 7, 5, 4, 3, and 0, and found that a window of size 4 produces the best result according to BLEU score and F1 measure. 4 4.1 Results To evaluate the performance of"
W14-3618,J96-1003,1,0.552787,"evelop and evaluate spelling correction systems for Arabic trained either on naturally occurring errors in texts written by humans (e.g., non-native speakers), or machines (e.g., 2 Related Work Automatic error detection and correction include automatic spelling checking, grammar checking and postediting. Numerous approaches (both supervised and unsupervised) have been explored to improve the fluency of the text and reduce the percentage of outof-vocabulary words using NLP tools, resources, and heuristics, e.g., morphological analyzers, language models, and edit-distance measure (Kukich, 1992; Oflazer, 1996; Zribi and Ben Ahmed, 2003; Shaalan et al., 2003; Haddad and Yaseen, 2007; Hassan et al., 2008; Habash, 2008; Shaalan et al., 2010). There has been a lot of work on error correction for English (e.g., (Golding and Roth, 1999)). Other approaches learn models of correction by training on paired examples of errors and their corrections, which is the main goal of this work. For Arabic, this issue was studied in various directions and in different research work. In 2003, Shaalan et al. (2003) presented work on the specification and classification of spelling errors in Arabic. Later on, Haddad and"
W14-3618,P02-1040,0,0.10657,"de-duplication are applied to both the train and dev sets. All but the last 1,000 sentences of the train data are used at the training set for the phrasebased model, the last 1,000 sentences of the train data are used as a tuning set, and the dev set is used for testing and evaluation. We use fast align, the aligner included with the cdec decoder (Dyer et al., 2010) as the word aligner with grow-diag as the symmetrization heuristic (Och and Ney, 2003), and build a 5-gram language model from the correct Arabic training data with KenLM (Heafield et al., 2013). The system is evaluated with BLEU (Papineni et al., 2002) and then scored for precision, recall, and F1 measure against the dev set reference. We tested several different reordering window sizes since this is not a standard translation task, so we may want shorter distance reordering. Although 7 is the default size, we tested 7, 5, 4, 3, and 0, and found that a window of size 4 produces the best result according to BLEU score and F1 measure. 4 4.1 Results To evaluate the performance of our system on the development data, we compare its output to the reference (gold annotation). We then compute the usual measures of precision, recall and f-measure. R"
W14-3618,pasha-etal-2014-madamira,0,0.169771,"Missing"
W14-3618,N12-1067,0,\N,Missing
W14-3618,shaalan-etal-2012-arabic,0,\N,Missing
W14-3618,zaghouani-etal-2014-large,1,\N,Missing
W14-3627,P06-1086,1,0.865853,"ia industry has traditionally played a dominant role in the Arab world, making the Egyptian dialect the most widely understood and used dialect. DA is now emerging as the language of informal communication online. DA differs phonologically, lexically, morphologically, and syntactically from MSA. And while MSA has an established standard orthography, the dialects do not: people write words reflecting their phonology and sometimes use roman script. Thus, MSA tools cannot effectively model DA; for instance, over one-third of Levantine verbs cannot be analyzed using an MSA morphological analyzer (Habash and Rambow, 2006). These differences make the direct use of MSA NLP tools and applications for handling dialects impractical. In this paper, we present a statistical machine translation system for English to Dialectal Arabic (DA), using Modern Standard Arabic (MSA) as a pivot. We create a core system to translate from English to MSA using a large bilingual parallel corpus. Then, we design two separate pathways for translation from MSA into DA: a two-step domain and dialect adaptation system and a one-step simultaneous domain and dialect adaptation system. Both variants of the adaptation systems are trained on"
W14-3627,W12-2301,1,0.940968,"for Machine Translation into Egyptian Arabic Serena Jeblee1 , Weston Feely1 , Houda Bouamor2 Alon Lavie1 , Nizar Habash3 and Kemal Oflazer2 1 Carnegie Mellon University {sjeblee, wfeely, alavie}@cs.cmu.edu 2 Carnegie Mellon University in Qatar hbouamor@qatar.cmu.edu, ko@cs.cmu.edu 3 New York University Abu Dhabi nizar.habash@nyu.edu Abstract chine translation (Zbib et al., 2012; Salloum and Habash, 2013; Salloum et al., 2014; Al-Mannai et al., 2014) and in terms of data collection (Cotterell and Callison-Burch, 2014; Bouamor et al., 2014; Salama et al., 2014) and basic enabling technologies (Habash et al., 2012; Pasha et al., 2014). However, the focus is on a small number of iconic dialects, (e.g., Egyptian). The Egyptian media industry has traditionally played a dominant role in the Arab world, making the Egyptian dialect the most widely understood and used dialect. DA is now emerging as the language of informal communication online. DA differs phonologically, lexically, morphologically, and syntactically from MSA. And while MSA has an established standard orthography, the dialects do not: people write words reflecting their phonology and sometimes use roman script. Thus, MSA tools cannot effective"
W14-3627,N13-1044,1,0.929805,"ey may not always be able to pinpoint exact linguistic differences. In the context of natural language processing (NLP), some Arabic dialects have started receiving increasing attention, particularly in the context of maEgyptian Arabic is much closer to MSA than it is to English, so one can get a system bet196 Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 196–206, c October 25, 2014, Doha, Qatar. 2014 Association for Computational Linguistics nologies such as morphological analyzers are becoming available for specific dialects (Habash et al., 2012; Habash et al., 2013). For Arabic and its dialects, several researchers have explored the idea of exploiting existing MSA rich resources to build tools for DA NLP. Different research work successfully translated DA to MSA as a bridge to translate to English (Sawaf, 2010; Salloum and Habash, 2013), or to enhance the performance of Arabic-based information retrieval systems (Shatnawi et al., 2012). Among the efforts on translation from DA to MSA, Abo Bakr et al. (2008) introduced a hybrid approach to transfer a sentence from Egyptian Arabic to MSA. Sajjad et al. (2013) used a dictionary of Egyptian/MSA words to tran"
W14-3627,W14-3628,0,0.190206,"Missing"
W14-3627,bouamor-etal-2014-multidialectal,1,0.847169,"r experimental setup and the results obtained. Then, we give an analysis of our system output in Section 7. Finally, we conclude and describe our future work in Section 8. 2 Related work Machine translation (MT) for dialectal Arabic (DA) is quite challenging given the limited resources to build rule-based models or train statistical models for MT. While there has been a considerable amount of work in the context of standard Arabic NLP (Habash, 2010), DA is impoverished in terms of available tools and resources compared to MSA, e.g., there are few parallel DAEnglish corpora (Zbib et al., 2012; Bouamor et al., 2014). The majority of DA resources are for speech recognition, although more and more resources for machine translation and enabling tech3 Using Phrase-Based MT as an Adaptation System For commercial use, MT output is usually postedited by a human translator in order to fix the errors generated by the MT system. This is often faster and cheaper than having a human translate 197 the document from scratch. However, we can apply statistical phrase-based MT to create an automatic machine post-editor (what we refer to in this paper as an adaptation system) to improve the output of an MT system, and mak"
W14-3627,W14-5311,0,0.0124183,"y translated DA to MSA as a bridge to translate to English (Sawaf, 2010; Salloum and Habash, 2013), or to enhance the performance of Arabic-based information retrieval systems (Shatnawi et al., 2012). Among the efforts on translation from DA to MSA, Abo Bakr et al. (2008) introduced a hybrid approach to transfer a sentence from Egyptian Arabic to MSA. Sajjad et al. (2013) used a dictionary of Egyptian/MSA words to transform Egyptian to MSA and showed improvement in the quality of machine translation. A similar but rule-based work was done by Mohamed et al. (2012). Boujelbane et al. (2013) and Hamdi et al. (2014) built a bilingual dictionary using explicit knowledge about the relation between Tunisian Arabic and MSA. These works are limited to a dictionary or rules that are not available for all dialects. Zbib et al. (2012) used crowdsourcing to translate sentences from Egyptian and Levantine into English, and thus built two bilingual corpora. The dialectal sentences were selected from a large corpus of Arabic web text. Then, they explored several methods for dialect/English MT. Their best Egyptian/English system was trained on dialect/English parallel data. They argued that differences in genre betwe"
W14-3627,I13-1048,0,0.0168271,"ent research work successfully translated DA to MSA as a bridge to translate to English (Sawaf, 2010; Salloum and Habash, 2013), or to enhance the performance of Arabic-based information retrieval systems (Shatnawi et al., 2012). Among the efforts on translation from DA to MSA, Abo Bakr et al. (2008) introduced a hybrid approach to transfer a sentence from Egyptian Arabic to MSA. Sajjad et al. (2013) used a dictionary of Egyptian/MSA words to transform Egyptian to MSA and showed improvement in the quality of machine translation. A similar but rule-based work was done by Mohamed et al. (2012). Boujelbane et al. (2013) and Hamdi et al. (2014) built a bilingual dictionary using explicit knowledge about the relation between Tunisian Arabic and MSA. These works are limited to a dictionary or rules that are not available for all dialects. Zbib et al. (2012) used crowdsourcing to translate sentences from Egyptian and Levantine into English, and thus built two bilingual corpora. The dialectal sentences were selected from a large corpus of Arabic web text. Then, they explored several methods for dialect/English MT. Their best Egyptian/English system was trained on dialect/English parallel data. They argued that di"
W14-3627,P13-2121,0,0.0484788,"Missing"
W14-3627,P11-2031,1,0.921571,"m, which is a reordering window size of 7 for all systems, except for the phrase-based onestep domain and dialect adaptation system, which performs better with no reordering (0.2 BLEU better than a window of 7, 0.6 BLEU better than a window of 4), but these small differences in BLEU scores are within noise. The greatest difference in scores from the reordering windows was in the two-step systems domain adaptation step (MSA to MSA) on top of the phrase-based core, where a reordering window of 7 was 0.7 BLEU better than a window of 0. 6 6.1 Evaluation and Results For evaluation we use multeval (Clark et al., 2011) to calculate BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), TER (Snover et al., 2006), and length of the test set for each system. We evaluate the core and adaptation systems on the MSA and Egyptian sides of the test set drawn from the 100k corpus, which we refer to as the 100k sets. The data used for evaluation is a genuine Egyptian Arabic generated from MSA, just like the data the systems were trained on. It is not practical to evaluate on naturallygenerated Egyptian Arabic in this case because the domain of our datasets is very formal, since most of the text comes from n"
W14-3627,W11-2123,0,0.0346794,"e one-step adaptation system, where no reordering produced the best result. We also tested two different heuristics for symmetrizing the word alignments: grow-diag and grow-diag-final-and (Och and Ney, 2003). We found that using grow-diag as our symmetrization heuristic produced slightly better scores on the 100k datasets. For the baseline and adaptation systems we built 5-gram language models with KenLM (Heafield et al., 2013) using the target side of the training set, and for the core system we used the large MSA language model described in section 4. We use KenLM because it has been shown (Heafield, 2011) to be faster and use less memory than SRILM (Stolcke, 2002) and IRSTLM (Federico et al., 2008). below each table. The difference in scores between the different reordering window sizes (7, 4, and 0) we tried for the adaptation systems was not large (between 0 and 0.7 BLEU). In the following tables we present the best results for each adaptation system, which is a reordering window size of 7 for all systems, except for the phrase-based onestep domain and dialect adaptation system, which performs better with no reordering (0.2 BLEU better than a window of 7, 0.6 BLEU better than a window of 4),"
W14-3627,cotterell-callison-burch-2014-multi,0,0.147926,"Missing"
W14-3627,2007.mtsummit-papers.34,0,0.0414651,"te 197 the document from scratch. However, we can apply statistical phrase-based MT to create an automatic machine post-editor (what we refer to in this paper as an adaptation system) to improve the output of an MT system, and make it more closely resemble the references. Simard et al. (2007) used a phrase-based MT system as an automatic posteditor for the output of a commercial rule-based MT system, showing that it produced better results than both the rule-based system alone and a single pass phrase-based MT system. This technique is also useful for adapting to a specific domain or dataset. Isabelle et al. (2007) used a statistical MT system to automatically post-edit the output of a generic rule-based MT system, to avoid manually customizing a system dictionary and to reduce the amount of manual post-editing required. For our adaptation systems, we build a core phrase-based MT system with a large amount of out-of-domain data, which allows us to have better coverage of the target language. For an adaptation system, we then build a second phrase-based MT system by translating the in-domain train, tune, and test sets through the core translation system, then using that data to build the second system. T"
W14-3627,P07-2045,0,0.00633831,"stem below. 1 198 http://arz.wikipedia.org/ System Design Baseline MT System 100K sent. English Translation Egyptian Arabic One-Step Adaptation MT System 5M sent. English Translation 100K sent. Domain & MSA Dialect Adaptation Egyptian Arabic Two-Step Adaptation MT System English 5M sent. 100K sent. Translation Domain Adaptation MSA 100K sent. In-domain MSA Dialect Adaptation Egyptian Arabic Figure 1: An overview of the different system architectures. Baseline System Two-Step Adaptation System Our baseline system is a single phrase-based English to Egyptian Arabic MT system, built using Moses (Koehn et al., 2007) on the 100k corpus described in Section 4. This system does not include any MSA data, nor does it have an adaptation system; it is a typical, one-pass MT system that translates English directly into Egyptian Arabic. We will show that using adaptation systems improves the results significantly. We also build a two-step adaptation system that consists of two adaptation steps: one to adapt the MSA output of the core system to the domain of the MSA in the 100k corpus, and a second system to translate the MSA output of the domain adaptation system into Egyptian Arabic. We use the first adaptation"
W14-3627,W11-2107,1,0.838244,"he phrase-based onestep domain and dialect adaptation system, which performs better with no reordering (0.2 BLEU better than a window of 7, 0.6 BLEU better than a window of 4), but these small differences in BLEU scores are within noise. The greatest difference in scores from the reordering windows was in the two-step systems domain adaptation step (MSA to MSA) on top of the phrase-based core, where a reordering window of 7 was 0.7 BLEU better than a window of 0. 6 6.1 Evaluation and Results For evaluation we use multeval (Clark et al., 2011) to calculate BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), TER (Snover et al., 2006), and length of the test set for each system. We evaluate the core and adaptation systems on the MSA and Egyptian sides of the test set drawn from the 100k corpus, which we refer to as the 100k sets. The data used for evaluation is a genuine Egyptian Arabic generated from MSA, just like the data the systems were trained on. It is not practical to evaluate on naturallygenerated Egyptian Arabic in this case because the domain of our datasets is very formal, since most of the text comes from news sources, and dialectal Arabic is generally used in informal situations.2 B"
W14-3627,P12-2035,1,0.927262,"ools for DA NLP. Different research work successfully translated DA to MSA as a bridge to translate to English (Sawaf, 2010; Salloum and Habash, 2013), or to enhance the performance of Arabic-based information retrieval systems (Shatnawi et al., 2012). Among the efforts on translation from DA to MSA, Abo Bakr et al. (2008) introduced a hybrid approach to transfer a sentence from Egyptian Arabic to MSA. Sajjad et al. (2013) used a dictionary of Egyptian/MSA words to transform Egyptian to MSA and showed improvement in the quality of machine translation. A similar but rule-based work was done by Mohamed et al. (2012). Boujelbane et al. (2013) and Hamdi et al. (2014) built a bilingual dictionary using explicit knowledge about the relation between Tunisian Arabic and MSA. These works are limited to a dictionary or rules that are not available for all dialects. Zbib et al. (2012) used crowdsourcing to translate sentences from Egyptian and Levantine into English, and thus built two bilingual corpora. The dialectal sentences were selected from a large corpus of Arabic web text. Then, they explored several methods for dialect/English MT. Their best Egyptian/English system was trained on dialect/English parallel"
W14-3627,P05-1071,1,0.686452,"and also to adapt to the Egyptian dialect. What we refer to as the “one-step” system is a core system plus one adaptation system, whereas the “two-step” system consists of the core plus two subsequent adaptation systems. We describe the systems in more detail in Section 5. sentences from NIST MT09 (NIST Multimodal Information Group, 2010b). We use a 5-gram MSA language model built using the SRILM toolkit (Stolcke, 2002) on 260 million words of MSA from the Arabic Gigaword (Parker et al., 2011). All our MSA parallel data and monolingual MSA language modeling data were tokenized with MADA v3.1 (Habash and Rambow, 2005) using the ATB (Arabic Treebank) tokenization scheme. For the adaptation systems, we build a 100k tri-parallel corpus Egyptian-MSA-English corpus. The MSA and English parts are extracted from the NIST corpus distributed by the Linguistic Data Consortium. The Egyptian sentences are obtained automatically by extending Mohamed et al. (2012) method for generating Egyptian Arabic from morphologically disambiguated MSA sentences. This rule-based method relies on 103 transformation rules covering essentially nouns, verbs and pronouns as well as certain lexical items. For each MSA sentence, this metho"
W14-3627,N07-1064,0,0.0331669,"ch recognition, although more and more resources for machine translation and enabling tech3 Using Phrase-Based MT as an Adaptation System For commercial use, MT output is usually postedited by a human translator in order to fix the errors generated by the MT system. This is often faster and cheaper than having a human translate 197 the document from scratch. However, we can apply statistical phrase-based MT to create an automatic machine post-editor (what we refer to in this paper as an adaptation system) to improve the output of an MT system, and make it more closely resemble the references. Simard et al. (2007) used a phrase-based MT system as an automatic posteditor for the output of a commercial rule-based MT system, showing that it produced better results than both the rule-based system alone and a single pass phrase-based MT system. This technique is also useful for adapting to a specific domain or dataset. Isabelle et al. (2007) used a statistical MT system to automatically post-edit the output of a generic rule-based MT system, to avoid manually customizing a system dictionary and to reduce the amount of manual post-editing required. For our adaptation systems, we build a core phrase-based MT"
W14-3627,J03-1002,0,0.00421305,"nts Since MSA and Egyptian are more similar to each other than they are to English, we tried several different reordering window sizes to find the optimal reordering distance for adapting MSA to Egyptian Arabic, including the typical reordering window of length 7, a smaller window of length 4, and no reordering at all. We found a reordering window 199 size of 7 to work best for all our systems, except for the one-step adaptation system, where no reordering produced the best result. We also tested two different heuristics for symmetrizing the word alignments: grow-diag and grow-diag-final-and (Och and Ney, 2003). We found that using grow-diag as our symmetrization heuristic produced slightly better scores on the 100k datasets. For the baseline and adaptation systems we built 5-gram language models with KenLM (Heafield et al., 2013) using the target side of the training set, and for the core system we used the large MSA language model described in section 4. We use KenLM because it has been shown (Heafield, 2011) to be faster and use less memory than SRILM (Stolcke, 2002) and IRSTLM (Federico et al., 2008). below each table. The difference in scores between the different reordering window sizes (7, 4,"
W14-3627,P02-1040,0,0.10335,"7 for all systems, except for the phrase-based onestep domain and dialect adaptation system, which performs better with no reordering (0.2 BLEU better than a window of 7, 0.6 BLEU better than a window of 4), but these small differences in BLEU scores are within noise. The greatest difference in scores from the reordering windows was in the two-step systems domain adaptation step (MSA to MSA) on top of the phrase-based core, where a reordering window of 7 was 0.7 BLEU better than a window of 0. 6 6.1 Evaluation and Results For evaluation we use multeval (Clark et al., 2011) to calculate BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), TER (Snover et al., 2006), and length of the test set for each system. We evaluate the core and adaptation systems on the MSA and Egyptian sides of the test set drawn from the 100k corpus, which we refer to as the 100k sets. The data used for evaluation is a genuine Egyptian Arabic generated from MSA, just like the data the systems were trained on. It is not practical to evaluate on naturallygenerated Egyptian Arabic in this case because the domain of our datasets is very formal, since most of the text comes from news sources, and dialectal Arabic is gener"
W14-3627,2006.amta-papers.25,0,0.0165285,"d dialect adaptation system, which performs better with no reordering (0.2 BLEU better than a window of 7, 0.6 BLEU better than a window of 4), but these small differences in BLEU scores are within noise. The greatest difference in scores from the reordering windows was in the two-step systems domain adaptation step (MSA to MSA) on top of the phrase-based core, where a reordering window of 7 was 0.7 BLEU better than a window of 0. 6 6.1 Evaluation and Results For evaluation we use multeval (Clark et al., 2011) to calculate BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), TER (Snover et al., 2006), and length of the test set for each system. We evaluate the core and adaptation systems on the MSA and Egyptian sides of the test set drawn from the 100k corpus, which we refer to as the 100k sets. The data used for evaluation is a genuine Egyptian Arabic generated from MSA, just like the data the systems were trained on. It is not practical to evaluate on naturallygenerated Egyptian Arabic in this case because the domain of our datasets is very formal, since most of the text comes from news sources, and dialectal Arabic is generally used in informal situations.2 Below we report BLEU scores"
W14-3627,pasha-etal-2014-madamira,1,0.871023,"Missing"
W14-3627,P11-2007,0,0.118831,"thod relies on 103 transformation rules covering essentially nouns, verbs and pronouns as well as certain lexical items. For each MSA sentence, this method provides more than one possible candidate, in its original version, the Egyptian sentence kept was chosen randomly. We extend the selection algorithm by scoring the different sentences using a language model. For this, we use SRILM with modified Kneser-Ney smoothing to build a 5-gram language model. The model is trained on a corpus including articles extracted from the Egyptian version of Wikipedia1 and the Egyptian side of the AOC corpus (Zaidan and Callison-Burch, 2011). We chose to include Egyptian Wikipedia for the formal level of sentences in it different from the regular DA written in blogs or microblogging websites (e.g., Twitter) and closer to the ones generated by our system. We split this data into train, tune, and test sets of 98,027, 960, and 961 sentences respectively, after removing duplicates across sets. The MSA corpus was tokenized using MADA and the Egyptian Arabic data was tokenized with MADA-ARZ v0.4 (Habash et al., 2013), both using the ATB tokenization scheme, with alif/ya normalization. 4 5 Data For the core English to MSA system, we use"
W14-3627,P13-2001,0,0.101077,"ble for specific dialects (Habash et al., 2012; Habash et al., 2013). For Arabic and its dialects, several researchers have explored the idea of exploiting existing MSA rich resources to build tools for DA NLP. Different research work successfully translated DA to MSA as a bridge to translate to English (Sawaf, 2010; Salloum and Habash, 2013), or to enhance the performance of Arabic-based information retrieval systems (Shatnawi et al., 2012). Among the efforts on translation from DA to MSA, Abo Bakr et al. (2008) introduced a hybrid approach to transfer a sentence from Egyptian Arabic to MSA. Sajjad et al. (2013) used a dictionary of Egyptian/MSA words to transform Egyptian to MSA and showed improvement in the quality of machine translation. A similar but rule-based work was done by Mohamed et al. (2012). Boujelbane et al. (2013) and Hamdi et al. (2014) built a bilingual dictionary using explicit knowledge about the relation between Tunisian Arabic and MSA. These works are limited to a dictionary or rules that are not available for all dialects. Zbib et al. (2012) used crowdsourcing to translate sentences from Egyptian and Levantine into English, and thus built two bilingual corpora. The dialectal sen"
W14-3627,N12-1006,0,0.393994,"ts on translation from DA to MSA, Abo Bakr et al. (2008) introduced a hybrid approach to transfer a sentence from Egyptian Arabic to MSA. Sajjad et al. (2013) used a dictionary of Egyptian/MSA words to transform Egyptian to MSA and showed improvement in the quality of machine translation. A similar but rule-based work was done by Mohamed et al. (2012). Boujelbane et al. (2013) and Hamdi et al. (2014) built a bilingual dictionary using explicit knowledge about the relation between Tunisian Arabic and MSA. These works are limited to a dictionary or rules that are not available for all dialects. Zbib et al. (2012) used crowdsourcing to translate sentences from Egyptian and Levantine into English, and thus built two bilingual corpora. The dialectal sentences were selected from a large corpus of Arabic web text. Then, they explored several methods for dialect/English MT. Their best Egyptian/English system was trained on dialect/English parallel data. They argued that differences in genre between MSA and DA make bridging through MSA of limited value. For this reason, while pivoting through MSA, it is important to consider the domain and add an additional step: domain adaptation. The majority of previous e"
W14-3627,salama-etal-2014-youdacc,1,0.710505,"Missing"
W14-3627,N13-1036,1,0.844451,"English, so one can get a system bet196 Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 196–206, c October 25, 2014, Doha, Qatar. 2014 Association for Computational Linguistics nologies such as morphological analyzers are becoming available for specific dialects (Habash et al., 2012; Habash et al., 2013). For Arabic and its dialects, several researchers have explored the idea of exploiting existing MSA rich resources to build tools for DA NLP. Different research work successfully translated DA to MSA as a bridge to translate to English (Sawaf, 2010; Salloum and Habash, 2013), or to enhance the performance of Arabic-based information retrieval systems (Shatnawi et al., 2012). Among the efforts on translation from DA to MSA, Abo Bakr et al. (2008) introduced a hybrid approach to transfer a sentence from Egyptian Arabic to MSA. Sajjad et al. (2013) used a dictionary of Egyptian/MSA words to transform Egyptian to MSA and showed improvement in the quality of machine translation. A similar but rule-based work was done by Mohamed et al. (2012). Boujelbane et al. (2013) and Hamdi et al. (2014) built a bilingual dictionary using explicit knowledge about the relation betwe"
W14-3627,P14-2125,1,0.860146,"Missing"
W14-3627,2010.amta-papers.5,0,0.287107,"than it is to English, so one can get a system bet196 Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 196–206, c October 25, 2014, Doha, Qatar. 2014 Association for Computational Linguistics nologies such as morphological analyzers are becoming available for specific dialects (Habash et al., 2012; Habash et al., 2013). For Arabic and its dialects, several researchers have explored the idea of exploiting existing MSA rich resources to build tools for DA NLP. Different research work successfully translated DA to MSA as a bridge to translate to English (Sawaf, 2010; Salloum and Habash, 2013), or to enhance the performance of Arabic-based information retrieval systems (Shatnawi et al., 2012). Among the efforts on translation from DA to MSA, Abo Bakr et al. (2008) introduced a hybrid approach to transfer a sentence from Egyptian Arabic to MSA. Sajjad et al. (2013) used a dictionary of Egyptian/MSA words to transform Egyptian to MSA and showed improvement in the quality of machine translation. A similar but rule-based work was done by Mohamed et al. (2012). Boujelbane et al. (2013) and Hamdi et al. (2014) built a bilingual dictionary using explicit knowled"
W15-1614,abuhakema-etal-2008-annotating,0,0.311605,"access to detailed error statistics. This can provide learners with a very useful feedback and help them improve their proficiency level. 129 Proceedings of LAW IX - The 9th Linguistic Annotation Workshop, pages 129–139, c Denver, Colorado, June 5, 2015. 2015 Association for Computational Linguistics These errors may take place in words, phrases, language structures, and the ways words or expressions are used (Granger, 2003). For Arabic, there are few projects that aim at developing Arabic learner corpora and annotating them but most of them are not freely available for users or researchers (Abuhakema et al., 2008; Hassan and Daud, 2011). In this paper, we present our annotation method and our efforts for extending an L1 large scale Arabic language corpus and its manually edited corrections to include annotated non-native Arabic learner text (L2). This work is part of the Qatar Arabic Language Bank (QALB) project (Zaghouani et al., 2014b), a large-scale error annotation effort that aims to create a manually corrected corpus of errors for a variety of Arabic texts (the target size is 2 million words).1 Our overarching goal is to use our annotated corpus to develop components for automatic detection and"
W15-1614,W13-1703,0,0.0240313,"ar/CMU-CS-QTR-124.pdf 2 130 Section 2; then we describe the corpus and the annotation guidelines in Sections 3 and 4. Afterwards, we present our annotation tool and pipeline in Sections 5 and 6. Finally, we present an evaluation of the annotation quality and discuss the L2 annotation challenges in Section 7. 2 Related Work Currently available manually corrected learner corpora are generally limited when it comes to the language, size and the genre of data. Several corpora of learners of English annotated for errors are publicly available (Rozovskaya and Roth, 2010; Yannakoudakis et al., 2011; Dahlmeier et al., 2013), ranging in size between 60K words and more than one million words. Dickinson and Ledbetter (2012) annotated errors in student essays written by learners of Hungarian at three proficiency levels at Indiana University. The annotation was performed using EXMARaLDA, a freely available tool that allows multiple and concurrent annotations (Schmidt, 2010). Student errors were marked according to various categories of phonological, spelling, agreement and derivation errors. For Arabic, very few learner corpora annotation project have been built. Abuhakema et al. (2008) annotated a small corpus of 9K"
W15-1614,dickinson-ledbetter-2012-annotating,0,0.136697,"ines in Sections 3 and 4. Afterwards, we present our annotation tool and pipeline in Sections 5 and 6. Finally, we present an evaluation of the annotation quality and discuss the L2 annotation challenges in Section 7. 2 Related Work Currently available manually corrected learner corpora are generally limited when it comes to the language, size and the genre of data. Several corpora of learners of English annotated for errors are publicly available (Rozovskaya and Roth, 2010; Yannakoudakis et al., 2011; Dahlmeier et al., 2013), ranging in size between 60K words and more than one million words. Dickinson and Ledbetter (2012) annotated errors in student essays written by learners of Hungarian at three proficiency levels at Indiana University. The annotation was performed using EXMARaLDA, a freely available tool that allows multiple and concurrent annotations (Schmidt, 2010). Student errors were marked according to various categories of phonological, spelling, agreement and derivation errors. For Arabic, very few learner corpora annotation project have been built. Abuhakema et al. (2008) annotated a small corpus of 9K words of Arabic written materials produced by native speakers of English in the US who learned Ara"
W15-1614,N13-1066,1,0.853751,"ly accepted Arabic punctuation rules. 132 Dialectal Usage Errors: In comparison to Standard Arabic, where there are clear spelling standards and conventions, Arabic dialects do not have official orthographic standards partly since they were not commonly written until recently. Today, Arabic dialects are often seen in social media, but also in published novels (and there is even an Egyptian Arabic Wikipedia). Habash et al. (2012) proposed a Conventional Orthography for Dialectal Arabic (or CODA) targeting Egyptian Arabic for computational modeling purposes and demonstrated how to map to it in (Eskander et al., 2013) and (Pasha et al., 2014; Habash et al., 2013). CODAs for other dialects have also been proposed (Zribi et al., 2014; Jarrar et al., 2014). In our current annotation task we neither address dialectal Arabic spelling normalization (Eskander et al., 2013), nor do we systematically translate dialectal words into Standard Arabic (Salloum and Habash, 2013). We recognize that the Arabic language is in a diglossic situation and borrowing is frequent. Most of the texts provided for annotation are in Standard Arabic, but dialectal words are sometimes mistakenly used. We are interested in reducing vario"
W15-1614,I08-1059,0,0.0165036,"such as POS, lemma, gender, number or person. The robust design of MADAMIRA allows it to consider different possible spellings of words, especially relating to Ya/Alif-Maqsura, Ha/Ta-Marbuta and Hamzated Alif forms, which are very common error sources. MADAMIRA selects the correct form in context, thus correcting for these errors which are often connected to lemma choice or morphology. 7 7.1 Evaluation Inter-Annotator Agreement Our annotation effort consists of a single annotation pass as commonly done in many annotation projects due to time and budget constraints (Rozovskaya and Roth, 2010; Gamon et al., 2008; Izumi et al., 2004; Nagata et al., 2006). In order to evaluate the quality of our correction annotations, we frequently measure the inter-annotator agreement (IAA) to ensure that the annotators are following the guidelines provided consistently. A high level of agreement between the annotators indicates that the annotation is reliable and the guidelines are useful in producing homogeneous and consistent data. We measure the IAA by averaging WER (Word Error Rate) over all pairs of annotations to compute the AWER (Average 135 Word Error Rate).7 For the purpose of this evaluation, the WER refer"
W15-1614,habash-etal-2012-conventional,1,0.127188,"text uses one of multiple widely acceptable transliterations, the annotators should not modify the word. Punctuation Errors: Punctuation errors should be corrected according to the commonly accepted Arabic punctuation rules. 132 Dialectal Usage Errors: In comparison to Standard Arabic, where there are clear spelling standards and conventions, Arabic dialects do not have official orthographic standards partly since they were not commonly written until recently. Today, Arabic dialects are often seen in social media, but also in published novels (and there is even an Egyptian Arabic Wikipedia). Habash et al. (2012) proposed a Conventional Orthography for Dialectal Arabic (or CODA) targeting Egyptian Arabic for computational modeling purposes and demonstrated how to map to it in (Eskander et al., 2013) and (Pasha et al., 2014; Habash et al., 2013). CODAs for other dialects have also been proposed (Zribi et al., 2014; Jarrar et al., 2014). In our current annotation task we neither address dialectal Arabic spelling normalization (Eskander et al., 2013), nor do we systematically translate dialectal words into Standard Arabic (Salloum and Habash, 2013). We recognize that the Arabic language is in a diglossic"
W15-1614,N13-1044,1,0.82105,"tal Usage Errors: In comparison to Standard Arabic, where there are clear spelling standards and conventions, Arabic dialects do not have official orthographic standards partly since they were not commonly written until recently. Today, Arabic dialects are often seen in social media, but also in published novels (and there is even an Egyptian Arabic Wikipedia). Habash et al. (2012) proposed a Conventional Orthography for Dialectal Arabic (or CODA) targeting Egyptian Arabic for computational modeling purposes and demonstrated how to map to it in (Eskander et al., 2013) and (Pasha et al., 2014; Habash et al., 2013). CODAs for other dialects have also been proposed (Zribi et al., 2014; Jarrar et al., 2014). In our current annotation task we neither address dialectal Arabic spelling normalization (Eskander et al., 2013), nor do we systematically translate dialectal words into Standard Arabic (Salloum and Habash, 2013). We recognize that the Arabic language is in a diglossic situation and borrowing is frequent. Most of the texts provided for annotation are in Standard Arabic, but dialectal words are sometimes mistakenly used. We are interested in reducing various spelling inconsistencies that frequently oc"
W15-1614,W10-1802,0,0.0226722,"non-native speakers of other languages such as English (Leacock et al., 2010; Rozovskaya and Roth, 2010). Lexical Correction: Finally, if it is impossible to fully correct the word using the previous four steps, there is a clear case of word choice errors and the annotator may have to replace the word used. This can be employed to especially correct inadequate lexical choices or unknown words. In the example given in 5 3. Correct derivation errors; but keep root intact. 4 The minimum edits approach in error correction have already been used in the Error-tagged Learner Corpus of Czech project (Hana et al., 2010) 133 Arabic transliteration is presented in the Habash-SoudiBuckwalter scheme (Habash et al., 2007): (in alphabetical orˇ der) AbtθjHxdðrzsšSDTDςγfqklmnhwy and the additional sym ˇ ¯  ˆ ð', yˆ Zø', ¯ bols: ’ Z, Â @, A @, A @, w h è, ý ø. 6 A clitic is a linguistic unit that is pronounced and written like an affix but is grammatically independent. Inflection Error Correction Original ˇ knt qd bdÂnA fy AlςAm AlmADy rHl¯h Alaý mk¯h. Correction ˇ knt qd bdÂt fy AlςAm AlmADy rHl¯h Alaý mk¯h. English Original Correction English Original Correction English Original Correction English  úÍ@ éÊgP  ú"
W15-1614,W14-3603,1,0.49852,"and conventions, Arabic dialects do not have official orthographic standards partly since they were not commonly written until recently. Today, Arabic dialects are often seen in social media, but also in published novels (and there is even an Egyptian Arabic Wikipedia). Habash et al. (2012) proposed a Conventional Orthography for Dialectal Arabic (or CODA) targeting Egyptian Arabic for computational modeling purposes and demonstrated how to map to it in (Eskander et al., 2013) and (Pasha et al., 2014; Habash et al., 2013). CODAs for other dialects have also been proposed (Zribi et al., 2014; Jarrar et al., 2014). In our current annotation task we neither address dialectal Arabic spelling normalization (Eskander et al., 2013), nor do we systematically translate dialectal words into Standard Arabic (Salloum and Habash, 2013). We recognize that the Arabic language is in a diglossic situation and borrowing is frequent. Most of the texts provided for annotation are in Standard Arabic, but dialectal words are sometimes mistakenly used. We are interested in reducing various spelling inconsistencies that frequently occur. So, as was done in the L1 annotation effort (Zaghouani et al., 2014b), we asked annotat"
W15-1614,W14-3605,1,0.412475,"us. The results obtained in the evaluation suggest that the annotators produced consistently similar results under the proposed guidelines. We believe that publishing this corpus will give researchers a common development and test set for developing related natural language processing applications. A subset of our L2 corpus will be used as part of the Second QALB Shared Task on Automatic Arabic Error Correction in conjunction with the ACL-2015 Workshop on Arabic NLP.9 This shared task follows the success of the First QALB Shared Task held in conjunction with EMNLP-2014 Workshop on Arabic NLP (Mohit et al., 2014). In the future, we will extend our annotation guidelines to address machine translation output correction (i.e., manual post-editing). We also plan to extend our systems for automatic correction of Arabic language errors (Jeblee et al., 2014; Rozovskaya et al., 2014) to handle L2 data, using the corpus discussed here for training and test purposes. 9 http://www.arabic-nlp.net/wanlp 137 Acknowledgements We thank anonymous reviewers for their valuable comments and suggestions. We also thank all our dedicated annotators: Noor Alzeer, Hoda Fathy, Hoda Ibrahim, Anissa Jrad, Samah Lakhal, Jihene Wa"
W15-1614,P06-1031,0,0.0280632,"erson. The robust design of MADAMIRA allows it to consider different possible spellings of words, especially relating to Ya/Alif-Maqsura, Ha/Ta-Marbuta and Hamzated Alif forms, which are very common error sources. MADAMIRA selects the correct form in context, thus correcting for these errors which are often connected to lemma choice or morphology. 7 7.1 Evaluation Inter-Annotator Agreement Our annotation effort consists of a single annotation pass as commonly done in many annotation projects due to time and budget constraints (Rozovskaya and Roth, 2010; Gamon et al., 2008; Izumi et al., 2004; Nagata et al., 2006). In order to evaluate the quality of our correction annotations, we frequently measure the inter-annotator agreement (IAA) to ensure that the annotators are following the guidelines provided consistently. A high level of agreement between the annotators indicates that the annotation is reliable and the guidelines are useful in producing homogeneous and consistent data. We measure the IAA by averaging WER (Word Error Rate) over all pairs of annotations to compute the AWER (Average 135 Word Error Rate).7 For the purpose of this evaluation, the WER refers to an annotation error and it is measure"
W15-1614,I13-2001,1,0.525343,"eyeglasses to read the book.’   @Q¯ @ ú Ë è @QÖÏ @ © A  P A¢ JË@ © A ú Ë H@ Table 1: Examples of the different parts of the correction priority order   ¯ h ‘mirror’ was replaced Table 1, the word è @QÖÏ @ AlmrA¯  P A¢ JË@ AlnDArAt ˇ by the word H@ ‘eyeglasses’. alignments starting from document tokenization to after human annotation. 5 6 The Annotation Tool In order to ensure the speed and efficiency of the annotation process, as well as better management, we provide the annotators with a web-based annotation framework, originally developed to manually correct errors in L1 texts (Obeid et al., 2013). The annotation interface allows annotators to perform different actions corresponding to the following types of corrections: (a) edit misspelled words; (b) move words that are not in the right location; (c) add missing words; (d) delete extraneous words; (e) merge words that have been split erroneously; and (f) split words that have been merged erroneously. In our final corpus output format, we record for each annotated file the list of actions taken by the annotator. These actions operate on one or two tokens depending on the action. We also supply token 134 The Annotation Pipeline The anno"
W15-1614,P02-1040,0,0.105018,"ear that ALC is less challenging than ALWC as shown in the IAA of the first round and second rounds. Overall, the high-level of agreement obtained in the second round shows that the annotators produced consistently similar results under the proposed guidelines; and their differences are all within acceptable variation. This of course makes the evaluation of automatic correction harder.8 7 The annotation manager is excluded from this evaluation. This problem might be solved by considering multiple references in the evaluation process similarly to what is done in machine translation evaluation (Papineni et al., 2002). Unfortu8 Original  ®Ö Ï @ úæîDKA ÐA« ú¯ éËA à@ ø ñK@ B@ . ZAKCJË@ ÉJ.¯ àA Anwy An sAnthy AlmqAl¯h fy ςAm AlAnsAn qbl AlθlAθA’. ‘I plan I will be-done the article in the year of humanity before Tuesday.’ Annotator 1 @ ÐA« á«  ®Ö Ï @ úæîE @ à @ ø ñK @ B éËA . ZAKCJË@ ÉJ.¯ àA ˇ Ânwy Ân Ânhy AlmqAl¯h ςn ςAm AlAnsAn qbl AlθlAθA’. ‘I plan to finish-off the article about the year of humanity before Tuesday.’ Annotator 2 @ ÕËA« á«  ®Ö Ï @ úæîE @ à @ ø ñK @ B éËA . ZAKCJË@ ÉJ.¯ àA ˇ Ânwy Ân Ânhy AlmqAl¯h ςn ςAlm AlAnsAn qbl AlθlAθA’. ‘I plan to finish-off the article about the"
W15-1614,pasha-etal-2014-madamira,1,0.792787,"Missing"
W15-1614,W10-1004,1,0.231979,"able at http://reports-archive.adm.cs.cmu.edu/ anon/qatar/CMU-CS-QTR-124.pdf 2 130 Section 2; then we describe the corpus and the annotation guidelines in Sections 3 and 4. Afterwards, we present our annotation tool and pipeline in Sections 5 and 6. Finally, we present an evaluation of the annotation quality and discuss the L2 annotation challenges in Section 7. 2 Related Work Currently available manually corrected learner corpora are generally limited when it comes to the language, size and the genre of data. Several corpora of learners of English annotated for errors are publicly available (Rozovskaya and Roth, 2010; Yannakoudakis et al., 2011; Dahlmeier et al., 2013), ranging in size between 60K words and more than one million words. Dickinson and Ledbetter (2012) annotated errors in student essays written by learners of Hungarian at three proficiency levels at Indiana University. The annotation was performed using EXMARaLDA, a freely available tool that allows multiple and concurrent annotations (Schmidt, 2010). Student errors were marked according to various categories of phonological, spelling, agreement and derivation errors. For Arabic, very few learner corpora annotation project have been built. A"
W15-1614,W14-3622,1,0.888722,"Missing"
W15-1614,N13-1036,1,0.851271,"shed novels (and there is even an Egyptian Arabic Wikipedia). Habash et al. (2012) proposed a Conventional Orthography for Dialectal Arabic (or CODA) targeting Egyptian Arabic for computational modeling purposes and demonstrated how to map to it in (Eskander et al., 2013) and (Pasha et al., 2014; Habash et al., 2013). CODAs for other dialects have also been proposed (Zribi et al., 2014; Jarrar et al., 2014). In our current annotation task we neither address dialectal Arabic spelling normalization (Eskander et al., 2013), nor do we systematically translate dialectal words into Standard Arabic (Salloum and Habash, 2013). We recognize that the Arabic language is in a diglossic situation and borrowing is frequent. Most of the texts provided for annotation are in Standard Arabic, but dialectal words are sometimes mistakenly used. We are interested in reducing various spelling inconsistencies that frequently occur. So, as was done in the L1 annotation effort (Zaghouani et al., 2014b), we asked annotators to flag the highly dialectal cases to be reviewed later by the annotation manager. The guidelines classify dialectal word issues into five categories inspired by Habash et al. (2008): dialectal lexical choice, p"
W15-1614,W08-1205,0,0.00798721,"etailed description of ALC is given at: http://www.arabiclearnercorpus.com/ 131 and structures, with some items overused and others significantly underused. They also contain varying degrees of grammatical, orthographic and lexical errors. Moreover, sentences written by Arabic L2 speaker have often a different structure and are not as fluent as sentences produced by a native speaker even when no clear mistakes can be found. Therefore, the correction task is complicated by the fact that the acceptability level of a given sentence differs widely within the native speaker annotators as stated by Tetreault and Chodorow (2008). These issues can be related to linguistic factors such as inter-language (L1 interference), the student’s teaching and learning methodology, and to the translation effect (conscious interference). Thus, correcting the Arabic L2 essays can be a very challenging task that requires a lot of interpretation efforts by the annotators. This will likely lead to lower inter-annotator agreement as there is often many possible ways to correct the L2 errors. In order to annotate the L2 corpus, we use our annotation guidelines designed for L1 (Zaghouani et al., 2014b) and add specific L2 annotation rules"
W15-1614,P11-1019,0,0.0242372,"ive.adm.cs.cmu.edu/ anon/qatar/CMU-CS-QTR-124.pdf 2 130 Section 2; then we describe the corpus and the annotation guidelines in Sections 3 and 4. Afterwards, we present our annotation tool and pipeline in Sections 5 and 6. Finally, we present an evaluation of the annotation quality and discuss the L2 annotation challenges in Section 7. 2 Related Work Currently available manually corrected learner corpora are generally limited when it comes to the language, size and the genre of data. Several corpora of learners of English annotated for errors are publicly available (Rozovskaya and Roth, 2010; Yannakoudakis et al., 2011; Dahlmeier et al., 2013), ranging in size between 60K words and more than one million words. Dickinson and Ledbetter (2012) annotated errors in student essays written by learners of Hungarian at three proficiency levels at Indiana University. The annotation was performed using EXMARaLDA, a freely available tool that allows multiple and concurrent annotations (Schmidt, 2010). Student errors were marked according to various categories of phonological, spelling, agreement and derivation errors. For Arabic, very few learner corpora annotation project have been built. Abuhakema et al. (2008) annot"
W15-1614,zaghouani-etal-2014-large,1,0.588046,"Missing"
W15-1614,zribi-etal-2014-conventional,1,0.33632,"Missing"
W15-1614,W14-3618,1,\N,Missing
W15-3204,W15-3214,0,0.0408316,"Missing"
W15-3204,I08-2131,0,0.0157927,"trated on English, especially on errors made by learners of English as a Second Language. Four competitions devoted to error correction for non-native English writers took place recently: HOO (Dale and Kilgarriff, 2011; Dale et al., 2012) and CoNLL (Ng et al., 2013; Ng et al., 2014). Shared tasks of this kind are extremely important, as they bring together researchers and promote the development of relevant techniques and dissemination of key resources, such as benchmark data sets. In the area of Arabic text correction, there has been a significant body of work, as well (Shaalan et al., 2003; Hassan et al., 2008). However, due to the lack of a common benchmark data set, making progress on this task has been difficult. The QALB shared task on automatic text correction of Arabic, 1 http://nlp.qatar.cmu.edu/qalb/ 26 Proceedings of the Second Workshop on Arabic Natural Language Processing, pages 26–35, c Beijing, China, July 26-31, 2015. 2014 Association for Computational Linguistics 2 Task Description The texts are manually annotated for errors by native Arabic speakers. The annotation begins with an initial automatic pre-processing step. Next, the files are processed with the morphological analysis and"
W15-3204,C12-2011,0,0.0931945,"Missing"
W15-3204,W14-3605,1,0.841304,"al Learning Systems, Columbia University 2 Carnegie Mellon University in Qatar 3 New York University Abu Dhabi 4 Ask.com alla@ccls.columbia.edu,hbouamor@qatar.cmu.edu,nizar.habash@nyu.edu wajdiz@qatar.cmu.edu,owo@qatar.cmu.edu,behrang@cmu.edu Abstract organized within the framework of the Qatar Arabic Language Bank (QALB) project,1 is the first effort aimed at constructing a benchmark data set, which will allow for development and evaluation of automatic correction systems for Arabic. In this paper, we present a summary of the second edition of the QALB competition. The first one – QALB-2014 (Mohit et al., 2014) – took place in conjunction with the Arabic NLP workshop at EMNLP-2014 and focused on errors found in online commentaries produced by native speakers of Arabic. QALB-2014 attracted a lot of attention and resulted in nine systems being submitted with a variety of approaches that included rule-based frameworks, machine-learning classifiers, and statistical machine translation methods. This year’s competition extends the first edition by adding another track that focuses on errors found in essays written by learners of Arabic. Eight teams participated in the competition this year, including seve"
W15-3204,W15-3216,0,0.0195413,"Missing"
W15-3204,W15-3220,0,0.0483413,"Missing"
W15-3204,W15-3217,1,0.853627,"Missing"
W15-3204,W15-3218,0,0.0309508,"Missing"
W15-3204,W15-3221,1,0.873886,"Missing"
W15-3204,W15-3215,0,0.125217,"Missing"
W15-3204,N12-1067,0,0.0704061,"ariety of techniques. For example, the CUFE system extracted rules from the morphological analyzer and learned their probabilities using the training data, while the UMMU system combined statistical machine6 Results In this section, we present the results of the competition. As was done in QALB-2014, we adopted the standard Precision (P), Recall (R), and F1 metric. This metric was also used in recent shared tasks on grammatical error correction in English: HOO competitions (Dale and Kilgarriff, 2011; Dale et al., 2012) and CoNLL (Ng et al., 2013). The results are computed using the M2 scorer (Dahlmeier and Ng, 2012) that was also used in the CoNLL shared tasks. Tables 8 and 9 present the official results of the evaluation on the test sets for the Aljazeera data and the L2 data, respectively. The results are sorted according to the F1 scores obtained by the 30 Rank 1 2 3 4 5 6 7 8 9 10 11 12 Team CUFE UMMU-1 GWU UMMU-2 QCRI QCMUQ TECH-2 TECH-1 TECH-3 ARIB-1 ARIB-2 SAHSOH MADAMIRA P 88.85 70.28 74.69 72.69 84.74 71.39 71.20 71.08 69.99 64.50 67.56 81.88 80.32 R 61.76 71.93 67.51 67.52 58.10 65.13 64.94 64.74 60.41 56.50 51.61 40.24 39.98 F1 72.87 71.10 70.92 70.01 68.94 68.12 67.93 67.76 64.85 60.23 58.52"
W15-3204,W11-2838,0,0.388272,"uage. The report includes an overview of the QALB corpus, which is the dataset used for training and evaluation, an overview of participating systems, results of the competition and an analysis of the results and systems. 1 Introduction The task of text correction has recently been attracting a lot of attention in the Natural Language Processing (NLP) community, but most of the effort in this area concentrated on English, especially on errors made by learners of English as a Second Language. Four competitions devoted to error correction for non-native English writers took place recently: HOO (Dale and Kilgarriff, 2011; Dale et al., 2012) and CoNLL (Ng et al., 2013; Ng et al., 2014). Shared tasks of this kind are extremely important, as they bring together researchers and promote the development of relevant techniques and dissemination of key resources, such as benchmark data sets. In the area of Arabic text correction, there has been a significant body of work, as well (Shaalan et al., 2003; Hassan et al., 2008). However, due to the lack of a common benchmark data set, making progress on this task has been difficult. The QALB shared task on automatic text correction of Arabic, 1 http://nlp.qatar.cmu.edu/qa"
W15-3204,I13-2001,1,0.600185,"ts are compared against gold annotations using Precision, Recall and F1 . Systems are ranked based on the F1 scores obtained on the test sets. 3 The QALB Corpus The QALB corpus was created as part of the QALB project. One of the goals of the QALB project is to develop a large manually corrected corpus for a variety of Arabic texts, including texts produced by native and non-native writers, as well as machine translation output. Within the framework of this project, comprehensive annotation guidelines and a specialized web-based annotation interface have been developed (Zaghouani et al., 2014; Obeid et al., 2013; Zaghouani et al., 2015a). 2 In the shared task, we specified two Add categories: add_before and add_after. Most of the add errors fall into the first category, and we combine these here into a single Add category. 3 Arabic transliteration is presented in the Habash-SoudiBuckwalter scheme (Habash et al., 2007): (in alphabetical ˇ order) AbtθjHxdðrzsšSDTDςγfqklmnhwy and the additional ˇ ¯ symbols: ’ Z, Â @, A @ , A @, wˆ ð', yˆ Zø', ¯h è, ý ø. 4 Tables 1 and 2, and the appendix are reproduced from Mohit et al. (2014) to help explain the format of the files used in QALB-2014 and QALB-2015 sha"
W15-3204,W12-2006,0,0.289248,"n overview of the QALB corpus, which is the dataset used for training and evaluation, an overview of participating systems, results of the competition and an analysis of the results and systems. 1 Introduction The task of text correction has recently been attracting a lot of attention in the Natural Language Processing (NLP) community, but most of the effort in this area concentrated on English, especially on errors made by learners of English as a Second Language. Four competitions devoted to error correction for non-native English writers took place recently: HOO (Dale and Kilgarriff, 2011; Dale et al., 2012) and CoNLL (Ng et al., 2013; Ng et al., 2014). Shared tasks of this kind are extremely important, as they bring together researchers and promote the development of relevant techniques and dissemination of key resources, such as benchmark data sets. In the area of Arabic text correction, there has been a significant body of work, as well (Shaalan et al., 2003; Hassan et al., 2008). However, due to the lack of a common benchmark data set, making progress on this task has been difficult. The QALB shared task on automatic text correction of Arabic, 1 http://nlp.qatar.cmu.edu/qalb/ 26 Proceedings o"
W15-3204,W12-5611,0,0.0600366,"Missing"
W15-3204,pasha-etal-2014-madamira,1,0.852468,"Missing"
W15-3204,zaghouani-etal-2014-large,1,0.849061,"workshop at EMNLP-2014 (Mohit et al., 2014). QALB-2014 addressed errors in online user comments written to Aljazeera articles by native Arabic speakers. This year’s competition includes two tracks – native and non-native. In addition to the Aljazeera commentaries written by native speakers, it also includes texts produced by learners of Arabic as a foreign language (L2). Both the native and the non-native data is written in Modern Standard Arabic and is part of the QALB corpus (see Section 3), a manuallycorrected collection of Arabic texts. The Aljazeera section of the corpus is presented in Zaghouani et al. (2014). The L2 data is extracted from two learner corpora of Arabic – the Arabic Learners Written Corpus (ALWC) (Farwaneh and Tamimi, 2012) and the Arabic Learner Corpus (ALC) (Alfaifi and Atwell, 2012). For details about the L2 data, we refer the reader to Zaghouani et al. (2015a). The shared task participants were provided with training and development data to build their systems, but were also free to make use of additional resources, including corpora, linguistic resources, and software, as long as these were publicly available. For evaluation, a standard framework developed for similar error co"
W15-3204,W15-1614,1,0.759406,"nd the English translation. The errors in the original and the corrected forms are underlined and co-indexed. Table 2 presents a subset of the errors for the example shown in Table 1 along with the error types and annotation actions. The Appendix at the end of the paper lists all annotation actions for that example.4 Essays written by L2 speakers differ from the native texts both because of the genre and the types of mistakes. For this reason, the general QALB L1 annotation guidelines were extended by adding new rules describing the error correction procedure in texts produced by L2 speakers (Zaghouani et al., 2015a). Because the genres are different, the writing styles exhibit different distributions of words, phrases, and structures. Further, while native texts mostly contain orthographic and punctuation mistakes, non-native writings also reveal lexical choice errors, missing and extraneous words (e.g. articles, prepositions), and mistakes in word The QALB-2015 shared task extends QALB2014, the first shared task on Arabic text correction that was created as a forum for competition and collaboration on automatic error correction in Modern Standard Arabic and took place in conjunction with the Arabic NL"
W15-3204,W15-3219,1,0.765882,"Missing"
W15-3209,2007.mtsummit-papers.20,1,0.567467,"stances of a word type are observed in a corpus, and (2) ambiguity where a word has multiple readings or interpretations. Undiacritized surface forms of an Arabic word might have as many as 200 readings depending on the complexity of its morphology. The lack of diacritics usually leads to considerable lexical ambiguity, as shown in the example in Table 1, a reason for which diacritization, aka vowel/diacritic restoration, has been shown to improve state-of-the-art Arabic automatic systems such as speech recognition (ASR) (Kirchhoff and Vergyri, 2005) and statistical machine translation (SMT) (Diab et al., 2007). Hence, diacritization has been receiving increased attention in several Arabic NLP applications. In general, building models to assign diacritics to each letter in a word requires a large amount of annotated training corpora covering different topics and domains to overcome the sparseness problem. The currently available diacritized MSA corpora are generally limited to the newswire genres (as distributed by the LDC) or religion related texts such as the Quran or the Tashkeela corpus.2 In this paper we present a pilot study where we annotate a sample of non-diacritized text extracted from fiv"
W15-3209,P06-1073,0,0.583986,"lts show that readers benefited from the disambiguating diacritics. This study was a MIN scheme exploration focused on heterophonic-homographic target verbs that have different pronunciations in active and Related Work The task of diacritization is about adding diacritics to the canonical underspecified written form. This task has been discussed in several research works in various NLP areas addressing various applications. Automatic Arabic Diacritization Much work has been done on recovery of diacritics over the past two decades by developing automatic methods yielding acceptable accuracies. Zitouni et al. (2006) built a diacritization framework based on such as number, gender, aspect, voice, etc. Whereas a lemma is a conventionalized citation form. 82 ATB News ATB BN ATB WebLog Tashkeela Wikipedia Total Size in words 2,478 3,093 3,177 5,172 2,850 16,770 GOLD annotation Yes Yes Yes Yes No - Table 2: The size of the data for annotation per corpus genre passive. classical Arabic books). This corpus contains over 6 million words fully diacritized. For our study we include a subset of 5k words from this corpus. In this work we are interested in two components: annotating large amounts of varied genres typ"
W15-3209,P05-1071,0,0.0659168,"n between Form I and Form II of Arabic verb derivations. Form II, indicates, in most cases, added causativity to the Form I meaning. Form II is marked by doubling the second rad ical of the root used in Form I: É¿ @/Akal/’ate’ maximum entropy classification to restore missing diacritics on each letter in a given word. Vergyri and Kirchhoff (2004) worked on automatic diacritization with the goal of improving automatic speech recognition (ASR). Different algorithms for diacritization based mainly on morphological analysis and lexeme-based language models were developed (Habash and Rambow, 2007; Habash and Rambow, 2005; Roth et al., 2008). Various approaches combining morphological analysis and/or Hidden Markov Models for automatic diacritization are found in the literature (Bebah et al., 2014; Alghamdi and Muzaffar, 2007; Rashwan et al., 2009). Rashwan et al. (2009) designed a stochastic Arabic diacritizer based on a hybrid of factorized and un-factorized textual features to automatically diacritize raw Arabic text. Emam and Fischer (2011) introduced a hierarchical approach for diacritization based on a search method in a set of dictionaries of sentences, phrases and words, using a top down strategy. More"
W15-3209,N07-2014,0,0.648924,"acritic is the distinction between Form I and Form II of Arabic verb derivations. Form II, indicates, in most cases, added causativity to the Form I meaning. Form II is marked by doubling the second rad ical of the root used in Form I: É¿ @/Akal/’ate’ maximum entropy classification to restore missing diacritics on each letter in a given word. Vergyri and Kirchhoff (2004) worked on automatic diacritization with the goal of improving automatic speech recognition (ASR). Different algorithms for diacritization based mainly on morphological analysis and lexeme-based language models were developed (Habash and Rambow, 2007; Habash and Rambow, 2005; Roth et al., 2008). Various approaches combining morphological analysis and/or Hidden Markov Models for automatic diacritization are found in the literature (Bebah et al., 2014; Alghamdi and Muzaffar, 2007; Rashwan et al., 2009). Rashwan et al. (2009) designed a stochastic Arabic diacritizer based on a hybrid of factorized and un-factorized textual features to automatically diacritize raw Arabic text. Emam and Fischer (2011) introduced a hierarchical approach for diacritization based on a search method in a set of dictionaries of sentences, phrases and words, using a"
W15-3209,maamouri-etal-2008-enhancing,0,0.396831,"very text genre, two annotators were asked to annotate independently a sample of 100 words. We measured the IAA between two annotators by averaging WER (Word Error Rate) over all pairs of words. The higher the WER between two annotations, the lower their agreement. The results given in Table 5, show clearly that the Advanced mode is the best strategy to adopt for this diacritization task. It is the less confusing method on all text genres (with WER between 1.56 and 5.58). We note that Wiki annotations in Advanced mode garner the highest IAA with a very low WER. We extended the LDC guidelines (Maamouri et al., 2008) by adding some diacritization rules: The shadda mark should not be added to the definite article (e.g., àñÒJ ÊË@/’lemon’ and not àñÒJ ÊË@); The sukuun sign should not be indicated at the end /’from’); The letters folof silent words (e.g., áÓ lowed by a long Alif, should not be diacritized  as it is a deterministic diacritization ( Y« @ñ ®Ë@/’the rules’); Abbreviations are not diacritized ( Õ»/’km’, /’kg’). Ñª» We also added an appendix that sumWe measure the reliability of the annotations by comparing them against gold standard annotations. In order to build the gold Wiki annotations, we hi"
W15-3209,pasha-etal-2014-madamira,1,0.916558,"Missing"
W15-3209,P08-2030,1,0.710351,"II of Arabic verb derivations. Form II, indicates, in most cases, added causativity to the Form I meaning. Form II is marked by doubling the second rad ical of the root used in Form I: É¿ @/Akal/’ate’ maximum entropy classification to restore missing diacritics on each letter in a given word. Vergyri and Kirchhoff (2004) worked on automatic diacritization with the goal of improving automatic speech recognition (ASR). Different algorithms for diacritization based mainly on morphological analysis and lexeme-based language models were developed (Habash and Rambow, 2007; Habash and Rambow, 2005; Roth et al., 2008). Various approaches combining morphological analysis and/or Hidden Markov Models for automatic diacritization are found in the literature (Bebah et al., 2014; Alghamdi and Muzaffar, 2007; Rashwan et al., 2009). Rashwan et al. (2009) designed a stochastic Arabic diacritizer based on a hybrid of factorized and un-factorized textual features to automatically diacritize raw Arabic text. Emam and Fischer (2011) introduced a hierarchical approach for diacritization based on a search method in a set of dictionaries of sentences, phrases and words, using a top down strategy. More recently, Abandah et"
W15-3209,W04-1612,0,0.67965,"kAtib/’writer’ and I.KA¿/kAtab/’to correspond’ distinguishes between the meanings of the word (lexical disambiguation) rather than their inflections. Any of diacritics may be used to mark lexical variation. A common example with the shadda (gemination) diacritic is the distinction between Form I and Form II of Arabic verb derivations. Form II, indicates, in most cases, added causativity to the Form I meaning. Form II is marked by doubling the second rad ical of the root used in Form I: É¿ @/Akal/’ate’ maximum entropy classification to restore missing diacritics on each letter in a given word. Vergyri and Kirchhoff (2004) worked on automatic diacritization with the goal of improving automatic speech recognition (ASR). Different algorithms for diacritization based mainly on morphological analysis and lexeme-based language models were developed (Habash and Rambow, 2007; Habash and Rambow, 2005; Roth et al., 2008). Various approaches combining morphological analysis and/or Hidden Markov Models for automatic diacritization are found in the literature (Bebah et al., 2014; Alghamdi and Muzaffar, 2007; Rashwan et al., 2009). Rashwan et al. (2009) designed a stochastic Arabic diacritizer based on a hybrid of factorize"
W15-3217,W08-0509,0,0.0366323,"Missing"
W15-3217,I08-2131,0,0.527985,"uction With the increased usage of computers in the processing of various languages comes the need for correcting errors introduced at different stages. Hence, the topic of text correction has seen a lot of interest in the past several years (Haddad and Yaseen, 2007; Rozovskaya et al., 2013). Numerous approaches have been explored to correct spelling errors in texts using NLP tools and resources (Kukich, 1992; Oflazer, 1996). The spelling correction for Arabic is an understudied problem in comparison to English, although small amount of research has been done previously (Shaalan et al., 2003; Hassan et al., 2008). The reason for this is the complexity of Arabic language and unavailability of language resources. For example, the Arabic spell checker in Microsoft Word gives incorrect suggests for even simple errors. First shared task on automatic Arabic text 2 Data Resources QALB: We trained and evaluated our system using the data provided for the shared task and the m2Scorer (Dahlmeier and Ng, 2012). These datasets are extracted from the QALB corpus of human-edited Arabic text produced by native speakers, non-native speakers and machines (Zaghouani et al., 2014). The corpus contains a large 144 Proceed"
W15-3217,E09-2008,0,0.0206763,"r for a regular set over a, b described by the regular expression (aba + bab)*, and we want to recognize the inputs that are slightly corrupted, for example, abaaaba may be matched to abaaba (correcting for a spurious a), or babbb may be matched to babbab (correcting for a deletion), or ababba may be matched to either abaaba (correcting a b to an a) or to ababab (correcting the reversal of the last two symbols). This method is perfect for handling mainly transposition errors resulting from swapping two letters , or typing errors of neighboring letters in the keyboard. We use the Foma library (Hulden, 2009) to build the finite-state tranducer using the Arabic Word-list as a dictionary.4 For each word, our system checks if the word is analyzed and recognized by the finite-state transducer. It then generates a list of correction candidates for the nonrecognized ones. The candidates are words having an edit distance lower than a certain threshold. We score the different candidates using a LM and consider the best one as the possible correction for each word. 4 ous system configurations on the L2 dev and test 2014 sets are given in Table 3. The results clearly show different modules are complementry"
W15-3217,C12-2011,0,0.494671,"Missing"
W15-3217,W14-3618,1,0.86265,"Missing"
W15-3217,N12-1067,0,0.163355,"Missing"
W15-3217,W14-3620,0,0.536639,"Missing"
W15-3217,P07-2045,0,0.00732275,"Missing"
W15-3217,W14-3309,1,0.929547,"f the statistics reported in Table 1 is taken from Diab et al. (2014) 2 The list is freely available at: http: //sourceforge.net/projects/ arabic-wordlist/ 145 Original Target English Characters à@ ñë ... à @ ñë ‘... Source Source Target H. ñKñJ Ë @ H. ñKñJ Ë@ ú ¯ ú ¯ éKYëA éKYëA ø YË @ ø YË@ which I have seen in Youtube is that è H X è @  # ø X È @ ... # à @# ð è# H. ð H ð ø È @# ø ¬#    ... # à @# ð è# H. ð H ð ø È @# ø ¬# è H X è @ # ø X È @ Table 2: Preparing the training and tuning and test corpus for alignment 3.2 Rule-based Corrector (Rules) verse (Sajjad et al., 2013a; Durrani et al., 2014a). The conversion of Arabic dialects to MSA at character-level can be seen as a spelling correction task where small character-level changes are made to convert a dialectal word into an MSA word. We also formulate our correction problem as a character-level machine translation problem, where the pre-processed incorrect Arabic text is considered as the source, and our target is the correct Arabic text provided by the Shared task organizers. The goal is to learn correspondences between errors and their corrections. All the train data is used to train our the phrase-based model. We treat sentenc"
W15-3217,W14-3605,0,0.0929078,"Missing"
W15-3217,E14-4029,1,0.927876,"f the statistics reported in Table 1 is taken from Diab et al. (2014) 2 The list is freely available at: http: //sourceforge.net/projects/ arabic-wordlist/ 145 Original Target English Characters à@ ñë ... à @ ñë ‘... Source Source Target H. ñKñJ Ë @ H. ñKñJ Ë@ ú ¯ ú ¯ éKYëA éKYëA ø YË @ ø YË@ which I have seen in Youtube is that è H X è @  # ø X È @ ... # à @# ð è# H. ð H ð ø È @# ø ¬#    ... # à @# ð è# H. ð H ð ø È @# ø ¬# è H X è @ # ø X È @ Table 2: Preparing the training and tuning and test corpus for alignment 3.2 Rule-based Corrector (Rules) verse (Sajjad et al., 2013a; Durrani et al., 2014a). The conversion of Arabic dialects to MSA at character-level can be seen as a spelling correction task where small character-level changes are made to convert a dialectal word into an MSA word. We also formulate our correction problem as a character-level machine translation problem, where the pre-processed incorrect Arabic text is considered as the source, and our target is the correct Arabic text provided by the Shared task organizers. The goal is to learn correspondences between errors and their corrections. All the train data is used to train our the phrase-based model. We treat sentenc"
W15-3217,J03-1002,0,0.00661281,"Missing"
W15-3217,J96-1003,1,0.68369,"and yeilds better correction quality with an F-score of 68.12 on L1test-2015 testset and 38.90 on the L2-test2015. This ranks us 2nd in the L2 subtask and 5th in the L1 subtask. 1 Introduction With the increased usage of computers in the processing of various languages comes the need for correcting errors introduced at different stages. Hence, the topic of text correction has seen a lot of interest in the past several years (Haddad and Yaseen, 2007; Rozovskaya et al., 2013). Numerous approaches have been explored to correct spelling errors in texts using NLP tools and resources (Kukich, 1992; Oflazer, 1996). The spelling correction for Arabic is an understudied problem in comparison to English, although small amount of research has been done previously (Shaalan et al., 2003; Hassan et al., 2008). The reason for this is the complexity of Arabic language and unavailability of language resources. For example, the Arabic spell checker in Microsoft Word gives incorrect suggests for even simple errors. First shared task on automatic Arabic text 2 Data Resources QALB: We trained and evaluated our system using the data provided for the shared task and the m2Scorer (Dahlmeier and Ng, 2012). These dataset"
W15-3217,pasha-etal-2014-madamira,0,0.131692,"Missing"
W15-3217,W13-3602,0,0.0694364,"We trained and tested our spelling corrector using the dataset provided by the shared task organizers. Our system outperforms the baseline system and yeilds better correction quality with an F-score of 68.12 on L1test-2015 testset and 38.90 on the L2-test2015. This ranks us 2nd in the L2 subtask and 5th in the L1 subtask. 1 Introduction With the increased usage of computers in the processing of various languages comes the need for correcting errors introduced at different stages. Hence, the topic of text correction has seen a lot of interest in the past several years (Haddad and Yaseen, 2007; Rozovskaya et al., 2013). Numerous approaches have been explored to correct spelling errors in texts using NLP tools and resources (Kukich, 1992; Oflazer, 1996). The spelling correction for Arabic is an understudied problem in comparison to English, although small amount of research has been done previously (Shaalan et al., 2003; Hassan et al., 2008). The reason for this is the complexity of Arabic language and unavailability of language resources. For example, the Arabic spell checker in Microsoft Word gives incorrect suggests for even simple errors. First shared task on automatic Arabic text 2 Data Resources QALB:"
W15-3217,W14-3622,0,0.351338,"Missing"
W15-3217,P13-2001,1,0.907081,"Missing"
W15-3217,2013.iwslt-evaluation.8,1,0.897486,"Missing"
W15-3217,zaghouani-etal-2014-large,1,\N,Missing
W15-3217,W11-2123,0,\N,Missing
W15-3221,C12-2011,0,0.660907,"Missing"
W15-3221,W14-3605,0,0.0432084,"with possible alternatives for each misspelled word using the Damerau-Levenshtein edit distance, then the right alternative for each misspelled word is selected stochastically using a lattice search, and an n-gram method. Shaalan et al. (2012) trained a Noisy Channel Model on word-based unigrams to detect and correct spelling errors. Dahlmeier and Ng (2012) built specialized decoders for English grammatical error correction. The goal of the QALB shared task is developing the of an automatic system for Arabic Error Correction. The QALB-2015 task is the extension of the first QALB shared task (Mohit et al., 2014) that took place last year. The QALB-2014 addressed errors in comments written to Aljazeera articles by native Arabic speakers (Zaghouani et al., 2014). This year’s competition includes two tracks, and, in addition to errors produced by native speakers, also includes correction of texts written by learners of Arabic as a foreign language (L2) (Zaghouani et al., 2015). The native track includes Alj-train-2014, Alj-dev-2014, Alj-test-2014 texts from QALB-2014. The L2 track includes L2train-2015 and L2-dev-2015. This data was released for the development of the systems. The systems were scored on"
W15-3221,D12-1052,0,0.0166011,"Task Description the optimal subset to train their system. Alkanhal et al. (2012) presented a stochastic approach for spelling correction of Arabic text. They used a context-based system to automatically correct misspelled words. First of all, a list is generated with possible alternatives for each misspelled word using the Damerau-Levenshtein edit distance, then the right alternative for each misspelled word is selected stochastically using a lattice search, and an n-gram method. Shaalan et al. (2012) trained a Noisy Channel Model on word-based unigrams to detect and correct spelling errors. Dahlmeier and Ng (2012) built specialized decoders for English grammatical error correction. The goal of the QALB shared task is developing the of an automatic system for Arabic Error Correction. The QALB-2015 task is the extension of the first QALB shared task (Mohit et al., 2014) that took place last year. The QALB-2014 addressed errors in comments written to Aljazeera articles by native Arabic speakers (Zaghouani et al., 2014). This year’s competition includes two tracks, and, in addition to errors produced by native speakers, also includes correction of texts written by learners of Arabic as a foreign language ("
W15-3221,J96-1003,0,0.0236878,"rror correction system was presented recently by Jeblee et al. (2014). In contrast to their approach, our system combines two level MT models: character level, then a word level. Related Work Automatic error detection and correction include automatic spelling checking, grammar checking and post-editing. Numerous approaches (both supervised and unsupervised) have been explored to improve the fluency of the text and reduce the percentage of out-of-vocabulary words using NLP tools, resources, and heuristics, e.g., morphological analyzers, language models, and edit-distance measure (Kukich, 1992; Oflazer, 1996; Zribi and Ben Ahmed, 2003; Shaalan et al., 2003; Haddad and Yaseen, 2007; Hassan et al., 2008; Habash, 2008; Shaalan et al., 2010). There has been a lot of work on error correction for English (e.g., (Golding and Roth, 1999)). For Arabic, this issue was studied in various directions and in different research work. In 2003, Shaalan et al. (2003) presented work on the specification and classification of spelling errors in Arabic. Later on, Haddad and Yaseen (2007) presented a hybrid approach using morphological features and rules to fine tune the word recognition and non-word correction method"
W15-3221,pasha-etal-2014-madamira,0,0.138145,"Missing"
W15-3221,P08-2015,0,0.0285458,"em combines two level MT models: character level, then a word level. Related Work Automatic error detection and correction include automatic spelling checking, grammar checking and post-editing. Numerous approaches (both supervised and unsupervised) have been explored to improve the fluency of the text and reduce the percentage of out-of-vocabulary words using NLP tools, resources, and heuristics, e.g., morphological analyzers, language models, and edit-distance measure (Kukich, 1992; Oflazer, 1996; Zribi and Ben Ahmed, 2003; Shaalan et al., 2003; Haddad and Yaseen, 2007; Hassan et al., 2008; Habash, 2008; Shaalan et al., 2010). There has been a lot of work on error correction for English (e.g., (Golding and Roth, 1999)). For Arabic, this issue was studied in various directions and in different research work. In 2003, Shaalan et al. (2003) presented work on the specification and classification of spelling errors in Arabic. Later on, Haddad and Yaseen (2007) presented a hybrid approach using morphological features and rules to fine tune the word recognition and non-word correction method. In order to build an Arabic spelling checker, Attia et al. (2012) developed semi-automatically, a dictionar"
W15-3221,I08-2131,0,0.618999,"ir approach, our system combines two level MT models: character level, then a word level. Related Work Automatic error detection and correction include automatic spelling checking, grammar checking and post-editing. Numerous approaches (both supervised and unsupervised) have been explored to improve the fluency of the text and reduce the percentage of out-of-vocabulary words using NLP tools, resources, and heuristics, e.g., morphological analyzers, language models, and edit-distance measure (Kukich, 1992; Oflazer, 1996; Zribi and Ben Ahmed, 2003; Shaalan et al., 2003; Haddad and Yaseen, 2007; Hassan et al., 2008; Habash, 2008; Shaalan et al., 2010). There has been a lot of work on error correction for English (e.g., (Golding and Roth, 1999)). For Arabic, this issue was studied in various directions and in different research work. In 2003, Shaalan et al. (2003) presented work on the specification and classification of spelling errors in Arabic. Later on, Haddad and Yaseen (2007) presented a hybrid approach using morphological features and rules to fine tune the word recognition and non-word correction method. In order to build an Arabic spelling checker, Attia et al. (2012) developed semi-automaticall"
W15-3221,W14-3618,1,0.530415,"ludes Alj-train-2014, Alj-dev-2014, Alj-test-2014 texts from QALB-2014. The L2 track includes L2train-2015 and L2-dev-2015. This data was released for the development of the systems. The systems were scored on blind test sets Alj-test2015 and L2-test-2015. 3 More recently, (Pasha et al., 2014) created MADAMIRA, a system for morphological analysis and disambiguation of Arabic, this system can be used to improve the accuracy of spelling checking system especially with Hamza spelling correction. A statistical machine translation model to train an error correction system was presented recently by Jeblee et al. (2014). In contrast to their approach, our system combines two level MT models: character level, then a word level. Related Work Automatic error detection and correction include automatic spelling checking, grammar checking and post-editing. Numerous approaches (both supervised and unsupervised) have been explored to improve the fluency of the text and reduce the percentage of out-of-vocabulary words using NLP tools, resources, and heuristics, e.g., morphological analyzers, language models, and edit-distance measure (Kukich, 1992; Oflazer, 1996; Zribi and Ben Ahmed, 2003; Shaalan et al., 2003; Hadda"
W15-3221,P07-2045,0,0.00336121,"stem is to find the correction c∗ defined as : 6.1 Data description All our models are built using training, development and testing data provided by the shared task organizers and described in Table 1. Alj-train-2014 L2-train-2015 Alj-dev-2014 Alj-test-2014 L2-dev-2015 c∗ = arg maxc p(c|e) = arg maxc p(e|c)p(c) p(e|c) is estimated in a translation model and p(c) is the target-side language model. The argmax is the task of the decoder and it represent the search for the best hypothesis in the space of possible correction c. The translation system is trained using the well known MOSES toolkit (Koehn et al., 2007). The system is constructed using data produced for the QALB shared task and described in Table 1 as follows: First, we generate the correct sentences2 of the QALB training corpus then translation and reordering models are trained. The language model (LM) is trained on the correct side of the QALB data and a selected part of the Arabic Gigaword corpus. # sentences 19,411 310 1,017 968 154 # tokens 1.1M 46.3k 58.9K 56.1k 26.3k Table 1: Train, dev and test data distribution 6.2 Baseline: MADAMIRA corrections MADAMIRA (?) is a tool, originally designed for morphological analysis and disambiguatio"
W15-3221,shaalan-etal-2012-arabic,0,\N,Missing
W15-3221,zaghouani-etal-2014-large,0,\N,Missing
W15-3221,W15-1614,1,\N,Missing
W16-4115,J11-4004,0,0.0319415,"ding of the annotation task. On the other hand, implicit ambiguity refers to those revealed after observing and contrasting the annotation done in the same task by other annotators. Annotators are generally asked to detect and resolve ambiguous cases, which can be a difficult task to accomplish. This leads to a lower inter-annotator agreement in such tasks. 2.3 Annotation Complexity There are many studies that evaluate the language complexity in addition to the quality of manual annotation and also allow the identification of many factors causing lower inter-annotator agreements. For example, Bayerl and Paul (2011) showed that there is a correlation between the inter-annotator agreement and the complexity of the annotation task; for instance, the larger the number of categories is, the lower the inter-annotator agreement is. Moreover, the categories prone to confusions are generally limited. This brings out two complexity issues related to the number of categories and to the existence of ambiguity between some the categories as explained in (Popescu-Belis, 2007). Furthermore, there are some annotation tasks for which the choice of a label is entirely left to the annotator, which can lead to even more co"
W16-4115,D15-1274,0,0.0235161,"ng ambiguity. In Arabic, diacritics are marks that reflect the phonological, morphological and grammatical rules. The lack of diacritics leads usually to considerable lexical and morphological ambiguity. Full diacritization has been shown to improve state-of-the-art Arabic automatic systems such as automatic speech recognition (ASR) systems (Kirchhoff and Vergyri, 2005) and statistical machine translation (SMT) (Diab et al., 2007). Hence, diacritization has been receiving increased attention in several Arabic NLP applications (Zitouni et al., 2006; Shahrour et al., 2015; Abandah et al., 2015; Belinkov and Glass, 2015). Building models to assign diacritics to each letter in a word requires a large amount of annotated training corpora covering different topics and domains to overcome the sparseness problem. The currently available MSA diacritized corpora are generally limited to religious texts such as the Holy Quran, educational texts or newswire stories distributed by the Linguistic Data Consortium. This paper presents a work carried out within a project to create an optimal diacritization scheme for Arabic orthographic representation (OptDiac) project (Zaghouani et al., 2016a; Bouamor et al., 2015). The o"
W16-4115,W15-3209,1,0.748269,"5; Belinkov and Glass, 2015). Building models to assign diacritics to each letter in a word requires a large amount of annotated training corpora covering different topics and domains to overcome the sparseness problem. The currently available MSA diacritized corpora are generally limited to religious texts such as the Holy Quran, educational texts or newswire stories distributed by the Linguistic Data Consortium. This paper presents a work carried out within a project to create an optimal diacritization scheme for Arabic orthographic representation (OptDiac) project (Zaghouani et al., 2016a; Bouamor et al., 2015). The overreaching goal of our project is to manually create a large-scale annotated corpus with the diacritics for a variety of Arabic texts. The creation of manually annotated corpora presents many challenges and issues related to the linguistic complexity of the Arabic language. In order to streamline the annotation process, we designed various annotation experimental conditions in order to answer the following questions: Can we automatically detect linguistic difficulties such as linguistic ambiguity? To what extent is there agreement between machines and human annotators when it comes to"
W16-4115,2007.mtsummit-papers.20,1,0.914733,"owels and diacritics rendering a mostly consonantal orthography (Schulz, 2004). Arabic diacritization is an orthographic way to describe Arabic word pronunciation, and avoid word reading ambiguity. In Arabic, diacritics are marks that reflect the phonological, morphological and grammatical rules. The lack of diacritics leads usually to considerable lexical and morphological ambiguity. Full diacritization has been shown to improve state-of-the-art Arabic automatic systems such as automatic speech recognition (ASR) systems (Kirchhoff and Vergyri, 2005) and statistical machine translation (SMT) (Diab et al., 2007). Hence, diacritization has been receiving increased attention in several Arabic NLP applications (Zitouni et al., 2006; Shahrour et al., 2015; Abandah et al., 2015; Belinkov and Glass, 2015). Building models to assign diacritics to each letter in a word requires a large amount of annotated training corpora covering different topics and domains to overcome the sparseness problem. The currently available MSA diacritized corpora are generally limited to religious texts such as the Holy Quran, educational texts or newswire stories distributed by the Linguistic Data Consortium. This paper presents"
W16-4115,palmer-etal-2008-pilot,1,0.782763,"raining of the annotators. This disagreement rate further increases due to the inherent natural ambiguity in the human language itself where various interpretations for a word are possible. Such linguistic ambiguity has been reported in many annotation projects involving various linguistic phenomenon, such as the coreference relations, the predicate-argument structure, the semantic roles and the L2 language errors (Versley and Tbingen, 2006; Iida et al., 2007), prosodic breaks (Jung and Kwon, 2011; Ruppenhofer et al., 2013; Rosen et al., 2013), as well as the various Arabic PropBank projects (Diab et al., 2008; Zaghouani et al., 2010; Zaghouani et al., 2012) and the Arabic TreeBank (Maamouri et al., 2010). Poesio and Artstein (2005) classify ambiguity into explicit and implicit types. The explicit ambiguity refers to the individuals’ understanding of the annotation task. On the other hand, implicit ambiguity refers to those revealed after observing and contrasting the annotation done in the same task by other annotators. Annotators are generally asked to detect and resolve ambiguous cases, which can be a difficult task to accomplish. This leads to a lower inter-annotator agreement in such tasks. 2."
W16-4115,2012.eamt-1.18,0,0.135091,"e analyses contain more than one possibility, the word is marked as ambiguous; otherwise, it is believed to be not ambiguous. Words that have no analysis generated using MADAMIRA are also considered ambiguous. For each sentence, we count the number of words that are marked as ambiguous using our approach, and then calculate the percentage of ambiguity. We sort the sentences according to their ambiguity percentages in descending order so that we give annotators ranked sentences for annotation. Because we are concerned with MSA dataset only, we further filter out dialectal sentences using AIDA (Elfardy and Diab, 2012), a tool that classifies words and sentences as MSA (formal Arabic) or DA (Dialectal Arabic). 5 Evaluation For the evaluation, we used a sample of 10K-Words from the CCA corpus representing 4 domains with approximately 2.5K-words per domain (children stories, economics,sports and politics). We have three experimental conditions for three evaluations carried over a period of six weeks. 1. The first condition (COND1): In the first experimental condition (COND1), four annotators were given raw undiacritized sentences and were asked to add the missing diacritics as per the guidelines. They either"
W16-4115,W07-1522,0,0.0406205,"l reasons that may cause disagreement in annotation decisions including human errors, lack of precision in the guidelines, and the lack of expertise and training of the annotators. This disagreement rate further increases due to the inherent natural ambiguity in the human language itself where various interpretations for a word are possible. Such linguistic ambiguity has been reported in many annotation projects involving various linguistic phenomenon, such as the coreference relations, the predicate-argument structure, the semantic roles and the L2 language errors (Versley and Tbingen, 2006; Iida et al., 2007), prosodic breaks (Jung and Kwon, 2011; Ruppenhofer et al., 2013; Rosen et al., 2013), as well as the various Arabic PropBank projects (Diab et al., 2008; Zaghouani et al., 2010; Zaghouani et al., 2012) and the Arabic TreeBank (Maamouri et al., 2010). Poesio and Artstein (2005) classify ambiguity into explicit and implicit types. The explicit ambiguity refers to the individuals’ understanding of the annotation task. On the other hand, implicit ambiguity refers to those revealed after observing and contrasting the annotation done in the same task by other annotators. Annotators are generally as"
W16-4115,W11-0405,0,0.0272132,"in annotation decisions including human errors, lack of precision in the guidelines, and the lack of expertise and training of the annotators. This disagreement rate further increases due to the inherent natural ambiguity in the human language itself where various interpretations for a word are possible. Such linguistic ambiguity has been reported in many annotation projects involving various linguistic phenomenon, such as the coreference relations, the predicate-argument structure, the semantic roles and the L2 language errors (Versley and Tbingen, 2006; Iida et al., 2007), prosodic breaks (Jung and Kwon, 2011; Ruppenhofer et al., 2013; Rosen et al., 2013), as well as the various Arabic PropBank projects (Diab et al., 2008; Zaghouani et al., 2010; Zaghouani et al., 2012) and the Arabic TreeBank (Maamouri et al., 2010). Poesio and Artstein (2005) classify ambiguity into explicit and implicit types. The explicit ambiguity refers to the individuals’ understanding of the annotation task. On the other hand, implicit ambiguity refers to those revealed after observing and contrasting the annotation done in the same task by other annotators. Annotators are generally asked to detect and resolve ambiguous ca"
W16-4115,maamouri-etal-2010-speech,1,0.827854,"ral ambiguity in the human language itself where various interpretations for a word are possible. Such linguistic ambiguity has been reported in many annotation projects involving various linguistic phenomenon, such as the coreference relations, the predicate-argument structure, the semantic roles and the L2 language errors (Versley and Tbingen, 2006; Iida et al., 2007), prosodic breaks (Jung and Kwon, 2011; Ruppenhofer et al., 2013; Rosen et al., 2013), as well as the various Arabic PropBank projects (Diab et al., 2008; Zaghouani et al., 2010; Zaghouani et al., 2012) and the Arabic TreeBank (Maamouri et al., 2010). Poesio and Artstein (2005) classify ambiguity into explicit and implicit types. The explicit ambiguity refers to the individuals’ understanding of the annotation task. On the other hand, implicit ambiguity refers to those revealed after observing and contrasting the annotation done in the same task by other annotators. Annotators are generally asked to detect and resolve ambiguous cases, which can be a difficult task to accomplish. This leads to a lower inter-annotator agreement in such tasks. 2.3 Annotation Complexity There are many studies that evaluate the language complexity in addition"
W16-4115,W12-2015,1,0.900794,"Missing"
W16-4115,W14-3605,1,0.871251,"Missing"
W16-4115,P06-1031,0,0.094128,"Missing"
W16-4115,I13-2001,1,0.828094,"or: the Shadda gemination mark, the Soukoun (absence of a vowel) and the Nunation marks at the end of a word. Moreover, in some cases, the letters followed by a long Alif  letter @, should not be diacritized as it is considered a deterministic diacritization as in AJJ Ó /miyvAqu/  ’Treaty’ and not AJJ Ó /miyvaAqu/.2 A summary of the most common Arabic diacritization rules is also added as a reference in the guidelines. 3.2 Annotation Tool We designed and implemented MANDIAC, a web-based annotation tool and a work-flow management interface (Obeid et al., 2016), the tool is based on QAWI (Obeid et al., 2013) a token-based editor, used to annotate and correct spelling errors in Arabic text for the Qatar Arabic Language Bank (QALB) project.3 The basic interface of the annotation tool is shown in Figure 1, apart from the surface controls, the interface allows annotators to select from an automatically generated diacritized words list and/or edit words manually as shown. The annotation interface allows users to undo/redo actions, and the history is kept over multiple sessions. The interface includes a timer to keep track of how long each sentence annotation has taken. We used the timer feature to mea"
W16-4115,pasha-etal-2014-madamira,1,0.873559,"Missing"
W16-4115,W10-1004,0,0.0710577,"Missing"
W16-4115,W15-3204,1,0.861846,"Missing"
W16-4115,D15-1152,0,0.0130569,"abic word pronunciation, and avoid word reading ambiguity. In Arabic, diacritics are marks that reflect the phonological, morphological and grammatical rules. The lack of diacritics leads usually to considerable lexical and morphological ambiguity. Full diacritization has been shown to improve state-of-the-art Arabic automatic systems such as automatic speech recognition (ASR) systems (Kirchhoff and Vergyri, 2005) and statistical machine translation (SMT) (Diab et al., 2007). Hence, diacritization has been receiving increased attention in several Arabic NLP applications (Zitouni et al., 2006; Shahrour et al., 2015; Abandah et al., 2015; Belinkov and Glass, 2015). Building models to assign diacritics to each letter in a word requires a large amount of annotated training corpora covering different topics and domains to overcome the sparseness problem. The currently available MSA diacritized corpora are generally limited to religious texts such as the Holy Quran, educational texts or newswire stories distributed by the Linguistic Data Consortium. This paper presents a work carried out within a project to create an optimal diacritization scheme for Arabic orthographic representation (OptDiac) project (Zagh"
W16-4115,W10-1836,1,0.835215,"tators. This disagreement rate further increases due to the inherent natural ambiguity in the human language itself where various interpretations for a word are possible. Such linguistic ambiguity has been reported in many annotation projects involving various linguistic phenomenon, such as the coreference relations, the predicate-argument structure, the semantic roles and the L2 language errors (Versley and Tbingen, 2006; Iida et al., 2007), prosodic breaks (Jung and Kwon, 2011; Ruppenhofer et al., 2013; Rosen et al., 2013), as well as the various Arabic PropBank projects (Diab et al., 2008; Zaghouani et al., 2010; Zaghouani et al., 2012) and the Arabic TreeBank (Maamouri et al., 2010). Poesio and Artstein (2005) classify ambiguity into explicit and implicit types. The explicit ambiguity refers to the individuals’ understanding of the annotation task. On the other hand, implicit ambiguity refers to those revealed after observing and contrasting the annotation done in the same task by other annotators. Annotators are generally asked to detect and resolve ambiguous cases, which can be a difficult task to accomplish. This leads to a lower inter-annotator agreement in such tasks. 2.3 Annotation Complexity"
W16-4115,W12-2511,1,0.86205,"t rate further increases due to the inherent natural ambiguity in the human language itself where various interpretations for a word are possible. Such linguistic ambiguity has been reported in many annotation projects involving various linguistic phenomenon, such as the coreference relations, the predicate-argument structure, the semantic roles and the L2 language errors (Versley and Tbingen, 2006; Iida et al., 2007), prosodic breaks (Jung and Kwon, 2011; Ruppenhofer et al., 2013; Rosen et al., 2013), as well as the various Arabic PropBank projects (Diab et al., 2008; Zaghouani et al., 2010; Zaghouani et al., 2012) and the Arabic TreeBank (Maamouri et al., 2010). Poesio and Artstein (2005) classify ambiguity into explicit and implicit types. The explicit ambiguity refers to the individuals’ understanding of the annotation task. On the other hand, implicit ambiguity refers to those revealed after observing and contrasting the annotation done in the same task by other annotators. Annotators are generally asked to detect and resolve ambiguous cases, which can be a difficult task to accomplish. This leads to a lower inter-annotator agreement in such tasks. 2.3 Annotation Complexity There are many studies th"
W16-4115,L16-1577,1,0.81745,"Missing"
W16-4115,L16-1295,1,0.886376,"Missing"
W16-4115,P06-1073,0,0.0303502,"hic way to describe Arabic word pronunciation, and avoid word reading ambiguity. In Arabic, diacritics are marks that reflect the phonological, morphological and grammatical rules. The lack of diacritics leads usually to considerable lexical and morphological ambiguity. Full diacritization has been shown to improve state-of-the-art Arabic automatic systems such as automatic speech recognition (ASR) systems (Kirchhoff and Vergyri, 2005) and statistical machine translation (SMT) (Diab et al., 2007). Hence, diacritization has been receiving increased attention in several Arabic NLP applications (Zitouni et al., 2006; Shahrour et al., 2015; Abandah et al., 2015; Belinkov and Glass, 2015). Building models to assign diacritics to each letter in a word requires a large amount of annotated training corpora covering different topics and domains to overcome the sparseness problem. The currently available MSA diacritized corpora are generally limited to religious texts such as the Holy Quran, educational texts or newswire stories distributed by the Linguistic Data Consortium. This paper presents a work carried out within a project to create an optimal diacritization scheme for Arabic orthographic representation"
W19-3822,P11-2062,1,0.89414,"Missing"
W19-3822,W19-3805,0,0.0359119,"Missing"
W19-3822,E17-2104,0,0.0155772,"ded, replacing each with its partner, while maintaining the same ground truth. The goal here is to encourage learning algorithms to not pick up on biased distinctions. Building on CDA, (Zmigrod et al., 2019) presented a generative model that allows conversion between masculine inflected and feminine inflected sentences in four morphologically rich languages (Hebrew, Spanish, French and Italian) with a focus on animate nouns. Specifically for MT, Rabinovich et al. (2016) presented work on the preservation of author gender. Some researchers suggested improvement through co-reference resolution (Gonzales and Tuggener, 2017; Luong and Popescu-Belis, 2016). Vanmassenhove et al. (2018) conducted a series of experiments to improve morphological agreement and improve translation quality in NMT systems for 20 language pairs (none of which were Arabic). They compiled large datasets from Europarl (Koehn, 2005), including speaker gender and age, and trained NMT systems with the tagged language pair. They showed that providing tags that indicate the speaker’s gender to the system leads to significant improvements. Similarly, Elaraby et al. (2018) marked speaker and listener gender as meta-data input on the source sentenc"
W19-3822,K17-2001,0,0.0532134,"Missing"
W19-3822,P17-4012,0,0.0197793,"e main reason for this setup is that character-level representations are reported to be good in capturing and learning morphological aspects (Ling et al., 2015; Kim et al., 2016), which is important for a morphologically rich language like Arabic. Furthermore, character-level NMT modeling requires less vocabulary and helps reduce out-of-vocabulary by translating unseen words. Our character-based NMT system is an encoder-decoder model that uses the general global attention architecture introduced by Luong and Manning (2015). All the NMT models we use have been trained with the OpenNMT toolkit (Klein et al., 2017) with no restriction on the input vocabulary size. Specifically, we use long short-term memory units (LSTM), with hidden units of size 500 and 2 layers in both the encoder and decoder. The model is trained for 13 epochs, using Adam with a learning rate of 0.002 and mini-batches of 40 with no pre-trained embeddings. Our char-level embeddings are learned within the training of the model. Using different combinations of the data sets presented in Section 4, we build four reinflection models. • in-to-M is a model trained to map from the Balanced Input corpus (and Synthetic F) to the TargetM corpus"
W19-3822,2010.jeptalnrecital-long.29,1,0.821127,"Missing"
W19-3822,W19-3821,0,0.0523214,"Missing"
W19-3822,L16-1147,0,0.174977,"of experiments to improve morphological agreement and improve translation quality in NMT systems for 20 language pairs (none of which were Arabic). They compiled large datasets from Europarl (Koehn, 2005), including speaker gender and age, and trained NMT systems with the tagged language pair. They showed that providing tags that indicate the speaker’s gender to the system leads to significant improvements. Similarly, Elaraby et al. (2018) marked speaker and listener gender as meta-data input on the source sentence in an English-to-Arabic NMT system. The training data came from OpenSubtitle (Lison and Tiedemann, 2016). The authors used rules to identify the gender in the Arabic text. Prates et al. (2018) used Google Translate to translate a set consisting of a list of jobs and gender-specific sentences from a variety of gender-neutral languages into English. They showed that occupations related to science, engineering and mathematics present a strong stereotype towards the male gender. More recently, Font and Costa-jussà (2019) studied the impact of gender debiasing on NMT between English and Spanish using debiased and gender-neutral word embeddings. Google Translate publicly announced an effort to address"
W19-3822,2015.iwslt-evaluation.11,0,0.0155322,"uences of characters rather than words and learn to encode and decode at the character-level. The main reason for this setup is that character-level representations are reported to be good in capturing and learning morphological aspects (Ling et al., 2015; Kim et al., 2016), which is important for a morphologically rich language like Arabic. Furthermore, character-level NMT modeling requires less vocabulary and helps reduce out-of-vocabulary by translating unseen words. Our character-based NMT system is an encoder-decoder model that uses the general global attention architecture introduced by Luong and Manning (2015). All the NMT models we use have been trained with the OpenNMT toolkit (Klein et al., 2017) with no restriction on the input vocabulary size. Specifically, we use long short-term memory units (LSTM), with hidden units of size 500 and 2 layers in both the encoder and decoder. The model is trained for 13 epochs, using Adam with a learning rate of 0.002 and mini-batches of 40 with no pre-trained embeddings. Our char-level embeddings are learned within the training of the model. Using different combinations of the data sets presented in Section 4, we build four reinflection models. • in-to-M is a"
W19-3822,W16-2202,0,0.0225837,"partner, while maintaining the same ground truth. The goal here is to encourage learning algorithms to not pick up on biased distinctions. Building on CDA, (Zmigrod et al., 2019) presented a generative model that allows conversion between masculine inflected and feminine inflected sentences in four morphologically rich languages (Hebrew, Spanish, French and Italian) with a focus on animate nouns. Specifically for MT, Rabinovich et al. (2016) presented work on the preservation of author gender. Some researchers suggested improvement through co-reference resolution (Gonzales and Tuggener, 2017; Luong and Popescu-Belis, 2016). Vanmassenhove et al. (2018) conducted a series of experiments to improve morphological agreement and improve translation quality in NMT systems for 20 language pairs (none of which were Arabic). They compiled large datasets from Europarl (Koehn, 2005), including speaker gender and age, and trained NMT systems with the tagged language pair. They showed that providing tags that indicate the speaker’s gender to the system leads to significant improvements. Similarly, Elaraby et al. (2018) marked speaker and listener gender as meta-data input on the source sentence in an English-to-Arabic NMT sy"
W19-3822,P02-1040,0,0.107643,"ttings: (a) the Balanced Input D EV and T EST, and (b) the English-to-Arabic Google Translate output of the English sentences corresponding the Balanced Input D EV and T EST, D EVGT and T ESTGT (Section 4). We evaluate sentence gender reinflection against the D EV and T EST portions of the TargetF and TargetM corpora as references (also, Section 4). In addition to the single and two-step system, we include a “do-nothing” baseline that simply passes the input to the output as is. Reinflection Evaluation Reinflection results for each setup are reported in Table 5 in terms of the MT metric BLEU (Papineni et al., 2002). It is important to note that all the reported scores are on AYT-normalized texts.6 This normalization helps reduce the number of cases in which Alif, Ya, and Ta Marbuta are inconsistently represented in the references. The table specifies columns for Target M, and Target F, which indicate which reference is used for evaluation. For the Balanced Input, the best performance was achieved using the two-step system. The BLEU scores are very high because most of the 6 AYT refers to the orthographic normalization of AlifHamza forms, Ya/Alif-Maqsura forms, and Ta-Marbuta/Ha forms (Habash, 2010) Bala"
W19-3822,pasha-etal-2014-madamira,1,0.873221,"Missing"
W19-3822,D18-1334,0,0.29945,"ame ground truth. The goal here is to encourage learning algorithms to not pick up on biased distinctions. Building on CDA, (Zmigrod et al., 2019) presented a generative model that allows conversion between masculine inflected and feminine inflected sentences in four morphologically rich languages (Hebrew, Spanish, French and Italian) with a focus on animate nouns. Specifically for MT, Rabinovich et al. (2016) presented work on the preservation of author gender. Some researchers suggested improvement through co-reference resolution (Gonzales and Tuggener, 2017; Luong and Popescu-Belis, 2016). Vanmassenhove et al. (2018) conducted a series of experiments to improve morphological agreement and improve translation quality in NMT systems for 20 language pairs (none of which were Arabic). They compiled large datasets from Europarl (Koehn, 2005), including speaker gender and age, and trained NMT systems with the tagged language pair. They showed that providing tags that indicate the speaker’s gender to the system leads to significant improvements. Similarly, Elaraby et al. (2018) marked speaker and listener gender as meta-data input on the source sentence in an English-to-Arabic NMT system. The training data came"
W19-3822,D18-1521,0,0.0536282,"rst-person-singular masculine on top of a state-of-the-art gender-blind MT system on a held-out test set. Next, we discuss some related work (Section 2) and Arabic linguistic facts (Section 3). We present our Arabic parallel gender corpus in Section 4, gender identification in Section 5, and gender reinflection and MT results in Section 6. 2 Related Work Gender bias has been detected, studied, and partially addressed for standard and contextualized word embeddings in a number of studies (Bolukbasi et al., 2016; Caliskan et al., 2017; Sutton et al., 2018; Basta et al., 2019; Garg et al., 2018; Zhao et al., 2018, 2019). These studies showed that training word embeddings on large human produced corpora such as news text leads to encoding societal biases including gender and race. Some of these studies focused on quantifying the bias, and proposed approaches for mitigating it within word embeddings. In the context of data augmentation solutions, Lu et al. (2018) introduced counterfactual data augmentation (CDA), a generic methodology to mitigate bias in neural NLP tasks, where for each training instance, a copy with an intervention on its targeted words is added, replacing each with its partner, while"
W19-3822,P19-1161,0,0.240143,"ender, gender-blind single-output MT from English often results in I. J.£ AK @ ÂnA Tbyb1 ‘I am a Ø AK @ ÂnA mmrD~ ‘I am a [fe[male] doctor’/ éQÜ male] nurse’, which is inappropriate for female doctors and male nurses, respectively. Part of this problem comes from humangenerated data that mirrors the social biases and inequalities of the world we live in, and that results in biased models and representations. Many research efforts responded to this problem by debiasing and balancing the models created from the data through model modification or data augmentation (Font and Costa-jussà, 2019; Zmigrod et al., 2019). However, ultimately, even the most balanced and unbiased of models can be useless in gender-blind systems that are designed to generate a single text output. Such systems are doomed to unsurprisingly pass on the biases of the models they use, as demonstrated in the doctor/nurse example above. In contrast, gender-aware systems should be designed to produce outputs that are as gender-specific as the input information they have access to. The input gender information may be contextual (e.g., the input ‘she is a doctor’), or extra linguistics (e.g., the gender feature provided in the user profil"
W19-4214,2012.eamt-1.60,0,0.0103923,"nts and blogs, consisting of over 10 million words for each subset’s dialect region. It is worth noting however, that the granularity of their dialect regions is coarser than the granularity of CORPUS 6. Hence, their Maghrebi dialect corresponds to two dialects in CORPUS 6, Tunis and Rabat, while the remaining three dialect regions have rather obvious one-to-one correspondences with CORPUS 6, i.e., Egyptian to Cairo, Levantine to Beirut, and Gulf to Doha. For MSA, which rarely occurs consistently (i.e., outside of brief instances of code-mixing) in such casual domains, we used the TED corpus (Cettolo and Girardi, 2012) for our monolingual data set, finding a compromise between domain relevance and corpus size. It contains about 2.5 million words. Obviously, CORPUS 6 is small relative to other MT corpora, but this is exactly why it is a meaningful evaluation corpus. Larger parallel corpora are often only available for better resourced languages/domains where fully supervised segmenters are also more likely to be available, negating the need to build one’s own segmenter. Furthermore, as parallel data becomes less sparse, tokenization necessarily has less of an effect since models can memorize and effectively"
W19-4214,E06-1047,1,0.671899,"popular approaches to segmentation. Furthermore, we discuss the construction of DE S EG’s grammar and its disambiguation algorithm. 3.1 Data Arabic and its Dialects Arabic is highly diaglossic (Ferguson, 1959), with the relatively consistent high register of Modern Standard Arabic being learned in schools across the Arab World. Meanwhile the often mutually unintelligible low register variants—collectively known as dialectal Arabic (DA)—are spoken colloquially. The phonological, morpho-syntactic, and lexical variation within the Arabic sprachbund is comparable to that among Romance languages (Chiang et al., 2006; Rouchdy, 2013; Erdmann et al., 2017), leading to problematic noise in multidialect corpora (Erdmann et al., 2018). Furthermore, lack of spelling conventions in DA exacerbates data sparsity, as does a rich morphology featuring templatic phenomena and robust cliticization, making it challenging to train quality segmenters even with much supervised data. 3.3 De-lexical Analysis The DE S EG grammar provides all possible delexical analyses of words by assuming any n-gram 115 Morph.Feat. (A) Prefix (B) PV.1US PV.1UP PV.2MS PV.2FS ∅ ∅ ∅ ∅ +t +nA +t +t/ty PV.2US ∅ +ty PV.2UP PV.3MS PV.3FS PV.3UP ∅ ∅"
W19-4214,W02-0506,0,0.0290285,"upervised options like MOR FESSOR (Creutz and Lagus, 2005) and byte pair encoding (BPE) (Sennrich et al., 2016) assume no 1 Exponents refer to recurring means by which morphosyntactic properties are realized within classes of words, e.g., adding suffix +s to get the third person singular present tense for verbs like WALK, TALK, and SKIP. 113 Proceedings of the 16th Workshop on Computational Research in Phonetics, Phonology, and Morphology, pages 113–124 c Florence, Italy. August 2, 2019 2019 Association for Computational Linguistics on automatic shallow and deterministic segmentation for MSA. Darwish (2002) used limited resources and greedy techniques to automatically learn rules and statistics to build a shallow morphological analyzer. There are many MSA morphological analyzers with rich representations and good coverage that required very intensive efforts to create (Beesley, 1998; Buckwalter, 2004; Attia, 2006, 2007; Smrž, 2007; Boudchiche et al., 2017). Buckwalter (2004) is perhaps the most commonly used among them, as it contributed the representations for the Penn Arabic treebank (PATB) (Maamouri and Bies, 2004). The PATB has been the most used resource for supervised morphological disambi"
W19-4214,N04-4038,0,0.0304678,"ited resources and greedy techniques to automatically learn rules and statistics to build a shallow morphological analyzer. There are many MSA morphological analyzers with rich representations and good coverage that required very intensive efforts to create (Beesley, 1998; Buckwalter, 2004; Attia, 2006, 2007; Smrž, 2007; Boudchiche et al., 2017). Buckwalter (2004) is perhaps the most commonly used among them, as it contributed the representations for the Penn Arabic treebank (PATB) (Maamouri and Bies, 2004). The PATB has been the most used resource for supervised morphological disambiguation (Diab et al., 2004; Habash and Rambow, 2005; Pasha et al., 2014; AlGahtani and McNaught, 2015; Zalmout and Habash, 2017). Some efforts have used other annotated resources and/or large unannotated data sets (Lee et al., 2003; Abdelali et al., 2016; Freihat et al., 2018). More closely related to this paper, Erdmann and Habash (2018) demonstrated that delexicalized information provides a cheap means of inducing morphological knowledge and thereby predicting lexical information in MSA. They employ a de-lexicalized grammar which is similar to ours, but they do not handle dialectal variants or spelling variation. The"
W19-4214,D11-1057,0,0.0583525,"e been many approaches to address this problem, varying along a number of dimensions: the degree of language independence or specificity, the required amount of machine learning supervision, the degree of depth and richness of the morphological representations. Language agnostic unsupervised models There are many works using minimally supervised to unsupervised models of morphology for connecting morphologically related words and identifying optimal (and at times application dependent) segmentations (Smith and Eisner, 2005; Creutz and Lagus, 2005; Snyder and Barzilay, 2008; Poon et al., 2009; Dreyer and Eisner, 2011; Stallard et al., 2012; Sirts and Goldwater, 2013; Narasimhan et al., 2015; Sennrich et al., 2016; Eskander et al., 2016b; Ataman et al., 2017; Ataman and Federico, 2018; Eskander et al., 2018). In this paper, we compare to two popular language agnostic segmentation systems: MORFESSOR (Creutz and Lagus, 2005) and BPE (Sennrich et al., 2016). Both train on large corpora of unannotated text in an unsupervised manner. Dialectal Arabic models Work on dialectal Arabic morphology and tokenization is relatively newer than work on MSA. Some of the earlier efforts worked on rule-based approaches to mo"
W19-4214,W18-5806,1,0.84461,"007; Smrž, 2007; Boudchiche et al., 2017). Buckwalter (2004) is perhaps the most commonly used among them, as it contributed the representations for the Penn Arabic treebank (PATB) (Maamouri and Bies, 2004). The PATB has been the most used resource for supervised morphological disambiguation (Diab et al., 2004; Habash and Rambow, 2005; Pasha et al., 2014; AlGahtani and McNaught, 2015; Zalmout and Habash, 2017). Some efforts have used other annotated resources and/or large unannotated data sets (Lee et al., 2003; Abdelali et al., 2016; Freihat et al., 2018). More closely related to this paper, Erdmann and Habash (2018) demonstrated that delexicalized information provides a cheap means of inducing morphological knowledge and thereby predicting lexical information in MSA. They employ a de-lexicalized grammar which is similar to ours, but they do not handle dialectal variants or spelling variation. They also do not use the grammar for segmentation, but for pruning word embedding clusters in order to predict the paradigm membership of forms encountered in raw text. non-standard domains. Using a corpus of several Arabic dialects exhibiting rich and complex morphology, unstandardized spelling, and variation borde"
W19-4214,W15-3212,0,0.0220223,"s and statistics to build a shallow morphological analyzer. There are many MSA morphological analyzers with rich representations and good coverage that required very intensive efforts to create (Beesley, 1998; Buckwalter, 2004; Attia, 2006, 2007; Smrž, 2007; Boudchiche et al., 2017). Buckwalter (2004) is perhaps the most commonly used among them, as it contributed the representations for the Penn Arabic treebank (PATB) (Maamouri and Bies, 2004). The PATB has been the most used resource for supervised morphological disambiguation (Diab et al., 2004; Habash and Rambow, 2005; Pasha et al., 2014; AlGahtani and McNaught, 2015; Zalmout and Habash, 2017). Some efforts have used other annotated resources and/or large unannotated data sets (Lee et al., 2003; Abdelali et al., 2016; Freihat et al., 2018). More closely related to this paper, Erdmann and Habash (2018) demonstrated that delexicalized information provides a cheap means of inducing morphological knowledge and thereby predicting lexical information in MSA. They employ a de-lexicalized grammar which is similar to ours, but they do not handle dialectal variants or spelling variation. They also do not use the grammar for segmentation, but for pruning word embedd"
W19-4214,P18-2049,0,0.0170982,"learning supervision, the degree of depth and richness of the morphological representations. Language agnostic unsupervised models There are many works using minimally supervised to unsupervised models of morphology for connecting morphologically related words and identifying optimal (and at times application dependent) segmentations (Smith and Eisner, 2005; Creutz and Lagus, 2005; Snyder and Barzilay, 2008; Poon et al., 2009; Dreyer and Eisner, 2011; Stallard et al., 2012; Sirts and Goldwater, 2013; Narasimhan et al., 2015; Sennrich et al., 2016; Eskander et al., 2016b; Ataman et al., 2017; Ataman and Federico, 2018; Eskander et al., 2018). In this paper, we compare to two popular language agnostic segmentation systems: MORFESSOR (Creutz and Lagus, 2005) and BPE (Sennrich et al., 2016). Both train on large corpora of unannotated text in an unsupervised manner. Dialectal Arabic models Work on dialectal Arabic morphology and tokenization is relatively newer than work on MSA. Some of the earlier efforts worked on rule-based approaches to model dialectal morphology directly (Habash and Rambow, 2006; Habash et al., 2012), or exploiting existing MSA resources (Salloum and Habash, 2014). Later, a number of anno"
W19-4214,2006.bcs-1.5,0,0.0234228,"ke WALK, TALK, and SKIP. 113 Proceedings of the 16th Workshop on Computational Research in Phonetics, Phonology, and Morphology, pages 113–124 c Florence, Italy. August 2, 2019 2019 Association for Computational Linguistics on automatic shallow and deterministic segmentation for MSA. Darwish (2002) used limited resources and greedy techniques to automatically learn rules and statistics to build a shallow morphological analyzer. There are many MSA morphological analyzers with rich representations and good coverage that required very intensive efforts to create (Beesley, 1998; Buckwalter, 2004; Attia, 2006, 2007; Smrž, 2007; Boudchiche et al., 2017). Buckwalter (2004) is perhaps the most commonly used among them, as it contributed the representations for the Penn Arabic treebank (PATB) (Maamouri and Bies, 2004). The PATB has been the most used resource for supervised morphological disambiguation (Diab et al., 2004; Habash and Rambow, 2005; Pasha et al., 2014; AlGahtani and McNaught, 2015; Zalmout and Habash, 2017). Some efforts have used other annotated resources and/or large unannotated data sets (Lee et al., 2003; Abdelali et al., 2016; Freihat et al., 2018). More closely related to this pape"
W19-4214,W07-0809,0,0.062349,"Missing"
W19-4214,W98-1007,0,0.125184,"ngular present tense for verbs like WALK, TALK, and SKIP. 113 Proceedings of the 16th Workshop on Computational Research in Phonetics, Phonology, and Morphology, pages 113–124 c Florence, Italy. August 2, 2019 2019 Association for Computational Linguistics on automatic shallow and deterministic segmentation for MSA. Darwish (2002) used limited resources and greedy techniques to automatically learn rules and statistics to build a shallow morphological analyzer. There are many MSA morphological analyzers with rich representations and good coverage that required very intensive efforts to create (Beesley, 1998; Buckwalter, 2004; Attia, 2006, 2007; Smrž, 2007; Boudchiche et al., 2017). Buckwalter (2004) is perhaps the most commonly used among them, as it contributed the representations for the Penn Arabic treebank (PATB) (Maamouri and Bies, 2004). The PATB has been the most used resource for supervised morphological disambiguation (Diab et al., 2004; Habash and Rambow, 2005; Pasha et al., 2014; AlGahtani and McNaught, 2015; Zalmout and Habash, 2017). Some efforts have used other annotated resources and/or large unannotated data sets (Lee et al., 2003; Abdelali et al., 2016; Freihat et al., 2018). Mo"
W19-4214,K18-3001,0,0.0342528,"age. It separates all characters in the corpus, then performs a pre-determined number of join operations, merging all instances of specified bigrams. Joins are determined such that the resulting corpus will contain as few tokens as possible given the number of join operations allowed. Thus, while the algorithm is unsupervised and easy to apply to any language, it is linguistically naive, assuming that morphological organization is driven solely by enumerative efficiency concerns. Likely for this reason, BPE has not been demonstrated to be particularly useful for applications beyond neural MT (Kann et al., 2018). DE S EG Our model, described in Section 3, finds a compromise between the convenience of language agnostic unsupervised systems and the performance of systems leveraging language specific resources. DE S EG can be run with a minimum base length of either 2 or 3 characters and a priority of base fertility maximization (f ) over greedy base length minimization, or vice versa (g). Minimum base length and priority are represented as subscripts in all relevant tables. 4.2 Intrinsic Language Modeling Evaluation Table 3 shows the LM results for tokenizing COR PUS 6 where all trainable segmenters ar"
W19-4214,L18-1607,1,0.839039,"large corpora of unannotated text in an unsupervised manner. Dialectal Arabic models Work on dialectal Arabic morphology and tokenization is relatively newer than work on MSA. Some of the earlier efforts worked on rule-based approaches to model dialectal morphology directly (Habash and Rambow, 2006; Habash et al., 2012), or exploiting existing MSA resources (Salloum and Habash, 2014). Later, a number of annotation efforts have led to the creation of varying sizes of dialectal annotated corpora following the style of the PATB (Maamouri et al., 2014; Jarrar et al., 2016; Al-Shargi et al., 2016; Khalifa et al., 2018; Alshargi et al., 2019). The created annotations supported models for dialectal Arabic analysis, disambiguation and tokenization building on the same successful approaches in MSA (Eskander et al., 2016a; Habash et al., 2013; Pasha et al., 2014; Zalmout et al., 2018; Zalmout and Habash, 2019). More closely related to this paper, Eldesouki et al. (2017) used de-lexicalized analyStandard Arabic models Modern Standard Arabic (MSA) morphological analysis, disambiguation and tokenization has been the focus of a large number of efforts. Khoja and Garside (1999) was one of the earliest published effo"
W19-4214,W17-1305,1,0.812767,"as Sadat and Habash (2006) demonstrate it to be the most effective scheme for low resource Arabic MT. 2 While Arabic exhibits many other nonconcatenative, templatic phenomena which complicate segmentation and tokenization, clitics are always concatenated to the outsides of base forms after the templatic pattern has been applied and are thus easier to separate. Occasionally, fusional processes can alter phonemes/graphemes on either side of base–clitic or clitic–clitic boundaries, but no templatic process is ever invoked to alter the internal structure of bases by affixing any clitic. We follow Khalifa et al. (2017)’s approach to We use our grammar to build a de-lexicalized morphological analyzer for all DA dialects targeting the D3 segmentation scheme (Habash, 2010), which separates all clitics and only clitics from the base forms to which they attach. We chose D3 2 With more data, the more effective schemes are ATB and D2 (Sadat and Habash, 2006). ATB resembles D3 but does not separate the definite article proclitic. D2 resembles ATB but does not separate the pronominal enclitic. 116 extending paradigms with possible clitic combinations, though we don’t require any stem lexical information. Hence, we c"
W19-4214,P03-1051,0,0.145123,"rage that required very intensive efforts to create (Beesley, 1998; Buckwalter, 2004; Attia, 2006, 2007; Smrž, 2007; Boudchiche et al., 2017). Buckwalter (2004) is perhaps the most commonly used among them, as it contributed the representations for the Penn Arabic treebank (PATB) (Maamouri and Bies, 2004). The PATB has been the most used resource for supervised morphological disambiguation (Diab et al., 2004; Habash and Rambow, 2005; Pasha et al., 2014; AlGahtani and McNaught, 2015; Zalmout and Habash, 2017). Some efforts have used other annotated resources and/or large unannotated data sets (Lee et al., 2003; Abdelali et al., 2016; Freihat et al., 2018). More closely related to this paper, Erdmann and Habash (2018) demonstrated that delexicalized information provides a cheap means of inducing morphological knowledge and thereby predicting lexical information in MSA. They employ a de-lexicalized grammar which is similar to ours, but they do not handle dialectal variants or spelling variation. They also do not use the grammar for segmentation, but for pruning word embedding clusters in order to predict the paradigm membership of forms encountered in raw text. non-standard domains. Using a corpus of"
W19-4214,W04-1602,0,0.260333,"r Computational Linguistics on automatic shallow and deterministic segmentation for MSA. Darwish (2002) used limited resources and greedy techniques to automatically learn rules and statistics to build a shallow morphological analyzer. There are many MSA morphological analyzers with rich representations and good coverage that required very intensive efforts to create (Beesley, 1998; Buckwalter, 2004; Attia, 2006, 2007; Smrž, 2007; Boudchiche et al., 2017). Buckwalter (2004) is perhaps the most commonly used among them, as it contributed the representations for the Penn Arabic treebank (PATB) (Maamouri and Bies, 2004). The PATB has been the most used resource for supervised morphological disambiguation (Diab et al., 2004; Habash and Rambow, 2005; Pasha et al., 2014; AlGahtani and McNaught, 2015; Zalmout and Habash, 2017). Some efforts have used other annotated resources and/or large unannotated data sets (Lee et al., 2003; Abdelali et al., 2016; Freihat et al., 2018). More closely related to this paper, Erdmann and Habash (2018) demonstrated that delexicalized information provides a cheap means of inducing morphological knowledge and thereby predicting lexical information in MSA. They employ a de-lexicaliz"
W19-4214,C16-1326,1,0.824911,"r specificity, the required amount of machine learning supervision, the degree of depth and richness of the morphological representations. Language agnostic unsupervised models There are many works using minimally supervised to unsupervised models of morphology for connecting morphologically related words and identifying optimal (and at times application dependent) segmentations (Smith and Eisner, 2005; Creutz and Lagus, 2005; Snyder and Barzilay, 2008; Poon et al., 2009; Dreyer and Eisner, 2011; Stallard et al., 2012; Sirts and Goldwater, 2013; Narasimhan et al., 2015; Sennrich et al., 2016; Eskander et al., 2016b; Ataman et al., 2017; Ataman and Federico, 2018; Eskander et al., 2018). In this paper, we compare to two popular language agnostic segmentation systems: MORFESSOR (Creutz and Lagus, 2005) and BPE (Sennrich et al., 2016). Both train on large corpora of unannotated text in an unsupervised manner. Dialectal Arabic models Work on dialectal Arabic morphology and tokenization is relatively newer than work on MSA. Some of the earlier efforts worked on rule-based approaches to model dialectal morphology directly (Habash and Rambow, 2006; Habash et al., 2012), or exploiting existing MSA resources (S"
W19-4214,W18-5808,0,0.0183242,"degree of depth and richness of the morphological representations. Language agnostic unsupervised models There are many works using minimally supervised to unsupervised models of morphology for connecting morphologically related words and identifying optimal (and at times application dependent) segmentations (Smith and Eisner, 2005; Creutz and Lagus, 2005; Snyder and Barzilay, 2008; Poon et al., 2009; Dreyer and Eisner, 2011; Stallard et al., 2012; Sirts and Goldwater, 2013; Narasimhan et al., 2015; Sennrich et al., 2016; Eskander et al., 2016b; Ataman et al., 2017; Ataman and Federico, 2018; Eskander et al., 2018). In this paper, we compare to two popular language agnostic segmentation systems: MORFESSOR (Creutz and Lagus, 2005) and BPE (Sennrich et al., 2016). Both train on large corpora of unannotated text in an unsupervised manner. Dialectal Arabic models Work on dialectal Arabic morphology and tokenization is relatively newer than work on MSA. Some of the earlier efforts worked on rule-based approaches to model dialectal morphology directly (Habash and Rambow, 2006; Habash et al., 2012), or exploiting existing MSA resources (Salloum and Habash, 2014). Later, a number of annotation efforts have led"
W19-4214,C16-1086,0,0.014934,"r specificity, the required amount of machine learning supervision, the degree of depth and richness of the morphological representations. Language agnostic unsupervised models There are many works using minimally supervised to unsupervised models of morphology for connecting morphologically related words and identifying optimal (and at times application dependent) segmentations (Smith and Eisner, 2005; Creutz and Lagus, 2005; Snyder and Barzilay, 2008; Poon et al., 2009; Dreyer and Eisner, 2011; Stallard et al., 2012; Sirts and Goldwater, 2013; Narasimhan et al., 2015; Sennrich et al., 2016; Eskander et al., 2016b; Ataman et al., 2017; Ataman and Federico, 2018; Eskander et al., 2018). In this paper, we compare to two popular language agnostic segmentation systems: MORFESSOR (Creutz and Lagus, 2005) and BPE (Sennrich et al., 2016). Both train on large corpora of unannotated text in an unsupervised manner. Dialectal Arabic models Work on dialectal Arabic morphology and tokenization is relatively newer than work on MSA. Some of the earlier efforts worked on rule-based approaches to model dialectal morphology directly (Habash and Rambow, 2006; Habash et al., 2012), or exploiting existing MSA resources (S"
W19-4214,E17-2045,0,0.014182,"gual corpus with all dialects pooled and the MT system on all the dialects pooled. Individual–individual trains six segmenters on relevant subsections of the monolingual data and six MT systems on the relevant partitions of CORPUS 6. Individual–pooled trains individual segmenters but one pan-Arabic MT system, which is reasonable to reduce the over generation of the morphological model but leverage shared information during MT. Neural MT has been used with dialects (Hassan et al., 2017), but given the extreme scarcity of in-domain data, statistical MT (Koehn et al., 2007) is the better choice (Farajian et al., 2017) for comparing quality of segmentation in our setting. DE S EG consistently outperforms unsupervised alternatives BPE and MORFESSOR in Table 4 while approaching and even beating state-of-the-art systems FARASA and MADAMIRA in the individual–pooled environment. The Fertility-based model DE S EGf 2 outperforms its greedy counterpart, supporting the argument that base fertility plays a meaningful role in morphological organization. 5 The best DA performance is achieved on Beirut for the pooled mode and Doha for the Individual. Beirut is the least verbose of all dialects in unsegmented space, and"
W19-4214,J97-4004,0,0.768968,"iding evidence for the value of linguistic input during preprocessing. 1 Introduction Non-standard domains, dialectal variation, and unstandardized spelling make segmentation challenging, though morphologically rich languages require good segmentation to enable downstream applications from syntactic parsing to machine translation (MT). For domains lacking sufficient annotated data to train segmenters, one must resort to language specific greedy techniques or language agnostic unsupervised techniques. Greedy techniques use maximum matching to identify base words, leveraging large dictionaries (Guo, 1997). Yet such dictionaries are often unavailable or too expensive for low resource languages. Language agnostic unsupervised options like MOR FESSOR (Creutz and Lagus, 2005) and byte pair encoding (BPE) (Sennrich et al., 2016) assume no 1 Exponents refer to recurring means by which morphosyntactic properties are realized within classes of words, e.g., adding suffix +s to get the third person singular present tense for verbs like WALK, TALK, and SKIP. 113 Proceedings of the 16th Workshop on Computational Research in Phonetics, Phonology, and Morphology, pages 113–124 c Florence, Italy. August 2, 2"
W19-4214,W12-2301,1,0.854086,"an et al., 2015; Sennrich et al., 2016; Eskander et al., 2016b; Ataman et al., 2017; Ataman and Federico, 2018; Eskander et al., 2018). In this paper, we compare to two popular language agnostic segmentation systems: MORFESSOR (Creutz and Lagus, 2005) and BPE (Sennrich et al., 2016). Both train on large corpora of unannotated text in an unsupervised manner. Dialectal Arabic models Work on dialectal Arabic morphology and tokenization is relatively newer than work on MSA. Some of the earlier efforts worked on rule-based approaches to model dialectal morphology directly (Habash and Rambow, 2006; Habash et al., 2012), or exploiting existing MSA resources (Salloum and Habash, 2014). Later, a number of annotation efforts have led to the creation of varying sizes of dialectal annotated corpora following the style of the PATB (Maamouri et al., 2014; Jarrar et al., 2016; Al-Shargi et al., 2016; Khalifa et al., 2018; Alshargi et al., 2019). The created annotations supported models for dialectal Arabic analysis, disambiguation and tokenization building on the same successful approaches in MSA (Eskander et al., 2016a; Habash et al., 2013; Pasha et al., 2014; Zalmout et al., 2018; Zalmout and Habash, 2019). More c"
W19-4214,P05-1071,1,0.57381,"greedy techniques to automatically learn rules and statistics to build a shallow morphological analyzer. There are many MSA morphological analyzers with rich representations and good coverage that required very intensive efforts to create (Beesley, 1998; Buckwalter, 2004; Attia, 2006, 2007; Smrž, 2007; Boudchiche et al., 2017). Buckwalter (2004) is perhaps the most commonly used among them, as it contributed the representations for the Penn Arabic treebank (PATB) (Maamouri and Bies, 2004). The PATB has been the most used resource for supervised morphological disambiguation (Diab et al., 2004; Habash and Rambow, 2005; Pasha et al., 2014; AlGahtani and McNaught, 2015; Zalmout and Habash, 2017). Some efforts have used other annotated resources and/or large unannotated data sets (Lee et al., 2003; Abdelali et al., 2016; Freihat et al., 2018). More closely related to this paper, Erdmann and Habash (2018) demonstrated that delexicalized information provides a cheap means of inducing morphological knowledge and thereby predicting lexical information in MSA. They employ a de-lexicalized grammar which is similar to ours, but they do not handle dialectal variants or spelling variation. They also do not use the gra"
W19-4214,P06-1086,1,0.835068,"Goldwater, 2013; Narasimhan et al., 2015; Sennrich et al., 2016; Eskander et al., 2016b; Ataman et al., 2017; Ataman and Federico, 2018; Eskander et al., 2018). In this paper, we compare to two popular language agnostic segmentation systems: MORFESSOR (Creutz and Lagus, 2005) and BPE (Sennrich et al., 2016). Both train on large corpora of unannotated text in an unsupervised manner. Dialectal Arabic models Work on dialectal Arabic morphology and tokenization is relatively newer than work on MSA. Some of the earlier efforts worked on rule-based approaches to model dialectal morphology directly (Habash and Rambow, 2006; Habash et al., 2012), or exploiting existing MSA resources (Salloum and Habash, 2014). Later, a number of annotation efforts have led to the creation of varying sizes of dialectal annotated corpora following the style of the PATB (Maamouri et al., 2014; Jarrar et al., 2016; Al-Shargi et al., 2016; Khalifa et al., 2018; Alshargi et al., 2019). The created annotations supported models for dialectal Arabic analysis, disambiguation and tokenization building on the same successful approaches in MSA (Eskander et al., 2016a; Habash et al., 2013; Pasha et al., 2014; Zalmout et al., 2018; Zalmout and"
W19-4214,Q13-1021,0,0.0257759,"varying along a number of dimensions: the degree of language independence or specificity, the required amount of machine learning supervision, the degree of depth and richness of the morphological representations. Language agnostic unsupervised models There are many works using minimally supervised to unsupervised models of morphology for connecting morphologically related words and identifying optimal (and at times application dependent) segmentations (Smith and Eisner, 2005; Creutz and Lagus, 2005; Snyder and Barzilay, 2008; Poon et al., 2009; Dreyer and Eisner, 2011; Stallard et al., 2012; Sirts and Goldwater, 2013; Narasimhan et al., 2015; Sennrich et al., 2016; Eskander et al., 2016b; Ataman et al., 2017; Ataman and Federico, 2018; Eskander et al., 2018). In this paper, we compare to two popular language agnostic segmentation systems: MORFESSOR (Creutz and Lagus, 2005) and BPE (Sennrich et al., 2016). Both train on large corpora of unannotated text in an unsupervised manner. Dialectal Arabic models Work on dialectal Arabic morphology and tokenization is relatively newer than work on MSA. Some of the earlier efforts worked on rule-based approaches to model dialectal morphology directly (Habash and Ramb"
W19-4214,maamouri-etal-2014-developing,1,0.855905,"utz and Lagus, 2005) and BPE (Sennrich et al., 2016). Both train on large corpora of unannotated text in an unsupervised manner. Dialectal Arabic models Work on dialectal Arabic morphology and tokenization is relatively newer than work on MSA. Some of the earlier efforts worked on rule-based approaches to model dialectal morphology directly (Habash and Rambow, 2006; Habash et al., 2012), or exploiting existing MSA resources (Salloum and Habash, 2014). Later, a number of annotation efforts have led to the creation of varying sizes of dialectal annotated corpora following the style of the PATB (Maamouri et al., 2014; Jarrar et al., 2016; Al-Shargi et al., 2016; Khalifa et al., 2018; Alshargi et al., 2019). The created annotations supported models for dialectal Arabic analysis, disambiguation and tokenization building on the same successful approaches in MSA (Eskander et al., 2016a; Habash et al., 2013; Pasha et al., 2014; Zalmout et al., 2018; Zalmout and Habash, 2019). More closely related to this paper, Eldesouki et al. (2017) used de-lexicalized analyStandard Arabic models Modern Standard Arabic (MSA) morphological analysis, disambiguation and tokenization has been the focus of a large number of effor"
W19-4214,Q15-1012,0,0.120812,"imensions: the degree of language independence or specificity, the required amount of machine learning supervision, the degree of depth and richness of the morphological representations. Language agnostic unsupervised models There are many works using minimally supervised to unsupervised models of morphology for connecting morphologically related words and identifying optimal (and at times application dependent) segmentations (Smith and Eisner, 2005; Creutz and Lagus, 2005; Snyder and Barzilay, 2008; Poon et al., 2009; Dreyer and Eisner, 2011; Stallard et al., 2012; Sirts and Goldwater, 2013; Narasimhan et al., 2015; Sennrich et al., 2016; Eskander et al., 2016b; Ataman et al., 2017; Ataman and Federico, 2018; Eskander et al., 2018). In this paper, we compare to two popular language agnostic segmentation systems: MORFESSOR (Creutz and Lagus, 2005) and BPE (Sennrich et al., 2016). Both train on large corpora of unannotated text in an unsupervised manner. Dialectal Arabic models Work on dialectal Arabic morphology and tokenization is relatively newer than work on MSA. Some of the earlier efforts worked on rule-based approaches to model dialectal morphology directly (Habash and Rambow, 2006; Habash et al.,"
W19-4214,pasha-etal-2014-madamira,1,0.898429,"Missing"
W19-4214,N09-1024,0,0.0403568,"domains. There have been many approaches to address this problem, varying along a number of dimensions: the degree of language independence or specificity, the required amount of machine learning supervision, the degree of depth and richness of the morphological representations. Language agnostic unsupervised models There are many works using minimally supervised to unsupervised models of morphology for connecting morphologically related words and identifying optimal (and at times application dependent) segmentations (Smith and Eisner, 2005; Creutz and Lagus, 2005; Snyder and Barzilay, 2008; Poon et al., 2009; Dreyer and Eisner, 2011; Stallard et al., 2012; Sirts and Goldwater, 2013; Narasimhan et al., 2015; Sennrich et al., 2016; Eskander et al., 2016b; Ataman et al., 2017; Ataman and Federico, 2018; Eskander et al., 2018). In this paper, we compare to two popular language agnostic segmentation systems: MORFESSOR (Creutz and Lagus, 2005) and BPE (Sennrich et al., 2016). Both train on large corpora of unannotated text in an unsupervised manner. Dialectal Arabic models Work on dialectal Arabic morphology and tokenization is relatively newer than work on MSA. Some of the earlier efforts worked on ru"
W19-4214,P06-1001,1,0.710713,"of some minimum length can be an open class base, provided the remaining characters comprise a supported affix pattern. Hence, a simple grammar which only supports words without affixes or with a single suffix, +s, would return two analyses for wugs: wugs and wug +s, and one for foo: foo. To build such a grammar for an Arabic dialect, we target clitic affixation, as this phenomenon is nontemplatic with minimal fusional edits, making it easier to model with a smaller grammar, yet it accounts for a great deal of sparsity, as Arabic clitics are as productive as regular inflectional exponents. as Sadat and Habash (2006) demonstrate it to be the most effective scheme for low resource Arabic MT. 2 While Arabic exhibits many other nonconcatenative, templatic phenomena which complicate segmentation and tokenization, clitics are always concatenated to the outsides of base forms after the templatic pattern has been applied and are thus easier to separate. Occasionally, fusional processes can alter phonemes/graphemes on either side of base–clitic or clitic–clitic boundaries, but no templatic process is ever invoked to alter the internal structure of bases by affixing any clitic. We follow Khalifa et al. (2017)’s ap"
W19-4214,C18-1113,1,0.829968,"ard domain, but have limited support for multiple colloquial variants of the language. Finally, we note that, linguistically, our work is inspired by Bertram et al. (2000) who find that prolific stems with large derivational families are accessed more quickly. Their work suggests that stem fertility—or the productivity with which a stem can combine with different affixes—is cognitively relevant to morphological organization. 3 To demonstrate how our model handles such challenging phenomena, we apply it to the CORPUS 6 subset of the MADAR-BTEC (Takezawa et al., 2002) corpus of Arabic dialects (Salameh et al., 2018). This consists of 12,000 sentences in the travel domain (9,000 for training) parallel between English, MSA, and the DA varieties spoken in Beirut, Cairo, Doha, Rabat, and Tunis. This comprises a representative sample of the breadth of intra-DA variation (Bouamor et al., 2018). In addition to CORPUS 6, we also use large amounts of raw monolingual data to train our segmenter and the unsupervised baselines. To avoid introducing even more noise, we restrict our monolingual datasets as much as possible to similar domains. For DA, we use the four subsets of Almeman and Lee (2013)’s web crawl of for"
W19-4214,K17-1043,0,0.135443,"ated to this paper, Eldesouki et al. (2017) used de-lexicalized analyStandard Arabic models Modern Standard Arabic (MSA) morphological analysis, disambiguation and tokenization has been the focus of a large number of efforts. Khoja and Garside (1999) was one of the earliest published efforts 114 3.2 sis strategy for four colloquial varieties of Arabic, though they also use minimal training data and extract features from an open class lexicon to learn either an SVM or bi-LSTM-CRF disambiguation model. They further show that domain adaptation from existing MSA training data is beneficial. Also, Samih et al. (2017) applied a related model to segmentation, allowing different Arabic dialects to inform one another, thus avoiding the need to perform dialect identification during preprocessing. We compare our model to MADAMIRA (Pasha et al., 2014) and FARASA (Abdelali et al., 2016), which represent the fully supervised state of the art for segmenting Arabic in the standard domain, but have limited support for multiple colloquial variants of the language. Finally, we note that, linguistically, our work is inspired by Bertram et al. (2000) who find that prolific stems with large derivational families are acces"
W19-4214,P16-1162,0,0.489551,"rich languages require good segmentation to enable downstream applications from syntactic parsing to machine translation (MT). For domains lacking sufficient annotated data to train segmenters, one must resort to language specific greedy techniques or language agnostic unsupervised techniques. Greedy techniques use maximum matching to identify base words, leveraging large dictionaries (Guo, 1997). Yet such dictionaries are often unavailable or too expensive for low resource languages. Language agnostic unsupervised options like MOR FESSOR (Creutz and Lagus, 2005) and byte pair encoding (BPE) (Sennrich et al., 2016) assume no 1 Exponents refer to recurring means by which morphosyntactic properties are realized within classes of words, e.g., adding suffix +s to get the third person singular present tense for verbs like WALK, TALK, and SKIP. 113 Proceedings of the 16th Workshop on Computational Research in Phonetics, Phonology, and Morphology, pages 113–124 c Florence, Italy. August 2, 2019 2019 Association for Computational Linguistics on automatic shallow and deterministic segmentation for MSA. Darwish (2002) used limited resources and greedy techniques to automatically learn rules and statistics to buil"
W19-4214,P05-1044,0,0.0605835,"resourced languages often lack such resources for non-standard dialects and domains. There have been many approaches to address this problem, varying along a number of dimensions: the degree of language independence or specificity, the required amount of machine learning supervision, the degree of depth and richness of the morphological representations. Language agnostic unsupervised models There are many works using minimally supervised to unsupervised models of morphology for connecting morphologically related words and identifying optimal (and at times application dependent) segmentations (Smith and Eisner, 2005; Creutz and Lagus, 2005; Snyder and Barzilay, 2008; Poon et al., 2009; Dreyer and Eisner, 2011; Stallard et al., 2012; Sirts and Goldwater, 2013; Narasimhan et al., 2015; Sennrich et al., 2016; Eskander et al., 2016b; Ataman et al., 2017; Ataman and Federico, 2018; Eskander et al., 2018). In this paper, we compare to two popular language agnostic segmentation systems: MORFESSOR (Creutz and Lagus, 2005) and BPE (Sennrich et al., 2016). Both train on large corpora of unannotated text in an unsupervised manner. Dialectal Arabic models Work on dialectal Arabic morphology and tokenization is relat"
W19-4214,P08-1084,0,0.0300749,"r non-standard dialects and domains. There have been many approaches to address this problem, varying along a number of dimensions: the degree of language independence or specificity, the required amount of machine learning supervision, the degree of depth and richness of the morphological representations. Language agnostic unsupervised models There are many works using minimally supervised to unsupervised models of morphology for connecting morphologically related words and identifying optimal (and at times application dependent) segmentations (Smith and Eisner, 2005; Creutz and Lagus, 2005; Snyder and Barzilay, 2008; Poon et al., 2009; Dreyer and Eisner, 2011; Stallard et al., 2012; Sirts and Goldwater, 2013; Narasimhan et al., 2015; Sennrich et al., 2016; Eskander et al., 2016b; Ataman et al., 2017; Ataman and Federico, 2018; Eskander et al., 2018). In this paper, we compare to two popular language agnostic segmentation systems: MORFESSOR (Creutz and Lagus, 2005) and BPE (Sennrich et al., 2016). Both train on large corpora of unannotated text in an unsupervised manner. Dialectal Arabic models Work on dialectal Arabic morphology and tokenization is relatively newer than work on MSA. Some of the earlier e"
W19-4214,P12-2063,0,0.0260065,"address this problem, varying along a number of dimensions: the degree of language independence or specificity, the required amount of machine learning supervision, the degree of depth and richness of the morphological representations. Language agnostic unsupervised models There are many works using minimally supervised to unsupervised models of morphology for connecting morphologically related words and identifying optimal (and at times application dependent) segmentations (Smith and Eisner, 2005; Creutz and Lagus, 2005; Snyder and Barzilay, 2008; Poon et al., 2009; Dreyer and Eisner, 2011; Stallard et al., 2012; Sirts and Goldwater, 2013; Narasimhan et al., 2015; Sennrich et al., 2016; Eskander et al., 2016b; Ataman et al., 2017; Ataman and Federico, 2018; Eskander et al., 2018). In this paper, we compare to two popular language agnostic segmentation systems: MORFESSOR (Creutz and Lagus, 2005) and BPE (Sennrich et al., 2016). Both train on large corpora of unannotated text in an unsupervised manner. Dialectal Arabic models Work on dialectal Arabic morphology and tokenization is relatively newer than work on MSA. Some of the earlier efforts worked on rule-based approaches to model dialectal morpholog"
W19-4214,takezawa-etal-2002-toward,0,0.125582,"tate of the art for segmenting Arabic in the standard domain, but have limited support for multiple colloquial variants of the language. Finally, we note that, linguistically, our work is inspired by Bertram et al. (2000) who find that prolific stems with large derivational families are accessed more quickly. Their work suggests that stem fertility—or the productivity with which a stem can combine with different affixes—is cognitively relevant to morphological organization. 3 To demonstrate how our model handles such challenging phenomena, we apply it to the CORPUS 6 subset of the MADAR-BTEC (Takezawa et al., 2002) corpus of Arabic dialects (Salameh et al., 2018). This consists of 12,000 sentences in the travel domain (9,000 for training) parallel between English, MSA, and the DA varieties spoken in Beirut, Cairo, Doha, Rabat, and Tunis. This comprises a representative sample of the breadth of intra-DA variation (Bouamor et al., 2018). In addition to CORPUS 6, we also use large amounts of raw monolingual data to train our segmenter and the unsupervised baselines. To avoid introducing even more noise, we restrict our monolingual datasets as much as possible to similar domains. For DA, we use the four sub"
W19-4214,P17-1184,0,0.0449813,"Missing"
W19-4214,N18-1087,1,0.845553,"directly (Habash and Rambow, 2006; Habash et al., 2012), or exploiting existing MSA resources (Salloum and Habash, 2014). Later, a number of annotation efforts have led to the creation of varying sizes of dialectal annotated corpora following the style of the PATB (Maamouri et al., 2014; Jarrar et al., 2016; Al-Shargi et al., 2016; Khalifa et al., 2018; Alshargi et al., 2019). The created annotations supported models for dialectal Arabic analysis, disambiguation and tokenization building on the same successful approaches in MSA (Eskander et al., 2016a; Habash et al., 2013; Pasha et al., 2014; Zalmout et al., 2018; Zalmout and Habash, 2019). More closely related to this paper, Eldesouki et al. (2017) used de-lexicalized analyStandard Arabic models Modern Standard Arabic (MSA) morphological analysis, disambiguation and tokenization has been the focus of a large number of efforts. Khoja and Garside (1999) was one of the earliest published efforts 114 3.2 sis strategy for four colloquial varieties of Arabic, though they also use minimal training data and extract features from an open class lexicon to learn either an SVM or bi-LSTM-CRF disambiguation model. They further show that domain adaptation from exi"
W19-4214,D17-1073,1,0.788504,"allow morphological analyzer. There are many MSA morphological analyzers with rich representations and good coverage that required very intensive efforts to create (Beesley, 1998; Buckwalter, 2004; Attia, 2006, 2007; Smrž, 2007; Boudchiche et al., 2017). Buckwalter (2004) is perhaps the most commonly used among them, as it contributed the representations for the Penn Arabic treebank (PATB) (Maamouri and Bies, 2004). The PATB has been the most used resource for supervised morphological disambiguation (Diab et al., 2004; Habash and Rambow, 2005; Pasha et al., 2014; AlGahtani and McNaught, 2015; Zalmout and Habash, 2017). Some efforts have used other annotated resources and/or large unannotated data sets (Lee et al., 2003; Abdelali et al., 2016; Freihat et al., 2018). More closely related to this paper, Erdmann and Habash (2018) demonstrated that delexicalized information provides a cheap means of inducing morphological knowledge and thereby predicting lexical information in MSA. They employ a de-lexicalized grammar which is similar to ours, but they do not handle dialectal variants or spelling variation. They also do not use the grammar for segmentation, but for pruning word embedding clusters in order to pr"
W19-4214,P19-1173,1,0.834397,"ambow, 2006; Habash et al., 2012), or exploiting existing MSA resources (Salloum and Habash, 2014). Later, a number of annotation efforts have led to the creation of varying sizes of dialectal annotated corpora following the style of the PATB (Maamouri et al., 2014; Jarrar et al., 2016; Al-Shargi et al., 2016; Khalifa et al., 2018; Alshargi et al., 2019). The created annotations supported models for dialectal Arabic analysis, disambiguation and tokenization building on the same successful approaches in MSA (Eskander et al., 2016a; Habash et al., 2013; Pasha et al., 2014; Zalmout et al., 2018; Zalmout and Habash, 2019). More closely related to this paper, Eldesouki et al. (2017) used de-lexicalized analyStandard Arabic models Modern Standard Arabic (MSA) morphological analysis, disambiguation and tokenization has been the focus of a large number of efforts. Khoja and Garside (1999) was one of the earliest published efforts 114 3.2 sis strategy for four colloquial varieties of Arabic, though they also use minimal training data and extract features from an open class lexicon to learn either an SVM or bi-LSTM-CRF disambiguation model. They further show that domain adaptation from existing MSA training data is"
W19-4622,L18-1577,0,0.242839,"Missing"
W19-4622,D14-1154,0,0.0736532,"f the Arab World. Although primarily spoken, written dialectal Arabic has been increasingly used on social media. Automatic dialect identification is helpful for tasks such as sentiment analysis (Al-Twairesh et al., 2016), author profiling (Sadat et al., 2014), and machine translation (Salloum et al., 2014). Most previous work, shared tasks, and evaluation campaigns on Arabic dialect identification were limited in terms of dialectal variety targeting coarse-grained regional dialect classes (around five) plus Modern Standard Arabic (MSA) (Zaidan and CallisonBurch, 2013; Elfardy and Diab, 2013; Darwish et al., 2014; Malmasi et al., 2016; Zampieri et al., 2017; El-Haj et al., 2018). There are of course some recent noteworthy exceptions (Bouamor et al., 2018; Zaghouani and Charfi, 2018; AbdulMageed et al., 2018). In this paper, we present the results and findings of the MADAR Shared Task on Arabic Fine1 http://wanlp2019.arabic-nlp.net https://camel.abudhabi.nyu.edu/madar/ 3 http://resources.camel-lab.com. 2 199 Proceedings of the Fourth Arabic Natural Language Processing Workshop, pages 199–207 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics 2 Region Gulf of Aden Task Des"
W19-4622,W19-4633,0,0.188447,"Missing"
W19-4622,J14-1006,0,0.370824,"Missing"
W19-4622,W19-4628,0,0.174819,"Missing"
W19-4622,L18-1573,0,0.0500337,"ic has been increasingly used on social media. Automatic dialect identification is helpful for tasks such as sentiment analysis (Al-Twairesh et al., 2016), author profiling (Sadat et al., 2014), and machine translation (Salloum et al., 2014). Most previous work, shared tasks, and evaluation campaigns on Arabic dialect identification were limited in terms of dialectal variety targeting coarse-grained regional dialect classes (around five) plus Modern Standard Arabic (MSA) (Zaidan and CallisonBurch, 2013; Elfardy and Diab, 2013; Darwish et al., 2014; Malmasi et al., 2016; Zampieri et al., 2017; El-Haj et al., 2018). There are of course some recent noteworthy exceptions (Bouamor et al., 2018; Zaghouani and Charfi, 2018; AbdulMageed et al., 2018). In this paper, we present the results and findings of the MADAR Shared Task on Arabic Fine1 http://wanlp2019.arabic-nlp.net https://camel.abudhabi.nyu.edu/madar/ 3 http://resources.camel-lab.com. 2 199 Proceedings of the Fourth Arabic Natural Language Processing Workshop, pages 199–207 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics 2 Region Gulf of Aden Task Description The MADAR Shared Task included two subtasks: the MADAR Tra"
W19-4622,W14-3601,0,0.0818796,"AJK éJ ËAÓ úæK .ÈA®£ . . Q¯ñÊK QK A« .ÈA®£@ .  Q .ÈA®£ @ è » ø YK.  ªK . .ÈAîE éÊ KA ¯ IJ   ªK . .P AªË@ ø P@P YË@ ÈAK X é J.Ë IJ èQ AK @ .ÈA®£@ . @ èPQk . ø YK. .ÈA®£ @ Q¯ñÊK QK @X .ÈA®£ . èQ YJ«@ .ÈA®£@ Q CË .ÈA®£ è  AªK. @ ø P@P YË@ ÈAK X ñºK QK IJ  ªK . .P AªË@ CË èQ ùªK @ .ÈA®£ . CË . ÈA®£ Q¯ñÊK. ø YK. @ èQ úæ @ .ú ÍA®£ .XBð CË ÈñK QÓ I.m&apos; ¨AJÓ éJ ËAÓ úæK .P Aª ¨AJÓ ÈñK QÓ I. m. &apos; .P Aª  CË .ÈA®£ IJ »Ag. YK P @ Table 2: An example from Corpus 26 for the English sentence ‘I’d like a children’s sweater.’ Corpus collection Inspired by the work of Mubarak and Darwish (2014) we collected a set of Twitter user profiles that reflects the way users from different regions in the Arab World tweet. Unlike previous work (Zaghouani and Charfi, 2018), we do not search Twitter based on specific dialectal keywords. Rather, we search for tweets that contain a set of 25 seed hashtags corresponding to the 22 states of the Arab League (e.g., #Algeria, #Egypt, #Kuwait, etc.), in addition to the hashtags: ”#ArabWorld”, ”#ArabLeague” and ”#Arab”. We collected an equal number of profiles (175 * 25 = 4,375) from the search results of each of the hashtags. The profiles were all manua"
W19-4622,W19-4636,0,0.0437051,"Missing"
W19-4622,W19-4623,0,0.0791391,"Missing"
W19-4622,P13-2081,0,0.245192,"ross different regions of the Arab World. Although primarily spoken, written dialectal Arabic has been increasingly used on social media. Automatic dialect identification is helpful for tasks such as sentiment analysis (Al-Twairesh et al., 2016), author profiling (Sadat et al., 2014), and machine translation (Salloum et al., 2014). Most previous work, shared tasks, and evaluation campaigns on Arabic dialect identification were limited in terms of dialectal variety targeting coarse-grained regional dialect classes (around five) plus Modern Standard Arabic (MSA) (Zaidan and CallisonBurch, 2013; Elfardy and Diab, 2013; Darwish et al., 2014; Malmasi et al., 2016; Zampieri et al., 2017; El-Haj et al., 2018). There are of course some recent noteworthy exceptions (Bouamor et al., 2018; Zaghouani and Charfi, 2018; AbdulMageed et al., 2018). In this paper, we present the results and findings of the MADAR Shared Task on Arabic Fine1 http://wanlp2019.arabic-nlp.net https://camel.abudhabi.nyu.edu/madar/ 3 http://resources.camel-lab.com. 2 199 Proceedings of the Fourth Arabic Natural Language Processing Workshop, pages 199–207 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics 2 Region"
W19-4622,W19-4632,0,0.15859,"Missing"
W19-4622,W19-4624,0,0.0644897,"Missing"
W19-4622,W19-4626,0,0.0577171,"Missing"
W19-4622,W19-4627,0,0.031473,"Missing"
W19-4622,W14-5904,0,0.183785,"to target a large set of dialect labels at the city and country levels. The data for the shared task was created or collected under the Multi-Arabic Dialect Applications and Resources (MADAR) project. A total of 21 teams from 15 countries participated in the shared task. 1 Introduction Arabic has a number of diverse dialects from across different regions of the Arab World. Although primarily spoken, written dialectal Arabic has been increasingly used on social media. Automatic dialect identification is helpful for tasks such as sentiment analysis (Al-Twairesh et al., 2016), author profiling (Sadat et al., 2014), and machine translation (Salloum et al., 2014). Most previous work, shared tasks, and evaluation campaigns on Arabic dialect identification were limited in terms of dialectal variety targeting coarse-grained regional dialect classes (around five) plus Modern Standard Arabic (MSA) (Zaidan and CallisonBurch, 2013; Elfardy and Diab, 2013; Darwish et al., 2014; Malmasi et al., 2016; Zampieri et al., 2017; El-Haj et al., 2018). There are of course some recent noteworthy exceptions (Bouamor et al., 2018; Zaghouani and Charfi, 2018; AbdulMageed et al., 2018). In this paper, we present the results a"
W19-4622,W19-4625,0,0.0407356,"Missing"
W19-4622,C18-1113,1,0.76062,"Variant Total *6 54,000 *6 6,000 * 26 41,600 * 26 5,200 * 26 5,200 Table 3: Distribution of the train, dev and test sets provided for Subtask 1. Corpus annotation Three annotators, all native speakers of Arabic were hired to complete this task. They were provided with a list of Twitter user profiles and their corresponding URLs. They were asked to inspect each profile by checking if the user indicated his/her location, checking his/her tweets, and label it with its corresponding country when possible. In the context of dialect identification, the country label here refers to the Twitter 4 In (Salameh et al., 2018), the Corpus 6 test set corresponds to the 2,000 sentences from Corpus 26 corresponding to the Corpus 6’s five cities and MSA. 201 DrBehbehaniAM Kuwait DrBehbehaniAM Kuwait DrBehbehaniAM Kuwait HederAshraf Egypt HederAshraf Egypt HederAshraf Egypt samykhalildz Algeria samykhalildz Algeria samykhalildz Algeria    ZAJ.£ B@ QK ñK á  @X  Ë àñËñ®Kð  éÊ ¯A ®Ë@  B  . ªK H. CË@ Bð , I ® ¯ð . IJ  áÓ  K B@ ZAÓ á JJ¢ËAK B@ J ÊK AÓ ZAÓ QK ñJË@ Ðñ¯ úÎ« PñJËAÓ éÓA I.Ê®Ë@ . . .  QK A« ñë AÓ ø P IJm&apos; ë B@ ú¯ àñk . IJ m.&apos; QK A« úÍ@ AJ J«AJK. øX B ð @ I.jK ¬YêË@ . . . ú"
W19-4622,W19-4634,0,0.0370171,"Missing"
W19-4622,P14-2125,1,0.867703,"e city and country levels. The data for the shared task was created or collected under the Multi-Arabic Dialect Applications and Resources (MADAR) project. A total of 21 teams from 15 countries participated in the shared task. 1 Introduction Arabic has a number of diverse dialects from across different regions of the Arab World. Although primarily spoken, written dialectal Arabic has been increasingly used on social media. Automatic dialect identification is helpful for tasks such as sentiment analysis (Al-Twairesh et al., 2016), author profiling (Sadat et al., 2014), and machine translation (Salloum et al., 2014). Most previous work, shared tasks, and evaluation campaigns on Arabic dialect identification were limited in terms of dialectal variety targeting coarse-grained regional dialect classes (around five) plus Modern Standard Arabic (MSA) (Zaidan and CallisonBurch, 2013; Elfardy and Diab, 2013; Darwish et al., 2014; Malmasi et al., 2016; Zampieri et al., 2017; El-Haj et al., 2018). There are of course some recent noteworthy exceptions (Bouamor et al., 2018; Zaghouani and Charfi, 2018; AbdulMageed et al., 2018). In this paper, we present the results and findings of the MADAR Shared Task on Arabic F"
W19-4622,W19-4639,0,0.0527086,"Missing"
W19-4622,W19-4629,0,0.0882889,"Missing"
W19-4622,L18-1111,0,0.126051,"s such as sentiment analysis (Al-Twairesh et al., 2016), author profiling (Sadat et al., 2014), and machine translation (Salloum et al., 2014). Most previous work, shared tasks, and evaluation campaigns on Arabic dialect identification were limited in terms of dialectal variety targeting coarse-grained regional dialect classes (around five) plus Modern Standard Arabic (MSA) (Zaidan and CallisonBurch, 2013; Elfardy and Diab, 2013; Darwish et al., 2014; Malmasi et al., 2016; Zampieri et al., 2017; El-Haj et al., 2018). There are of course some recent noteworthy exceptions (Bouamor et al., 2018; Zaghouani and Charfi, 2018; AbdulMageed et al., 2018). In this paper, we present the results and findings of the MADAR Shared Task on Arabic Fine1 http://wanlp2019.arabic-nlp.net https://camel.abudhabi.nyu.edu/madar/ 3 http://resources.camel-lab.com. 2 199 Proceedings of the Fourth Arabic Natural Language Processing Workshop, pages 199–207 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics 2 Region Gulf of Aden Task Description The MADAR Shared Task included two subtasks: the MADAR Travel Domain Dialect Identification subtask, and the MADAR Twitter User Dialect Identification subtask. 2.1"
W19-4622,W19-4637,0,0.383619,"Missing"
W19-5512,C12-2096,0,0.917124,"bles, images, and graphics. Therefore, most of the established PDF to text conversion products on the market (i.e., pdf2text) generate highly noisy unstructured texts containing abbreviations, non-standard words, false starts, missing punctuation, missing letter case information, and other text disfluencies. Building NLP applications customized for such texts is very challenging as most of the NLP tools (i.e. POS tagging, parsing, etc.) and applications (i.e. information extraction, machine translation) require as input a well-formatted clean text, where sentence boundaries are clearly marked [1]. Despite its important role in NLP, sentence boundary detection (SBD) has so far not received enough attention. Previous research in the area has been confined to formal texts only (news, European Parliament proceedings, etc.) where existing rule-based and machine learning approaches are extremely accurate (when the data is perfectly clean). No sentence boundary detection research to date has addressed the problem in noisy texts extracted automatically from machine-readable formats (generally PDF file format) files such as financial documents. In this shared task, we focus on extracting well"
W19-5512,J93-2004,0,0.0717971,"LP), collocated with IJCAI-2019.2 A total of 9 teams from 7 countries submitted runs and contributed 7 system description papers. All system description papers are included in the FinNLP workshop proceedings and cited in this report. The large number of teams and submitted systems suggests that such shared tasks can indeed generate significant interest in the Finance and NLP research community. 2 Previous Work on SBD While SBD is a foundational pre-processing task, previous research has been confined to clean texts in standard areas such as the news and limited datasets such as the WSJ corpus [2] or the Brown corpus [3]. SBD has been largely explored following several approaches that could be classified into three major classes: (a) rule-based SBD, using hand-crafted heuristics and lists [4]; (b) machine learning approaches to SBD [5; 6; 7; 3]; and more recently (c) deep learning methods [8]. Most of these approaches give fairly accurate results. These systems are based on a number of assumptions [4] that do not hold for noisy texts extracted automatically from PDFs (data is perfectly clean). 1 https://sites.google.com/nlg.csie.ntu.edu.tw/ finnlp/shared-task-finsbd 2 https://sites.goo"
W19-5512,N09-2061,0,0.829238,"AI-2019.2 A total of 9 teams from 7 countries submitted runs and contributed 7 system description papers. All system description papers are included in the FinNLP workshop proceedings and cited in this report. The large number of teams and submitted systems suggests that such shared tasks can indeed generate significant interest in the Finance and NLP research community. 2 Previous Work on SBD While SBD is a foundational pre-processing task, previous research has been confined to clean texts in standard areas such as the news and limited datasets such as the WSJ corpus [2] or the Brown corpus [3]. SBD has been largely explored following several approaches that could be classified into three major classes: (a) rule-based SBD, using hand-crafted heuristics and lists [4]; (b) machine learning approaches to SBD [5; 6; 7; 3]; and more recently (c) deep learning methods [8]. Most of these approaches give fairly accurate results. These systems are based on a number of assumptions [4] that do not hold for noisy texts extracted automatically from PDFs (data is perfectly clean). 1 https://sites.google.com/nlg.csie.ntu.edu.tw/ finnlp/shared-task-finsbd 2 https://sites.google.com/nlg.csie.ntu.edu"
W19-5512,H89-2048,0,0.47742,"number of teams and submitted systems suggests that such shared tasks can indeed generate significant interest in the Finance and NLP research community. 2 Previous Work on SBD While SBD is a foundational pre-processing task, previous research has been confined to clean texts in standard areas such as the news and limited datasets such as the WSJ corpus [2] or the Brown corpus [3]. SBD has been largely explored following several approaches that could be classified into three major classes: (a) rule-based SBD, using hand-crafted heuristics and lists [4]; (b) machine learning approaches to SBD [5; 6; 7; 3]; and more recently (c) deep learning methods [8]. Most of these approaches give fairly accurate results. These systems are based on a number of assumptions [4] that do not hold for noisy texts extracted automatically from PDFs (data is perfectly clean). 1 https://sites.google.com/nlg.csie.ntu.edu.tw/ finnlp/shared-task-finsbd 2 https://sites.google.com/nlg.csie.ntu.edu.tw/ finnlp/home 74 Proceedings of the First Workshop on Financial Technology and Natural Language Processing (FinNLP@IJCAI 2019), pages 74-80, Macao, China, August 12, 2019. Figure 1: Example of the data json file Figure 2: Exa"
W19-5512,A97-1004,0,0.921335,"number of teams and submitted systems suggests that such shared tasks can indeed generate significant interest in the Finance and NLP research community. 2 Previous Work on SBD While SBD is a foundational pre-processing task, previous research has been confined to clean texts in standard areas such as the news and limited datasets such as the WSJ corpus [2] or the Brown corpus [3]. SBD has been largely explored following several approaches that could be classified into three major classes: (a) rule-based SBD, using hand-crafted heuristics and lists [4]; (b) machine learning approaches to SBD [5; 6; 7; 3]; and more recently (c) deep learning methods [8]. Most of these approaches give fairly accurate results. These systems are based on a number of assumptions [4] that do not hold for noisy texts extracted automatically from PDFs (data is perfectly clean). 1 https://sites.google.com/nlg.csie.ntu.edu.tw/ finnlp/shared-task-finsbd 2 https://sites.google.com/nlg.csie.ntu.edu.tw/ finnlp/home 74 Proceedings of the First Workshop on Financial Technology and Natural Language Processing (FinNLP@IJCAI 2019), pages 74-80, Macao, China, August 12, 2019. Figure 1: Example of the data json file Figure 2: Exa"
W19-5512,J06-4003,0,0.870462,"number of teams and submitted systems suggests that such shared tasks can indeed generate significant interest in the Finance and NLP research community. 2 Previous Work on SBD While SBD is a foundational pre-processing task, previous research has been confined to clean texts in standard areas such as the news and limited datasets such as the WSJ corpus [2] or the Brown corpus [3]. SBD has been largely explored following several approaches that could be classified into three major classes: (a) rule-based SBD, using hand-crafted heuristics and lists [4]; (b) machine learning approaches to SBD [5; 6; 7; 3]; and more recently (c) deep learning methods [8]. Most of these approaches give fairly accurate results. These systems are based on a number of assumptions [4] that do not hold for noisy texts extracted automatically from PDFs (data is perfectly clean). 1 https://sites.google.com/nlg.csie.ntu.edu.tw/ finnlp/shared-task-finsbd 2 https://sites.google.com/nlg.csie.ntu.edu.tw/ finnlp/home 74 Proceedings of the First Workshop on Financial Technology and Natural Language Processing (FinNLP@IJCAI 2019), pages 74-80, Macao, China, August 12, 2019. Figure 1: Example of the data json file Figure 2: Exa"
W19-5512,W17-6618,0,0.179243,"shared tasks can indeed generate significant interest in the Finance and NLP research community. 2 Previous Work on SBD While SBD is a foundational pre-processing task, previous research has been confined to clean texts in standard areas such as the news and limited datasets such as the WSJ corpus [2] or the Brown corpus [3]. SBD has been largely explored following several approaches that could be classified into three major classes: (a) rule-based SBD, using hand-crafted heuristics and lists [4]; (b) machine learning approaches to SBD [5; 6; 7; 3]; and more recently (c) deep learning methods [8]. Most of these approaches give fairly accurate results. These systems are based on a number of assumptions [4] that do not hold for noisy texts extracted automatically from PDFs (data is perfectly clean). 1 https://sites.google.com/nlg.csie.ntu.edu.tw/ finnlp/shared-task-finsbd 2 https://sites.google.com/nlg.csie.ntu.edu.tw/ finnlp/home 74 Proceedings of the First Workshop on Financial Technology and Natural Language Processing (FinNLP@IJCAI 2019), pages 74-80, Macao, China, August 12, 2019. Figure 1: Example of the data json file Figure 2: Example of a pdf to text conversion: Text extracted"
W19-5512,W15-5938,0,0.466939,"n generalization towards user-generated Web content. They evaluated several systems on a variety of data sets and report a performance decrease when moving from corpora with formal language to those that are less formal. Thus, designing and implementing approaches customized to different domains attracted the attention of several researchers. Griffis et al., [9] evaluated popular off-the-shelf NLP toolkits on the task of SBD for a set of corpora in the clinical domain. López and Pardo [10] tackle SBD on informal user-generated content such as web reviews, comments, and posts. Rudrapal et al,. [11] present a study on SBD in social media context. SBD from speech transcriptions has also gained a lot of attention due to the necessity of finding sentential segments in the stream of transcripts, automatically recognized [12; 13]. Although text extracted from financial documents such as financial prospectuses, faces problems of text quality caused by segmentation issues, SBD (similarly to most other NLP tasks) has not received much attention in this domain. 3 ""1."",""2."") and to mark the end of a sentence. Also, the dash sign (-) could be used as a hyphen or to mark the math minus sign. Moreove"
W19-5512,P14-5010,0,0.00399273,"as financial prospectuses, faces problems of text quality caused by segmentation issues, SBD (similarly to most other NLP tasks) has not received much attention in this domain. 3 ""1."",""2."") and to mark the end of a sentence. Also, the dash sign (-) could be used as a hyphen or to mark the math minus sign. Moreover, financial prospectuses contain a large number of financial institutions names that appear with their legal form abbreviations (i.e.,""S.A""for Société Anonyme,""LTD."" for Limited Company, etc. Hence, applying commonly used sentence segmentation tools (i.e., Stanford sentence segmenter [14]) that typically rely on punctuation marks or capitalization in the sentence boundary detection (SBD) process is impractical.3 4 Shared Task Data Next, we discuss the corpora used for the English and French subtasks. 4.1 Corpus annotation Financial prospectuses are available online in a pdf format and are also made available from asset managers. We compiled a list of 11 prospectuses in English (140 pages on average) and 92 in French (25 pages on average). These prospectuses are first converted to a text document format using the freely available tool pdf2text4 . Every line in these documents i"
W19-5512,E12-2021,0,0.0331905,"ses, along with their original PDFs.5 We gave them detailed annotation guidelines and asked them to go through every text segment, understand it and mark the boundaries of what they estimate corresponds to a sentence. A sentence is defined as a set of words that is complete in itself, typically containing a subject and predicate, conveying a statement, question, exclamation, or command, and consisting of a main clause and sometimes one or more subordinate clauses. We deliberately asked them not to rely on capitalization and punctuation markers only. We use the online annotation framework BRAT [15]. The tool displays to the annotator each document in a text segment per line format. The annotation labels used in building the corpus are the following: Task Description As part of the First Workshop on FinTech and Natural Language Processing (FinNLP), we introduced the FinSBD shared task which aims at sentence boundary detection in noisy text extracted from financial prospectuses, in two languages: English and French. Systems participating in this shared task were given a set of textual documents extracted from pdf files, which are to be automatically segmented to extract a set of well deli"
W19-5512,W19-5513,0,0.803973,"as a classification task of the center word of a given window. They use two classification methods: (1) LSTM with attention and (2) CNN, both on top of pretrained Glove word level embeddings and specific word embeddings that encode upper case letters. English ES Average 0.89 0.885 0.9 0.875 0.91 0.87 0.89 0.86 0.9 0.855 0.89 0.855 0.88 0.855 0.87 0.845 0.87 0.845 0.89 0.835 0.88 0.835 0.85 0.83 0.86 0.83 0.86 0.83 0.86 0.815 0.86 0.78 0.86 0.77 0.67 0.625 Table 4: Results obtained by the participants for the FinSBD English task. The teams are ordered by the F1 average value (last column). AIG [16] Similarly to many of the other teams, AIG implemented two models: (1) BI-LSTM with CRF on top of pretrained GLOVE word embeddings, and (2) a fined-tuned version of BERT for the sequence labeling. 6 BS 0.88 0.85 0.83 0.83 0.81 0.82 0.83 0.82 0.82 0.78 0.79 0.81 0.8 0.8 0.77 0.7 0.68 0.58 Team seernet aiai1 NUIG1 NUIG2 isi1 isi2 AI_Blues1 AI_Blues2 PolyU_CBS-CFA_RFC1 mhirano1 PolyU2 PolyU_CBS-CFA_NN1 PolyU_CBS-CFA_RFC2 mhirano2 aiai2 Results and Discussion In this section, we describe the evaluation metrics used in the shared task and we give an analysis of the results obtained for the various"
W19-5512,W19-5514,0,0.36836,"French Validation 9 119008 6267 3141 15.9 96.18 90.06 Testing 9 106577 5610 2981 19.22 93.62 89.23 Table 1: Distribution of the Training, Validation and Testing sets used in the English and French corpora. subtask EN subtask FR papers # teams 9 7 8 # std runs 18 15 - Team AIG1 seernet1 aiai1 isi1 NUIG1 isi2 AIG2 AI_Blues2 AI_Blues1 mhirano1 aiai2 NUIG2 HITS-SBD2 HITS-SBD1 PolyU_CBS-CFA_NN1 PolyU_CBS-CFA_RFC1 PolyU_CBS-CFA_RFC2 mhirano2 Table 2: Statistics on the participation in the French and English subatsks. of the word context, and (2) a ruled-based method based on pattern matching. aiai [17] They defined the task as a classification task of the center word of a given window. They use two classification methods: (1) LSTM with attention and (2) CNN, both on top of pretrained Glove word level embeddings and specific word embeddings that encode upper case letters. English ES Average 0.89 0.885 0.9 0.875 0.91 0.87 0.89 0.86 0.9 0.855 0.89 0.855 0.88 0.855 0.87 0.845 0.87 0.845 0.89 0.835 0.88 0.835 0.85 0.83 0.86 0.83 0.86 0.83 0.86 0.815 0.86 0.78 0.86 0.77 0.67 0.625 Table 4: Results obtained by the participants for the FinSBD English task. The teams are ordered by the F1 average va"
W19-5512,W19-5519,0,0.0267851,"87 0.85 0.87 0.85 0.88 0.845 0.68 0.675 0.02 0.015 Table 5: Results obtained by the participants for the FinSBD French task. The teams are ordered by the F1 average value (last column). Discussion Simple ruled-based methods based on obvious punctuation characters can perform very well on SBD, but in order to perform extremely well, we need to take into account the long tail exceptions specially Table 4 reports the results obtained on FinSBD English by the teams detailed in the previous section. For the results on FinSBD French, please check table 5. 78 Team AIG [16] seernet aiai [17] isi NUIG [18] AI_Blues [19] mhirano [20] HITS-SBD [21] PolyU_CBS [22] Affiliation American International Group, United Kingdom SeerNet Technologies, LLC, India OPT, Inc and Herbin institute of technology, Japan and China Information Sciences Institute (University of Southern California), USA National University of Ireland Galway, Ireland Accenture Solutions Pvt Ltd, India The University of Tokyo, Japan Heidelberg Institute for Theoretical Studies, Germany The Hong Kong Polytechnic University, China Tasks English only English and French English and French English and French English and French English and Fr"
