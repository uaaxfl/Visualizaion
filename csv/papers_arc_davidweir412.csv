2021.repl4nlp-1.7,Structure-aware Sentence Encoder in Bert-Based {S}iamese Network,2021,-1,-1,2,0,2463,qiwei peng,Proceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP-2021),0,"Recently, impressive performance on various natural language understanding tasks has been achieved by explicitly incorporating syntax and semantic information into pre-trained models, such as BERT and RoBERTa. However, this approach depends on problem-specific fine-tuning, and as widely noted, BERT-like models exhibit weak performance, and are inefficient, when applied to unsupervised similarity comparison tasks. Sentence-BERT (SBERT) has been proposed as a general-purpose sentence embedding method, suited to both similarity comparison and downstream tasks. In this work, we show that by incorporating structural information into SBERT, the resulting model outperforms SBERT and previous general sentence encoders on unsupervised semantic textual similarity (STS) datasets and transfer classification tasks."
2021.findings-acl.296,Representing Syntax and Composition with Geometric Transformations,2021,-1,-1,3,0,8208,lorenzo bertolini,Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,0,None
2021.eacl-main.89,Data Augmentation for Hypernymy Detection,2021,-1,-1,4,1,10629,thomas kober,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,0,"The automatic detection of hypernymy relationships represents a challenging problem in NLP. The successful application of state-of-the-art supervised approaches using distributed representations has generally been impeded by the limited availability of high quality training data. We have developed two novel data augmentation techniques which generate new training examples from existing ones. First, we combine the linguistic principles of hypernym transitivity and intersective modifier-noun composition to generate additional pairs of vectors, such as {``}small dog - dog{''} or {``}small dog - animal{''}, for which a hypernymy relationship can be assumed. Second, we use generative adversarial networks (GANs) to generate pairs of vectors for which the hypernymy relation can also be assumed. We furthermore present two complementary strategies for extending an existing dataset by leveraging linguistic resources such as WordNet. Using an evaluation across 3 different datasets for hypernymy detection and 2 different vector spaces, we demonstrate that both of the proposed automatic data augmentation and dataset extension strategies substantially improve classifier performance."
2020.coling-main.36,Leveraging {HTML} in Free Text Web Named Entity Recognition,2020,-1,-1,2,0,21085,colin ashby,Proceedings of the 28th International Conference on Computational Linguistics,0,"HTML tags are typically discarded in free text Named Entity Recognition from Web pages. We investigate whether these discarded tags might be used to improve NER performance. We compare Text+Tags sentences with their Text-Only equivalents, over five datasets, two free text segmentation granularities and two NER models. We find an increased F1 performance for Text+Tags of between 0.9{\%} and 13.2{\%} over all datasets, variants and models. This performance increase, over datasets of varying entity types, HTML density and construction quality, indicates our method is flexible and adaptable. These findings imply that a similar technique might be of use in other Web-aware NLP tasks, including the enrichment of deep language models."
W17-1910,One Representation per Word - Does it make Sense for Composition?,2017,31,2,5,1,10629,thomas kober,"Proceedings of the 1st Workshop on Sense, Concept and Entity Representations and their Applications",0,"In this paper, we investigate whether an a priori disambiguation of word senses is strictly necessary or whether the meaning of a word in context can be disambiguated through composition alone. We evaluate the performance of off-the-shelf single-vector and multi-sense vector models on a benchmark phrase similarity task and a novel task for word-sense discrimination. We find that single-sense vector models perform as well or better than multi-sense vector models despite arguably less clean elementary representations. Our findings furthermore show that simple composition functions such as pointwise addition are able to recover sense specific information from a single-sense vector model remarkably well."
P17-2069,Improving Semantic Composition with Offset Inference,2017,32,0,4,1,10629,thomas kober,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Count-based distributional semantic models suffer from sparsity due to unobserved but plausible co-occurrences in any text collection. This problem is amplified for models like Anchored Packed Trees (APTs), that take the grammatical type of a co-occurrence into account. We therefore introduce a novel form of distributional inference that exploits the rich type structure in APTs and infers missing data by the same mechanism that is used for semantic composition."
E17-2085,When a Red Herring in Not a Red Herring: Using Compositional Methods to Detect Non-Compositional Phrases,2017,20,0,4,1,2465,julie weeds,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers",0,"Non-compositional phrases such as \textit{red herring} and weakly compositional phrases such as \textit{spelling bee} are an integral part of natural language (Sag, 2002). They are also the phrases that are difficult, or even impossible, for good compositional distributional models of semantics. Compositionality detection therefore provides a good testbed for compositional methods. We compare an integrated compositional distributional approach, using sparse high dimensional representations, with the ad-hoc compositional approach of applying simple composition operations to state-of-the-art neural embeddings."
W16-2502,A critique of word similarity as a method for evaluating distributional semantic models,2016,26,28,5,0,33869,miroslav batchkarov,Proceedings of the 1st Workshop on Evaluating Vector-Space Representations for {NLP},0,"This paper aims to re-think the role of the word similarity task in distributional semantics research. We argue while it is a valuable tool, it should be used with care because it provides only an approximate measure of the quality of a distributional model. Word similarity evaluations assume there exists a single notion of similarity that is independent of a particular application. Further, the small size and low inter-annotator agreement of existing data sets makes it challenging to find significant differences between models."
J16-4006,Aligning Packed Dependency Trees: A Theory of Composition for Distributional Semantics,2016,0,1,1,1,2464,david weir,Computational Linguistics,0,We present a new framework for compositional distributional semantics in which the distributional contexts of lexemes are expressed in terms of anchored packed dependency trees. We show that these structures have the potential to capture the full sentential contexts of a lexeme and provide a uniform basis for the composition of distributional knowledge in a way that captures both mutual disambiguation and generalization.
D16-1175,Improving Sparse Word Representations with Distributional Inference for Semantic Composition,2016,48,6,4,1,10629,thomas kober,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,"Distributional models are derived from co-occurrences in a corpus, where only a small proportion of all possible plausible co-occurrences will be observed. This results in a very sparse vector space, requiring a mechanism for inferring missing knowledge. Most methods face this challenge in ways that render the resulting word representations uninterpretable, with the consequence that semantic composition becomes hard to model. In this paper we explore an alternative which involves explicitly inferring unobserved co-occurrences using the distributional neighbourhood. We show that distributional inference improves sparse word representations on several word similarity benchmarks and demonstrate that our model is competitive with the state-of-the-art for adjective-noun, noun-noun and verb-object compositions while being fully interpretable."
W15-2906,Optimising Agile Social Media Analysis,2015,25,1,2,1,10629,thomas kober,"Proceedings of the 6th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis",0,"Agile social media analysis involves building bespoke, one-off classification pipelines tailored to the analysis of specific datasets. In this study we investigate how the DUALIST architecture can be optimised for agile social media analysis. We evaluate several semi-supervised learning algorithms in conjunction with a Na ive Bayes model, and show how these modifications can improve the performance of bespoke classifiers for a variety of tasks on a large range of datasets."
W14-1502,Distributional Composition using Higher-Order Dependency Vectors,2014,29,8,2,1,2465,julie weeds,Proceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositionality ({CVSC}),0,This paper concerns how to apply compositional methods to vectors based on grammatical dependency relation vectors. We demonstrate the potential of a novel approach which uses higher-order grammatical dependency relations as features. We apply the approach to adjective-noun compounds with promising results in the prediction of the vectors for (held-out) observed phrases.
P14-1058,Learning to Predict Distributions of Words Across Domains,2014,36,15,2,0.790923,9798,danushka bollegala,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Although the distributional hypothesis has been applied successfully in many natural language processing tasks, systems using distributional information have been limited to a single domain because the distribution of a word can vary between domains as the wordxe2x80x99s predominant meaning changes. However, if it were possible to predict how the distribution of a word changes from one domain to another, the predictions could be used to adapt a system trained in one domain to work in another. We propose an unsupervised method to predict the distribution of a word in one domain, given its distribution in another domain. We evaluate our method on two tasks: cross-domain partof-speech tagging and cross-domain sentiment classification. In both tasks, our method significantly outperforms competitive baselines and returns results that are statistically comparable to current stateof-the-art methods, while requiring no task-specific customisations."
C14-2025,Method51 for Mining Insight from Social Media Datasets,2014,12,2,2,0,40207,simon wibberley,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: System Demonstrations",0,"We present Method51, a social media analysis software platform with a set of accompanying methodologies. We discuss a series of case studies illustrating the platformxe2x80x99s application, and motivating our methodological proposals."
C14-1212,Learning to Distinguish Hypernyms and Co-Hyponyms,2014,39,80,4,1,2465,julie weeds,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"This work is concerned with distinguishing different semantic relations which exist between distributionally similar words. We compare a novel approach based on training a linear Support Vector Machine on pairs of feature vectors with state-of-the-art methods based on distributional similarity. We show that the new supervised approach does better even when there is minimal information about the target words in the training data, giving a 15% reduction in error rate over unsupervised approaches."
W13-2705,Language Technology for Agile Social Media Science,2013,20,2,2,0,40207,simon wibberley,"Proceedings of the 7th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities",0,"We present an extension of the DUALIST tool that enables social scientists to engage directly with large Twitter datasets. Our approach supports collaborative construction of classifiers and associated gold standard data sets. The tool can be used to build classifier cascades that decomposes tweet streams, and provide analysis of targeted conversations. A central concern is to provide an environment in which social science researchers can rapidly develop an informed sense of what the datasets look like. The intent is that they develop, not only an informed view as to how the data could be fruitfully analysed, but also how feasible it is to analyse it in that way."
W11-0136,Algebraic Approaches to Compositional Distributional Semantics,2011,-1,-1,2,1,37161,daoud clarke,Proceedings of the Ninth International Conference on Computational Semantics ({IWCS} 2011),0,None
P11-1014,Using Multiple Sources to Construct a Sentiment Sensitive Thesaurus for Cross-Domain Sentiment Classification,2011,20,80,2,0.790923,9798,danushka bollegala,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,1,"We describe a sentiment classification method that is applicable when we do not have any labeled data for a target domain but have some labeled data for multiple other domains, designated as the source domains. We automatically create a sentiment sensitive thesaurus using both labeled and unlabeled data from multiple source domains to find the association between words that express similar sentiments in different domains. The created thesaurus is then used to expand feature vectors to train a binary classifier. Unlike previous cross-domain sentiment classification methods, our method can efficiently learn from multiple source domains. Our method significantly outperforms numerous baselines and returns results that are better than or comparable to previous cross-domain sentiment classification methods on a benchmark dataset containing Amazon user reviews for different types of products."
J11-3004,Dependency Parsing Schemata and Mildly Non-Projective Dependency Parsing,2011,67,19,3,0.833333,2685,carlos gomezrodriguez,Computational Linguistics,0,"We introduce dependency parsing schemata, a formal framework based on Sikkel's parsing schemata for constituency parsers, which can be used to describe, analyze, and compare dependency parsing algorithms. We use this framework to describe several well-known projective and non-projective dependency parsers, build correctness proofs, and establish formal relationships between them. We then use the framework to define new polynomial-time parsing algorithms for various mildly non-projective dependency formalisms, including well-nested structures with their gap degree bounded by a constant k in time O(n52k), and a new class that includes all gap degree k structures present in several natural language treebanks (which we call mildly ill-nested structures for gap degree k) in time O(n43k). Finally, we illustrate how the parsing schema framework can be applied to Link Grammar, a dependency-related formalism."
W10-2806,Semantic Composition with Quotient Algebras,2010,-1,-1,3,1,37161,daoud clarke,Proceedings of the 2010 Workshop on {GE}ometrical Models of Natural Language Semantics,0,None
N09-1061,Optimal Reduction of Rule Length in Linear Context-Free Rewriting Systems,2009,17,28,4,1,2685,carlos gomezrodriguez,Proceedings of Human Language Technologies: The 2009 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics,0,"Linear Context-free Rewriting Systems (LCFRS) is an expressive grammar formalism with applications in syntax-based machine translation. The parsing complexity of an LCFRS is exponential in both the rank of a production, defined as the number of nonterminals on its right-hand side, and a measure for the discontinuity of a phrase, called fan-out. In this paper, we present an algorithm that transforms an LCFRS into a strongly equivalent form in which all productions have rank at most 2, and has minimal fan-out. Our results generalize previous work on Synchronous Context-Free Grammar, and are particularly relevant for machine translation from or to languages that require syntactic analyses with discontinuous constituents."
E09-1034,Parsing Mildly Non-Projective Dependency Structures,2009,28,21,2,1,2685,carlos gomezrodriguez,Proceedings of the 12th Conference of the {E}uropean Chapter of the {ACL} ({EACL} 2009),0,"We present parsing algorithms for various mildly non-projective dependency formalisms. In particular, algorithms are presented for: all well-nested structures of gap degree at most 1, with the same complexity as the best existing parsers for constituency formalisms of equivalent generative power; all well-nested structures with gap degree bounded by any constant k; and a new class of structures with gap degree up to k that includes some ill-nested structures. The third case includes all the gap degree k structures in a number of dependency treebanks."
P08-1110,A Deductive Approach to Dependency Parsing,2008,22,11,3,1,2685,carlos gomezrodriguez,Proceedings of ACL-08: HLT,1,"We define a new formalism, based on Sikkelxe2x80x99s parsing schemata for constituency parsers, that can be used to describe, analyze and compare dependency parsing algorithms. This"
W07-2304,Modelling control in generation,2007,10,3,2,0,32465,roger evans,Proceedings of the Eleventh {E}uropean Workshop on Natural Language Generation ({ENLG} 07),0,"In this paper we present a view of natural language generation in which the control structure of the generator is clearly separated from the content decisions made during generation, allowing us to explore and compare different control strategies in a systematic way. Our approach factors control into two components, a 'generation tree' which maps out the relationships between different decisions, and an algorithm for traversing such a tree which determines which choices are actually made. We illustrate the approach with examples of stylistic control and automatic text revision using both generative and empirical techniques. We argue that this approach provides a useful basis for the theoretical study of control in generation, and a framework for implementing generators with a range of control strategies. We also suggest that this approach can be developed into tool for analysing and adapting control aspects of other advanced wide-coverage generation systems."
W05-1202,The Distributional Similarity of Sub-Parses,2005,21,18,2,1,2465,julie weeds,Proceedings of the {ACL} Workshop on Empirical Modeling of Semantic Equivalence and Entailment,0,"This work explores computing distributional similarity between sub-parses, i. e., fragments of a parse tree, as an extension to general lexical distributional similarity techniques. In the same way that lexical distributional similarity is used to estimate lexical semantic similarity, we propose using distributional similarity between subparses to estimate the semantic similarity of phrases. Such a technique will allow us to identify paraphrases where the component words are not semantically similar. We demonstrate the potential of the method by applying it to a small number of examples and showing that the paraphrases are more similar than the non-paraphrases."
J05-4002,Co-occurrence Retrieval: A Flexible Framework for Lexical Distributional Similarity,2005,57,126,2,1,2465,julie weeds,Computational Linguistics,0,"Techniques that exploit knowledge of distributional similarity between words have been proposed in many areas of Natural Language Processing. For example, in language modeling, the sparse data problem can be alleviated by estimating the probabilities of unseen co-occurrences of events from the probabilities of seen co-occurrences of similar events. In other applications, distributional similarity is taken to be an approximation to semantic similarity. However, due to the wide range of potential applications and the lack of a strict definition of the concept of distributional similarity, many methods of calculating distributional similarity have been proposed or adopted.In this work, a flexible, parameterized framework for calculating distributional similarity is proposed. Within this framework, the problem of finding distributionally similar words is cast as one of co-occurrence retrieval (CR) for which precision and recall can be measured by analogy with the way they are measured in document retrieval. As will be shown, a number of popular existing measures of distributional similarity are simulated with parameter settings within the CR framework. In this article, the CR framework is then used to systematically investigate three fundamental questions concerning distributional similarity. First, is the relationship of lexical similarity necessarily symmetric, or are there advantages to be gained from considering it as an asymmetric relationship? Second, are some co-occurrences inherently more salient than others in the calculation of distributional similarity? Third, is it necessary to consider the difference in the extent to which each word occurs in each co-occurrence type?Two application-based tasks are used for evaluation: automatic thesaurus generation and pseudo-disambiguation. It is possible to achieve significantly better results on both these tasks by varying the parameters within the CR framework rather than using other existing distributional similarity measures; it will also be shown that any single unparameterized measure is unlikely to be able to do better on both tasks. This is due to an inherent asymmetry in lexical substitutability and therefore also in lexical distributional similarity."
C04-1146,Characterising Measures of Lexical Distributional Similarity,2004,21,208,2,1,2465,julie weeds,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"This work investigates the variation in a word's distributionally nearest neighbours with respect to the similarity measure used. We identify one type of variation as being the relative frequency of the neighbour words with respect to the frequency of the target word. We then demonstrate a three-way connection between relative frequency of similar words, a concept of distributional gnerality and the semantic relation of hyponymy. Finally, we consider the impact that this has on one application of distributional similarity methods (judging the compositionality of collocations)."
W03-1011,A General Framework for Distributional Similarity,2003,17,89,2,1,2465,julie weeds,Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing,0,"We present a general framework for distributional similarity based on the concepts of precision and recall. Different parameter settings within this framework approximate different existing similarity measures as well as many more which have, until now, been unexplored. We show that optimal parameter settings outperform two existing state-of-the-art similarity measures on two evaluation tasks for high and low frequency nouns."
W02-2229,Evaluation of {LTAG} Parsing with Supertag Compaction,2002,16,2,3,0,53118,olga shaumyan,Proceedings of the Sixth International Workshop on Tree Adjoining Grammar and Related Frameworks ({TAG}+6),0,"One of the biggest concerns that has been raised over the feasibility of using large-scale LTAGs in NLP is the amount of redundancy within a grammar?s elementary tree set. This has led to various proposals on how best to represent grammars in a way that makes them compact and easily maintained (Vijay-Shanker and Schabes, 1992; Becker, 1993; Becker, 1994; Evans, Gazdar and Weir, 1995; Candito, 1996). Unfortunately, while this work can help to make the storage of grammars more efficient, it does nothing to prevent the problem reappearing when the grammar is processed by a parser and the complete set of trees is reproduced. In this paper we are concerned with an approach that addresses this problem of computational redundancy in the trees, and evaluate its effectiveness."
J02-2003,Class-Based Probability Estimation Using a Semantic Hierarchy,2002,25,107,2,1,20968,stephen clark,Computational Linguistics,0,"This article concerns the estimation of a particular kind of probability, namely, the probability of a noun sense appearing as a particular argument of a predicate. In order to overcome the accompanying sparse-data problem, the proposal here is to define the probabilities in terms of senses from a semantic hierarchy and exploit the fact that the senses can be grouped into classes consisting of semantically similar senses. There is a particular focus on the problem of how to determine a suitable class for a given sense, or, alternatively, how to determine a suitable level of generalization in the hierarchy. A procedure is developed that uses a chi-square test to determine a suitable level of generalization. In order to test the performance of the estimation method, a pseudo-disambiguation task is used, together with two alternative estimation methods. Each method uses a different generalization procedure; the first alternative uses the minimum description length principle, and the second uses Resnik's measure of selectional preference. In addition, the performance of our method is investigated using both the standard Pearson chi-square statistic and the log-likelihood chi-square statistic."
N01-1013,Class-Based Probability Estimation Using a Semantic Hierarchy,2001,25,26,2,1,20968,stephen clark,Second Meeting of the North {A}merican Chapter of the Association for Computational Linguistics,0,"This paper concerns the acquisition of a particular kind of lexical knowledge, namely the knowledge of which noun senses can fill argument slots of predicates. Probabilities are used to represent the knowledge, and classes from a semantic hierarchy are used to estimate the probabilities. There is a particular focus on the problem of how to determine a suitable class, or level of generalisation, in the hierarchy. A pseudo disambiguation task is used to compare different class-based estimation methods."
J01-1004,{D}-Tree Substitution Grammars,2001,40,38,3,0.139892,1354,owen rambow,Computational Linguistics,0,"There is considerable interest among computational linguists in lexicalized grammatical frame-works; lexicalized tree adjoining grammar (LTAG) is one widely studied example. In this paper, we investigate how derivations in LTAG can be viewed not as manipulations of trees but as manipulations of tree descriptions. Changing the way the lexicalized formalism is viewed raises questions as to the desirability of certain aspects of the formalism. We present a new formalism, d-tree substitution grammar (DSG). Derivations in DSG involve the composition of d-trees, special kinds of tree descriptions. Trees are read off from derived d-trees. We show how the DSG formalism, which is designed to inherit many of the characterestics of LTAG, can be used to express a variety of linguistic analyses not available in LTAG."
W00-2007,Engineering a Wide-Coverage Lexicalized Grammar,2000,6,6,5,0,17923,john carroll,Proceedings of the Fifth International Workshop on Tree Adjoining Grammar and Related Frameworks ({TAG}+5),0,"We discuss a number of practical issues that have arisen in the development of a wide-coverage lexicalized grammar for English. In particular, we consider the way in which the design of the grammar and of its encoding was influenced by issues relating to the size of the grammar. 1. Introduction Hand-crafting a wide-coverage grammar is a difficult task, requiring consideration of a seemingly endless number of constructions in an attempt to produce a treatment that is as uniform and comprehensive as possible. In this paper we discuss a number of practical issues that have arisen in the development of a wide-coverage lexicalized grammar for English: the LEXSYS grammar. In particular, we consider the way in which the design of the grammar and of its encodingxe2x80x94from the viewpoint both of the grammar writer and of the parsing mechanismxe2x80x94was"
C00-1029,A Class-based Probabilistic approach to Structural Disambiguation,2000,18,14,2,1,20968,stephen clark,{COLING} 2000 Volume 1: The 18th International Conference on Computational Linguistics,0,"Knowledge of which words are able to fill particular argument slots of a predicate can be used for structural disambiguation. This paper describes a proposal for acquiring such knowledge, and in line with much of the recent work in this area, a probabilistic approach is taken. We develop a novel way of using a semantic hierarchy to estimate the probabilities, and demonstrate the general approach using a prepositional phrase attachment experiment."
W99-0631,An Iterative Approach to Estimating Frequencies over a Semantic Hierarchy,1999,10,14,2,1,20968,stephen clark,1999 Joint {SIGDAT} Conference on Empirical Methods in Natural Language Processing and Very Large Corpora,0,"This paper is concerned with using a semant ic hierarchy to es t imate the frequency with which a word sense appears as a given argument of a verb, assuming the da ta is not sense disambiguated. The s tandard approach is to split the count for any noun appearing in the da t a equally among the alternat ive senses of the noun. This can lead to inaccurate estimates. We describe a rees t imation process which uses the accumulated counts of hypernyms of the al ternat ive senses in order to redis t r ibute the count. In order to choose a hypernym for each alternative sense, we employ a novel technique which uses a X 2 test to measure the homogeneity of sets of concepts in the hierarchy."
E99-1029,Parsing with an Extended Domain of Locality,1999,23,7,5,0,17923,john carroll,Ninth Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"One of the claimed benefits of Tree Adjoining Grammars is that they have an extended domain of locality (EDOL). We consider how this can be exploited to limit the need for feature structure unification during parsing. We compare two wide-coverage lexicalized grammars of English, LEXSYS and XTAG, finding that the two grammars exploit EDOL in different ways."
W98-0108,The {L}ex{S}ys project,1998,8,4,5,0,17923,john carroll,Proceedings of the Fourth International Workshop on Tree Adjoining Grammars and Related Frameworks ({TAG}+4),0,None
P98-1061,A Structure-sharing Parser for Lexicalized Grammars,1998,10,13,2,1,32465,roger evans,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 1",1,"In wide-coverage lexicalized grammars many of the elementary structures have substructures in common. This means that in conventional parsing algorithms some of the computation associated with different structures is duplicated. In this paper we describe a precompilation technique for such grammars which allows some of this computation to be shared. In our approach the elementary structures of the grammar are transformed into finite state automata which can be merged and minimised using standard algorithms, and then parsed using an automaton-based parser. We present algorithms for constructing automata from elementary structures, merging and minimising them, and string recognition and parse recovery with the resulting grammar."
C98-1059,A structure-sharing parser for lexicalized grammars,1998,10,13,2,1,32465,roger evans,{COLING} 1998 Volume 1: The 17th International Conference on Computational Linguistics,0,"In wide-coverage lexicalized grammars many of the elementary structures have substructures in common. This means that in conventional parsing algorithms some of the computation associated with different structures is duplicated. In this paper we describe a precompilation technique for such grammars which allows some of this computation to be shared. In our approach the elementary structures of the grammar are transformed into finite state automata which can be merged and minimised using standard algorithms, and then parsed using an automaton-based parser. We present algorithms for constructing automata from elementary structures, merging and minimising them, and string recognition and parse recovery with the resulting grammar."
1997.iwpt-1.6,Encoding Frequency Information in Lexicalized Grammars,1997,-1,-1,2,0,17923,john carroll,Proceedings of the Fifth International Workshop on Parsing Technologies,0,"We address the issue of how to associate frequency information with lexicalized grammar formalisms, using Lexicalized Tree Adjoining Grammar as a representative framework. We consider systematically a number of alternative probabilistic frameworks, evaluating their adequacy from both a theoretical and empirical perspective using data from existing large treebanks. We also propose three orthogonal approaches fo r backing off probability estimates to cope with the large number of parameters involved."
1997.iwpt-1.11,Automaton-based Parsing for Lexicalised Grammars,1997,0,11,2,1,32465,roger evans,Proceedings of the Fifth International Workshop on Parsing Technologies,0,In wide-coverage lexicalized grammars many of the elementary structures have substructures in common. This means that during parsing some of the computation associated with different structures is duplicated. This paper explores ways in which the grammar can be precompiled into finite state automata so that some of this shared structure results in shared computation at run-time.
P95-1011,Encoding {L}exicalized {T}ree {A}djoining {G}rammars with a Nonmonotonic Inheritance Hierarchy,1995,23,20,3,1,32465,roger evans,33rd Annual Meeting of the Association for Computational Linguistics,1,"This paper shows how DATR, a widely used formal language for lexical knowledge representation, can be used to define an LTAG lexicon as an inheritance hierarchy with internal lexical rules. A bottom-up featural encoding is used for LTAG trees and this allows lexical rules to be implemented as covariation constraints within feature structures. Such an approach eliminates the considerable redundancy otherwise associated with an LTAG lexicon."
P95-1021,{D}-Tree Grammars,1995,18,95,3,0.139892,1354,owen rambow,33rd Annual Meeting of the Association for Computational Linguistics,1,"DTG are designed to share some of the advantages of TAG while overcoming some of its limitations. DTG involve two composition operations called subsertion and sister-adjunction. The most distinctive feature of DTG is that, unlike TAG, there is complete uniformity in the way that the two DTG operations relate lexical items: subsertion always corresponds to complementation and sister-adjunction to modification. Furthermore, DTG, unlike TAG, can provide a uniform analysis for wh-movement in English and Kashmiri, despite the fact that the wh element in Kashmiri appears in sentence-second position, and not sentence-initial position as in English."
E95-1011,A Tractable Extension of Linear Indexed Grammars,1995,16,10,2,0,34289,bill keller,Seventh Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"Vijay-Shanker and Weir (1993) show that Linear Indexed Grammars (LIG) can be processed in polynomial time by exploiting constraints which make possible the extensive use of structure-sharing. This paper describes a formalism that is more powerful than LIG, but which can also be processed in polynomial time using similar techniques. The formalism, which we refer to as Partially Linear PATR (PLPATR) manipulates feature structures rather than stacks."
1995.iwpt-1.30,Parsing {D}-Tree Grammars,1995,0,11,2,0.402787,12137,vijayshanker,Proceedings of the Fourth International Workshop on Parsing Technologies,0,
J93-4002,Parsing Some Constrained Grammar Formalisms,1993,22,79,2,0.503484,12137,vijayshanker,Computational Linguistics,0,In this paper we present a scheme to extend a recognition algorithm for Context-Free Grammars (CFG) that can be used to derive polynomial-time recognition algorithms for a set of for-malisms that generate a superset of languages generated by CFG. We describe the scheme by developing a Cocke-Kasami-Younger (CKY)-like pure bottom-up recognition algorithm for Linear Indexed Grammars and show how it can be adapted to give algorithms for Tree Adjoining Grammars and Combinatory Categorial Grammars. This is the only polynomial-time recognition algorithm for Combinatory Categorial Grammars that we are aware of.The main contribution of this paper is the general scheme we propose for parsing a variety of formalisms whose derivation process is controlled by an explicit or implicit stack. The ideas presented here can be suitably modified for other parsing styles or used in the generalized framework set out by Lang (1990).
P92-1018,Linear Context-Free Rewriting Systems and Deterministic Tree-Walking Transducers,1992,12,26,1,1,2464,david weir,30th Annual Meeting of the Association for Computational Linguistics,1,"We show that the class of string languages generated by linear context-free rewriting systems is equal to the class of output languages of deterministic tree-walking transducers. From equivalences that have previously been established we know that this class of languages is also equal to the string languages generated by context-free hypergraph grammars, multicomponent tree-adjoining grammars, and multiple context-free grammars and to the class of yields of images of the regular tree languages under finite-copying top-down tree transducers."
W90-0204,Multicomponent {T}ree {A}djoining {G}rammars,1990,0,0,1,1,2464,david weir,Proceedings of the First International Workshop on Tree Adjoining Grammar and Related Frameworks ({TAG}+1),0,None
P90-1001,Polynomial Time Parsing of {C}ombinatory {C}ategorial {G}rammars,1990,11,49,2,0.246914,12137,vijayshanker,28th Annual Meeting of the Association for Computational Linguistics,1,"In this paper we present a polynomial time parsing algorithm for Combinatory Categorial Grammar. The recognition phase extends the CKY algorithm for CFG. The process of generating a representation of the parse trees has two phases. Initially, a shared forest is build that encodes the set of all derivation trees for the input string. This shared forest is then pruned to remove all spurious ambiguity."
W89-0218,Recognition of {C}ombinatory {C}ategorial {G}rammars and Linear Indexed Grammars,1989,0,1,2,0.454545,12137,vijayshanker,Proceedings of the First International Workshop on Parsing Technologies,0,
P88-1034,{C}ombinatory {C}ategorial {G}rammars: Generative Power and Relationship to Linear Context-Free Rewriting Systems,1988,14,39,1,1,2464,david weir,26th Annual Meeting of the Association for Computational Linguistics,1,"Recent results have established that there is a family of languages that is exactly the class of languages generated by three independently developed grammar formalisms: Tree Adjoining Grammars, Head Grammars, and Linear Indexed Grammars. In this paper we show that Combinatory Categorial Grammars also generates the same class of languages. We discuss the structural descriptions produced by Combinatory Categorial Grammars and compare them to those of grammar formalisms in the class of Linear Context-Free Rewriting Systems. We also discuss certain extensions of Combinatory Categorial Grammars and their effect on the weak generative capacity."
P87-1015,Characterizing Structural Descriptions Produced by Various Grammatical Formalisms,1987,13,217,2,0,12137,vijayshanker,25th Annual Meeting of the Association for Computational Linguistics,1,"We consider the structural descriptions produced by various grammatical formalisms in terms of the complexity of the paths and the relationship between paths in the sets of structural descriptions that each system can generate. In considering the relationship between formalisms, we show that it is useful to abstract away from the details of the formalism, and examine the nature of their derivation process as reflected by properties of their derivation trees. We find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars. On the basis of this observation, we describe a class of formalisms which we call Linear Context-Free Rewriting Systems, and show they are recognizable in polynomial time and generate only semilinear languages."
C86-1048,Tree Adjoining and Head Wrapping,1986,4,30,2,0,12137,vijayshanker,Coling 1986 Volume 1: The 11th International Conference on Computational Linguistics,0,"In this paper we discuss the formal relationship between thw classes of languages generated by Tree Adjoining Grammars and Head Grammars. In particular, we show that Head Languages are included in Tree Adjoining Languages and that Tree Adjoining Grammars are equivalent to a modification of Head Grammars called Modified Head Grammars. The inclusion of MHL in HL, and thus the equivalence of HG's and TAG's in the most general case remains to be established."
