2009.eamt-1.9,W07-1608,0,0.0185651,"he preposition translation module is later used in subsequent modules in the structural transfer and generation phases. Note that errors from previous modules affect the quality of the preposition translation phase, and this makes the separate evaluation of preposition translation a difficult task. We will get back to this problem in Section 6. tactic and semantic information. While the latter rules have been coded manually, the first two resources have been automatically extracted from monolingual corpora. One important contribution of this paper is the evaluation methodology. Previous work (Husain et al., 2007; Gustavii, 2005) on preposition translation measured only accuracy gains with respect to simple baselines, and focused on small sets of frequent prepositions. Our methodology measures both precision and recall over all prepositions occurring in a small corpus of randomly chosen sentences. Once the evaluation corpus has been compiled, the evaluation is fully automatic. The results of this paper shows that all proposed techniques improve over the baselines, including a translation dictionary compiled from an aligned corpus, and over a full-fledged statistical machine translation (SMT) system. T"
2009.eamt-1.9,P91-1020,0,0.200151,"ual corpora. The results obtained using a new evaluation methodology show that all proposed techniques improve precision over the baselines, including a translation dictionary compiled from an aligned corpus, and a state-of-the-art statistical Machine Translation system. The results also show that linguistic information in all three techniques are complementary, and that a combination of them obtains the best F-score results overall. 1 Introduction Since the first Machine Translation (MT) systems up to today’s, performing well the translation of the prepositions is relevant for any MT system; Japkowicz and Wiebe (1991) claimed that doing it correctly is difficult because prepositions cannot be translated in a systematic or coherent way. Koehn (2003) remarked the importance of the correct translation of prepositions and he also reported that the main reason for noun phrase (NP) and 1 When we use here the word postposition, we would like to refer to grammatical cases and postpositions c 2009 European Association for Machine Translation. Proceedings of the 13th Annual Conference of the EAMT, pages 58–65, Barcelona, May 2009 58 mation. The freely available open-source Matxin system is the first MT system availa"
2009.eamt-1.9,N03-1017,0,0.00828042,"s were used to reduce the errors caused by unergative verbs and wrong verb attachment decisions. 4.2 Head word PRONOUN PRONOUN PERSON LOCATION talde (group) LOCATION ORGANIZATION partidu (match) Table 2: Dependency triples for verb ikusi. pendency triples to disambiguate the prepositions heading verbal complements. 5.1 Baselines The baseline dictionary uses the preposition translations in the Elhuyar dictionary (Elhuyar, 2000), the most popular Spanish-Basque dictionary. The first postposition is taken as the preferred translation. The aligned corpora baseline was constructed applying Giza++ (Koehn et al., 2003) to the Consumer magazine parallel corpus (Alcazar, 2006). This corpus contains 60,000 parallel sentences in Spanish (1.3 Mwords) and Basque (1 Mwords). The Basque part of the corpus was morphologically analyzed and segmented, i.e. word forms were split into their lemma and postposition (e.g.: etxetik (from the house) → etxe (the house) + tik (from)). After preprocessing the Basque sentences, we aligned the text automatically and extracted for each Spanish preposition its most frequent corresponding Basque postposition. This alignment technique proved to be superior to wordbase alignment (Agir"
2009.eamt-1.9,W06-3114,0,0.0168814,"erent baselines and techniques to translate prepositions. Our evaluation methodology is proposed in Section 6, which is followed by Section 7 with the results. Finally, Section 8 is devoted to conclusions and future work. 2 Preposition translation in RBMT The last decade has seen the raise of SMT techniques, and less research on rule-based techniques. Nevertheless, translation involving a lessresourced language poses serious difficulties for SMT, specially caused by the smaller size of parallel corpora. Morphologically-rich languages have also been proved to be difficult for SMT, as shown in (Koehn and Monz, 2006), where SMT systems lag well behind commercial RBMT systems. At present, domain-specific translation memories for Basque are no bigger than two or three million words, much smaller than corpora used for other languages (the Europarl parallel corpus, for instance, has ca. 30 Mwords). Having limited digital resources, the rule-based approach is suitable for the development of an MT system for Basque, along with a focus on the enhancement of the core RBMT system with statistical and linguistic infor3 Related work Koehn (2003) envisages MT as a divide and conquer task where improving NP/PP transla"
2009.eamt-1.9,2007.mtsummit-papers.40,1,0.920443,"Missing"
2009.eamt-1.9,W04-2608,0,0.0291545,"focus on the enhancement of the core RBMT system with statistical and linguistic infor3 Related work Koehn (2003) envisages MT as a divide and conquer task where improving NP/PP translation will carry an improvement of the whole system. That study concluded that the main source of re-ranking errors in NP/PPs translation was the inability to correctly predict the phrase start (preposition or determiner) without context; it can sometimes only be resolved when the English verb is chosen and its subcategorization is known. There are two main approaches to disambiguate prepositions (Mamidi, 2004; Alam, 2004; Trujillo, 1992): context based (used in transfer systems and more suitable for languages that are structurally different) and concept based (used in interlingua 59 Freq. 4289.78 1534.24 975.31 476.70 166.68 systems and more suitable for languages which are very close). Most of the systems are context based and they use transfer rules given with semantic information for the nouns which are head and complement of the preposition. Transitivity transitive intransitive transitive intransitive transitive Postpositions ABS,ERG ABS ABS,ERG,INE ABS,INE ABS,ERG,INS Table 1: Subcategorization for verb"
2009.eamt-1.9,przybocki-etal-2006-edit,0,0.127068,". It is a rule-based transfer system based on deep syntactic analysis. which currently translates from Spanish into Basque, and is currently being adapted to the English-Basque pair. The current development status shows that it is useful for content assimilation, for text understanding indeed, but that it is not yet suitable for unrestricted use in text dissemination. Matxin has been evaluated and compared with the state-of-the-art corpus-based Matrex MT system (Stroppa et al., 2006; Labaka, 2007) translating from Spanish to Basque. The evaluation was performed using the edit-distance metric (Przybocki et al., 2006), based on the HTER (humantargeted translation edit rate) presented in (Snover et al., 2006), and the comparative results have shown that Matxin performs significantly better: 43.60 vs. 57.97 in the parallel corpus where Matrex was trained, and 40.41 vs. 71.87 in an out-ofdomain corpus. The preposition translation module of Matxin is located in the structural transfer phase and uses the information carried over from the syntactic analysis and lexical transfer modules. The system currently uses Freeling analyzer for Spanish (Atserias et al., 2006). The output of the preposition translation modu"
2009.eamt-1.9,atserias-etal-2006-freeling,0,0.0225619,"Missing"
2009.eamt-1.9,2006.amta-papers.25,0,0.0164625,"es from Spanish into Basque, and is currently being adapted to the English-Basque pair. The current development status shows that it is useful for content assimilation, for text understanding indeed, but that it is not yet suitable for unrestricted use in text dissemination. Matxin has been evaluated and compared with the state-of-the-art corpus-based Matrex MT system (Stroppa et al., 2006; Labaka, 2007) translating from Spanish to Basque. The evaluation was performed using the edit-distance metric (Przybocki et al., 2006), based on the HTER (humantargeted translation edit rate) presented in (Snover et al., 2006), and the comparative results have shown that Matxin performs significantly better: 43.60 vs. 57.97 in the parallel corpus where Matrex was trained, and 40.41 vs. 71.87 in an out-ofdomain corpus. The preposition translation module of Matxin is located in the structural transfer phase and uses the information carried over from the syntactic analysis and lexical transfer modules. The system currently uses Freeling analyzer for Spanish (Atserias et al., 2006). The output of the preposition translation module is later used in subsequent modules in the structural transfer and generation phases. Not"
2009.eamt-1.9,1992.tmi-1.2,0,0.307609,"e enhancement of the core RBMT system with statistical and linguistic infor3 Related work Koehn (2003) envisages MT as a divide and conquer task where improving NP/PP translation will carry an improvement of the whole system. That study concluded that the main source of re-ranking errors in NP/PPs translation was the inability to correctly predict the phrase start (preposition or determiner) without context; it can sometimes only be resolved when the English verb is chosen and its subcategorization is known. There are two main approaches to disambiguate prepositions (Mamidi, 2004; Alam, 2004; Trujillo, 1992): context based (used in transfer systems and more suitable for languages that are structurally different) and concept based (used in interlingua 59 Freq. 4289.78 1534.24 975.31 476.70 166.68 systems and more suitable for languages which are very close). Most of the systems are context based and they use transfer rules given with semantic information for the nouns which are head and complement of the preposition. Transitivity transitive intransitive transitive intransitive transitive Postpositions ABS,ERG ABS ABS,ERG,INE ABS,INE ABS,ERG,INS Table 1: Subcategorization for verb ikusi (to see). ("
2009.eamt-1.9,2005.eamt-1.16,0,0.110714,"ation module is later used in subsequent modules in the structural transfer and generation phases. Note that errors from previous modules affect the quality of the preposition translation phase, and this makes the separate evaluation of preposition translation a difficult task. We will get back to this problem in Section 6. tactic and semantic information. While the latter rules have been coded manually, the first two resources have been automatically extracted from monolingual corpora. One important contribution of this paper is the evaluation methodology. Previous work (Husain et al., 2007; Gustavii, 2005) on preposition translation measured only accuracy gains with respect to simple baselines, and focused on small sets of frequent prepositions. Our methodology measures both precision and recall over all prepositions occurring in a small corpus of randomly chosen sentences. Once the evaluation corpus has been compiled, the evaluation is fully automatic. The results of this paper shows that all proposed techniques improve over the baselines, including a translation dictionary compiled from an aligned corpus, and over a full-fledged statistical machine translation (SMT) system. The results also s"
2009.eamt-1.9,E06-1032,0,\N,Missing
2009.eamt-1.9,W06-2113,0,\N,Missing
2009.eamt-1.9,2006.amta-papers.26,1,\N,Missing
2020.acl-main.652,P17-1171,0,0.0545817,"Missing"
2020.acl-main.652,D18-1241,0,0.300611,"s are some of the most active ones and contain knowledge of general interest, making it easily accessible for crowdworkers. DoQA contains two scenarios: in the standard scenario the test data comprises the questions and the target document from which the answers need to be extracted; in the information retrieval (IR) scenario the test data contains the questions, but the target document is unknown, and the system needs to select the documents which contain the answers among all documents in the collection. Previous work on conversational QA datasets include CoQA (Reddy et al., 2018) and QuAC (Choi et al., 2018). The main focus of CoQA are reading comprehension questions, which are produced with access to the target paragraph. The topic of the questions are delimited by the paragraph, which leads to specific questions about details in the paragraph. Choi et al. (2018) observed that a large percentage of CoQA answers are named entities or short noun phrases. In QuAC, the topic of the conversation is set by a title and first paragraph of a Wikipedia article about people. The user makes up questions about the person of interest. Note that, contrary to our setting, there is no real information need in an"
2020.acl-main.652,P17-1162,0,0.0543763,"Missing"
2020.acl-main.652,P17-1167,0,0.0895749,"Missing"
2020.acl-main.652,Q18-1023,0,0.0515395,"Missing"
2020.acl-main.652,P18-2124,0,0.151341,"Missing"
2020.acl-main.652,D16-1264,0,0.483415,"and Travel). In all cases scores over 50 F1 are reported. Regarding the IR scenario, an IR module complements the conversational system, with a relatively modest drop in performance. The gap with respect to human performance is over 30 points, showing that there is still ample room for system improvement. 2 Related Work Conversational QA systems stem from the body of work on Reading Comprehension, whose goal is to test the capacity of a system to understand a document by answering any question posed over its content. Recent work on the field has resulted in the creation of multiple datasets (Rajpurkar et al., 2016; Trischler et al., 2017; Nguyen et al., 2016; Koˇcisk´y et al., 2018; Dunn et al., 2017). These datasets are typically composed of multiple question/answer pairs, often along with a reference passage from which the answer is curated. Whereas the questions are always in free text form, some datasets represent the answers as a contiguous span in the reference passage, while others contain free form answers. The former are usually referred as extractive, whereas the latter are called abstractive. All in all, in these QA datasets the queries are unrelated to each other, and thus there is no dialo"
2020.acl-main.652,N18-1059,0,0.0493118,"Missing"
2020.acl-main.652,W17-2623,0,0.0366228,"es scores over 50 F1 are reported. Regarding the IR scenario, an IR module complements the conversational system, with a relatively modest drop in performance. The gap with respect to human performance is over 30 points, showing that there is still ample room for system improvement. 2 Related Work Conversational QA systems stem from the body of work on Reading Comprehension, whose goal is to test the capacity of a system to understand a document by answering any question posed over its content. Recent work on the field has resulted in the creation of multiple datasets (Rajpurkar et al., 2016; Trischler et al., 2017; Nguyen et al., 2016; Koˇcisk´y et al., 2018; Dunn et al., 2017). These datasets are typically composed of multiple question/answer pairs, often along with a reference passage from which the answer is curated. Whereas the questions are always in free text form, some datasets represent the answers as a contiguous span in the reference passage, while others contain free form answers. The former are usually referred as extractive, whereas the latter are called abstractive. All in all, in these QA datasets the queries are unrelated to each other, and thus there is no dialogue structure involved."
2020.acl-main.652,S17-2003,0,0.0689775,"Missing"
2020.acl-main.658,P19-1310,0,0.0576863,"Missing"
2020.acl-main.658,D18-1214,0,0.0511819,"Missing"
2020.acl-main.658,D18-1549,0,0.113759,"Missing"
2020.acl-main.658,2020.acl-main.421,1,0.822517,"RT (mBERT) has been shown to also be effective at learning cross-lingual representations in an unsupervised way.1 The main idea is to combine monolingual corpora in different languages, upsampling those with less data, and training a regular BERT model on the combined data. Conneau and Lample (2019) follow a similar approach but perform a more thorough evaluation and report substantially 1 https://github.com/google-research/ bert/blob/master/multilingual.md stronger results,2 which was further scaled up by Conneau et al. (2019). Several recent studies (Wu and Dredze, 2019; Pires et al., 2019; Artetxe et al., 2020b; Wu et al., 2019) analyze mBERT to get a better understanding of its capabilities. Unsupervised machine translation Early attempts to build machine translation systems using monolingual data alone go back to statistical decipherment (Ravi and Knight, 2011; Dou and Knight, 2012, 2013). However, this approach was only shown to work in limited settings, and the first convincing results on standard benchmarks were achieved by Artetxe et al. (2018c) and Lample et al. (2018a) on unsupervised Neural Machine Translation (NMT). Both approaches rely on cross-lingual word embeddings to initialize a sha"
2020.acl-main.658,P18-1231,0,0.0275146,"named entities and frequent words, and have pervasive gaps in the gold-standard targets (Czarnowska et al., 2019; Kementchedjhieva et al., 2019). More generally, most of these datasets are limited to relatively close languages and comparable corpora. Lack of an established cross-lingual benchmark. At the same time, there is no de facto standard benchmark to evaluate cross-lingual models beyond translation. Existing approaches have been evaluated in a wide variety of tasks including dependency parsing (Schuster et al., 2019), named entity recognition (Rahimi et al., 2019), sentiment analysis (Barnes et al., 2018), natural language inference (Conneau et al., 2018b), and document classification (Schwenk and Li, 2018). XNLI (Conneau et al., 2018b) and MLDoc (Schwenk and Li, 2018) are common choices, but they have their own problems: MultiNLI, the dataset from which XNLI was derived, has been shown to contain superficial cues that can be exploited (Gururangan et al., 2018), while MLDoc can be solved by keyword matching (Artetxe et al., 2020b). There are nonEnglish counterparts for more challenging tasks such as question answering (Cui et al., 2019; Hsu et al., 2019), but these only exist for a handful of"
2020.acl-main.658,Q17-1010,0,0.0770184,"they assume document-level sequences (Conneau and Lample, 2019) or sentence-level sequences (Artetxe et al., 2018c; Lample et al., 2018a). Nature of atomic symbols. A more important consideration is the nature of the atomic symbols in such sequences. To the best of our knowledge, previous work assumes some form of word segmentation or tokenization (e.g., splitting by whitespaces or punctuation marks). Early work on cross-lingual word embeddings considered such tokens as atomic units. However, more recent work (Hoshen and Wolf, 2018; Glavaš et al., 2019) has primarily used fastText embeddings (Bojanowski et al., 2017) which incorporate subword information into the embedding learning, although the vocabulary is still defined at the token level. In addition, there have also been approaches that incorporate character-level information into the alignment learning itself (Heyman et al., 2017; Riley and Gildea, 2018). In contrast, most work on contextual word embeddings and unsupervised machine translation operates with a subword vocabulary (Devlin et al., 2019; Conneau and Lample, 2019). While the above distinction might seem irrelevant from a practical perspective, we think that it is important from a more fun"
2020.acl-main.658,buck-etal-2014-n,0,0.0197389,"Missing"
2020.acl-main.658,D18-1024,0,0.0275962,"s is acceptable and ultimately necessary for UCL. However, we believe that any connection stemming from a (partly) shared writing system belongs to a different category, and should be considered a separate cross-lingual signal. Our rationale is that a given writing system pertains to a specific form to encode a language, but cannot be considered to be part of the language itself.6 4.3 Multilinguality While most work in unsupervised cross-lingual learning considers two languages at a time, there have recently been some attempts to extend these methods to multiple languages (Duong et al., 2017; Chen and Cardie, 2018; Heyman et al., 2019), and most work on unsupervised cross-lingual pretraining is multilingual (Pires et al., 2019; Conneau 6 As a matter of fact, languages existed well before writing was invented, and a given language can have different writing systems or new ones can be designed. 7379 Monolingual signal Cross-lingual signal Sequence of symbols Sets of sentences/documents Tokens/subwords Linguistic analysis Shared writing system Identical words String similarity Table 1: Different types of monolingual and crosslingual signals that have been used for unsupervised cross-lingual learning, orde"
2020.acl-main.658,P17-1176,0,0.0594735,"s String similarity Table 1: Different types of monolingual and crosslingual signals that have been used for unsupervised cross-lingual learning, ordered roughly from least to most linguistic knowledge (top to bottom). and Lample, 2019). When considering parallel data across a subset of the language pairs, multilinguality gives rise to additional scenarios. For instance, the scenario where two languages have no parallel data between each other but are well connected through a third (pivot) language has been explored by several authors in the context of machine translation (Cheng et al., 2016; Chen et al., 2017). However, given that the languages in question are still indirectly connected through parallel data, this scenario does not fall within the unsupervised category, and is instead commonly known as zero-resource machine translation. An alternative scenario explored in the contemporaneous work of Liu et al. (2020) is where a set of languages are connected through parallel data, and there is a separate language with monolingual data only. We argue that, when it comes to the isolated language, such a scenario should still be considered as UCL, as it does not rely on any parallel data for that part"
2020.acl-main.658,D18-1269,0,0.171286,"unt of supervision required was greatly reduced via crosslingual word embedding mappings, which work by separately learning monolingual word embeddings in each language and mapping them into a shared space through a linear transformation. Early work required a bilingual dictionary to learn such a transformation (Mikolov et al., 2013a; Faruqui and Dyer, 2014). This requirement was later reduced with self-learning (Artetxe et al., 2017), and ultimately removed via unsupervised initialization heuristics (Artetxe et al., 2018a; Hoshen and Wolf, 2018) and adversarial learning (Zhang et al., 2017a; Conneau et al., 2018a). Finally, several recent methods have formulated cross-lingual embedding alignment as an optimal transport problem (Zhang et al., 2017b; Grave et al., 2019; Alvarez-Melis and Jaakkola, 2018). 2.2 Deep multilingual pretraining Following the success in learning shallow word embeddings (Mikolov et al., 2013b; Pennington et al., 2014), there has been an increasing interest in learning contextual word representations (Dai and Le, 2015; Peters et al., 2018; Howard and Ruder, 2018). Recent research has been dominated by BERT (Devlin et al., 2019), which uses a bidirectional transformer encoder tra"
2020.acl-main.658,D19-1169,0,0.021601,"recognition (Rahimi et al., 2019), sentiment analysis (Barnes et al., 2018), natural language inference (Conneau et al., 2018b), and document classification (Schwenk and Li, 2018). XNLI (Conneau et al., 2018b) and MLDoc (Schwenk and Li, 2018) are common choices, but they have their own problems: MultiNLI, the dataset from which XNLI was derived, has been shown to contain superficial cues that can be exploited (Gururangan et al., 2018), while MLDoc can be solved by keyword matching (Artetxe et al., 2020b). There are nonEnglish counterparts for more challenging tasks such as question answering (Cui et al., 2019; Hsu et al., 2019), but these only exist for a handful of languages. More recent datasets such as XQuAD 7381 Methodological issues Examples Validation and hyperparameter tuning Systematic tuning with parallel data or on test data Evaluation on favorable conditions Typologically similar languages; always including English; training on the same domain Over-reliance on translation tasks Overfitting to bilingual lexicon induction; known issues with existing datasets Lack of an established benchmark Evaluation on many different tasks; problems with common tasks (MLDoc and XNLI) Table 2: Methodolog"
2020.acl-main.658,D13-1173,0,0.0177374,"of tokenization. Such a tabula rasa approach is potentially applicable to any arbitrary language, even when its writing system is not known, but has so far only been explored for a limited number of languages in a monolingual setting (Hahn and Baroni, 2019). Linguistic information. Finally, one can exploit additional linguistic knowledge through linguistic analysis such as lemmatization, part-of-speech tagging, or syntactic parsing. For instance, before the advent of unsupervised NMT, statistical deci7378 pherment was already shown to benefit from incorporating syntactic dependency relations (Dou and Knight, 2013). For other tasks such as unsupervised POS tagging (Snyder et al., 2008), monolingual tag dictionaries have been used. While such approaches could still be considered unsupervised from a cross-lingual perspective, we argue that the interest of this research direction is greatly limited by two factors: (i) from a theoretical perspective, it assumes some fundamental knowledge that is not directly inferred from the raw monolingual corpora; and (ii) from a more practical perspective, it is not reasonable to assume that such resources are available in the less resourced settings where this research"
2020.acl-main.658,E17-1084,0,0.0251037,"inguistics universals is acceptable and ultimately necessary for UCL. However, we believe that any connection stemming from a (partly) shared writing system belongs to a different category, and should be considered a separate cross-lingual signal. Our rationale is that a given writing system pertains to a specific form to encode a language, but cannot be considered to be part of the language itself.6 4.3 Multilinguality While most work in unsupervised cross-lingual learning considers two languages at a time, there have recently been some attempts to extend these methods to multiple languages (Duong et al., 2017; Chen and Cardie, 2018; Heyman et al., 2019), and most work on unsupervised cross-lingual pretraining is multilingual (Pires et al., 2019; Conneau 6 As a matter of fact, languages existed well before writing was invented, and a given language can have different writing systems or new ones can be designed. 7379 Monolingual signal Cross-lingual signal Sequence of symbols Sets of sentences/documents Tokens/subwords Linguistic analysis Shared writing system Identical words String similarity Table 1: Different types of monolingual and crosslingual signals that have been used for unsupervised cross"
2020.acl-main.658,E14-1049,0,0.0301901,"deep multilingual pre-training (§2.2), and unsupervised machine translation (§2.3). 2.1 2.3 Cross-lingual word embeddings Cross-lingual word embedding methods traditionally relied on parallel corpora (Gouws et al., 2015; Luong et al., 2015). Nonetheless, the amount of supervision required was greatly reduced via crosslingual word embedding mappings, which work by separately learning monolingual word embeddings in each language and mapping them into a shared space through a linear transformation. Early work required a bilingual dictionary to learn such a transformation (Mikolov et al., 2013a; Faruqui and Dyer, 2014). This requirement was later reduced with self-learning (Artetxe et al., 2017), and ultimately removed via unsupervised initialization heuristics (Artetxe et al., 2018a; Hoshen and Wolf, 2018) and adversarial learning (Zhang et al., 2017a; Conneau et al., 2018a). Finally, several recent methods have formulated cross-lingual embedding alignment as an optimal transport problem (Zhang et al., 2017b; Grave et al., 2019; Alvarez-Melis and Jaakkola, 2018). 2.2 Deep multilingual pretraining Following the success in learning shallow word embeddings (Mikolov et al., 2013b; Pennington et al., 2014), the"
2020.acl-main.658,P19-1070,1,0.884346,"Missing"
2020.acl-main.658,L18-1550,0,0.0236778,"s3 of which nearly half have less than 10,000 articles. While one could hope to overcome this by taking the entire web as a corpus, as facilitated by Common Crawl4 and similar initiatives, this is not 3 https://en.wikipedia.org/wiki/List_ of_Wikipedias 4 https://commoncrawl.org/ always feasible for low-resource languages. First, the presence of less resourced languages on the web is very limited, with only a few hundred languages recognized as being used in websites.5 This situation is further complicated by the limited coverage of existing tools such as language detectors (Buck et al., 2014; Grave et al., 2018), which only cover a few hundred languages. Alternatively, speech could also serve as a source of monolingual data (e.g., by recording public radio stations). However, this is an unexplored direction within UCL, and collecting, processing and effectively capitalizing on speech data is far from trivial, particularly for low-resource languages. All in all, we conclude that the alleged scenario involving no parallel data and sufficient monolingual data is not met in the real world in the terms explored by recent UCL research. Needless to say, effectively exploiting unlabeled data is important in"
2020.acl-main.658,N18-2017,0,0.0393367,"Missing"
2020.acl-main.658,D19-1632,0,0.14985,"ols in different languages to be disjoint, without prior knowledge of the relationship between them. Needless to say, any form of learning requires making assumptions, as one needs some criterion to prefer one mapping over another. In the case of UCL, such assumptions stem from the structural similarity across languages (e.g. semantically equivalent words in different languages are assumed to occur in similar contexts). In practice, these assumptions weaken as the distribution of the datasets diverges, and some UCL models have been reported to break under a domain shift (Søgaard et al., 2018; Guzmán et al., 2019; Marchisio et al., 2020). Similarly, approaches that leverage linguistic features such as syntactic dependencies may assume that these are similar across languages. In addition, one can also assume that the sets of symbols that are used to represent different languages have some commonalities. This departs from the strict definition of UCL above, establishing some prior connections between the sets of symbols in different languages. Such an assumption is reasonable from a practical perspective, as there are a few scripts (e.g. Latin, Arabic or Cyrillic) that cover a large fraction of language"
2020.acl-main.658,Q19-1033,0,0.0144797,"derlying assumptions might not generalize to different writing systems (e.g. logographic instead of alphabetic). For instance, subword tokenization has been shown to perform poorly on reduplicated words (Vania and Lopez, 2017). In relation to that, one could also consider the text in each language as a stream of discrete character-like symbols without any notion of tokenization. Such a tabula rasa approach is potentially applicable to any arbitrary language, even when its writing system is not known, but has so far only been explored for a limited number of languages in a monolingual setting (Hahn and Baroni, 2019). Linguistic information. Finally, one can exploit additional linguistic knowledge through linguistic analysis such as lemmatization, part-of-speech tagging, or syntactic parsing. For instance, before the advent of unsupervised NMT, statistical deci7378 pherment was already shown to benefit from incorporating syntactic dependency relations (Dou and Knight, 2013). For other tasks such as unsupervised POS tagging (Snyder et al., 2008), monolingual tag dictionaries have been used. While such approaches could still be considered unsupervised from a cross-lingual perspective, we argue that the inte"
2020.acl-main.658,N19-1188,0,0.0821336,"Missing"
2020.acl-main.658,E17-1102,0,0.0631211,"Missing"
2020.acl-main.658,D18-1043,0,0.121563,"corpora (Gouws et al., 2015; Luong et al., 2015). Nonetheless, the amount of supervision required was greatly reduced via crosslingual word embedding mappings, which work by separately learning monolingual word embeddings in each language and mapping them into a shared space through a linear transformation. Early work required a bilingual dictionary to learn such a transformation (Mikolov et al., 2013a; Faruqui and Dyer, 2014). This requirement was later reduced with self-learning (Artetxe et al., 2017), and ultimately removed via unsupervised initialization heuristics (Artetxe et al., 2018a; Hoshen and Wolf, 2018) and adversarial learning (Zhang et al., 2017a; Conneau et al., 2018a). Finally, several recent methods have formulated cross-lingual embedding alignment as an optimal transport problem (Zhang et al., 2017b; Grave et al., 2019; Alvarez-Melis and Jaakkola, 2018). 2.2 Deep multilingual pretraining Following the success in learning shallow word embeddings (Mikolov et al., 2013b; Pennington et al., 2014), there has been an increasing interest in learning contextual word representations (Dai and Le, 2015; Peters et al., 2018; Howard and Ruder, 2018). Recent research has been dominated by BERT (Devl"
2020.acl-main.658,P18-1031,1,0.811822,"initialization heuristics (Artetxe et al., 2018a; Hoshen and Wolf, 2018) and adversarial learning (Zhang et al., 2017a; Conneau et al., 2018a). Finally, several recent methods have formulated cross-lingual embedding alignment as an optimal transport problem (Zhang et al., 2017b; Grave et al., 2019; Alvarez-Melis and Jaakkola, 2018). 2.2 Deep multilingual pretraining Following the success in learning shallow word embeddings (Mikolov et al., 2013b; Pennington et al., 2014), there has been an increasing interest in learning contextual word representations (Dai and Le, 2015; Peters et al., 2018; Howard and Ruder, 2018). Recent research has been dominated by BERT (Devlin et al., 2019), which uses a bidirectional transformer encoder trained on masked language modeling and next sentence prediction, which led to impressive gains on various downstream tasks. While the above approaches are limited to a single language, a multilingual extension of BERT (mBERT) has been shown to also be effective at learning cross-lingual representations in an unsupervised way.1 The main idea is to combine monolingual corpora in different languages, upsampling those with less data, and training a regular BERT model on the combined"
2020.acl-main.658,D19-1607,0,0.0194325,"i et al., 2019), sentiment analysis (Barnes et al., 2018), natural language inference (Conneau et al., 2018b), and document classification (Schwenk and Li, 2018). XNLI (Conneau et al., 2018b) and MLDoc (Schwenk and Li, 2018) are common choices, but they have their own problems: MultiNLI, the dataset from which XNLI was derived, has been shown to contain superficial cues that can be exploited (Gururangan et al., 2018), while MLDoc can be solved by keyword matching (Artetxe et al., 2020b). There are nonEnglish counterparts for more challenging tasks such as question answering (Cui et al., 2019; Hsu et al., 2019), but these only exist for a handful of languages. More recent datasets such as XQuAD 7381 Methodological issues Examples Validation and hyperparameter tuning Systematic tuning with parallel data or on test data Evaluation on favorable conditions Typologically similar languages; always including English; training on the same domain Over-reliance on translation tasks Overfitting to bilingual lexicon induction; known issues with existing datasets Lack of an established benchmark Evaluation on many different tasks; problems with common tasks (MLDoc and XNLI) Table 2: Methodological issues pertain"
2020.acl-main.658,kamholz-etal-2014-panlex,0,0.0171766,"irs in the real world” (Xu et al., 2018) is largely inaccurate. For instance, the JW300 parallel corpus covers 343 languages with around 100,000 parallel sentences per language pair on average (Agi´c and Vuli´c, 2019), and the multilingual Bible corpus collected by Mayer and Cysouw (2014) covers 837 language varieties (each with a unique ISO 639-3 code). Moreover, the PanLex project aims to collect multilingual lexica for all human languages in the world, and already covers 6,854 language varieties with at least 20 lexemes, 2,364 with at least 200 lexemes, and 369 with at least 2,000 lexemes (Kamholz et al., 2014). While 20 or 200 lexemes might seem insufficient, weakly supervised cross-lingual word embedding methods already proved effective with as little as 25 word pairs (Artetxe et al., 2017). More recent methods have focused on completely removing this weak supervision (Conneau et al., 2018a; Artetxe et al., 2018a), which can hardly be justified from a practical perspective given the existence of such resources and additional training signals stemming from a (partially) shared script (§4.2). Finally, given the availability of sufficient monolingual data, noisy parallel data can often be obtained by"
2020.acl-main.658,D19-1328,0,0.0166423,"eam tasks. In particular, they observe that some mapping methods that are specifically designed for bilingual lexicon induction perform poorly on other tasks, showing the risk of relying excessively on translation benchmarks for evaluating cross-lingual models. Moreover, existing translation benchmarks have been shown to have several issues on their own. In particular, bilingual lexicon induction datasets have been reported to misrepresent morphological variations, overly focus on named entities and frequent words, and have pervasive gaps in the gold-standard targets (Czarnowska et al., 2019; Kementchedjhieva et al., 2019). More generally, most of these datasets are limited to relatively close languages and comparable corpora. Lack of an established cross-lingual benchmark. At the same time, there is no de facto standard benchmark to evaluate cross-lingual models beyond translation. Existing approaches have been evaluated in a wide variety of tasks including dependency parsing (Schuster et al., 2019), named entity recognition (Rahimi et al., 2019), sentiment analysis (Barnes et al., 2018), natural language inference (Conneau et al., 2018b), and document classification (Schwenk and Li, 2018). XNLI (Conneau et al"
2020.acl-main.658,J82-2005,0,0.68359,"Missing"
2020.acl-main.658,2020.tacl-1.47,0,0.107835,"ase-based Statistical Machine Translation (SMT), obtaining large improvements over the original NMT-based systems (Lample et al., 2018b; Artetxe et al., 2018b). This alternative approach uses cross-lingual n-gram embeddings to build an initial phrase table, which is combined with an n-gram language model and a distortion model, and further refined through iterative backtranslation. There have been several follow-up attempts to combine NMT and SMT based approaches (Marie and Fujita, 2018; Ren et al., 2019; Artetxe et al., 2019b). More recently, Conneau and Lample (2019), Song et al. (2019) and Liu et al. (2020) obtain strong results using deep multilingual pretraining rather than cross-lingual word embeddings to initialize unsupervised NMT systems. 3 Motivating fully unsupervised learning In this section, we challenge the narrative of motivating UCL based on a lack of parallel resources. We argue that the strict unsupervised scenario cannot be motivated from an immediate practical perspective, and elucidate what we believe should be the true goals of this research direction. 2 The full version of their model (XLM) requires parallel corpora for their translation language modeling objective, but the a"
2020.acl-main.658,W15-1521,0,0.0312387,"translation) in UCL (§6), and conclude with a summary of our recommendations (§7). 7375 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7375–7388 c July 5 - 10, 2020. 2020 Association for Computational Linguistics 2 Background In this section, we briefly review existing work on UCL, covering cross-lingual word embeddings (§2.1), deep multilingual pre-training (§2.2), and unsupervised machine translation (§2.3). 2.1 2.3 Cross-lingual word embeddings Cross-lingual word embedding methods traditionally relied on parallel corpora (Gouws et al., 2015; Luong et al., 2015). Nonetheless, the amount of supervision required was greatly reduced via crosslingual word embedding mappings, which work by separately learning monolingual word embeddings in each language and mapping them into a shared space through a linear transformation. Early work required a bilingual dictionary to learn such a transformation (Mikolov et al., 2013a; Faruqui and Dyer, 2014). This requirement was later reduced with self-learning (Artetxe et al., 2017), and ultimately removed via unsupervised initialization heuristics (Artetxe et al., 2018a; Hoshen and Wolf, 2018) and adversarial learning"
2020.acl-main.658,2020.wmt-1.68,0,0.0586124,"Missing"
2020.acl-main.658,D18-1399,1,0.927958,"ng autoencoding, backtranslation, and optionally adversarial learning. Subsequent work adapted these principles to unsupervised phrase-based Statistical Machine Translation (SMT), obtaining large improvements over the original NMT-based systems (Lample et al., 2018b; Artetxe et al., 2018b). This alternative approach uses cross-lingual n-gram embeddings to build an initial phrase table, which is combined with an n-gram language model and a distortion model, and further refined through iterative backtranslation. There have been several follow-up attempts to combine NMT and SMT based approaches (Marie and Fujita, 2018; Ren et al., 2019; Artetxe et al., 2019b). More recently, Conneau and Lample (2019), Song et al. (2019) and Liu et al. (2020) obtain strong results using deep multilingual pretraining rather than cross-lingual word embeddings to initialize unsupervised NMT systems. 3 Motivating fully unsupervised learning In this section, we challenge the narrative of motivating UCL based on a lack of parallel resources. We argue that the strict unsupervised scenario cannot be motivated from an immediate practical perspective, and elucidate what we believe should be the true goals of this research direction."
2020.acl-main.658,W19-5330,0,0.0262475,"optimal hyperparameter choice might not necessarily transfer well across languages. In contrast, Conneau et al. (2018a) and Lample et al. (2018a) propose an unsupervised validation criterion that is defined over monolingual data and shown to correlate well with test performance. This enables systematic tuning on the language pair of interest, but still requires parallel data to guide the development of the unsupervised validation criterion itself. A parallel validation set has also been used for systematic tuning in 7380 the context of unsupervised machine translation (Marie and Fujita, 2018; Marie et al., 2019; Stojanovski et al., 2019). While this is motivated as a way to abstract away the issue of unsupervised tuning—which the authors consider to be an open problem—we argue that any systematic use of parallel data should not be considered UCL. Finally, previous work often does not report the validation scheme used. In particular, unsupervised crosslingual word embedding methods have almost exclusively been evaluated on bilingual lexicons that do not have a validation set, and presumably use the test set to guide development to some extent. Our position is that a completely blind development model"
2020.acl-main.658,mayer-cysouw-2014-creating,0,0.0141436,"ual corpus. From this argument, it follows that monolingual data is cheaper to obtain than parallel data, so unsupervised crosslingual learning should in principle be more generally applicable than supervised learning. However, we argue that the common claim that the requirement for parallel data “may not be met for many language pairs in the real world” (Xu et al., 2018) is largely inaccurate. For instance, the JW300 parallel corpus covers 343 languages with around 100,000 parallel sentences per language pair on average (Agi´c and Vuli´c, 2019), and the multilingual Bible corpus collected by Mayer and Cysouw (2014) covers 837 language varieties (each with a unique ISO 639-3 code). Moreover, the PanLex project aims to collect multilingual lexica for all human languages in the world, and already covers 6,854 language varieties with at least 20 lexemes, 2,364 with at least 200 lexemes, and 369 with at least 2,000 lexemes (Kamholz et al., 2014). While 20 or 200 lexemes might seem insufficient, weakly supervised cross-lingual word embedding methods already proved effective with as little as 25 word pairs (Artetxe et al., 2017). More recent methods have focused on completely removing this weak supervision (Co"
2020.acl-main.658,D14-1162,0,0.0914393,"013a; Faruqui and Dyer, 2014). This requirement was later reduced with self-learning (Artetxe et al., 2017), and ultimately removed via unsupervised initialization heuristics (Artetxe et al., 2018a; Hoshen and Wolf, 2018) and adversarial learning (Zhang et al., 2017a; Conneau et al., 2018a). Finally, several recent methods have formulated cross-lingual embedding alignment as an optimal transport problem (Zhang et al., 2017b; Grave et al., 2019; Alvarez-Melis and Jaakkola, 2018). 2.2 Deep multilingual pretraining Following the success in learning shallow word embeddings (Mikolov et al., 2013b; Pennington et al., 2014), there has been an increasing interest in learning contextual word representations (Dai and Le, 2015; Peters et al., 2018; Howard and Ruder, 2018). Recent research has been dominated by BERT (Devlin et al., 2019), which uses a bidirectional transformer encoder trained on masked language modeling and next sentence prediction, which led to impressive gains on various downstream tasks. While the above approaches are limited to a single language, a multilingual extension of BERT (mBERT) has been shown to also be effective at learning cross-lingual representations in an unsupervised way.1 The main"
2020.acl-main.658,N18-1202,0,0.0160418,"oved via unsupervised initialization heuristics (Artetxe et al., 2018a; Hoshen and Wolf, 2018) and adversarial learning (Zhang et al., 2017a; Conneau et al., 2018a). Finally, several recent methods have formulated cross-lingual embedding alignment as an optimal transport problem (Zhang et al., 2017b; Grave et al., 2019; Alvarez-Melis and Jaakkola, 2018). 2.2 Deep multilingual pretraining Following the success in learning shallow word embeddings (Mikolov et al., 2013b; Pennington et al., 2014), there has been an increasing interest in learning contextual word representations (Dai and Le, 2015; Peters et al., 2018; Howard and Ruder, 2018). Recent research has been dominated by BERT (Devlin et al., 2019), which uses a bidirectional transformer encoder trained on masked language modeling and next sentence prediction, which led to impressive gains on various downstream tasks. While the above approaches are limited to a single language, a multilingual extension of BERT (mBERT) has been shown to also be effective at learning cross-lingual representations in an unsupervised way.1 The main idea is to combine monolingual corpora in different languages, upsampling those with less data, and training a regular BE"
2020.acl-main.658,P19-1493,0,0.0930796,"gual extension of BERT (mBERT) has been shown to also be effective at learning cross-lingual representations in an unsupervised way.1 The main idea is to combine monolingual corpora in different languages, upsampling those with less data, and training a regular BERT model on the combined data. Conneau and Lample (2019) follow a similar approach but perform a more thorough evaluation and report substantially 1 https://github.com/google-research/ bert/blob/master/multilingual.md stronger results,2 which was further scaled up by Conneau et al. (2019). Several recent studies (Wu and Dredze, 2019; Pires et al., 2019; Artetxe et al., 2020b; Wu et al., 2019) analyze mBERT to get a better understanding of its capabilities. Unsupervised machine translation Early attempts to build machine translation systems using monolingual data alone go back to statistical decipherment (Ravi and Knight, 2011; Dou and Knight, 2012, 2013). However, this approach was only shown to work in limited settings, and the first convincing results on standard benchmarks were achieved by Artetxe et al. (2018c) and Lample et al. (2018a) on unsupervised Neural Machine Translation (NMT). Both approaches rely on cross-lingual word embeddin"
2020.acl-main.658,P19-1015,0,0.0363163,"morphological variations, overly focus on named entities and frequent words, and have pervasive gaps in the gold-standard targets (Czarnowska et al., 2019; Kementchedjhieva et al., 2019). More generally, most of these datasets are limited to relatively close languages and comparable corpora. Lack of an established cross-lingual benchmark. At the same time, there is no de facto standard benchmark to evaluate cross-lingual models beyond translation. Existing approaches have been evaluated in a wide variety of tasks including dependency parsing (Schuster et al., 2019), named entity recognition (Rahimi et al., 2019), sentiment analysis (Barnes et al., 2018), natural language inference (Conneau et al., 2018b), and document classification (Schwenk and Li, 2018). XNLI (Conneau et al., 2018b) and MLDoc (Schwenk and Li, 2018) are common choices, but they have their own problems: MultiNLI, the dataset from which XNLI was derived, has been shown to contain superficial cues that can be exploited (Gururangan et al., 2018), while MLDoc can be solved by keyword matching (Artetxe et al., 2020b). There are nonEnglish counterparts for more challenging tasks such as question answering (Cui et al., 2019; Hsu et al., 201"
2020.acl-main.658,P11-1002,0,0.0760115,"the combined data. Conneau and Lample (2019) follow a similar approach but perform a more thorough evaluation and report substantially 1 https://github.com/google-research/ bert/blob/master/multilingual.md stronger results,2 which was further scaled up by Conneau et al. (2019). Several recent studies (Wu and Dredze, 2019; Pires et al., 2019; Artetxe et al., 2020b; Wu et al., 2019) analyze mBERT to get a better understanding of its capabilities. Unsupervised machine translation Early attempts to build machine translation systems using monolingual data alone go back to statistical decipherment (Ravi and Knight, 2011; Dou and Knight, 2012, 2013). However, this approach was only shown to work in limited settings, and the first convincing results on standard benchmarks were achieved by Artetxe et al. (2018c) and Lample et al. (2018a) on unsupervised Neural Machine Translation (NMT). Both approaches rely on cross-lingual word embeddings to initialize a shared encoder, and train it in conjunction with the decoder using a combination of denoising autoencoding, backtranslation, and optionally adversarial learning. Subsequent work adapted these principles to unsupervised phrase-based Statistical Machine Translat"
2020.acl-main.658,P18-2062,0,0.110805,"es some form of word segmentation or tokenization (e.g., splitting by whitespaces or punctuation marks). Early work on cross-lingual word embeddings considered such tokens as atomic units. However, more recent work (Hoshen and Wolf, 2018; Glavaš et al., 2019) has primarily used fastText embeddings (Bojanowski et al., 2017) which incorporate subword information into the embedding learning, although the vocabulary is still defined at the token level. In addition, there have also been approaches that incorporate character-level information into the alignment learning itself (Heyman et al., 2017; Riley and Gildea, 2018). In contrast, most work on contextual word embeddings and unsupervised machine translation operates with a subword vocabulary (Devlin et al., 2019; Conneau and Lample, 2019). While the above distinction might seem irrelevant from a practical perspective, we think that it is important from a more fundamental point of view (e.g. in relation to the distributional hypothesis as discussed in §3.2). Moreover, some of the underlying assumptions might not generalize to different writing systems (e.g. logographic instead of alphabetic). For instance, subword tokenization has been shown to perform poor"
2020.acl-main.658,D18-1042,1,0.810075,"h et al., 2017; Søgaard et al., 2018) or string-level similarity across languages (Riley and Gildea, 2018; Artetxe et al., 2019b) as training signals. Other methods use a joint subword vocabulary for all languages, indirectly exploiting the commonalities in their writing system (Lample et al., 2018b; Conneau and Lample, 2019). However, past work greatly differs on the nature and relevance that is attributed to such a training signal. The reliance on identically spelled words has been considered as a weak form of supervision in the cross-lingual word embedding literature (Søgaard et al., 2018; Ruder et al., 2018), and significant effort has been put into developing strictly unsupervised methods that do not rely on such signal (Conneau et al., 2018a). In contrast, the unsupervised machine translation literature has not payed much attention to this factor, and has often relied on identical words (Artetxe et al., 2018c), string-level similarity (Artetxe et al., 2019b), or a joint subword vocabulary (Lample et al., 2018b; Conneau and Lample, 2019) under the unsupervised umbrella. The same is true for unsupervised deep multilingual pretraining, where a shared subword vocabulary has been a common component"
2020.acl-main.658,N19-1162,0,0.0215779,"uction datasets have been reported to misrepresent morphological variations, overly focus on named entities and frequent words, and have pervasive gaps in the gold-standard targets (Czarnowska et al., 2019; Kementchedjhieva et al., 2019). More generally, most of these datasets are limited to relatively close languages and comparable corpora. Lack of an established cross-lingual benchmark. At the same time, there is no de facto standard benchmark to evaluate cross-lingual models beyond translation. Existing approaches have been evaluated in a wide variety of tasks including dependency parsing (Schuster et al., 2019), named entity recognition (Rahimi et al., 2019), sentiment analysis (Barnes et al., 2018), natural language inference (Conneau et al., 2018b), and document classification (Schwenk and Li, 2018). XNLI (Conneau et al., 2018b) and MLDoc (Schwenk and Li, 2018) are common choices, but they have their own problems: MultiNLI, the dataset from which XNLI was derived, has been shown to contain superficial cues that can be exploited (Gururangan et al., 2018), while MLDoc can be solved by keyword matching (Artetxe et al., 2020b). There are nonEnglish counterparts for more challenging tasks such as quest"
2020.acl-main.658,L18-1560,0,0.0284769,"al., 2019; Kementchedjhieva et al., 2019). More generally, most of these datasets are limited to relatively close languages and comparable corpora. Lack of an established cross-lingual benchmark. At the same time, there is no de facto standard benchmark to evaluate cross-lingual models beyond translation. Existing approaches have been evaluated in a wide variety of tasks including dependency parsing (Schuster et al., 2019), named entity recognition (Rahimi et al., 2019), sentiment analysis (Barnes et al., 2018), natural language inference (Conneau et al., 2018b), and document classification (Schwenk and Li, 2018). XNLI (Conneau et al., 2018b) and MLDoc (Schwenk and Li, 2018) are common choices, but they have their own problems: MultiNLI, the dataset from which XNLI was derived, has been shown to contain superficial cues that can be exploited (Gururangan et al., 2018), while MLDoc can be solved by keyword matching (Artetxe et al., 2020b). There are nonEnglish counterparts for more challenging tasks such as question answering (Cui et al., 2019; Hsu et al., 2019), but these only exist for a handful of languages. More recent datasets such as XQuAD 7381 Methodological issues Examples Validation and hyperpa"
2020.acl-main.658,D08-1109,0,0.0583697,"any arbitrary language, even when its writing system is not known, but has so far only been explored for a limited number of languages in a monolingual setting (Hahn and Baroni, 2019). Linguistic information. Finally, one can exploit additional linguistic knowledge through linguistic analysis such as lemmatization, part-of-speech tagging, or syntactic parsing. For instance, before the advent of unsupervised NMT, statistical deci7378 pherment was already shown to benefit from incorporating syntactic dependency relations (Dou and Knight, 2013). For other tasks such as unsupervised POS tagging (Snyder et al., 2008), monolingual tag dictionaries have been used. While such approaches could still be considered unsupervised from a cross-lingual perspective, we argue that the interest of this research direction is greatly limited by two factors: (i) from a theoretical perspective, it assumes some fundamental knowledge that is not directly inferred from the raw monolingual corpora; and (ii) from a more practical perspective, it is not reasonable to assume that such resources are available in the less resourced settings where this research direction has more potential for impact. 4.2 Cross-lingual training sig"
2020.acl-main.658,P18-1072,1,0.918484,"Missing"
2020.acl-main.658,W19-5344,0,0.0253622,"er choice might not necessarily transfer well across languages. In contrast, Conneau et al. (2018a) and Lample et al. (2018a) propose an unsupervised validation criterion that is defined over monolingual data and shown to correlate well with test performance. This enables systematic tuning on the language pair of interest, but still requires parallel data to guide the development of the unsupervised validation criterion itself. A parallel validation set has also been used for systematic tuning in 7380 the context of unsupervised machine translation (Marie and Fujita, 2018; Marie et al., 2019; Stojanovski et al., 2019). While this is motivated as a way to abstract away the issue of unsupervised tuning—which the authors consider to be an open problem—we argue that any systematic use of parallel data should not be considered UCL. Finally, previous work often does not report the validation scheme used. In particular, unsupervised crosslingual word embedding methods have almost exclusively been evaluated on bilingual lexicons that do not have a validation set, and presumably use the test set to guide development to some extent. Our position is that a completely blind development model without any parallel data"
2020.acl-main.658,P17-1184,0,0.022059,"n contextual word embeddings and unsupervised machine translation operates with a subword vocabulary (Devlin et al., 2019; Conneau and Lample, 2019). While the above distinction might seem irrelevant from a practical perspective, we think that it is important from a more fundamental point of view (e.g. in relation to the distributional hypothesis as discussed in §3.2). Moreover, some of the underlying assumptions might not generalize to different writing systems (e.g. logographic instead of alphabetic). For instance, subword tokenization has been shown to perform poorly on reduplicated words (Vania and Lopez, 2017). In relation to that, one could also consider the text in each language as a stream of discrete character-like symbols without any notion of tokenization. Such a tabula rasa approach is potentially applicable to any arbitrary language, even when its writing system is not known, but has so far only been explored for a limited number of languages in a monolingual setting (Hahn and Baroni, 2019). Linguistic information. Finally, one can exploit additional linguistic knowledge through linguistic analysis such as lemmatization, part-of-speech tagging, or syntactic parsing. For instance, before the"
2020.acl-main.658,D19-1449,0,0.0899592,"Missing"
2020.acl-main.658,P16-1024,0,0.106921,"Missing"
2020.acl-main.658,W18-5446,0,0.0677908,"Missing"
2020.acl-main.658,D19-1077,0,0.0171822,"language, a multilingual extension of BERT (mBERT) has been shown to also be effective at learning cross-lingual representations in an unsupervised way.1 The main idea is to combine monolingual corpora in different languages, upsampling those with less data, and training a regular BERT model on the combined data. Conneau and Lample (2019) follow a similar approach but perform a more thorough evaluation and report substantially 1 https://github.com/google-research/ bert/blob/master/multilingual.md stronger results,2 which was further scaled up by Conneau et al. (2019). Several recent studies (Wu and Dredze, 2019; Pires et al., 2019; Artetxe et al., 2020b; Wu et al., 2019) analyze mBERT to get a better understanding of its capabilities. Unsupervised machine translation Early attempts to build machine translation systems using monolingual data alone go back to statistical decipherment (Ravi and Knight, 2011; Dou and Knight, 2012, 2013). However, this approach was only shown to work in limited settings, and the first convincing results on standard benchmarks were achieved by Artetxe et al. (2018c) and Lample et al. (2018a) on unsupervised Neural Machine Translation (NMT). Both approaches rely on cross-l"
2020.acl-main.658,D18-1268,0,0.0181008,"pervised variant using masked language modeling alone. 7376 3.1 How practical is the strict unsupervised scenario? Monolingual resources subsume parallel resources. For instance, each side of a parallel corpus effectively serves as a monolingual corpus. From this argument, it follows that monolingual data is cheaper to obtain than parallel data, so unsupervised crosslingual learning should in principle be more generally applicable than supervised learning. However, we argue that the common claim that the requirement for parallel data “may not be met for many language pairs in the real world” (Xu et al., 2018) is largely inaccurate. For instance, the JW300 parallel corpus covers 343 languages with around 100,000 parallel sentences per language pair on average (Agi´c and Vuli´c, 2019), and the multilingual Bible corpus collected by Mayer and Cysouw (2014) covers 837 language varieties (each with a unique ISO 639-3 code). Moreover, the PanLex project aims to collect multilingual lexica for all human languages in the world, and already covers 6,854 language varieties with at least 20 lexemes, 2,364 with at least 200 lexemes, and 369 with at least 2,000 lexemes (Kamholz et al., 2014). While 20 or 200 l"
2020.acl-main.658,P17-1179,0,0.199662,"include translation as well as pretraining multilingual representations. We will use the term interchangeably with “cross-lingual learning”. ∗ Equal contribution. Recent work in this direction has increasingly focused on purely unsupervised cross-lingual learning (UCL)—i.e., cross-lingual learning without any parallel signal across the languages. We provide an overview in §2. Such work has been motivated by the apparent dearth of parallel data for most of the world’s languages. In particular, previous work has noted that “data encoding cross-lingual equivalence is often expensive to obtain” (Zhang et al., 2017a) whereas “monolingual data is much easier to find” (Lample et al., 2018a). Overall, it has been argued that unsupervised cross-lingual learning “opens up opportunities for the processing of extremely low-resource languages and domains that lack parallel data completely” (Zhang et al., 2017a). We challenge this narrative and argue that the scenario of no parallel data and sufficient monolingual data is unrealistic and not reflected in the real world (§3.1). Nevertheless, UCL is an important research direction and we advocate for its study based on an inherent scientific interest (to better un"
2020.acl-main.658,D17-1207,0,0.119875,"include translation as well as pretraining multilingual representations. We will use the term interchangeably with “cross-lingual learning”. ∗ Equal contribution. Recent work in this direction has increasingly focused on purely unsupervised cross-lingual learning (UCL)—i.e., cross-lingual learning without any parallel signal across the languages. We provide an overview in §2. Such work has been motivated by the apparent dearth of parallel data for most of the world’s languages. In particular, previous work has noted that “data encoding cross-lingual equivalence is often expensive to obtain” (Zhang et al., 2017a) whereas “monolingual data is much easier to find” (Lample et al., 2018a). Overall, it has been argued that unsupervised cross-lingual learning “opens up opportunities for the processing of extremely low-resource languages and domains that lack parallel data completely” (Zhang et al., 2017a). We challenge this narrative and argue that the scenario of no parallel data and sufficient monolingual data is unrealistic and not reflected in the real world (§3.1). Nevertheless, UCL is an important research direction and we advocate for its study based on an inherent scientific interest (to better un"
2020.acl-main.658,D12-1025,0,\N,Missing
2020.acl-main.658,P17-1042,1,\N,Missing
2020.acl-main.658,P19-1494,1,\N,Missing
2020.acl-main.658,N19-1423,0,\N,Missing
2020.acl-main.658,D19-1090,1,\N,Missing
2020.acl-main.658,2020.emnlp-main.484,0,\N,Missing
2020.acl-main.658,2020.emnlp-main.618,1,\N,Missing
2020.acl-main.84,P17-1089,0,0.357655,"nnotated_tree_files/single_files complex NL questions (Damljanovic et al., 2010; Zheng et al., 2017). Parsing-based approaches use a natural language parser to analyze and reason about the grammatical structure of a query (Li and Jagadish, 2014; Saha et al., 2016). Grammar-based approaches only allow the user to formulate queries according to certain pre-defined rules, thus focus primarily on increasing the precision of answers (Song et al., 2015; Ferr´e, 2017). More recent systems use a neural machine translation approach similar to translating natural languages, say, from French to English (Iyer et al., 2017a; Basik et al., 2018; Cheng et al., 2019; Liu et al., 2019; Guo et al., 2019; Cheng et al., 2019). Data Resources. We will now review the major data resources that have recently been used for evaluating NLI2DB systems. These resources are mainly created following two approaches: (1) Both NL and structured queries are manually created, and (2) structured queries are automatically generated, and then humans create the corresponding NL questions. Regarding fully manually created resources, (Yu et al., 2018) provided Spider, a dataset with 5,600 SQL queries, over 200 databases and 10,181 NL quest"
2020.acl-main.84,H94-1010,0,0.622644,"esources are mainly created following two approaches: (1) Both NL and structured queries are manually created, and (2) structured queries are automatically generated, and then humans create the corresponding NL questions. Regarding fully manually created resources, (Yu et al., 2018) provided Spider, a dataset with 5,600 SQL queries, over 200 databases and 10,181 NL questions annotated by 11 students, where some questions were manually paraphrased to increase the variability. (Finegan-Dollak et al., 2018) released Advising, with 4.5k questions about university course advising and SQL queries. (Dahl et al., 1994) created ATIS, a dataset with 5k user questions about flight-booking manually annotated with SQL queries and modified by (Iyer et al., 2017b) to reduce nesting. (Zelle and Mooney, 1996) created GeoQuery, with 877 questions about US geography annotated with Prolog and converted to SQL by (Popescu et al., 2003) and (Giordani and Moschitti, 2012). There are also smaller datasets about restaurants with 378 questions (Tang and Mooney, 2000), the Yelp website with 196 questions and IMDB with 131 questions (Yaghmazadeh et al., 2017). Resources using an automatic step usually rely on generating struct"
2020.acl-main.84,D19-1356,0,0.0129795,"anovic et al., 2010; Zheng et al., 2017). Parsing-based approaches use a natural language parser to analyze and reason about the grammatical structure of a query (Li and Jagadish, 2014; Saha et al., 2016). Grammar-based approaches only allow the user to formulate queries according to certain pre-defined rules, thus focus primarily on increasing the precision of answers (Song et al., 2015; Ferr´e, 2017). More recent systems use a neural machine translation approach similar to translating natural languages, say, from French to English (Iyer et al., 2017a; Basik et al., 2018; Cheng et al., 2019; Liu et al., 2019; Guo et al., 2019; Cheng et al., 2019). Data Resources. We will now review the major data resources that have recently been used for evaluating NLI2DB systems. These resources are mainly created following two approaches: (1) Both NL and structured queries are manually created, and (2) structured queries are automatically generated, and then humans create the corresponding NL questions. Regarding fully manually created resources, (Yu et al., 2018) provided Spider, a dataset with 5,600 SQL queries, over 200 databases and 10,181 NL questions annotated by 11 students, where some questions were ma"
2020.acl-main.84,D15-1166,0,0.017082,"nd Argentina as billing country? How many different genres do the tracks have, which were bought by customers who live in France? Which customers made at least 35 purchases, excluding titles from the Chico Science & Nacao Zumbi album? Table 4: Example questions from OTTA and Spider. We grouped the examples by the hardness scores. The examples are for the Chinook domain, which is an online music store database. t (during the pre-training phase H E is replaced by (i) H R ). Then let αt be the attention weight for the i-th token. For each token we add the loss 2017), we apply attention based on (Luong et al., 2015), where e ht−1 = tanh(Wc [ht−1 : ct ]). For the selection of the terms, we have four output matrices WR , WT , WA , WC , where WR encodes the grammar rules (i.e. for the nonterminal symbols), and WT , WA , WC encode the table names, attributes and comparison operations, respectively. Depending on the current frontier node, the next output is computed by: at = argmax(softmax(WR ∗ ht )) (3) Grammar Encoder. The tree encoder, which we use for the pre-training, is based on the same GRU architecture as the decoder. The hidden states for each rule are computed by: ht = GRU([at−1 : apt : nft ], ht−1"
2020.acl-main.84,P18-1033,0,0.100093,"ources. We will now review the major data resources that have recently been used for evaluating NLI2DB systems. These resources are mainly created following two approaches: (1) Both NL and structured queries are manually created, and (2) structured queries are automatically generated, and then humans create the corresponding NL questions. Regarding fully manually created resources, (Yu et al., 2018) provided Spider, a dataset with 5,600 SQL queries, over 200 databases and 10,181 NL questions annotated by 11 students, where some questions were manually paraphrased to increase the variability. (Finegan-Dollak et al., 2018) released Advising, with 4.5k questions about university course advising and SQL queries. (Dahl et al., 1994) created ATIS, a dataset with 5k user questions about flight-booking manually annotated with SQL queries and modified by (Iyer et al., 2017b) to reduce nesting. (Zelle and Mooney, 1996) created GeoQuery, with 877 questions about US geography annotated with Prolog and converted to SQL by (Popescu et al., 2003) and (Giordani and Moschitti, 2012). There are also smaller datasets about restaurants with 378 questions (Tang and Mooney, 2000), the Yelp website with 196 questions and IMDB with"
2020.acl-main.84,W00-1317,0,0.0447859,"lly paraphrased to increase the variability. (Finegan-Dollak et al., 2018) released Advising, with 4.5k questions about university course advising and SQL queries. (Dahl et al., 1994) created ATIS, a dataset with 5k user questions about flight-booking manually annotated with SQL queries and modified by (Iyer et al., 2017b) to reduce nesting. (Zelle and Mooney, 1996) created GeoQuery, with 877 questions about US geography annotated with Prolog and converted to SQL by (Popescu et al., 2003) and (Giordani and Moschitti, 2012). There are also smaller datasets about restaurants with 378 questions (Tang and Mooney, 2000), the Yelp website with 196 questions and IMDB with 131 questions (Yaghmazadeh et al., 2017). Resources using an automatic step usually rely on generating structured queries using templates created by experts. (Zhong et al., 2017) created WikiSQL, a collection of 80k pairs of SQL queries and NL questions made using Wikipedia. However, SQL queries are relatively simple because each of the databases consists of only a single table without foreign keys. Hence, the queries do not contain joins. (Dubey et al., 2019) developed 898 LC-QuAD 2.0, with 30,000 complex NL questions and SPARQL queries over"
2020.acl-main.84,P15-1129,0,0.20624,"Missing"
2020.acl-main.84,P17-1041,0,0.0977204,"d to more than 5 in Spider). 2) It allows us to cover the whole range of data present in the database structure and not to focus on the most prominent examples. 3) Our annotation procedure provides alignments between operations in the formal language and words in the question, which are an additional source of supervision when training. We applied our approach1 to five datasets, yielding a large corpus called OTTA2 which consists of 3,792 complex NL questions plus their corresponding OTs as well as the token assignment for one of our domains. Besides, we have adapted a stateof-the-art system (Yin and Neubig, 2017) to work on to operation trees, and included a mechanism to profit from token alignment annotations when training. The system yields better results with up to 7 point increase when trained on aligned OTs. 2 Related Work In this section, we first review the related work in the area of Natural Language Interfaces to Databases (NLI2DB). Afterwards, we focus on the data resources that are currently available to evaluate these systems. Natural Language Interfaces to Databases. There is a vast amount of literature on NLI2DB. A recent survey on methods and technologies is provided by (Affolter et al."
2020.acl-main.84,D18-1425,0,0.482414,"or Text-to-SQL, is a key task in natural language processing and the semantic web. It is usually approached by mapping a natural language question (NL question) into executable queries in formal representations such as logical forms, SPARQL or SQL. The state-of-the-art in this problem uses machine learning techniques to learn the mapping. Unfortunately, the construction of labeled corpora to train intensive, which is slowing down progress in this area. In particular, it usually requires recruiting SQL or SPARQL experts to write queries for natural language questions. For instance, in Spider (Yu et al., 2018), the authors recruited students to write SQL queries. They worked 500 person-hours to generate 5,600 queries, which corresponds to more than 5 minutes per question. As a more cost-effective alternative to writing formal queries manually, some authors propose to use templates to generate them automatically. For instance, LC-QUAD 2.0 (Dubey et al., 2019) used 22 templates based on the structure of the target knowledge graph. Constructing templates is also time-consuming, and the expressiveness of the automatically produced queries is limited. Apart from the high cost of generating queries, the"
2020.acl-main.84,Q17-1010,0,\N,Missing
2020.acl-main.84,P19-1444,0,\N,Missing
2020.acl-main.84,J19-1002,0,\N,Missing
2020.acl-srw.34,P19-1309,1,0.667856,"ther language pairs. 1 Introduction Parallel corpora constitute an essential training data resource for machine translation as well as other cross-lingual NLP tasks. However, large parallel corpora are only available for a handful of language pairs while the rest relies on semi-supervised or unsupervised methods for training. Since monolingual data are generally more abundant, parallel sentence mining from non-parallel corpora provides another opportunity for low-resource language pairs. An effective approach to parallel data mining is based on multilingual sentence embeddings (Schwenk, 2018; Artetxe and Schwenk, 2019b). However, existing methods to generate crosslingual representations are either heavily supervised or only apply to static word embeddings. An alternative approach to unsupervised multilingual training is that of Devlin et al. (2018) or Lample and Conneau (2019), who train a masked language model (M-BERT, XLM) on a concatenation of monolingual corpora in different languages to learn a joint structure of these languages together. While several authors (Pires et al., 2019; Wu and Dredze, 2019; Karthikeyan et al., 2019; Libovick´y et al., 2019) bring evidence of cross-lingual transfer within th"
2020.acl-srw.34,Q19-1038,1,0.698776,"ther language pairs. 1 Introduction Parallel corpora constitute an essential training data resource for machine translation as well as other cross-lingual NLP tasks. However, large parallel corpora are only available for a handful of language pairs while the rest relies on semi-supervised or unsupervised methods for training. Since monolingual data are generally more abundant, parallel sentence mining from non-parallel corpora provides another opportunity for low-resource language pairs. An effective approach to parallel data mining is based on multilingual sentence embeddings (Schwenk, 2018; Artetxe and Schwenk, 2019b). However, existing methods to generate crosslingual representations are either heavily supervised or only apply to static word embeddings. An alternative approach to unsupervised multilingual training is that of Devlin et al. (2018) or Lample and Conneau (2019), who train a masked language model (M-BERT, XLM) on a concatenation of monolingual corpora in different languages to learn a joint structure of these languages together. While several authors (Pires et al., 2019; Wu and Dredze, 2019; Karthikeyan et al., 2019; Libovick´y et al., 2019) bring evidence of cross-lingual transfer within th"
2020.acl-srw.34,D18-2029,0,0.0564986,"Missing"
2020.acl-srw.34,W18-6317,0,0.025539,"rable Corpora 255 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 255–262 c July 5 - July 10, 2020. 2020 Association for Computational Linguistics et al. (2017) derive sentence embeddings from internal representations of a neural machine translation system with a shared encoder. The universal sentence encoder (USE) (Cer et al., 2018; Yang et al., 2019) family covers sentence embedding models with a multi-task dual-encoder training framework including the tasks of question-answer prediction or natural language inference. Guo et al. (2018) directly optimize the cosine similarity between the source and target sentences using a bidirectional dual-encoder. These approaches rely on heavy supervision by parallel corpora which is not available for low-resource languages. Unsupervised multilingual word embeddings. Cross-lingual embeddings of words can be obtained by post-hoc alignment of monolingual word embeddings (Mikolov et al., 2013) and mean-pooled with IDF weights to represent sentences (Litschko et al., 2019). Unsupervised techniques to find a linear mapping between embedding spaces were proposed by Artetxe et al. (2018) and Co"
2020.acl-srw.34,P19-1493,0,0.117045,"Missing"
2020.acl-srw.34,P19-1492,1,0.775032,"se approaches rely on heavy supervision by parallel corpora which is not available for low-resource languages. Unsupervised multilingual word embeddings. Cross-lingual embeddings of words can be obtained by post-hoc alignment of monolingual word embeddings (Mikolov et al., 2013) and mean-pooled with IDF weights to represent sentences (Litschko et al., 2019). Unsupervised techniques to find a linear mapping between embedding spaces were proposed by Artetxe et al. (2018) and Conneau et al. (2018), using iterative self-learning or adversarial training. Several recent studies (Patra et al., 2019; Ormazabal et al., 2019) criticize this simplified approach, showing that even the embedding spaces of closely related languages are not isometric. Vuli´c et al. (2019) question the robustness of unsupervised mapping methods in challenging circumstances. Cross-lingual LM pretraining. Ma et al. (2019); Reimers and Gurevych (2019) derive monolingual sentence embeddings by mean-pooling contextualized word embeddings from BERT. Schuster et al. (2019); Wang et al. (2019b) propose mapping such contextualized embeddings into the multilingual space and report favorable results on the task of dependency parsing. Pires et al."
2020.acl-srw.34,P19-1018,0,0.0258298,"al dual-encoder. These approaches rely on heavy supervision by parallel corpora which is not available for low-resource languages. Unsupervised multilingual word embeddings. Cross-lingual embeddings of words can be obtained by post-hoc alignment of monolingual word embeddings (Mikolov et al., 2013) and mean-pooled with IDF weights to represent sentences (Litschko et al., 2019). Unsupervised techniques to find a linear mapping between embedding spaces were proposed by Artetxe et al. (2018) and Conneau et al. (2018), using iterative self-learning or adversarial training. Several recent studies (Patra et al., 2019; Ormazabal et al., 2019) criticize this simplified approach, showing that even the embedding spaces of closely related languages are not isometric. Vuli´c et al. (2019) question the robustness of unsupervised mapping methods in challenging circumstances. Cross-lingual LM pretraining. Ma et al. (2019); Reimers and Gurevych (2019) derive monolingual sentence embeddings by mean-pooling contextualized word embeddings from BERT. Schuster et al. (2019); Wang et al. (2019b) propose mapping such contextualized embeddings into the multilingual space and report favorable results on the task of dependen"
2020.acl-srw.34,D19-1410,0,0.128866,"DF weights to represent sentences (Litschko et al., 2019). Unsupervised techniques to find a linear mapping between embedding spaces were proposed by Artetxe et al. (2018) and Conneau et al. (2018), using iterative self-learning or adversarial training. Several recent studies (Patra et al., 2019; Ormazabal et al., 2019) criticize this simplified approach, showing that even the embedding spaces of closely related languages are not isometric. Vuli´c et al. (2019) question the robustness of unsupervised mapping methods in challenging circumstances. Cross-lingual LM pretraining. Ma et al. (2019); Reimers and Gurevych (2019) derive monolingual sentence embeddings by mean-pooling contextualized word embeddings from BERT. Schuster et al. (2019); Wang et al. (2019b) propose mapping such contextualized embeddings into the multilingual space and report favorable results on the task of dependency parsing. Pires et al. (2019) extract contextualized embeddings directly from unsupervised multilingual LMs and use them for parallel sentence retrieval. Other authors improve the alignment of representations in a multilingual LM using a parallel corpus as an anchor (Cao et al., 2020) or using iterative self-learning (Wang et a"
2020.acl-srw.34,P19-1178,0,0.0378085,"Missing"
2020.acl-srw.34,N19-1162,0,0.0168312,"spaces were proposed by Artetxe et al. (2018) and Conneau et al. (2018), using iterative self-learning or adversarial training. Several recent studies (Patra et al., 2019; Ormazabal et al., 2019) criticize this simplified approach, showing that even the embedding spaces of closely related languages are not isometric. Vuli´c et al. (2019) question the robustness of unsupervised mapping methods in challenging circumstances. Cross-lingual LM pretraining. Ma et al. (2019); Reimers and Gurevych (2019) derive monolingual sentence embeddings by mean-pooling contextualized word embeddings from BERT. Schuster et al. (2019); Wang et al. (2019b) propose mapping such contextualized embeddings into the multilingual space and report favorable results on the task of dependency parsing. Pires et al. (2019) extract contextualized embeddings directly from unsupervised multilingual LMs and use them for parallel sentence retrieval. Other authors improve the alignment of representations in a multilingual LM using a parallel corpus as an anchor (Cao et al., 2020) or using iterative self-learning (Wang et al., 2019a). None of these works apply multilingual embeddings to mine parallel sentences. Our work is the first in impro"
2020.acl-srw.34,P18-2037,0,0.405792,"e results for other language pairs. 1 Introduction Parallel corpora constitute an essential training data resource for machine translation as well as other cross-lingual NLP tasks. However, large parallel corpora are only available for a handful of language pairs while the rest relies on semi-supervised or unsupervised methods for training. Since monolingual data are generally more abundant, parallel sentence mining from non-parallel corpora provides another opportunity for low-resource language pairs. An effective approach to parallel data mining is based on multilingual sentence embeddings (Schwenk, 2018; Artetxe and Schwenk, 2019b). However, existing methods to generate crosslingual representations are either heavily supervised or only apply to static word embeddings. An alternative approach to unsupervised multilingual training is that of Devlin et al. (2018) or Lample and Conneau (2019), who train a masked language model (M-BERT, XLM) on a concatenation of monolingual corpora in different languages to learn a joint structure of these languages together. While several authors (Pires et al., 2019; Wu and Dredze, 2019; Karthikeyan et al., 2019; Libovick´y et al., 2019) bring evidence of cross"
2020.acl-srw.34,W17-2619,0,0.0242961,"supervised methods to model multilingual sentence embeddings and unsupervised methods to model multilingual word embeddings which can be aggregated into sentences. Furthermore, our approach is closely related to the recent research in cross-lingual language model (LM) pretraining. Supervised multilingual sentence embeddings. The state-of-the-art performance in parallel data mining is achieved by LASER (Artetxe and Schwenk, 2019b) – a multilingual BiLSTM model sharing a single encoder for 93 languages trained on parallel corpora to produce language agnostic sentence representations. Similarly, Schwenk and Douze (2017); Schwenk (2018); Espana-Bonet 1 11th Workshop on Building and Using Comparable Corpora 255 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 255–262 c July 5 - July 10, 2020. 2020 Association for Computational Linguistics et al. (2017) derive sentence embeddings from internal representations of a neural machine translation system with a shared encoder. The universal sentence encoder (USE) (Cer et al., 2018; Yang et al., 2019) family covers sentence embedding models with a multi-task dual-encoder training framework includi"
2020.acl-srw.34,P16-1162,0,0.0146403,"o the method remains unsupervised. In this section, we describe the pretrained model (Section 3.1), the fine-tuning objective (Section 3.2) and the extraction of sentence embeddings (Section 3.3). We provide details on the unsupervised MT system in Section 3.4. 3.1 XLM Pretraining The starting point for our experiments is a crosslingual language model (XLM) (Lample and Conneau, 2019) of the BERT family pretrained on concatenated monolingual texts in 100 languages using the masked language model (MLM) training objective (Devlin et al., 2018). The model processes the input in BPE subword units (Sennrich et al., 2016) with a shared vocabulary for all languages. In this work, we use the publicly available pretrained model XLM-1002 (Lample and Conneau, 2019) with 16 transformer layers, 16 attention heads and a hidden unit size of 1280. The model was trained on monolingual corpora in 100 languages with the BPE vocabulary of 240k subwords. 3.2 XLM Fine-tuning with a Translation Objective When parallel data is available, it can be leveraged in training of the multilingual language model using a translation language model loss (TLM) (Lample and Conneau, 2019). Pairs of sentences are concatenated, random tokens a"
2020.acl-srw.34,D19-1449,0,0.0533004,"Missing"
2020.acl-srw.34,D19-1575,0,0.0182848,"Artetxe et al. (2018) and Conneau et al. (2018), using iterative self-learning or adversarial training. Several recent studies (Patra et al., 2019; Ormazabal et al., 2019) criticize this simplified approach, showing that even the embedding spaces of closely related languages are not isometric. Vuli´c et al. (2019) question the robustness of unsupervised mapping methods in challenging circumstances. Cross-lingual LM pretraining. Ma et al. (2019); Reimers and Gurevych (2019) derive monolingual sentence embeddings by mean-pooling contextualized word embeddings from BERT. Schuster et al. (2019); Wang et al. (2019b) propose mapping such contextualized embeddings into the multilingual space and report favorable results on the task of dependency parsing. Pires et al. (2019) extract contextualized embeddings directly from unsupervised multilingual LMs and use them for parallel sentence retrieval. Other authors improve the alignment of representations in a multilingual LM using a parallel corpus as an anchor (Cao et al., 2020) or using iterative self-learning (Wang et al., 2019a). None of these works apply multilingual embeddings to mine parallel sentences. Our work is the first in improving unsupervised c"
2020.acl-srw.34,D19-1077,0,0.0143831,"ve approach to parallel data mining is based on multilingual sentence embeddings (Schwenk, 2018; Artetxe and Schwenk, 2019b). However, existing methods to generate crosslingual representations are either heavily supervised or only apply to static word embeddings. An alternative approach to unsupervised multilingual training is that of Devlin et al. (2018) or Lample and Conneau (2019), who train a masked language model (M-BERT, XLM) on a concatenation of monolingual corpora in different languages to learn a joint structure of these languages together. While several authors (Pires et al., 2019; Wu and Dredze, 2019; Karthikeyan et al., 2019; Libovick´y et al., 2019) bring evidence of cross-lingual transfer within the model, its internal representations are not entirely language agnostic. We propose a method to further align representations from such models into the cross-lingual space and use them to derive sentence embeddings. Our approach is completely unsupervised and is applicable even for very distant language pairs. The proposed method outperforms previous unsupervised approaches on the BUCC 20181 shared task, and is even competitive with several supervised baselines. The paper is organized as fol"
2020.acl-srw.34,W17-2512,0,0.0208881,"sentence mining task (News test set). The supervised and unsupervised winners are highlighted in bold. Artetxe and Schwenk (2019b) values obtained using the public implementation of the LASER toolkit. 4.4 Evaluation I: Parallel Corpus Mining We measure the performance of our method on the BUCC shared task of parallel corpus mining where the system is expected to search two comparable non-aligned corpora and identify pairs of parallel sentences. We evaluate on two data sets – the original BUCC 2018 corpus created by inserting parallel sentences into monolingual texts extracted from Wikipedia (Zweigenbaum et al., 2017) and a new BUCC-like data set (News train and test) which we created by shuffling 10k parallel sentence from News Commentary into 400k monolingual sentences from News Crawl. The BUCC and News data sets are comparable in size and contain parallel sentences from the same source, but differ in overall domain. In order to score all candidate sentence pairs, we use the margin-based approach of Artetxe and Schwenk (2019a) which was proved to eliminate the hubness problem of embedding spaces and yield superior results (Artetxe and Schwenk, 2019b). The score relies on cosine similarity to measure the"
2020.coling-main.230,2020.acl-main.652,1,0.878955,"r makes a set of interrelated questions to the system, which extracts the answers from reference text (Choi et al., 2018). These systems are trained on datasets of human-human dialogues collected using Wizard-of-Oz techniques, where two crowdsourcers are paired at random to emulate the questioner and the answerer. Several projects have shown that it is possible to train effective systems using such datasets. For instance, QuAC includes question and answers about popular people in Wikipedia (Choi et al., 2018), and DoQA includes question-answer conversations on cooking, movies and travel FAQs (Campos et al., 2020). Building such datasets comes at a cost, which limits the widespread use of conversational systems built using supervised learning. The fact that conversational systems interact naturally with users poses an exciting opportunity to improve them after deployment. Given enough training data, a company can deploy a basic conversational system, enough to be accepted and used by users. Once the system is deployed, the interaction with users and their feedback can be used to improve the system. In this work we focus on the case where a CQA system trained off-line is deployed and receives explicit b"
2020.coling-main.230,D18-1241,0,0.0512798,"ack is derived from gold annotations. The results show that our method is able to improve over the initial supervised system, getting close to a fully-supervised system that has access to the same labeled examples in in-domain experiments (QuAC), and even matching in out-of-domain experiments (DoQA). Our work opens the prospect to exploit interactions with real users and improve conversational systems after deployment. 1 Introduction In Conversational Question Answering (CQA) systems, the user makes a set of interrelated questions to the system, which extracts the answers from reference text (Choi et al., 2018). These systems are trained on datasets of human-human dialogues collected using Wizard-of-Oz techniques, where two crowdsourcers are paired at random to emulate the questioner and the answerer. Several projects have shown that it is possible to train effective systems using such datasets. For instance, QuAC includes question and answers about popular people in Wikipedia (Choi et al., 2018), and DoQA includes question-answer conversations on cooking, movies and travel FAQs (Campos et al., 2020). Building such datasets comes at a cost, which limits the widespread use of conversational systems b"
2020.coling-main.230,N19-1423,0,0.00573438,"t understands the passage. There are two main methods: the extractive method, in which the answer is selected as a contiguous span in the reference passage, and the abstractive method, in which the answer text is generated. Many datasets (Rajpurkar et al., 2016; Rajpurkar et al., 2018; Dunn et al., 2017; Koˇcisk`y et al., 2018; Trischler et al., 2017; Bajaj et al., 2016) and systems have been proposed to address this task, where the extractive scenario has drawn special attention (Wang and Jiang, 2017; Seo et al., 2017). Lately, with the incursion of large pre-trained language models as BERT (Devlin et al., 2019), XLNet (Yang et al., 2019) and their relatives, the state of the art has been dominated by systems that use the representations obtained with these pre-trained language models. The systems learn answer pointer networks that consist of two 1 https://github.com/jjacampos/FeedbackWeightedLearning 2562 classifiers, one for spotting the start token of the answer span and another for spotting the end token of the answer span. In reading comprehension, the questions are individual and isolated, that is, they do not have any dialogue structure. Due to the increasing interest on modelling the conversa"
2020.coling-main.230,P19-1358,0,0.101709,"Within this framework of lifelong learning, we particularly focus on building a system that adapts to changes in the data distribution after deployment (Agirre et al., 2019). There have been efforts for learning actively from dialogue during deployment. The question answering (QA) setting was explored in Weston (2016) and Li et al. (2017), where they analyzed a variety of learning strategies for different dialogue tasks with diverse types of feedback. In these studies they also touch on forward prediction, which uses explicit user correction. This idea was later applied to chit-chat systems (Hancock et al., 2019). These works relied on users explicitly providing the correct answer. This strong assumption was relaxed in Weston (2016), where the user provides binary feedback on correct and incorrect answers in a synthetic question answering task (Weston et al., 2015). Our work also uses binary feedback and tests it in more realistic CQA datasets. In a similar online setup to ours, Liu et al. (2018b) explored contextual multi-armed bandits for dialogue response selection using a customized version of Thompson sampling. In this work they use the Ubuntu Dialogue Corpus (Lowe et al., 2015) for user simulati"
2020.coling-main.230,N18-1187,0,0.182184,"Missing"
2020.coling-main.230,W15-4640,0,0.020904,"-chat systems (Hancock et al., 2019). These works relied on users explicitly providing the correct answer. This strong assumption was relaxed in Weston (2016), where the user provides binary feedback on correct and incorrect answers in a synthetic question answering task (Weston et al., 2015). Our work also uses binary feedback and tests it in more realistic CQA datasets. In a similar online setup to ours, Liu et al. (2018b) explored contextual multi-armed bandits for dialogue response selection using a customized version of Thompson sampling. In this work they use the Ubuntu Dialogue Corpus (Lowe et al., 2015) for user simulation. In the case of task-oriented dialogue systems, Liu et al. (2018a) propose a hybrid learning method with supervised pre-training and further improvement using human teaching and feedback. For the human teaching case they use imitation learning with explicit corrections done by an expert. After that, they resort to reinforcement learning for further improvement thanks to long term rewards defined by task completion. 4 Experiments In this section we present the experiments with feedback-weighted learning (FWL). In the experiments we first build a supervised system (S0 ), and"
2020.coling-main.230,W19-4102,0,0.031364,"QA datasets where questions and answers are interrelated have been created following the Wizard-ofOz technique. Among all the datasets we can highlight QuAC (Choi et al., 2018), CoQA (Reddy et al., 2019) and DoQA (Campos et al., 2020). While the first two datasets cover more formal domains as Wikipedia articles and literature, the latter covers different domains extracted from online forums as StackExchange. Contextual versions of the previously mentioned reading comprehension models have successfully modelled the conversational structure in those datasets (Qu et al., 2019b; Qu et al., 2019a; Ohsugi et al., 2019; Ju et al., 2019). 3 Importance Sampling for Learning After Deployment In our learning after deployment scenario we start by training an initial S0 system in an off-line and supervised way. This first system follows the traditional workflow where we have access to limited supervised training and development data. Then, we take the best performing system on the development data and deploy it to serve user queries. In this deployment phase, every time a user makes a query x, the system generates an answer y and the user gives binary feedback to it. Over time, the system generates different answ"
2020.coling-main.230,D14-1162,0,0.0861155,"Missing"
2020.coling-main.230,D16-1264,0,0.120899,"t interactions with real users and improve conversational systems after deployment. All the code and dataset splits are made publicly available 1 . 2 Related Work on Conversational Question Answering CQA research builds on reading comprehension. In reading comprehension the system has to answer questions about a certain passage of text in order to show that it understands the passage. There are two main methods: the extractive method, in which the answer is selected as a contiguous span in the reference passage, and the abstractive method, in which the answer text is generated. Many datasets (Rajpurkar et al., 2016; Rajpurkar et al., 2018; Dunn et al., 2017; Koˇcisk`y et al., 2018; Trischler et al., 2017; Bajaj et al., 2016) and systems have been proposed to address this task, where the extractive scenario has drawn special attention (Wang and Jiang, 2017; Seo et al., 2017). Lately, with the incursion of large pre-trained language models as BERT (Devlin et al., 2019), XLNet (Yang et al., 2019) and their relatives, the state of the art has been dominated by systems that use the representations obtained with these pre-trained language models. The systems learn answer pointer networks that consist of two 1"
2020.coling-main.230,P18-2124,0,0.0164386,"users and improve conversational systems after deployment. All the code and dataset splits are made publicly available 1 . 2 Related Work on Conversational Question Answering CQA research builds on reading comprehension. In reading comprehension the system has to answer questions about a certain passage of text in order to show that it understands the passage. There are two main methods: the extractive method, in which the answer is selected as a contiguous span in the reference passage, and the abstractive method, in which the answer text is generated. Many datasets (Rajpurkar et al., 2016; Rajpurkar et al., 2018; Dunn et al., 2017; Koˇcisk`y et al., 2018; Trischler et al., 2017; Bajaj et al., 2016) and systems have been proposed to address this task, where the extractive scenario has drawn special attention (Wang and Jiang, 2017; Seo et al., 2017). Lately, with the incursion of large pre-trained language models as BERT (Devlin et al., 2019), XLNet (Yang et al., 2019) and their relatives, the state of the art has been dominated by systems that use the representations obtained with these pre-trained language models. The systems learn answer pointer networks that consist of two 1 https://github.com/jjac"
2020.coling-main.230,Q19-1016,0,0.0433041,"of two 1 https://github.com/jjacampos/FeedbackWeightedLearning 2562 classifiers, one for spotting the start token of the answer span and another for spotting the end token of the answer span. In reading comprehension, the questions are individual and isolated, that is, they do not have any dialogue structure. Due to the increasing interest on modelling the conversational structure behind user questions, several CQA datasets where questions and answers are interrelated have been created following the Wizard-ofOz technique. Among all the datasets we can highlight QuAC (Choi et al., 2018), CoQA (Reddy et al., 2019) and DoQA (Campos et al., 2020). While the first two datasets cover more formal domains as Wikipedia articles and literature, the latter covers different domains extracted from online forums as StackExchange. Contextual versions of the previously mentioned reading comprehension models have successfully modelled the conversational structure in those datasets (Qu et al., 2019b; Qu et al., 2019a; Ohsugi et al., 2019; Ju et al., 2019). 3 Importance Sampling for Learning After Deployment In our learning after deployment scenario we start by training an initial S0 system in an off-line and supervise"
2020.coling-main.230,W17-2623,0,0.0124032,"code and dataset splits are made publicly available 1 . 2 Related Work on Conversational Question Answering CQA research builds on reading comprehension. In reading comprehension the system has to answer questions about a certain passage of text in order to show that it understands the passage. There are two main methods: the extractive method, in which the answer is selected as a contiguous span in the reference passage, and the abstractive method, in which the answer text is generated. Many datasets (Rajpurkar et al., 2016; Rajpurkar et al., 2018; Dunn et al., 2017; Koˇcisk`y et al., 2018; Trischler et al., 2017; Bajaj et al., 2016) and systems have been proposed to address this task, where the extractive scenario has drawn special attention (Wang and Jiang, 2017; Seo et al., 2017). Lately, with the incursion of large pre-trained language models as BERT (Devlin et al., 2019), XLNet (Yang et al., 2019) and their relatives, the state of the art has been dominated by systems that use the representations obtained with these pre-trained language models. The systems learn answer pointer networks that consist of two 1 https://github.com/jjacampos/FeedbackWeightedLearning 2562 classifiers, one for spotting t"
2020.emnlp-main.326,W18-6539,0,0.0323688,"Missing"
2020.emnlp-main.326,P17-1103,0,0.071079,"Missing"
2020.emnlp-main.326,D15-1166,0,0.0277494,"Missing"
2020.emnlp-main.326,2020.acl-main.64,0,0.13663,"to three domains, evaluating several stateof-the-art chatbots, and drawing comparisons to related work. The framework is released as a ready-to-use tool. 1 Introduction Evaluation is a long-standing issue in developing conversational dialogue systems (i.e., chatbots). The underlying difficulty in evaluation lies in the problem’s open-ended nature, as chatbots do not solve a clearly-defined task whose success can be measured in relation to an a priori defined ground truth. Automatic metrics have so far failed to show high correlation with human evaluations (Liu et al., 2016; Lowe et al., 2017; Mehri and Eskenazi, 2020). Human evaluation approaches are mainly classified according to the following: single-turn vs. multi-turn evaluation, and direct user evaluation vs. expert judge evaluation. Single-turn analysis is usually performed by a human judge that rates a single response of the bot to a given context, whereas multi-turn analysis is often performed by a user that interacts with the bot and rates the interaction. Single-turn ratings disregard the multiturn nature of a dialogue (See et al., 2019). Although more and more multi-turn evaluations are performed, most of them are based on human-bot conversation"
2020.emnlp-main.326,P19-1534,0,0.0596159,"Missing"
2020.emnlp-main.326,N19-1170,0,0.138067,"ve so far failed to show high correlation with human evaluations (Liu et al., 2016; Lowe et al., 2017; Mehri and Eskenazi, 2020). Human evaluation approaches are mainly classified according to the following: single-turn vs. multi-turn evaluation, and direct user evaluation vs. expert judge evaluation. Single-turn analysis is usually performed by a human judge that rates a single response of the bot to a given context, whereas multi-turn analysis is often performed by a user that interacts with the bot and rates the interaction. Single-turn ratings disregard the multiturn nature of a dialogue (See et al., 2019). Although more and more multi-turn evaluations are performed, most of them are based on human-bot conversations, which are costly to obtain and tend to suffer from low quality (Dinan et al., 2020a). The instructions to be followed by annotators are often chosen ad-hoc and there are no unified definitions. Compounded with the use of often criticized Likert scales (Amidei et al., 2019a), these evaluations often yield a low agreement. The required cost and time efforts also inhibit the widespread use of such evaluations, which raises questions on the replicability, robustness, and thus significa"
2020.emnlp-main.618,2020.acl-main.421,1,0.946926,"ome previous findings in cross-lingual transfer learning need to be reconsidered in the light of this phenomenon. Based on the gained insights, we also improve the state-of-the-art in XNLI for the translate-test and zero-shot approaches by 4.3 and 2.8 points, respectively. 1 Introduction While most NLP resources are English-specific, there have been several recent efforts to build multilingual benchmarks. One possibility is to collect and annotate data in multiple languages separately (Clark et al., 2020), but most existing datasets have been created through translation (Conneau et al., 2018; Artetxe et al., 2020). This approach has two desirable properties: it relies on existing professional translation services rather than requiring expertise in multiple languages, and it results in parallel evaluation sets that offer a meaningful measure of the cross-lingual transfer gap of different models. The resulting multilingual datasets are generally used for evaluation only, relying on existing English datasets for training. Closely related to that, cross-lingual transfer learning aims to leverage large datasets available in one language—typically English—to build multilingual models that can generalize to o"
2020.emnlp-main.618,D15-1075,0,0.0417336,"for Question Answering (QA). A notable exception is TyDi QA (Clark et al., 2020), a contemporaneous QA dataset that was separately annotated in 11 languages. Other cross-lingual datasets leverage existing multilingual resources, as it is the case of MLDoc (Schwenk and Li, 2018) for document classification and Wikiann (Pan et al., 2017) for named entity recognition. Concurrent to our work, Hu et al. (2020) combine some of these datasets into a single multilingual benchmark, and evaluate some well-known methods on it. Annotation artifacts. Several studies have shown that NLI datasets like SNLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2018) contain spurious patterns that can be exploited to obtain strong results without making real inferential decisions. For instance, Gururangan et al. (2018) and Poliak et al. (2018) showed that a hypothesis-only baseline performs better than chance due to cues on their lexical choice and sentence length. Similarly, McCoy et al. (2019) showed that NLI models tend to predict entailment for sentence pairs with a high lexical overlap. Several authors have worked on adversarial datasets to diagnose these issues and provide a more challenging benchmark (Naik et al"
2020.emnlp-main.618,buck-etal-2014-n,0,0.150085,"Missing"
2020.emnlp-main.618,2020.lrec-1.677,0,0.0774837,"Missing"
2020.emnlp-main.618,2020.tacl-1.30,0,0.061861,"tly can reduce the lexical overlap between them, which current models are highly sensitive to. We show that some previous findings in cross-lingual transfer learning need to be reconsidered in the light of this phenomenon. Based on the gained insights, we also improve the state-of-the-art in XNLI for the translate-test and zero-shot approaches by 4.3 and 2.8 points, respectively. 1 Introduction While most NLP resources are English-specific, there have been several recent efforts to build multilingual benchmarks. One possibility is to collect and annotate data in multiple languages separately (Clark et al., 2020), but most existing datasets have been created through translation (Conneau et al., 2018; Artetxe et al., 2020). This approach has two desirable properties: it relies on existing professional translation services rather than requiring expertise in multiple languages, and it results in parallel evaluation sets that offer a meaningful measure of the cross-lingual transfer gap of different models. The resulting multilingual datasets are generally used for evaluation only, relying on existing English datasets for training. Closely related to that, cross-lingual transfer learning aims to leverage l"
2020.emnlp-main.618,2020.acl-main.747,0,0.0739969,"Missing"
2020.emnlp-main.618,D18-1269,0,0.390593,"ive to. We show that some previous findings in cross-lingual transfer learning need to be reconsidered in the light of this phenomenon. Based on the gained insights, we also improve the state-of-the-art in XNLI for the translate-test and zero-shot approaches by 4.3 and 2.8 points, respectively. 1 Introduction While most NLP resources are English-specific, there have been several recent efforts to build multilingual benchmarks. One possibility is to collect and annotate data in multiple languages separately (Clark et al., 2020), but most existing datasets have been created through translation (Conneau et al., 2018; Artetxe et al., 2020). This approach has two desirable properties: it relies on existing professional translation services rather than requiring expertise in multiple languages, and it results in parallel evaluation sets that offer a meaningful measure of the cross-lingual transfer gap of different models. The resulting multilingual datasets are generally used for evaluation only, relying on existing English datasets for training. Closely related to that, cross-lingual transfer learning aims to leverage large datasets available in one language—typically English—to build multilingual models t"
2020.emnlp-main.618,N19-1423,0,0.235386,"ffects the generalization ability of current models. Based on the gained insights, we improve the state-of-the-art in XNLI, and show that some previous findings need to be reconsidered in the light of this phenomenon. 2 Related work Cross-lingual transfer learning. Current crosslingual models work by pre-training multilingual representations using some form of language modeling, which are then fine-tuned on the relevant task and transferred to different languages. Some authors leverage parallel data to that end (Conneau and Lample, 2019; Huang et al., 2019), but training a model akin to BERT (Devlin et al., 2019) on the combination of monolingual corpora in multiple languages is also effective (Conneau et al., 2020). Closely related to our work, Singh et al. (2019) showed that replacing segments of the training data with their translation during fine-tuning is helpful. However, they attribute this behavior to a data augmentation effect, which we believe should be reconsidered given the new evidence we provide. Multilingual benchmarks. Most benchmarks covering a wide set of languages have been created through translation, as it is the case of XNLI (Conneau et al., 2018) for NLI, PAWS-X (Yang et al., 20"
2020.emnlp-main.618,N13-1073,0,0.0213103,"ed in English and translated into 6 other languages. In both cases, the translation was done by professional translators at the document level (i.e., when translating a question, the text answering it was also shown). For our BT-XX and MT-XX variants, we translate the context paragraph and the questions independently, and map the answer spans using the same procedure as Carrino et al. (2020).3 For the T RANSLATE -T EST approach, we use the official machine translated versions of MLQA, run inference over them, and map the predicted answer spans back to the target language.4 3 We use FastAlign (Dyer et al., 2013) for word alignment, and discard the few questions for which the mapping method fails (when none of the tokens in the answer span are aligned). 4 We use the same procedure as for the training set except that (i) given the small size of the test set, we combine it with WikiMatrix (Schwenk et al., 2019) to aid word alignment, (ii) we use Jieba for Chinese segmentation instead of the Moses tokenizer, and (iii) for the few unaligned spans, we return the English answer. 7676 Model Train en fr es de el bg ru tr ar vi th zh hi sw ur avg Test set machine translated into English (TRANSLATE-TEST) O RIG"
2020.emnlp-main.618,D18-1045,0,0.0203606,"v5.0 (Espl`a et al., 2019). For English-Finnish, we train for 40 epochs on Europarl and Wiki Titles from WMT 2019 (Barrault et al., 2019), ParaCrawl v5.0, and DGT, EUbookshop and TildeMODEL from OPUS (Tiedemann, 2012). In both cases, we remove sentences longer than 250 tokens, with a source/target ratio exceeding 1.5, or for which langid.py (Lui and Baldwin, 2012) predicts a different language, resulting in a final corpus size of 48M and 7M sentence pairs, respectively. We use sampling decoding with a temperature of 0.5 for inference, which produces more diverse translations than beam search (Edunov et al., 2018) and performed better in our preliminary experiments. 2 https://github.com/pytorch/fairseq 3.3 Tasks and evaluation procedure We use the following tasks for our experiments: Natural Language Inference (NLI). Given a premise and a hypothesis, the task is to determine whether there is an entailment, neutral or contradiction relation between them. We fine-tune our models on MultiNLI (Williams et al., 2018) for 10 epochs using the same settings as Liu et al. (2019). In most of our experiments, we evaluate on XNLI (Conneau et al., 2018), which comprises 2490 development and 5010 test instances in 1"
2020.emnlp-main.618,2020.acl-main.253,0,0.0311854,"h these artifacts in multilingual settings. Translationese. Translated texts are known to have unique features like simplification, explicitation, normalization and interference, which are refer to as translationese (Volansky et al., 2013). This phenomenon has been reported to have a notable impact in machine translation evaluation (Zhang and Toral, 2019; Graham et al., 2019). For instance, back-translation brings large BLEU gains for reversed test sets (i.e., when translationese is on the source side and original text is used as reference), but its effect diminishes in the natural direction (Edunov et al., 2020). While connected, the phenomenon we analyze is different in that it arises from translation inconsistencies due to the lack of context, and affects cross-lingual transfer learning rather than machine translation. 3 Experimental design Our goal is to analyze the effect of both human and machine translation in cross-lingual models. For that purpose, the core idea of our work is to (i) use machine translation to either translate the training set into other languages, or generate English paraphrases of it through back-translation, and (ii) evaluate the resulting systems on original, human transla"
2020.emnlp-main.618,W19-6721,0,0.0282282,"Missing"
2020.emnlp-main.618,P18-2103,0,0.0132646,"iNLI (Williams et al., 2018) contain spurious patterns that can be exploited to obtain strong results without making real inferential decisions. For instance, Gururangan et al. (2018) and Poliak et al. (2018) showed that a hypothesis-only baseline performs better than chance due to cues on their lexical choice and sentence length. Similarly, McCoy et al. (2019) showed that NLI models tend to predict entailment for sentence pairs with a high lexical overlap. Several authors have worked on adversarial datasets to diagnose these issues and provide a more challenging benchmark (Naik et al., 2018; Glockner et al., 2018; Nie et al., 2020). Besides NLI, other tasks like QA have also been found to be susceptible to annotation artifacts (Jia and Liang, 2017; Kaushik and Lipton, 2018). While previous work has focused on the monolingual scenario, we show that translation can interfere with these artifacts in multilingual settings. Translationese. Translated texts are known to have unique features like simplification, explicitation, normalization and interference, which are refer to as translationese (Volansky et al., 2013). This phenomenon has been reported to have a notable impact in machine translation evaluati"
2020.emnlp-main.618,N18-2017,0,0.0579794,"Missing"
2020.emnlp-main.618,D17-1215,0,0.0280766,"ions. For instance, Gururangan et al. (2018) and Poliak et al. (2018) showed that a hypothesis-only baseline performs better than chance due to cues on their lexical choice and sentence length. Similarly, McCoy et al. (2019) showed that NLI models tend to predict entailment for sentence pairs with a high lexical overlap. Several authors have worked on adversarial datasets to diagnose these issues and provide a more challenging benchmark (Naik et al., 2018; Glockner et al., 2018; Nie et al., 2020). Besides NLI, other tasks like QA have also been found to be susceptible to annotation artifacts (Jia and Liang, 2017; Kaushik and Lipton, 2018). While previous work has focused on the monolingual scenario, we show that translation can interfere with these artifacts in multilingual settings. Translationese. Translated texts are known to have unique features like simplification, explicitation, normalization and interference, which are refer to as translationese (Volansky et al., 2013). This phenomenon has been reported to have a notable impact in machine translation evaluation (Zhang and Toral, 2019; Graham et al., 2019). For instance, back-translation brings large BLEU gains for reversed test sets (i.e., whe"
2020.emnlp-main.618,D18-1546,0,0.0133113,"ururangan et al. (2018) and Poliak et al. (2018) showed that a hypothesis-only baseline performs better than chance due to cues on their lexical choice and sentence length. Similarly, McCoy et al. (2019) showed that NLI models tend to predict entailment for sentence pairs with a high lexical overlap. Several authors have worked on adversarial datasets to diagnose these issues and provide a more challenging benchmark (Naik et al., 2018; Glockner et al., 2018; Nie et al., 2020). Besides NLI, other tasks like QA have also been found to be susceptible to annotation artifacts (Jia and Liang, 2017; Kaushik and Lipton, 2018). While previous work has focused on the monolingual scenario, we show that translation can interfere with these artifacts in multilingual settings. Translationese. Translated texts are known to have unique features like simplification, explicitation, normalization and interference, which are refer to as translationese (Volansky et al., 2013). This phenomenon has been reported to have a notable impact in machine translation evaluation (Zhang and Toral, 2019; Graham et al., 2019). For instance, back-translation brings large BLEU gains for reversed test sets (i.e., when translationese is on the"
2020.emnlp-main.618,D18-2012,0,0.0135019,"(BT-ES and BT-FI), and (iii) a machine translated version in Spanish or Finnish (MT-ES and MT-FI). For sentences occurring multiple times in the training set (e.g., premises repeated for multiple hypotheses), we use the exact same translation for all occurrences, as our goal is to understand the inherent effect of translation rather than its potential application as a data augmentation method. In order to train the machine translation systems for MT-XX and BT-XX, we use the big Transformer model (Vaswani et al., 2017) with the same settings as Ott et al. (2018) and SentencePiece tokenization (Kudo and Richardson, 2018) with a joint vocabulary of 32k subwords. For English-Spanish, we train for 10 epochs on all parallel data from WMT 2013 (Bojar et al., 2013) and ParaCrawl v5.0 (Espl`a et al., 2019). For English-Finnish, we train for 40 epochs on Europarl and Wiki Titles from WMT 2019 (Barrault et al., 2019), ParaCrawl v5.0, and DGT, EUbookshop and TildeMODEL from OPUS (Tiedemann, 2012). In both cases, we remove sentences longer than 250 tokens, with a source/target ratio exceeding 1.5, or for which langid.py (Lui and Baldwin, 2012) predicts a different language, resulting in a final corpus size of 48M and 7M"
2020.emnlp-main.618,2020.acl-main.653,0,0.456468,"u et al., 2020). Closely related to our work, Singh et al. (2019) showed that replacing segments of the training data with their translation during fine-tuning is helpful. However, they attribute this behavior to a data augmentation effect, which we believe should be reconsidered given the new evidence we provide. Multilingual benchmarks. Most benchmarks covering a wide set of languages have been created through translation, as it is the case of XNLI (Conneau et al., 2018) for NLI, PAWS-X (Yang et al., 2019) for adversarial paraphrase identification, and XQuAD (Artetxe et al., 2020) and MLQA (Lewis et al., 2020) for Question Answering (QA). A notable exception is TyDi QA (Clark et al., 2020), a contemporaneous QA dataset that was separately annotated in 11 languages. Other cross-lingual datasets leverage existing multilingual resources, as it is the case of MLDoc (Schwenk and Li, 2018) for document classification and Wikiann (Pan et al., 2017) for named entity recognition. Concurrent to our work, Hu et al. (2020) combine some of these datasets into a single multilingual benchmark, and evaluate some well-known methods on it. Annotation artifacts. Several studies have shown that NLI datasets like SNLI"
2020.emnlp-main.618,2021.ccl-1.108,0,0.119194,"Missing"
2020.emnlp-main.618,P12-3005,0,0.0226209,"th the same settings as Ott et al. (2018) and SentencePiece tokenization (Kudo and Richardson, 2018) with a joint vocabulary of 32k subwords. For English-Spanish, we train for 10 epochs on all parallel data from WMT 2013 (Bojar et al., 2013) and ParaCrawl v5.0 (Espl`a et al., 2019). For English-Finnish, we train for 40 epochs on Europarl and Wiki Titles from WMT 2019 (Barrault et al., 2019), ParaCrawl v5.0, and DGT, EUbookshop and TildeMODEL from OPUS (Tiedemann, 2012). In both cases, we remove sentences longer than 250 tokens, with a source/target ratio exceeding 1.5, or for which langid.py (Lui and Baldwin, 2012) predicts a different language, resulting in a final corpus size of 48M and 7M sentence pairs, respectively. We use sampling decoding with a temperature of 0.5 for inference, which produces more diverse translations than beam search (Edunov et al., 2018) and performed better in our preliminary experiments. 2 https://github.com/pytorch/fairseq 3.3 Tasks and evaluation procedure We use the following tasks for our experiments: Natural Language Inference (NLI). Given a premise and a hypothesis, the task is to determine whether there is an entailment, neutral or contradiction relation between them."
2020.emnlp-main.618,P19-1334,0,0.0178622,"rrent to our work, Hu et al. (2020) combine some of these datasets into a single multilingual benchmark, and evaluate some well-known methods on it. Annotation artifacts. Several studies have shown that NLI datasets like SNLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2018) contain spurious patterns that can be exploited to obtain strong results without making real inferential decisions. For instance, Gururangan et al. (2018) and Poliak et al. (2018) showed that a hypothesis-only baseline performs better than chance due to cues on their lexical choice and sentence length. Similarly, McCoy et al. (2019) showed that NLI models tend to predict entailment for sentence pairs with a high lexical overlap. Several authors have worked on adversarial datasets to diagnose these issues and provide a more challenging benchmark (Naik et al., 2018; Glockner et al., 2018; Nie et al., 2020). Besides NLI, other tasks like QA have also been found to be susceptible to annotation artifacts (Jia and Liang, 2017; Kaushik and Lipton, 2018). While previous work has focused on the monolingual scenario, we show that translation can interfere with these artifacts in multilingual settings. Translationese. Translated te"
2020.emnlp-main.618,C18-1198,0,0.0591891,"al., 2015) and MultiNLI (Williams et al., 2018) contain spurious patterns that can be exploited to obtain strong results without making real inferential decisions. For instance, Gururangan et al. (2018) and Poliak et al. (2018) showed that a hypothesis-only baseline performs better than chance due to cues on their lexical choice and sentence length. Similarly, McCoy et al. (2019) showed that NLI models tend to predict entailment for sentence pairs with a high lexical overlap. Several authors have worked on adversarial datasets to diagnose these issues and provide a more challenging benchmark (Naik et al., 2018; Glockner et al., 2018; Nie et al., 2020). Besides NLI, other tasks like QA have also been found to be susceptible to annotation artifacts (Jia and Liang, 2017; Kaushik and Lipton, 2018). While previous work has focused on the monolingual scenario, we show that translation can interfere with these artifacts in multilingual settings. Translationese. Translated texts are known to have unique features like simplification, explicitation, normalization and interference, which are refer to as translationese (Volansky et al., 2013). This phenomenon has been reported to have a notable impact in machi"
2020.emnlp-main.618,2020.acl-main.441,0,0.0604596,"2018) contain spurious patterns that can be exploited to obtain strong results without making real inferential decisions. For instance, Gururangan et al. (2018) and Poliak et al. (2018) showed that a hypothesis-only baseline performs better than chance due to cues on their lexical choice and sentence length. Similarly, McCoy et al. (2019) showed that NLI models tend to predict entailment for sentence pairs with a high lexical overlap. Several authors have worked on adversarial datasets to diagnose these issues and provide a more challenging benchmark (Naik et al., 2018; Glockner et al., 2018; Nie et al., 2020). Besides NLI, other tasks like QA have also been found to be susceptible to annotation artifacts (Jia and Liang, 2017; Kaushik and Lipton, 2018). While previous work has focused on the monolingual scenario, we show that translation can interfere with these artifacts in multilingual settings. Translationese. Translated texts are known to have unique features like simplification, explicitation, normalization and interference, which are refer to as translationese (Volansky et al., 2013). This phenomenon has been reported to have a notable impact in machine translation evaluation (Zhang and Toral"
2020.emnlp-main.618,W18-6301,0,0.0316787,"ack-translation using Spanish or Finnish as pivot (BT-ES and BT-FI), and (iii) a machine translated version in Spanish or Finnish (MT-ES and MT-FI). For sentences occurring multiple times in the training set (e.g., premises repeated for multiple hypotheses), we use the exact same translation for all occurrences, as our goal is to understand the inherent effect of translation rather than its potential application as a data augmentation method. In order to train the machine translation systems for MT-XX and BT-XX, we use the big Transformer model (Vaswani et al., 2017) with the same settings as Ott et al. (2018) and SentencePiece tokenization (Kudo and Richardson, 2018) with a joint vocabulary of 32k subwords. For English-Spanish, we train for 10 epochs on all parallel data from WMT 2013 (Bojar et al., 2013) and ParaCrawl v5.0 (Espl`a et al., 2019). For English-Finnish, we train for 40 epochs on Europarl and Wiki Titles from WMT 2019 (Barrault et al., 2019), ParaCrawl v5.0, and DGT, EUbookshop and TildeMODEL from OPUS (Tiedemann, 2012). In both cases, we remove sentences longer than 250 tokens, with a source/target ratio exceeding 1.5, or for which langid.py (Lui and Baldwin, 2012) predicts a differe"
2020.emnlp-main.618,P17-1178,0,0.0181614,"Most benchmarks covering a wide set of languages have been created through translation, as it is the case of XNLI (Conneau et al., 2018) for NLI, PAWS-X (Yang et al., 2019) for adversarial paraphrase identification, and XQuAD (Artetxe et al., 2020) and MLQA (Lewis et al., 2020) for Question Answering (QA). A notable exception is TyDi QA (Clark et al., 2020), a contemporaneous QA dataset that was separately annotated in 11 languages. Other cross-lingual datasets leverage existing multilingual resources, as it is the case of MLDoc (Schwenk and Li, 2018) for document classification and Wikiann (Pan et al., 2017) for named entity recognition. Concurrent to our work, Hu et al. (2020) combine some of these datasets into a single multilingual benchmark, and evaluate some well-known methods on it. Annotation artifacts. Several studies have shown that NLI datasets like SNLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2018) contain spurious patterns that can be exploited to obtain strong results without making real inferential decisions. For instance, Gururangan et al. (2018) and Poliak et al. (2018) showed that a hypothesis-only baseline performs better than chance due to cues on their lexical cho"
2020.emnlp-main.618,S18-2023,0,0.0601046,"Missing"
2020.emnlp-main.618,D16-1264,0,0.0589293,"nstances in 15 languages. These were originally annotated in English, and the resulting premises and hypotheses were independently translated into the rest of the languages by professional translators. For the T RANSLATE -T EST approach, we use the machine translated versions from the authors. Following Conneau et al. (2020), we select the best epoch checkpoint according to the average accuracy in the development set. Question Answering (QA). Given a context paragraph and a question, the task is to identify the span answering the question in the context. We fine-tune our models on SQuAD v1.1 (Rajpurkar et al., 2016) for 2 epochs using the same settings as Liu et al. (2019), and report test results for the last epoch. We use two datasets for evaluation: XQuAD (Artetxe et al., 2020), a subset of the SQuAD development set translated into 10 other languages, and MLQA (Lewis et al., 2020) a dataset consisting of parallel context paragraphs plus the corresponding questions annotated in English and translated into 6 other languages. In both cases, the translation was done by professional translators at the document level (i.e., when translating a question, the text answering it was also shown). For our BT-XX an"
2020.emnlp-main.618,tiedemann-2012-parallel,0,0.0195398,"ation method. In order to train the machine translation systems for MT-XX and BT-XX, we use the big Transformer model (Vaswani et al., 2017) with the same settings as Ott et al. (2018) and SentencePiece tokenization (Kudo and Richardson, 2018) with a joint vocabulary of 32k subwords. For English-Spanish, we train for 10 epochs on all parallel data from WMT 2013 (Bojar et al., 2013) and ParaCrawl v5.0 (Espl`a et al., 2019). For English-Finnish, we train for 40 epochs on Europarl and Wiki Titles from WMT 2019 (Barrault et al., 2019), ParaCrawl v5.0, and DGT, EUbookshop and TildeMODEL from OPUS (Tiedemann, 2012). In both cases, we remove sentences longer than 250 tokens, with a source/target ratio exceeding 1.5, or for which langid.py (Lui and Baldwin, 2012) predicts a different language, resulting in a final corpus size of 48M and 7M sentence pairs, respectively. We use sampling decoding with a temperature of 0.5 for inference, which produces more diverse translations than beam search (Edunov et al., 2018) and performed better in our preliminary experiments. 2 https://github.com/pytorch/fairseq 3.3 Tasks and evaluation procedure We use the following tasks for our experiments: Natural Language Infere"
2020.emnlp-main.618,N18-1101,0,0.265601,"ble exception is TyDi QA (Clark et al., 2020), a contemporaneous QA dataset that was separately annotated in 11 languages. Other cross-lingual datasets leverage existing multilingual resources, as it is the case of MLDoc (Schwenk and Li, 2018) for document classification and Wikiann (Pan et al., 2017) for named entity recognition. Concurrent to our work, Hu et al. (2020) combine some of these datasets into a single multilingual benchmark, and evaluate some well-known methods on it. Annotation artifacts. Several studies have shown that NLI datasets like SNLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2018) contain spurious patterns that can be exploited to obtain strong results without making real inferential decisions. For instance, Gururangan et al. (2018) and Poliak et al. (2018) showed that a hypothesis-only baseline performs better than chance due to cues on their lexical choice and sentence length. Similarly, McCoy et al. (2019) showed that NLI models tend to predict entailment for sentence pairs with a high lexical overlap. Several authors have worked on adversarial datasets to diagnose these issues and provide a more challenging benchmark (Naik et al., 2018; Glockner et al., 2018; Nie e"
2020.emnlp-main.618,D19-1382,0,0.0655812,"n et al., 2019) on the combination of monolingual corpora in multiple languages is also effective (Conneau et al., 2020). Closely related to our work, Singh et al. (2019) showed that replacing segments of the training data with their translation during fine-tuning is helpful. However, they attribute this behavior to a data augmentation effect, which we believe should be reconsidered given the new evidence we provide. Multilingual benchmarks. Most benchmarks covering a wide set of languages have been created through translation, as it is the case of XNLI (Conneau et al., 2018) for NLI, PAWS-X (Yang et al., 2019) for adversarial paraphrase identification, and XQuAD (Artetxe et al., 2020) and MLQA (Lewis et al., 2020) for Question Answering (QA). A notable exception is TyDi QA (Clark et al., 2020), a contemporaneous QA dataset that was separately annotated in 11 languages. Other cross-lingual datasets leverage existing multilingual resources, as it is the case of MLDoc (Schwenk and Li, 2018) for document classification and Wikiann (Pan et al., 2017) for named entity recognition. Concurrent to our work, Hu et al. (2020) combine some of these datasets into a single multilingual benchmark, and evaluate so"
2020.emnlp-main.618,W19-5208,0,0.0387003,"e et al., 2020). Besides NLI, other tasks like QA have also been found to be susceptible to annotation artifacts (Jia and Liang, 2017; Kaushik and Lipton, 2018). While previous work has focused on the monolingual scenario, we show that translation can interfere with these artifacts in multilingual settings. Translationese. Translated texts are known to have unique features like simplification, explicitation, normalization and interference, which are refer to as translationese (Volansky et al., 2013). This phenomenon has been reported to have a notable impact in machine translation evaluation (Zhang and Toral, 2019; Graham et al., 2019). For instance, back-translation brings large BLEU gains for reversed test sets (i.e., when translationese is on the source side and original text is used as reference), but its effect diminishes in the natural direction (Edunov et al., 2020). While connected, the phenomenon we analyze is different in that it arises from translation inconsistencies due to the lack of context, and affects cross-lingual transfer learning rather than machine translation. 3 Experimental design Our goal is to analyze the effect of both human and machine translation in cross-lingual models. For"
2020.emnlp-main.618,L18-1560,0,0.0778383,"red given the new evidence we provide. Multilingual benchmarks. Most benchmarks covering a wide set of languages have been created through translation, as it is the case of XNLI (Conneau et al., 2018) for NLI, PAWS-X (Yang et al., 2019) for adversarial paraphrase identification, and XQuAD (Artetxe et al., 2020) and MLQA (Lewis et al., 2020) for Question Answering (QA). A notable exception is TyDi QA (Clark et al., 2020), a contemporaneous QA dataset that was separately annotated in 11 languages. Other cross-lingual datasets leverage existing multilingual resources, as it is the case of MLDoc (Schwenk and Li, 2018) for document classification and Wikiann (Pan et al., 2017) for named entity recognition. Concurrent to our work, Hu et al. (2020) combine some of these datasets into a single multilingual benchmark, and evaluate some well-known methods on it. Annotation artifacts. Several studies have shown that NLI datasets like SNLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2018) contain spurious patterns that can be exploited to obtain strong results without making real inferential decisions. For instance, Gururangan et al. (2018) and Poliak et al. (2018) showed that a hypothesis-only baseline p"
2020.emnlp-main.618,P16-1009,0,0.0267267,"l models. In fact, the type of text a system is trained on does not typically match the type of text it is exposed to at test time: T RANSLATE -T EST systems are trained on original data and evaluated on machine translated test sets, Z ERO -S HOT systems are trained on original data and evaluated on human translated test sets, and T RANSLATE -T RAIN systems are trained on machine translated data and evaluated on human translated test sets. Despite overlooked to date, we show that such mismatch has a notable impact in the performance of existing cross-lingual models. By using back-translation (Sennrich et al., 2016) to paraphrase each training instance, we obtain another English version of the training set that better resembles the test set, obtaining substantial improvements for the T RANSLATE -T EST and Z ERO -S HOT approaches in cross-lingual Natural Language Inference (NLI). While improvements brought by machine translation have previously been attributed to data augmentation (Singh et al., 2019), we reject this hypothesis and show that the phenomenon is only present in translated test sets, but not in original ones. Instead, our analysis reveals that 1 We use the term original to refer to non-transl"
2020.emnlp-main.618,W13-2201,0,\N,Missing
2020.emnlp-main.618,W19-5301,0,\N,Missing
2020.lrec-1.55,2020.lrec-1.588,1,0.807353,"he experiments were carried out using the extractive information of the train/dev/test splits of ElkarHizketak. The baselines that use the monolingual BERTeus model are trained and evaluated using only the ElkarHizketak dataset (native training). Regarding the cross-lingual models, apart from the just mentioned approach, another two different cross-lingual transfer learning approaches are followed: • zero-shot cross-lingual transfer: we use the train data of QuAC for training the model, and evaluate it on ElkarHizketak. BERTeus: We have used the pre-trained BERT model for the Basque Language (Agerri et al., 2020) due to the low representation this language has in the official multilingual BERT model. This Basque BERT model has been trained on a corpus comprising the Basque Wikipedia and news articles from Basque media. • low resource cross-lingual transfer: once we have the previous model, we fine tune it using the small train split of ElkarHizketak and test it on ElkarHizketak test split. For completeness, both development and test figures are shown. mBERT ours: We have pre-trained a multilingual BERT model with the intention of performing transfer ex440 Model Majority without dialogue history BERTeu"
2020.lrec-1.55,C18-1139,0,0.0153044,"first attempt to create a conversational QA dataset in a language other than English. Contextualized word embeddings are representations that are sensitive to the context where the word appear. These models are first pre-trained on big corpora using a language modeling loss. The pre-trained model is then fine-tuned to the task at hand, using manually annotated datasets and appropriate loss functions. They have been successfully used in a variety of natural language processing tasks, including QA and dialogue systems (Devlin et al., 2019; Qu et al., 2019). ELMO (Peters et al., 2018) and Flair (Akbik et al., 2018) are language models built upon LSTM-based architectures. BERT (Devlin et al., 2019) is a model based on a transformer architecture, and pre-trained using a masked language model objective. BERT has been very successful on many NLP tasks, and several variants exists, such as RoBERTA (Liu et al., 2019b) and ALBERT (Lan et al., 2020). These models are trained for English, but some authors have built pre-trained models for other languages such as French (Martin et al., 2019). Interestingly, the knowledge learned by pre-trained models such as BERT has been shown to be transferable across domains."
2020.lrec-1.55,D18-1241,0,0.034992,"Missing"
2020.lrec-1.55,N19-1423,0,0.509734,"interactions, analogous to QuAC but for Basque. Due to the lack of Basque speakers in crowdsourcing platforms, we used social media to recruit volunteers. The resulting dataset contains close to 400 dialogues and more than 1600 question and answers, and its small size presents a realistic low436 resource scenario for CQA systems. Figure 1 displays an example of a dialogue related to a Wikipedia section about the biography of Edorta Jimenez, a Basque writer, with translations into English. Current CQA systems rely heavily on pre-trained language models such as ELMO (Peters et al., 2018), BERT (Devlin et al., 2019) and derived models (Liu et al., 2019b; Lample and Conneau, 2019). These models are first trained on large numbers of text using a language model loss, and then finetuned on the train data of a CQA dataset. The most recent models include multilingual versions (Devlin et al., 2019; Lample and Conneau, 2019), where the text in different languages is represented in a common space. The multilingual versions allow to transfer trained models to languages other than that used to fine tune them. In this paper we test the performance of variants of BERT in low-resource scenarios: native training data o"
2020.lrec-1.55,P17-1162,0,0.0549055,"Missing"
2020.lrec-1.55,P17-1167,0,0.0291856,"comparable to those obtained for the analogous QuAC English dataset. (3) Our experiments show that dialogue history models are not directly transferable from one language to another. The dataset is freely available with an open license1 . To our knowledge, this is the first nonEnglish conversational QA dataset, the first conversational dataset for Basque, and the first cross-lingual transfer results on conversational question answering. 2. Related Work Work in conversational QA systems has led to the creation of a variety of datasets for the task (Nguyen et al., 2016; Rajpurkar et al., 2016; Iyyer et al., 2017; Trischler et al., 2017; Koˇcisk´y et al., 2018; Dunn et al., 2017). MS MARCO (Nguyen et al., 2016), NewsQA (Trischler et al., 2017) or SearchQA (Dunn et al., 2017) are some examples of reading comprehension datasets that require systems to understand a document to properly answer the queries. SequentialQA (Iyyer et al., 2017) comprises more than 6.000 question sequences where each question refers and refines previous ones, and therefore can be seen as different turns in a dialogue. More similar to our work, CoQA (Reddy et al., 2018) and QuAC (Choi et al., 2018) are two datasets that contain"
2020.lrec-1.55,Q18-1023,0,0.0538942,"Missing"
2020.lrec-1.55,P19-1227,0,0.103463,"asque. Due to the lack of Basque speakers in crowdsourcing platforms, we used social media to recruit volunteers. The resulting dataset contains close to 400 dialogues and more than 1600 question and answers, and its small size presents a realistic low436 resource scenario for CQA systems. Figure 1 displays an example of a dialogue related to a Wikipedia section about the biography of Edorta Jimenez, a Basque writer, with translations into English. Current CQA systems rely heavily on pre-trained language models such as ELMO (Peters et al., 2018), BERT (Devlin et al., 2019) and derived models (Liu et al., 2019b; Lample and Conneau, 2019). These models are first trained on large numbers of text using a language model loss, and then finetuned on the train data of a CQA dataset. The most recent models include multilingual versions (Devlin et al., 2019; Lample and Conneau, 2019), where the text in different languages is represented in a common space. The multilingual versions allow to transfer trained models to languages other than that used to fine tune them. In this paper we test the performance of variants of BERT in low-resource scenarios: native training data only, zeroshot transfer (English train"
2020.lrec-1.55,N18-1202,0,0.0741895,"tak, a small dataset of CQA interactions, analogous to QuAC but for Basque. Due to the lack of Basque speakers in crowdsourcing platforms, we used social media to recruit volunteers. The resulting dataset contains close to 400 dialogues and more than 1600 question and answers, and its small size presents a realistic low436 resource scenario for CQA systems. Figure 1 displays an example of a dialogue related to a Wikipedia section about the biography of Edorta Jimenez, a Basque writer, with translations into English. Current CQA systems rely heavily on pre-trained language models such as ELMO (Peters et al., 2018), BERT (Devlin et al., 2019) and derived models (Liu et al., 2019b; Lample and Conneau, 2019). These models are first trained on large numbers of text using a language model loss, and then finetuned on the train data of a CQA dataset. The most recent models include multilingual versions (Devlin et al., 2019; Lample and Conneau, 2019), where the text in different languages is represented in a common space. The multilingual versions allow to transfer trained models to languages other than that used to fine tune them. In this paper we test the performance of variants of BERT in low-resource scena"
2020.lrec-1.55,D16-1264,0,0.0836777,"Missing"
2020.lrec-1.55,W17-2623,0,0.124438,"sque 1. Introduction Conversational Question Answering (CQA) systems meet user information needs by having conversations with them. Users pose initial queries in free form text, and the systems usually answer the queries by returning relevant excerpts extracted from a reference passage. The answers returned by the system invite users to pose follow up questions, which are again answered, therefore creating a conversation between the system and the user. The field has received much attention in the last years, and there exist nowadays a variety of datasets for the task (Rajpurkar et al., 2016; Trischler et al., 2017; Nguyen et al., 2016; Koˇcisk´y et al., 2018; Dunn et al., 2017; Choi et al., 2018). Some of the datasets are very large. For instance, QuAC (Choi et al., 2018) contains thousands of dialogues and tens of thousands of question answering turns, which have been collected using wizard-of-oz techniques with paid crowdworkers. The dataset is built on top of Wikipedia sections about popular people and organizations. The high results of current systems are encouraging, and seem to show that the technology is ready for industrial adoption. Unfortunately, all current datasets are in English, and data"
2020.lrec-1.588,C18-1139,0,0.508845,"els used in this work are publicly available. Keywords: Neural language representation models, Information Extraction, Less-resourced languages 1. Introduction Word embeddings and pre-trained language models allow to build rich representations of text and have enabled improvements across most NLP tasks. The most successful models include word embeddings like FastText (Bojanowski et al., 2017), and, more recently, pre-trained language models which can be used to produce contextual embeddings or directly fine-tuned for each task. Good examples of the later are character-based models like Flair (Akbik et al., 2018) and masked language models like BERT (Devlin et al., 2019). In many cases the teams that developed the algorithms also release their models, which facilitates both reproducibility and their application in downstream tasks. Given that the models are expensive to train, many companies and research groups tend to use those models, rather than building their own. This could be suboptimal as, for many languages, the models have been trained on easily obtained public corpora, which tends to be smaller and/or of lower quality that other existing corpora. In addition, pre-trained language models for"
2020.lrec-1.588,N19-1078,0,0.0527324,"he word. In this case, the extracted hidden state contains information propagated from the end of the sentence to this point. Both hidden states are concatenated to generate the final embedding. Pooled Contextualized Embeddings: Flair embeddings, however, struggle to generate an appropriate word representation for words in underspecified contexts, namely, in sentences in which, for example, local information is not sufficient to know the named entity type of a given word. In order to address this issue a variant of the original Flair embeddings is proposed: “Pooled Contextualized Embeddings” (Akbik et al., 2019). In this approach, every contextualized embedding is kept into a memory which is later used in a pooling operation to obtain a global word representation consisting of the concatenation of all the local contextualized embeddings obtained for a given word. They reported significant improvements for NER by using this new version of Flair embeddings. Note that this does not affect to the way the Flair pre-trained embedding models are calculated. The pooling operation is involved in the process of using such pre-trained models in order to obtain word representations for a given task such as NER o"
2020.lrec-1.588,Q17-1010,0,0.647349,"ults than publicly available versions in downstream NLP tasks, including topic classification, sentiment classification, PoS tagging and NER. This work sets a new state-of-the-art in those tasks for Basque. All benchmarks and models used in this work are publicly available. Keywords: Neural language representation models, Information Extraction, Less-resourced languages 1. Introduction Word embeddings and pre-trained language models allow to build rich representations of text and have enabled improvements across most NLP tasks. The most successful models include word embeddings like FastText (Bojanowski et al., 2017), and, more recently, pre-trained language models which can be used to produce contextual embeddings or directly fine-tuned for each task. Good examples of the later are character-based models like Flair (Akbik et al., 2018) and masked language models like BERT (Devlin et al., 2019). In many cases the teams that developed the algorithms also release their models, which facilitates both reproducibility and their application in downstream tasks. Given that the models are expensive to train, many companies and research groups tend to use those models, rather than building their own. This could be"
2020.lrec-1.588,N19-1423,0,0.273673,"ural language representation models, Information Extraction, Less-resourced languages 1. Introduction Word embeddings and pre-trained language models allow to build rich representations of text and have enabled improvements across most NLP tasks. The most successful models include word embeddings like FastText (Bojanowski et al., 2017), and, more recently, pre-trained language models which can be used to produce contextual embeddings or directly fine-tuned for each task. Good examples of the later are character-based models like Flair (Akbik et al., 2018) and masked language models like BERT (Devlin et al., 2019). In many cases the teams that developed the algorithms also release their models, which facilitates both reproducibility and their application in downstream tasks. Given that the models are expensive to train, many companies and research groups tend to use those models, rather than building their own. This could be suboptimal as, for many languages, the models have been trained on easily obtained public corpora, which tends to be smaller and/or of lower quality that other existing corpora. In addition, pre-trained language models for non-English languages are not always available. In that cas"
2020.lrec-1.588,L18-1550,0,0.0198096,"(Bojanowski et al., 2017). The approach is implemented in the FastText library5 , and two types of pre-trained word vectors are available (based on 300 dimensions): Wiki word vectors (FastText-official-wikipedia) were trained on Wikipedia using the skip-gram model described in (Bojanowski et al., 2017) with default parameters (a window size of 5, 3-6 length character n-grams and 5 negatives). Common Crawl word vectors (FastText-officialcommon-crawl) were trained on Common Crawl and Wikipedia using CBOW with position-weights, with character n-grams of length 5, a window size 5 and 10 negatives(Grave et al., 2018). In this work, we trained BMC word vectors (FastTextBMC) of 300 dimensions on the BMC corpus described above, using CBOW with position-weights and the default parameters of the original paper. (Bojanowski et al., 2017). 3.2. Contextual String Embeddings: Flair Flair refers to both a deep learning system and to a specific type of character-based contextual word embeddings. Flair (embeddings and system) have been successfully applied to sequence labeling tasks obtaining state-of-the-art results for a number of English Named Entity Recognition (NER) and Part-of-Speech tagging benchmarks (Akbik e"
2020.lrec-1.588,P19-1027,0,0.165638,"ik et al., 2018), which are built upon LSTM-based architectures and trained as language models. More recently, Devlin et al. (2019) introduced BERT, a model based on the transformer architecture trained as a masked language model, which has obtained very good results on a variety of NLP tasks. The multilingual counterpart of BERT, called mBERT, is a single language model pre-trained from corpora in more than 100 languages. mBERT enables to perform transfer knowledge techniques among languages, so that systems can be trained on datasets in languages different to the one used to fine tune them (Heinzerling and Strube, 2019; Pires et al., 2019). When pre-training mBERT the corpora sizes in different languages are very diverse, with English corpora being order of magnitudes larger than that of the minority languages. The authors alleviate this issue by oversampling examples of lower resource languages. However, as the parameters and vocabulary is shared across the languages, this means that each language has to share the quota of substrings and parameters with the rest of the languages. As shown in the introduction, this causes tokenization problems for low resource languages. CamemBERT (Martin et al., 2019) is a"
2020.lrec-1.588,P18-1007,0,0.0195995,"the original multilingual BERT model. In this section we describe the methods used for creating the vocabulary, the model architecture, the pre-training objective and procedure. The main differences between our model and the original implementation are the corpus used for the pre-training, the algorithm for sub-word vocabulary creation and the usage of a different masking strategy that is not available for the BERTBASE model yet. Sub-word vocabulary We create a cased sub-word vocabulary containing 50,000 tokens using the unigram language model based sub-word segmentation algorithm proposed by Kudo (2018). We do not use the same algorithm as BERT because the WordPiece (Wu et al., 2016) implementation they originally used is not publicly available. We have increased the vocabulary size from 30,000 sub-word 6 5 https://fasttext.cc/ BERT language models A single layer of size 128 was used, with word reprojection and a dropout value of 0.3068. 4783 units up to 50,000 expecting to be beneficial for the Basque language due to its agglutinative nature. Our vocabulary is learned from the whole training corpus but we do not cover all the characters in order to avoid very rare ones. We set the coverage"
2020.lrec-1.588,D14-1162,0,0.0882807,"for the four tasks, our goal was not to train and use the systems in the most optimal manner. Instead, and as mentioned before, we have focused on head-to-head comparisons. In this sense, better results could be expected using variations of pretrained models that have reported better results for English (Liu et al., 2019), or making an effort in developing better adaptations to each of the tasks. 2. Related work Deep learning methods in NLP rely on the ability to represent words as continuous vectors on a low dimensional space, called word embeddings. Word2vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014) are among the best models that build word embeddings by analyzing co-occurrence patterns extracted from large corpora. FastText (Bojanowski et al., 2017) proposes an improvement over those models, consisting on embedding subword units, thereby attempting to introduce morphological information. Rich morphology languages such as Basque should especially profit from such word representations. FastText distributes embeddings for more than 150 languages trained on Common Crawl and Wikipedia. In this paper we build FastText embeddings using a carefully collected corpus in Basque and show that it pe"
2020.lrec-1.588,N18-1202,0,0.351381,"alculate one vector irrespective of the fact that the same word banku may convey different senses when used in different contexts, namely, “financial institution”,“bench”, “supply or stock”, among others. In order to address this problem, contextual word embeddings are proposed; the idea is to be able to generate different word representations ac1 http://ixa2.si.ehu.es/text-representation-models/ basque 2 Also available from Hugginface in https://huggingface. co/ixa-ehu/berteus-base-cased 3 cording to the context in which the word appears. Examples of such contextual representations are ELMO (Peters et al., 2018) and Flair (Akbik et al., 2018), which are built upon LSTM-based architectures and trained as language models. More recently, Devlin et al. (2019) introduced BERT, a model based on the transformer architecture trained as a masked language model, which has obtained very good results on a variety of NLP tasks. The multilingual counterpart of BERT, called mBERT, is a single language model pre-trained from corpora in more than 100 languages. mBERT enables to perform transfer knowledge techniques among languages, so that systems can be trained on datasets in languages different to the one used to f"
2020.lrec-1.588,P19-1493,0,0.0201834,"ilt upon LSTM-based architectures and trained as language models. More recently, Devlin et al. (2019) introduced BERT, a model based on the transformer architecture trained as a masked language model, which has obtained very good results on a variety of NLP tasks. The multilingual counterpart of BERT, called mBERT, is a single language model pre-trained from corpora in more than 100 languages. mBERT enables to perform transfer knowledge techniques among languages, so that systems can be trained on datasets in languages different to the one used to fine tune them (Heinzerling and Strube, 2019; Pires et al., 2019). When pre-training mBERT the corpora sizes in different languages are very diverse, with English corpora being order of magnitudes larger than that of the minority languages. The authors alleviate this issue by oversampling examples of lower resource languages. However, as the parameters and vocabulary is shared across the languages, this means that each language has to share the quota of substrings and parameters with the rest of the languages. As shown in the introduction, this causes tokenization problems for low resource languages. CamemBERT (Martin et al., 2019) is a recent effort to bui"
2020.nlpcovid19-2.15,D19-1371,0,0.0195916,"n practice. The classical BM25F search algorithm (Zaragoza et al., 2004) is used to retrieve the most relevant paragraphs given a natural language question. Question Answering Given a question in natural language and a paragraph, the QA module returns the answer to the question in the paragraph or “No answer” otherwise. The implemented system is based on neural network techniques. More specifically, we have used the SciBERT language representation model, which is a pretrained language model based on BERT, but trained on a large corpus of scientific text, including text from biomedical domain (Beltagy et al., 2019). Following the usual reading comprehension method we use SciBERT as a pointer network, which selects an answer start and end index given a question and a paragraph. We used both SQuAD and QuAC to fine-tune SciBERT for QA. Manual inspection revealed that a system fine-tuned on SQuAD2.0 produced answers that were specially good for COVID related questions seeking short answers. However, we also observed that a fine-tuning with SQuAD2.0 and QuAC produced answers of better quality, particularly for questions which require longer answers. We thus decided to use a SciBERT model fine-tuned first on"
2020.nlpcovid19-2.15,2020.acl-main.652,1,0.818695,"its correlation to human satisfaction. Our findings are in line with de Vries et al. (2020), who claim that due to the tradition of prioritizing quantity of data to naturalness and real-world use cases, researchers could end up with findings that are not relevant for the real world. In this sense the system trained on the SQuAD 2.0 dataset, which is not built with prospective users, achieves highest automatic metrics but falls behind when attending to user preference. There have been recent efforts for building QA datasets closer to real-world use cases (Choi et al., 2018; Reddy et al., 2019; Campos et al., 2020; Castelli et al., 2020). Among all of them, we show that the system trained also on the QuAC dataset is preferred by the users. The contributions of our paper are two-fold. First, we describe the QA system that is able to answer COVID related questions to aid clinical scientists to find the information they need effectively among thousands of scientific papers. Second, we report some analysis on the shortcomings that automatic evaluation of QA systems suffer from. 2 Related Work The release of the CORD-19 dataset has attracted the interest of a wide variety of researchers, who have focused on"
2020.nlpcovid19-2.15,D18-1241,0,0.0685619,"listed in the tasks of the challenge. The system is not tailored towards specific questions, and can be readily used to answer any other question. One of the challenges during development was the lack of resources for automatically evaluating the system. After submission we have been able to partially evaluate the main components of our system, using related shared-tasks and datasets that have been recently released and were not available at the time. The automatic evaluation revealed that the submitted system was not optimal, contradicting some design choices, such as using the QuAC dataset (Choi et al., 2018) to fine-tune the QA system in addition to the SQuAD 2.0 (Rajpurkar et al., 2018) dataset. We thus performed an additional A/B test to check whether users preferred the system using QuAC or not. The test confirmed our intuition, and raises questions on the suitability of automatic metrics and its correlation to human satisfaction. Our findings are in line with de Vries et al. (2020), who claim that due to the tradition of prioritizing quantity of data to naturalness and real-world use cases, researchers could end up with findings that are not relevant for the real world. In this sense the syst"
2020.nlpcovid19-2.15,2021.ccl-1.108,0,0.0605296,"Missing"
2020.nlpcovid19-2.15,2020.nlpcovid19-acl.18,0,0.0288341,"Missing"
2020.nlpcovid19-2.15,P18-2124,0,0.206702,"cific questions, and can be readily used to answer any other question. One of the challenges during development was the lack of resources for automatically evaluating the system. After submission we have been able to partially evaluate the main components of our system, using related shared-tasks and datasets that have been recently released and were not available at the time. The automatic evaluation revealed that the submitted system was not optimal, contradicting some design choices, such as using the QuAC dataset (Choi et al., 2018) to fine-tune the QA system in addition to the SQuAD 2.0 (Rajpurkar et al., 2018) dataset. We thus performed an additional A/B test to check whether users preferred the system using QuAC or not. The test confirmed our intuition, and raises questions on the suitability of automatic metrics and its correlation to human satisfaction. Our findings are in line with de Vries et al. (2020), who claim that due to the tradition of prioritizing quantity of data to naturalness and real-world use cases, researchers could end up with findings that are not relevant for the real world. In this sense the system trained on the SQuAD 2.0 dataset, which is not built with prospective users, a"
2020.nlpcovid19-2.15,Q19-1016,0,0.0150054,"tomatic metrics and its correlation to human satisfaction. Our findings are in line with de Vries et al. (2020), who claim that due to the tradition of prioritizing quantity of data to naturalness and real-world use cases, researchers could end up with findings that are not relevant for the real world. In this sense the system trained on the SQuAD 2.0 dataset, which is not built with prospective users, achieves highest automatic metrics but falls behind when attending to user preference. There have been recent efforts for building QA datasets closer to real-world use cases (Choi et al., 2018; Reddy et al., 2019; Campos et al., 2020; Castelli et al., 2020). Among all of them, we show that the system trained also on the QuAC dataset is preferred by the users. The contributions of our paper are two-fold. First, we describe the QA system that is able to answer COVID related questions to aid clinical scientists to find the information they need effectively among thousands of scientific papers. Second, we report some analysis on the shortcomings that automatic evaluation of QA systems suffer from. 2 Related Work The release of the CORD-19 dataset has attracted the interest of a wide variety of researchers"
2021.acl-long.506,D18-1214,0,0.0326137,"Missing"
2021.acl-long.506,P17-1042,1,0.796074,"g maps that are only locally linear (Nakashole, 2018), or learning a separate map for each word (Glavaˇs and Vuli´c, 2020). However, all these methods are supervised, and have the same fundamental limitation of aligning a set of separately trained embeddings (Ormazabal et al., 2019). Self-learning. While early mapping methods relied on a bilingual dictionary to learn the alignment, this requirement was alleviated thanks to selflearning, which iteratively re-induces the dictionary during training. This enabled learning CLWEs in a semi-supervised fashion starting from a weak initial dictionary (Artetxe et al., 2017), or in a completely unsupervised manner when combined with adversarial training (Conneau et al., 2018a) or initialization heuristics (Artetxe et al., 2018; Hoshen and Wolf, 2018). Our proposed method also incorporates a self-learning procedure, showing that this technique can also be effective with non-mapping methods. Joint CLWE methods. Before the popularization of offline mapping, most CLWE methods extended monolingual embedding algorithms by either incorporating an explicit cross-lingual term in their learning objective, or directly replacing words with their translation equivalents in th"
2021.acl-long.506,P18-1073,1,0.87472,"present words from two or more languages in a shared space, so that semantically similar words in different languages are close to each other. Early work focused on jointly learning CLWEs in two languages, relying on a strong cross-lingual supervision in the form of parallel corpora (Luong et al., 2015; Gouws et al., 2015) or bilingual dictionaries (Gouws and Søgaard, 2015; Duong et al., 2016). However, these approaches were later superseded by offline mapping methods, which separately train word embeddings in different languages and align them in an unsupervised manner through self-learning (Artetxe et al., 2018; Hoshen and Wolf, 2018) or adversarial training (Zhang et al., 2017; Conneau et al., 2018a). Despite the advantage of not requiring any parallel resources, mapping methods critically rely on the underlying embeddings having a similar structure, which is known as the isometry assumption. Several authors have observed that this assumption does not generally hold, severely hindering the performance of these methods (Søgaard et al., 2018; Nakashole and Flauger, 2018; Patra et al., 2019). In later work, Ormazabal et al. (2019) showed that this issue arises from trying to align separately trained e"
2021.acl-long.506,P19-1494,1,0.850814,"nitial dictionary has a negligible impact in the performance of our proposed method, which supports the idea that our approach converges to a similar solution given any reasonable initialization. We report our XNLI results in Table 5. We observe that our method is competitive with the baseline 5.3 Ablation study 12 In particular, most mapping methods use the official Wikipedia embeddings from fastText. Unfortunately, the preprocessed corpus used to train these embeddings is not public, so works that explore other approaches, like ours, need to use their own pre-processed copy of Wikipedia. 13 Artetxe et al. (2019) report even stronger results based on unsupervised machine translation instead of direct retrieval with CLWEs. Note, however, that their method still relies on cross-lingual embeddings to build the underlying phrase-table, so our improvements should be largely orthogonal to theirs. So as to understand the role of self-learning and the iterative restarts in our approach, we perform an ablation study and report our results in Table 6. We observe that the contribution of these components is greatly dependant on the initial dictionary. For the numeral initialization, the basic method works poorly"
2021.acl-long.506,2020.acl-main.421,1,0.777019,"pervision, ranging from bilingual dictionaries (Gouws and Søgaard, 2015; Duong et al., 2016) to parallel or documentaligned corpora (Luong et al., 2015; Gouws et al., 2015; Vuli´c and Moens, 2016). More recently, Lample et al. (2018) reported positive results learning regular word embeddings over concatenated monolingual corpora in different languages, relying on identical words as anchor points. Wang et al. (2019) further improved this approach by applying a conventional mapping method afterwards. As shown later in our experiments, our approach outperforms theirs by a large margin. Freezing. Artetxe et al. (2020) showed that it is possible to transfer an English transformer to a new language by freezing all the inner parameters of the network and learning a new set of embeddings for the new language through masked language modeling. This works because the frozen transformer parameters constrain the resulting representations to be aligned with English. Similarly, our proposed approach uses frozen output vectors in the target language as anchor points to learn aligned embeddings in the source language. 3 Proposed method Let xi and x ˜i be the input and output vectors of the ith word in the source langua"
2021.acl-long.506,Q17-1010,0,0.0500202,"fer learning on XNLI. 6479 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6479–6489 August 1–6, 2021. ©2021 Association for Computational Linguistics 2 Related work Word embeddings. Embedding methods learn static word representations based on co-occurrence statistics from a corpus. Most approaches use two different matrices to represent the words and the contexts, which are known as the input and output vectors, respectively (Mikolov et al., 2013; Pennington et al., 2014; Bojanowski et al., 2017). The output vectors play an auxiliary role, being discarded after training. Our method takes advantage of this fact, leveraging translated output vectors as anchor points to learn cross-lingual embeddings. To that end, we build on the Skip-Gram with Negative Sampling (SGNS) algorithm (Mikolov et al., 2013), which trains a binary classifier to distinguish whether each output word co-occurs with the given input word in the training corpus or was instead sampled from a noise distribution. Mapping CLWE methods. Offline mapping methods separately train word embeddings for each language, and then l"
2021.acl-long.506,D18-1269,0,0.300463,"ords in different languages are close to each other. Early work focused on jointly learning CLWEs in two languages, relying on a strong cross-lingual supervision in the form of parallel corpora (Luong et al., 2015; Gouws et al., 2015) or bilingual dictionaries (Gouws and Søgaard, 2015; Duong et al., 2016). However, these approaches were later superseded by offline mapping methods, which separately train word embeddings in different languages and align them in an unsupervised manner through self-learning (Artetxe et al., 2018; Hoshen and Wolf, 2018) or adversarial training (Zhang et al., 2017; Conneau et al., 2018a). Despite the advantage of not requiring any parallel resources, mapping methods critically rely on the underlying embeddings having a similar structure, which is known as the isometry assumption. Several authors have observed that this assumption does not generally hold, severely hindering the performance of these methods (Søgaard et al., 2018; Nakashole and Flauger, 2018; Patra et al., 2019). In later work, Ormazabal et al. (2019) showed that this issue arises from trying to align separately trained embeddings, as joint learning methods are not susceptible to it. In this paper, we propose"
2021.acl-long.506,D16-1136,0,0.0852078,"initial dictionary. Our approach outperforms conventional mapping methods on bilingual lexicon induction, and obtains competitive results in the downstream XNLI task. 1 Introduction Cross-lingual word embeddings (CLWEs) represent words from two or more languages in a shared space, so that semantically similar words in different languages are close to each other. Early work focused on jointly learning CLWEs in two languages, relying on a strong cross-lingual supervision in the form of parallel corpora (Luong et al., 2015; Gouws et al., 2015) or bilingual dictionaries (Gouws and Søgaard, 2015; Duong et al., 2016). However, these approaches were later superseded by offline mapping methods, which separately train word embeddings in different languages and align them in an unsupervised manner through self-learning (Artetxe et al., 2018; Hoshen and Wolf, 2018) or adversarial training (Zhang et al., 2017; Conneau et al., 2018a). Despite the advantage of not requiring any parallel resources, mapping methods critically rely on the underlying embeddings having a similar structure, which is known as the isometry assumption. Several authors have observed that this assumption does not generally hold, severely hi"
2021.acl-long.506,P19-1070,0,0.0240521,"Missing"
2021.acl-long.506,2020.acl-main.675,0,0.0486004,"Missing"
2021.acl-long.506,N15-1157,0,0.0889411,"uce the dependency on the initial dictionary. Our approach outperforms conventional mapping methods on bilingual lexicon induction, and obtains competitive results in the downstream XNLI task. 1 Introduction Cross-lingual word embeddings (CLWEs) represent words from two or more languages in a shared space, so that semantically similar words in different languages are close to each other. Early work focused on jointly learning CLWEs in two languages, relying on a strong cross-lingual supervision in the form of parallel corpora (Luong et al., 2015; Gouws et al., 2015) or bilingual dictionaries (Gouws and Søgaard, 2015; Duong et al., 2016). However, these approaches were later superseded by offline mapping methods, which separately train word embeddings in different languages and align them in an unsupervised manner through self-learning (Artetxe et al., 2018; Hoshen and Wolf, 2018) or adversarial training (Zhang et al., 2017; Conneau et al., 2018a). Despite the advantage of not requiring any parallel resources, mapping methods critically rely on the underlying embeddings having a similar structure, which is known as the isometry assumption. Several authors have observed that this assumption does not genera"
2021.acl-long.506,D18-1043,0,0.290726,"or more languages in a shared space, so that semantically similar words in different languages are close to each other. Early work focused on jointly learning CLWEs in two languages, relying on a strong cross-lingual supervision in the form of parallel corpora (Luong et al., 2015; Gouws et al., 2015) or bilingual dictionaries (Gouws and Søgaard, 2015; Duong et al., 2016). However, these approaches were later superseded by offline mapping methods, which separately train word embeddings in different languages and align them in an unsupervised manner through self-learning (Artetxe et al., 2018; Hoshen and Wolf, 2018) or adversarial training (Zhang et al., 2017; Conneau et al., 2018a). Despite the advantage of not requiring any parallel resources, mapping methods critically rely on the underlying embeddings having a similar structure, which is known as the isometry assumption. Several authors have observed that this assumption does not generally hold, severely hindering the performance of these methods (Søgaard et al., 2018; Nakashole and Flauger, 2018; Patra et al., 2019). In later work, Ormazabal et al. (2019) showed that this issue arises from trying to align separately trained embeddings, as joint lear"
2021.acl-long.506,D19-1328,0,0.0119385,"ping systems. We complement these results with additional experiments on a downstream task, where our method obtains competitive results, as well as an ablation study and a systematic error analysis. We identify a striking tendency of our method to translate words identically, even if it has no notion of the words being identically spelled. Thanks to this, our method is particularly strong at translating named entities, but we show that our improvements are not limited to this phenomenon. These insights confirm the value of accompanying quantitative results on BLI with qualitative evaluation (Kementchedjhieva et al., 2019) and/or other tasks (Glavaˇs et al., 2019). 6486 In the future, we would like to further explore CLWE methods that go beyond the currently dominant mapping paradigm. In particular, we would like to remove the requirement of a seed dictionary altogether by using adversarial learning, and explore more elaborated context translation and dictionary re-induction schemes. Acknowledgments Aitor Ormazabal, Aitor Soroa, Gorka Labaka and Eneko Agirre were supported by the Basque Government (excellence research group IT1343-19 and DeepText project KK-2020/00088), project BigKnowledge (Ayudas Fundaci´on B"
2021.acl-long.506,J82-2005,0,0.601739,"Missing"
2021.acl-long.506,2020.coling-main.526,0,0.044508,"Missing"
2021.acl-long.506,W15-1521,0,0.164949,"oints, and incorporates self-learning and iterative restarts to reduce the dependency on the initial dictionary. Our approach outperforms conventional mapping methods on bilingual lexicon induction, and obtains competitive results in the downstream XNLI task. 1 Introduction Cross-lingual word embeddings (CLWEs) represent words from two or more languages in a shared space, so that semantically similar words in different languages are close to each other. Early work focused on jointly learning CLWEs in two languages, relying on a strong cross-lingual supervision in the form of parallel corpora (Luong et al., 2015; Gouws et al., 2015) or bilingual dictionaries (Gouws and Søgaard, 2015; Duong et al., 2016). However, these approaches were later superseded by offline mapping methods, which separately train word embeddings in different languages and align them in an unsupervised manner through self-learning (Artetxe et al., 2018; Hoshen and Wolf, 2018) or adversarial training (Zhang et al., 2017; Conneau et al., 2018a). Despite the advantage of not requiring any parallel resources, mapping methods critically rely on the underlying embeddings having a similar structure, which is known as the isometry assump"
2021.acl-long.506,2020.emnlp-main.215,0,0.0175638,"ds separately train word embeddings for each language, and then learn a mapping to align them into a shared space. Most of these methods align the embeddings through a linear map—often enforcing orthogonality constraints—and, as such, they rely on the assumption that the geometric structure of the separately learned embeddings is similar. This assumption has been shown to fail under unfavorable conditions, severely hindering the performance of these methods (Søgaard et al., 2018; Vuli´c et al., 2020). Existing attempts to mitigate this issue include learning non-linear maps in a latent space (Mohiuddin et al., 2020), employing maps that are only locally linear (Nakashole, 2018), or learning a separate map for each word (Glavaˇs and Vuli´c, 2020). However, all these methods are supervised, and have the same fundamental limitation of aligning a set of separately trained embeddings (Ormazabal et al., 2019). Self-learning. While early mapping methods relied on a bilingual dictionary to learn the alignment, this requirement was alleviated thanks to selflearning, which iteratively re-induces the dictionary during training. This enabled learning CLWEs in a semi-supervised fashion starting from a weak initial di"
2021.acl-long.506,D18-1063,0,0.0500434,"Missing"
2021.acl-long.506,D18-1047,0,0.0161733,"a mapping to align them into a shared space. Most of these methods align the embeddings through a linear map—often enforcing orthogonality constraints—and, as such, they rely on the assumption that the geometric structure of the separately learned embeddings is similar. This assumption has been shown to fail under unfavorable conditions, severely hindering the performance of these methods (Søgaard et al., 2018; Vuli´c et al., 2020). Existing attempts to mitigate this issue include learning non-linear maps in a latent space (Mohiuddin et al., 2020), employing maps that are only locally linear (Nakashole, 2018), or learning a separate map for each word (Glavaˇs and Vuli´c, 2020). However, all these methods are supervised, and have the same fundamental limitation of aligning a set of separately trained embeddings (Ormazabal et al., 2019). Self-learning. While early mapping methods relied on a bilingual dictionary to learn the alignment, this requirement was alleviated thanks to selflearning, which iteratively re-induces the dictionary during training. This enabled learning CLWEs in a semi-supervised fashion starting from a weak initial dictionary (Artetxe et al., 2017), or in a completely unsupervise"
2021.acl-long.506,P18-2036,0,0.0134589,"ping methods, which separately train word embeddings in different languages and align them in an unsupervised manner through self-learning (Artetxe et al., 2018; Hoshen and Wolf, 2018) or adversarial training (Zhang et al., 2017; Conneau et al., 2018a). Despite the advantage of not requiring any parallel resources, mapping methods critically rely on the underlying embeddings having a similar structure, which is known as the isometry assumption. Several authors have observed that this assumption does not generally hold, severely hindering the performance of these methods (Søgaard et al., 2018; Nakashole and Flauger, 2018; Patra et al., 2019). In later work, Ormazabal et al. (2019) showed that this issue arises from trying to align separately trained embeddings, as joint learning methods are not susceptible to it. In this paper, we propose an alternative approach that does not have this limitation, but can still work without any parallel resources. The core idea of our method is to fix the target language embeddings, and learn aligned embeddings for the source language from scratch. This prevents structural mismatches that result from independently training embeddings in different languages, as the learning of"
2021.acl-long.506,P19-1492,1,0.897614,"languages and align them in an unsupervised manner through self-learning (Artetxe et al., 2018; Hoshen and Wolf, 2018) or adversarial training (Zhang et al., 2017; Conneau et al., 2018a). Despite the advantage of not requiring any parallel resources, mapping methods critically rely on the underlying embeddings having a similar structure, which is known as the isometry assumption. Several authors have observed that this assumption does not generally hold, severely hindering the performance of these methods (Søgaard et al., 2018; Nakashole and Flauger, 2018; Patra et al., 2019). In later work, Ormazabal et al. (2019) showed that this issue arises from trying to align separately trained embeddings, as joint learning methods are not susceptible to it. In this paper, we propose an alternative approach that does not have this limitation, but can still work without any parallel resources. The core idea of our method is to fix the target language embeddings, and learn aligned embeddings for the source language from scratch. This prevents structural mismatches that result from independently training embeddings in different languages, as the learning of the source embeddings is tailored to each particular set of"
2021.acl-long.506,D14-1162,0,0.0995886,"o-shot crosslingual transfer learning on XNLI. 6479 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6479–6489 August 1–6, 2021. ©2021 Association for Computational Linguistics 2 Related work Word embeddings. Embedding methods learn static word representations based on co-occurrence statistics from a corpus. Most approaches use two different matrices to represent the words and the contexts, which are known as the input and output vectors, respectively (Mikolov et al., 2013; Pennington et al., 2014; Bojanowski et al., 2017). The output vectors play an auxiliary role, being discarded after training. Our method takes advantage of this fact, leveraging translated output vectors as anchor points to learn cross-lingual embeddings. To that end, we build on the Skip-Gram with Negative Sampling (SGNS) algorithm (Mikolov et al., 2013), which trains a binary classifier to distinguish whether each output word co-occurs with the given input word in the training corpus or was instead sampled from a noise distribution. Mapping CLWE methods. Offline mapping methods separately train word embeddings for"
2021.acl-long.506,P18-1072,0,0.0395946,"Missing"
2021.acl-long.506,2020.emnlp-main.257,0,0.0204108,"Missing"
2021.acl-long.506,N18-1101,0,0.0117864,"ction (BLI) and Cross-lingual Natural Language Inference (XNLI). BLI. Following common practice, we induce a bilingual dictionary through CSLS retrieval (Conneau et al., 2018a) for each set of cross-lingual embeddings, and evaluate the precision at 1 (P@1) with respect to the gold standard test dictionary from the MUSE dataset (Conneau et al., 2018a). For the few out-of-vocabulary source words, we revert to copying as a back-off strategy,9 so our reported numbers are directly comparable to prior work in terms of coverage. XNLI. We train an English natural language inference model on MultiNLI (Williams et al., 2018), and evaluate the zero-shot cross-lingual transfer performance on the XNLI test set (Conneau et al., 2018b) for the subset of our languages covered by it. To that end, we follow Glavaˇs et al. (2019) and train an Enhanced Sequential Inference Model (ESIM) on top of our original English embeddings, which are kept frozen during training. At test time, we transfer into the rest of the languages by plugging in the corresponding aligned embeddings. Note that we use the exact same English model for our proposed method and the baseline MUSE and ICP systems,10 which only differ in the set of aligned"
2021.acl-long.506,D18-1268,0,0.0349071,"Missing"
2021.acl-long.506,P17-1179,0,0.0252406,"mantically similar words in different languages are close to each other. Early work focused on jointly learning CLWEs in two languages, relying on a strong cross-lingual supervision in the form of parallel corpora (Luong et al., 2015; Gouws et al., 2015) or bilingual dictionaries (Gouws and Søgaard, 2015; Duong et al., 2016). However, these approaches were later superseded by offline mapping methods, which separately train word embeddings in different languages and align them in an unsupervised manner through self-learning (Artetxe et al., 2018; Hoshen and Wolf, 2018) or adversarial training (Zhang et al., 2017; Conneau et al., 2018a). Despite the advantage of not requiring any parallel resources, mapping methods critically rely on the underlying embeddings having a similar structure, which is known as the isometry assumption. Several authors have observed that this assumption does not generally hold, severely hindering the performance of these methods (Søgaard et al., 2018; Nakashole and Flauger, 2018; Patra et al., 2019). In later work, Ormazabal et al. (2019) showed that this issue arises from trying to align separately trained embeddings, as joint learning methods are not susceptible to it. In t"
2021.acl-long.506,N19-1161,0,0.0387267,"Missing"
2021.emnlp-main.92,2020.acl-main.142,0,0.0194596,"edict the entire masked span. LUKE (Yamada et al., 2020) further pretrains a LM predicting entities from Wikipedia, and using entity information as an additional input embedding layer. K-Adapter (Wang et al., 2020) fixes the parameters of the pretrained LM and use Adapters to infuse factual and linguistic knowledge from Wikipedia and dependency parsing. TACRED (Zhang et al., 2017) is the largest and most widely used dataset for RE in English. It is derived from the TAC-KBP relation set, with labels obtained via crowdsourcing. Although alternate versions of TACRED have been published recently (Alt et al., 2020; Stoica et al., 2021), the state of the art is mainly tested in the original version. tual response to a given prompt. The manual generation of effective prompts is costly and requires domain expertise. Gao et al. (2020) provide an effective way to generate prompts for text classification tasks that surpasses the performance of hand picked ones. The approach uses few-shot training with a generative T5 model (Raffel et al., 2020) to learn to decode effective prompts. Similarly, Liu et al. (2021) automatically search prompts in a embedding space which can be simultaneously finetuned along with"
2021.emnlp-main.92,D15-1075,0,0.045924,"pirical Methods in Natural Language Processing, pages 1199–1212 c November 7–11, 2021. 2021 Association for Computational Linguistics tailment engine for few-shot RE. The results on the widely used TACRED (Zhang et al., 2017) RE dataset in zero- and few-shot scenarios are excellent, well over state-of-the-art systems using the same amount of data. In addition our method scales well with large pre-trained LMs and large amounts of training data, reporting the best results on TACRED to date. 2 Related Work Textual Entailment. It was first presented by Dagan et al. (2006) and further developed by Bowman et al. (2015) who called it Natural Language Inference (NLI). Given a textual premise and hypothesis, the task is to decide whether the premise entails or contradicts (or is neutral to) the hypothesis. The current state-of-the-art uses large pre-trained LM fine-tuned in NLI datasets (Lan et al., 2020; Liu et al., 2019; Conneau et al., 2020; Lewis et al., 2020; He et al., 2021). Relation Extraction. The best results to date on RE are obtained by fine-tuning large pre-trained language models equipped with a classification head. Joshi et al. (2020) pretrains a masked language model on random contiguous spans"
2021.emnlp-main.92,2021.naacl-main.272,0,0.0241625,"inference and the issue of detecting no-relation. Zero-Shot and Few-Shot learning. Brown et al. Partially vs. fullly unseen labels in RE. Exist(2020) showed that task descriptions (prompts) can ing zero/few-shot RE models usually see some labe fed into LMs for task-agnostic and few-shot per- bels during training (label partially unseen), which formance. In addition, (Schick and Schütze, 2020; helps generalize to the unseen label (Levy et al., Schick and Schütze, 2021; Tam et al., 2021) extend 2017; Obamuyide and Vlachos, 2018; Han et al., the method and allow finetuning of LMs on a va- 2018; Chen and Li, 2021). These approaches do riety of tasks. Prompt-based prediction treats the not fully address the data scarcity problem. In this downstream task as a (masked) language modeling work we address the more challenging label fully problem, where the model directly generates a tex- unseen scenario. 1200 Figure 1: General workflow of our entailment-based RE approach. 3 Entailment for RE In this section we describe our models for zeroand few-shot RE. 3.1 Zero-shot relation extraction We reformulate RE as an entailment task: given the input text containing the two entity mentions as the premise and the ve"
2021.emnlp-main.92,2020.acl-main.747,0,0.0279349,"same amount of data. In addition our method scales well with large pre-trained LMs and large amounts of training data, reporting the best results on TACRED to date. 2 Related Work Textual Entailment. It was first presented by Dagan et al. (2006) and further developed by Bowman et al. (2015) who called it Natural Language Inference (NLI). Given a textual premise and hypothesis, the task is to decide whether the premise entails or contradicts (or is neutral to) the hypothesis. The current state-of-the-art uses large pre-trained LM fine-tuned in NLI datasets (Lan et al., 2020; Liu et al., 2019; Conneau et al., 2020; Lewis et al., 2020; He et al., 2021). Relation Extraction. The best results to date on RE are obtained by fine-tuning large pre-trained language models equipped with a classification head. Joshi et al. (2020) pretrains a masked language model on random contiguous spans to learn span-boundaries and predict the entire masked span. LUKE (Yamada et al., 2020) further pretrains a LM predicting entities from Wikipedia, and using entity information as an additional input embedding layer. K-Adapter (Wang et al., 2020) fixes the parameters of the pretrained LM and use Adapters to infuse factual and l"
2021.emnlp-main.92,2020.emnlp-main.49,0,0.0187224,"e pre-trained language model. Note that previous prompt-based models run their zero-shot models on a semi-supervised setting in which some amount of labeled data is given in training. Prompts can be easily generated for text classification. Other tasks require more elaborate templates (Goswami et al., 2020; Li et al., 2021) and currently no effective prompt-based methods for RE exist. Besides prompt-based methods, the use of pivot tasks has been widely use for few/zero-shot learning. For instance, relation and event extraction have been cast as a question answering problem (Levy et al., 2017; Du and Cardie, 2020), associating each slot label to at least one natural language question. Closer to our work, NLI has been shown too to be a successful pivoting task for text classification (Yin et al., 2019, 2020; Wang et al., 2021; Sainz and Rigau, 2021). These works verbalize the labels, and apply an entailment engine to check whether the input text entails the label description. In similar work to ours, the relation between entailment and RE was explored by Obamuyide and Vlachos (2018). In their work they present some preliminary experiments where they cast RE as entailment, but only evaluate performance a"
2021.emnlp-main.92,D18-1514,0,0.0328217,"Missing"
2021.emnlp-main.92,2020.tacl-1.5,0,0.0206288,"st presented by Dagan et al. (2006) and further developed by Bowman et al. (2015) who called it Natural Language Inference (NLI). Given a textual premise and hypothesis, the task is to decide whether the premise entails or contradicts (or is neutral to) the hypothesis. The current state-of-the-art uses large pre-trained LM fine-tuned in NLI datasets (Lan et al., 2020; Liu et al., 2019; Conneau et al., 2020; Lewis et al., 2020; He et al., 2021). Relation Extraction. The best results to date on RE are obtained by fine-tuning large pre-trained language models equipped with a classification head. Joshi et al. (2020) pretrains a masked language model on random contiguous spans to learn span-boundaries and predict the entire masked span. LUKE (Yamada et al., 2020) further pretrains a LM predicting entities from Wikipedia, and using entity information as an additional input embedding layer. K-Adapter (Wang et al., 2020) fixes the parameters of the pretrained LM and use Adapters to infuse factual and linguistic knowledge from Wikipedia and dependency parsing. TACRED (Zhang et al., 2017) is the largest and most widely used dataset for RE in English. It is derived from the TAC-KBP relation set, with labels obt"
2021.emnlp-main.92,K17-1034,0,0.131227,"alizations (Puri and Catanzaro, 2019; Schick and Schütze, 2021; Schick and Schütze, 2020) as an alternative to standard fine-tuning (Gao et al., 2020; Scao and Rush, 2021). In these methods, the prompts are input to the LM together with the example, and the language modelling objective is used in learning and inference. In a different direction, some authors reformulate the target task (e.g. document classification) as a pivot task (typically question answering or textual entailment), which allows the use of readily available question answering (or entailment) training data (Yin et al., 2019; Levy et al., 2017). In all cases, the underlying idea is to cast the target task into a formulation which allows us to exploit the knowledge implicit in pre-trained LM (prompt-based) or general-purpose question answering or entailment engines (pivot tasks). Prompt-based approaches are very effective when the label verbalization is given by one or two words (e.g. text classification), as they can be easily predicted by language models, but strive in cases where the label requires a more elaborate description, as in RE. We thus propose to reformulate RE as an entailment problem, where the verbalizations of the re"
2021.emnlp-main.92,2020.acl-main.703,0,0.0143512,"In addition our method scales well with large pre-trained LMs and large amounts of training data, reporting the best results on TACRED to date. 2 Related Work Textual Entailment. It was first presented by Dagan et al. (2006) and further developed by Bowman et al. (2015) who called it Natural Language Inference (NLI). Given a textual premise and hypothesis, the task is to decide whether the premise entails or contradicts (or is neutral to) the hypothesis. The current state-of-the-art uses large pre-trained LM fine-tuned in NLI datasets (Lan et al., 2020; Liu et al., 2019; Conneau et al., 2020; Lewis et al., 2020; He et al., 2021). Relation Extraction. The best results to date on RE are obtained by fine-tuning large pre-trained language models equipped with a classification head. Joshi et al. (2020) pretrains a masked language model on random contiguous spans to learn span-boundaries and predict the entire masked span. LUKE (Yamada et al., 2020) further pretrains a LM predicting entities from Wikipedia, and using entity information as an additional input embedding layer. K-Adapter (Wang et al., 2020) fixes the parameters of the pretrained LM and use Adapters to infuse factual and linguistic knowledge"
2021.emnlp-main.92,2021.naacl-main.69,0,0.0117238,"the performance of hand picked ones. The approach uses few-shot training with a generative T5 model (Raffel et al., 2020) to learn to decode effective prompts. Similarly, Liu et al. (2021) automatically search prompts in a embedding space which can be simultaneously finetuned along with the pre-trained language model. Note that previous prompt-based models run their zero-shot models on a semi-supervised setting in which some amount of labeled data is given in training. Prompts can be easily generated for text classification. Other tasks require more elaborate templates (Goswami et al., 2020; Li et al., 2021) and currently no effective prompt-based methods for RE exist. Besides prompt-based methods, the use of pivot tasks has been widely use for few/zero-shot learning. For instance, relation and event extraction have been cast as a question answering problem (Levy et al., 2017; Du and Cardie, 2020), associating each slot label to at least one natural language question. Closer to our work, NLI has been shown too to be a successful pivoting task for text classification (Yin et al., 2019, 2020; Wang et al., 2021; Sainz and Rigau, 2021). These works verbalize the labels, and apply an entailment engine"
2021.emnlp-main.92,2021.ccl-1.108,0,0.0360333,"Missing"
2021.emnlp-main.92,W18-5511,0,0.0162897,"zero-shot learning. For instance, relation and event extraction have been cast as a question answering problem (Levy et al., 2017; Du and Cardie, 2020), associating each slot label to at least one natural language question. Closer to our work, NLI has been shown too to be a successful pivoting task for text classification (Yin et al., 2019, 2020; Wang et al., 2021; Sainz and Rigau, 2021). These works verbalize the labels, and apply an entailment engine to check whether the input text entails the label description. In similar work to ours, the relation between entailment and RE was explored by Obamuyide and Vlachos (2018). In their work they present some preliminary experiments where they cast RE as entailment, but only evaluate performance as binary entailment, not as a RE task. As a consequence they do not have competing positive labels and avoid RE inference and the issue of detecting no-relation. Zero-Shot and Few-Shot learning. Brown et al. Partially vs. fullly unseen labels in RE. Exist(2020) showed that task descriptions (prompts) can ing zero/few-shot RE models usually see some labe fed into LMs for task-agnostic and few-shot per- bels during training (label partially unseen), which formance. In additi"
2021.emnlp-main.92,2021.naacl-main.208,0,0.0675091,"Missing"
2021.emnlp-main.92,2021.eacl-main.20,0,0.510719,"upervised system on the same conditions), and only 4 points short of the state-of-the-art (which uses 20 times more training data). We also show that the performance can be improved significantly with larger entailment models, up to 12 points in zero-shot, giving the best results to date on TACRED when fully trained. The analysis shows that our few-shot systems are especially effective when discriminating between relations, and that the performance difference in low data regimes comes mainly from identifying no-relation cases. 1 Introduction and label verbalizations (Puri and Catanzaro, 2019; Schick and Schütze, 2021; Schick and Schütze, 2020) as an alternative to standard fine-tuning (Gao et al., 2020; Scao and Rush, 2021). In these methods, the prompts are input to the LM together with the example, and the language modelling objective is used in learning and inference. In a different direction, some authors reformulate the target task (e.g. document classification) as a pivot task (typically question answering or textual entailment), which allows the use of readily available question answering (or entailment) training data (Yin et al., 2019; Levy et al., 2017). In all cases, the underlying idea is to ca"
2021.emnlp-main.92,2021.emnlp-main.407,0,0.0425574,"e performance as binary entailment, not as a RE task. As a consequence they do not have competing positive labels and avoid RE inference and the issue of detecting no-relation. Zero-Shot and Few-Shot learning. Brown et al. Partially vs. fullly unseen labels in RE. Exist(2020) showed that task descriptions (prompts) can ing zero/few-shot RE models usually see some labe fed into LMs for task-agnostic and few-shot per- bels during training (label partially unseen), which formance. In addition, (Schick and Schütze, 2020; helps generalize to the unseen label (Levy et al., Schick and Schütze, 2021; Tam et al., 2021) extend 2017; Obamuyide and Vlachos, 2018; Han et al., the method and allow finetuning of LMs on a va- 2018; Chen and Li, 2021). These approaches do riety of tasks. Prompt-based prediction treats the not fully address the data scarcity problem. In this downstream task as a (masked) language modeling work we address the more challenging label fully problem, where the model directly generates a tex- unseen scenario. 1200 Figure 1: General workflow of our entailment-based RE approach. 3 Entailment for RE In this section we describe our models for zeroand few-shot RE. 3.1 Zero-shot relation extrac"
2021.emnlp-main.92,2020.emnlp-main.523,0,0.217584,"onstraints. In order to ensure that the manual work involved is limited and practical in real-world applications, we allowed at most 15 minutes of manual labor per relation. The verbalizations are used as-is for zero-shot RE, but we also recast labelled RE examples as entailment pairs and fine-tune the enGiven a context where two entities appear, the Relation Extraction (RE) task aims to predict the semantic relation (if any) holding between the two entities. Methods that fine-tune large pretrained language models (LM) with large amounts of labelled data have established the state of the art (Yamada et al., 2020). Nevertheless, due to differing languages, domains and the cost of human annotation, there is typically a very small number of labelled examples in real-world applications, and such models perform poorly (Schick and Schütze, 2021). As an alternative, methods that only need a few examples (few-shot) or no examples (zero-shot) 1 have emerged. For instance, prompt based learning Code and splits available at: https://github.com/ proposes hand-made or automatically learned task osainz59/Ask2Transformers 1199 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pa"
2021.emnlp-main.92,D19-1404,0,0.0863277,"ion and label verbalizations (Puri and Catanzaro, 2019; Schick and Schütze, 2021; Schick and Schütze, 2020) as an alternative to standard fine-tuning (Gao et al., 2020; Scao and Rush, 2021). In these methods, the prompts are input to the LM together with the example, and the language modelling objective is used in learning and inference. In a different direction, some authors reformulate the target task (e.g. document classification) as a pivot task (typically question answering or textual entailment), which allows the use of readily available question answering (or entailment) training data (Yin et al., 2019; Levy et al., 2017). In all cases, the underlying idea is to cast the target task into a formulation which allows us to exploit the knowledge implicit in pre-trained LM (prompt-based) or general-purpose question answering or entailment engines (pivot tasks). Prompt-based approaches are very effective when the label verbalization is given by one or two words (e.g. text classification), as they can be easily predicted by language models, but strive in cases where the label requires a more elaborate description, as in RE. We thus propose to reformulate RE as an entailment problem, where the verb"
2021.emnlp-main.92,2020.emnlp-main.660,0,0.0356535,"Missing"
2021.emnlp-main.92,D17-1004,0,0.342665,"rld applications, and such models perform poorly (Schick and Schütze, 2021). As an alternative, methods that only need a few examples (few-shot) or no examples (zero-shot) 1 have emerged. For instance, prompt based learning Code and splits available at: https://github.com/ proposes hand-made or automatically learned task osainz59/Ask2Transformers 1199 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1199–1212 c November 7–11, 2021. 2021 Association for Computational Linguistics tailment engine for few-shot RE. The results on the widely used TACRED (Zhang et al., 2017) RE dataset in zero- and few-shot scenarios are excellent, well over state-of-the-art systems using the same amount of data. In addition our method scales well with large pre-trained LMs and large amounts of training data, reporting the best results on TACRED to date. 2 Related Work Textual Entailment. It was first presented by Dagan et al. (2006) and further developed by Bowman et al. (2015) who called it Natural Language Inference (NLI). Given a textual premise and hypothesis, the task is to decide whether the premise entails or contradicts (or is neutral to) the hypothesis. The current stat"
2021.emnlp-main.92,N18-1101,0,0.0810662,"Missing"
agirre-de-lacalle-2004-publicly,mihalcea-2002-bootstrapping,0,\N,Missing
agirre-de-lacalle-2004-publicly,J98-1006,0,\N,Missing
agirre-de-lacalle-2004-publicly,martinez-agirre-2004-effect,1,\N,Missing
agirre-de-lacalle-2004-publicly,C00-1072,0,\N,Missing
agirre-etal-2004-exploring,atserias-etal-2004-cross,1,\N,Missing
agirre-etal-2004-exploring,W02-1304,1,\N,Missing
agirre-etal-2004-exploring,briscoe-carroll-2002-robust,0,\N,Missing
agirre-etal-2004-exploring,P02-1050,0,\N,Missing
agirre-etal-2006-methodology,agirre-etal-2006-preliminary,1,\N,Missing
agirre-etal-2006-methodology,J96-2004,0,\N,Missing
agirre-etal-2006-preliminary,kingsbury-palmer-2002-treebank,0,\N,Missing
agirre-etal-2006-preliminary,W03-1707,0,\N,Missing
agirre-etal-2006-preliminary,A92-1016,0,\N,Missing
agirre-etal-2006-preliminary,P98-1013,0,\N,Missing
agirre-etal-2006-preliminary,C98-1013,0,\N,Missing
agirre-etal-2006-preliminary,agirre-etal-2006-methodology,1,\N,Missing
agirre-etal-2010-exploring,agirre-soroa-2008-using,1,\N,Missing
agirre-etal-2010-exploring,agirre-de-lacalle-2004-publicly,1,\N,Missing
agirre-etal-2010-exploring,E09-1005,1,\N,Missing
agirre-etal-2010-exploring,H93-1061,0,\N,Missing
agirre-etal-2010-exploring,R09-1039,1,\N,Missing
agirre-etal-2010-exploring,J06-1003,0,\N,Missing
agirre-etal-2010-exploring,D07-1061,0,\N,Missing
agirre-etal-2010-exploring,N09-1003,1,\N,Missing
agirre-etal-2010-exploring,P06-1127,0,\N,Missing
agirre-etal-2012-matching,D11-1072,0,\N,Missing
agirre-etal-2012-matching,E06-1002,0,\N,Missing
agirre-etal-2012-matching,D11-1074,0,\N,Missing
agirre-etal-2012-matching,P11-1115,0,\N,Missing
agirre-etal-2012-matching,P11-1095,0,\N,Missing
agirre-etal-2012-matching,D07-1074,0,\N,Missing
agirre-soroa-2008-using,W04-0811,0,\N,Missing
agirre-soroa-2008-using,H05-1052,0,\N,Missing
agirre-soroa-2008-using,P00-1064,0,\N,Missing
agirre-soroa-2008-using,H93-1061,0,\N,Missing
agirre-soroa-2008-using,P04-1036,0,\N,Missing
atserias-etal-2004-cross,habash-dorr-2002-handling,0,\N,Missing
atserias-etal-2004-cross,briscoe-carroll-2002-robust,1,\N,Missing
atserias-etal-2004-cross,H92-1045,0,\N,Missing
atserias-etal-2004-cross,magnini-cavaglia-2000-integrating,1,\N,Missing
atserias-etal-2004-cross,agirre-etal-2004-exploring,1,\N,Missing
C02-1112,S01-1028,1,0.901898,"follows a two-step process: 1. Choosing the representation as a set of features for the context of occurrence of the target word senses. 2. Applying a Machine Learning (ML) algorithm to train on the extracted features and tag the target word in the test examples. Current WSD systems attain high performances for coarse word sense differences (two or three senses) if enough training material is available. In contrast, the performance for finer-grained sense differences (e.g. WordNet senses as used in Senseval 2 (Preiss & Yarowsky, 2001)) is far from application needs. Nevertheless, recent work (Agirre and Martinez, 2001a) shows that it is possible to exploit the precision-coverage trade-off and build a high precision WSD system Lluís Màrquez TALP Research Center Polytechnical University of Catalonia Barcelona, Spain lluism@lsi.upc.es that tags a limited number of target words with a predefined precision. This paper explores the contribution of a broad set of syntactically motivated features that ranges from the presence of complements and adjuncts, and the detection of subcategorization frames, up to grammatical relations instantiated with specific words. The performance of the syntactic features is measured"
C02-1112,A00-1031,0,0.0122473,"al features correspond to open-class lemmas that appear in windows of different sizes around the target word. In this experiment, we used two different window-sizes: 4 lemmas around the target (coded as win_lem_4w), and the lemmas in the sentence plus the 2 previous and 2 following sentences (win_lem_2s). Local features include bigrams and trigrams (coded as big_, trig_ respectively) that contain the target word. An index (+1, -1, 0) is used to indicate the position of the target in the bigram or trigram, which can be formed by part of speech, lemmas or word forms (wf, lem, pos). We used TnT (Brants, 2000) for PoS tagging. For instance, we could extract the following features for the target word known from the sample sentence below: word form “whole” occurring in a 2 sentence window (win_wf_2s), the bigram “known widely” where target is the last word (big_wf_+1) and the trigram “RB RB N” formed by the two PoS before the target word (trig_pos_+1). “There is nothing in the whole range of human experience more widely known and universally …” 4. Set of Syntactic Features. In order to extract syntactic features from the tagged examples, we needed a parser that would meet the following requirements:"
C02-1112,W01-1808,0,0.0287421,"target word (trig_pos_+1). “There is nothing in the whole range of human experience more widely known and universally …” 4. Set of Syntactic Features. In order to extract syntactic features from the tagged examples, we needed a parser that would meet the following requirements: free for research, able to provide the whole structure with named syntactic relations (in contrast to shallow parsers), positively evaluated on wellestablished corpora, domain independent, and fast enough. Three parsers fulfilled all the requirements: Link Grammar (Sleator and Temperley, 1993), Minipar (Lin, 1993) and (Carroll & Briscoe, 2001). We installed the first two parsers, and performed a set of small experiments (John Carroll helped out running his own parser). Unfortunately, we did not have a comparative evaluation to help choosing the best. We performed a little comparative test, and all parsers looked similar. At this point we chose Minipar mainly because it was fast, easy to install and the output could be easily processed. The choice of the parser did not condition the design of the experiments (cf. section 7). From the output of the parser, we extracted different sets of features. First, we distinguish between direct"
C02-1112,P96-1025,0,0.0275141,"the pre-selection of significative words to look up in the context of the target word. Ng (1996) uses a basic set of features similar to those defined by Yarowsky, but they also use syntactic information: verb-object and subjectverb relations. The results obtained by the syntactic features are poor, and no analysis of the features or any reason for the low performance is given. Stetina et al. (1998) achieve good results with syntactic relations as features. They use a measure of semantic distance based on WordNet to find similar features. The features are extracted using a statistical parser (Collins, 1996), and consist of the head and modifiers of each phrase. Unfortunately, they do not provide a comparison with a baseline system that would only use basic features. The Senseval-2 workshop was held in Toulouse in July 2001 (Preiss & Yarowsky, 2001). Most of the supervised systems used only a basic set of local and topical features to train their ML systems. Regarding syntactic information, in the Japanese tasks, several groups relied on dependency trees to extract features that were used by different models (SVM, Bayes, or vector space models). For the English tasks, the team from the University"
C02-1112,J94-4003,0,0.0603174,"Missing"
C02-1112,P93-1016,0,0.0270377,"PoS before the target word (trig_pos_+1). “There is nothing in the whole range of human experience more widely known and universally …” 4. Set of Syntactic Features. In order to extract syntactic features from the tagged examples, we needed a parser that would meet the following requirements: free for research, able to provide the whole structure with named syntactic relations (in contrast to shallow parsers), positively evaluated on wellestablished corpora, domain independent, and fast enough. Three parsers fulfilled all the requirements: Link Grammar (Sleator and Temperley, 1993), Minipar (Lin, 1993) and (Carroll & Briscoe, 2001). We installed the first two parsers, and performed a set of small experiments (John Carroll helped out running his own parser). Unfortunately, we did not have a comparative evaluation to help choosing the best. We performed a little comparative test, and all parsers looked similar. At this point we chose Minipar mainly because it was fast, easy to install and the output could be easily processed. The choice of the parser did not condition the design of the experiments (cf. section 7). From the output of the parser, we extracted different sets of features. First,"
C02-1112,P96-1006,0,0.149392,"Missing"
C02-1112,W98-0701,0,0.126818,"-speech tags and special classes of words, such as “Weekday”. These features have been used by other approaches, with variations such as the size of the window, the distinction between open class/closed class words, or the pre-selection of significative words to look up in the context of the target word. Ng (1996) uses a basic set of features similar to those defined by Yarowsky, but they also use syntactic information: verb-object and subjectverb relations. The results obtained by the syntactic features are poor, and no analysis of the features or any reason for the low performance is given. Stetina et al. (1998) achieve good results with syntactic relations as features. They use a measure of semantic distance based on WordNet to find similar features. The features are extracted using a statistical parser (Collins, 1996), and consist of the head and modifiers of each phrase. Unfortunately, they do not provide a comparison with a baseline system that would only use basic features. The Senseval-2 workshop was held in Toulouse in July 2001 (Preiss & Yarowsky, 2001). Most of the supervised systems used only a basic set of local and topical features to train their ML systems. Regarding syntactic informatio"
C02-1112,P94-1013,0,0.288745,"n WSD system based on the precision-coverage trade-off is also investigated. The paper is structured as follows. Section 2 reviews the features previously used in the literature. Section 3 defines a basic feature set based on the preceding review. Section 4 presents the syntactic features as defined in our work, alongside the parser used. In section 5 the two ML algorithms are presented, as well as the strategies for the precision-coverage trade-off. Section 6 shows the experimental setting and the results. Finally section 7 draws the conclusions and summarizes further work. 2. Previous work. Yarowsky (1994) defined a basic set of features that has been widely used (with some variations) by other WSD systems. It consisted on words appearing in a window of ±k positions around the target and bigrams and trigrams constructed with the target word. He used words, lemmas, coarse part-of-speech tags and special classes of words, such as “Weekday”. These features have been used by other approaches, with variations such as the size of the window, the distinction between open class/closed class words, or the pre-selection of significative words to look up in the context of the target word. Ng (1996) uses a"
C02-1112,C02-1013,0,\N,Missing
C08-1003,W00-1322,0,0.366549,"Missing"
C08-1003,H05-1053,0,0.535881,"d experiments on just the target domain. We use unlabeled data in order to improve the results of a system trained and tested in the target domain. These results are complementary to the domain adaptation experiments, and also provide an upperbound for semi-supervised domain adaptation. We call these experiments the target domain scenario. Note that both scenarios are semi-supervised, in that our focus is on the use of unlabeled data in addition to the available labeled data. The experiments were performed on a publicly available corpus which was designed to study the effect of domain in WSD (Koeling et al., 2005). It comprises 41 nouns closely related to the S PORTS and F INANCES domains with 300 examples for each. The 300 examples were drawn from the British National Corpus (Leech, 1992) (B NC), the S PORTS section of the Reuters corpus (Leech, In this paper we explore robustness and domain adaptation issues for Word Sense Disambiguation (WSD) using Singular Value Decomposition (SVD) and unlabeled data. We focus on the semi-supervised domain adaptation scenario, where we train on the source corpus and test on the target corpus, and try to improve results using unlabeled data. Our method yields up to"
C08-1003,S07-1074,1,0.787771,"Missing"
C08-1003,W06-2911,0,0.0570176,"educe the space of the term-todocument matrix, and then computed the similarity between train and test instances using a mapping to the reduced space (similar to our SMA method in Section 4.2). They combined other knowledge sources into a complex kernel using SVM. They report improved performance on a number of languages in the Senseval-3 lexical sample dataset. Our present paper differs from theirs in that we propose an additional method to use SVD (the OMT method, Section 4.2), and that we evaluate the contribution of unlabeled data and SVD in isolation, leaving combination for future work. Ando (2006) used Alternative Structured Optimization, which is closely related to Structural Learning (cited above). He first trained one linear predictor for each target word, and then performed SVD on 7 carefully selected submatrices 18 algorithms used in the experiments. of the feature-to-predictor matrix of weights. The system attained small but consistent improvements (no significance data was given) on the Senseval3 lexical sample datasets using SVD and unlabeled data. We have previously shown (Agirre et al., 2005; Agirre and Lopez de Lacalle, 2007) that performing SVD on the feature-to-documents m"
C08-1003,N01-1006,0,0.0522286,"Missing"
C08-1003,W06-1615,0,0.567754,"Missing"
C08-1003,N01-1011,0,0.0887843,"We relied on the usual features used in previous WSD work, grouped in three main sets. Local collocations comprise the bigrams and trigrams formed around the target word (using either lemmas, word-forms, and PoS tags1 ), those formed with the previous/posterior lemma/word-form in the sentence, and the content words in a ±4-word window around the target. Syntactic dependencies2 use the object, subject, noun-modifier, preposition, and sibling lemmas, when available. Finally, Bag-of-words features are the lemmas of the content words in the whole context, plus the salient bigrams in the context (Pedersen, 2001). 4.2 Features from the reduced space Apart from the original space of features, we have the so called SVD features, obtained from the projection of the feature vectors into the reduced space (Deerwester et al., 1990). Basically, we set a term-by-document or feature-by-example matrix M from the corpus (see section below for more details). SVD decomposes it into three matrices, M = U ΣV T . If the desired number of dimensions in the reduced space is p, we select p rows from Σ and V , yielding Σp and Vp respectively. We can map any feature vector ~t (which represents either a train or test examp"
C08-1003,P07-1007,0,0.17455,"Missing"
C08-1003,S07-1016,0,0.0524008,"the k most similar labeled examples to the test example. The similarity among instances is measured by the cosine of their vectors. The test instance is labeled with the sense obtaining the maximum the sum of the weighted vote of the k most similar contexts. We set k to 5 based on previous results (Agirre and Lopez de Lacalle, 2007). space is constructed only with terms, which correspond to bag-of-words features, and thus discards the rest of the features. Given that the WSD literature has shown that all features, including local and syntactic features, are necessary for optimal performance (Pradhan et al., 2007), we propose the following alternative to construct the matrix. One Matrix per Target word (SVD - OMT). For each word: (i) construct a corpus with its occurrences in the labeled and, if desired, unlabeled corpora, (ii) extract all features, (iii) build the featureby-example matrix, (iv) decompose it with SVD, and (v) project all the labeled training and test data for the word. Note that this variant performs one SVD process for each target word separately, hence its name. We proposed this technique in (Agirre et al., 2005). An important parameter when doing SVD is the number of dimensions in t"
C08-1003,W04-3237,0,0.113779,"y use related unlabeled text and include it in the term-by-document matrix to expand it and capture better the interesting properties of the data. Their approach is similar to our SMA method in Section 4.2). In the supervised setting, a recent paper by Daum´e III (2007) shows that, using a very simple feature augmentation method coupled with Support Vector Machines, he is able to effectively use both labeled target and source data to provide the best results in a number of NLP tasks. His method improves or equals over previously explored more sophisticated methods (Daum´e III and Marcu, 2006; Chelba and Acero, 2004). Regarding WSD, some initial works made basic analysis of the particular issues. Escudero et al. (2000) tested the supervised adaptation setting on the DSO corpus, which had examples from the Brown corpus and Wall Street Journal corpus. They found that the source corpus did not help when tagging the target corpus, showing that Though not addressing domain adaptation, other works on WSD also used SVD and are closely related to the present paper. Gliozzo et al. (2005) used SVD to reduce the space of the term-todocument matrix, and then computed the similarity between train and test instances us"
C08-1003,rose-etal-2002-reuters,0,0.0572505,"Missing"
C08-1003,P07-1033,0,0.533321,"Missing"
C08-1003,W00-1326,1,\N,Missing
C08-1003,P05-1050,0,\N,Missing
C10-1005,J93-2003,0,0.047958,"ient for the CL-ESA data requirements. 3.2 In this model an estimation of how likely is that d′ is a translation of dq is performed. It is based on the adaptation of the Bayes rule for MT: p(d′ |dq ) = p(d′ ) p(dq |d′ ) . p(dq ) (1) As p(dq ) does not depend on d′ , it is neglected. From an MT point of view, the conditional probability p(dq |d′ ) is known as translation model probability and is computed on the basis of a statistical bilingual dictionary. p(d′ ) is known as language model probability; it describes the target language L′ in order to obtain grammatically acceptable translations (Brown et al., 1993). Translating dq into L′ is not the concern of this method, rather it focuses on retrieving texts written in L′ which are potential translations of dq . Therefore, Barr´on-Cede˜no et al. (2008) proposed replacing the language model (the one used in T+MA) by that known as length model. This model depends on text’s character lengths instead of language structures. Multiple translations from d into L′ are possible, and it is uncommon to find a pair of translated texts d and d′ such that |d |= |d′ |. Nevertheless, the length of such translations is closely related to a translation length factor. I"
C10-1005,clough-etal-2002-building,0,0.11682,"1 where µ and σ are the mean and the standard deviation of the character lengths between translations of texts from L into L′ . If the length of d′ is not the expected given dq , it receives a low qualification. The translation model probability is defined as: p(d |d′ ) = YX p(x, y), In other Information Retrieval tasks a plethora of corpora is available for experimental and comparison purposes. However, plagiarism implies an ethical infringement and, to the best of our knowledge, there is no corpora of actual cases available, other than some seminal efforts on creating corpora of text reuse (Clough et al., 2002), artificial plagiarism (Potthast et al., 2009), and simulated plagiarism (Clough and Stevenson, 2010). The problem is worse for cross-language plagiarism. Therefore, in our experiments we use two parallel corpora: Software, an en-eu translation memory of software manuals generously supplied by Elhuyar Fundazioa5 ; and Consumer, a corpus extracted from a consumer oriented magazine that includes articles written in Spanish along with their Basque, Catalan, and Galician translations6 (Alc´azar, 2006). Software includes 288, 000 parallel sentences; 8.66 (6.83) words per sentence in the English (B"
C10-1005,P07-2045,0,0.00738927,"Missing"
C10-1005,N04-1034,0,0.0137599,"Missing"
C10-1005,J03-1002,0,0.00763746,"Missing"
C10-1005,steinberger-etal-2006-jrc,0,0.0300396,"m of CLPD in Basque, with source texts written in Spanish (the co-official language of the Figure 1: First sentences from the Wikipedia articles “Party of European Socialists” (en),“Partido Socialista Europeo” (es), and “Europako Alderdi Sozialista” (eu) (Wikipedia, 2010b). 100 words are contained in the en, es and eu articles, respectively). Of high relevance is that the two corpora used in this work were manually constructed by translating English and Spanish text into Basque. In the experiments carried out by Potthast et al. (2010), which inspired our work, texts from the JCRAcquis corpus (Steinberger et al., 2006) and Wikipedia were used. The first one is a multilingual corpus with no clear definition of source and target languages, whereas in Wikipedia no specific relationship exists between the different languages in which a topic may be broached. In some cases (cf. Fig. 1) they are clearly co-derived, but in others they are completely independent. CLPD has been investigated just recently, mainly by adapting models formerly proposed for cross-language information retrieval. This is the case of cross-language explicit semantic analysis (CL-ESA), proposed by Potthast et al. (2008). In this case the com"
C10-1005,barron-cedeno-etal-2010-corpus,1,\N,Missing
C10-2002,N09-1003,1,0.88659,"er settings. The results show that our method is specially effective for realistic, non-optimal settings, adding robustness to the IR engine. We also explored the effect of document length, and show that our method is specially successful with shorter documents. 1 Arantxa Otegi Lexical semantic resources such as WordNet (Fellbaum, 1998) might provide a principled and explicit remedy for the brittleness of keyword matches. WordNet has been used with success in psycholinguistic datasets of word similarity and relatedness, where it often surpasses distributional methods based on keyword matches (Agirre et al., 2009b). WordNet has been applied to IR before. Some authors extended the query with related terms (Voorhees, 1994; Liu et al., 2005), while others have explicitly represented and indexed word senses after performing word sense disambiguation (WSD) (Gonzalo et al., 1998; Stokoe et al., 2003; Kim et al., 2004). More recently, a CLEF task was organized (Agirre et al., 2008; Agirre et al., 2009a) where queries and documents were semantically disambiguated, and participants reported mixed results. Introduction Since the earliest days of IR, researchers noted the potential pitfalls of keyword retrieval,"
C10-2002,W98-0705,0,0.283235,"Arantxa Otegi Lexical semantic resources such as WordNet (Fellbaum, 1998) might provide a principled and explicit remedy for the brittleness of keyword matches. WordNet has been used with success in psycholinguistic datasets of word similarity and relatedness, where it often surpasses distributional methods based on keyword matches (Agirre et al., 2009b). WordNet has been applied to IR before. Some authors extended the query with related terms (Voorhees, 1994; Liu et al., 2005), while others have explicitly represented and indexed word senses after performing word sense disambiguation (WSD) (Gonzalo et al., 1998; Stokoe et al., 2003; Kim et al., 2004). More recently, a CLEF task was organized (Agirre et al., 2008; Agirre et al., 2009a) where queries and documents were semantically disambiguated, and participants reported mixed results. Introduction Since the earliest days of IR, researchers noted the potential pitfalls of keyword retrieval, such as synonymy, polysemy, hyponymy or anaphora. Although in principle these linguistic phenomena should be taken into account in order to obtain high retrieval relevance, the lack of algorithmic models prohibited any systematic study of the effect of this phenom"
C10-2002,P08-1082,0,0.0826097,"Missing"
C10-2002,N06-1052,0,0.0232691,"ntza.otegi@ehu.es Expansion (QE) methods, which typically analyze term co-occurrence statistics in the corpus and in the highest scored documents for the original query in order to select terms for expanding the query terms (Manning et al., 2009). Document expansion (DE) is a natural alternative to QE, but surprisingly it was not investigated until very recently. Several researchers have used distributional methods from similar documents in the collection in order to expand the documents with related terms that do not actually occur in the document (Liu and Croft, 2004; Kurland and Lee, 2004; Tao et al., 2006; Mei et al., 2008; Huang et al., 2009). The work presented here is complementary, in that we also explore DE, but use WordNet instead of distributional methods. The use of semantic information to improve IR is a long-standing goal. This paper presents a novel Document Expansion method based on a WordNet-based system to find related concepts and words. Expansion words are indexed separately, and when combined with the regular index, they improve the results in three datasets over a state-of-the-art IR engine. Considering that many IR systems are not robust in the sense that they need careful f"
C10-2002,E09-1005,1,\N,Missing
C12-1007,C00-1001,0,0.0631879,"Missing"
C12-1007,W02-0906,1,0.621282,"k puskatu zituen. March-ine-sg Millar-erg-sg wheel all-abs-pl break auxiliary. Millar broke all the wheels in March. 3.2 Acquisition of verbal subcategorization information Verbal subcategorization information was extracted from 4 different sources: a subcategorization dictionary built from monolingual Basque corpus, web queries, monolingual English corpus, and a traditional monolingual Basque dictionary. The subcategorization dictionary was obtained from Basque monolingual corpus, initially built with the purpose of making attachment decisions for a shallow parser on its way to full parsing (Aldezabal et al., 2002). For each of the 2,571 verbs this dictionary lists information about transitivity of the verb, noun1 -case-verb triples or noun-case-verb-auxiliary quadruples and estimated frequency of each. This dictionary was automatically built from raw corpora, comprising a compilation of 18 months of news from Euskaldunon Egunkaria (a journal written in Basque). The size of the corpus is around 780,000 sentences, approximately 10M words. From the 5,572 different verb lemmas in the corpus, the subcategorization dictionary was compiled for the 2,751 verbs occurring at least 10 times. The corpus was parsed"
C12-1007,D11-1113,0,0.0220176,"Missing"
C12-1007,W04-1506,0,0.0332196,"Missing"
C12-1007,W06-2922,0,0.0376406,"Missing"
C12-1007,N07-1049,0,0.0323397,"Missing"
C12-1007,J07-4002,0,0.0364555,"Missing"
C12-1007,ballesteros-nivre-2012-maltoptimizer-system,0,0.139957,"Missing"
C12-1007,P11-1070,0,0.0157295,"om automatically parsed English data. We used the BNC corpus parsed with the RASP parser (Briscoe et al., 2006) containing 47,145,584 syntactic relations, where 10,447,129 are verb-noun dependency relations. The method has the following steps: 1. Translate the dependent lemma and the verb lemma using a bilingual dictionary. 2. Build all possible translation pairs. 3. Collect frequencies of each pair in the English corpus, depending on the label (subject or object) assigned by the English parser. The third source is the result of directly querying the web. The web can be seen as a vast corpus (Bansal and Klein, 2011; Nakov, 2007; Lapata and Keller, 2005) where, in principle, we have bigger chances of finding low frequency combinations not found with the monolingual or crosslingual approaches presented so far. The following steps were pursued: 1. Obtain the lemma of the ambiguous element, create all possible subject and object unambiguous inflected forms using a language generation tool, that is, lemma+ergative+plural and lemma+absolutive+singular pairs. 2. Obtain the three different inflected forms of the verb (verb+future aspect marking, verb+perfect aspect marking and verb+ imperfect aspect marking) us"
C12-1007,W11-3804,0,0.0428859,"Missing"
C12-1007,P06-4020,0,0.0228694,"Missing"
C12-1007,P05-1022,0,0.0195978,"Missing"
C12-1007,P06-2029,0,0.0554761,"Missing"
C12-1007,W05-1505,0,0.0288052,"Missing"
C12-1007,J93-1005,0,0.393856,"ally significant (p-value < 0.00009). In addition we also show the performance over objects and subjects. Note that the post-processed version improves both object and subject recognition. 6 Related Work and Discussion As mentioned in the introduction, this work is framed following a parse correction strategy. In parsing, not all ambiguities show the same complexity, and not all the languages behave the same way with respect to the distribution of the ambiguities. In English, for example, prepositional phrase attachment (PP-attachment for short) ambiguity traditionally stirred interest since (Hindle and Rooth, 1993; Ratnaparkhi, 1998), among others, for being both common and difficult to solve. These seminal works presented PP-attachment resolution in isolation, with no evaluation over full sentences or integration with a parser or with the results of a parser. With the proliferation of statistical parsers the attention moved from solving specific ambiguities to treat ambiguities as a whole. Statistical parsers learned from treebanks, though, make it difficult to reach any conclusion on what is the relevant information for resolving specific ambiguities, and whether those need to be encoded explicitly i"
C12-1007,leturia-etal-2008-analysis,0,0.0195143,"t marking and verb+ imperfect aspect marking) using the same generation tool. 3. Generate the corresponding different transitive and intransitive auxiliaries as well. It is not feasible to use all possible transitive/intransitive auxiliaries because the query number would explode so we took into account the 20 most frequent forms. 4. Construct all possible element+case+verb+auxiliary quadruples. For each element-verb candidate we get approximately 60 quadruples, 5. Search Google and collect hits. Unfortunately Google does not recognize documents in Basque as a separate language. Some authors (Leturia et al., 2008) add certain common Basque words to the query, in order to reduces the number of texts in other languages returned by Google. We solved the problem using another heuristic, restricting to documents not in Spanish nor in English. The first one because Basque borrows vocabulary from Spanish and several times Basque texts are wrongly tagged as Spanish texts, and the other because of the same reason plus the fact that is the most common language on the web. Variability on the web does not cause a problem in this case because all the searches concerning each element-verb candidate are performed at"
C12-1007,P98-2177,0,0.0762652,"e < 0.00009). In addition we also show the performance over objects and subjects. Note that the post-processed version improves both object and subject recognition. 6 Related Work and Discussion As mentioned in the introduction, this work is framed following a parse correction strategy. In parsing, not all ambiguities show the same complexity, and not all the languages behave the same way with respect to the distribution of the ambiguities. In English, for example, prepositional phrase attachment (PP-attachment for short) ambiguity traditionally stirred interest since (Hindle and Rooth, 1993; Ratnaparkhi, 1998), among others, for being both common and difficult to solve. These seminal works presented PP-attachment resolution in isolation, with no evaluation over full sentences or integration with a parser or with the results of a parser. With the proliferation of statistical parsers the attention moved from solving specific ambiguities to treat ambiguities as a whole. Statistical parsers learned from treebanks, though, make it difficult to reach any conclusion on what is the relevant information for resolving specific ambiguities, and whether those need to be encoded explicitly in treebanks. Focusin"
C12-1007,W06-2112,0,0.0458329,"Missing"
C12-1007,C98-2172,0,\N,Missing
C12-1054,W04-2214,0,0.0316181,"subject/ 880 a controlled vocabulary of keywords (or subject headings), which are widely used in libraries to catalogue materials and facilitate information access. Similarly, in the medical domain MeSH4 , created and maintained by the National Library of Medicine, provides a controlled vocabulary of medical subject headings. In computational linguistics, WordNet (Fellbaum, 1998) is a commonly used lexical knowledge base that links concepts in various ways. WordNet has been expanded with WordNet domain labels which group together words from different syntactic categories and different senses (Bentivogli et al., 2004). These domain labels are organised into a hierarchical structure. There is a body of previous work on automatically deriving taxonomies and relations from free text. Hearst (1992) was perhaps the earliest significant effort to derive hyponym-hypernym relations from free text using the now eponymous Hearst patterns which code common syntactic forms of the hyponymy pattern (e.g. ‘Vehicles such as Cars’). These patterns were hand-crafted. More recently Snow et al. (2004) developed on this work by using an existing knowledge base to automatically derive lexico-syntactic patterns containing the hy"
C12-1054,C92-2082,0,0.28413,"domain MeSH4 , created and maintained by the National Library of Medicine, provides a controlled vocabulary of medical subject headings. In computational linguistics, WordNet (Fellbaum, 1998) is a commonly used lexical knowledge base that links concepts in various ways. WordNet has been expanded with WordNet domain labels which group together words from different syntactic categories and different senses (Bentivogli et al., 2004). These domain labels are organised into a hierarchical structure. There is a body of previous work on automatically deriving taxonomies and relations from free text. Hearst (1992) was perhaps the earliest significant effort to derive hyponym-hypernym relations from free text using the now eponymous Hearst patterns which code common syntactic forms of the hyponymy pattern (e.g. ‘Vehicles such as Cars’). These patterns were hand-crafted. More recently Snow et al. (2004) developed on this work by using an existing knowledge base to automatically derive lexico-syntactic patterns containing the hyponym-hypernym pairs. An alternative to creating hierarchies of concepts from the pattern-based methods is to use statistical methods. Sanderson and Croft (1999) used an approach t"
C14-1213,E09-1005,1,0.730288,", 2011). Nowadays it is one of the most widely used NED systems and attains performances close to state-of-the-art (Daiber et al., 2013)We used the default values of the parameters for all the experiments in this paper. We also tested an in-house reimplementation of the generative probabilistic model presented in (Han and Sun, 2011). This is a state-of-the-art system which got the same accuracy as the best participant (72.0) when evaluated in the non-NIL subset of TAC2013. UKB is a freely-available system for performing Word Sense Disambiguation and Similarity based on random walks on graphs (Agirre and Soroa, 2009). Instead of using it on WordNet, we represented Wikipedia as a graph, where vertices are the wikipedia articles and edges represents bidirectional hyperlinks among Wikipedia pages, effectively implementing a NED system. We used a Wikipedia dump from 2013 in our experiments. UKB is a competitive, state-of-the-art system which attained a score of 69.0 when evaluated in the non-NIL subset of the TAC2013 dataset. The input of the systems is the context of each mention to be disambiguated, in the form of a 100 token window centered in the target mention. In NED, the identification of the correct m"
C14-1213,W09-2404,0,0.0628236,"Missing"
C14-1213,H92-1045,0,0.750493,"Missing"
C14-1213,P11-1095,0,0.286581,"Missing"
C14-1213,P03-1054,0,0.00849852,"yntactic collocations and around 250 examples of propositions. Note that both AIDA and TAC2009 contain mentions that were not linked to a Wikipedia article because the mention referred to an entity which was not listed in the entity inventory. We ignored all those cases (called NIL cases), as we would need to investigate, for each NIL, which actual entity they refer to. The collocations were extracted from the TAC KBP collection (Ji et al., 2010), comprising 1.7 million documents, 1.3 millions from newswire and 0.5 millions from the web. We have parsed them with the Stanford CoreNLP software (Klein and Manning, 2003), obtaining around 650 million dependencies (De Marneffe and Manning, 2008). We selected subject, object, prepositional complements and adjectival modifiers as the source for syntactic collocations. In order to provide more specific collocations, we implemented the syntactic patterns proposed in (Pe˜nas and Hovy, 2010), which produce so-called propositions. The result is a database with 16 million distinct propositions. Table 1 shows the six patterns used in this work, together with some examples. In order to know whether a mention is ambiguous, we built a dictionary based on Wikipedia which l"
C14-1213,J13-4004,0,0.0608289,"Missing"
C14-1213,W00-1326,1,0.571223,"Missing"
C14-1213,C10-2113,1,0.863781,"Missing"
C14-1213,spitkovsky-chang-2012-cross,0,0.0439696,"t, prepositional complements and adjectival modifiers as the source for syntactic collocations. In order to provide more specific collocations, we implemented the syntactic patterns proposed in (Pe˜nas and Hovy, 2010), which produce so-called propositions. The result is a database with 16 million distinct propositions. Table 1 shows the six patterns used in this work, together with some examples. In order to know whether a mention is ambiguous, we built a dictionary based on Wikipedia which lists, for each string mention, which entities it can refer to. We followed the construction method of (Spitkovsky and Chang, 2012), which checked article titles, redirects, disambiguation pages and hyperlinks to find mention strings that can be used to refer to entities. Contrary to them, we could not access hyperlinks in the web, so we could use only those in Wikipedia. According to our dictionary, the ambiguity of the mentions that we are studying is very high, 26.4 entities on average for the mentions in AIDA, and 62.6 entities on average for the mentions in TAC2009. 3 One entity per discourse In order to estimate OSPD we divided the number of times a mention string referred to different entities in the document with"
C14-1213,H93-1052,0,0.509804,"Missing"
C14-1213,D11-1072,0,\N,Missing
C14-1213,W08-1301,0,\N,Missing
C16-1293,P05-1048,0,0.2182,"containing ambiguous words, which have multiple meanings, the state-of-the-art phrase-based SMT is still suffering from inaccurate lexical choice which makes translations unable to correctly convey the meaning of source sentences. Recent studies show that in order to improve translation quality, one must correctly identify the most likely senses of source-side ambiguous words when selecting target translation (Gao et al., 2013; Zou et al., 2013; Zhang et al., 2014). One common approach to deal with ambiguity is to incorporate a word sense disambiguation (WSD) system into SMT system. At first, Carpuat and Wu (2005) attempt to use the senses of source ambiguous words predicted by a standard formulation of WSD directly in a word-based SMT but results are disappointing. They are skeptical of the assumption that WSD systems are useful for SMT. Instead of the standard WSD task, Vickrey et al. (2005) propose a novel formulation of WSD for SMT: directly predicting possible target translation candidates as senses for ambiguous source words. This reformulated WSD has been shown to help SMT by several subsequent studies, including later work by Carpuat and Wu (2007). Following this WSD reformulation for SMT, they"
C16-1293,D07-1007,0,0.208485,"guation (WSD) system into SMT system. At first, Carpuat and Wu (2005) attempt to use the senses of source ambiguous words predicted by a standard formulation of WSD directly in a word-based SMT but results are disappointing. They are skeptical of the assumption that WSD systems are useful for SMT. Instead of the standard WSD task, Vickrey et al. (2005) propose a novel formulation of WSD for SMT: directly predicting possible target translation candidates as senses for ambiguous source words. This reformulated WSD has been shown to help SMT by several subsequent studies, including later work by Carpuat and Wu (2007). Following this WSD reformulation for SMT, they integrate the WSD training, where sense definitions are drawn automatically from all phrasal translation candidates rather than from a predefined sense inventory into a phrase-based SMT. In addition to WSD, topic model (Blei et al., 2003) is yet another technique used to detect most likely senses of source words. Various topic-specific lexicon translation models are proposed to improve translation quality. These models can be classified into two categories: word-level translation models (Zhao and Xing, 2006; Tam et al., 2007) and phrase-level mo"
C16-1293,P07-1005,0,0.0391078,"models. Section 5 presents the way that we integrate supersense-based translation models into SMT. Section 6 discusses our experiments and results. In section 7 we summarize our findings and directions for future work. 2 Related Work The problem of accurate lexical choice is an unsolved challenge for phrase-based SMT. Much work has been done to identify proper senses of source ambiguous words to aid system in choosing appropriate translations. Integrating WSD into an SMT system is typical of this work as described in Section 1 (Carpuat and Wu, 2005; Vickrey et al., 2005; Carpuat and Wu, 2007; Chan et al., 2007). Exploring topic model for SMT is another attempt. Gong et al. (2010) introduce document-level topics to help SMT generate target translations. They use a monolingual LDA model to assign a specific topic to the document to be translated. Similarly, each phrase pair is also assigned with one specific topic. A phrase pair will be filtered from phrase table if its topic mismatches the document topic. Xiao et al. (2012) propose a topic similarity model which incorporates the rule-topic distributions on both the source and target side into a hierarchical phrase-based system for rule selection. 1 h"
C16-1293,P96-1041,0,0.145086,"mum entropy based reordering model (Xiong et al., 2006). Our training corpora are English-Spanish sentences from the Europarl parallel corpus (Koehn, 2005) consisting of 1.9M sentence pairs with 51M English words and 54M Spanish words. We ran GIZA++ on the training data in both directions and then applied the “grow-diag-final” refinement rule (Koehn et al., 2003) to obtain final word alignments. Our phrase table was generated according to the word-aligned data. As to the language model, we trained a separate 5-gram LM using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1996) on each subcorpus4 and then interpolated them according to the corpus used for tuning. We trained our MaxEnt classifiers with the off-the-shelf MaxEnt tool.5 We performed 100 iterations of the L-BFGS algorithm implemented in the training toolkit on the collected training events from the sense-annotated data. We set the Gaussian prior to 1 to avoid overfitting. 4 There are 12 subcorpora: commoncrawl, europarl, kde4, news2007, news2008, news2009, news2010, news2011, news2012, newscommentary, openoffice, un 5 http://homepages.inf.ed.ac.uk/lzhang10/maxenttoolkit.html 3119 System Base Max ss Max h"
C16-1293,W06-1670,0,0.162193,"into a hierarchical phrase-based system for rule selection. 1 https://code.google.com/archive/p/word2vec/ 3115 noun verb Tops body food person quantity time body consumption perception act cognition group phenomenon relation animal communication location plant shape artifact event motive possession state attribute feeling object process substance change contact possession cognition creation social communication emotion stative competition motion weather Table 1: Supersense labels for nouns and verbs in WordNet. Supersenses are useful and have been used as high-level features in various tasks. Ciaramita and Altun (2006) define a tagset based on WordNet supersenses to perform broad-coverage word sense disambiguation and information extraction which they approach as a unified tagging problem. They achieve considerable improvements over the first sense baseline. Koo and Collins (2005) utilize supersense reranking that provides a partial disambiguation step in syntactic parse to build useful latent semantic features. Other tasks like preposition sense disambiguation (Ye and Baldwin, 2007), noun compound interpretation (Tratz and Hovy, 2010) can also employ supersenses to improve performance. Word embeddings, als"
C16-1293,W03-1022,0,0.0610166,"u et al. (2013) propose a method to learn bilingual word embeddings for recognizing and quantifying semantic similarities across languages. Our work is to integrate supersenses into a phrase-based SMT. We adopt two ways to train our supersense-based models: one is a MaxEnt classifier-based model which is closely related to Xiong and Zhang’s (2014) work, the other is a model built on supersense embeddings that are different from word embeddings in that supersense embeddings can provide more high-level semantic information than word embeddings. 3 Supersense Tagging Supersenses, first defined by Ciaramita and Johnson (2003), are coarse-grained semantic labels used by lexicographers to facilitate the development of WordNet. There are 45 supersense labels, 26 for nouns, 15 for verbs, 3 for adjectives and 1 for adverbs, used in WordNet to classify synsets into several domains based on syntactic category and semantic coherence. Normally, an ambiguous word belongs to several synsets. Since supersense labels are assigned to synsets, word sense ambiguity can be preserved to a certain degree at this level. In this paper, we focus on noun and verb supersenses. Table 1 shows the corresponding supersense labels in WordNet."
C16-1293,W02-1001,0,0.0128247,"social social body stative stative change consumption consumption WN Senses 1 6 2 3 4 8 5 7 Gloss give help or assistance; be of service contribute to the furtherance of improve the condition of be of use abstain from doing; always used with a negative improve; change for the better help to some food; help with food or drink take or use Table 2: An example of grouping different fine-grained senses of a word into supersenses. WN senses: senses from WordNet. that is based on a Hidden Markov Model (HMM) trained in a discriminative way. That is, the model can be seen as a perceptron-trained HMM (Collins, 2002) that jointly models observation/label sequences. The model is trained on Semcor Corpus (Miller et al., 1993) following the experimental setting described in Ciaramita and Altun (2006). WordNet fine-grained senses are mapped to their corresponding supersense. In our case, only nouns and verbs are mapped, labeling as “NULL” the rest of the tokens (including adjectives and adverbs). In some cases “noun.Tops” refers to more specific supersenses, such as “food”, “person”, or “animal”. In those cases we substitute the “noun.Tops” with more specific label (e.g “animal” as “noun.animal”). Although th"
C16-1293,N03-1017,0,0.0755443,"nslation quality. • Whether supersense embeddings can play a role in lexical selection. 6.1 Setup Our baseline is a state-of-the-art SMT system which adapts Bracketing Transduction Grammars (Wu, 1997) to phrasal translation and augment itself with a maximum entropy based reordering model (Xiong et al., 2006). Our training corpora are English-Spanish sentences from the Europarl parallel corpus (Koehn, 2005) consisting of 1.9M sentence pairs with 51M English words and 54M Spanish words. We ran GIZA++ on the training data in both directions and then applied the “grow-diag-final” refinement rule (Koehn et al., 2003) to obtain final word alignments. Our phrase table was generated according to the word-aligned data. As to the language model, we trained a separate 5-gram LM using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1996) on each subcorpus4 and then interpolated them according to the corpus used for tuning. We trained our MaxEnt classifiers with the off-the-shelf MaxEnt tool.5 We performed 100 iterations of the L-BFGS algorithm implemented in the training toolkit on the collected training events from the sense-annotated data. We set the Gaussian prior to 1"
C16-1293,2005.mtsummit-papers.11,0,0.0137104,"ranslation using massive training data. With the trained supersense-based translation model, we would like to investigate the following two questions: • Whether coarse-grained supersenses can improve translation quality. • Whether supersense embeddings can play a role in lexical selection. 6.1 Setup Our baseline is a state-of-the-art SMT system which adapts Bracketing Transduction Grammars (Wu, 1997) to phrasal translation and augment itself with a maximum entropy based reordering model (Xiong et al., 2006). Our training corpora are English-Spanish sentences from the Europarl parallel corpus (Koehn, 2005) consisting of 1.9M sentence pairs with 51M English words and 54M Spanish words. We ran GIZA++ on the training data in both directions and then applied the “grow-diag-final” refinement rule (Koehn et al., 2003) to obtain final word alignments. Our phrase table was generated according to the word-aligned data. As to the language model, we trained a separate 5-gram LM using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1996) on each subcorpus4 and then interpolated them according to the corpus used for tuning. We trained our MaxEnt classifiers with the o"
C16-1293,H05-1064,0,0.0312498,"fact event motive possession state attribute feeling object process substance change contact possession cognition creation social communication emotion stative competition motion weather Table 1: Supersense labels for nouns and verbs in WordNet. Supersenses are useful and have been used as high-level features in various tasks. Ciaramita and Altun (2006) define a tagset based on WordNet supersenses to perform broad-coverage word sense disambiguation and information extraction which they approach as a unified tagging problem. They achieve considerable improvements over the first sense baseline. Koo and Collins (2005) utilize supersense reranking that provides a partial disambiguation step in syntactic parse to build useful latent semantic features. Other tasks like preposition sense disambiguation (Ye and Baldwin, 2007), noun compound interpretation (Tratz and Hovy, 2010) can also employ supersenses to improve performance. Word embeddings, also called distributed word representations, are used in many natural language processing areas such as information retrieval (Manning et al., 2008), search query expansions (Jones et al., 2006), or representing semantics of words (Reisinger and Mooney, 2010). As to SM"
C16-1293,H93-1061,0,0.449264,"help or assistance; be of service contribute to the furtherance of improve the condition of be of use abstain from doing; always used with a negative improve; change for the better help to some food; help with food or drink take or use Table 2: An example of grouping different fine-grained senses of a word into supersenses. WN senses: senses from WordNet. that is based on a Hidden Markov Model (HMM) trained in a discriminative way. That is, the model can be seen as a perceptron-trained HMM (Collins, 2002) that jointly models observation/label sequences. The model is trained on Semcor Corpus (Miller et al., 1993) following the experimental setting described in Ciaramita and Altun (2006). WordNet fine-grained senses are mapped to their corresponding supersense. In our case, only nouns and verbs are mapped, labeling as “NULL” the rest of the tokens (including adjectives and adverbs). In some cases “noun.Tops” refers to more specific supersenses, such as “food”, “person”, or “animal”. In those cases we substitute the “noun.Tops” with more specific label (e.g “animal” as “noun.animal”). Although the tagger learns 41 semantic categories, we included (B) beginning and (I) continuation as supersense prefixes"
C16-1293,P03-1021,0,0.048436,"r both hardware and software. This material was collected using a support service via chat, this implies that the corpus is composed by naturally occurring utterances produced by users while interacting with a service. Only interactions composed by one question and the respective answer were included in the corpus. We also divided our test set batch2 into two parts equally batch2a and batch2q respectively. In other words, we used three test sets to verify the effectiveness of our proposed models. We adopted the case-insensitive BLEU-4 (Papineni et al., 2002) as evaluation metric and ran MERT (Och, 2003) three times to alleviate the instability. We reported average BLEU scores over the three runs as final results. 6.2 Results Our first group of experiments are designed to investigate whether supersenses can be modeled like hidden senses using a MaxEnt classifier. We use the same experiment settings as Xiong and Zhang (2014) did. Especially, we also find that 10-word window is the most suitable window for extracting semantic information according to experiments. Table 3 shows the experimental results for the two SMT systems equipped with multiple MaxEnt classifiers trained on supersenses and h"
C16-1293,P02-1040,0,0.0951977,"pairs in the domain of computer and IT troubleshooting for both hardware and software. This material was collected using a support service via chat, this implies that the corpus is composed by naturally occurring utterances produced by users while interacting with a service. Only interactions composed by one question and the respective answer were included in the corpus. We also divided our test set batch2 into two parts equally batch2a and batch2q respectively. In other words, we used three test sets to verify the effectiveness of our proposed models. We adopted the case-insensitive BLEU-4 (Papineni et al., 2002) as evaluation metric and ran MERT (Och, 2003) three times to alleviate the instability. We reported average BLEU scores over the three runs as final results. 6.2 Results Our first group of experiments are designed to investigate whether supersenses can be modeled like hidden senses using a MaxEnt classifier. We use the same experiment settings as Xiong and Zhang (2014) did. Especially, we also find that 10-word window is the most suitable window for extracting semantic information according to experiments. Table 3 shows the experimental results for the two SMT systems equipped with multiple M"
C16-1293,N10-1013,0,0.0388089,"ense baseline. Koo and Collins (2005) utilize supersense reranking that provides a partial disambiguation step in syntactic parse to build useful latent semantic features. Other tasks like preposition sense disambiguation (Ye and Baldwin, 2007), noun compound interpretation (Tratz and Hovy, 2010) can also employ supersenses to improve performance. Word embeddings, also called distributed word representations, are used in many natural language processing areas such as information retrieval (Manning et al., 2008), search query expansions (Jones et al., 2006), or representing semantics of words (Reisinger and Mooney, 2010). As to SMT, Zou et al. (2013) propose a method to learn bilingual word embeddings for recognizing and quantifying semantic similarities across languages. Our work is to integrate supersenses into a phrase-based SMT. We adopt two ways to train our supersense-based models: one is a MaxEnt classifier-based model which is closely related to Xiong and Zhang’s (2014) work, the other is a model built on supersense embeddings that are different from word embeddings in that supersense embeddings can provide more high-level semantic information than word embeddings. 3 Supersense Tagging Supersenses, fi"
C16-1293,P10-1070,0,0.0122164,"useful and have been used as high-level features in various tasks. Ciaramita and Altun (2006) define a tagset based on WordNet supersenses to perform broad-coverage word sense disambiguation and information extraction which they approach as a unified tagging problem. They achieve considerable improvements over the first sense baseline. Koo and Collins (2005) utilize supersense reranking that provides a partial disambiguation step in syntactic parse to build useful latent semantic features. Other tasks like preposition sense disambiguation (Ye and Baldwin, 2007), noun compound interpretation (Tratz and Hovy, 2010) can also employ supersenses to improve performance. Word embeddings, also called distributed word representations, are used in many natural language processing areas such as information retrieval (Manning et al., 2008), search query expansions (Jones et al., 2006), or representing semantics of words (Reisinger and Mooney, 2010). As to SMT, Zou et al. (2013) propose a method to learn bilingual word embeddings for recognizing and quantifying semantic similarities across languages. Our work is to integrate supersenses into a phrase-based SMT. We adopt two ways to train our supersense-based model"
C16-1293,H05-1097,0,0.227933,"on quality, one must correctly identify the most likely senses of source-side ambiguous words when selecting target translation (Gao et al., 2013; Zou et al., 2013; Zhang et al., 2014). One common approach to deal with ambiguity is to incorporate a word sense disambiguation (WSD) system into SMT system. At first, Carpuat and Wu (2005) attempt to use the senses of source ambiguous words predicted by a standard formulation of WSD directly in a word-based SMT but results are disappointing. They are skeptical of the assumption that WSD systems are useful for SMT. Instead of the standard WSD task, Vickrey et al. (2005) propose a novel formulation of WSD for SMT: directly predicting possible target translation candidates as senses for ambiguous source words. This reformulated WSD has been shown to help SMT by several subsequent studies, including later work by Carpuat and Wu (2007). Following this WSD reformulation for SMT, they integrate the WSD training, where sense definitions are drawn automatically from all phrasal translation candidates rather than from a predefined sense inventory into a phrase-based SMT. In addition to WSD, topic model (Blei et al., 2003) is yet another technique used to detect most"
C16-1293,J97-3002,0,0.01683,"12) ssrci , stsrci ) ssrci ∈Γ where Γ is a set of source phrases which have translation rules in the phrase table. 6 Experiments In this section, we conducted a series of experiments on English-to-Spanish translation using massive training data. With the trained supersense-based translation model, we would like to investigate the following two questions: • Whether coarse-grained supersenses can improve translation quality. • Whether supersense embeddings can play a role in lexical selection. 6.1 Setup Our baseline is a state-of-the-art SMT system which adapts Bracketing Transduction Grammars (Wu, 1997) to phrasal translation and augment itself with a maximum entropy based reordering model (Xiong et al., 2006). Our training corpora are English-Spanish sentences from the Europarl parallel corpus (Koehn, 2005) consisting of 1.9M sentence pairs with 51M English words and 54M Spanish words. We ran GIZA++ on the training data in both directions and then applied the “grow-diag-final” refinement rule (Koehn et al., 2003) to obtain final word alignments. Our phrase table was generated according to the word-aligned data. As to the language model, we trained a separate 5-gram LM using the SRILM toolki"
C16-1293,P12-1079,1,0.932049,"owing this WSD reformulation for SMT, they integrate the WSD training, where sense definitions are drawn automatically from all phrasal translation candidates rather than from a predefined sense inventory into a phrase-based SMT. In addition to WSD, topic model (Blei et al., 2003) is yet another technique used to detect most likely senses of source words. Various topic-specific lexicon translation models are proposed to improve translation quality. These models can be classified into two categories: word-level translation models (Zhao and Xing, 2006; Tam et al., 2007) and phrase-level models (Xiao et al., 2012). Especially, Xiong and Zhang (2014) propose a sense-based translation model that integrates hidden word senses into machine ∗ Corresponding author This work is licensed under a Creative Commons Attribution 4.0 International Licence. http://creativecommons.org/licenses/by/4.0/ Licence details: 3114 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 3114–3123, Osaka, Japan, December 11-17 2016. translation to investigate whether hidden senses are useful for SMT. They resort to word sense induction (WSI) and build a broad-coverage"
C16-1293,P14-1137,1,0.817405,"SMT, they integrate the WSD training, where sense definitions are drawn automatically from all phrasal translation candidates rather than from a predefined sense inventory into a phrase-based SMT. In addition to WSD, topic model (Blei et al., 2003) is yet another technique used to detect most likely senses of source words. Various topic-specific lexicon translation models are proposed to improve translation quality. These models can be classified into two categories: word-level translation models (Zhao and Xing, 2006; Tam et al., 2007) and phrase-level models (Xiao et al., 2012). Especially, Xiong and Zhang (2014) propose a sense-based translation model that integrates hidden word senses into machine ∗ Corresponding author This work is licensed under a Creative Commons Attribution 4.0 International Licence. http://creativecommons.org/licenses/by/4.0/ Licence details: 3114 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 3114–3123, Osaka, Japan, December 11-17 2016. translation to investigate whether hidden senses are useful for SMT. They resort to word sense induction (WSI) and build a broad-coverage sense tagger that relies on the nonp"
C16-1293,P06-1066,1,0.719581,"e phrase table. 6 Experiments In this section, we conducted a series of experiments on English-to-Spanish translation using massive training data. With the trained supersense-based translation model, we would like to investigate the following two questions: • Whether coarse-grained supersenses can improve translation quality. • Whether supersense embeddings can play a role in lexical selection. 6.1 Setup Our baseline is a state-of-the-art SMT system which adapts Bracketing Transduction Grammars (Wu, 1997) to phrasal translation and augment itself with a maximum entropy based reordering model (Xiong et al., 2006). Our training corpora are English-Spanish sentences from the Europarl parallel corpus (Koehn, 2005) consisting of 1.9M sentence pairs with 51M English words and 54M Spanish words. We ran GIZA++ on the training data in both directions and then applied the “grow-diag-final” refinement rule (Koehn et al., 2003) to obtain final word alignments. Our phrase table was generated according to the word-aligned data. As to the language model, we trained a separate 5-gram LM using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1996) on each subcorpus4 and then int"
C16-1293,S07-1051,0,0.0291408,"abels for nouns and verbs in WordNet. Supersenses are useful and have been used as high-level features in various tasks. Ciaramita and Altun (2006) define a tagset based on WordNet supersenses to perform broad-coverage word sense disambiguation and information extraction which they approach as a unified tagging problem. They achieve considerable improvements over the first sense baseline. Koo and Collins (2005) utilize supersense reranking that provides a partial disambiguation step in syntactic parse to build useful latent semantic features. Other tasks like preposition sense disambiguation (Ye and Baldwin, 2007), noun compound interpretation (Tratz and Hovy, 2010) can also employ supersenses to improve performance. Word embeddings, also called distributed word representations, are used in many natural language processing areas such as information retrieval (Manning et al., 2008), search query expansions (Jones et al., 2006), or representing semantics of words (Reisinger and Mooney, 2010). As to SMT, Zou et al. (2013) propose a method to learn bilingual word embeddings for recognizing and quantifying semantic similarities across languages. Our work is to integrate supersenses into a phrase-based SMT."
C16-1293,P06-2124,0,0.0338829,"ent studies, including later work by Carpuat and Wu (2007). Following this WSD reformulation for SMT, they integrate the WSD training, where sense definitions are drawn automatically from all phrasal translation candidates rather than from a predefined sense inventory into a phrase-based SMT. In addition to WSD, topic model (Blei et al., 2003) is yet another technique used to detect most likely senses of source words. Various topic-specific lexicon translation models are proposed to improve translation quality. These models can be classified into two categories: word-level translation models (Zhao and Xing, 2006; Tam et al., 2007) and phrase-level models (Xiao et al., 2012). Especially, Xiong and Zhang (2014) propose a sense-based translation model that integrates hidden word senses into machine ∗ Corresponding author This work is licensed under a Creative Commons Attribution 4.0 International Licence. http://creativecommons.org/licenses/by/4.0/ Licence details: 3114 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 3114–3123, Osaka, Japan, December 11-17 2016. translation to investigate whether hidden senses are useful for SMT. They r"
C16-1293,D13-1141,0,0.177155,"gle words, are used as translation units so that useful context information can be captured for selecting appropriate translations. Even so, when translating sentences containing ambiguous words, which have multiple meanings, the state-of-the-art phrase-based SMT is still suffering from inaccurate lexical choice which makes translations unable to correctly convey the meaning of source sentences. Recent studies show that in order to improve translation quality, one must correctly identify the most likely senses of source-side ambiguous words when selecting target translation (Gao et al., 2013; Zou et al., 2013; Zhang et al., 2014). One common approach to deal with ambiguity is to incorporate a word sense disambiguation (WSD) system into SMT system. At first, Carpuat and Wu (2005) attempt to use the senses of source ambiguous words predicted by a standard formulation of WSD directly in a word-based SMT but results are disappointing. They are skeptical of the assumption that WSD systems are useful for SMT. Instead of the standard WSD task, Vickrey et al. (2005) propose a novel formulation of WSD for SMT: directly predicting possible target translation candidates as senses for ambiguous source words."
C96-1005,H92-1046,0,0.0813737,"Missing"
C96-1005,J92-1001,0,0.00808683,"Missing"
C96-1005,H93-1061,0,0.0630953,"Missing"
C96-1005,H94-1046,0,0.0211457,"Missing"
C96-1005,W95-0105,0,0.086135,"Missing"
C96-1005,C92-2070,0,0.673012,"Missing"
C96-1005,H91-1077,0,\N,Missing
C96-1005,E95-1016,0,\N,Missing
C96-1005,C92-1056,0,\N,Missing
C96-1005,H93-1052,0,\N,Missing
C98-1003,C96-1005,1,0.749235,"knowledge (lexical, syntactic, semantic . . . . ) should be represented, utilised and combined to aid in this determination. This study relies on the integrated use of three kinds of knowledge (syntagmatic, paradigmatic and statistical) in order to improve first guess accuracy in non-word context-sensitive correction for general unrestricted texts. Our techniques were applied to the corrections posed by ispell. Constraint Grammar (Karlsson et al. 1995) was chosen to represent syntagmatic knowledge. Its use as a part of speech tagger for English has been highly successful. Conceptual Density (Agirre and Rigau 1996) is the paradigmatic component chosen to discriminate semantically among potential noun corrections. This technique measures &quot;affinity distance&quot; between nouns using Wordnet (Miller 1990). Finally, general and document word-occurrence frequency-rates complete the set of knowledge sources combined. We knowingly did not use any model of common misspellings, the main reason being that we did not want to use knowledge about the error source. This work focuses on language models, not error models (typing errors, common misspellings, OCR mistakes, speech recognition mistakes, etc.). The system was ev"
C98-1003,P96-1010,0,0.0901313,"Missing"
C98-1003,P94-1013,0,\N,Missing
C98-2176,P97-1007,1,0.897256,"Missing"
C98-2176,W97-0313,0,0.0338297,"Missing"
C98-2176,C92-2070,0,0.0901388,"Missing"
C98-2176,W90-0108,0,\N,Missing
C98-2176,C92-4189,0,\N,Missing
C98-2176,P95-1026,0,\N,Missing
C98-2176,P81-1030,0,\N,Missing
D16-1250,W15-1521,0,0.040811,"approach is to learn a linear mapping that minimizes the distances between equivalences listed in a bilingual dictionary. In this paper, we propose a framework that generalizes previous work, provides an efficient exact method to learn the optimal linear transformation and yields the best bilingual results in translation induction while preserving monolingual performance in an analogy task. 1 Introduction Bilingual word embeddings have attracted a lot of attention in recent times (Zou et al., 2013; Koˇcisk´y et al., 2014; Chandar A P et al., 2014; Gouws et al., 2014; Gouws and Søgaard, 2015; Luong et al., 2015; Wick et al., 2016). A common approach to obtain them is to train the embeddings in both languages independently and then learn a mapping that minimizes the distances between equivalences listed in a bilingual dictionary. The learned transformation can also be applied to words missing in the dictionary, which can be used to induce new translations with a direct application in machine translation (Mikolov et al., 2013b; Zhao et al., 2015). The first method to learn bilingual word embedding mappings was proposed by Mikolov et al. (2013b), who learn the linear transformation that minimizes the s"
D16-1250,N15-1104,0,0.671988,"Missing"
D16-1250,N15-1176,0,0.0219783,"ttracted a lot of attention in recent times (Zou et al., 2013; Koˇcisk´y et al., 2014; Chandar A P et al., 2014; Gouws et al., 2014; Gouws and Søgaard, 2015; Luong et al., 2015; Wick et al., 2016). A common approach to obtain them is to train the embeddings in both languages independently and then learn a mapping that minimizes the distances between equivalences listed in a bilingual dictionary. The learned transformation can also be applied to words missing in the dictionary, which can be used to induce new translations with a direct application in machine translation (Mikolov et al., 2013b; Zhao et al., 2015). The first method to learn bilingual word embedding mappings was proposed by Mikolov et al. (2013b), who learn the linear transformation that minimizes the sum of squared Euclidean distances for the dictionary entries. Subsequent work has proposed alternative optimization objectives to learn Beyond linear mappings, Lu et al. (2015) apply deep canonical correlation analysis to learn a nonlinear transformation for each language. Finally, additional techniques have been used to address the hubness problem in Mikolov et al. (2013b), both through the neighbor retrieval method (Dinu et al., 2015) a"
D16-1250,D13-1141,0,0.0114373,"into a single space has multiple applications. In order to map from a source space into a target space, a common approach is to learn a linear mapping that minimizes the distances between equivalences listed in a bilingual dictionary. In this paper, we propose a framework that generalizes previous work, provides an efficient exact method to learn the optimal linear transformation and yields the best bilingual results in translation induction while preserving monolingual performance in an analogy task. 1 Introduction Bilingual word embeddings have attracted a lot of attention in recent times (Zou et al., 2013; Koˇcisk´y et al., 2014; Chandar A P et al., 2014; Gouws et al., 2014; Gouws and Søgaard, 2015; Luong et al., 2015; Wick et al., 2016). A common approach to obtain them is to train the embeddings in both languages independently and then learn a mapping that minimizes the distances between equivalences listed in a bilingual dictionary. The learned transformation can also be applied to words missing in the dictionary, which can be used to induce new translations with a direct application in machine translation (Mikolov et al., 2013b; Zhao et al., 2015). The first method to learn bilingual word"
D16-1250,N15-1028,0,\N,Missing
D16-1250,E14-1049,0,\N,Missing
D18-1399,P17-1042,1,0.81531,"strict the vocabulary to the most frequent 200,000 unigrams, 400,000 bigrams and 400,000 trigrams. 3.2 Cross-lingual mapping Cross-lingual mapping methods take independently trained word embeddings in two languages, and learn a linear transformation to map them to a shared cross-lingual space (Mikolov et al., 2013a; Artetxe et al., 2018a). Most mapping methods are supervised, and rely on a bilingual dictionary, typically in the range of a few thousand entries, although a recent line of work has managed to achieve comparable results in a fully unsupervised manner based on either self-learning (Artetxe et al., 2017, 2018b) or adversarial training (Zhang et al., 2017a,b; Conneau et al., 2018). In our case, we use the method of Artetxe et al. 3634 (2018b) to map the n-gram embeddings to a shared cross-lingual space using their open source implementation VecMap1 . Originally designed for word embeddings, this method builds an initial mapping by connecting the intra-lingual similarity distribution of embeddings in different languages, and iteratively improves this solution through selflearning. The method applies a frequency-based vocabulary cut-off, learning the mapping over the 20,000 most frequent words"
D18-1399,P18-1073,1,0.685012,"ploit continuous representations that mitigate the sparsity problem, and overcome the locality problem by making use of unconstrained contexts. Thanks to this additional flexibility, NMT can more effectively exploit large parallel corpora, although SMT is still superior when the training corpus is not big enough (Koehn and Knowles, 2017). Somewhat paradoxically, while most machine translation research has focused on resource-rich settings where NMT has indeed superseded SMT, a recent line of work has managed to train an NMT system without any supervision, relying on monolingual corpora alone (Artetxe et al., 2018c; Lample et al., 2018). Given the scarcity of parallel corpora for most language pairs, including lessresourced languages but also many combinations of major languages, this research line opens exciting opportunities to bring effective machine translation to many more scenarios. Nevertheless, existing solutions are still far behind their supervised counterparts, greatly limiting their practical usability. For instance, existing unsupervised NMT systems obtain between 15-16 BLEU points in WMT 2014 English-French translation, whereas a state-of-the-art NMT system obtains around 41 (Artetxe et a"
D18-1399,D18-1549,0,0.135131,"Missing"
D18-1399,J90-2002,0,0.890624,"ints. The remaining of this paper is structured as follows. Section 2 introduces phrase-based SMT. Section 3 presents our unsupervised approach to learn cross-lingual n-gram embeddings, which are the basis of our proposal. Section 4 describes the proposed unsupervised SMT system itself, while Section 5 discusses its iterative refinement through backtranslation. Section 6 describes the experiments run and the results obtained. Section 7 discusses the related work on the topic, and Section 8 concludes the paper. 2 Background: phrase-based SMT While originally motivated as a noisy channel model (Brown et al., 1990), phrase-based SMT is now formulated as a log-linear combination of several statistical models that score translation candidates (Koehn et al., 2003). The parameters of these scoring functions are estimated independently based on frequency counts, and their weights are then tuned in a separate validation set. At inference time, a decoder tries to find the translation candidate with the highest score according to the resulting combined model. The specific scoring models found in a standard SMT system are as follows: • Phrase table. The phrase table is a collection of source language n-grams and"
D18-1399,D12-1025,0,0.320799,"ould get a reasonable understanding of the original text from them. This suggests that unsupervised machine translation can indeed be a usable alternative in low resource settings. 7 Related work Similar to our approach, statistical decipherment also attempts to build machine translation systems from monolingual corpora. For that purpose, existing methods treat the source language as ciphertext, and model its generation through a noisy channel model involving two steps: the generation of the original English sentence and the probabilistic replacement of the words in it (Ravi and Knight, 2011; Dou and Knight, 2012). The English generative process is modeled using an n-gram language model, and the channel model parameters are estimated using either expectation maximization or Bayesian inference. Subsequent work has attempted to enrich these models with additional information like syntactic knowledge (Dou and Knight, 2013) and word embeddings (Dou et al., 2015). Nevertheless, these systems work in a word-by-word basis and have 3639 only been shown to work in limited settings, being often evaluated in word-level translation. In contrast, our method builds a fully featured phrasebased SMT system, and achiev"
D18-1399,D13-1173,0,0.0486152,"l corpora. For that purpose, existing methods treat the source language as ciphertext, and model its generation through a noisy channel model involving two steps: the generation of the original English sentence and the probabilistic replacement of the words in it (Ravi and Knight, 2011; Dou and Knight, 2012). The English generative process is modeled using an n-gram language model, and the channel model parameters are estimated using either expectation maximization or Bayesian inference. Subsequent work has attempted to enrich these models with additional information like syntactic knowledge (Dou and Knight, 2013) and word embeddings (Dou et al., 2015). Nevertheless, these systems work in a word-by-word basis and have 3639 only been shown to work in limited settings, being often evaluated in word-level translation. In contrast, our method builds a fully featured phrasebased SMT system, and achieves competitive performance in a standard machine translation task. More recently, Artetxe et al. (2018c) and Lample et al. (2018) have managed to train a standard attentional encoder-decoder NMT system from monolingual corpora alone. For that purpose, they use a shared encoder for both languages with pretrained"
D18-1399,P15-1081,0,0.0191838,"ds treat the source language as ciphertext, and model its generation through a noisy channel model involving two steps: the generation of the original English sentence and the probabilistic replacement of the words in it (Ravi and Knight, 2011; Dou and Knight, 2012). The English generative process is modeled using an n-gram language model, and the channel model parameters are estimated using either expectation maximization or Bayesian inference. Subsequent work has attempted to enrich these models with additional information like syntactic knowledge (Dou and Knight, 2013) and word embeddings (Dou et al., 2015). Nevertheless, these systems work in a word-by-word basis and have 3639 only been shown to work in limited settings, being often evaluated in word-level translation. In contrast, our method builds a fully featured phrasebased SMT system, and achieves competitive performance in a standard machine translation task. More recently, Artetxe et al. (2018c) and Lample et al. (2018) have managed to train a standard attentional encoder-decoder NMT system from monolingual corpora alone. For that purpose, they use a shared encoder for both languages with pretrained cross-lingual embeddings, and train th"
D18-1399,N03-1017,0,0.0586958,"proves results further, yielding, for instance, 14.08 and 26.22 BLEU points in WMT 2014 English-German and English-French, respectively, an improvement of more than 7-10 BLEU points over previous unsupervised systems, and closing the gap with supervised SMT (Moses trained on Europarl) down to 2-5 BLEU points. Our implementation is available at https:// github.com/artetxem/monoses. 1 Introduction Neural Machine Translation (NMT) has recently become the dominant paradigm in machine translation (Vaswani et al., 2017). In contrast to more rigid Statistical Machine Translation (SMT) architectures (Koehn et al., 2003), NMT models are trained end-to-end, exploit continuous representations that mitigate the sparsity problem, and overcome the locality problem by making use of unconstrained contexts. Thanks to this additional flexibility, NMT can more effectively exploit large parallel corpora, although SMT is still superior when the training corpus is not big enough (Koehn and Knowles, 2017). Somewhat paradoxically, while most machine translation research has focused on resource-rich settings where NMT has indeed superseded SMT, a recent line of work has managed to train an NMT system without any supervision,"
D18-1399,P03-1021,0,0.0809954,"Missing"
D18-1399,J03-1002,0,0.0194585,"n a standard SMT system are as follows: • Phrase table. The phrase table is a collection of source language n-grams and a list of their possible translations in the target language along with different scores for each of 3633 them. So as to translate longer sequences, the decoder combines these partial n-gram translations, and ranks the resulting candidates according to their corresponding scores and the rest of scoring functions. In order to build the phrase table, SMT computes word alignments in both directions from a parallel corpus, symmetrizes these alignments using different heuristics (Och and Ney, 2003), extracts the set of consistent phrase pairs, and scores them based on frequency counts. For that purpose, standard SMT uses 4 scores for each phrase table entry: the direct and inverse lexical weightings, which are derived from word level alignments, and the direct and inverse phrase translation probabilities, which are computed at the phrase level. • Language model. The language model assigns a probability to a word sequence in the target language. Traditional SMT uses ngram language models for that, which use simple frequency counts over a large monolingual corpus with back-off and smoothi"
D18-1399,P11-1002,0,0.136607,"e and fluent, and one could get a reasonable understanding of the original text from them. This suggests that unsupervised machine translation can indeed be a usable alternative in low resource settings. 7 Related work Similar to our approach, statistical decipherment also attempts to build machine translation systems from monolingual corpora. For that purpose, existing methods treat the source language as ciphertext, and model its generation through a noisy channel model involving two steps: the generation of the original English sentence and the probabilistic replacement of the words in it (Ravi and Knight, 2011; Dou and Knight, 2012). The English generative process is modeled using an n-gram language model, and the channel model parameters are estimated using either expectation maximization or Bayesian inference. Subsequent work has attempted to enrich these models with additional information like syntactic knowledge (Dou and Knight, 2013) and word embeddings (Dou et al., 2015). Nevertheless, these systems work in a word-by-word basis and have 3639 only been shown to work in limited settings, being often evaluated in word-level translation. In contrast, our method builds a fully featured phrasebased"
D18-1399,N13-1073,0,0.0579698,"n the other direction. As detailed in Algorithm 2, this process is repeated iteratively until some convergence criterion is met. While this procedure would be expected to produce a more accurate model at each iteration, it also happens to be very expensive computationally. In order to accelerate our experiments, we use a random subset of 2 million sentences from each monolingual corpus for training4 , in addition to the 10,000 separate sentences that are held out as a validation set for MERT tuning, and perform a fixed number of 3 iterations of the above algorithm. Moreover, we use FastAlign (Dyer et al., 2013) instead of GIZA++ to make word alignment faster. Other than that, training over the synthetic 4 Note that we reuse the original language model, which is trained in the full corpus. 3636 WMT-14 WMT-16 FR-EN EN-FR DE-EN EN-DE DE-EN EN-DE Artetxe et al. (2018c) Lample et al. (2018) Yang et al. (2018) 15.56 14.31 15.58 15.13 15.05 16.97 10.21 - 6.55 - 13.33 14.62 9.64 10.86 Proposed system 25.87 26.22 17.43 14.08 23.05 18.23 Table 1: Results of the proposed method in comparison to existing unsupervised NMT systems (BLEU). parallel corpus is done through standard Moses tools with default settings."
D18-1399,P16-1009,0,0.776514,"ergence work well in practice. Finally, the word translation probabilities w(f¯i |¯ ej ) are computed using the same formula defined for phrase translation probabilities (see above), with the difference that the partition function goes over unigrams only. 4.2 Unsupervised tuning As discussed in Section 2, standard SMT uses MERT over a small parallel corpus to tune the weights of the different scoring functions combined through its log-linear model. Given that we only have access to monolingual corpora in our scenario, we propose to generate a synthetic parallel corpus through backtranslation (Sennrich et al., 2016) and apply MERT tuning over it, iteratively repeating the process in both directions (see Algorithm 1). For that purpose, we reserve a random subset of 10,000 sentences from each monolingual corpora, and run the proposed algorithm over them for 10 iterations, which we find to be enough for convergence. 5 Iterative refinement The procedure described in Section 4 suffices to train an SMT system from monolingual corpora which, as shown by our experiments in Section 6, already outperforms previous unsupervised systems. Nevertheless, our proposed method still makes important simplifications that co"
D18-1399,P13-2121,0,0.0726288,"Missing"
D18-1399,W17-3204,0,0.0369659,"m/monoses. 1 Introduction Neural Machine Translation (NMT) has recently become the dominant paradigm in machine translation (Vaswani et al., 2017). In contrast to more rigid Statistical Machine Translation (SMT) architectures (Koehn et al., 2003), NMT models are trained end-to-end, exploit continuous representations that mitigate the sparsity problem, and overcome the locality problem by making use of unconstrained contexts. Thanks to this additional flexibility, NMT can more effectively exploit large parallel corpora, although SMT is still superior when the training corpus is not big enough (Koehn and Knowles, 2017). Somewhat paradoxically, while most machine translation research has focused on resource-rich settings where NMT has indeed superseded SMT, a recent line of work has managed to train an NMT system without any supervision, relying on monolingual corpora alone (Artetxe et al., 2018c; Lample et al., 2018). Given the scarcity of parallel corpora for most language pairs, including lessresourced languages but also many combinations of major languages, this research line opens exciting opportunities to bring effective machine translation to many more scenarios. Nevertheless, existing solutions are s"
D18-1399,P18-1005,0,0.354176,"he scarcity of parallel corpora for most language pairs, including lessresourced languages but also many combinations of major languages, this research line opens exciting opportunities to bring effective machine translation to many more scenarios. Nevertheless, existing solutions are still far behind their supervised counterparts, greatly limiting their practical usability. For instance, existing unsupervised NMT systems obtain between 15-16 BLEU points in WMT 2014 English-French translation, whereas a state-of-the-art NMT system obtains around 41 (Artetxe et al., 2018c; Lample et al., 2018; Yang et al., 2018). In this paper, we explore whether the rigid and modular nature of SMT is more suitable for these unsupervised settings, and propose a novel unsupervised SMT system that can be trained on monolingual corpora alone. For that purpose, we present a natural extension of the skip-gram model (Mikolov et al., 2013b) that simultaneously learns word and phrase embeddings, which are then mapped to a cross-lingual space through selflearning (Artetxe et al., 2018b). We use the resulting cross-lingual phrase embeddings to induce a phrase table, and combine it with a language model and a distance-based dis"
D18-1399,P17-1179,0,0.0536574,"grams, 400,000 bigrams and 400,000 trigrams. 3.2 Cross-lingual mapping Cross-lingual mapping methods take independently trained word embeddings in two languages, and learn a linear transformation to map them to a shared cross-lingual space (Mikolov et al., 2013a; Artetxe et al., 2018a). Most mapping methods are supervised, and rely on a bilingual dictionary, typically in the range of a few thousand entries, although a recent line of work has managed to achieve comparable results in a fully unsupervised manner based on either self-learning (Artetxe et al., 2017, 2018b) or adversarial training (Zhang et al., 2017a,b; Conneau et al., 2018). In our case, we use the method of Artetxe et al. 3634 (2018b) to map the n-gram embeddings to a shared cross-lingual space using their open source implementation VecMap1 . Originally designed for word embeddings, this method builds an initial mapping by connecting the intra-lingual similarity distribution of embeddings in different languages, and iteratively improves this solution through selflearning. The method applies a frequency-based vocabulary cut-off, learning the mapping over the 20,000 most frequent words in each language. We kept this cut-off to learn the"
D18-1399,D17-1207,0,0.0421625,"grams, 400,000 bigrams and 400,000 trigrams. 3.2 Cross-lingual mapping Cross-lingual mapping methods take independently trained word embeddings in two languages, and learn a linear transformation to map them to a shared cross-lingual space (Mikolov et al., 2013a; Artetxe et al., 2018a). Most mapping methods are supervised, and rely on a bilingual dictionary, typically in the range of a few thousand entries, although a recent line of work has managed to achieve comparable results in a fully unsupervised manner based on either self-learning (Artetxe et al., 2017, 2018b) or adversarial training (Zhang et al., 2017a,b; Conneau et al., 2018). In our case, we use the method of Artetxe et al. 3634 (2018b) to map the n-gram embeddings to a shared cross-lingual space using their open source implementation VecMap1 . Originally designed for word embeddings, this method builds an initial mapping by connecting the intra-lingual similarity distribution of embeddings in different languages, and iteratively improves this solution through selflearning. The method applies a frequency-based vocabulary cut-off, learning the mapping over the 20,000 most frequent words in each language. We kept this cut-off to learn the"
D18-1399,N15-1176,0,0.0470213,"Missing"
E09-1005,agirre-soroa-2008-using,1,0.891153,"awbacks of these approaches stems from the fact that senses are compared in a pairwise fashion and thus the number of computations can grow exponentially with the number of words. Although alternatives like simulated annealing (Cowie et al., 1992) and conceptual density (Agirre and Rigau, 1996) were tried, most of past knowledge based WSD was done in a suboptimal word-by-word process, i.e., disambiguating words one at a time. Recently, graph-based methods for knowledgebased WSD have gained much attention in the NLP community (Sinha and Mihalcea, 2007; Navigli and Lapata, 2007; Mihalcea, 2005; Agirre and Soroa, 2008). These methods use well-known graph-based techniques to find and exploit the structural properties of the graph underlying a particular LKB. Because the graph is analyzed as a whole, these techniques have the remarkable property of being able to find globally optimal solutions, given the relations between entities. Graphbased WSD methods are particularly suited for disambiguating word sequences, and they manage to exploit the interrelations among the senses in the given context. In this sense, they provide a principled solution to the exponential explosion problem, with excellent performance."
E09-1005,atserias-etal-2004-spanish,0,0.0259038,"uced, and hence the rank of node j increases. Besides, the strength of the vote from i to j also depends on the rank of node i: the more important node i is, the more strength its votes will have. Alternatively, PageRank can also be viewed as the result of a random walk process, where the final rank of node i represents the probability of a random walk over the graph ending on node i, at a sufficiently large time. Let G be a graph with N vertices v1 , . . . , vN and di be the outdegree of node i; let M be a 2 (1) http://ixa2.si.ehu.es/ukb 34 • MCR16 + Xwn: The Multilingual Central Repository (Atserias et al., 2004b) is a lexical knowledge base built within the MEANING project3 . This LKB comprises the original WordNet 1.6 synsets and relations, plus some relations from other WordNet versions automatically mapped4 into version 1.6: WordNet 2.0 relations and eXtended WordNet relations (Mihalcea and Moldovan, 2001) (gold, silver and normal relations). The resulting graph has 99, 632 vertices and 637, 290 relations. damping factors. Some preliminary experiments with higher iteration counts showed that although sometimes the node ranks varied, the relative order among particular word synsets remained stable"
E09-1005,H92-1046,0,0.381008,"asque Country Donostia, Basque Contry {e.agirre,a.soroa}@ehu.es Abstract Traditional knowledge-based WSD systems assign a sense to an ambiguous word by comparing each of its senses with those of the surrounding context. Typically, some semantic similarity metric is used for calculating the relatedness among senses (Lesk, 1986; McCarthy et al., 2004). One of the major drawbacks of these approaches stems from the fact that senses are compared in a pairwise fashion and thus the number of computations can grow exponentially with the number of words. Although alternatives like simulated annealing (Cowie et al., 1992) and conceptual density (Agirre and Rigau, 1996) were tried, most of past knowledge based WSD was done in a suboptimal word-by-word process, i.e., disambiguating words one at a time. Recently, graph-based methods for knowledgebased WSD have gained much attention in the NLP community (Sinha and Mihalcea, 2007; Navigli and Lapata, 2007; Mihalcea, 2005; Agirre and Soroa, 2008). These methods use well-known graph-based techniques to find and exploit the structural properties of the graph underlying a particular LKB. Because the graph is analyzed as a whole, these techniques have the remarkable pro"
E09-1005,P00-1064,0,0.00675098,"ded WordNet. Here the differences are in many cases significant. These results are surprising, as we would expect that the manually disambiguated gloss relations from WordNet 3.0 would lead to better results, compared to the automatically disambiguated gloss relations from the eXtended WordNet (linked to version 1.7). The lower performance of WNet30+gloss can be due to the fact that the Senseval all words data set is tagged using WordNet 1.7 synsets. When using a different LKB for WSD, a mapping to WordNet 1.7 is required. Although the mapping is cited as having a correctness on the high 90s (Daude et al., 2000), it could have introduced sufficient noise to counteract the benefits of the hand-disambiguated glosses. Table 1 also shows the most frequent sense (MFS), as well as the best supervised systems (Snyder and Palmer, 2004; Palmer et al., 2001) that participated in each competition (SMUaw and GAMBL, respectively). The MFS is a baseline for supervised systems, but it is consid5 Comparison to Related work In this section we will briefly describe some graph-based methods for knowledge-based WSD. The methods here presented cope with the problem of sequence-labeling, i.e., they disambiguate all the wo"
E09-1005,S07-1008,0,0.0363883,"Missing"
E09-1005,P04-1036,0,0.72456,"Missing"
E09-1005,C96-1005,1,0.319104,"rre,a.soroa}@ehu.es Abstract Traditional knowledge-based WSD systems assign a sense to an ambiguous word by comparing each of its senses with those of the surrounding context. Typically, some semantic similarity metric is used for calculating the relatedness among senses (Lesk, 1986; McCarthy et al., 2004). One of the major drawbacks of these approaches stems from the fact that senses are compared in a pairwise fashion and thus the number of computations can grow exponentially with the number of words. Although alternatives like simulated annealing (Cowie et al., 1992) and conceptual density (Agirre and Rigau, 1996) were tried, most of past knowledge based WSD was done in a suboptimal word-by-word process, i.e., disambiguating words one at a time. Recently, graph-based methods for knowledgebased WSD have gained much attention in the NLP community (Sinha and Mihalcea, 2007; Navigli and Lapata, 2007; Mihalcea, 2005; Agirre and Soroa, 2008). These methods use well-known graph-based techniques to find and exploit the structural properties of the graph underlying a particular LKB. Because the graph is analyzed as a whole, these techniques have the remarkable property of being able to find globally optimal sol"
E09-1005,H05-1052,0,0.855302,"of the major drawbacks of these approaches stems from the fact that senses are compared in a pairwise fashion and thus the number of computations can grow exponentially with the number of words. Although alternatives like simulated annealing (Cowie et al., 1992) and conceptual density (Agirre and Rigau, 1996) were tried, most of past knowledge based WSD was done in a suboptimal word-by-word process, i.e., disambiguating words one at a time. Recently, graph-based methods for knowledgebased WSD have gained much attention in the NLP community (Sinha and Mihalcea, 2007; Navigli and Lapata, 2007; Mihalcea, 2005; Agirre and Soroa, 2008). These methods use well-known graph-based techniques to find and exploit the structural properties of the graph underlying a particular LKB. Because the graph is analyzed as a whole, these techniques have the remarkable property of being able to find globally optimal solutions, given the relations between entities. Graphbased WSD methods are particularly suited for disambiguating word sequences, and they manage to exploit the interrelations among the senses in the given context. In this sense, they provide a principled solution to the exponential explosion problem, wi"
E09-1005,S01-1005,0,0.0476183,"ull graph of the LKB efficiently, performing better than previous approaches in English all-words datasets. We also show that the algorithm can be easily ported to other languages with good results, with the only requirement of having a wordnet. In addition, we make an analysis of the performance of the algorithm, showing that it is efficient and that it could be tuned to be faster. 1 Introduction Word Sense Disambiguation (WSD) is a key enabling-technology that automatically chooses the intended sense of a word in context. Supervised WSD systems are the best performing in public evaluations (Palmer et al., 2001; Snyder and Palmer, 2004; Pradhan et al., 2007) but they need large amounts of hand-tagged data, which is typically very expensive to build. Given the relatively small amount of training data available, current state-of-the-art systems only beat the simple most frequent sense (MFS) baseline1 by a small margin. As an alternative to supervised systems, knowledge-based WSD systems exploit the information present in a lexical knowledge base (LKB) to perform WSD, without using any further corpus evidence. 1 This baseline consists of tagging all occurrences in the test data with the sense of the wo"
E09-1005,S07-1016,0,0.00893313,"Missing"
E09-1005,W04-0811,0,0.653325,"efficiently, performing better than previous approaches in English all-words datasets. We also show that the algorithm can be easily ported to other languages with good results, with the only requirement of having a wordnet. In addition, we make an analysis of the performance of the algorithm, showing that it is efficient and that it could be tuned to be faster. 1 Introduction Word Sense Disambiguation (WSD) is a key enabling-technology that automatically chooses the intended sense of a word in context. Supervised WSD systems are the best performing in public evaluations (Palmer et al., 2001; Snyder and Palmer, 2004; Pradhan et al., 2007) but they need large amounts of hand-tagged data, which is typically very expensive to build. Given the relatively small amount of training data available, current state-of-the-art systems only beat the simple most frequent sense (MFS) baseline1 by a small margin. As an alternative to supervised systems, knowledge-based WSD systems exploit the information present in a lexical knowledge base (LKB) to perform WSD, without using any further corpus evidence. 1 This baseline consists of tagging all occurrences in the test data with the sense of the word that occurs more often"
E09-1005,C92-1056,0,\N,Missing
E09-1006,S07-1074,1,0.660957,"Missing"
E09-1006,C08-1003,1,0.224135,"Missing"
E09-1006,martinez-agirre-2004-effect,1,0.888428,"Missing"
E09-1006,W06-2911,0,0.0482562,"Missing"
E09-1006,W06-1615,0,0.157599,"d unlabeled data. Gliozzo et al. (2005) used SVD to reduce the space of the term-to-document matrix, and then computed the similarity between train and test 43 instances using a mapping to the reduced space (similar to our SMA method in Section 4.2). They combined other knowledge sources into a complex kernel using SVM. They report improved performance on a number of languages in the Senseval3 lexical sample dataset. Our present paper differs from theirs in that we propose an additional method to use SVD (the OMT method), and that we focus on domain adaptation. In the semi-supervised setting, Blitzer et al. (2006) used Structural Correspondence Learning and unlabeled data to adapt a Part-of-Speech tagger. They carefully select so-called ‘pivot features’ to learn linear predictors, perform SVD on the weights learned by the predictor, and thus learn correspondences among features in both source and target domains. Our technique also uses SVD, but we directly apply it to all features, and thus avoid the need to define pivot features. In preliminary work we unsuccessfully tried to carry along the idea of pivot features to WSD. On the contrary, in (Agirre and Lopez de Lacalle, 2008) we show that methods clo"
E09-1006,P07-1007,0,0.210493,"Missing"
E09-1006,rose-etal-2002-reuters,0,0.0250512,"e content words in a ±4-word window around the target. Syntactic dependencies use the object, subject, noun-modifier, preposition, and sibling lemmas, when available. Finally, Bag-of-words features are the lemmas of the content words in the whole context, plus the salient bigrams in the context (Pedersen, 2001). We refer to these features as original features. Data sets The dataset we use was designed for domainrelated WSD experiments by Koeling et al. (2005), and is publicly available. The examples come from the B NC (Leech, 1992) and the S PORTS and F INANCES sections of the Reuters corpus (Rose et al., 2002), comprising around 300 examples (roughly 100 from each of those corpora) for each of the 41 nouns. The nouns were selected because they were salient in either the S PORTS or F INANCES domains, or because they had senses linked to those domains. The occurrences were hand-tagged with the senses from WordNet (W N) version 1.7.1 (Fellbaum, 1998). In our experi4.2 SVD features Apart from the original space of features, we have used the so called SVD features, obtained from the projection of the feature vectors into the reduced space (Deerwester et al., 1990). Basically, 44 we set a term-by-documen"
E09-1006,W04-3237,0,0.150619,"Missing"
E09-1006,P07-1033,0,0.222255,"Missing"
E09-1006,D08-1105,0,0.23355,"Missing"
E09-1006,W00-1322,0,0.189941,"Missing"
E09-1006,H05-1053,0,0.291205,"n adaptation. 3 ments the B NC examples play the role of general source corpora, and the F INANCES and S PORTS examples the role of two specific domain target corpora. Compared to the DSO corpus used in prior work (cf. Section 2) this corpus has been explicitly created for domain adaptation studies. DSO contains texts coming from the Brown corpus and the Wall Street Journal, but the texts are not classified according to specific domains (e.g. Sports, Finances), which make DSO less suitable to study domain adaptation. The fact that the selected nouns are related to the target domain makes the (Koeling et al., 2005) corpus more demanding than the DSO corpus, because one would expect the performance of a generic WSD system to drop when moving to the domain corpus for domainrelated words (cf. Table 1), while the performance would be similar for generic words. In addition to the labeled data, we also use unlabeled data coming from the three sources used in the labeled corpus: the ’written’ part of the B NC (89.7M words), the F INANCES part of Reuters (32.5M words), and the S PORTS part (9.1M words). 4 Original and SVD features In this section, we review the features and two methods to apply SVD over the fea"
E09-1006,N01-1011,0,0.033876,"res We relied on the usual features used in previous WSD work, grouped in three main sets. Local collocations comprise the bigrams and trigrams formed around the target word (using either lemmas, word-forms, or PoS tags) , those formed with the previous/posterior lemma/word-form in the sentence, and the content words in a ±4-word window around the target. Syntactic dependencies use the object, subject, noun-modifier, preposition, and sibling lemmas, when available. Finally, Bag-of-words features are the lemmas of the content words in the whole context, plus the salient bigrams in the context (Pedersen, 2001). We refer to these features as original features. Data sets The dataset we use was designed for domainrelated WSD experiments by Koeling et al. (2005), and is publicly available. The examples come from the B NC (Leech, 1992) and the S PORTS and F INANCES sections of the Reuters corpus (Rose et al., 2002), comprising around 300 examples (roughly 100 from each of those corpora) for each of the 41 nouns. The nouns were selected because they were salient in either the S PORTS or F INANCES domains, or because they had senses linked to those domains. The occurrences were hand-tagged with the senses"
E09-1006,S07-1016,0,0.0227411,"led corpora, (ii) build the term-bydocument matrix, (iii) decompose it with SVD, and (iv) map the labeled data (train/test). This technique is very similar to previous work on SVD (Gliozzo et al., 2005; Zelikovitz and Hirsh, 2001). The dimensionality reduction is performed once, over the whole unlabeled corpus, and it is then applied to the labeled data of each word. The reduced space is constructed only with terms, which correspond to bag-of-words features, and thus discards the rest of the features. Given that the WSD literature shows that all features are necessary for optimal performance (Pradhan et al., 2007), we propose the following alternative to construct the matrix. 5 Learning methods k- NN is a memory based learning method, where the neighbors are the k most similar labeled examples to the test example. The similarity among instances is measured by the cosine of their vectors. The test instance is labeled with the sense obtaining the maximum sum of the weighted vote of the k most similar contexts. We set k to 5 based on previous results published in (Agirre and Lopez de Lacalle, 2007). Regarding SVM, we used linear kernels, but also purpose-built kernels for the reduced spaces and the combin"
E09-1006,W00-1326,1,\N,Missing
E09-1006,P05-1050,0,\N,Missing
I11-1175,N09-1003,1,0.908636,"09). The work presented here is complementary, in that we explore QE, but we use an approach based on semantic relatedness instead of distributional methods. In a closely related work, (Agirre et al., 2010) proposed a WordNet-based document expansion method using random walks: given a document, a random walk algorithm over the WordNet graph, In order to test the performance of our method we selected several datasets with different domains, topic typologies and document lengths. Given the relevance among the community using WordNet-related methods, we selected the Robust-WSD dataset from CLEF (Agirre et al., 2009a), which is a typical ad-hoc dataset on news. As we think that our method is specially relevant for short queries and/or short documents, we also selected the Yahoo! Answers dataset, which contains questions and answers as phrased by real users on diverse topics (Surdeanu et al., 2008), and ResPubliQA, a paragraph retrieval task on European Union laws organized at CLEF (Pe˜nas et al., 2009). The results show that our method provide improvements in all three datasets, when compared to the query likelihood baseline, and that they compare favorably to pseudo-relevance feedback in two datasets. T"
I11-1175,C10-2002,1,0.849963,"ocument. The use of different words creates a lexical gap between the query and the document. In order to bridge the gap, IR has resorted to distributional semantic models. Most research concentrated on Query Expansion (QE) methods, which typically analyze term co-occurrence statistics in the corpus and/or in the highest scored documents in order to select terms for expanding the query terms (Manning et al., 2009). The work presented here is complementary, in that we explore QE, but we use an approach based on semantic relatedness instead of distributional methods. In a closely related work, (Agirre et al., 2010) proposed a WordNet-based document expansion method using random walks: given a document, a random walk algorithm over the WordNet graph, In order to test the performance of our method we selected several datasets with different domains, topic typologies and document lengths. Given the relevance among the community using WordNet-related methods, we selected the Robust-WSD dataset from CLEF (Agirre et al., 2009a), which is a typical ad-hoc dataset on news. As we think that our method is specially relevant for short queries and/or short documents, we also selected the Yahoo! Answers dataset, whi"
I11-1175,D07-1061,0,0.0330372,"expand documents, following the BM25 probabilistic method for IR, obtaining some improvements, specially when parameters had not been optimized. In contrast to their work, we investigate methods to apply relatedness to query expansion, and we compare the results with pseudo-relevance feedback. Besides, we found that a language modeling (Ponte and Croft, 1998) approach to IR combined with inference networks (Turtle and Croft, 1991) offered more flexibility for query expansion. Our work stems from the use of random walks over the WordNet graph to compute the relatedness between pairs of words (Hughes and Ramage, 2007). In this work a single word was input to the random walk algorithm, obtaining the probability distribution over all WordNet synsets. The similarity of two words was computed as the similarity of the distributions of each word. In later work, (Agirre et al., 2009b) tested different configurations of the graph, and obtained the best results for a WordNet-based system, comparable to the results of a distributional similarity method which used a crawl of the entire web. The same authors later released their UKB software, which is the one we use here. 3 Relatedness-based Query Expansion (RQE) The"
I11-1175,P08-1082,0,0.0539607,"Missing"
J13-3006,P08-1037,1,0.925043,"Missing"
J13-3006,P11-2123,1,0.853466,"hen test item Texas in Example (6) is to be labeled, the higher similarity to Dallas and New York, in contrast to the lower similarity to November and winter, would be used to label the argument with the Location role. The automatic acquisition of selectional preferences is a well-studied topic in NLP. Many methods using semantic classes and selectional preferences have been proposed and applied to a variety of syntactic–semantic ambiguity problems, including syntactic parsing (Hindle 1990; Resnik 1993b; Pantel and Lin 2000; Agirre, Baldwin, and Martinez 2008; Koo, Carreras, and Collins 2008; Agirre et al. 2011), word sense disambiguation (Resnik 1993a; Agirre and Martinez 2001; McCarthy and Carroll 2003), pronoun resolution (Bergsma, Lin, and Goebel 2008) and named-entity recognition (Ratinov and Roth 2009). In addition, selectional preferences have been shown to be effective to improve the quality of inference and information extraction rules (Pantel et al. 2007; Ritter, Mausam, and Etzioni 2010). In some cases, the aforementioned papers do not mention selectional preferences, but all of them use some notion of preferring certain semantic types over others in order to accomplish their respective ta"
J13-3006,W01-0703,1,0.790514,"Missing"
J13-3006,C96-1005,1,0.358301,"are usually more useful for characterizing selectional preferences, as in the <tool> class for the instrument role of break. The priority of using specific synsets over more general ones is, thus, justified in the sense that they may better represent the most relevant semantic characteristics of the selectional preferences. The alternative method (SPwn ) is based on the depth of the concepts in the WordNet hierarchy and the frequency of the nouns. The use of the depth in hierarchies to model the specificity of concepts (the deeper the more specific) is not new (Rada et al. 1989; Sussna 1993; Agirre and Rigau 1996). Our method tries to be conservative with respect to generalization: When we check which SP is a better fit for a given target head, we always prefer the SP that contains the most specific generalization for the target head (the lowest synset which is a hypernym of the target word). 641 Computational Linguistics Volume 39, Number 3 Table 4 Excerpt from the selectional preferences for write-Arg0 according to SPwn , showing from deeper to shallower the synsets in WordNet which are connected to head words in Table 1. Depth lists the depth of synsets in WordNet. Description includes the words and"
J13-3006,J10-4006,0,0.243113,"on of Resnik (1993b) in order to be coherent with the formulae presented in this paper. 636 Zapirain et al. Selectional Preferences for Semantic Role Classification The models return weights for (verb, syntactic function, noun) triples, and correlation with human plausibility judgment is used for evaluation. Resnik’s selectional preference scored best among WordNet-based methods (Li and Abe 1998; Clark and Weir 2002). Despite its earlier publication, Resnik’s method is still the most popular ´ Pado, ´ and Erk 2007; Erk, Pado, ´ representative among WordNet-based methods (Pado, and Pado´ 2010; Baroni and Lenci 2010). We also chose to use Resnik’s model in this paper. One of the disadvantages of the WordNet-based models, compared with the distributional similarity models, is that they require that the heads are present in WordNet. This limitation can negatively influence the coverage of the model, and also its generalization ability. 2.2 Distributional Similarity Models Distributional similarity models assume that a word is characterized by the words it co-occurs with. In the simplest model, co-occurring words are taken from a fixed-size context window. Each word w would be represented by the set of words"
J13-3006,D08-1007,0,0.0621817,"Missing"
J13-3006,boas-2002-bilingual,0,0.013958,"e predicates. For instance, consider the following sentence, in which the arguments of the predicate to send have been annotated with their respective semantic roles.1 (1) [Mr. Smith]Agent sent [the report]Object [to me]Recipient [this morning]Temporal . Recognizing these event structures has been shown to be important for a broad spectrum of NLP applications. Information extraction, summarization, question answering, machine translation, among others, can benefit from this shallow semantic analysis at sentence level, which opens the door for exploiting the semantic relations among arguments (Boas 2002; Surdeanu et al. 2003; Narayanan and Harabagiu 2004; Melli et al. 2005; Moschitti et al. 2007; Higashinaka and Isozaki 2008; Surdeanu, Ciaramita, and Zaragoza 2011). In M`arquez et al. (2008) the reader can find a broad introduction to SRL, covering several historical and definitional aspects of the problem, including also references to the main resources and systems. State-of-the-art systems leverage existing hand-tagged corpora (Fillmore, Ruppenhofer, and Baker 2004; Palmer, Gildea, and Kingsbury 2005) to learn supervised machine learning systems, and typically perform SRL in two sequential"
J13-3006,E03-1034,0,0.0636171,"P(c0 ) R(p, r) (3) The numerator formalizes the goodness of fit for the best semantic class c0 that contains w0 . The hypernym (i.e., superclass) of w0 yielding the maximum value is chosen. The denominator models how restrictive the selectional preference is for p and r, as modeled in Equation (1). Variations of Resnik’s idea to find a suitable level of generalization have been explored in later years. Li and Abe (1998) applied the minimum-description length principle. Alternatively, Clark and Weir (2002) devised a procedure to decide when a class should be preferred rather than its children. Brockmann and Lapata (2003) compared several class-based models (including Resnik’s selectional preferences) on a syntactic plausibility judgment task for German. 3 We slightly modified the notation of Resnik (1993b) in order to be coherent with the formulae presented in this paper. 636 Zapirain et al. Selectional Preferences for Semantic Role Classification The models return weights for (verb, syntactic function, noun) triples, and correlation with human plausibility judgment is used for evaluation. Resnik’s selectional preference scored best among WordNet-based methods (Li and Abe 1998; Clark and Weir 2002). Despite i"
J13-3006,W04-2412,1,0.876656,"Missing"
J13-3006,W05-0620,1,0.896889,"Missing"
J13-3006,N10-1058,1,0.848114,"esnik’s model tends to always predict the most frequent roles whereas our model covers a wider role selection. Resnik’s tendency to overgeneralize makes more frequent roles cover all the vocabulary, and the weighting system penalizes roles with fewer occurrences. 12 We verified that the lexical model shows higher classification accuracy than all the more elaborate SP models on the subset of cases covered by both the lexical and the SP models. In this situation, if we aimed at constructing the best role classifier with SPs alone we could devise a back-off strategy, in the style of Chambers and Jurafsky (2010), which uses the predictions of the lexical model when present and one of the SP models if not. As presented in Section 5, however, our main goal is to integrate these SP models in a real end-to-end SRL system, so we keep their analysis as independent predictors for the moment. 650 Zapirain et al. Selectional Preferences for Semantic Role Classification The results for distributional models indicate that the SPs using Lin’s ready-made pre thesaurus (simLin ) outperforms Pado´ and Lapata’s distributional similarity model (Pado´ and Lapata 2007) calculated over the BNC (simLin ) in both Tables 1"
J13-3006,A00-2018,0,0.119058,"state-of-the-art semantic role labeling system (Surdeanu et al. 2007). SwiRL ranked second among the systems that did not implement model combination at the CoNLL-2005 shared task and fifth overall (Carreras and M`arquez 2005). Because the focus of this section is on role classification, we modified the SRC component of SwiRL to use gold argument boundaries, that is, we assume that semantic role identification works perfectly. Nevertheless, for a realistic evaluation, all the features in the role classification model are generated using actual syntactic trees generated by the Charniak parser (Charniak 2000). The key idea behind our approach is model combination: We generate a battery of base models using all resources available and we combine their outputs using multiple strategies. Our pool of base models contains 13 different models: The first is the 13 The data sets used for the experiments reported in this section are exactly the ones described in Section 4.1. 651 Computational Linguistics Volume 39, Number 3 unmodified SwiRL SRC, the next six are the selected SP models from the previous section, and the last six are variants of SwiRL SRC. In each variant, the feature set of the unmodified S"
J13-3006,J02-2003,0,0.0199537,"s SPRes (p, r, w0 ), is formulated as follows:3 SPRes (p, r, w0 ) = max c0 ∈hyp(w0 ) P(c0 |p, r)log P(c0 |p,r) P(c0 ) R(p, r) (3) The numerator formalizes the goodness of fit for the best semantic class c0 that contains w0 . The hypernym (i.e., superclass) of w0 yielding the maximum value is chosen. The denominator models how restrictive the selectional preference is for p and r, as modeled in Equation (1). Variations of Resnik’s idea to find a suitable level of generalization have been explored in later years. Li and Abe (1998) applied the minimum-description length principle. Alternatively, Clark and Weir (2002) devised a procedure to decide when a class should be preferred rather than its children. Brockmann and Lapata (2003) compared several class-based models (including Resnik’s selectional preferences) on a syntactic plausibility judgment task for German. 3 We slightly modified the notation of Resnik (1993b) in order to be coherent with the formulae presented in this paper. 636 Zapirain et al. Selectional Preferences for Semantic Role Classification The models return weights for (verb, syntactic function, noun) triples, and correlation with human plausibility judgment is used for evaluation. Resn"
J13-3006,P97-1067,0,0.0336605,"ntains John loves Mary, then the pair (ncsubj, love) would be in the set T for John. The measure uses information-theoretic principles, and I(w, d, v) represents the information content of the triple (Lin 1998). Although the use of co-occurrence vectors for words to compute similarity has been ¨ standard practice, some authors have argued for more complex uses. Schutze (1998) builds vectors for each context of occurrence of a word, combining the co-occurrence vectors for each word in the context. The vectors for contexts were used to induce senses and to improve information retrieval results. Edmonds (1997) built a lexical cooccurrence network, and applied it to a lexical choice task. Chakraborti et al. (2007) used transitivity over co-occurrence relations, with good results on several classification tasks. Note that all these works use second order and higher order to refer to their method. In this paper, we will also use second order to refer to a new method which goes beyond the usual co-occurrence vectors (cf. Section 3.3). A full review of distributional models is out of the scope of this paper, as we are interested in showing that some of those models can be used successfully to improve SR"
J13-3006,P07-1028,0,0.76274,"tion could be more fine-grained, as defined by the WordNet hierarchy (Resnik 1993b; Agirre and Martinez 2001; McCarthy and Carroll 2003), and other lexical resources could be used as well. Other authors have used automatically induced hierarchical word classes, clustered according to occurrence information from corpora (Koo, Carreras, and Collins 2008; Ratinov and Roth 2009). On the other extreme, each word would be its own semantic class, as in the lexical model, but one could also model selectional preference using distributional similarity (Grefenstette 1992; Lin 1998; Pantel and Lin 2000; Erk 2007; Bergsma, Lin, and Goebel 2008). In this paper we will focus on WordNet-based models that use the whole hierarchy and on distributional similarity models, and we will use the lexical model as baseline. 2.1 WordNet-Based Models Resnik (1993b) proposed the modeling of selectional preferences using semantic classes from WordNet and applied the model to tackle some ambiguity issues in syntax, such as noun-compounds, coordination, and prepositional phrase attachment. Given two alternative structures, Resnik used selectional preferences to choose the attachment maximizing the fitness of the head to"
J13-3006,J10-4007,0,0.0471056,"Missing"
J13-3006,J02-3001,0,0.690118,"yntactic ambiguity. For instance, Pantel and Lin (2000) obtained very good results on PP-attachment using the distributional similarity measure defined by Lin (1998). Distributional similarity was used to overcome sparsity problems: Alongside the counts in the training data of the target words, the counts of words similar to the target ones were also used. Although not made explicit, Lin was actually using a distributional similarity model of selectional preferences. The application of distributional selectional preferences to semantic roles (as opposed to syntactic functions) is more recent. Gildea and Jurafsky (2002) are the only ones applying selectional preferences in a real SRL task. They used distributional clustering and WordNet-based techniques on a SRL task on FrameNet roles. They report a very small improvement of the overall performance when using distributional clustering techniques. In this paper we present complementary experiments, with a different role set and annotated corpus (PropBank), a wider range of selectional preference models, and the analysis of out-of-domain results. Other papers applying semantic preferences in the context of semantic roles rely on the evaluation of artificial ta"
J13-3006,P92-1052,0,0.05076,"oth training examples for Temporal (i.e., November and winter), and <geographical area> covers the examples for Location. When test words Texas and December occur in Examples (6) and (7), the semantic classes to which they belong can be used to tag the first as Location and the second as Temporal. As an alternative to the use of WordNet, one can also apply automatically acquired distributional similarity thesauri. Distributional similarity methods analyze the cooccurrence patterns of words and are able to capture, for instance, that December is more closely related to November than to Dallas (Grefenstette 1992). Distributional similarity is typically used on-line (i.e., given a pair of words, their similarity is computed on the go), 634 Zapirain et al. Selectional Preferences for Semantic Role Classification but, in order to speed up its use, it has also been used to produce off-line a full thesauri, storing, for every word, the weighted list of all outstanding similar words (Lin 1998). In the Distributional similarity model, when test item Texas in Example (6) is to be labeled, the higher similarity to Dallas and New York, in contrast to the lower similarity to November and winter, would be used to"
J13-3006,I08-1055,0,0.0292023,"end have been annotated with their respective semantic roles.1 (1) [Mr. Smith]Agent sent [the report]Object [to me]Recipient [this morning]Temporal . Recognizing these event structures has been shown to be important for a broad spectrum of NLP applications. Information extraction, summarization, question answering, machine translation, among others, can benefit from this shallow semantic analysis at sentence level, which opens the door for exploiting the semantic relations among arguments (Boas 2002; Surdeanu et al. 2003; Narayanan and Harabagiu 2004; Melli et al. 2005; Moschitti et al. 2007; Higashinaka and Isozaki 2008; Surdeanu, Ciaramita, and Zaragoza 2011). In M`arquez et al. (2008) the reader can find a broad introduction to SRL, covering several historical and definitional aspects of the problem, including also references to the main resources and systems. State-of-the-art systems leverage existing hand-tagged corpora (Fillmore, Ruppenhofer, and Baker 2004; Palmer, Gildea, and Kingsbury 2005) to learn supervised machine learning systems, and typically perform SRL in two sequential steps: argument identification and argument classification. Whereas the former is mostly a syntactic recognition task, the"
J13-3006,P90-1034,0,0.513268,"every word, the weighted list of all outstanding similar words (Lin 1998). In the Distributional similarity model, when test item Texas in Example (6) is to be labeled, the higher similarity to Dallas and New York, in contrast to the lower similarity to November and winter, would be used to label the argument with the Location role. The automatic acquisition of selectional preferences is a well-studied topic in NLP. Many methods using semantic classes and selectional preferences have been proposed and applied to a variety of syntactic–semantic ambiguity problems, including syntactic parsing (Hindle 1990; Resnik 1993b; Pantel and Lin 2000; Agirre, Baldwin, and Martinez 2008; Koo, Carreras, and Collins 2008; Agirre et al. 2011), word sense disambiguation (Resnik 1993a; Agirre and Martinez 2001; McCarthy and Carroll 2003), pronoun resolution (Bergsma, Lin, and Goebel 2008) and named-entity recognition (Ratinov and Roth 2009). In addition, selectional preferences have been shown to be effective to improve the quality of inference and information extraction rules (Pantel et al. 2007; Ritter, Mausam, and Etzioni 2010). In some cases, the aforementioned papers do not mention selectional preferences"
J13-3006,P08-1068,0,0.105046,"Missing"
J13-3006,P99-1004,0,0.246709,"at co-occur with it, T(w). In a more elaborate model, each word w would be represented as a vector   i (w) corresponds to the weight of the ith word in with weights, where T of words T(w) the vector. The weights can be calculated following a simple frequency of co-occurrence, or using some other formula. Then, given two words w and w0 , their similarity can be computed using any similarity measure between their co-occurrence sets or vectors. For instance, early work by Grefenstette (1992) used the Jaccard similarity coefficient of the two sets T(w) and T(w0 ) (cf. Equation (4) in Figure 1). Lee (1999) reviews a wide range of similarity functions,   0 ) (cf. Equation (5) and T(w including Jaccard and the cosine between two vectors T(w) in Figure 1). In the context of lexical semantics, the similarity measure defined by Lin (1998) has been very successful. This measure (cf. Equation (6) in Figure 1) takes into account syntactic dependencies (d) in its co-occurrence model. In this case, the set T(w) of cooccurrences of w contains pairs (d,v) of dependencies and words, representing the fact simJac (w, w0 ) = simcos (w, w0 ) =   simLin (w, w0 ) =  |T(w) ∩ T(w0 )| |T(w) ∪ T(w0 )| n"
J13-3006,J98-2002,0,0.0537239,"erence of a predicate p and role r for a head w0 of any potential argument, noted as SPRes (p, r, w0 ), is formulated as follows:3 SPRes (p, r, w0 ) = max c0 ∈hyp(w0 ) P(c0 |p, r)log P(c0 |p,r) P(c0 ) R(p, r) (3) The numerator formalizes the goodness of fit for the best semantic class c0 that contains w0 . The hypernym (i.e., superclass) of w0 yielding the maximum value is chosen. The denominator models how restrictive the selectional preference is for p and r, as modeled in Equation (1). Variations of Resnik’s idea to find a suitable level of generalization have been explored in later years. Li and Abe (1998) applied the minimum-description length principle. Alternatively, Clark and Weir (2002) devised a procedure to decide when a class should be preferred rather than its children. Brockmann and Lapata (2003) compared several class-based models (including Resnik’s selectional preferences) on a syntactic plausibility judgment task for German. 3 We slightly modified the notation of Resnik (1993b) in order to be coherent with the formulae presented in this paper. 636 Zapirain et al. Selectional Preferences for Semantic Role Classification The models return weights for (verb, syntactic function, noun)"
J13-3006,P98-2127,0,0.198549,"ional similarity thesauri. Distributional similarity methods analyze the cooccurrence patterns of words and are able to capture, for instance, that December is more closely related to November than to Dallas (Grefenstette 1992). Distributional similarity is typically used on-line (i.e., given a pair of words, their similarity is computed on the go), 634 Zapirain et al. Selectional Preferences for Semantic Role Classification but, in order to speed up its use, it has also been used to produce off-line a full thesauri, storing, for every word, the weighted list of all outstanding similar words (Lin 1998). In the Distributional similarity model, when test item Texas in Example (6) is to be labeled, the higher similarity to Dallas and New York, in contrast to the lower similarity to November and winter, would be used to label the argument with the Location role. The automatic acquisition of selectional preferences is a well-studied topic in NLP. Many methods using semantic classes and selectional preferences have been proposed and applied to a variety of syntactic–semantic ambiguity problems, including syntactic parsing (Hindle 1990; Resnik 1993b; Pantel and Lin 2000; Agirre, Baldwin, and Marti"
J13-3006,S07-1005,0,0.346404,"Missing"
J13-3006,W06-2106,0,0.177585,"Missing"
J13-3006,H94-1020,0,0.407013,"Missing"
J13-3006,J08-2001,1,0.891986,"Missing"
J13-3006,J03-4004,0,0.257012,"New York, in contrast to the lower similarity to November and winter, would be used to label the argument with the Location role. The automatic acquisition of selectional preferences is a well-studied topic in NLP. Many methods using semantic classes and selectional preferences have been proposed and applied to a variety of syntactic–semantic ambiguity problems, including syntactic parsing (Hindle 1990; Resnik 1993b; Pantel and Lin 2000; Agirre, Baldwin, and Martinez 2008; Koo, Carreras, and Collins 2008; Agirre et al. 2011), word sense disambiguation (Resnik 1993a; Agirre and Martinez 2001; McCarthy and Carroll 2003), pronoun resolution (Bergsma, Lin, and Goebel 2008) and named-entity recognition (Ratinov and Roth 2009). In addition, selectional preferences have been shown to be effective to improve the quality of inference and information extraction rules (Pantel et al. 2007; Ritter, Mausam, and Etzioni 2010). In some cases, the aforementioned papers do not mention selectional preferences, but all of them use some notion of preferring certain semantic types over others in order to accomplish their respective task. In fact, one could use different notions of semantic types. In one extreme, we would have a"
J13-3006,P07-1098,0,0.0545287,"s of the predicate to send have been annotated with their respective semantic roles.1 (1) [Mr. Smith]Agent sent [the report]Object [to me]Recipient [this morning]Temporal . Recognizing these event structures has been shown to be important for a broad spectrum of NLP applications. Information extraction, summarization, question answering, machine translation, among others, can benefit from this shallow semantic analysis at sentence level, which opens the door for exploiting the semantic relations among arguments (Boas 2002; Surdeanu et al. 2003; Narayanan and Harabagiu 2004; Melli et al. 2005; Moschitti et al. 2007; Higashinaka and Isozaki 2008; Surdeanu, Ciaramita, and Zaragoza 2011). In M`arquez et al. (2008) the reader can find a broad introduction to SRL, covering several historical and definitional aspects of the problem, including also references to the main resources and systems. State-of-the-art systems leverage existing hand-tagged corpora (Fillmore, Ruppenhofer, and Baker 2004; Palmer, Gildea, and Kingsbury 2005) to learn supervised machine learning systems, and typically perform SRL in two sequential steps: argument identification and argument classification. Whereas the former is mostly a sy"
J13-3006,C04-1100,0,0.0643825,"der the following sentence, in which the arguments of the predicate to send have been annotated with their respective semantic roles.1 (1) [Mr. Smith]Agent sent [the report]Object [to me]Recipient [this morning]Temporal . Recognizing these event structures has been shown to be important for a broad spectrum of NLP applications. Information extraction, summarization, question answering, machine translation, among others, can benefit from this shallow semantic analysis at sentence level, which opens the door for exploiting the semantic relations among arguments (Boas 2002; Surdeanu et al. 2003; Narayanan and Harabagiu 2004; Melli et al. 2005; Moschitti et al. 2007; Higashinaka and Isozaki 2008; Surdeanu, Ciaramita, and Zaragoza 2011). In M`arquez et al. (2008) the reader can find a broad introduction to SRL, covering several historical and definitional aspects of the problem, including also references to the main resources and systems. State-of-the-art systems leverage existing hand-tagged corpora (Fillmore, Ruppenhofer, and Baker 2004; Palmer, Gildea, and Kingsbury 2005) to learn supervised machine learning systems, and typically perform SRL in two sequential steps: argument identification and argument classif"
J13-3006,P10-1045,0,0.0304033,"Missing"
J13-3006,J07-2002,0,0.165313,"Missing"
J13-3006,D07-1042,0,0.235467,"Missing"
J13-3006,J05-1004,0,0.62839,"Missing"
J13-3006,N07-1071,0,0.014092,"erences have been proposed and applied to a variety of syntactic–semantic ambiguity problems, including syntactic parsing (Hindle 1990; Resnik 1993b; Pantel and Lin 2000; Agirre, Baldwin, and Martinez 2008; Koo, Carreras, and Collins 2008; Agirre et al. 2011), word sense disambiguation (Resnik 1993a; Agirre and Martinez 2001; McCarthy and Carroll 2003), pronoun resolution (Bergsma, Lin, and Goebel 2008) and named-entity recognition (Ratinov and Roth 2009). In addition, selectional preferences have been shown to be effective to improve the quality of inference and information extraction rules (Pantel et al. 2007; Ritter, Mausam, and Etzioni 2010). In some cases, the aforementioned papers do not mention selectional preferences, but all of them use some notion of preferring certain semantic types over others in order to accomplish their respective task. In fact, one could use different notions of semantic types. In one extreme, we would have a small set of coarse semantic classes. For instance, some authors have used the 26 so-called “semantic fields” used to classify all nouns in WordNet (Agirre, Baldwin, and Martinez 2008; Agirre et al. 2011). The classification could be more fine-grained, as defined"
J13-3006,P00-1014,0,0.249246,"sparseness as one of the main reasons. In this work, we will focus on Semantic Role Classification (SRC), and we will show that selectional preferences (SP) are useful for generalizing lexical features, helping fight sparseness and domain shifts, and improving SRC results. Selectional preferences try to model the kind of words that can fill a specific argument of a predicate, and have been widely used in computational linguistics since the early days (Wilks 1975). Both semantic classes from existing lexical resources like WordNet (Resnik 1993b) and distributional similarity based on corpora (Pantel and Lin 2000) have been successfully used for acquiring selectional preferences, and in this work we have used several of those models. The contributions of this work to the field of SRL are the following: 1. We formalize and implement a method that applies several selectional preference models to Semantic Role Classification, introducing for the first time the use of selectional preferences for prepositions, in addition to selectional preferences for verbs. 2. We show that the selectional preference models are able to generalize lexical features and improve role classification performance in a controlled"
J13-3006,J08-2006,0,0.0842412,"Missing"
J13-3006,W09-1119,0,0.00868279,"th the Location role. The automatic acquisition of selectional preferences is a well-studied topic in NLP. Many methods using semantic classes and selectional preferences have been proposed and applied to a variety of syntactic–semantic ambiguity problems, including syntactic parsing (Hindle 1990; Resnik 1993b; Pantel and Lin 2000; Agirre, Baldwin, and Martinez 2008; Koo, Carreras, and Collins 2008; Agirre et al. 2011), word sense disambiguation (Resnik 1993a; Agirre and Martinez 2001; McCarthy and Carroll 2003), pronoun resolution (Bergsma, Lin, and Goebel 2008) and named-entity recognition (Ratinov and Roth 2009). In addition, selectional preferences have been shown to be effective to improve the quality of inference and information extraction rules (Pantel et al. 2007; Ritter, Mausam, and Etzioni 2010). In some cases, the aforementioned papers do not mention selectional preferences, but all of them use some notion of preferring certain semantic types over others in order to accomplish their respective task. In fact, one could use different notions of semantic types. In one extreme, we would have a small set of coarse semantic classes. For instance, some authors have used the 26 so-called “semantic fi"
J13-3006,H93-1054,0,0.155446,"rgument classification subtask, and suggested the lexical data sparseness as one of the main reasons. In this work, we will focus on Semantic Role Classification (SRC), and we will show that selectional preferences (SP) are useful for generalizing lexical features, helping fight sparseness and domain shifts, and improving SRC results. Selectional preferences try to model the kind of words that can fill a specific argument of a predicate, and have been widely used in computational linguistics since the early days (Wilks 1975). Both semantic classes from existing lexical resources like WordNet (Resnik 1993b) and distributional similarity based on corpora (Pantel and Lin 2000) have been successfully used for acquiring selectional preferences, and in this work we have used several of those models. The contributions of this work to the field of SRL are the following: 1. We formalize and implement a method that applies several selectional preference models to Semantic Role Classification, introducing for the first time the use of selectional preferences for prepositions, in addition to selectional preferences for verbs. 2. We show that the selectional preference models are able to generalize lexica"
J13-3006,P10-1044,0,0.130344,"Missing"
J13-3006,J98-1004,0,0.108058,"s co-occurring with w, and I(w, d, v) is the mutual information between w and d, v. 637 Computational Linguistics Volume 39, Number 3 that the corpus contains an occurrence of w having dependency d with v. For instance, if the corpus contains John loves Mary, then the pair (ncsubj, love) would be in the set T for John. The measure uses information-theoretic principles, and I(w, d, v) represents the information content of the triple (Lin 1998). Although the use of co-occurrence vectors for words to compute similarity has been ¨ standard practice, some authors have argued for more complex uses. Schutze (1998) builds vectors for each context of occurrence of a word, combining the co-occurrence vectors for each word in the context. The vectors for contexts were used to induce senses and to improve information retrieval results. Edmonds (1997) built a lexical cooccurrence network, and applied it to a lexical choice task. Chakraborti et al. (2007) used transitivity over co-occurrence relations, with good results on several classification tasks. Note that all these works use second order and higher order to refer to their method. In this paper, we will also use second order to refer to a new method whi"
J13-3006,D11-1012,0,0.155516,"Missing"
J13-3006,P03-1002,1,0.820408,"s. For instance, consider the following sentence, in which the arguments of the predicate to send have been annotated with their respective semantic roles.1 (1) [Mr. Smith]Agent sent [the report]Object [to me]Recipient [this morning]Temporal . Recognizing these event structures has been shown to be important for a broad spectrum of NLP applications. Information extraction, summarization, question answering, machine translation, among others, can benefit from this shallow semantic analysis at sentence level, which opens the door for exploiting the semantic relations among arguments (Boas 2002; Surdeanu et al. 2003; Narayanan and Harabagiu 2004; Melli et al. 2005; Moschitti et al. 2007; Higashinaka and Isozaki 2008; Surdeanu, Ciaramita, and Zaragoza 2011). In M`arquez et al. (2008) the reader can find a broad introduction to SRL, covering several historical and definitional aspects of the problem, including also references to the main resources and systems. State-of-the-art systems leverage existing hand-tagged corpora (Fillmore, Ruppenhofer, and Baker 2004; Palmer, Gildea, and Kingsbury 2005) to learn supervised machine learning systems, and typically perform SRL in two sequential steps: argument ident"
J13-3006,J11-2003,1,0.89747,"Missing"
J13-3006,P09-2019,1,0.908931,"Missing"
J13-3006,N07-1070,0,\N,Missing
J13-3006,P10-1046,0,\N,Missing
J13-3006,P92-1053,0,\N,Missing
J13-3006,C98-2122,0,\N,Missing
J14-1003,N09-1003,1,0.0595825,"Missing"
J14-1003,P08-1037,1,0.0810727,"Missing"
J14-1003,P11-2123,1,0.760424,"wledge on a variety of English data sets and a data set on Spanish. We include a detailed analysis of the factors that affect the algorithm. The algorithm and the LKBs used are publicly available, and the results easily reproducible. 1. Introduction Word Sense Disambiguation (WSD) is a key enabling technology that automatically chooses the intended sense of a word in context. It has been the focus of intensive research since the beginning of Natural Language Processing (NLP), and more recently it has been shown to be useful in several tasks such as parsing (Agirre, Baldwin, and Martinez 2008; Agirre et al. 2011), machine translation (Carpuat and Wu 2007; Chan, ¨ Ng, and Chiang 2007), information retrieval (P´erez-Aguera and Zaragoza 2008; Zhong and Ng 2012), question answering (Surdeanu, Ciaramita, and Zaragoza 2008), and ∗ Informatika Fakultatea, Manuel Lardizabal 1, 20018 Donostia, Basque Country. E-mail: e.agirre@ehu.es. ∗∗ IKERBASQUE, Basque Foundation for Science, 48011, Bilbao, Basque Country. E-mail: oier.lopezdelacalle@gmail.com. † Informatika Fakultatea, Manuel Lardizabal 1, 20018 Donostia, Basque Country. E-mail: a.soroa@ehu.es. Submission received: 30 July 2011; Revised submission received"
J14-1003,C96-1005,1,0.199438,"ditional knowledge-based WSD systems assign a sense to an ambiguous word by comparing each of its senses with those of the surrounding context. Typically, some semantic similarity metric is used for calculating the relatedness among senses (Lesk 1986; Patwardhan, Banerjee, and Pedersen 2003). The metric varies between counting word overlaps between definitions of the words (Lesk 1986) to finding distances between concepts following the structure of the LKB (Patwardhan, Banerjee, and Pedersen 2003). Usually the distances are calculated using only hierarchical relations on the LKB (Sussna 1993; Agirre and Rigau 1996). Combining both intuitions, Jiang and Conrath (1997) present a metric that combines statistics from corpus and a lexical taxonomy structure. One of the major drawbacks of these approaches stems from the fact that senses are compared in a pairwise fashion and thus the number of computations grows exponentially with the number of words—that is, for a sequence of n words where each has up to k senses they need to consider up to kn sense sequences. Although alternatives like simulated annealing (Cowie, Guthrie, and Guthrie 1992) and conceptual density 1 http://ixa2.si.ehu.es/ukb. 59 Computational"
J14-1003,agirre-soroa-2008-using,1,0.120686,"to k senses, the algorithms had to consider up to kn sense sequences. Greedy methods were often used to avoid the combinatorial explosion (Patwardhan, Banerjee, and Pedersen 2003). As an alternative, graph-based methods are able to exploit the structural properties of the graph underlying a particular LKB. These methods are able to consider all possible combinations of occurring senses on a particular context, and thus offer a way to analyze efficiently the inter-relations among them, gaining much attention in the NLP community (Mihalcea 2005; Navigli and Lapata 2007; Sinha and Mihalcea 2007; Agirre and Soroa 2008; Navigli and Lapata 2010). The nodes in the graph represent the concepts (word senses) in the LKB, and edges in the graph represent relations between them, such as subclass and part-of. Network analysis techniques based on random walks like PageRank (Brin and Page 1998) can then be used to choose the senses that are most relevant in the graph, and thus output those senses. 58 ´ Agirre, Lopez de Lacalle, and Soroa Random Walks for Knowledge-Based WSD In order to deal with large knowledge bases containing more than 100,000 concepts (Fellbaum 1998), previous algorithms had to extract subsets of"
J14-1003,E09-1005,1,0.450244,"walks over large LKBs. The algorithm outperforms other graph-based algorithms when using a LKB built from WordNet and eXtended WordNet. The algorithm and LKB combination compares favorably to the state-of-the-art in knowledge-based WSD on a wide variety of data sets, including four English and one Spanish data set. (2) A detailed analysis of the factors that affect the algorithm. (3) The algorithm together with the corresponding graphs are publicly available1 and can be applied easily to sense inventories and knowledge bases different from WordNet. The algorithm for WSD was first presented in Agirre and Soroa (2009). In this article, we present further evaluation on two more recent data sets, analyze the parameters and options of the system, compare it to the state of the art, and discuss the relation of our algorithm with PageRank and the MFS heuristic. 2. Related Work Traditional knowledge-based WSD systems assign a sense to an ambiguous word by comparing each of its senses with those of the surrounding context. Typically, some semantic similarity metric is used for calculating the relatedness among senses (Lesk 1986; Patwardhan, Banerjee, and Pedersen 2003). The metric varies between counting word ove"
J14-1003,S07-1070,0,0.0660895,"Missing"
J14-1003,atserias-etal-2004-spanish,0,0.247279,"Missing"
J14-1003,W97-0703,0,0.229726,"Missing"
J14-1003,D07-1007,0,0.0106246,"nd a data set on Spanish. We include a detailed analysis of the factors that affect the algorithm. The algorithm and the LKBs used are publicly available, and the results easily reproducible. 1. Introduction Word Sense Disambiguation (WSD) is a key enabling technology that automatically chooses the intended sense of a word in context. It has been the focus of intensive research since the beginning of Natural Language Processing (NLP), and more recently it has been shown to be useful in several tasks such as parsing (Agirre, Baldwin, and Martinez 2008; Agirre et al. 2011), machine translation (Carpuat and Wu 2007; Chan, ¨ Ng, and Chiang 2007), information retrieval (P´erez-Aguera and Zaragoza 2008; Zhong and Ng 2012), question answering (Surdeanu, Ciaramita, and Zaragoza 2008), and ∗ Informatika Fakultatea, Manuel Lardizabal 1, 20018 Donostia, Basque Country. E-mail: e.agirre@ehu.es. ∗∗ IKERBASQUE, Basque Foundation for Science, 48011, Bilbao, Basque Country. E-mail: oier.lopezdelacalle@gmail.com. † Informatika Fakultatea, Manuel Lardizabal 1, 20018 Donostia, Basque Country. E-mail: a.soroa@ehu.es. Submission received: 30 July 2011; Revised submission received: 15 November 2012; Accepted for publicati"
J14-1003,P07-1005,0,0.0890249,"Missing"
J14-1003,S07-1054,0,0.30332,"Missing"
J14-1003,H92-1046,0,0.637309,"Missing"
J14-1003,W06-1663,0,0.0450149,"e used semi-automatic and fully automatic methods to enrich WordNet with additional relations. Mihalcea and Moldovan (2001) disambiguated WordNet glosses in a resource called eXtended WordNet. The disambiguated glosses have been shown to improve results of a graph-based system (Agirre and Soroa 2008), and we have also used them in our experiments. Navigli and Velardi (2005) enriched WordNet with cooccurrence relations semi-automatically and showed that those relations are effective in a number of graph-based WSD systems (Navigli and Velardi 2005; Navigli and Lapata 2007, 2010). More recently, Cuadros and Rigau (2006, 2007, 2008) learned automatically so-called KnowNets, and showed that the new provided relations improved WSD performance when plugged into a simple vector-based WSD system. Finally, Ponzetto and Navigli (2010) have acquired relations automatically from Wikipedia, released as WordNet++, and have shown that they are beneficial in a graph-based WSD algorithm. All of these relations are publicly available with the exception of Navigli and Velardi (2005), but note that the system is available on-line.2 Disambiguation is typically performed by applying a ranking algorithm over the graph, and then"
J14-1003,S07-1015,0,0.148169,"Missing"
J14-1003,P00-1064,0,0.178159,"Missing"
J14-1003,W04-0827,0,0.0722021,"Missing"
J14-1003,W00-1322,0,0.246028,"Missing"
J14-1003,D07-1061,0,0.0475121,"nitialize the random walk with the words in the context of the target word, and thus we obtain a context-dependent PageRank. We will show that this method is indeed effective for WSD. Note that in order to use other centrality algorithms (e.g., HITS [Kleinberg 1998]), previous authors had to build a subgraph first. In principle, those algorithms could be made context-dependent when using the full graph and altering their formulae, but we are not aware of such variations. Random walks over WordNet using Personalized PageRank have been also used to measure semantic similarity between two words (Hughes and Ramage 2007; Agirre 61 Computational Linguistics Volume 40, Number 1 et al. 2009). In those papers, the random walks are initialized with a single word, whereas we use all content words in the context. The results obtained by the authors, especially in the latter paper, are well above other WordNet-based methods. Most previous work on knowledge-based WSD has presented results on one or two general domain corpora for English. We present our results on four general domain data sets for English and a Spanish data set (M`arquez et al. 2007). Alternatively, some researchers have applied knowledge-based WSD to"
J14-1003,O97-1002,0,0.381894,"to an ambiguous word by comparing each of its senses with those of the surrounding context. Typically, some semantic similarity metric is used for calculating the relatedness among senses (Lesk 1986; Patwardhan, Banerjee, and Pedersen 2003). The metric varies between counting word overlaps between definitions of the words (Lesk 1986) to finding distances between concepts following the structure of the LKB (Patwardhan, Banerjee, and Pedersen 2003). Usually the distances are calculated using only hierarchical relations on the LKB (Sussna 1993; Agirre and Rigau 1996). Combining both intuitions, Jiang and Conrath (1997) present a metric that combines statistics from corpus and a lexical taxonomy structure. One of the major drawbacks of these approaches stems from the fact that senses are compared in a pairwise fashion and thus the number of computations grows exponentially with the number of words—that is, for a sequence of n words where each has up to k senses they need to consider up to kn sense sequences. Although alternatives like simulated annealing (Cowie, Guthrie, and Guthrie 1992) and conceptual density 1 http://ixa2.si.ehu.es/ukb. 59 Computational Linguistics Volume 40, Number 1 (Agirre and Rigau 19"
J14-1003,H05-1053,0,0.230575,"Missing"
J14-1003,S07-1008,0,0.0437951,"Missing"
J14-1003,J07-4005,0,0.0556294,"each has up to k senses they need to consider up to kn sense sequences. Although alternatives like simulated annealing (Cowie, Guthrie, and Guthrie 1992) and conceptual density 1 http://ixa2.si.ehu.es/ukb. 59 Computational Linguistics Volume 40, Number 1 (Agirre and Rigau 1996) were tried, most of the knowledge-based WSD at the time was done in a suboptimal word-by-word greedy process, namely, disambiguating words one at a time (Patwardhan, Banerjee, and Pedersen 2003). Still, some recent work on finding predominant senses in domains has applied such similarity-based techniques with success (McCarthy et al. 2007). Recently, graph-based methods for knowledge-based WSD have gained much attention in the NLP community (Mihalcea 2005; Navigli and Velardi 2005; Navigli and Lapata 2007; Sinha and Mihalcea 2007; Agirre and Soroa 2008; Navigli and Lapata 2010). These methods use well-known graph-based techniques to find and exploit the structural properties of the graph underlying a particular LKB. Graph-based techniques consider all the sense combinations of the words occurring on a particular context at once, and thus offer a way to analyze the relations among them with respect to the whole graph. They are p"
J14-1003,H05-1052,0,0.253866,"pairwise similarity for a sequence of n words where each has up to k senses, the algorithms had to consider up to kn sense sequences. Greedy methods were often used to avoid the combinatorial explosion (Patwardhan, Banerjee, and Pedersen 2003). As an alternative, graph-based methods are able to exploit the structural properties of the graph underlying a particular LKB. These methods are able to consider all possible combinations of occurring senses on a particular context, and thus offer a way to analyze efficiently the inter-relations among them, gaining much attention in the NLP community (Mihalcea 2005; Navigli and Lapata 2007; Sinha and Mihalcea 2007; Agirre and Soroa 2008; Navigli and Lapata 2010). The nodes in the graph represent the concepts (word senses) in the LKB, and edges in the graph represent relations between them, such as subclass and part-of. Network analysis techniques based on random walks like PageRank (Brin and Page 1998) can then be used to choose the senses that are most relevant in the graph, and thus output those senses. 58 ´ Agirre, Lopez de Lacalle, and Soroa Random Walks for Knowledge-Based WSD In order to deal with large knowledge bases containing more than 100,000"
J14-1003,H93-1061,0,0.65186,"Missing"
J14-1003,S07-1043,0,0.0235631,"plementation in the next subsection which shows that, when using the same LKB, our method obtains better results. (2) Although not reported in the table, an unsupervised system using automatically acquired training examples from bilingual data (Chan and Ng 2005) obtained very good results on S2AW nouns (77.2 F1, compared with our 70.3 F1 in Table 2). The automatically acquired training examples are used in addition to hand-annotated data in Zhong10 (Zhong and Ng 2010), also reported in the table (see below). We report the best unsupervised systems in S07AW and S07CG on the same row. JU-SKNSB (Naskar and Bandyopadhyay 2007) is a system based on an extended version Table 3 Comparison with state-of-the-art results (F1). The top rows report knowledge-based and unsupervised systems, followed by our system (PPRw2w ). Below we report systems that use annotated data to some degree: (1) MFS or counts from hand-annotated corpora, (2) fully supervised systems, including the best supervised participants in each exercise. Best result among unsupervised systems in each column is shown in bold. Please see text for references of each system. System S2AW S3AW Mih05 Sinha07 Tsatsa10 Agirre08 Nav10 JU-SKNSB / TKB-UO Ponz10 54.2 5"
J14-1003,P96-1006,0,0.626408,"Missing"
J14-1003,W97-0201,0,0.55767,"Missing"
J14-1003,S01-1005,0,0.0511821,"Missing"
J14-1003,P10-1154,0,0.308589,"stance, Chan and Ng (2005) present an unsupervised method to obtain training examples from bilingual data, which was used together with SemCor and DSO to train one of the best performing supervised systems to date (Zhong and Ng 2010). In view of the problems of supervised systems, knowledge-based WSD is emerging as a powerful alternative. Knowledge-based WSD systems exploit the information in a lexical knowledge base (LKB) to perform WSD. They currently perform below supervised systems on general domain data, but are attaining performance close or above MFS without access to hand-tagged data (Ponzetto and Navigli 2010). In this sense, they provide a complementary strand of research which could be combined with supervised ´ methods, as shown for instance in Navigli (2008). In addition, Agirre, Lopez de Lacalle, and Soroa (2009) show that knowledge-based WSD systems can outperform supervised systems in a domain-specific data set, where MFS from general domains also fails. In this article, we will focus our attention on knowledge-based methods. Early work for knowledge-based WSD was based on measures of similarity between pairs of concepts. In order to maximize pairwise similarity for a sequence of n words whe"
J14-1003,W04-0811,0,0.238937,"Missing"
J14-1003,W04-0856,0,0.170834,"Missing"
J14-1003,P08-1082,0,0.0168788,"Missing"
J14-1003,S07-1057,0,0.0964627,"is derived from hand-annotated corpora. In the case of the English WordNet, the use of the first sense also falls in this category, as the order of senses in WordNet is based on sense counts in hand-annotated corpora. Note that for wordnets in other languages, hand-annotated corpus is scarce, and thus our main results do not use this information. Section 6.4.7 analyzes the results of our system when combined with this information. Among supervised systems, the best supervised systems at competition time are reported in a single row (Mihalcea 2002; Decadt et al. 2004; Chan, Ng, and Zhong 2007; Tratz et al. 2007). We also report Zhong10 (Zhong and Ng 2010), which is a freely available supervised system giving some of the strongest results in WSD. We will now discuss in detail the systems that are most similar to our own. We first review the WordNet versions and relations used by each system. Mih05 (Mihalcea 2005) and Sinha07 (Sinha and Mihalcea 2007) apply several similarity methods, which use WordNet information from versions 1.7.1 and 2.0, respectively, including all relations and the text in the glosses.5 Tsatsa10 (Tsatsaronis, Varlamis, and Nørv˚ag 2010) uses WordNet 2.0. Agirre08 (Agirre and Soro"
J14-1003,P10-4014,0,0.878024,": (1) Nav10 (Navigli and Lapata 2010) obtained better results on S07AW. We will compare both systems in more detail below, and also include a reimplementation in the next subsection which shows that, when using the same LKB, our method obtains better results. (2) Although not reported in the table, an unsupervised system using automatically acquired training examples from bilingual data (Chan and Ng 2005) obtained very good results on S2AW nouns (77.2 F1, compared with our 70.3 F1 in Table 2). The automatically acquired training examples are used in addition to hand-annotated data in Zhong10 (Zhong and Ng 2010), also reported in the table (see below). We report the best unsupervised systems in S07AW and S07CG on the same row. JU-SKNSB (Naskar and Bandyopadhyay 2007) is a system based on an extended version Table 3 Comparison with state-of-the-art results (F1). The top rows report knowledge-based and unsupervised systems, followed by our system (PPRw2w ). Below we report systems that use annotated data to some degree: (1) MFS or counts from hand-annotated corpora, (2) fully supervised systems, including the best supervised participants in each exercise. Best result among unsupervised systems in each"
J14-1003,P12-1029,0,0.0190084,"rithm and the LKBs used are publicly available, and the results easily reproducible. 1. Introduction Word Sense Disambiguation (WSD) is a key enabling technology that automatically chooses the intended sense of a word in context. It has been the focus of intensive research since the beginning of Natural Language Processing (NLP), and more recently it has been shown to be useful in several tasks such as parsing (Agirre, Baldwin, and Martinez 2008; Agirre et al. 2011), machine translation (Carpuat and Wu 2007; Chan, ¨ Ng, and Chiang 2007), information retrieval (P´erez-Aguera and Zaragoza 2008; Zhong and Ng 2012), question answering (Surdeanu, Ciaramita, and Zaragoza 2008), and ∗ Informatika Fakultatea, Manuel Lardizabal 1, 20018 Donostia, Basque Country. E-mail: e.agirre@ehu.es. ∗∗ IKERBASQUE, Basque Foundation for Science, 48011, Bilbao, Basque Country. E-mail: oier.lopezdelacalle@gmail.com. † Informatika Fakultatea, Manuel Lardizabal 1, 20018 Donostia, Basque Country. E-mail: a.soroa@ehu.es. Submission received: 30 July 2011; Revised submission received: 15 November 2012; Accepted for publication: 17 February 2013. doi:10.1162/COLI a 00164 © 2014 Association for Computational Linguistics Computatio"
J14-1003,S07-1006,0,\N,Missing
J14-1003,C92-1056,0,\N,Missing
J14-1003,S07-1016,0,\N,Missing
J14-1003,J14-4005,0,\N,Missing
K18-1017,D17-1277,0,0.210086,"Missing"
K18-1017,C14-1213,1,0.894811,"Missing"
K18-1017,P16-1059,0,0.403782,"s; (2) a context model which measures to which extent the entities fit well in the context of the mention, using textual features; (3) a coherence model which prefers entities that are related to the other entities in the document. The first and second models are local in that they only require a short context of occurrence and disambiguate each mention in the document separately. The third model is global, in that all mentions are disambiguated simultaneously (Ratinov et al., 2011). Recent work has shown that local models can be improved adding a global coherence model (Ratinov et al., 2011; Globerson et al., 2016). In this work we focus on a local model, and a global model could improve the results further. All local and global systems mentioned above, as well as the current state-of-the-art systems (Lazic et al., 2015; Globerson et al., 2016; Yamada et al., 2016; Ganea and Hofmann, 2017), rely on single models for each of the above, that is, they have a single mention model, context model and coherence model for all entities, e.g. the 500K ambiguous entity mentions occurring more than 10 times in Wikipedia. While this has the advantage of reusing the parameters across mentions, it also makes the probl"
K18-1017,P16-1179,1,0.857083,"Missing"
K18-1017,P11-1095,0,0.0120078,"ed on indomain training data. 1 Introduction Named Entity Disambiguation (NED), also known as Entity Linking or Entity Resolution, is a task where entity mentions in running text need to be linked to its entity entry in a Knowledge Base (KB), such as Wikidata, Wikipedia or other derived resources like DBpedia (Bunescu and Pasca, 2006; McNamee and Dang, 2009; Hoffart et al., 2011). This task is challenging, as some entity mentions like “London” can refer to a number of places, people, fictional characters, brands, movies, books or songs. Given a mention in context, NED methods (Cucerzan, 2007; Han and Sun, 2011; Ratinov et al., 2011; Lazic et al., 2015) typically rely on three models: (1) a mention model which collects possible entities which can be referred to by the 171 Proceedings of the 22nd Conference on Computational Natural Language Learning (CoNLL 2018), pages 171–180 c Brussels, Belgium, October 31 - November 1, 2018. 2018 Association for Computational Linguistics of mentions having very limited training data, e.g. 10 occurrences linking to an entity in Wikipedia. Our results will show that data-augmentation and transfer learning allow us to overcome the sparseness problem, yielding the bes"
K18-1017,E06-1002,0,0.089015,"ounts. Transferring an LSTM which is learned on all datasets is the most effective context representation option for the word experts in all frequency bands. The experiments show that our system trained on out-ofdomain Wikipedia data surpasses comparable NED systems which have been trained on indomain training data. 1 Introduction Named Entity Disambiguation (NED), also known as Entity Linking or Entity Resolution, is a task where entity mentions in running text need to be linked to its entity entry in a Knowledge Base (KB), such as Wikidata, Wikipedia or other derived resources like DBpedia (Bunescu and Pasca, 2006; McNamee and Dang, 2009; Hoffart et al., 2011). This task is challenging, as some entity mentions like “London” can refer to a number of places, people, fictional characters, brands, movies, books or songs. Given a mention in context, NED methods (Cucerzan, 2007; Han and Sun, 2011; Ratinov et al., 2011; Lazic et al., 2015) typically rely on three models: (1) a mention model which collects possible entities which can be referred to by the 171 Proceedings of the 22nd Conference on Computational Natural Language Learning (CoNLL 2018), pages 171–180 c Brussels, Belgium, October 31 - November 1, 2"
K18-1017,D11-1072,0,0.365745,"Missing"
K18-1017,D18-2029,0,0.0677648,"Missing"
K18-1017,L16-1139,1,0.901558,"Missing"
K18-1017,Q15-1036,0,0.0617361,"in the document. The first and second models are local in that they only require a short context of occurrence and disambiguate each mention in the document separately. The third model is global, in that all mentions are disambiguated simultaneously (Ratinov et al., 2011). Recent work has shown that local models can be improved adding a global coherence model (Ratinov et al., 2011; Globerson et al., 2016). In this work we focus on a local model, and a global model could improve the results further. All local and global systems mentioned above, as well as the current state-of-the-art systems (Lazic et al., 2015; Globerson et al., 2016; Yamada et al., 2016; Ganea and Hofmann, 2017), rely on single models for each of the above, that is, they have a single mention model, context model and coherence model for all entities, e.g. the 500K ambiguous entity mentions occurring more than 10 times in Wikipedia. While this has the advantage of reusing the parameters across mentions, it also makes the problem unnecessarily complex. In this paper we propose to break the task of NED into 500K classification tasks, one for each target mention, as opposed to building a single model for all 500K mentions. The advanta"
K18-1017,Q15-1023,0,0.0127726,"Lazic et al., 2015) sup. Sparse BoW Continuous BoW LSTM Transfer LSTM (Lazic et al., 2015)† semi-sup. (Chang et al., 2016)* (Yamada et al., 2016)* Local & Global models (Cucerzan, 2012)* (Chisholm and Hachey, 2015)* (Globerson et al., 2016)* (Yamada et al., 2016)* tac10 tac11 tac12 — 85.82 86.96 86.73 87.32 — 84.5* 84.6* 74.5 80.25 81.55 81.44 84.41 79.3† — — 68.7 63.12 67.49 67.32 72.58 74.2† — — — 80.7* 87.2* 85.2* — — 84.3* 72.0* — 82.4* (2012) present a detailed overview of all possible components, but in this section we will focus on the most relevant high performing systems. Please see (Ling et al., 2015) for a more detailed review of past research. 4.1 Among local systems that are trained on Wikipedia alone, (Lazic et al., 2015) was the best performing one to date. Their system is based on probabilistic estimation, with a rich preprocessing pipeline, including dependency parsing, common noun phrase identificacion and coreference resolution. They present the results for both a supervised version, and a graphbased semi-supervised extension which improves results. We think that the results of our method could be improved using richer pre-processing, specially the use of coreference to find longe"
K18-1017,N15-1026,0,0.0365803,"Missing"
K18-1017,N18-1202,0,0.181126,"mada et al., 2016; Sil et al., 2018), ours is trained on Wikipedia and tested out-of-domain. From another perspective, a set of 500K classification problems provides a great experimental framework for testing text representation and classification algorithms. More specifically, deep learning methods provide end-to-end algorithms to learn both representations and classifiers jointly (LeCun et al., 2015). In fact, learning text representations models has become a center topic in natural language understanding, as it allows to transfer representation models across tasks (Conneau and Kiela, 2018; Peters et al., 2018; Wang et al., 2018). In this paper, we explore several popular text representation options, as well as dataaugmentation (Zhang and LeCun, 2015) and transfer learning (Bengio, 2012). All training examples and models in this paper, as well as the pytorch code to reproduce results is availabe 1 . This paper is structured as follows. We first present our models. Section 3 presents the experiments, followed by related work and conclusions. 2 On the other hand, Wikipedia editors have manually added hyperlinks to articles, where the anchor text corresponds to the mention, and the url corresponds to"
K18-1017,P11-1138,0,0.548433,"barrena,a.soroa,e.agirre@ehu.eus Abstract mention string (aliases or surface forms), possibly weighted according to prior probabilities; (2) a context model which measures to which extent the entities fit well in the context of the mention, using textual features; (3) a coherence model which prefers entities that are related to the other entities in the document. The first and second models are local in that they only require a short context of occurrence and disambiguate each mention in the document separately. The third model is global, in that all mentions are disambiguated simultaneously (Ratinov et al., 2011). Recent work has shown that local models can be improved adding a global coherence model (Ratinov et al., 2011; Globerson et al., 2016). In this work we focus on a local model, and a global model could improve the results further. All local and global systems mentioned above, as well as the current state-of-the-art systems (Lazic et al., 2015; Globerson et al., 2016; Yamada et al., 2016; Ganea and Hofmann, 2017), rely on single models for each of the above, that is, they have a single mention model, context model and coherence model for all entities, e.g. the 500K ambiguous entity mentions oc"
K18-1017,spitkovsky-chang-2012-cross,0,0.0211468,"5) and transfer learning (Bengio, 2012). All training examples and models in this paper, as well as the pytorch code to reproduce results is availabe 1 . This paper is structured as follows. We first present our models. Section 3 presents the experiments, followed by related work and conclusions. 2 On the other hand, Wikipedia editors have manually added hyperlinks to articles, where the anchor text corresponds to the mention, and the url corresponds to the entity. We first built a candidate model as a dictionary that links each text anchor to possible entities, using the method presented in (Spitkovsky and Chang, 2012; Barrena et al., 2016). Let M be the set of all unique mention strings m, E the set of all target entities e, and Em = {e1 , . . . , em } the set of entities that can be referred by mention m. We kept the 30 most frequent candidates for each mention for the sake of efficiency. We report the sizes of E and M below. We then extracted annotated examples by scanning through the page contents for hyperlinks that link anchors (the mentions) to the corresponding Wikipedia pages (the entities). For each such hyperlink, we build a context c by first tokenizing and removing the stop words, and then ext"
K18-1017,W18-5446,0,0.164984,"l et al., 2018), ours is trained on Wikipedia and tested out-of-domain. From another perspective, a set of 500K classification problems provides a great experimental framework for testing text representation and classification algorithms. More specifically, deep learning methods provide end-to-end algorithms to learn both representations and classifiers jointly (LeCun et al., 2015). In fact, learning text representations models has become a center topic in natural language understanding, as it allows to transfer representation models across tasks (Conneau and Kiela, 2018; Peters et al., 2018; Wang et al., 2018). In this paper, we explore several popular text representation options, as well as dataaugmentation (Zhang and LeCun, 2015) and transfer learning (Bengio, 2012). All training examples and models in this paper, as well as the pytorch code to reproduce results is availabe 1 . This paper is structured as follows. We first present our models. Section 3 presents the experiments, followed by related work and conclusions. 2 On the other hand, Wikipedia editors have manually added hyperlinks to articles, where the anchor text corresponds to the mention, and the url corresponds to the entity. We first"
K18-1017,K16-1025,0,0.451098,"s are local in that they only require a short context of occurrence and disambiguate each mention in the document separately. The third model is global, in that all mentions are disambiguated simultaneously (Ratinov et al., 2011). Recent work has shown that local models can be improved adding a global coherence model (Ratinov et al., 2011; Globerson et al., 2016). In this work we focus on a local model, and a global model could improve the results further. All local and global systems mentioned above, as well as the current state-of-the-art systems (Lazic et al., 2015; Globerson et al., 2016; Yamada et al., 2016; Ganea and Hofmann, 2017), rely on single models for each of the above, that is, they have a single mention model, context model and coherence model for all entities, e.g. the 500K ambiguous entity mentions occurring more than 10 times in Wikipedia. While this has the advantage of reusing the parameters across mentions, it also makes the problem unnecessarily complex. In this paper we propose to break the task of NED into 500K classification tasks, one for each target mention, as opposed to building a single model for all 500K mentions. The advantage of this approach is that each of the 500K"
K18-1028,Q16-1028,0,0.0229841,"rvised ones. 1 Introduction Word embeddings have recently become a central topic in natural language processing. Several unsupervised methods have been proposed to efficiently train dense vector representations of words (Mikolov et al., 2013; Pennington et al., 2014; Bojanowski et al., 2017) and successfully applied in a variety of tasks like parsing (Bansal et al., 2014), topic modeling (Batmanghelich et al., 2016) and document classification (Taddy, 2015). While there is still an active research line to better understand these models from a theoretical perspective (Levy and Goldberg, 2014c; Arora et al., 2016; Gittens et al., 2017), the fundamental idea behind all of them is to assign a similar vector representation to similar words. For that purpose, most embedding models build upon co-occurrence statistics from large monolingual corpora, following the distributional hypothesis that similar words tend to occur in similar contexts (Harris, 1954). Nevertheless, the above argument does not formalize what “similar words” means, and it is not 1. We propose a linear transformation with a free parameter that adjusts the perfor1 Also referred to as functional similarity or just similarity. Also referred"
K18-1028,W16-2501,0,0.0409144,"Missing"
K18-1028,P14-2131,0,0.114359,"on how embeddings encode divergent linguistic information. In addition, we explore the relation between intrinsic and extrinsic evaluation, as the effect of our transformations in downstream tasks is higher for unsupervised systems than for supervised ones. 1 Introduction Word embeddings have recently become a central topic in natural language processing. Several unsupervised methods have been proposed to efficiently train dense vector representations of words (Mikolov et al., 2013; Pennington et al., 2014; Bojanowski et al., 2017) and successfully applied in a variety of tasks like parsing (Bansal et al., 2014), topic modeling (Batmanghelich et al., 2016) and document classification (Taddy, 2015). While there is still an active research line to better understand these models from a theoretical perspective (Levy and Goldberg, 2014c; Arora et al., 2016; Gittens et al., 2017), the fundamental idea behind all of them is to assign a similar vector representation to similar words. For that purpose, most embedding models build upon co-occurrence statistics from large monolingual corpora, following the distributional hypothesis that similar words tend to occur in similar contexts (Harris, 1954). Nevertheles"
K18-1028,N15-1184,0,0.236194,"ings, they might wrongly conclude that fasttext is vastly superior to glove at encoding semantic similarity information, but this proves to be a mere illusion after applying our post-processing. As such, intrinsic evaluation combined with our post-processing provides a more complete and dynamic picture of the information that is truly encoded by different embedding models. 6 Related work There have been several proposals to learn word embeddings that are specialized in certain linguistic aspects. For instance, Kiela et al. (2015) use a joint-learning approach and two variants of retrofitting (Faruqui et al., 2015a) to specialize word embeddings for either similarity or relatedness. At the same time, Levy and Goldberg (2014a) propose a modification of skip-gram that uses a dependency-based context instead of a sliding windows, which produces embeddings that are more tailored towards genuine similarity than relatedness. Bansal et al. (2014) follow a similar approach to train specialized embeddings that are used as features for dependency parsing. Finally, Mitchell and Steedman (2015) exploit morphology and word order information to learn embeddings that decompose into orthogonal • Supervised systems tha"
K18-1028,W16-2502,0,0.0256611,"decomposition, so given the co-occurrence matrix M = U SV T , the word vectors will correspond to the first n dimensions of W = U S α , where the parameter α plays a similar role as in our method. Note, however, that our proposal is more general and can be applied to any set of word vectors in a post-processing step, including neural embedding models that have superseded these traditional count-based models as we in fact do in this paper. Finally, there are others authors that have also pointed limitations in the intrinsic evaluation of word embeddings. For instance, Faruqui et al. (2016) and Batchkarov et al. (2016) argue that word similarity has many problems like the subjectivity and difficulty of the task, the lack of statistical significance and the inability to account for polysemy, warning that results should be interpreted with care. Chiu et al. (2016) analyze the correlation between results on word similarity benchmarks and sequence labeling tasks, and conAcknowledgments This research was partially supported by the Spanish MINECO (TUNER TIN2015-65308-C51-R, MUSTER PCIN-2015-226 and TADEEP TIN2015-70214-P, cofunded by EU FEDER), the UPV/EHU (excellence research group), and the NVIDIA GPU grant pro"
K18-1028,W16-2506,0,0.0278732,"trix using singular value decomposition, so given the co-occurrence matrix M = U SV T , the word vectors will correspond to the first n dimensions of W = U S α , where the parameter α plays a similar role as in our method. Note, however, that our proposal is more general and can be applied to any set of word vectors in a post-processing step, including neural embedding models that have superseded these traditional count-based models as we in fact do in this paper. Finally, there are others authors that have also pointed limitations in the intrinsic evaluation of word embeddings. For instance, Faruqui et al. (2016) and Batchkarov et al. (2016) argue that word similarity has many problems like the subjectivity and difficulty of the task, the lack of statistical significance and the inability to account for polysemy, warning that results should be interpreted with care. Chiu et al. (2016) analyze the correlation between results on word similarity benchmarks and sequence labeling tasks, and conAcknowledgments This research was partially supported by the Spanish MINECO (TUNER TIN2015-65308-C51-R, MUSTER PCIN-2015-226 and TADEEP TIN2015-70214-P, cofunded by EU FEDER), the UPV/EHU (excellence research group),"
K18-1028,P16-2087,0,0.0209852,"nguistic information. In addition, we explore the relation between intrinsic and extrinsic evaluation, as the effect of our transformations in downstream tasks is higher for unsupervised systems than for supervised ones. 1 Introduction Word embeddings have recently become a central topic in natural language processing. Several unsupervised methods have been proposed to efficiently train dense vector representations of words (Mikolov et al., 2013; Pennington et al., 2014; Bojanowski et al., 2017) and successfully applied in a variety of tasks like parsing (Bansal et al., 2014), topic modeling (Batmanghelich et al., 2016) and document classification (Taddy, 2015). While there is still an active research line to better understand these models from a theoretical perspective (Levy and Goldberg, 2014c; Arora et al., 2016; Gittens et al., 2017), the fundamental idea behind all of them is to assign a similar vector representation to similar words. For that purpose, most embedding models build upon co-occurrence statistics from large monolingual corpora, following the distributional hypothesis that similar words tend to occur in similar contexts (Harris, 1954). Nevertheless, the above argument does not formalize what"
K18-1028,P15-1144,0,0.120637,"ings, they might wrongly conclude that fasttext is vastly superior to glove at encoding semantic similarity information, but this proves to be a mere illusion after applying our post-processing. As such, intrinsic evaluation combined with our post-processing provides a more complete and dynamic picture of the information that is truly encoded by different embedding models. 6 Related work There have been several proposals to learn word embeddings that are specialized in certain linguistic aspects. For instance, Kiela et al. (2015) use a joint-learning approach and two variants of retrofitting (Faruqui et al., 2015a) to specialize word embeddings for either similarity or relatedness. At the same time, Levy and Goldberg (2014a) propose a modification of skip-gram that uses a dependency-based context instead of a sliding windows, which produces embeddings that are more tailored towards genuine similarity than relatedness. Bansal et al. (2014) follow a similar approach to train specialized embeddings that are used as features for dependency parsing. Finally, Mitchell and Steedman (2015) exploit morphology and word order information to learn embeddings that decompose into orthogonal • Supervised systems tha"
K18-1028,Q17-1010,0,0.686075,"refer to these two aspects as the two axes of similarity with two ends each: the semantics/syntax axis and the similarity/relatedness axis. In this paper, we propose a new method to tailor any given set of embeddings towards a specific end in these axes. Our method is inspired by the work on first order and second order cooccurrences (Sch¨utze, 1998), generalized as a continuous parameter of a linear transformation applied to the embeddings that we call similarity order. While there have been several proposals to learn specialized word embeddings (Levy and Goldberg, 2014a; Kiela et al., 2015; Bojanowski et al., 2017), previous work explicitly altered the training objective and often relied on external resources like knowledge bases, whereas the proposed method is applied as a post-processing of any pre-trained embedding model and does not require any additional resource. As such, our work shows that standard embedding models are able to encode divergent linguistic information but have limits on how this information is surfaced, and analyzes the implications that this has in both intrinsic evaluation and downstream tasks. This paper makes the following contributions: Following the recent success of word em"
K18-1028,P12-1015,0,0.0326848,"3). On the other hand, word similarity measures the correlation6 between the similarity scores produced by a model and a gold standard created by human annotators for a given set of word pairs. As discussed before, there is not a single definition of what human similarity scores should capture, which has lead to a distinction between genuine similarity datasets and relatedness datasets. In order to better understand the effect of our postprocessing in each case, we conduct our experiments in SimLex-999 (Hill et al., 2015), a genuine similarity dataset that consists of 999 word pairs, and MEN (Bruni et al., 2012), a relatedness dataset that consists of 3,000 word pairs7 . So as to make our evaluation more robust, we run the above experiments for three popular embedding methods, using large pre-trained models released by their respective authors as follows: Word2vec (Mikolov et al., 2013) is the original implementation of the CBOW and skip-gram architectures that popularized neural word embeddings. We use the pre-trained model published in the project homepage8 , which was trained on about 100 billion words of the Google News dataset and consists of 300-dimensional vectors for 3 million words and phras"
K18-1028,P17-1007,0,0.0146493,"duction Word embeddings have recently become a central topic in natural language processing. Several unsupervised methods have been proposed to efficiently train dense vector representations of words (Mikolov et al., 2013; Pennington et al., 2014; Bojanowski et al., 2017) and successfully applied in a variety of tasks like parsing (Bansal et al., 2014), topic modeling (Batmanghelich et al., 2016) and document classification (Taddy, 2015). While there is still an active research line to better understand these models from a theoretical perspective (Levy and Goldberg, 2014c; Arora et al., 2016; Gittens et al., 2017), the fundamental idea behind all of them is to assign a similar vector representation to similar words. For that purpose, most embedding models build upon co-occurrence statistics from large monolingual corpora, following the distributional hypothesis that similar words tend to occur in similar contexts (Harris, 1954). Nevertheless, the above argument does not formalize what “similar words” means, and it is not 1. We propose a linear transformation with a free parameter that adjusts the perfor1 Also referred to as functional similarity or just similarity. Also referred to as associative simil"
K18-1028,J06-1003,0,0.415197,"et al., 2017)13 . This task is akin to word similarity, but instead of assessing the similarity of individual word pairs, it is the similarity of entire sentence pairs as scored by the model that is compared against the gold standard produced by human annotators14 . This evaluation is attractive for our purposes because, while the state-of-the-art systems are supervised and based on elaborated deep learning or feature engineer13 http://ixa2.si.ehu.es/stswiki/index. php/STSbenchmark 14 Following common practice, we report Pearson. 12 Agreeing with the fact that relatedness subsumes similarity (Budanitsky and Hirst, 2006) 286 Centroid DAM word2vec Original Best 65.77 66.43 α = -0.30 72.65 73.08 α = 0.10 glove Original Best 64.54 68.96 α = -0.50 74.89 76.36 α = -0.70 fasttext Original Best 69.84 70.74 α = -0.20 77.33 77.33 α = 0.00 Table 2: Results in semantic textual similarity as measured by Pearson correlation for the original embeddings and the best post-processed model with the corresponding value of α. The DAM scores are averaged across 10 runs. word2vec glove fasttext Pearson correlation 80 70 ●● ●● ●● ● ● ● ●● ●●●●●●● ● ● ●● ●● ● ●● ●●● ●● ●●●● ● ● ●● ● ● ● ● ●●● ●● ●●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● M"
K18-1028,J15-4004,0,0.0733024,"at the transformation can be smoothly adjusted to the desired level. 3 (Mikolov et al., 2013). On the other hand, word similarity measures the correlation6 between the similarity scores produced by a model and a gold standard created by human annotators for a given set of word pairs. As discussed before, there is not a single definition of what human similarity scores should capture, which has lead to a distinction between genuine similarity datasets and relatedness datasets. In order to better understand the effect of our postprocessing in each case, we conduct our experiments in SimLex-999 (Hill et al., 2015), a genuine similarity dataset that consists of 999 word pairs, and MEN (Bruni et al., 2012), a relatedness dataset that consists of 3,000 word pairs7 . So as to make our evaluation more robust, we run the above experiments for three popular embedding methods, using large pre-trained models released by their respective authors as follows: Word2vec (Mikolov et al., 2013) is the original implementation of the CBOW and skip-gram architectures that popularized neural word embeddings. We use the pre-trained model published in the project homepage8 , which was trained on about 100 billion words of t"
K18-1028,D15-1242,0,0.0521454,"Missing"
K18-1028,S17-2001,1,0.781404,"similarity order with our post-processing, as both models get practically the same results with differences below 0.1 points. At the same time, although less pronounced than with semantic/syntactic analogies12 , the results show clear differences in the optimal configurations for genuine similarity (SimLex-999) and relatedness (MEN), with smaller values of α (i.e. lower similarity levels) favoring the former. 4 Extrinsic evaluation In order to better understand the effect of the proposed post-processing in downstream systems, we adopt the STS Benchmark dataset on semantic textual similarity (Cer et al., 2017)13 . This task is akin to word similarity, but instead of assessing the similarity of individual word pairs, it is the similarity of entire sentence pairs as scored by the model that is compared against the gold standard produced by human annotators14 . This evaluation is attractive for our purposes because, while the state-of-the-art systems are supervised and based on elaborated deep learning or feature engineer13 http://ixa2.si.ehu.es/stswiki/index. php/STSbenchmark 14 Following common practice, we report Pearson. 12 Agreeing with the fact that relatedness subsumes similarity (Budanitsky an"
K18-1028,P14-2050,0,0.480599,"in sing - singing) (Mikolov et al., 2013). We refer to these two aspects as the two axes of similarity with two ends each: the semantics/syntax axis and the similarity/relatedness axis. In this paper, we propose a new method to tailor any given set of embeddings towards a specific end in these axes. Our method is inspired by the work on first order and second order cooccurrences (Sch¨utze, 1998), generalized as a continuous parameter of a linear transformation applied to the embeddings that we call similarity order. While there have been several proposals to learn specialized word embeddings (Levy and Goldberg, 2014a; Kiela et al., 2015; Bojanowski et al., 2017), previous work explicitly altered the training objective and often relied on external resources like knowledge bases, whereas the proposed method is applied as a post-processing of any pre-trained embedding model and does not require any additional resource. As such, our work shows that standard embedding models are able to encode divergent linguistic information but have limits on how this information is surfaced, and analyzes the implications that this has in both intrinsic evaluation and downstream tasks. This paper makes the following contrib"
K18-1028,J98-1004,0,0.487558,"Missing"
K18-1028,P15-2008,0,0.0222492,"n between intrinsic and extrinsic evaluation, as the effect of our transformations in downstream tasks is higher for unsupervised systems than for supervised ones. 1 Introduction Word embeddings have recently become a central topic in natural language processing. Several unsupervised methods have been proposed to efficiently train dense vector representations of words (Mikolov et al., 2013; Pennington et al., 2014; Bojanowski et al., 2017) and successfully applied in a variety of tasks like parsing (Bansal et al., 2014), topic modeling (Batmanghelich et al., 2016) and document classification (Taddy, 2015). While there is still an active research line to better understand these models from a theoretical perspective (Levy and Goldberg, 2014c; Arora et al., 2016; Gittens et al., 2017), the fundamental idea behind all of them is to assign a similar vector representation to similar words. For that purpose, most embedding models build upon co-occurrence statistics from large monolingual corpora, following the distributional hypothesis that similar words tend to occur in similar contexts (Harris, 1954). Nevertheless, the above argument does not formalize what “similar words” means, and it is not 1. W"
K18-1028,W14-1618,0,0.4753,"in sing - singing) (Mikolov et al., 2013). We refer to these two aspects as the two axes of similarity with two ends each: the semantics/syntax axis and the similarity/relatedness axis. In this paper, we propose a new method to tailor any given set of embeddings towards a specific end in these axes. Our method is inspired by the work on first order and second order cooccurrences (Sch¨utze, 1998), generalized as a continuous parameter of a linear transformation applied to the embeddings that we call similarity order. While there have been several proposals to learn specialized word embeddings (Levy and Goldberg, 2014a; Kiela et al., 2015; Bojanowski et al., 2017), previous work explicitly altered the training objective and often relied on external resources like knowledge bases, whereas the proposed method is applied as a post-processing of any pre-trained embedding model and does not require any additional resource. As such, our work shows that standard embedding models are able to encode divergent linguistic information but have limits on how this information is surfaced, and analyzes the implications that this has in both intrinsic evaluation and downstream tasks. This paper makes the following contrib"
K18-1028,P15-1126,0,0.0175944,"in certain linguistic aspects. For instance, Kiela et al. (2015) use a joint-learning approach and two variants of retrofitting (Faruqui et al., 2015a) to specialize word embeddings for either similarity or relatedness. At the same time, Levy and Goldberg (2014a) propose a modification of skip-gram that uses a dependency-based context instead of a sliding windows, which produces embeddings that are more tailored towards genuine similarity than relatedness. Bansal et al. (2014) follow a similar approach to train specialized embeddings that are used as features for dependency parsing. Finally, Mitchell and Steedman (2015) exploit morphology and word order information to learn embeddings that decompose into orthogonal • Supervised systems that use pre-trained em288 clude that most intrinsic evaluations are poor predictors of downstream performance. In relation to that, our work explains how embeddings encode divergent linguistic information and the different effect this has in intrinsic evaluation and downstream tasks, showing that the proposed postprocessing can be easily used together with any intrinsic evaluation benchmark to get a more complete picture of the representations learned. semantic and syntactic"
K18-1028,D16-1244,0,0.0731773,"Missing"
K18-1028,D14-1162,0,0.108493,"out any external resource can tailor it to achieve better results in those aspects, providing a new perspective on how embeddings encode divergent linguistic information. In addition, we explore the relation between intrinsic and extrinsic evaluation, as the effect of our transformations in downstream tasks is higher for unsupervised systems than for supervised ones. 1 Introduction Word embeddings have recently become a central topic in natural language processing. Several unsupervised methods have been proposed to efficiently train dense vector representations of words (Mikolov et al., 2013; Pennington et al., 2014; Bojanowski et al., 2017) and successfully applied in a variety of tasks like parsing (Bansal et al., 2014), topic modeling (Batmanghelich et al., 2016) and document classification (Taddy, 2015). While there is still an active research line to better understand these models from a theoretical perspective (Levy and Goldberg, 2014c; Arora et al., 2016; Gittens et al., 2017), the fundamental idea behind all of them is to assign a similar vector representation to similar words. For that purpose, most embedding models build upon co-occurrence statistics from large monolingual corpora, following th"
K18-1028,N16-1091,0,0.0386432,"Missing"
K18-1028,P16-2083,0,0.0590185,"Missing"
K18-1028,P13-2087,0,\N,Missing
L16-1064,saralegi-lopez-de-lacalle-2010-dictionary,1,0.889266,"Missing"
L16-1064,2006.amta-papers.25,0,0.0350836,"exts are more helpful for the iterative translation selection algorithm. For example, the reformulated query qr=“PS 2 joko berriak” (“new PS 2 games”) was wrongly translated to tr(qr)=“PS 2 set new” with the second strategy. The third strategy, which uses the translation of the initial query as context tr(“PS 2 joku”)=“PS 2 game”, provides a correct translation tr(qr)=“PS 2 game new”. Table 2. presents the results, showing that better translations are obtained with this third strategy. 5.1. Evaluation of query translation The quality of translations was evaluated by means of the HTER measure (Snover et al., 2006). This measure computes the average amount of editing that a human would have to perform to correct the output of a system. A lower HTER value means a better translation. We do not penalize wrong word order in the translation because it does not have any negative effect on the retrieval process. In addition to this, according to (Snover et al., 2006), HTER achieves higher correlations than BLEU with human judgements. A fluent speaker in Basque and English performed the minimum number of edits over the English translations provided by the strategies which will be introduced in this section, in"
L16-1064,W00-1312,0,0.229291,"Missing"
L16-1139,S07-1074,1,0.813316,"Missing"
L16-1139,D11-1072,0,0.361144,"Missing"
L16-1139,N03-5008,1,0.688171,"Wikipedia articles. To ensure that our training data is natural language (and not, e.g., lists or tables), we only include text marked as paragraphs (i.e., enclosed between HTML tags &lt;P&gt; and &lt;/P&gt;). The relevant training subset for a target string then consists of example contexts with anchor texts containing the string.7 We take spans of up to 100 tokens to the left and another 100 to the right. Given this training data, we applied standard machine learning techniques to perform supervised disambiguation of entities. We trained a maximum entropy multi-class classifier8 for each target string (Manning and Klein, 2003). Then, given a mention of the target string in the test data, we applied its classifier to the context of the mention, and returned the corresponding article. We did not construct classifiers for strings whose training data maps to a unique entity. Instead, in those cases, a default classifier falls back to LNRM cascade’s output. From each context, we extracted features (see Figure 4) commonly used for supervised classification in the WSD 7 The target string is a substring of the anchor text after case normalization. 8 `2 -regularization setting (Agirre and Lopez de Lacalle, 2007; Zhong and N"
L16-1139,W04-0807,0,0.150416,"Missing"
L16-1139,W04-0811,0,0.0452795,"Missing"
L16-1139,spitkovsky-chang-2012-cross,1,0.851634,"scends Wikipedia by including anchors (i) from the greater web; and (ii) to Wikipedia pages that may not (yet) exist. For the purposes of NED, it could make sense to discard all but the articles that correspond to named entities. We keep everything, however, since not all articles have a known entity type, and because we would like to construct a resource that is generally useful for disambiguating concepts. Our dictionary can disambiguate mentions directly, simply by returning the highest-scoring entry for a given string. The construction of this dictionary is explained with more details in (Spitkovsky and Chang, 2012). We developed several variants of the dictionary: EXCT, LNRM, FUZZ and HEUR. For a given string-article pair, where the string has been observed as the anchor-text of a total of y inter-Wikipedia and v external links, of which x (and, respectively, u) pointed to a page that is represented by the article in the pair, we set the pair’s score to be a ratio (x + u)/(y + v). We call this dictionary exact (EXCT), as it matches precisely the raw strings found using the methods outlined above. For example, Figure 1 shows all eight articles that have been referred to by the string “Hank Williams.” Not"
L16-1139,P10-4014,0,0.0259469,"lein, 2003). Then, given a mention of the target string in the test data, we applied its classifier to the context of the mention, and returned the corresponding article. We did not construct classifiers for strings whose training data maps to a unique entity. Instead, in those cases, a default classifier falls back to LNRM cascade’s output. From each context, we extracted features (see Figure 4) commonly used for supervised classification in the WSD 7 The target string is a substring of the anchor text after case normalization. 8 `2 -regularization setting (Agirre and Lopez de Lacalle, 2007; Zhong and Ng, 2010): • the anchor text; • the unordered set of lemmas in the span; • lemma for noun/verb/adjective in a four-token window around the anchor text; • lemma/word for noun/verb/adjective before and after the anchor text; • word/lemma/part-of-speech bigram and trigrams including the anchor text. 5.1. Variations Over the course of developing our system, we tested several variations of the core algorithm: Classifier: We tried maximum entropy models (MAXENT) and support vector machines (SVM). Dictionary: A dictionary influences supervised classification in two places. First, when building the training da"
L16-1268,S10-1013,1,0.943639,"Missing"
L16-1268,J14-1003,1,0.901281,"Missing"
L16-1268,D14-1110,0,0.091557,"Missing"
L16-1268,H93-1061,0,0.83636,"Missing"
L16-1268,S13-2040,0,0.221915,"Missing"
L16-1268,S01-1005,0,0.0195329,"Missing"
L16-1268,W97-0108,0,0.195731,"s. One of the reasons why IMS is performing so well is that it partly overcomes the knowledge bottleneck problem by making use of parallel data from two different languages and thus generating more training data for the LFS. Other supervised approaches focus on improving the performance of the mapping function by reducing the number of possible classes. The rationale behind this approach is to reduce the knowledge bottleneck problem by combining the training data from related senses. Good results have been reported for these approaches on WordNet Domains, Supersenses, and Base Level Concepts (Peh and Ng, 1997; Izquierdo et al., 2007). In this paper, we propose an approach to modify the output from a WSD system using a MFS classifier. We do not attempt to overcome the knowledge bottleneck problem, but we try to correct the systems for their MFS bias by reducing the mapping function to MFS and LFS. 3. System Description The starting point for our system is the output from a WSD system. We report the results for the UKB and the IMS systems. A feature set containing mostly static features, focusing predominantly on frequency and domain properties of lemmas, is combined with the WSD output and fed into"
L16-1268,S07-1016,0,0.0785243,"Missing"
L16-1268,P15-1173,0,0.0826637,"Missing"
L16-1268,W04-0811,0,0.0750197,"Missing"
L16-1268,steinberger-etal-2012-jrc,0,0.0299295,"B+C IMS IMS+C 65.9 66.1 60.6 59.4 65.9 66.1 60.6 59.4 25.3 36.3 20.9 31.5 sval2015 relation between the system sense entropy and the sense entropy in Semcor. In addition, we use mostly static features, which are the same in training, development, and test for a particular lemma. These features include TF-IDF, part of speech, number of senses and system sense entropy, as well as WordNet domains, and the WordNet Supersense. Finally, we use one feature that is dependent on the corpus used in training, development, and test. This feature makes use of the domain classifier JRC EuroVoc Indexer JEX (Steinberger et al., 2012). We compare the domain distribution of Semcor to the domain distribution of the instances of a lemma. Finally, the output from the MFS classifier and a WSD system are combined to obtain the final sense assignment. The algorithm is visualized in Algorithm 1. UKB UKB+C IMS IMS+C 68.5 69.5 67.1 64.8 67.1 68.1 65.8 63.6 20.8 27.3 17.3 23.5 Table 2: In this Table, the WSD results are presented for the competitions sval2013 and sval2015, respectively. Three measures are used to show the performance of UKB and IMS WSD systems with the MFS classifier (+C) and without. Precision (Pwsd) and Recall (Rws"
L16-1268,P10-4014,0,0.127711,"e initialized using the knowledge from the graph. Next, the node weights are updated with respect to the knowledge found in the local context of a target word, resulting in context-dependent PageRank. In general, unsupervised approaches do not suffer greatly from the knowledge bottleneck problem. However, recent work has shown that they also have a strong bias towards the MFS (Calvo and Gelbukh, 2015). Supervised approaches attempt to maximize the performance of the mapping function by training word and sense experts using (mostly) sense-labeled training data. The It makes sense (IMS) system (Zhong and Ng, 2010) is one of the best performing supervised approaches, which makes use of linear support vector machines with mostly local contextual features. The biggest challenge for supervised approaches is the reliance on manually sense-tagged training data, which is expensive and time-consuming to create, especially for high polysemous words. One of the reasons why IMS is performing so well is that it partly overcomes the knowledge bottleneck problem by making use of parallel data from two different languages and thus generating more training data for the LFS. Other supervised approaches focus on improvi"
L16-1268,S15-2049,0,\N,Missing
L16-1441,E09-1005,1,0.672807,"ne translation reported in the literature had resorted to. While we acknowledged that our work was only in a preliminary state – our reported gains were minimal and based on a very controlled evaluation trained on a small, in-domain corpus – we found any improvement at all to be a good outcome, and recognized the need to continue our experimentation on a larger, open-domain corpus in the future. 3. Integrating WSD Output into the Machine Translation Pipeline Our chosen WSD algorithm is UKB, a collection of tools and algorithms for performing graph-based WSD over a pre-existing knowledge base (Agirre and Soroa, 2009; Agirre et al., 2014). Based on the graph-based WSD method pioneered by a number of researchers (Navigli and Velardi, 2005; Mihalcea, 2005; Sinha and Mihalcea, 2007; Navigli and Lapata, 2007; Agirre and Soroa, 2008), UKB allows for WordNet-style knowledge bases to be represented as weighted graphs, where word senses correspond to nodes and the relationships or dependencies between pairs of senses correspond to the edges between those nodes. By calculating the probability of a ‘random walk’ over the graph from a target word’s node ending on any other node in the graph – with the nodes (senses)"
L16-1441,J14-1003,1,0.88208,"Missing"
L16-1441,P14-1023,0,0.0242498,"calculated from an embedded space obtained using a shallow neural network language model (NNLM) (Mikolov et al., 2013), which have become an important tool in NLP and for semantics in particular in recent years. We used a ‘Skipgram’ model, in which each current word is used as the input of a log-linear classifier with a continuous projection layer that predicts the previous and subsequent words within a defined context window. Skip-grams have been shown to be one of the most accurate models available in a variety of semantics-based NLP tasks, such as word similarity and semantic-relatedness (Baroni et al., 2014). In order to extract the domain-specific thesaurus, an automatically-built corpus of 109 million words was built to 2778 provide informative context to the Skip-gram model, comprising 209,000 articles and documents about computer science and information technology extracted from Wikipedia, plus KDE and OpenOffice manuals. The Skip-gram model is then able to learn – following Harris’ (1954) distributional hypothesis of a word’s semantic features being related to its co-occurrence patterns – word representations as dense scalar vectors of 300 dimensions, with each of these dimensions representi"
L16-1441,P05-1048,0,0.267881,"d token may have multiple distinct meanings. To use a classic example, the word ‘bank’ could be interpreted in the sense of the financial institution or as the slope of land at the side of a river. Some successful approaches to WSD in recent years have been ‘knowledge-based’, with classes of words stored in lexical ontologies such as WordNet (Fellbaum, 1998) where the collective meanings of open-class words (nouns, verbs, adjectives and adverbs) are grouped together as ‘synsets’. While it has long been assumed that an optimally successful MT system must incorporate some kind of WSD component (Carpuat and Wu, 2005), attempts to integrate WSD components into machine translation systems have met with mixed – and usually limited – success. Early attempts at ‘projecting’ word senses directly into a machine translation system (Carpuat and Wu, 2005) were followed by various complete reformulations of the disambiguation process (Carpuat and Wu, 2007; Xiong and Zhang, 2014) – some of which yielded small improvements in translation quality – but the question of whether pure word senses from traditional, knowledge-based WSD approaches can be useful for machine translation still remains. This paper provides furthe"
L16-1441,2007.tmi-papers.6,0,0.197209,"WordNet (Fellbaum, 1998) where the collective meanings of open-class words (nouns, verbs, adjectives and adverbs) are grouped together as ‘synsets’. While it has long been assumed that an optimally successful MT system must incorporate some kind of WSD component (Carpuat and Wu, 2005), attempts to integrate WSD components into machine translation systems have met with mixed – and usually limited – success. Early attempts at ‘projecting’ word senses directly into a machine translation system (Carpuat and Wu, 2005) were followed by various complete reformulations of the disambiguation process (Carpuat and Wu, 2007; Xiong and Zhang, 2014) – some of which yielded small improvements in translation quality – but the question of whether pure word senses from traditional, knowledge-based WSD approaches can be useful for machine translation still remains. This paper provides further evidence to support our previous work (Neale et al., 2015), which experimented in including the output from WSD as contextual features in maxent-based translation models in search of improved performance for machine translation from English to Portuguese. Training our transfer model using a much larger dataset – approximately 1.9"
L16-1441,P07-1005,0,0.0604865,"Missing"
L16-1441,W07-0719,0,0.083017,"Missing"
L16-1441,H05-1052,0,0.0109657,"were minimal and based on a very controlled evaluation trained on a small, in-domain corpus – we found any improvement at all to be a good outcome, and recognized the need to continue our experimentation on a larger, open-domain corpus in the future. 3. Integrating WSD Output into the Machine Translation Pipeline Our chosen WSD algorithm is UKB, a collection of tools and algorithms for performing graph-based WSD over a pre-existing knowledge base (Agirre and Soroa, 2009; Agirre et al., 2014). Based on the graph-based WSD method pioneered by a number of researchers (Navigli and Velardi, 2005; Mihalcea, 2005; Sinha and Mihalcea, 2007; Navigli and Lapata, 2007; Agirre and Soroa, 2008), UKB allows for WordNet-style knowledge bases to be represented as weighted graphs, where word senses correspond to nodes and the relationships or dependencies between pairs of senses correspond to the edges between those nodes. By calculating the probability of a ‘random walk’ over the graph from a target word’s node ending on any other node in the graph – with the nodes (senses) ‘recommending’ each other and being more or less important based on the importance of the other nodes which recommend them – the most appr"
L16-1441,W15-5708,1,0.469514,"to machine translation systems have met with mixed – and usually limited – success. Early attempts at ‘projecting’ word senses directly into a machine translation system (Carpuat and Wu, 2005) were followed by various complete reformulations of the disambiguation process (Carpuat and Wu, 2007; Xiong and Zhang, 2014) – some of which yielded small improvements in translation quality – but the question of whether pure word senses from traditional, knowledge-based WSD approaches can be useful for machine translation still remains. This paper provides further evidence to support our previous work (Neale et al., 2015), which experimented in including the output from WSD as contextual features in maxent-based translation models in search of improved performance for machine translation from English to Portuguese. Training our transfer model using a much larger dataset – approximately 1.9 million English-Portuguese aligned sentences from Europarl – we find that the very small gains reported previously are now statistically significant, confirming our original hypothesis that adding word senses provided by WSD tools as contextual features of a translation model can improve machine translation performance witho"
L16-1441,L16-1483,1,0.857045,"Missing"
L16-1441,W15-4101,1,0.356594,"nslation, TectoMT breaks down source language input and reconstructs target language output according to four layers of representation: the word layer (raw text), the morphological layer, the analytical layer (shallowsyntax) and the tectogrammatical layer (deep-syntax). The three scenarios needed for machine translation – one for analysis (of the source language), one for transfer (of tectogrammatical nodes from source to target language) and one for synthesis (of the target language) – are constructed from a combination of blocks, for which a pipeline for Portuguese has already been created (Silva et al., 2015). To integrate word senses into the machine translation pipeline as this paper describes, new combinations of blocks were introduced in the analysis scenario to first convert input sentences into the context format needed to perform WSD using UKB – which we run over a graph-based representation of the English WordNet (Fellbaum, 1998). Another block runs UKB over these sentence-level contexts, returning the appropriate 8-digit synset identifiers for each target word and mapping them back onto the respective word in the source language input. This process happens at the analytical layer, where s"
L16-1441,P14-1137,0,0.0837546,"98) where the collective meanings of open-class words (nouns, verbs, adjectives and adverbs) are grouped together as ‘synsets’. While it has long been assumed that an optimally successful MT system must incorporate some kind of WSD component (Carpuat and Wu, 2005), attempts to integrate WSD components into machine translation systems have met with mixed – and usually limited – success. Early attempts at ‘projecting’ word senses directly into a machine translation system (Carpuat and Wu, 2005) were followed by various complete reformulations of the disambiguation process (Carpuat and Wu, 2007; Xiong and Zhang, 2014) – some of which yielded small improvements in translation quality – but the question of whether pure word senses from traditional, knowledge-based WSD approaches can be useful for machine translation still remains. This paper provides further evidence to support our previous work (Neale et al., 2015), which experimented in including the output from WSD as contextual features in maxent-based translation models in search of improved performance for machine translation from English to Portuguese. Training our transfer model using a much larger dataset – approximately 1.9 million English-Portugue"
L16-1441,agirre-soroa-2008-using,1,\N,Missing
L16-1483,agerri-etal-2014-ixa,0,0.0139978,"us into a pivot language, English, and this into the remaining six languages. The current annotated corpus covers the first 2,000 sentences of the QTLeap corpus, which have been used to train the MT systems in the project. 3. Annotation Tools In this section, we describe the NERC, NED, WSD and coreference tools used to annotate the corpora. We have chosen the tools based on their performance and their ease of use. We also describe the annotation formats. 3.1. Named-entity recognition and classification Basque, English and Spanish ixa-pipe-nerc is a multilingual NERC tagger, part of IXA pipes (Agerri et al., 2014). Every model has been trained with the averaged Perceptron algorithm as described in Collins (2002) and as implemented in Apache OpenNLP. The datasets used for training the models are the following: Egunkaria dataset for Basque, a combination of Ontonotes 4.0, CoNLL 2003 and MUC 7 for English, and CoNLL 2002 for Spanish. Bulgarian The Bulgarian NERC is a rule-based module. It uses a gazetteer with names categorized in four types: Person, Location, Organization, Other. The identification of new names is based on two factors – sure positions in the text and classifying contextual information, s"
L16-1483,E09-1005,1,0.857409,"ese extraction of DBpedia Spotlight on a local server, then takes an input text pre-processed with lemmas, Part of Speech tags and named entities using the LX-Suite (Branco and Silva, 2006) and converts it to the ’spotted’ format understood by Spotlight. This spotted input text is then disambiguated using DBpedia Spotlight, returning among other information links to existing Portuguese DBpedia resource pages for each named entity discovered. 3024 3.3. Word-sense disambiguation Basque, English and Spanish ixa-pipe-wsd-ukb is based on UKB, a collection of programs for performing graphbased WSD (Agirre and Soroa, 2009). It applies the socalled Personalized PageRank on a Lexical Knowledge Base (LKB) to rank the vertices of the LKB and thus perform disambiguation. WordNet 3.0 is the LKB used for this processing. Bulgarian The basic version of Bulgarian WSD is implemented on the assumption of one sense per discourse and bigram statistics. Czech Two different approaches were used for Czech WSD. The first approach based on the work of Duˇsek et al. (2015) focuses on verbal WSD. The second approach followed for the annotation is a straightforward way of achieving compatibility with English WordNet IDs. Since the"
L16-1483,barreto-etal-2006-open,1,0.822658,"Missing"
L16-1483,bojar-etal-2012-joy,1,0.897675,"Missing"
L16-1483,E06-2024,1,0.746535,"It offers the “disambiguate” and “candidates” service endpoints. The former takes the spotted text input and it returns the DBpedia resource page for each entity. The later is similar to disambiguate, but returns a ranked list of candidates. Portuguese The NED module for Portuguese, LX-NED, uses DBpedia Spotlight to find links to resources about entities identified in pre-processed input text. It creates a process to run a Portuguese extraction of DBpedia Spotlight on a local server, then takes an input text pre-processed with lemmas, Part of Speech tags and named entities using the LX-Suite (Branco and Silva, 2006) and converts it to the ’spotted’ format understood by Spotlight. This spotted input text is then disambiguated using DBpedia Spotlight, returning among other information links to existing Portuguese DBpedia resource pages for each named entity discovered. 3024 3.3. Word-sense disambiguation Basque, English and Spanish ixa-pipe-wsd-ukb is based on UKB, a collection of programs for performing graphbased WSD (Agirre and Soroa, 2009). It applies the socalled Personalized PageRank on a Lexical Knowledge Base (LKB) to rank the vertices of the LKB and thus perform disambiguation. WordNet 3.0 is the"
L16-1483,W02-2004,0,0.181331,"Missing"
L16-1483,W02-1001,0,0.104764,"covers the first 2,000 sentences of the QTLeap corpus, which have been used to train the MT systems in the project. 3. Annotation Tools In this section, we describe the NERC, NED, WSD and coreference tools used to annotate the corpora. We have chosen the tools based on their performance and their ease of use. We also describe the annotation formats. 3.1. Named-entity recognition and classification Basque, English and Spanish ixa-pipe-nerc is a multilingual NERC tagger, part of IXA pipes (Agerri et al., 2014). Every model has been trained with the averaged Perceptron algorithm as described in Collins (2002) and as implemented in Apache OpenNLP. The datasets used for training the models are the following: Egunkaria dataset for Basque, a combination of Ontonotes 4.0, CoNLL 2003 and MUC 7 for English, and CoNLL 2002 for Spanish. Bulgarian The Bulgarian NERC is a rule-based module. It uses a gazetteer with names categorized in four types: Person, Location, Organization, Other. The identification of new names is based on two factors – sure positions in the text and classifying contextual information, such as, titles for persons, types of geographical objects or organizations, etc. The disambiguation"
L16-1483,W15-2111,1,0.864612,"Missing"
L16-1483,hajic-etal-2012-announcing,1,0.885975,"Missing"
L16-1483,W03-1901,0,0.0627303,"n to comparing the cores and the morphological information (gender and number) of the two expressions. As such, we found it easier to directly implement equivalent tests in-code instead of having to feed the extracted features to the Weka J48 classifier proper. 3.5. Annotation formats Basque, Bulgarian, Czech, English and Spanish These corpora are annotated in the NAF format. The NAF format (Fokkens et al., 2014) is a linguistic annotation format designed for complex NLP pipelines that combines strengths of the Linguistic Annotation Framework (LAF) and the NLP Interchange Formats described by Ide and Romary (2003). Because of its layered extensible format, it can easily be incorporated in a variety of NLP modules that may require different linguistic information as their input. Portuguese The corpus for Portuguese is divided into 4 text files - the raw corpus, and one file for the output of each of the three tools used to process it (WSD, NED and coreference). For each of the three tools output is provided in a standoff annotation format, consisting of one token per line (ID of each token in a markable pair in the case of the coreference tool), the appropriate output element of the respective tools (wo"
L16-1483,2005.mtsummit-papers.11,0,0.506663,"ces, the less language-specific differences will remain between the representations of the meaning of the source and target texts. As a result, chances of success are expected to increase considerably by MT systems that are based on deeper semantic engineering approaches. Following this assumption, one of the approaches taken by the QTLeap project1 is to enrich MT training resources with lexico-semantic information. In this work, we present a solid effort to build multilingual parallel corpora annotated at multiple semantic levels. Our overall goal is to enrich two parallel corpora, Europarl (Koehn, 2005) and the QTLeap corpus (Agirre et al., 2015b), with token, lemma, part-of-speech (POS), namedentity recognition and classification (NERC), named-entity disambiguation (NED), word-sense disambiguation (WSD) and coreference for six languages covered in the QTLeap project, namely, Basque (EU), Bulgarian (BG), Czech (CS), English (EN), Portuguese (PT) and Spanish (ES). Specifically, this paper presents the first release of such corpora, which includes NERC, NED, WSD and coreferencelevel annotation for these six languages. Additionally, some languages have extra annotations, such as wikification (E"
L16-1483,W11-1902,0,0.0321161,"ce in Czech. English and Spanish ixa-pipe-coref is loosely based on the Stanford Multi Sieve Pass system (Lee et al., 2013). The system consists of a number of rule-based sieves. Each sieve pass is applied in a deterministic manner, reusing the information generated by the previous sieve and the mention processing. The order in which the sieves are applied favors a highest precision approach and aims at improving the recall with the subsequent application of each of the sieve passes. This is illustrated by the evaluation results of the CoNLL 2011 Coreference Evaluation task (Lee et al., 2013; Lee et al., 2011), in which the Stanford system obtained the best results. The results show a pattern which has also been shown in other results reported with other evaluation sets (Raghunathan et al., 2010), namely, the fact that a large part of the performance of the multi-pass sieve system is based on a set of significant sieves. Thus, this module so far focuses on a subset of sieves only, namely, Speaker Match, Exact Match, Precise Constructs, Strict Head Match and Pronoun Match (Lee et al., 2013). Portuguese For the Portuguese coreference tool, a decision tree classifier was experimented with. Given a pai"
L16-1483,J13-4004,0,0.0359801,"owed for the annotation is a straightforward way of achieving compatibility with English WordNet IDs. Since the Czech corpus contains the same sentences as the English corpus, the English WordNet ID annotation from this corpus is projected onto Czech words using GIZA++ word alignment. Portuguese The Portuguese WSD tool, LX-WSD, is also based on UKB. The LKB from which UKB returns word senses within the pipeline has been generated from an extraction of the Portuguese MultiWordNet6 . 3.4. Coreference Basque ixa-pipe-coref-eu is an adaptation of the Stanford Deterministic Coreference Resolution (Lee et al., 2013), which gives state-of-the art performance for English. The original system applies a succession of ten independent deterministic coreference models or sieves. During the adaptation process, firstly, a baseline system has been created which receives as input texts processed by Basque analysis tools and uses specifically adapted static lists to identify language dependent features like gender, animacy or number. Afterwards, improvements over the baseline system have been applied, adapting and replacing some of the original sieves (Soraluze et al., 2015), taking into account that morphosyntactic"
L16-1483,S07-1008,0,0.017093,"Missing"
L16-1483,S07-1006,0,0.117234,"Missing"
L16-1483,W11-1901,0,0.087359,"Missing"
L16-1483,D10-1048,0,0.0133306,"h sieve pass is applied in a deterministic manner, reusing the information generated by the previous sieve and the mention processing. The order in which the sieves are applied favors a highest precision approach and aims at improving the recall with the subsequent application of each of the sieve passes. This is illustrated by the evaluation results of the CoNLL 2011 Coreference Evaluation task (Lee et al., 2013; Lee et al., 2011), in which the Stanford system obtained the best results. The results show a pattern which has also been shown in other results reported with other evaluation sets (Raghunathan et al., 2010), namely, the fact that a large part of the performance of the multi-pass sieve system is based on a set of significant sieves. Thus, this module so far focuses on a subset of sieves only, namely, Speaker Match, Exact Match, Precise Constructs, Strict Head Match and Pronoun Match (Lee et al., 2013). Portuguese For the Portuguese coreference tool, a decision tree classifier was experimented with. Given a pair of expressions, the classifier returns a true or false value that indicates whether those expressions are coreferent. The classifier was trained over the Summit Corpus (Collovini et al., 2"
L16-1483,P14-5003,1,0.902631,"Missing"
L16-1483,tiedemann-2012-parallel,0,0.0371283,"with corresponding document IDs. Then, sentence boundaries were identified and aligned (for further collection and processing information, see Koehn (2005)). The Europarl corpus consists of monolingual data as well as bilingual parallel data with English as pivot language. In our effort, we have annotated the BG, CS, ES and PT parts of the corpus separately while the EN side of the ESEN language pair was used as pivot language to link all six languages. Given that Europarl does not include Basque, we annotated an alternative publicly available Basque-English parallel corpus, the GNOME corpus (Tiedemann, 2012), which includes GNOME localization files. 2.2. QTLeap corpus The QTLeap corpus consists of 4,000 pairs of questions and respective answers in the domain of IT troubleshooting http://hdl.handle.net/11234/1-1477 4 http://www.statmt.org/europarl/ 3023 for both hardware and software, distributed in four 1,000pair batches (Gaudio et al., 2016). This material was collected using a real-life, commercial online support service via chat. The QTLeap corpus is a unique resource in that it is a multilingual data set with parallel utterances in different languages (Basque, Bulgarian, Czech, Dutch, English"
L16-1483,E14-4045,0,0.0404676,"Missing"
L16-1483,M95-1005,0,0.280516,"Missing"
martinez-agirre-2004-effect,mihalcea-2002-bootstrapping,0,\N,Missing
martinez-agirre-2004-effect,J98-1006,0,\N,Missing
martinez-agirre-2004-effect,agirre-de-lacalle-2004-publicly,1,\N,Missing
martinez-agirre-2004-effect,H93-1061,0,\N,Missing
martinez-agirre-2004-effect,P95-1026,0,\N,Missing
martinez-agirre-2004-effect,W00-1326,1,\N,Missing
martinez-agirre-2004-effect,W00-1702,1,\N,Missing
martinez-agirre-2004-effect,S01-1001,0,\N,Missing
N09-1003,E09-1005,1,0.220187,"raph G (Haveliwala, 2002). Basically, personalized PageRank is computed by modifying the random jump distribution vector in the traditional PageRank equation. In our case, we concentrate all probability mass in the target word. Regarding PageRank implementation details, we chose a damping value of 0.85 and finish the calculation after 30 iterations. These are default values, and we did not optimize them. Our similarity method is similar, but simpler, to that used by (Hughes and Ramage, 2007), which report very good results on similarity datasets. More details of our algorithm can be found in (Agirre and Soroa, 2009). The algorithm and needed resouces are publicly available1 . 2.1 WordNet relations and versions The WordNet versions that we use in this work are the Multilingual Central Repository or MCR (Atserias et al., 2004) (which includes English WordNet version 1.6 and wordnets for several other languages like Spanish, Italian, Catalan and Basque), and WordNet version 3.02 . We used all the relations in MCR (except cooccurrence relations and selectional preference relations) and in WordNet 3.0. Given the recent availability of the disambiguated gloss relations for WordNet 3.03 , we also used a version"
N09-1003,J06-1003,0,0.324061,"ies of words from corpora (Sahami and Heilman, 2006; Chen et al., 2006; Bollegala et al., 2007). In this paper, we explore both families. For the first one we apply graph based algorithms to WordNet, and for the second we induce distributional similarities collected from a 1.6 Terabyte Web corpus. Previous work suggests that distributional similarities suffer from certain limitations, which make 19 them less useful than knowledge resources for semantic similarity. For example, Lin (1998b) finds similar phrases like captive-westerner which made sense only in the context of the corpus used, and Budanitsky and Hirst (2006) highlight other problems that stem from the imbalance and sparseness of the corpora. Comparatively, the experiments in this paper demonstrate that distributional similarities can perform as well as the knowledge-based approaches, and a combination of the two can exceed the performance of results previously reported on the same datasets. An application to cross-lingual (CL) similarity identification is also described, with applications such as CL Information Retrieval or CL sponsored search. A discussion on the differences between learning similarity and relatedness scores is provided. The pap"
N09-1003,P06-1127,0,0.0129682,"tant problem in lexical semantics. It has applications in many natural language processing tasks, such as Textual Entailment, Word Sense Disambiguation or Information Extraction, and other related areas like Information Retrieval. The techniques used to solve this problem can be roughly classified into two main categories: those relying on pre-existing knowledge resources (thesauri, semantic networks, taxonomies or encyclopedias) (Alvarez and Lim, 2007; Yang and Powers, 2005; Hughes and Ramage, 2007) and those inducing distributional properties of words from corpora (Sahami and Heilman, 2006; Chen et al., 2006; Bollegala et al., 2007). In this paper, we explore both families. For the first one we apply graph based algorithms to WordNet, and for the second we induce distributional similarities collected from a 1.6 Terabyte Web corpus. Previous work suggests that distributional similarities suffer from certain limitations, which make 19 them less useful than knowledge resources for semantic similarity. For example, Lin (1998b) finds similar phrases like captive-westerner which made sense only in the context of the corpus used, and Budanitsky and Hirst (2006) highlight other problems that stem from th"
N09-1003,P00-1064,0,0.0140928,"uage dependent). As our WordNet-based method uses the graph of the concepts and relations, we can easily compute the similarity between words from different languages. For example, consider a English-Spanish pair like car – coche. Given that the Spanish WordNet is included in MCR we can use MCR as the common knowledge-base for the relations. We can then compute the personalized PageRank for each of car and coche on the same underlying graph, and then compare the similarity between both probability distributions. As an alternative, we also tried to use publicly available mappings for wordnets (Daude et al., 2000)4 in order to create a 3.0 version of the Spanish WordNet. The mapping was used to link Spanish variants to 3.0 synsets. We used the English WordNet 3.0, including glosses, to construct the graph. The two Spanish WordNet versions are referred to as MCR16 and WN30g. 3 Context-based methods In this section, we describe the distributional methods used for calculating similarities between words, and profiting from the use of a large Web-based corpus. This work is motivated by previous studies that make use of search engines in order to collect cooccurrence statistics between words. Turney (2001) u"
N09-1003,D07-1061,0,0.483155,"-lingual task with minor losses. 1 Introduction Measuring semantic similarity and relatedness between terms is an important problem in lexical semantics. It has applications in many natural language processing tasks, such as Textual Entailment, Word Sense Disambiguation or Information Extraction, and other related areas like Information Retrieval. The techniques used to solve this problem can be roughly classified into two main categories: those relying on pre-existing knowledge resources (thesauri, semantic networks, taxonomies or encyclopedias) (Alvarez and Lim, 2007; Yang and Powers, 2005; Hughes and Ramage, 2007) and those inducing distributional properties of words from corpora (Sahami and Heilman, 2006; Chen et al., 2006; Bollegala et al., 2007). In this paper, we explore both families. For the first one we apply graph based algorithms to WordNet, and for the second we induce distributional similarities collected from a 1.6 Terabyte Web corpus. Previous work suggests that distributional similarities suffer from certain limitations, which make 19 them less useful than knowledge resources for semantic similarity. For example, Lin (1998b) finds similar phrases like captive-westerner which made sense on"
N09-1003,O97-1002,0,0.0918022,"Missing"
N09-1003,P98-2127,0,0.0331888,"ias) (Alvarez and Lim, 2007; Yang and Powers, 2005; Hughes and Ramage, 2007) and those inducing distributional properties of words from corpora (Sahami and Heilman, 2006; Chen et al., 2006; Bollegala et al., 2007). In this paper, we explore both families. For the first one we apply graph based algorithms to WordNet, and for the second we induce distributional similarities collected from a 1.6 Terabyte Web corpus. Previous work suggests that distributional similarities suffer from certain limitations, which make 19 them less useful than knowledge resources for semantic similarity. For example, Lin (1998b) finds similar phrases like captive-westerner which made sense only in the context of the corpus used, and Budanitsky and Hirst (2006) highlight other problems that stem from the imbalance and sparseness of the corpora. Comparatively, the experiments in this paper demonstrate that distributional similarities can perform as well as the knowledge-based approaches, and a combination of the two can exceed the performance of results previously reported on the same datasets. An application to cross-lingual (CL) similarity identification is also described, with applications such as CL Information R"
N09-1003,J07-2002,0,0.0193389,"e entire corpus using an implementation of an Inductive Dependency parser as described in Nivre (2006). For each word w we collect a template of the syntactic context. We consider sequences of governing words (e.g. the parent, grand-parent, etc.) as well as collections of descendants (e.g., immediate children, grandchildren, etc.). This information is then encoded as a contextual template. For example, the context template cooks &lt;term&gt; delicious could be contexts for nouns such as food, meals, pasta, etc. This captures both syntactic preferences as well as selectional preferences. Contrary to Pado and Lapata (2007), we do not use the labels of the syntactic dependencies. Once the vectors have been obtained, the frequency for each dimension in every vector is weighted using the other vectors as contrast set, with the χ2 test, and finally the cosine similarity between vectors is used to calculate the similarity between each pair of terms. Except for the syntactic dependency approach, where closed-class words are needed by the parser, in the other cases we have removed stopwords (pronouns, prepositions, determiners and modal and auxiliary verbs). 3.1 Corpus used We have used a corpus of four billion docume"
N09-1003,W06-2501,0,0.105102,"Missing"
N09-1003,P94-1019,0,0.55483,"Missing"
N09-1003,C98-2122,0,\N,Missing
N10-1058,W05-0620,1,0.899021,"Missing"
N10-1058,A00-2018,0,0.0292806,"Missing"
N10-1058,P07-1028,0,0.14412,"t parsers and SRL systems use just lexical features, which can be seen as the most simple form of SP, 1 http://www.surdeanu.name/mihai/swirl/ 373 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 373–376, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics where the headword needs to be seen in the training data, and otherwise the SP is not satisfied. Gildea and Jurafsky (2002) showed barely significant improvements in semantic role classification of NPs for FrameNet roles using distributional clusters. In (Erk, 2007) a number of SP models are tested in a pseudo-task related to SRL. More recently, we showed (Zapirain et al., 2009) that several methods to automatically generate SPs generalize well and outperform lexical match in a large dataset for semantic role classification, but the impact on a full system was not explored. In this work we apply a subset of the SP methods proposed in (Zapirain et al., 2009). These methods can be split in two main families, depending on the resource used to compute similarity: WordNetbased methods and distributional methods. Both families define a similarity score between"
N10-1058,J02-3001,0,0.663127,"Classification SPs have been widely believed to be an important knowledge source when parsing and performing SRL, especially role classification. Still, present parsers and SRL systems use just lexical features, which can be seen as the most simple form of SP, 1 http://www.surdeanu.name/mihai/swirl/ 373 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 373–376, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics where the headword needs to be seen in the training data, and otherwise the SP is not satisfied. Gildea and Jurafsky (2002) showed barely significant improvements in semantic role classification of NPs for FrameNet roles using distributional clusters. In (Erk, 2007) a number of SP models are tested in a pseudo-task related to SRL. More recently, we showed (Zapirain et al., 2009) that several methods to automatically generate SPs generalize well and outperform lexical match in a large dataset for semantic role classification, but the impact on a full system was not explored. In this work we apply a subset of the SP methods proposed in (Zapirain et al., 2009). These methods can be split in two main families, dependi"
N10-1058,P98-2127,0,0.0232366,"two words was computed using the cosine (or Jaccard measure) of the co-occurrence vectors of the two words. Co-occurrence vectors where constructed using freely available software (Pad´o and Lapata, 2007) run over the British National Corpus. We used the optimal parameters (Pad´o and Lapata, 2007, p. 179). We will refer to these similarities as simcos and simJac , respectively. In contrast, second order similarity uses vectors of similar words, i.e., the similarity of two words was computed using the cosine (or Jaccard measure) between the thesaurus entries of those words in Lin’s thesaurus (Lin, 1998). We refer to these as sim2cos and sim2Jac . Given a target sentence with a verb and its arguments, the task of SR classification is to assign the correct role to each of the arguments. When using SPs alone, we only use the headwords of the ar374 guments, and each argument is classified independently of the rest. For each headword, we select the role (r) of the verb (c) which fits best the head word (w), where the goodness of fit (SPsim (v, r, w)) is modeled using one of the similarity models above, between the headword w and the headwords seen in training data for role r of verb v. This selec"
N10-1058,J07-2002,0,0.0496725,"Missing"
N10-1058,N07-1070,0,0.0906038,"that all systems showed a significant performance degradation (∼10 F1 points) when applied to test data from a different genre of that of the training Mihai Surdeanu Stanford NLP Group Stanford Univ. mihais@stanford.edu set. Pradhan et al. (2008) showed that this performance degradation is essentially caused by the argument classification subtask, and suggested the lexical data sparseness as one of the main reasons. The same authors studied the contribution of the different feature types in SRL and concluded that the lexical features were the most salient features in argument classification (Pradhan et al., 2007). In recent work, we showed (Zapirain et al., 2009) how automatically generated selectional preferences (SP) for verbs were able to perform better than pure lexical features in a role classification experiment, disconnected from a full-fledged SRL system. SPs introduce semantic generalizations on the type of arguments preferred by the predicates and, thus, they are expected to improve results on infrequent and unknown words. The positive effect was especially relevant for out-of-domain data. In this paper we advance (Zapirain et al., 2009) in two directions: (1) We learn separate SPs for prepo"
N10-1058,J08-2006,0,0.183777,"count. Semantic information is usually captured through lexicalized features on the predicate and the head–word of the argument to be classified. Since lexical features tend to be sparse, SRL systems are prone to overfit the training data and generalize poorly to new corpora. Indeed, the SRL evaluation exercises at CoNLL2004 and 2005 (Carreras and M`arquez, 2005) observed that all systems showed a significant performance degradation (∼10 F1 points) when applied to test data from a different genre of that of the training Mihai Surdeanu Stanford NLP Group Stanford Univ. mihais@stanford.edu set. Pradhan et al. (2008) showed that this performance degradation is essentially caused by the argument classification subtask, and suggested the lexical data sparseness as one of the main reasons. The same authors studied the contribution of the different feature types in SRL and concluded that the lexical features were the most salient features in argument classification (Pradhan et al., 2007). In recent work, we showed (Zapirain et al., 2009) how automatically generated selectional preferences (SP) for verbs were able to perform better than pure lexical features in a role classification experiment, disconnected fr"
N10-1058,H93-1054,0,0.563865,"Missing"
N10-1058,P09-2019,1,0.279698,"Missing"
N10-1058,C98-2122,0,\N,Missing
N15-1066,P11-1040,0,0.395988,"ct magnitude is 7.3 (cf. Table 2). The second row shows a first tweet with ambiguous information, which leads our baseline model to extract the incorrect country (in bold; correct country is Peru). The following two tweets help disambiguate the context. The last row shows a tweet containing information (in bold) that is missing in the knowledge base. ing data by aligning a knowledge base of known event instances with tweets (Mintz et al., 2009; Hoffmann et al., 2011), which is then used to train a supervised extraction model (sequence tagger in our case). In seminal work on event extraction, (Benson et al., 2011) applied DS to both detect tweets about local events and then extracted values about two arguments (artist and venue). In our work, we work on automatically selected tweets, and scale the task to complex events with a large number of arguments. We focus on the domain of earthquakes, where each event has up to 20 arguments. Table 2 summarizes this task. The contributions of this work are the following: 1. To our knowledge, this is one of the first works that analyzes the problem of distantly supervised extraction of complex events with many arguments from microblogs. 2. Our analysis shows (Sect"
N15-1066,D12-1091,0,0.0457082,"Missing"
N15-1066,doddington-etal-2004-automatic,0,0.110275,"Missing"
N15-1066,C96-1079,0,0.731575,"Missing"
N15-1066,P11-1055,0,0.0609118,"Table 1: Challenges and opportunities for event extraction from Twitter. The first row shows a tweet with approximate information (in bold); the correct magnitude is 7.3 (cf. Table 2). The second row shows a first tweet with ambiguous information, which leads our baseline model to extract the incorrect country (in bold; correct country is Peru). The following two tweets help disambiguate the context. The last row shows a tweet containing information (in bold) that is missing in the knowledge base. ing data by aligning a knowledge base of known event instances with tweets (Mintz et al., 2009; Hoffmann et al., 2011), which is then used to train a supervised extraction model (sequence tagger in our case). In seminal work on event extraction, (Benson et al., 2011) applied DS to both detect tweets about local events and then extracted values about two arguments (artist and venue). In our work, we work on automatically selected tweets, and scale the task to complex events with a large number of arguments. We focus on the domain of earthquakes, where each event has up to 20 arguments. Table 2 summarizes this task. The contributions of this work are the following: 1. To our knowledge, this is one of the first"
N15-1066,P09-1113,0,0.859192,"tsunami alert issued Table 1: Challenges and opportunities for event extraction from Twitter. The first row shows a tweet with approximate information (in bold); the correct magnitude is 7.3 (cf. Table 2). The second row shows a first tweet with ambiguous information, which leads our baseline model to extract the incorrect country (in bold; correct country is Peru). The following two tweets help disambiguate the context. The last row shows a tweet containing information (in bold) that is missing in the knowledge base. ing data by aligning a knowledge base of known event instances with tweets (Mintz et al., 2009; Hoffmann et al., 2011), which is then used to train a supervised extraction model (sequence tagger in our case). In seminal work on event extraction, (Benson et al., 2011) applied DS to both detect tweets about local events and then extracted values about two arguments (artist and venue). In our work, we work on automatically selected tweets, and scale the task to complex events with a large number of arguments. We focus on the domain of earthquakes, where each event has up to 20 arguments. Table 2 summarizes this task. The contributions of this work are the following: 1. To our knowledge, t"
N15-1066,N10-1021,0,0.117399,"Missing"
N15-1066,reschke-etal-2014-event,1,0.817525,"considerable attention recently. Nevertheless, most of these works focus on the extraction of binary relations from well-formed documents (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). We use the much noisier Twitter as the underlying text, and extract complex events instead of binary relations. We note, however, that the idea of feature aggregation is inspired by these works (Mintz et al., 2009; Riedel et al., 2010), but, to our knowledge, we are the first to apply it to event extraction and sequence tagging. In the DS space, our work is closest to (Reschke et al., 2014), which use it to extract complex events (airplane crashes) from newswire text. Because they focus on newswire, they do not need to address the potential for inaccurate or ambiguous information, which is the main focus of our work. 8 Discussion: An alternate evaluation measure Designing relevant measures for lenient evaluations, such as the one discussed here, is an open research issue. For example, the method proposed in Section 4 gives partial credit to all reported (positive) numeric values in the interval [0, 2g], where g is the correct value for the corresponding slot (see the equation in"
N15-1066,D12-1042,1,0.880611,"notate two argument mentions in the first tweet from Table 1, country and affected-country, as follows: Earthquake in <country>Honduras</country>. So strong it was felt in <affectedcountry>Guatemala</affected-country> as well. 7.1 offshore atlantic. Note that the magnitude in the tweet is different from the one reported in the KB and it will thus be left unmarked (we revisit this issue in Section 5). Using this automatically-generated data, we trained a sequential tagger based on Conditional Random Fields (CRF)9 . Based on the output of the CRF, we inferred the arguments values using noisyor (Surdeanu et al., 2012), which selects the value with the largest probability for each single-valued argument by aggregating the individual mention probabilities produced by the CRF.10 In the case of multi-valued arguments (affected-country and affected-region) we choose all values that had been annotated by the sequential tagger. 3 Initial results and analysis The left block in Table 3 reports the results on development (5-fold cross-validation) of the initial event 8 We identified two types of arguments: those that have binary (yes/no) values (tsunami and landslides) and those having other values. For the first ty"
N15-1165,N09-1003,1,0.755997,"Missing"
N15-1165,agirre-etal-2010-exploring,1,0.950805,"Our technique opens up exciting opportunities to combine distributional and knowledge-based word representations. 1 Figure 1: Main architecture for generating KB word embeddings. A random walk algorithm over the KB produces a synthetic corpus, which is fed into a NNLM to produce continuous word representations. Introduction Graph-based techniques over Knowledge Bases (KB) like WordNet (Fellbaum, 1998) have been widely used in NLP tasks, including word sense disambiguation (Agirre et al., 2014; Moro et al., 2014), semantic similarity and semantic relatedness between terms (Agirre et al., 2009; Agirre et al., 2010; Pilehvar et al., 2013). For instance, Agirre et al. (2009; 2010) apply a random walk algorithm based on Personalized PageRank to WordNet, presenting the best results to date among WordNetbased methods for the well-known WS353 wordsimilarity dataset (Finkelstein et al., 2001). For each target word, the method performs a personalized random walk on the WordNet graph. At convergence, the target word is represented as a vector in a multi-dimensional conceptual space, with one dimension for each concept in the KB. The good results of the algorithm contrast with the large dimensionality of the vec"
N15-1165,P14-1023,0,0.213115,"timizing the likelihood of existing unlabeled text. More recently, Mikolov et al. have developed simpler NNLM architectures (Mikolov et al., 2013a; Mikolov et al., 2013b; Mikolov et al., 2013c), which drastically reduced computational complexity by deleting the hidden layer, enabling to compute accurate word representations from very large corpora. The representations obtained by these methods are compact, taking 1.5G for 3M words on 300-dimensional space, and have been shown to outperform other distributional corpus-based methods on several tasks, including the WS353 word similarity dataset (Baroni et al., 2014). In this work we propose to encode the meaning of words using the structural information in knowledge bases. That is, instead of modeling the meaning based on the co-occurrences of words in corpora, we model the meaning based on random walks over the knowledge base. Each random walk is seen as a context for words in the vocabulary, and fed into the NNLM architecture, which optimizes the likelihood of those contexts (cf. Fig. 1). The resulting word representations are more compact than those produced by regular random walk algorithms (300 vs. tens of thousands), and produce very good results o"
N15-1165,N13-1090,0,0.775244,"ay 31 – June 5, 2015. 2015 Association for Computational Linguistics Turian et al., 2010). NNLM extract meaning from unlabeled corpora following the distributional hypothesis (Harris, 1954), where semantic features of a word are related to its co-occurrence patterns. NNLM learn word representations in the form of dense scalar vectors in n-dimensional spaces (e.g. 300 dimensions), in which each dimension is a latent semantic feature. The representations are obtained by optimizing the likelihood of existing unlabeled text. More recently, Mikolov et al. have developed simpler NNLM architectures (Mikolov et al., 2013a; Mikolov et al., 2013b; Mikolov et al., 2013c), which drastically reduced computational complexity by deleting the hidden layer, enabling to compute accurate word representations from very large corpora. The representations obtained by these methods are compact, taking 1.5G for 3M words on 300-dimensional space, and have been shown to outperform other distributional corpus-based methods on several tasks, including the WS353 word similarity dataset (Baroni et al., 2014). In this work we propose to encode the meaning of words using the structural information in knowledge bases. That is, instea"
N15-1165,Q14-1019,0,0.0361687,"to corpus-based continuous representations, improving the stateof-the-art in the similarity dataset. Our technique opens up exciting opportunities to combine distributional and knowledge-based word representations. 1 Figure 1: Main architecture for generating KB word embeddings. A random walk algorithm over the KB produces a synthetic corpus, which is fed into a NNLM to produce continuous word representations. Introduction Graph-based techniques over Knowledge Bases (KB) like WordNet (Fellbaum, 1998) have been widely used in NLP tasks, including word sense disambiguation (Agirre et al., 2014; Moro et al., 2014), semantic similarity and semantic relatedness between terms (Agirre et al., 2009; Agirre et al., 2010; Pilehvar et al., 2013). For instance, Agirre et al. (2009; 2010) apply a random walk algorithm based on Personalized PageRank to WordNet, presenting the best results to date among WordNetbased methods for the well-known WS353 wordsimilarity dataset (Finkelstein et al., 2001). For each target word, the method performs a personalized random walk on the WordNet graph. At convergence, the target word is represented as a vector in a multi-dimensional conceptual space, with one dimension for each"
N15-1165,P13-1132,0,0.0124099,"p exciting opportunities to combine distributional and knowledge-based word representations. 1 Figure 1: Main architecture for generating KB word embeddings. A random walk algorithm over the KB produces a synthetic corpus, which is fed into a NNLM to produce continuous word representations. Introduction Graph-based techniques over Knowledge Bases (KB) like WordNet (Fellbaum, 1998) have been widely used in NLP tasks, including word sense disambiguation (Agirre et al., 2014; Moro et al., 2014), semantic similarity and semantic relatedness between terms (Agirre et al., 2009; Agirre et al., 2010; Pilehvar et al., 2013). For instance, Agirre et al. (2009; 2010) apply a random walk algorithm based on Personalized PageRank to WordNet, presenting the best results to date among WordNetbased methods for the well-known WS353 wordsimilarity dataset (Finkelstein et al., 2001). For each target word, the method performs a personalized random walk on the WordNet graph. At convergence, the target word is represented as a vector in a multi-dimensional conceptual space, with one dimension for each concept in the KB. The good results of the algorithm contrast with the large dimensionality of the vectors that it needs to pr"
N15-1165,D11-1014,0,0.0681162,"nkelstein et al., 2001). For each target word, the method performs a personalized random walk on the WordNet graph. At convergence, the target word is represented as a vector in a multi-dimensional conceptual space, with one dimension for each concept in the KB. The good results of the algorithm contrast with the large dimensionality of the vectors that it needs to produce, 117K dimensions (one per synset) for WordNet. In recent years a wide variety of Neural Network Language Models (NNLM) have been successfully employed in several tasks, including word similarity (Collobert and Weston, 2008; Socher et al., 2011; 1434 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1434–1439, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics Turian et al., 2010). NNLM extract meaning from unlabeled corpora following the distributional hypothesis (Harris, 1954), where semantic features of a word are related to its co-occurrence patterns. NNLM learn word representations in the form of dense scalar vectors in n-dimensional spaces (e.g. 300 dimensions), in which each dimension is a latent semantic feature. The representations"
N15-1165,P10-1040,0,0.0142789,"ion for each concept in the KB. The good results of the algorithm contrast with the large dimensionality of the vectors that it needs to produce, 117K dimensions (one per synset) for WordNet. In recent years a wide variety of Neural Network Language Models (NNLM) have been successfully employed in several tasks, including word similarity (Collobert and Weston, 2008; Socher et al., 2011; 1434 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1434–1439, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics Turian et al., 2010). NNLM extract meaning from unlabeled corpora following the distributional hypothesis (Harris, 1954), where semantic features of a word are related to its co-occurrence patterns. NNLM learn word representations in the form of dense scalar vectors in n-dimensional spaces (e.g. 300 dimensions), in which each dimension is a latent semantic feature. The representations are obtained by optimizing the likelihood of existing unlabeled text. More recently, Mikolov et al. have developed simpler NNLM architectures (Mikolov et al., 2013a; Mikolov et al., 2013b; Mikolov et al., 2013c), which drastically r"
N15-1165,D14-1167,0,0.0196997,"d relatedness and similarity: WS353 (Finkelstein et al., 2001) and SL999 (Hill et al., 2014b), respectively. We also show that the obtained representations are complementary to those of random walks alone and to distributional representations obtained by the same NNLM algorithm, improving the results. Some recent work has explored embedding KBs in low-dimensional continous vector spaces, representing each entity in a k-dimensional vector and characterizing typed relations between entities in the KB (e.g. born-in-city in Freebase or part-of in WordNet) as operations in the k-dimensional space (Wang et al., 2014). The model estimates the parameters which maximize the likelihood of the triples, which 1435 can then be used to infer new typed relations which are missing in the KB. In contrast, we use the relations to explicitly model the context of words, in two complementary approaches to embed information in KBs into continuous spaces. 2 NNLM Neural Network Language Models have become a useful tool in NLP on the last years, specially in semantics. We have used the two models proposed in (Mikolov et al., 2013c) due to their simplicity and effectiveness in word similarity and relatedness tasks (Baroni et"
N15-1165,J15-4004,0,\N,Missing
N15-1165,J14-1003,1,\N,Missing
P08-1037,J07-4002,0,0.269959,"Missing"
P08-1037,W00-1320,0,0.706722,"line Bikel parser thus represents an advancement in state-of-the-art performance. That we speciﬁcally present results for PP attachment in a parsing context is a combination of us supporting the new research direction for PP attachment established by Atterer and Sch¨utze, and us wishing to reinforce the ﬁndings of Stetina and Nagao that word sense information signiﬁcantly enhances PP attachment performance in this new setting. Lexical semantics in parsing There have been a number of attempts to incorporate word sense information into parsing tasks. The most closely related research is that of Bikel (2000), who merged the Brown portion of the Penn Treebank with SemCor (similarly to our approach in Section 4.1), and used this as the basis for evaluation of a generative bilexical model for joint WSD and parsing. He evaluated his proposed model in a parsing context both with and without WordNet-based sense information, and found that the introduction of sense information either had no impact or degraded parse performance. The only successful applications of word sense information to parsing that we are aware of are Xiong et al. (2005) and Fujita et al. (2007). Xiong et al. (2005) experimented with"
P08-1037,P05-1022,0,0.0092543,"evel of generalisation differs across POS and even the relative syntactic role, e.g. ﬁner-grained semantics are needed for the objects than subjects of verbs. On the other hand, the parsing strategy is very simple, as we just substitute words by their semantic class and then train statistical parsers on the transformed input. The semantic class should be an information source that the parsers take into account in addition to analysing the actual words used. Tighter integration of semantics into the parsing models, possibly in the form of discriminative reranking models (Collins and Koo, 2005; Charniak and Johnson, 2005; McClosky et al., 2006), is a promising way forward in this regard. 7 Conclusions In this work we have trained two state-of-the-art statistical parsers on semantically-enriched input, where content words have been substituted with their semantic classes. This simple method allows us to incorporate lexical semantic information into the parser, without having to reimplement a full statistical parser. We tested the two parsers in both a full parsing and a PP attachment context. This paper shows that semantic classes achieve signiﬁcant improvement both on full parsing and PP attachment tasks rela"
P08-1037,C02-1126,0,0.036289,"Missing"
P08-1037,J05-1003,0,0.0177864,"that the appropriate level of generalisation differs across POS and even the relative syntactic role, e.g. ﬁner-grained semantics are needed for the objects than subjects of verbs. On the other hand, the parsing strategy is very simple, as we just substitute words by their semantic class and then train statistical parsers on the transformed input. The semantic class should be an information source that the parsers take into account in addition to analysing the actual words used. Tighter integration of semantics into the parsing models, possibly in the form of discriminative reranking models (Collins and Koo, 2005; Charniak and Johnson, 2005; McClosky et al., 2006), is a promising way forward in this regard. 7 Conclusions In this work we have trained two state-of-the-art statistical parsers on semantically-enriched input, where content words have been substituted with their semantic classes. This simple method allows us to incorporate lexical semantic information into the parser, without having to reimplement a full statistical parser. We tested the two parsers in both a full parsing and a PP attachment context. This paper shows that semantic classes achieve signiﬁcant improvement both on full parsing"
P08-1037,P96-1025,0,0.0195782,"information can indeed enhance the performance of syntactic disambiguation. 1 Introduction Traditionally, parse disambiguation has relied on structural features extracted from syntactic parse trees, and made only limited use of semantic information. There is both empirical evidence and linguistic intuition to indicate that semantic features can enhance parse disambiguation performance, however. For example, a number of different parsers have been shown to beneﬁt from lexicalisation, that is, the conditioning of structural features on the lexical head of the given constituent (Magerman, 1995; Collins, 1996; Charniak, 1997; Charniak, 2000; Collins, 2003). As an example of lexicalisation, we may observe in our training data that knife often occurs as the manner adjunct of open in prepositional phrases headed by with (c.f. open with a knife), which would provide strong evidence for with (a) knife attaching to open and not box in open the box with a knife. It would not, however, provide any insight into the correct attachment of with scissors in open the box with scissors, as the disambiguation model would not be able to predict that knife and scissors are semantically similar and thus likely to ha"
P08-1037,J03-4003,0,0.060359,"of syntactic disambiguation. 1 Introduction Traditionally, parse disambiguation has relied on structural features extracted from syntactic parse trees, and made only limited use of semantic information. There is both empirical evidence and linguistic intuition to indicate that semantic features can enhance parse disambiguation performance, however. For example, a number of different parsers have been shown to beneﬁt from lexicalisation, that is, the conditioning of structural features on the lexical head of the given constituent (Magerman, 1995; Collins, 1996; Charniak, 1997; Charniak, 2000; Collins, 2003). As an example of lexicalisation, we may observe in our training data that knife often occurs as the manner adjunct of open in prepositional phrases headed by with (c.f. open with a knife), which would provide strong evidence for with (a) knife attaching to open and not box in open the box with a knife. It would not, however, provide any insight into the correct attachment of with scissors in open the box with scissors, as the disambiguation model would not be able to predict that knife and scissors are semantically similar and thus likely to have the same attachment preferences. In order to"
P08-1037,P94-1016,0,0.178508,"and ﬁrst-level hypernyms produced a signiﬁcant improvement over their basic model. Fujita et al. (2007) extended this work in implementing a discriminative parse selection model incorporating word sense information mapped onto upper-level ontologies of differing depths. Based on gold-standard sense information, they achieved large-scale improvements over a basic parse selection model in the context of the Hinoki treebank. Other notable examples of the successful incorporation of lexical semantics into parsing, not through word sense information but indirectly via selectional preferences, are Dowding et al. (1994) and Hektoen (1997). For a broader review of WSD in NLP applications, see Resnik (2006). 3 Integrating Semantics into Parsing Our approach to providing the parsers with sense information is to make available the semantic denotation of each word in the form of a semantic class. This is done simply by substituting the original words with semantic codes. For example, in the earlier example of open with a knife we could substitute both knife and scissors with the class TOOL, and thus directly facilitate semantic generalisation within the parser. There are three main aspects that we have to conside"
P08-1037,W07-1204,0,0.305836,". The most closely related research is that of Bikel (2000), who merged the Brown portion of the Penn Treebank with SemCor (similarly to our approach in Section 4.1), and used this as the basis for evaluation of a generative bilexical model for joint WSD and parsing. He evaluated his proposed model in a parsing context both with and without WordNet-based sense information, and found that the introduction of sense information either had no impact or degraded parse performance. The only successful applications of word sense information to parsing that we are aware of are Xiong et al. (2005) and Fujita et al. (2007). Xiong et al. (2005) experimented with ﬁrst-sense and hypernym features from HowNet and CiLin (both WordNets for Chinese) in a generative parse model applied to the Chinese Penn Treebank. The combination of word sense and ﬁrst-level hypernyms produced a signiﬁcant improvement over their basic model. Fujita et al. (2007) extended this work in implementing a discriminative parse selection model incorporating word sense information mapped onto upper-level ontologies of differing depths. Based on gold-standard sense information, they achieved large-scale improvements over a basic parse selection"
P08-1037,W01-0521,0,0.036849,"Missing"
P08-1037,1997.iwpt-1.15,0,0.634222,"produced a signiﬁcant improvement over their basic model. Fujita et al. (2007) extended this work in implementing a discriminative parse selection model incorporating word sense information mapped onto upper-level ontologies of differing depths. Based on gold-standard sense information, they achieved large-scale improvements over a basic parse selection model in the context of the Hinoki treebank. Other notable examples of the successful incorporation of lexical semantics into parsing, not through word sense information but indirectly via selectional preferences, are Dowding et al. (1994) and Hektoen (1997). For a broader review of WSD in NLP applications, see Resnik (2006). 3 Integrating Semantics into Parsing Our approach to providing the parsers with sense information is to make available the semantic denotation of each word in the form of a semantic class. This is done simply by substituting the original words with semantic codes. For example, in the earlier example of open with a knife we could substitute both knife and scissors with the class TOOL, and thus directly facilitate semantic generalisation within the parser. There are three main aspects that we have to consider in this process:"
P08-1037,N06-2015,0,0.00853735,"Missing"
P08-1037,J98-2002,0,0.0122187,"A selection of SFs is presented in Table 1 for illustration purposes. We experiment with both full synsets and SFs as instances of ﬁne-grained and coarse-grained semantic representation, respectively. As an example of the difference in these two representations, knife in its tool sense is in the EDGE TOOL USED AS A CUTTING INSTRUMENT singleton synset, and also in the ARTIFACT SF along with thousands of other words including cutter. Note that these are the two extremes of semantic granularity in WordNet, and we plan to experiment with intermediate representation levels in future research (c.f. Li and Abe (1998), McCarthy and Carroll (2003), Xiong et al. (2005), Fujita et al. (2007)). As a hybrid representation, we tested the effect of merging words with their corresponding SF (e.g. knife+ARTIFACT ). This is a form of semantic specialisation rather than generalisation, and allows the parser to discriminate between the different senses of each word, but not generalise across words. For each of these three semantic representations, we experimented with substituting each of: (1) all open-class POSs (nouns, verbs, adjectives and adverbs), (2) nouns only, and (3) verbs only. There are thus a total of 9 co"
P08-1037,P98-2127,0,0.0468725,"ote that the ﬁrst sense predictions are based largely on the same dataset as we use in our evaluation, such that the predictions are tuned to our dataset and not fully unsupervised. 3. Automatic Sense Ranking (ASR): First sense tagging as for First Sense above, except that an unsupervised system is used to automatically predict the most frequent sense for each word based on an independent corpus. The method we use to predict the ﬁrst sense is that of McCarthy et al. (2004), which was obtained using a thesaurus automatically created from the British National Corpus (BNC) applying the method of Lin (1998), coupled with WordNetbased similarity measures. This method is fully unsupervised and completely unreliant on any annotations from our dataset. 4 1. Gold-standard: Gold-standard annotations from SemCor. This gives us the upper bound performance of the semantic representation. 321 verbs of eating and drinking verbs of feeling verbs of seeing, hearing, feeling There are some differences with the most frequent sense in SemCor, due to extra corpora used in WordNet development, and also changes in WordNet from the original version used for the SemCor tagging. S YSTEM Baseline SF SFn SFv word + SF"
P08-1037,P95-1037,0,0.0185297,"that word sense information can indeed enhance the performance of syntactic disambiguation. 1 Introduction Traditionally, parse disambiguation has relied on structural features extracted from syntactic parse trees, and made only limited use of semantic information. There is both empirical evidence and linguistic intuition to indicate that semantic features can enhance parse disambiguation performance, however. For example, a number of different parsers have been shown to beneﬁt from lexicalisation, that is, the conditioning of structural features on the lexical head of the given constituent (Magerman, 1995; Collins, 1996; Charniak, 1997; Charniak, 2000; Collins, 2003). As an example of lexicalisation, we may observe in our training data that knife often occurs as the manner adjunct of open in prepositional phrases headed by with (c.f. open with a knife), which would provide strong evidence for with (a) knife attaching to open and not box in open the box with a knife. It would not, however, provide any insight into the correct attachment of with scissors in open the box with scissors, as the disambiguation model would not be able to predict that knife and scissors are semantically similar and th"
P08-1037,J93-2004,0,0.0412602,"Below, we outline these tasks. Parsing As our baseline parsers, we use two state-of-theart lexicalised parsing models, namely the Bikel parser (Bikel, 2004) and Charniak parser (Charniak, 2000). While a detailed description of the respective parsing models is beyond the scope of this paper, it is worth noting that both parsers induce a context free grammar as well as a generative parsing model from a training set of parse trees, and use a development set to tune internal parameters. Traditionally, the two parsers have been trained and evaluated over the WSJ portion of the Penn Treebank (PTB: Marcus et al. (1993)). We diverge from this norm in focusing exclusively on a sense-annotated subset of the Brown Corpus portion of the Penn Treebank, in order to investigate the upper bound performance of the models given gold-standard sense information. PP attachment in a parsing context Prepositional phrase attachment (PP attachment) is the problem of determining the correct attachment site for a PP, conventionally in the form of the noun 318 or verb in a V NP PP structure (Ratnaparkhi et al., 1994; Mitchell, 2004). For instance, in I ate a pizza with anchovies, the PP with anchovies could attach either to the"
P08-1037,J03-4004,0,0.0329229,"is presented in Table 1 for illustration purposes. We experiment with both full synsets and SFs as instances of ﬁne-grained and coarse-grained semantic representation, respectively. As an example of the difference in these two representations, knife in its tool sense is in the EDGE TOOL USED AS A CUTTING INSTRUMENT singleton synset, and also in the ARTIFACT SF along with thousands of other words including cutter. Note that these are the two extremes of semantic granularity in WordNet, and we plan to experiment with intermediate representation levels in future research (c.f. Li and Abe (1998), McCarthy and Carroll (2003), Xiong et al. (2005), Fujita et al. (2007)). As a hybrid representation, we tested the effect of merging words with their corresponding SF (e.g. knife+ARTIFACT ). This is a form of semantic specialisation rather than generalisation, and allows the parser to discriminate between the different senses of each word, but not generalise across words. For each of these three semantic representations, we experimented with substituting each of: (1) all open-class POSs (nouns, verbs, adjectives and adverbs), (2) nouns only, and (3) verbs only. There are thus a total of 9 combinations of representation"
P08-1037,P04-1036,0,0.0610971,"1: A selection of WordNet SFs 2. First Sense (1 ST): All token instances of a given word are tagged with their most frequent sense in WordNet.4 Note that the ﬁrst sense predictions are based largely on the same dataset as we use in our evaluation, such that the predictions are tuned to our dataset and not fully unsupervised. 3. Automatic Sense Ranking (ASR): First sense tagging as for First Sense above, except that an unsupervised system is used to automatically predict the most frequent sense for each word based on an independent corpus. The method we use to predict the ﬁrst sense is that of McCarthy et al. (2004), which was obtained using a thesaurus automatically created from the British National Corpus (BNC) applying the method of Lin (1998), coupled with WordNetbased similarity measures. This method is fully unsupervised and completely unreliant on any annotations from our dataset. 4 1. Gold-standard: Gold-standard annotations from SemCor. This gives us the upper bound performance of the semantic representation. 321 verbs of eating and drinking verbs of feeling verbs of seeing, hearing, feeling There are some differences with the most frequent sense in SemCor, due to extra corpora used in WordNet d"
P08-1037,N06-1020,0,0.00748381,"rs across POS and even the relative syntactic role, e.g. ﬁner-grained semantics are needed for the objects than subjects of verbs. On the other hand, the parsing strategy is very simple, as we just substitute words by their semantic class and then train statistical parsers on the transformed input. The semantic class should be an information source that the parsers take into account in addition to analysing the actual words used. Tighter integration of semantics into the parsing models, possibly in the form of discriminative reranking models (Collins and Koo, 2005; Charniak and Johnson, 2005; McClosky et al., 2006), is a promising way forward in this regard. 7 Conclusions In this work we have trained two state-of-the-art statistical parsers on semantically-enriched input, where content words have been substituted with their semantic classes. This simple method allows us to incorporate lexical semantic information into the parser, without having to reimplement a full statistical parser. We tested the two parsers in both a full parsing and a PP attachment context. This paper shows that semantic classes achieve signiﬁcant improvement both on full parsing and PP attachment tasks relative to the baseline par"
P08-1037,H94-1048,0,0.517562,"eters. Traditionally, the two parsers have been trained and evaluated over the WSJ portion of the Penn Treebank (PTB: Marcus et al. (1993)). We diverge from this norm in focusing exclusively on a sense-annotated subset of the Brown Corpus portion of the Penn Treebank, in order to investigate the upper bound performance of the models given gold-standard sense information. PP attachment in a parsing context Prepositional phrase attachment (PP attachment) is the problem of determining the correct attachment site for a PP, conventionally in the form of the noun 318 or verb in a V NP PP structure (Ratnaparkhi et al., 1994; Mitchell, 2004). For instance, in I ate a pizza with anchovies, the PP with anchovies could attach either to the verb (c.f. ate with anchovies) or to the noun (c.f. pizza with anchovies), of which the noun is the correct attachment site. With I ate a pizza with friends, on the other hand, the verb is the correct attachment site. PP attachment is a structural ambiguity problem, and as such, a subproblem of parsing. Traditionally the so-called RRR data (Ratnaparkhi et al., 1994) has been used to evaluate PP attachment algorithms. RRR consists of 20,081 training and 3,097 test quadruples of the"
P08-1037,W97-0109,0,0.0957769,"rb (c.f. ate with anchovies) or to the noun (c.f. pizza with anchovies), of which the noun is the correct attachment site. With I ate a pizza with friends, on the other hand, the verb is the correct attachment site. PP attachment is a structural ambiguity problem, and as such, a subproblem of parsing. Traditionally the so-called RRR data (Ratnaparkhi et al., 1994) has been used to evaluate PP attachment algorithms. RRR consists of 20,081 training and 3,097 test quadruples of the form (v,n1,p,n2), where the attachment decision is either v or n1. The best published results over RRR are those of Stetina and Nagao (1997), who employ WordNet sense predictions from an unsupervised WSD method within a decision tree classiﬁer. Their work is particularly inspiring in that it signiﬁcantly outperformed the plethora of lexicalised probabilistic models that had been proposed to that point, and has not been beaten in later attempts. In a recent paper, Atterer and Sch¨utze (2007) criticised the RRR dataset because it assumes that an oracle parser provides the two hypothesised structures to choose between. This is needed to derive the fact that there are two possible attachment sites, as well as information about the lex"
P08-1037,I05-1007,0,0.0689819,"ation into parsing tasks. The most closely related research is that of Bikel (2000), who merged the Brown portion of the Penn Treebank with SemCor (similarly to our approach in Section 4.1), and used this as the basis for evaluation of a generative bilexical model for joint WSD and parsing. He evaluated his proposed model in a parsing context both with and without WordNet-based sense information, and found that the introduction of sense information either had no impact or degraded parse performance. The only successful applications of word sense information to parsing that we are aware of are Xiong et al. (2005) and Fujita et al. (2007). Xiong et al. (2005) experimented with ﬁrst-sense and hypernym features from HowNet and CiLin (both WordNets for Chinese) in a generative parse model applied to the Chinese Penn Treebank. The combination of word sense and ﬁrst-level hypernyms produced a signiﬁcant improvement over their basic model. Fujita et al. (2007) extended this work in implementing a discriminative parse selection model incorporating word sense information mapped onto upper-level ontologies of differing depths. Based on gold-standard sense information, they achieved large-scale improvements over"
P08-1037,A00-2018,0,\N,Missing
P08-1037,J04-4004,0,\N,Missing
P08-1037,C98-2122,0,\N,Missing
P08-1063,W05-0620,1,0.882424,"Missing"
P08-1063,H94-1020,0,0.239361,"Missing"
P08-1063,J05-1004,0,0.670746,"lysis allows to determine “who” did “what” to “whom”, “when” and “where”, and, thus, characterize the participants and properties of the events established by the predicates. This kind of semantic analysis is very interesting for a broad spectrum of NLP applications (information extraction, summarization, question answering, machine translation, etc.), since it opens the door to exploit the semantic relations among linguistic constituents. The properties of the semantically annotated corpora available have conditioned the type of research and systems that have been developed so far. PropBank (Palmer et al., 2005) is the most widely used corpus for training SRL systems, probably because it contains running text from the Penn Treebank corpus with annotations on all verbal predicates. Also, a few evaluation exercises on SRL have been conducted on this corpus in the CoNLL-2004 and 2005 conferences. However, a serious criticisms to the PropBank corpus refers to the role set it uses, which consists of a set of numbered core arguments, whose semantic translation is verb-dependent. While Arg0 and Arg1 are intended to indicate the general roles of Agent and Theme, other argument numbers do not generalize acros"
P08-1063,S07-1016,0,0.0984302,"uistic theory (e.g., Agent, Theme, Patient, Recipient, Cause, etc.). We foresee two advantages of using such thematic roles. On the one hand, statistical SRL systems trained from them could generalize better and, therefore, be more robust and portable, as suggested in (Yi et al., 2007). On the other hand, roles in a paradigm like VerbNet would allow for inferences over the assigned roles, which is only possible in a more limited way with PropBank. In a previous paper (Zapirain et al., 2008), we presented a first comparison between the two previous role sets on the SemEval-2007 Task 17 corpus (Pradhan et al., 2007). The SemEval-2007 corpus only 550 Proceedings of ACL-08: HLT, pages 550–558, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics comprised examples about 50 different verbs. The results of that paper were, thus, considered preliminary, as they could depend on the small amount of data (both in training data and number of verbs) or the specific set of verbs being used. Now, we extend those experiments to the entire PropBank corpus, and we include two extra experiments on domain shifts (using the Brown corpus as test set) and on grouping VerbNet labels. More concrete"
P08-1063,N07-1069,0,0.468698,"Missing"
P08-1063,S07-1077,1,0.89617,"Missing"
P09-2019,E03-1034,0,0.404843,"Missing"
P09-2019,W05-0620,1,0.840172,"Missing"
P09-2019,P07-1028,0,0.614192,"l preferences in a real SRL task. They used distributional clustering and WordNet-based techniques on a SRL task on FrameNet roles. They report a very small improvement of the overall performance when using distributional clustering techniques. In this paper we present complementary experiments, with a different role set and annotated corpus (PropBank), a wider range of selectional preference models, and the analysis of out-of-domain results. Other papers applying semantic preferences in the context of semantic roles, rely on the evaluation on pseudo tasks or human plausibility judgments. In (Erk, 2007) a distributional similarity–based model for selectional preferences is introduced, reminiscent of that of Pantel and Lin (2000). The results over 100 frame-specific roles showed that distributional similarities get smaller error rates than Resnik and EM, with Lin’s formula having the smallest error rate. Moreover, coverage of distributional similarities and Resnik are rather low. Our distributional model for selectional preferences follows her formalization. Currently, there are several models of distributional similarity that could be used for selectional preferences. More recently, Pad´o an"
P09-2019,J02-3001,0,0.342338,", we tried the optimal parameters as described in (Pad´o and Lapata, 2007, p. 179): word-based space, medium context, loglikelihood association, and 2,000 basis elements. We tested Jaccard, cosine and Lin’s measure (Lin, 1998) for similarity, yielding simjac , simcos and simlin , respectively. Distributional similarity has also been used to tackle syntactic ambiguity. Pantel and Lin (2000) obtained very good results using the distributional similarity measure defined by Lin (1998). The application of selectional preferences to semantic roles (as opposed to syntactic functions) is more recent. Gildea and Jurafsky (2002) is the only one applying selectional preferences in a real SRL task. They used distributional clustering and WordNet-based techniques on a SRL task on FrameNet roles. They report a very small improvement of the overall performance when using distributional clustering techniques. In this paper we present complementary experiments, with a different role set and annotated corpus (PropBank), a wider range of selectional preference models, and the analysis of out-of-domain results. Other papers applying semantic preferences in the context of semantic roles, rely on the evaluation on pseudo tasks o"
P09-2019,P98-2127,0,0.719365,"evaluation. Resnik’s selectional preference scored best among classbased methods, but it performed equal to a simple, purely lexical, conditional probability model. JFK was assassinated (in Dallas)Location JFK was assassinated (in November)T emporal 73 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 73–76, c Suntec, Singapore, 4 August 2009. 2009 ACL and AFNLP WordNet-based SP models: we use Resnik’s selectional preference model. Distributional SP models: Given the availability of publicly available resources for distributional similarity, we used 1) a ready-made thesaurus (Lin, 1998), and 2) software (Pad´o and Lapata, 2007) which we run on the British National Corpus (BNC). In the first case, Lin constructed his thesaurus based on his own similarity formula run over a large parsed corpus comprising journalism texts. The thesaurus lists, for each word, the most similar words, with their weight. In order to get the similarity for two words, we could check the entry in the thesaurus for either word. But given that the thesaurus is not symmetric, we take the average of both similarities. We will refer to this similarity measure as simth lin . Another option is to use second-"
P09-2019,J07-2002,0,0.0769415,"Missing"
P09-2019,P00-1014,0,0.0602223,"here we compute the similarity of two words using the entries in the thesaurus, either using the cosine or Jaccard measures. We will refer to these similarity measures th2 as simth2 jac and simcos hereinafter. For the second case, we tried the optimal parameters as described in (Pad´o and Lapata, 2007, p. 179): word-based space, medium context, loglikelihood association, and 2,000 basis elements. We tested Jaccard, cosine and Lin’s measure (Lin, 1998) for similarity, yielding simjac , simcos and simlin , respectively. Distributional similarity has also been used to tackle syntactic ambiguity. Pantel and Lin (2000) obtained very good results using the distributional similarity measure defined by Lin (1998). The application of selectional preferences to semantic roles (as opposed to syntactic functions) is more recent. Gildea and Jurafsky (2002) is the only one applying selectional preferences in a real SRL task. They used distributional clustering and WordNet-based techniques on a SRL task on FrameNet roles. They report a very small improvement of the overall performance when using distributional clustering techniques. In this paper we present complementary experiments, with a different role set and ann"
P09-2019,J08-2006,0,0.0691101,"WordNet based models, which have a lower word coverage compared to distributional similarity–based models. 5 prec. .779 .589 .573 .607 .580 .635 .657 .654 co-occurrences extracted from the BNC (simJac , simcos simLin ), and the results obtained when using Lin’s thesaurus directly (simth Lin ) and as a th2 ). second-order vector (simth2 and sim cos Jac As expected, the lexical baseline attains very high precision in all datasets, which underscores the importance of the lexical head word features in argument classification. The recall is quite low, specially in Brown, confirming and extending (Pradhan et al., 2008), which also reports similar performance drops when doing argument classification on out-of-domain data. One of the main goals of our experiments is to overcome the data sparseness of lexical features both on in-domain and out-of-domain data. All our selectional preference models improve over the lexical matching baseline in recall, up to 30 absolute percentage points in the WSJ test dataset and 44 absolute percentage points in the Brown corpus. This comes at the cost of reduced precision, but the overall F-score shows that all selectional preference models improve over the baseline, with up t"
P09-2019,H93-1054,0,0.300645,"nd generalize poorly to new corpora. This work explores the usefulness of selectional preferences to alleviate the lexical dependence of SRL systems. Selectional preferences introduce semantic generalizations on the type of arguments preferred by the predicates. Therefore, they are expected to improve generalization on infrequent and unknown words, and increase the discriminative power of the argument classifiers. For instance, consider these two sentences: 2 Related Work Automatic acquisition of selectional preferences is a relatively old topic, and will mention the most relevant references. Resnik (1993) proposed to model selectional preferences using semantic classes from WordNet in order to tackle ambiguity issues in syntax (noun-compounds, coordination, PP-attachment). Brockman and Lapata (2003) compared several class-based models (including Resnik’s selectional preferences) on a syntactic plausibility judgement task for German. The models return weights for (verb, syntactic function, noun) triples, and the correlation with human plausibility judgement is used for evaluation. Resnik’s selectional preference scored best among classbased methods, but it performed equal to a simple, purely le"
P09-2019,N07-1070,0,\N,Missing
P09-2019,C98-2122,0,\N,Missing
P11-2123,P08-1037,1,0.85019,"s been an interesting research avenue since the early days of NLP, and several research works have tried to test the intuition that semantics should help parsing, as can be exemplified by the classical PP attachment experiments (Ratnaparkhi, 1994). Although there have been some significant results (see Section 2), this issue continues to be elusive. In principle, dependency parsing offers good prospects for experimenting with word-to-word-semantic relationships. We present a set of experiments using semantic classes in dependency parsing of the Penn Treebank (PTB). We extend the tests made in Agirre et al. (2008), who used different types of semantic information, obtaining significant improvements in two constituency parsers, showing how semantic information helps in constituency parsing. As our baseline parser, we use MaltParser (Nivre, 2006). We will evaluate the parser on both the full PTB (Marcus et al. 1993) and on a senseRelated Work Agirre et al. (2008) trained two state-of-the-art statistical parsers (Charniak, 2000; Bikel, 2004) on semantically-enriched input, where content words had been substituted with their semantic classes. This was done trying to overcome the limitations of lexicalized"
P11-2123,W10-1409,0,0.0250465,"Missing"
P11-2123,W07-2217,0,0.225938,"Missing"
P11-2123,P96-1025,0,0.0264451,"information, obtaining significant improvements in two constituency parsers, showing how semantic information helps in constituency parsing. As our baseline parser, we use MaltParser (Nivre, 2006). We will evaluate the parser on both the full PTB (Marcus et al. 1993) and on a senseRelated Work Agirre et al. (2008) trained two state-of-the-art statistical parsers (Charniak, 2000; Bikel, 2004) on semantically-enriched input, where content words had been substituted with their semantic classes. This was done trying to overcome the limitations of lexicalized approaches to parsing (Magerman, 1995; Collins, 1996; Charniak, 1997; Collins, 2003), where related words, like scissors and knife cannot be generalized. This simple method allowed incorporating lexical semantic information into the parser. They tested the parsers in both a full parsing and a PP attachment context. The experiments showed that semantic classes gave significant improvement relative to the baseline, demonstrating that a simplistic approach to incorporating lexical semantics into a parser significantly improves its performance. This work presented the first results over both WordNet and the Penn Treebank to show that semantic proce"
P11-2123,W00-1320,0,0.189552,"lins, 2003), where related words, like scissors and knife cannot be generalized. This simple method allowed incorporating lexical semantic information into the parser. They tested the parsers in both a full parsing and a PP attachment context. The experiments showed that semantic classes gave significant improvement relative to the baseline, demonstrating that a simplistic approach to incorporating lexical semantics into a parser significantly improves its performance. This work presented the first results over both WordNet and the Penn Treebank to show that semantic processing helps parsing. Collins (2000) tested a combined parsing/word sense disambiguation model based in WordNet which did not obtain improvements in parsing. Koo et al. (2008) presented a semisupervised method for training dependency parsers, using word clusters derived from a large unannotated corpus as features. They demonstrate the effectiveness of the approach in a series of dependency parsing experiments on PTB and the Prague Dependency Treebank, showing that the cluster-based features yield substantial gains in performance across a wide range of conditions. Suzuki et al. (2009) also experiment with the same method combined"
P11-2123,J03-4003,0,0.00944448,"ant improvements in two constituency parsers, showing how semantic information helps in constituency parsing. As our baseline parser, we use MaltParser (Nivre, 2006). We will evaluate the parser on both the full PTB (Marcus et al. 1993) and on a senseRelated Work Agirre et al. (2008) trained two state-of-the-art statistical parsers (Charniak, 2000; Bikel, 2004) on semantically-enriched input, where content words had been substituted with their semantic classes. This was done trying to overcome the limitations of lexicalized approaches to parsing (Magerman, 1995; Collins, 1996; Charniak, 1997; Collins, 2003), where related words, like scissors and knife cannot be generalized. This simple method allowed incorporating lexical semantic information into the parser. They tested the parsers in both a full parsing and a PP attachment context. The experiments showed that semantic classes gave significant improvement relative to the baseline, demonstrating that a simplistic approach to incorporating lexical semantics into a parser significantly improves its performance. This work presented the first results over both WordNet and the Penn Treebank to show that semantic processing helps parsing. Collins (20"
P11-2123,W07-1204,0,0.339922,"Missing"
P11-2123,W07-2416,0,0.0408988,"Missing"
P11-2123,1997.iwpt-1.15,0,0.307704,"Missing"
P11-2123,P08-1068,0,0.047626,"ic information into the parser. They tested the parsers in both a full parsing and a PP attachment context. The experiments showed that semantic classes gave significant improvement relative to the baseline, demonstrating that a simplistic approach to incorporating lexical semantics into a parser significantly improves its performance. This work presented the first results over both WordNet and the Penn Treebank to show that semantic processing helps parsing. Collins (2000) tested a combined parsing/word sense disambiguation model based in WordNet which did not obtain improvements in parsing. Koo et al. (2008) presented a semisupervised method for training dependency parsers, using word clusters derived from a large unannotated corpus as features. They demonstrate the effectiveness of the approach in a series of dependency parsing experiments on PTB and the Prague Dependency Treebank, showing that the cluster-based features yield substantial gains in performance across a wide range of conditions. Suzuki et al. (2009) also experiment with the same method combined with semi-supervised learning. 699 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pa"
P11-2123,P95-1037,0,0.0201255,"pes of semantic information, obtaining significant improvements in two constituency parsers, showing how semantic information helps in constituency parsing. As our baseline parser, we use MaltParser (Nivre, 2006). We will evaluate the parser on both the full PTB (Marcus et al. 1993) and on a senseRelated Work Agirre et al. (2008) trained two state-of-the-art statistical parsers (Charniak, 2000; Bikel, 2004) on semantically-enriched input, where content words had been substituted with their semantic classes. This was done trying to overcome the limitations of lexicalized approaches to parsing (Magerman, 1995; Collins, 1996; Charniak, 1997; Collins, 2003), where related words, like scissors and knife cannot be generalized. This simple method allowed incorporating lexical semantic information into the parser. They tested the parsers in both a full parsing and a PP attachment context. The experiments showed that semantic classes gave significant improvement relative to the baseline, demonstrating that a simplistic approach to incorporating lexical semantics into a parser significantly improves its performance. This work presented the first results over both WordNet and the Penn Treebank to show that"
P11-2123,J93-2004,0,0.038773,"tion 2), this issue continues to be elusive. In principle, dependency parsing offers good prospects for experimenting with word-to-word-semantic relationships. We present a set of experiments using semantic classes in dependency parsing of the Penn Treebank (PTB). We extend the tests made in Agirre et al. (2008), who used different types of semantic information, obtaining significant improvements in two constituency parsers, showing how semantic information helps in constituency parsing. As our baseline parser, we use MaltParser (Nivre, 2006). We will evaluate the parser on both the full PTB (Marcus et al. 1993) and on a senseRelated Work Agirre et al. (2008) trained two state-of-the-art statistical parsers (Charniak, 2000; Bikel, 2004) on semantically-enriched input, where content words had been substituted with their semantic classes. This was done trying to overcome the limitations of lexicalized approaches to parsing (Magerman, 1995; Collins, 1996; Charniak, 1997; Collins, 2003), where related words, like scissors and knife cannot be generalized. This simple method allowed incorporating lexical semantic information into the parser. They tested the parsers in both a full parsing and a PP attachmen"
P11-2123,P04-1036,0,0.0166737,"(wordform+SF), WSF_N (wordform+SF for nouns) and WSF_V (for verbs). For a given semantic representation, we need some form of WSD to determine the semantics of each token occurrence of a target word. We experimented with three options: a) gold-standard (GOLD) annotations from SemCor, which gives the upper bound performance of the semantic representation, b) first Sense (1ST), where all token instances of a given word are tagged with their most frequent sense in WordNet, and c) automatic Sense Ranking (ASR) which uses the sense returned by an unsupervised system based on an independent corpus (McCarthy et al. 2004). For the full Penn Treebank experiments, we only had access to the first sense, taken from Wordnet 1.7. 4 Results In the following two subsections, we will first present the results in the SemCor/PTB intersection, with the option of using gold, 1st sense and automatic sense information (subsection 4.1) and the next subsection (4.2) will show the results on the full PTB, using 1st sense information. All results are shown as labelled attachment score (LAS). 4.1 Semcor/PTB (GOLD/1ST/ASR) We conducted a series of experiments testing: • Each individual semantic feature, which gives 9 possibilities"
P11-2123,H94-1048,0,0.0188251,"Missing"
P11-2123,I05-1007,0,0.183342,"Missing"
P11-2123,P11-2033,1,0.824536,"Missing"
P11-2123,A00-2018,0,\N,Missing
P11-2123,J04-4004,0,\N,Missing
P11-2123,P10-1001,0,\N,Missing
P11-2123,D09-1058,0,\N,Missing
P11-2123,D07-1096,1,\N,Missing
P13-4026,W12-1014,1,0.797453,"ed. Consequently links to relevant articles in Wikipedia were added to each the meta-data of each artefact using Wikipedia Miner (Milne and Witten, 2008) to provide background information. In addition to the link, Wikipedia Miner returns a confidence value between 0 and 1 for each link based on the context of the item. The accuracy of the links added by Wikipedia Miner were evaluated using the meta-data associated with 21 randomly selected artefacts. Three annotators analysed the links added and found that a confidence value of 0.5 represented a good balance between accuracy and coverage. See Fernando and Stevenson (2012) for further details. Filtered 466,958 1,219,731 14,983 1,701,672 Table 1: Number of artefacts in Europeana collections before and after filtering 4 Data Processing A range of pre-preprocessing steps were carried out on these collections to provide additional information to support navigation in the PATHS system. 4.1 Artefact Similarity 4.3 We begin by computing the similarity between the various artefacts in the Europeana collections. This information is useful for navigation and recommendation but is not available in the Europeana collections since they are drawn from a diverse range of sour"
P13-4026,C12-1054,1,0.879666,"Missing"
P13-4026,W07-0907,0,0.022716,"this information is used within the system. 1 2 Related Work Heitzman et al. (1997) describe the ILEX system which acts as a guide through the jewellery collection of the National Museum of Scotland. The user explores the collection through a set of web pages which provide descriptions of each artefact that are personalised for each user. The system makes use of information about the artefacts the user has viewed to build up a model of their interests and uses this to customise the descriptions of each artefact and provide recommendations for further artefacts in which they may be interested. Grieser et al. (2007) also explore providing recommendations based on the artefacts a user has viewed so far. They make use of a range of techniques including language modelling, geospatial modelling and analysis of previous visitors’ behaviour to provide recommendations to visitors to the Melbourne Museum. Grieser et al. (2011) explore methods for determining the similarity between museum artefacts, commenting that this is useful for navigation through these collections and important for personalisation (Bowen and Filippini-Fantoni, 2004; O’Donnell et al., 2001), recommendation (Bohnert et al., 2009; Trant, 2009)"
P14-2106,P08-1068,0,0.258192,"nversion. 1 Introduction This work presents a set of experiments to investigate the use of lexical semantic information in dependency parsing of English. Whether semantics improve parsing is one interesting research topic both on parsing and lexical semantics. Broadly speaking, we can classify the methods to incorporate semantic information into parsers in two: systems using static lexical semantic repositories, such as WordNet or similar ontologies (Agirre et al., 2008; Agirre et al., 2011; Fujita et al., 2010), and systems using dynamic semantic clusters automatically acquired from corpora (Koo et al., 2008; Suzuki et al., 2009). Our main objective will be to determine whether static semantic knowledge can help parsing. We will apply different types of semantic information to three dependency parsers. Specifically, we will test the following questions: • Does semantic information in WordNet help dependency parsing? Agirre et al. (2011) found improvements in dependency parsing 2 Related work Broadly speaking, we can classify the attempts to add external knowledge to a parser in two sets: using large semantic repositories such as WordNet and approaches that use information automatically acquired f"
P14-2106,P08-1037,1,0.914971,"ombinations, showing that the semantically enhanced parsers yield a small significant gain only on the more semantically oriented LTH treebank conversion. 1 Introduction This work presents a set of experiments to investigate the use of lexical semantic information in dependency parsing of English. Whether semantics improve parsing is one interesting research topic both on parsing and lexical semantics. Broadly speaking, we can classify the methods to incorporate semantic information into parsers in two: systems using static lexical semantic repositories, such as WordNet or similar ontologies (Agirre et al., 2008; Agirre et al., 2011; Fujita et al., 2010), and systems using dynamic semantic clusters automatically acquired from corpora (Koo et al., 2008; Suzuki et al., 2009). Our main objective will be to determine whether static semantic knowledge can help parsing. We will apply different types of semantic information to three dependency parsers. Specifically, we will test the following questions: • Does semantic information in WordNet help dependency parsing? Agirre et al. (2011) found improvements in dependency parsing 2 Related work Broadly speaking, we can classify the attempts to add external kno"
P14-2106,S12-1031,0,0.0345955,"Missing"
P14-2106,P11-2123,1,0.834298,"r introducing related work in section 2, section 3 describes the treebank conversions, parsers and semantic features. Section 4 presents the results and section 5 draws the main conclusions. This paper presents experiments with WordNet semantic classes to improve dependency parsing. We study the effect of semantic classes in three dependency parsers, using two types of constituencyto-dependency conversions of the English Penn Treebank. Overall, we can say that the improvements are small and not significant using automatic POS tags, contrary to previously published results using gold POS tags (Agirre et al., 2011). In addition, we explore parser combinations, showing that the semantically enhanced parsers yield a small significant gain only on the more semantically oriented LTH treebank conversion. 1 Introduction This work presents a set of experiments to investigate the use of lexical semantic information in dependency parsing of English. Whether semantics improve parsing is one interesting research topic both on parsing and lexical semantics. Broadly speaking, we can classify the methods to incorporate semantic information into parsers in two: systems using static lexical semantic repositories, such"
P14-2106,P04-1036,0,0.0398379,"UMENT singleton synset, and also in the ARTIFACT SF along with thousands of words including cutter. These are the two extremes of semantic granularity in WordNet. For each semantic representation, we need to determine the semantics of each occurrence of a target word. Agirre et al. (2011) used i) gold-standard annotations from SemCor, a subset of the PTB, to give an upper bound performance of the semantic representation, ii) first sense, where all instances of a word were tagged with their most frequent sense, and iii) automatic sense ranking, predicting the most frequent sense for each word (McCarthy et al., 2004). As we will make use of the full PTB, we only have access to the first sense information. Clusters. Koo et al. (2008) describe a semi4 Results In all the experiments we employed a baseline feature set using word forms and parts of speech, and an enriched feature set (WordNet or clusters). We firstly tested the addition of each individual semantic feature to each parser, evaluating its contribution to the parser’s performance. For the combinations, instead of feature-engineering each parser with the wide array of different possibilities for features, as in Agirre et al. (2011), we adopted the"
P14-2106,J04-4004,0,0.106253,"Missing"
P14-2106,W10-1409,0,0.0387869,"Missing"
P14-2106,P05-1012,0,0.0747214,"output are lower than those for Penn2Malt conversions. 3.2 We have made use of three parsers representative of successful paradigms in dependency parsing. MaltParser (Nivre et al., 2007) is a deterministic transition-based dependency parser that obtains a dependency tree in linear-time in a single pass over the input using a stack of partially analyzed items and the remaining input sequence, by means of history-based feature models. We added two features that inspect the semantic feature at the top of the stack and the next input token. MST 3 represents global, exhaustive graphbased parsing (McDonald et al., 2005; McDonald et al., 2006) that finds the highest scoring directed spanning tree in a graph. The learning procedure is global since model parameters are set relative to classifying the entire dependency graph, in contrast to the local but richer contexts used by transition-based parsers. The system can be trained using first or second order models. The second order projective algorithm performed best on both conversions, and we used it in the rest of the evaluations. We modified the system in order to add semantic features, combining them with wordforms and POS tags, on the parent and child node"
P14-2106,W06-2932,0,0.0292674,"those for Penn2Malt conversions. 3.2 We have made use of three parsers representative of successful paradigms in dependency parsing. MaltParser (Nivre et al., 2007) is a deterministic transition-based dependency parser that obtains a dependency tree in linear-time in a single pass over the input using a stack of partially analyzed items and the remaining input sequence, by means of history-based feature models. We added two features that inspect the semantic feature at the top of the stack and the next input token. MST 3 represents global, exhaustive graphbased parsing (McDonald et al., 2005; McDonald et al., 2006) that finds the highest scoring directed spanning tree in a graph. The learning procedure is global since model parameters are set relative to classifying the entire dependency graph, in contrast to the local but richer contexts used by transition-based parsers. The system can be trained using first or second order models. The second order projective algorithm performed best on both conversions, and we used it in the rest of the evaluations. We modified the system in order to add semantic features, combining them with wordforms and POS tags, on the parent and child nodes of each arc. ZPar4 (Zh"
P14-2106,A00-2018,0,0.262203,"Missing"
P14-2106,W02-1001,0,0.126521,"Missing"
P14-2106,W09-3829,0,0.0385685,"Missing"
P14-2106,J03-4003,0,0.120933,"Missing"
P14-2106,N06-2033,0,0.0327289,"the best performing system was evaluated on the test set (section 23). 651 Parsers Best baseline (ZPar) Best single parser (ZPar + Clusters) Best combination (3 baseline parsers) Best combination of 3 parsers: 3 baselines + 3 SF extensions Best combination of 3 parsers: 3 baselines + 3 SS extensions Best combination of 3 parsers: 3 baselines + 3 cluster extensions LAS 91.52 91.74 (+0.22) 91.90 (+0.38) UAS 92.57 92.63 93.01 91.93 (+0.41) 92.95 91.87 (+0.35) 92.92 91.90 (+0.38) 92.90 4.2 Subsection 4.1 presented the results of the base algorithms and their extensions based on semantic features. Sagae and Lavie (2006) report improvements over the best single parser when combining three transition-based models and one graph-based model. The same technique was also used by the winning team of the CoNLL 2007 Shared Task (Hall et al., 2007), combining six transition-based parsers. We used MaltBlender5 , a tool for merging the output of several dependency parsers, using the Chu-Liu/Edmonds directed MST algorithm. After several tests we noticed that weighted voting by each parser’s labeled accuracy gave good results, using it in the rest of the experiments. We trained different types of combination: • Base algor"
P14-2106,D07-1097,1,0.82722,"F extensions Best combination of 3 parsers: 3 baselines + 3 SS extensions Best combination of 3 parsers: 3 baselines + 3 cluster extensions LAS 91.52 91.74 (+0.22) 91.90 (+0.38) UAS 92.57 92.63 93.01 91.93 (+0.41) 92.95 91.87 (+0.35) 92.92 91.90 (+0.38) 92.90 4.2 Subsection 4.1 presented the results of the base algorithms and their extensions based on semantic features. Sagae and Lavie (2006) report improvements over the best single parser when combining three transition-based models and one graph-based model. The same technique was also used by the winning team of the CoNLL 2007 Shared Task (Hall et al., 2007), combining six transition-based parsers. We used MaltBlender5 , a tool for merging the output of several dependency parsers, using the Chu-Liu/Edmonds directed MST algorithm. After several tests we noticed that weighted voting by each parser’s labeled accuracy gave good results, using it in the rest of the experiments. We trained different types of combination: • Base algorithms. This set includes the 3 baseline algorithms, MaltParser, MST, and ZPar. • Extended parsers, adding semantic information to the baselines. We include the three base algorithms and their semantic extensions (SF, SS, an"
P14-2106,N10-1091,0,0.0577312,"Missing"
P14-2106,W07-2416,0,0.0729244,"Missing"
P14-2106,D09-1058,0,0.0187841,"uction This work presents a set of experiments to investigate the use of lexical semantic information in dependency parsing of English. Whether semantics improve parsing is one interesting research topic both on parsing and lexical semantics. Broadly speaking, we can classify the methods to incorporate semantic information into parsers in two: systems using static lexical semantic repositories, such as WordNet or similar ontologies (Agirre et al., 2008; Agirre et al., 2011; Fujita et al., 2010), and systems using dynamic semantic clusters automatically acquired from corpora (Koo et al., 2008; Suzuki et al., 2009). Our main objective will be to determine whether static semantic knowledge can help parsing. We will apply different types of semantic information to three dependency parsers. Specifically, we will test the following questions: • Does semantic information in WordNet help dependency parsing? Agirre et al. (2011) found improvements in dependency parsing 2 Related work Broadly speaking, we can classify the attempts to add external knowledge to a parser in two sets: using large semantic repositories such as WordNet and approaches that use information automatically acquired from corpora. In the fi"
P14-2106,P10-1001,0,0.0300867,"Missing"
P14-2106,N12-1052,0,0.0566838,"Missing"
P14-2106,W03-3023,0,0.153063,"Missing"
P14-2106,D08-1059,1,0.816794,"6) that finds the highest scoring directed spanning tree in a graph. The learning procedure is global since model parameters are set relative to classifying the entire dependency graph, in contrast to the local but richer contexts used by transition-based parsers. The system can be trained using first or second order models. The second order projective algorithm performed best on both conversions, and we used it in the rest of the evaluations. We modified the system in order to add semantic features, combining them with wordforms and POS tags, on the parent and child nodes of each arc. ZPar4 (Zhang and Clark, 2008; Zhang and Nivre, 2011) performs transition-based dependency parsing with a stack of partial analysis and a queue of remaining inputs. In contrast to MaltParser (local model and greedy deterministic search) ZPar applies global discriminative learning and beam search. We extend the feature set of ZPar to include semantic features. Each set of semantic information is represented by two atomic Experimental Framework In this section we will briefly describe the PTBbased datasets (subsection 3.1), followed by the data-driven parsers used for the experiments (subsection 3.2). Finally, we will descr"
P14-2106,P11-2033,1,0.836904,"st scoring directed spanning tree in a graph. The learning procedure is global since model parameters are set relative to classifying the entire dependency graph, in contrast to the local but richer contexts used by transition-based parsers. The system can be trained using first or second order models. The second order projective algorithm performed best on both conversions, and we used it in the rest of the evaluations. We modified the system in order to add semantic features, combining them with wordforms and POS tags, on the parent and child nodes of each arc. ZPar4 (Zhang and Clark, 2008; Zhang and Nivre, 2011) performs transition-based dependency parsing with a stack of partial analysis and a queue of remaining inputs. In contrast to MaltParser (local model and greedy deterministic search) ZPar applies global discriminative learning and beam search. We extend the feature set of ZPar to include semantic features. Each set of semantic information is represented by two atomic Experimental Framework In this section we will briefly describe the PTBbased datasets (subsection 3.1), followed by the data-driven parsers used for the experiments (subsection 3.2). Finally, we will describe the different types"
P14-2106,P07-1122,1,\N,Missing
P14-2106,W07-1204,0,\N,Missing
P15-2045,D11-1049,0,0.085884,"Missing"
P15-2045,N13-1095,0,0.0510502,"ned using distant supervision tend not to perform as well as those based on manually labelled data. This work proposes a novel method for detecting potential false negative training examples using a knowledge inference method. Results show that our approach improves the performance of relation extraction systems trained using distantly supervised data. 1 2 Related Work Distant supervision is widely used to train relation extraction systems with Freebase and Wikipedia commonly being used as knowledge bases, e.g. (Mintz et al., 2009; Riedel et al., 2010; Krause et al., 2012; Zhang et al., 2013; Min et al., 2013; Ritter et al., 2013). The main advantage is its ability to automatically generate large amounts of training data automatically. On the other hand, this automatically labelled data is noisy and usually generates lower performance than approaches trained using manually labelled data. A range of filtering approaches have been applied to address this problem including multi-class SVM (Nguyen and Moschitti, 2011) and Multi-Instance learning methods (Riedel et al., 2010; Surdeanu et al., 2012). These approaches take into account the fact that entities might occur in different relations at the same"
P15-2045,P05-1022,0,0.0134751,"stant supervision is carried out for a target UMLS relation by identifying instance pairs and using them to create a set of positive instance pairs. Any pairs which also occur as an instance pair of another UMLS relation are removed from this set. A set of negative instance pairs is then created by forming new combinations that do not occur within the positive instance pairs. Sentences containing a positive or negative instance pair are then extracted to generate positive and negative training examples for the relation. These candidate sentences are then stemmed (Porter, 1997) and PoS tagged (Charniak and Johnson, 2005). The sets of positive and negative training examples are then filtered to remove sentences that meet any of the following criteria: contain the same positive pair more than once; contain both a positive and negative pair; more than 5 words between the two elements of the instance pair; contain very common instance pairs. 3.2 weight 10.53 6.17 2.80 -0.06 Table 1: Example PRA-induced paths and weights for the NCI relation biological-process-involvesgene-product. The paths induced by PRA are used to identify potential false negatives in the negative training examples (Section 3.1). Each negative"
P15-2045,P09-1113,0,0.161407,"inimal effort. However, this data may contain errors and consequently systems trained using distant supervision tend not to perform as well as those based on manually labelled data. This work proposes a novel method for detecting potential false negative training examples using a knowledge inference method. Results show that our approach improves the performance of relation extraction systems trained using distantly supervised data. 1 2 Related Work Distant supervision is widely used to train relation extraction systems with Freebase and Wikipedia commonly being used as knowledge bases, e.g. (Mintz et al., 2009; Riedel et al., 2010; Krause et al., 2012; Zhang et al., 2013; Min et al., 2013; Ritter et al., 2013). The main advantage is its ability to automatically generate large amounts of training data automatically. On the other hand, this automatically labelled data is noisy and usually generates lower performance than approaches trained using manually labelled data. A range of filtering approaches have been applied to address this problem including multi-class SVM (Nguyen and Moschitti, 2011) and Multi-Instance learning methods (Riedel et al., 2010; Surdeanu et al., 2012). These approaches take in"
P15-2045,Q13-1030,0,0.0488415,"supervision tend not to perform as well as those based on manually labelled data. This work proposes a novel method for detecting potential false negative training examples using a knowledge inference method. Results show that our approach improves the performance of relation extraction systems trained using distantly supervised data. 1 2 Related Work Distant supervision is widely used to train relation extraction systems with Freebase and Wikipedia commonly being used as knowledge bases, e.g. (Mintz et al., 2009; Riedel et al., 2010; Krause et al., 2012; Zhang et al., 2013; Min et al., 2013; Ritter et al., 2013). The main advantage is its ability to automatically generate large amounts of training data automatically. On the other hand, this automatically labelled data is noisy and usually generates lower performance than approaches trained using manually labelled data. A range of filtering approaches have been applied to address this problem including multi-class SVM (Nguyen and Moschitti, 2011) and Multi-Instance learning methods (Riedel et al., 2010; Surdeanu et al., 2012). These approaches take into account the fact that entities might occur in different relations at the same time and may not nece"
P15-2045,W11-0902,0,0.391962,"Missing"
P15-2045,D12-1042,0,0.190986,"sed as knowledge bases, e.g. (Mintz et al., 2009; Riedel et al., 2010; Krause et al., 2012; Zhang et al., 2013; Min et al., 2013; Ritter et al., 2013). The main advantage is its ability to automatically generate large amounts of training data automatically. On the other hand, this automatically labelled data is noisy and usually generates lower performance than approaches trained using manually labelled data. A range of filtering approaches have been applied to address this problem including multi-class SVM (Nguyen and Moschitti, 2011) and Multi-Instance learning methods (Riedel et al., 2010; Surdeanu et al., 2012). These approaches take into account the fact that entities might occur in different relations at the same time and may not necessarily express the target relation. Other approaches focus directly on the noise in the data. For instance Takamatsu et al. (2012) use a generative model to predict incorrect data while Intxaurrondo et al. (2013) use a range of heuristics including PMI to remove noise. Augenstein et al. (2014) apply techniques to detect highly ambiguous entity pairs and discard them from their labelled training set. This work proposes a novel approach to the problem by applying an in"
P15-2045,P12-1076,0,0.159924,"On the other hand, this automatically labelled data is noisy and usually generates lower performance than approaches trained using manually labelled data. A range of filtering approaches have been applied to address this problem including multi-class SVM (Nguyen and Moschitti, 2011) and Multi-Instance learning methods (Riedel et al., 2010; Surdeanu et al., 2012). These approaches take into account the fact that entities might occur in different relations at the same time and may not necessarily express the target relation. Other approaches focus directly on the noise in the data. For instance Takamatsu et al. (2012) use a generative model to predict incorrect data while Intxaurrondo et al. (2013) use a range of heuristics including PMI to remove noise. Augenstein et al. (2014) apply techniques to detect highly ambiguous entity pairs and discard them from their labelled training set. This work proposes a novel approach to the problem by applying an inference learning method to identify potential false negatives in distantly labelled data. Our method makes use of a modified version of PRA to learn relation paths from a knowledge base and uses this information to identify false negatives. Introduction Dista"
P15-2045,P13-2141,0,0.168429,"quently systems trained using distant supervision tend not to perform as well as those based on manually labelled data. This work proposes a novel method for detecting potential false negative training examples using a knowledge inference method. Results show that our approach improves the performance of relation extraction systems trained using distantly supervised data. 1 2 Related Work Distant supervision is widely used to train relation extraction systems with Freebase and Wikipedia commonly being used as knowledge bases, e.g. (Mintz et al., 2009; Riedel et al., 2010; Krause et al., 2012; Zhang et al., 2013; Min et al., 2013; Ritter et al., 2013). The main advantage is its ability to automatically generate large amounts of training data automatically. On the other hand, this automatically labelled data is noisy and usually generates lower performance than approaches trained using manually labelled data. A range of filtering approaches have been applied to address this problem including multi-class SVM (Nguyen and Moschitti, 2011) and Multi-Instance learning methods (Riedel et al., 2010; Surdeanu et al., 2012). These approaches take into account the fact that entities might occur in different rel"
P15-2045,P10-1030,0,\N,Missing
P15-2045,P11-2048,0,\N,Missing
P16-1179,P14-2013,0,0.0123141,"ould also introduce additional disambiguation probabilities (Barrena et al., 2015), or apply more sophisticated parameter estimation methods (Lazic et al., 2015). Table 8 includes other high performing or wellknown systems, which usually use complex methods to combine features coming from different sources, where our results are only second to those of (Chisholm and Hachey, 2015) in the CoNLL dataset and best in TAC 2014 DEL. The goal of this paper is not to provide the best performing system, but yet, the results show that our use of background information allows to obtain very good results. Alhelbawy and Gaizauskas (2014) combines local and coherence features by means of a graph ranking scheme, obtaining very good results on the CONLL 2003 dataset. They evaluate on the full dataset, i.e. they test on train, testa and testb (20K, 4.8K and 4.4K mentions respectively). Our results on the same dataset are 84.25 (Full) and 88.07 (Full weighted), but note that we do tune the parameters on testa, so this might be slighly over-estimated. Our system does not use global coherence, and therefore their method is complementary to our NED system. In principle, our proposal for enriching context should improve the results of"
P16-1179,C14-1213,1,0.545309,"Missing"
P16-1179,S15-1011,1,0.879825,"Missing"
P16-1179,W11-0110,0,0.0142721,"and Roth, 2013) use relational information from Wikipedia to add constraints to the coherence model, and is somehow reminiscent of our use dependency templates, although they focus on recognizing a fixed set of relations between entities (as in information extraction) and do not model selectional preferences. (Barrena et al., 2014) explored the use of syntactic collocations to ensure coherence, but did not model any selectional preferences. Previous work on word sense disambiguation using selectional preference includes (McCarthy and Carroll, 2003) among others, but they report low results. (Brown et al., 2011) applied wordNet hypernyms for disambiguating verbs, but they did not test the improvement of this feature. (Taghipour and Ng, 2015) use embeddings as features which are fed into a supervised classifier, but our method is different, as we use embeddings to find similar words to be fed as additional context. None of the state-of-the-art systems, e.g. (Zhong and Ng, 2010), uses any model of selectional preferences. 8 Discussion We performed an analysis of the cases where our background models worsened the disambiguation performance. Both distributional similarity and selectional preferences rely"
P16-1179,E06-1002,0,0.170521,"selectional preferences for syntactic positions. We show, using a generative N¨aive Bayes model for NED, that the additional sources of context are complementary, and improve results in the CoNLL 2003 and TAC KBP DEL 2014 datasets, yielding the third best and the best results, respectively. We provide examples and analysis which show the value of the acquired background information. 1 Introduction The goal of Named Entity Disambiguation (NED) is to link each mention of named entities in a document to a knowledge-base of instances. The task is also known as Entity Linking or Entity Resolution (Bunescu and Pasca, 2006; McNamee and Dang, 2009; Hachey et al., 2012). NED is confounded by the ambiguity of named entity mentions. For instance, according to Wikipedia, Liechtenstein can refer to the micro-state, several towns, two castles or a national football team, among other instances. Another ambiguous entity is Derbyshire which can refer to a county in England or a cricket team. Most NED research use knowledge-bases derived or closely related to Wikipedia. For a given mention in context, NED systems (Hachey et al., 2012; Lazic et al., 2015) typically rely on two models: (1) a mention module returns possible"
P16-1179,D13-1184,0,0.0699434,"o the rest of the state-of-theart, as they artificially insert the gold standard entity in the candidate list.12 In (Chisholm and Hachey, 2015) the authors explore the use of links gathered from the web as an additional source of information for NED. They present a complex two-staged supervised system that incorporates global coherence features, with large amount of noisy training. Again, using additional training data seems an interesting future direction complementary to ours. We are not aware of other works which try to use additional sources of context or background information as we do. (Cheng and Roth, 2013) use relational information from Wikipedia to add constraints to the coherence model, and is somehow reminiscent of our use dependency templates, although they focus on recognizing a fixed set of relations between entities (as in information extraction) and do not model selectional preferences. (Barrena et al., 2014) explored the use of syntactic collocations to ensure coherence, but did not model any selectional preferences. Previous work on word sense disambiguation using selectional preference includes (McCarthy and Carroll, 2003) among others, but they report low results. (Brown et al., 20"
P16-1179,Q15-1011,0,0.104626,"o tune the parameters on testa, so this might be slighly over-estimated. Our system does not use global coherence, and therefore their method is complementary to our NED system. In principle, our proposal for enriching context should improve the results of their system. Pershina et al. (2015) propose a system closely resembling (Alhelbawy and Gaizauskas, 2014). They report the best known results on CONNL 2003 so far, but unfortunately, their results are not directly comparable to the rest of the state-of-theart, as they artificially insert the gold standard entity in the candidate list.12 In (Chisholm and Hachey, 2015) the authors explore the use of links gathered from the web as an additional source of information for NED. They present a complex two-staged supervised system that incorporates global coherence features, with large amount of noisy training. Again, using additional training data seems an interesting future direction complementary to ours. We are not aware of other works which try to use additional sources of context or background information as we do. (Cheng and Roth, 2013) use relational information from Wikipedia to add constraints to the coherence model, and is somehow reminiscent of our us"
P16-1179,P07-1028,0,0.0380211,"the most notable team to carry the name Glamorgan is Glamorgan County Cricket Club, Trevor Barsby is a cricketer, as are all other people in the distributional context. When using these similar entities as context, our system does return the correct entity for this mention. In the second example, the words in the context lead the model to return the football team for Liechtenstein, instead of the country, without being aware that the nominal event “visit to” prefers locations arguments. This kind of background information, known as selections preferences, can be easily acquired from corpora (Erk, 2007). Figure 1 shows the most frequent entities found as arguments of “visit to” in the Reuters corpus. When using these filler entities as context, the context model does return the correct entity for this mention. In this article we explore the addition of two kinds of background information induced from corpora to the usual context of occurrence: (1) given a mention we use distributionally similar entities as additional context; (2) given a mention and the syntactic dependencies in the context sentence, we use the selectional preferences of those syntactic dependencies as additional context. We"
P16-1179,P11-1095,0,0.527785,"frequent dependency was MOD, followed by SUBJ and OBJ 5 The selectional preferences include 400K different named entities as fillers. Note that selectional preferences are different from dependency path features. Dependency path features refer to features in the immediate context of the entity mention, and are sometimes added as additional features of supervised classifiers. Selectional preferences are learnt collecting fillers in the same dependency path, but the fillers occur elsewhere in the corpus. 3 NED system Our disambiguation system is a N¨aive Bayes model as initially introduced by (Han and Sun, 2011a), but adapted to integrate the background information extracted from the Reuters corpus. The model is trained using Wikipedia,6 which is also used to generate the entity candidates for each mention. Following usual practice, candidate generation is performed off-line by constructing an association between strings and Wikipedia articles, which we call dictionary. The association is performed using article titles, redirections, disambiguation pages, and textual anchors. Each association is scored with the number of times the string was 5 1.5M, 0.8M and 0.7M respectively We used a dump from 25-"
P16-1179,N15-1035,0,0.0188925,"of our use dependency templates, although they focus on recognizing a fixed set of relations between entities (as in information extraction) and do not model selectional preferences. (Barrena et al., 2014) explored the use of syntactic collocations to ensure coherence, but did not model any selectional preferences. Previous work on word sense disambiguation using selectional preference includes (McCarthy and Carroll, 2003) among others, but they report low results. (Brown et al., 2011) applied wordNet hypernyms for disambiguating verbs, but they did not test the improvement of this feature. (Taghipour and Ng, 2015) use embeddings as features which are fed into a supervised classifier, but our method is different, as we use embeddings to find similar words to be fed as additional context. None of the state-of-the-art systems, e.g. (Zhong and Ng, 2010), uses any model of selectional preferences. 8 Discussion We performed an analysis of the cases where our background models worsened the disambiguation performance. Both distributional similarity and selectional preferences rely on correct mention detection in the background corpus. We detected 12 https://github.com/masha-p/PPRforNED/ readme.txt 1910 that me"
P16-1179,P10-4014,0,0.038073,"tions to ensure coherence, but did not model any selectional preferences. Previous work on word sense disambiguation using selectional preference includes (McCarthy and Carroll, 2003) among others, but they report low results. (Brown et al., 2011) applied wordNet hypernyms for disambiguating verbs, but they did not test the improvement of this feature. (Taghipour and Ng, 2015) use embeddings as features which are fed into a supervised classifier, but our method is different, as we use embeddings to find similar words to be fed as additional context. None of the state-of-the-art systems, e.g. (Zhong and Ng, 2010), uses any model of selectional preferences. 8 Discussion We performed an analysis of the cases where our background models worsened the disambiguation performance. Both distributional similarity and selectional preferences rely on correct mention detection in the background corpus. We detected 12 https://github.com/masha-p/PPRforNED/ readme.txt 1910 that mentions where missed, which caused some coverage issues. In addition, the small size of the background corpus sometimes produces arbitrary contexts. For instance, subject position fillers of “score” include mostly basketball players like Mic"
P16-1179,D11-1072,0,0.25864,"Missing"
P16-1179,Q15-1036,0,0.230844,". The task is also known as Entity Linking or Entity Resolution (Bunescu and Pasca, 2006; McNamee and Dang, 2009; Hachey et al., 2012). NED is confounded by the ambiguity of named entity mentions. For instance, according to Wikipedia, Liechtenstein can refer to the micro-state, several towns, two castles or a national football team, among other instances. Another ambiguous entity is Derbyshire which can refer to a county in England or a cricket team. Most NED research use knowledge-bases derived or closely related to Wikipedia. For a given mention in context, NED systems (Hachey et al., 2012; Lazic et al., 2015) typically rely on two models: (1) a mention module returns possible entities which can be referred to by the mention, ordered by prior probabilities; (2) a conFigure 1: Two examples where NED systems fail, motivating our two background models: similar entities (top) and selectional preferences (bottom). The logos correspond to the gold label. text model orders the entities according to the context of the mention, using features extracted from annotated training data. In addition, some systems check whether the entity is coherent with the rest of entities mentioned in the document, although (L"
P16-1179,J03-4004,0,0.0656082,"additional sources of context or background information as we do. (Cheng and Roth, 2013) use relational information from Wikipedia to add constraints to the coherence model, and is somehow reminiscent of our use dependency templates, although they focus on recognizing a fixed set of relations between entities (as in information extraction) and do not model selectional preferences. (Barrena et al., 2014) explored the use of syntactic collocations to ensure coherence, but did not model any selectional preferences. Previous work on word sense disambiguation using selectional preference includes (McCarthy and Carroll, 2003) among others, but they report low results. (Brown et al., 2011) applied wordNet hypernyms for disambiguating verbs, but they did not test the improvement of this feature. (Taghipour and Ng, 2015) use embeddings as features which are fed into a supervised classifier, but our method is different, as we use embeddings to find similar words to be fed as additional context. None of the state-of-the-art systems, e.g. (Zhong and Ng, 2010), uses any model of selectional preferences. 8 Discussion We performed an analysis of the cases where our background models worsened the disambiguation performance."
P16-1179,N15-1026,0,0.126734,"Missing"
P17-1042,D16-1250,1,0.740024,"XA NLP group University of the Basque Country (UPV/EHU) {mikel.artetxe,gorka.labaka,e.agirre}@ehu.eus Abstract 2015; Vuli´c and Moens, 2016; Mogadala and Rettinger, 2016), but large amounts of such corpora are not always available for some language pairs. An alternative approach that we follow here is to independently train the embeddings for each language on monolingual corpora, and then learn a linear transformation to map the embeddings from one space into the other by minimizing the distances in a bilingual dictionary, usually in the range of a few thousand entries (Mikolov et al., 2013a; Artetxe et al., 2016). However, dictionaries of that size are not readily available for many language pairs, specially those involving less-resourced languages. In this work, we reduce the need of large bilingual dictionaries to much smaller seed dictionaries. Our method can work with as little as 25 word pairs, which are straightforward to obtain assuming some basic knowledge of the languages involved. The method can also work with trivially generated seed dictionaries of numerals (i.e. 1-1, 2-2, 3-3, 4-4...) making it possible to learn bilingual word embeddings without any real bilingual data. In either case, we"
P17-1042,C12-1089,0,0.55408,"sults comparable to those of systems that use richer resources. 1 Introduction Multilingual word embeddings have attracted a lot of attention in recent times. In addition to having a direct application in inherently crosslingual tasks like machine translation (Zou et al., 2013) and crosslingual entity linking (Tsai and Roth, 2016), they provide an excellent mechanism for transfer learning, where a model trained in a resource-rich language is transferred to a less-resourced one, as shown with part-of-speech tagging (Zhang et al., 2016), parsing (Xiao and Guo, 2014) and document classification (Klementiev et al., 2012). Most methods to learn these multilingual word embeddings make use of large parallel corpora (Gouws et al., 2015; Luong et al., 2015), but there have been several proposals to relax this requirement, given its scarcity in most language pairs. A possible relaxation is to use document-aligned or label-aligned comparable corpora (Søgaard et al., 451 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 451–462 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1042 D = 1-a, 2-b, 3-"
P17-1042,P15-2001,0,0.0192163,"thods, as statistical word alignment already provides a reliable way to derive dictionaries from bilingual corpora and, in fact, this is how the test dictionary itself is built in our case. For that reason, we carried out some experiments in crosslingual word similarity as a way to test our method in a different task and allowing to compare it to systems that use richer bilingual data. There are no many crosslingual word similarity datasets, and we used the RG-65 and WordSim353 crosslingual datasets for English-German and the WordSim-353 crosslingual dataset for EnglishItalian as published by Camacho-Collados et al. (2015) 5 . As for the convergence criterion, we decide to stop training when the improvement on the average dot product for the induced dictionary falls below a given threshold from one iteration to the next. After length normalization, the dot product ranges from -1 to 1, so we decide to set this threshold at 1e-6, which we find to be a very conservative value yet enough that training takes a reasonable amount of time. The curves in the next section confirm that this was a reasonable choice. This convergence criterion is usually met in less than 100 iterations, each of them taking 5 minutes on a mo"
P17-1042,P15-1027,0,0.494067,"embedding space into the other based on a bilingual dictionary. The first of such methods is due to Mikolov et al. (2013a), who learn the linear transformation that minimizes the sum of squared Euclidean distances for the dictionary entries. The same optimization objective is used by Zhang et al. (2016), who constrain the transformation matrix to be orthogonal. Xing et al. (2015) incorporate length normalization in the training of word embeddings and maximize the cosine similarity instead, enforcing the orthogonality constraint to preserve the length normalization after the mapping. Finally, Lazaridou et al. (2015) use max-margin optimization with intruder negative sampling. Instead of learning a single linear transformation from the source language into the target language, Faruqui and Dyer (2014) use canonical correlation analysis to map both languages to a shared vector space. Lu et al. (2015) extend this work and apply deep canonical correlation analysis to learn non-linear transformations. Artetxe et al. (2016) propose a general framework that clarifies the relation between Mikolov et al. (2013a), Xing et al. (2015), Faruqui and Dyer (2014) and Zhang et al. (2016) as variants of the 2.2 Unsupervise"
P17-1042,N15-1028,0,0.233964,"Missing"
P17-1042,W15-1521,0,0.693593,"on in recent times. In addition to having a direct application in inherently crosslingual tasks like machine translation (Zou et al., 2013) and crosslingual entity linking (Tsai and Roth, 2016), they provide an excellent mechanism for transfer learning, where a model trained in a resource-rich language is transferred to a less-resourced one, as shown with part-of-speech tagging (Zhang et al., 2016), parsing (Xiao and Guo, 2014) and document classification (Klementiev et al., 2012). Most methods to learn these multilingual word embeddings make use of large parallel corpora (Gouws et al., 2015; Luong et al., 2015), but there have been several proposals to relax this requirement, given its scarcity in most language pairs. A possible relaxation is to use document-aligned or label-aligned comparable corpora (Søgaard et al., 451 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 451–462 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1042 D = 1-a, 2-b, 3-c, 4-x, 5-y D= 1-a, 2-b, 3-c. Learn W using D and rotate X Learn D using nearest neighbor W X Z XW XW and Z in same space Figure 1: A"
P17-1042,P15-1081,0,0.0214277,"nstructs the original embeddings, and a discriminator that distinguishes mapped embeddings from real target language embeddings, whereas the latter adds a regularization term to the training of word embeddings that pushes the mean and variance of each dimension in different languages close to each other. Although promising, the reported performance in both cases is poor in comparison to other methods. Finally, the induction of bilingual knowledge from monolingual corpora is closely related to the decipherment scenario, for which models that incorporate word embeddings have also been proposed (Dou et al., 2015). However, decipherment is only concerned with translating text from one language to another and relies on complex statistical models that are designed specifically for that purpose, while our approach is more general and learns task-independent multilingual embeddings. 453 3.2 duction method that we adopt in our work with these efficiency requirements in mind. 3.1 As discussed in Section 2.1, practically all previous work uses nearest neighbor retrieval for word translation induction based on embedding mappings. In nearest neighbor retrieval, each source language word is assigned the closest"
P17-1042,P14-5010,0,0.0043412,"training set. In addition to English-Italian, we selected two other languages from different language families with publicly available resources. We thus created analogous datasets for English-German and English-Finnish. In the case of German, the embeddings were trained on the 0.9 billion word corpus SdeWaC, which is part of the WaCky collection (Baroni et al., 2009) that was also used for English and Italian. Given that Finnish is not included in this collection, we used the 2.8 billion word Common Crawl corpus provided at WMT 20164 instead, which we tokenized using the Stanford Tokenizer (Manning et al., 2014). In addition to that, we created training and test sets for both pairs from their respective Europarl dictionaries from OPUS following the exact same procedure used for English-Italian, and the word embeddings were also trained using the same configuration as Dinu et al. (2015). Given that the main focus of our work is on small seed dictionaries, we created random subsets of 2,500, 1,000, 500, 250, 100, 75, 50 and 25 entries from the original training dictionaries of 5,000 entries. This was done by shuffling once the training dictionaries and taking their first k entries, so it is guaranteed"
P17-1042,E14-1049,0,0.645265,"squared Euclidean distances for the dictionary entries. The same optimization objective is used by Zhang et al. (2016), who constrain the transformation matrix to be orthogonal. Xing et al. (2015) incorporate length normalization in the training of word embeddings and maximize the cosine similarity instead, enforcing the orthogonality constraint to preserve the length normalization after the mapping. Finally, Lazaridou et al. (2015) use max-margin optimization with intruder negative sampling. Instead of learning a single linear transformation from the source language into the target language, Faruqui and Dyer (2014) use canonical correlation analysis to map both languages to a shared vector space. Lu et al. (2015) extend this work and apply deep canonical correlation analysis to learn non-linear transformations. Artetxe et al. (2016) propose a general framework that clarifies the relation between Mikolov et al. (2013a), Xing et al. (2015), Faruqui and Dyer (2014) and Zhang et al. (2016) as variants of the 2.2 Unsupervised and weakly supervised bilingual embeddings As mentioned before, our method works with as little as 25 word pairs, while the methods discussed previously use thousands of pairs. The only"
P17-1042,W16-1614,0,0.11667,"and the dictionary inA practical aspect for reducing the need of bilingual supervision is on the design of the seed dictionary. This is analyzed in depth by Vuli´c and Korhonen (2016), who propose using documentaligned corpora to extract the training dictionary. A more common approach is to rely on shared words and cognates (Peirsman and Pad´o, 2010; Smith et al., 2017), eliminating the need of bilingual data in practice. Our use of shared numerals exploits the same underlying idea, but relies on even less bilingual evidence and should thus generalize better to distant language pairs. Miceli Barone (2016) and Cao et al. (2016) go one step further and attempt to learn bilingual embeddings without any bilingual evidence. The former uses adversarial autoencoders (Makhzani et al., 2016), combining an encoder that maps the source language embeddings into the target language, a decoder that reconstructs the original embeddings, and a discriminator that distinguishes mapped embeddings from real target language embeddings, whereas the latter adds a regularization term to the training of word embeddings that pushes the mean and variance of each dimension in different languages close to each other. Alth"
P17-1042,P16-1157,0,0.0152688,"nish is a non-indoeuropean agglutinative language, making the task considerably more difficult for this language pair. In this regard, we believe that the good results with small dictionaries are a strong indication of the robustness of our method, showing that it is able to learn good bilingual mappings from very little bilingual ev4.3 Crosslingual word similarity In addition to the baseline systems in Section 4.2, in the crosslingual similarity experiments we also tested the method by Luong et al. (2015), which is the state-of-the-art for bilingual word embeddings based on parallel corpora (Upadhyay et al., 2016)6 . As this method is an extension of word2vec, we used the same hyperparameters as for the monolingual embeddings when possible (see Section 4.1), and leave the default ones otherwise. We used Europarl as our parallel corpus to train this method as done by the authors, which consists of nearly 2 million parallel sentences. As shown in the results in Table 2, our method obtains the best results in all cases, surpassing the rest of the dictionary-based methods by 1-3 points depending on the dataset. But, most importantly, it does not suffer from any significant degradation for using smaller dic"
P17-1042,P16-1024,0,0.257326,"Missing"
P17-1042,N16-1083,0,0.0241103,"Missing"
P17-1042,D13-1168,0,0.0117963,"Missing"
P17-1042,N10-1135,0,0.0831751,"Missing"
P17-1042,W14-1613,0,0.032603,"atically generated list of numerals, obtaining results comparable to those of systems that use richer resources. 1 Introduction Multilingual word embeddings have attracted a lot of attention in recent times. In addition to having a direct application in inherently crosslingual tasks like machine translation (Zou et al., 2013) and crosslingual entity linking (Tsai and Roth, 2016), they provide an excellent mechanism for transfer learning, where a model trained in a resource-rich language is transferred to a less-resourced one, as shown with part-of-speech tagging (Zhang et al., 2016), parsing (Xiao and Guo, 2014) and document classification (Klementiev et al., 2012). Most methods to learn these multilingual word embeddings make use of large parallel corpora (Gouws et al., 2015; Luong et al., 2015), but there have been several proposals to relax this requirement, given its scarcity in most language pairs. A possible relaxation is to use document-aligned or label-aligned comparable corpora (Søgaard et al., 451 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 451–462 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics"
P17-1042,P15-1165,0,0.14311,"Missing"
P17-1042,N15-1104,0,0.751757,"earn bilingual word embeddings. 2.1 Bilingual embedding mappings Methods to induce bilingual mappings work by independently learning the embeddings in each language using monolingual corpora, and then learning a transformation from one embedding space into the other based on a bilingual dictionary. The first of such methods is due to Mikolov et al. (2013a), who learn the linear transformation that minimizes the sum of squared Euclidean distances for the dictionary entries. The same optimization objective is used by Zhang et al. (2016), who constrain the transformation matrix to be orthogonal. Xing et al. (2015) incorporate length normalization in the training of word embeddings and maximize the cosine similarity instead, enforcing the orthogonality constraint to preserve the length normalization after the mapping. Finally, Lazaridou et al. (2015) use max-margin optimization with intruder negative sampling. Instead of learning a single linear transformation from the source language into the target language, Faruqui and Dyer (2014) use canonical correlation analysis to map both languages to a shared vector space. Lu et al. (2015) extend this work and apply deep canonical correlation analysis to learn"
P17-1042,N16-1156,0,0.53192,"rd dictionary or even an automatically generated list of numerals, obtaining results comparable to those of systems that use richer resources. 1 Introduction Multilingual word embeddings have attracted a lot of attention in recent times. In addition to having a direct application in inherently crosslingual tasks like machine translation (Zou et al., 2013) and crosslingual entity linking (Tsai and Roth, 2016), they provide an excellent mechanism for transfer learning, where a model trained in a resource-rich language is transferred to a less-resourced one, as shown with part-of-speech tagging (Zhang et al., 2016), parsing (Xiao and Guo, 2014) and document classification (Klementiev et al., 2012). Most methods to learn these multilingual word embeddings make use of large parallel corpora (Gouws et al., 2015; Luong et al., 2015), but there have been several proposals to relax this requirement, given its scarcity in most language pairs. A possible relaxation is to use document-aligned or label-aligned comparable corpora (Søgaard et al., 451 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 451–462 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association"
P17-1042,N15-1176,0,0.010847,"Missing"
P17-1042,D13-1141,0,0.0449556,"e need of bilingual resources using a very simple self-learning approach that can be combined with any dictionary-based mapping technique. Our method exploits the structural similarity of embedding spaces, and works with as little bilingual evidence as a 25 word dictionary or even an automatically generated list of numerals, obtaining results comparable to those of systems that use richer resources. 1 Introduction Multilingual word embeddings have attracted a lot of attention in recent times. In addition to having a direct application in inherently crosslingual tasks like machine translation (Zou et al., 2013) and crosslingual entity linking (Tsai and Roth, 2016), they provide an excellent mechanism for transfer learning, where a model trained in a resource-rich language is transferred to a less-resourced one, as shown with part-of-speech tagging (Zhang et al., 2016), parsing (Xiao and Guo, 2014) and document classification (Klementiev et al., 2012). Most methods to learn these multilingual word embeddings make use of large parallel corpora (Gouws et al., 2015; Luong et al., 2015), but there have been several proposals to relax this requirement, given its scarcity in most language pairs. A possible"
P17-1042,C16-1171,0,\N,Missing
P18-1073,E14-1049,0,0.457923,"methods work by independently training word embeddings in two languages, and then mapping them to a shared space using a linear transformation. Most of these methods are supervised, and use a bilingual dictionary of a few thousand entries to learn the mapping. Existing approaches can be classified into regression methods, which map the embeddings in one language using a leastsquares objective (Mikolov et al., 2013; Shigeto et al., 2015; Dinu et al., 2015), canonical methods, which map the embeddings in both languages to a shared space using canonical correlation analysis and extensions of it (Faruqui and Dyer, 2014; Lu et al., 2015), orthogonal methods, which map the embeddings in one or both languages under the constraint of the transformation being orthogonal (Xing et al., 2015; Artetxe et al., 2016; Zhang et al., 2016; Smith et al., 2017), and margin methods, which map the embeddings in one language to maximize the margin between the correct translations and the rest of the candidates (Lazaridou et al., 2015). Artetxe et al. (2018a) showed that many of them could be generalized as part of a multi-step framework of linear transformations. A related research line is to adapt these methods to the semi-s"
P18-1073,P15-1027,0,0.383442,"ngs (see Figure 1). We combine this initialization with a more robust self-learning method, which is able to start from the weak initial solution and iteratively improve the mapping. Coupled together, we provide a fully unsupervised crosslingual mapping method that is effective in realistic settings, converges to a good solution in all cases tested, and sets a new state-of-the-art in bilingual lexicon extraction, even surpassing previous supervised methods. Introduction Cross-lingual embedding mappings have shown to be an effective way to learn bilingual word embeddings (Mikolov et al., 2013; Lazaridou et al., 2015). The underlying idea is to independently train the embeddings in different languages using monolingual corpora, and then map them to a shared space through a linear transformation. This allows to learn high-quality cross-lingual representations without expensive supervision, opening new research avenues like unsupervised neural machine translation (Artetxe et al., 2018b; Lample et al., 2018). While most embedding mapping methods rely on a small seed dictionary, adversarial training has recently produced exciting results in fully unsu789 Proceedings of the 56th Annual Meeting of the Associatio"
P18-1073,N15-1028,0,0.0657679,"ently training word embeddings in two languages, and then mapping them to a shared space using a linear transformation. Most of these methods are supervised, and use a bilingual dictionary of a few thousand entries to learn the mapping. Existing approaches can be classified into regression methods, which map the embeddings in one language using a leastsquares objective (Mikolov et al., 2013; Shigeto et al., 2015; Dinu et al., 2015), canonical methods, which map the embeddings in both languages to a shared space using canonical correlation analysis and extensions of it (Faruqui and Dyer, 2014; Lu et al., 2015), orthogonal methods, which map the embeddings in one or both languages under the constraint of the transformation being orthogonal (Xing et al., 2015; Artetxe et al., 2016; Zhang et al., 2016; Smith et al., 2017), and margin methods, which map the embeddings in one language to maximize the margin between the correct translations and the rest of the candidates (Lazaridou et al., 2015). Artetxe et al. (2018a) showed that many of them could be generalized as part of a multi-step framework of linear transformations. A related research line is to adapt these methods to the semi-supervised scenario"
P18-1073,D16-1250,1,0.91417,"bilingual dictionary of a few thousand entries to learn the mapping. Existing approaches can be classified into regression methods, which map the embeddings in one language using a leastsquares objective (Mikolov et al., 2013; Shigeto et al., 2015; Dinu et al., 2015), canonical methods, which map the embeddings in both languages to a shared space using canonical correlation analysis and extensions of it (Faruqui and Dyer, 2014; Lu et al., 2015), orthogonal methods, which map the embeddings in one or both languages under the constraint of the transformation being orthogonal (Xing et al., 2015; Artetxe et al., 2016; Zhang et al., 2016; Smith et al., 2017), and margin methods, which map the embeddings in one language to maximize the margin between the correct translations and the rest of the candidates (Lazaridou et al., 2015). Artetxe et al. (2018a) showed that many of them could be generalized as part of a multi-step framework of linear transformations. A related research line is to adapt these methods to the semi-supervised scenario, where the training dictionary is much smaller and used as part of a bootstrapping process. While similar ideas where already explored for traditional count-based vector s"
P18-1073,P17-1042,1,0.900071,"hich map the embeddings in one language to maximize the margin between the correct translations and the rest of the candidates (Lazaridou et al., 2015). Artetxe et al. (2018a) showed that many of them could be generalized as part of a multi-step framework of linear transformations. A related research line is to adapt these methods to the semi-supervised scenario, where the training dictionary is much smaller and used as part of a bootstrapping process. While similar ideas where already explored for traditional count-based vector space models (Peirsman and Pad´o, 2010; Vuli´c and Moens, 2013), Artetxe et al. (2017) brought this approach to pre-trained low-dimensional word A practical approach for reducing the need of bilingual supervision is to design heuristics to build the seed dictionary. The role of the seed lexicon in learning cross-lingual embedding mappings is analyzed in depth by Vuli´c and Korhonen (2016), who propose using document-aligned corpora to extract the training dictionary. A more common approach is to rely on shared words and cognates (Peirsman and Pad´o, 2010; Smith et al., 2017), while Artetxe et al. (2017) go further and restrict themselves to shared numerals. However, while these"
P18-1073,W16-1614,0,0.0654663,"xe et al. (2017) go further and restrict themselves to shared numerals. However, while these approaches are meant to eliminate the need of bilingual data in practice, they also make strong assumptions on the writing systems of languages (e.g. that they all use a common alphabet or Arabic numerals). Closer to our work, a recent line of fully unsupervised approaches drops these assumptions completely, and attempts to learn cross-lingual embedding mappings based on distributional information alone. For that purpose, existing methods rely on adversarial training. This was first proposed by Miceli Barone (2016), who combine an encoder that maps source language embeddings into the target language, a decoder that reconstructs the source language embeddings from the mapped embeddings, and a discriminator that discriminates between the mapped embeddings and the true target language embed790 and directly related to their Euclidean distance1 , and can be taken as a measure of their similarity. dings. Despite promising, they conclude that their model “is not competitive with other cross-lingual representation approaches”. Zhang et al. (2017a) use a very similar architecture, but incorporate additional tech"
P18-1073,N10-1135,0,0.0850369,"Missing"
P18-1073,D18-1549,0,0.156352,"Missing"
P18-1073,tiedemann-2012-parallel,0,0.0284518,"s work in different conditions, including more challenging settings, we carry out our experiments in the widely used dataset of Dinu et al. (2015) and the subsequent extensions of Artetxe et al. (2017, 2018a), which together comprise English-Italian, English-German, English-Finnish and EnglishSpanish. More concretely, the dataset consists of 300-dimensional CBOW embeddings trained on WacKy crawling corpora (English, Italian, German), Common Crawl (Finnish) and WMT News Crawl (Spanish). The gold standards were derived from dictionaries built from Europarl word alignments and available at OPUS (Tiedemann, 2012), split in a test set of 1,500 entries and a training set of 5,000 that we do not use in our experiments. The datasets are freely available. As a non-european agglutinative language, the English-Finnish pair is particularly challengIn order to build the initial dictionary, we compute X ′ and Z ′ as detailed in Section 3.2 and apply the above procedure over them. As the only difference, this first solution does not use the stochastic zeroing in the similarity matrix, as there is no need to encourage diversity (X ′ and Z ′ are only used once), and the threshold for vocabulary cutoff is set to k"
P18-1073,P16-1024,0,0.119415,"Missing"
P18-1073,D13-1168,0,0.102455,"Missing"
P18-1073,N15-1104,0,0.294358,"ervised, and use a bilingual dictionary of a few thousand entries to learn the mapping. Existing approaches can be classified into regression methods, which map the embeddings in one language using a leastsquares objective (Mikolov et al., 2013; Shigeto et al., 2015; Dinu et al., 2015), canonical methods, which map the embeddings in both languages to a shared space using canonical correlation analysis and extensions of it (Faruqui and Dyer, 2014; Lu et al., 2015), orthogonal methods, which map the embeddings in one or both languages under the constraint of the transformation being orthogonal (Xing et al., 2015; Artetxe et al., 2016; Zhang et al., 2016; Smith et al., 2017), and margin methods, which map the embeddings in one language to maximize the margin between the correct translations and the rest of the candidates (Lazaridou et al., 2015). Artetxe et al. (2018a) showed that many of them could be generalized as part of a multi-step framework of linear transformations. A related research line is to adapt these methods to the semi-supervised scenario, where the training dictionary is much smaller and used as part of a bootstrapping process. While similar ideas where already explored for traditiona"
P18-1073,P17-1179,0,0.541326,"methods rely on adversarial training. This was first proposed by Miceli Barone (2016), who combine an encoder that maps source language embeddings into the target language, a decoder that reconstructs the source language embeddings from the mapped embeddings, and a discriminator that discriminates between the mapped embeddings and the true target language embed790 and directly related to their Euclidean distance1 , and can be taken as a measure of their similarity. dings. Despite promising, they conclude that their model “is not competitive with other cross-lingual representation approaches”. Zhang et al. (2017a) use a very similar architecture, but incorporate additional techniques like noise injection to aid training and report competitive results on bilingual lexicon extraction. Conneau et al. (2018) drop the reconstruction component, regularize the mapping to be orthogonal, and incorporate an iterative refinement process akin to self-learning, reporting very strong results on a large bilingual lexicon extraction dataset. Finally, Zhang et al. (2017b) adopt the earth mover’s distance for training, optimized through a Wasserstein generative adversarial network followed by an alternating optimizati"
P18-1073,D17-1207,0,0.39139,"methods rely on adversarial training. This was first proposed by Miceli Barone (2016), who combine an encoder that maps source language embeddings into the target language, a decoder that reconstructs the source language embeddings from the mapped embeddings, and a discriminator that discriminates between the mapped embeddings and the true target language embed790 and directly related to their Euclidean distance1 , and can be taken as a measure of their similarity. dings. Despite promising, they conclude that their model “is not competitive with other cross-lingual representation approaches”. Zhang et al. (2017a) use a very similar architecture, but incorporate additional techniques like noise injection to aid training and report competitive results on bilingual lexicon extraction. Conneau et al. (2018) drop the reconstruction component, regularize the mapping to be orthogonal, and incorporate an iterative refinement process akin to self-learning, reporting very strong results on a large bilingual lexicon extraction dataset. Finally, Zhang et al. (2017b) adopt the earth mover’s distance for training, optimized through a Wasserstein generative adversarial network followed by an alternating optimizati"
P18-1073,N16-1156,0,0.0613836,"f a few thousand entries to learn the mapping. Existing approaches can be classified into regression methods, which map the embeddings in one language using a leastsquares objective (Mikolov et al., 2013; Shigeto et al., 2015; Dinu et al., 2015), canonical methods, which map the embeddings in both languages to a shared space using canonical correlation analysis and extensions of it (Faruqui and Dyer, 2014; Lu et al., 2015), orthogonal methods, which map the embeddings in one or both languages under the constraint of the transformation being orthogonal (Xing et al., 2015; Artetxe et al., 2016; Zhang et al., 2016; Smith et al., 2017), and margin methods, which map the embeddings in one language to maximize the margin between the correct translations and the rest of the candidates (Lazaridou et al., 2015). Artetxe et al. (2018a) showed that many of them could be generalized as part of a multi-step framework of linear transformations. A related research line is to adapt these methods to the semi-supervised scenario, where the training dictionary is much smaller and used as part of a bootstrapping process. While similar ideas where already explored for traditional count-based vector space models (Peirsma"
P19-1019,P15-1081,0,0.0131111,"ems with monolingual corpora go back to statistical decipherment (Ravi and Knight, 2011; Dou and Knight, 2012). These methods see the source language as ciphertext produced by a noisy channel model that first generates the original English text and then probabilistically replaces the words in it. The English generative process is modeled using an n-gram language model, and the channel model parameters are estimated using either expectation maximization or Bayesian inference. This basic approach was later improved by incorporating syntactic knowledge (Dou and Knight, 2013) and word embeddings (Dou et al., 2015). Nevertheless, these methods were only shown to work in limited settings, being most often evaluated in word-level translation. More recently, the task got a renewed interest after the concurrent work of Artetxe et al. (2018c) and Lample et al. (2018a) on unsupervised NMT which, for the first time, obtained promising results in standard machine translation benchmarks using monolingual corpora only. Both methods build upon the recent work on unsupervised cross-lingual embedding mappings, which independently train word embeddings in two languages and learn a linear transformation to map them to"
P19-1019,N13-1073,0,0.060691,"749 million tokens in French, 1,606 millions in German, and 2,109 millions in English, from which we take a random subset of 2,000 sentences for tuning (Section 3.3). Preprocessing is done using standard Moses tools, and involves punctuation normalization, tokenization with aggressive hyphen splitting, and truecasing. Our SMT implementation is based on Moses10 , and we use the KenLM (Heafield et al., 2013) tool included in it to estimate our 5-gram language model with modified Kneser-Ney smoothing. Our unsupervised tuning implementation is based on Z-MERT (Zaidan, 2009), and we use FastAlign (Dyer et al., 2013) for word alignment within the joint refinement procedure. Finally, we use the big transformer implementation from fairseq11 for our NMT system, training with a total batch size of 20,000 tokens across 8 GPUs with the exact same hyperparameters as Ott et al. (2018). We use newstest2014 as our test set for 10 11 5.1 Main results Table 1 reports the results of the proposed system in comparison to previous work. As it can be seen, our full system obtains the best published results in all cases, outperforming the previous stateof-the-art by 5-7 BLEU points in all datasets and translation direction"
P19-1019,P17-1042,1,0.837208,"shown to work in limited settings, being most often evaluated in word-level translation. More recently, the task got a renewed interest after the concurrent work of Artetxe et al. (2018c) and Lample et al. (2018a) on unsupervised NMT which, for the first time, obtained promising results in standard machine translation benchmarks using monolingual corpora only. Both methods build upon the recent work on unsupervised cross-lingual embedding mappings, which independently train word embeddings in two languages and learn a linear transformation to map them to a shared space through self-learning (Artetxe et al., 2017, 2018a) or adversarial training (Conneau et al., 2018). The resulting crosslingual embeddings are used to initialize a shared encoder for both languages, and the entire system is trained using a combination of denoising autoencoding, back-translation and, in the case of Lample et al. (2018a), adversarial training. This method was further improved by Yang et al. (2018), who use two language-specific encoders sharing only a subset of their parameters, and incorporate a local and a global generative adversarial network. Concurrent to our work, Lample and Conneau (2019) report strong results init"
P19-1019,D18-1045,0,0.283144,"f-the-art in unsupervised machine translation by 5-7 BLEU points in all these datasets and translation directions. Our system also outperforms the supervised WMT 2014 shared task winner in English-to-German, and is around 2 BLEU points behind it in the rest of translation directions, suggesting that unsupervised machine translation can be a usable alternative in practical settings. Introduction The recent advent of neural sequence-to-sequence modeling has resulted in significant progress in the field of machine translation, with large improvements in standard benchmarks (Vaswani et al., 2017; Edunov et al., 2018) and the first solid claims of human parity in certain settings (Hassan et al., 2018). Unfortunately, these systems rely on large amounts of parallel corpora, which are only available for a few combinations of major languages like English, German and French. Aiming to remove this dependency on parallel data, a recent research line has managed to train unsupervised machine translation systems using monolingual corpora only. The first such systems were based on Neural Machine Translation (NMT), and combined denoising autoencoding and back-translation to train a dual model iniThe remaining of thi"
P19-1019,P18-1073,1,0.0533936,"supervised Machine Translation Mikel Artetxe, Gorka Labaka, Eneko Agirre IXA NLP Group University of the Basque Country (UPV/EHU) {mikel.artetxe, gorka.labaka, e.agirre}@ehu.eus Abstract tialized with cross-lingual embeddings (Artetxe et al., 2018c; Lample et al., 2018a). Nevertheless, these early systems were later superseded by Statistical Machine Translation (SMT) based approaches, which induced an initial phrase-table through cross-lingual embedding mappings, combined it with an n-gram language model, and further improved the system through iterative backtranslation (Lample et al., 2018b; Artetxe et al., 2018b). While machine translation has traditionally relied on large amounts of parallel corpora, a recent research line has managed to train both Neural Machine Translation (NMT) and Statistical Machine Translation (SMT) systems using monolingual corpora only. In this paper, we identify and address several deficiencies of existing unsupervised SMT approaches by exploiting subword information, developing a theoretically well founded unsupervised tuning method, and incorporating a joint refinement procedure. Moreover, we use our improved SMT system to initialize a dual NMT model, which is further fi"
P19-1019,D18-1399,1,0.136031,"supervised Machine Translation Mikel Artetxe, Gorka Labaka, Eneko Agirre IXA NLP Group University of the Basque Country (UPV/EHU) {mikel.artetxe, gorka.labaka, e.agirre}@ehu.eus Abstract tialized with cross-lingual embeddings (Artetxe et al., 2018c; Lample et al., 2018a). Nevertheless, these early systems were later superseded by Statistical Machine Translation (SMT) based approaches, which induced an initial phrase-table through cross-lingual embedding mappings, combined it with an n-gram language model, and further improved the system through iterative backtranslation (Lample et al., 2018b; Artetxe et al., 2018b). While machine translation has traditionally relied on large amounts of parallel corpora, a recent research line has managed to train both Neural Machine Translation (NMT) and Statistical Machine Translation (SMT) systems using monolingual corpora only. In this paper, we identify and address several deficiencies of existing unsupervised SMT approaches by exploiting subword information, developing a theoretically well founded unsupervised tuning method, and incorporating a joint refinement procedure. Moreover, we use our improved SMT system to initialize a dual NMT model, which is further fi"
P19-1019,P13-2121,0,0.056362,"Missing"
P19-1019,D18-1549,0,0.145662,"Missing"
P19-1019,D12-1025,0,0.036531,"l corpus through backtranslation, and a new NMT model is trained on top of it from scratch, repeating the process iteratively. Ren et al. (2019) follow a similar approach, but use SMT as posterior regularization at each iteration. As shown later in our experiments, our proposed NMT hybridization obtains substantially larger absolute gains than all these previous approaches, even if our initial SMT system is stronger and thus more challenging to improve upon. Early attempts to build machine translation systems with monolingual corpora go back to statistical decipherment (Ravi and Knight, 2011; Dou and Knight, 2012). These methods see the source language as ciphertext produced by a noisy channel model that first generates the original English text and then probabilistically replaces the words in it. The English generative process is modeled using an n-gram language model, and the channel model parameters are estimated using either expectation maximization or Bayesian inference. This basic approach was later improved by incorporating syntactic knowledge (Dou and Knight, 2013) and word embeddings (Dou et al., 2015). Nevertheless, these methods were only shown to work in limited settings, being most often e"
P19-1019,J82-2005,0,0.799078,"Missing"
P19-1019,P18-1005,0,0.141734,"ods build upon the recent work on unsupervised cross-lingual embedding mappings, which independently train word embeddings in two languages and learn a linear transformation to map them to a shared space through self-learning (Artetxe et al., 2017, 2018a) or adversarial training (Conneau et al., 2018). The resulting crosslingual embeddings are used to initialize a shared encoder for both languages, and the entire system is trained using a combination of denoising autoencoding, back-translation and, in the case of Lample et al. (2018a), adversarial training. This method was further improved by Yang et al. (2018), who use two language-specific encoders sharing only a subset of their parameters, and incorporate a local and a global generative adversarial network. Concurrent to our work, Lample and Conneau (2019) report strong results initializing an unsupervised NMT system with a cross-lingual language model. Following the initial work on unsupervised NMT, it was argued that the modular architecture of phrase-based SMT was more suitable for this problem, and Lample et al. (2018b) and Artetxe et al. (2018b) adapted the same principles discussed above to train an unsupervised SMT model, obtaining large i"
P19-1019,P03-1021,0,0.129273,"(McCallum et al., 2005), for future work. f where the temperature τ is estimated using maximum likelihood estimation over a dictionary induced in the reverse direction. In addition to the phrase translation probabilities in both directions, the forward and reverse lexical weightings 3.3 Unsupervised tuning Having trained the underlying statistical models independently, SMT tuning aims to adjust the weights of their resulting log-linear combination to optimize some evaluation metric like BLEU in a parallel validation corpus, which is typically done through Minimum Error Rate Training or MERT (Och, 2003). Needless to say, this cannot be done in strictly unsupervised settings, but we argue that 1 https://github.com/artetxem/ phrase2vec 2 So as to keep the model size within a reasonable limit, we restrict the vocabulary to the most frequent 200,000 unigrams, 400,000 bigrams and 400,000 trigrams. 3 https://github.com/artetxem/vecmap 196 where the length penalty LP = LP(E) · LP(F ) penalizes excessively long translations:5   len(TF →E (TE→F (E))) LP(E) = max 1, len(E) it would still be desirable to optimize some unsupervised criterion that is expected to correlate well with test performance. Un"
P19-1019,W18-6301,0,0.0247339,"ion with aggressive hyphen splitting, and truecasing. Our SMT implementation is based on Moses10 , and we use the KenLM (Heafield et al., 2013) tool included in it to estimate our 5-gram language model with modified Kneser-Ney smoothing. Our unsupervised tuning implementation is based on Z-MERT (Zaidan, 2009), and we use FastAlign (Dyer et al., 2013) for word alignment within the joint refinement procedure. Finally, we use the big transformer implementation from fairseq11 for our NMT system, training with a total batch size of 20,000 tokens across 8 GPUs with the exact same hyperparameters as Ott et al. (2018). We use newstest2014 as our test set for 10 11 5.1 Main results Table 1 reports the results of the proposed system in comparison to previous work. As it can be seen, our full system obtains the best published results in all cases, outperforming the previous stateof-the-art by 5-7 BLEU points in all datasets and translation directions. A substantial part of this improvement comes from our more principled unsupervised SMT ap12 Note that it is only the test set that is from WMT 2016. All the training data comes from WMT 2014 News Crawl, so it is likely that our results could be further improved"
P19-1019,W18-6319,0,0.0256025,"26.9 26.4 Table 1: Results of the proposed method in comparison to previous work (BLEU). Overall best results are in bold, the best ones in each group are underlined. ∗ Detokenized BLEU equivalent to the official mteval-v13a.pl script. The rest use tokenized BLEU with multi-bleu.perl (or similar). French-English, and both newstest2014 and newstest2016 (from WMT 201612 ) for GermanEnglish. Following common practice, we report tokenized BLEU scores as computed by the multi-bleu.perl script included in Moses. In addition to that, we also report detokenized BLEU scores as computed by SacreBLEU13 (Post, 2018), which is equivalent to the official mteval-v13a.pl script. We next present the results of our proposed system in comparison to previous work in Section 5.1. Section 5.2 then compares the obtained results to those of different supervised systems. Finally, Section 5.3 presents some translation examples from our system. points from every 10 iterations. 5 Experiments and results In order to make our experiments comparable to previous work, we use the French-English and German-English datasets from the WMT 2014 shared task. More concretely, our training data consists of the concatenation of all N"
P19-1019,P11-1002,0,0.172675,"t the synthetic parallel corpus through backtranslation, and a new NMT model is trained on top of it from scratch, repeating the process iteratively. Ren et al. (2019) follow a similar approach, but use SMT as posterior regularization at each iteration. As shown later in our experiments, our proposed NMT hybridization obtains substantially larger absolute gains than all these previous approaches, even if our initial SMT system is stronger and thus more challenging to improve upon. Early attempts to build machine translation systems with monolingual corpora go back to statistical decipherment (Ravi and Knight, 2011; Dou and Knight, 2012). These methods see the source language as ciphertext produced by a noisy channel model that first generates the original English text and then probabilistically replaces the words in it. The English generative process is modeled using an n-gram language model, and the channel model parameters are estimated using either expectation maximization or Bayesian inference. This basic approach was later improved by incorporating syntactic knowledge (Dou and Knight, 2013) and word embeddings (Dou et al., 2015). Nevertheless, these methods were only shown to work in limited setti"
P19-1019,P16-1009,0,0.800056,"ion 6 concludes the paper. 194 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 194–203 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics 2 Related work unsupervised NMT systems. More concretely, both approaches learn cross-lingual n-gram embeddings from monolingual corpora based on the mapping method discussed earlier, and use them to induce an initial phrase-table that is combined with an n-gram language model and a distortion model. This initial system is then refined through iterative back-translation (Sennrich et al., 2016) which, in the case of Artetxe et al. (2018b), is preceded by an unsupervised tuning step. Our work identifies some deficiencies in these previous systems, and proposes a more principled approach to unsupervised SMT that incorporates subword information, uses a theoretically better founded unsupervised tuning method, and applies a joint refinement procedure, outperforming these previous systems by a substantial margin. Very recently, some authors have tried to combine both SMT and NMT to build hybrid unsupervised machine translation systems. This idea was already explored by Lample et al. (201"
P19-1019,D13-1173,0,\N,Missing
P19-1492,P17-1042,1,0.901941,"rable corpora (Vuli´c and Moens, 2016) or large bilingual dictionaries (Duong et al., 2016) have also been proposed. For a more detailed survey, the reader is referred to Ruder et al. (2017). In contrast, offline mapping approaches work by aligning separately trained word embeddings in different languages. For that purpose, early methods required a training dictionary, which was used to learn a linear transformation that mapped these embeddings into a common space (Mikolov et al., 2013a; Artetxe et al., 2018a). The amount of required supervision was later reduced through selflearning methods (Artetxe et al., 2017), and then completely eliminated through adversarial training (Zhang et al., 2017; Conneau et al., 2018) or more robust iterative approaches combined with initialization heuristics (Artetxe et al., 2018b; Hoshen and Wolf, 2018). There are several authors that have discussed the potential limitations of these mapping approaches. For instance, Søgaard et al. (2018) observe that the assumption that separately trained embeddings are approximately isomorphic is not true in general, showing that the performance of mapping methods is conditioned by the language pair, the comparability of the training"
P19-1492,D18-1043,0,0.254107,"aches work by aligning separately trained word embeddings in different languages. For that purpose, early methods required a training dictionary, which was used to learn a linear transformation that mapped these embeddings into a common space (Mikolov et al., 2013a; Artetxe et al., 2018a). The amount of required supervision was later reduced through selflearning methods (Artetxe et al., 2017), and then completely eliminated through adversarial training (Zhang et al., 2017; Conneau et al., 2018) or more robust iterative approaches combined with initialization heuristics (Artetxe et al., 2018b; Hoshen and Wolf, 2018). There are several authors that have discussed the potential limitations of these mapping approaches. For instance, Søgaard et al. (2018) observe that the assumption that separately trained embeddings are approximately isomorphic is not true in general, showing that the performance of mapping methods is conditioned by the language pair, the comparability of the training corpora, and the parameters of the word embedding algorithms. Similarly, Patra et al. (2019) show that the isomorphism assumption weakens as the languages involved become increasingly etymologically distant. Finally, Nakashole"
P19-1492,W15-1521,0,0.255871,"clear whether this mismatch is a consequence of separately training both embedding spaces, and thus an inherent limitation of mapping approaches, or an insurmountable obstacle that arises from the linguistic divergences across languages, and hence a more general issue when learning cross-lingual word embeddings. Introduction Cross-lingual word embeddings have attracted a lot of attention in recent times. Existing methods can be broadly classified into two categories: joint methods, which simultaneously learn word representations for multiple languages on parallel corpora (Gouws et al., 2015; Luong et al., 2015), and mapping methods, which independently train word embeddings in different languages and map them to a shared space through linear transformations (Mikolov et al., 2013a; Artetxe et al., 2018a). While early work in cross-lingual word embeddings was dominated by joint approaches, recent research has almost exclusively focused on mapping methods, which have the advantage of requirThe goal of this paper is to shed light on this matter so as to better understand the nature and extension of these limitations. For that purpose, we experiment with parallel corpora, which allows us to compare mappi"
P19-1492,P18-1073,1,0.905812,"the linguistic divergences across languages, and hence a more general issue when learning cross-lingual word embeddings. Introduction Cross-lingual word embeddings have attracted a lot of attention in recent times. Existing methods can be broadly classified into two categories: joint methods, which simultaneously learn word representations for multiple languages on parallel corpora (Gouws et al., 2015; Luong et al., 2015), and mapping methods, which independently train word embeddings in different languages and map them to a shared space through linear transformations (Mikolov et al., 2013a; Artetxe et al., 2018a). While early work in cross-lingual word embeddings was dominated by joint approaches, recent research has almost exclusively focused on mapping methods, which have the advantage of requirThe goal of this paper is to shed light on this matter so as to better understand the nature and extension of these limitations. For that purpose, we experiment with parallel corpora, which allows us to compare mapping methods and joint methods under the exact same conditions, and analyze the properties of the resulting embeddings. Our results show that, under these conditions, joint learning yields to more"
P19-1492,P18-2036,0,0.195788,"lf, 2018). There are several authors that have discussed the potential limitations of these mapping approaches. For instance, Søgaard et al. (2018) observe that the assumption that separately trained embeddings are approximately isomorphic is not true in general, showing that the performance of mapping methods is conditioned by the language pair, the comparability of the training corpora, and the parameters of the word embedding algorithms. Similarly, Patra et al. (2019) show that the isomorphism assumption weakens as the languages involved become increasingly etymologically distant. Finally, Nakashole and Flauger (2018) argue that embedding spaces in different languages are linearly equivalent only at local regions, but their global structure is different. Nevertheless, neither of these works does systematically analyze the extent to which these limitations are inherent to mapping approaches. To the best of our knowledge, ours is the first work comparing joint and mapping methods in the exact same conditions, characterizing the nature and impact of such limitations. Experimental design We next describe the cross-lingual embedding methods, evaluation measures and datasets used in our experiments. 3.1 Cross-li"
P19-1492,D16-1136,0,0.101565,"2019 Association for Computational Linguistics 2 3 Related work Cross-lingual word embeddings represent words from multiple languages in a common vector space. So as to train them, joint methods simultaneously learn the embeddings in the different languages, which requires some form of cross-lingual supervision. This supervision usually comes from parallel corpora, which can be aligned at the word level (Luong et al., 2015), or only at the sentence level (Gouws et al., 2015). In addition to that, methods that rely on comparable corpora (Vuli´c and Moens, 2016) or large bilingual dictionaries (Duong et al., 2016) have also been proposed. For a more detailed survey, the reader is referred to Ruder et al. (2017). In contrast, offline mapping approaches work by aligning separately trained word embeddings in different languages. For that purpose, early methods required a training dictionary, which was used to learn a linear transformation that mapped these embeddings into a common space (Mikolov et al., 2013a; Artetxe et al., 2018a). The amount of required supervision was later reduced through selflearning methods (Artetxe et al., 2017), and then completely eliminated through adversarial training (Zhang e"
P19-1492,N13-1073,0,0.0179014,"ld of 1e-5 and 5 training iterations. Having done that, we map the resulting monolingual embeddings to a cross-lingual space using the unsupervised mode in VecMap1 (Artetxe et al., 2018b), which builds an initial solution based on heuristics and iteratively improves it through self-learning. Joint learning: We use the BiVec2 tool proposed by Luong et al. (2015), an extension of skip-gram that, given a word aligned parallel corpus, learns to predict the context of both the source word and the target word aligned with it. For that purpose, we first word align our training corpus using FastText (Dyer et al., 2013). Given that BiVec is a natural extension of skip-gram, we use the exact same hyperparameters as for the mapping method. In both cases, we restrict the vocabulary to the most frequent 200,000 words. 3.2 Evaluation measures We use the following measures to characterize cross-lingual embeddings: Isomorphism. Intuitively, the notion of isomorphism captures the idea of how well the embeddings in both languages fit together (i.e. the degree of their structural similarity). So as to measure it, we use the eigenvalue similarity metric proposed by Søgaard et al. (2018). For that purpose, we first cent"
P19-1492,W18-6488,0,0.0156138,"etrieval. Note that, in addition to having a practical application, BLI performance is an informative measure of the quality of the embeddings, as a good cross-lingual representation should place equivalent words close to each other. 3.3 Datasets We experiment with 4 language pairs with English as the target language, covering 3 relatively close languages (German, Spanish and Italian) and a non-indoeuropean agglutinative language (Finnish). All embeddings were trained on the BiCleaner v3.0 version of the ParaCrawl corpus,4 a parallel corpus collected through crawling and filtered according to Sánchez-Cartagena et al. (2018). The size of this corpus changes from one language to another: German and Spanish are the largest (503 and 492 million tokens in the English side, respectively), followed by Italian (308 million tokens), and Finnish (55 million tokens). As for the evaluation dictionaries for BLI, we use two datasets that have been widely used in the literature. The first one, which we call Eparl, was first introduced by Dinu et al. (2015) and subsequently extended by Artetxe et al. (2017) and Artetxe et al. (2018a), and consists of 1,500 test entries extracted from Europarl word alignments 4992 4 https://para"
P19-1492,P18-1072,0,0.113707,"Missing"
P19-1492,P17-1179,0,0.143559,", 2016) have also been proposed. For a more detailed survey, the reader is referred to Ruder et al. (2017). In contrast, offline mapping approaches work by aligning separately trained word embeddings in different languages. For that purpose, early methods required a training dictionary, which was used to learn a linear transformation that mapped these embeddings into a common space (Mikolov et al., 2013a; Artetxe et al., 2018a). The amount of required supervision was later reduced through selflearning methods (Artetxe et al., 2017), and then completely eliminated through adversarial training (Zhang et al., 2017; Conneau et al., 2018) or more robust iterative approaches combined with initialization heuristics (Artetxe et al., 2018b; Hoshen and Wolf, 2018). There are several authors that have discussed the potential limitations of these mapping approaches. For instance, Søgaard et al. (2018) observe that the assumption that separately trained embeddings are approximately isomorphic is not true in general, showing that the performance of mapping methods is conditioned by the language pair, the comparability of the training corpora, and the parameters of the word embedding algorithms. Similarly, Patra e"
P19-1494,D18-1214,0,0.0377093,"ons of words that were missing in the training dictionary by taking their nearest neighbor in the target language. The amount of required supervision was later reduced through self-learning methods (Artetxe et al., 2017), and then completely eliminated through adversarial training (Zhang et al., 2017a; Conneau et al., 2018) or more robust iterative approaches combined with initialization heuristics (Artetxe et al., 2018b; Hoshen and Wolf, 2018). At the same time, several recent methods have formulated embedding mappings as an optimal transport problem (Zhang et al., 2017b; Grave et al., 2018; Alvarez-Melis and Jaakkola, 2018). In addition to that, a large body of work has focused on addressing the hubness problem that arises when directly inducing bilingual dictionaries from cross-lingual embeddings, either through the retrieval method (Dinu et al., 2015; Smith et al., 2017; Conneau et al., 2018) or the mapping itself (Lazaridou et al., 2015; Shigeto et al., 2015; Joulin et al., 2018). While all these previous methods directly induce bilingual dictionaries from cross-lingually mapped embeddings, our proposed method combines them with unsupervised machine translation techniques, outperforming them all by a substant"
P19-1494,P17-1042,1,0.854883,"mbedding mappings, which work by aligning independently trained word embeddings in different languages. For that purpose, early methods required a training dictionary, which was used to learn a linear transformation that mapped these embeddings into a shared crosslingual space (Mikolov et al., 2013; Artetxe et al., 2018a). The resulting cross-lingual embeddings are then used to induce the translations of words that were missing in the training dictionary by taking their nearest neighbor in the target language. The amount of required supervision was later reduced through self-learning methods (Artetxe et al., 2017), and then completely eliminated through adversarial training (Zhang et al., 2017a; Conneau et al., 2018) or more robust iterative approaches combined with initialization heuristics (Artetxe et al., 2018b; Hoshen and Wolf, 2018). At the same time, several recent methods have formulated embedding mappings as an optimal transport problem (Zhang et al., 2017b; Grave et al., 2018; Alvarez-Melis and Jaakkola, 2018). In addition to that, a large body of work has focused on addressing the hubness problem that arises when directly inducing bilingual dictionaries from cross-lingual embeddings, either t"
P19-1494,P18-1073,1,0.912865,"oints over CSLS retrieval, establishing a new state-of-the-art in the standard MUSE dataset. 1 Introduction Cross-lingual word embedding mappings have attracted a lot of attention in recent times. These methods work by independently training word embeddings in different languages, and mapping them to a shared space through linear transformations. While early methods required a training dictionary to find the initial alignment (Mikolov et al., 2013), fully unsupervised methods have managed to obtain comparable results based on either adversarial training (Conneau et al., 2018) or selflearning (Artetxe et al., 2018b). A prominent application of these methods is Bilingual Lexicon Induction (BLI), that is, using In this paper, we go one step further and, rather than directly inducing the bilingual dictionary from the cross-lingual word embeddings, we use them to build an unsupervised machine translation system, and extract a bilingual dictionary from a synthetic parallel corpus generated with it. This allows us to take advantage of a strong language model and naturally extract translation equivalences through statistical word alignment. At the same time, our method can be used as a drop-in replacement of"
P19-1494,D18-1399,1,0.920694,"oints over CSLS retrieval, establishing a new state-of-the-art in the standard MUSE dataset. 1 Introduction Cross-lingual word embedding mappings have attracted a lot of attention in recent times. These methods work by independently training word embeddings in different languages, and mapping them to a shared space through linear transformations. While early methods required a training dictionary to find the initial alignment (Mikolov et al., 2013), fully unsupervised methods have managed to obtain comparable results based on either adversarial training (Conneau et al., 2018) or selflearning (Artetxe et al., 2018b). A prominent application of these methods is Bilingual Lexicon Induction (BLI), that is, using In this paper, we go one step further and, rather than directly inducing the bilingual dictionary from the cross-lingual word embeddings, we use them to build an unsupervised machine translation system, and extract a bilingual dictionary from a synthetic parallel corpus generated with it. This allows us to take advantage of a strong language model and naturally extract translation equivalences through statistical word alignment. At the same time, our method can be used as a drop-in replacement of"
P19-1494,P19-1019,1,0.856747,"In addition to the phrase translation probabilities in both directions, we also estimate the forward and reverse lexical weightings by aligning each word in the target phrase with the one in the source phrase most likely generating it, and taking the product of their respective translation probabilities. We then combine this phrase-table with a distortion model and a 5-gram language model estimated in the target language corpus, which results in a phrase-based machine translation system. So as to optimize the weights of the resulting model, we use the unsupervised tuning procedure proposed by Artetxe et al. (2019), which combines a cyclic consistency loss and a language modeling loss over a subset of 2,000 sentences from each monolingual corpora. Having done that, we generate a synthetic parallel corpus by translating the source language monolingual corpus with the resulting machine translation system.3 We then word align this corpus using FastAlign (Dyer et al., 2013) with default hyperparameters and the grow-diag-finaland symmetrization heuristic. Finally, we build a phrase-table from the word aligned corpus, and extract a bilingual dictionary from it by discarding all non-unigram entries. For words"
P19-1494,Q17-1010,0,0.378654,"any other points in high-dimensional spaces, which has been reported to severely affect cross-lingual embedding mappings (Dinu et al., 2015). 2 The original paper refers to this method as globally corrected retrieval. 5002 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5002–5007 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics 2 Proposed method The input of our method is a set of cross-lingual word embeddings and the monolingual corpora used to train them. In our experiments, we use fastText embeddings (Bojanowski et al., 2017) mapped through VecMap (Artetxe et al., 2018b), but the algorithm described next can also work with any other word embedding and cross-lingual mapping method. The general idea of our method is to to build an unsupervised phrase-based statistical machine translation system (Lample et al., 2018; Artetxe et al., 2018c, 2019), and use it to generate a synthetic parallel corpus from which to extract a bilingual dictionary. For that purpose, we first derive phrase embeddings from the input word embeddings by taking the 400,000 most frequent bigrams and and the 400,000 most frequent trigrams in each"
P19-1494,D12-1025,0,0.0581822,"ngs trained on Wikipedia with the same hyperparameters. retrieval over mapped embeddings, obtains substantially better results without requiring any additional resource. As such, we argue that 1) future work in cross-lingual word embeddings should consider other evaluation tasks in addition to BLI, and 2) future work in BLI should consider other alternatives in addition to direct retrieval over crosslingual embedding mappings. 5 Related work While BLI has been previously tackled using count-based vector space models (Vuli´c and Moens, 2013) and statistical decipherment (Ravi and Knight, 2011; Dou and Knight, 2012), these methods have recently been superseded by crosslingual embedding mappings, which work by aligning independently trained word embeddings in different languages. For that purpose, early methods required a training dictionary, which was used to learn a linear transformation that mapped these embeddings into a shared crosslingual space (Mikolov et al., 2013; Artetxe et al., 2018a). The resulting cross-lingual embeddings are then used to induce the translations of words that were missing in the training dictionary by taking their nearest neighbor in the target language. The amount of require"
P19-1494,N13-1073,0,0.0639365,"nd a 5-gram language model estimated in the target language corpus, which results in a phrase-based machine translation system. So as to optimize the weights of the resulting model, we use the unsupervised tuning procedure proposed by Artetxe et al. (2019), which combines a cyclic consistency loss and a language modeling loss over a subset of 2,000 sentences from each monolingual corpora. Having done that, we generate a synthetic parallel corpus by translating the source language monolingual corpus with the resulting machine translation system.3 We then word align this corpus using FastAlign (Dyer et al., 2013) with default hyperparameters and the grow-diag-finaland symmetrization heuristic. Finally, we build a phrase-table from the word aligned corpus, and extract a bilingual dictionary from it by discarding all non-unigram entries. For words with more than one entry, we rank translation candidates according to their direct translation probability. 3 Experimental settings In order to compare our proposed method headto-head with other BLI methods, the experimental setting needs to fix the monolingual embedding training method, as well as the cross-lingual mapping algorithm and the evaluation diction"
P19-1494,P19-1070,0,0.216208,"Missing"
P19-1494,D18-1043,0,0.0623802,"these embeddings into a shared crosslingual space (Mikolov et al., 2013; Artetxe et al., 2018a). The resulting cross-lingual embeddings are then used to induce the translations of words that were missing in the training dictionary by taking their nearest neighbor in the target language. The amount of required supervision was later reduced through self-learning methods (Artetxe et al., 2017), and then completely eliminated through adversarial training (Zhang et al., 2017a; Conneau et al., 2018) or more robust iterative approaches combined with initialization heuristics (Artetxe et al., 2018b; Hoshen and Wolf, 2018). At the same time, several recent methods have formulated embedding mappings as an optimal transport problem (Zhang et al., 2017b; Grave et al., 2018; Alvarez-Melis and Jaakkola, 2018). In addition to that, a large body of work has focused on addressing the hubness problem that arises when directly inducing bilingual dictionaries from cross-lingual embeddings, either through the retrieval method (Dinu et al., 2015; Smith et al., 2017; Conneau et al., 2018) or the mapping itself (Lazaridou et al., 2015; Shigeto et al., 2015; Joulin et al., 2018). While all these previous methods directly induc"
P19-1494,D18-1330,0,0.0240602,"tialization heuristics (Artetxe et al., 2018b; Hoshen and Wolf, 2018). At the same time, several recent methods have formulated embedding mappings as an optimal transport problem (Zhang et al., 2017b; Grave et al., 2018; Alvarez-Melis and Jaakkola, 2018). In addition to that, a large body of work has focused on addressing the hubness problem that arises when directly inducing bilingual dictionaries from cross-lingual embeddings, either through the retrieval method (Dinu et al., 2015; Smith et al., 2017; Conneau et al., 2018) or the mapping itself (Lazaridou et al., 2015; Shigeto et al., 2015; Joulin et al., 2018). While all these previous methods directly induce bilingual dictionaries from cross-lingually mapped embeddings, our proposed method combines them with unsupervised machine translation techniques, outperforming them all by a substantial margin. 6 Conclusions and future work We propose a new approach to BLI which, instead of directly inducing bilingual dictionaries from cross-lingual embedding mappings, uses them to build an unsupervised machine translation system, which is then used to generate a synthetic parallel corpus from which to extract bilingual lexica. Our approach does not require a"
P19-1494,P07-2045,0,0.00496398,"2018) but, instead of using the pre-trained Wikipedia embeddings distributed with it, we extract monolingual corpora from Wikipedia ourselves and train our own embeddings trying to be as faithful as possible to the original settings. This allows us to compare our proposed method to previous retrieval techniques in the exact same conditions, while keeping our results as comparable as possible to previous work reporting results for the MUSE dataset. More concretely, we use WikiExtractor4 to extract plain text from Wikipedia dumps, and preprocess the resulting corpus using standard Moses tools (Koehn et al., 2007) by applying sentence splitting, punctuation normalization, tokenization with aggressive hyphen splitting, and lowercasing. We then train word embeddings for each language using the skip-gram implementation of fastText (Bojanowski et al., 2017) with default hyperparameters, restricting the vocabulary to the 200,000 most frequent tokens. The official embeddings in 3 For efficiency purposes, we restricted the size of the synthetic parallel corpus to a maximum of 10 million sentences, and use cube-pruning for faster decoding. As such, our results could likely be improved by translating the full m"
P19-1494,J82-2005,0,0.727459,"Missing"
P19-1494,P15-1027,0,0.0246216,"robust iterative approaches combined with initialization heuristics (Artetxe et al., 2018b; Hoshen and Wolf, 2018). At the same time, several recent methods have formulated embedding mappings as an optimal transport problem (Zhang et al., 2017b; Grave et al., 2018; Alvarez-Melis and Jaakkola, 2018). In addition to that, a large body of work has focused on addressing the hubness problem that arises when directly inducing bilingual dictionaries from cross-lingual embeddings, either through the retrieval method (Dinu et al., 2015; Smith et al., 2017; Conneau et al., 2018) or the mapping itself (Lazaridou et al., 2015; Shigeto et al., 2015; Joulin et al., 2018). While all these previous methods directly induce bilingual dictionaries from cross-lingually mapped embeddings, our proposed method combines them with unsupervised machine translation techniques, outperforming them all by a substantial margin. 6 Conclusions and future work We propose a new approach to BLI which, instead of directly inducing bilingual dictionaries from cross-lingual embedding mappings, uses them to build an unsupervised machine translation system, which is then used to generate a synthetic parallel corpus from which to extract bilin"
P19-1494,D13-1168,0,0.0760126,"Missing"
P19-1494,D18-1268,0,0.104777,"Missing"
P19-1494,P17-1179,0,0.0826342,"ifferent languages. For that purpose, early methods required a training dictionary, which was used to learn a linear transformation that mapped these embeddings into a shared crosslingual space (Mikolov et al., 2013; Artetxe et al., 2018a). The resulting cross-lingual embeddings are then used to induce the translations of words that were missing in the training dictionary by taking their nearest neighbor in the target language. The amount of required supervision was later reduced through self-learning methods (Artetxe et al., 2017), and then completely eliminated through adversarial training (Zhang et al., 2017a; Conneau et al., 2018) or more robust iterative approaches combined with initialization heuristics (Artetxe et al., 2018b; Hoshen and Wolf, 2018). At the same time, several recent methods have formulated embedding mappings as an optimal transport problem (Zhang et al., 2017b; Grave et al., 2018; Alvarez-Melis and Jaakkola, 2018). In addition to that, a large body of work has focused on addressing the hubness problem that arises when directly inducing bilingual dictionaries from cross-lingual embeddings, either through the retrieval method (Dinu et al., 2015; Smith et al., 2017; Conneau et al"
P19-1494,D18-1063,0,0.0822933,"Missing"
P19-1494,D17-1207,0,0.0779351,"ifferent languages. For that purpose, early methods required a training dictionary, which was used to learn a linear transformation that mapped these embeddings into a shared crosslingual space (Mikolov et al., 2013; Artetxe et al., 2018a). The resulting cross-lingual embeddings are then used to induce the translations of words that were missing in the training dictionary by taking their nearest neighbor in the target language. The amount of required supervision was later reduced through self-learning methods (Artetxe et al., 2017), and then completely eliminated through adversarial training (Zhang et al., 2017a; Conneau et al., 2018) or more robust iterative approaches combined with initialization heuristics (Artetxe et al., 2018b; Hoshen and Wolf, 2018). At the same time, several recent methods have formulated embedding mappings as an optimal transport problem (Zhang et al., 2017b; Grave et al., 2018; Alvarez-Melis and Jaakkola, 2018). In addition to that, a large body of work has focused on addressing the hubness problem that arises when directly inducing bilingual dictionaries from cross-lingual embeddings, either through the retrieval method (Dinu et al., 2015; Smith et al., 2017; Conneau et al"
P19-1494,P11-1002,0,0.0605316,"nd use fastText embeddings trained on Wikipedia with the same hyperparameters. retrieval over mapped embeddings, obtains substantially better results without requiring any additional resource. As such, we argue that 1) future work in cross-lingual word embeddings should consider other evaluation tasks in addition to BLI, and 2) future work in BLI should consider other alternatives in addition to direct retrieval over crosslingual embedding mappings. 5 Related work While BLI has been previously tackled using count-based vector space models (Vuli´c and Moens, 2013) and statistical decipherment (Ravi and Knight, 2011; Dou and Knight, 2012), these methods have recently been superseded by crosslingual embedding mappings, which work by aligning independently trained word embeddings in different languages. For that purpose, early methods required a training dictionary, which was used to learn a linear transformation that mapped these embeddings into a shared crosslingual space (Mikolov et al., 2013; Artetxe et al., 2018a). The resulting cross-lingual embeddings are then used to induce the translations of words that were missing in the training dictionary by taking their nearest neighbor in the target language"
P19-1574,P18-2043,0,0.0135221,"are represented in embeddings. While WSD and EL are important, they conflate (a) the evaluation of the information content of an embedding with (b) a model’s ability to extract that information based on contextual clues. We mostly focus on (a) here. Also, in contrast to WSD datasets, WIKI-PSE is not based on inferred sense tags and not based on artificial ambiguity, i.e., pseudowords (Gale et al., 1992; Sch¨utze, 1992), but on real senses marked by Wikipedia hyperlinks. There has been work in generating dictionary definitions from word embeddings (Noraset et al., 2017; Bosc and Vincent, 2018; Gadetsky et al., 2018). Gadetsky et al. (2018) explicitly adress ambiguity and generate definitions for words conditioned on their embeddings and selected contexts. This also conflates (a) and (b). Some prior work also looks at how ambiguity affects word embeddings. Arora et al. (2018) posit that a word embedding is a linear combination of its sense embeddings and that senses can be extracted via sparse coding. Mu et al. (2017) argue that sense and word vectors are linearly related and show that word embeddings are intersections of sense subspaces. Working with synthetic data, Yaghoobzadeh and Sch¨utze (2016) evalu"
P19-1574,W16-2507,0,0.021121,"chutze 1 Microsoft Research Montr´eal Center for Data Science, New York University 3 IXA NLP Group, University of the Basque Country 4 CIS, LMU Munich yayaghoo@microsoft.com 2 Abstract tools to analyze them. However, the main tool for analyzing their semantic content is still lookWord embeddings typically represent differing at nearest neighbors of embeddings. Nearest ent meanings of a word in a single conflated neighbors are based on full-space similarity nevector. Empirical analysis of embeddings of glecting the multifacetedness property of words ambiguous words is currently limited by the (Gladkova and Drozd, 2016) and making them unsmall size of manually annotated resources stable (Wendlandt et al., 2018). and by the fact that word senses are treated as unrelated individual concepts. We present As an alternative, we propose diagnostic clasa large dataset based on manual Wikipedia ansification of embeddings into semantic classes notations and word senses, where word senses as a probing task to reveal their meaning confrom different words are related by semantic tent. We will refer to semantic classes as Sclasses. This is the basis for novel diagnosclasses. We use S-classes such as food, drug tic tests f"
P19-1574,P12-1092,0,0.0647545,"n transittransit line, transit, finance-currency, disease, chemistry, body part, finance-stock exchange, law, medicine-medical treatment, medicinedrug, broadcast-tv channel, medicine-symptom, biology, visual art-color Table 1: S-classes in WIKI-PSE sorted by frequency. Figure 1: Example of how we build WIKI-PSE. There are three sentences linking “apple” to different entities. There are two mentions (m2 ,m3 ) with the organization sense (S-class) and one mention (m1 ) with the food sense (S-class). nated from embeddings, i.e., that a separate embedding is needed for each sense (Sch¨utze, 1998; Huang et al., 2012; Neelakantan et al., 2014; Li and Jurafsky, 2015; Camacho-Collados and Pilehvar, 2018). This can improve performance on contextual word similarity, but a recent study (Dubossarsky et al., 2018) questions this finding. WIKI-PSE allows us to compute sense embeddings; we will analyze their effect on word embeddings in our diagnostic classifications. 3 WIKI-PSE Resource class in the corpus. There exist sense annotated corpora like SemCor (Miller et al., 1993), but due to the cost of annotation, those corpora are usually limited in size, which can hurt the quality of the trained word embeddings –"
P19-1574,E09-1045,0,0.0468017,"Missing"
P19-1574,H93-1061,0,0.577046,"(2006) and Izquierdo et al. (2009) for word erally represented well in a single-vector emsense disambiguation, but have not been used for bedding – if the sense is frequent. (ii) A clasanalyzing embeddings. sifier can accurately predict whether a word Analysis based on S-classes is only promising if is single-sense or multi-sense, based only on we have high-quality S-class annotations. Existing its embedding. (iii) Although rare senses are datasets are either too small to train embeddings, not well represented in single-vector embeddings, this does not have negative impact on an e.g., SemCor (Miller et al., 1993), or artificially NLP application whose performance depends generated (Yaghoobzadeh and Sch¨utze, 2016). on frequent senses. Therefore, we build WIKI-PSE, a WIKIpediabased resource for Probing Semantics in word Em1 Introduction beddings. We focus on common and proper nouns, and use their S-classes as proxies for senses. Word embeddings learned by methods like For example, “lamb” has the senses food and Word2vec (Mikolov et al., 2013) and Glove (Penliving-thing. nington et al., 2014) have had a big impact on Embeddings do not explicitly address ambigunatural language processing (NLP) and inform"
P19-1574,J17-3004,1,0.894956,"Missing"
P19-1574,D14-1162,0,0.0902657,"Missing"
P19-1574,N18-1202,0,0.0635981,"ns, and use their S-classes as proxies for senses. Word embeddings learned by methods like For example, “lamb” has the senses food and Word2vec (Mikolov et al., 2013) and Glove (Penliving-thing. nington et al., 2014) have had a big impact on Embeddings do not explicitly address ambigunatural language processing (NLP) and informaity; multiple senses of a word are crammed into a tion retrieval (IR). They are effective and effisingle vector. This is not a problem in some applicient for many tasks. More recently, contextualcations (Li and Jurafsky, 2015); one possible exized embeddings like ELMo (Peters et al., 2018) planation is that this is an effect of sparse coding and BERT (Devlin et al., 2018) have further imthat supports the recovery of individual meanings proved performance. To understand both word from a single vector (Arora et al., 2018). But amand contextualized embeddings, which still rely on biguity has an adverse effect in other scenarios, word/subword embeddings at their lowest layer, e.g., Xiao and Guo (2014) see the need of filterwe must peek inside the blackbox embeddings. ing out embeddings of ambiguous words in depenGiven the importance of word embeddings, atdency parsing. tempts have"
P19-1574,J14-4005,0,0.0355501,"Missing"
P19-1574,J98-1004,1,0.561454,"Missing"
P19-1574,E17-1119,0,0.0136715,"n only work if the individual S-classes are recognizable, which is not the case for rare senses in regular word embeddings. NLP Application Experiments Our primary goal is to probe meanings in word embeddings without confounding factors like contextual usage. However, to give insights on how our probing results relate to NLP tasks, we evaluate our embeddings when used to represent word tokens.7 Note that our objective here is not to improve over other baselines, but to perform analysis. We select mention, sentence and sentence-pair classification datasets. For mention classification, we adapt Shimaoka et al. (2017)’s setup:8 training, evaluation (FIGER dataset) and implementation. The task is to predict the contextual fine-grained types of entity mentions. We lowercase the dataset to match the vocabularies of GLOVE(6B), FASTTEXT(Wiki) and our embeddings. For sentence and sentence-pair classifications, we use the SentEval9 (Conneau and Kiela, 2018) setup for four datasets: MR (Pang and Lee, 2005) (positive/negative sentiment prediction for movie reviews) , CR (Hu and Liu, 2004) (positive/negative sentiment prediction for product reviews), SUBJ (Pang and Lee, 2004) (subjectivity/objectivity prediction) an"
P19-1574,J17-4004,0,0.0247803,"ector embeddings has little negative impact – this indicates that for many common NLP benchmarks only frequent senses are needed. 2 Related Work S-classes (semantic classes) are a central concept in semantics and in the analysis of semantic phenomena (Yarowsky, 1992; Ciaramita and Johnson, 2003; Senel et al., 2018). They have been used for analyzing ambiguity by Kohomban and Lee (2005), Ciaramita and Altun (2006), and Izquierdo et al. (2009), inter alia. There are some datasets designed for interpreting word embedding dimensions using S-classes, e.g., SEMCAT (Senel et al., 2018) and HyperLex (Vulic et al., 2017). The main differentiator of our work is our probing approach using supervised classification of word embeddings. Also, we do not use WordNet senses but Wikipedia entity annotations since WordNettagged corpora are small. In this paper, we probe word embeddings with supervised classification. Probing the layers of neural networks has become very popular. Conneau et al. (2018) probe sentence embeddings on how well they predict linguistically motivated classes. Hupkes et al. (2018) apply diagnostic classifiers to test hypotheses about the hidden states of RNNs. Focusing on embeddings, Kann et al."
P19-1574,N18-1190,0,0.0191456,"oup, University of the Basque Country 4 CIS, LMU Munich yayaghoo@microsoft.com 2 Abstract tools to analyze them. However, the main tool for analyzing their semantic content is still lookWord embeddings typically represent differing at nearest neighbors of embeddings. Nearest ent meanings of a word in a single conflated neighbors are based on full-space similarity nevector. Empirical analysis of embeddings of glecting the multifacetedness property of words ambiguous words is currently limited by the (Gladkova and Drozd, 2016) and making them unsmall size of manually annotated resources stable (Wendlandt et al., 2018). and by the fact that word senses are treated as unrelated individual concepts. We present As an alternative, we propose diagnostic clasa large dataset based on manual Wikipedia ansification of embeddings into semantic classes notations and word senses, where word senses as a probing task to reveal their meaning confrom different words are related by semantic tent. We will refer to semantic classes as Sclasses. This is the basis for novel diagnosclasses. We use S-classes such as food, drug tic tests for an embedding’s content: we probe and living-thing to define word senses. Sword embeddings"
P19-1574,W14-1613,0,0.0158333,"ve and effisingle vector. This is not a problem in some applicient for many tasks. More recently, contextualcations (Li and Jurafsky, 2015); one possible exized embeddings like ELMo (Peters et al., 2018) planation is that this is an effect of sparse coding and BERT (Devlin et al., 2018) have further imthat supports the recovery of individual meanings proved performance. To understand both word from a single vector (Arora et al., 2018). But amand contextualized embeddings, which still rely on biguity has an adverse effect in other scenarios, word/subword embeddings at their lowest layer, e.g., Xiao and Guo (2014) see the need of filterwe must peek inside the blackbox embeddings. ing out embeddings of ambiguous words in depenGiven the importance of word embeddings, atdency parsing. tempts have been made to construct diagnostic 5740 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5740–5753 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics We present the first comprehensive empirical analysis of ambiguity in word embeddings. Our resource, WIKI-PSE, enables novel diagnostic tests that help explain how (and how well) e"
P19-1574,D15-1083,1,0.89959,"Missing"
P19-1574,P16-1023,1,0.930111,"Missing"
P19-1574,C92-2070,0,0.622512,"(i) Single-vector embeddings can represent many non-rare senses well. (ii) A classifier can accurately predict whether a word is single-sense or multi-sense, based only on its embedding. (iii) In experiments with five common datasets for mention, sentence and sentencepair classification tasks, the lack of representation of rare senses in single-vector embeddings has little negative impact – this indicates that for many common NLP benchmarks only frequent senses are needed. 2 Related Work S-classes (semantic classes) are a central concept in semantics and in the analysis of semantic phenomena (Yarowsky, 1992; Ciaramita and Johnson, 2003; Senel et al., 2018). They have been used for analyzing ambiguity by Kohomban and Lee (2005), Ciaramita and Altun (2006), and Izquierdo et al. (2009), inter alia. There are some datasets designed for interpreting word embedding dimensions using S-classes, e.g., SEMCAT (Senel et al., 2018) and HyperLex (Vulic et al., 2017). The main differentiator of our work is our probing approach using supervised classification of word embeddings. Also, we do not use WordNet senses but Wikipedia entity annotations since WordNettagged corpora are small. In this paper, we probe wo"
P19-1574,D15-1200,0,\N,Missing
P19-1574,C04-1051,0,\N,Missing
P19-1574,W03-1022,0,\N,Missing
P19-1574,W06-1670,0,\N,Missing
P19-1574,P04-1035,0,\N,Missing
P19-1574,P05-1005,0,\N,Missing
P19-1574,D14-1113,0,\N,Missing
P19-1574,N15-1142,0,\N,Missing
P19-1574,K16-1006,0,\N,Missing
P19-1574,P16-1191,0,\N,Missing
P19-1574,L18-1269,0,\N,Missing
P19-1574,D18-1200,0,\N,Missing
P19-1574,D18-1181,0,\N,Missing
P19-1574,N19-1423,0,\N,Missing
P19-1574,Q18-1034,0,\N,Missing
P97-1007,A92-1044,1,0.878522,"Missing"
P97-1007,C96-1005,1,0.845135,"Missing"
P97-1007,P81-1030,0,0.0381477,"Missing"
P97-1007,H91-1077,0,0.0527603,"Missing"
P97-1007,H94-1046,0,0.0228879,"Missing"
P97-1007,H92-1046,0,0.0897578,"Missing"
P97-1007,W95-0105,0,0.0524789,"Missing"
P97-1007,J92-1001,0,0.205861,"Missing"
P97-1007,J90-1003,0,\N,Missing
P97-1007,C94-1042,0,\N,Missing
P97-1007,C92-1056,0,\N,Missing
P97-1007,C92-2070,0,\N,Missing
P97-1007,P95-1026,0,\N,Missing
P97-1007,P94-1013,0,\N,Missing
P97-1007,P92-1053,0,\N,Missing
P98-1003,C96-1005,1,0.749317,"knowledge (lexical, syntactic, semantic . . . . ) should be represented, utilised and combined to aid in this determination. This study relies on the integrated use of three kinds of knowledge (syntagmatic, paradigmatic and statistical) in order to improve first guess accuracy in non-word context-sensitive correction for general unrestricted texts. Our techniques were applied to the corrections posed by ispell. Constraint Grammar (Karlsson et al. 1995) was chosen to represent syntagmatic knowledge. Its use as a part of speech tagger for English has been highly successful. Conceptual Density (Agirre and Rigau 1996) is the paradigmatic component chosen to discriminate semantically among potential noun corrections. This technique measures ""affinity distance"" between nouns using Wordnet (Miller 1990). Finally, general and document word-occurrence frequency-rates complete the set of knowledge sources combined. We knowingly did not use any model of common misspellings, the main reason being that we did not want to use knowledge about the error source. This work focuses on language models, not error models (typing errors, common misspellings, OCR mistakes, speech recognition mistakes, etc.). The system was ev"
P98-1003,P96-1010,0,0.0806449,"Missing"
P98-1003,P94-1013,0,\N,Missing
P98-2181,P97-1007,1,0.89878,"Missing"
P98-2181,W97-0313,0,0.0356749,"Missing"
P98-2181,C92-2070,0,0.0484014,"Missing"
P98-2181,W90-0108,0,\N,Missing
P98-2181,C92-4189,0,\N,Missing
P98-2181,P95-1026,0,\N,Missing
P98-2181,P81-1030,0,\N,Missing
pociello-etal-2008-wnterm,magnini-cavaglia-2000-integrating,0,\N,Missing
pociello-etal-2008-wnterm,alegria-etal-2004-xml,0,\N,Missing
pociello-etal-2008-wnterm,agirre-etal-2006-methodology,1,\N,Missing
R09-1080,N09-1003,1,0.442367,"mpalmer/projects/verbnet. html http://demo.patrickpantel.com/Content/verbocean/ personalised PageRank is computed by modifying the random jump distribution vector in the traditional PageRank equation. In our case, we concentrate all probability mass in the words present in the target text. Regarding PageRank implementation details, we chose a damping value of 0.85 and finish the calculation after 30 iterations. We have not optimised these values for this task. We used all the relations in WordNet 3.06 , including the disambiguated glosses7. This similarity method was used for word similarity [1] which report very good results on word similarity datasets. This baseline calculates similarity between two texts by counting the number of overlapping words. In order to do this we have used the software package Text::Similarity10 . This method has been applied both considering all the words that appear in the texts and discarding stop words. For the last, we have used the list of stop words of the English stemmer Snowball11 . 3.3 3.5 Semantic Vectors Semantic Vectors [19]8 is an open source (BSD license) software package that creates WORDSPACE models from plain text. Its aim is to provide a"
R09-1080,I05-5002,0,0.0239519,"nd semantic similarity (a modified version of the longest common subsequence and Second order Co-occurrence PMI respectively) between words and common-word order similarity. SenseClusters 1 is a language independent and unsupervised tool that clusters short contexts. It represents contexts using first or second order feature vectors. In order to reduce dimensionality it applies Singular Value Decomposition. Apart from the aforementioned systems, it is worth mentioning two datasets that have been used to evaluate approaches to short text similarity. The first is the Microsoft paraphrase corpus [3], extracted from news sources. It is made up of 5,801 pairs of sentences, each together with a human judgement indicating whether the two sentences can be considered paraphrases or not. The second is the Pilot Short Text Semantic Similarity Benchmark Data Set [10], which contains 30 sentence pairs from the Collins Cobuild dictionary. In this case the judgements are not binary, but on a scale (from 0.0 for minimum similarity to 4.0 for maximum similarity). 3 Methods The current section describes the different methods that we have applied. Approaches include Textual Entailment based on lexical a"
R09-1080,W07-1411,1,0.798945,"by defining the concept of Textual Entailment as a one-way meaning relation between two snippets. Moreover, a series of Workshops, called Recognising Textual Entailment (RTE2 ) challenges and the Answer Validation Exercise (AVE3 ) competitions, have been recently proposed with the objective of providing suitable frameworks to evaluate textual entailment systems . To address the specific semantic similarity phenomenon we are dealing with in this research work (i.e. semantic similarity between WordNet glosses and Wikipedia categories), we used our in-house textual entailment system presented in [6]. This system has been previously used to support other NLP applications rather than puristic textual entailment tasks. For 1 2 3 instance, in Question Answering [14] and automatic text summarisation [12] . As a brief system overview, it is worth mentioning the most relevant inferences implemented aimed at solving entailment relations: • Lexical inferences based on lexical distance measures. For instance, the Needleman-Wunsch algorithm, Smith-Waterman algorithm, a matching of consecutive subsequences, Jaro distance, Euclidean distance, IDF specificity based on word frequencies extracted from c"
R09-1080,J98-1004,0,0.307257,"Missing"
R09-1080,toral-etal-2008-named,1,0.391459,"Missing"
R09-1080,widdows-ferraro-2008-semantic,0,0.0138606,"the relations in WordNet 3.06 , including the disambiguated glosses7. This similarity method was used for word similarity [1] which report very good results on word similarity datasets. This baseline calculates similarity between two texts by counting the number of overlapping words. In order to do this we have used the software package Text::Similarity10 . This method has been applied both considering all the words that appear in the texts and discarding stop words. For the last, we have used the list of stop words of the English stemmer Snowball11 . 3.3 3.5 Semantic Vectors Semantic Vectors [19]8 is an open source (BSD license) software package that creates WORDSPACE models from plain text. Its aim is to provide an easy-to-use and efficient tool which can fit both research and production users. It uses a random projection algorithm to perform dimension reduction as this is a simpler and more efficient technique than other alternatives such as Singular Value Decomposition. It relies on Apache Lucene9 for tokenisation and indexing in order to create a term document matrix. Once the reference corpus has been tokenised and indexed, Semantic Vectors creates a WORDSPACE model from the resu"
S01-1028,W01-0703,1,0.881715,"Missing"
S01-1028,J01-3001,0,0.0324269,"Missing"
S01-1028,P94-1013,0,0.322503,"Missing"
S01-1028,W00-1702,1,\N,Missing
S07-1001,P00-1064,0,0.0127234,"ets the occurrence identifier, the sense tag (if in training), and the list of features that apply to the occurrence. 5 http://ixa2.si.ehu.es/semeval-clir/ 6 http://en.wikipedia.org/wiki/ Information retrieval 4 Allocation. Using topic-specific synset similarity measures, they create predictions for each word in each document using only word frequency information. The disambiguation process took aprox. 12 hours on a cluster of 48 machines (dual Xeons with 4GB of RAM). Note that contrary to the specifications, this team returned WordNet 2.1 senses, so we had to map automatically to 1.6 senses (Daude et al., 2000). UNIBA This team uses a a knowledge-based WSD system that attempts to disambiguate all words in a text by exploiting WordNet relations. The main assumption is that a specific strategy for each Part-Of-Speech (POS) is better than a single strategy. Nouns are disambiguated basically using hypernymy links. Verbs are disambiguated according to the nouns surrounding them, and adjectives and adverbs use glosses. ORGANIZERS In addition to the regular participants, and out of the competition, the organizers run a regular supervised WSD system trained on Semcor. The system is based on a single k-NN cl"
S07-1001,P97-1010,0,\N,Missing
S07-1001,D07-1007,0,\N,Missing
S07-1001,P07-1005,0,\N,Missing
S07-1001,W99-0624,0,\N,Missing
S07-1002,W06-3814,1,0.81499,"Missing"
S07-1002,N06-2015,0,0.0461989,"aximum score to that instance. Finally, the resulting test corpus is evaluated according to the usual precision and recall measures for supervised word sense disambiguation systems. 3 Results In this section we will introduce the gold standard and corpus used, the description of the systems and the results obtained. Finally we provide some material for discussion. Gold Standard The data used for the actual evaluation was borrowed from the SemEval-2007 “English lexical sample subtask” of task 17. The texts come from the Wall Street Journal corpus, and were hand-annotated with OntoNotes senses (Hovy et al., 2006). Note that OntoNotes senses are coarser than WordNet senses, and thus the number of senses to be induced is smaller in this case. Participants were provided with information about 100 target words (65 verbs and 35 nouns), each target word having a set of contexts where the word appears. After removing the sense tags from the train corpus, the train and test parts were joined into the official corpus and given to the participants. Participants had to tag with the induced senses all the examples in this corpus. Table 1 summarizes the size of the corpus. Participant systems In total there were 6"
S07-1002,H05-1053,0,0.00998491,"(Fellbaum, 1998) allow for a small improvement over the simple most frequent sense heuristic, as attested in the all-words track of the last Senseval competition (Snyder and Palmer, 2004). In theory, larger amounts of training data (SemCor has approx. 500M words) would improve the performance of supervised WSD, but no current project exists to provide such an expensive resource. Another problem of the supervised approach is that the inventory and distribution of senses changes dramatically from one domain to the other, requiring additional hand-tagging of corpora (Mart´ınez and Agirre, 2000; Koeling et al., 2005). Supervised WSD is based on the “fixed-list of senses” paradigm, where the senses for a target word are a closed list coming from a dictionary or lexicon. Lexicographers and semanticists have long warned about the problems of such an approach, where senses are listed separately as discrete entities, and have argued in favor of more complex representations, where, for instance, senses are dense regions in a continuum (Cruse, 2000). Unsupervised Word Sense Induction and Discrimination (WSID, also known as corpus-based unsupervised systems) has followed this line of thinking, and tries to induce"
S07-1002,W00-1326,1,0.845383,"Missing"
S07-1002,H93-1061,0,0.182601,"systems. We reused the SemEval2007 English lexical sample subtask of task 17, and set up both clustering-style unsupervised evaluation (using OntoNotes senses as gold-standard) and a supervised evaluation (using the part of the dataset for mapping). We provide a comparison to the results of the systems participating in the lexical sample subtask of task 17. 1 Introduction Word Sense Disambiguation (WSD) is a key enabling-technology. Supervised WSD techniques are the best performing in public evaluations, but need large amounts of hand-tagging data. Existing hand-annotated corpora like SemCor (Miller et al., 1993), which is annotated with WordNet senses (Fellbaum, 1998) allow for a small improvement over the simple most frequent sense heuristic, as attested in the all-words track of the last Senseval competition (Snyder and Palmer, 2004). In theory, larger amounts of training data (SemCor has approx. 500M words) would improve the performance of supervised WSD, but no current project exists to provide such an expensive resource. Another problem of the supervised approach is that the inventory and distribution of senses changes dramatically from one domain to the other, requiring additional hand-tagging"
S07-1002,W05-0605,0,0.0122372,"ses (also known as the gold-standard) with the clustering result. The gold standard tags are taken to be the definition of the classes, and standard measures from the clustering literature can be used to evaluate the clusters against the classes. A second alternative would be to devise a method to map the clusters returned by the systems to the senses in a lexicon. Pantel and Lin (2002) automatically map the senses to WordNet, and then measure the quality of the mapping. More recently, the mapping has been used to test the system on publicly available benchmarks (Purandare and Pedersen, 2004; Niu et al., 2005). A third alternative is to evaluate the systems according to some performance in an application, e.g. information retrieval (Sch¨utze, 1998). This is a very attractive idea, but requires expensive system development and it is sometimes difficult to separate the reasons for the good (or bad) performance. In this task we decided to adopt the first two alternatives, since they allow for comparison over publicly available systems of any kind. With this goal on mind we gave all the participants an unlabeled corpus, and asked them to induce the senses and create a clustering solution on it. We eval"
S07-1002,W04-2406,0,0.426149,"tagged with some reference senses (also known as the gold-standard) with the clustering result. The gold standard tags are taken to be the definition of the classes, and standard measures from the clustering literature can be used to evaluate the clusters against the classes. A second alternative would be to devise a method to map the clusters returned by the systems to the senses in a lexicon. Pantel and Lin (2002) automatically map the senses to WordNet, and then measure the quality of the mapping. More recently, the mapping has been used to test the system on publicly available benchmarks (Purandare and Pedersen, 2004; Niu et al., 2005). A third alternative is to evaluate the systems according to some performance in an application, e.g. information retrieval (Sch¨utze, 1998). This is a very attractive idea, but requires expensive system development and it is sometimes difficult to separate the reasons for the good (or bad) performance. In this task we decided to adopt the first two alternatives, since they allow for comparison over publicly available systems of any kind. With this goal on mind we gave all the participants an unlabeled corpus, and asked them to induce the senses and create a clustering solu"
S07-1002,J98-1004,0,0.881221,"Missing"
S07-1002,W04-0811,0,0.036541,"the dataset for mapping). We provide a comparison to the results of the systems participating in the lexical sample subtask of task 17. 1 Introduction Word Sense Disambiguation (WSD) is a key enabling-technology. Supervised WSD techniques are the best performing in public evaluations, but need large amounts of hand-tagging data. Existing hand-annotated corpora like SemCor (Miller et al., 1993), which is annotated with WordNet senses (Fellbaum, 1998) allow for a small improvement over the simple most frequent sense heuristic, as attested in the all-words track of the last Senseval competition (Snyder and Palmer, 2004). In theory, larger amounts of training data (SemCor has approx. 500M words) would improve the performance of supervised WSD, but no current project exists to provide such an expensive resource. Another problem of the supervised approach is that the inventory and distribution of senses changes dramatically from one domain to the other, requiring additional hand-tagging of corpora (Mart´ınez and Agirre, 2000; Koeling et al., 2005). Supervised WSD is based on the “fixed-list of senses” paradigm, where the senses for a target word are a closed list coming from a dictionary or lexicon. Lexicograph"
S07-1074,H93-1061,0,0.0445421,"ombinations (due to CPU requirements, not all combination were feasible). 4. Choose the x best combination overall, and optimize word by word among these combination. We set x = 8 for this work, k was fixed in 5, and d = 200 (except with SVD-OMT[topic] which was d = 50). Table 1 shows the best results for 3 fold crossvalidation in SemEval lexical sample training corpus. The figures show that optimizing each word the performance increases 0.7 percentage points over the best combination. 4.2 Optimization for the all-words task To train the classifiers for the all-words task we just used Semcor (Miller et al., 1993). In (Agirre et al., 2006) we already tested our approach on the Senseval-3 all-words task. The best performance for the Senseval-3 all-words task was obtained with k = 5 and d = 200, but we decided to to perform further experiments to search for the best combination. We tested the performance of the combination of single k-NN training on Semcor and testing both on the Senseval-3 all-words data (cf. Table 2) and on the training data from SemEval-2007 lexical sample (cf. Table 3). Note that tables 2 and 3 show contradictory results. Given that in SemEval-2007 lexical sample Combination all feat"
S07-1074,N01-1006,0,0.0144893,"Bag-of-words features: we extract the lemmas of the content words in the whole context, and in a ±4-word window around the target. We also obtain salient bigrams in the context, with the methods and the software described in (Pedersen, 2001). Domain features: The WordNet Domains resource was used to identify the most relevant domains in the context. Following the relevance formula presented in (Magnini and Cavagli´a, 2000), we defined 2 feature types: (1) the most relevant domain, and (2) a list of domains above a predefined threshold3 . 1 The PoS tagging was performed with the fnTBL toolkit (Ngai and Florian, 2001). 2 This software was kindly provided by David Yarowsky’s group, from Johns Hopkins University. 3 The software to obtain the relevant domains was kindly provided by Gerard Escudero’s group, from Universitat Politec342 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 342–345, c Prague, June 2007. 2007 Association for Computational Linguistics 2.2 SVD features Singular Value Decomposition (SVD) is an interesting solution to the sparse data problem. This technique reduces the dimensions of the vectorial space finding correlations and collapsing features."
S07-1074,N01-1011,0,0.141735,"d-forms, or PoS tags1 . Other local features are those formed with the previous/posterior lemma/word-form in the context. Syntactic dependencies: syntactic dependencies were extracted using heuristic patterns, and regular expressions defined with the PoS tags around the target2 . The following relations were used: object, subject, noun-modifier, preposition, and sibling. Bag-of-words features: we extract the lemmas of the content words in the whole context, and in a ±4-word window around the target. We also obtain salient bigrams in the context, with the methods and the software described in (Pedersen, 2001). Domain features: The WordNet Domains resource was used to identify the most relevant domains in the context. Following the relevance formula presented in (Magnini and Cavagli´a, 2000), we defined 2 feature types: (1) the most relevant domain, and (2) a list of domains above a predefined threshold3 . 1 The PoS tagging was performed with the fnTBL toolkit (Ngai and Florian, 2001). 2 This software was kindly provided by David Yarowsky’s group, from Johns Hopkins University. 3 The software to obtain the relevant domains was kindly provided by Gerard Escudero’s group, from Universitat Politec342"
S07-1074,magnini-cavaglia-2000-integrating,0,\N,Missing
S07-1075,S07-1002,1,0.930206,"07 competition, on word sense induction and Web people search, respectively, with mixed results. 1 Introduction This paper describes a graph-based unsupervised system for induction and classification. Given a set of data to be classified, the system first induces the possible clusters and then clusters the data accordingly. The paper is organized as follows. Section 2 gives an description of the general framework of our system. Sections 3 and 4 presents in more detail the implementation of the framework for the Semeval2007 WEPS task (Artiles et al., 2007) and Semeval2007 sense induction task (Agirre and Soroa, 2007), respectively. Section 5 presents the results obtained in both tasks, and Section 6 draws some conclusions. 2 A graph based system for unsupervised classification The system performs a two stage graph based clustering where a co-occurrence graph is first clustered First step: calculating hub score vectors In a first step, and for each entity to be clustered, a graph consisting on context word co-occurrences is built. Vertices in the co-occurrence graph are words and two vertices share an edge whenever they cooccur in the same context. Besides, each edge receives a weight, which indicates how"
S07-1075,W06-3814,1,0.882703,"Missing"
S07-1075,W06-1669,1,0.887189,"Missing"
S07-1075,S07-1012,0,0.0768092,"system has participated in tasks 2 and 13 of the SemEval-2007 competition, on word sense induction and Web people search, respectively, with mixed results. 1 Introduction This paper describes a graph-based unsupervised system for induction and classification. Given a set of data to be classified, the system first induces the possible clusters and then clusters the data accordingly. The paper is organized as follows. Section 2 gives an description of the general framework of our system. Sections 3 and 4 presents in more detail the implementation of the framework for the Semeval2007 WEPS task (Artiles et al., 2007) and Semeval2007 sense induction task (Agirre and Soroa, 2007), respectively. Section 5 presents the results obtained in both tasks, and Section 6 draws some conclusions. 2 A graph based system for unsupervised classification The system performs a two stage graph based clustering where a co-occurrence graph is first clustered First step: calculating hub score vectors In a first step, and for each entity to be clustered, a graph consisting on context word co-occurrences is built. Vertices in the co-occurrence graph are words and two vertices share an edge whenever they cooccur in the same conte"
S07-1075,atserias-etal-2006-freeling,0,0.0602508,"Missing"
S07-1076,P06-1013,0,0.0199303,"the integration of this method into a supervised system by different means. Thus, this paper describes both the unsupervised system (UBC-UMB-1), and the combined supervised system (UBC-UMB-2) submitted to the all-words task. Our motivation in building unsupervised systems comes from the difﬁculty of creating hand-tagged data for all words and all languages, which is colloquially known as the knowledge acquisition bottleneck. There have also been promising results in recent work on the combination of unsupervised approaches that suggest the gap with respect to supervised systems is narrowing (Brody et al., 2006). In this section, we will describe the standalone algorithms (three unsupervised and one supervised) and the combination schemes we explored. The unsupervised methods are based on different intuitions for disambiguation (topical features, local context, and WordNet relations), which is a desirable characteristic for combining algorithms. 2.1 Topic Signatures (TS) Topic signatures (Agirre and de Lacalle, 2004) are lists of words related to a particular sense. They can be built from a variety of sources, and be used directly to perform WSD. Cuadros and Rigau (2006) present a detailed evaluation"
S07-1076,W06-1663,0,0.012481,"o supervised systems is narrowing (Brody et al., 2006). In this section, we will describe the standalone algorithms (three unsupervised and one supervised) and the combination schemes we explored. The unsupervised methods are based on different intuitions for disambiguation (topical features, local context, and WordNet relations), which is a desirable characteristic for combining algorithms. 2.1 Topic Signatures (TS) Topic signatures (Agirre and de Lacalle, 2004) are lists of words related to a particular sense. They can be built from a variety of sources, and be used directly to perform WSD. Cuadros and Rigau (2006) present a detailed evaluation of topic signatures built from a variety of knowledge sources. In this work we built those coming from the following: • the relations in the Multilingual Central Repository (TS-MCR) • the relations in the Extended WordNet (TSXWN) In order to apply this resource for WSD, we simply measured the word-overlap between the target context and each of the senses of the target word. The sense with highest overlap is chosen as the correct sense. 350 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 350–353, c Prague, June 2007. 200"
S07-1076,U06-1008,1,0.677397,"se coming from the following: • the relations in the Multilingual Central Repository (TS-MCR) • the relations in the Extended WordNet (TSXWN) In order to apply this resource for WSD, we simply measured the word-overlap between the target context and each of the senses of the target word. The sense with highest overlap is chosen as the correct sense. 350 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 350–353, c Prague, June 2007. 2007 Association for Computational Linguistics 2.2 Relatives in Context (RIC) This is an unsupervised method presented in Martinez et al. (2006). This algorithm makes use of the WordNet relatives of the target word for disambiguation. The process is carried out in these steps: (i) obtain a set of close relatives from WordNet for each sense (the relatives can be polysemous); (ii) for each test instance deﬁne all possible word sequences that include the target word; (iii) for each word sequence, substitute the target word with each relative and query a web search engine; (iv) rank queries according to the following factors: length of the query, distance of the relative to the target word, and number of hits; and (v) select the sense ass"
S07-1076,J01-3001,0,0.0361181,"nstraints. 2.5 Combination of systems We explored two approaches to combine the standalone systems. The ﬁrst consisted simply of adding up the normalized weights that each system would give to each sense. We tested this voting approach both for the unsupervised and supervised settings. The second method could only be applied in combination with the supervised kNN system. The idea was to include the unsupervised predictions as weighted features for the supervised system. We refer to this method as “stacking”, and it has been previously used to integrate heterogeneous knowledge sources for WSD (Stevenson and Wilks, 2001). 3 Development experiments We tested the single algorithms and their combination over both Semcor and the training distribution of the SemEval-2007 lexical-sample subtask of task 17 (S07LS for short). The goal of these experiments was to obtain an estimate of the expected performance, and submit the most promising conﬁguration. We present ﬁrst the tests on the unsupervised setting, and then the supervised setting. It is important to note that the hand-tagged corpora was not used to ﬁne-tune the parameters of the unsupervised algorithms. 3.1 Unsupervised systems For the ﬁrst evaluation of our"
S07-1076,agirre-de-lacalle-2004-publicly,1,\N,Missing
S07-1077,W01-0703,1,0.803584,"ematic roles are allowed. 3 Including Selectional Preferences Selectional Preferences (SP) try to capture the fact that linguistic elements prefer arguments of a certain semantic class, e.g. a verb like ‘eat’ prefers as subject edible things, and as subject animate entities, as in “She was eating an apple” They can be learned from corpora, generalizing from the observed argument heads (e.g. ‘apple’, ‘biscuit’, etc.) into abstract classes (e.g. edible things). In our case we 1 http://mallet.cs.umass.edu Restriction 5 applies to PropBank output. Restriction 6 applies to VerbNet output 2 follow (Agirre and Martinez, 2001) and use WordNet (Fellbaum, 1998) as the generalization classes (the concept &lt;food,nutrient>). The aim of using Selectional Preferences (SP) in SRL is to generalize from the argument heads in the training instances into general word classes. In theory, using word classes might overcome the data sparseness problem for the head-based features, but at the cost of introducing some noise. More specifically, given a verb, we study the occurrences of the target verb in a training corpus (e.g. the PropBank corpus), and learn a set of SPs for each argument and adjunct of that verb. For instance, given"
S07-1077,W04-2415,1,0.897212,"Missing"
S07-1077,J02-3001,0,0.595616,"selected by exploring the sentence spans or regions defined by the clause boundaries, and they are labeled with BIO tags depending on the location of the token: at the beginning, inside, or outside of a verb argument. After this data pre-processing step, we obtain a more compact and easier to process data representation, making also impossible overlapping and embedded argument predictions. 2.2 Feature Representation Apart from Selectional Preferences (cf. Section 3) and those extracted from provided semantic information, most of the features we used are borrowed from the existing literature (Gildea and Jurafsky, 2002; Xue and Palmer, 2004; Surdeanu et al., forthcoming). 354 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 354–357, c Prague, June 2007. 2007 Association for Computational Linguistics On the verb predicate: • Form; Lemma; POS tag; Chunk type and Type of verb phrase; Verb voice; Binary flag indicating if the verb is a start/end of a clause. • Subcategorization, i.e., the phrase structure rule expanding the verb parent node. • VerbNet class of the verb (in the ”close” track only). On the focus constituent: • Type; Head; • First and last words and POS t"
S07-1077,J05-1004,0,0.0932139,"set and it ranks first in the SRL subtask of the Semeval-2007 task 17. 1 We participated in both the “close” and the “open” tracks of Semeval2007 with the same system, making use, in the second case, of the larger CoNLL2005 training set. 2 System Description 2.1 Data Representation Introduction In Semantic Role Labeling (SRL) the goal is to identify word sequences or arguments accompanying the predicate and assign them labels depending on their semantic relation. In this task we disambiguate argument structures in two ways: predicting VerbNet (Kipper et al., 2000) thematic roles and PropBank (Palmer et al., 2005) numbered arguments, as well as adjunct arguments. In this paper we describe our system for the SRL subtask of the Semeval2007 task 17. It is based on the architecture and features of the system named ‘model 2’ of (Surdeanu et al., forthcoming), but it introduces two changes: we use Maximum Entropy for learning instead of AdaBoost and we enlarge the feature set with combined features and other semantic features. Traditionally, most of the features used in SRL are extracted from automatically generated syntactic and lexical annotations. In this task, we also experiment with provided hand labele"
S07-1077,W04-3212,0,0.413048,"sentence spans or regions defined by the clause boundaries, and they are labeled with BIO tags depending on the location of the token: at the beginning, inside, or outside of a verb argument. After this data pre-processing step, we obtain a more compact and easier to process data representation, making also impossible overlapping and embedded argument predictions. 2.2 Feature Representation Apart from Selectional Preferences (cf. Section 3) and those extracted from provided semantic information, most of the features we used are borrowed from the existing literature (Gildea and Jurafsky, 2002; Xue and Palmer, 2004; Surdeanu et al., forthcoming). 354 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 354–357, c Prague, June 2007. 2007 Association for Computational Linguistics On the verb predicate: • Form; Lemma; POS tag; Chunk type and Type of verb phrase; Verb voice; Binary flag indicating if the verb is a start/end of a clause. • Subcategorization, i.e., the phrase structure rule expanding the verb parent node. • VerbNet class of the verb (in the ”close” track only). On the focus constituent: • Type; Head; • First and last words and POS tags of the constituent"
S10-1013,J07-4005,0,0.597125,"0.505 ±0.026 0.350 R verbs 0.450 ±0.034 0.454 ±0.034 0.291 ±0.025 0.403 ±0.033 0.293 R 0.529 ±0.021 0.521 ±0.018 0.496 ±0.019 0.462 ±0.020 0.294 R nouns 0.530 ±0.024 0.522 ±0.023 0.507 ±0.020 0.472 ±0.024 0.308 R verbs 0.528 ±0.038 0.519 ±0.035 0.468 ±0.037 0.437 ±0.035 0.257 Table 3: Overall results for the domain WSD datasets, ordered by recall. This is the only group using hand-tagged data from the target domain. Their best run ranked 1st. with two variants. In the first (IIITH1), the vertices of the graph are initialized following the ranking scores obtained from predominant senses as in (McCarthy et al., 2007). In the second (IIITH2), the graph is initialized with keyness values as in IIITTH: They presented a personalized PageRank algorithm over a graph constructed from WordNet similar to (Agirre and Soroa, 2009), 77 CFILT-2 CFILT-1 IIITH1-d.l.ppr.05 IIITH2-d.l.ppr.05 BLC20SCBG BLC20SC CFILT-3 Treematch Treematch-2 Kyoto-2 Treematch-3 RACAI-MFS UCF-WS HIT-CIR-DMFS UCF-WS-domain IIITH2-d.r.l.baseline.05 IIITH1-d.l.baseline.05 RACAI-2MFS-BOW IIITH1-d.l.ppv.05 IIITH2-d.r.l.ppv.05 UCF-WS-domain.noPropers Kyoto-1 BLC20BG NLEL-WSD-PDB RACAI-Lexical-Chains MFS NLEL-WSD Rel. Sem. Trees Rel. Sem. Trees-2 Re"
S10-1013,W04-0807,0,0.0284195,"omain for WSD in four languages (Chinese, Dutch, English, Italian). 11 teams participated, with supervised and knowledge-based systems, mainly in the English dataset. The results show that in all languages the participants where able to beat the most frequent sense heuristic as estimated from general corpora. The most successful approaches used some sort of supervision in the form of hand-tagged examples from the domain. 1 Introduction Word Sense Disambiguation (WSD) competitions have focused on general domain texts, as attested in previous Senseval and SemEval competitions (Kilgarriff, 2001; Mihalcea et al., 2004; Snyder and Palmer, 2004; Pradhan et al., 2007). Spe1 http://xmlgroup.iit.cnr.it/SemEval2010/ and http://semeval2.fbk.eu/ 2 http://www.kyoto-project.eu/ 75 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 75–80, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics independent of the application domain. The paper is structured as follows. We first present the preparation of the data. Section 3 reviews participant systems and Section 4 the results. Finally, Section 5 presents the conclusions. 2 Chinese Dutch English Italian The"
S10-1013,E09-1005,1,0.474766,".308 R verbs 0.528 ±0.038 0.519 ±0.035 0.468 ±0.037 0.437 ±0.035 0.257 Table 3: Overall results for the domain WSD datasets, ordered by recall. This is the only group using hand-tagged data from the target domain. Their best run ranked 1st. with two variants. In the first (IIITH1), the vertices of the graph are initialized following the ranking scores obtained from predominant senses as in (McCarthy et al., 2007). In the second (IIITH2), the graph is initialized with keyness values as in IIITTH: They presented a personalized PageRank algorithm over a graph constructed from WordNet similar to (Agirre and Soroa, 2009), 77 CFILT-2 CFILT-1 IIITH1-d.l.ppr.05 IIITH2-d.l.ppr.05 BLC20SCBG BLC20SC CFILT-3 Treematch Treematch-2 Kyoto-2 Treematch-3 RACAI-MFS UCF-WS HIT-CIR-DMFS UCF-WS-domain IIITH2-d.r.l.baseline.05 IIITH1-d.l.baseline.05 RACAI-2MFS-BOW IIITH1-d.l.ppv.05 IIITH2-d.r.l.ppv.05 UCF-WS-domain.noPropers Kyoto-1 BLC20BG NLEL-WSD-PDB RACAI-Lexical-Chains MFS NLEL-WSD Rel. Sem. Trees Rel. Sem. Trees-2 Rel. Cliques 0.3 0.35 0.4 0.45 0.5 0.55 Figure 1: Plot for all the systems which participated in English domain WSD. Each point correspond to one system (denoted in axis Y) according each recall and confidence"
S10-1013,S07-1016,0,0.0570391,"English, Italian). 11 teams participated, with supervised and knowledge-based systems, mainly in the English dataset. The results show that in all languages the participants where able to beat the most frequent sense heuristic as estimated from general corpora. The most successful approaches used some sort of supervision in the form of hand-tagged examples from the domain. 1 Introduction Word Sense Disambiguation (WSD) competitions have focused on general domain texts, as attested in previous Senseval and SemEval competitions (Kilgarriff, 2001; Mihalcea et al., 2004; Snyder and Palmer, 2004; Pradhan et al., 2007). Spe1 http://xmlgroup.iit.cnr.it/SemEval2010/ and http://semeval2.fbk.eu/ 2 http://www.kyoto-project.eu/ 75 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 75–80, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics independent of the application domain. The paper is structured as follows. We first present the preparation of the data. Section 3 reviews participant systems and Section 4 the results. Finally, Section 5 presents the conclusions. 2 Chinese Dutch English Italian The data made available to the participants included"
S10-1013,S07-1097,0,0.0246659,"wledge-based WSD system was based on an algorithm originally described in (Schwartz and Gomez, 2008), in which selectors are acquired from the Web via searching with local context of a given word. The sense is chosen based on the similarity or relatedness between the senses of the target word and various types of selectors. In some runs they include predominant senses(McCarthy et al., 2007). The best run ranked 13th. NLEL-WSD(-PDB): The system used for the participation is based on an ensemble of different methods using fuzzy-Borda voting. A similar system was proposed in SemEval-2007 task-7 (Buscaldi and Rosso, 2007). In this case, the component method used where the following ones: 1) Most Frequent Sense from SemCor; 2) Conceptual Density ; 3) Supervised Domain Relative Entropy classifier based on WordNet Domains; 4) Supervised Bayesian classifier based on WordNet Domains probabilities; and 5) Unsupervised Knownet-20 classifiers. The best run ranked 24th. UMCC-DLSI (Relevant): The team submitted three different runs using a knowledge-based system. The first two runs use domain vectors and the third is based on cliques, which measure how much a concept is correlated to the sentence by obtaining Relevant S"
S10-1013,W08-2114,0,0.0667055,"e was calculated analytically. The first sense baseline for each language was taken from each wordnet. The first sense baseline in English and Chinese corresponds to the most frequent sense, as estimated from out-of-domain corpora. In Dutch and Italian, it followed the intuitions of the lexicographer. Note that we don’t have the most frequent sense baseline from the domain texts, which would surely show higher results (Koeling et al., 2005). thesauri from bilingual parallel corpora. The system ranked 14. UCFWS: This knowledge-based WSD system was based on an algorithm originally described in (Schwartz and Gomez, 2008), in which selectors are acquired from the Web via searching with local context of a given word. The sense is chosen based on the similarity or relatedness between the senses of the target word and various types of selectors. In some runs they include predominant senses(McCarthy et al., 2007). The best run ranked 13th. NLEL-WSD(-PDB): The system used for the participation is based on an ensemble of different methods using fuzzy-Borda voting. A similar system was proposed in SemEval-2007 task-7 (Buscaldi and Rosso, 2007). In this case, the component method used where the following ones: 1) Most"
S10-1013,W04-0811,0,0.196041,"anguages (Chinese, Dutch, English, Italian). 11 teams participated, with supervised and knowledge-based systems, mainly in the English dataset. The results show that in all languages the participants where able to beat the most frequent sense heuristic as estimated from general corpora. The most successful approaches used some sort of supervision in the form of hand-tagged examples from the domain. 1 Introduction Word Sense Disambiguation (WSD) competitions have focused on general domain texts, as attested in previous Senseval and SemEval competitions (Kilgarriff, 2001; Mihalcea et al., 2004; Snyder and Palmer, 2004; Pradhan et al., 2007). Spe1 http://xmlgroup.iit.cnr.it/SemEval2010/ and http://semeval2.fbk.eu/ 2 http://www.kyoto-project.eu/ 75 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 75–80, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics independent of the application domain. The paper is structured as follows. We first present the preparation of the data. Section 3 reviews participant systems and Section 4 the results. Finally, Section 5 presents the conclusions. 2 Chinese Dutch English Italian The data made available to th"
S10-1013,E09-1045,0,0.0508574,"Missing"
S10-1013,vossen-etal-2008-kyoto,1,0.763195,"ses in specific domains, the context of the senses might change, and new senses and terms might be involved. Both supervised and knowledge-based systems are affected by these issues: while the first suffer from different context and sense priors, the later suffer from lack of coverage of domain-related words and information. The main goal of this task is to provide a multilingual testbed to evaluate WSD systems when faced with full-texts from a specific domain. All datasets and related information are publicly available from the task websites1 . This task was designed in the context of Kyoto (Vossen et al., 2008)2 , an Asian-European project that develops a community platform for modeling knowledge and finding facts across languages and cultures. The platform operates as a Wiki system with an ontological support that social communities can use to agree on the meaning of terms in specific domains of their interest. Kyoto focuses on the environmental domain because it poses interesting challenges for information sharing, but the techniques and platforms are Domain portability and adaptation of NLP components and Word Sense Disambiguation systems present new challenges. The difficulties found by supervis"
S10-1013,S01-1004,0,0.00916615,"the environment domain for WSD in four languages (Chinese, Dutch, English, Italian). 11 teams participated, with supervised and knowledge-based systems, mainly in the English dataset. The results show that in all languages the participants where able to beat the most frequent sense heuristic as estimated from general corpora. The most successful approaches used some sort of supervision in the form of hand-tagged examples from the domain. 1 Introduction Word Sense Disambiguation (WSD) competitions have focused on general domain texts, as attested in previous Senseval and SemEval competitions (Kilgarriff, 2001; Mihalcea et al., 2004; Snyder and Palmer, 2004; Pradhan et al., 2007). Spe1 http://xmlgroup.iit.cnr.it/SemEval2010/ and http://semeval2.fbk.eu/ 2 http://www.kyoto-project.eu/ 75 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 75–80, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics independent of the application domain. The paper is structured as follows. We first present the preparation of the data. Section 3 reviews participant systems and Section 4 the results. Finally, Section 5 presents the conclusions. 2 Chinese Dut"
S10-1013,H05-1053,0,0.486241,". Note that this method of estimating statistical significance might be more strict than other pairwise methods. We also include the results of two baselines. The random baseline was calculated analytically. The first sense baseline for each language was taken from each wordnet. The first sense baseline in English and Chinese corresponds to the most frequent sense, as estimated from out-of-domain corpora. In Dutch and Italian, it followed the intuitions of the lexicographer. Note that we don’t have the most frequent sense baseline from the domain texts, which would surely show higher results (Koeling et al., 2005). thesauri from bilingual parallel corpora. The system ranked 14. UCFWS: This knowledge-based WSD system was based on an algorithm originally described in (Schwartz and Gomez, 2008), in which selectors are acquired from the Web via searching with local context of a given word. The sense is chosen based on the similarity or relatedness between the senses of the target word and various types of selectors. In some runs they include predominant senses(McCarthy et al., 2007). The best run ranked 13th. NLEL-WSD(-PDB): The system used for the participation is based on an ensemble of different methods"
S10-1013,N09-1004,0,\N,Missing
S10-1013,W00-0901,0,\N,Missing
S10-1093,E09-1005,1,0.892281,"sources. Our participation in Semeval task #17 focused on producing running systems for all languages in the task, and we attained good results in all except Chinese. Due to the pressure of the time-constraints in the competition, the system is still under development, and we expect results to improve in the near future. 1 2 We will present in turn UKB, the Tybots, and the lexical knowledge-bases used. 2.1 UKB UKB is a knowledge-based unsupervised WSD system which exploits the structure of an underlying Language Knowledge Base (LKB) and finds the most relevant concepts given an input context (Agirre and Soroa, 2009). UKB starts by taking the LKB as a graph of concepts G = (V, E) with a set of vertices V derived from LKB concepts and a set of edges E representing relations among them. Giving an input context, UKB applies the so called Personalized PageRank (Haveliwala, 2002) over it to obtain the most representative senses for the context. PageRank (Brin and Page, 1998) is a method for scoring the vertices V of a graph according to each node’s structural importance. The algorithm can be viewed as random walk process that postulate the existence of a particle that randomly traverses the graph, but at any t"
S10-1093,S10-1013,1,0.884699,"Missing"
S10-1093,bosma-vossen-2010-bootstrapping,1,0.72995,"et 3.0 with gloss relations (Fellbaum, 1998). Dutch: The Dutch LKB is part of the Cornetto database version 1.3 (Vossen et al., 2008). The Cornetto database can be obtained from the Dutch/Flanders Taalunie3 . Cornetto comprises taxonomic relations and equivalence rela2 #rels. Table 1: Wordnets and their sizes (entries, synsets, relations and links to WN30g). Tybots (Term Yielding Robots) are text mining software that mine domain terms from corpus (e.g. web pages), organizing them in a hierarchical structure, connecting them to wordnets and ontologies to create a semantic model for the domain (Bosma and Vossen, 2010). The software is freely available using Subversion 2 . Tybots try to establish a view on the terminology of the domain which is as complete as possible, discovering relations between terms and ranking terms by domain relevance. Preceding term extraction, we perform tokenization, part-of-speech tagging and lemmatization, which is stored in Kyoto Annotation Format (KAF) (Bosma et al., 2009). Tybots work through KAF documents, acquire domain relevant terms based on the syntactic features, gather cooccurrence statistics to decide which terms are significant in the domain and produce a thesaurus w"
S10-1093,P98-2127,0,0.0122903,"y et al., 2007), which is used to label all occurrences of the target word in the test dataset. In order to build the thesaurus with the lists of related words, we used Tybots (c.f. section 2.2), one for each corpus of the evaluation dataset, i.e. Chinese, Dutch, English, and Italian. We used the background documents provided by the organizers, which we processed using the linguistic processors of the project to obtain the documents in KAF. We used the Tybots with the following settings. We discarded co-occurring words with frequencies below 105 . Distributional similarity was computed using (Lin, 1998). Finally, we used up to 50 related words for each target word. As in run1, we used the monolingual graphs for the LKBs in each language. Table 2: Overall results of our runs, including precision (P) and recall (R), overall and for each PoS. We include the First Sense (1sense) and random baselines, as well as the best run, as provided by the organizers. 3.2 Run2: UKB using related words Run1: UKB using context The first run is an application of the UKB tool in the standard setting, as described in (Agirre and Soroa, 2009). Given the input text, we split it in sentences, and we disambiguate eac"
S10-1093,J07-4005,0,0.0212144,"ea is to first obtain a list of related words for each of the target words, as collected from a domain corpus. On a second step each target word is disambiguated using the N most related words as context (see below). For instance, in order to disambiguate the word environment, we would not take into account the context of occurrence (as in Section 3.2), but we would use the list of most related words in the thesaurus (e.g. “biodiversity, agriculture, ecosystem, nature, life, climate, . . .”). Using UKB over these contexts we obtain the most predominant sense for each target word in the domain(McCarthy et al., 2007), which is used to label all occurrences of the target word in the test dataset. In order to build the thesaurus with the lists of related words, we used Tybots (c.f. section 2.2), one for each corpus of the evaluation dataset, i.e. Chinese, Dutch, English, and Italian. We used the background documents provided by the organizers, which we processed using the linguistic processors of the project to obtain the documents in KAF. We used the Tybots with the following settings. We discarded co-occurring words with frequencies below 105 . Distributional similarity was computed using (Lin, 1998). Fin"
S10-1093,C98-2122,0,\N,Missing
S12-1051,W07-0718,0,0.194874,"Missing"
S12-1051,W08-0309,0,0.0142237,"Missing"
S12-1051,P11-1020,0,0.0767679,"Missing"
S12-1051,C04-1051,0,0.824876,"Missing"
S12-1051,N06-2015,0,0.158425,"Missing"
S12-1091,S12-1051,1,0.774913,"Missing"
S12-1091,P09-1053,0,0.0256554,"enough to reduce performance on both the MSRpar and SMTeuroparl test sets. This proved to be enough to reduce the scores between these two datasets. 9 Dataset MSRpar MSRvid SMTeur Prec 0.8901 0.9775 0.8842 Rec 0.8853 0.9827 0.8842 F1 0.8877 0.9801 0.8842 Table 4: Results on classifying pairs by source dataset, using five-fold cross validation over training data. be to include the use of syntactic information, in order to obtain better predicate-argument information. Syntactic information has proven useful for the paraphrase identification task over MSRpar, as demonstrated in studies such as (Das and Smith, 2009) and (Socher et al., 2011). Furthermore, a qualitative assessment of the pairs across different datasets showed relatively significant differences, which would strengthen the argument for developing features and methods specific to each dataset. Another improvement would be to develop a better dataset predictor for System 3. Also recognizing there may be ways to normalize and rescale scores across datasets so the regression models used do not have to account for differing means and standard deviations. Finally, there are other bodies of source data that may be adapted for use with the STS task"
S12-1091,I05-5003,0,0.791203,"e used the term frequency of the lemmas. 4 BLEU Features BLEU is a measure developed to automatically assess how closely sentences generated by machine translation systems match reference human generated texts. BLEU is a directional measurement, and works on the assumption that the more lexically similar a system generated sentence is to a reference sentence, a human generated translation, the better the system sentence is. This can also be seen as a standin for the semantic similarity of the pairs, as was shown when BLEU was applied to the paraphrase identification identification problem in (Finch et al., 2005). The BLEU score for a given system sentence and reference sentence of order N is computed using Formula 2. BLEU(sys, ref ) = B · exp N X 1 n=1 N log(pn ) (2) B is a brevity penalty used to prevent degenerate translations. Given this has little bearing on our experiments, we set its value to 1 for our experiments. Following (Papineni et al., 2002), we give each order n equal weight in the geometric mean. The probability of an order n-gram from the system sentence being found in the reference, pn , is given in Formula 3. P pn = ngram∈sys countsys∧ref (ngram) P ngram∈sys countsys (ngram) (3) cou"
S12-1091,W04-1013,0,0.0104986,"third system maintained three separate regressors, each trained specifically for the STS dataset they were drawn from. It used a multinomial classifier to predict which dataset regressor would be most appropriate to score a given pair, and used it to score that pair. This system underperformed, primarily due to errors in the dataset predictor. 1 1. System 1, which used a combination of semantic similarity, lexical similarity, and precision focused part-of-speech (POS) features. 2. System 2, which used features from System 1, with the addition of skip-bigram features derived from the ROUGE-S (Lin, 2004) measure. POS variants of skip-bigrams were incorporated as well. 3. System 3, used the features from above to first classify the dataset the pair was drawn from, and then applied regressors trained for that dataset. Introduction Previous semantic similarity tasks, such as paraphrase identification or recognizing textual entailment, have focused on performing binary decisions. These problems are usually framed in terms of identifying whether a pair of texts exhibit the needed similarity or entailment relationship or not. In many cases, such as producing a ranking over similarity Our systems ch"
S12-1091,N03-5008,0,0.0400035,"Missing"
S12-1091,P02-1040,0,0.0828092,"s (MSRpar), MSR Research Video Description Corpus (MSRvid), and WMT2008 Development dataset (SMTeuroparl). We trained individual regressors for each of these datasets, and applied them to their counterparts in the testing set. Both Systems 1 and 2 used the following types of features: 617 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 617–623, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics 1. Resource based word to word semantic similarities. 2. Cosine-based lexical similarity measure. 3. Bilingual Evaluation Understudy (BLEU) (Papineni et al., 2002) lexical overlap. 4. Precision focused Part of Speech (POS) features. System 2 added the following features: 1. Lexically motivated skip-bigram overlap. 2. Precision focused skip-bigram POS features. One of the primary motivations for our the choice of features was to use relatively simple and fast features, which can be scaled up to large datasets, given appropriate caching and pre-generated lookups. As the test phase included surprise datasets, whose origin was not disclosed, we also trained a fourth model using all of the training data from all three datasets. Systems 1 and 2 employed this"
S12-1091,N04-3012,0,0.0583197,"generate a corresponding word similarity matrix W, with scores generated using the Semantic Matrix. 2.2 WordNet Similarity We used several methods to obtain word to word similarities from WordNet. WordNet is a lexicalsemantic resource that describes typed relationships between synsets, semantic categories a word may belong to. Similarity scoring methods identify the synsets associated with a pair of words, and then use this relationship graph to generate a score. The first set of scorers were generated from the Leacock-Chodorow, Lin, and Wu-Palmer measures from the WordNet Similarity package (Pedersen et al., 2004). For each of these measures, we averaged across all of the possible synsets between a given pair of words. Another scorer we used was Personalized PageRank (PPR) (Agirre et al., 2010), a topic sensitive variant of the PageRank algorithm (Page et al., 1999) that uses a random walk process to identify the significant nodes of a graph given its link structure. We first derived a graph G from WordNet, treating synsets as the vertices and the relationships between synsets as the edges. To obtain a signature for a given word, we apply topic sensitive PageRank (Haveliwala, 2002) over G, using the sy"
S12-1091,N03-1033,0,0.0102141,"Missing"
S12-1091,agirre-etal-2010-exploring,1,\N,Missing
S13-1004,S12-1051,1,0.661393,"t together. • (1) The two sentences are not equivalent, but are on the same topic. The woman is playing the violin. The young lady enjoys listening to the guitar. • (0) The two sentences are on different topics. John went horse back riding at dawn with a whole group of friends. Sunrise at dawn is a magnificent view to take in if you wake up early enough for it. Figure 1: Annotation values with explanations and examples for the core STS task. In 2012 we held the first pilot task at SemEval 2012, as part of the *SEM 2012 conference, with great success: 35 teams participated with 88 system runs (Agirre et al., 2012). In addition, we held a DARPA sponsored workshop at Columbia University1 . In 2013, STS was selected as the official Shared Task of the *SEM 2013 conference. Accordingly, in STS 2013, we set up two tasks: The core task CORE, which is similar to the 2012 task; and a pilot task on typed-similarity TYPED between semi-structured records. For CORE, we provided all the STS 2012 data as training data, and the test data was drawn from related but different datasets. This is in contrast to the STS 2012 task where the train/test data were drawn from the same datasets. The 2012 datasets comprised the fo"
S13-1004,P98-1013,0,0.04578,"(OnWN) and FrameNet-WordNet (FnWN). These pairs are sampled based on the string similarity ranging from 0.4 to 0.9. String similarity is used to measure the similarity between a pair of glosses. The OnWN subset comprises 561 gloss pairs from OntoNotes 4.0 (Hovy et al., 2006) and WordNet 3.0 (Fellbaum, 1998). 370 out of the 561 pairs are sampled from the 110K sense-mapped pairs as made available from the authors. The rest, 291 pairs, are sampled from unmapped sense pairs with a string similarity ranging from 0.5 to 0.9. The FnWN subset has 189 manually mapped pairs of senses from FrameNet 1.5 (Baker et al., 1998) to WordNet 3.1. They are ran35 domly selected from 426 mapped pairs. In combination, both datasets comprise 750 pairs of glosses. 2.2 Typed-similarity TYPED task This task is devised in the context of the PATHS project,3 which aims to assist users in accessing digital libraries looking for items. The project tests methods that offer suggestions about items that might be useful to recommend, to assist in the interpretation of the items, and to support the user in the discovery and exploration of the collections. Hence the task is about comparing pairs of items. The pairs are generated in the E"
S13-1004,S12-1059,0,0.0590663,"_coefficient# Calculating_a_weighted_correlation a one-tailed parametric test based on Fisher’s ztransformation (Press et al., 2002, equation 14.5.10). 4.2 The Baseline Systems For the CORE dataset, we produce scores using a simple word overlap baseline system. We tokenize the input sentences splitting at white spaces, and then represent each sentence as a vector in the multidimensional token space. Each dimension has 1 if the token is present in the sentence, 0 otherwise. Vector similarity is computed using the cosine similarity metric. We also run two freely available sysˇ c et tems, DKPro (Bar et al., 2012) and TakeLab (Sari´ 5 al., 2012) from STS 2012, and evaluate them on the CORE dataset. They serve as two strong contenders since they ranked 1st (DKPro) and 2nd (TakeLab) in last year’s STS task. For the TYPED dataset, we first produce XML files for each of the items, using the fields as provided to participants. Then we run named entity recognition and classification (NERC) and date detection using Stanford CoreNLP. This is followed by calculating the similarity score for each of the types as follows. • General: cosine similarity of TF-IDF vectors of tokens from all fields. • Author: cosine s"
S13-1004,N12-1017,0,0.00854843,"ies on several (1-4) reference translations. HYTER, on the other hand, leverages millions of translations. The HTER set comprises 150 pairs, where one sentence is machine translation output and the corresponding sentence is a human post-edited translation. We sample the data from the dataset used in the DARPA GALE project with an HTER score ranging from 0 to 120. The HYTER set has 600 pairs from 3 subsets (each subset contains 200 pairs): a. reference Figure 3: Annotation instructions for TYPED task vs. machine translation. b. reference vs. Finite State Transducer (FST) generated translation (Dreyer and Marcu, 2012). c. machine translation vs. FST generated translation. The HYTER data set is used in (Dreyer and Marcu, 2012). The OnWN/FnWN dataset contains gloss pairs from two sources: OntoNotes-WordNet (OnWN) and FrameNet-WordNet (FnWN). These pairs are sampled based on the string similarity ranging from 0.4 to 0.9. String similarity is used to measure the similarity between a pair of glosses. The OnWN subset comprises 561 gloss pairs from OntoNotes 4.0 (Hovy et al., 2006) and WordNet 3.0 (Fellbaum, 1998). 370 out of the 561 pairs are sampled from the 110K sense-mapped pairs as made available from the au"
S13-1004,N06-2015,0,0.104937,"ation instructions for TYPED task vs. machine translation. b. reference vs. Finite State Transducer (FST) generated translation (Dreyer and Marcu, 2012). c. machine translation vs. FST generated translation. The HYTER data set is used in (Dreyer and Marcu, 2012). The OnWN/FnWN dataset contains gloss pairs from two sources: OntoNotes-WordNet (OnWN) and FrameNet-WordNet (FnWN). These pairs are sampled based on the string similarity ranging from 0.4 to 0.9. String similarity is used to measure the similarity between a pair of glosses. The OnWN subset comprises 561 gloss pairs from OntoNotes 4.0 (Hovy et al., 2006) and WordNet 3.0 (Fellbaum, 1998). 370 out of the 561 pairs are sampled from the 110K sense-mapped pairs as made available from the authors. The rest, 291 pairs, are sampled from unmapped sense pairs with a string similarity ranging from 0.5 to 0.9. The FnWN subset has 189 manually mapped pairs of senses from FrameNet 1.5 (Baker et al., 1998) to WordNet 3.1. They are ran35 domly selected from 426 mapped pairs. In combination, both datasets comprise 750 pairs of glosses. 2.2 Typed-similarity TYPED task This task is devised in the context of the PATHS project,3 which aims to assist users in acce"
S13-1004,2006.amta-papers.25,0,0.0305115,"the headlines come from a different EMM cluster, then we computed the string similarity between those pairs. Accordingly, we sampled 375 headline pairs of headlines that occur in the same EMM cluster, aiming for pairs equally distributed between minimal and maximal similarity using simple string similarity. We sample another 375 pairs from the different EMM cluster in the same manner. The SMT dataset comprises pairs of sentences used in machine translation evaluation. We have two different sets based on the evaluation metric used: an HTER set, and a HYTER set. Both metrics use the TER metric (Snover et al., 2006) to measure the similarity of pairs. HTER typically relies on several (1-4) reference translations. HYTER, on the other hand, leverages millions of translations. The HTER set comprises 150 pairs, where one sentence is machine translation output and the corresponding sentence is a human post-edited translation. We sample the data from the dataset used in the DARPA GALE project with an HTER score ranging from 0 to 120. The HYTER set has 600 pairs from 3 subsets (each subset contains 200 pairs): a. reference Figure 3: Annotation instructions for TYPED task vs. machine translation. b. reference vs"
S13-1004,S12-1060,0,0.328958,"Missing"
S13-1004,C98-1013,0,\N,Missing
S13-1018,E09-1005,1,0.849802,"se2 . • Subject and description: cosine similarity of TF.IDF vectors of respective fields. IDF values were calculated using a subset of Europeana items (the Culture Grid collection), available internally. These preliminary scores were im2 urlhttp://wordnetcode.princeton.edu/standofffiles/morphosemantic-links.xls 132 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference c and the Shared Task, pages 132–137, Atlanta, Georgia, June 13-14, 2013. 2013 Association for Computational Linguistics proved using TF.IDF based on Wikipedia, UKB (Agirre and Soroa, 2009) and a more informed time similarity measure. We describe each of these processes in turn. 2.1 TF.IDF A common approach to computing document similarity is to represent documents as Bag-Of-Words (BOW). Each BOW is a vector consisting of the words contained in the document, where each dimension corresponds to a word, and the weight is the frequency in the corresponding document. The similarity between two documents can be computed as the cosine of the angle between their vectors. This is the approached use above. This approach can be improved giving more weight to words which occur in only a fe"
S13-1018,agirre-etal-2010-exploring,1,0.860547,"idf2w qP 2 2 w∈a (tfw,a × idfw ) × w∈b (tfw,b × idfw ) P qP w∈a,b where tfw,x is the frequency of the term w in x ∈ {a, b} and idfw is the inverted document frequency of the word w measured in Wikipedia. We substituted the preliminary general similarity score by the obtained using the TF.IDF presented in this section. 2.2 UKB The semantic disambiguation UKB3 algorithm (Agirre and Soroa, 2009) applies personalized PageRank on a graph generated from the English WordNet (Fellbaum, 1998), or alternatively, from Wikipedia. This algorithm has proven to be very competitive in word similarity tasks (Agirre et al., 2010). To compute similarity using UKB we represent WordNet as a graph G = (V, E) as follows: graph nodes represent WordNet concepts (synsets) and 3 http://ixa2.si.ehu.es/ukb/ 133 dictionary words; relations among synsets are represented by undirected edges; and dictionary words are linked to the synsets associated to them by directed edges. Our method is provided with a pair of vectors of words and a graph-based representation of WordNet. We first compute the personalized PageRank over WordNet separately for each of the vector of words, producing a probability distribution over WordNet synsets. We"
S13-1018,P05-1045,0,0.00503141,"y, using the training data provided by the organisers with the previously defined similarity measures as features. We begin by describing our basic system in Section 2, followed by the machine learning system in 1 http://www.europeana.eu/ Basic system The items in this task are taken from Europeana. They cannot be redistributed, so we used the urls and scripts provided by the organizers to extract the corresponding metadata. We analysed the text in the metadata, performing lemmatization, PoS tagging, named entity recognition and classification (NERC) and date detection using Stanford CoreNLP (Finkel et al., 2005; Toutanova et al., 2003). A preliminary score for each similarity type was then calculated as follows: • General: cosine similarity of TF.IDF vectors of tokens, taken from all fields. • Author: cosine similarity of TF.IDF vectors of dc:Creator field. • People involved, time period and location: cosine similarity of TF.IDF vectors of location/date/people entities recognized by NERC in all fields. • Events: cosine similarity of TF.IDF vectors of event verbs and nouns. A list of verbs and nouns possibly denoting events was derived using the WordNet Morphosemantic Database2 . • Subject and descri"
S13-1018,N03-1033,0,0.0199478,"data provided by the organisers with the previously defined similarity measures as features. We begin by describing our basic system in Section 2, followed by the machine learning system in 1 http://www.europeana.eu/ Basic system The items in this task are taken from Europeana. They cannot be redistributed, so we used the urls and scripts provided by the organizers to extract the corresponding metadata. We analysed the text in the metadata, performing lemmatization, PoS tagging, named entity recognition and classification (NERC) and date detection using Stanford CoreNLP (Finkel et al., 2005; Toutanova et al., 2003). A preliminary score for each similarity type was then calculated as follows: • General: cosine similarity of TF.IDF vectors of tokens, taken from all fields. • Author: cosine similarity of TF.IDF vectors of dc:Creator field. • People involved, time period and location: cosine similarity of TF.IDF vectors of location/date/people entities recognized by NERC in all fields. • Events: cosine similarity of TF.IDF vectors of event verbs and nouns. A list of verbs and nouns possibly denoting events was derived using the WordNet Morphosemantic Database2 . • Subject and description: cosine similarity"
S13-1018,L10-1000,0,\N,Missing
S13-1018,W12-1012,1,\N,Missing
S14-2010,S14-2085,0,0.0392393,"Missing"
S14-2010,S14-2069,0,0.0326403,"Missing"
S14-2010,S14-2128,0,0.0328229,"Missing"
S14-2010,P13-1024,1,0.0618966,"data set is a subset of the PASCAL VOC-2008 data set (Rashtchian et al., 2010), which consists of 1,000 images and has been used by a number of image description systems. It was also sampled from string similarity values between 0.6 and 1. Deft-forum and Deft-news are from DEFT data.2 Deft-forum contains the forum post sentences, and Deft-news are news summaries. We selected 450 pairs for Deft-forum and 300 pairs for Deft-news. They are sampled evenly from string similarities falling in the interval 0.6 to 1. The Tweets data set contains tweet-news pairs selected from the corpus released in (Guo et al., 2013), where each pair contains a sentence that pertains to the news title, while the other one represents a Twitter comment on that particular news. They are evenly sampled from string similarity values between 0.5 and 1. Table 1 shows the explanations and values associated with each score between 5 and 0. As in prior years, we used Amazon Mechanical Turk (AMT)3 to crowdsource the annotation of the English pairs.4 Annotators are presented with the Table 2: English subtask: Summary of train (2012 and 2013) and test (2014) datasets. a DARPA sponsored workshop at Columbia University.1 In 2013, STS wa"
S14-2010,S14-2112,0,0.0317369,"Missing"
S14-2010,N06-2015,0,0.0715746,"ferent similarity ranges, hence we built two sets of headline pairs: (i) a set where the pairs come from the same EMM cluster, (ii) and another set where the headlines come from a different EMM cluster, then we computed the string similarity between those pairs. Accordingly, we sampled 375 headline pairs of headlines that occur in the same EMM cluster, aiming for pairs equally distributed between minimal and maximal similarity using simple string similarity. We sampled other 375 pairs from the different EMM cluster in the same manner. For OnWN, we used the sense definition pairs of OntoNotes (Hovy et al., 2006) and WordNet (Fellbaum, 1998). Different from previous tasks, the two definition sentences in a pair belong to different senses. We sampled 750 pairs based on a string similarity ranging from 0.5 to 1. The Images data set is a subset of the PASCAL VOC-2008 data set (Rashtchian et al., 2010), which consists of 1,000 images and has been used by a number of image description systems. It was also sampled from string similarity values between 0.6 and 1. Deft-forum and Deft-news are from DEFT data.2 Deft-forum contains the forum post sentences, and Deft-news are news summaries. We selected 450 pairs"
S14-2010,S12-1051,1,0.623306,"r as both tasks have been defined to date in the literature) in that, rather than being a binary yes/no decision (e.g. a vehicle is not a car), we define STS to be a graded similarity notion (e.g. a vehicle and a car are more similar than a wave and a car). A quantifiable graded bidirectional notion of textual similarity is useful for a myriad of NLP tasks such as MT evaluation, information extraction, question answering, summarization, etc. In 2012 we held the first pilot task at SemEval 2012, as part of the *SEM 2012 conference, with great success: 35 teams participated with 88 system runs (Agirre et al., 2012). In addition, we held In Semantic Textual Similarity, systems rate the degree of semantic equivalence between two text snippets. This year, the participants were challenged with new data sets for English, as well as the introduction of Spanish, as a new language in which to assess semantic similarity. For the English subtask, we exposed the systems to a diversity of testing scenarios, by preparing additional OntoNotesWordNet sense mappings and news headlines, as well as introducing new genres, including image descriptions, DEFT discussion forums, DEFT newswire, and tweet-newswire headline map"
S14-2010,S14-2131,0,0.0336314,"Missing"
S14-2010,S14-2072,0,0.0817436,"Missing"
S14-2010,Q14-1018,0,0.0731,"Missing"
S14-2010,S14-2039,0,0.0993932,"Missing"
S14-2010,S14-2078,0,0.101731,"Missing"
S14-2010,S14-2022,0,0.0306687,"Missing"
S14-2010,S14-2046,0,0.022257,"Missing"
S14-2010,D13-1179,0,0.0175763,"Missing"
S14-2010,S12-1060,0,0.0199826,"Missing"
S14-2010,S14-2093,0,0.0281526,"Missing"
S14-2010,W10-0721,0,0.050451,"d 375 headline pairs of headlines that occur in the same EMM cluster, aiming for pairs equally distributed between minimal and maximal similarity using simple string similarity. We sampled other 375 pairs from the different EMM cluster in the same manner. For OnWN, we used the sense definition pairs of OntoNotes (Hovy et al., 2006) and WordNet (Fellbaum, 1998). Different from previous tasks, the two definition sentences in a pair belong to different senses. We sampled 750 pairs based on a string similarity ranging from 0.5 to 1. The Images data set is a subset of the PASCAL VOC-2008 data set (Rashtchian et al., 2010), which consists of 1,000 images and has been used by a number of image description systems. It was also sampled from string similarity values between 0.6 and 1. Deft-forum and Deft-news are from DEFT data.2 Deft-forum contains the forum post sentences, and Deft-news are news summaries. We selected 450 pairs for Deft-forum and 300 pairs for Deft-news. They are sampled evenly from string similarities falling in the interval 0.6 to 1. The Tweets data set contains tweet-news pairs selected from the corpus released in (Guo et al., 2013), where each pair contains a sentence that pertains to the new"
S14-2010,W10-0707,0,\N,Missing
S14-2010,P94-1019,0,\N,Missing
S14-2010,Q14-1017,0,\N,Missing
S14-2010,S14-2138,0,\N,Missing
S15-1007,P08-1037,1,0.807182,"sters are sensible when compared to those of OntoNotes and WordNet Supersenses. 1 Introduction Word sense ambiguity is a major hurdle for accurate information extraction, summarization and machine translation. The utility of Word Sense Disambiguation (WSD) depends on the accuracy and on how useful the sense distinctions are. The first issue is quantitative, as it can be measured using a WSD system on certain dataset. The second examines whether the sense distinctions are appropriate, which varies from application to application. Although usefulness can be explored in a downstream application (Agirre et al., 2008), it is usually assessed subjectively, discussing the quality of the sense distinctions (Palmer et al., 2007). Both issues 61 Eneko Agirre University of the Basque Country e.agirre@ehu.eus (performance and usefulness) are linked to the granularity of the sense inventory, and conflict with each other: finer granularity might produce more useful distinctions but the accuracy would be worse, and vice-versa. WordNet (Fellbaum, 1998) is the most widely used resource to build word sense disambiguation tools and word sense annotated corpora, including recent large efforts (Passonneau et al., 2012), b"
S15-1007,N06-2015,0,0.176011,"Missing"
S15-1007,S14-1001,0,0.0399523,"to this tendency to join frequent senses, perhaps discounting frequency from confusion measures. 7 Comparison to Supersenses We also perform a qualitative study comparing our coarse grained senses to WN Supersenses. Supersenses are based on the lexicographer file names for WordNet, where all senses of the word that belong to the same lexicographer file (e.g. the artifact file) are joined together. They include 15 sense for verbs and 26 for nouns. Although WordNet also provide supersenses for adjective and adverbs, these are not semantically motivated and do not provide any higher abstraction (Johannsen et al., 2014). Table 2 show the results for the target 45 words (adjectives included). The average polysemy of the supersenses is lower for all parts of speech with respect to our clustered senses and OntoNotes. Note that, word-wise, polysemy varies significantly: many words keep one or two senses, while others maintain high polysemy level (roughly similar to fine-grained senses). IMS and MFS performances are similar to OntoNotes. Tables 6 to 8 show the differences in clustering for the same set of words (add-v, level-n, and helpn). In the case of add-v (Table 6), we produce two coarse grained sense agains"
S15-1007,W02-1006,0,0.10089,"Missing"
S15-1007,P99-1020,0,0.149846,"Missing"
S15-1007,H93-1061,0,0.736807,"Missing"
S15-1007,S07-1006,0,0.279885,"Missing"
S15-1007,P96-1006,0,0.563321,"Missing"
S15-1007,Q14-1025,0,0.0631034,"on automatic measures, while our method is based on human annotations. 3 MASC Crowdsourced annotations The corpus used in the experiments is part of the Manually Annotated Sub-Corpus of the Open American National Corpus, which contains a subsidiary word sense sentence corpus consisting of approximately one thousand sentences per word annotated with WordNet 3.0 sense labels (Passonneau et al., 2012). In this work we make use of a publicly available subset of 45 words (17 nouns, 19 verbs and 9 adjective, see Table2) that have been annotated, 1000 sentences per target word, using crowdsourcing (Passonneau and Carpenter, 2014). The authors collected between 20 and 25 labels for every sentence. They showed that a probabilistic annotation model based on crowdsourced data was effective, with favorable quality when compared to a conventional expert-guided annotation model. 4 Clustering Procedure Having access to multiple annotations of the same item allows to identify correlations among senses of a word. In particular, we can mine how many times the annotators confused 2 particular senses of a word. If two senses are confused very often, it will signal that the annotators find the differences between the two senses dif"
S15-1007,passonneau-etal-2012-masc,0,0.304864,"ation (Agirre et al., 2008), it is usually assessed subjectively, discussing the quality of the sense distinctions (Palmer et al., 2007). Both issues 61 Eneko Agirre University of the Basque Country e.agirre@ehu.eus (performance and usefulness) are linked to the granularity of the sense inventory, and conflict with each other: finer granularity might produce more useful distinctions but the accuracy would be worse, and vice-versa. WordNet (Fellbaum, 1998) is the most widely used resource to build word sense disambiguation tools and word sense annotated corpora, including recent large efforts (Passonneau et al., 2012), but its fine-grainedness has been mentioned to be a problem (Hovy et al., 2006; Palmer et al., 2007). We think that a desiderata for a sense inventory would be that it provides useful sense distinctions and useful performance across a large range of applications. We would also add that it should be tightly integrated with WordNet, given its prevalence on NLP applications, and we thus focus on sense inventories which are mapped to WordNet. In order to asses usefulness, we need specific measures. Downstream application is difficult, and unfeasible for new proposals, as a full-fledged sense inv"
S15-1007,N01-1010,0,0.741148,"Missing"
S15-1007,W99-0500,0,0.563472,"Missing"
S15-1007,P10-4014,0,0.229109,"Missing"
S15-1007,D07-1107,0,\N,Missing
S15-1011,P11-1095,0,0.51305,"P (e)P (s|e)P (cgrf |e) P (e)P (s|e)P (cbow |e)P (cgrf |e) Best (state-of-the-art) Aida 67.54 75.05 76.83 83.28 84.89 Kore 35.42 60.42 54.86 70.83 71.50 Tac09 67.04 77.19 79.40* 82.21* 79.00 Tac10 76.96 85.20* 83.92* 85.98* 80.60 Tac11 67.83 75.55 79.75 81.85* 80.10 Tac12 46.20 57.06 70.13* 71.65* 68.50 Tac13 66.54 74.56* 70.21 73.99* 71.80 Tac14 62.01 71.21 71.28 76.48 79.60 Table 1: Bold marks the best value among probability combinations, and * those results that overcome the best value reported in the state-of-the-art: (Houlsby and Ciaramita, 2014) for Aida, (Moro et al., 2014) for Kore, (Han and Sun, 2011) for Tac09 and see TAC-KBP proceedings for the rest8 . Test P (e)P (s|e)P (cbow |e)P (cgrf |e) P (e)α P (s|e)β P (cbow |e)γ P (cgrf |e)δ (Moro et al., 2014) (Hoffart et al., 2011) (Houlsby and Ciaramita, 2014) Aida 83.28 84.88 82.10 82.54 84.89 Table 2: Micro accuracy results for Aida introducing the Weighted Full Model in row 2. those reported by (Hoffart et al., 2011; Moro et al., 2014) (respectively, 82.5412 and 82.10). Unfortunately the parameter distribution seems to depend on the test dataset, as the same parameters failed to improve the results on the other datasets. 6 Related Work The"
S15-1011,Q14-1019,0,0.0545964,"(s|e) P (e)P (s|e)P (cbow |e) P (e)P (s|e)P (cgrf |e) P (e)P (s|e)P (cbow |e)P (cgrf |e) Best (state-of-the-art) Aida 67.54 75.05 76.83 83.28 84.89 Kore 35.42 60.42 54.86 70.83 71.50 Tac09 67.04 77.19 79.40* 82.21* 79.00 Tac10 76.96 85.20* 83.92* 85.98* 80.60 Tac11 67.83 75.55 79.75 81.85* 80.10 Tac12 46.20 57.06 70.13* 71.65* 68.50 Tac13 66.54 74.56* 70.21 73.99* 71.80 Tac14 62.01 71.21 71.28 76.48 79.60 Table 1: Bold marks the best value among probability combinations, and * those results that overcome the best value reported in the state-of-the-art: (Houlsby and Ciaramita, 2014) for Aida, (Moro et al., 2014) for Kore, (Han and Sun, 2011) for Tac09 and see TAC-KBP proceedings for the rest8 . Test P (e)P (s|e)P (cbow |e)P (cgrf |e) P (e)α P (s|e)β P (cbow |e)γ P (cgrf |e)δ (Moro et al., 2014) (Hoffart et al., 2011) (Houlsby and Ciaramita, 2014) Aida 83.28 84.88 82.10 82.54 84.89 Table 2: Micro accuracy results for Aida introducing the Weighted Full Model in row 2. those reported by (Hoffart et al., 2011; Moro et al., 2014) (respectively, 82.5412 and 82.10). Unfortunately the parameter distribution seems to depend on the test dataset, as the same parameters failed to improve the results on the other"
S15-1011,P11-1138,0,0.293385,"ext, are complementary. We test our method in eight datasets, improving the state-of-the-art results in five, without any tuning, showing that it is robust to out-ofdomain scenarios. When tuning combination weights, we match the best reported results on the widely-used AIDA-CoNLL test-b. 1 Introduction Linking mentions occurring in documents to a knowledge base is the main goal of Entity Linking or Named Entity Disambiguation (NED). This problem has attracted a great number of papers in the NLP and IR communities, and a large number of techniques, including local context and global inference (Ratinov et al., 2011). We propose to use a probabilistic framework that combines entity popularity, name popularity, local mention context and global hyperlink structure, relying on information in Wikipedia alone. Entity and name popularity are useful disambiguation clues in the absence of any context. The local mention context provides direct clues (in the form of words in context) to disambiguate each mention separately. The hyperlink structure of Wikipedia provides a global coherence measure for all entities mentioned in the same context. Eneko Agirre IXA NLP Group UPV/EHU Donostia, Basque Country e.agirre@ehu."
S15-1011,D11-1072,0,\N,Missing
S15-2032,agerri-etal-2014-ixa,1,0.840032,"tag and the similarity score. 3.1.1 Input Handling and Chunking Module We use the Stanford NLP parser (Klein and Manning, 2003) to linguistically process input sentences and register lowercased token information (lemma, part of speech analysis and dependency structure is also needed for the following module). The next step consists of determining segments or token regions. This information is gathered according to the specified scenario (GS or SYS). In the case of the GS scenario the baseline obviously uses gold standard input; and, in the SYS scenario the baseline uses the ixa-pipes-chunker (Agerri et al., 2014). Ixa-pipes-chunk has been trained using the Apache OpenNLP API (OpenNLP, 2011), which is a maximum entropy chunker. Nevertheless, the chunker’s output has been improved using simple regular expressions to fit to our task proposal. Actually, we developed four rules to optimize how conjunctions, punctuations and prepositions are handled. In brief, the developed rules try to join consequent chunks forming new chunks consisting of the previous ones, for instance, we found significant improvement if prepositional phrases followed by a nominal phrase were unified as a single chunk. We also develope"
S15-2032,C14-1068,0,0.053166,"Missing"
S15-2032,N13-1092,0,0.0929416,"Missing"
S15-2032,P03-1054,0,0.00490512,"fy segments over sentence pairs, and then, make alignments between them. First of all, the input handling and chunking module is responsible for linguistically processing the given input, and for creating the internal representation of the sentences. Once the input is processed the alignment module identifies related and unrelated segments among sentences. Finally, by using segment pair based features the classification module and the scoring module produce respectively the final relatedness tag and the similarity score. 3.1.1 Input Handling and Chunking Module We use the Stanford NLP parser (Klein and Manning, 2003) to linguistically process input sentences and register lowercased token information (lemma, part of speech analysis and dependency structure is also needed for the following module). The next step consists of determining segments or token regions. This information is gathered according to the specified scenario (GS or SYS). In the case of the GS scenario the baseline obviously uses gold standard input; and, in the SYS scenario the baseline uses the ixa-pipes-chunker (Agerri et al., 2014). Ixa-pipes-chunk has been trained using the Apache OpenNLP API (OpenNLP, 2011), which is a maximum entropy"
S15-2032,Q14-1018,0,0.0369857,". Actually, we developed four rules to optimize how conjunctions, punctuations and prepositions are handled. In brief, the developed rules try to join consequent chunks forming new chunks consisting of the previous ones, for instance, we found significant improvement if prepositional phrases followed by a nominal phrase were unified as a single chunk. We also developed some rules to unify nominal phrases separated by punctuations or conjunctions, or a combination of those. 3.1.2 Alignment Module The alignment module mainly focuses on the work done by the monolingual word aligner described in (Sultan et al., 2014), and HungarianMunkres algorithm. The monolingual word aligner is a simple and ready-to-use system that has demonstrated state-ofthe-art performance. To begin with we start by constructing the token to token link matrix in which each element at position (i,j) determines that there exists a link between token i (from sentence 1) and token j (from sentence 2). A link exists in the matrix if and 180 only if the monolingual word aligner has determined that both tokens are related. Then, the system uses token regions to group individual tokens into segments, and calculates the weight between every"
S15-2032,N03-1033,0,0.00552957,"rity score. We also participated in the pilot on Interpretable STS, where we apply a pipeline which first aligns tokens, then chunks, and finally uses supervised systems to label and score each chunk alignment. Note that some of the authors participated in the organization of the task. We scrupulously separated the tasks in such a way that the developers of the systems did not have access to the test sets, and that they only had access to the same training data as the rest of the participants. Building Cubes The first step is to produce parse trees for the sentences using the Stanford Parser (Toutanova et al., 2003). After parsing the sentences each pair of sentences can be represented by a NxM matrix, being N is the number of nodes of the parse tree of the first sentence, and M the number of nodes of the parse tree of the second sentence. Note that some nodes (terminals) correspond to words, while others (nonterminals) represent phrases. We can have as many matrices as we wish, and fill them with different similarity scores, forming a cube. In this first attempt we used three layers: 1. Euclidean distance between Collobert and Weston Word Vector (Collobert and Weston, 2008). The vector representations f"
S15-2032,S15-2045,1,\N,Missing
S15-2045,agerri-etal-2014-ixa,1,0.57453,"onal and knowledge-based similarity are widely used, and also syntactic analysis and named entity recognition. Most teams add a machine learning algorithm to learn the output scores, but note that Samsung team did not use it in their best run. 4 3.6 The baseline system used for the interpretable subtask consists of a cascade concatenation of several procedures. First, we undertake a brief NLP step in which input sentences are tokenized using simple regular expressions. Additionally, this step collects chunk regions coming either from gold standard or from the chunking done by ixa-pipes-chunk (Agerri et al., 2014). This is followed by a lowercased token aligning phase, which consists of aligning (or linking) identical tokens across the input sentences. Then we use chunk boundaries as token regions to group individual tokens into groups, and compute all links across groups. The weight of the link across groups is proportional to the number of links counted between within-group tokens. The next phase consists of an optimization step in which groups x,y that have the highest link weight are identified, as well as the chunks that are linked to either x or y but not with a maximum alignment weight (thus ena"
S15-2045,S12-1051,1,0.519741,"TS also differs from both TE and paraphrasing (in as far as both tasks have been defined to date in the literature) in that rather than being a binary yes/no decision (e.g. a vehicle is not a car), we define STS to be a graded similarity notion (e.g. a vehicle and a car are more similar than a wave and a car). A quantifiable graded bidirectional notion of textual similarity is useful for many NLP tasks such as MT evaluation, information extraction, question answering, summarization. In 2012, we held the first pilot task at SemEval 2012, as part of the *SEM 2012 conference, with great success (Agirre et al., 2012). In addition, we 252 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 252–263, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics held a DARPA sponsored workshop at Columbia University.1 In 2013, STS was selected as the official shared task of the *SEM 2013 conference, with two subtasks: a core task, which was similar to the 2012 task, and a pilot task on typed-similarity between semi-structured records. In 2014, new datasets including new genres were used, and we expanded the evaluations to address sentence similarity"
S15-2045,S14-2010,1,0.800447,"f the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 252–263, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics held a DARPA sponsored workshop at Columbia University.1 In 2013, STS was selected as the official shared task of the *SEM 2013 conference, with two subtasks: a core task, which was similar to the 2012 task, and a pilot task on typed-similarity between semi-structured records. In 2014, new datasets including new genres were used, and we expanded the evaluations to address sentence similarity in a new language, namely Spanish (Agirre et al., 2014). This year we presented three subtasks: the English subtask, the Spanish subtask and the interpretable pilot subtask. The English subtask comprised pairs from headlines and image descriptions, and it also introduced new genres, including answer pairs from a tutorial dialogue system and from Q&A websites, and pairs from a dataset tagged with committed belief annotations. For the Spanish subtask, additional pairs from news and Wikipedia articles were selected. The annotations for both tasks leveraged crowdsourcing. Finally, with the interpretable STS pilot subtask, we wanted to start exploring"
S15-2045,P14-1023,0,0.0115737,"7070 0.7251 0.7311 0.7250 0.7422 0.6364 0.7775 0.7032 0.7130 0.7189 0.4616 0.7533 0.6111 0.5379 0.5424 0.5672 0.6558 0.4919 0.5912 0.6964 0.7114 0.6364 Table 3: Task 2a: English evaluation results in terms of Pearson correlation. 259 Rank 61 42 29 63 56 57 70 69 71 62 44 49 43 74 72 73 34 28 26 1 3 5 19 18 16 8 9 2 12 13 23 41 59 20 47 22 11 15 33 45 55 24 10 17 50 51 52 4 7 6 46 36 27 39 31 30 32 25 53 14 40 37 35 68 21 58 66 65 64 48 67 60 42 38 54 approach for the top three participants (DLS@CU, ExBThemis, Samsung). They use WordNet (Miller, 1995), Mikolov Embeddings (Mikolov et al., 2013; Baroni et al., 2014) and PPDB (Ganitkevitch et al., 2013). In general, generic NLP tools such as lemmatization, PoS tagging, distributional word embeddings, distributional and knowledge-based similarity are widely used, and also syntactic analysis and named entity recognition. Most teams add a machine learning algorithm to learn the output scores, but note that Samsung team did not use it in their best run. 4 3.6 The baseline system used for the interpretable subtask consists of a cascade concatenation of several procedures. First, we undertake a brief NLP step in which input sentences are tokenized using simple"
S15-2045,N13-1092,0,0.0796576,"the mentioned works, we first identified the segments (chunks in our case) in each sentence separately, and then aligned them. In a different strand of work, Nielsen et al. (2009) defined a textual entailment model where the “facets” (words under some syntactic/semantic relation) in the response of a student were linked to the concepts in the reference answer. The link would signal whether each facet in the response was entailed by the reference answer or not, but would not explicitly mark which parts of the reference answer caused the entailment. This model was later followed by Levy et al. (2013). Our task was different in that we identified the corresponding chunks in both sentences. We think that, in the future, the aligned facets could provide complementary information to chunks. For interpretable STS the similarity scores range from 0 to 5, as in the English subtask. With respect to the relation between the aligned chunks, the present pilot only allowed 1:1 alignments. As a consequence, we had to include a special alignment context tag (ALIC) to simulate those chunks which had some semantic similarity or relatedness in the other sentence, but could not have been aligned because of"
S15-2045,P13-2080,0,0.0179598,"Contrary to the mentioned works, we first identified the segments (chunks in our case) in each sentence separately, and then aligned them. In a different strand of work, Nielsen et al. (2009) defined a textual entailment model where the “facets” (words under some syntactic/semantic relation) in the response of a student were linked to the concepts in the reference answer. The link would signal whether each facet in the response was entailed by the reference answer or not, but would not explicitly mark which parts of the reference answer caused the entailment. This model was later followed by Levy et al. (2013). Our task was different in that we identified the corresponding chunks in both sentences. We think that, in the future, the aligned facets could provide complementary information to chunks. For interpretable STS the similarity scores range from 0 to 5, as in the English subtask. With respect to the relation between the aligned chunks, the present pilot only allowed 1:1 alignments. As a consequence, we had to include a special alignment context tag (ALIC) to simulate those chunks which had some semantic similarity or relatedness in the other sentence, but could not have been aligned because of"
S15-2045,W10-0721,0,0.0157602,"ns student answers Q&A forum answers commited belief Table 2: English subtask: Summary of train (2012, 2013, 2014) and test (2015) datasets. lines come from a different EMM cluster. Then, we computed the string similarity between those pairs. Accordingly, we sampled 1000 headline pairs of headlines that occur in the same EMM cluster, aiming for pairs equally distributed between minimal and maximal similarity using simple string similarity as a metric. We sampled another 1000 pairs from the different EMM cluster in the same manner. The Images dataset is a subset of the PASCAL VOC-2008 dataset (Rashtchian et al., 2010), which consists of 1000 images with around 10 descriptions each, and has been used by a number of image description systems. It was also sampled using string similarity, discarding those that had been used in previous years. We organized two bins with 1000 pairs each: one with pairs of descriptions from the same image, and the other one with pairs of descriptions from different images. The source of the Answers-student pairs is the BEETLE corpus (Dzikovska et al., 2010), which is a question-answer dataset collected and annotated during the evaluation of the BEETLE II tutorial dialogue system."
S15-2045,W00-0726,0,0.313421,"Missing"
S15-2045,S12-1060,0,0.211502,"Missing"
S15-2045,W10-0707,0,\N,Missing
S15-2132,S13-2002,0,0.0321759,"Missing"
S15-2132,girardi-etal-2014-cromer,1,0.731015,"types). Some examples of target entities are Steve Jobs (PERSON), Apple Inc. (ORGANISATION), Airbus A380 (PRODUCT), and Nasdaq (FINANCIAL). The annotation procedure for the creation of gold standard timelines for the target entities required one person month. It consisted of four steps, as described below. Entity annotation. All occurrences of the target entities in the four corpora were marked following (Tonelli et al., 2014). Cross-document co-reference was annotated according to the NewsReader crossdocument annotation guidelines (Speranza and Minard, 2014). For this task, we used CROMER3 (Girardi et al., 2014), a tool designed specifically for cross-document annotation. 2 3 http://en.wikinews.org. https://hlt.fbk.eu/technologies/cromer 780 Event and time anchor annotation. Using CROMER, the corpora were annotated with events following the NewsReader cross-document annotation guidelines (Speranza and Minard, 2014). The annotation of events as defined in (Tonelli et al., 2014) was restricted by limiting the annotation to events that could be placed on a timeline. Thus, we did not annotate adjectival events, cognitive events, counter-factual events (which certainly did not happen), uncertain events (w"
S15-2132,R09-1032,0,0.0178229,"he aim of the Cross-Document Event Ordering task is to build timelines from English news articles. To provide focus to the timeline creation, the task is presented as an ordering task in which events involving a particular target entity are to be ordered chronologically. The task focuses on cross-document event coreference resolution and cross-document temporal relation extraction. Additionally, it has also been the focus of the 6th i2b2 NLP Challenge for clinical records (Sun et al., 2013). The cross-document aspect, however, has not often been explored. One example is the work described in (Ji et al., 2009) using the ACE 2005 training corpora. Here the authors link pre-defined events involving the same centroid entities (i.e. entities frequently participating in events) on a timeline. Nominal coreference resolution has been the topic of SemEval 2010 Task on Coreference Resolution in Multiple Languages (Recasens et al., 2010). TimeLine is a pilot task that goes beyond the above-mentioned evaluation exercises by addressing coreference resolution for events and temporal relation extraction at a cross document level. This task was motivated by work done in the NewsReader project1 . The goal of the N"
S15-2132,S15-2132,1,0.106103,"Missing"
S15-2132,W09-2411,0,0.11861,"Missing"
S15-2132,S13-2003,0,0.0583595,"Missing"
S15-2132,P11-2061,0,0.189729,"m the other corpora. On the other hand, on average, Stock Market timelines contain events from a higher number of different documents, i.e. 9.1, versus 6.2 for Airbus and 5.7 for GM. 5 2004 2005-06-05 2011-01 2011-08-24 2011-10-06 fighting keynote leave step_down described BEFORE SIMULTANEOUS Explicit relations Evaluation Methodology Implicit relations The evaluation methodology of this task is based on the evaluation metric used for TempEval-3 (UzZaman et al., 2013) to evaluate relations in terms of recall, precision and F1 -score. The metric captures the temporal awareness of an annotation (UzZaman and Allen, 2011). Temporal awareness is defined as the performance of an annotation as identifying and categorizing temporal relations, which implies the correct recognition and classification of the temporal entities involved in the relations. We calculate the Precision by checking the number of reduced system relations that can be verified from the reference annotation temporal closure graph, out of number of temporal relations in the reduced system relations. Similarly, we calculate the Recall by checking the number of reduced reference annotation relaFigure 2: Explicit and implicit relations resulting fro"
S15-2132,S13-2001,0,0.277531,"step further than previous evaluation challenges by requiring participant systems to perform both event coreference and temporal relation extraction across documents. Four teams submitted the output of their systems to the four proposed subtracks for a total of 13 runs, the best of which obtained an F1 -score of 7.85 in the main track (timeline creation from raw text). 1 • TempEval-1 (2007): Temporal Relation Identification (Verhagen et al., 2009) • TempEval-2 (2010): Evaluating Events, Time Expressions, and Temporal Relations (Verhagen et al., 2010) • TempEval-3 (2013): Temporal Annotation (UzZaman et al., 2013) Introduction In any domain, it is important that professionals have access to high quality knowledge for taking wellinformed decisions. As daily tasks of information professionals revolve around reconstructing a chain of previous events, an insightful way of presenting information to them is by means of timelines. The aim of the Cross-Document Event Ordering task is to build timelines from English news articles. To provide focus to the timeline creation, the task is presented as an ordering task in which events involving a particular target entity are to be ordered chronologically. The task f"
S15-2132,S10-1010,0,\N,Missing
S15-2132,S10-1001,0,\N,Missing
S16-1081,S16-1103,0,0.0432732,"n of Sultan et al. (2015)’s very successful STS model enhanced with additional features found to work well in the literature. The team in second place overall, UWB, combines a large number of diverse similarity models and features (Brychcin and Svoboda, 2016). Similar to Samsung, UWB includes both manually engineered NLP features (e.g., character n-gram overlap) with sophisticated models from deep learning (e.g., Tree LSTMs). The third place team, MayoNLPTeam, also achieves their best results using a combination of a more traditionally engineered NLP pipeline with a deep learning based model (Afzal et al., 2016). Specifically, MayoNLPTeam combines a pipeline that makes use of linguistic resources such as WordNet and well understood concepts such as the information content of a word (Resnik, 1995) with a deep learning method known as Deep Structured Semantic Model (DSSM) (Huang et al., 2013). The next two teams in overall performance, ECNU and NaCTeM, make use of large feature sets, including features based on word embeddings. However, they did not incorporate the more sophisticated deep learning based models explored by Samsung, UWB and MayoNLPTeam (Tian and Lan, 2016; Przybyła et al., 2016). The nex"
S16-1081,S12-1051,1,0.454212,"for replicating human judgements regarding the degree to which a translation generated by an machine translation system corresponds to a reference translation produced by a human translator. STS systems plausibly could be used as a drop-in replacement for existing translation evaluation metrics (e.g., BLEU, MEANT, ME498 TEOR, TER).1 The cross-lingual STS subtask that is newly introduced this year is similarly related to machine translation quality estimation. The STS shared task has been held annually since 2012, providing a venue for the evaluation of state-of-the-art algorithms and models (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015). During this time, a diverse set of genres and data sources have been explored (i.a., news headlines, video and image descriptions, glosses from lexical resources including WordNet (Miller, 1995; Christiane Fellbaum, 1998), FrameNet (Baker et al., 1998), OntoNotes (Hovy et al., 2006), web discussion forums, and Q&A data sets). This year’s 1 Both monolingual and cross-lingual STS score what is referred to in the machine translation literature as adequacy and ignore fluency unless it obscures meaning. While popular machine translat"
S16-1081,S13-1004,1,0.536348,"n judgements regarding the degree to which a translation generated by an machine translation system corresponds to a reference translation produced by a human translator. STS systems plausibly could be used as a drop-in replacement for existing translation evaluation metrics (e.g., BLEU, MEANT, ME498 TEOR, TER).1 The cross-lingual STS subtask that is newly introduced this year is similarly related to machine translation quality estimation. The STS shared task has been held annually since 2012, providing a venue for the evaluation of state-of-the-art algorithms and models (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015). During this time, a diverse set of genres and data sources have been explored (i.a., news headlines, video and image descriptions, glosses from lexical resources including WordNet (Miller, 1995; Christiane Fellbaum, 1998), FrameNet (Baker et al., 1998), OntoNotes (Hovy et al., 2006), web discussion forums, and Q&A data sets). This year’s 1 Both monolingual and cross-lingual STS score what is referred to in the machine translation literature as adequacy and ignore fluency unless it obscures meaning. While popular machine translation evaluation techni"
S16-1081,S14-2010,1,0.564132,"g the degree to which a translation generated by an machine translation system corresponds to a reference translation produced by a human translator. STS systems plausibly could be used as a drop-in replacement for existing translation evaluation metrics (e.g., BLEU, MEANT, ME498 TEOR, TER).1 The cross-lingual STS subtask that is newly introduced this year is similarly related to machine translation quality estimation. The STS shared task has been held annually since 2012, providing a venue for the evaluation of state-of-the-art algorithms and models (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015). During this time, a diverse set of genres and data sources have been explored (i.a., news headlines, video and image descriptions, glosses from lexical resources including WordNet (Miller, 1995; Christiane Fellbaum, 1998), FrameNet (Baker et al., 1998), OntoNotes (Hovy et al., 2006), web discussion forums, and Q&A data sets). This year’s 1 Both monolingual and cross-lingual STS score what is referred to in the machine translation literature as adequacy and ignore fluency unless it obscures meaning. While popular machine translation evaluation techniques do not assess fl"
S16-1081,S16-1101,1,0.859309,"Missing"
S16-1081,S16-1086,0,0.0343004,"Missing"
S16-1081,P98-1013,0,0.0704238,"E498 TEOR, TER).1 The cross-lingual STS subtask that is newly introduced this year is similarly related to machine translation quality estimation. The STS shared task has been held annually since 2012, providing a venue for the evaluation of state-of-the-art algorithms and models (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015). During this time, a diverse set of genres and data sources have been explored (i.a., news headlines, video and image descriptions, glosses from lexical resources including WordNet (Miller, 1995; Christiane Fellbaum, 1998), FrameNet (Baker et al., 1998), OntoNotes (Hovy et al., 2006), web discussion forums, and Q&A data sets). This year’s 1 Both monolingual and cross-lingual STS score what is referred to in the machine translation literature as adequacy and ignore fluency unless it obscures meaning. While popular machine translation evaluation techniques do not assess fluency independent from adequacy, it is possible that the deeper semantic assessment being performed by STS systems could benefit from being paired with a separate fluency module. evaluation adds new data sets drawn from plagiarism detection and post-edited machine translation"
S16-1081,S16-1117,0,0.0317832,"Missing"
S16-1081,S16-1089,0,0.463664,"STS. The overall winner, Samsung Poland NLP Team, proposes a textual similarity model that is a novel hybrid of recursive auto-encoders from deep learning with penalty and reward signals extracted from WordNet (Rychalska et al., 2016). To obtain even better performance, this model is combined in an ensemble with a number of other similarity models including a version of Sultan et al. (2015)’s very successful STS model enhanced with additional features found to work well in the literature. The team in second place overall, UWB, combines a large number of diverse similarity models and features (Brychcin and Svoboda, 2016). Similar to Samsung, UWB includes both manually engineered NLP features (e.g., character n-gram overlap) with sophisticated models from deep learning (e.g., Tree LSTMs). The third place team, MayoNLPTeam, also achieves their best results using a combination of a more traditionally engineered NLP pipeline with a deep learning based model (Afzal et al., 2016). Specifically, MayoNLPTeam combines a pipeline that makes use of linguistic resources such as WordNet and well understood concepts such as the information content of a word (Resnik, 1995) with a deep learning method known as Deep Structure"
S16-1081,D15-1181,0,0.0283646,"nik, 1995) with a deep learning method known as Deep Structured Semantic Model (DSSM) (Huang et al., 2013). The next two teams in overall performance, ECNU and NaCTeM, make use of large feature sets, including features based on word embeddings. However, they did not incorporate the more sophisticated deep learning based models explored by Samsung, UWB and MayoNLPTeam (Tian and Lan, 2016; Przybyła et al., 2016). The next team in the rankings, UMD-TTIC-UW, only makes use of a single deep learning model (He et al., 2016). The team extends a multi-perspective convolutional neural network (MPCNN) (He et al., 2015) with a simple word level attentional mecha17 To see how much of this is related to the Q&A domain in particular, we will investigate including difficult non-Q&A evaluation data in future STS competitions. nism based on the aggregate cosine similarity of a word in one text with all of the words in a paired text. The submission is notable for how well it performs without any manual feature engineering. Finally, the best performing system on the postediting data, RICOH’s Run-n, introduces a novel IRbased approach for textual similarity that incorporates word alignment information (Itoh, 2016). 6"
S16-1081,S16-1170,0,0.023392,"s such as WordNet and well understood concepts such as the information content of a word (Resnik, 1995) with a deep learning method known as Deep Structured Semantic Model (DSSM) (Huang et al., 2013). The next two teams in overall performance, ECNU and NaCTeM, make use of large feature sets, including features based on word embeddings. However, they did not incorporate the more sophisticated deep learning based models explored by Samsung, UWB and MayoNLPTeam (Tian and Lan, 2016; Przybyła et al., 2016). The next team in the rankings, UMD-TTIC-UW, only makes use of a single deep learning model (He et al., 2016). The team extends a multi-perspective convolutional neural network (MPCNN) (He et al., 2015) with a simple word level attentional mecha17 To see how much of this is related to the Q&A domain in particular, we will investigate including difficult non-Q&A evaluation data in future STS competitions. nism based on the aggregate cosine similarity of a word in one text with all of the words in a paired text. The submission is notable for how well it performs without any manual feature engineering. Finally, the best performing system on the postediting data, RICOH’s Run-n, introduces a novel IRbased"
S16-1081,N06-2015,0,0.0113871,"ual STS subtask that is newly introduced this year is similarly related to machine translation quality estimation. The STS shared task has been held annually since 2012, providing a venue for the evaluation of state-of-the-art algorithms and models (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015). During this time, a diverse set of genres and data sources have been explored (i.a., news headlines, video and image descriptions, glosses from lexical resources including WordNet (Miller, 1995; Christiane Fellbaum, 1998), FrameNet (Baker et al., 1998), OntoNotes (Hovy et al., 2006), web discussion forums, and Q&A data sets). This year’s 1 Both monolingual and cross-lingual STS score what is referred to in the machine translation literature as adequacy and ignore fluency unless it obscures meaning. While popular machine translation evaluation techniques do not assess fluency independent from adequacy, it is possible that the deeper semantic assessment being performed by STS systems could benefit from being paired with a separate fluency module. evaluation adds new data sets drawn from plagiarism detection and post-edited machine translations. We also introduce an evaluat"
S16-1081,S16-1106,0,0.0956972,"ween the best and median scores to highlight the extent to which top scoring systems outperformed the typical level of performance achieved on each data set. The best overall performance is obtained by Samsung Poland NLP Team’s EN1 system, which achieves an overall correlation of 0.778 (Rychalska et al., 2016). This system also performs best on three out of the five individual evaluation sets: answer-answer, headlines, plagiarism. The EN1 system achieves competitive performance on the postediting data with a correlation score of 0.83516. The best system on the postediting data, RICOH’s Run-n (Itoh, 2016), obtains a score of 0.867. Like all systems, EN1 struggles on the question-question data, achieving a correlation of 0.687. Another system submitted by the Samsung Poland NLP Team named 15 The median scores reported here do not include late or corrected systems. The median scores for the on-time systems without corrections are: ALL 0.68923; plagiarism 0.78949; answer-answer 0.48018; postediting 0.81241; headlines 0.76439; question-question 0.57140. Team Run ALL Ans.-Ans. HDL Plagiarism Postediting Ques.-Ques. Samsung Poland NLP Team UWB MayoNLPTeam Samsung Poland NLP Team NaCTeM ECNU UMD-TTIC"
S16-1081,P14-2124,0,0.0245172,"roximately 0.25 drop in correlation on the news data as compare to the multi-source setting; 2) systems performing evenly on both data sets. 6.5.1 Methods In terms of approaches, most runs rely on a monolingual framework. They automatically translate the Spanish member of a sentence pair into English and then compute monolingual semantic similarity using a system developed for English. In contrast, the CNRC team (Lo et al., 2016) provides a true crosslingual system that makes use of embedding space phrase similarity, the score from XMEANT, a crosslingual machine translation evaluation metric (Lo et al., 2014), and precision and recall features for material filling aligned cross-lingual semantic roles (e.g., action, agent, patient). The FBK HLT team (Ataman et al., 2016) proposes a model combining cross-lingual word embeddings with features from QuEst (Specia et al., 2013), a tool for machine translation quality estimation. The RTM system (Bic¸ici, 2016) also builds on methods developed for machine translation quality estimation and is applicable to both cross-lingual and monolingual similarity. The GWU NLP team (Aldarmaki and Diab, 2016) uses a shared cross-lingual vector space to directly assess"
S16-1081,S16-1102,0,0.0363344,"Missing"
S16-1081,P14-5010,0,0.0120217,"data sources we use for the evaluation sets. 3.1.1 Selection Heuristics Unless otherwise noted, pairs are heuristically selected using a combination of lexical surface form and word embedding similarity between a candidate pair of text snippets. The heuristics are used to find pairs sharing some minimal level of either surface or embedding space similarity. An approximately equal number of candidate sentence pairs are produced using our lexical surface form and word embedding selection heuristics. Both heuristics make use of a Penn Treebank style tokenization of the text provided by CoreNLP (Manning et al., 2014). 500 year 2016 2016 2016 dataset Trial News Multi-source pairs 103 301 294 source Sampled ≤ 2015 STS en-es news articles en news headlines, short-answer plag., MT postedits, Q&A forum answers, Q&A forum questions Table 3: Spanish-English subtask: Trial and test data sets. Surface Lexical Similarity Our surface form selection heuristic uses an information theoretic measure based on unigram overlap (Lin, 1998). As shown in equation (1), surface level lexical similarity between two snippets s1 and s2 is computed as a log probability weighted sum of the words common to both snippets divided by a"
S16-1081,D14-1162,0,0.109685,"Missing"
S16-1081,S16-1093,0,0.0144173,"ased model (Afzal et al., 2016). Specifically, MayoNLPTeam combines a pipeline that makes use of linguistic resources such as WordNet and well understood concepts such as the information content of a word (Resnik, 1995) with a deep learning method known as Deep Structured Semantic Model (DSSM) (Huang et al., 2013). The next two teams in overall performance, ECNU and NaCTeM, make use of large feature sets, including features based on word embeddings. However, they did not incorporate the more sophisticated deep learning based models explored by Samsung, UWB and MayoNLPTeam (Tian and Lan, 2016; Przybyła et al., 2016). The next team in the rankings, UMD-TTIC-UW, only makes use of a single deep learning model (He et al., 2016). The team extends a multi-perspective convolutional neural network (MPCNN) (He et al., 2015) with a simple word level attentional mecha17 To see how much of this is related to the Q&A domain in particular, we will investigate including difficult non-Q&A evaluation data in future STS competitions. nism based on the aggregate cosine similarity of a word in one text with all of the words in a paired text. The submission is notable for how well it performs without any manual feature engin"
S16-1081,S16-1091,0,0.0291873,"representations of the two snippets. 6.4 English Subtask The rankings for the English STS subtask are given in Tables 4 and 5. The baseline system ranked 100th. Table 6 provides the best and median scores for each of the individual evaluation sets as well as overall.15 The table also provides the difference between the best and median scores to highlight the extent to which top scoring systems outperformed the typical level of performance achieved on each data set. The best overall performance is obtained by Samsung Poland NLP Team’s EN1 system, which achieves an overall correlation of 0.778 (Rychalska et al., 2016). This system also performs best on three out of the five individual evaluation sets: answer-answer, headlines, plagiarism. The EN1 system achieves competitive performance on the postediting data with a correlation score of 0.83516. The best system on the postediting data, RICOH’s Run-n (Itoh, 2016), obtains a score of 0.867. Like all systems, EN1 struggles on the question-question data, achieving a correlation of 0.687. Another system submitted by the Samsung Poland NLP Team named 15 The median scores reported here do not include late or corrected systems. The median scores for the on-time sy"
S16-1081,P13-4014,0,0.0272301,"Missing"
S16-1081,2011.eamt-1.12,0,0.00782041,"swers. This corpus provides a collection of short answers to computer science questions that exhibit varying degrees of plagiarism from related Wikipedia articles.4 The short answers include text that was constructed by each of the following four strategies: 1) copying and pasting individual sentences from Wikipedia; 2) light revision of material copied from Wikipedia; 3) heavy revision of material from Wikipedia; 4) non-plagiarised answers produced without even looking at Wikipedia. This corpus is segmented into individual sentences using CoreNLP (Manning et al., 2014). 3.1.4 Postediting The Specia (2011) EAMT 2011 corpus provides machine translations of French news data using the Moses machine translation system (Koehn et al., 2007) paired with postedited corrections of those translations.5 The corrections were provided by human translators instructed to perform the minimum useful for finding semantically similar text snippets that differ in surface form. 4 Questions: A. What is inheritance in object orientated programming?, B. Explain the PageRank algorithm that is used by the Google search engine, C. Explain the Vector Space Model that is used for Information Retrieval., D. Explain Bayes Th"
S16-1081,S15-2027,0,0.0111195,"r to be significantly worse than the monolingual submissions even though the systems are being asked to perform the more challenging problem of evaluating crosslingual sentence pairs. While the correlations are not directly comparable, they do seem to motivate a more direct comparison between cross-lingual and monolingual STS systems. In terms of performance on the manually culled news data set, the highest overall rank is achieved by an unsupervised system submitted by team UWB (Brychcin and Svoboda, 2016). The unsupervised UWB system builds on the word alignment based STS method proposed by Sultan et al. (2015). However, when calculating the final similarity score, it weights both the aligned and unaligned words by their inverse document frequency. This system is able to attain a 0.912 correlation on the news data, while ranking second on the multi-source data set. For the multi-source test set, the highest scoring submission is a supervised system from the UWB team that combines multiple signals originating from lexical, syntactic and semantic similarity approaches in a regression-based model, achieving a 0.819 correlation. This is modestly better than the second place unsupervised approach that ac"
S16-1081,S16-1094,0,0.00995156,"th a deep learning based model (Afzal et al., 2016). Specifically, MayoNLPTeam combines a pipeline that makes use of linguistic resources such as WordNet and well understood concepts such as the information content of a word (Resnik, 1995) with a deep learning method known as Deep Structured Semantic Model (DSSM) (Huang et al., 2013). The next two teams in overall performance, ECNU and NaCTeM, make use of large feature sets, including features based on word embeddings. However, they did not incorporate the more sophisticated deep learning based models explored by Samsung, UWB and MayoNLPTeam (Tian and Lan, 2016; Przybyła et al., 2016). The next team in the rankings, UMD-TTIC-UW, only makes use of a single deep learning model (He et al., 2016). The team extends a multi-perspective convolutional neural network (MPCNN) (He et al., 2015) with a simple word level attentional mecha17 To see how much of this is related to the Q&A domain in particular, we will investigate including difficult non-Q&A evaluation data in future STS competitions. nism based on the aggregate cosine similarity of a word in one text with all of the words in a paired text. The submission is notable for how well it performs without"
S16-1081,C98-1013,0,\N,Missing
S16-1081,P07-2045,0,\N,Missing
S16-1082,agerri-etal-2014-ixa,1,0.844594,"each of the evaluation scenarios. The organizers provided a script to check if the run files are well formed. Nine teams participated on the gold chunks scenario, and out of them six teams also participated in the system chunks scenario. Regarding the datasets, all the teams gave their results for the three datasets, Baseline System The baseline system consists of a cascade concatenation of several procedures. First, input sentences are tokenized using simple regular expressions. Additionally, we collect chunks coming either from the gold standard or from the chunking done by ixapipes-chunk (Agerri et al., 2014). This is followed by a lower-cased token aligning phase, which consists of aligning (or linking) identical tokens across the input sentences. Then we use chunk boundaries as token regions to group individual tokens into groups, and compute all links across groups. The weight of the link across groups is proportional to the number of links counted between within-group tokens. The next phase consists of an optimization step in which groups x,y that have the highest link weight are identified, as well as the chunks that are linked to either x or y but not with a maximum alignment weight (thus en"
S16-1082,S15-2030,0,0.0872358,"• features, including WordNet and word embeddings. The system performs better in the system chunks scenario than in the gold chunks one. Therefore, there is no specific advantage of using chunked sentence pairs and their system is very powerful. The Answer-Students dataset has better performance than Headlines and Images. They obtain better results training a single system for the three datasets (compared to training a classifier separately for each dataset). Inspire (Kazmi and Sch¨uller, 2016): The authors propose a system based on logic programming which extends the basic ideas of NeroSim (Banjade et al., 2015). The rule based system makes use of several resources to prepare the input and uses Answer Set Programming to determine chunk boundaries. IISCNLP (Tekumalla and Sharmistha, 2016): The system uses an algorithm, iMATCH, for the alignment of multiple non-contiguous chunks based on Integer Linear Programming (ILP). Similarity type and score assignment for pairs of chunks is done using a supervised multiclass classification technique based on Random Forest Classifier. Vrep (Henry and Sands, 2016): features are extracted to create a learned rule-based classifier to assign a label. It uses semantic"
S16-1082,S16-1082,1,0.106867,"were lower for the system chunks. Both type and score are bounded by the alignment results and it is thus natural that alignment results are higher. Comparing type and score results, the type results are generally lower, possibly due to the harder task of guessing the correct label. The final results are bounded by both type and score, and the systems doing best in type are the ones doing best overall. From the results we can see that labeling the type was the most challenging. Regarding the overall test results for type and score (+TS) across datasets, UWB (Konop´ık et al., 2016) and DTSim (Banjade et al., 2016) obtained the best results for the gold chunks scenario, and DTSim and FBK-HLT-NLP (Magnolini et al., 2016) for the system chunks scenario. In addition, DTSim obtained the best overall results even though they have not good results for the Answer-Students dataset. 7 Systems, tools and resources Most of the teams reported input text processing such as lemmatization and part of speech tagging, and in some cases named-entity recognition and syntactic parsing. Additional resources such as WordNet, distributional embeddings, paraphrases from PPDB and global STS sentence scores were also used. Parti"
S16-1082,S13-2045,0,0.0371517,"ds for Headlines, and Student to Student-Answers. spend from three to five hours reading the material, building and observing circuits in the simulator and interacting with a dialogue-based tutor. They used the keyboard to interact with the system, and the computer tutor asked them questions and provided feedback via a text-based chat interface. The data from 73 undergraduate volunteer participants at south-eastern US university were recorded and annotated to form the BEETLE human-computer dialogue corpus (Dzikovska et al., 2010; Dzikovska et al., 2012), and later used in a SemEval 2015 task (Dzikovska et al., 2013). In the present corpus, we include sentence pairs composed of a student answer and the reference answer of a teacher. We have rejected those answers containing pronouns whose antecedent is not in the sentence (pronominal coreference), as the question is not included in the train data and, therefore, it is not possible to deduce which is the antecedent. There are also some dataset-specific details that are mentioned in the same section. The next pair sentences are an example of the AnswerStudents corpus. 2. Align chunks in order, from the clearest and strongest correspondences to the most uncl"
S16-1082,J07-3002,0,0.0101413,"alent as, in this dataset, X, Y, and Z always refer to switches X, Y, and Z. The same criteria is followed when annotating bulb c and C as equivalent, as A, B and C are always used to refer to bulb A, B and C. In the same way closed path and a path are equivalent, as paths are always considered to be closed. For further details related to such a corpus specific criteria refer to the annotation guidelines. 3 Evaluation Metrics The official evaluation is based on (Melamed, 1998), which uses the F1 of precision and recall of token alignments (in the context of alignment for Machine Translation). Fraser and Marcu (2007) argue that F1 is a better measure than other alternatives such as the Alignment Error Rate. The idea is that, for each pair of chunks that are aligned, we consider that any pairs of tokens in the chunks are also aligned with some weight. The weight of each token-token alignment is the inverse of the number of alignments of each token (so-called fan out factor, Melamed, 1998). Precision is measured as the ratio of token-token alignments that exist in both system and gold standard files, divided by the number of alignments in the system. Recall is measured similarly, as the ratio of token-token"
S16-1082,S16-1171,0,0.127777,"Missing"
S16-1082,S16-1124,0,0.063019,"Missing"
S16-1082,P13-2080,0,0.0414963,"Contrary to the mentioned works, we first identified the segments (chunks in our case) in each sentence separately, and then aligned them. In a different strand of work, Nielsen et al. (2009) defined a textual entailment model where the “facets” (words under some syntactic/semantic relation) in the response of a student were linked to the concepts in the reference answer. The link would signal whether each facet in the response was entailed by the reference answer or not, but would not explicitly mark which parts of the reference answer caused the entailment. This model was later followed by Levy et al. (2013). Our task was different in that we identified the corresponding chunks in both sentences. We think that, in the future, the aligned facets could provide complementary information to chunks. The SemEval Semantic Textual Similarity (STS) task in 2015 contained a subtask on Interpretable STS (Agirre et al., 2015), showing that the task is feasible, with high inter-annotator agreement and system scores well above baselines. The datasets comprised news headlines and image captions. For 2016, the pilot subtask has been updated into a standalone task. The restriction from the iSTS 2015 task to allow"
S16-1082,S16-1119,1,0.539962,"ures are extracted to create a learned rule-based classifier to assign a label. It uses semantic and syntactic (form of the chunks) relationship features. Rev (Ping Ping et al., 2016): The system consists of rules based on the analysis of the Headlines dataset considering lexical overlapping, part of speech tags and synonymy. Venseseval: This system is an adaptation of a pre-existing textual entailment system, VENSES, which first performs a semantic analysis of the text including argument structure and then looks for bridging information between chunks using several knowledge resources. iUBC (Lopez-Gazpio et al., 2016): A two layer architecture is used to produce the similarity type and score of pairs of chunks. The top layer consists of two models: a classifier and a regressor. The bottom layer consists of a recurrent neural network that processes input and feeds composed semantic feature vectors to the 519 top layer. Both layers are trained at the same time by propagating gradients. 8 Conclusions Last year, the Interpretable STS task was introduced as a pilot subtask of the STS task. At the present edition, it has been presented as an independent task that has attracted nine teams. In addition to the imag"
S16-1082,S16-1121,0,0.0814849,"s natural that alignment results are higher. Comparing type and score results, the type results are generally lower, possibly due to the harder task of guessing the correct label. The final results are bounded by both type and score, and the systems doing best in type are the ones doing best overall. From the results we can see that labeling the type was the most challenging. Regarding the overall test results for type and score (+TS) across datasets, UWB (Konop´ık et al., 2016) and DTSim (Banjade et al., 2016) obtained the best results for the gold chunks scenario, and DTSim and FBK-HLT-NLP (Magnolini et al., 2016) for the system chunks scenario. In addition, DTSim obtained the best overall results even though they have not good results for the Answer-Students dataset. 7 Systems, tools and resources Most of the teams reported input text processing such as lemmatization and part of speech tagging, and in some cases named-entity recognition and syntactic parsing. Additional resources such as WordNet, distributional embeddings, paraphrases from PPDB and global STS sentence scores were also used. Participants also revealed that most of their systems were built using some kind of distributional or knowledge-"
S16-1082,W10-0707,0,\N,Missing
S16-1082,W10-0721,0,\N,Missing
S16-1082,N12-1021,0,\N,Missing
S16-1082,S16-1122,0,\N,Missing
S16-1119,agerri-etal-2014-ixa,0,0.101335,"ANN producing as output a d-dimensional vector for each input segment. Features computed out of these vectors are then fed to both a regressor and a classifier that produce the similarity score and the relation label. In the backward propagation (stripped arrow) weights are adjusted in the recurrent ANN combining the gradients that propagate from the above models. with gold standard segment marks over raw sentence pairs. The current component is only used in the syschunks scenario. To identify and segment raw input sentences we use python’s NLTK library (Bird, 2006) and the ixapipes-chunker (Agerri et al., 2014). Once the chunks are marked we use regular expressions to tune them according to the task’s chunk definition. We developed four rules to optimize how conjunctions, punctuations and prepositions are handled. These rules aim to merge consequent chunks to form new chunks1 . The output of the component are the same sentence pairs as the ones provided as input, but incorporating chunk marks to denote the start and end of segments. 1 We found significant improvement if prepositional phrases followed by a nominal phrase are unified as a single chunk. The other three rules unify nominal phrases separ"
S16-1119,S16-1082,1,0.601336,"Missing"
S16-1119,P06-4018,0,0.0851298,"segments are processed by a recurrent ANN producing as output a d-dimensional vector for each input segment. Features computed out of these vectors are then fed to both a regressor and a classifier that produce the similarity score and the relation label. In the backward propagation (stripped arrow) weights are adjusted in the recurrent ANN combining the gradients that propagate from the above models. with gold standard segment marks over raw sentence pairs. The current component is only used in the syschunks scenario. To identify and segment raw input sentences we use python’s NLTK library (Bird, 2006) and the ixapipes-chunker (Agerri et al., 2014). Once the chunks are marked we use regular expressions to tune them according to the task’s chunk definition. We developed four rules to optimize how conjunctions, punctuations and prepositions are handled. These rules aim to merge consequent chunks to form new chunks1 . The output of the component are the same sentence pairs as the ones provided as input, but incorporating chunk marks to denote the start and end of segments. 1 We found significant improvement if prepositional phrases followed by a nominal phrase are unified as a single chunk. Th"
S16-1119,D14-1162,0,0.0898275,"S2 ~ d |) and angle Element wise distance (|S1 ~ d ∗S2 ~ d ) are computed as features, as proposed in (S1 Tai et al. (2015). The distance and angle concatenation yields a 2 * d-dimensional vector. This resulting vector is used as input in top layer models. As regards the top layer models, feedforward neural networks are used for both. All the parameters of the models are summarized in Table 1. The scientific computing framework Torch has been used to build the whole component (Collobert et al., 2011). Note that this component doesn’t use any type of lexicalized or domain specific feature but Pennington et al. (2014) word embeddings. 3 Development Initial experiments (section 3.2) have been carried out using the official train and test splits from iSTS 2015 (Agirrea et al., 2015). The 2016 interpretable STS task released three train datasets: Images, Headlines and Answer-Students. These datasets have been used to train the models using 10-fold cross-validation. In Section 3.1 we describe in detail the set up of 773 Bottom layer ANN: RNN or LSTM Input Glove word embeddings Output Sentence representation Input-dim 300 Memory-dim 150 Output-dim 150 Non linearity Sigmoid function Learning rate 0.05 Regulariza"
S16-1119,Q14-1018,0,0.032478,"ment The alignment component focuses on making optimal segment connections for each sentence pair. The algorithm is as follows. To begin with, the module constructs a tokentoken matrix in which each element (i,j) determines that there exists a connection between token i and token j2 . The token-token matrix is populated using the weighted sum of the following metrics: lowercased token overlap, stemmed or lemmatized token overlap, cosine similarity between Mikolov’s pretrained word vectors (Mikolov et al., 2013) and the alignment prediction provided by the monolingual word aligner described in Sultan et al. (2014). Once the token-token matrix is built, the alignment component makes use of segment regions to group individual tokens. The strength of each segment connection is proportional to the weights of the interconnected tokens. By carrying out this operation over all segments in the pair the module obtains the chunk-chunk matrix3 . Once the chunkchunk matrix has been computed, the last step is to use the Hungarian-Munkres algorithm (Clapper, 2009) to discover the segments (x,y) that maximize the connection weights. The alignment is done as follows: the segments that maximize the alignment strength a"
S16-1119,P15-1150,0,0.214485,"ecurrent ANN. While the models on the top layer are trained to produce scores and labels, the underlying recurrent net tries to capture the semantic representation of input segments and feed it upwards. Both models on the top layer are trained in a supervised manner at the same time, and the delta error messages computed on them are used to train the net of the bottom layer. That is, the gradient propagating from both models on the top layer is used to train the weights of the ANN (Figure 1). A similar architecture with one top layer propagating gradients to an ANN has previously been used in Tai et al. (2015), which we use as motivation for our work. The whole model works as follows: the ANN from the bottom layer processes segment words one at a time until there are no more words left. At each time step the net updates its internal memory state so that it keeps on capturing the semantic representation of the segment. Once the two segments have been processed the net outputs both segment representation d-dimensional vectors. These vectors are used to compute features for the models in the top layer. ~ d − S2 ~ d |) and angle Element wise distance (|S1 ~ d ∗S2 ~ d ) are computed as features, as prop"
S17-2001,S17-2013,0,0.019824,"Missing"
S17-2001,S17-2031,0,0.0137096,"Missing"
S17-2001,P98-1013,0,0.169413,"Missing"
S17-2001,S15-2045,1,0.888131,"elated but not particularly similar). To encourage and support research in this area, the STS shared task has been held annually since 2012, providing a venue for evaluation of state-ofthe-art algorithms and models (Agirre et al., 2012, 2013, 2014, 2015, 2016). During this time, diverse similarity methods and data sets1 have been explored. Early methods focused on lexical semantics, surface form matching and basic syntacˇ c et al., 2012a; tic similarity (B¨ar et al., 2012; Sari´ Jimenez et al., 2012a). During subsequent evaluations, strong new similarity signals emerged, such as Sultan et al. (2015)’s alignment based method. More recently, deep learning became competitive with top performing feature engineered systems (He et al., 2016). The best performance tends to be obtained by ensembling feature engineered and deep learning models (Rychalska et al., 2016). Significant research effort has focused on STS over English sentence pairs.2 English STS is a Semantic Textual Similarity (STS) measures the meaning similarity of sentences. Applications include machine translation (MT), summarization, generation, question answering (QA), short answer grading, semantic search, dialog and conversati"
S17-2001,L16-1662,0,0.0083775,"ubmission uses sentence IC exclusively. Another ensembles IC with Sultan et al. (2015)’s alignment method, while a third ensembles IC with cosine similarity of summed word embeddings with an IDF weighting scheme. Sentence IC in isolation outperforms all systems except those from ECNU. Combining sentence IC with word embedding similarity performs best. CompiLIG (Ferrero et al., 2017) The best Spanish-English performance on SNLI sentences was achieved by CompiLIG using features including: cross-lingual conceptual similarity using DBNary (Serasset, 2015), cross-language MultiVec word embeddings (Berard et al., 2016), and Brychcin and Svoboda (2016)’s improvements to Sultan et al. (2015)’s method. LIM-LIG (Nagoudi et al., 2017) Using only weighted word embeddings, LIM-LIG took second place on Arabic.17 Arabic word embeddings are summed into sentence embeddings using uniform, POS and IDF weighting schemes. Sentence similarity is computed by cosine similarity. POS and IDF outperform uniform weighting. Combining the IDF and POS weights by multiplication is reported by LIM-LIG to achieve r 0.7667, higher than all submitted Arabic (track 1) systems. HCTI (Shao, 2017) Third place overall is obtained by HCTI wit"
S17-2001,S17-2030,0,0.0302147,"Missing"
S17-2001,S17-2021,0,0.0336317,"Missing"
S17-2001,S16-1081,1,0.903281,"n an English sentence and its Arabic machine translation5 where they perform post-editing to correct errors. Spanish translation is completed by a University of Sheffield graduate student who is a native Spanish speaker and fluent in English. Turkish translations are obtained from SDL.6 3.4 Crowdsourced Annotations Crowdsourced annotation is performed on Amazon Mechanical Turk.8 Annotators examine the STS pairings of English SNLI sentences. STS labels are then transferred to the translated pairs for crosslingual and non-English tracks. The annotation instructions and template are identical to Agirre et al. (2016). Labels are collected in batches of 20 pairs with annotators paid $1 USD per batch. Five annotations are collected per pair. The MTurk master9 qualification is required to perform the task. Gold scores average the five individual annotations. This section describes the preparation of the evaluation data. For SNLI data, this includes the selection of sentence pairs, annotation of pairs with STS labels and the translation of the original English sentences. WMT quality estimation data is directly annotated with STS labels. 3.3 Annotation 4.1 Table 2 summarizes the evaluation data by track. The s"
S17-2001,W14-3302,1,0.742822,"Missing"
S17-2001,S12-1051,1,0.784858,"nd paraphrase detection in that it captures gradations of meaning overlap rather than making binary classifications of particular relationships. While semantic relatedness expresses a graded semantic relationship as well, it is non-specific about the nature of the relationship with contradictory material still being a candidate for a high score (e.g., “night” and “day” are highly related but not particularly similar). To encourage and support research in this area, the STS shared task has been held annually since 2012, providing a venue for evaluation of state-ofthe-art algorithms and models (Agirre et al., 2012, 2013, 2014, 2015, 2016). During this time, diverse similarity methods and data sets1 have been explored. Early methods focused on lexical semantics, surface form matching and basic syntacˇ c et al., 2012a; tic similarity (B¨ar et al., 2012; Sari´ Jimenez et al., 2012a). During subsequent evaluations, strong new similarity signals emerged, such as Sultan et al. (2015)’s alignment based method. More recently, deep learning became competitive with top performing feature engineered systems (He et al., 2016). The best performance tends to be obtained by ensembling feature engineered and deep lear"
S17-2001,S17-2012,0,0.0153726,"Missing"
S17-2001,S17-2032,0,0.0142311,"Missing"
S17-2001,D15-1075,0,0.136451,"s/missing. John said he is considered a witness but not a suspect. “He is not a suspect anymore.” John said. The two sentences are not equivalent, but share some details. They flew out of the nest in groups. They flew into the nest together. The two sentences are not equivalent, but are on the same topic. The woman is playing the violin. The young lady enjoys listening to the guitar. The two sentences are completely dissimilar. The black dog is running through the snow. A race car driver is driving his car through the mud. Evaluation Data The Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015) is the primary evaluation data source with the exception that one of the 3 Previous years of the STS shared task include more data sources. This year the task draws from two data sources and includes a diverse set of languages and language-pairs. 4 HTER is the minimal number of edits required for correction of a translation divided by its length after correction. pilot track on cross-lingual Spanish-English STS. The English tracks attracted the most participation and have the largest use of the evaluation data in ongoing research. 2 Track 1 2 3 4a 4b 5 6 Language(s) Arabic (ar-ar) Arabic-Engl"
S17-2001,S16-1089,0,0.0248981,"Missing"
S17-2001,S17-2015,0,0.0563979,"fication of document relatedness and document relatedness within a corpus. 15 Within the highlighted submissions, the following use a monolingual English system fed by MT: ECNU, BIT, HCTI 11 https://competitions.codalab.org/ competitions/16051 12 Words obtained using Arabic (ar), Spanish (es) and English (en) Treebank tokenizers. 13 http://translate.google.com 5 Team ECNU (Tian et al., 2017) ECNU (Tian et al., 2017) ECNU (Tian et al., 2017) BIT (Wu et al., 2017)* BIT (Wu et al., 2017)* BIT (Wu et al., 2017) HCTI (Shao, 2017) MITRE (Henderson et al., 2017) MITRE (Henderson et al., 2017) FCICU (Hassan et al., 2017) neobility (Zhuang and Chang, 2017) FCICU (Hassan et al., 2017) STS-UHH (Kohail et al., 2017) RTV HCTI (Shao, 2017) RTV MatrusriIndia STS-UHH (Kohail et al., 2017) SEF@UHH (Duma and Menzel, 2017) SEF@UHH (Duma and Menzel, 2017) RTV SEF@UHH (Duma and Menzel, 2017) neobility (Zhuang and Chang, 2017) neobility (Zhuang and Chang, 2017) MatrusriIndia NLPProxem UMDeep (Barrow and Peskov, 2017) NLPProxem UMDeep (Barrow and Peskov, 2017) Lump (Espa˜na Bonet and Barr´on-Cede˜no, 2017)* Lump (Espa˜na Bonet and Barr´on-Cede˜no, 2017)* Lump (Espa˜na Bonet and Barr´on-Cede˜no, 2017)* NLPProxem RTM (Bic¸ici"
S17-2001,D15-1181,0,0.0245131,"Missing"
S17-2001,N16-1108,0,0.0260604,"Missing"
S17-2001,S16-1170,0,0.0147665,"Missing"
S17-2001,D17-1070,0,0.281621,"Missing"
S17-2001,N16-1162,0,0.0181244,"Missing"
S17-2001,C04-1051,0,0.847641,"Missing"
S17-2001,N06-2015,0,0.0472258,"Missing"
S17-2001,S17-2024,0,0.0294688,"Missing"
S17-2001,S17-2019,0,0.0294083,"Missing"
S17-2001,P15-1162,0,0.0488426,"Missing"
S17-2001,S12-1061,0,0.675418,"ationship with contradictory material still being a candidate for a high score (e.g., “night” and “day” are highly related but not particularly similar). To encourage and support research in this area, the STS shared task has been held annually since 2012, providing a venue for evaluation of state-ofthe-art algorithms and models (Agirre et al., 2012, 2013, 2014, 2015, 2016). During this time, diverse similarity methods and data sets1 have been explored. Early methods focused on lexical semantics, surface form matching and basic syntacˇ c et al., 2012a; tic similarity (B¨ar et al., 2012; Sari´ Jimenez et al., 2012a). During subsequent evaluations, strong new similarity signals emerged, such as Sultan et al. (2015)’s alignment based method. More recently, deep learning became competitive with top performing feature engineered systems (He et al., 2016). The best performance tends to be obtained by ensembling feature engineered and deep learning models (Rychalska et al., 2016). Significant research effort has focused on STS over English sentence pairs.2 English STS is a Semantic Textual Similarity (STS) measures the meaning similarity of sentences. Applications include machine translation (MT), summarizat"
S17-2001,marelli-etal-2014-sick,0,0.0431263,"from English STS shared tasks (2012-2017). 2 5 4 3 2 1 0 Table 1: Similarity scores with explanations and English examples from Agirre et al. (2013). cross-lingual tracks explores data from the WMT 2014 quality estimation task (Bojar et al., 2014).3 Sentences pairs in SNLI derive from Flickr30k image captions (Young et al., 2014) and are labeled with the entailment relations: entailment, neutral, and contradiction. Drawing from SNLI allows STS models to be evaluated on the type of data used to assess textual entailment methods. However, since entailment strongly cues for semantic relatedness (Marelli et al., 2014), we construct our own sentence pairings to deter gold entailment labels from informing evaluation set STS scores. Track 4b investigates the relationship between STS and MT quality estimation by providing STS labels for WMT quality estimation data. The data includes Spanish translations of English sentences from a variety of methods including RBMT, SMT, hybrid-MT and human translation. Translations are annotated with the time required for human correction by post-editing and Human-targeted Translation Error Rate (HTER) (Snover et al., 2006).4 Participants are not allowed to use the gold qualit"
S17-2001,P16-1089,0,0.0231566,"Missing"
S17-2001,S17-2025,0,0.0295389,"Missing"
S17-2001,H92-1116,0,0.16712,"Missing"
S17-2001,W16-1609,0,0.0245746,"Arabic, Spanish and Turkish. The primary evaluation criteria combines performance on all of the different language conditions except English-Turkish, which was run as a surprise language track. Even with this departure from prior years, the task attracted 31 teams producing 84 submissions. STS shared task data sets have been used extensively for research on sentence level similarity and semantic representations (i.a., Arora et al. (2017); Conneau et al. (2017); Mu et al. (2017); Pagliardini et al. (2017); Wieting and Gimpel (2017); He and Lin (2016); Hill et al. (2016); Kenter et al. (2016); Lau and Baldwin (2016); Wieting et al. (2016b,a); He et al. (2015); Pham et al. (2015)). To encourage the use of a common evaluation set for assessing new methods, we present the STS Benchmark, a publicly available selection of data from English STS shared tasks (2012-2017). 2 5 4 3 2 1 0 Table 1: Similarity scores with explanations and English examples from Agirre et al. (2013). cross-lingual tracks explores data from the WMT 2014 quality estimation task (Bojar et al., 2014).3 Sentences pairs in SNLI derive from Flickr30k image captions (Young et al., 2014) and are labeled with the entailment relations: entailment"
S17-2001,P17-2099,0,0.0191097,"Missing"
S17-2001,S17-2029,0,0.0284965,"Missing"
S17-2001,S17-2017,0,0.0940842,"jerva and Ostling, 2017) ¨ ResSim (Bjerva and Ostling, 2017) LIPN-IIMAS (Arroyo-Fern´andez and Meza Ruiz, 2017) LIPN-IIMAS (Arroyo-Fern´andez and Meza Ruiz, 2017) hjpwhu hjpwhu compiLIG (Ferrero et al., 2017) compiLIG (Ferrero et al., 2017) compiLIG (Ferrero et al., 2017) DT TEAM (Maharjan et al., 2017) DT TEAM (Maharjan et al., 2017) DT TEAM (Maharjan et al., 2017) FCICU (Hassan et al., 2017) ITNLPAiKF (Liu et al., 2017) ITNLPAiKF (Liu et al., 2017) ITNLPAiKF (Liu et al., 2017) L2F/INESC-ID (Fialho et al., 2017)* L2F/INESC-ID (Fialho et al., 2017) L2F/INESC-ID (Fialho et al., 2017)* LIM-LIG (Nagoudi et al., 2017) LIM-LIG (Nagoudi et al., 2017) LIM-LIG (Nagoudi et al., 2017) MatrusriIndia NRC* NRC OkadaNaoya ´ OPI-JSA (Spiewak et al., 2017) ´ OPI-JSA (Spiewak et al., 2017) ´ OPI-JSA (Spiewak et al., 2017) PurdueNLP (Lee et al., 2017) PurdueNLP (Lee et al., 2017) PurdueNLP (Lee et al., 2017) QLUT (Meng et al., 2017)* QLUT (Meng et al., 2017) QLUT (Meng et al., 2017)* SIGMA SIGMA SIGMA SIGMA PKU 2 SIGMA PKU 2 SIGMA PKU 2 STS-UHH (Kohail et al., 2017) UCSC-NLP UdL (Al-Natsheh et al., 2017) UdL (Al-Natsheh et al., 2017)* UdL (Al-Natsheh et al., 2017) Primary 73.16 70.44 69.40 67.89 67.03 66.62 65.98 65.90"
S17-2001,P10-1023,0,0.039078,"2017) Fourth place overall is MITRE that, like ECNU, takes an ambitious feature engineering approach complemented by deep learning. Ensembled components inˇ c clude: alignment similarity; TakeLab STS (Sari´ et al., 2012b); string similarity measures such as matching n-grams, summarization and MT metrics (BLEU, WER, PER, ROUGE); a RNN and recurrent convolutional neural networks (RCNN) over word alignments; and a BiLSTM that is state-ofthe-art for textual entailment (Chen et al., 2016). FCICU (Hassan et al., 2017) Fifth place overall is FCICU that computes a sense-base alignment using BabelNet (Navigli and Ponzetto, 2010). BabelNet synsets are multilingual allowing non-English and cross-lingual pairs to be processed similarly to English pairs. Alignment similarity scores are used with two runs: one that combines the scores within a string kernel and another that uses them with a weighted variant of Sultan et al. (2015)’s method. Both runs average the Babelnet based scores with soft-cardinality (Jimenez et al., 2012b). BIT (Wu et al., 2017) Second place overall is achieved by BIT primarily using sentence information content (IC) informed by WordNet and BNC word frequencies. One submission uses sentence IC exclu"
S17-2001,S17-2022,0,0.0368865,"Missing"
S17-2001,N18-1049,0,0.148448,"Missing"
S17-2001,S17-2014,0,0.250083,"ow and Peskov, 2017) Lump (Espa˜na Bonet and Barr´on-Cede˜no, 2017)* Lump (Espa˜na Bonet and Barr´on-Cede˜no, 2017)* Lump (Espa˜na Bonet and Barr´on-Cede˜no, 2017)* NLPProxem RTM (Bic¸ici, 2017)* UMDeep (Barrow and Peskov, 2017) RTM (Bic¸ici, 2017)* RTM (Bic¸ici, 2017)* ¨ ResSim (Bjerva and Ostling, 2017) ¨ ResSim (Bjerva and Ostling, 2017) ¨ ResSim (Bjerva and Ostling, 2017) LIPN-IIMAS (Arroyo-Fern´andez and Meza Ruiz, 2017) LIPN-IIMAS (Arroyo-Fern´andez and Meza Ruiz, 2017) hjpwhu hjpwhu compiLIG (Ferrero et al., 2017) compiLIG (Ferrero et al., 2017) compiLIG (Ferrero et al., 2017) DT TEAM (Maharjan et al., 2017) DT TEAM (Maharjan et al., 2017) DT TEAM (Maharjan et al., 2017) FCICU (Hassan et al., 2017) ITNLPAiKF (Liu et al., 2017) ITNLPAiKF (Liu et al., 2017) ITNLPAiKF (Liu et al., 2017) L2F/INESC-ID (Fialho et al., 2017)* L2F/INESC-ID (Fialho et al., 2017) L2F/INESC-ID (Fialho et al., 2017)* LIM-LIG (Nagoudi et al., 2017) LIM-LIG (Nagoudi et al., 2017) LIM-LIG (Nagoudi et al., 2017) MatrusriIndia NRC* NRC OkadaNaoya ´ OPI-JSA (Spiewak et al., 2017) ´ OPI-JSA (Spiewak et al., 2017) ´ OPI-JSA (Spiewak et al., 2017) PurdueNLP (Lee et al., 2017) PurdueNLP (Lee et al., 2017) PurdueNLP (Lee et al., 2017)"
S17-2001,P14-5010,0,0.0119011,"Missing"
S17-2001,D14-1162,0,0.109043,"Missing"
S17-2001,P15-1094,0,0.0153938,"Missing"
S17-2001,S12-1060,0,0.0229108,"Missing"
S17-2001,C16-1009,0,0.0254292,"lect methods are highlighted below. As directed by the SemEval workshop organizers, the CodaLab research platform hosts the task.11 6.4 Rankings Baseline The baseline is the cosine of binary sentence vectors with each dimension representing whether an individual word appears in a sentence.12 For crosslingual pairs, non-English sentences are translated into English using state-of-the-art machine translation.13 The baseline achieves an average correlation of 53.7 with human judgment on tracks 1-5 and would rank 23rd overall out the 44 system submissions that participated in all tracks. 14 e.g., Reimers et al. (2016) report success using STS labels with alternative metrics such as normalized Cumulative Gain (nCG), normalized Discounted Cumulative Gain (nDCG) and F1 to more accurately predict performance on the downstream tasks: text reuse detection, binary classification of document relatedness and document relatedness within a corpus. 15 Within the highlighted submissions, the following use a monolingual English system fed by MT: ECNU, BIT, HCTI 11 https://competitions.codalab.org/ competitions/16051 12 Words obtained using Arabic (ar), Spanish (es) and English (en) Treebank tokenizers. 13 http://transla"
S17-2001,S16-1091,0,0.00933941,"2015, 2016). During this time, diverse similarity methods and data sets1 have been explored. Early methods focused on lexical semantics, surface form matching and basic syntacˇ c et al., 2012a; tic similarity (B¨ar et al., 2012; Sari´ Jimenez et al., 2012a). During subsequent evaluations, strong new similarity signals emerged, such as Sultan et al. (2015)’s alignment based method. More recently, deep learning became competitive with top performing feature engineered systems (He et al., 2016). The best performance tends to be obtained by ensembling feature engineered and deep learning models (Rychalska et al., 2016). Significant research effort has focused on STS over English sentence pairs.2 English STS is a Semantic Textual Similarity (STS) measures the meaning similarity of sentences. Applications include machine translation (MT), summarization, generation, question answering (QA), short answer grading, semantic search, dialog and conversational systems. The STS shared task is a venue for assessing the current state-of-the-art. The 2017 task focuses on multilingual and cross-lingual pairs with one sub-track exploring MT quality estimation (MTQE) data. The task obtained strong participation from 31 tea"
S17-2001,Q15-1025,0,0.02311,"Missing"
S17-2001,D16-1157,0,0.0328236,"Missing"
S17-2001,P16-2068,0,0.0331584,"Missing"
S17-2001,P17-1190,0,0.0172847,"Missing"
S17-2001,S17-2007,0,0.0512581,"ormalized Discounted Cumulative Gain (nDCG) and F1 to more accurately predict performance on the downstream tasks: text reuse detection, binary classification of document relatedness and document relatedness within a corpus. 15 Within the highlighted submissions, the following use a monolingual English system fed by MT: ECNU, BIT, HCTI 11 https://competitions.codalab.org/ competitions/16051 12 Words obtained using Arabic (ar), Spanish (es) and English (en) Treebank tokenizers. 13 http://translate.google.com 5 Team ECNU (Tian et al., 2017) ECNU (Tian et al., 2017) ECNU (Tian et al., 2017) BIT (Wu et al., 2017)* BIT (Wu et al., 2017)* BIT (Wu et al., 2017) HCTI (Shao, 2017) MITRE (Henderson et al., 2017) MITRE (Henderson et al., 2017) FCICU (Hassan et al., 2017) neobility (Zhuang and Chang, 2017) FCICU (Hassan et al., 2017) STS-UHH (Kohail et al., 2017) RTV HCTI (Shao, 2017) RTV MatrusriIndia STS-UHH (Kohail et al., 2017) SEF@UHH (Duma and Menzel, 2017) SEF@UHH (Duma and Menzel, 2017) RTV SEF@UHH (Duma and Menzel, 2017) neobility (Zhuang and Chang, 2017) neobility (Zhuang and Chang, 2017) MatrusriIndia NLPProxem UMDeep (Barrow and Peskov, 2017) NLPProxem UMDeep (Barrow and Peskov, 2017) Lump (Espa˜n"
S17-2001,S17-2016,0,0.204045,"Missing"
S17-2001,S15-2001,0,0.0578438,"elated but not particularly similar). To encourage and support research in this area, the STS shared task has been held annually since 2012, providing a venue for evaluation of state-ofthe-art algorithms and models (Agirre et al., 2012, 2013, 2014, 2015, 2016). During this time, diverse similarity methods and data sets1 have been explored. Early methods focused on lexical semantics, surface form matching and basic syntacˇ c et al., 2012a; tic similarity (B¨ar et al., 2012; Sari´ Jimenez et al., 2012a). During subsequent evaluations, strong new similarity signals emerged, such as Sultan et al. (2015)’s alignment based method. More recently, deep learning became competitive with top performing feature engineered systems (He et al., 2016). The best performance tends to be obtained by ensembling feature engineered and deep learning models (Rychalska et al., 2016). Significant research effort has focused on STS over English sentence pairs.2 English STS is a Semantic Textual Similarity (STS) measures the meaning similarity of sentences. Applications include machine translation (MT), summarization, generation, question answering (QA), short answer grading, semantic search, dialog and conversati"
S17-2001,Q14-1006,0,0.0320307,"Lin (2016); Hill et al. (2016); Kenter et al. (2016); Lau and Baldwin (2016); Wieting et al. (2016b,a); He et al. (2015); Pham et al. (2015)). To encourage the use of a common evaluation set for assessing new methods, we present the STS Benchmark, a publicly available selection of data from English STS shared tasks (2012-2017). 2 5 4 3 2 1 0 Table 1: Similarity scores with explanations and English examples from Agirre et al. (2013). cross-lingual tracks explores data from the WMT 2014 quality estimation task (Bojar et al., 2014).3 Sentences pairs in SNLI derive from Flickr30k image captions (Young et al., 2014) and are labeled with the entailment relations: entailment, neutral, and contradiction. Drawing from SNLI allows STS models to be evaluated on the type of data used to assess textual entailment methods. However, since entailment strongly cues for semantic relatedness (Marelli et al., 2014), we construct our own sentence pairings to deter gold entailment labels from informing evaluation set STS scores. Track 4b investigates the relationship between STS and MT quality estimation by providing STS labels for WMT quality estimation data. The data includes Spanish translations of English sentences f"
S17-2001,2006.amta-papers.25,0,0.0602904,"e entailment strongly cues for semantic relatedness (Marelli et al., 2014), we construct our own sentence pairings to deter gold entailment labels from informing evaluation set STS scores. Track 4b investigates the relationship between STS and MT quality estimation by providing STS labels for WMT quality estimation data. The data includes Spanish translations of English sentences from a variety of methods including RBMT, SMT, hybrid-MT and human translation. Translations are annotated with the time required for human correction by post-editing and Human-targeted Translation Error Rate (HTER) (Snover et al., 2006).4 Participants are not allowed to use the gold quality estimation annotations to inform STS scores. Task Overview STS is the assessment of pairs of sentences according to their degree of semantic similarity. The task involves producing real-valued similarity scores for sentence pairs. Performance is measured by the Pearson correlation of machine scores with human judgments. The ordinal scale in Table 1 guides human annotation, ranging from 0 for no meaning overlap to 5 for meaning equivalence. Intermediate values reflect interpretable levels of partial overlap in meaning. The annotation scale"
S17-2001,S17-2023,0,0.0172414,"Missing"
S17-2001,S17-2018,0,0.0148193,"Missing"
S17-2001,S15-2027,0,0.00981403,"Missing"
S17-2001,S17-2028,0,0.624183,"abels with alternative metrics such as normalized Cumulative Gain (nCG), normalized Discounted Cumulative Gain (nDCG) and F1 to more accurately predict performance on the downstream tasks: text reuse detection, binary classification of document relatedness and document relatedness within a corpus. 15 Within the highlighted submissions, the following use a monolingual English system fed by MT: ECNU, BIT, HCTI 11 https://competitions.codalab.org/ competitions/16051 12 Words obtained using Arabic (ar), Spanish (es) and English (en) Treebank tokenizers. 13 http://translate.google.com 5 Team ECNU (Tian et al., 2017) ECNU (Tian et al., 2017) ECNU (Tian et al., 2017) BIT (Wu et al., 2017)* BIT (Wu et al., 2017)* BIT (Wu et al., 2017) HCTI (Shao, 2017) MITRE (Henderson et al., 2017) MITRE (Henderson et al., 2017) FCICU (Hassan et al., 2017) neobility (Zhuang and Chang, 2017) FCICU (Hassan et al., 2017) STS-UHH (Kohail et al., 2017) RTV HCTI (Shao, 2017) RTV MatrusriIndia STS-UHH (Kohail et al., 2017) SEF@UHH (Duma and Menzel, 2017) SEF@UHH (Duma and Menzel, 2017) RTV SEF@UHH (Duma and Menzel, 2017) neobility (Zhuang and Chang, 2017) neobility (Zhuang and Chang, 2017) MatrusriIndia NLPProxem UMDeep (Barrow a"
S17-2001,S14-2010,1,\N,Missing
S17-2001,S17-2027,0,\N,Missing
S17-2001,W17-4759,0,\N,Missing
U06-1008,N06-2036,0,0.0161641,"l method is explained in Section 4. Section 5 presents our experimental setting, and in Section 6 we report the performance of our technique and the improvement over the monosemous relatives method. Section 7 is devoted to compare our system to other unsupervised techniques and analyse the prospects for system combination. Finally, we conclude and discuss future work in Section 8. 2 Related Work The construction of unsupervised WSD systems applicable to all words in context has been the goal of many research initiatives, as can be seen in special journals and devoted books - see for instance (Agirre and Edmonds, 2006) for a recent book. We will now describe different trends that are being explored. Some recent techniques seek to alleviate the knowledge acquisition bottleneck by combining training data from different words. Kohomban and Lee (2005) build semantic classifiers by merging data from words in the same semantic class. Once the class is selected, simple heuristics are applied to obtain the fine-grained sense. The classifier fol3 Monosemous Relatives method The “monosemous relatives” approach is a technique to acquire training examples automatically and then feed them to a Machine Learning (ML) meth"
U06-1008,W04-0811,0,0.104501,"Missing"
U06-1008,mihalcea-2002-bootstrapping,0,0.505407,"(Fellbaum, 1998). A well known approach for unsupervised WSD consists of the automatic acquisition of training data by means of monosemous relatives (Leacock et al., 1998). This technique roughly follows these steps: (i) select a set of monosemous words that are related to the different senses of the target word, (ii) query the Internet to obtain examples for each relative, (iii) create a collection of training examples for each sense, and (iv) use an ML algorithm trained on the acquired collections to tag the test instances. This method has been used to bootstrap large sense-tagged corpora (Mihalcea, 2002; Agirre and Martinez, 2004). Two important shortcomings of this method are the lack of monosemous relatives for some senses of the target words, and the noise introduced by some distant relatives. In this paper we directly address those problems by developing a new method that makes use of polysemous relatives and relies on the context of the target word to reduce the presence of noisy examples. The remaining of the paper is organised as follows. In Section 2 we describe related work in this area. Section 3 briefly introduces the monosemous relatives algorithm, and our novel method is explain"
U06-1008,J96-2004,0,0.0250267,"Missing"
U06-1008,W05-0605,0,0.0646559,"Missing"
U06-1008,W06-1670,0,0.0352819,"Missing"
U06-1008,W04-0850,0,0.0528597,"Missing"
U06-1008,P05-3009,0,0.0133486,"of the different features. The church was rebuilt in the 13th century and further modifications and restoration were carried out in the 15th century. 44 We can extract different features from this context, for instance using a dependency parser. We can obtain that there is a object-verb relation between church and rebuild. Then we can incorporate this knowledge to the relative-based query and obtain training examples that are closer to our target sentence. In order to implement this approach with rich features we require tools that allow for linguistic queries, such as the linguist’s engine (Resnik and Elkiss, 2005), but other approach would be to use simple features, such as strings of words, in order to benefit directly from the examples coming from search engines in the Internet. In this paper we decided to explore the latter technique to observe the performance we can achieve with simple features. Thus, in the example above, we query the Internet with snippets such as “The cathedral was rebuilt” to retrieve training examples. We will go back to the example at the end of this section. With this method we can obtain a separate training set starting from each test instance and the pool of relatives for"
U06-1008,H05-1069,1,0.893661,"Missing"
U06-1008,S01-1004,0,0.0151229,"this paper we devised a simple algorithm to rank queries according to the three factors, but we plan to apply other techniques in the acquired training data in the future. Thus, we build a disambiguation algorithm that can be explained in the following four steps: 1. Obtain pool of relatives: for each sense of the target word we gather its synonyms, hyponyms, and hypernyms. We also take polysemous nouns, as we expect that in similar local contexts the relative will keep its related meaning. 5 Experimental setting For our experiments we relied on the lexicalsample datasets of both Senseval-2 (Kilgarriff, 2001) and Senseval-3 (Mihalcea et al., 2004). We 45 Query The nave was rebuilt in the 13th century The abbey was rebuilt in the 13th century The cathedral was rebuilt in the 13th century The Catholic Church was rebuilt in The Christian church was rebuilt The church service was The religious service was Sense 2 2 2 1 1 3 3 S2LS Word MR art 61.1 authority 22.0 bar 52.1 bum 18.8 chair 62.9 channel 28.7 child 1.6 church 62.1 circuit 52.8 day 2.2 detention 16.7 dyke 89.3 facility 26.8 fatigue 73.8 feeling 51.0 grip 8.0 hearth 37.5 holiday 7.4 lady 79.3 material 50.8 mouth 41.2 nation 80.6 nature 44.4 po"
U06-1008,W06-2007,1,0.868399,"Missing"
U06-1008,P05-1005,0,0.0189502,"system to other unsupervised techniques and analyse the prospects for system combination. Finally, we conclude and discuss future work in Section 8. 2 Related Work The construction of unsupervised WSD systems applicable to all words in context has been the goal of many research initiatives, as can be seen in special journals and devoted books - see for instance (Agirre and Edmonds, 2006) for a recent book. We will now describe different trends that are being explored. Some recent techniques seek to alleviate the knowledge acquisition bottleneck by combining training data from different words. Kohomban and Lee (2005) build semantic classifiers by merging data from words in the same semantic class. Once the class is selected, simple heuristics are applied to obtain the fine-grained sense. The classifier fol3 Monosemous Relatives method The “monosemous relatives” approach is a technique to acquire training examples automatically and then feed them to a Machine Learning (ML) method. This algorithm is based on (Leacock et al., 1998), and follows these steps: (i) select a set of monosemous words that are related to the different senses of the target word, (ii) query the Internet to obtain examples for each rel"
U06-1008,J98-1006,0,0.852726,"s in foreign languages back into English and achieved good results on English WSD. Regarding portability, methods to automatically rank the senses of a word given a raw corpus, such as (McCarthy et al., 2004), have shown good flexibility to adapt to different domains, which is a desirable feature of all-words systems. We will compare the performance of the latter two systems and our approach in Section 7. English, relying on WordNet as thesaurus (Fellbaum, 1998). A well known approach for unsupervised WSD consists of the automatic acquisition of training data by means of monosemous relatives (Leacock et al., 1998). This technique roughly follows these steps: (i) select a set of monosemous words that are related to the different senses of the target word, (ii) query the Internet to obtain examples for each relative, (iii) create a collection of training examples for each sense, and (iv) use an ML algorithm trained on the acquired collections to tag the test instances. This method has been used to bootstrap large sense-tagged corpora (Mihalcea, 2002; Agirre and Martinez, 2004). Two important shortcomings of this method are the lack of monosemous relatives for some senses of the target words, and the nois"
U06-1008,P98-2127,0,0.0642606,"developing such resources is difficult and sometimes not feasible, which has been motivating us to explore unsupervised techniques to open up the knowledge acquisition bottleneck in WSD. The unsupervised systems that we will apply on this paper require raw corpora and a thesaurus with relations between word senses and words. Although these resources are not available for all languages, there is a growing number of WordNets in different languages that can be used1 . Other approach would be to apply methods based on distributional similarity to build a thesaurus automatically from raw corpora (Lin, 1998). The relations can then be applied in our algorithm. In this paper we have focused on the results we can obtain for The current situation for Word Sense Disambiguation (WSD) is somewhat stuck due to lack of training data. We present in this paper a novel disambiguation algorithm that improves previous systems based on acquisition of examples by incorporating local context information. With a basic configuration, our method is able to obtain state-of-the-art performance. We complemented this work by evaluating other well-known methods in the same dataset, and analysing the comparative results"
U06-1008,S01-1026,0,0.0719669,"Missing"
U06-1008,magnini-cavaglia-2000-integrating,0,0.0214303,"Missing"
U06-1008,P04-1036,0,0.276704,"aluated in the all-words task of Senseval-2. However, parallel corpora is an expensive resource to obtain for all target words. A related approach is to use monolingual corpora in a second language and use bilingual dictionaries to translate the training data (Wang and Carroll, 2005). Instead of using bilingual dictionaries, Wang and Martinez (2006) tried to apply machine translation on translating text snippets in foreign languages back into English and achieved good results on English WSD. Regarding portability, methods to automatically rank the senses of a word given a raw corpus, such as (McCarthy et al., 2004), have shown good flexibility to adapt to different domains, which is a desirable feature of all-words systems. We will compare the performance of the latter two systems and our approach in Section 7. English, relying on WordNet as thesaurus (Fellbaum, 1998). A well known approach for unsupervised WSD consists of the automatic acquisition of training data by means of monosemous relatives (Leacock et al., 1998). This technique roughly follows these steps: (i) select a set of monosemous words that are related to the different senses of the target word, (ii) query the Internet to obtain examples"
U06-1008,W04-0807,0,0.248868,"d Senseval 3 lexical sample datasets. 1 Introduction Word Sense Disambiguation (WSD) is an intermediate task that potentially can benefit many other NLP systems, from machine translation to indexing of biomedical texts. The goal of WSD is to ground the meaning of words in certain contexts into concepts as defined in some dictionary or lexical repository. Since 1998, the Senseval challenges have been serving as showcases for the state-of-the-art WSD systems. In each competition, Senseval has been growing in participants, labelling tasks, and target languages. The most recent Senseval workshop (Mihalcea et al., 2004) has again shown 1 http://www.globalwordnet.org/gwa/wordnet table.htm Proceedings of the 2006 Australasian Language Technology Workshop (ALTW2006), pages 42–50. 42 lows memory-based learning, and the examples are weighted according to their semantic similarity to the target word. Niu et al. (2005) use all-words training data to build a word-independent model to compute the similarity between two contexts. A maximum entropy algorithm is trained with the all-words corpus, and the model is used for clustering the instances of a given target word. One of the problems of clustering algorithms for W"
U06-1008,P94-1013,0,0.0752537,"emcor), or a prior algorithm like (McCarthy et al., 2004). In this paper we present the results of the basic approach that uses all the retrieved examples per sense, which is the best standalone unsupervised alternative. • Metaphors: the relative cathedral (2nd sense) appears in very different collocations that are not related to any sense of church, e.g. the cathedral of football. • Named entities: the relative kirk (2nd sense), which is a name for a Scottish church, will retrieve sentences that use Kirk as a proper noun. The ML technique Agirre and Martinez (2004) applied is Decision Lists (Yarowsky, 1994). In this method, the sense sk with the highest weighted feature fi is selected, according to its loglikelihood (see Formula 1). For this implementation, they used a simple smoothing method: the cases where the denominator is zero are smoothed by the constant 0.1. weight(sk , fi ) = log( P P r(sk |fi ) ) j6=k P r(sj |fi ) • Frequent words as relatives: relatives like hebraism (1st sense) could provide useful examples, but if the query is not restricted can also be the source of many noisy examples. The idea behind the “relatives in context” method is to combine local contexts of the target wor"
U06-1008,C98-2122,0,\N,Missing
U06-1008,W04-3204,1,\N,Missing
vossen-etal-2008-kyoto,W02-1304,1,\N,Missing
vossen-etal-2008-kyoto,W01-0703,1,\N,Missing
vossen-etal-2008-kyoto,magnini-cavaglia-2000-integrating,0,\N,Missing
vossen-etal-2008-kyoto,atserias-etal-2004-towards,1,\N,Missing
vossen-etal-2008-kyoto,soria-etal-2006-moving,1,\N,Missing
vossen-etal-2008-kyoto,chou-huang-2006-hantology,1,\N,Missing
vossen-etal-2008-kyoto,W06-1003,1,\N,Missing
W00-1326,J98-1001,0,0.0395951,"Missing"
W00-1326,H93-1061,0,0.399557,"Missing"
W00-1326,P96-1006,0,0.48276,"lem. The paper is organized as follows. The resources used and the experimental settings are presented first. Section 3 presents the collocations considered and Section 4 explains how decision lists have been adapted to n-way ambiguities. Sections 5 and 6 show the incorpus and cross-corpora experiments, respectively. Section 7 discusses the effect of drawing training and testing data from the same documents. Section 8 evaluates the impact of genre and topic variations, which is fiarther discussed in Section 9. Finally, Section 10 presents some conclusions. 1 Resources used The DSO collection (Ng and Lee, 1996) focuses on 191 frequent and polysemous words (nouns and verbs), and contains around 1,000 sentences per word. Overall, there are 112,800 sentences, where 192,874 occurrences of the target words were hand-tagged with WordNet senses (Miller et al., 1990). The DSO collection was built with examples from the Wall Street Journal (WSJ) and Brown Corpus (BC). The Brown Corpus is balanced, and the texts are classified according some predefined categories (el. Table 1). The examples from the Brown Corpus comprise 78,080 occurrences of word senses, and the examples from the WSJ 114,794 occurrences. The"
W00-1326,W99-0502,0,0.0402376,"Missing"
W00-1326,H93-1052,0,0.384116,"llowing genre and topic variations. This explains the low results when performing word sense disambiguation across corpora. In fact, we demonstrate that when two independent corpora share a related genre/topic, the word sense disambiguation results would be better. Future work on word sense disambiguation will have to take into account genre and topic as important parameters on their models. Introduction In the early nineties two famous papers claimed that the behavior of word senses in texts adhered to two principles: one sense per discourse (Gale et al., 1992) and one sense per collocation (Yarowsky, 1993). These hypotheses were shown to hold for some particular corpora (totaling 380 Mwords) on words with 2-way ambiguity. The word sense distinctions came from different sources (translations into French, homophones, homographs, pseudo-words, etc.), but no dictionary or lexical resource was linked to them. In the case of the one sense per collocation paper, several corpora were used, but nothing is said on whether the collocations hold across corpora. Since the papers were published, word sense disambiguation has m o v e d to deal with fine207 This study has special significance at this point of"
W00-1326,J98-1006,0,\N,Missing
W00-1326,W00-1322,0,\N,Missing
W00-1326,C94-2174,0,\N,Missing
W00-1326,P95-1026,0,\N,Missing
W00-1326,A00-1031,0,\N,Missing
W00-1326,P94-1013,0,\N,Missing
W00-1326,W00-1702,1,\N,Missing
W00-1702,C96-1005,1,0.828824,"Missing"
W00-1702,W00-1702,1,0.107184,"alent to choosing at random in ties. The experiments are organized as follows: • Evaluate decision lists on SemCor and DSO separately, focusing on baseline features, other features, local vs. topical features, learning curve, noise, overall in SemCor and overall in DSO (section 4). All experiments were performed using 10-fold cross-validation. • Evaluate cross-corpora tagging. Train on DSO and tag SemCor and vice versa (section 5). • Evaluate the Web corpus. Train on Webacquired texts and tag SemCor (section 6). Because of length limitations, it is not possible to show all the data, refer to (Agirre & Martinez, 2000) for more comprehensive results. 4 Results on SemCor and DSO data We first defined an initial set of features and compared the results with the random baseline (Rand) and the most frequent sense baseline (MFS). The basic combination of features comprises word-form bigrams and trigrams, part of speech bigrams and trigrams, a bag with the word-forms in a window spanning 4 words left and right, and a bag with the word forms in the sentence. The results for SemCor and DSO are shown in Table 1. We want to point out the following: • The number of examples per word sense is very low for SemCor (aroun"
W00-1702,J98-1006,0,0.753691,"urek ) = Log ( Pr( sensei |featurek ) ) ∑ Pr( sense j |featurek ) j ≠i Features with 0 or negative values were are not inserted in the decision list. When testing, the decision list is checked in order and the feature with highest weight that is present in the test sentence selects the winning word sense. An example is shown below. The probabilities have been estimated using the maximum likelihood estimate, smoothed using a simple method: when the denominator in the formula is 0 we replace it with 0.1. We analyzed several features already mentioned in the literature (Yarowsky, 1994; Ng, 1997; Leacock et al. 1998), and new features like the word sense or semantic field of the words around the target which are available in SemCor. Different sets of features have been created to test the influence of each feature type in the results: a basic set of features (section 4), several extensions (section 4.2). The example below shows three senses of the noun interest, an example, and some of the features for the decision lists of interest that appear in the example shown. Sense 1: interest, involvement => curiosity, wonder Sense 2: interest, interestingness => power, powerfulness, potency Sense 3: sake, interes"
W00-1702,H93-1061,0,0.0453621,", thesaurus, homographs, ...) instead of widely recognized semantic lexical resources (ontologies like Sensus, Cyc, EDR, WordNet, EuroWordNet, David Martínez IxA NLP group. 649 pk. Donostia, Basque Country, E-20.080 jibmaird@si.ehu.es etc., or machine-readable dictionaries like OALDC, Webster&apos;s, LDOCE, etc.) which usually have fine-grained sense differences. We chose to work with WordNet (Miller et al. 1990). 3. Unavailability of training data: current handtagged corpora seem not to be enough for state-ofthe-art systems. We test how far can we go with existing hand-tagged corpora like SemCor (Miller et al. 1993) and the DSO corpus (Ng and Lee, 1996), which have been tagged with word senses from WordNet. Besides we test an algorithm that automatically acquires training examples from the Web (Mihalcea & Moldovan, 1999). In this paper we focus on one of the most successful algorithms to date (Yarowsky 1994), as attested in the Senseval competition (Kilgarriff & Palmer, 2000). We will evaluate it on both SemCor and DSO corpora, and will try to test how far could we go with such big corpora. Besides, the usefulness of hand tagging using WordNet senses will be tested, training on one corpus and testing in"
W00-1702,H94-1046,0,0.0120846,"or semantic files does not significantly alter the results. Using a simplified set of PoS tags (only 5 tags) does not degrade performance. Local features, i.e. collocations, are the strongest kind of features, but topical features enable to extend the coverage. • Kinds of words: the highest results can be expected for words with a dominating word sense. Nouns attain better performance with local features when enough data is provided. Individual words exhibit distinct behavior regarding to the feature sets. • SemCor has been cited as having scarce data to train supervised learning algorithms (Miller et al., 1994). Church, for instance, occurs 128 times, but duty only 25 times and account 27. We found Word Account Age Church Duty Head Interest Member People Die Include Know Seek Understand PoS # Examples Rand. DL on SemCor N 1175 .10 .00/.85 .29/.97 N 630 .20 .46/.98 N 386 .33 .35/1.0 N 449 .33 N 3636 .03 .04/.44 .25/.88 N 1043 .14 N 696 .20 .16/.86 N 591 .25 .16/.95 V 1615 .09 .04/.93 V 577 .25 .11/.99 V 1423 .09 .07/.64 .49/.98 V 714 .20 V 780 .20 .12/.92 Table 9: Results on Web data. out that SemCor nevertheless provides enough data to perform some basic general disambiguation, at 0.68 precision on"
W00-1702,P96-1006,0,0.282183,"widely recognized semantic lexical resources (ontologies like Sensus, Cyc, EDR, WordNet, EuroWordNet, David Martínez IxA NLP group. 649 pk. Donostia, Basque Country, E-20.080 jibmaird@si.ehu.es etc., or machine-readable dictionaries like OALDC, Webster&apos;s, LDOCE, etc.) which usually have fine-grained sense differences. We chose to work with WordNet (Miller et al. 1990). 3. Unavailability of training data: current handtagged corpora seem not to be enough for state-ofthe-art systems. We test how far can we go with existing hand-tagged corpora like SemCor (Miller et al. 1993) and the DSO corpus (Ng and Lee, 1996), which have been tagged with word senses from WordNet. Besides we test an algorithm that automatically acquires training examples from the Web (Mihalcea & Moldovan, 1999). In this paper we focus on one of the most successful algorithms to date (Yarowsky 1994), as attested in the Senseval competition (Kilgarriff & Palmer, 2000). We will evaluate it on both SemCor and DSO corpora, and will try to test how far could we go with such big corpora. Besides, the usefulness of hand tagging using WordNet senses will be tested, training on one corpus and testing in the other. This will allow us to compa"
W00-1702,W99-0502,0,0.0189563,"Missing"
W00-1702,P94-1013,0,0.817804,"etc.) which usually have fine-grained sense differences. We chose to work with WordNet (Miller et al. 1990). 3. Unavailability of training data: current handtagged corpora seem not to be enough for state-ofthe-art systems. We test how far can we go with existing hand-tagged corpora like SemCor (Miller et al. 1993) and the DSO corpus (Ng and Lee, 1996), which have been tagged with word senses from WordNet. Besides we test an algorithm that automatically acquires training examples from the Web (Mihalcea & Moldovan, 1999). In this paper we focus on one of the most successful algorithms to date (Yarowsky 1994), as attested in the Senseval competition (Kilgarriff & Palmer, 2000). We will evaluate it on both SemCor and DSO corpora, and will try to test how far could we go with such big corpora. Besides, the usefulness of hand tagging using WordNet senses will be tested, training on one corpus and testing in the other. This will allow us to compare hand tagged data with automatically acquired data. If new ways out of the acquisition bottleneck are to be explored, previous questions about supervised algorithms should be answered: how much data is needed, how much noise can they accept, can they be port"
W00-1702,J98-1001,0,\N,Missing
W00-1702,P95-1026,0,\N,Missing
W01-0703,P93-1016,0,0.0222404,"ted corpus. 1 Introduction Previous literature on selectional preference has usually learned preferences for words in the form of classes, e.g., the object of eat is an edible entity. This paper extends previous statistical models to classes of verbs, yielding a relation between classes in a hierarchy, as opposed to a relation between a word and a class. The model is trained using subject-verb and object-verb associations extracted from Semcor, a corpus (Miller et al., 1993) tagged with WordNet word-senses (Miller et al., 1990). The syntactic relations were extracted using the Minipar parser (Lin, 1993). A peculiarity of this exercise is the use of a small sensedisambiguated corpus, in contrast to using a large corpus of ambiguous words. We think that David Martinez IXA NLP Group University of the Basque Country 649 pk. 20.080 Donostia. Spain. jibmaird@si.ehu.es two factors can help alleviate the scarcity of data: the fact that using disambiguated words provides purer data, and the ability to use classes of verbs in the preferences. Nevertheless, the approach can be easily extended to larger, nondisambiguated corpora. We have defined a word sense disambiguation exercise in order to evaluate"
W01-0703,H93-1061,0,0.0129359,"l is tested on a word sense disambiguation task which uses subject-verb and object-verb relationships extracted from a small sense-disambiguated corpus. 1 Introduction Previous literature on selectional preference has usually learned preferences for words in the form of classes, e.g., the object of eat is an edible entity. This paper extends previous statistical models to classes of verbs, yielding a relation between classes in a hierarchy, as opposed to a relation between a word and a class. The model is trained using subject-verb and object-verb associations extracted from Semcor, a corpus (Miller et al., 1993) tagged with WordNet word-senses (Miller et al., 1990). The syntactic relations were extracted using the Minipar parser (Lin, 1993). A peculiarity of this exercise is the use of a small sensedisambiguated corpus, in contrast to using a large corpus of ambiguous words. We think that David Martinez IXA NLP Group University of the Basque Country 649 pk. 20.080 Donostia. Spain. jibmaird@si.ehu.es two factors can help alleviate the scarcity of data: the fact that using disambiguated words provides purer data, and the ability to use classes of verbs in the preferences. Nevertheless, the approach can"
W01-0703,P92-1053,0,0.0148559,"s, e.g. a verb like ‘eat’ prefers as object edible things, and as subject animate entities, as in, (1) “She was eating an apple”. Selectional preferences get more complex than it might seem: (2) “The acid ate the metal”, (3) “This car eats a lot of gas”, (4) “We ate our savings”, etc. Corpus-based approaches for selectional preference learning extract a number of (e.g. verb/subject) relations from large corpora and use an algorithm to generalize from the set of nouns for each verb separately. Usually, nouns are generalized using classes (concepts) from a lexical knowledge base (e.g. WordNet). Resnik (1992, 1997) defines an informationtheoretic measure of the association between a verb and nominal WordNet classes: selectional association. He uses verb-argument pairs from Brown. Evaluation is performed applying intuition and WSD. Our measure follows in part from his formalization. Abe and Li (1995) follow a similar approach, but they employ a different informationtheoretic measure (the minimum description length principle) to select the set of concepts in a hierarchy that generalize best the selectional preferences for a verb. The argument pairs are extracted from the WSJ corpus, and evaluation"
W01-0703,W97-0209,0,0.173669,"Missing"
W01-0703,W98-0701,0,0.0600365,"ciation between a verb and nominal WordNet classes: selectional association. He uses verb-argument pairs from Brown. Evaluation is performed applying intuition and WSD. Our measure follows in part from his formalization. Abe and Li (1995) follow a similar approach, but they employ a different informationtheoretic measure (the minimum description length principle) to select the set of concepts in a hierarchy that generalize best the selectional preferences for a verb. The argument pairs are extracted from the WSJ corpus, and evaluation is performed using intuition and PP-attachment resolution. Stetina et al. (1998) extract word-arg-word triples for all possible combinations, and use a measure of “relational probability” based on frequency and similarity. They provide an algorithm to disambiguate all words in a sentence. It is directly applied to WSD with good results. 3 Our approach The model explored in this paper emerges as a result of the following observations: • Distinguishing verb senses can be useful. The examples for eat above are taken from WordNet, and each corresponds to a different word sense1: example (1) is from the “take in solid food” sense of eat, (2) from the ”cause to rust” sense, and"
W01-0703,W00-1702,1,\N,Missing
W02-0801,A00-1031,0,0.00712382,"nitions above. One triple is obtained twice from two different definitions. furrow#ground#by#plow surco#tierra#con#arado surco#tierra#con#arado Definitions that do not have a matching triple are discarded, leaving Basque triples without matching triple ambiguous. For instance we could not find triples for irauli#INS#egin(cf. example in section 2.1). The instrumental suffix is sometimes translated without prepositions (in this case “… made turning …”). Looking up the bilingual dictionaries for translation requires lemmatization and Part of Speech tagging. For English we use the TnT PoS tagger (Brants, 2000) and WordNet for lemmatization (Miller et al., 1990). For Spanish we use (Atserias et al., 1998). 2.4 Disambiguation For each Basque case suffix, Spanish preposition and English preposition we have a list of interpretations (cf. Table 1). We assign the interpretations of the preposition to each Spanish/English triple. The intersection of all the interpretations is assigned to it. Continuing with out example, we can see that the intersection between the interpretations of the English by preposition (three interpretations) and the interpretations of the Spanish con preposition (four interpretati"
W02-0801,dorr-etal-1998-thematic,0,\N,Missing
W02-1304,W02-1304,1,0.0511956,"Missing"
W02-1304,W00-1702,1,0.907606,"Missing"
W02-1304,W01-0703,1,0.824628,"Missing"
W02-1304,W99-0603,0,0.0765567,"Missing"
W02-1304,W00-1322,1,0.899983,"Missing"
W02-1304,J98-1001,0,0.0564481,"Missing"
W02-1304,J98-1006,0,0.0986902,"Missing"
W02-1304,magnini-cavaglia-2000-integrating,1,0.726135,"Missing"
W02-1304,W00-1326,1,0.886817,"Missing"
W02-1304,P98-2247,0,0.048344,"Missing"
W02-1304,S01-1029,1,0.792942,"Missing"
W02-1304,W97-0201,0,0.436936,"Missing"
W02-1304,S01-1017,1,\N,Missing
W02-1304,W00-1325,0,\N,Missing
W02-1304,P00-1064,0,\N,Missing
W02-1304,W00-0706,1,\N,Missing
W02-1304,P95-1026,0,\N,Missing
W02-1304,W02-0801,1,\N,Missing
W02-1304,C98-2242,0,\N,Missing
W04-0801,C00-1001,0,0.0372793,"Missing"
W04-0801,agirre-etal-2004-exploring,1,0.816165,"(Aduriz et al. 2000), filtered using some heuristics to ensure quality of context, and finally filtered for PoS mismatches. Table 1 shows the number of examples from each source. 2.4 Words chosen Basically, the words employed in this task are the same words used in Senseval 2 (40 words, 15 nouns, 15 verbs and 10 adjectives), only the sense inventory changed. Besides, in Senseval 3 we replaced 5 verbs with new ones. The reason for this is that in the context of the MEANING project1 we are exploring multilingual lexical acquisition, and there are ongoing experiments that focus on those verbs. (Agirre et al. 2004; Atserias et al. 2004). In fact, 10 words in the English lexical-sample have translations in the Basque, Catalan, Italian, Romanian and Spanish lexical tasks: channel, crown, letter, program, party (nouns), simple (adjective), play, win, lose, decide (verbs). 2.5 Selection of examples from corpora The minimum number of examples for each word according to the task specifications was calculated as follows: N=75+15*senses+7*multiwords As the number of senses in WordNet is very high, we decided to first estimate the number of senses and multiwords that really occur in the corpus. The taggers were"
W04-0801,atserias-etal-2004-cross,1,0.821741,"), filtered using some heuristics to ensure quality of context, and finally filtered for PoS mismatches. Table 1 shows the number of examples from each source. 2.4 Words chosen Basically, the words employed in this task are the same words used in Senseval 2 (40 words, 15 nouns, 15 verbs and 10 adjectives), only the sense inventory changed. Besides, in Senseval 3 we replaced 5 verbs with new ones. The reason for this is that in the context of the MEANING project1 we are exploring multilingual lexical acquisition, and there are ongoing experiments that focus on those verbs. (Agirre et al. 2004; Atserias et al. 2004). In fact, 10 words in the English lexical-sample have translations in the Basque, Catalan, Italian, Romanian and Spanish lexical tasks: channel, crown, letter, program, party (nouns), simple (adjective), play, win, lose, decide (verbs). 2.5 Selection of examples from corpora The minimum number of examples for each word according to the task specifications was calculated as follows: N=75+15*senses+7*multiwords As the number of senses in WordNet is very high, we decided to first estimate the number of senses and multiwords that really occur in the corpus. The taggers were provided with a suffic"
W04-0813,C00-1001,0,0.0604085,"mble in cross-validation. The main diﬀerence between the Basque and English systems was the feature set. A rich set of features was used for English, including syntactic dependencies and domain information, extracted with diﬀerent tools, and also from external resources like WordNet Domains (Magnini and Cavagli´ a, 2000). The features for Basque were diﬀerent, as Basque is an agglutinative language, and syntactic information is given by inﬂectional suﬃxes. We tried to represent this information in local features, relying on the analysis of a deep morphological analyzer developed in our group (Aduriz et al., 2000). In order to improve the performance of the algorithms, diﬀerent smoothing techniques were David Martinez IXA NLP Group Basque Country University Donostia, Spain davidm@si.ehu.es tested on the English Senseval-2 lexical sample data (Agirre and Martinez, 2004), and applied to Senseval-3. These methods helped to obtain better estimations for the features, and to avoid the problem of 0 counts Decision Lists and Naive Bayes. This paper is organized as follows. The learning algorithms are ﬁrst introduced in Section 2, and Section 3 describes the features applied to each task. In Section 4, we pres"
W04-0813,magnini-cavaglia-2000-integrating,0,0.0373521,"Missing"
W04-0813,W97-0323,0,0.0317258,"ethods and Parameters DL: On Senseval-2 data, we observed that DL improved signiﬁcantly its performance with a smoothing technique based on (Yarowsky, 1995a). For our implementation, the smoothed probabilities were obtained by grouping the observations by raw frequencies and feature types. As this method seems sensitive to the feature types and the amount of examples, we tested 3 DL versions: DL smooth (using smoothed probabilities), DL ﬁxed (replacing 0 counts with 0.1), and DL discard (discarding features appearing with only one sense). NB: We applied a simple smoothing method presented in (Ng, 1997), where zero counts are replaced by the probability of the given sense divided by the number of examples. V: The same smoothing method used for NB was applied for vectors. For Basque, two versions were tested: as the Basque parser can return ambiguous analyses, partial weights are assigned to the features in the context, and we can chose to use these partial weights (p), or assign the full weight to all features (f). SVM: No smoothing was applied. We estimated the soft margin using a greedy process in cross-validation on the training data per each word. Combination: Single voting was used, whe"
W04-0813,N01-1006,0,0.0416795,"t words in the whole context, and in a ±4-word window around the target. We also obtain salient bigrams in the context, with the methods and the software described in (Pedersen, 2001). Domain features: The WordNet Domains resource was used to identify the most relevant domains in the context. Following the relevance formula presented in (Magnini and Cavagli´ a, 2000), we deﬁned 2 feature types: (1) the most relevant domain, and (2) a list of domains above a predeﬁned threshold3 . Other experiments using domains from SUMO, the EuroWordNet 1 The PoS tagging was performed with the fnTBL toolkit (Ngai and Florian, 2001). 2 This software was kindly provided by David Yarowsky’s group, from Johns Hopkins University. 3 The software to obtain the relevant domains was kindly provided by Gerard Escudero’s group, from Universitat Politecnica de Catalunya top-ontology, and WordNet’s Semantic Fields were performed, but these features were discarded from the ﬁnal set. 3.2 Features for Basque Basque is an agglutinative language, and syntactic information is given by inﬂectional sufﬁxes. The morphological analysis of the text is a necessary previous step in order to select informative features. The data provided by the t"
W04-0813,N01-1011,0,0.262954,"wordforms, or PoS tags1 . Other local features are those formed with the previous/posterior lemma/word-form in the context. Syntactic dependencies: syntactic dependencies were extracted using heuristic patterns, and regular expressions deﬁned with the PoS tags around the target2 . The following relations were used: object, subject, noun-modiﬁer, preposition, and sibling. Bag-of-words features: we extract the lemmas of the content words in the whole context, and in a ±4-word window around the target. We also obtain salient bigrams in the context, with the methods and the software described in (Pedersen, 2001). Domain features: The WordNet Domains resource was used to identify the most relevant domains in the context. Following the relevance formula presented in (Magnini and Cavagli´ a, 2000), we deﬁned 2 feature types: (1) the most relevant domain, and (2) a list of domains above a predeﬁned threshold3 . Other experiments using domains from SUMO, the EuroWordNet 1 The PoS tagging was performed with the fnTBL toolkit (Ngai and Florian, 2001). 2 This software was kindly provided by David Yarowsky’s group, from Johns Hopkins University. 3 The software to obtain the relevant domains was kindly provide"
W04-0813,P95-1026,0,0.415154,"e learning algorithms are ﬁrst introduced in Section 2, and Section 3 describes the features applied to each task. In Section 4, we present the experiments performed on training data before submission; this section also covers the ﬁnal conﬁguration of each algorithm, and the performance obtained on training data. Finally, the oﬃcial results in Senseval-3 are presented and discussed in Section 5. 2 Learning Algorithms The algorithms presented in this section rely on features extracted from the context of the target word to make their decisions. The Decision List (DL) algorithm is described in (Yarowsky, 1995b). In this algorithm the sense with the highest weighted feature is selected, as shown below. We can avoid undetermined values by discarding features that have a 0 probability in the divisor. More sophisticated smoothing techniques have also been tried (cf. Section 4). arg max w(sk , fi ) = log(  k P r(sk |fi ) ) j=k P r(sj |fi ) The Naive Bayes (NB) algorithm is based on the conditional probability of each sense given the features in the context. It also requires smoothing. arg max P (sk ) k m i=1 P (fi |sk ) For the Vector Space Model (V) algorithm, we represent each occurrence context a"
W04-0861,W04-0828,1,0.681474,"Missing"
W04-0861,W04-0837,1,0.823382,"Missing"
W04-0861,P94-1013,0,0.0414439,"Catalonia. The integration was carried out by the TALP group.   Naive Bayes (NB) is the well–known Bayesian algorithm that classifies an example by choosing the class that maximizes the product, over all features, of the conditional probability of the class given the feature. The provider of this module is IXA. Conditional probabilities were smoothed by Laplace correction.  Decision List (DL) are lists of weighted classification rules involving the evaluation of one single feature. At classification time, the algorithm applies the rule with the highest weight that matches the test example (Yarowsky, 1994). The provider is IXA and they also applied smoothing to generate more robust decision lists.  In the Vector Space Model method (cosVSM), each example is treated as a binary-valued feature vector. For each sense, one centroid vector is obtained from training. Centroids are compared with the vectors representing test examples, using the cosine similarity function, and the closest centroid is used to classify the example. No smoothing is required for this method provided by IXA. 2 The WSD Modules Support Vector Machines (SVM) find the hyperplane (in a high dimensional feature space) that separa"
W04-3204,agirre-de-lacalle-2004-publicly,1,0.739887,"ated these nouns in 2 sets, depending on the number of examples they have in Semcor: Set A contained the 16 nouns with more than 10 examples in Semcor, and Set B the remaining low-frequency words. 4 Building the monosemous relatives web corpus In order to build this corpus3 , we have acquired 1000 Google snippets for each monosemous word in WordNet 1.7. Then, for each word sense of the ambiguous words, we gathered the examples of its monosemous relatives (see below). This method is inspired in (Leacock et al., 1998), and has shown to be effective in experiments of topic signature acquisition (Agirre and Lopez, 2004). This last paper also shows that it is possible to gather examples based on 3 The automatically acquired corpus will be referred indistinctly as web-corpus, or monosemous-corpus monosemous relatives for nearly all noun senses in WordNet4 . The basic assumption is that for a given word sense of the target word, if we had a monosemous synonym of the word sense, then the examples of the synonym should be very similar to the target word sense, and could therefore be used to train a classifier of the target word sense. The same, but in a lesser extent, can be applied to other monosemous relatives,"
W04-3204,W00-1702,1,0.919628,"n Lists The learning method used to measure the quality of the corpus is Decision Lists (DL). This algorithm is described in (Yarowsky, 1994). In this method, the sense sk with the highest weighted feature fi is selected, according to its log-likelihood (see Formula 1). For our implementation, we applied a simple smoothing method: the cases where the denominator is zero are smoothed by the constant 0.1 . weight(sk , fi ) = log(  3.2 P r(sk |fi ) ) j=k P r(sj |fi ) (1) Features In order to represent the context, we used a basic set of features frequently used in the literature for WSD tasks (Agirre and Martinez, 2000). We distinguish two types of features: • Local features: Bigrams and trigrams, formed by the word-form, lemma, and part-of-speech2 of the surrounding words. Also the content lemmas in a ±4 word window around the target. • Topical features: All the content lemmas in the context. 2 The PoS tagging was performed using TnT (Brants, 2000) We have analyzed the results using local and topical features separately, and also using both types together (combination). 3.3 Hand-tagged corpora Semcor was used as training data for our supervised system. This corpus offers tagged examples for many words, and"
W04-3204,A00-1031,0,0.00285494,"the denominator is zero are smoothed by the constant 0.1 . weight(sk , fi ) = log(  3.2 P r(sk |fi ) ) j=k P r(sj |fi ) (1) Features In order to represent the context, we used a basic set of features frequently used in the literature for WSD tasks (Agirre and Martinez, 2000). We distinguish two types of features: • Local features: Bigrams and trigrams, formed by the word-form, lemma, and part-of-speech2 of the surrounding words. Also the content lemmas in a ±4 word window around the target. • Topical features: All the content lemmas in the context. 2 The PoS tagging was performed using TnT (Brants, 2000) We have analyzed the results using local and topical features separately, and also using both types together (combination). 3.3 Hand-tagged corpora Semcor was used as training data for our supervised system. This corpus offers tagged examples for many words, and has been widely used for WSD. It was necessary to use an automatic mapping between the WordNet 1.6 senses in Semcor and the WordNet 1.7 senses in testing (Daude et al., 2000). For evaluation, the test part of the Senseval-2 English lexical-sample task was chosen. The advantage of this corpus was that we could focus on a word-set with"
W04-3204,P00-1064,0,0.00998965,"s. Also the content lemmas in a ±4 word window around the target. • Topical features: All the content lemmas in the context. 2 The PoS tagging was performed using TnT (Brants, 2000) We have analyzed the results using local and topical features separately, and also using both types together (combination). 3.3 Hand-tagged corpora Semcor was used as training data for our supervised system. This corpus offers tagged examples for many words, and has been widely used for WSD. It was necessary to use an automatic mapping between the WordNet 1.6 senses in Semcor and the WordNet 1.7 senses in testing (Daude et al., 2000). For evaluation, the test part of the Senseval-2 English lexical-sample task was chosen. The advantage of this corpus was that we could focus on a word-set with enough examples for testing. Besides, it is a different corpus, so the evaluation is more realistic than that made using cross-validation. The test examples whose senses were multiwords or phrasal verbs were removed, because they can be efficiently detected with other methods in a preprocess. It is important to note that the training part of Senseval-2 lexical-sample was not used in the construction of the systems, as our goal was to"
W04-3204,S01-1001,0,0.0183951,"have used to train disambiguation systems. The corpus-building process has highlighted important factors, such as the distribution of senses (bias). The corpus has been used to train WSD algorithms that include supervised methods (combining automatic and manuallytagged examples), minimally supervised (requiring sense bias information from hand-tagged corpora), and fully unsupervised. These methods were tested on the Senseval-2 lexical sample test set, and compared successfully to other systems with minimum or no supervision. 1 Introduction The results of recent WSD exercises, e.g. Senseval21 (Edmonds and Cotton, 2001) show clearly that WSD methods based on hand-tagged examples are the ones performing best. However, the main drawback for supervised WSD is the knowledge acquisition bottleneck: the systems need large amounts of costly hand-tagged data. The situation is more dramatic for lesser studied languages. In order to overcome this problem, different research lines have been explored: automatic acquisition of training examples (Mihalcea, 2002), bootstrapping techniques (Yarowsky, 1995), or active learning (ArgamonEngelson and Dagan, 1999). In this work, we have focused on the automatic acquisition of ex"
W04-3204,S01-1018,0,0.0581651,"ame test data and set of nouns. From the 5 unsupervised systems presented in the Senseval-2 lexical-sample task as unsupervised, the WASP-Bench system relied on lexicographers to hand-code information semi-automatically (Tugwell and Kilgarriff, 2001). This system does not use the training data, but as it uses manually coded knowledge we think it falls clearly in the supervised category. The results for the other 4 systems and our own are shown in Table 7. We show the results for the totally unsupervised system and the minimally unsupervised system (Semcor bias). We classified the UNED system (Fernandez-Amoros et al., 2001) as minimally supervised. It does not use hand-tagged examples for training, but some of the heuristics that are applied by the system rely on the bias information available in Semcor. The distribution of senses is used to discard low-frequency senses, and also to choose the first sense as a back-off strategy. On the same conditions, our minimally supervised system attains 49.8 recall, nearly 5 points more. The rest of the systems are fully unsupervised, and they perform significantly worse than our system. 6 Conclusions and Future Work This paper explores the large-scale acquisition of sense-"
W04-3204,fernandez-etal-2004-automatic,0,0.0310314,"Missing"
W04-3204,J98-1006,0,0.839126,", which would tag each word with the sense occurring most frequently in Semcor. In our approach, we will also rely on Semcor as the basic resource, both for training examples and as an indicator of the distribution of the senses of the target word. The goal of our experiment is to evaluate up to which point we can automatically acquire examples for word senses and train accurate supervised WSD systems on them. This is a very promising line of research, but one which remains relatively understudied (cf. Section 2). The method we applied is based on the monosemous relatives of the target words (Leacock et al., 1998), and we studied some parameters that affect the quality of the acquired corpus, such as the distribution of the number of training instances per each word sense (bias), and the type of features used for disambiguation (local vs. topical). Basically, we built three systems, one fully supervised (using examples from both Semcor and automatically acquired examples), one minimally supervised (using the distribution of senses in Semcor and automatically acquired examples) and another fully unsupervised (using an automatically acquired sense rank (McCarthy et al., 2004) and automatically acquired e"
W04-3204,P98-2127,0,0.0138491,"Distribution of examples for the senses of authority in different corpora. Pr (proportional) and MR (minimum ratio) columns correspond to different ways to apply Semcor bias. each sense. In order to test the impact of bias, different settings have been tried: • No bias: we take an equal amount of examples for each sense. • Web bias: we take all examples gathered from the web. • Automatic ranking: the number of examples is given by a ranking obtained following the method described in (McCarthy et al., 2004). They used a thesaurus automatically created from the BNC corpus with the method from (Lin, 1998), coupled with WordNet-based similarity measures. • Semcor bias: we take a number of examples proportional to the bias of the word senses in Semcor. For example, Table 1 shows the number of examples per type (0,1,...) that are acquired for church following the Semcor bias. The last column gives the number of examples in Semcor. We have to note that the 3 first methods do not require any hand-labeled data, and that the fourth relies in Semcor. The way to apply the bias is not straightforward in some cases. In our first approach for Semcorbias, we assigned 1,000 examples to the major sense in Se"
W04-3204,W00-1326,1,0.834551,"Missing"
W04-3204,P04-1036,0,0.135688,"elatives of the target words (Leacock et al., 1998), and we studied some parameters that affect the quality of the acquired corpus, such as the distribution of the number of training instances per each word sense (bias), and the type of features used for disambiguation (local vs. topical). Basically, we built three systems, one fully supervised (using examples from both Semcor and automatically acquired examples), one minimally supervised (using the distribution of senses in Semcor and automatically acquired examples) and another fully unsupervised (using an automatically acquired sense rank (McCarthy et al., 2004) and automatically acquired examples). This paper is structured as follows. First, Section 2 describes previous work on the field. Section 3 introduces the experimental setting for evaluating the acquired corpus. Section 4 is devoted to the process of building the corpus, which is evaluated in Section 5. Finally, the conclusions are given in Section 6. 2 Previous work As we have already mentioned, there is little work on this very promising area. In (Leacock et al., 1998), the method to obtain sense-tagged examples using monosemous relatives is presented. In this work, they retrieve the same n"
W04-3204,mihalcea-2002-bootstrapping,0,0.702145,"est set, and compared successfully to other systems with minimum or no supervision. 1 Introduction The results of recent WSD exercises, e.g. Senseval21 (Edmonds and Cotton, 2001) show clearly that WSD methods based on hand-tagged examples are the ones performing best. However, the main drawback for supervised WSD is the knowledge acquisition bottleneck: the systems need large amounts of costly hand-tagged data. The situation is more dramatic for lesser studied languages. In order to overcome this problem, different research lines have been explored: automatic acquisition of training examples (Mihalcea, 2002), bootstrapping techniques (Yarowsky, 1995), or active learning (ArgamonEngelson and Dagan, 1999). In this work, we have focused on the automatic acquisition of examples. When supervised systems have no specific training examples for a target word, they need to rely on publicly available all-words sense-tagged corpora like Semcor (Miller et al., 1993), which is tagged with WordNet word senses. The systems performing best in the English all-words task in Senseval-2 were basically supervised systems trained on Semcor. Unfortunately, for most of the words, this cor1 http://www.senseval.org. David"
W04-3204,H93-1061,0,0.292651,"leneck: the systems need large amounts of costly hand-tagged data. The situation is more dramatic for lesser studied languages. In order to overcome this problem, different research lines have been explored: automatic acquisition of training examples (Mihalcea, 2002), bootstrapping techniques (Yarowsky, 1995), or active learning (ArgamonEngelson and Dagan, 1999). In this work, we have focused on the automatic acquisition of examples. When supervised systems have no specific training examples for a target word, they need to rely on publicly available all-words sense-tagged corpora like Semcor (Miller et al., 1993), which is tagged with WordNet word senses. The systems performing best in the English all-words task in Senseval-2 were basically supervised systems trained on Semcor. Unfortunately, for most of the words, this cor1 http://www.senseval.org. David Martinez IXA NLP Group University of the Basque Country Donostia, Spain davidm@si.ehu.es pus only provides a handful of tagged examples. In fact, only a few systems could overcome the Most Frequent Sense (MFS) baseline, which would tag each word with the sense occurring most frequently in Semcor. In our approach, we will also rely on Semcor as the ba"
W04-3204,S01-1037,0,0.0111625,"et B (< 10) all words 51.9 40.1 47.8 50.5 47.4 50.9 47.7 49.8 Semcor + Web 51.6 47.8 50.3 MFS & Web 51.9 47.8 50.5 Table 6: Recall training in Semcor, the acquired web corpus (Semcor bias), and a combination of both, compared to that of the Semcor MFS. available. This made possible to compare our results and those of other systems deemed unsupervised by the organizers on the same test data and set of nouns. From the 5 unsupervised systems presented in the Senseval-2 lexical-sample task as unsupervised, the WASP-Bench system relied on lexicographers to hand-code information semi-automatically (Tugwell and Kilgarriff, 2001). This system does not use the training data, but as it uses manually coded knowledge we think it falls clearly in the supervised category. The results for the other 4 systems and our own are shown in Table 7. We show the results for the totally unsupervised system and the minimally unsupervised system (Semcor bias). We classified the UNED system (Fernandez-Amoros et al., 2001) as minimally supervised. It does not use hand-tagged examples for training, but some of the heuristics that are applied by the system rely on the bias information available in Semcor. The distribution of senses is used"
W04-3204,P94-1013,0,0.0749358,"iterative process, the system obtained new seeds from the retrieved examples. An experiment in the lexical-sample task showed that the method was useful for a subset of the Senseval-2 testing words (results for 5 words are provided). 3 Experimental Setting for Evaluation In this section we will present the Decision List method, the features used to represent the context, the two hand-tagged corpora used in the experiment and the word-set used for evaluation. 3.1 Decision Lists The learning method used to measure the quality of the corpus is Decision Lists (DL). This algorithm is described in (Yarowsky, 1994). In this method, the sense sk with the highest weighted feature fi is selected, according to its log-likelihood (see Formula 1). For our implementation, we applied a simple smoothing method: the cases where the denominator is zero are smoothed by the constant 0.1 . weight(sk , fi ) = log(  3.2 P r(sk |fi ) ) j=k P r(sj |fi ) (1) Features In order to represent the context, we used a basic set of features frequently used in the literature for WSD tasks (Agirre and Martinez, 2000). We distinguish two types of features: • Local features: Bigrams and trigrams, formed by the word-form, lemma, and"
W04-3204,P95-1026,0,0.285835,"systems with minimum or no supervision. 1 Introduction The results of recent WSD exercises, e.g. Senseval21 (Edmonds and Cotton, 2001) show clearly that WSD methods based on hand-tagged examples are the ones performing best. However, the main drawback for supervised WSD is the knowledge acquisition bottleneck: the systems need large amounts of costly hand-tagged data. The situation is more dramatic for lesser studied languages. In order to overcome this problem, different research lines have been explored: automatic acquisition of training examples (Mihalcea, 2002), bootstrapping techniques (Yarowsky, 1995), or active learning (ArgamonEngelson and Dagan, 1999). In this work, we have focused on the automatic acquisition of examples. When supervised systems have no specific training examples for a target word, they need to rely on publicly available all-words sense-tagged corpora like Semcor (Miller et al., 1993), which is tagged with WordNet word senses. The systems performing best in the English all-words task in Senseval-2 were basically supervised systems trained on Semcor. Unfortunately, for most of the words, this cor1 http://www.senseval.org. David Martinez IXA NLP Group University of the B"
W04-3204,C98-2122,0,\N,Missing
W06-1669,W06-3814,1,0.224507,"Missing"
W06-1669,W04-0807,0,0.0274208,"Missing"
W06-1669,H05-1052,0,0.051105,"lected as its sense. Most of the unsupervised WSD work has been based on the vector space model, where each example is represented by a vector of features (e.g. the words occurring in the context), and the induced senses are either clusters of examples (Sch¨utze, 1998; Purandare and Pedersen, 2004) or clusters of words (Pantel and Lin, 2002). Recently, V´eronis (V´eronis, 2004) has proposed HyperLex, an application of graph models to WSD based on the small-world properties of cooccurrence graphs. Graph-based methods have gained attention in several areas of NLP, including knowledge-based WSD (Mihalcea, 2005; Navigli and Velardi, 2005) and summarization (Erkan and Radev, 2004; Mihalcea and Tarau, 2004). The HyperLex algorithm presented in (V´eronis, 2004) is entirely corpus-based. It builds a cooccurrence graph for all pairs of words cooccurring in the context of the target word. V´eronis shows that this kind of graph fulfills the properties of small world graphs, and thus possesses highly connected This paper explores the use of two graph algorithms for unsupervised induction and tagging of nominal word senses based on corpora. Our main contribution is the optimization of the free parameters of"
W06-1669,H93-1061,0,0.398686,"rds tasks. The results show that, in spite of the information loss inherent to mapping the induced senses to the gold-standard, the optimization of parameters based on a small sample of nouns carries over to all nouns, performing close to supervised systems in the lexical sample task and yielding the second-best WSD systems for the Senseval-3 all-words task. 1 Introduction Word sense disambiguation (WSD) is a key enabling-technology. Supervised WSD techniques are the best performing in public evaluations, but need large amounts of hand-tagged data. Existing hand-annotated corpora like SemCor (Miller et al., 1993), which is annotated with WordNet senses (Fellbaum, 1998) allow for a small improvement over the simple most frequent sense heuristic, as attested in the all-words track of the last Senseval competition (Snyder and Palmer, 2004). In theory, larger amounts of training data (SemCor has approx. 700K words) would improve the performance of supervised WSD, but no current project exists to provide such an expensive resource. Supervised WSD is based on the “fixed-list of senses” paradigm, where the senses for a target word are a closed list coming from a dic1 Unsupervised WSD approaches prefer the te"
W06-1669,W05-0605,0,0.0149339,"et, and then measured the quality of the mapping. More recently, tagged corpora have been used to map the induced senses, and then compare the systems over publicly available benchmarks (Puran2.3 Using hubs for WSD Once the hubs that represent the senses of the word are selected (following any of the methods presented in the last section), each of them is linked to the target word with edges weighting 0, and the Minimum Spanning Tree (MST) of the whole graph is calculated and stored. 5 As G is undirected, the in-degree of a vertex v is equal to its out-degree. 587 is. dare and Pedersen, 2004; Niu et al., 2005; Agirre et al., 2006), which offers the advantage of comparing to other systems, but converts the whole system into semi-supervised. See Section 5 for more details on these systems. Note that the mapping introduces noise and information loss, which is a disadvantage when comparing to other systems that rely on the gold-standard senses. Yet another possibility is to evaluate the induced senses against a gold standard as a clustering task. Induced senses are clusters, gold standard senses are classes, and measures from the clustering literature like entropy or purity can be used. In this case t"
W06-1669,W04-2406,0,0.432566,"duce word senses directly from the corpus. Typical unsupervised WSD systems involve clustering techniques, which group together similar examples. Given a set of induced clusters (which represent word uses or senses1 ), each new occurrence of the target word will be compared to the clusters and the most similar cluster will be selected as its sense. Most of the unsupervised WSD work has been based on the vector space model, where each example is represented by a vector of features (e.g. the words occurring in the context), and the induced senses are either clusters of examples (Sch¨utze, 1998; Purandare and Pedersen, 2004) or clusters of words (Pantel and Lin, 2002). Recently, V´eronis (V´eronis, 2004) has proposed HyperLex, an application of graph models to WSD based on the small-world properties of cooccurrence graphs. Graph-based methods have gained attention in several areas of NLP, including knowledge-based WSD (Mihalcea, 2005; Navigli and Velardi, 2005) and summarization (Erkan and Radev, 2004; Mihalcea and Tarau, 2004). The HyperLex algorithm presented in (V´eronis, 2004) is entirely corpus-based. It builds a cooccurrence graph for all pairs of words cooccurring in the context of the target word. V´eroni"
W06-1669,J98-1004,0,0.750175,"Missing"
W06-1669,W04-0811,0,0.038389,"rming close to supervised systems in the lexical sample task and yielding the second-best WSD systems for the Senseval-3 all-words task. 1 Introduction Word sense disambiguation (WSD) is a key enabling-technology. Supervised WSD techniques are the best performing in public evaluations, but need large amounts of hand-tagged data. Existing hand-annotated corpora like SemCor (Miller et al., 1993), which is annotated with WordNet senses (Fellbaum, 1998) allow for a small improvement over the simple most frequent sense heuristic, as attested in the all-words track of the last Senseval competition (Snyder and Palmer, 2004). In theory, larger amounts of training data (SemCor has approx. 700K words) would improve the performance of supervised WSD, but no current project exists to provide such an expensive resource. Supervised WSD is based on the “fixed-list of senses” paradigm, where the senses for a target word are a closed list coming from a dic1 Unsupervised WSD approaches prefer the term ’word uses’ to ’word senses’. In this paper we use them interchangeably to refer to both the induced clusters, and to the word senses from some reference lexicon. 585 Proceedings of the 2006 Conference on Empirical Methods in"
W06-1669,W04-3252,0,\N,Missing
W06-3814,W04-0807,0,0.0444102,"es” envisioned in (Cruse, 2000). We now think that the idea of having many micro-senses is very attractive for further exploration, especially if we are able to organize them into coarser hubs. 6.3 Comparison to related work Table 4 shows the performance of different systems on the nouns of the S3LS benchmark. When not reported separately, we obtained the results for nouns running the official scorer program on the filtered results, as available in the S3LS web page. The second column shows the type of system (supervised, unsupervised). We include three supervised systems, the winner of S3LS (Mihalcea et al., 2004), an in-house system (kNN-all, CITATION OMITTED) which uses optimized kNN, and the same in-house system restricted to bag-of-words features only (kNN-bow), i.e. discarding other local features like bigrams or trigrams (which is what most unsupervised systems do). The table shows that we are one point from the bag-ofwords classifier kNN-bow, which is an impressive result if we take into account the information loss of the mapping step and that we tuned our parameters on a different set of words. The full kNN system is state-of-the-art, only 4 points below the S3LS win95 System S3LS-best kNN-all"
W06-3814,H93-1061,0,0.0169005,"e). Our results for nouns show that thanks to the optimization of parameters and the mapping method, HyperLex obtains results close to supervised systems using the same kind of bag-ofwords features. Given the information loss inherent in any mapping step and the fact that the parameters were tuned for another set of words, these are very interesting results. 1 Introduction Word sense disambiguation (WSD) is a key enabling technology. Supervised WSD techniques are the best performing in public evaluations, but need large amounts of hand-tagging data. Existing handannotated corpora like SemCor (Miller et al., 1993), which is annotated with WordNet senses (Fellbaum, 1998) allow for a small improvement over the simple most frequent sense heuristic, as attested in the allwords track of the last Senseval competition (Snyder and Palmer, 2004). In theory, larger amounts of training data (SemCor has approx. 500M words) would improve the performance of supervised WSD, but no current project exists to provide such an expensive resource. Supervised WSD is based on the “fixed-list of senses” paradigm, where the senses for a target word are a closed list coming from a dictionary or lexicon. Lexicographers and seman"
W06-3814,W05-0605,0,0.200491,"in a lexicon. Pantel and Lin (2002) automatically map the senses to WordNet, and then measure the quality of the mapping. More recently, the mapping has been used to test the system on publicly available benchmarks (Purandare and Pedp1 p2 p3 p4 p5 p6 p7 Default value 5 10 0.9 4 6 0.8 0.001 p180 Range Best 2-3 2 3-4 3 0.7-0.9 0.7 4 4 6-7 6 0.5-0.8 0.6 0.0005-0.001 0.0009 p1800 Range Best 1-3 2 2-4 3 0.5-0.7 0.5 4 4 3-7 3 0.4-0.8 0.7 0.0005-0.001 0.0009 p6700 Range Best 1-3 1 2-4 3 0.3-0.7 0.4 4 4 1-7 1 0.6-0.95 0.95 0.0009-0.003 0.001 Table 1: Parameters of the HyperLex algorithm ersen, 2004; Niu et al., 2005). See Section 6 for more details on these systems. Yet another possibility is to evaluate the induced senses against a gold standard as a clustering task. Induced senses are clusters, gold standard senses are classes, and measures from the clustering literature like entropy or purity can be used. As we wanted to focus on the comparison against a standard data-set, we decided to leave aside this otherwise interesting option. In this section we present a framework for automatically evaluating unsupervised WSD systems against publicly available hand-tagged corpora. The framework uses three data s"
W06-3814,W04-2406,0,0.126924,"r instance, senses are dense regions in a continuum (Cruse, 2000). Unsupervised WSD has followed this line of thinking, and tries to induce word senses directly from the corpus. Typical unsupervised WSD systems involve clustering techniques, which group together similar examples. Given a set of induced clusters (which represent word uses or senses1 ), each new occurrence of the target word will be compared to the clusters and the most similar cluster will be selected as its sense. Most of the unsupervised WSD work has been based on the vector space model (Sch¨utze, 1998; Pantel and Lin, 2002; Purandare and Pedersen, 2004), where each example is represented by a vector of features (e.g. the words occurring in the context). Recently, V´eronis (V´eronis, 2004) has 1 Unsupervised WSD approaches prefer the term ’word uses’ to ’word senses’. In this paper we use them interchangeably to refer to both the induced clusters, and to the word senses from some reference lexicon. 89 Workshop on TextGraphs, at HLT-NAACL 2006, pages 89–96, c New York City, June 2006. 2006 Association for Computational Linguistics proposed HyperLex, an application of graph models to WSD based on the small-world properties of cooccurrence graph"
W06-3814,J98-1004,0,0.775251,"Missing"
W06-3814,W04-0811,0,0.0472142,"ss inherent in any mapping step and the fact that the parameters were tuned for another set of words, these are very interesting results. 1 Introduction Word sense disambiguation (WSD) is a key enabling technology. Supervised WSD techniques are the best performing in public evaluations, but need large amounts of hand-tagging data. Existing handannotated corpora like SemCor (Miller et al., 1993), which is annotated with WordNet senses (Fellbaum, 1998) allow for a small improvement over the simple most frequent sense heuristic, as attested in the allwords track of the last Senseval competition (Snyder and Palmer, 2004). In theory, larger amounts of training data (SemCor has approx. 500M words) would improve the performance of supervised WSD, but no current project exists to provide such an expensive resource. Supervised WSD is based on the “fixed-list of senses” paradigm, where the senses for a target word are a closed list coming from a dictionary or lexicon. Lexicographers and semanticists have long warned about the problems of such an approach, where senses are listed separately as discrete entities, and have argued in favor of more complex representations, where, for instance, senses are dense regions i"
W09-2420,C08-1003,1,0.83081,"Missing"
W09-2420,W06-1615,0,0.185914,"Missing"
W09-2420,P07-1007,0,0.123157,"Missing"
W09-2420,W04-3237,0,0.0779774,"Missing"
W09-2420,P07-1033,0,0.148042,"Missing"
W09-2420,W00-1322,0,0.0374768,"Missing"
W09-2420,S01-1004,0,0.385256,"The difficulties found by supervised systems to adapt might change the way we assess the strengths and weaknesses of supervised and knowledgebased WSD systems. Unfortunately, all existing evaluation datasets for specific domains are lexical-sample corpora. With this paper we want to motivate the creation of an allwords test dataset for WSD on the environment domain in several languages, and present the overall design of this SemEval task. 1 Introduction Word Sense Disambiguation (WSD) competitions have focused on general domain texts, as attested in the last Senseval and Semeval competitions (Kilgarriff, 2001; Mihalcea et al., 2004; Pradhan et al., 2007). Specific domains pose fresh challenges to WSD systems: the context in which the senses occur might change, distributions and predominant senses vary, some words tend to occur in fewer senses in specific domains, and new senses and terms might be involved. Both supervised and knowledge-based systems are affected by these issues: while the first suffer from different context and sense priors, the later suffer from lack of coverage of domain-related words and information. 123 Domain adaptation of supervised techniques is a hot issue in Natural Langu"
W09-2420,H05-1053,0,0.676819,"ated words and information. 123 Domain adaptation of supervised techniques is a hot issue in Natural Language Processing, including Word Sense Disambiguation. Supervised Word Sense Disambiguation systems trained on general corpora are known to perform worse when applied to specific domains (Escudero et al., 2000; Mart´ınez and Agirre, 2000), and domain adaptation techniques have been proposed as a solution to this problem with mixed results. Current research on applying WSD to specific domains has been evaluated on three available lexicalsample datasets (Ng and Lee, 1996; Weeber et al., 2001; Koeling et al., 2005). This kind of dataset contains hand-labeled examples for a handful of selected target words. As the systems are evaluated on a few words, the actual performance of the systems over complete texts can not be measured. Differences in behavior of WSD systems when applied to lexical-sample and all-words datasets have been observed on previous Senseval and Semeval competitions (Kilgarriff, 2001; Mihalcea et al., 2004; Pradhan et al., 2007): supervised systems attain results on the high 80’s and beat the most frequent baseline by a large margin for lexical-sample datasets, but results on the all-wo"
W09-2420,W04-0807,0,0.189564,"ound by supervised systems to adapt might change the way we assess the strengths and weaknesses of supervised and knowledgebased WSD systems. Unfortunately, all existing evaluation datasets for specific domains are lexical-sample corpora. With this paper we want to motivate the creation of an allwords test dataset for WSD on the environment domain in several languages, and present the overall design of this SemEval task. 1 Introduction Word Sense Disambiguation (WSD) competitions have focused on general domain texts, as attested in the last Senseval and Semeval competitions (Kilgarriff, 2001; Mihalcea et al., 2004; Pradhan et al., 2007). Specific domains pose fresh challenges to WSD systems: the context in which the senses occur might change, distributions and predominant senses vary, some words tend to occur in fewer senses in specific domains, and new senses and terms might be involved. Both supervised and knowledge-based systems are affected by these issues: while the first suffer from different context and sense priors, the later suffer from lack of coverage of domain-related words and information. 123 Domain adaptation of supervised techniques is a hot issue in Natural Language Processing, includi"
W09-2420,H93-1061,0,0.37412,"cific domain. Good results in this setting would show that supervised domain adaptation is working, and that generic WSD systems can be supplemented with hand-tagged examples from the target domain. There is an additional setting, where a generic WSD system is supplemented with untagged examples from the domain. Good results in this setting would show that semi-supervised domain adaptation works, and that generic WSD systems can be supplemented with untagged examples from the target domain in order to improve their results. Most of current all-words generic supervised WSD systems take SemCor (Miller et al., 1993) as their source corpus, i.e. they are trained on SemCor examples and then applied to new examples. SemCor is the largest publicly available annotated corpus. It’s mainly a subset of the Brown Corpus, plus the novel The Red Badge of Courage. The Brown corpus is balanced, yet not from the general domain, as it comprises 500 documents drawn from different domains, each approximately 2000 words long. Although the Brown corpus is balanced, SemCor is not, as the documents were not chosen at random. 4 State-of-the-art in WSD for specific domains Initial work on domain adaptation for WSD systems show"
W09-2420,P96-1006,0,0.698695,"fer from lack of coverage of domain-related words and information. 123 Domain adaptation of supervised techniques is a hot issue in Natural Language Processing, including Word Sense Disambiguation. Supervised Word Sense Disambiguation systems trained on general corpora are known to perform worse when applied to specific domains (Escudero et al., 2000; Mart´ınez and Agirre, 2000), and domain adaptation techniques have been proposed as a solution to this problem with mixed results. Current research on applying WSD to specific domains has been evaluated on three available lexicalsample datasets (Ng and Lee, 1996; Weeber et al., 2001; Koeling et al., 2005). This kind of dataset contains hand-labeled examples for a handful of selected target words. As the systems are evaluated on a few words, the actual performance of the systems over complete texts can not be measured. Differences in behavior of WSD systems when applied to lexical-sample and all-words datasets have been observed on previous Senseval and Semeval competitions (Kilgarriff, 2001; Mihalcea et al., 2004; Pradhan et al., 2007): supervised systems attain results on the high 80’s and beat the most frequent baseline by a large margin for lexica"
W09-2420,S07-1016,0,0.278613,"ems to adapt might change the way we assess the strengths and weaknesses of supervised and knowledgebased WSD systems. Unfortunately, all existing evaluation datasets for specific domains are lexical-sample corpora. With this paper we want to motivate the creation of an allwords test dataset for WSD on the environment domain in several languages, and present the overall design of this SemEval task. 1 Introduction Word Sense Disambiguation (WSD) competitions have focused on general domain texts, as attested in the last Senseval and Semeval competitions (Kilgarriff, 2001; Mihalcea et al., 2004; Pradhan et al., 2007). Specific domains pose fresh challenges to WSD systems: the context in which the senses occur might change, distributions and predominant senses vary, some words tend to occur in fewer senses in specific domains, and new senses and terms might be involved. Both supervised and knowledge-based systems are affected by these issues: while the first suffer from different context and sense priors, the later suffer from lack of coverage of domain-related words and information. 123 Domain adaptation of supervised techniques is a hot issue in Natural Language Processing, including Word Sense Disambigu"
W09-2420,rose-etal-2002-reuters,0,0.0411856,"d a total of 192,800 occurrences of these words were tagged with WordNet 1.5 senses, more than 1,000 instances per word in average. The examples from BC comprise 78,080 occurrences of word senses, and examples from WSJ consist on 114,794 occurrences. In domain adaptation experiments, the Brown Corpus examples play the role of general corpora, and the examples from the WSJ play the role of domain-specific examples. Koeling et al. (2005) present a corpus were the examples are drawn from the balanced B NC corpus (Leech, 1992) and the S PORTS and F INANCES sections of the newswire Reuters corpus (Rose et al., 2002), comprising around 300 examples (roughly 100 from each of those corpora) for each of the 41 nouns. The nouns were selected because they were 124 salient in either the S PORTS or F INANCES domains, or because they had senses linked to those domains. The occurrences were hand-tagged with the senses from WordNet version 1.7.1 (Fellbaum, 1998). In domain adaptation experiments the B NC examples play the role of general corpora, and the F INANCES and S PORTS examples the role of two specific domain corpora. Finally, a dataset for biomedicine was developed by Weeber et al. (2001), and has been used"
W09-2420,D08-1105,0,0.0322595,"Missing"
W09-2420,W04-0811,0,\N,Missing
W09-2420,vossen-etal-2008-kyoto,1,\N,Missing
W09-2420,E09-1006,1,\N,Missing
W09-2420,E09-1005,1,\N,Missing
W09-2420,E09-1045,0,\N,Missing
W09-2420,W00-0901,0,\N,Missing
W09-2420,W00-1326,1,\N,Missing
W09-2420,J07-4005,0,\N,Missing
W09-2420,W08-2114,0,\N,Missing
W09-2420,S07-1097,0,\N,Missing
W09-3206,N09-1003,1,0.631483,"Missing"
W09-3206,J06-1003,0,0.41968,"ssful measure, Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), treats each article as its own dimension in a vector space. Texts are compared by first projecting them into the space of Wikipedia articles and then comparing the resulting vectors. In addition to article text, Wikipedia stores a great deal of information about the relationships between the articles in the form of hyperlinks, info boxes, and category pages. Despite a long history of research demonstrating the effectiveness of incorporating link information into relatedness measures based on the WordNet graph (Budanitsky and Hirst, 2006), previous work on Wikipedia has made limited use of this relationship information, using only category links (Bunescu and Pasca, 2006) or just the actual links in a page (Gabrilovich and Markovitch, 2007; Milne and Witten, 2008). In this work, we combine previous approaches by converting Wikipedia into a graph, mapping input texts into the graph, and performing random walks based on Personalized PageRank (Haveliwala, 2002) to obtain stationary distributions that characterize each text. Semantic relatedness between two texts is computed by comparing their distributions. In contrast to previous"
W09-3206,E06-1002,0,0.0140215,"pace. Texts are compared by first projecting them into the space of Wikipedia articles and then comparing the resulting vectors. In addition to article text, Wikipedia stores a great deal of information about the relationships between the articles in the form of hyperlinks, info boxes, and category pages. Despite a long history of research demonstrating the effectiveness of incorporating link information into relatedness measures based on the WordNet graph (Budanitsky and Hirst, 2006), previous work on Wikipedia has made limited use of this relationship information, using only category links (Bunescu and Pasca, 2006) or just the actual links in a page (Gabrilovich and Markovitch, 2007; Milne and Witten, 2008). In this work, we combine previous approaches by converting Wikipedia into a graph, mapping input texts into the graph, and performing random walks based on Personalized PageRank (Haveliwala, 2002) to obtain stationary distributions that characterize each text. Semantic relatedness between two texts is computed by comparing their distributions. In contrast to previous work, we explore the use of all these link types when conComputing semantic relatedness of natural language texts is a key component o"
W09-3206,D07-1061,1,0.852577,"information not present in the page text. 2 PageRank it is chosen from a nonuniform distribution of nodes, specified by a teleport vector. The final weight of node i represents the proportion of time the random particle spends visiting it after a sufficiently long time, and corresponds to that node’s structural importance in the graph. Because the resulting vector is the stationary distribution of a Markov chain, it is unique for a particular walk formulation. As the teleport vector is nonuniform, the stationary distribution will be biased towards specific parts of the graph. In the case of (Hughes and Ramage, 2007) and (Agirre and Soroa, 2009), the teleport vector is used to reflect the input texts to be compared, by biasing the stationary distribution towards the neighborhood of each word’s mapping. The computation of relatedness for a word pair can be summarized in three steps: First, each input word is mapped with to its respective synsets in the graph, creating its teleport vector. In the case words with multiple synsets (senses), the synsets are weighted uniformly. Personalized PageRank is then executed to compute the stationary distribution for each word, using their respective teleport vectors. F"
W09-3206,E09-1005,1,\N,Missing
W10-3301,alvez-etal-2008-complete,1,\N,Missing
W10-3301,E09-1005,1,\N,Missing
W13-2701,P10-1158,0,0.0225347,"e digital content in the form of exhibitions, tours and trails. M¨akel¨a et al. (2007) describe a system which utilises semantically annotated content to generate personalised ‘exhibitions’ from a structured narrative-based search query. Similarly, Zdrahal et al. (2008) demonstrate how pathways can be generated through a collection of semantically related documents to provide a means of exploration, using non-NLP clustering and path creation techniques. Sophisticated approaches such as linear programming and evolutionary algorithms have also been proposed for generating summaries and stories (McIntyre and Lapata, 2010; Woodsend and Lapata, 2010). In contrast, Wang et al. (2007) use a recommender system approach to generate museum tours on the basis of ratings stored within a dynamic user model, and Pechenizkiy and Calders (2007) propose the additional use of data mining techniques on log data to improve this type of tour personalisation. Initial user requirements interviews with 22 expert users in the heritage, education and professional domains found a strong affinity with the path metaphor, revealing a range of different interpretations of what it means in the CH context and how they could be employed in"
W13-2701,P10-1058,0,0.0293729,"rm of exhibitions, tours and trails. M¨akel¨a et al. (2007) describe a system which utilises semantically annotated content to generate personalised ‘exhibitions’ from a structured narrative-based search query. Similarly, Zdrahal et al. (2008) demonstrate how pathways can be generated through a collection of semantically related documents to provide a means of exploration, using non-NLP clustering and path creation techniques. Sophisticated approaches such as linear programming and evolutionary algorithms have also been proposed for generating summaries and stories (McIntyre and Lapata, 2010; Woodsend and Lapata, 2010). In contrast, Wang et al. (2007) use a recommender system approach to generate museum tours on the basis of ratings stored within a dynamic user model, and Pechenizkiy and Calders (2007) propose the additional use of data mining techniques on log data to improve this type of tour personalisation. Initial user requirements interviews with 22 expert users in the heritage, education and professional domains found a strong affinity with the path metaphor, revealing a range of different interpretations of what it means in the CH context and how they could be employed in an online environment to en"
W13-2701,agirre-etal-2012-matching,1,\N,Missing
W13-2701,L12-1000,0,\N,Missing
W14-4704,E09-1005,1,0.770643,"ques consider a given Knowledge Base (KB) as a graph, where vertices represent KB concepts and relations among concepts are represented by edges. For this particular task we represented WikiPedia as a graph, where articles are the vertices and links between articles are the edges. Contrary to other work using Wikipedia links (Gabrilovich and Markovitch, 2007; Milne and Witten, 2008), the use of the whole graph allows to apply algorithms that take into account the whole structure of Wikipedia. We applied PageRank and Personalized PageRank on the Wikipedia graph using freely available software (Agirre and Soroa, 2009; Agirre et al., 2014)3 . The PageRank algorithm (Brin and Page, 1998) ranks the vertices in a graph according to their relative structural importance. The main idea of PageRank is that whenever a link from vi to vj exists in a graph, a vote from node i to node j is produced, and hence the rank of node j increases. Besides, the strength of the vote from i to j also depends on the rank of node i: the more important node i is, the more strength its votes will have. Alternatively, PageRank can also be viewed as the result of a random walk process, where the final rank of node i represents the pro"
W14-4704,N09-1003,1,0.888771,"Disambiguation or Information Extraction, and other related areas like Information Retrieval. Most of the proposed techniques are evaluated over manually curated word similarity datasets like WordSim353 (Finkelstein et al., 2002), in which the weights returned by the systems for word pairs are compared with human ratings. The techniques used to solve this problem can be roughly classified into two main categories: those relying on pre-existing knowledge resources (thesauri, semantic networks, taxonomies or encyclopedias) (Alvarez and Lim, 2007; Yang and Powers, 2005; Hughes and Ramage, 2007; Agirre et al., 2009; Agirre et al., 2010) and those inducing distributional properties of words from corpora (Sahami and Heilman, 2006; Chen et al., 2006; Bollegala et al., 2007; Agirre et al., 2009; Mikolov et al., 2013). Our main objective when participating in the CogAlex shared task was to check how a sample of each kind of technique would cope with the task. We thus selected one of the best corpus-based models to date and another approach based on random walks over WordNet and Wikipedia. 2 Word Embeddings Neural Networks have become quite a useful tool in NLP on the last years, specially in semantics. A lot"
W14-4704,agirre-etal-2010-exploring,1,0.703945,"formation Extraction, and other related areas like Information Retrieval. Most of the proposed techniques are evaluated over manually curated word similarity datasets like WordSim353 (Finkelstein et al., 2002), in which the weights returned by the systems for word pairs are compared with human ratings. The techniques used to solve this problem can be roughly classified into two main categories: those relying on pre-existing knowledge resources (thesauri, semantic networks, taxonomies or encyclopedias) (Alvarez and Lim, 2007; Yang and Powers, 2005; Hughes and Ramage, 2007; Agirre et al., 2009; Agirre et al., 2010) and those inducing distributional properties of words from corpora (Sahami and Heilman, 2006; Chen et al., 2006; Bollegala et al., 2007; Agirre et al., 2009; Mikolov et al., 2013). Our main objective when participating in the CogAlex shared task was to check how a sample of each kind of technique would cope with the task. We thus selected one of the best corpus-based models to date and another approach based on random walks over WordNet and Wikipedia. 2 Word Embeddings Neural Networks have become quite a useful tool in NLP on the last years, specially in semantics. A lot of models have been d"
W14-4704,P14-1023,0,0.0135501,"in NLP on the last years, specially in semantics. A lot of models have been developed, but all of them share two characteristics: they learn meaning from non-labeled corpora and represent meaning in a distributional way. These models learn the meaning of words from corpora, and they represent it distributionally by the so-called embeddings. This embeddings are low-dimensional and dense vectors composed by integers, where the dimensions are latent semantic features of words. We have used the Mikolov model (Mikolov et al., 2013) for this task, due to its effectiveness in similarity experiments (Baroni et al., 2014). This neural network reduces the computational complexity of previous architectures by deleting the hidden layer, and also, it’s able to train with larger corpora (more than 109 words) and extract embeddings with larger dimensionality. This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 31 Zock/Rapp/Huang (eds.): Proceedings of the 4th Workshop on Cognitive Aspects of the Lexicon, pages 31–34, Dublin, Ireland, August 23, 2014. The Miko"
W14-4704,J06-1003,0,0.194346,"Missing"
W14-4704,P06-1127,0,0.040705,"ated over manually curated word similarity datasets like WordSim353 (Finkelstein et al., 2002), in which the weights returned by the systems for word pairs are compared with human ratings. The techniques used to solve this problem can be roughly classified into two main categories: those relying on pre-existing knowledge resources (thesauri, semantic networks, taxonomies or encyclopedias) (Alvarez and Lim, 2007; Yang and Powers, 2005; Hughes and Ramage, 2007; Agirre et al., 2009; Agirre et al., 2010) and those inducing distributional properties of words from corpora (Sahami and Heilman, 2006; Chen et al., 2006; Bollegala et al., 2007; Agirre et al., 2009; Mikolov et al., 2013). Our main objective when participating in the CogAlex shared task was to check how a sample of each kind of technique would cope with the task. We thus selected one of the best corpus-based models to date and another approach based on random walks over WordNet and Wikipedia. 2 Word Embeddings Neural Networks have become quite a useful tool in NLP on the last years, specially in semantics. A lot of models have been developed, but all of them share two characteristics: they learn meaning from non-labeled corpora and represent m"
W14-4704,D07-1061,0,0.0286782,"al Entailment, Word Sense Disambiguation or Information Extraction, and other related areas like Information Retrieval. Most of the proposed techniques are evaluated over manually curated word similarity datasets like WordSim353 (Finkelstein et al., 2002), in which the weights returned by the systems for word pairs are compared with human ratings. The techniques used to solve this problem can be roughly classified into two main categories: those relying on pre-existing knowledge resources (thesauri, semantic networks, taxonomies or encyclopedias) (Alvarez and Lim, 2007; Yang and Powers, 2005; Hughes and Ramage, 2007; Agirre et al., 2009; Agirre et al., 2010) and those inducing distributional properties of words from corpora (Sahami and Heilman, 2006; Chen et al., 2006; Bollegala et al., 2007; Agirre et al., 2009; Mikolov et al., 2013). Our main objective when participating in the CogAlex shared task was to check how a sample of each kind of technique would cope with the task. We thus selected one of the best corpus-based models to date and another approach based on random walks over WordNet and Wikipedia. 2 Word Embeddings Neural Networks have become quite a useful tool in NLP on the last years, speciall"
W14-4704,N13-1090,0,0.0206542,"353 (Finkelstein et al., 2002), in which the weights returned by the systems for word pairs are compared with human ratings. The techniques used to solve this problem can be roughly classified into two main categories: those relying on pre-existing knowledge resources (thesauri, semantic networks, taxonomies or encyclopedias) (Alvarez and Lim, 2007; Yang and Powers, 2005; Hughes and Ramage, 2007; Agirre et al., 2009; Agirre et al., 2010) and those inducing distributional properties of words from corpora (Sahami and Heilman, 2006; Chen et al., 2006; Bollegala et al., 2007; Agirre et al., 2009; Mikolov et al., 2013). Our main objective when participating in the CogAlex shared task was to check how a sample of each kind of technique would cope with the task. We thus selected one of the best corpus-based models to date and another approach based on random walks over WordNet and Wikipedia. 2 Word Embeddings Neural Networks have become quite a useful tool in NLP on the last years, specially in semantics. A lot of models have been developed, but all of them share two characteristics: they learn meaning from non-labeled corpora and represent meaning in a distributional way. These models learn the meaning of wo"
W14-4704,J14-1003,1,\N,Missing
W15-0114,S01-1001,0,0.0908231,"e granularity; (d) inclusion and exclusion of major feature classes; (e) variable context width (further broken down by part-of-speech of keyword); (f) number of training examples; (g) baseline probability of the most likely sense; (h) sense distributional entropy; (i) number of senses per keyword; (j) divergence between training and test data; (k) degree of (artificially introduced) noise in the training data; (l) the effectiveness of an algorithms confidence rankings. Their analysis was based on the annotated examples for a handful of words, as released in the SensEval2 lexical sample task (Edmonds and Cotton, 2001). In particular they found that the performance of all systems decreased for words with higher number of senses (as opposed to words with few senses) and for those with more uniform distributions of senses (as opposed to words with skewed distributions of senses). The distribution of senses was measured using the entropy of the probability distribution of thePsenses, normalized by the number of senses1 Hr (P ) = H(P )/log2 (#senses), where H(P ) = − i∈senses p(i)log2 p(i) (Yarowsky and Florian, 2002). In this paper we quantify the correlation of those two factors with the performance of a WSD"
W15-0114,ide-etal-2008-masc,0,0.0136978,"s. Our work is also related to (Plank et al., 2014), which showed that multiple crowdsourced annotations of the same item allow to improve the performance in PoS tagging. They incorporate the uncertainty of the annotators into the loss function of the model by measuring the inter-annotator agreement on a small sample of data, with good results. Our work can be seen as preliminary evidence that such a method can be also applied to WSD. 3 Measuring annotation agreement The corpus used in the experiments is a subset of MASC, the Manually Annotated Sub-Corpus of the Open American National Corpus (Ide et al., 2008), which contains a subsidiary word sense sentence corpus consisting of approximately one thousand sentences per word annotated with WordNet 3.0 sense labels (Passonneau et al., 2012). In this work we make use of a publicly available subset of 45 words (17 nouns, 19 verbs and 9 adjectives, see Table 4) that have been annotated, 1000 sentences per target word, using crowdsourcing (Passonneau and Carpenter, 2014). The authors collected between 20 and 25 labels for every sentence. We measured annotation agreement using the multiple annotations in the corpus and calculate the annotation entropy of"
W15-0114,passonneau-etal-2012-masc,0,0.0723407,"incorporate the uncertainty of the annotators into the loss function of the model by measuring the inter-annotator agreement on a small sample of data, with good results. Our work can be seen as preliminary evidence that such a method can be also applied to WSD. 3 Measuring annotation agreement The corpus used in the experiments is a subset of MASC, the Manually Annotated Sub-Corpus of the Open American National Corpus (Ide et al., 2008), which contains a subsidiary word sense sentence corpus consisting of approximately one thousand sentences per word annotated with WordNet 3.0 sense labels (Passonneau et al., 2012). In this work we make use of a publicly available subset of 45 words (17 nouns, 19 verbs and 9 adjectives, see Table 4) that have been annotated, 1000 sentences per target word, using crowdsourcing (Passonneau and Carpenter, 2014). The authors collected between 20 and 25 labels for every sentence. We measured annotation agreement using the multiple annotations in the corpus and calculate the annotation entropy of an example and word sense distribution entropy as follows. In annotation entropy, we use directly the true-category probabilities from Dawid and Sekene’s model (Passonneau and Carpen"
W15-0114,Q14-1025,0,0.0402628,"ch a method can be also applied to WSD. 3 Measuring annotation agreement The corpus used in the experiments is a subset of MASC, the Manually Annotated Sub-Corpus of the Open American National Corpus (Ide et al., 2008), which contains a subsidiary word sense sentence corpus consisting of approximately one thousand sentences per word annotated with WordNet 3.0 sense labels (Passonneau et al., 2012). In this work we make use of a publicly available subset of 45 words (17 nouns, 19 verbs and 9 adjectives, see Table 4) that have been annotated, 1000 sentences per target word, using crowdsourcing (Passonneau and Carpenter, 2014). The authors collected between 20 and 25 labels for every sentence. We measured annotation agreement using the multiple annotations in the corpus and calculate the annotation entropy of an example and word sense distribution entropy as follows. In annotation entropy, we use directly the true-category probabilities from Dawid and Sekene’s model (Passonneau and Carpenter, 2014) associated to each example to measure its entropy (as defined in the previous section). The annotation entropy for a word is the average of the entropy for each example. In sense entropy, on the other hand, we measure th"
W15-0114,E14-1078,0,0.031231,"002). In this paper we quantify the correlation of those two factors with the performance of a WSD system, in order to compare their contribution. In addition, we analyse a new factor, agreement between annotators, which can be used not only to know which words are more difficult, but also to characterize which examples are more difficult to disambiguate. To our knowledge, this is the first work which quantifies the contribution of each of these factors towards the performance on WSD, and the only one which analyses example difficulty for WSD on empirical grounds. Our work is also related to (Plank et al., 2014), which showed that multiple crowdsourced annotations of the same item allow to improve the performance in PoS tagging. They incorporate the uncertainty of the annotators into the loss function of the model by measuring the inter-annotator agreement on a small sample of data, with good results. Our work can be seen as preliminary evidence that such a method can be also applied to WSD. 3 Measuring annotation agreement The corpus used in the experiments is a subset of MASC, the Manually Annotated Sub-Corpus of the Open American National Corpus (Ide et al., 2008), which contains a subsidiary word"
W15-0114,P10-4014,0,0.0363335,"ses Sense entropy Annotation entropy Full model Full interaction R2 0.067 0.105 0.123 0.357 0.330 F-test p = 0.085 p = 0.017 p = 0.011 p = 0.0001 p = 0.002 Table 1: Regression analysis summary. The first three rows refer to the simplest models, where each factor (annotation entropy, sense entropy and number of senses) is taken in isolation. The full model takes all three factors with no interaction, and the full interaction includes all three factors and interactions. R2 and F-test indicate whether the model is fitted. The Word Sense Disambiguation algorithm of choice is It Makes Sense (IMS) (Zhong and Ng, 2010), which reports the best WSD results to date. We used it out-of-the-box, using the default parametrization and built-in feature extraction. The system always returned an answer, so the accuracy, precision, recall and F1 score are equal. As explored in (Yarowsky and Florian, 2002), the variability of the accuracy across words can be related to many factors, including the distribution of senses and the number of senses of the word in the dataset. In this work, we introduce annotation agreement to the analysis. Figure 1 shows how each factor is correlated with the performance of the IMS WSD syste"
W15-0114,N06-2015,0,\N,Missing
W15-1007,agerri-etal-2014-ixa,0,0.0257674,"Missing"
W15-1007,P08-1045,0,0.0890747,"Missing"
W15-1007,P07-2045,0,0.00394272,"Missing"
W15-1007,W10-3707,0,0.0578954,"Missing"
W15-1007,P13-1059,0,\N,Missing
W15-2711,agirre-etal-2006-methodology,1,0.773099,"series of English lexical-sample words from Passonneau et al. (2012), with several annotation rounds. We include the second, third and fourth round of annotation in our experiments. We use on the whole dataset (M ASCEW) pooling all the rounds together, as well as on each round independently, namely MASCE 2, MASCE 3 and M ASCE 4. 6 F NTW The English Twitter FrameNet data of Søgaard et al. (2015). We treat the framename layer as a word-sense layer, and disregard the arguments. 7 E NSST The English supersense-annotated data of Johannsen et al. (2014). 8 E USC The Basque lexical-sample SemCor of Agirre et al. (2006). 9 DASST The Danish supersense-annotated data of Mart´ınez Alonso et al. (2015). Table 1 provides the characteristics of the datasets. The annotation task can be lexical-sample (ls) or all-words (aw). The number of instances is different from the number of sentences for all-words annotation. The type of annotators can be expert (ex) or crowdsourced (cs). The α scores can differ from those reported in the datasets’ documentation given our example-selection criteria. The last two columns describe the target variables of observed agreement (Ao ) and the proportion of low-, midand high-agreement"
W15-2711,J08-4004,0,0.0589102,"Introduction Sense-annotation tasks show less-than-perfect agreement scores. However, variation in agreement is not the result of featureless, white noise in the annotations; Krippendorff (2011) defines disagreement as by chance—caused by unavoidable inconsistencies in annotator behavior—and systematic—caused by properties of the data. Our goal is to predict the agreement of senseannotated examples by examining their linguistic properties. If we can identify properties predictive of low or high agreement, then we can claim that some of the agreement variation in the data is indeed systematic. Artstein and Poesio (2008) provide an interpretation of Kripperdorff’s α coefficient to describe the reliability of a whole annotation task and the way that observed agreement (Ao ) is calculated for each example. Strictly speaking, the value of α only provides an indication of the replicability 2 Related work In their study, Yarowsky and Florian (2002) examine the relation between agreement variation and predictive power of word-sense disambiguation systems, which is later expanded by Lopez de Lacalle and Agirre (2015a). Our work is different in that we do not study the relation between agreement and performance, but"
W15-2711,passonneau-etal-2010-word,0,0.413538,"Missing"
W15-2711,H92-1045,0,0.452662,"f_n c_slength_n c_maxidf_n c_content_proportion_n a_targetfreq_n a_headfreq_n Correlation with Ao Figure 1: Correlation between the numeric-valued features and Ao for MASCC and FNTW properties of the senses such abstractness. Context features can also be expanded by adding information from word sense induction and distributional models. Moreover, if we are to examine agreement variation in full-document (as opposed to sentence-bysentence) annotation, we suggest that documentlevel frequency would help concretize the meaning of a certain word, following the principle of one sense per discourse (Gale et al., 1992). If the numeric prediction of agreement is desirable over classification, a metric like annotation entropy (Lopez de Lacalle and Agirre, 2015a) is worth considering as an alternative measure to Ao , since it an information-theoretical measure that also gives account for distribution skewness. ther all-words or groupings of lexical-sample annotations for different words (e.g. MASC 2 contains examples of fair-j, know-v, land-n, etc.), which means that some of the class- or lemma-dependent features might be swamped by the superposition of features from the other words. Nevertheless, the systems"
W15-2711,passonneau-etal-2012-masc,0,0.0148026,"esearch efforts advocate for models of annotator behavior (Passonneau et al., 2009; Passonneau et al., 2010; Passonneau and Carpenter, 2014; Cohn and Specia, 2013). 3 Data We conduct our study on sense-annotated datasets, keeping only the examples with at least two annotations per item. In the datasets with two annotators and one adjudicator, we disregard adjudications given their potentially different bias. 1 MASCC The English crowdsourced lexicalsample word-sense corpus from Passonneau and Carpenter (2014). 2-5 MASCE * The expert annotations for a series of English lexical-sample words from Passonneau et al. (2012), with several annotation rounds. We include the second, third and fourth round of annotation in our experiments. We use on the whole dataset (M ASCEW) pooling all the rounds together, as well as on each round independently, namely MASCE 2, MASCE 3 and M ASCE 4. 6 F NTW The English Twitter FrameNet data of Søgaard et al. (2015). We treat the framename layer as a word-sense layer, and disregard the arguments. 7 E NSST The English supersense-annotated data of Johannsen et al. (2014). 8 E USC The Basque lexical-sample SemCor of Agirre et al. (2006). 9 DASST The Danish supersense-annotated data of"
W15-2711,S14-1001,1,0.873417,"Missing"
W15-2711,P14-2083,0,0.0814323,"2002) examine the relation between agreement variation and predictive power of word-sense disambiguation systems, which is later expanded by Lopez de Lacalle and Agirre (2015a). Our work is different in that we do not study the relation between agreement and performance, but between example properties and agreement. Mart´ınez Alonso (2013) experiments with prediction of agreement for coarse-sense annotation. Tomuro (2001) uses the disagreement between annotators of two English sense-annotated corpora to provide insights on the relations between synsets, and more recent studies (Jurgens, 2013; Plank et al., 2014; Jurgens, 2014; Lopez de Lacalle and Agirre, 2015b) have empirically tackled 89 Proceedings of the EMNLP 2015 Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics, pages 89–94, c Lisboa, Portugal, 18 September 2015. 2015 Association for Computational Linguistics. stance, we obtain features for a word w and its syntactic parent p in a sentence s, organized in feature groups. The word identities of w and p are not included in the features to keep the models more general. Number of features are in parentheses. Frequency(2) We calculate the frequency of w and p, scaling"
W15-2711,N13-1062,0,0.0538415,"y and Florian (2002) examine the relation between agreement variation and predictive power of word-sense disambiguation systems, which is later expanded by Lopez de Lacalle and Agirre (2015a). Our work is different in that we do not study the relation between agreement and performance, but between example properties and agreement. Mart´ınez Alonso (2013) experiments with prediction of agreement for coarse-sense annotation. Tomuro (2001) uses the disagreement between annotators of two English sense-annotated corpora to provide insights on the relations between synsets, and more recent studies (Jurgens, 2013; Plank et al., 2014; Jurgens, 2014; Lopez de Lacalle and Agirre, 2015b) have empirically tackled 89 Proceedings of the EMNLP 2015 Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics, pages 89–94, c Lisboa, Portugal, 18 September 2015. 2015 Association for Computational Linguistics. stance, we obtain features for a word w and its syntactic parent p in a sentence s, organized in feature groups. The word identities of w and p are not included in the features to keep the models more general. Number of features are in parentheses. Frequency(2) We calculate the frequency"
W15-2711,jurgens-2014-analysis,0,0.0211186,"lation between agreement variation and predictive power of word-sense disambiguation systems, which is later expanded by Lopez de Lacalle and Agirre (2015a). Our work is different in that we do not study the relation between agreement and performance, but between example properties and agreement. Mart´ınez Alonso (2013) experiments with prediction of agreement for coarse-sense annotation. Tomuro (2001) uses the disagreement between annotators of two English sense-annotated corpora to provide insights on the relations between synsets, and more recent studies (Jurgens, 2013; Plank et al., 2014; Jurgens, 2014; Lopez de Lacalle and Agirre, 2015b) have empirically tackled 89 Proceedings of the EMNLP 2015 Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics, pages 89–94, c Lisboa, Portugal, 18 September 2015. 2015 Association for Computational Linguistics. stance, we obtain features for a word w and its syntactic parent p in a sentence s, organized in feature groups. The word identities of w and p are not included in the features to keep the models more general. Number of features are in parentheses. Frequency(2) We calculate the frequency of w and p, scaling by log(rank(x)"
W15-2711,C12-1095,0,0.0256845,"Missing"
W15-2711,W15-0114,1,0.858821,"Missing"
W15-2711,S15-1007,1,0.770663,"Missing"
W15-2711,W15-1806,1,0.706152,"ral annotation rounds. We include the second, third and fourth round of annotation in our experiments. We use on the whole dataset (M ASCEW) pooling all the rounds together, as well as on each round independently, namely MASCE 2, MASCE 3 and M ASCE 4. 6 F NTW The English Twitter FrameNet data of Søgaard et al. (2015). We treat the framename layer as a word-sense layer, and disregard the arguments. 7 E NSST The English supersense-annotated data of Johannsen et al. (2014). 8 E USC The Basque lexical-sample SemCor of Agirre et al. (2006). 9 DASST The Danish supersense-annotated data of Mart´ınez Alonso et al. (2015). Table 1 provides the characteristics of the datasets. The annotation task can be lexical-sample (ls) or all-words (aw). The number of instances is different from the number of sentences for all-words annotation. The type of annotators can be expert (ex) or crowdsourced (cs). The α scores can differ from those reported in the datasets’ documentation given our example-selection criteria. The last two columns describe the target variables of observed agreement (Ao ) and the proportion of low-, midand high-agreement instances, cf. 3.2 for details. 3.1 3.2 Target variable Regression Instance-wise"
W15-2711,D10-1004,0,0.0507306,"Missing"
W15-2711,Q14-1025,0,0.0830639,"Missing"
W15-2711,W09-2402,0,\N,Missing
W15-2711,P13-1004,0,\N,Missing
W15-5707,P13-3023,0,0.0471446,"Missing"
W15-5707,W12-3132,0,0.0582104,"Missing"
W15-5707,W10-1730,0,0.0332396,"Missing"
W15-5707,P09-2037,0,0.0133725,"Lingua/Interset/ Tagset/ES/Conll2009.pm 5 https://github.com/ufal/treex/blob/master/lib/Treex/Block/HamleDT/ES/Harmonize.pm 58 Figure 3: a-level and t-level English analysis. variant given the source t-lemma and formeme, and other contextual information, and it is calculated as a linear combination of two main components: • The discriminative TM (Mareˇcek et al., 2010) is a set of maximum entropy (MaxEnt) models (Berger et al., 1996) trained for each specific source t-lemma and formeme, where the prediction is ˇ based on features extracted from the source tree (Crouse et al., 1998; Zabokrtsk´ y and Popel, 2009). • The dictionary TM is a bilingual dictionary that contains a list of possible translation equivalents based on relative frequencies and no contextual features. Both components are trained on the parallel corpora at the t-level. The final score assigned to each tlemma and formeme in the TMs is calculated through interpolation. For the t-lemmas, weights of 0.5 and 1 are assigned to the dictionary TM and the discriminative TM, respectively. In the case of formemes, the values are reversed. Using these two TMs, we obtain a weighted n-best list of translation variants for each t-lemma and each f"
W15-5707,W08-0325,0,0.0337859,"ucture often move the level of linguistic abstraction a step deeper into semantic roles and relations, which should entail a simpler transfer step because of the greater structural similarity between the deep structures of the source and target languages as compared to the surface realizations; better generalization of the language as it operates on lemmas of content words and grammatical constructions are abstracted with their meaning captured by language-independent attributes; and improved grammaticality of the output given the explicit representation of target-side sentence structure. ˇ ˇ y et al., 2008; Popel and Zabokrtsk´ y, 2010) has emerged as a potential archiTectoMT (Zabokrtsk´ tecture to develop such an approach, together with other deep-transfer systems such as Matxin (Mayor et al., 2011) and the one proposed by Gasser (2012). In contrast to those systems, TectoMT combines linguistic knowledge and statistical techniques, particularly during transfer, and it aims at transfer on the so-called tectogrammatical layer (Hajiˇcov´a, 2000), a layer of deep syntactic dependency trees. In this paper we present a description of the work done to develop a TectoMT system for both directions of E"
W15-5707,zeman-2008-reusable,0,0.0118352,"word forms and term elements) via standard input and output NAF through standard output. The NAF format is a linguistic annotation format designed for complex NLP pipelines (Fokkens et al., 2014). The analyses generated by the ixa-pipes tools follow the AnCora guidelines both for morphological tags and dependency tree structures. This mostly equates to the a-layer in the TectoMT stratification. Therefore, to fully integrate the analyses into Treex and generate the expected a-tree, the analyses had to be mapped to a universal PoS and dependency tags. TectoMT currently uses the Interset tagset (Zeman, 2008) and HamleDT guidelines (Zeman et al., 2014). To implement this mapping, we used existing modules such as the Interset driver for Spanish AnCora Treebank tagset4 by Dan Zeman and Zdenek Zabokrtsky, and the Harmonization Treex block for Spanish AnCora-style dependencies5 by Dan Zeman, Zdenek Zabokrtsky and Martin Popel. On top of these, and in order to form the t-level tree, we used 16 additional blocks: 1. Language-independent blocks. 11 of the blocks were simply reused from the languageindependent set already available in Treex. These mainly re-arrange nodes, mark heads (coordinations, clause"
W16-2332,N03-1017,0,0.00891535,"provide several translation options for each node along with their estimated probability. The best options are then selected using a Hidden Markov Tree Model (HMTM) ˇ with a target-language tree model (Zabokrtsk´ y and Popel, 2009). For this specific task, where we need to work on a specific domain, an extended version of TectoMT was used allowing interpolation of multiple TMs (Rosa et al., 2015). Moses All the systems submitted that were based on Moses have been trained on a phrase-based model by Giza++ or mGiza with “grow-diag-finaland” symmetrization and “msd-bidirectional-fe” reordering (Koehn et al., 2003). For the language pairs where big quantities of domain-specific monolingual data were available along with the generic domain data, separate language models (domain-specific and generic) were interpolated against our ICT domain-specific development set. For LM training and interpolation, the SRILM toolkit (Stolcke, 2002) was used. The method of truecasing has been adopted for several language pairs where it proved useful. 3 TectoMT The deep translation is based on the TectoMT system, an open-source MT system based on the Treex platform for general natural-language processing. TectoMT uses a c"
W16-2332,C10-3009,0,0.0136209,"obabilities in both directions, lexical weightings in both directions, a phrase length penalty, a ”phrase-mslr-fe” lexicalized reordering model and a target language model. As for the language model, a 5-gram model was trained. The weights for the different components were adjusted to optimize BLEU using MERT tuning over the Batch1 development set, with an n-best list of size 100. For the TectoMT system, EU-Treex existing tools were used in order to get the a-layer. Eustagger is a robust and wide coverage morphological analyzer and POS tagger. The dependency parser is based on the MATE-tools (Bjrkelund et al., 2010). Basque models have been trained using the Basque Dependency Treebank (BDT) corpus (Aduriz et al., 2003). Transformation from the a-level analysis into t-level is partially performed with language-independent blocks thanks to the support of Interset (Zeman, 2008). The English-to-Basque TectoMT system uses the PaCo2 and the Batch1 corpora to train two separate translation models, and they are used to create an interpolated list of translation candidates. In addition to that, the terminological equivalences extracted from the localization PO files (VLC, LO and KDE) as well as the domain terms e"
W16-2332,W16-2334,1,0.789389,"Missing"
W16-2332,2005.mtsummit-papers.11,0,0.0123228,"PO files (VLC, LO and KDE) as well as the domain terms extracted from Wikipedia are used to identify domain terms before syntactic analysis and to ensure domain translation on transfer. Finally, an extra module to treat non linguistic elements (URLs, shell commands, ...) has been used to identify the elements that should be maintained untranslated on the output. Both systems were trained using the same training corpora: the 7th version of the Europarl corpus was used for both translation and language modDutch The Moses system for Dutch was trained on the third version of the Europarl corpus (Koehn, 2005) and the in-domain KDE4 Localization data (Tiedemann, 2012). Words are aligned with GIZA++ and tuning was done with MERT. The applied heuristics for the Dutch baselines were set to “grow-diag-final-and” alignment and “msdbidirectional-fe” reordering. For the creation of the language models, IRSTLM was used to train a 5-gram language model with Kneser-Ney smoothing on the monolingual part of the training corpora. For the TectoMT system, the analysis of Dutch input uses the Alpino system (Noord, 2006), a 438 of tectogrammatical trees. Two separate models were trained and interpolated, the first"
W16-2332,P14-5010,0,0.00318097,"ed Moses with the following factors: ENWordForm-BGLemma|Lemma|BGPOStag, where ENWordForm-BGLemma is an English word form when there is no appropriate Bulgarian one, or the Bulgarian lemma; BGPOStag is the appropriate Bulgarian tag representing grammatical features like number, tense, etc. adaptation and MERT training. Batch2 domain corpus was used for testing during development. The Moses system, EU-Moses, uses factored models to allow lemma-based word-alignment. After word alignment, the rest of the training process is based on lowercased word-forms and standard parameters: Stanford CoreNLP (Manning et al., 2014) and Eustagger (Alegria et al., 2002) tools are used for tokenization and lemmatization, MGIZA for word alignment with the ”growdiag-final-and” symmetrization heuristic, a maximum length of 75 tokens per sentence and 5 tokens per phrase, translation probabilities in both directions, lexical weightings in both directions, a phrase length penalty, a ”phrase-mslr-fe” lexicalized reordering model and a target language model. As for the language model, a 5-gram model was trained. The weights for the different components were adjusted to optimize BLEU using MERT tuning over the Batch1 development se"
W16-2332,W15-4101,1,0.800257,"was performed by the Moses tokenizer. No lemmatization or compound splitting was used and the casing was obtained with the Moses truecaser. For the training, a phrase-based model was used with a language model order of 5, with Kneser-Ney smoothing, which was interpolated using the SRILM tool. The word alignment was done with Giza++ on full forms and the final tuning was done using MERT. The Europarl corpus was used for the training data, both as monolingual data for training language models and as parallel data for training the phrase-table. Regarding the English-to-Portuguese TectoMT system (Silva et al., 2015)(Rodrigues et al., 2016a), PT-Treex, in order to get the a-layer the Portuguese system resorted to LX-Suite (Branco and Silva, 2006), a set of pre-existing shallow processing tools for Portuguese that include a sentence segmenter, a tokenizer, a POS tagger, a morphological analyser and a dependency parser, all with state-of-the-art performance. Treex blocks were created to be called and interfaced with these tools. After running the shallow processing tools, the dependency output of the parser is converted into Universal Dependencies (UD) (de Marneffe et al., 2014). These dependencies are then"
W16-2332,W10-1730,1,0.894585,"Missing"
W16-2332,W15-5712,1,0.718195,"ansfer, and synthesis 4 Basque Both English-Basque submissions are trained on the same training corpora. That is, the PaCO2eneu corpus for translation and language modeling, and the in-domain Batch1 corpus for domain 436 tors retrieved from POS tagged, lemmatized parallel corpora; and BG-DeepMoses — a system that also is based on standard factored Moses but the translation is done in two steps: (1) semanticsbased translation of the source language text to a mixed source-target language text which is then (2) translated to the target language via Moses. The latter system builds on Simov et al. (2015). As training data for both systems the following corpora were used: the Setimes parallel corpus, the Europarl parallel corpus and a corpus created on the basis of the documentation of LibreOffice. The corpora are linguistically processed with the IXA2 pipeline for the English part and the BTB pipeline for the Bulgarian. The analyses include POS tagging, lemmatization and WSD, using the UKB system,3 which provides graph-based methods for Word Sense Disambiguation and lexical similarity measurements. For the BG-Moses system, the following factors have been constructed: WordForm|Lemma|POStag. Fo"
W16-2332,H05-1066,0,0.184414,"Missing"
W16-2332,P14-5003,0,0.0466518,"Missing"
W16-2332,2006.jeptalnrecital-invite.2,1,0.754357,"Missing"
W16-2332,tiedemann-2012-parallel,0,0.0377489,"extracted from Wikipedia are used to identify domain terms before syntactic analysis and to ensure domain translation on transfer. Finally, an extra module to treat non linguistic elements (URLs, shell commands, ...) has been used to identify the elements that should be maintained untranslated on the output. Both systems were trained using the same training corpora: the 7th version of the Europarl corpus was used for both translation and language modDutch The Moses system for Dutch was trained on the third version of the Europarl corpus (Koehn, 2005) and the in-domain KDE4 Localization data (Tiedemann, 2012). Words are aligned with GIZA++ and tuning was done with MERT. The applied heuristics for the Dutch baselines were set to “grow-diag-final-and” alignment and “msdbidirectional-fe” reordering. For the creation of the language models, IRSTLM was used to train a 5-gram language model with Kneser-Ney smoothing on the monolingual part of the training corpora. For the TectoMT system, the analysis of Dutch input uses the Alpino system (Noord, 2006), a 438 of tectogrammatical trees. Two separate models were trained and interpolated, the first model with over 1.9 million sentences from Europarl (Koehn,"
W16-2332,L16-1094,1,0.833385,"ag-final-and” alignment and “msdbidirectional-fe” reordering. For the creation of the language models, IRSTLM was used to train a 5-gram language model with Kneser-Ney smoothing on the monolingual part of the training corpora. For the TectoMT system, the analysis of Dutch input uses the Alpino system (Noord, 2006), a 438 of tectogrammatical trees. Two separate models were trained and interpolated, the first model with over 1.9 million sentences from Europarl (Koehn, 2005) and the second model composed of the Batch1, the Microsoft Terminology Collection and ˇ the LibreOffice localization data (Stajner et al., 2016). Each pair of parallel sentences, one in English and one in Portuguese, are analyzed by Treex up to the t-layer level, where each pair of trees are fed into the model. The TectoMT synthesis (Rodrigues et al., 2016b) included other two lexical-semanticsrelated modules, the HideIT and gazetteers. The HideIT module handles entities that do not require translation such as URLs and shell commands. The gazetteers are specialized lexicons that handle the translation of named entities from the ITdomain such as menu items and button names. Finally, synset IDs were used as additional contextual feature"
W16-2332,P09-2037,1,0.925288,"uage-specific additions and distinguishes two levels of syntactic description: and Spanish, Charles University in Prague for Czech, by University of Groningen for Dutch, by University of Lisbon for Portuguese and by IICTBAS of the Bulgarian Academy of Sciences for Bulgarian. For each language two different systems were submitted, corresponding to different phases of the project, namely a phrase-based MT system built using Moses (Koehn et al., 2007), and a system exploiting deep language engineering approaches, that in all the languages but Bulgarian was imˇ plemented using TectoMT (Zabokrtsk´ y and Popel, 2009). For Bulgarian, its second MT system is not based on TectoMT, but on exploiting deep factors in Moses. All 12 systems are constrained, that is trained only on the data provided by the WMT16 IT-task organizers. We present briefly the Moses common setting and the TectoMT structure and then more detailed information for each language system are provided. In the last Section, results based on BLEU and TrueSkill are given and discussed. 2 • Surface dependency syntax (a-layer) – surface dependency trees containing all the tokens in the sentence. • Deep syntax (t-layer) – dependency trees that conta"
W16-2332,W08-0325,0,0.300026,"Missing"
W16-2332,L16-1438,1,0.826183,"Missing"
W16-2332,zeman-2008-reusable,0,0.0263149,"djusted to optimize BLEU using MERT tuning over the Batch1 development set, with an n-best list of size 100. For the TectoMT system, EU-Treex existing tools were used in order to get the a-layer. Eustagger is a robust and wide coverage morphological analyzer and POS tagger. The dependency parser is based on the MATE-tools (Bjrkelund et al., 2010). Basque models have been trained using the Basque Dependency Treebank (BDT) corpus (Aduriz et al., 2003). Transformation from the a-level analysis into t-level is partially performed with language-independent blocks thanks to the support of Interset (Zeman, 2008). The English-to-Basque TectoMT system uses the PaCo2 and the Batch1 corpora to train two separate translation models, and they are used to create an interpolated list of translation candidates. In addition to that, the terminological equivalences extracted from the localization PO files (VLC, LO and KDE) as well as the domain terms extracted from Wikipedia are used to identify domain terms before syntactical analysis and to ensure domain translation on transfer. Finally, an extra module to treat non linguistic elements (URLs, shell commands, ...) has been used, to identify the elements that s"
W16-2332,W15-5711,1,0.91741,"Missing"
W16-2332,de-marneffe-etal-2014-universal,0,\N,Missing
W16-2332,E06-2024,1,\N,Missing
W16-2332,P07-2045,0,\N,Missing
W16-2332,W13-2208,0,\N,Missing
W16-2332,bojar-etal-2012-joy,1,\N,Missing
W16-2332,L16-1441,1,\N,Missing
W16-6405,W12-3132,0,0.045366,"Missing"
W16-6405,W15-3009,0,0.0248943,"Missing"
W16-6405,W08-0325,0,0.0248875,"sults with simple techniques, e.g. force-translating domain-specific expressions. In such approaches, multiword entries are translated as if they were a single token-with-spaces, failing to represent the internal structure which makes TectoMT a powerful translation engine. In this work we enrich source and target multiword terms with syntactic structure, and seamlessly integrate them in the tree-based transfer phase of TectoMT. Our experiments on the IT domain using the Microsoft terminological resource show improvement in Spanish, Basque and Portuguese. 1 Introduction ˇ ˇ TectoMT (Zabokrtsk´ y et al., 2008; Popel and Zabokrtsk´ y, 2010) has emerged as an architecture to develop deep-transfer systems, where the translation step is done a deep level of analysis, in contrast to methods based on surface sequences of words. TectoMT combines linguistic knowledge and statistical techniques, particularly during transfer, and it aims at transfer on the so-called tectogrammatical layer (Hajiˇcov´a, 2000), a layer of deep syntactic dependency trees. In domain adaptation of machine translation, a typical scenario is as follows: there is an MT system trained on large general-domain data, and there is a bili"
W16-6405,zesch-etal-2008-extracting,0,0.0608747,"Missing"
W18-2505,P16-1085,0,0.36085,"Missing"
W18-2505,J14-1003,1,0.870351,"Missing"
W18-2505,E09-1005,1,0.922462,"r, graph-based approaches represent the knowledge base as a graph, and apply several well-known graph analysis algorithms to perform WSD. 1 http://compling.hss.ntu.edu.sg/omw/ 29 Proceedings of Workshop for NLP Open Source Software, pages 29–33 c Melbourne, Australia, July 20, 2018. 2018 Association for Computational Linguistics UKB is a collection of programs which was first released for performing graph-based Word Sense Disambiguation using a pre-existing knowledge base such as WordNet, and attained state-ofthe-art results among knowledge-based systems when evaluated on standard benchmarks (Agirre and Soroa, 2009; Agirre et al., 2014). In addition, UKB has been extended to perform disambiguation of medical entities (Agirre et al., 2010), named-entities (Erbs et al., 2012; Agirre et al., 2015), word similarity (Agirre et al., 2009) and to create knowledge-based word embeddings (Goikoetxea et al., 2015). All programs are open source2,3 and are accompanied by the resources and instructions necessary to reproduce the results. The software is quite popular, with 60 stars and 26 forks in github, as well as more than eight thousand direct downloads from the website since 2011. The software is coded in C++ an"
W18-2505,K16-1006,0,0.155847,"Missing"
W18-2505,H93-1061,0,0.635989,"r as well. The difference is of nearly 10 absolute F1 points overall.5 This decrease could be caused by the fact that Raganato et al. (2017a) did not use sense frequencies. In addition to UKB, the table also reports the best performing knowledge-based systems on this dataset. Raganato et al. (2017a) run several wellknown algorithms when presenting their datasets. We also report (Chaplot and Sakajhutdinov, 2018), tion that describe the frequencies of the associations between a word and its possible senses. The frequencies are often derived from manually sense annotated corpora, such as Semcor (Miller et al., 1993). We use the sense frequency accompanying Wordnet, which, according to the documentation, ”represents the decimal number of times the sense is tagged in various semantic concordance texts”. The frequencies are smoothed adding one to all counts (dict weight smooth). The sense frequency is used when initializing context words, and is also used to produce the final sense weights as a linear combination of sense frequencies and graph-based sense probabilities. The use of sense frequencies with UKB was introduced in (Agirre et al., 2014). 4 Comparison to the state-of-the-art We evaluate UKB on the"
W18-2505,N09-1003,1,0.527001,"oftware, pages 29–33 c Melbourne, Australia, July 20, 2018. 2018 Association for Computational Linguistics UKB is a collection of programs which was first released for performing graph-based Word Sense Disambiguation using a pre-existing knowledge base such as WordNet, and attained state-ofthe-art results among knowledge-based systems when evaluated on standard benchmarks (Agirre and Soroa, 2009; Agirre et al., 2014). In addition, UKB has been extended to perform disambiguation of medical entities (Agirre et al., 2010), named-entities (Erbs et al., 2012; Agirre et al., 2015), word similarity (Agirre et al., 2009) and to create knowledge-based word embeddings (Goikoetxea et al., 2015). All programs are open source2,3 and are accompanied by the resources and instructions necessary to reproduce the results. The software is quite popular, with 60 stars and 26 forks in github, as well as more than eight thousand direct downloads from the website since 2011. The software is coded in C++ and released under the GPL v3.0 license. When UKB was released, the papers specified the optimal parameters for WSD (Agirre and Soroa, 2009; Agirre et al., 2014), as well as other key issues like the underlying knowledge-bas"
W18-2505,Q14-1019,0,0.563686,"Missing"
W18-2505,E17-1010,0,0.461973,"e experiments of (Agirre and Soroa, 2009) somewhat arbitrarily, and never changed afterwards. 2 http://ixa2.si.ehu.eus/ukb 3 https://github.com/asoroa/ukb 30 UKB (this work) UKB (elsewhere)†‡ Chaplot and Sakajhutdinov (2018) ‡ Babelfy (Moro et al., 2014)† MFS Basile et al. (2014)† Banerjee and Pedersen (2003)† All 67.3 57.5 66.9 65.5 65.2 63.7 48.7 S2 68.8 60.6 69.0 67.0 66.8 63.0 50.6 S3 66.1 54.1 66.9 63.5 66.2 63.7 44.5 S07 53.0 42.0 55.6 51.6 55.2 56.7 32.0 S13 68.8 59.0 65.3 66.4 63.0 66.2 53.6 S15 70.3 61.2 69.6 70.3 67.8 64.6 51.0 Table 1: F1 results for knowledge-based systems on the (Raganato et al., 2017a) dataset. Top rows show conflicting results for UKB. † for results reported in (Raganato et al., 2017a), ‡ for results reported in (Chaplot and Sakajhutdinov, 2018). Best results in bold. S2 stands for Senseval-2, S3 for Senseval-3, S07 for Semeval-2007, S13 for Semeval-2013 and S15 for Semeval-2015. Yuan et al. (2016) Raganato et al. (2017b) Iacobacci et al. (2016)† Melamud et al. (2016)† IMS (Zhong and Ng, 2010)† All 71.5 69.9 69.7 69.4 68.8 S2 73.8 72.0 73.3 72.3 72.8 S3 71.8 69.1 69.6 68.2 69.2 S07 63.5 64.8 61.1 61.5 60.0 S13 69.5 66.9 66.7 67.2 65.0 S15 72.6 71.5 70.4 71.7 69.3 Table 2"
W18-2505,C14-1151,0,0.502743,"Missing"
W18-2505,D17-1120,0,0.381741,"e experiments of (Agirre and Soroa, 2009) somewhat arbitrarily, and never changed afterwards. 2 http://ixa2.si.ehu.eus/ukb 3 https://github.com/asoroa/ukb 30 UKB (this work) UKB (elsewhere)†‡ Chaplot and Sakajhutdinov (2018) ‡ Babelfy (Moro et al., 2014)† MFS Basile et al. (2014)† Banerjee and Pedersen (2003)† All 67.3 57.5 66.9 65.5 65.2 63.7 48.7 S2 68.8 60.6 69.0 67.0 66.8 63.0 50.6 S3 66.1 54.1 66.9 63.5 66.2 63.7 44.5 S07 53.0 42.0 55.6 51.6 55.2 56.7 32.0 S13 68.8 59.0 65.3 66.4 63.0 66.2 53.6 S15 70.3 61.2 69.6 70.3 67.8 64.6 51.0 Table 1: F1 results for knowledge-based systems on the (Raganato et al., 2017a) dataset. Top rows show conflicting results for UKB. † for results reported in (Raganato et al., 2017a), ‡ for results reported in (Chaplot and Sakajhutdinov, 2018). Best results in bold. S2 stands for Senseval-2, S3 for Senseval-3, S07 for Semeval-2007, S13 for Semeval-2013 and S15 for Semeval-2015. Yuan et al. (2016) Raganato et al. (2017b) Iacobacci et al. (2016)† Melamud et al. (2016)† IMS (Zhong and Ng, 2010)† All 71.5 69.9 69.7 69.4 68.8 S2 73.8 72.0 73.3 72.3 72.8 S3 71.8 69.1 69.6 68.2 69.2 S07 63.5 64.8 61.1 61.5 60.0 S13 69.5 66.9 66.7 67.2 65.0 S15 72.6 71.5 70.4 71.7 69.3 Table 2"
W18-2505,P10-4014,0,0.698767,"Missing"
W18-2505,N15-1165,1,0.864323,"ociation for Computational Linguistics UKB is a collection of programs which was first released for performing graph-based Word Sense Disambiguation using a pre-existing knowledge base such as WordNet, and attained state-ofthe-art results among knowledge-based systems when evaluated on standard benchmarks (Agirre and Soroa, 2009; Agirre et al., 2014). In addition, UKB has been extended to perform disambiguation of medical entities (Agirre et al., 2010), named-entities (Erbs et al., 2012; Agirre et al., 2015), word similarity (Agirre et al., 2009) and to create knowledge-based word embeddings (Goikoetxea et al., 2015). All programs are open source2,3 and are accompanied by the resources and instructions necessary to reproduce the results. The software is quite popular, with 60 stars and 26 forks in github, as well as more than eight thousand direct downloads from the website since 2011. The software is coded in C++ and released under the GPL v3.0 license. When UKB was released, the papers specified the optimal parameters for WSD (Agirre and Soroa, 2009; Agirre et al., 2014), as well as other key issues like the underlying knowledge-base version, specific set of relations to be used, and method to pre-proce"
