1991.mtsummit-papers.4,W91-0109,1,0.813531,"Missing"
1991.mtsummit-papers.4,W90-0107,1,0.89702,"Missing"
1991.mtsummit-papers.4,W91-0114,1,0.719369,"Missing"
1991.mtsummit-papers.4,E89-1032,0,0.125206,"Missing"
1991.mtsummit-papers.4,P90-1017,0,0.0819618,"Missing"
1991.mtsummit-papers.4,J85-4002,0,0.110244,"Missing"
1991.mtsummit-papers.4,P91-1025,0,0.129776,"Missing"
1991.mtsummit-papers.4,A88-1003,1,0.89245,"Missing"
1991.mtsummit-papers.4,A88-1026,1,0.76461,"Missing"
1991.mtsummit-papers.4,C90-2052,0,\N,Missing
1991.mtsummit-papers.4,A88-1034,0,\N,Missing
1994.amta-1.10,H93-1038,1,0.792898,"Missing"
1994.amta-1.10,1993.tmi-1.4,1,0.733597,"Missing"
1994.amta-1.18,W90-0108,0,0.0148409,"source text and does not, for the most part, include implicit information necessary for target language generation. The job of the inference module is to make that implicit material explicit. Currently, inference is the least developed part of the working system; its only jobs are to reorganize relative clauses and to insert topical items into semantic roles. 3.6 Semantic Ranking (SENSUS) Semantic analysis has now given us a set of candidate meanings, in terms of concepts and relations from our knowledge base. This inventory of concepts is a synthesis of resources like the PENMAN Upper Model (Bateman 1990), ONTOS (Carlson & Nirenburg 1990), the WordNet thesaurus (Miller 1990), and the Longman Dictionary of Contemporary English (Longman-Group 1978). It contains roughly 70,000 terms organized into inheritance networks; (Knight & Luk 1994) describe its construction. The concept inventory and its functional interface are part of a system called SENSUS. The other part is a set of constraints on which concepts are naturally related to which others in our world. Thus, SENSUS is a world model, (mostly) independent of particular natural languages. It can be used to rank candidate meanings produced by an"
1994.amta-1.18,H94-1028,0,0.057023,"Missing"
1994.amta-1.18,J93-2003,0,0.00533199,"mple translations from our system appear in Section 4. 2 System Design: Philosophy At an abstract level, all MT systems are composed of two types of components: transformers (T) and ranker/pruners (R). A T-component takes some input structure and outputs zero or more new structures. The inputs and outputs may be of the same type (e.g., string -> string) or different types (e.g., string —> parse tree). An R-component takes a set of structures of the same type, assigns each structure a score, then prunes away some number of low-scoring structures. For example, the statistical MT system Candide (Brown et al. 1993) operates in a T-R-R fashion. A French string is transformed into many thousands of French-English string pairs. These translations are ranked and pruned first by a translation model, then by an English-only language model. Both rankers are &quot;soft&quot;, never assigning a zero score. The final pruning leaves only a single translation. The LINGSTAT system (Yamron et al. 1994), works with a T-R-T-R design. The first T-component transforms a Japanese sentence into many possible parse trees, as stipulated by a context-free grammar. A statistical ranker chooses one &quot;best&quot; tree and prunes the rest. A seco"
1994.amta-1.18,P85-1008,0,0.00997741,"mantic rules look like: ((NP -> S NP) ((X2 syn form) = (*NOT* rentaidome)) ((X0 sem instance) = rc-modified-object) ((X0 sem head) = (X2 sem)) ((X0 sem rel-mod) = (X1 sem)) (*OR* (((X1 map subject-role) =c X2)) (((X1 map object-role) =c X2)) (((X1 map object2-role) =c X2)))) The current semantic rule base contains at least one rule for every syntactic rule in the grammar. The output of semantic analysis is represented in a number of ways. Inside the analyzer, semantic information takes the form of directed acyclic graphs. For semantic ranking, we view the graphs as lists of assertions, as in (Hobbs 1985). We also use the SPL format of the PENMAN generation system (Penman 1989). Here is a sample output, representing the sentence &quot;the new company plans to launch in February&quot;: (|b-709 |/ (have as a goal| :SENSER (|c-710 |/ |company/business| :Q-MOD (|n-711 |/ |newˆvirgin|)) :PHENOMENON (|f-712 |/ |found, launch| :TEMPORAL-LOCATING (|c-713 |/ |calendar month |:MONTH-INDEX 2) :AGENT |c-710|) :THEME |c-710|) 137 The acquisition of a large Japanese semantic lexicon is a difficult task. We are tackling this problem with a combination of automatic and manual techniques. (Okumura & Hovy 1994) and (Knig"
1994.amta-1.18,P94-1045,0,0.0374657,"un phrase (X2). It also tests an inflectional feature of the clause, inherits syntactic features from the child noun phrase to the parent, and adds new features. HAX outputs a parse forest, represented as a list of constituents with features that were assigned to them during parsing. A full parse may or may not be found. We strive for full parses whenever possible, because we cannot rely on deep semantics to patch everything up. Since one unanticipated word or punctuation mark may splinter the input into four or five pieces, we are working on techniques for word-skipping, inspired in part by (Lavie 1994). Ongoing research project. We are investigating methods to repair trouble spots in grammatical analysis. In particular, we are looking at statistical differences between fully-parsed and not-fully-parsed sentences. These differences include relative distributions of part-of-speech bigrams. Our goal is to provide automatic feedback for grammar development and to identify word skipping or tag repair possibilities for exploitation at runtime. ______________________________________________________________________________ 3.4 Semantic Analyzer (CAPULET) Like our parser HAX, the semantic analyzer C"
1994.amta-1.18,P89-1005,0,0.0136227,"nalyzer (CAPULET) Like our parser HAX, the semantic analyzer CAPULET operates bottom-up. It assigns zero or more semantic interpretations to each syntactic constituent in the HAX parse forest. At the bottom, word meanings are retrieved from a semantic lexicon. Moving up the parse trees, meanings of constituents are computed compositionally from the meanings of their children, according to a database of semantic combination rules. Semantic rules have the same format as syntactic rules; they are keyed to one another via their context-free parts, in the style of (Dowty, Wall, & Peters 1981) and (Moore 1989). Semantic rules look like: ((NP -> S NP) ((X2 syn form) = (*NOT* rentaidome)) ((X0 sem instance) = rc-modified-object) ((X0 sem head) = (X2 sem)) ((X0 sem rel-mod) = (X1 sem)) (*OR* (((X1 map subject-role) =c X2)) (((X1 map object-role) =c X2)) (((X1 map object2-role) =c X2)))) The current semantic rule base contains at least one rule for every syntactic rule in the grammar. The output of semantic analysis is represented in a number of ways. Inside the analyzer, semantic information takes the form of directed acyclic graphs. For semantic ranking, we view the graphs as lists of assertions, as"
1994.amta-1.18,H94-1026,0,0.0215113,"finitive knowledge is missing? There are many approaches to these questions. Our working hypotheses are (1) a great deal of useful knowledge can be extracted from online dictionaries and text; and (2) statistical methods, properly integrated, can effectively fill knowledge gaps until better knowledge bases or linguistic theories arrive. This paper describes completed and ongoing research on these hypotheses. This research is tightly coupled with our development effort on a large-scale Japanese-English MT system, as part of the ARPAsponsored PANGLOSS project (NMSU/CRL, USC/ISI, & CMU/CMT 1994; Nirenburg & Frederking 1994; Knight & Luk 1994). Sample translations from our system appear in Section 4. 2 System Design: Philosophy At an abstract level, all MT systems are composed of two types of components: transformers (T) and ranker/pruners (R). A T-component takes some input structure and outputs zero or more new structures. The inputs and outputs may be of the same type (e.g., string -> string) or different types (e.g., string —> parse tree). An R-component takes a set of structures of the same type, assigns each structure a score, then prunes away some number of low-scoring structures. For example, the statist"
1994.amta-1.18,C92-3168,0,0.0264904,"ract We summarize recent machine translation (MT) research at the Information Sciences Institute of USC, and we describe its application to the development of a Japanese-English newspaper MT system. Our work aims at scaling up grammar-based, knowledge-based MT techniques. This scale-up involves the use of statistical methods, both in acquiring effective knowledge resources and in making reasonable linguistic choices in the face of knowledge gaps. 1 Goals Knowledge-based machine translation (KBMT) techniques have yielded high quality MT systems in narrow problem domains (Nirenburg et al. 1992; Nyberg & Mitamura 1992). This high quality is delivered by algorithms and resources that permit some access to the meaning of texts. But can KBMT be scaled up to unrestricted newspaper articles? We believe it can, provided we address two additional questions: 1. In constructing a KBMT system, how can we acquire knowledge resources (lexical, grammatical, conceptual) on a large scale? 2. In applying a KBMT system, what do we do when definitive knowledge is missing? There are many approaches to these questions. Our working hypotheses are (1) a great deal of useful knowledge can be extracted from online dictionaries and"
1994.amta-1.18,1994.amta-1.23,1,0.809697,"assertions, as in (Hobbs 1985). We also use the SPL format of the PENMAN generation system (Penman 1989). Here is a sample output, representing the sentence &quot;the new company plans to launch in February&quot;: (|b-709 |/ (have as a goal| :SENSER (|c-710 |/ |company/business| :Q-MOD (|n-711 |/ |newˆvirgin|)) :PHENOMENON (|f-712 |/ |found, launch| :TEMPORAL-LOCATING (|c-713 |/ |calendar month |:MONTH-INDEX 2) :AGENT |c-710|) :THEME |c-710|) 137 The acquisition of a large Japanese semantic lexicon is a difficult task. We are tackling this problem with a combination of automatic and manual techniques. (Okumura & Hovy 1994) and (Knight & Luk 1994) describe algorithms for using bilingual dictionaries to propose links between non-English words and concepts in a knowledge base. We have also built an interface called the Acquisitor, which allows a person to identify word-concept links by conceptually tagging words in context. This provides a distribution of senses for each word and potentially offers data for topic-based disambiguation algorithms. This acquisition work requires substantial resources; at present we are working with five part-time knowledge enterers (&quot;acquisitors&quot;). Another issue is handling unknown w"
1994.amta-1.18,H94-1029,0,0.0143834,"ypes (e.g., string —> parse tree). An R-component takes a set of structures of the same type, assigns each structure a score, then prunes away some number of low-scoring structures. For example, the statistical MT system Candide (Brown et al. 1993) operates in a T-R-R fashion. A French string is transformed into many thousands of French-English string pairs. These translations are ranked and pruned first by a translation model, then by an English-only language model. Both rankers are &quot;soft&quot;, never assigning a zero score. The final pruning leaves only a single translation. The LINGSTAT system (Yamron et al. 1994), works with a T-R-T-R design. The first T-component transforms a Japanese sentence into many possible parse trees, as stipulated by a context-free grammar. A statistical ranker chooses one &quot;best&quot; tree and prunes the rest. A second T-component turns that tree into a set of possible English translations, using a bilingual dictionary and word ordering rules. These translations are ranked by a language model similar to the one employed by Candide. 134 KBMT systems typically use a T-T-R-T sequence. First, the source text is turned into a set of syntactic analyses. These analyses are typically not"
1994.amta-1.18,H94-1096,0,\N,Missing
2002.tmi-tutorials.2,J99-4005,1,\N,Missing
2002.tmi-tutorials.2,W01-1408,0,\N,Missing
2002.tmi-tutorials.2,J93-2003,0,\N,Missing
2002.tmi-tutorials.2,2002.tmi-papers.20,0,\N,Missing
2002.tmi-tutorials.2,P97-1047,0,\N,Missing
2002.tmi-tutorials.2,C00-2123,0,\N,Missing
2002.tmi-tutorials.2,1999.mtsummit-1.34,0,\N,Missing
2002.tmi-tutorials.2,P01-1030,1,\N,Missing
2002.tmi-tutorials.2,takezawa-etal-2002-toward,0,\N,Missing
2002.tmi-tutorials.2,2001.mtsummit-papers.22,0,\N,Missing
2003.mtsummit-papers.6,C00-1078,0,0.0349095,"Missing"
2003.mtsummit-papers.6,A00-2018,1,0.515738,"Missing"
2003.mtsummit-papers.6,P01-1017,1,0.892719,"Missing"
2003.mtsummit-papers.6,P97-1003,0,0.237838,"Missing"
2003.mtsummit-papers.6,P01-1030,1,0.879232,"Missing"
2003.mtsummit-papers.6,W01-1406,0,0.0372217,"Missing"
2003.mtsummit-papers.6,P02-1038,0,0.279021,"Missing"
2003.mtsummit-papers.6,P02-1025,0,0.0737252,"Missing"
2003.mtsummit-papers.6,P01-1067,1,0.865271,"Missing"
2003.mtsummit-papers.6,P02-1039,1,0.891478,"Missing"
2003.mtsummit-systems.2,W02-1018,1,0.87931,"Missing"
2003.mtsummit-systems.2,P01-1050,1,\N,Missing
2003.mtsummit-tttt.3,J93-2003,0,0.00748443,"Missing"
2003.mtsummit-tttt.3,C82-1057,0,0.339696,"Missing"
2003.mtsummit-tttt.3,N03-1024,1,0.836485,"Missing"
2004.iwslt-evaluation.9,N03-1017,1,0.0272244,"Missing"
2004.iwslt-evaluation.9,J04-4002,1,\N,Missing
2005.iwslt-1.10,N03-1017,0,0.00591353,"Missing"
2005.iwslt-1.10,P03-1021,0,0.0123824,"Missing"
2005.iwslt-1.10,N04-1035,1,\N,Missing
2005.iwslt-1.10,J93-2004,0,\N,Missing
2005.iwslt-1.10,J04-4004,0,\N,Missing
2005.iwslt-1.10,J03-4003,0,\N,Missing
2005.iwslt-1.10,J04-4002,0,\N,Missing
2005.iwslt-1.10,P05-3025,1,\N,Missing
2005.iwslt-1.10,P05-1033,0,\N,Missing
2005.iwslt-1.10,J03-1002,0,\N,Missing
2005.iwslt-1.10,2004.iwslt-evaluation.9,1,\N,Missing
2006.amta-papers.8,J93-2003,0,0.017409,"Missing"
2006.amta-papers.8,N03-1017,0,0.0147799,"bei r4 ⇓ [jingfang]2 [jibi]1 Figure 2: A synatx-directed translation process for Example (1). VP VBD (r3 ) was VP-C x1 :VBN IN PP → bei x2 x1 x2 :NP-C by which captures the fact that the agent (NP-C, “the police”) and the verb (VBN, “killed”) are always inverted between English and Chinese in a passive voice. Finally, we apply rules r4 and r5 which perform phrasal translations for the two remaining sub-trees in (d), respectively, and get the completed Chinese string in (e). 67 ◦ 2 Previous Work It is helpful to compare this approach with recent efforts in statistical MT. Phrase-based models (Koehn et al., 2003; Och and Ney, 2004) are good at learning local translations that are pairs of (consecutive) sub-strings, but often insufficient in modeling the reorderings of phrases themselves, especially between language pairs with very different word-order. This is because the generative capacity of these models lies within the realm of finite-state machinery (Kumar and Byrne, 2003), which is unable to process nested structures and long-distance dependencies in natural languages. Syntax-based models aim to alleviate this problem by exploiting the power of synchronous rewriting systems. Both Yamada and Kni"
2006.amta-papers.8,koen-2004-pharaoh,0,0.0531818,"end up with 140 English sentences to translate in both dev and test sets. Note that this arrangement makes sure the test set is blind. 6.2 Systems We implemented our system as follows: for each input sentence, we first run Algorithm 1, which returns the 1-best translation and also builds the derivation forest of all translations for this sentence. Then we extract the top-k non-duplicate translated strings from this forest using the algorithm in Section 5.2 and rescore them with the trigram model and the length penalty. We compared our system with a state-of-theart phrase-based system Pharaoh (Koehn, 2004) on the evaluation data. Since the target language is Chinese, we will report character-based BLEU scores instead of word-based to ensure our results are independent of Chinese tokenizations (although our language models are word-based). Feature weights of both systems are tuned for BLEU-4 (up to 4-grams) on the dev set. For Pharaoh, we use the standard minimum error-rate training (Och, 2003) (David Chiang’s implementation); and for our system, since there are only two independent features (as we always fix α = 1), we use a simple grid-based line-optimization along the language-model weight ax"
2006.amta-papers.8,N03-1019,0,0.00473659,"ranslations for the two remaining sub-trees in (d), respectively, and get the completed Chinese string in (e). 67 ◦ 2 Previous Work It is helpful to compare this approach with recent efforts in statistical MT. Phrase-based models (Koehn et al., 2003; Och and Ney, 2004) are good at learning local translations that are pairs of (consecutive) sub-strings, but often insufficient in modeling the reorderings of phrases themselves, especially between language pairs with very different word-order. This is because the generative capacity of these models lies within the realm of finite-state machinery (Kumar and Byrne, 2003), which is unable to process nested structures and long-distance dependencies in natural languages. Syntax-based models aim to alleviate this problem by exploiting the power of synchronous rewriting systems. Both Yamada and Knight (2001) and Chiang (2005) use SCFGs as the underlying model and do parsing and transformation in a joint search, essentially over a packed forest of parse-trees. To this end, their methods are not directed by a syntactic tree. Although their method potentially considers more than one single parse-tree as in our case, the packed representation of the forest restricts t"
2006.amta-papers.8,A00-2018,0,0.040742,"-tree as in our case, the packed representation of the forest restricts the scope of each transfer step to a one-level context-free rule, while our approach decouples the source-language analyzer and the recursive converter, so that the latter can have an extended domain of locality. In addition, our model also enjoys a speed-up by this decoupling, with each of the two stages having a smaller search space. In fact, the recursive transfer step can be done by a linear-time algorithm (see Section 5), and the parsing step is also fast with the modern Treebank parsers, for instance (Collins, 1999; Charniak, 2000). In contrast, their decodings are reported to be computationally expensive and Chiang (2005) uses aggressive pruning to make it tractable. There also exists a compromise between these two approaches, which uses a k-best list of parse trees (for a relatively small k) to approximate the full forest (see future work). Our model, being linguistically motivated, is also more expressive than the formally syntaxbased models of Chiang (2005) and Wu (1997). Consider, again, the passive example in rule r3 . In Chiang’s SCFG, there is only one nonterminal X, so a corresponding rule would be I was [aslee"
2006.amta-papers.8,P05-1033,0,0.810076,"2004) are good at learning local translations that are pairs of (consecutive) sub-strings, but often insufficient in modeling the reorderings of phrases themselves, especially between language pairs with very different word-order. This is because the generative capacity of these models lies within the realm of finite-state machinery (Kumar and Byrne, 2003), which is unable to process nested structures and long-distance dependencies in natural languages. Syntax-based models aim to alleviate this problem by exploiting the power of synchronous rewriting systems. Both Yamada and Knight (2001) and Chiang (2005) use SCFGs as the underlying model and do parsing and transformation in a joint search, essentially over a packed forest of parse-trees. To this end, their methods are not directed by a syntactic tree. Although their method potentially considers more than one single parse-tree as in our case, the packed representation of the forest restricts the scope of each transfer step to a one-level context-free rule, while our approach decouples the source-language analyzer and the recursive converter, so that the latter can have an extended domain of locality. In addition, our model also enjoys a speed-"
2006.amta-papers.8,C04-1090,0,0.561123,"a corresponding rule would be I was [asleep]1 by [sunset]2 . and translate it into Chinese as a passive voice. This produces very odd Chinese translation, because here “was A by B” in the English sentence is not a passive construction. By contrast, our model applies rule r3 only if A is a past participle (VBN) and B is a noun phrase (NP-C). This example also shows that, one-level SCFG rule, even if informed by the Treebank as in (Yamada and Knight, 2001), is not enough to capture a common construction like this which is five levels deep (from VP to “by”). Recent works on dependency-based MT (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005) are closest to this work in the sense that their translations are also based on source-language parse trees. The difference is that they use dependency trees instead of constituent trees. Although they share with this work the basic motivations and similar speed-up, it is difficult to specify reordering information within dependency elementary structures, so they either resort to heuristics (Lin) or a separate ordering model for linearization (the other two works). Our approach, in contrast, explicitly models the re-ordering of sub-trees within indi"
2006.amta-papers.8,P05-1066,0,0.184638,"Missing"
2006.amta-papers.8,N06-1045,1,0.682453,"hs. It consists of a normal forward phase for the 1-best derivation and a recursive backward phase for the 2nd, 3rd, . . . , k th derivations. Unfortunately, different derivations may have the same yield (a problem called spurious ambiguity), due to multi-level LHS of our rules. In practice, this results in a very small ratio of unique strings among top-k derivations, while the rescoring approach prefers diversity within the k-best list. To alleviate this problem, determinization techniques have been proposed by Mohri and Riley (2002) for finite-state automata and extended to tree automata by May and Knight (2006). These methods eliminate spurious ambiguity by effectively transforming the grammar into an equivalent deterministic form. However, this transformation often leads to a blow-up in forest size, which is exponential in the original size in the worst-case. So instead of determinization, here we present a simple-yet-effective extension to the Algorithm 3 of Huang and Chiang (2005) that guarantees to output unique translated strings: • keep a hash-table of unique strings at each vertex in the hypergraph • when asking for the next-best derivation of a vertex, keep asking until we get a new string,"
2006.amta-papers.8,P02-1038,0,0.170149,"τ ∗ ) = {d |E(d) = τ ∗ } that translates English tree τ into some Chinese string and apply the Viterbi approximation again to search for the best derivation d∗ : c∗ = C(d∗ ) = C(argmax Pr(d)) (6) d∈D(τ ∗ ) Assuming different rules in a derivation are applied independently, we approximate Pr(d) as Y Pr(r) (7) Pr(d) = r∈d where the probability Pr(r) of the rule r is estimated by conditioning on the root symbol ρ(t(r)): Pr(r) = Pr(t(r), s(r) |ρ(t(r))) c(r) = P 0 r 0 :ρ(t(r 0 ))=ρ(t(r)) c(r ) (8) where c(r) is the count (or frequency) of rule r in the training data. 4.2 Log-Linear Model Following Och and Ney (2002), we extend the direct model into a general log-linear framework in order to incorporate other features: c∗ = argmax Pr(c |e)α · Pr(c)β · e−λ|c| (9) c where Pr(c) is the language model and e−λ|c |is the length penalty term based on |c|, the length of the translation. Parameters α, β, and λ are the weights of relevant features. Note that positive λ prefers longer translations, thus we call λ the length-bonus parameter. We use a standard trigram model for Pr(c). 5 Search Algorithms We first present a linear-time algorithm for searching the best derivation under the direct model, and then extend"
2006.amta-papers.8,P05-1067,0,0.754859,"nding rule would be I was [asleep]1 by [sunset]2 . and translate it into Chinese as a passive voice. This produces very odd Chinese translation, because here “was A by B” in the English sentence is not a passive construction. By contrast, our model applies rule r3 only if A is a past participle (VBN) and B is a noun phrase (NP-C). This example also shows that, one-level SCFG rule, even if informed by the Treebank as in (Yamada and Knight, 2001), is not enough to capture a common construction like this which is five levels deep (from VP to “by”). Recent works on dependency-based MT (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005) are closest to this work in the sense that their translations are also based on source-language parse trees. The difference is that they use dependency trees instead of constituent trees. Although they share with this work the basic motivations and similar speed-up, it is difficult to specify reordering information within dependency elementary structures, so they either resort to heuristics (Lin) or a separate ordering model for linearization (the other two works). Our approach, in contrast, explicitly models the re-ordering of sub-trees within individual transfer rules."
2006.amta-papers.8,J04-4002,0,0.149326,"2 [jibi]1 Figure 2: A synatx-directed translation process for Example (1). VP VBD (r3 ) was VP-C x1 :VBN IN PP → bei x2 x1 x2 :NP-C by which captures the fact that the agent (NP-C, “the police”) and the verb (VBN, “killed”) are always inverted between English and Chinese in a passive voice. Finally, we apply rules r4 and r5 which perform phrasal translations for the two remaining sub-trees in (d), respectively, and get the completed Chinese string in (e). 67 ◦ 2 Previous Work It is helpful to compare this approach with recent efforts in statistical MT. Phrase-based models (Koehn et al., 2003; Och and Ney, 2004) are good at learning local translations that are pairs of (consecutive) sub-strings, but often insufficient in modeling the reorderings of phrases themselves, especially between language pairs with very different word-order. This is because the generative capacity of these models lies within the realm of finite-state machinery (Kumar and Byrne, 2003), which is unable to process nested structures and long-distance dependencies in natural languages. Syntax-based models aim to alleviate this problem by exploiting the power of synchronous rewriting systems. Both Yamada and Knight (2001) and Chian"
2006.amta-papers.8,P03-2041,0,0.850735,"rget-language string with the highest probability. However, the structural divergence across languages often results in non-isomorphic parse-trees that is beyond the power of SCFGs. For example, the S(VO) structure in English is translated into a VSO wordorder in Arabic, an instance of complex reordering not captured by any SCFG (Fig. 1). To alleviate the non-isomorphism problem, (synchronous) grammars with richer expressive power have been proposed whose rules apply to larger fragments of the tree. For example, Shieber and Schabes (1990) introduce synchronous treeadjoining grammar (STAG) and Eisner (2003) uses a synchronous tree-substitution grammar (STSG), which is a restricted version of STAG with no adjunctions. STSGs and STAGs generate more tree relations than SCFGs, e.g. the nonisomorphic tree pair in Fig. 1. This extra expressive power lies in the extended domain of locality (EDL) (Joshi and Schabes, 1997), i.e., elementary structures beyond the scope of one-level contextfree productions. Besides being linguistically motivated, the need for EDL is also supported by empirical findings in MT that one-level rules are often inadequate (Fox, 2002; Galley et al., 2004). Similarly, in the tree-"
2006.amta-papers.8,P03-1021,0,0.0783409,"ted strings from this forest using the algorithm in Section 5.2 and rescore them with the trigram model and the length penalty. We compared our system with a state-of-theart phrase-based system Pharaoh (Koehn, 2004) on the evaluation data. Since the target language is Chinese, we will report character-based BLEU scores instead of word-based to ensure our results are independent of Chinese tokenizations (although our language models are word-based). Feature weights of both systems are tuned for BLEU-4 (up to 4-grams) on the dev set. For Pharaoh, we use the standard minimum error-rate training (Och, 2003) (David Chiang’s implementation); and for our system, since there are only two independent features (as we always fix α = 1), we use a simple grid-based line-optimization along the language-model weight axis. For a given language-model weight β, we use binary search to find the best length bonus parameter λ that leads to a length-ratio closest to 1 against the reference. dev set BLEU-4 25.96 ±2.8 22.10 ±2.6 test set (140 sentences) BLEU-4 BLEU-8 23.54 ±1.9 6.739 ±1.2 24.53 ±2.2 7.309 ±1.9 26.01 ±2.7 26.95 ±2.8 25.74 ±2.3 26.69 ±2.4 8.489 ±2.1 9.323 ±2.2 6.3 Results and Statistical Significance"
2006.amta-papers.8,W02-1039,0,0.210419,"nchronous treeadjoining grammar (STAG) and Eisner (2003) uses a synchronous tree-substitution grammar (STSG), which is a restricted version of STAG with no adjunctions. STSGs and STAGs generate more tree relations than SCFGs, e.g. the nonisomorphic tree pair in Fig. 1. This extra expressive power lies in the extended domain of locality (EDL) (Joshi and Schabes, 1997), i.e., elementary structures beyond the scope of one-level contextfree productions. Besides being linguistically motivated, the need for EDL is also supported by empirical findings in MT that one-level rules are often inadequate (Fox, 2002; Galley et al., 2004). Similarly, in the tree-transducer terminology, Graehl and Knight (2004) define extended tree transducers that have multi-level trees on the source-side. Since syntax-directed translation models sep66 Proceedings of the 7th Conference of the Association for Machine Translation in the Americas, pages 66-73, Cambridge, August 2006. ©2006 The Association for Machine Translation in the Americas arate the source-language analysis from the recursive transformation, the domains of locality in these two modules are orthogonal to each other: in this work, we use a CFG-based Treeb"
2006.amta-papers.8,N04-1035,1,0.81719,"reeadjoining grammar (STAG) and Eisner (2003) uses a synchronous tree-substitution grammar (STSG), which is a restricted version of STAG with no adjunctions. STSGs and STAGs generate more tree relations than SCFGs, e.g. the nonisomorphic tree pair in Fig. 1. This extra expressive power lies in the extended domain of locality (EDL) (Joshi and Schabes, 1997), i.e., elementary structures beyond the scope of one-level contextfree productions. Besides being linguistically motivated, the need for EDL is also supported by empirical findings in MT that one-level rules are often inadequate (Fox, 2002; Galley et al., 2004). Similarly, in the tree-transducer terminology, Graehl and Knight (2004) define extended tree transducers that have multi-level trees on the source-side. Since syntax-directed translation models sep66 Proceedings of the 7th Conference of the Association for Machine Translation in the Americas, pages 66-73, Cambridge, August 2006. ©2006 The Association for Machine Translation in the Americas arate the source-language analysis from the recursive transformation, the domains of locality in these two modules are orthogonal to each other: in this work, we use a CFG-based Treebank parser but focus o"
2006.amta-papers.8,N04-1014,1,0.849397,"-substitution grammar (STSG), which is a restricted version of STAG with no adjunctions. STSGs and STAGs generate more tree relations than SCFGs, e.g. the nonisomorphic tree pair in Fig. 1. This extra expressive power lies in the extended domain of locality (EDL) (Joshi and Schabes, 1997), i.e., elementary structures beyond the scope of one-level contextfree productions. Besides being linguistically motivated, the need for EDL is also supported by empirical findings in MT that one-level rules are often inadequate (Fox, 2002; Galley et al., 2004). Similarly, in the tree-transducer terminology, Graehl and Knight (2004) define extended tree transducers that have multi-level trees on the source-side. Since syntax-directed translation models sep66 Proceedings of the 7th Conference of the Association for Machine Translation in the Americas, pages 66-73, Cambridge, August 2006. ©2006 The Association for Machine Translation in the Americas arate the source-language analysis from the recursive transformation, the domains of locality in these two modules are orthogonal to each other: in this work, we use a CFG-based Treebank parser but focus on the extended domain in the recursive converter. Following Galley et al."
2006.amta-papers.8,W05-1506,1,0.28971,"caching the best solution for future use 16: return cache[η] . returns the best string with its prob. However, integrating the n-gram model Pr(C(d)) with the translation model in the search is computationally very expensive. As a standard alternative, rather than aiming at the exact best derivation, we search for top-k derivations under the direct model using Algorithm 1, and then rerank the k-best list with the language model and length penalty. Like other instances of dynamic programming, Algorithm 1 can be viewed as a hypergraph search problem. To this end, we use an efficient algorithm by Huang and Chiang (2005, Algorithm 3) that solves the general k-best derivations problem in monotonic hypergraphs. It consists of a normal forward phase for the 1-best derivation and a recursive backward phase for the 2nd, 3rd, . . . , k th derivations. Unfortunately, different derivations may have the same yield (a problem called spurious ambiguity), due to multi-level LHS of our rules. In practice, this results in a very small ratio of unique strings among top-k derivations, while the rescoring approach prefers diversity within the k-best list. To alleviate this problem, determinization techniques have been propos"
2006.amta-papers.8,W06-1608,0,0.0648092,"Missing"
2006.amta-papers.8,P05-1034,0,0.570793,"as [asleep]1 by [sunset]2 . and translate it into Chinese as a passive voice. This produces very odd Chinese translation, because here “was A by B” in the English sentence is not a passive construction. By contrast, our model applies rule r3 only if A is a past participle (VBN) and B is a noun phrase (NP-C). This example also shows that, one-level SCFG rule, even if informed by the Treebank as in (Yamada and Knight, 2001), is not enough to capture a common construction like this which is five levels deep (from VP to “by”). Recent works on dependency-based MT (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005) are closest to this work in the sense that their translations are also based on source-language parse trees. The difference is that they use dependency trees instead of constituent trees. Although they share with this work the basic motivations and similar speed-up, it is difficult to specify reordering information within dependency elementary structures, so they either resort to heuristics (Lin) or a separate ordering model for linearization (the other two works). Our approach, in contrast, explicitly models the re-ordering of sub-trees within individual transfer rules. In addition, it is mo"
2006.amta-papers.8,C90-3045,0,0.426197,"vation (a sequence of translation steps) that converts the whole tree into some target-language string with the highest probability. However, the structural divergence across languages often results in non-isomorphic parse-trees that is beyond the power of SCFGs. For example, the S(VO) structure in English is translated into a VSO wordorder in Arabic, an instance of complex reordering not captured by any SCFG (Fig. 1). To alleviate the non-isomorphism problem, (synchronous) grammars with richer expressive power have been proposed whose rules apply to larger fragments of the tree. For example, Shieber and Schabes (1990) introduce synchronous treeadjoining grammar (STAG) and Eisner (2003) uses a synchronous tree-substitution grammar (STSG), which is a restricted version of STAG with no adjunctions. STSGs and STAGs generate more tree relations than SCFGs, e.g. the nonisomorphic tree pair in Fig. 1. This extra expressive power lies in the extended domain of locality (EDL) (Joshi and Schabes, 1997), i.e., elementary structures beyond the scope of one-level contextfree productions. Besides being linguistically motivated, the need for EDL is also supported by empirical findings in MT that one-level rules are often"
2006.amta-papers.8,J97-3002,0,0.123979,"by a linear-time algorithm (see Section 5), and the parsing step is also fast with the modern Treebank parsers, for instance (Collins, 1999; Charniak, 2000). In contrast, their decodings are reported to be computationally expensive and Chiang (2005) uses aggressive pruning to make it tractable. There also exists a compromise between these two approaches, which uses a k-best list of parse trees (for a relatively small k) to approximate the full forest (see future work). Our model, being linguistically motivated, is also more expressive than the formally syntaxbased models of Chiang (2005) and Wu (1997). Consider, again, the passive example in rule r3 . In Chiang’s SCFG, there is only one nonterminal X, so a corresponding rule would be I was [asleep]1 by [sunset]2 . and translate it into Chinese as a passive voice. This produces very odd Chinese translation, because here “was A by B” in the English sentence is not a passive construction. By contrast, our model applies rule r3 only if A is a past participle (VBN) and B is a noun phrase (NP-C). This example also shows that, one-level SCFG rule, even if informed by the Treebank as in (Yamada and Knight, 2001), is not enough to capture a common"
2006.amta-papers.8,P01-1067,1,0.929016,"n et al., 2003; Och and Ney, 2004) are good at learning local translations that are pairs of (consecutive) sub-strings, but often insufficient in modeling the reorderings of phrases themselves, especially between language pairs with very different word-order. This is because the generative capacity of these models lies within the realm of finite-state machinery (Kumar and Byrne, 2003), which is unable to process nested structures and long-distance dependencies in natural languages. Syntax-based models aim to alleviate this problem by exploiting the power of synchronous rewriting systems. Both Yamada and Knight (2001) and Chiang (2005) use SCFGs as the underlying model and do parsing and transformation in a joint search, essentially over a packed forest of parse-trees. To this end, their methods are not directed by a syntactic tree. Although their method potentially considers more than one single parse-tree as in our case, the packed representation of the forest restricts the scope of each transfer step to a one-level context-free rule, while our approach decouples the source-language analyzer and the recursive converter, so that the latter can have an extended domain of locality. In addition, our model al"
2006.amta-papers.8,J03-4003,0,\N,Missing
2006.amta-papers.8,J08-3004,1,\N,Missing
2006.amta-papers.8,W90-0102,0,\N,Missing
2007.mtsummit-tutorials.1,P01-1067,1,\N,Missing
2007.mtsummit-ucnlg.1,P05-1074,0,0.0859781,"Missing"
2007.mtsummit-ucnlg.1,N04-1035,1,0.805696,"Missing"
2007.mtsummit-ucnlg.1,2006.amta-papers.8,1,0.881387,"Missing"
2007.mtsummit-ucnlg.1,P05-1009,0,0.125407,"Missing"
2007.mtsummit-ucnlg.1,N07-1022,0,0.0531821,"Missing"
2008.amta-papers.7,P08-2015,0,0.034546,"s simple pattern matching to do morphological analysis for Arabic-English SMT, and were able to improve translation for tasks with small or out-of-domain training corpora. The BAMA toolkit provides many analyses based on hand-designed linguistic rules, while the MADA toolkit builds upon that foundation using statistics to determine the proper analysis. Sadat and Habash (2006) also showed that it was possible to combine the use of several variations of morphological analysis both while decoding (combining multiple phrase tables) and rescoring the combined outputs of distinct systems. Recently, Habash (2008) explored techniques for handling unknown source words in Arabic-English SMT including spelling correction and morphological variation by enriching the phrase table, rather than using lattices as we do in this work. Lattices have been used for NLP tasks for some 3 Using Alignments to Aid Morphological Analysis of Common Words The rich morphology of Arabic can often interfere with the collection of statistics over training data in a statistical MT system. One English word or phrase will coincide with multiple variations of the same Arabic root word with different affixes, thus fragmenting the p"
2008.amta-papers.7,N04-4015,0,0.0328039,"and for several different language pairs, that considering the morphology of a language can improve the quality of statistical MT. For European languages such as Spanish, Catalan, Serbian, German, and Czech, using morphological knowledge to deterministically modify data leads to gains (Nießen and Ney, 2004; Popovi´c and Ney, 2004; Goldwater and McClosky, 2005). Dyer (2007) improves a Czech-English MT system by training multiple models on original and simplified versions of the data, combines them, then represents the same variations in the decoder input using a confusion network. For Arabic, Lee (2004) demonstrates a gain in SMT quality for smaller training corpora by using automatically aligned parallel corpora to determine the best way to tokenize Arabic to match the parallel English, relying on an English POS tagger and a morphological stemmer. However, the gains did not carry over to larger corpora. Habash and Sadat (2006) compared the use of the BAMA (Buckwalter, 2002) and MADA (Habash and Rambow, 2005) toolkits as well as simple pattern matching to do morphological analysis for Arabic-English SMT, and were able to improve translation for tasks with small or out-of-domain training corp"
2008.amta-papers.7,J04-2003,0,0.029471,"ing data conditions. of common structure onto which the new knowledge sources can write their suggestions. When all of the above methods are integrated via lattices, we obtain +1.3 and +1.6 B LEU score improvements on top of strong Arabic-English baselines. 2 Related Work It has been demonstrated several times, and for several different language pairs, that considering the morphology of a language can improve the quality of statistical MT. For European languages such as Spanish, Catalan, Serbian, German, and Czech, using morphological knowledge to deterministically modify data leads to gains (Nießen and Ney, 2004; Popovi´c and Ney, 2004; Goldwater and McClosky, 2005). Dyer (2007) improves a Czech-English MT system by training multiple models on original and simplified versions of the data, combines them, then represents the same variations in the decoder input using a confusion network. For Arabic, Lee (2004) demonstrates a gain in SMT quality for smaller training corpora by using automatically aligned parallel corpora to determine the best way to tokenize Arabic to match the parallel English, relying on an English POS tagger and a morphological stemmer. However, the gains did not carry over to larger"
2008.amta-papers.7,P03-1021,0,0.0424381,"Missing"
2008.amta-papers.7,popovic-ney-2004-towards,0,0.0660396,"Missing"
2008.amta-papers.7,P06-1001,0,0.0127948,"relying on an English POS tagger and a morphological stemmer. However, the gains did not carry over to larger corpora. Habash and Sadat (2006) compared the use of the BAMA (Buckwalter, 2002) and MADA (Habash and Rambow, 2005) toolkits as well as simple pattern matching to do morphological analysis for Arabic-English SMT, and were able to improve translation for tasks with small or out-of-domain training corpora. The BAMA toolkit provides many analyses based on hand-designed linguistic rules, while the MADA toolkit builds upon that foundation using statistics to determine the proper analysis. Sadat and Habash (2006) also showed that it was possible to combine the use of several variations of morphological analysis both while decoding (combining multiple phrase tables) and rescoring the combined outputs of distinct systems. Recently, Habash (2008) explored techniques for handling unknown source words in Arabic-English SMT including spelling correction and morphological variation by enriching the phrase table, rather than using lattices as we do in this work. Lattices have been used for NLP tasks for some 3 Using Alignments to Aid Morphological Analysis of Common Words The rich morphology of Arabic can oft"
2008.amta-papers.7,P95-1022,0,0.0455739,"Missing"
2008.amta-papers.7,P96-1021,0,0.060917,"training data. Statistics collected for these words are therefore not as robust as they should be. This paper presents a novel, lightweight method for addressing this 1 Throughout this paper, we use the Buckwalter transliteration of Arabic letters for easier recognition by English readers. 89 [8th AMTA conference, Hawaii, 21-25 October 2008] time, especially in the speech community. Decoding a lattice containing the output from an ASR system, rather than the single best analysis of spoken word, is a widely-used and proven technique (Ney, 1999; Saleem et al., 2004; Matusov et al., 2005, etc.). Wu (1996) allowed for multiple Chinese segmentations using a technique that is equivalent to a fully connected lattice. Recently, Dyer et al. (2008) present lattices as a useful generalization for textbased MT, applying them to source language alternatives such as Chinese segmentation variations and Arabic morphological variations. The Arabic morphological analysis used the BAMA toolkit to segment the source text, and a unigram LM to disambiguate between alternatives. The focus of this paper is handling common sources of vocabulary sparseness in Arabic-English MT, not morphology per se. Many of the abo"
2008.amta-papers.7,D07-1079,1,0.80506,"instead of “Mahmoud Abbas”), which lacks both the Arabic letter d and the space. The token does not occur in the training corpus, but mHmwdEbAs, which includes the missing d (but still lacks the missing space) happens to occur 4 times in the training corpus. Our spelling correction adds the missing d, which then in turn enables the decoder to correctly translate the partially spell-corrected Arabic token to “Mahmoud Abbas”. 6 Experiments and Evaluation We evaluate these techniques, both separately and jointly, using the statistical syntax-based MT system described by Galley et al. (2006) and DeNeefe et al. (2007). Syntax-based rules translate a string into an English parse tree via a CKY decoder. This decoder is extended to handle input lattices using the basic technique of van Noord (1995). 94 [8th AMTA conference, Hawaii, 21-25 October 2008] Dataset training newswire development set web development set newswire test set web test set # of sentences 2,033,696 1,385 2,516 1,500 2,530 # of Arabic words 37,544,015 37,681 51,776 40,287 50,365 # of English words 44,225,727 n/a n/a n/a n/a Table 4: Description of datasets used in end-to-end MT experiments Experiment baseline common word morphology rare word"
2008.amta-papers.7,P08-1115,0,0.0314005,"ghtweight method for addressing this 1 Throughout this paper, we use the Buckwalter transliteration of Arabic letters for easier recognition by English readers. 89 [8th AMTA conference, Hawaii, 21-25 October 2008] time, especially in the speech community. Decoding a lattice containing the output from an ASR system, rather than the single best analysis of spoken word, is a widely-used and proven technique (Ney, 1999; Saleem et al., 2004; Matusov et al., 2005, etc.). Wu (1996) allowed for multiple Chinese segmentations using a technique that is equivalent to a fully connected lattice. Recently, Dyer et al. (2008) present lattices as a useful generalization for textbased MT, applying them to source language alternatives such as Chinese segmentation variations and Arabic morphological variations. The Arabic morphological analysis used the BAMA toolkit to segment the source text, and a unigram LM to disambiguate between alternatives. The focus of this paper is handling common sources of vocabulary sparseness in Arabic-English MT, not morphology per se. Many of the above works use morphological toolkits, while in this work we explore lightweight techniques that use the parallel data as the main source of"
2008.amta-papers.7,W07-0729,0,0.0192286,"can write their suggestions. When all of the above methods are integrated via lattices, we obtain +1.3 and +1.6 B LEU score improvements on top of strong Arabic-English baselines. 2 Related Work It has been demonstrated several times, and for several different language pairs, that considering the morphology of a language can improve the quality of statistical MT. For European languages such as Spanish, Catalan, Serbian, German, and Czech, using morphological knowledge to deterministically modify data leads to gains (Nießen and Ney, 2004; Popovi´c and Ney, 2004; Goldwater and McClosky, 2005). Dyer (2007) improves a Czech-English MT system by training multiple models on original and simplified versions of the data, combines them, then represents the same variations in the decoder input using a confusion network. For Arabic, Lee (2004) demonstrates a gain in SMT quality for smaller training corpora by using automatically aligned parallel corpora to determine the best way to tokenize Arabic to match the parallel English, relying on an English POS tagger and a morphological stemmer. However, the gains did not carry over to larger corpora. Habash and Sadat (2006) compared the use of the BAMA (Buck"
2008.amta-papers.7,D07-1006,0,0.0366726,"Missing"
2008.amta-papers.7,N04-1035,1,0.811526,"Missing"
2008.amta-papers.7,P06-1121,1,0.784194,"n mHmwEbAs (“Mahmouabbas” instead of “Mahmoud Abbas”), which lacks both the Arabic letter d and the space. The token does not occur in the training corpus, but mHmwdEbAs, which includes the missing d (but still lacks the missing space) happens to occur 4 times in the training corpus. Our spelling correction adds the missing d, which then in turn enables the decoder to correctly translate the partially spell-corrected Arabic token to “Mahmoud Abbas”. 6 Experiments and Evaluation We evaluate these techniques, both separately and jointly, using the statistical syntax-based MT system described by Galley et al. (2006) and DeNeefe et al. (2007). Syntax-based rules translate a string into an English parse tree via a CKY decoder. This decoder is extended to handle input lattices using the basic technique of van Noord (1995). 94 [8th AMTA conference, Hawaii, 21-25 October 2008] Dataset training newswire development set web development set newswire test set web test set # of sentences 2,033,696 1,385 2,516 1,500 2,530 # of Arabic words 37,544,015 37,681 51,776 40,287 50,365 # of English words 44,225,727 n/a n/a n/a n/a Table 4: Description of datasets used in end-to-end MT experiments Experiment baseline common"
2008.amta-papers.7,H05-1085,0,0.0158026,"which the new knowledge sources can write their suggestions. When all of the above methods are integrated via lattices, we obtain +1.3 and +1.6 B LEU score improvements on top of strong Arabic-English baselines. 2 Related Work It has been demonstrated several times, and for several different language pairs, that considering the morphology of a language can improve the quality of statistical MT. For European languages such as Spanish, Catalan, Serbian, German, and Czech, using morphological knowledge to deterministically modify data leads to gains (Nießen and Ney, 2004; Popovi´c and Ney, 2004; Goldwater and McClosky, 2005). Dyer (2007) improves a Czech-English MT system by training multiple models on original and simplified versions of the data, combines them, then represents the same variations in the decoder input using a confusion network. For Arabic, Lee (2004) demonstrates a gain in SMT quality for smaller training corpora by using automatically aligned parallel corpora to determine the best way to tokenize Arabic to match the parallel English, relying on an English POS tagger and a morphological stemmer. However, the gains did not carry over to larger corpora. Habash and Sadat (2006) compared the use of t"
2008.amta-papers.7,P05-1071,0,0.0373998,"h MT system by training multiple models on original and simplified versions of the data, combines them, then represents the same variations in the decoder input using a confusion network. For Arabic, Lee (2004) demonstrates a gain in SMT quality for smaller training corpora by using automatically aligned parallel corpora to determine the best way to tokenize Arabic to match the parallel English, relying on an English POS tagger and a morphological stemmer. However, the gains did not carry over to larger corpora. Habash and Sadat (2006) compared the use of the BAMA (Buckwalter, 2002) and MADA (Habash and Rambow, 2005) toolkits as well as simple pattern matching to do morphological analysis for Arabic-English SMT, and were able to improve translation for tasks with small or out-of-domain training corpora. The BAMA toolkit provides many analyses based on hand-designed linguistic rules, while the MADA toolkit builds upon that foundation using statistics to determine the proper analysis. Sadat and Habash (2006) also showed that it was possible to combine the use of several variations of morphological analysis both while decoding (combining multiple phrase tables) and rescoring the combined outputs of distinct"
2008.amta-papers.7,N06-2013,0,0.0377649,"and Ney, 2004; Goldwater and McClosky, 2005). Dyer (2007) improves a Czech-English MT system by training multiple models on original and simplified versions of the data, combines them, then represents the same variations in the decoder input using a confusion network. For Arabic, Lee (2004) demonstrates a gain in SMT quality for smaller training corpora by using automatically aligned parallel corpora to determine the best way to tokenize Arabic to match the parallel English, relying on an English POS tagger and a morphological stemmer. However, the gains did not carry over to larger corpora. Habash and Sadat (2006) compared the use of the BAMA (Buckwalter, 2002) and MADA (Habash and Rambow, 2005) toolkits as well as simple pattern matching to do morphological analysis for Arabic-English SMT, and were able to improve translation for tasks with small or out-of-domain training corpora. The BAMA toolkit provides many analyses based on hand-designed linguistic rules, while the MADA toolkit builds upon that foundation using statistics to determine the proper analysis. Sadat and Habash (2006) also showed that it was possible to combine the use of several variations of morphological analysis both while decoding"
2008.amta-srw.2,C94-2195,0,0.075956,"sion, recall, and f-measure of extracted syntax-based translation rules fore the head that they modify. Thus, PP-attachment ambiguity is not preserved from English to Chinese. Given the bilingual word alignments shown in Figure 2, we can deduce that the ordering of constituents on the Chinese side is “VP PP NP”, indicating that the PP modifies the NP in Chinese, and presumably therefore in English as well. 1.2 Related Work PP-Attachment Disambiguation Most previous work in PP-attachment disambiguation for English, whether unsupervised (Hindle and Rooth, 1993; Ratnaparkhi, 1998) or supervised (Brill and Resnik, 1994; Collins and Brooks, 1995), has focused on monolingual information such as relationships among the lexical heads of the VP (“answered”), NP (“questions”), and PP (“from”) constituents, as well as the lexical head of the NP dominated by the PP (“reporters”). The statistical parsers of (Charniak, 1997) and (Collins, 1997) implement a variety of monolingual lexical and structural features to resolve syntactic ambiguities while constructing parse trees. In contrast to these approaches, our approach uses bilingual word alignments to resolve PP-attachment ambiguity in English. In this respect, our"
2008.amta-srw.2,D08-1092,0,0.113038,"Missing"
2008.amta-srw.2,W95-0103,0,0.0253407,"ure of extracted syntax-based translation rules fore the head that they modify. Thus, PP-attachment ambiguity is not preserved from English to Chinese. Given the bilingual word alignments shown in Figure 2, we can deduce that the ordering of constituents on the Chinese side is “VP PP NP”, indicating that the PP modifies the NP in Chinese, and presumably therefore in English as well. 1.2 Related Work PP-Attachment Disambiguation Most previous work in PP-attachment disambiguation for English, whether unsupervised (Hindle and Rooth, 1993; Ratnaparkhi, 1998) or supervised (Brill and Resnik, 1994; Collins and Brooks, 1995), has focused on monolingual information such as relationships among the lexical heads of the VP (“answered”), NP (“questions”), and PP (“from”) constituents, as well as the lexical head of the NP dominated by the PP (“reporters”). The statistical parsers of (Charniak, 1997) and (Collins, 1997) implement a variety of monolingual lexical and structural features to resolve syntactic ambiguities while constructing parse trees. In contrast to these approaches, our approach uses bilingual word alignments to resolve PP-attachment ambiguity in English. In this respect, our approach is similar to that"
2008.amta-srw.2,P91-1017,0,0.302496,"Missing"
2008.amta-srw.2,N04-1035,1,0.929478,"sing Bilingual Chinese-English Word Alignments to Resolve PP-Attachment Ambiguity in English Victoria Fossum Dept. of Computer Science University of Michigan Ann Arbor, MI 48104 vfossum@umich.edu Kevin Knight Information Sciences Institute University of Southern California Marina del Rey, CA 90292 knight@isi.edu Abstract alignment deteriorates as the quality of the parses and word alignments decreases. To quantify the impact of parse and alignment quality upon rule quality in such a system, we extract a gold-standard set of rules by applying the minimal rule extraction algorithm described in (Galley et al., 2004) to a bilingual corpus with gold alignments and gold English parse trees. We then extract rules from the same corpus using two different sources of automatically produced alignments (GIZA++ union and GIZA++ refined) and an automatic parser (Collins, 1997), and compute the precision, recall, and f-measure of the extracted rules against the gold-standard rule set. Table 1 illustrates that errors in the automatic alignments have a somewhat greater impact upon extracted rules than errors in the automatic parses. Nonetheless, errors introduced by the automatic parses still impact rule f-measure, ca"
2008.amta-srw.2,P06-1121,1,0.738969,"and NP-bracketing ambiguity. An example of ambiguous PP-attachment in English is shown in Figure 1: the PP “from reporters” can modify the VP “answered” or the NP “questions”. Errors in English parse trees negatively impact the quality of syntax-based MT systems trained using those parses. (Quirk and Corston-Oliver, 2006) show that, in treelet translation, BLEU scores on English-German and English-Japanese experiments degrade as the amount of training data used to train the source dependency parser decreases. In the string-to-tree syntax-based MT system described in (Galley et al., 2004) and (Galley et al., 2006), the quality of translation rules extracted from each English parse tree and bilingual Chinese-English word As long as syntactic ambiguities are not preserved across languages, we can use bilingual word alignments to disambiguate the construction. For example, in Chinese, PP’s generally appear directly be245 [8th AMTA conference, Hawaii, 21-25 October 2008] VP VP VP answered NP NP PP questions IN NP from reporters VP NP answered questions Figure 1: PP-attachment ambiguity in English VP NP VP answered NP PP questions IN NP from reporters I ê V { ¯ ANSWER -ED REPORTER ’S QUESTIONS VP VP PP PP"
2008.amta-srw.2,J93-1005,0,0.749082,"stions Figure 1: PP-attachment ambiguity in English VP NP VP answered NP PP questions IN NP from reporters I ê V { ¯ ANSWER -ED REPORTER ’S QUESTIONS VP VP PP PP NP Figure 2: Resolving PP-attachment ambiguity using Chinese-English word alignments 246 PP IN NP from reporters [8th AMTA conference, Hawaii, 21-25 October 2008] gold Rule Prec. 100.00 Rule Rec. 100.00 Rule F. 100.00 gold 56.01 65.57 60.41 gold 63.57 52.31 57.39 Collins 73.25 76.04 74.62 Collins 44.04 54.46 48.70 Collins 50.70 44.18 47.22 Alignment Parse gold GIZA++ refined GIZA++ union gold GIZA++ refined GIZA++ union proaches of (Hindle and Rooth, 1993) and (Ratnaparkhi, 1998), (Schwartz et al., 2003) estimate the probability of each possible attachment decision as follows: they first identify unambiguous instances of PP-attachment in English text, then compute the relative frequency of each attachment decision using these instances, conditioned on the verbs, nouns, and prepositions (or some subset thereof) appearing in the ambiguous construction. They subsequently use these statistics, computed over unambiguous instances, to estimate the probability of a PP attaching to an NP or VP in unseen (potentially ambiguous) English text. (Schwartz e"
2008.amta-srw.2,W06-1608,0,0.0855017,"Missing"
2008.amta-srw.2,P98-2177,0,0.133456,"t ambiguity in English VP NP VP answered NP PP questions IN NP from reporters I ê V { ¯ ANSWER -ED REPORTER ’S QUESTIONS VP VP PP PP NP Figure 2: Resolving PP-attachment ambiguity using Chinese-English word alignments 246 PP IN NP from reporters [8th AMTA conference, Hawaii, 21-25 October 2008] gold Rule Prec. 100.00 Rule Rec. 100.00 Rule F. 100.00 gold 56.01 65.57 60.41 gold 63.57 52.31 57.39 Collins 73.25 76.04 74.62 Collins 44.04 54.46 48.70 Collins 50.70 44.18 47.22 Alignment Parse gold GIZA++ refined GIZA++ union gold GIZA++ refined GIZA++ union proaches of (Hindle and Rooth, 1993) and (Ratnaparkhi, 1998), (Schwartz et al., 2003) estimate the probability of each possible attachment decision as follows: they first identify unambiguous instances of PP-attachment in English text, then compute the relative frequency of each attachment decision using these instances, conditioned on the verbs, nouns, and prepositions (or some subset thereof) appearing in the ambiguous construction. They subsequently use these statistics, computed over unambiguous instances, to estimate the probability of a PP attaching to an NP or VP in unseen (potentially ambiguous) English text. (Schwartz et al., 2003) differs fro"
2008.amta-srw.2,2003.mtsummit-papers.44,0,0.337224,"h VP NP VP answered NP PP questions IN NP from reporters I ê V { ¯ ANSWER -ED REPORTER ’S QUESTIONS VP VP PP PP NP Figure 2: Resolving PP-attachment ambiguity using Chinese-English word alignments 246 PP IN NP from reporters [8th AMTA conference, Hawaii, 21-25 October 2008] gold Rule Prec. 100.00 Rule Rec. 100.00 Rule F. 100.00 gold 56.01 65.57 60.41 gold 63.57 52.31 57.39 Collins 73.25 76.04 74.62 Collins 44.04 54.46 48.70 Collins 50.70 44.18 47.22 Alignment Parse gold GIZA++ refined GIZA++ union gold GIZA++ refined GIZA++ union proaches of (Hindle and Rooth, 1993) and (Ratnaparkhi, 1998), (Schwartz et al., 2003) estimate the probability of each possible attachment decision as follows: they first identify unambiguous instances of PP-attachment in English text, then compute the relative frequency of each attachment decision using these instances, conditioned on the verbs, nouns, and prepositions (or some subset thereof) appearing in the ambiguous construction. They subsequently use these statistics, computed over unambiguous instances, to estimate the probability of a PP attaching to an NP or VP in unseen (potentially ambiguous) English text. (Schwartz et al., 2003) differs from the other unsupervised"
2008.amta-srw.2,N01-1026,0,0.21961,"Missing"
2020.acl-main.756,P19-1309,0,0.170204,"where we crawl a large JapaneseChinese parallel corpus from various websites and build open-domain machine translation systems between Japanese and Chinese, by filtering the web crawled parallel corpus. In addition, a small amount of clean parallel data is available, in the software domain. In order to confirm our results on a public data, we also apply our filter to the WMT 2018 German-English Parallel Corpus Filtering shared task. Previous work on parallel corpus filtering performs poorly in our scenario as it either requires large clean parallel corpora or dictionaries (Xu and Koehn, 2017; Artetxe and Schwenk, 2019; Junczys-Dowmunt, 2018; Chaudhary et al., 2019), or relies on multilingual word embeddings and neglects context when measuring translation parallelism (Hangya and Fraser, 2018). In this paper, we propose a simple but effective parallel corpus filtering method. Multilingual BERT (Devlin et al., 2019) projects multilingual sentences into a shared space and has shown a great potential for cross-lingual model transfer (Pires et al., 2019). We use pre-trained multilingual BERT as prior knowledge and fine-tune it on a synthetic dataset. This multilingual BERT-based classifier forms an acceptability"
2020.acl-main.756,D11-1033,0,0.0293139,"are not applicable in our scenario where available data is noisy. Ondrej Bojar (2020) organize an open domain translation challenge where participants are provided a large, noisy set of Japanese-Chinese segment pairs built from web data, and the task is to clean the noisy data and build an end-to-end machine translation system. Work on data selection is also related. Moore and Lewis (2010); Junczys-Dowmunt (2018) select domain-related data by computing the crossentropy difference between in-domain and outdomain language models. Duh et al. (2013) use neural language models for data selection. Axelrod et al. (2011) and Axelrod et al. (2015) expand cross-entropy difference filtering to both sides of the parallel corpus. Since we aim to build a general machine translation system, instead of selecting data that are relevant to a specific domain, we select data whose domains are as general as possible, by using Generative Pre-training (GPT) models trained on large and diverse corpora. 3 Method In this section we introduce a language detection filter, a translation-acceptability filter, and a domain filter. Each filter produces a score for every candidate source/target sentence pair. The partial score produc"
2020.acl-main.756,2015.iwslt-papers.9,0,0.0308503,"scenario where available data is noisy. Ondrej Bojar (2020) organize an open domain translation challenge where participants are provided a large, noisy set of Japanese-Chinese segment pairs built from web data, and the task is to clean the noisy data and build an end-to-end machine translation system. Work on data selection is also related. Moore and Lewis (2010); Junczys-Dowmunt (2018) select domain-related data by computing the crossentropy difference between in-domain and outdomain language models. Duh et al. (2013) use neural language models for data selection. Axelrod et al. (2011) and Axelrod et al. (2015) expand cross-entropy difference filtering to both sides of the parallel corpus. Since we aim to build a general machine translation system, instead of selecting data that are relevant to a specific domain, we select data whose domains are as general as possible, by using Generative Pre-training (GPT) models trained on large and diverse corpora. 3 Method In this section we introduce a language detection filter, a translation-acceptability filter, and a domain filter. Each filter produces a score for every candidate source/target sentence pair. The partial score produced by each filter ranges f"
2020.acl-main.756,W19-5435,0,0.125748,"Missing"
2020.acl-main.756,W12-3131,0,0.0288318,"pose a novel approach to filter noisy parallel corpora by using pre-trained language models. Our approach outperforms strong baselines and achieves a new state-of-the-art. • We devise an unsupervised filtering approach that does not require an identifiable clean subset of parallel segments. Our unsupervised method matches the results of previous supervised methods. • We release a large web-crawled JapaneseChinese parallel corpus which can be a useful resource for machine translation research on non-English language pairs.1 2 Related Work Several recent works address parallel corpus filtering. Denkowski et al. (2012), Dyer et al. (2010) and Heafield (2011) use language models and word alignments to determine how likely sentences are to be a good translation of another. Xu and Koehn (2017) introduce a noise filtering tool, Zipporah, that discriminates parallel and non-parallel sentences based on word-frequency vectors and a dictionary. Junczys-Dowmunt (2018) proposes a dual conditional cross-entropy filtering method, which achieved first place in the WMT 2018 GermanEnglish Parallel Corpus Filtering shared task. They train two translation models in inverse directions on millions of parallel sentences and sc"
2020.acl-main.756,N19-1423,0,0.029353,"nfirm our results on a public data, we also apply our filter to the WMT 2018 German-English Parallel Corpus Filtering shared task. Previous work on parallel corpus filtering performs poorly in our scenario as it either requires large clean parallel corpora or dictionaries (Xu and Koehn, 2017; Artetxe and Schwenk, 2019; Junczys-Dowmunt, 2018; Chaudhary et al., 2019), or relies on multilingual word embeddings and neglects context when measuring translation parallelism (Hangya and Fraser, 2018). In this paper, we propose a simple but effective parallel corpus filtering method. Multilingual BERT (Devlin et al., 2019) projects multilingual sentences into a shared space and has shown a great potential for cross-lingual model transfer (Pires et al., 2019). We use pre-trained multilingual BERT as prior knowledge and fine-tune it on a synthetic dataset. This multilingual BERT-based classifier forms an acceptability filter that determines whether or not a sentence pair consists of a bona-fide translation. As the domain of training data largely affects machine translation model performance, we also introduce a domain filter. It uses the pre-trained Generative Pre-training (GPT) as in-domain language 8545 Proceed"
2020.acl-main.756,P13-2119,0,0.0304964,"methods require large amounts of clean parallel training data and are not applicable in our scenario where available data is noisy. Ondrej Bojar (2020) organize an open domain translation challenge where participants are provided a large, noisy set of Japanese-Chinese segment pairs built from web data, and the task is to clean the noisy data and build an end-to-end machine translation system. Work on data selection is also related. Moore and Lewis (2010); Junczys-Dowmunt (2018) select domain-related data by computing the crossentropy difference between in-domain and outdomain language models. Duh et al. (2013) use neural language models for data selection. Axelrod et al. (2011) and Axelrod et al. (2015) expand cross-entropy difference filtering to both sides of the parallel corpus. Since we aim to build a general machine translation system, instead of selecting data that are relevant to a specific domain, we select data whose domains are as general as possible, by using Generative Pre-training (GPT) models trained on large and diverse corpora. 3 Method In this section we introduce a language detection filter, a translation-acceptability filter, and a domain filter. Each filter produces a score for"
2020.acl-main.756,P10-4002,0,0.0334039,"filter noisy parallel corpora by using pre-trained language models. Our approach outperforms strong baselines and achieves a new state-of-the-art. • We devise an unsupervised filtering approach that does not require an identifiable clean subset of parallel segments. Our unsupervised method matches the results of previous supervised methods. • We release a large web-crawled JapaneseChinese parallel corpus which can be a useful resource for machine translation research on non-English language pairs.1 2 Related Work Several recent works address parallel corpus filtering. Denkowski et al. (2012), Dyer et al. (2010) and Heafield (2011) use language models and word alignments to determine how likely sentences are to be a good translation of another. Xu and Koehn (2017) introduce a noise filtering tool, Zipporah, that discriminates parallel and non-parallel sentences based on word-frequency vectors and a dictionary. Junczys-Dowmunt (2018) proposes a dual conditional cross-entropy filtering method, which achieved first place in the WMT 2018 GermanEnglish Parallel Corpus Filtering shared task. They train two translation models in inverse directions on millions of parallel sentences and score sentence pairs b"
2020.acl-main.756,W18-6477,0,0.321882,"crawled parallel corpus. In addition, a small amount of clean parallel data is available, in the software domain. In order to confirm our results on a public data, we also apply our filter to the WMT 2018 German-English Parallel Corpus Filtering shared task. Previous work on parallel corpus filtering performs poorly in our scenario as it either requires large clean parallel corpora or dictionaries (Xu and Koehn, 2017; Artetxe and Schwenk, 2019; Junczys-Dowmunt, 2018; Chaudhary et al., 2019), or relies on multilingual word embeddings and neglects context when measuring translation parallelism (Hangya and Fraser, 2018). In this paper, we propose a simple but effective parallel corpus filtering method. Multilingual BERT (Devlin et al., 2019) projects multilingual sentences into a shared space and has shown a great potential for cross-lingual model transfer (Pires et al., 2019). We use pre-trained multilingual BERT as prior knowledge and fine-tune it on a synthetic dataset. This multilingual BERT-based classifier forms an acceptability filter that determines whether or not a sentence pair consists of a bona-fide translation. As the domain of training data largely affects machine translation model performance,"
2020.acl-main.756,W11-2123,0,0.736535,"orpora by using pre-trained language models. Our approach outperforms strong baselines and achieves a new state-of-the-art. • We devise an unsupervised filtering approach that does not require an identifiable clean subset of parallel segments. Our unsupervised method matches the results of previous supervised methods. • We release a large web-crawled JapaneseChinese parallel corpus which can be a useful resource for machine translation research on non-English language pairs.1 2 Related Work Several recent works address parallel corpus filtering. Denkowski et al. (2012), Dyer et al. (2010) and Heafield (2011) use language models and word alignments to determine how likely sentences are to be a good translation of another. Xu and Koehn (2017) introduce a noise filtering tool, Zipporah, that discriminates parallel and non-parallel sentences based on word-frequency vectors and a dictionary. Junczys-Dowmunt (2018) proposes a dual conditional cross-entropy filtering method, which achieved first place in the WMT 2018 GermanEnglish Parallel Corpus Filtering shared task. They train two translation models in inverse directions on millions of parallel sentences and score sentence pairs based on the word-nor"
2020.acl-main.756,E17-2068,0,0.104382,"Missing"
2020.acl-main.756,W18-6478,0,0.106795,"neseChinese parallel corpus from various websites and build open-domain machine translation systems between Japanese and Chinese, by filtering the web crawled parallel corpus. In addition, a small amount of clean parallel data is available, in the software domain. In order to confirm our results on a public data, we also apply our filter to the WMT 2018 German-English Parallel Corpus Filtering shared task. Previous work on parallel corpus filtering performs poorly in our scenario as it either requires large clean parallel corpora or dictionaries (Xu and Koehn, 2017; Artetxe and Schwenk, 2019; Junczys-Dowmunt, 2018; Chaudhary et al., 2019), or relies on multilingual word embeddings and neglects context when measuring translation parallelism (Hangya and Fraser, 2018). In this paper, we propose a simple but effective parallel corpus filtering method. Multilingual BERT (Devlin et al., 2019) projects multilingual sentences into a shared space and has shown a great potential for cross-lingual model transfer (Pires et al., 2019). We use pre-trained multilingual BERT as prior knowledge and fine-tune it on a synthetic dataset. This multilingual BERT-based classifier forms an acceptability filter that determines"
2020.acl-main.756,W18-2709,0,0.026032,"blicly-available parallel corpora are mostly paired with English, such as German-English, French-English, Chinese-English, etc., and their domains are limited. For building machine translation systems between non-English language pairs, such as Chinese and Japanese, existing parallel corpora are insufficient and often low quality. To address this problem, system builders have trained NMT systems on web-crawled data and achieved promising results (Xu and Koehn, 2017; JunczysDowmunt, 2018; Schwenk, 2018; Schwenk et al., 2019). However, data automatically crawled from the web is extremely noisy. Khayrallah and Koehn (2018) and Belinkov and Bisk (2018) show that neural translation models are far more sensitive to noisy parallel training data than statistical machine translation. Data selection methods that can filter noisy parallel sentences from large-scale web crawled resources are in demand. In this paper, we study the problem in a realworld scenario where we crawl a large JapaneseChinese parallel corpus from various websites and build open-domain machine translation systems between Japanese and Chinese, by filtering the web crawled parallel corpus. In addition, a small amount of clean parallel data is availa"
2020.acl-main.756,L16-1147,0,0.0740035,"Missing"
2020.acl-main.756,W18-6480,0,0.0195923,"n inverse directions on millions of parallel sentences and score sentence pairs based on the word-normalized conditional cross-entropy from the translation models. Artetxe and Schwenk (2019) and Schwenk (2018) propose a margin-based scoring method that compares the 1 http://iwslt.org/doku.php?id=open_ domain_translation similarity of the source and target sentence representations. The sentence representations are produced by a sentence encoder trained on clean parallel data via a neural encoder-decoder architecture. Other works based on sentence embeddings include Hangya and Fraser (2018) and Littell et al. (2018), as well as Schwenk et al. (2019), which mines millions of parallel sentences in 1620 language pairs from Wikipedia. These encoder-decoder based methods require large amounts of clean parallel training data and are not applicable in our scenario where available data is noisy. Ondrej Bojar (2020) organize an open domain translation challenge where participants are provided a large, noisy set of Japanese-Chinese segment pairs built from web data, and the task is to clean the noisy data and build an end-to-end machine translation system. Work on data selection is also related. Moore and Lewis (2"
2020.acl-main.756,D17-1319,0,0.211914,"s that we make publicly available. 1 Introduction Training modern neural machine translation (NMT) systems requires large parallel-text resources. Publicly-available parallel corpora are mostly paired with English, such as German-English, French-English, Chinese-English, etc., and their domains are limited. For building machine translation systems between non-English language pairs, such as Chinese and Japanese, existing parallel corpora are insufficient and often low quality. To address this problem, system builders have trained NMT systems on web-crawled data and achieved promising results (Xu and Koehn, 2017; JunczysDowmunt, 2018; Schwenk, 2018; Schwenk et al., 2019). However, data automatically crawled from the web is extremely noisy. Khayrallah and Koehn (2018) and Belinkov and Bisk (2018) show that neural translation models are far more sensitive to noisy parallel training data than statistical machine translation. Data selection methods that can filter noisy parallel sentences from large-scale web crawled resources are in demand. In this paper, we study the problem in a realworld scenario where we crawl a large JapaneseChinese parallel corpus from various websites and build open-domain machin"
2020.acl-main.756,W18-6481,0,0.0410603,"Missing"
2020.acl-main.756,W18-6482,0,0.0396522,"Missing"
2020.acl-main.756,P10-2041,0,0.53146,"l BERT-based classifier forms an acceptability filter that determines whether or not a sentence pair consists of a bona-fide translation. As the domain of training data largely affects machine translation model performance, we also introduce a domain filter. It uses the pre-trained Generative Pre-training (GPT) as in-domain language 8545 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8545–8554 c July 5 - 10, 2020. 2020 Association for Computational Linguistics model and is an extension of the existing crossentropy difference based domain filter (Moore and Lewis, 2010; Junczys-Dowmunt, 2018). We evaluate our proposed method on the WMT 2018 German-English Parallel Corpus Filtering shared task and achieve a new state-of-the-art. Our unsupervised method achieves comparable performance to the top system that is trained on millions of clean parallel sentence pairs. Our proposed methods also significantly outperform baselines in our own Japanese-Chinese parallel corpus filtering task. We make the following contributions: • We propose a novel approach to filter noisy parallel corpora by using pre-trained language models. Our approach outperforms strong baselines"
2020.acl-main.756,P19-1493,0,0.105385,"work on parallel corpus filtering performs poorly in our scenario as it either requires large clean parallel corpora or dictionaries (Xu and Koehn, 2017; Artetxe and Schwenk, 2019; Junczys-Dowmunt, 2018; Chaudhary et al., 2019), or relies on multilingual word embeddings and neglects context when measuring translation parallelism (Hangya and Fraser, 2018). In this paper, we propose a simple but effective parallel corpus filtering method. Multilingual BERT (Devlin et al., 2019) projects multilingual sentences into a shared space and has shown a great potential for cross-lingual model transfer (Pires et al., 2019). We use pre-trained multilingual BERT as prior knowledge and fine-tune it on a synthetic dataset. This multilingual BERT-based classifier forms an acceptability filter that determines whether or not a sentence pair consists of a bona-fide translation. As the domain of training data largely affects machine translation model performance, we also introduce a domain filter. It uses the pre-trained Generative Pre-training (GPT) as in-domain language 8545 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8545–8554 c July 5 - 10, 2020. 2020 Association fo"
2020.acl-main.756,P18-2037,0,0.2051,"duction Training modern neural machine translation (NMT) systems requires large parallel-text resources. Publicly-available parallel corpora are mostly paired with English, such as German-English, French-English, Chinese-English, etc., and their domains are limited. For building machine translation systems between non-English language pairs, such as Chinese and Japanese, existing parallel corpora are insufficient and often low quality. To address this problem, system builders have trained NMT systems on web-crawled data and achieved promising results (Xu and Koehn, 2017; JunczysDowmunt, 2018; Schwenk, 2018; Schwenk et al., 2019). However, data automatically crawled from the web is extremely noisy. Khayrallah and Koehn (2018) and Belinkov and Bisk (2018) show that neural translation models are far more sensitive to noisy parallel training data than statistical machine translation. Data selection methods that can filter noisy parallel sentences from large-scale web crawled resources are in demand. In this paper, we study the problem in a realworld scenario where we crawl a large JapaneseChinese parallel corpus from various websites and build open-domain machine translation systems between Japanes"
2020.acl-main.756,tiedemann-2012-parallel,0,0.0569658,"Missing"
2020.acl-main.756,W18-6453,0,\N,Missing
2020.emnlp-main.458,D18-1399,0,0.0400813,"Missing"
2020.emnlp-main.458,Q17-1010,0,0.0739879,"Missing"
2020.emnlp-main.458,2020.eamt-1.5,0,0.0231963,"Missing"
2020.emnlp-main.458,W99-0906,1,0.660655,"Missing"
2020.emnlp-main.458,2020.wmt-1.68,0,0.0267543,"Missing"
2020.emnlp-main.458,W17-5403,0,0.0614383,"Missing"
2020.emnlp-main.458,P14-2115,1,0.812899,"explore exposing the internals of characters and syllables to the analyzer, as Chinese characters sharing written components often sound similar. We find it compelling that pronunciation dictionaries are largely redundant with non-parallel text and speech corpora, even for writing systems as complex as Chinese. We also expect results may be of use in dealing with novel ways to write Chinese, such as N¨ushu script (Zhang et al., 2016), with acoustic modeling of other Chinese languages and dialects, and with novel ways to phonetically encode and decode Chinese in online censorship applications (Zhang et al., 2014). 2 Chinese Writing The most-popular modern Chinese writing system renders each spoken syllable token with a single character token (hanzi). There are over 400 syllable types in Mandarin1 and several thousand character types. The mapping is many-to-many: • Almost every syllable type can be written 1 In this paper, we use standard pinyin syllable representation, and we refer strictly to Mandarin pronunciation. 5687 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 5687–5693, c November 16–20, 2020. 2020 Association for Computational Linguistics with d"
2020.emnlp-main.471,D12-1025,1,0.862848,"Missing"
2020.emnlp-main.471,C14-1218,0,0.0178795,"a → d), (b → e), (c → f), etc. The Caesar cipher can be easily solved by algorithm, since there are only 26 offsets to check. The algorithm need only be able to recognize which of the 26 candidate plaintexts form good English. Since 25 of the candidates will be gibberish, even the simplest language model will suffice. A simple substitution cipher uses a substitution table built by randomly permuting the alphabet. Since there are 26! ≈ 4 · 1026 possible tables, algorithmic decipherment is more difficult. However, there are many successful algorithms, e.g., (Hart, 1994; Knight and Yamada, 1999; Hauer et al., 2014; Olson, 2007; Ravi and Knight, 2008; Corlett and Penn, 2010). Many of these systems 1 Our typology is geared toward explaining our contribution in the context of related systems. For a fuller picture of classical cryptology, the reader is directly to Kahn (1996) and Singh (2000). For example, we do not discuss here systems in which a substitution key evolves during the encoding process, such as the Vigen`ere cipher or the German Enigma machine. 5845 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 5845–5854, c November 16–20, 2020. 2020 Association"
2020.emnlp-main.471,D18-1102,0,0.0231021,"Missing"
2020.emnlp-main.471,W11-1202,1,0.844172,"Missing"
2020.emnlp-main.471,W99-0906,1,0.286401,"stitution system, e.g., (a → d), (b → e), (c → f), etc. The Caesar cipher can be easily solved by algorithm, since there are only 26 offsets to check. The algorithm need only be able to recognize which of the 26 candidate plaintexts form good English. Since 25 of the candidates will be gibberish, even the simplest language model will suffice. A simple substitution cipher uses a substitution table built by randomly permuting the alphabet. Since there are 26! ≈ 4 · 1026 possible tables, algorithmic decipherment is more difficult. However, there are many successful algorithms, e.g., (Hart, 1994; Knight and Yamada, 1999; Hauer et al., 2014; Olson, 2007; Ravi and Knight, 2008; Corlett and Penn, 2010). Many of these systems 1 Our typology is geared toward explaining our contribution in the context of related systems. For a fuller picture of classical cryptology, the reader is directly to Kahn (1996) and Singh (2000). For example, we do not discuss here systems in which a substitution key evolves during the encoding process, such as the Vigen`ere cipher or the German Enigma machine. 5845 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 5845–5854, c November 16–20, 20"
2020.emnlp-main.471,D14-1184,0,0.0411499,"Missing"
2020.emnlp-main.471,W19-6126,0,0.0144865,"408 Copiale cipher Rossignols’ Grand Chiffre Book-based key Beale cipher Mexico-Nauen code Scovell code Wilkinson code (this work) Figure 1: Simplified typology of substitution-based cryptosystems, with some examples. Ciphers involve character-level substitutions (e.g, f → q), while codes involve word-level substitutions (e.g., forest → 5731). Introduction Cryptography has been used since antiquity to encode important secrets. There are many unsolved ciphers of historical interest, residing in national libraries, private archives, and recent corpora collection projects (Megyesi et al., 2019; Pettersson and Megyesi, 2019). Solving classical ciphers with automatic methods is a needed step in analyzing these materials. In this work, we are concerned with automatic algorithms for solving a historically-common type of book code, in which word tokens are systematically replaced with numerical codes. Encoding and decoding are done with reference to a dictionary possessed by both sender and recipient. While this type of code is common, automatic decipherment algorithms do not yet exist. The contributions of our work are: • We develop a algorithm for solving dictionary-based substitution codes. The algorithm uses a kn"
2020.emnlp-main.471,D08-1085,1,0.693944,"e Caesar cipher can be easily solved by algorithm, since there are only 26 offsets to check. The algorithm need only be able to recognize which of the 26 candidate plaintexts form good English. Since 25 of the candidates will be gibberish, even the simplest language model will suffice. A simple substitution cipher uses a substitution table built by randomly permuting the alphabet. Since there are 26! ≈ 4 · 1026 possible tables, algorithmic decipherment is more difficult. However, there are many successful algorithms, e.g., (Hart, 1994; Knight and Yamada, 1999; Hauer et al., 2014; Olson, 2007; Ravi and Knight, 2008; Corlett and Penn, 2010). Many of these systems 1 Our typology is geared toward explaining our contribution in the context of related systems. For a fuller picture of classical cryptology, the reader is directly to Kahn (1996) and Singh (2000). For example, we do not discuss here systems in which a substitution key evolves during the encoding process, such as the Vigen`ere cipher or the German Enigma machine. 5845 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 5845–5854, c November 16–20, 2020. 2020 Association for Computational Linguistics searc"
2020.emnlp-main.471,P11-1025,1,0.85907,"Missing"
2020.emnlp-main.471,N06-2001,0,0.179487,"Missing"
2020.inlg-1.44,D19-1299,0,0.0133532,"019; Gong et al., 2019; Iso et al., 2019), Graph-toText (Flanigan et al., 2016; Song et al., 2018; Zhu et al., 2019; Koncel-Kedziorski et al., 2019), and Topic-to-text (Tang et al., 2019), and Knowledge Base Description (Kiddon et al., 2016; Gardent et al., 2017; Koncel-Kedziorski et al., 2019); (2) Knowledge Synthesis, which retrieves knowledge base and organizes text answers, such as Video Caption Generation (Whitehead et al., 2018), KB-supported Dialogue Generation (Han et al., 2015; Zhou et al., 2018; Parthasarathi and Pineau, 2018; Liu et al., 2018a; Young et al., 2018; Wen et al., 2018; Chen et al., 2019; Liu et al., 2019b), 391 Knowledge-guided comment Generation (Li et al., 2019b), paper generation (Wang et al., 2018b, 2019; Cachola et al., 2020) , and abstractive summarization (Gu et al., 2016; Sharma et al., 2019; Huang et al., 2020). 5 Application Limitations and Ethical Statement The types of evidence we have designed in this paper are limited to NLP, ML or related areas, and thus they are not applicable to other scientific domains such as biomedical science and chemistry. Whether ReviewRobot is essentially beneficial to the scientific community also depends on who uses it. Here are som"
2020.inlg-1.44,E17-1059,0,0.0266285,"to explain grammatical errors as feedback to improve paper writing. (Xing et al., 2020; Luu et al., 2020) extract paper-paper relations and 8 https://naacl2018.wordpress.com/2018/02/26/acceptanceand-author-feedback/ use them to guide citation text generation. Review Generation in other Domains. Automatic review generation techniques have been applied to many other domains including music (Tata and Di Eugenio, 2010), restaurants (Oraby et al., 2017; Juuti et al., 2018; Li et al., 2019a; Braˇzinskas et al., 2020), and products (Catherine and Cohen, 2018; Li et al., 2019a; Li and Tuzhilin, 2019; Dong et al., 2017; Ni and McAuley, 2018; Braˇzinskas et al., 2020). These methods generally apply a sequence-to-sequence model with attention to aspects and attributes (e.g. food type). Compared to these domains, paper review generation is much more challenging because it requires the model to perform deep understanding on paper content, construct knowledge graphs to compare knowledge elements across sections and papers, and synthesize information as input evidence for comment generation. Controlled Knowledge-Driven Generation. There have been some other studies on text generation controlled by sentiment (Hu e"
2020.inlg-1.44,N16-1087,0,0.0241933,"ee levels: (1) Knowledge Description, which transforms structured data into unstructured text, such as Table-to-Text Generation (Mei et al., 2016; Lebret et al., 2016; Chisholm et al., 2017; Sha et al., 2018; Liu et al., 2018b; Nema et al., 2018; Wang et al., 2018a; Moryossef et al., 2019; Nie et al., 2019; Castro Ferreira et al., 2019; Wang et al., 2020; Shahidi et al., 2020) and its variants in low-resource (Ma et al., 2019) and multi-lingual setting (Kaffee et al., 2018a,b), Data-to-Document (Wiseman et al., 2017; Puduppully et al., 2019; Gong et al., 2019; Iso et al., 2019), Graph-toText (Flanigan et al., 2016; Song et al., 2018; Zhu et al., 2019; Koncel-Kedziorski et al., 2019), and Topic-to-text (Tang et al., 2019), and Knowledge Base Description (Kiddon et al., 2016; Gardent et al., 2017; Koncel-Kedziorski et al., 2019); (2) Knowledge Synthesis, which retrieves knowledge base and organizes text answers, such as Video Caption Generation (Whitehead et al., 2018), KB-supported Dialogue Generation (Han et al., 2015; Zhou et al., 2018; Parthasarathi and Pineau, 2018; Liu et al., 2018a; Young et al., 2018; Wen et al., 2018; Chen et al., 2019; Liu et al., 2019b), 391 Knowledge-guided comment Generation"
2020.inlg-1.44,P17-1017,0,0.0118964,"2017; Sha et al., 2018; Liu et al., 2018b; Nema et al., 2018; Wang et al., 2018a; Moryossef et al., 2019; Nie et al., 2019; Castro Ferreira et al., 2019; Wang et al., 2020; Shahidi et al., 2020) and its variants in low-resource (Ma et al., 2019) and multi-lingual setting (Kaffee et al., 2018a,b), Data-to-Document (Wiseman et al., 2017; Puduppully et al., 2019; Gong et al., 2019; Iso et al., 2019), Graph-toText (Flanigan et al., 2016; Song et al., 2018; Zhu et al., 2019; Koncel-Kedziorski et al., 2019), and Topic-to-text (Tang et al., 2019), and Knowledge Base Description (Kiddon et al., 2016; Gardent et al., 2017; Koncel-Kedziorski et al., 2019); (2) Knowledge Synthesis, which retrieves knowledge base and organizes text answers, such as Video Caption Generation (Whitehead et al., 2018), KB-supported Dialogue Generation (Han et al., 2015; Zhou et al., 2018; Parthasarathi and Pineau, 2018; Liu et al., 2018a; Young et al., 2018; Wen et al., 2018; Chen et al., 2019; Liu et al., 2019b), 391 Knowledge-guided comment Generation (Li et al., 2019b), paper generation (Wang et al., 2018b, 2019; Cachola et al., 2020) , and abstractive summarization (Gu et al., 2016; Sharma et al., 2019; Huang et al., 2020). 5 App"
2020.inlg-1.44,P19-1106,0,0.0248858,"w sentences that are also supported by corresponding sentences in the target papers. This process is very time-consuming and expensive. We need to build a better review infrastructure in our community, e.g., asking authors to provide feedback and rating to select constructive reviews as in NAACL20188 . 4 Related Work Paper Acceptance Prediction. Kang et al. (2018) has constructed a paper review corpus, PeerRead, and trained paper acceptance classifiers. Huang (2018) applies an interesting visual feature to compare the pdf layouts and proves its effectiveness to make paper acceptance decision. Ghosal et al. (2019) applies sentiment analysis features to improve acceptance prediction. The KDD2014 PC chairs exploit author status and review comments for predicting paper acceptance (Leskovec and Wang, 2014). We extend these methods to score prediction and comment generation with detailed knowledge element level evidence for each specific review category. Paper Review Generation. Bartoli et al. (2016) proposes the first deep neural network framework to generate paper review comments. The generator is trained with 48 papers from their own lab. In comparison, we perform more concrete and explainable review gen"
2020.inlg-1.44,D19-5615,0,0.0120979,"ration can be roughly divided into the following three levels: (1) Knowledge Description, which transforms structured data into unstructured text, such as Table-to-Text Generation (Mei et al., 2016; Lebret et al., 2016; Chisholm et al., 2017; Sha et al., 2018; Liu et al., 2018b; Nema et al., 2018; Wang et al., 2018a; Moryossef et al., 2019; Nie et al., 2019; Castro Ferreira et al., 2019; Wang et al., 2020; Shahidi et al., 2020) and its variants in low-resource (Ma et al., 2019) and multi-lingual setting (Kaffee et al., 2018a,b), Data-to-Document (Wiseman et al., 2017; Puduppully et al., 2019; Gong et al., 2019; Iso et al., 2019), Graph-toText (Flanigan et al., 2016; Song et al., 2018; Zhu et al., 2019; Koncel-Kedziorski et al., 2019), and Topic-to-text (Tang et al., 2019), and Knowledge Base Description (Kiddon et al., 2016; Gardent et al., 2017; Koncel-Kedziorski et al., 2019); (2) Knowledge Synthesis, which retrieves knowledge base and organizes text answers, such as Video Caption Generation (Whitehead et al., 2018), KB-supported Dialogue Generation (Han et al., 2015; Zhou et al., 2018; Parthasarathi and Pineau, 2018; Liu et al., 2018a; Young et al., 2018; Wen et al., 2018; Chen et al., 2019; Liu"
2020.inlg-1.44,E17-1060,0,0.0190896,"on as input evidence for comment generation. Controlled Knowledge-Driven Generation. There have been some other studies on text generation controlled by sentiment (Hu et al., 2017), topic (Krishna and Srinivasan, 2018), text style (Shen et al., 2017; Liu et al., 2019a; Tikhonov et al., 2019), and facts (Wang et al., 2020). The usage of external supportive knowledge in text generation can be roughly divided into the following three levels: (1) Knowledge Description, which transforms structured data into unstructured text, such as Table-to-Text Generation (Mei et al., 2016; Lebret et al., 2016; Chisholm et al., 2017; Sha et al., 2018; Liu et al., 2018b; Nema et al., 2018; Wang et al., 2018a; Moryossef et al., 2019; Nie et al., 2019; Castro Ferreira et al., 2019; Wang et al., 2020; Shahidi et al., 2020) and its variants in low-resource (Ma et al., 2019) and multi-lingual setting (Kaffee et al., 2018a,b), Data-to-Document (Wiseman et al., 2017; Puduppully et al., 2019; Gong et al., 2019; Iso et al., 2019), Graph-toText (Flanigan et al., 2016; Song et al., 2018; Zhu et al., 2019; Koncel-Kedziorski et al., 2019), and Topic-to-text (Tang et al., 2019), and Knowledge Base Description (Kiddon et al., 2016; Gard"
2020.inlg-1.44,P16-1154,0,0.0256106,"edge Base Description (Kiddon et al., 2016; Gardent et al., 2017; Koncel-Kedziorski et al., 2019); (2) Knowledge Synthesis, which retrieves knowledge base and organizes text answers, such as Video Caption Generation (Whitehead et al., 2018), KB-supported Dialogue Generation (Han et al., 2015; Zhou et al., 2018; Parthasarathi and Pineau, 2018; Liu et al., 2018a; Young et al., 2018; Wen et al., 2018; Chen et al., 2019; Liu et al., 2019b), 391 Knowledge-guided comment Generation (Li et al., 2019b), paper generation (Wang et al., 2018b, 2019; Cachola et al., 2020) , and abstractive summarization (Gu et al., 2016; Sharma et al., 2019; Huang et al., 2020). 5 Application Limitations and Ethical Statement The types of evidence we have designed in this paper are limited to NLP, ML or related areas, and thus they are not applicable to other scientific domains such as biomedical science and chemistry. Whether ReviewRobot is essentially beneficial to the scientific community also depends on who uses it. Here are some example scenarios where ReviewRobot should and should not be used: • Should-Do: Reviewers use ReviewRobot merely as an assistant to write more constructive comments and compare notes. background"
2020.inlg-1.44,D14-1179,0,0.00835998,"Missing"
2020.inlg-1.44,P17-1055,0,0.091861,"ed for State-of-theart systems Cloze-style reading comprehension problem • attention-over-attention reader, n-best re-ranking strategy is verified in the related work section Neural architecture Used for Cloze-style reading comprehension problem Used for Large-scale training data Attention mechanism (Bahdanau et al., 2015; Hermann et al., 2015) • 5 new knowledge elements • 1 new architecture Overall Recom- • All features mentioned in the above categories mendations • Abstract Table 1: Evidence Extraction for the example paper Attention-over-Attention Neural Networks for Reading Comprehension (Cui et al., 2017) 2 2.1 Approach 2018): Overview Figure 1 illustrates the overall architecture of ReviewRobot. ReviewRobot first constructs knowledge graphs (KGs) for each target paper and a large collection of background papers, then it extracts evidence by comparing knowledge elements across multiple sections and papers, and uses the evidence to predict scores and generate comments for each review category. We adopt the following most common categories from NeurIPS20193 and PeerRead (Kang et al., 3 https://nips.cc/Conferences/2019/ PaperInformation/ReviewerGuidelines 386 • Summary: What is this paper about?"
2020.inlg-1.44,2020.acl-main.457,0,0.0164717,"2016; Gardent et al., 2017; Koncel-Kedziorski et al., 2019); (2) Knowledge Synthesis, which retrieves knowledge base and organizes text answers, such as Video Caption Generation (Whitehead et al., 2018), KB-supported Dialogue Generation (Han et al., 2015; Zhou et al., 2018; Parthasarathi and Pineau, 2018; Liu et al., 2018a; Young et al., 2018; Wen et al., 2018; Chen et al., 2019; Liu et al., 2019b), 391 Knowledge-guided comment Generation (Li et al., 2019b), paper generation (Wang et al., 2018b, 2019; Cachola et al., 2020) , and abstractive summarization (Gu et al., 2016; Sharma et al., 2019; Huang et al., 2020). 5 Application Limitations and Ethical Statement The types of evidence we have designed in this paper are limited to NLP, ML or related areas, and thus they are not applicable to other scientific domains such as biomedical science and chemistry. Whether ReviewRobot is essentially beneficial to the scientific community also depends on who uses it. Here are some example scenarios where ReviewRobot should and should not be used: • Should-Do: Reviewers use ReviewRobot merely as an assistant to write more constructive comments and compare notes. background papers, and summarize the pros and cons o"
2020.inlg-1.44,N18-2101,0,0.0182384,"9), and facts (Wang et al., 2020). The usage of external supportive knowledge in text generation can be roughly divided into the following three levels: (1) Knowledge Description, which transforms structured data into unstructured text, such as Table-to-Text Generation (Mei et al., 2016; Lebret et al., 2016; Chisholm et al., 2017; Sha et al., 2018; Liu et al., 2018b; Nema et al., 2018; Wang et al., 2018a; Moryossef et al., 2019; Nie et al., 2019; Castro Ferreira et al., 2019; Wang et al., 2020; Shahidi et al., 2020) and its variants in low-resource (Ma et al., 2019) and multi-lingual setting (Kaffee et al., 2018a,b), Data-to-Document (Wiseman et al., 2017; Puduppully et al., 2019; Gong et al., 2019; Iso et al., 2019), Graph-toText (Flanigan et al., 2016; Song et al., 2018; Zhu et al., 2019; Koncel-Kedziorski et al., 2019), and Topic-to-text (Tang et al., 2019), and Knowledge Base Description (Kiddon et al., 2016; Gardent et al., 2017; Koncel-Kedziorski et al., 2019); (2) Knowledge Synthesis, which retrieves knowledge base and organizes text answers, such as Video Caption Generation (Whitehead et al., 2018), KB-supported Dialogue Generation (Han et al., 2015; Zhou et al., 2018; Parthasarathi and Pinea"
2020.inlg-1.44,N18-1149,0,0.170369,"Missing"
2020.inlg-1.44,D16-1032,0,0.0181868,"16; Chisholm et al., 2017; Sha et al., 2018; Liu et al., 2018b; Nema et al., 2018; Wang et al., 2018a; Moryossef et al., 2019; Nie et al., 2019; Castro Ferreira et al., 2019; Wang et al., 2020; Shahidi et al., 2020) and its variants in low-resource (Ma et al., 2019) and multi-lingual setting (Kaffee et al., 2018a,b), Data-to-Document (Wiseman et al., 2017; Puduppully et al., 2019; Gong et al., 2019; Iso et al., 2019), Graph-toText (Flanigan et al., 2016; Song et al., 2018; Zhu et al., 2019; Koncel-Kedziorski et al., 2019), and Topic-to-text (Tang et al., 2019), and Knowledge Base Description (Kiddon et al., 2016; Gardent et al., 2017; Koncel-Kedziorski et al., 2019); (2) Knowledge Synthesis, which retrieves knowledge base and organizes text answers, such as Video Caption Generation (Whitehead et al., 2018), KB-supported Dialogue Generation (Han et al., 2015; Zhou et al., 2018; Parthasarathi and Pineau, 2018; Liu et al., 2018a; Young et al., 2018; Wen et al., 2018; Chen et al., 2019; Liu et al., 2019b), 391 Knowledge-guided comment Generation (Li et al., 2019b), paper generation (Wang et al., 2018b, 2019; Cachola et al., 2020) , and abstractive summarization (Gu et al., 2016; Sharma et al., 2019; Huan"
2020.inlg-1.44,N19-1238,0,0.0216935,"Missing"
2020.inlg-1.44,N18-1153,0,0.045738,"Missing"
2020.inlg-1.44,D16-1128,0,0.0361734,"Missing"
2020.inlg-1.44,P19-1190,0,0.111919,"ch review category following a rich set of evidence, and use a much larger data set. Nagata (2019) generates comment sentences to explain grammatical errors as feedback to improve paper writing. (Xing et al., 2020; Luu et al., 2020) extract paper-paper relations and 8 https://naacl2018.wordpress.com/2018/02/26/acceptanceand-author-feedback/ use them to guide citation text generation. Review Generation in other Domains. Automatic review generation techniques have been applied to many other domains including music (Tata and Di Eugenio, 2010), restaurants (Oraby et al., 2017; Juuti et al., 2018; Li et al., 2019a; Braˇzinskas et al., 2020), and products (Catherine and Cohen, 2018; Li et al., 2019a; Li and Tuzhilin, 2019; Dong et al., 2017; Ni and McAuley, 2018; Braˇzinskas et al., 2020). These methods generally apply a sequence-to-sequence model with attention to aspects and attributes (e.g. food type). Compared to these domains, paper review generation is much more challenging because it requires the model to perform deep understanding on paper content, construct knowledge graphs to compare knowledge elements across sections and papers, and synthesize information as input evidence for comment genera"
2020.inlg-1.44,D19-1319,0,0.0223965,"ates comment sentences to explain grammatical errors as feedback to improve paper writing. (Xing et al., 2020; Luu et al., 2020) extract paper-paper relations and 8 https://naacl2018.wordpress.com/2018/02/26/acceptanceand-author-feedback/ use them to guide citation text generation. Review Generation in other Domains. Automatic review generation techniques have been applied to many other domains including music (Tata and Di Eugenio, 2010), restaurants (Oraby et al., 2017; Juuti et al., 2018; Li et al., 2019a; Braˇzinskas et al., 2020), and products (Catherine and Cohen, 2018; Li et al., 2019a; Li and Tuzhilin, 2019; Dong et al., 2017; Ni and McAuley, 2018; Braˇzinskas et al., 2020). These methods generally apply a sequence-to-sequence model with attention to aspects and attributes (e.g. food type). Compared to these domains, paper review generation is much more challenging because it requires the model to perform deep understanding on paper content, construct knowledge graphs to compare knowledge elements across sections and papers, and synthesize information as input evidence for comment generation. Controlled Knowledge-Driven Generation. There have been some other studies on text generation controlled"
2020.inlg-1.44,P19-1479,0,0.115089,"ch review category following a rich set of evidence, and use a much larger data set. Nagata (2019) generates comment sentences to explain grammatical errors as feedback to improve paper writing. (Xing et al., 2020; Luu et al., 2020) extract paper-paper relations and 8 https://naacl2018.wordpress.com/2018/02/26/acceptanceand-author-feedback/ use them to guide citation text generation. Review Generation in other Domains. Automatic review generation techniques have been applied to many other domains including music (Tata and Di Eugenio, 2010), restaurants (Oraby et al., 2017; Juuti et al., 2018; Li et al., 2019a; Braˇzinskas et al., 2020), and products (Catherine and Cohen, 2018; Li et al., 2019a; Li and Tuzhilin, 2019; Dong et al., 2017; Ni and McAuley, 2018; Braˇzinskas et al., 2020). These methods generally apply a sequence-to-sequence model with attention to aspects and attributes (e.g. food type). Compared to these domains, paper review generation is much more challenging because it requires the model to perform deep understanding on paper content, construct knowledge graphs to compare knowledge elements across sections and papers, and synthesize information as input evidence for comment genera"
2020.inlg-1.44,P18-1138,0,0.024255,"on. Controlled Knowledge-Driven Generation. There have been some other studies on text generation controlled by sentiment (Hu et al., 2017), topic (Krishna and Srinivasan, 2018), text style (Shen et al., 2017; Liu et al., 2019a; Tikhonov et al., 2019), and facts (Wang et al., 2020). The usage of external supportive knowledge in text generation can be roughly divided into the following three levels: (1) Knowledge Description, which transforms structured data into unstructured text, such as Table-to-Text Generation (Mei et al., 2016; Lebret et al., 2016; Chisholm et al., 2017; Sha et al., 2018; Liu et al., 2018b; Nema et al., 2018; Wang et al., 2018a; Moryossef et al., 2019; Nie et al., 2019; Castro Ferreira et al., 2019; Wang et al., 2020; Shahidi et al., 2020) and its variants in low-resource (Ma et al., 2019) and multi-lingual setting (Kaffee et al., 2018a,b), Data-to-Document (Wiseman et al., 2017; Puduppully et al., 2019; Gong et al., 2019; Iso et al., 2019), Graph-toText (Flanigan et al., 2016; Song et al., 2018; Zhu et al., 2019; Koncel-Kedziorski et al., 2019), and Topic-to-text (Tang et al., 2019), and Knowledge Base Description (Kiddon et al., 2016; Gardent et al., 2017; Koncel-Kedziorski"
2020.inlg-1.44,P19-1600,0,0.090912,"sequence-to-sequence model with attention to aspects and attributes (e.g. food type). Compared to these domains, paper review generation is much more challenging because it requires the model to perform deep understanding on paper content, construct knowledge graphs to compare knowledge elements across sections and papers, and synthesize information as input evidence for comment generation. Controlled Knowledge-Driven Generation. There have been some other studies on text generation controlled by sentiment (Hu et al., 2017), topic (Krishna and Srinivasan, 2018), text style (Shen et al., 2017; Liu et al., 2019a; Tikhonov et al., 2019), and facts (Wang et al., 2020). The usage of external supportive knowledge in text generation can be roughly divided into the following three levels: (1) Knowledge Description, which transforms structured data into unstructured text, such as Table-to-Text Generation (Mei et al., 2016; Lebret et al., 2016; Chisholm et al., 2017; Sha et al., 2018; Liu et al., 2018b; Nema et al., 2018; Wang et al., 2018a; Moryossef et al., 2019; Nie et al., 2019; Castro Ferreira et al., 2019; Wang et al., 2020; Shahidi et al., 2020) and its variants in low-resource (Ma et al., 2019) and"
2020.inlg-1.44,D19-1187,0,0.134256,"sequence-to-sequence model with attention to aspects and attributes (e.g. food type). Compared to these domains, paper review generation is much more challenging because it requires the model to perform deep understanding on paper content, construct knowledge graphs to compare knowledge elements across sections and papers, and synthesize information as input evidence for comment generation. Controlled Knowledge-Driven Generation. There have been some other studies on text generation controlled by sentiment (Hu et al., 2017), topic (Krishna and Srinivasan, 2018), text style (Shen et al., 2017; Liu et al., 2019a; Tikhonov et al., 2019), and facts (Wang et al., 2020). The usage of external supportive knowledge in text generation can be roughly divided into the following three levels: (1) Knowledge Description, which transforms structured data into unstructured text, such as Table-to-Text Generation (Mei et al., 2016; Lebret et al., 2016; Chisholm et al., 2017; Sha et al., 2018; Liu et al., 2018b; Nema et al., 2018; Wang et al., 2018a; Moryossef et al., 2019; Nie et al., 2019; Castro Ferreira et al., 2019; Wang et al., 2020; Shahidi et al., 2020) and its variants in low-resource (Ma et al., 2019) and"
2020.inlg-1.44,D18-1360,0,0.0127694,"stics and Examples for Template Generalization sit with respect to existing literature? Are the references adequate? • Potential Impact: How significant is the work described? If the ideas are novel, will they also be useful or inspirational? Does the paper bring any new insights into the nature of the problem? 2.2 Knowledge Graph Construction Generating meaningful and explainable reviews requires ReviewRobot to understand the knowledge elements of each paper. We apply a state-of-theart Information Extraction (IE) system for Natural Language Processing (NLP) and Machine Learning (ML) domains (Luan et al., 2018) to construct the following knowledge graphs (KGs): • GPτ : A KG constructed from the abstract and conclusion sections of a target paper under review Pτ , which describes the main techniques. ¯ Pτ : A KG constructed from the related work • G section of Pτ , which describes related techniques. • GB : A background KG constructed from all of the old NLP/ML papers published before the publication year of Pτ , in order to teach ReviewRobot what’s happening in the field. assigned one of six types: Task, Method, Evaluation Metric, Material, Other Scientific Terms, and Generic Terms. Following the pre"
2020.inlg-1.44,P19-1197,0,0.0163511,"017; Liu et al., 2019a; Tikhonov et al., 2019), and facts (Wang et al., 2020). The usage of external supportive knowledge in text generation can be roughly divided into the following three levels: (1) Knowledge Description, which transforms structured data into unstructured text, such as Table-to-Text Generation (Mei et al., 2016; Lebret et al., 2016; Chisholm et al., 2017; Sha et al., 2018; Liu et al., 2018b; Nema et al., 2018; Wang et al., 2018a; Moryossef et al., 2019; Nie et al., 2019; Castro Ferreira et al., 2019; Wang et al., 2020; Shahidi et al., 2020) and its variants in low-resource (Ma et al., 2019) and multi-lingual setting (Kaffee et al., 2018a,b), Data-to-Document (Wiseman et al., 2017; Puduppully et al., 2019; Gong et al., 2019; Iso et al., 2019), Graph-toText (Flanigan et al., 2016; Song et al., 2018; Zhu et al., 2019; Koncel-Kedziorski et al., 2019), and Topic-to-text (Tang et al., 2019), and Knowledge Base Description (Kiddon et al., 2016; Gardent et al., 2017; Koncel-Kedziorski et al., 2019); (2) Knowledge Synthesis, which retrieves knowledge base and organizes text answers, such as Video Caption Generation (Whitehead et al., 2018), KB-supported Dialogue Generation (Han et al., 2"
2020.inlg-1.44,N16-1086,0,0.0158925,"ns and papers, and synthesize information as input evidence for comment generation. Controlled Knowledge-Driven Generation. There have been some other studies on text generation controlled by sentiment (Hu et al., 2017), topic (Krishna and Srinivasan, 2018), text style (Shen et al., 2017; Liu et al., 2019a; Tikhonov et al., 2019), and facts (Wang et al., 2020). The usage of external supportive knowledge in text generation can be roughly divided into the following three levels: (1) Knowledge Description, which transforms structured data into unstructured text, such as Table-to-Text Generation (Mei et al., 2016; Lebret et al., 2016; Chisholm et al., 2017; Sha et al., 2018; Liu et al., 2018b; Nema et al., 2018; Wang et al., 2018a; Moryossef et al., 2019; Nie et al., 2019; Castro Ferreira et al., 2019; Wang et al., 2020; Shahidi et al., 2020) and its variants in low-resource (Ma et al., 2019) and multi-lingual setting (Kaffee et al., 2018a,b), Data-to-Document (Wiseman et al., 2017; Puduppully et al., 2019; Gong et al., 2019; Iso et al., 2019), Graph-toText (Flanigan et al., 2016; Song et al., 2018; Zhu et al., 2019; Koncel-Kedziorski et al., 2019), and Topic-to-text (Tang et al., 2019), and Knowledge"
2020.inlg-1.44,N19-1236,0,0.0258224,"Missing"
2020.inlg-1.44,D19-1316,0,0.0171174,"cting paper acceptance (Leskovec and Wang, 2014). We extend these methods to score prediction and comment generation with detailed knowledge element level evidence for each specific review category. Paper Review Generation. Bartoli et al. (2016) proposes the first deep neural network framework to generate paper review comments. The generator is trained with 48 papers from their own lab. In comparison, we perform more concrete and explainable review generation by predicting scores and generating comments for each review category following a rich set of evidence, and use a much larger data set. Nagata (2019) generates comment sentences to explain grammatical errors as feedback to improve paper writing. (Xing et al., 2020; Luu et al., 2020) extract paper-paper relations and 8 https://naacl2018.wordpress.com/2018/02/26/acceptanceand-author-feedback/ use them to guide citation text generation. Review Generation in other Domains. Automatic review generation techniques have been applied to many other domains including music (Tata and Di Eugenio, 2010), restaurants (Oraby et al., 2017; Juuti et al., 2018; Li et al., 2019a; Braˇzinskas et al., 2020), and products (Catherine and Cohen, 2018; Li et al., 2"
2020.inlg-1.44,N18-1139,0,0.0480425,"Missing"
2020.inlg-1.44,P18-2112,0,0.021782,"cal errors as feedback to improve paper writing. (Xing et al., 2020; Luu et al., 2020) extract paper-paper relations and 8 https://naacl2018.wordpress.com/2018/02/26/acceptanceand-author-feedback/ use them to guide citation text generation. Review Generation in other Domains. Automatic review generation techniques have been applied to many other domains including music (Tata and Di Eugenio, 2010), restaurants (Oraby et al., 2017; Juuti et al., 2018; Li et al., 2019a; Braˇzinskas et al., 2020), and products (Catherine and Cohen, 2018; Li et al., 2019a; Li and Tuzhilin, 2019; Dong et al., 2017; Ni and McAuley, 2018; Braˇzinskas et al., 2020). These methods generally apply a sequence-to-sequence model with attention to aspects and attributes (e.g. food type). Compared to these domains, paper review generation is much more challenging because it requires the model to perform deep understanding on paper content, construct knowledge graphs to compare knowledge elements across sections and papers, and synthesize information as input evidence for comment generation. Controlled Knowledge-Driven Generation. There have been some other studies on text generation controlled by sentiment (Hu et al., 2017), topic (K"
2020.inlg-1.44,W19-8619,0,0.0116313,"text generation controlled by sentiment (Hu et al., 2017), topic (Krishna and Srinivasan, 2018), text style (Shen et al., 2017; Liu et al., 2019a; Tikhonov et al., 2019), and facts (Wang et al., 2020). The usage of external supportive knowledge in text generation can be roughly divided into the following three levels: (1) Knowledge Description, which transforms structured data into unstructured text, such as Table-to-Text Generation (Mei et al., 2016; Lebret et al., 2016; Chisholm et al., 2017; Sha et al., 2018; Liu et al., 2018b; Nema et al., 2018; Wang et al., 2018a; Moryossef et al., 2019; Nie et al., 2019; Castro Ferreira et al., 2019; Wang et al., 2020; Shahidi et al., 2020) and its variants in low-resource (Ma et al., 2019) and multi-lingual setting (Kaffee et al., 2018a,b), Data-to-Document (Wiseman et al., 2017; Puduppully et al., 2019; Gong et al., 2019; Iso et al., 2019), Graph-toText (Flanigan et al., 2016; Song et al., 2018; Zhu et al., 2019; Koncel-Kedziorski et al., 2019), and Topic-to-text (Tang et al., 2019), and Knowledge Base Description (Kiddon et al., 2016; Gardent et al., 2017; Koncel-Kedziorski et al., 2019); (2) Knowledge Synthesis, which retrieves knowledge base and organiz"
2020.inlg-1.44,P17-1187,0,0.0220323,"Missing"
2020.inlg-1.44,W17-4904,0,0.0278238,"ng scores and generating comments for each review category following a rich set of evidence, and use a much larger data set. Nagata (2019) generates comment sentences to explain grammatical errors as feedback to improve paper writing. (Xing et al., 2020; Luu et al., 2020) extract paper-paper relations and 8 https://naacl2018.wordpress.com/2018/02/26/acceptanceand-author-feedback/ use them to guide citation text generation. Review Generation in other Domains. Automatic review generation techniques have been applied to many other domains including music (Tata and Di Eugenio, 2010), restaurants (Oraby et al., 2017; Juuti et al., 2018; Li et al., 2019a; Braˇzinskas et al., 2020), and products (Catherine and Cohen, 2018; Li et al., 2019a; Li and Tuzhilin, 2019; Dong et al., 2017; Ni and McAuley, 2018; Braˇzinskas et al., 2020). These methods generally apply a sequence-to-sequence model with attention to aspects and attributes (e.g. food type). Compared to these domains, paper review generation is much more challenging because it requires the model to perform deep understanding on paper content, construct knowledge graphs to compare knowledge elements across sections and papers, and synthesize information"
2020.inlg-1.44,D18-1073,0,0.0149775,"ng (Kaffee et al., 2018a,b), Data-to-Document (Wiseman et al., 2017; Puduppully et al., 2019; Gong et al., 2019; Iso et al., 2019), Graph-toText (Flanigan et al., 2016; Song et al., 2018; Zhu et al., 2019; Koncel-Kedziorski et al., 2019), and Topic-to-text (Tang et al., 2019), and Knowledge Base Description (Kiddon et al., 2016; Gardent et al., 2017; Koncel-Kedziorski et al., 2019); (2) Knowledge Synthesis, which retrieves knowledge base and organizes text answers, such as Video Caption Generation (Whitehead et al., 2018), KB-supported Dialogue Generation (Han et al., 2015; Zhou et al., 2018; Parthasarathi and Pineau, 2018; Liu et al., 2018a; Young et al., 2018; Wen et al., 2018; Chen et al., 2019; Liu et al., 2019b), 391 Knowledge-guided comment Generation (Li et al., 2019b), paper generation (Wang et al., 2018b, 2019; Cachola et al., 2020) , and abstractive summarization (Gu et al., 2016; Sharma et al., 2019; Huang et al., 2020). 5 Application Limitations and Ethical Statement The types of evidence we have designed in this paper are limited to NLP, ML or related areas, and thus they are not applicable to other scientific domains such as biomedical science and chemistry. Whether ReviewRobot is essentially bene"
2020.inlg-1.44,P19-1195,0,0.0199132,"Missing"
2020.inlg-1.44,2020.acl-main.355,0,0.0208021,"rishna and Srinivasan, 2018), text style (Shen et al., 2017; Liu et al., 2019a; Tikhonov et al., 2019), and facts (Wang et al., 2020). The usage of external supportive knowledge in text generation can be roughly divided into the following three levels: (1) Knowledge Description, which transforms structured data into unstructured text, such as Table-to-Text Generation (Mei et al., 2016; Lebret et al., 2016; Chisholm et al., 2017; Sha et al., 2018; Liu et al., 2018b; Nema et al., 2018; Wang et al., 2018a; Moryossef et al., 2019; Nie et al., 2019; Castro Ferreira et al., 2019; Wang et al., 2020; Shahidi et al., 2020) and its variants in low-resource (Ma et al., 2019) and multi-lingual setting (Kaffee et al., 2018a,b), Data-to-Document (Wiseman et al., 2017; Puduppully et al., 2019; Gong et al., 2019; Iso et al., 2019), Graph-toText (Flanigan et al., 2016; Song et al., 2018; Zhu et al., 2019; Koncel-Kedziorski et al., 2019), and Topic-to-text (Tang et al., 2019), and Knowledge Base Description (Kiddon et al., 2016; Gardent et al., 2017; Koncel-Kedziorski et al., 2019); (2) Knowledge Synthesis, which retrieves knowledge base and organizes text answers, such as Video Caption Generation (Whitehead et al., 201"
2020.inlg-1.44,D19-1323,0,0.0195583,"tion (Kiddon et al., 2016; Gardent et al., 2017; Koncel-Kedziorski et al., 2019); (2) Knowledge Synthesis, which retrieves knowledge base and organizes text answers, such as Video Caption Generation (Whitehead et al., 2018), KB-supported Dialogue Generation (Han et al., 2015; Zhou et al., 2018; Parthasarathi and Pineau, 2018; Liu et al., 2018a; Young et al., 2018; Wen et al., 2018; Chen et al., 2019; Liu et al., 2019b), 391 Knowledge-guided comment Generation (Li et al., 2019b), paper generation (Wang et al., 2018b, 2019; Cachola et al., 2020) , and abstractive summarization (Gu et al., 2016; Sharma et al., 2019; Huang et al., 2020). 5 Application Limitations and Ethical Statement The types of evidence we have designed in this paper are limited to NLP, ML or related areas, and thus they are not applicable to other scientific domains such as biomedical science and chemistry. Whether ReviewRobot is essentially beneficial to the scientific community also depends on who uses it. Here are some example scenarios where ReviewRobot should and should not be used: • Should-Do: Reviewers use ReviewRobot merely as an assistant to write more constructive comments and compare notes. background papers, and summariz"
2020.inlg-1.44,P18-1150,0,0.0141311,"e Description, which transforms structured data into unstructured text, such as Table-to-Text Generation (Mei et al., 2016; Lebret et al., 2016; Chisholm et al., 2017; Sha et al., 2018; Liu et al., 2018b; Nema et al., 2018; Wang et al., 2018a; Moryossef et al., 2019; Nie et al., 2019; Castro Ferreira et al., 2019; Wang et al., 2020; Shahidi et al., 2020) and its variants in low-resource (Ma et al., 2019) and multi-lingual setting (Kaffee et al., 2018a,b), Data-to-Document (Wiseman et al., 2017; Puduppully et al., 2019; Gong et al., 2019; Iso et al., 2019), Graph-toText (Flanigan et al., 2016; Song et al., 2018; Zhu et al., 2019; Koncel-Kedziorski et al., 2019), and Topic-to-text (Tang et al., 2019), and Knowledge Base Description (Kiddon et al., 2016; Gardent et al., 2017; Koncel-Kedziorski et al., 2019); (2) Knowledge Synthesis, which retrieves knowledge base and organizes text answers, such as Video Caption Generation (Whitehead et al., 2018), KB-supported Dialogue Generation (Han et al., 2015; Zhou et al., 2018; Parthasarathi and Pineau, 2018; Liu et al., 2018a; Young et al., 2018; Wen et al., 2018; Chen et al., 2019; Liu et al., 2019b), 391 Knowledge-guided comment Generation (Li et al., 2019b)"
2020.inlg-1.44,P15-2050,0,0.0193771,"Missing"
2020.inlg-1.44,D19-1513,0,0.0232792,"Text Generation (Mei et al., 2016; Lebret et al., 2016; Chisholm et al., 2017; Sha et al., 2018; Liu et al., 2018b; Nema et al., 2018; Wang et al., 2018a; Moryossef et al., 2019; Nie et al., 2019; Castro Ferreira et al., 2019; Wang et al., 2020; Shahidi et al., 2020) and its variants in low-resource (Ma et al., 2019) and multi-lingual setting (Kaffee et al., 2018a,b), Data-to-Document (Wiseman et al., 2017; Puduppully et al., 2019; Gong et al., 2019; Iso et al., 2019), Graph-toText (Flanigan et al., 2016; Song et al., 2018; Zhu et al., 2019; Koncel-Kedziorski et al., 2019), and Topic-to-text (Tang et al., 2019), and Knowledge Base Description (Kiddon et al., 2016; Gardent et al., 2017; Koncel-Kedziorski et al., 2019); (2) Knowledge Synthesis, which retrieves knowledge base and organizes text answers, such as Video Caption Generation (Whitehead et al., 2018), KB-supported Dialogue Generation (Han et al., 2015; Zhou et al., 2018; Parthasarathi and Pineau, 2018; Liu et al., 2018a; Young et al., 2018; Wen et al., 2018; Chen et al., 2019; Liu et al., 2019b), 391 Knowledge-guided comment Generation (Li et al., 2019b), paper generation (Wang et al., 2018b, 2019; Cachola et al., 2020) , and abstractive summ"
2020.inlg-1.44,P10-1140,0,0.0868757,"Missing"
2020.inlg-1.44,D19-1406,0,0.0194965,"e model with attention to aspects and attributes (e.g. food type). Compared to these domains, paper review generation is much more challenging because it requires the model to perform deep understanding on paper content, construct knowledge graphs to compare knowledge elements across sections and papers, and synthesize information as input evidence for comment generation. Controlled Knowledge-Driven Generation. There have been some other studies on text generation controlled by sentiment (Hu et al., 2017), topic (Krishna and Srinivasan, 2018), text style (Shen et al., 2017; Liu et al., 2019a; Tikhonov et al., 2019), and facts (Wang et al., 2020). The usage of external supportive knowledge in text generation can be roughly divided into the following three levels: (1) Knowledge Description, which transforms structured data into unstructured text, such as Table-to-Text Generation (Mei et al., 2016; Lebret et al., 2016; Chisholm et al., 2017; Sha et al., 2018; Liu et al., 2018b; Nema et al., 2018; Wang et al., 2018a; Moryossef et al., 2019; Nie et al., 2019; Castro Ferreira et al., 2019; Wang et al., 2020; Shahidi et al., 2020) and its variants in low-resource (Ma et al., 2019) and multi-lingual setting (Ka"
2020.inlg-1.44,I08-4006,0,0.0247663,"rd representations (unsure about wsi) by modeling sememe information than other competitive baselines. Meaningful Comparison * [SYSTEM] The following related papers are missing: 1. About low-dimensional semantic space: (a) Unsupervised approximate-semantic vocabulary learning for human action and video classification (Zhao and Ip, 2013) Qiong Zhao and Horace HS Ip. 2013. Unsupervised Approximate-semantic Vocabulary Learning for Human Action and Video Classification. Pattern Recognition Letters, 34(15):1870–1878. 2. About sememes: (a) Chinese Word Sense Disambiguation with PageRank and HowNet (Wang et al., 2008): Jinghua Wang, Jianyi Liu, and Ping Zhang. 2008. Chinese Word Sense Disambiguation with PageRank and HowNet. In Proceedings of the Sixth SIGHAN Workshop on Chinese Language Processing. (b) A maximum entropy approach to HowNet-based Chinese word sense disambiguation (Wong and Yang, 2002): Ping Wai Wong and Yongsheng Yang. 2002. A Maximum Entropy Approach to HowNet-based Chinese Word Sense Disambiguation. In COLING-02: SEMANET: Building and Using Semantic Networks. 3. About word similarity and word analogy: (a) Open IE as an Intermediate Structure for Semantic Tasks (Stanovsky et al., 2015): Ga"
2020.inlg-1.44,P19-1191,1,0.875487,"Missing"
2020.inlg-1.44,P18-2042,1,0.848758,"tion. There have been some other studies on text generation controlled by sentiment (Hu et al., 2017), topic (Krishna and Srinivasan, 2018), text style (Shen et al., 2017; Liu et al., 2019a; Tikhonov et al., 2019), and facts (Wang et al., 2020). The usage of external supportive knowledge in text generation can be roughly divided into the following three levels: (1) Knowledge Description, which transforms structured data into unstructured text, such as Table-to-Text Generation (Mei et al., 2016; Lebret et al., 2016; Chisholm et al., 2017; Sha et al., 2018; Liu et al., 2018b; Nema et al., 2018; Wang et al., 2018a; Moryossef et al., 2019; Nie et al., 2019; Castro Ferreira et al., 2019; Wang et al., 2020; Shahidi et al., 2020) and its variants in low-resource (Ma et al., 2019) and multi-lingual setting (Kaffee et al., 2018a,b), Data-to-Document (Wiseman et al., 2017; Puduppully et al., 2019; Gong et al., 2019; Iso et al., 2019), Graph-toText (Flanigan et al., 2016; Song et al., 2018; Zhu et al., 2019; Koncel-Kedziorski et al., 2019), and Topic-to-text (Tang et al., 2019), and Knowledge Base Description (Kiddon et al., 2016; Gardent et al., 2017; Koncel-Kedziorski et al., 2019); (2) Knowledge Synthesis,"
2020.inlg-1.44,2020.acl-main.101,0,0.0148584,"nd attributes (e.g. food type). Compared to these domains, paper review generation is much more challenging because it requires the model to perform deep understanding on paper content, construct knowledge graphs to compare knowledge elements across sections and papers, and synthesize information as input evidence for comment generation. Controlled Knowledge-Driven Generation. There have been some other studies on text generation controlled by sentiment (Hu et al., 2017), topic (Krishna and Srinivasan, 2018), text style (Shen et al., 2017; Liu et al., 2019a; Tikhonov et al., 2019), and facts (Wang et al., 2020). The usage of external supportive knowledge in text generation can be roughly divided into the following three levels: (1) Knowledge Description, which transforms structured data into unstructured text, such as Table-to-Text Generation (Mei et al., 2016; Lebret et al., 2016; Chisholm et al., 2017; Sha et al., 2018; Liu et al., 2018b; Nema et al., 2018; Wang et al., 2018a; Moryossef et al., 2019; Nie et al., 2019; Castro Ferreira et al., 2019; Wang et al., 2020; Shahidi et al., 2020) and its variants in low-resource (Ma et al., 2019) and multi-lingual setting (Kaffee et al., 2018a,b), Data-to-"
2020.inlg-1.44,C18-1320,0,0.0127207,"duppully et al., 2019; Gong et al., 2019; Iso et al., 2019), Graph-toText (Flanigan et al., 2016; Song et al., 2018; Zhu et al., 2019; Koncel-Kedziorski et al., 2019), and Topic-to-text (Tang et al., 2019), and Knowledge Base Description (Kiddon et al., 2016; Gardent et al., 2017; Koncel-Kedziorski et al., 2019); (2) Knowledge Synthesis, which retrieves knowledge base and organizes text answers, such as Video Caption Generation (Whitehead et al., 2018), KB-supported Dialogue Generation (Han et al., 2015; Zhou et al., 2018; Parthasarathi and Pineau, 2018; Liu et al., 2018a; Young et al., 2018; Wen et al., 2018; Chen et al., 2019; Liu et al., 2019b), 391 Knowledge-guided comment Generation (Li et al., 2019b), paper generation (Wang et al., 2018b, 2019; Cachola et al., 2020) , and abstractive summarization (Gu et al., 2016; Sharma et al., 2019; Huang et al., 2020). 5 Application Limitations and Ethical Statement The types of evidence we have designed in this paper are limited to NLP, ML or related areas, and thus they are not applicable to other scientific domains such as biomedical science and chemistry. Whether ReviewRobot is essentially beneficial to the scientific community also depends on who us"
2020.inlg-1.44,D18-1433,1,0.840706,"Shahidi et al., 2020) and its variants in low-resource (Ma et al., 2019) and multi-lingual setting (Kaffee et al., 2018a,b), Data-to-Document (Wiseman et al., 2017; Puduppully et al., 2019; Gong et al., 2019; Iso et al., 2019), Graph-toText (Flanigan et al., 2016; Song et al., 2018; Zhu et al., 2019; Koncel-Kedziorski et al., 2019), and Topic-to-text (Tang et al., 2019), and Knowledge Base Description (Kiddon et al., 2016; Gardent et al., 2017; Koncel-Kedziorski et al., 2019); (2) Knowledge Synthesis, which retrieves knowledge base and organizes text answers, such as Video Caption Generation (Whitehead et al., 2018), KB-supported Dialogue Generation (Han et al., 2015; Zhou et al., 2018; Parthasarathi and Pineau, 2018; Liu et al., 2018a; Young et al., 2018; Wen et al., 2018; Chen et al., 2019; Liu et al., 2019b), 391 Knowledge-guided comment Generation (Li et al., 2019b), paper generation (Wang et al., 2018b, 2019; Cachola et al., 2020) , and abstractive summarization (Gu et al., 2016; Sharma et al., 2019; Huang et al., 2020). 5 Application Limitations and Ethical Statement The types of evidence we have designed in this paper are limited to NLP, ML or related areas, and thus they are not applicable to oth"
2020.inlg-1.44,2020.acl-main.550,0,0.0276593,"ation with detailed knowledge element level evidence for each specific review category. Paper Review Generation. Bartoli et al. (2016) proposes the first deep neural network framework to generate paper review comments. The generator is trained with 48 papers from their own lab. In comparison, we perform more concrete and explainable review generation by predicting scores and generating comments for each review category following a rich set of evidence, and use a much larger data set. Nagata (2019) generates comment sentences to explain grammatical errors as feedback to improve paper writing. (Xing et al., 2020; Luu et al., 2020) extract paper-paper relations and 8 https://naacl2018.wordpress.com/2018/02/26/acceptanceand-author-feedback/ use them to guide citation text generation. Review Generation in other Domains. Automatic review generation techniques have been applied to many other domains including music (Tata and Di Eugenio, 2010), restaurants (Oraby et al., 2017; Juuti et al., 2018; Li et al., 2019a; Braˇzinskas et al., 2020), and products (Catherine and Cohen, 2018; Li et al., 2019a; Li and Tuzhilin, 2019; Dong et al., 2017; Ni and McAuley, 2018; Braˇzinskas et al., 2020). These methods gene"
2020.inlg-1.44,D19-1548,0,0.0133477,"h transforms structured data into unstructured text, such as Table-to-Text Generation (Mei et al., 2016; Lebret et al., 2016; Chisholm et al., 2017; Sha et al., 2018; Liu et al., 2018b; Nema et al., 2018; Wang et al., 2018a; Moryossef et al., 2019; Nie et al., 2019; Castro Ferreira et al., 2019; Wang et al., 2020; Shahidi et al., 2020) and its variants in low-resource (Ma et al., 2019) and multi-lingual setting (Kaffee et al., 2018a,b), Data-to-Document (Wiseman et al., 2017; Puduppully et al., 2019; Gong et al., 2019; Iso et al., 2019), Graph-toText (Flanigan et al., 2016; Song et al., 2018; Zhu et al., 2019; Koncel-Kedziorski et al., 2019), and Topic-to-text (Tang et al., 2019), and Knowledge Base Description (Kiddon et al., 2016; Gardent et al., 2017; Koncel-Kedziorski et al., 2019); (2) Knowledge Synthesis, which retrieves knowledge base and organizes text answers, such as Video Caption Generation (Whitehead et al., 2018), KB-supported Dialogue Generation (Han et al., 2015; Zhou et al., 2018; Parthasarathi and Pineau, 2018; Liu et al., 2018a; Young et al., 2018; Wen et al., 2018; Chen et al., 2019; Liu et al., 2019b), 391 Knowledge-guided comment Generation (Li et al., 2019b), paper generation"
2020.iwslt-1.1,P18-4020,0,0.0279555,"Missing"
2020.iwslt-1.1,2005.iwslt-1.19,0,0.174539,"Missing"
2020.iwslt-1.1,2020.iwslt-1.11,0,0.0306756,"Missing"
2020.iwslt-1.1,W18-6319,0,0.0215087,"Missing"
2020.iwslt-1.1,2013.iwslt-papers.14,0,0.069836,"Missing"
2020.iwslt-1.1,2020.iwslt-1.9,0,0.053316,"Missing"
2020.iwslt-1.1,rousseau-etal-2014-enhancing,0,0.0549836,"Missing"
2020.iwslt-1.1,2020.iwslt-1.22,0,0.0613503,"Missing"
2020.wmt-1.7,N12-1047,0,0.0429897,"rch Input: a model list Ωcand sorted by the scores on development data. Output: a final model list Φf inal 1 for all combination of 2 models that model ∈ top-8 models do 2 obtain translation by ensemble decoding and evaluate with BLEU score; 3 end 4 Choose the best 2 model combination as the initial Φf inal ; 5 while there is tiny improvement as the model number increases do 6 choose one single model from the rest of Ωcand to the Φf inal which performs better when combined with Φf inal ; 7 end 3.7 We obtain n-best hypotheses with an ensemble model and then train a re-ranker using k-best MIRA (Cherry and Foster, 2012) on the validation set. Kbest MIRA works with a batch tuning to learn a re-ranker for the n-best hypotheses. The features we use for re-ranking are: • Length Features: length ratio and length difference between the source sentences and hypotheses are composed of two parts: documents created originally in Chinese (translation style) and documents created originally in English (native style). For the Chinese→English task, if the Chinese sentences are created from native Chinese corpus, then the corresponding English sentences are in translation style, so the model fine-tuned on these parallel se"
2020.wmt-1.7,N19-1423,0,0.0591367,"Missing"
2020.wmt-1.7,P13-2119,0,0.027705,"n-domain data obtained by the above methods are adopted to fine-tuning the single model and provide about a 2 BLEU scores improvement. In-domain Data Selection and Fine-tuning 3.5 Domain adaptation plays an important role in improving the performance towards given test data. A practical method for domain adaptation is training on the large-scale data and then fine-tuning on the in-domain data (Luong and Manning, 2015). We select the small in-domain corpus with several approaches, including N-grams language model similarity and binary classification. N-grams: We adopt the algorithm proposed in Duh et al. (2013); Axelrod et al. (2011), which selects sentence pairs from the large out-of-domain corpus that are similar to the in-domain data. In our work, we train a tri-grams token-level language model for English and a bi-grams character-level language model for Chinese. We use the parallel texts as the out-of-domain corpus and all available test sets in the past WMT tasks and News Commentary as the in-domain corpus. We score the sentence pairs with bilingual cross-entropy differences as follows: CE(HI−SRC , HO−SRC )+CE(HI−T GT , HO−T GT ) (1) where we denote out-of-domain corpus as O, indomain corpus a"
2020.wmt-1.7,N13-1073,0,0.0449118,"le improvement in the performance while maintaining a manageable network size. We adopt a Transformer with FFN size of 8, 192 and a Transformer with FFN 106 • Normalize punctuation with Moses scripts • Filter out the sentences longer than 120 words or sentences including a single word more than 40 characters. • Filter out the sentences which contain HTML tags or duplicated translations. • Filter out the sentences whose languages detected by fastText1 (Joulin et al., 2017) are not identical to the translation direction. • Filter out the sentences whose alignment scores obtained by fast-align2 (Dyer et al., 2013) are low. • Filter out the sentences whose n-gram scores from language models are low. • Filter out the sentences whose length ratio between the source and target are not in range of 1 : 3 and 3 : 1 1 2 https://github.com/facebookresearch/fastText https://github.com/clab/fast align In this paper, we also filter out noisy sentence pairs with the translation acceptability filter proposed in (Zhang et al., 2020). Specifically, we feed the sentence pair (s, t) into multilingual BERT, which accepts two-sentence input due to its nextsentence prediction objective. Instead of using the [CLS] token rep"
2020.wmt-1.7,D18-1045,0,0.12771,"lation direction, we train several variants of Transformer (Vaswani et al., 2017) models on the provided parallel data enlarged with synthetic data from monolingual data. We experiment with several techniques proposed in the past translation tasks and adopt effective ones as components of our system. Our data preparation pipeline consists of data filtering, data augmentation, and data selection. For data filtering, we filter sentence pairs based on language model scoring, alignment model scoring, etc. For data augmentation, we experiment with iterative back-translation (Sennrich et al., 2016; Edunov et al., 2018) methods and iterative knowledge distillation (Freitag et al., 2017) methods. We leverage source-side monolingual data by applying iterative knowledge distillation, and target-side monolingual data by back-translation methods, including greedy search, beam search, and noised beam search. For data selection, we select an in-domain corpus with N-grams language models and binary classifiers. A tri-gram token-level language model and a bi-gram character-level language model are introduced for English and Chinese respectively. Out-of-domain To enhance a single model, we use several variants of Tran"
2020.wmt-1.7,W19-5317,0,0.0285832,"ikely to overfit, we set the dropout rate from 0.1 to 0.3 and use a label smoothing rate of 0.2. 2.4 We reverse the source sentences of the bilingual corpus and train a Transformer with source reversed. In this way, the model can learns a different meaning of the positional embeddings, which helps capture the source sentences from a different perspective. Viewing source in a reversed order provides another kind of model diversity and data diversity and presents positive effects in the final model ensemble. 3 System Overiew 3.1 Data Filtering Previous works (Sun et al., 2019; Xia et al., 2019; Guo et al., 2019) show that the translation performance improves as the quality of parallel corpus improves. We filter the training bilingual corpus with the following schemes: The original Transformer leverages position information by taking absolute positional embeddings as inputs and does not explicitly capture the information in its structure. Thus the original Transformer cannot leverage position information efficiently. Here we used relative positional embeddings in the self-attention mechanism proposed in Shaw et al. (2018) for the encoder layers and decoder layers. We do an ablation study and find that"
2020.wmt-1.7,D11-1033,0,0.0264772,"ned by the above methods are adopted to fine-tuning the single model and provide about a 2 BLEU scores improvement. In-domain Data Selection and Fine-tuning 3.5 Domain adaptation plays an important role in improving the performance towards given test data. A practical method for domain adaptation is training on the large-scale data and then fine-tuning on the in-domain data (Luong and Manning, 2015). We select the small in-domain corpus with several approaches, including N-grams language model similarity and binary classification. N-grams: We adopt the algorithm proposed in Duh et al. (2013); Axelrod et al. (2011), which selects sentence pairs from the large out-of-domain corpus that are similar to the in-domain data. In our work, we train a tri-grams token-level language model for English and a bi-grams character-level language model for Chinese. We use the parallel texts as the out-of-domain corpus and all available test sets in the past WMT tasks and News Commentary as the in-domain corpus. We score the sentence pairs with bilingual cross-entropy differences as follows: CE(HI−SRC , HO−SRC )+CE(HI−T GT , HO−T GT ) (1) where we denote out-of-domain corpus as O, indomain corpus as I. HI−SRC denotes lan"
2020.wmt-1.7,P84-1044,0,0.638848,"Missing"
2020.wmt-1.7,E17-2068,0,0.0320918,"embedding dimension, FFN size, number of heads, and number of layers. We find that using a larger FFN size (8, 192 or 15, 000) gives a reasonable improvement in the performance while maintaining a manageable network size. We adopt a Transformer with FFN size of 8, 192 and a Transformer with FFN 106 • Normalize punctuation with Moses scripts • Filter out the sentences longer than 120 words or sentences including a single word more than 40 characters. • Filter out the sentences which contain HTML tags or duplicated translations. • Filter out the sentences whose languages detected by fastText1 (Joulin et al., 2017) are not identical to the translation direction. • Filter out the sentences whose alignment scores obtained by fast-align2 (Dyer et al., 2013) are low. • Filter out the sentences whose n-gram scores from language models are low. • Filter out the sentences whose length ratio between the source and target are not in range of 1 : 3 and 3 : 1 1 2 https://github.com/facebookresearch/fastText https://github.com/clab/fast align In this paper, we also filter out noisy sentence pairs with the translation acceptability filter proposed in (Zhang et al., 2020). Specifically, we feed the sentence pair (s,"
2020.wmt-1.7,P17-4012,0,0.0391002,"finetuning data, all available newstest data and News Model Ensemble Ensemble learning is a widely used technique in the real-world tasks, which provides performance improvement by taking advantages of multiple single models. In neural machine translation, a practical way of the model ensemble is to combine the full probability distribution over the target vocabulary of different models at each step during sequence prediction. We experiment with the max, avg, and log-avg strategies, and find the log-avg strategy achieves the best performance. We implement a model ensemble module in OpenNMT3 (Klein et al., 2017). In our experiments, we observe that simply enlarging the size of ensemble models does not necessarily improve translation performance. However, brute-force search of all models is prohibitively expensive and unrealistic. As the number of models increases, the decoding of the ensemble will take more time than a single model and exceed the limits of computer resource capacity. Therefore, we adopt a greedy model ensemble algorithm (Li et al., 2019) as shown in Algorithm 1. Since model and data diversity are important factors for an ensemble system, we train diverse models with different initial"
2020.wmt-1.7,W19-5325,0,0.0717757,"max, avg, and log-avg strategies, and find the log-avg strategy achieves the best performance. We implement a model ensemble module in OpenNMT3 (Klein et al., 2017). In our experiments, we observe that simply enlarging the size of ensemble models does not necessarily improve translation performance. However, brute-force search of all models is prohibitively expensive and unrealistic. As the number of models increases, the decoding of the ensemble will take more time than a single model and exceed the limits of computer resource capacity. Therefore, we adopt a greedy model ensemble algorithm (Li et al., 2019) as shown in Algorithm 1. Since model and data diversity are important factors for an ensemble system, we train diverse models with different initialization seeds, different parameters, different architectures, and different training data sets. All the models are fine-tuned to achieve superior performance. 3.6 Domain Style Translation Translation performance differs in different topic domains. For intuitive explanation, we take native style and translation style as an example, and our topic domains are generated by using unsupervised clustering, not limited to these two styles. Native style an"
2020.wmt-1.7,2015.iwslt-evaluation.11,0,0.0163892,"rpus are regarded as negative data. Then BERT is exploited to score the sentence pairs. We sort all sentence pairs and select the top 600K sentences with the highest scores as fine-tuning data. All the in-domain data obtained by the above methods are adopted to fine-tuning the single model and provide about a 2 BLEU scores improvement. In-domain Data Selection and Fine-tuning 3.5 Domain adaptation plays an important role in improving the performance towards given test data. A practical method for domain adaptation is training on the large-scale data and then fine-tuning on the in-domain data (Luong and Manning, 2015). We select the small in-domain corpus with several approaches, including N-grams language model similarity and binary classification. N-grams: We adopt the algorithm proposed in Duh et al. (2013); Axelrod et al. (2011), which selects sentence pairs from the large out-of-domain corpus that are similar to the in-domain data. In our work, we train a tri-grams token-level language model for English and a bi-grams character-level language model for Chinese. We use the parallel texts as the out-of-domain corpus and all available test sets in the past WMT tasks and News Commentary as the in-domain c"
2020.wmt-1.7,W18-6319,0,0.0269198,"idate the model every 1, 000 steps on the development data and save the checkpoints with the best BLEU scores. After training, we average the last 10 checkpoints for every single model of the general domain. In the fine-tuning phase, we use the averaged model obtained in the training phase as pre-train weights for domain models, and train with indomain data selected as in Section 3.4 for 10, 000 steps without early stop. After fine-tuning, we average the last 10 checkpoints for every single model of the specific domain. For evaluation, we adopt the cased BLEU scores calculated with SacreBLEU (Post, 2018). 4.2 Pre-processing and Post-processing In pre-processing, we conduct data filtering, tokenization, subword encoding. For Chinese sentences, we use the DiDi tokenizer for tokenization. For English data, we do punctuation normalization and use Spacy4 tokenizer for tokenization. We filter parallel sentences as described in Section 3.1. Finally, we collect a preprocessed bilingual training 4 https://github.com/explosion/spaCy Chinese → English We adopt methods in Section 3 for Chinese → English task. Firstly we adopt techniques of iterative back-translation and knowledge distillation for generat"
2020.wmt-1.7,P16-1009,0,0.0349239,"rection. For this translation direction, we train several variants of Transformer (Vaswani et al., 2017) models on the provided parallel data enlarged with synthetic data from monolingual data. We experiment with several techniques proposed in the past translation tasks and adopt effective ones as components of our system. Our data preparation pipeline consists of data filtering, data augmentation, and data selection. For data filtering, we filter sentence pairs based on language model scoring, alignment model scoring, etc. For data augmentation, we experiment with iterative back-translation (Sennrich et al., 2016; Edunov et al., 2018) methods and iterative knowledge distillation (Freitag et al., 2017) methods. We leverage source-side monolingual data by applying iterative knowledge distillation, and target-side monolingual data by back-translation methods, including greedy search, beam search, and noised beam search. For data selection, we select an in-domain corpus with N-grams language models and binary classifiers. A tri-gram token-level language model and a bi-gram character-level language model are introduced for English and Chinese respectively. Out-of-domain To enhance a single model, we use se"
2020.wmt-1.7,N18-2074,0,0.131721,"17) methods. We leverage source-side monolingual data by applying iterative knowledge distillation, and target-side monolingual data by back-translation methods, including greedy search, beam search, and noised beam search. For data selection, we select an in-domain corpus with N-grams language models and binary classifiers. A tri-gram token-level language model and a bi-gram character-level language model are introduced for English and Chinese respectively. Out-of-domain To enhance a single model, we use several variants of Transformer, including Transformer with relative position attention (Shaw et al., 2018), Transformer with larger feedforward inner (FFN) size (8, 192 or 15, 000), and Transformer with reversed source. We then ensemble these models with adequate model diversity and data diversity to further improve the performance. Domain conflicts influence the translation performance significantly. For example, there exist differences between written English and spoken English. Usually, a model cannot do the best in all domains due to the conflicts. In this work, we propose to obtain domain information with unsupervised clustering and exploit this information for translation. Specifically, we p"
2020.wmt-1.7,W19-5341,0,0.185498,"mer with a larger FFN size is more likely to overfit, we set the dropout rate from 0.1 to 0.3 and use a label smoothing rate of 0.2. 2.4 We reverse the source sentences of the bilingual corpus and train a Transformer with source reversed. In this way, the model can learns a different meaning of the positional embeddings, which helps capture the source sentences from a different perspective. Viewing source in a reversed order provides another kind of model diversity and data diversity and presents positive effects in the final model ensemble. 3 System Overiew 3.1 Data Filtering Previous works (Sun et al., 2019; Xia et al., 2019; Guo et al., 2019) show that the translation performance improves as the quality of parallel corpus improves. We filter the training bilingual corpus with the following schemes: The original Transformer leverages position information by taking absolute positional embeddings as inputs and does not explicitly capture the information in its structure. Thus the original Transformer cannot leverage position information efficiently. Here we used relative positional embeddings in the self-attention mechanism proposed in Shaw et al. (2018) for the encoder layers and decoder layers."
2020.wmt-1.7,W19-5348,0,0.039035,"Missing"
2020.wmt-1.7,2020.acl-main.756,1,0.882869,"Missing"
2021.blackboxnlp-1.30,P17-1080,0,0.021865,"ful mathematical knowledge. As a simple example, a mathematician may look at a few values of the function 2n − 1: n 2n − 1 1 1 2 3 3 7 4 15 5 31 6 63 7 127 ... ... and notice patterns such as ‘if n is even, then 2n − 1 is divisible by 3’. This type of reasoning is frequent in the early stages of mathematical work. In order for software to assist people in identifying such patterns, it needs to have an internal representation of the basic properties of integers. Just as learned word representations can capture attributes like the word’s part of speech or syntactic dependency label (Köhn, 2015; Belinkov et al., 2017), we would like to develop integer embeddings that capture primality, divisibility by 3, etc. In this paper, we learn integer embeddings from mathematical resources and probe them for mathematical knowledge. Unlike most natural-language features, many number-theoretic properties are ∗ Research performed at DiDi Labs. 2, 3, 5, 7, 11, 13, 17, 19, 23, 29, ... (primes) 0, 1, 1, 2, 3, 5, 8, 13, 21, ... (Fibonacci) 1, 1, 2, 5, 14, 42, 132, 429, ... (Catalan) 0, 1, 3, 6, 2, 7, 13, 20, 12, 21, ... (Recamán) highly regular and often may be easily determined from the integer value itself (e.g. convertin"
2021.blackboxnlp-1.30,Q19-1004,0,0.0132093,"capture concepts that are useful for mathematical applications. We probe the integer embeddings for mathematical knowledge, apply them to a set of numerical reasoning tasks, and show that by learning the representations from mathematical sequence data, we can substantially improve over number embeddings learned from English text corpora. 1 Figure 1: Example sequences and their IDs in the Online Encyclopedia of Integer Sequences (OEIS). Introduction Word vector representations learned by neural models have been shown to capture linguistic knowledge that can be useful for downstream NLP tasks (Belinkov and Glass, 2019). In this work, we look at whether similarly-trained integer embeddings can represent useful mathematical knowledge. As a simple example, a mathematician may look at a few values of the function 2n − 1: n 2n − 1 1 1 2 3 3 7 4 15 5 31 6 63 7 127 ... ... and notice patterns such as ‘if n is even, then 2n − 1 is divisible by 3’. This type of reasoning is frequent in the early stages of mathematical work. In order for software to assist people in identifying such patterns, it needs to have an internal representation of the basic properties of integers. Just as learned word representations can capt"
2021.blackboxnlp-1.30,Q17-1010,0,0.0519004,"alysis (Hofmann, 1999) performs dimensionality reduction on a “document-term” matrix using truncated singular value decomposition. The rows of the matrix represent the integer sequences (as available in OEIS), the columns stand for the integer types, and the values in the cells are equal to the number of occurrences of each integer in each sequence. The resulting matrix is sparse, and its low rank yields vectors with a maximum of 65 dimensions. This method takes OEIS co-occurrences into account, but not the ordering within the sequences. FastText: we learn 100-dimensional FastText embeddings (Bojanowski et al., 2017), both with and without the additional subword-level information. 3.2 Pre-trained Embeddings Prior work has successfully employed word representations that are pre-trained on large text corpora for NLP applications. Naturally, these text corpora include numerical data as well, and some properties of a number can be inferred from the textual context (e.g., if an integer follows the phrase the year, it most likely has four digits). While the training data for these embeddings is not designed to encode integer properties, it may contain billions of tokens, compared to only ∼13M in OEIS. It has be"
2021.blackboxnlp-1.30,D15-1246,0,0.0190579,"epresent useful mathematical knowledge. As a simple example, a mathematician may look at a few values of the function 2n − 1: n 2n − 1 1 1 2 3 3 7 4 15 5 31 6 63 7 127 ... ... and notice patterns such as ‘if n is even, then 2n − 1 is divisible by 3’. This type of reasoning is frequent in the early stages of mathematical work. In order for software to assist people in identifying such patterns, it needs to have an internal representation of the basic properties of integers. Just as learned word representations can capture attributes like the word’s part of speech or syntactic dependency label (Köhn, 2015; Belinkov et al., 2017), we would like to develop integer embeddings that capture primality, divisibility by 3, etc. In this paper, we learn integer embeddings from mathematical resources and probe them for mathematical knowledge. Unlike most natural-language features, many number-theoretic properties are ∗ Research performed at DiDi Labs. 2, 3, 5, 7, 11, 13, 17, 19, 23, 29, ... (primes) 0, 1, 1, 2, 3, 5, 8, 13, 21, ... (Fibonacci) 1, 1, 2, 5, 14, 42, 132, 429, ... (Catalan) 0, 1, 3, 6, 2, 7, 13, 20, 12, 21, ... (Recamán) highly regular and often may be easily determined from the integer valu"
2021.blackboxnlp-1.30,P14-2050,0,0.0446145,"a include numerical data as well, and some properties of a number can be inferred from the textual context (e.g., if an integer follows the phrase the year, it most likely has four digits). While the training data for these embeddings is not designed to encode integer properties, it may contain billions of tokens, compared to only ∼13M in OEIS. It has been shown that word embeddings retain some knowledge of the integer properties (Naik et al., 2019), so we exploit these resources as a baseline. We use three sets of pre-trained embeddings: GloVe (Pennington et al., 2014), SkipGram bagof-words (Levy and Goldberg, 2014), and FastText (Bojanowski et al., 2017). For all three models, we select the versions that performed best in the numerical knowledge tests of Naik et al. (2019).3 Thawani et al. (2021) provide a comprehensive description of prior work on number representations learned from text. Prior work concentrates on basic numeracy (counting, paraphrasing, rela2 For implementation details, see Appendix C. Specific model versions and links to download them are listed in Appendix B. 390 3 Evenness Embeddings Divisibility by 3 Divisibility by 4 Primality Single All Single All Single All Single All Random ba"
2021.blackboxnlp-1.30,N13-1090,0,0.0386231,"tests 0.28 0.25 0.27 0.34 0.18 Table 8: Peformance on the multiple-choice numerical analogy questions. The answer to a:b :: c:? is chosen by highest cosine similarity to v(c) − v(a) + v(b). OEIS–FastText is trained with subword information. prompts are now much longer (42 tokens on average compared to 5). Limiting the search to only the last 5 tokens of the prompt improves accuracy, but it still does not outperform the OEIS–LSTM. Mathematical analogies. A traditional benchmark for evaluating word representations is word analogy, i.e. answering questions of the form “a is to b as c is to ...” (Mikolov et al., 2013a). We perform a similar mathematical analogy test, collecting 79 questions from numerical aptitude practice tests. All questions are multiple-choice (3 to 5 answer options), to mitigate non-uniqueness of the solution. Table 6 shows five sample analogy problems from our dataset, annotated with the intended analogy explanations.7 Following Mikolov et al. (2013b), we use vector arithmetic to solve analogies: out of the given options, for the task a:b :: c:? we choose the number whose embedding has the highest cosine similarity to the vector v(c) − v(a) + v(b). Table 8 shows that only the FastTex"
2021.blackboxnlp-1.30,D14-1162,0,0.0911642,"r NLP applications. Naturally, these text corpora include numerical data as well, and some properties of a number can be inferred from the textual context (e.g., if an integer follows the phrase the year, it most likely has four digits). While the training data for these embeddings is not designed to encode integer properties, it may contain billions of tokens, compared to only ∼13M in OEIS. It has been shown that word embeddings retain some knowledge of the integer properties (Naik et al., 2019), so we exploit these resources as a baseline. We use three sets of pre-trained embeddings: GloVe (Pennington et al., 2014), SkipGram bagof-words (Levy and Goldberg, 2014), and FastText (Bojanowski et al., 2017). For all three models, we select the versions that performed best in the numerical knowledge tests of Naik et al. (2019).3 Thawani et al. (2021) provide a comprehensive description of prior work on number representations learned from text. Prior work concentrates on basic numeracy (counting, paraphrasing, rela2 For implementation details, see Appendix C. Specific model versions and links to download them are listed in Appendix B. 390 3 Evenness Embeddings Divisibility by 3 Divisibility by 4 Primality Singl"
2021.blackboxnlp-1.30,2021.naacl-main.53,0,0.0223664,". While the training data for these embeddings is not designed to encode integer properties, it may contain billions of tokens, compared to only ∼13M in OEIS. It has been shown that word embeddings retain some knowledge of the integer properties (Naik et al., 2019), so we exploit these resources as a baseline. We use three sets of pre-trained embeddings: GloVe (Pennington et al., 2014), SkipGram bagof-words (Levy and Goldberg, 2014), and FastText (Bojanowski et al., 2017). For all three models, we select the versions that performed best in the numerical knowledge tests of Naik et al. (2019).3 Thawani et al. (2021) provide a comprehensive description of prior work on number representations learned from text. Prior work concentrates on basic numeracy (counting, paraphrasing, rela2 For implementation details, see Appendix C. Specific model versions and links to download them are listed in Appendix B. 390 3 Evenness Embeddings Divisibility by 3 Divisibility by 4 Primality Single All Single All Single All Single All Random baseline 0.50 0.50 0.67 0.67 0.75 0.75 0.87 0.87 GloVe–840B–300D SkipGram–BoW–5 FastText–Wiki 0.51 0.50 0.51 0.76 0.52 0.61 0.67 0.67 0.67 0.47 0.67 0.66 0.75 0.75 0.75 0.71 0.75 0.76 0.8"
2021.blackboxnlp-1.30,P19-1329,0,0.0152165,"ed Embeddings Prior work has successfully employed word representations that are pre-trained on large text corpora for NLP applications. Naturally, these text corpora include numerical data as well, and some properties of a number can be inferred from the textual context (e.g., if an integer follows the phrase the year, it most likely has four digits). While the training data for these embeddings is not designed to encode integer properties, it may contain billions of tokens, compared to only ∼13M in OEIS. It has been shown that word embeddings retain some knowledge of the integer properties (Naik et al., 2019), so we exploit these resources as a baseline. We use three sets of pre-trained embeddings: GloVe (Pennington et al., 2014), SkipGram bagof-words (Levy and Goldberg, 2014), and FastText (Bojanowski et al., 2017). For all three models, we select the versions that performed best in the numerical knowledge tests of Naik et al. (2019).3 Thawani et al. (2021) provide a comprehensive description of prior work on number representations learned from text. Prior work concentrates on basic numeracy (counting, paraphrasing, rela2 For implementation details, see Appendix C. Specific model versions and lin"
2021.emnlp-demo.23,E17-1099,0,0.0141781,"ions that are more stable, et al., 2020): but which update more slowly. - Final Bleu. We measure the Bleu MT accuracy of Translate-t. Improving on translate-k, we send an final, target-language utterances against a reference ASR output to MT if at least t seconds have elapsed set of human translations. Anti-flicker devices that since the last MT call. “lock in” partial translations will generally decrease Mask-k. We suppress the last k words from the MT final Bleu. output, providing time for the translation to settle - Translation lag. We measure (roughly) the averdown (Cho and Esipova, 2016; Gu et al., 2017; Ma age difference, in seconds, between when a word et al., 2019; Arivazhagan et al., 2020). We often was spoken and when its translation was finalized use Mask-4 in practice. on the screen. Biased MT decoding. We encourage word-by-word - Normalized erasure. We quantify flicker as m/n, 199 Figure 4: Cross-lingual word guessing game for extrinsic evaluation. Roles: one player is given a word to describe in their language, one or more players look at the captions displayed to guess the correct word. Screenshot shown is that of the describer of the word. Word is “eyelash” (left panel) - the part"
2021.emnlp-demo.23,2020.emnlp-tutorials.6,0,0.020722,"rstiness. There have been recent working systems in this space, such as Ma et al. (2019); Cho et al. (2013); Wang et al. (2016) that provide live captions for single-speaker lectures and does not focus on multiparty meetings. Very recently, there are news reports of systems that offer live translations of multiparty meetings (ZDNet, 2021), but their technical details are unclear. The area of simultaneous translation has attracted a lot of attention is recent years. The recent editions of the Workshop on Automatic Simultaneous Translation (Wu et al., 2021, 2020) and the tutorial in EMNLP 2020 (Huang et al., 2020) provide us an overview of the state-of-the-art and challenges in live translation. Recent technical advances include 3 System Description newer architectures such as prefix-to-prefix (Ma et al., 2019) and adaptive policy methods such as The overall system architecture is shown in imitation learning (Zheng et al., 2019) and mono- Figure 1. It mainly consists of two components tonic attention (Raffel et al., 2017). The solu- (1) Frontend: runs on the user’s computer locally tions in this space are differentiated into speech-to- (2) Backend: runs on the server and consists of speech and speech-t"
2021.emnlp-demo.23,P02-1040,0,0.112124,"Missing"
2021.emnlp-demo.23,D08-1102,0,0.0338278,"the Google API does not have biaseddecoding available to reduce flicker. Metrics are explained in §4. Right 4 metrics are introduced by our work. pronunciation dictionary required by Kaldi ASR is generated automatically by rules. Profanity is detected using a standard list of keywords and starred in both ASR and translation output. We have an experimental module to detect the speaker’s language automatically (instead of being manually set by the user). The advantage of such a feature is to allow code-switching between multiple languages which is a common behavior among multilingual speakers (Solorio and Liu, 2008; Sivasankaran et al., 2018). MT decoding to match the string output by the previous MT call (Arivazhagan et al., 2020), avoiding translation variations that, while acceptable, unfortunately introduce flicker. Preserve linebreaks. When possible, we prevent words from jumping back & forth across linebreaks. Smooth scrolling. We reduce perception of flicker by scrolling lines smoothly, pixel-wise. 4 Evaluation Dataset: In order to evaluate our system in Captioning Strategies: Here we describe how we the most-appropriate deployment scenario, we deploy ASR and MT capabilities to create translacons"
2021.emnlp-demo.23,2020.wmt-1.7,1,0.73447,"peaker as feedback/confirmation to them. Each of the transcribed text returned by ASR is fed to the MT system to translate it into the caption language selected by the reader, from scratch. This strategy is termed as re-translation. Since the ASR stream continually returns a revised or an extended string, the input to MT is noisy and will lead to the captions overwritten frequently (termed as flicker) leading to a high cognitive load on the reading. We employ several techniques to have better user experience while they are reading the captions (elaborated more below). in-house DiDi MT system (Chen et al., 2020) as well as call Google MT API. For Kaldi ASR, we adapt pre-trained Kaldi models to videoconferencing domain by interpolating the pre-trained language model with our in-domain language models. For WeNet, we use use the Unified Conformer model and language model interpolation is planned for future work.5 Following Arivazhagan et al. (2020), we modify the decoding procedure of MT in OpenNMT’s ctranslate toolkit6 to mitigate the issue of flicker mentioned above. We include several additional features to enhance user experience. We use NVIDIA NeMo toolkit7 to punctuate the captions and predict if"
2021.emnlp-demo.23,C16-2007,0,0.0265164,"ntend consists of different “views” e.g. live captions view - captures audio from another browser window/zoom/system to show translated captions. to-end system performance. We are in the process of releasing the system as open-source software for the purpose of furthering research in the area. 2 Related Work use case of short exchanges in a meeting scenario compared to long speech, we introduce additional metrics initial lag, incremental caption lag, mean word burstiness and max word burstiness. There have been recent working systems in this space, such as Ma et al. (2019); Cho et al. (2013); Wang et al. (2016) that provide live captions for single-speaker lectures and does not focus on multiparty meetings. Very recently, there are news reports of systems that offer live translations of multiparty meetings (ZDNet, 2021), but their technical details are unclear. The area of simultaneous translation has attracted a lot of attention is recent years. The recent editions of the Workshop on Automatic Simultaneous Translation (Wu et al., 2021, 2020) and the tutorial in EMNLP 2020 (Huang et al., 2020) provide us an overview of the state-of-the-art and challenges in live translation. Recent technical advance"
2021.emnlp-demo.23,P19-1582,0,0.0134519,"tings (ZDNet, 2021), but their technical details are unclear. The area of simultaneous translation has attracted a lot of attention is recent years. The recent editions of the Workshop on Automatic Simultaneous Translation (Wu et al., 2021, 2020) and the tutorial in EMNLP 2020 (Huang et al., 2020) provide us an overview of the state-of-the-art and challenges in live translation. Recent technical advances include 3 System Description newer architectures such as prefix-to-prefix (Ma et al., 2019) and adaptive policy methods such as The overall system architecture is shown in imitation learning (Zheng et al., 2019) and mono- Figure 1. It mainly consists of two components tonic attention (Raffel et al., 2017). The solu- (1) Frontend: runs on the user’s computer locally tions in this space are differentiated into speech-to- (2) Backend: runs on the server and consists of speech and speech-to-text, in the former there is ASR and MT modules that interact with the Flask a speech synthesis component. Our work falls in server. The modular architecture allows us to the latter bucket, since we only display captions to swap the ASR and MT components from different the user. Since we have access to separate audio-"
benjamin-etal-2002-translation,W99-0906,1,\N,Missing
benjamin-etal-2002-translation,J99-4005,1,\N,Missing
benjamin-etal-2002-translation,W98-1005,1,\N,Missing
benjamin-etal-2002-translation,W01-1409,0,\N,Missing
benjamin-etal-2002-translation,P02-1039,1,\N,Missing
benjamin-etal-2002-translation,P01-1067,1,\N,Missing
benjamin-etal-2002-translation,P01-1050,1,\N,Missing
benjamin-etal-2002-translation,knight-al-onaizan-1998-translation,1,\N,Missing
benjamin-etal-2002-translation,P97-1017,1,\N,Missing
benjamin-etal-2002-translation,W01-0504,1,\N,Missing
benjamin-etal-2002-translation,P01-1030,1,\N,Missing
braune-etal-2014-mapping,W03-2401,0,\N,Missing
braune-etal-2014-mapping,C12-1083,1,\N,Missing
braune-etal-2014-mapping,H94-1050,0,\N,Missing
braune-etal-2014-mapping,C02-2025,0,\N,Missing
braune-etal-2014-mapping,P02-1040,0,\N,Missing
braune-etal-2014-mapping,knight-al-onaizan-1998-translation,1,\N,Missing
braune-etal-2014-mapping,P13-1091,1,\N,Missing
braune-etal-2014-mapping,P89-1005,0,\N,Missing
braune-etal-2014-mapping,P13-2131,1,\N,Missing
braune-etal-2014-mapping,basile-etal-2012-developing,0,\N,Missing
braune-etal-2014-mapping,W13-2322,1,\N,Missing
braune-etal-2014-mapping,J98-4003,1,\N,Missing
braune-etal-2014-mapping,W01-0807,0,\N,Missing
braune-etal-2014-mapping,W12-6207,1,\N,Missing
C10-1106,W09-1804,1,0.887675,"Missing"
C10-1106,P08-1085,0,0.277853,"Missing"
C10-1106,P07-1094,0,0.0945569,"Missing"
C10-1106,P09-1039,0,0.0627814,"Missing"
C10-1106,J94-2001,0,0.826927,"solver to do model minimization. However, solving this problem exactly using an integer programming formulation is intractable for practical purposes. We propose a novel two-stage greedy approximation scheme to replace the IP. Our method runs fast, while yielding highly accurate tagging results. We also compare our method against standard EM training, and show that we consistently obtain better tagging accuracies on test data of varying sizes for English and Italian. 1 tˆ = arg max P (w, t) t Introduction The task of unsupervised part-of-speech (POS) tagging with a dictionary as formulated by Merialdo (1994) is: given a raw word sequence and a dictionary of legal POS tags for each word type, tag each word token in the text. A common approach to modeling such sequence labeling problems is to build a bigram Hidden Markov Model (HMM) parameterized by tag-bigram transition probabilities P (ti |ti−1 ) and word-tag emission probabilities P (wi |ti ). Given a word sequence w and a tag sequence t, of length N , the joint probability P (w, t) is given by: P (w, t) = N Y i=1 P (wi |ti ) · P (ti |ti−1 ) (1) (2) Ravi and Knight (2009) attack the Merialdo task in two stages. In the first stage, they search fo"
C10-1106,C04-1197,0,0.0356354,"Missing"
C10-1106,D08-1085,1,0.894014,"Missing"
C10-1106,P09-1057,1,0.934644,"rvised Tagging Sujith Ravi and Ashish Vaswani and Kevin Knight and David Chiang University of Southern California Information Sciences Institute {sravi,avaswani,knight,chiang}@isi.edu Abstract We can train this model using the Expectation Maximization (EM) algorithm (Dempster and Rubin, 1977) which learns P (wi |ti ) and P (ti |ti−1 ) that maximize the likelihood of the observed data. Once the parameters are learnt, we can find the best tagging using the Viterbi algorithm. Model minimization has been shown to work well for the task of unsupervised part-of-speech tagging with a dictionary. In (Ravi and Knight, 2009), the authors invoke an integer programming (IP) solver to do model minimization. However, solving this problem exactly using an integer programming formulation is intractable for practical purposes. We propose a novel two-stage greedy approximation scheme to replace the IP. Our method runs fast, while yielding highly accurate tagging results. We also compare our method against standard EM training, and show that we consistently obtain better tagging accuracies on test data of varying sizes for English and Italian. 1 tˆ = arg max P (w, t) t Introduction The task of unsupervised part-of-speech"
C10-1106,P05-1044,0,0.0539854,"Missing"
C12-1083,bohnet-wanner-2010-open,0,0.0225855,"ds for graphs in NLP. The standard model for graphshaped meaning representations in NLP are feature structures, which can be constructed from strings using unification grammars (Moore, 1989). However, while powerful in representation, unification grammars have unfavorable algorithmic properties, lack an intuitive probabilistic extension, and require hand-built rules. Other formal devices to accept and transduce feature structure graphs have rarely been discussed. Notable exceptions are Quernheim and Knight (2012) who discuss formal devices to accept and transduce feature structure graphs, and Bohnet and Wanner (2010) who present a toolkit for manually engineering graph-to-string transducer rules for natural language generation. We believe that we are the first to use hyperedge replacement grammars in the NLP literature and can only refer to the formal HRG survey by (Drewes et al., 1997). 1371 7 Conclusion We have introduced a new model for semantically-driven statistical machine translation using graph-structured meaning representations. Our approach is based on the class of weighted synchronous hyperedge replacement grammars, a rewriting formalism for graph-string pairs that intuitively extends context-f"
C12-1083,J90-2002,0,0.554639,"n Figure 3a using the canonical grammar in a, as created by the CANSEM algorithm. 1363 3 Learning Grammars from Annotated Data 3.1 Aligning Strings and Graphs A0 :an t :ca A1 na root:miss poss:anna Anna misses her cat. Figure 5: Edge-word alignment example. Like much of SMT, alignments lie at the center of our semantics-based approach. However, in our case the alignments are between edges of the graph and words of the string. Figure 5 illustrates such an alignment. By listing out edge labels in a linear order, the graph-to-string alignment problem reduces to ordinary token-to-token alignment (Brown et al., 1990). We experiment with two strategies: (1) IBM Model 4 (M4) as implemented in GIZA++ (Och and Ney, 2003), and (2) a novel aligner that relies on the relative structure of the MR graph and the natural language syntax. For M4, we traverse the graph in a fixed breadth first order to get a sequence of edge labels and feed this, along with the tokenized natural language string, to GIZA++. We then use the edge label order to map the aligned edge labels back to their respective edges. We also experiment with a novel variant of IBM alignment Model 2 (Brown et al., 1990) that we call the dependency depth"
C12-1083,P10-1146,0,0.0168125,"Missing"
C12-1083,P05-1066,0,0.0698714,"Missing"
C12-1083,D09-1076,1,0.470877,"Missing"
C12-1083,P09-2036,0,0.0190644,"meaning. Such systems often struggle to generate correct translations that involve non-local phenomena such as argument reorderings across languages, deep embeddings, empty categories and anaphora. With the increasing availability of syntactically-annotated data in many languages, it has become possible to more directly integrate syntax into data-driven approaches. Such syntax-based SMT systems can automatically extract larger rules, and learn syntactic reorderings for translation (Yamada and Knight, 2001; Venugopal and Zollmann, 2006; Galley et al., 2004; Chiang, 2007; Zollmann et al., 2008; DeNero et al., 2009; Shen et al., 2010; Genzel, 2010). However, many problems remain unsolved. For illustration of a specific phenomenon difficult to capture without an intermediate meaning representation, consider the following translation example using a state-of-the-art German→English SMT system 1 : Source Anna fehlt ihrem Kater System output *Anna is missing her cat Reference Anna’s cat is missing her SMT systems are frequently unable to preserve basic meaning structures (e.g. “who does what to whom”) across languages when confronted with verbs that realize their arguments differently. A system using an inte"
C12-1083,W08-1111,0,0.0145247,"the parameters of statistical generation models is popular, but not much attention has been paid to the scenario where no handwritten rules exist or the mapping between semantic structure and output language is unknown in the training data (the scenario we assume in this paper). The WASP system (Wong and Mooney, 2006) can also be used as a generator. Lu and Ng (2011) automatically learn to generate English and Chinese from sentences paired with lambda calculus. Other examples are (Varges and Mellish, 2001) who learn a semantic grammar form a semantically annotated treebank automatically, and (DeVault et al., 2008) who infer a TAG for generation automatically from semantically annotated example sentences. Formal language approaches to probabilistic tree transformation are popular (e.g. in syntaxbased MT) and recently a formulation of such methods as tree transducers (Comon et al., 2007) has gained prominence in NLP (Knight and May, 2009; Graehl and Knight, 2004). In contrast, little work has been done on methods for graphs in NLP. The standard model for graphshaped meaning representations in NLP are feature structures, which can be constructed from strings using unification grammars (Moore, 1989). Howev"
C12-1083,P10-4002,0,0.0113576,"vation. While parsing arbitrary graphs with SHRGs is NP-complete, we use a polynomial time chart parsing algorithms (which are exponential in the maximum size of the graph fragments on the rule right hand side) for connected graphs (Drewes et al., 1997). In the case of the CANSEM algorithm, we use a parser specialized for the canonical HRG which can be parsed even more efficiently in O (nc ), with c the maximum number of rule right hand side nodes (worst case c = 3 for experiments in this paper). Finally, we rerank the natural language output by incorporating a language model (Heafield, 2011; Dyer et al., 2010). For the SYNSEM algorithm, this is integrated into the parsing algorithm via cube pruning (Chiang, 2007). In the CANSEM algorithm reranking is performed on an k-best list of generated natural language output using a standard n-gram language model. Hypothesis meaning representations might be similarly reranked using a ‘language model’ defined on MR graphs, but we leave this for future work. In our experiments, language model weights were selected empirically based on initial evaluations of the development set. 4.2 Parameter Estimation As with CFGs, there are two strategies for estimating the p"
C12-1083,P03-2041,0,0.00810631,"ere) is strictly harder than direct translation, as the test set contains numerous sentences annotated with identical meaning representations but different natural language realizations. We are optimistic about the potential for an extended version of the current system in which generation is conditioned on both semantics and source language. 6 Related Work We view our semantics-based approach to MT as a continuation of recent work in statistical MT (SMT) that abstracts away from the surface string level by capturing syntactic reorderings in translation (Yamada and Knight, 2001; Gildea, 2003; Eisner, 2003; Collins et al., 2005), or using larger syntactic fragments instead of phrases (Galley et al., 2004, 2006; Chiang, 2007). These systems combine the benefits of rule-based MT and SMT by defining their translation model using syntactic translation rules from the source syntax, into the target syntax, or both. Our syntax-driven approach to rule extraction is inspired by (Chiang, 2007, 2010), while the canonical grammar approach is based on (Galley et al., 2004, 2006). However, we induce synchronous graph grammars between surface form and meaning representation, instead of transfer rules between"
C12-1083,P06-1121,1,0.586811,"rom the meaning representation automatically to build a true statistical semanticsbased MT system. In the semantic parsing literature, there are other learning based approaches to analysis into meaning representations. Zettlemoyer and Collins (2005) use an automatically induced, semantically augmented CCG and a log-linear model to parse into lambda expressions, and Ge and Mooney (2005) integrate syntactic parsing with semantic parsing for recovering Prolog queries. Lu et al. (2008) learn a generative model over tree shaped meaning representation and natural language sentences. Wong and Mooney (2006)’s WASP system is similar to ours because it draws on techniques from SMT, using word alignment algorithms to learn synchronous CFGs which translate between syntax and semantics. In fact, Jones et al. (2012) recasts many semantic parsing approaches as tree transduction, which is closely related to synchronous grammar parsing (Shieber, 2004). To our knowledge we are the first to address semantic parsing into graph-based representations as a learning task using synchronous graph grammars. In generation, learning the parameters of statistical generation models is popular, but not much attention h"
C12-1083,N04-1035,1,0.938738,"phrases can be translated without reference to syntax or meaning. Such systems often struggle to generate correct translations that involve non-local phenomena such as argument reorderings across languages, deep embeddings, empty categories and anaphora. With the increasing availability of syntactically-annotated data in many languages, it has become possible to more directly integrate syntax into data-driven approaches. Such syntax-based SMT systems can automatically extract larger rules, and learn syntactic reorderings for translation (Yamada and Knight, 2001; Venugopal and Zollmann, 2006; Galley et al., 2004; Chiang, 2007; Zollmann et al., 2008; DeNero et al., 2009; Shen et al., 2010; Genzel, 2010). However, many problems remain unsolved. For illustration of a specific phenomenon difficult to capture without an intermediate meaning representation, consider the following translation example using a state-of-the-art German→English SMT system 1 : Source Anna fehlt ihrem Kater System output *Anna is missing her cat Reference Anna’s cat is missing her SMT systems are frequently unable to preserve basic meaning structures (e.g. “who does what to whom”) across languages when confronted with verbs that r"
C12-1083,W05-0602,0,0.028053,"r good performance especially in narrow domains, rule-based MT was the predominant paradigm in deployed MT systems. In contrast, while our system adopts a semantic transfer based paradigm, we learn weighted transfer rules into and from the meaning representation automatically to build a true statistical semanticsbased MT system. In the semantic parsing literature, there are other learning based approaches to analysis into meaning representations. Zettlemoyer and Collins (2005) use an automatically induced, semantically augmented CCG and a log-linear model to parse into lambda expressions, and Ge and Mooney (2005) integrate syntactic parsing with semantic parsing for recovering Prolog queries. Lu et al. (2008) learn a generative model over tree shaped meaning representation and natural language sentences. Wong and Mooney (2006)’s WASP system is similar to ours because it draws on techniques from SMT, using word alignment algorithms to learn synchronous CFGs which translate between syntax and semantics. In fact, Jones et al. (2012) recasts many semantic parsing approaches as tree transduction, which is closely related to synchronous grammar parsing (Shieber, 2004). To our knowledge we are the first to a"
C12-1083,C10-1043,0,0.0196763,"generate correct translations that involve non-local phenomena such as argument reorderings across languages, deep embeddings, empty categories and anaphora. With the increasing availability of syntactically-annotated data in many languages, it has become possible to more directly integrate syntax into data-driven approaches. Such syntax-based SMT systems can automatically extract larger rules, and learn syntactic reorderings for translation (Yamada and Knight, 2001; Venugopal and Zollmann, 2006; Galley et al., 2004; Chiang, 2007; Zollmann et al., 2008; DeNero et al., 2009; Shen et al., 2010; Genzel, 2010). However, many problems remain unsolved. For illustration of a specific phenomenon difficult to capture without an intermediate meaning representation, consider the following translation example using a state-of-the-art German→English SMT system 1 : Source Anna fehlt ihrem Kater System output *Anna is missing her cat Reference Anna’s cat is missing her SMT systems are frequently unable to preserve basic meaning structures (e.g. “who does what to whom”) across languages when confronted with verbs that realize their arguments differently. A system using an intermediate meaning representation ne"
C12-1083,P03-1011,0,0.0129076,"s formulated here) is strictly harder than direct translation, as the test set contains numerous sentences annotated with identical meaning representations but different natural language realizations. We are optimistic about the potential for an extended version of the current system in which generation is conditioned on both semantics and source language. 6 Related Work We view our semantics-based approach to MT as a continuation of recent work in statistical MT (SMT) that abstracts away from the surface string level by capturing syntactic reorderings in translation (Yamada and Knight, 2001; Gildea, 2003; Eisner, 2003; Collins et al., 2005), or using larger syntactic fragments instead of phrases (Galley et al., 2004, 2006; Chiang, 2007). These systems combine the benefits of rule-based MT and SMT by defining their translation model using syntactic translation rules from the source syntax, into the target syntax, or both. Our syntax-driven approach to rule extraction is inspired by (Chiang, 2007, 2010), while the canonical grammar approach is based on (Galley et al., 2004, 2006). However, we induce synchronous graph grammars between surface form and meaning representation, instead of transfer"
C12-1083,N04-1014,1,0.352589,". Lu and Ng (2011) automatically learn to generate English and Chinese from sentences paired with lambda calculus. Other examples are (Varges and Mellish, 2001) who learn a semantic grammar form a semantically annotated treebank automatically, and (DeVault et al., 2008) who infer a TAG for generation automatically from semantically annotated example sentences. Formal language approaches to probabilistic tree transformation are popular (e.g. in syntaxbased MT) and recently a formulation of such methods as tree transducers (Comon et al., 2007) has gained prominence in NLP (Knight and May, 2009; Graehl and Knight, 2004). In contrast, little work has been done on methods for graphs in NLP. The standard model for graphshaped meaning representations in NLP are feature structures, which can be constructed from strings using unification grammars (Moore, 1989). However, while powerful in representation, unification grammars have unfavorable algorithmic properties, lack an intuitive probabilistic extension, and require hand-built rules. Other formal devices to accept and transduce feature structure graphs have rarely been discussed. Notable exceptions are Quernheim and Knight (2012) who discuss formal devices to ac"
C12-1083,W09-1201,0,0.0247227,"Missing"
C12-1083,W11-2123,0,0.00521563,"lded by the derivation. While parsing arbitrary graphs with SHRGs is NP-complete, we use a polynomial time chart parsing algorithms (which are exponential in the maximum size of the graph fragments on the rule right hand side) for connected graphs (Drewes et al., 1997). In the case of the CANSEM algorithm, we use a parser specialized for the canonical HRG which can be parsed even more efficiently in O (nc ), with c the maximum number of rule right hand side nodes (worst case c = 3 for experiments in this paper). Finally, we rerank the natural language output by incorporating a language model (Heafield, 2011; Dyer et al., 2010). For the SYNSEM algorithm, this is integrated into the parsing algorithm via cube pruning (Chiang, 2007). In the CANSEM algorithm reranking is performed on an k-best list of generated natural language output using a standard n-gram language model. Hypothesis meaning representations might be similarly reranked using a ‘language model’ defined on MR graphs, but we leave this for future work. In our experiments, language model weights were selected empirically based on initial evaluations of the development set. 4.2 Parameter Estimation As with CFGs, there are two strategies"
C12-1083,N06-2015,0,0.0111525,"r cat Reference Anna’s cat is missing her SMT systems are frequently unable to preserve basic meaning structures (e.g. “who does what to whom”) across languages when confronted with verbs that realize their arguments differently. A system using an intermediate meaning representation need not suffer from this problem. Instead of learning many bilingual translation rules over all possible realizations of this pattern, it can rely on monolingual realizations to preserve meaning in translation. Due to the recent emergence of large, multilingual, semantically annotated resources such as OntoNotes (Hovy et al., 2006), we believe the time is ripe for data-driven, semantics-based machine translation. In this paper we present a pilot statistical, semantic machine translation system which treats MT as a two-step process of analysis into meaning in the source language, and decoding from meaning in the target language. Our system assumes that meaning representations are directed acyclic graphs; beyond that, it is completely agnostic with respect to the details of the formalism, including the inventory of node and edge labels used. Figure 1 illustrates a pipeline via one possible graph as semantic pivot. The pro"
C12-1083,P12-1051,1,0.155831,"meaning representations. Zettlemoyer and Collins (2005) use an automatically induced, semantically augmented CCG and a log-linear model to parse into lambda expressions, and Ge and Mooney (2005) integrate syntactic parsing with semantic parsing for recovering Prolog queries. Lu et al. (2008) learn a generative model over tree shaped meaning representation and natural language sentences. Wong and Mooney (2006)’s WASP system is similar to ours because it draws on techniques from SMT, using word alignment algorithms to learn synchronous CFGs which translate between syntax and semantics. In fact, Jones et al. (2012) recasts many semantic parsing approaches as tree transduction, which is closely related to synchronous grammar parsing (Shieber, 2004). To our knowledge we are the first to address semantic parsing into graph-based representations as a learning task using synchronous graph grammars. In generation, learning the parameters of statistical generation models is popular, but not much attention has been paid to the scenario where no handwritten rules exist or the mapping between semantic structure and output language is unknown in the training data (the scenario we assume in this paper). The WASP sy"
C12-1083,P07-2045,0,0.00320073,"NSEM Sample sentence Reference give me cities with the largest population what is the population of washington what are in washington what city has the largest population how many people live in washington how many rivers in washington (b) SYNSEM Figure 8: Sample translation output. source language are analyzed into a language-independent meaning representation, and that meaning representation is then used to generate a semantically equivalent sentence in the target language. The scores reported for SYNSEM are especially heartening in light of the fact that a standard phrase-based SMT system (Koehn et al., 2007), trained and tuned on the same corpus, achieves a BLEU score of 45.13 for ZH–EN translation. Note that the semantic translation task (at least as formulated here) is strictly harder than direct translation, as the test set contains numerous sentences annotated with identical meaning representations but different natural language realizations. We are optimistic about the potential for an extended version of the current system in which generation is conditioned on both semantics and source language. 6 Related Work We view our semantics-based approach to MT as a continuation of recent work in st"
C12-1083,D10-1119,0,0.029886,"‘Which rivers cross Ohio?’), is produced by only looking at the query expression itself (GQN). The second (GQE), shown in Figure 6b is a transformation of GQN to more closely match the English syntax using the gold alignments. Note that this reshaped graph better fits the assumptions of the SYNSEM algorithm and should, in theory, produce better SHRGs for English. It is less clear whether an English biased intermediate representation would perform better for translation, as it could conceivably hurt translation to and from other languages. We use the standard 600 train/280 test sentence split (Kwiatkowski et al., 2010), and run 10 1367 DD M4 Prec. Rec. f1 74.8 54.6 46.9 53.7 57.7 54.1 Table 1: Evaluation of English alignment, vs. gold alignments fold cross-validation on the training data during development. We also use the standard list of named entities paired with the corresponding edges to create some fallback rules for handling previously unseen named entities. 5.2 Alignment Table 1 shows results for the alignment algorithms described in Section 3.1 on English and the GQN MR. We report precision, recall and f1 -measure on alignment pairs. The DEPDEP algorithm (DD) performs somewhat better than IBM Model"
C12-1083,1989.mtsummit-1.15,0,0.582692,"t al., 2004, 2006). However, we induce synchronous graph grammars between surface form and meaning representation, instead of transfer rules between source and target form. As with other translation work using synchronous tree grammars, such as synchronous TSG (Chiang, 2010) and synchronous TAG (DeNeefe and 1370 Knight, 2009), our SHRGs can also be applied in both directions. However, none of these SMT approaches use an intermediate semantic representation. A lot of research has been done in the early days of MT on translation systems using such representations (Uchida, 1987; Nirenburg, 1989; Landsbergen, 1989). These systems usually required hand-crafted rules and large knowledge bases and do not learn translation models from data automatically. Until recently, because of their good performance especially in narrow domains, rule-based MT was the predominant paradigm in deployed MT systems. In contrast, while our system adopts a semantic transfer based paradigm, we learn weighted transfer rules into and from the meaning representation automatically to build a true statistical semanticsbased MT system. In the semantic parsing literature, there are other learning based approaches to analysis into mean"
C12-1083,P03-1056,0,0.0174368,"io))) to a MR graph: (a) language-neutral, (b) English-biased and (c) an illustration of how (b) matches the English dependency analysis. Our experiments use the GEOQUERY data set (Tang and Mooney, 2001), originally a parallel corpus of 880 English questions about US geography paired with Prolog style database queries and later translated into Chinese (Lu and Ng, 2011). For English there are gold Penn Treebankstyle syntax annotations as well as gold alignments pairing every word with the best predicate in the query. For Chinese, we make use of automatic parses provided by the Stanford Parser (Levy and Manning, 2003). The database queries—expressions in an unambiguous formal language—serve as a rough encoding of sentence meaning, which we use as our meaning representation in the machine translation pipeline. Though they do not, strictly speaking, encode a linguistic notion of semantics, a statistical MT system can still learn meaningful associations with this languageindependent representation. (For instance the German ‘Gib mir die Bevölkerung von Kalifornien!’ [‘Give me the population of California!’] would match the the same Prolog query as English ‘How many people live in California?’.) For input to ou"
C12-1083,D08-1082,0,0.0175623,"d MT systems. In contrast, while our system adopts a semantic transfer based paradigm, we learn weighted transfer rules into and from the meaning representation automatically to build a true statistical semanticsbased MT system. In the semantic parsing literature, there are other learning based approaches to analysis into meaning representations. Zettlemoyer and Collins (2005) use an automatically induced, semantically augmented CCG and a log-linear model to parse into lambda expressions, and Ge and Mooney (2005) integrate syntactic parsing with semantic parsing for recovering Prolog queries. Lu et al. (2008) learn a generative model over tree shaped meaning representation and natural language sentences. Wong and Mooney (2006)’s WASP system is similar to ours because it draws on techniques from SMT, using word alignment algorithms to learn synchronous CFGs which translate between syntax and semantics. In fact, Jones et al. (2012) recasts many semantic parsing approaches as tree transduction, which is closely related to synchronous grammar parsing (Shieber, 2004). To our knowledge we are the first to address semantic parsing into graph-based representations as a learning task using synchronous grap"
C12-1083,D11-1149,0,0.0621796,":a0/answer o t (c) r ive o o t :ohio a (b) x av :a 1/ oh i r x :t r /r a0 :traverse : v :ri :answer (a) r e which r :ohio t o rivers cross ohio root Two different ways of converting the GEOQUERY Prolog expression answer(A,(river(A),traverse(A,ohio))) to a MR graph: (a) language-neutral, (b) English-biased and (c) an illustration of how (b) matches the English dependency analysis. Our experiments use the GEOQUERY data set (Tang and Mooney, 2001), originally a parallel corpus of 880 English questions about US geography paired with Prolog style database queries and later translated into Chinese (Lu and Ng, 2011). For English there are gold Penn Treebankstyle syntax annotations as well as gold alignments pairing every word with the best predicate in the query. For Chinese, we make use of automatic parses provided by the Stanford Parser (Levy and Manning, 2003). The database queries—expressions in an unambiguous formal language—serve as a rough encoding of sentence meaning, which we use as our meaning representation in the machine translation pipeline. Though they do not, strictly speaking, encode a linguistic notion of semantics, a statistical MT system can still learn meaningful associations with thi"
C12-1083,P89-1005,0,0.684234,"Vault et al., 2008) who infer a TAG for generation automatically from semantically annotated example sentences. Formal language approaches to probabilistic tree transformation are popular (e.g. in syntaxbased MT) and recently a formulation of such methods as tree transducers (Comon et al., 2007) has gained prominence in NLP (Knight and May, 2009; Graehl and Knight, 2004). In contrast, little work has been done on methods for graphs in NLP. The standard model for graphshaped meaning representations in NLP are feature structures, which can be constructed from strings using unification grammars (Moore, 1989). However, while powerful in representation, unification grammars have unfavorable algorithmic properties, lack an intuitive probabilistic extension, and require hand-built rules. Other formal devices to accept and transduce feature structure graphs have rarely been discussed. Notable exceptions are Quernheim and Knight (2012) who discuss formal devices to accept and transduce feature structure graphs, and Bohnet and Wanner (2010) who present a toolkit for manually engineering graph-to-string transducer rules for natural language generation. We believe that we are the first to use hyperedge re"
C12-1083,J03-1002,0,0.0047739,"ars from Annotated Data 3.1 Aligning Strings and Graphs A0 :an t :ca A1 na root:miss poss:anna Anna misses her cat. Figure 5: Edge-word alignment example. Like much of SMT, alignments lie at the center of our semantics-based approach. However, in our case the alignments are between edges of the graph and words of the string. Figure 5 illustrates such an alignment. By listing out edge labels in a linear order, the graph-to-string alignment problem reduces to ordinary token-to-token alignment (Brown et al., 1990). We experiment with two strategies: (1) IBM Model 4 (M4) as implemented in GIZA++ (Och and Ney, 2003), and (2) a novel aligner that relies on the relative structure of the MR graph and the natural language syntax. For M4, we traverse the graph in a fixed breadth first order to get a sequence of edge labels and feed this, along with the tokenized natural language string, to GIZA++. We then use the edge label order to map the aligned edge labels back to their respective edges. We also experiment with a novel variant of IBM alignment Model 2 (Brown et al., 1990) that we call the dependency depth based aligner (DEPDEP, or DD for short) which uses depth within the graph and the dependency analysis"
C12-1083,oepen-lonning-2006-discriminant,0,0.012092,"respect to the details of the formalism, including the inventory of node and edge labels used. Figure 1 illustrates a pipeline via one possible graph as semantic pivot. The proposed framework is flexible enough to handle numerous existing meaning representations, including the programming language syntax of the GEOQUERY corpus (Wong and Mooney, 2006) (used for the experiments in this paper), the PropBank-style structures (Palmer et al., 2005) used for the CoNLL shared task on recognizing semantic dependencies (Hajiˇc et al., 2009), and the Elementary Dependency Structures of the LOGON corpus (Oepen and Lønning, 2006). 1 Google Translate, 08/31/2012 1360 Anna fehlt ihrem Kater patient instance MISS instance agent CAT owner ANNA instance Anna’s cat is missing her Figure 1: A string to meaning graph to string translation pipeline. Experimental results demonstrate that our system is capable of learning semantic abstractions, and more specifically, to both analyse text into these abstractions and decode them back into text in multiple languages. The need to manipulate graph structures adds an additional level of complexity to the standard MT task. While the problems of parsing and rule-extraction are well-stud"
C12-1083,J05-1004,0,0.0147987,"e, and decoding from meaning in the target language. Our system assumes that meaning representations are directed acyclic graphs; beyond that, it is completely agnostic with respect to the details of the formalism, including the inventory of node and edge labels used. Figure 1 illustrates a pipeline via one possible graph as semantic pivot. The proposed framework is flexible enough to handle numerous existing meaning representations, including the programming language syntax of the GEOQUERY corpus (Wong and Mooney, 2006) (used for the experiments in this paper), the PropBank-style structures (Palmer et al., 2005) used for the CoNLL shared task on recognizing semantic dependencies (Hajiˇc et al., 2009), and the Elementary Dependency Structures of the LOGON corpus (Oepen and Lønning, 2006). 1 Google Translate, 08/31/2012 1360 Anna fehlt ihrem Kater patient instance MISS instance agent CAT owner ANNA instance Anna’s cat is missing her Figure 1: A string to meaning graph to string translation pipeline. Experimental results demonstrate that our system is capable of learning semantic abstractions, and more specifically, to both analyse text into these abstractions and decode them back into text in multiple"
C12-1083,P02-1040,0,0.0925377,"lead to improved analysis, and in accordance with our expectations the syntactically-guided DD alignments appear to help SYNSEM but not CANSEM. CANSEM EN ZH SYNSEM M4 DD GOLD M4 DD GOLD 67.9 67.8 56.4 – 72.4 – 81.5 76.8 81.8 – 84.4 – Table 2: Evaluation of analysis ( f1 ), vs. gold MRs in development set CANSEM EN ZH SYNSEM M4 DD GOLD M4 DD GOLD 51.89 50.28 48.82 – 55.24 – 52.47 45.82 42.91 – 53.3 – Table 3: Evaluation of generation (BLEU), vs. gold strings in development set 1368 5.4 Generation: Graph to String We evaluate text generated from gold MR graphs using the well-known BLEU measure (Papineni et al., 2002). Table 3 shows results for English (EN) and Chinese (ZH), varying rule extraction and alignment model as before. As before, M4 alignments help CANSEM more than DD alignments; however, here the trend also carries through to SYNSEM. Also in contrast to the analysis results, the two systems perform comparably on their best English results, and CANSEM outperforms SYNSEM on Chinese. While not targeted directly at the generation task (and not comparable to the existing literature, which reports BLEU scores on the test set), these results are promising: They are close to state-of-the-art for generat"
C12-1083,W12-4209,1,0.812969,"nence in NLP (Knight and May, 2009; Graehl and Knight, 2004). In contrast, little work has been done on methods for graphs in NLP. The standard model for graphshaped meaning representations in NLP are feature structures, which can be constructed from strings using unification grammars (Moore, 1989). However, while powerful in representation, unification grammars have unfavorable algorithmic properties, lack an intuitive probabilistic extension, and require hand-built rules. Other formal devices to accept and transduce feature structure graphs have rarely been discussed. Notable exceptions are Quernheim and Knight (2012) who discuss formal devices to accept and transduce feature structure graphs, and Bohnet and Wanner (2010) who present a toolkit for manually engineering graph-to-string transducer rules for natural language generation. We believe that we are the first to use hyperedge replacement grammars in the NLP literature and can only refer to the formal HRG survey by (Drewes et al., 1997). 1371 7 Conclusion We have introduced a new model for semantically-driven statistical machine translation using graph-structured meaning representations. Our approach is based on the class of weighted synchronous hyper"
C12-1083,J10-4005,0,0.0155767,"often struggle to generate correct translations that involve non-local phenomena such as argument reorderings across languages, deep embeddings, empty categories and anaphora. With the increasing availability of syntactically-annotated data in many languages, it has become possible to more directly integrate syntax into data-driven approaches. Such syntax-based SMT systems can automatically extract larger rules, and learn syntactic reorderings for translation (Yamada and Knight, 2001; Venugopal and Zollmann, 2006; Galley et al., 2004; Chiang, 2007; Zollmann et al., 2008; DeNero et al., 2009; Shen et al., 2010; Genzel, 2010). However, many problems remain unsolved. For illustration of a specific phenomenon difficult to capture without an intermediate meaning representation, consider the following translation example using a state-of-the-art German→English SMT system 1 : Source Anna fehlt ihrem Kater System output *Anna is missing her cat Reference Anna’s cat is missing her SMT systems are frequently unable to preserve basic meaning structures (e.g. “who does what to whom”) across languages when confronted with verbs that realize their arguments differently. A system using an intermediate meaning re"
C12-1083,W04-3312,0,0.0300877,"arse into lambda expressions, and Ge and Mooney (2005) integrate syntactic parsing with semantic parsing for recovering Prolog queries. Lu et al. (2008) learn a generative model over tree shaped meaning representation and natural language sentences. Wong and Mooney (2006)’s WASP system is similar to ours because it draws on techniques from SMT, using word alignment algorithms to learn synchronous CFGs which translate between syntax and semantics. In fact, Jones et al. (2012) recasts many semantic parsing approaches as tree transduction, which is closely related to synchronous grammar parsing (Shieber, 2004). To our knowledge we are the first to address semantic parsing into graph-based representations as a learning task using synchronous graph grammars. In generation, learning the parameters of statistical generation models is popular, but not much attention has been paid to the scenario where no handwritten rules exist or the mapping between semantic structure and output language is unknown in the training data (the scenario we assume in this paper). The WASP system (Wong and Mooney, 2006) can also be used as a generator. Lu and Ng (2011) automatically learn to generate English and Chinese from"
C12-1083,1987.mtsummit-1.10,0,0.917372,"approach is based on (Galley et al., 2004, 2006). However, we induce synchronous graph grammars between surface form and meaning representation, instead of transfer rules between source and target form. As with other translation work using synchronous tree grammars, such as synchronous TSG (Chiang, 2010) and synchronous TAG (DeNeefe and 1370 Knight, 2009), our SHRGs can also be applied in both directions. However, none of these SMT approaches use an intermediate semantic representation. A lot of research has been done in the early days of MT on translation systems using such representations (Uchida, 1987; Nirenburg, 1989; Landsbergen, 1989). These systems usually required hand-crafted rules and large knowledge bases and do not learn translation models from data automatically. Until recently, because of their good performance especially in narrow domains, rule-based MT was the predominant paradigm in deployed MT systems. In contrast, while our system adopts a semantic transfer based paradigm, we learn weighted transfer rules into and from the meaning representation automatically to build a true statistical semanticsbased MT system. In the semantic parsing literature, there are other learning b"
C12-1083,N01-1001,0,0.0266115,"ing into graph-based representations as a learning task using synchronous graph grammars. In generation, learning the parameters of statistical generation models is popular, but not much attention has been paid to the scenario where no handwritten rules exist or the mapping between semantic structure and output language is unknown in the training data (the scenario we assume in this paper). The WASP system (Wong and Mooney, 2006) can also be used as a generator. Lu and Ng (2011) automatically learn to generate English and Chinese from sentences paired with lambda calculus. Other examples are (Varges and Mellish, 2001) who learn a semantic grammar form a semantically annotated treebank automatically, and (DeVault et al., 2008) who infer a TAG for generation automatically from semantically annotated example sentences. Formal language approaches to probabilistic tree transformation are popular (e.g. in syntaxbased MT) and recently a formulation of such methods as tree transducers (Comon et al., 2007) has gained prominence in NLP (Knight and May, 2009; Graehl and Knight, 2004). In contrast, little work has been done on methods for graphs in NLP. The standard model for graphshaped meaning representations in NLP"
C12-1083,W06-3119,0,0.00997337,"rlying assumption that surface phrases can be translated without reference to syntax or meaning. Such systems often struggle to generate correct translations that involve non-local phenomena such as argument reorderings across languages, deep embeddings, empty categories and anaphora. With the increasing availability of syntactically-annotated data in many languages, it has become possible to more directly integrate syntax into data-driven approaches. Such syntax-based SMT systems can automatically extract larger rules, and learn syntactic reorderings for translation (Yamada and Knight, 2001; Venugopal and Zollmann, 2006; Galley et al., 2004; Chiang, 2007; Zollmann et al., 2008; DeNero et al., 2009; Shen et al., 2010; Genzel, 2010). However, many problems remain unsolved. For illustration of a specific phenomenon difficult to capture without an intermediate meaning representation, consider the following translation example using a state-of-the-art German→English SMT system 1 : Source Anna fehlt ihrem Kater System output *Anna is missing her cat Reference Anna’s cat is missing her SMT systems are frequently unable to preserve basic meaning structures (e.g. “who does what to whom”) across languages when confron"
C12-1083,J82-2005,0,0.816934,"gen Struktur und mit einer sprachneutralen Struktur. Unsere Arbeit zeigt dass semantikbasierte maschinelle Übersetzung vielversprechend ist. ∗ The authors contributed equally to this work and are listed in randomized order. Proceedings of COLING 2012: Technical Papers, pages 1359–1376, COLING 2012, Mumbai, December 2012. 1359 1 Introduction In this paper, we introduce a model for semantic machine translation using a graph-structured meaning representation. While it has been claimed since the inception of machine translation that a semantic model is necessary to achieve human-like translation (Weaver, 1955; Bar-Hillel, 1960), most recent work in MT has instead focused on phrase-based approaches. Statistical phrase-based systems rely on large volumes of parallel training data to learn translation probabilities across two languages; while, given sufficient data, phrase-based systems can cope with some of the ambiguity problems identified by early MT researchers, they are limited by the underlying assumption that surface phrases can be translated without reference to syntax or meaning. Such systems often struggle to generate correct translations that involve non-local phenomena such as argument re"
C12-1083,N06-1056,0,0.249012,"tion system which treats MT as a two-step process of analysis into meaning in the source language, and decoding from meaning in the target language. Our system assumes that meaning representations are directed acyclic graphs; beyond that, it is completely agnostic with respect to the details of the formalism, including the inventory of node and edge labels used. Figure 1 illustrates a pipeline via one possible graph as semantic pivot. The proposed framework is flexible enough to handle numerous existing meaning representations, including the programming language syntax of the GEOQUERY corpus (Wong and Mooney, 2006) (used for the experiments in this paper), the PropBank-style structures (Palmer et al., 2005) used for the CoNLL shared task on recognizing semantic dependencies (Hajiˇc et al., 2009), and the Elementary Dependency Structures of the LOGON corpus (Oepen and Lønning, 2006). 1 Google Translate, 08/31/2012 1360 Anna fehlt ihrem Kater patient instance MISS instance agent CAT owner ANNA instance Anna’s cat is missing her Figure 1: A string to meaning graph to string translation pipeline. Experimental results demonstrate that our system is capable of learning semantic abstractions, and more specific"
C12-1083,P01-1067,1,0.340912,"y are limited by the underlying assumption that surface phrases can be translated without reference to syntax or meaning. Such systems often struggle to generate correct translations that involve non-local phenomena such as argument reorderings across languages, deep embeddings, empty categories and anaphora. With the increasing availability of syntactically-annotated data in many languages, it has become possible to more directly integrate syntax into data-driven approaches. Such syntax-based SMT systems can automatically extract larger rules, and learn syntactic reorderings for translation (Yamada and Knight, 2001; Venugopal and Zollmann, 2006; Galley et al., 2004; Chiang, 2007; Zollmann et al., 2008; DeNero et al., 2009; Shen et al., 2010; Genzel, 2010). However, many problems remain unsolved. For illustration of a specific phenomenon difficult to capture without an intermediate meaning representation, consider the following translation example using a state-of-the-art German→English SMT system 1 : Source Anna fehlt ihrem Kater System output *Anna is missing her cat Reference Anna’s cat is missing her SMT systems are frequently unable to preserve basic meaning structures (e.g. “who does what to whom”)"
C12-1083,C08-1144,0,0.0186118,"reference to syntax or meaning. Such systems often struggle to generate correct translations that involve non-local phenomena such as argument reorderings across languages, deep embeddings, empty categories and anaphora. With the increasing availability of syntactically-annotated data in many languages, it has become possible to more directly integrate syntax into data-driven approaches. Such syntax-based SMT systems can automatically extract larger rules, and learn syntactic reorderings for translation (Yamada and Knight, 2001; Venugopal and Zollmann, 2006; Galley et al., 2004; Chiang, 2007; Zollmann et al., 2008; DeNero et al., 2009; Shen et al., 2010; Genzel, 2010). However, many problems remain unsolved. For illustration of a specific phenomenon difficult to capture without an intermediate meaning representation, consider the following translation example using a state-of-the-art German→English SMT system 1 : Source Anna fehlt ihrem Kater System output *Anna is missing her cat Reference Anna’s cat is missing her SMT systems are frequently unable to preserve basic meaning structures (e.g. “who does what to whom”) across languages when confronted with verbs that realize their arguments differently. A"
C12-1083,J08-3004,1,\N,Missing
C12-1083,P13-2131,1,\N,Missing
C12-1083,J07-2003,0,\N,Missing
C18-1313,W13-2322,1,0.958139,"s one way of capturing document level semantics, by annotating coreference, implicit roles and bridging relations on top of gold Abstract Meaning Representations of sentence-level semantics. We present the methodology of developing this corpus, alongside analysis of its quality and a plausible baseline for comparison. It is hoped that this Multi-Sentence AMR corpus (MS-AMR) illustrates a feasible approach to developing rich representations of document meaning, useful for tasks such as information extraction and question answering. 1 Introduction Although Abstract Meaning Representation (AMR) (Banarescu et al., 2013) shows promise for a range of tasks such as summarization (Liu et al., 2015; Viet et al., 2017) and information extraction (Garg et al., 2016), it is restricted to capturing the semantics of individual sentences. For many purposes, when examining the semantics of a document, one also needs access to cross-sentence information such as coreference. We suggest that the AMR approach to semantic representation has useful characteristics for an extension to discourse-level representations. AMR represents sentence meaning in a simple, readable semantic graph, such that annotators may directly mark co"
C18-1313,W16-1004,1,0.894831,"Missing"
C18-1313,L18-1266,1,0.839029,"checked against speaker metadata to confirm that annotators were properly keeping track of discourse 3695 Files AMRs Tokens Coreference Chains Implicit Roles Bridging Relations Train 284 7826 122000 3810 2386 1792 Test 9 201 3700 87 67 54 Double Annotation 43 588 8200 381 371 160 Table 1: Basic statistics about size of the MS-AMR corpus participants, and certain highly anaphoric elements (such as “he” and “she”) flagged whenever not annotated as anaphoric. As the AMR corpus was also being corrected and revised during this annotation to improve predicate coverage and treatment of comparatives (Bonial et al., 2018), annotations were stored with their concept labels and double-checked for changes in the underlying AMR. Final data (included in an upcoming AMR public release) includes a description of each AMR document (as defined by LDC segmentations of multi-thread discourse into tractable discussions), defined as an ordered list of AMRs identified by their IDs. Each coreference cluster identifies explicit mentions by the ID within the normal AMR release and the variable within that AMR; implicit roles are identified by the identity of the predicate they are an argument of, with a label for the numbered"
C18-1313,P13-2131,1,0.941548,"tence, we separately annotated two multi-sentence AMR annotations for that same document, each annotated on top of a different set of AMRs. The challenges in evaluating this kind of agreement presage the challenges of measuring the quality of system-predicted MS-AMR. In traditional annotations of coreference over surface forms, one can assume that two mentions are identical when they refer to the same span of text. However, with two separate AMRs, one cannot directly infer whether two mentions reference the same span, but must determine a mapping between the variables of each AMR. The SMATCH (Cai and Knight, 2013) algorithm, used for evaluating AMRs, calculates such a mapping. Using SMATCH to align those AMRs and coreference scores from the reference implementation of the scoring metrics (Pradhan et al., 2014b), we find the CoNLL-2012 F1 to be 66.72. Such a score is limited in that it was measured over a very small exploration set (40 AMRs), but it provides some preliminary suggestion that the identical underlying AMRs used in the IAA numbers above are not dramatically inflating the inter-annotator agreement. 3.3 Implicit Role Agreement Implicit roles have been annotated in prior corpora, but those ann"
C18-1313,N13-3004,0,0.0802189,"an be added to coreference clusters just like any other variable. This is similar to prior works in implicit role annotation (Gerber and Chai, 2010) in that we are using semantic role inventories to prompt annotators with possible implicit roles, while adding the innovation of fitting this within a coreference task. However, while previous annotations prompted annotators with an implicit role and asked them to look through prior text for its referents, this annotation fits implicit roles into the task of coreference labeling. An example of the actual act of annotation with the Anafora toolkit(Chen and Styler, 2013) for these additional implicit role options can be seen in Figure 2. This annotation also labels some examples of “bridging” coreference relations (Clark, 1977; Poesio et al., 1997). We annotate two more common bridging relations, part/whole relations (as in example 1) and set/member relations (as in example 2), with a focus upon those between named entities and common nouns. 1. I think this shows that pretty much every President can do any design thing they want with both the 3694 Figure 2: Annotation interface, illustrating implicit role links. Annotators click on boxes within the AMR (left)"
C18-1313,D16-1245,0,0.018099,"r mental state verb – such as the person being interested in “that’s interesting”, or the cause of “I laughed out loud”. A similar issue involved the recipient or listener role for verbs like “say-01” or “ask-01”, which can sometimes be inferable from context but are low in prominence. 4 4.1 Measuring MS-AMR similarity — scoring system performance A baseline implementation We present a simple baseline that hints at a lower bound for the task. We use the publicly available version of the Brandeis transition-based AMR parser (Wang et al., 2016) combined with an off-the-shelf coreference system (Clark and Manning, 2016) using an AMR-to-surface-form aligner (Flanigan et al., 2014) to convert the surface coreference to links between AMR nodes. 4.2 Simple evaluation of system prediction As mentioned in section 3.2, one hurdle in evaluating system predictions comes from the absence of clearly alignable “mentions” for use in coreference metrics. We can use the SMATCH metric on each individual sentence within an AMR document to resolve this, as calculating a SMATCH score involves determining the highest scoring alignment between variables within a system prediction and a gold AMR. We can then score against that ma"
C18-1313,cmejrek-etal-2004-prague,0,0.084218,"Missing"
C18-1313,P14-1134,0,0.090278,"hat’s interesting”, or the cause of “I laughed out loud”. A similar issue involved the recipient or listener role for verbs like “say-01” or “ask-01”, which can sometimes be inferable from context but are low in prominence. 4 4.1 Measuring MS-AMR similarity — scoring system performance A baseline implementation We present a simple baseline that hints at a lower bound for the task. We use the publicly available version of the Brandeis transition-based AMR parser (Wang et al., 2016) combined with an off-the-shelf coreference system (Clark and Manning, 2016) using an AMR-to-surface-form aligner (Flanigan et al., 2014) to convert the surface coreference to links between AMR nodes. 4.2 Simple evaluation of system prediction As mentioned in section 3.2, one hurdle in evaluating system predictions comes from the absence of clearly alignable “mentions” for use in coreference metrics. We can use the SMATCH metric on each individual sentence within an AMR document to resolve this, as calculating a SMATCH score involves determining the highest scoring alignment between variables within a system prediction and a gold AMR. We can then score against that mapping. The simple baseline outlined above was evaluated accor"
C18-1313,P10-1160,0,0.418977,"e within-sentence AMR has all predicates annotated with PropBank senses, we have access to the lexicon with a list of all numbered arguments we might expect for that predicate. We can therefore produce a list of the numbered arguments that are not explicitly filled in the text and show these unfilled roles to annotators. These unfilled roles are temporarily added during annotation – as is illustrated with the arg3 and arg4 of the predicate “arrive01” in Figure 1 – and can be added to coreference clusters just like any other variable. This is similar to prior works in implicit role annotation (Gerber and Chai, 2010) in that we are using semantic role inventories to prompt annotators with possible implicit roles, while adding the innovation of fitting this within a coreference task. However, while previous annotations prompted annotators with an implicit role and asked them to look through prior text for its referents, this annotation fits implicit roles into the task of coreference labeling. An example of the actual act of annotation with the Anafora toolkit(Chen and Styler, 2013) for these additional implicit role options can be seen in Figure 2. This annotation also labels some examples of “bridging” c"
C18-1313,J12-4003,0,0.138305,"erence, but only cover a small portion of the total semantics of a document, filtering only the elements relevant to a task-specific ontology (Song et al., 2015; Bies et al., 2016). Li Song and Xue (2018) annotates dropped pronouns over Chinese AMR annotators, but only deals with implicit roles in specific constructions. Annotations independent of AMR have provided implicit role annotations with PropBank or NomBank(Palmer et al., 2005; Meyers et al., 2004) semantic roles, but both resources were limited to a small inventory of 5-10 predicate types, rather than all implicit arguments in a text(Gerber and Chai, 2012; Moor et al., 2013). Bridging information has also been annotated in a range of recent corpora (Nedoluzhko et al., 2009; Poesio et al., 2008; O’Gorman et al., 2016; Roesiger, 2018) and resulted in systems (Poesio et al., 2004) capable of predicting such relations. Some bridging relations were also captured by within-sentence AMR annotations, encoded by the “include-91” relation. While MS-AMR focuses upon capturing the propositional content of a document, it does not capture other dimensions of discourse annotation, such as rhetorical structure. Corpora such as RST (Carlson 3696 et al., 2003),"
C18-1313,N15-1114,0,0.360852,"it roles and bridging relations on top of gold Abstract Meaning Representations of sentence-level semantics. We present the methodology of developing this corpus, alongside analysis of its quality and a plausible baseline for comparison. It is hoped that this Multi-Sentence AMR corpus (MS-AMR) illustrates a feasible approach to developing rich representations of document meaning, useful for tasks such as information extraction and question answering. 1 Introduction Although Abstract Meaning Representation (AMR) (Banarescu et al., 2013) shows promise for a range of tasks such as summarization (Liu et al., 2015; Viet et al., 2017) and information extraction (Garg et al., 2016), it is restricted to capturing the semantics of individual sentences. For many purposes, when examining the semantics of a document, one also needs access to cross-sentence information such as coreference. We suggest that the AMR approach to semantic representation has useful characteristics for an extension to discourse-level representations. AMR represents sentence meaning in a simple, readable semantic graph, such that annotators may directly mark coreference relations upon the AMR graph itself. AMR also annotates implicit"
C18-1313,H05-1004,0,0.0566913,"ease. 3.1 Coreference Chain Quality When dealing with two MS-AMR annotations done over the same set of gold AMRs – as with interannotator agreement data here – we can treat a given variable in each AMR as a possible “mention”, just as one might treat a span of text in a document. With that assumption, we may therefore measure MS-AMR IAA coreference scores using standard means of measuring coreference quality. One current approach – used because it is the standard for scoring coreference systems – is to use an average of the BCUB (Bagga and Baldwin, 1998), MUC (Vilain et al., 1995) and CEAF-E (Luo, 2005) metrics, referred to as the “CoNLL-2012 F1 score” (calculated using the reference implementation of Pradhan et al (2014a)). Under those assumptions, the annotations get a CoNLL-2012 F1 of 69.86, using the reference implementation for these scores. For comparison, more traditional span-based coreference annotations (O’Gorman et al., 2016) found inter-annotator agreement of 65.5 for event F1, and 70.4 for entity F1 (CoNLL F1 score). This is therefore in the rough range of where one might expect human performance to be. However, this does not have actual comparability to other annotation schemes"
C18-1313,meyers-etal-2004-annotating,0,0.0250154,"of connected graphs, rather than many different layers of annotation. Annotations such as ACE and ERE also capture roles and entity annotations alongside coreference, but only cover a small portion of the total semantics of a document, filtering only the elements relevant to a task-specific ontology (Song et al., 2015; Bies et al., 2016). Li Song and Xue (2018) annotates dropped pronouns over Chinese AMR annotators, but only deals with implicit roles in specific constructions. Annotations independent of AMR have provided implicit role annotations with PropBank or NomBank(Palmer et al., 2005; Meyers et al., 2004) semantic roles, but both resources were limited to a small inventory of 5-10 predicate types, rather than all implicit arguments in a text(Gerber and Chai, 2012; Moor et al., 2013). Bridging information has also been annotated in a range of recent corpora (Nedoluzhko et al., 2009; Poesio et al., 2008; O’Gorman et al., 2016; Roesiger, 2018) and resulted in systems (Poesio et al., 2004) capable of predicting such relations. Some bridging relations were also captured by within-sentence AMR annotations, encoded by the “include-91” relation. While MS-AMR focuses upon capturing the propositional co"
C18-1313,miltsakaki-etal-2004-penn,0,0.19873,"Missing"
C18-1313,W13-0211,0,0.0153168,"a small portion of the total semantics of a document, filtering only the elements relevant to a task-specific ontology (Song et al., 2015; Bies et al., 2016). Li Song and Xue (2018) annotates dropped pronouns over Chinese AMR annotators, but only deals with implicit roles in specific constructions. Annotations independent of AMR have provided implicit role annotations with PropBank or NomBank(Palmer et al., 2005; Meyers et al., 2004) semantic roles, but both resources were limited to a small inventory of 5-10 predicate types, rather than all implicit arguments in a text(Gerber and Chai, 2012; Moor et al., 2013). Bridging information has also been annotated in a range of recent corpora (Nedoluzhko et al., 2009; Poesio et al., 2008; O’Gorman et al., 2016; Roesiger, 2018) and resulted in systems (Poesio et al., 2004) capable of predicting such relations. Some bridging relations were also captured by within-sentence AMR annotations, encoded by the “include-91” relation. While MS-AMR focuses upon capturing the propositional content of a document, it does not capture other dimensions of discourse annotation, such as rhetorical structure. Corpora such as RST (Carlson 3696 et al., 2003), SDRT (Baldridge et"
C18-1313,W09-3017,0,0.0456552,"Missing"
C18-1313,W16-5706,1,0.924886,"Missing"
C18-1313,J05-1004,1,0.754767,"name :op1 “Paris”) He arrived at noon (a / arrive-01 : ARG 1 (h / he) : ARG 3 (i / implicit role: start point) : ARG 4 (i2 / implicit role: end point; destination) : TIME (d / date-entity :dayperiod (n3 / noon))) Figure 1: Example of MS-AMR annotation; annotators link coreferent variables (such as marking a relation between between p and h (in red)) and implicit roles, here linking the destination (arg4) in the second sentence to the previous variable c (in blue) of how that meaning was expressed. AMRs capture basic representations of semantic roles (using the numbered arguments of PropBank (Palmer et al., 2005)), as well as within-sentence coreference, named entity and entity linking information. Thus, a simple sentence such as “Bill left for Paris” can be represented as in the first AMR in Figure 1, with “leave-11” denoting the physical departure sense of “leave”, numbered arguments such as “arg2” denoting semantic roles that are unique to that sense (e.g. for leave11, arg2 is the destination), and “Bill” and “Paris” both having named entity labels (person and city) as well as links to Wikipedia when possible. One can therefore think of an AMR as a graph of the meaning of a sentence, with “variable"
C18-1313,W97-1301,0,0.0174545,"inventories to prompt annotators with possible implicit roles, while adding the innovation of fitting this within a coreference task. However, while previous annotations prompted annotators with an implicit role and asked them to look through prior text for its referents, this annotation fits implicit roles into the task of coreference labeling. An example of the actual act of annotation with the Anafora toolkit(Chen and Styler, 2013) for these additional implicit role options can be seen in Figure 2. This annotation also labels some examples of “bridging” coreference relations (Clark, 1977; Poesio et al., 1997). We annotate two more common bridging relations, part/whole relations (as in example 1) and set/member relations (as in example 2), with a focus upon those between named entities and common nouns. 1. I think this shows that pretty much every President can do any design thing they want with both the 3694 Figure 2: Annotation interface, illustrating implicit role links. Annotators click on boxes within the AMR (left) to add them to coreference chains (full chains shown on the right), as with the link between the implicit topic (“i2”) and the earlier “l / lie” mention. Residence and the Office W"
C18-1313,P04-1019,0,0.0156849,"uns over Chinese AMR annotators, but only deals with implicit roles in specific constructions. Annotations independent of AMR have provided implicit role annotations with PropBank or NomBank(Palmer et al., 2005; Meyers et al., 2004) semantic roles, but both resources were limited to a small inventory of 5-10 predicate types, rather than all implicit arguments in a text(Gerber and Chai, 2012; Moor et al., 2013). Bridging information has also been annotated in a range of recent corpora (Nedoluzhko et al., 2009; Poesio et al., 2008; O’Gorman et al., 2016; Roesiger, 2018) and resulted in systems (Poesio et al., 2004) capable of predicting such relations. Some bridging relations were also captured by within-sentence AMR annotations, encoded by the “include-91” relation. While MS-AMR focuses upon capturing the propositional content of a document, it does not capture other dimensions of discourse annotation, such as rhetorical structure. Corpora such as RST (Carlson 3696 et al., 2003), SDRT (Baldridge et al., 2007), GraphBank (Wolf et al., 2004) or PDTB (Miltsakaki et al., ) therefore capture dimensions of meaning that differ from the propositional content captured in this corpus. 3 Measuring MS-AMR Corpus Q"
C18-1313,poesio-artstein-2008-anaphoric,0,0.146342,"Missing"
C18-1313,W11-1901,1,0.913919,"o “the White House”) 2. I also liked “Deception point”. So I have read all four of his books and enjoyed them. (set/member relation) Those examples illustrate the emphasis upon capturing part/whole and set/member relations that require contextual understanding; annotators were instructed not to link part/whole relations that are only knowable through world knowledge, specifically those between named, wikified entities (such as knowing that Damascus is part of Syria). This annotation also captures more event coreference phenomena than what is captured in OntoNotesstyle coreference annotations (Pradhan et al., 2011). While those prior annotations focused upon nominal coreference, capturing verbal mentions only occasionally (when they were coreferent with a nominal mention), multi-sentence AMR annotators were instructed to link together coreferent variables regardless of their part of speech. Furthermore, because of the AMR normalization of surface-form variation, complex details regarding how to represent an event (such as the span to use for light verbs) is already normalized into single PropBank rolesets during AMR annotation. Annotation guidelines are publicly available at https://github.com/timjogorm"
C18-1313,P14-2006,0,0.241317,"s – as with interannotator agreement data here – we can treat a given variable in each AMR as a possible “mention”, just as one might treat a span of text in a document. With that assumption, we may therefore measure MS-AMR IAA coreference scores using standard means of measuring coreference quality. One current approach – used because it is the standard for scoring coreference systems – is to use an average of the BCUB (Bagga and Baldwin, 1998), MUC (Vilain et al., 1995) and CEAF-E (Luo, 2005) metrics, referred to as the “CoNLL-2012 F1 score” (calculated using the reference implementation of Pradhan et al (2014a)). Under those assumptions, the annotations get a CoNLL-2012 F1 of 69.86, using the reference implementation for these scores. For comparison, more traditional span-based coreference annotations (O’Gorman et al., 2016) found inter-annotator agreement of 65.5 for event F1, and 70.4 for entity F1 (CoNLL F1 score). This is therefore in the rough range of where one might expect human performance to be. However, this does not have actual comparability to other annotation schemes, as this data uses both event and entity coreference, and does not encompass within-sentence coreference (which is alre"
C18-1313,L18-1058,0,0.0215915,"ng and Xue (2018) annotates dropped pronouns over Chinese AMR annotators, but only deals with implicit roles in specific constructions. Annotations independent of AMR have provided implicit role annotations with PropBank or NomBank(Palmer et al., 2005; Meyers et al., 2004) semantic roles, but both resources were limited to a small inventory of 5-10 predicate types, rather than all implicit arguments in a text(Gerber and Chai, 2012; Moor et al., 2013). Bridging information has also been annotated in a range of recent corpora (Nedoluzhko et al., 2009; Poesio et al., 2008; O’Gorman et al., 2016; Roesiger, 2018) and resulted in systems (Poesio et al., 2004) capable of predicting such relations. Some bridging relations were also captured by within-sentence AMR annotations, encoded by the “include-91” relation. While MS-AMR focuses upon capturing the propositional content of a document, it does not capture other dimensions of discourse annotation, such as rhetorical structure. Corpora such as RST (Carlson 3696 et al., 2003), SDRT (Baldridge et al., 2007), GraphBank (Wolf et al., 2004) or PDTB (Miltsakaki et al., ) therefore capture dimensions of meaning that differ from the propositional content captur"
C18-1313,S10-1008,1,0.841856,"Missing"
C18-1313,W15-0812,0,0.026372,"emantic data and coreference have been represented in the multiple layers of the OntoNotes corpus (Pradhan et al., 2011) and the Prague Czech-English Dependency ˇ Treebank (PCEDT) (Cmejrek et al., 2004). This MS-AMR data differs primarily in that the data is more directly joined into a single set of connected graphs, rather than many different layers of annotation. Annotations such as ACE and ERE also capture roles and entity annotations alongside coreference, but only cover a small portion of the total semantics of a document, filtering only the elements relevant to a task-specific ontology (Song et al., 2015; Bies et al., 2016). Li Song and Xue (2018) annotates dropped pronouns over Chinese AMR annotators, but only deals with implicit roles in specific constructions. Annotations independent of AMR have provided implicit role annotations with PropBank or NomBank(Palmer et al., 2005; Meyers et al., 2004) semantic roles, but both resources were limited to a small inventory of 5-10 predicate types, rather than all implicit arguments in a text(Gerber and Chai, 2012; Moor et al., 2013). Bridging information has also been annotated in a range of recent corpora (Nedoluzhko et al., 2009; Poesio et al., 20"
C18-1313,M95-1005,0,0.120834,"” column, and provided in the release. 3.1 Coreference Chain Quality When dealing with two MS-AMR annotations done over the same set of gold AMRs – as with interannotator agreement data here – we can treat a given variable in each AMR as a possible “mention”, just as one might treat a span of text in a document. With that assumption, we may therefore measure MS-AMR IAA coreference scores using standard means of measuring coreference quality. One current approach – used because it is the standard for scoring coreference systems – is to use an average of the BCUB (Bagga and Baldwin, 1998), MUC (Vilain et al., 1995) and CEAF-E (Luo, 2005) metrics, referred to as the “CoNLL-2012 F1 score” (calculated using the reference implementation of Pradhan et al (2014a)). Under those assumptions, the annotations get a CoNLL-2012 F1 of 69.86, using the reference implementation for these scores. For comparison, more traditional span-based coreference annotations (O’Gorman et al., 2016) found inter-annotator agreement of 65.5 for event F1, and 70.4 for entity F1 (CoNLL F1 score). This is therefore in the rough range of where one might expect human performance to be. However, this does not have actual comparability to o"
C18-1313,S16-1181,0,0.0207727,"rence, most commonly with the non-focused element in a communication or mental state verb – such as the person being interested in “that’s interesting”, or the cause of “I laughed out loud”. A similar issue involved the recipient or listener role for verbs like “say-01” or “ask-01”, which can sometimes be inferable from context but are low in prominence. 4 4.1 Measuring MS-AMR similarity — scoring system performance A baseline implementation We present a simple baseline that hints at a lower bound for the task. We use the publicly available version of the Brandeis transition-based AMR parser (Wang et al., 2016) combined with an off-the-shelf coreference system (Clark and Manning, 2016) using an AMR-to-surface-form aligner (Flanigan et al., 2014) to convert the surface coreference to links between AMR nodes. 4.2 Simple evaluation of system prediction As mentioned in section 3.2, one hurdle in evaluating system predictions comes from the absence of clearly alignable “mentions” for use in coreference metrics. We can use the SMATCH metric on each individual sentence within an AMR document to resolve this, as calculating a SMATCH score involves determining the highest scoring alignment between variables"
C98-1112,P96-1027,0,0.142228,"he grammar is not organized around English syntax. Nitrogen&apos;s algorithm operates bottom-up, efficiently encoding multiple analyses in a lattice data structure to allow structure sharing, analogous to the way a chart is used in bottom-up parsing. In contrast, traditional generation control mechanisms work top-down, either deterministically (Meteer et el., 1987; Penman, 1989) or by backtracking to previous choice points (Elhadad, 1993b). This unnecessarily duplicates work at run time, unless sophisticated control directives are included in the search engine (Elhadad and Robin, 1992). Recently, (Kay, 1996) has explored a bottom-up approach to generation as well, using a chart rather than a word lattice. Nitrogen&apos;s generation is robust and scalable. It can generate output even for unexpected or incomplete input, and is designed for broad coverage. It does not require the detailed, difficult-to-obtain knowledge bases that other NLG systems require, since it relies instead on corpus-based statistics to make a wide variety of linguistic decisions. Currently the quality of the output is limited by the use of only word bigram statistical information, which cannot handle long-distance agreement, or di"
C98-1112,P95-1034,1,0.709809,". Thus, shifting such linguistic decisions to the generator is significantly helpful for client applications. However, at the same time, it imposes enormous needs for knowledge on the generator program. Traditional large-scale NLG already requires immense amounts of knowledge, as does any large-scale AI enterprise. NLG operating on a scale of 200,000 entities (concepts, relations, and words) requires large and sophisticated lexicons, grammars, ontologies, collocation lists, and morphological tables. Acquiring and applying accurate, detailed knowledge of this breadth poses difficult problems. (Knight and Hatzivassiloglou, 1995) suggested tistical knowledge. Nitrogen performs sentence realization and some components of sentence planning-namely, mapping domain concepts to content words, and to some extent, mapping semantic relations to grammatical ones. It contributes: meaning I symbolic generator ~- lexicon +- grammar * A flexible input representation based o11 conceptual meanings and the relations between them. word lattice of possible renderings * A new grammar formalism for defining the mapping of meanings onto word lattices. $ [. sta.tistical extractor ] e--corpus . A new efficient algorithm to do this mapping. •"
C98-1112,H94-1020,0,0.045286,"ected or incomplete input, and is designed for broad coverage. It does not require the detailed, difficult-to-obtain knowledge bases that other NLG systems require, since it relies instead on corpus-based statistics to make a wide variety of linguistic decisions. Currently the quality of the output is limited by the use of only word bigram statistical information, which cannot handle long-distance agreement, or distinguish likely collocations from unlikely grammatical structure. However, we plan to remedy these problems by using statistical information extracted from the Penn Treebank corpus (Marcus et al., 1994) to rank tagged lattices and parse forests. Nitrogen&apos;s rule matching is much less expensive than graph unification, and lattices generated for sub-AMRs are cached and reused in subsequent references. The semantic roles used in the grammar formalism cover most common syntactic phenomena, though our grammar does not yet generate questions, or infer pronouns from explicit coreference. Nitrogen has been used extensively as part of a semantics-based Japanese-English MT system (Knight et al., 1995). Japanese analysis provides AMR&apos;s, which Nitrogen transforms into word lattices on the order of hundre"
C98-1112,P89-1002,0,0.0587581,"Missing"
D07-1038,N04-1035,1,0.884736,"than that used to obtain the word-for-word alignments). Weights for the second model are then set, typically by counting and smoothing, and this weighted model is used for translation. Realistic approaches scale to large data sets and have yielded better BLEU performance than their idealistic counterparts, but there is a disconnect between the first model (hereafter, the alignment model) and the second (the translation model). Examples of realistic systems are the phrase-based ATS system of Och and Ney (2004), the phrasal-syntax hybrid system Hiero (Chiang, 2005), and the GHKM syntax system (Galley et al., 2004; Galley et al., 2006). For an alignment model, most of these use the Aachen HMM approach (Vogel et al., 1996), the implementation of IBM Model 4 in GIZA++ (Och and Ney, 2000) or, more recently, the semi-supervised EMD algorithm (Fraser and Marcu, 2006). The two-model approach of the realistic path has undeniable empirical advantages and scales to large data sets, but new research tends to focus on development of higher order translation models that are informed only by low-order alignments. We would like to add the analytic power gained from modern translation models to the underlying alignme"
D07-1038,P06-1121,1,0.861846,"tain the word-for-word alignments). Weights for the second model are then set, typically by counting and smoothing, and this weighted model is used for translation. Realistic approaches scale to large data sets and have yielded better BLEU performance than their idealistic counterparts, but there is a disconnect between the first model (hereafter, the alignment model) and the second (the translation model). Examples of realistic systems are the phrase-based ATS system of Och and Ney (2004), the phrasal-syntax hybrid system Hiero (Chiang, 2005), and the GHKM syntax system (Galley et al., 2004; Galley et al., 2006). For an alignment model, most of these use the Aachen HMM approach (Vogel et al., 1996), the implementation of IBM Model 4 in GIZA++ (Och and Ney, 2000) or, more recently, the semi-supervised EMD algorithm (Fraser and Marcu, 2006). The two-model approach of the realistic path has undeniable empirical advantages and scales to large data sets, but new research tends to focus on development of higher order translation models that are informed only by low-order alignments. We would like to add the analytic power gained from modern translation models to the underlying alignment model without sacri"
D07-1038,P01-1030,1,0.236925,"odels. This leads to extraction of more useful linguistic patterns and improved BLEU scores on translation experiments in Chinese and Arabic. 1 Methods of statistical MT Roughly speaking, there are two paths commonly taken in statistical machine translation (Figure 1). The idealistic path uses an unsupervised learning algorithm such as EM (Demptser et al., 1977) to learn parameters for some proposed translation model from a bitext training corpus, and then directly translates using the weighted model. Some examples of the idealistic approach are the direct IBM word model (Berger et al., 1994; Germann et al., 2001), the phrase-based approach of Marcu and Wong (2002), and the syntax approaches of Wu (1996) and Yamada and Knight (2001). Idealistic approaches are conceptually simple and thus easy to relate to observed phenomena. However, as more parameters are added to the model the idealistic approach has not scaled well, for it is increasingly difficult to incorporate large amounts of training data efficiently over an increasingly large search space. Additionally, the EM procedure has a tendency to overfit its training data when the input units have varying explanatory powers, such as variable-size phras"
D07-1038,N04-1014,1,0.587779,"RDS 9,864,294 GIZA CORPUS C HINESE WORDS 7,520,779 RE - ALIGNMENT EXPERIMENT TYPE RULES TUNE TEST baseline initial adjusted 19,138,252 18,698,549 26,053,341 39.08 39.49 39.76 37.77 38.39 38.69 Table 2: A comparison of Chinese BLEU performance between the GIZA baseline (no re-alignment), realignment as proposed in Section 3.2, and re-alignment as modified in Section 5.4 algorithm (Galley et al., 2004) to obtain a rule set for each bootstrap alignment. Now we need an EM algorithm for learning Ythe parameters of the rule set that maximize p(tree, string). Such an algorithm is precorpus sented by Graehl and Knight (2004). The algorithm consists of two components: D ERIV, which is a procedure for constructing a packed forest of derivation trees of rules that explain a (tree, string) bitext corpus given that corpus and a rule set, and T RAIN, which is an iterative parameter-setting procedure. We initially attempted to use the top-down D E RIV algorithm of Graehl and Knight (2004), but as the constraints of the derivation forests are largely lexical, too much time was spent on exploring deadends. Instead we build derivation forests using the following sequence of operations: 1. Binarize rules using the synchrono"
D07-1038,W02-1018,0,0.0498816,"istic patterns and improved BLEU scores on translation experiments in Chinese and Arabic. 1 Methods of statistical MT Roughly speaking, there are two paths commonly taken in statistical machine translation (Figure 1). The idealistic path uses an unsupervised learning algorithm such as EM (Demptser et al., 1977) to learn parameters for some proposed translation model from a bitext training corpus, and then directly translates using the weighted model. Some examples of the idealistic approach are the direct IBM word model (Berger et al., 1994; Germann et al., 2001), the phrase-based approach of Marcu and Wong (2002), and the syntax approaches of Wu (1996) and Yamada and Knight (2001). Idealistic approaches are conceptually simple and thus easy to relate to observed phenomena. However, as more parameters are added to the model the idealistic approach has not scaled well, for it is increasingly difficult to incorporate large amounts of training data efficiently over an increasingly large search space. Additionally, the EM procedure has a tendency to overfit its training data when the input units have varying explanatory powers, such as variable-size phrases or variable-height trees. The realistic path also"
D07-1038,P00-1056,0,0.402881,"nslation. Realistic approaches scale to large data sets and have yielded better BLEU performance than their idealistic counterparts, but there is a disconnect between the first model (hereafter, the alignment model) and the second (the translation model). Examples of realistic systems are the phrase-based ATS system of Och and Ney (2004), the phrasal-syntax hybrid system Hiero (Chiang, 2005), and the GHKM syntax system (Galley et al., 2004; Galley et al., 2006). For an alignment model, most of these use the Aachen HMM approach (Vogel et al., 1996), the implementation of IBM Model 4 in GIZA++ (Och and Ney, 2000) or, more recently, the semi-supervised EMD algorithm (Fraser and Marcu, 2006). The two-model approach of the realistic path has undeniable empirical advantages and scales to large data sets, but new research tends to focus on development of higher order translation models that are informed only by low-order alignments. We would like to add the analytic power gained from modern translation models to the underlying alignment model without sacrificing the efficiency and empirical gains of the two-model approach. By adding the 360 Proceedings of the 2007 Joint Conference on Empirical Methods in N"
D07-1038,J04-4002,0,0.0114153,"elds a set of patterns or rules for a second translation model (which often has a wider parameter space than that used to obtain the word-for-word alignments). Weights for the second model are then set, typically by counting and smoothing, and this weighted model is used for translation. Realistic approaches scale to large data sets and have yielded better BLEU performance than their idealistic counterparts, but there is a disconnect between the first model (hereafter, the alignment model) and the second (the translation model). Examples of realistic systems are the phrase-based ATS system of Och and Ney (2004), the phrasal-syntax hybrid system Hiero (Chiang, 2005), and the GHKM syntax system (Galley et al., 2004; Galley et al., 2006). For an alignment model, most of these use the Aachen HMM approach (Vogel et al., 1996), the implementation of IBM Model 4 in GIZA++ (Och and Ney, 2000) or, more recently, the semi-supervised EMD algorithm (Fraser and Marcu, 2006). The two-model approach of the realistic path has undeniable empirical advantages and scales to large data sets, but new research tends to focus on development of higher order translation models that are informed only by low-order alignments."
D07-1038,H94-1028,0,0.060612,"Missing"
D07-1038,J93-2003,0,0.0369719,"exercise for the reader). A string-totree decoder constructs a derivation forest of derivation trees where the right sides of the rules in a tree, taken together, explain a candidate source sentence. It then outputs the English tree corresponding to the highest-scoring derivation in the forest. 3 Introducing syntax into the alignment model We now lay the ground for a syntactically motivated alignment model. We begin by reviewing an alignment model commonly seen in realistic MT systems and compare it to a syntactically-aware alignment model. 3.1 The traditional IBM alignment model IBM Model 4 (Brown et al., 1993) learns a set of 4 probability tables to compute p(f |e) given a foreign sentence f and its target translation e via the following (greatly simplified) generative story: NPB PP NPB NNP POS taiwan ’s R1: NN IN surplus in x0:NPB R2: x1:NN → NPB NNP POS taiwan ’s PP NN IN NP-C trade between NPB ¥ x1 DT CD NNS the two shores R10: x0:NPB R11: R10: x0:NNP x0:NPB → x0 NPB R17: POS R12: NNP → Ñl x0:IN R4: IN → R13: ó x1:NP-C → PP IN ó x0 ¥ R5: NP-C → between Ü NNP x0:POS taiwan Ñl ’s x0:NP-C → x0 PP IN x0:NP-C in → x1 x0 NP-C x0:NPB PP IN R5: x1:PP x0:NPB R6: → x1 x0 → x0 NPB R19: in in x1:NN R18: PO"
D07-1038,P05-1033,0,0.111985,"el (which often has a wider parameter space than that used to obtain the word-for-word alignments). Weights for the second model are then set, typically by counting and smoothing, and this weighted model is used for translation. Realistic approaches scale to large data sets and have yielded better BLEU performance than their idealistic counterparts, but there is a disconnect between the first model (hereafter, the alignment model) and the second (the translation model). Examples of realistic systems are the phrase-based ATS system of Och and Ney (2004), the phrasal-syntax hybrid system Hiero (Chiang, 2005), and the GHKM syntax system (Galley et al., 2004; Galley et al., 2006). For an alignment model, most of these use the Aachen HMM approach (Vogel et al., 1996), the implementation of IBM Model 4 in GIZA++ (Och and Ney, 2000) or, more recently, the semi-supervised EMD algorithm (Fraser and Marcu, 2006). The two-model approach of the realistic path has undeniable empirical advantages and scales to large data sets, but new research tends to focus on development of higher order translation models that are informed only by low-order alignments. We would like to add the analytic power gained from mo"
D07-1038,D07-1079,1,0.347425,"in Viterbi derivation trees, then use the annotated partial alignments to obtain Viterbi alignments. 4. Use the new alignments as input to the MT system described below. 5.2 The MT system setup A truly idealistic MT system would directly apply the rule weight parameters learned via EM to a machine translation task. As mentioned in Section 1, we maintain the two-model, or realistic approach. Below we briefly describe the translation model, focusing on comparison with the previously described alignment model. Galley et al. (2006) provides a more complete description of the translation model and DeNeefe et al. (2007) provides a more complete description of the end-to-end translation pipeline. Although in principle the re-alignment model and translation model learn parameter weights over the same rule space, in practice we limit the rules used for re-alignment to the set of smallest rules that explain the training corpus and are consistent with the bootstrap alignments. This is a compromise made to reduce the search space for EM. The translation model learns multiple derivations of rules consistent with the re-alignments for each sentence, and learns (a) Chinese re-alignment corpus has 9,864,294 English an"
D07-1038,P06-1097,0,0.184644,"better BLEU performance than their idealistic counterparts, but there is a disconnect between the first model (hereafter, the alignment model) and the second (the translation model). Examples of realistic systems are the phrase-based ATS system of Och and Ney (2004), the phrasal-syntax hybrid system Hiero (Chiang, 2005), and the GHKM syntax system (Galley et al., 2004; Galley et al., 2006). For an alignment model, most of these use the Aachen HMM approach (Vogel et al., 1996), the implementation of IBM Model 4 in GIZA++ (Och and Ney, 2000) or, more recently, the semi-supervised EMD algorithm (Fraser and Marcu, 2006). The two-model approach of the realistic path has undeniable empirical advantages and scales to large data sets, but new research tends to focus on development of higher order translation models that are informed only by low-order alignments. We would like to add the analytic power gained from modern translation models to the underlying alignment model without sacrificing the efficiency and empirical gains of the two-model approach. By adding the 360 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp."
D07-1038,P03-1021,0,0.0114356,"Missing"
D07-1038,C96-2141,0,0.592313,"by counting and smoothing, and this weighted model is used for translation. Realistic approaches scale to large data sets and have yielded better BLEU performance than their idealistic counterparts, but there is a disconnect between the first model (hereafter, the alignment model) and the second (the translation model). Examples of realistic systems are the phrase-based ATS system of Och and Ney (2004), the phrasal-syntax hybrid system Hiero (Chiang, 2005), and the GHKM syntax system (Galley et al., 2004; Galley et al., 2006). For an alignment model, most of these use the Aachen HMM approach (Vogel et al., 1996), the implementation of IBM Model 4 in GIZA++ (Och and Ney, 2000) or, more recently, the semi-supervised EMD algorithm (Fraser and Marcu, 2006). The two-model approach of the realistic path has undeniable empirical advantages and scales to large data sets, but new research tends to focus on development of higher order translation models that are informed only by low-order alignments. We would like to add the analytic power gained from modern translation models to the underlying alignment model without sacrificing the efficiency and empirical gains of the two-model approach. By adding the 360 P"
D07-1038,P96-1021,0,0.0471829,"on experiments in Chinese and Arabic. 1 Methods of statistical MT Roughly speaking, there are two paths commonly taken in statistical machine translation (Figure 1). The idealistic path uses an unsupervised learning algorithm such as EM (Demptser et al., 1977) to learn parameters for some proposed translation model from a bitext training corpus, and then directly translates using the weighted model. Some examples of the idealistic approach are the direct IBM word model (Berger et al., 1994; Germann et al., 2001), the phrase-based approach of Marcu and Wong (2002), and the syntax approaches of Wu (1996) and Yamada and Knight (2001). Idealistic approaches are conceptually simple and thus easy to relate to observed phenomena. However, as more parameters are added to the model the idealistic approach has not scaled well, for it is increasingly difficult to incorporate large amounts of training data efficiently over an increasingly large search space. Additionally, the EM procedure has a tendency to overfit its training data when the input units have varying explanatory powers, such as variable-size phrases or variable-height trees. The realistic path also learns a model of translation, but uses"
D07-1038,J97-3002,0,0.106154,"erative parameter-setting procedure. We initially attempted to use the top-down D E RIV algorithm of Graehl and Knight (2004), but as the constraints of the derivation forests are largely lexical, too much time was spent on exploring deadends. Instead we build derivation forests using the following sequence of operations: 1. Binarize rules using the synchronous binarization algorithm for tree-to-string transducers described in Zhang et al. (2006). 2. Construct a parse chart with a CKY parser simultaneously constrained on the foreign string and English tree, similar to the bilingual parsing of Wu (1997) 1 . 3. Recover all reachable edges by traversing the chart, starting from the topmost entry. Since the chart is constructed bottom-up, leaf lexical constraints are encountered immediately, resulting in a narrower search space and faster running time than the top-down D ERIV algorithm for this application. Derivation forest construction takes around 400 hours of cumulative machine time (4processor machines) for Chinese. The actual running of EM iterations (which directly implements the T RAIN algorithm of Graehl and Knight (2004)) 1 In the cases where a rule is not synchronous-binarizable stan"
D07-1038,P01-1067,1,0.346039,"in Chinese and Arabic. 1 Methods of statistical MT Roughly speaking, there are two paths commonly taken in statistical machine translation (Figure 1). The idealistic path uses an unsupervised learning algorithm such as EM (Demptser et al., 1977) to learn parameters for some proposed translation model from a bitext training corpus, and then directly translates using the weighted model. Some examples of the idealistic approach are the direct IBM word model (Berger et al., 1994; Germann et al., 2001), the phrase-based approach of Marcu and Wong (2002), and the syntax approaches of Wu (1996) and Yamada and Knight (2001). Idealistic approaches are conceptually simple and thus easy to relate to observed phenomena. However, as more parameters are added to the model the idealistic approach has not scaled well, for it is increasingly difficult to incorporate large amounts of training data efficiently over an increasingly large search space. Additionally, the EM procedure has a tendency to overfit its training data when the input units have varying explanatory powers, such as variable-size phrases or variable-height trees. The realistic path also learns a model of translation, but uses that model only to obtain Vi"
D07-1038,N06-1033,1,0.658697,"re for constructing a packed forest of derivation trees of rules that explain a (tree, string) bitext corpus given that corpus and a rule set, and T RAIN, which is an iterative parameter-setting procedure. We initially attempted to use the top-down D E RIV algorithm of Graehl and Knight (2004), but as the constraints of the derivation forests are largely lexical, too much time was spent on exploring deadends. Instead we build derivation forests using the following sequence of operations: 1. Binarize rules using the synchronous binarization algorithm for tree-to-string transducers described in Zhang et al. (2006). 2. Construct a parse chart with a CKY parser simultaneously constrained on the foreign string and English tree, similar to the bilingual parsing of Wu (1997) 1 . 3. Recover all reachable edges by traversing the chart, starting from the topmost entry. Since the chart is constructed bottom-up, leaf lexical constraints are encountered immediately, resulting in a narrower search space and faster running time than the top-down D ERIV algorithm for this application. Derivation forest construction takes around 400 hours of cumulative machine time (4processor machines) for Chinese. The actual runnin"
D07-1038,J04-4004,0,\N,Missing
D07-1038,J08-3004,1,\N,Missing
D07-1038,D08-1076,0,\N,Missing
D07-1078,A00-2018,0,0.0483426,"t collapses the two NNP’s, so as to generalize this rule, getting rule R 5 and rule R6 . We also need to consistently syntactify the root labels of R4 and the new frontier label of R6 such that these two rules can be composed. Since labeling is not a concern of this paper, we simply label new nodes with X-bar where X here is the parent label. With all these in place, we now can translate the foreign sentence by composing R 6 and R4 in Figure 1. Binarizing the syntax trees for syntax-based machine translation is similar in spirit to generalizing parsing models via markovization (Collins, 1997; Charniak, 2000). But in translation modeling, it is unclear how to effectively markovize the translation rules, especially when the rules are complex like those proposed by Galley et al. (2006). In this paper, we explore the generalization ability of simple binarization methods like left-, right-, and head-binarization, and also their combinations. Simple binarization methods binarize syntax trees in a consistent fashion (left-, right-, or head-) and 746 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 746–754, Pra"
D07-1078,J07-2003,0,0.115537,"ization methods in terms of BLEU on Chinese-to-English translation tasks. 7.1 Experimental setup Our bitext consists of 16M words, all in the mainland-news domain. Our development set is a 925-line subset of the 993-line NIST02 evaluation set. We removed long sentences from the NIST02 evaluation set to speed up discriminative training. The test set is the full 919-line NIST03 evaluation set. We used a bottom-up, CKY-style decoder that works with binary xRs rules obtained via a synchronous binarization procedure (Zhang et al., 2006). The decoder prunes hypotheses using strategies described in (Chiang, 2007). The parse trees on the English side of the bitexts were generated using a parser (Soricut, 2004) implementing the Collins parsing models (Collins, 1997). We used the EM procedure described in (Knight and Graehl, 2004) to perform the inside-outside algorithm on synchronous derivation forests and to generate the Viterbi derivation forest. We used the rule extractor described in (Galley et al., 2006) to extract rules from (e-parse, f, a)-tuples, but we made an important modification: new nodes 752 Table 1 shows the BLEU scores of mixed-cased and detokenized translations of different systems. We"
D07-1078,P97-1003,0,0.0992221,"w tree node that collapses the two NNP’s, so as to generalize this rule, getting rule R 5 and rule R6 . We also need to consistently syntactify the root labels of R4 and the new frontier label of R6 such that these two rules can be composed. Since labeling is not a concern of this paper, we simply label new nodes with X-bar where X here is the parent label. With all these in place, we now can translate the foreign sentence by composing R 6 and R4 in Figure 1. Binarizing the syntax trees for syntax-based machine translation is similar in spirit to generalizing parsing models via markovization (Collins, 1997; Charniak, 2000). But in translation modeling, it is unclear how to effectively markovize the translation rules, especially when the rules are complex like those proposed by Galley et al. (2006). In this paper, we explore the generalization ability of simple binarization methods like left-, right-, and head-binarization, and also their combinations. Simple binarization methods binarize syntax trees in a consistent fashion (left-, right-, or head-) and 746 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning,"
D07-1078,P03-2041,0,0.134446,"CA, 90292 {wwang,kknight,dmarcu}@languageweaver.com Abstract We show that phrase structures in Penn Treebank style parses are not optimal for syntaxbased machine translation. We exploit a series of binarization methods to restructure the Penn Treebank style trees such that syntactified phrases smaller than Penn Treebank constituents can be acquired and exploited in translation. We find that by employing the EM algorithm for determining the binarization of a parse tree among a set of alternative binarizations gives us the best translation result. 1 Introduction Syntax-based translation models (Eisner, 2003; Galley et al., 2006; Marcu et al., 2006) are usually built directly from Penn Treebank (PTB) (Marcus et al., 1993) style parse trees by composing treebank grammar rules. As a result, often no substructures corresponding to partial PTB constituents are extracted to form translation rules. Syntax translation models acquired by composing treebank grammar rules assume that long rewrites are not decomposable into smaller steps. This effectively restricts the generalization power of the induced model. For example, suppose we have an xRs (Knight and Graehl, 2004) rule R 1 in Figure 1 that translate"
D07-1078,N04-1035,1,0.126799,"mployed to make the grammar fit in a CKY parser. In our work, we are focused on binarization of parse trees. Tree binarization generalizes the resulting grammar and changes its probability distribution. In tree binarization, synchronous grammars built from restructured (binarized) training trees still contain non-binary, multi-level rules and thus still require the binarization transformation so as to be employed by a CKY parser. The translation model we are using in this paper belongs to the xRs formalism (Knight and Graehl, 2004), which has been proved successful for machine translation in (Galley et al., 2004; Galley et al., 2006; Marcu et al., 2006). 3 Concepts We focus on tree-to-string (in noisy-channel model sense) translation models. Translation models of this type are typically trained on tuples of a sourcelanguage sentence f, a target language (e.g., English) parse tree π that yields e and translates from f, and the word alignments a between e and f. Such a tuple is called an alignment graph in (Galley et al., 2004). The graph (1) in Figure 2 is such an alignment graph. (1) unbinarized tree NPB NNP1 NNP2 NNP3 NNP4* viktor chernomyrdin VIKTOR−CHERNOMYRDIN (2) left-binarization (3) right-/hea"
D07-1078,P06-1121,1,0.147896,"ng,kknight,dmarcu}@languageweaver.com Abstract We show that phrase structures in Penn Treebank style parses are not optimal for syntaxbased machine translation. We exploit a series of binarization methods to restructure the Penn Treebank style trees such that syntactified phrases smaller than Penn Treebank constituents can be acquired and exploited in translation. We find that by employing the EM algorithm for determining the binarization of a parse tree among a set of alternative binarizations gives us the best translation result. 1 Introduction Syntax-based translation models (Eisner, 2003; Galley et al., 2006; Marcu et al., 2006) are usually built directly from Penn Treebank (PTB) (Marcus et al., 1993) style parse trees by composing treebank grammar rules. As a result, often no substructures corresponding to partial PTB constituents are extracted to form translation rules. Syntax translation models acquired by composing treebank grammar rules assume that long rewrites are not decomposable into smaller steps. This effectively restricts the generalization power of the induced model. For example, suppose we have an xRs (Knight and Graehl, 2004) rule R 1 in Figure 1 that translates the Chinese phrase"
D07-1078,J99-4004,0,0.10699,"g all the way to the left, for example, from tree (1) to tree (2) and to tree (4) in Figure 2, does not enable us to acquire a substructure that yields NNP3 NNP4 and their translational equivalences. To obtain more factorizable sub-phrases, we need to parallel-binarize in both directions. 4.2 Parallel binarization Simple binarizations transform a parse tree into another single parse tree. Parallel binarization will transform a parse tree into a binarization forest, desirably packed to enable dynamic programming when extracting translation rules from it. Borrowing terms from parsing semirings (Goodman, 1999), a packed forest is composed of additive forest nodes (⊕-nodes) and multiplicative forest nodes (⊗-nodes). In the binarization forest, a ⊗node corresponds to a tree node in the unbinarized tree; and this ⊗-node composes several ⊕-nodes, forming a one-level substructure that is observed in the unbinarized tree. A ⊕-node corresponds to alternative ways of binarizing the same tree node in the unbinarized tree and it contains one or more ⊗nodes. The same ⊕-node can appear in more than one place in the packed forest, enabling sharing. Figure 3 shows a packed forest obtained by packing trees (4) an"
D07-1078,N04-1014,1,0.65413,"1 Introduction Syntax-based translation models (Eisner, 2003; Galley et al., 2006; Marcu et al., 2006) are usually built directly from Penn Treebank (PTB) (Marcus et al., 1993) style parse trees by composing treebank grammar rules. As a result, often no substructures corresponding to partial PTB constituents are extracted to form translation rules. Syntax translation models acquired by composing treebank grammar rules assume that long rewrites are not decomposable into smaller steps. This effectively restricts the generalization power of the induced model. For example, suppose we have an xRs (Knight and Graehl, 2004) rule R 1 in Figure 1 that translates the Chinese phrase RUSSIA MINISTER VIKTOR-CHERNOMYRDIN into an English NPB tree fragment yielding an English phrase. Also suppose that we want to translate a Chinese phrase VIKTOR-CHERNOMYRDIN AND HIS COLLEAGUE into English. What we desire is that if we have another rule R2 as shown in Figure 1, we could somehow compose it with R1 to obtain the desirable translation. We unfortunately cannot do this because R1 and R2 are not further decomposable and their substructures cannot be re-used. The requirement that all translation rules have exactly one root node"
D07-1078,W06-1606,1,0.615337,"nguageweaver.com Abstract We show that phrase structures in Penn Treebank style parses are not optimal for syntaxbased machine translation. We exploit a series of binarization methods to restructure the Penn Treebank style trees such that syntactified phrases smaller than Penn Treebank constituents can be acquired and exploited in translation. We find that by employing the EM algorithm for determining the binarization of a parse tree among a set of alternative binarizations gives us the best translation result. 1 Introduction Syntax-based translation models (Eisner, 2003; Galley et al., 2006; Marcu et al., 2006) are usually built directly from Penn Treebank (PTB) (Marcus et al., 1993) style parse trees by composing treebank grammar rules. As a result, often no substructures corresponding to partial PTB constituents are extracted to form translation rules. Syntax translation models acquired by composing treebank grammar rules assume that long rewrites are not decomposable into smaller steps. This effectively restricts the generalization power of the induced model. For example, suppose we have an xRs (Knight and Graehl, 2004) rule R 1 in Figure 1 that translates the Chinese phrase RUSSIA MINISTER VIKTO"
D07-1078,J93-2004,0,0.0333178,"style parses are not optimal for syntaxbased machine translation. We exploit a series of binarization methods to restructure the Penn Treebank style trees such that syntactified phrases smaller than Penn Treebank constituents can be acquired and exploited in translation. We find that by employing the EM algorithm for determining the binarization of a parse tree among a set of alternative binarizations gives us the best translation result. 1 Introduction Syntax-based translation models (Eisner, 2003; Galley et al., 2006; Marcu et al., 2006) are usually built directly from Penn Treebank (PTB) (Marcus et al., 1993) style parse trees by composing treebank grammar rules. As a result, often no substructures corresponding to partial PTB constituents are extracted to form translation rules. Syntax translation models acquired by composing treebank grammar rules assume that long rewrites are not decomposable into smaller steps. This effectively restricts the generalization power of the induced model. For example, suppose we have an xRs (Knight and Graehl, 2004) rule R 1 in Figure 1 that translates the Chinese phrase RUSSIA MINISTER VIKTOR-CHERNOMYRDIN into an English NPB tree fragment yielding an English phras"
D07-1078,P04-1084,0,0.0211306,"the binarization bias for each tree node from the parallel alternatives. The EM-binarization yields best translation performance. The rest of the paper is organized as follows. Section 2 describes related research. Section 3 defines the concepts necessary for describing the binarizations methods. Section 4 describes the tree binarization methods in details. Section 5 describes the forest-based rule extraction algorithm, and section 6 explains how we restructure the trees using the EM algorithm. The last two sections are for experiments and conclusions. 2 Related Research Several researchers (Melamed et al., 2004; Zhang et al., 2006) have already proposed methods for binarizing synchronous grammars in the context of machine translation. Grammar binarization usually maintains an equivalence to the original grammar such that binarized grammars generate the same lan747 guage and assign the same probability to each string as the original grammar does. Grammar binarization is often employed to make the grammar fit in a CKY parser. In our work, we are focused on binarization of parse trees. Tree binarization generalizes the resulting grammar and changes its probability distribution. In tree binarization, sy"
D07-1078,W05-0908,0,0.0127138,"nous derivation forests and to generate the Viterbi derivation forest. We used the rule extractor described in (Galley et al., 2006) to extract rules from (e-parse, f, a)-tuples, but we made an important modification: new nodes 752 Table 1 shows the BLEU scores of mixed-cased and detokenized translations of different systems. We see that all the binarization methods improve the baseline system that does not apply any binarization algorithm. The EM-binarization performs the best among all the restructuring methods, leading to 1.0 BLEU point improvement. We also computed the bootstrap p-values (Riezler and Maxwell, 2005) for the pairwise BLEU comparison between the baseline system and any of the system trained from binarized trees. The significance test shows that the EM binarization result is statistically significant better than the baseline system (p &gt; 0.005), even though the baseline is already quite strong. To our best knowledge, 37.94 is the highest BLEU score on this test set to date. Also as shown in Table 1, the grammars trained from the binarized training trees are almost two times of the grammar size with no binarization. The extra rules are substructures factored out by these binarization methods."
D07-1078,N06-1033,1,0.539983,"for each tree node from the parallel alternatives. The EM-binarization yields best translation performance. The rest of the paper is organized as follows. Section 2 describes related research. Section 3 defines the concepts necessary for describing the binarizations methods. Section 4 describes the tree binarization methods in details. Section 5 describes the forest-based rule extraction algorithm, and section 6 explains how we restructure the trees using the EM algorithm. The last two sections are for experiments and conclusions. 2 Related Research Several researchers (Melamed et al., 2004; Zhang et al., 2006) have already proposed methods for binarizing synchronous grammars in the context of machine translation. Grammar binarization usually maintains an equivalence to the original grammar such that binarized grammars generate the same lan747 guage and assign the same probability to each string as the original grammar does. Grammar binarization is often employed to make the grammar fit in a CKY parser. In our work, we are focused on binarization of parse trees. Tree binarization generalizes the resulting grammar and changes its probability distribution. In tree binarization, synchronous grammars bu"
D07-1078,J02-1005,0,\N,Missing
D07-1078,J08-3004,1,\N,Missing
D07-1079,J93-2003,0,0.0123288,"ased machine translation model on several levels. We briefly describe each model, highlighting points where they differ. We include a quantitative comparison of the phrase pairs that each model has to work with, as well as the reasons why some phrase pairs are not learned by the syntax-based model. We then evaluate proposed improvements to the syntax-based extraction techniques in light of phrase pairs captured. We also compare the translation accuracy for all variations. 1 Introduction String models are popular in statistical machine translation. Approaches include word substitution systems (Brown et al., 1993), phrase substitution systems (Koehn et al., 2003; Och and Ney, 2004), and synchronous context-free grammar systems (Wu and Wong, 1998; Chiang, 2005), all of which train on string pairs and seek to establish connections between source and target strings. By contrast, explicit syntax approaches seek to directly model the relations learned from parsed data, including models between source trees and target trees (Gildea, 2003; Eisner, 2003; Melamed, 2004; Cowan et al., 2006), source trees and target strings (Quirk et al., 2005; Huang et al., 2006), or source strings and target trees (Yamada and K"
D07-1079,P05-1033,0,0.253777,"on of the phrase pairs that each model has to work with, as well as the reasons why some phrase pairs are not learned by the syntax-based model. We then evaluate proposed improvements to the syntax-based extraction techniques in light of phrase pairs captured. We also compare the translation accuracy for all variations. 1 Introduction String models are popular in statistical machine translation. Approaches include word substitution systems (Brown et al., 1993), phrase substitution systems (Koehn et al., 2003; Och and Ney, 2004), and synchronous context-free grammar systems (Wu and Wong, 1998; Chiang, 2005), all of which train on string pairs and seek to establish connections between source and target strings. By contrast, explicit syntax approaches seek to directly model the relations learned from parsed data, including models between source trees and target trees (Gildea, 2003; Eisner, 2003; Melamed, 2004; Cowan et al., 2006), source trees and target strings (Quirk et al., 2005; Huang et al., 2006), or source strings and target trees (Yamada and Knight, 2001; Galley et al., 2004). It is unclear which of these important pursuits will best explain human translation data, as each has adA great nu"
D07-1079,J03-4003,0,0.00486268,"t linguistic knowledge from parallel corpora, but each has fundamentally different constraints and assumptions. To compare the models empirically, we extracted phrase pairs (for the ATS model) and translation rules (for the GHKM model) from parallel training corpora described in Table 1. The ATS model was limited to phrases of length 10 on the source side, and length 20 on the target side. A superset of the parallel data was word aligned by GIZA union (Och and Ney, 2003) and EMD (Fraser and Marcu, 2006). The English side of training data was parsed using an implementation of Collins’ model 2 (Collins, 2003). Document IDs # of segments # of words in foreign corpus # of words in English corpus Chinese LDC2003E07 LDC2003E14 LDC2005T06 329,031 7,520,779 9,864,294 Arabic LDC2004T17 LDC2004T18 LDC2005E46 140,511 3,147,420 4,067,454 Table 1: parallel corpora used to train both models Table 2 shows the total number of GHKM rules extracted, and a breakdown of the different kinds of rules. Non-lexical rules are those whose source side is composed entirely of variables — there are no source words in them. Because of this, they potentially apply to any sentence. Lexical rules (their counterpart) far outnumb"
D07-1079,W06-1628,0,0.194461,"Missing"
D07-1079,P03-2041,0,0.113635,"accuracy for all variations. 1 Introduction String models are popular in statistical machine translation. Approaches include word substitution systems (Brown et al., 1993), phrase substitution systems (Koehn et al., 2003; Och and Ney, 2004), and synchronous context-free grammar systems (Wu and Wong, 1998; Chiang, 2005), all of which train on string pairs and seek to establish connections between source and target strings. By contrast, explicit syntax approaches seek to directly model the relations learned from parsed data, including models between source trees and target trees (Gildea, 2003; Eisner, 2003; Melamed, 2004; Cowan et al., 2006), source trees and target strings (Quirk et al., 2005; Huang et al., 2006), or source strings and target trees (Yamada and Knight, 2001; Galley et al., 2004). It is unclear which of these important pursuits will best explain human translation data, as each has adA great number of MT models have been recently proposed, and other papers have gone over the expressive advantages of syntax-based approaches. But it is rare to see an in-depth, quantitative study of strengths and weaknesses of particular models with respect to each other. This is important for a sci"
D07-1079,P06-1097,1,0.521845,"s |fwords ), and p(fwords |ewords ). 4 Differences in Phrasal Coverage Both the ATS model and the GHKM model extract linguistic knowledge from parallel corpora, but each has fundamentally different constraints and assumptions. To compare the models empirically, we extracted phrase pairs (for the ATS model) and translation rules (for the GHKM model) from parallel training corpora described in Table 1. The ATS model was limited to phrases of length 10 on the source side, and length 20 on the target side. A superset of the parallel data was word aligned by GIZA union (Och and Ney, 2003) and EMD (Fraser and Marcu, 2006). The English side of training data was parsed using an implementation of Collins’ model 2 (Collins, 2003). Document IDs # of segments # of words in foreign corpus # of words in English corpus Chinese LDC2003E07 LDC2003E14 LDC2005T06 329,031 7,520,779 9,864,294 Arabic LDC2004T17 LDC2004T18 LDC2005E46 140,511 3,147,420 4,067,454 Table 1: parallel corpora used to train both models Table 2 shows the total number of GHKM rules extracted, and a breakdown of the different kinds of rules. Non-lexical rules are those whose source side is composed entirely of variables — there are no source words in th"
D07-1079,N04-1035,1,0.716228,"titution systems (Koehn et al., 2003; Och and Ney, 2004), and synchronous context-free grammar systems (Wu and Wong, 1998; Chiang, 2005), all of which train on string pairs and seek to establish connections between source and target strings. By contrast, explicit syntax approaches seek to directly model the relations learned from parsed data, including models between source trees and target trees (Gildea, 2003; Eisner, 2003; Melamed, 2004; Cowan et al., 2006), source trees and target strings (Quirk et al., 2005; Huang et al., 2006), or source strings and target trees (Yamada and Knight, 2001; Galley et al., 2004). It is unclear which of these important pursuits will best explain human translation data, as each has adA great number of MT models have been recently proposed, and other papers have gone over the expressive advantages of syntax-based approaches. But it is rare to see an in-depth, quantitative study of strengths and weaknesses of particular models with respect to each other. This is important for a scientific understanding of how these models work in practice. Our main novel contribution is a comparison of phrase-based and syntax-based extraction methods and phrase pair coverage. We also add"
D07-1079,P06-1121,1,0.651314,"s/mt/ mt06eval official results.html 756 Phrase pairs are extracted over the entire training corpus. Due to differing alignments, some phrase pairs that cannot be learned from one example may be learned from another. These pairs are then counted, once for each time they are seen in a training example, and these counts are used as the basis for maximum likelihood probability features, such as p(f |e) and p(e|f ). 3 Syntax-based Extraction The GHKM syntax-based extraction method for learning statistical syntax-based translation rules, presented first in (Galley et al., 2004) and expanded on in (Galley et al., 2006), is similar to phrase-based extraction in that it extracts rules consistent with given word alignments. A primary difference is the use of syntax trees on the target side, rather than sequences of words. The basic unit of translation is the translation rule, consisting of a sequence of words and variables in the source language, a syntax tree in the target language having words or variables at the leaves, and again a vector of feature values which describe this pair’s likelihood. Translation rules can: • look like phrase pairs with syntax decoration: NPB(NNP(prime) NNP(minister) ↔ NNP(keizo)"
D07-1079,P03-1011,0,0.0154978,"he translation accuracy for all variations. 1 Introduction String models are popular in statistical machine translation. Approaches include word substitution systems (Brown et al., 1993), phrase substitution systems (Koehn et al., 2003; Och and Ney, 2004), and synchronous context-free grammar systems (Wu and Wong, 1998; Chiang, 2005), all of which train on string pairs and seek to establish connections between source and target strings. By contrast, explicit syntax approaches seek to directly model the relations learned from parsed data, including models between source trees and target trees (Gildea, 2003; Eisner, 2003; Melamed, 2004; Cowan et al., 2006), source trees and target strings (Quirk et al., 2005; Huang et al., 2006), or source strings and target trees (Yamada and Knight, 2001; Galley et al., 2004). It is unclear which of these important pursuits will best explain human translation data, as each has adA great number of MT models have been recently proposed, and other papers have gone over the expressive advantages of syntax-based approaches. But it is rare to see an in-depth, quantitative study of strengths and weaknesses of particular models with respect to each other. This is impor"
D07-1079,2006.amta-papers.8,1,0.137531,"on. Approaches include word substitution systems (Brown et al., 1993), phrase substitution systems (Koehn et al., 2003; Och and Ney, 2004), and synchronous context-free grammar systems (Wu and Wong, 1998; Chiang, 2005), all of which train on string pairs and seek to establish connections between source and target strings. By contrast, explicit syntax approaches seek to directly model the relations learned from parsed data, including models between source trees and target trees (Gildea, 2003; Eisner, 2003; Melamed, 2004; Cowan et al., 2006), source trees and target strings (Quirk et al., 2005; Huang et al., 2006), or source strings and target trees (Yamada and Knight, 2001; Galley et al., 2004). It is unclear which of these important pursuits will best explain human translation data, as each has adA great number of MT models have been recently proposed, and other papers have gone over the expressive advantages of syntax-based approaches. But it is rare to see an in-depth, quantitative study of strengths and weaknesses of particular models with respect to each other. This is important for a scientific understanding of how these models work in practice. Our main novel contribution is a comparison of phr"
D07-1079,N03-1017,1,0.0988731,"We briefly describe each model, highlighting points where they differ. We include a quantitative comparison of the phrase pairs that each model has to work with, as well as the reasons why some phrase pairs are not learned by the syntax-based model. We then evaluate proposed improvements to the syntax-based extraction techniques in light of phrase pairs captured. We also compare the translation accuracy for all variations. 1 Introduction String models are popular in statistical machine translation. Approaches include word substitution systems (Brown et al., 1993), phrase substitution systems (Koehn et al., 2003; Och and Ney, 2004), and synchronous context-free grammar systems (Wu and Wong, 1998; Chiang, 2005), all of which train on string pairs and seek to establish connections between source and target strings. By contrast, explicit syntax approaches seek to directly model the relations learned from parsed data, including models between source trees and target trees (Gildea, 2003; Eisner, 2003; Melamed, 2004; Cowan et al., 2006), source trees and target strings (Quirk et al., 2005; Huang et al., 2006), or source strings and target trees (Yamada and Knight, 2001; Galley et al., 2004). It is unclear"
D07-1079,W06-1606,1,0.648295,"them. Since this is a clear deficiency, we will focus on analyzing these phrase pairs (which we call ATS-useful) and the reasons they were not learned. Table 4 shows a breakdown, categorizing each of these missing ATS-useful phrase pairs and the reasons they were not able to be learned. The most common reason is straightforward: by extracting only the minimally-sized rules, GHKM is unable to learn many larger phrases that ATS learns. If GHKM can make a word-level analysis, it will do that, at the expense of a phrase-level analysis. Galley et al. (2006) propose one solution to this problem and Marcu et al. (2006) propose another, both of which we explore in Sections 5.1 and 5.2. context is too constrained. For example, ATS can easily learn the phrase ® ↔ prime minister and is then free to use it in many contexts. But GHKM learns 45 different rules, each that translate this phrase pair in a unique context. Figure 6 shows a sampling. Notice that though many variations are present, the decoder is unable to use any of these rules to produce certain noun phrases, such as “current Japanese Prime Minister Shinzo Abe”, because no rule has the proper number of English modifiers. NPB(NNP(prime) NNP(minister) x"
D07-1079,P04-1083,0,0.041989,"all variations. 1 Introduction String models are popular in statistical machine translation. Approaches include word substitution systems (Brown et al., 1993), phrase substitution systems (Koehn et al., 2003; Och and Ney, 2004), and synchronous context-free grammar systems (Wu and Wong, 1998; Chiang, 2005), all of which train on string pairs and seek to establish connections between source and target strings. By contrast, explicit syntax approaches seek to directly model the relations learned from parsed data, including models between source trees and target trees (Gildea, 2003; Eisner, 2003; Melamed, 2004; Cowan et al., 2006), source trees and target strings (Quirk et al., 2005; Huang et al., 2006), or source strings and target trees (Yamada and Knight, 2001; Galley et al., 2004). It is unclear which of these important pursuits will best explain human translation data, as each has adA great number of MT models have been recently proposed, and other papers have gone over the expressive advantages of syntax-based approaches. But it is rare to see an in-depth, quantitative study of strengths and weaknesses of particular models with respect to each other. This is important for a scientific underst"
D07-1079,J03-1002,0,0.00571114,"e , fwords |ehead ), p(ewords |fwords ), and p(fwords |ewords ). 4 Differences in Phrasal Coverage Both the ATS model and the GHKM model extract linguistic knowledge from parallel corpora, but each has fundamentally different constraints and assumptions. To compare the models empirically, we extracted phrase pairs (for the ATS model) and translation rules (for the GHKM model) from parallel training corpora described in Table 1. The ATS model was limited to phrases of length 10 on the source side, and length 20 on the target side. A superset of the parallel data was word aligned by GIZA union (Och and Ney, 2003) and EMD (Fraser and Marcu, 2006). The English side of training data was parsed using an implementation of Collins’ model 2 (Collins, 2003). Document IDs # of segments # of words in foreign corpus # of words in English corpus Chinese LDC2003E07 LDC2003E14 LDC2005T06 329,031 7,520,779 9,864,294 Arabic LDC2004T17 LDC2004T18 LDC2005E46 140,511 3,147,420 4,067,454 Table 1: parallel corpora used to train both models Table 2 shows the total number of GHKM rules extracted, and a breakdown of the different kinds of rules. Non-lexical rules are those whose source side is composed entirely of variables"
D07-1079,J04-4002,0,0.085683,"each model, highlighting points where they differ. We include a quantitative comparison of the phrase pairs that each model has to work with, as well as the reasons why some phrase pairs are not learned by the syntax-based model. We then evaluate proposed improvements to the syntax-based extraction techniques in light of phrase pairs captured. We also compare the translation accuracy for all variations. 1 Introduction String models are popular in statistical machine translation. Approaches include word substitution systems (Brown et al., 1993), phrase substitution systems (Koehn et al., 2003; Och and Ney, 2004), and synchronous context-free grammar systems (Wu and Wong, 1998; Chiang, 2005), all of which train on string pairs and seek to establish connections between source and target strings. By contrast, explicit syntax approaches seek to directly model the relations learned from parsed data, including models between source trees and target trees (Gildea, 2003; Eisner, 2003; Melamed, 2004; Cowan et al., 2006), source trees and target strings (Quirk et al., 2005; Huang et al., 2006), or source strings and target trees (Yamada and Knight, 2001; Galley et al., 2004). It is unclear which of these impor"
D07-1079,P03-1021,0,0.0125312,"rees, so that wide constituents are broken down into multiple levels of tree structure. The approach we take here is head-out binarization (Wang et al., 2007), where any constituent with more than two children is split into partial constituents. The children to the left of the head word Category of ATS-useful phrase pairs Too large Extra target words in GHKM rules Extra source words in GHKM rules Other (e.g. parse failures) Total missing useful phrase pairs Chinese 12 218 424 9 663 Arabic 9 27 792 7 835 with four references for measuring BLEU. Tuning was done using Maximum BLEU hill-climbing (Och, 2003). Features used for the ATS system were the standard set. For the syntax-based translation system, we used a similar set of features. Table 8: reasons that ATS-useful phrase pairs are still not extracted as phrasal rules, with composed and SPMT model 1 rules in place Development set Test set are binarized one direction, while the children to the right are binarized the other direction. The top node retains its original label (e.g. NPB), while the new partial constituents are labeled with a bar (e.g. NPB). Figure 7 shows an example. Figure 7: head-out binarization in the target language: S, NPB"
D07-1079,P05-1034,0,0.0730264,"al machine translation. Approaches include word substitution systems (Brown et al., 1993), phrase substitution systems (Koehn et al., 2003; Och and Ney, 2004), and synchronous context-free grammar systems (Wu and Wong, 1998; Chiang, 2005), all of which train on string pairs and seek to establish connections between source and target strings. By contrast, explicit syntax approaches seek to directly model the relations learned from parsed data, including models between source trees and target trees (Gildea, 2003; Eisner, 2003; Melamed, 2004; Cowan et al., 2006), source trees and target strings (Quirk et al., 2005; Huang et al., 2006), or source strings and target trees (Yamada and Knight, 2001; Galley et al., 2004). It is unclear which of these important pursuits will best explain human translation data, as each has adA great number of MT models have been recently proposed, and other papers have gone over the expressive advantages of syntax-based approaches. But it is rare to see an in-depth, quantitative study of strengths and weaknesses of particular models with respect to each other. This is important for a scientific understanding of how these models work in practice. Our main novel contribution i"
D07-1079,D07-1078,1,0.313261,"ot the smallest rule that can explain the phrase pair, but it is still valuable for its syntactic context. 5.3 Restructuring Trees Table 8 updates the causes of missing ATS-useful phrase pairs. Most are now caused by syntactic constraints, thus we need to address these in some way. GHKM translation rules are affected by large, flat constituents in syntax trees, as in the prime minister example earlier. One way to soften this constraint is to binarize the trees, so that wide constituents are broken down into multiple levels of tree structure. The approach we take here is head-out binarization (Wang et al., 2007), where any constituent with more than two children is split into partial constituents. The children to the left of the head word Category of ATS-useful phrase pairs Too large Extra target words in GHKM rules Extra source words in GHKM rules Other (e.g. parse failures) Total missing useful phrase pairs Chinese 12 218 424 9 663 Arabic 9 27 792 7 835 with four references for measuring BLEU. Tuning was done using Maximum BLEU hill-climbing (Och, 2003). Features used for the ATS system were the standard set. For the syntax-based translation system, we used a similar set of features. Table 8: reaso"
D07-1079,P98-2230,0,0.0514584,"antitative comparison of the phrase pairs that each model has to work with, as well as the reasons why some phrase pairs are not learned by the syntax-based model. We then evaluate proposed improvements to the syntax-based extraction techniques in light of phrase pairs captured. We also compare the translation accuracy for all variations. 1 Introduction String models are popular in statistical machine translation. Approaches include word substitution systems (Brown et al., 1993), phrase substitution systems (Koehn et al., 2003; Och and Ney, 2004), and synchronous context-free grammar systems (Wu and Wong, 1998; Chiang, 2005), all of which train on string pairs and seek to establish connections between source and target strings. By contrast, explicit syntax approaches seek to directly model the relations learned from parsed data, including models between source trees and target trees (Gildea, 2003; Eisner, 2003; Melamed, 2004; Cowan et al., 2006), source trees and target strings (Quirk et al., 2005; Huang et al., 2006), or source strings and target trees (Yamada and Knight, 2001; Galley et al., 2004). It is unclear which of these important pursuits will best explain human translation data, as each h"
D07-1079,P01-1067,1,0.221445,"t al., 1993), phrase substitution systems (Koehn et al., 2003; Och and Ney, 2004), and synchronous context-free grammar systems (Wu and Wong, 1998; Chiang, 2005), all of which train on string pairs and seek to establish connections between source and target strings. By contrast, explicit syntax approaches seek to directly model the relations learned from parsed data, including models between source trees and target trees (Gildea, 2003; Eisner, 2003; Melamed, 2004; Cowan et al., 2006), source trees and target strings (Quirk et al., 2005; Huang et al., 2006), or source strings and target trees (Yamada and Knight, 2001; Galley et al., 2004). It is unclear which of these important pursuits will best explain human translation data, as each has adA great number of MT models have been recently proposed, and other papers have gone over the expressive advantages of syntax-based approaches. But it is rare to see an in-depth, quantitative study of strengths and weaknesses of particular models with respect to each other. This is important for a scientific understanding of how these models work in practice. Our main novel contribution is a comparison of phrase-based and syntax-based extraction methods and phrase pair"
D07-1079,N06-1033,1,0.126112,"-based extraction in that it extracts rules consistent with given word alignments. A primary difference is the use of syntax trees on the target side, rather than sequences of words. The basic unit of translation is the translation rule, consisting of a sequence of words and variables in the source language, a syntax tree in the target language having words or variables at the leaves, and again a vector of feature values which describe this pair’s likelihood. Translation rules can: • look like phrase pairs with syntax decoration: NPB(NNP(prime) NNP(minister) ↔ NNP(keizo) BÁÈ® D# NNP(obuchi)) (Zhang et al., 2006). During decoding, features from each translation rule are combined with a language model using a log-linear model to compute the score of the entire translation. The GHKM extractor learns translation rules from an aligned parallel corpus where the target side has been parsed. This corpus is conceptually a list of tuples of <source sentence, target tree, bi-directional word alignments&gt; which serve as training examples, one of which is shown in Figure 3. • carry extra contextual constraints: VP(VBD(said) x0 :SBAR-C) ↔ x 0  can translate to (according to this rule, said only if some Chinese se"
D07-1079,C98-2225,0,\N,Missing
D08-1085,D07-1090,0,0.0388751,"a bit too rosy, according to our empirical message equivocation curves. Our experience confirms this as well, as 1-gram frequency counts over a 173-letter cipher are generally insufficient to pin down a solution. 6 Conclusion We provide a method for deciphering letter substitution ciphers with low-order models of English. This method, based on integer programming, requires very little coding and can perform an optimal search over the key space. We conclude by noting that English language models currently used in speech recognition (Chelba and Jelinek, 1999) and automated language translation (Brants et al., 2007) are much more powerful, employing, for example, 7-gram word models (not letter models) trained on trillions of words. Obtaining optimal keys according to such models will permit the automatic decipherment of shorter ciphers, but this requires more specialized search than what is provided by general integer programming solvers. Methods such as these should also be useful for natural language decipherment problems such as character code conversion, phonetic decipherment, and word substitution ciphers with applications in machine translation (Knight et al., 2006). 819 7 Acknowledgements The auth"
D08-1085,H94-1005,0,0.00815647,"nguage model, whose job is to assign some probability to any sequence of letters. According to a 1gram model of English, the probability of a plaintext p1 ...pn is given by: P (p1 ...pn ) = P (p1 ) · P (p2 ) · ... · P (pn ) That is, we obtain the probability of a sequence by multiplying together the probabilities of the individual letters that make it up. This model assigns a probability to any letter sequence, and the probabilities of all letter sequences sum to one. We collect letter probabilities (including space) from 50 million words of text available from the Linguistic Data Consortium (Graff and Finch, 1994). We also estimate 2- and 3-gram models using the same resources: P (p1 ...pn ) = each letter in the phrase, estimates the same probability for both the phrases “het oxf” and “the fox”, since the same letters occur in both phrases. On the other hand, the 2-gram and 3-gram models, which take context into account, are able to distinguish between the English and non-English phrases better, and hence assign a higher probability to the English phrase “the fox”. Model 1-gram 2-gram 3-gram = 1-gram: 4.19 2-gram: 3.51 3-gram: 2.93 P (p1 |ST ART ) · P (p2 |p1 ) · P (p3 |p2 ) · P (p1 |ST ART ) · P (p2 |"
D08-1085,P06-2065,1,0.397315,"erment accuracy varies as a function of cipher length and n-gram order. We also make an empirical investigation of Shannon’s (1949) theory of uncertainty in decipherment. 1 Introduction A number of papers have explored algorithms for automatically solving letter-substitution ciphers. Some use heuristic methods to search for the best deterministic key (Peleg and Rosenfeld, 1979; Ganesan and Sherman, 1993; Jakobsen, 1995; Olson, 2007), often using word dictionaries to guide that search. Others use expectation-maximization (EM) to search for the best probabilistic key using letter n-gram models (Knight et al., 2006). In this paper, we introduce an exact decipherment method based on integer programming. We carry out extensive decipherment experiments using letter n-gram models, and we find that our accuracy rates far exceed those of EM-based methods. We also empirically explore the concepts in Shannon’s (1949) paper on information theory as applied to cipher systems. We provide quantitative plots for uncertainty in decipherment, including the famous unicity distance, which estimates how long a cipher must be to virtually eliminate such uncertainty. • We outline an exact letter-substitution decipherment me"
D08-1093,P07-1038,0,0.020617,"slations can be produced and measured. Although a real estimate of the impact of a parser design decision in this scenario can only be gauged from the quality of the translations produced, it is impractical to create such estimates for each design decision. On the other hand, estimates using the solution proposed in this paper can be obtained fast, before submitting the parser output to a costly training procedure. 2 Related Work and Experimental Framework There have been previous studies which explored the problem of automatically predicting the task difficulty for various NLP applications. (Albrecht and Hwa, 2007) presented a regression based method for developing automatic evaluation metrics for machine translation systems without directly relying on human reference translations. (Hoshino and Nakagawa, 2007) built a computer-adaptive system for generating questions to teach English grammar and vocabulary to students, by predicting the difficulty level of a question using various features. There have been a few studies of English parser accuracy in domains/genres other than WSJ (Gildea, 2001; Bacchiani et al., 2006; McClosky et al., 2006), but in order to make measurements for such studies, it is neces"
D08-1093,P08-1087,0,0.0121278,"notation involved) and effective recipe for measuring the performance of a statistical parser on any given domain. 1 Introduction Statistical natural language parsers have recently become more accurate and more widely available. As a result, they are being used in a variety of applications, such as question answering (Hermjakob, 2001), speech recognition (Chelba and Jelinek, 1998), language modeling (Roark, 2001), language generation (Soricut, 2006) and, most notably, machine translation (Charniak et al., 2003; Galley et al., 2004; Collins et al., 2005; Marcu et al., 2006; Huang et al., 2006; Avramidis and Koehn, 2008). These applications are employed on a wide range of domains and genres, and therefore the question of how accurate a parser is on the domain and genre of interest becomes acute. Ideally, one would want to have available a recipe for precisely answering this question: “given a parser and a particular domain of interest, how accurate are the parse trees produced?” The only recipe that is implicitly given in the large literature on parsing to date is to have human annotators build parse trees for a sample set from the domain of interest, and consequently use them to compute a PARSEVAL (Black et"
D08-1093,H91-1060,0,0.0220532,"hn, 2008). These applications are employed on a wide range of domains and genres, and therefore the question of how accurate a parser is on the domain and genre of interest becomes acute. Ideally, one would want to have available a recipe for precisely answering this question: “given a parser and a particular domain of interest, how accurate are the parse trees produced?” The only recipe that is implicitly given in the large literature on parsing to date is to have human annotators build parse trees for a sample set from the domain of interest, and consequently use them to compute a PARSEVAL (Black et al., 1991) score that is indicative of the intrinsic performance of the parser. Given the wide range of domains and genres for which NLP applications are of interest, combined with the high expertise required from human annotators to produce parse tree annotations, this recipe is, albeit precise, too expensive. The other recipe that is currently used on a large scale is to measure the performance of a parser on existing treebanks, such as WSJ (Marcus et al., 1993), and assume that the accuracy measure will carry over to the domains of interest. This recipe, albeit cheap, cannot provide any guarantee reg"
D08-1093,P05-1022,0,0.765075,"mains/genres other than WSJ (Gildea, 2001; Bacchiani et al., 2006; McClosky et al., 2006), but in order to make measurements for such studies, it is necessary to have gold-standard parses in the non888 WSJ domain of interest. Gildea (2001) studied how well WSJ-trained parsers do on the Brown corpus, for which a gold standard exists. He looked at sentences with 40 words or less. (Bacchiani et al., 2006) carried out a similar experiment on sentences of all lengths, and (McClosky et al., 2006) report additional results. The table below shows results from our own measurements of Charniak parser1 (Charniak and Johnson, 2005) accuracy (F-measure on sentences of all lengths), which are consistent with these studies. For the Brown corpus, the test set was formed from every tenth sentence in the corpus (Gildea, 2001). Training Set Test Set WSJ sec. 02-21 (39,832 sent.) WSJ sec. 24 WSJ sec. 23 Brown-test Sent. count 1308 2343 2186 Charniak accuracy 90.48 91.13 86.34 Here we investigate algorithms for predicting the accuracy of a parser P on sentences, chunks of sentences, and whole corpora. We also investigate and contrast several scenarios for prediction: (1) the predictor looks at the input text only, (2) the predic"
D08-1093,2003.mtsummit-papers.6,1,0.676395,"accurately predicts parser performance on data from these new domains. As a result, we have a cheap (no annotation involved) and effective recipe for measuring the performance of a statistical parser on any given domain. 1 Introduction Statistical natural language parsers have recently become more accurate and more widely available. As a result, they are being used in a variety of applications, such as question answering (Hermjakob, 2001), speech recognition (Chelba and Jelinek, 1998), language modeling (Roark, 2001), language generation (Soricut, 2006) and, most notably, machine translation (Charniak et al., 2003; Galley et al., 2004; Collins et al., 2005; Marcu et al., 2006; Huang et al., 2006; Avramidis and Koehn, 2008). These applications are employed on a wide range of domains and genres, and therefore the question of how accurate a parser is on the domain and genre of interest becomes acute. Ideally, one would want to have available a recipe for precisely answering this question: “given a parser and a particular domain of interest, how accurate are the parse trees produced?” The only recipe that is implicitly given in the large literature on parsing to date is to have human annotators build parse"
D08-1093,P98-1035,0,0.00969915,"es. In this paper, we propose a technique that automatically takes into account certain characteristics of the domains of interest, and accurately predicts parser performance on data from these new domains. As a result, we have a cheap (no annotation involved) and effective recipe for measuring the performance of a statistical parser on any given domain. 1 Introduction Statistical natural language parsers have recently become more accurate and more widely available. As a result, they are being used in a variety of applications, such as question answering (Hermjakob, 2001), speech recognition (Chelba and Jelinek, 1998), language modeling (Roark, 2001), language generation (Soricut, 2006) and, most notably, machine translation (Charniak et al., 2003; Galley et al., 2004; Collins et al., 2005; Marcu et al., 2006; Huang et al., 2006; Avramidis and Koehn, 2008). These applications are employed on a wide range of domains and genres, and therefore the question of how accurate a parser is on the domain and genre of interest becomes acute. Ideally, one would want to have available a recipe for precisely answering this question: “given a parser and a particular domain of interest, how accurate are the parse trees pr"
D08-1093,P05-1066,0,0.0104477,"ta from these new domains. As a result, we have a cheap (no annotation involved) and effective recipe for measuring the performance of a statistical parser on any given domain. 1 Introduction Statistical natural language parsers have recently become more accurate and more widely available. As a result, they are being used in a variety of applications, such as question answering (Hermjakob, 2001), speech recognition (Chelba and Jelinek, 1998), language modeling (Roark, 2001), language generation (Soricut, 2006) and, most notably, machine translation (Charniak et al., 2003; Galley et al., 2004; Collins et al., 2005; Marcu et al., 2006; Huang et al., 2006; Avramidis and Koehn, 2008). These applications are employed on a wide range of domains and genres, and therefore the question of how accurate a parser is on the domain and genre of interest becomes acute. Ideally, one would want to have available a recipe for precisely answering this question: “given a parser and a particular domain of interest, how accurate are the parse trees produced?” The only recipe that is implicitly given in the large literature on parsing to date is to have human annotators build parse trees for a sample set from the domain of"
D08-1093,J03-4003,0,0.0828782,"rees for the Brown test set helps us decide which prediction is better. Our predictions are much closer to the actual Charniak parser performance on the Brown-test set, with the adjusted prediction at 86.96 compared to the actual F-score of 86.34. 6 Ranking Parser Performance One of the main goals for computing F-score figures (either by traditional PARSEVAL evaluation against gold standards or by methods such as the one proposed in this paper) is to compare parsing accuracy when confronted with a choice between various parser deployments. Not only are there many parsing techniques available (Collins, 2003; Charniak and Johnson, 2005; Petrov and Klein, 2007; McClosky et al., 2006; Huang, 2008), but recent annotation efforts in providing training material for statistical parsing (LDC, 2005; LDC, 2006a; LDC, 2006b; LDC, 2006c; LDC, 2007) have compounded the difficulty of the choices (“Do I parse using parser X?”, “Do I train parser X using the treebank Y or Z?”). In this section, we show how our predictor can provide guidance when dealing with some of these choices, namely the choice of the training material to use with a statistical parser, prior to its application in an NLP task. For the experi"
D08-1093,N04-1035,1,0.557497,"ser performance on data from these new domains. As a result, we have a cheap (no annotation involved) and effective recipe for measuring the performance of a statistical parser on any given domain. 1 Introduction Statistical natural language parsers have recently become more accurate and more widely available. As a result, they are being used in a variety of applications, such as question answering (Hermjakob, 2001), speech recognition (Chelba and Jelinek, 1998), language modeling (Roark, 2001), language generation (Soricut, 2006) and, most notably, machine translation (Charniak et al., 2003; Galley et al., 2004; Collins et al., 2005; Marcu et al., 2006; Huang et al., 2006; Avramidis and Koehn, 2008). These applications are employed on a wide range of domains and genres, and therefore the question of how accurate a parser is on the domain and genre of interest becomes acute. Ideally, one would want to have available a recipe for precisely answering this question: “given a parser and a particular domain of interest, how accurate are the parse trees produced?” The only recipe that is implicitly given in the large literature on parsing to date is to have human annotators build parse trees for a sample s"
D08-1093,W01-0521,0,0.112482,"h explored the problem of automatically predicting the task difficulty for various NLP applications. (Albrecht and Hwa, 2007) presented a regression based method for developing automatic evaluation metrics for machine translation systems without directly relying on human reference translations. (Hoshino and Nakagawa, 2007) built a computer-adaptive system for generating questions to teach English grammar and vocabulary to students, by predicting the difficulty level of a question using various features. There have been a few studies of English parser accuracy in domains/genres other than WSJ (Gildea, 2001; Bacchiani et al., 2006; McClosky et al., 2006), but in order to make measurements for such studies, it is necessary to have gold-standard parses in the non888 WSJ domain of interest. Gildea (2001) studied how well WSJ-trained parsers do on the Brown corpus, for which a gold standard exists. He looked at sentences with 40 words or less. (Bacchiani et al., 2006) carried out a similar experiment on sentences of all lengths, and (McClosky et al., 2006) report additional results. The table below shows results from our own measurements of Charniak parser1 (Charniak and Johnson, 2005) accuracy (F-m"
D08-1093,W97-0302,0,0.049047,"tical parsing (LDC, 2005; LDC, 2006a; LDC, 2006b; LDC, 2006c; LDC, 2007) have compounded the difficulty of the choices (“Do I parse using parser X?”, “Do I train parser X using the treebank Y or Z?”). In this section, we show how our predictor can provide guidance when dealing with some of these choices, namely the choice of the training material to use with a statistical parser, prior to its application in an NLP task. For the experiments reported in this paper, we use as parser P, our in-house implementation of the Collins parser (Collins, 2003), to which various speed-related enhancements (Goodman, 1997) have been applied. This choice has been made to better reflect a scenario in which parser P would be used in a data-intensive application such as syntax-driven machine translation, in which the parser must be able to run through hundreds of millions of training words in a timely manner. We use the more accurate, but slower Charniak parser (Charniak and Johnson, 2005) as the reference parser Pref in our predictor (see Section 3.3). In order to predict the Collinsstyle parser behavior on the ranking task, we use the same predictor model (including feature weights and adjustment parameters) that"
D08-1093,W01-1203,0,0.0212184,"the absence of gold-standard parse trees. In this paper, we propose a technique that automatically takes into account certain characteristics of the domains of interest, and accurately predicts parser performance on data from these new domains. As a result, we have a cheap (no annotation involved) and effective recipe for measuring the performance of a statistical parser on any given domain. 1 Introduction Statistical natural language parsers have recently become more accurate and more widely available. As a result, they are being used in a variety of applications, such as question answering (Hermjakob, 2001), speech recognition (Chelba and Jelinek, 1998), language modeling (Roark, 2001), language generation (Soricut, 2006) and, most notably, machine translation (Charniak et al., 2003; Galley et al., 2004; Collins et al., 2005; Marcu et al., 2006; Huang et al., 2006; Avramidis and Koehn, 2008). These applications are employed on a wide range of domains and genres, and therefore the question of how accurate a parser is on the domain and genre of interest becomes acute. Ideally, one would want to have available a recipe for precisely answering this question: “given a parser and a particular domain o"
D08-1093,2006.amta-papers.8,1,0.772156,"have a cheap (no annotation involved) and effective recipe for measuring the performance of a statistical parser on any given domain. 1 Introduction Statistical natural language parsers have recently become more accurate and more widely available. As a result, they are being used in a variety of applications, such as question answering (Hermjakob, 2001), speech recognition (Chelba and Jelinek, 1998), language modeling (Roark, 2001), language generation (Soricut, 2006) and, most notably, machine translation (Charniak et al., 2003; Galley et al., 2004; Collins et al., 2005; Marcu et al., 2006; Huang et al., 2006; Avramidis and Koehn, 2008). These applications are employed on a wide range of domains and genres, and therefore the question of how accurate a parser is on the domain and genre of interest becomes acute. Ideally, one would want to have available a recipe for precisely answering this question: “given a parser and a particular domain of interest, how accurate are the parse trees produced?” The only recipe that is implicitly given in the large literature on parsing to date is to have human annotators build parse trees for a sample set from the domain of interest, and consequently use them to c"
D08-1093,P08-1067,0,0.0354199,"r prediction. For the experiments presented in this section we use as Pref , the parser from (Bikel, 2002). Intuitively, the requirement for choosing parser Pref in conjunction with parser P seems to be that they are different enough to produce non-identical trees when presented with the same input, and at the same time to be accurate enough to produce reliable parse trees. The choice of P as (Charniak and Johnson, 2005) and Pref as (Bikel, 2002) fits this bill, but many other choices can be made regarding Pref , such as (Klein and Manning, 2003; Petrov and Klein, 2007; McClosky et al., 2006; Huang, 2008). We leave the task of creating features based on the consensus of multiple parsers as future work. The correlation given by the reference parser– based feature Pref on the test set is the highest among all the features we explored. Feature set Pref 3.4 dev (r) 0.40 test (r) 0.36 The Aggregated Power of Features The table below lists all the individual features we have described in this section, sorted according to the correlation value obtained on the test set. Feature set Pref labelSYN lexCount500 lexBool500 lexCount1000 lexBool1000 Length lexCount100 lexBool100 rootSYN UNK LM-PPL puncSYN de"
D08-1093,P03-1054,0,0.011043,"e of parser P against Pref . We use this F-measure score as a feature for prediction. For the experiments presented in this section we use as Pref , the parser from (Bikel, 2002). Intuitively, the requirement for choosing parser Pref in conjunction with parser P seems to be that they are different enough to produce non-identical trees when presented with the same input, and at the same time to be accurate enough to produce reliable parse trees. The choice of P as (Charniak and Johnson, 2005) and Pref as (Bikel, 2002) fits this bill, but many other choices can be made regarding Pref , such as (Klein and Manning, 2003; Petrov and Klein, 2007; McClosky et al., 2006; Huang, 2008). We leave the task of creating features based on the consensus of multiple parsers as future work. The correlation given by the reference parser– based feature Pref on the test set is the highest among all the features we explored. Feature set Pref 3.4 dev (r) 0.40 test (r) 0.36 The Aggregated Power of Features The table below lists all the individual features we have described in this section, sorted according to the correlation value obtained on the test set. Feature set Pref labelSYN lexCount500 lexBool500 lexCount1000 lexBool100"
D08-1093,N07-1006,0,0.0122313,"olean-based counterparts. Since these features measure different but overlapping pieces of the information available, it is to be expected that some of the feature combinations would provide better correlation that the individual features, but the gains are not strictly additive. By taking the individual features that provide the best discriminative power, we are able to get a correlation score of 0.42 on the test set. 3.5 dev (r) 0.55 test (r) 0.42 Optimizing for Maximum Correlation If our goal is to obtain the highest correlations with the F-score measure, is SVM regression the best method? Liu and Gildea (2007) recently introduced Maximum Correlation Training (MCT), a search procedure that follows the gradient of the formula for correlation coefficient (r). We implemented MCT, but obtained no better results. Moreover, it required many random re-starts just to obtain results comparable to SVM regression (Table 1). 4 WSJ-test (r) 0.42 0.61 0.62 0.69 0.79 WSJ-test (rms error) 0.098 0.026 0.019 0.015 0.011 Table 2: Performance of predictor on n-sentence chunks from WSJ-test (Correlation and rms error between actual/predicted accuracies). Table 1: Comparison of correlation (r) obtained using MCT versus S"
D08-1093,W06-1606,1,0.742809,"ins. As a result, we have a cheap (no annotation involved) and effective recipe for measuring the performance of a statistical parser on any given domain. 1 Introduction Statistical natural language parsers have recently become more accurate and more widely available. As a result, they are being used in a variety of applications, such as question answering (Hermjakob, 2001), speech recognition (Chelba and Jelinek, 1998), language modeling (Roark, 2001), language generation (Soricut, 2006) and, most notably, machine translation (Charniak et al., 2003; Galley et al., 2004; Collins et al., 2005; Marcu et al., 2006; Huang et al., 2006; Avramidis and Koehn, 2008). These applications are employed on a wide range of domains and genres, and therefore the question of how accurate a parser is on the domain and genre of interest becomes acute. Ideally, one would want to have available a recipe for precisely answering this question: “given a parser and a particular domain of interest, how accurate are the parse trees produced?” The only recipe that is implicitly given in the large literature on parsing to date is to have human annotators build parse trees for a sample set from the domain of interest, and conseq"
D08-1093,J93-2004,0,0.0309262,"e is to have human annotators build parse trees for a sample set from the domain of interest, and consequently use them to compute a PARSEVAL (Black et al., 1991) score that is indicative of the intrinsic performance of the parser. Given the wide range of domains and genres for which NLP applications are of interest, combined with the high expertise required from human annotators to produce parse tree annotations, this recipe is, albeit precise, too expensive. The other recipe that is currently used on a large scale is to measure the performance of a parser on existing treebanks, such as WSJ (Marcus et al., 1993), and assume that the accuracy measure will carry over to the domains of interest. This recipe, albeit cheap, cannot provide any guarantee regarding the performance of a parser on a new domain, and, as experiments in this paper show, can give wrong indications regarding important decisions for the design of NLP systems that use a syntactic parser as an important component. This paper proposes another method for measuring the performance of a parser on a given domain that is both cheap and effective. It is a fully automated procedure (no expensive annotation involved) that uses properties of bo"
D08-1093,P06-1043,0,0.604855,"y predicting the task difficulty for various NLP applications. (Albrecht and Hwa, 2007) presented a regression based method for developing automatic evaluation metrics for machine translation systems without directly relying on human reference translations. (Hoshino and Nakagawa, 2007) built a computer-adaptive system for generating questions to teach English grammar and vocabulary to students, by predicting the difficulty level of a question using various features. There have been a few studies of English parser accuracy in domains/genres other than WSJ (Gildea, 2001; Bacchiani et al., 2006; McClosky et al., 2006), but in order to make measurements for such studies, it is necessary to have gold-standard parses in the non888 WSJ domain of interest. Gildea (2001) studied how well WSJ-trained parsers do on the Brown corpus, for which a gold standard exists. He looked at sentences with 40 words or less. (Bacchiani et al., 2006) carried out a similar experiment on sentences of all lengths, and (McClosky et al., 2006) report additional results. The table below shows results from our own measurements of Charniak parser1 (Charniak and Johnson, 2005) accuracy (F-measure on sentences of all lengths), which are c"
D08-1093,P03-1021,0,0.00309409,"Missing"
D08-1093,N07-1051,0,0.0277637,"f . We use this F-measure score as a feature for prediction. For the experiments presented in this section we use as Pref , the parser from (Bikel, 2002). Intuitively, the requirement for choosing parser Pref in conjunction with parser P seems to be that they are different enough to produce non-identical trees when presented with the same input, and at the same time to be accurate enough to produce reliable parse trees. The choice of P as (Charniak and Johnson, 2005) and Pref as (Bikel, 2002) fits this bill, but many other choices can be made regarding Pref , such as (Klein and Manning, 2003; Petrov and Klein, 2007; McClosky et al., 2006; Huang, 2008). We leave the task of creating features based on the consensus of multiple parsers as future work. The correlation given by the reference parser– based feature Pref on the test set is the highest among all the features we explored. Feature set Pref 3.4 dev (r) 0.40 test (r) 0.36 The Aggregated Power of Features The table below lists all the individual features we have described in this section, sorted according to the correlation value obtained on the test set. Feature set Pref labelSYN lexCount500 lexBool500 lexCount1000 lexBool1000 Length lexCount100 lex"
D08-1093,J01-2004,0,0.0635174,"automatically takes into account certain characteristics of the domains of interest, and accurately predicts parser performance on data from these new domains. As a result, we have a cheap (no annotation involved) and effective recipe for measuring the performance of a statistical parser on any given domain. 1 Introduction Statistical natural language parsers have recently become more accurate and more widely available. As a result, they are being used in a variety of applications, such as question answering (Hermjakob, 2001), speech recognition (Chelba and Jelinek, 1998), language modeling (Roark, 2001), language generation (Soricut, 2006) and, most notably, machine translation (Charniak et al., 2003; Galley et al., 2004; Collins et al., 2005; Marcu et al., 2006; Huang et al., 2006; Avramidis and Koehn, 2008). These applications are employed on a wide range of domains and genres, and therefore the question of how accurate a parser is on the domain and genre of interest becomes acute. Ideally, one would want to have available a recipe for precisely answering this question: “given a parser and a particular domain of interest, how accurate are the parse trees produced?” The only recipe that is"
D08-1093,C98-1035,0,\N,Missing
D08-1093,P06-1121,1,\N,Missing
D09-1076,C90-3001,0,0.896715,"ned statistical models, from word-based models to phrase-based models, and from string-based models to tree-based models. Recently there is a swing back to incorporating more linguistic information again, but this time linguistic insight carefully guides the setup of empirically learned models. Shieber (2007) recently argued that probabilistic Synchronous Tree Adjoining Grammars (Shieber and Schabes, 1990) have the right combination of properties that satisfy both linguists and empirical MT practitioners. So far, though, most work in this area has been either more linguistic than statistical (Abeille et al., 1990) or statistically-based, but linguistically light (Nesson et al., 2006). AldfAE AlwTnY (d) NP ↔ NN3 NN2 JJ1 JJ1 NN2 NN3 NP ↔ NN2 NN1 NN1 NN2 NP ↔ NN2 JJ1 JJ1 NN2 NP ↔ NN1 NN1 Figure 1: Rule (a) can be learned from this training example. Arguably, the more general rules (b) (d) should also be learnable. To mitigate this problem, the parse trees used as training data for these systems can be binarized (Wang et al., 2007). Binarization allows rules with partial constituents to be learned, resulting in more general rules, richer statistics, and better phrasal coverage (DeNeefe et al., 2007), but n"
D09-1076,J95-4002,0,0.349475,"ubstitution and adjunction on tree pairs. To facilitate this synchronous behavior, links between pairs of nodes in each tree pair define the possible sites for substitution and adjunction to happen. One application of STAG is machine translation (Abeille et al., 1990). One negative aspect of TAG is the computational complexity: O(n6 ) time is required for monolingual parsing (and thus decoding), and STAG requires O(n12 ) for bilingual parsing (which might be used for training the model directly on bilingual data). Tree Insertion Grammars (TIG) are a restricted form of TAG that was introduced (Schabes and Waters, 1995) to keep the same benefits as TAG (adjoining of unbounded material) without the computational complexity— TIG parsing is O(n3 ). This reduction is due to a limitation on adjoining: auxiliary trees can only introduce tree material to the left or the right of the node adjoined to. Thus an auxiliary tree can be classified by direction as left or right adjoining. 728 adjunction NP DT NP the NN↓ substitution NP NP JJ↓ NP* DT =⇒ the substitution NN JJ minister defense NP JJ NP defense NN minister Figure 2: TAG grammars use substitution and adjunction operations to construct trees. Substitution repla"
D09-1076,J03-4003,0,0.0106335,"discussed previously (at the end of Section 3.2), we only allow subsets of adjoining combinations seen in training data, so this number is substantially lower for large values of k. 5 Experiments All experiments are trained with a subset (171,000 sentences or 4 million words) of the ArabicEnglish training data from the constrained data track of the NIST 2008 MT Evaluation, leaving out LDC2004T18, LDC2007E07, and the UN data. The training data is aligned using the LEAF technique (Fraser and Marcu, 2007). The English side of the training data is parsed with an implementation of Collins Model 2 (Collins, 2003) then head-out binarized. The tuning data (1,178 sentences) and devtest data (1,298 sentences) are Input: Synchronous TIG rule r with j adjoining sites, S ↔ T , where S and T are trees Output: a weakly equivalent xLNTs rule S! ↔ t1 . . . tn , where S ! is a one-level tree, and 2 · j helper rules for adjoining Run time: O(|S |+ |T |) begin rules ← {}, lhs-state ← concat(‘q’, get-root(S), get-root(T )) site-and-word-list-s ← get-sites-and-words-in-order(S) site-and-word-list-t ← get-sites-and-words-in-order(T ) if r is adjoining then lhs-state ← concat(lhs-state, get-adjoin-dir(S), get-adjoin-di"
D09-1076,C90-3045,0,0.558212,"MT task. NP JJ NN (a) NN national defense minister (b) (c) wzyr 1 Introduction Statistical MT has changed a lot in recent years. We have seen quick progress from manually crafted linguistic models to empirically learned statistical models, from word-based models to phrase-based models, and from string-based models to tree-based models. Recently there is a swing back to incorporating more linguistic information again, but this time linguistic insight carefully guides the setup of empirically learned models. Shieber (2007) recently argued that probabilistic Synchronous Tree Adjoining Grammars (Shieber and Schabes, 1990) have the right combination of properties that satisfy both linguists and empirical MT practitioners. So far, though, most work in this area has been either more linguistic than statistical (Abeille et al., 1990) or statistically-based, but linguistically light (Nesson et al., 2006). AldfAE AlwTnY (d) NP ↔ NN3 NN2 JJ1 JJ1 NN2 NN3 NP ↔ NN2 NN1 NN1 NN2 NP ↔ NN2 JJ1 JJ1 NN2 NP ↔ NN1 NN1 Figure 1: Rule (a) can be learned from this training example. Arguably, the more general rules (b) (d) should also be learnable. To mitigate this problem, the parse trees used as training data for these systems ca"
D09-1076,D07-1079,1,0.242062,"tical (Abeille et al., 1990) or statistically-based, but linguistically light (Nesson et al., 2006). AldfAE AlwTnY (d) NP ↔ NN3 NN2 JJ1 JJ1 NN2 NN3 NP ↔ NN2 NN1 NN1 NN2 NP ↔ NN2 JJ1 JJ1 NN2 NP ↔ NN1 NN1 Figure 1: Rule (a) can be learned from this training example. Arguably, the more general rules (b) (d) should also be learnable. To mitigate this problem, the parse trees used as training data for these systems can be binarized (Wang et al., 2007). Binarization allows rules with partial constituents to be learned, resulting in more general rules, richer statistics, and better phrasal coverage (DeNeefe et al., 2007), but no principled required vs. optional decision has been made. This method’s key weakness is that binarization always keeps adjacent siblings together, so there is no way to group the head with a required complement if optional information intervenes between the two. Furthermore, if all kinds of children are considered equally optional, then we have removed important syntactic constraints, which may end up permitting too much freedom. In addition, spurious alignments may limit the binarization tech727 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pa"
D09-1076,W07-0412,0,0.507702,"B LEU over a baseline statistical syntax-based MT model on a large-scale Arabic/English MT task. NP JJ NN (a) NN national defense minister (b) (c) wzyr 1 Introduction Statistical MT has changed a lot in recent years. We have seen quick progress from manually crafted linguistic models to empirically learned statistical models, from word-based models to phrase-based models, and from string-based models to tree-based models. Recently there is a swing back to incorporating more linguistic information again, but this time linguistic insight carefully guides the setup of empirically learned models. Shieber (2007) recently argued that probabilistic Synchronous Tree Adjoining Grammars (Shieber and Schabes, 1990) have the right combination of properties that satisfy both linguists and empirical MT practitioners. So far, though, most work in this area has been either more linguistic than statistical (Abeille et al., 1990) or statistically-based, but linguistically light (Nesson et al., 2006). AldfAE AlwTnY (d) NP ↔ NN3 NN2 JJ1 JJ1 NN2 NN3 NP ↔ NN2 NN1 NN1 NN2 NP ↔ NN2 JJ1 JJ1 NN2 NP ↔ NN1 NN1 Figure 1: Rule (a) can be learned from this training example. Arguably, the more general rules (b) (d) should also"
D09-1076,D07-1006,0,0.013606,"ion site, then remove epsilon-only branches. For k adjunction sites this could possibly results in 2k rules. But as discussed previously (at the end of Section 3.2), we only allow subsets of adjoining combinations seen in training data, so this number is substantially lower for large values of k. 5 Experiments All experiments are trained with a subset (171,000 sentences or 4 million words) of the ArabicEnglish training data from the constrained data track of the NIST 2008 MT Evaluation, leaving out LDC2004T18, LDC2007E07, and the UN data. The training data is aligned using the LEAF technique (Fraser and Marcu, 2007). The English side of the training data is parsed with an implementation of Collins Model 2 (Collins, 2003) then head-out binarized. The tuning data (1,178 sentences) and devtest data (1,298 sentences) are Input: Synchronous TIG rule r with j adjoining sites, S ↔ T , where S and T are trees Output: a weakly equivalent xLNTs rule S! ↔ t1 . . . tn , where S ! is a one-level tree, and 2 · j helper rules for adjoining Run time: O(|S |+ |T |) begin rules ← {}, lhs-state ← concat(‘q’, get-root(S), get-root(T )) site-and-word-list-s ← get-sites-and-words-in-order(S) site-and-word-list-t ← get-sites-a"
D09-1076,N04-1035,1,0.326303,") · si,rs Padj (ra |dir(ra ), &rootL (ra ), rootR (ra )') · ! # Pifadj (decisionadjoin |si,ra , ra ) si,ra Note that while every new substitution site requires an additional rule to be added, adjunction sites may or may not introduce an additional rule based on the rule-specific Pifadj probability. This allows adjunction to represent linguistic optionality. 2 Here and in the following, we use site as shorthand for synchronous site pair. 730 3 Learning the Model Instead of using bilingual parsing to directly train our model from strings as done by Nesson et al. (2006), we follow the method of Galley et al. (2004) by dividing the training process into steps. First, we word align the parallel sentences and parse the English (target) side. Then, we transform the aligned tree/string training data into derivation trees of minimal translation rules (Section 3.1). Finally, we learn our probability models Psub , Pifadj , and Padj by collecting counts over the derivation trees (Section 3.2). This method is quick enough to allow us to scale our learning process to largescale data sets. 3.1 Generating Derivation Trees and Rules There are four steps in transforming the training data into derivation trees and rule"
D09-1076,D07-1078,1,0.305949,"combination of properties that satisfy both linguists and empirical MT practitioners. So far, though, most work in this area has been either more linguistic than statistical (Abeille et al., 1990) or statistically-based, but linguistically light (Nesson et al., 2006). AldfAE AlwTnY (d) NP ↔ NN3 NN2 JJ1 JJ1 NN2 NN3 NP ↔ NN2 NN1 NN1 NN2 NP ↔ NN2 JJ1 JJ1 NN2 NP ↔ NN1 NN1 Figure 1: Rule (a) can be learned from this training example. Arguably, the more general rules (b) (d) should also be learnable. To mitigate this problem, the parse trees used as training data for these systems can be binarized (Wang et al., 2007). Binarization allows rules with partial constituents to be learned, resulting in more general rules, richer statistics, and better phrasal coverage (DeNeefe et al., 2007), but no principled required vs. optional decision has been made. This method’s key weakness is that binarization always keeps adjacent siblings together, so there is no way to group the head with a required complement if optional information intervenes between the two. Furthermore, if all kinds of children are considered equally optional, then we have removed important syntactic constraints, which may end up permitting too m"
D09-1076,P06-1121,1,0.230329,"Missing"
D09-1076,W08-0411,0,0.0127327,"adjoining. We have described a method to translate using this model. And we have demonstrated that linguistically-motivated adjoining improves the end-to-end MT results. There are many potential directions for research to proceed. One possibility is to investigate other methods of making the required vs. optional decision, either using linguistic resources such as COMLEX or automatically learning the distinction using EM (as done for tree binarization by Wang et al. (2007)). In addition, most ideas presented here are extendable to rules with linguistic trees on both sides (using insights from Lavie et al. (2008)). Also worth investigating is the direct integration of bilingual dictionaries into the grammar (as suggested by Shieber (2007)). Lastly, rule composition and different amounts of lexicalization (Galley et al., 2006; Marcu et al., 2006; DeNeefe et al., 2007) or context modeling (Mari˜no et al., 2006) have been successful with other models. Acknowledgments We thank David Chiang for suggestions about adjoining models, Michael Pust and Jens-S¨onke V¨ockler for developing parts of the experimental framework, and other colleagues at ISI for their helpful input. We also thank the anonymous reviewer"
D09-1076,W06-1606,1,0.682388,"Missing"
D09-1076,J93-2004,0,0.0336069,"G, adjunctions are permitted at any non-terminal with the same label as the root and foot node of the auxiliary tree, while in STAG adjunctions are restricted to linked sites. Nesson et al. (2006) introduce a probabilistic, synchronous variant of TIG and demonstrate its use for machine translation, showing results that beat both word-based and phrase-based MT models on a limited-vocabulary, small-scale training and test set. Training the model uses an O(n6 ) bilingual parsing algorithm, and decoding is O(n3 ). Though this model uses trees in the formal sense, it does not create Penn Treebank (Marcus et al., 1993) style linguistic trees, but uses only one non-terminal label (X) to create those trees using six simple rule structures. The grammars we use in this paper share some properties in common with those of Nesson et al. (2006) in that they are of the probabilistic, synchronous tree-insertion variety. All pairs of sites (both adjunction and substitution in our case) are explicitly linked. Adjunction sites are restricted by direction: at each linked site, the source and target side each specify one allowed direction. The result is that each synchronous adjunction site can be classified into one of f"
D09-1076,J06-4004,0,0.0195124,"Missing"
D09-1076,2006.amta-papers.15,0,0.74793,"nd from string-based models to tree-based models. Recently there is a swing back to incorporating more linguistic information again, but this time linguistic insight carefully guides the setup of empirically learned models. Shieber (2007) recently argued that probabilistic Synchronous Tree Adjoining Grammars (Shieber and Schabes, 1990) have the right combination of properties that satisfy both linguists and empirical MT practitioners. So far, though, most work in this area has been either more linguistic than statistical (Abeille et al., 1990) or statistically-based, but linguistically light (Nesson et al., 2006). AldfAE AlwTnY (d) NP ↔ NN3 NN2 JJ1 JJ1 NN2 NN3 NP ↔ NN2 NN1 NN1 NN2 NP ↔ NN2 JJ1 JJ1 NN2 NP ↔ NN1 NN1 Figure 1: Rule (a) can be learned from this training example. Arguably, the more general rules (b) (d) should also be learnable. To mitigate this problem, the parse trees used as training data for these systems can be binarized (Wang et al., 2007). Binarization allows rules with partial constituents to be learned, resulting in more general rules, richer statistics, and better phrasal coverage (DeNeefe et al., 2007), but no principled required vs. optional decision has been made. This method’"
D09-1076,P03-1021,0,0.0152935,"Missing"
D09-1076,P00-1058,0,\N,Missing
D09-1076,W90-0102,0,\N,Missing
D10-1051,P05-1074,0,0.045281,"Missing"
D10-1051,J93-2003,0,0.0397002,"S or S*) is likely to be realized. This model is trained with the same method described in Section 3 and is augmented with the CMU pronunciation dictionary. Finally, P(e) is a word-trigram model built from a 10,000-line corpus of 105 English love poems. We select the first line of our poem from the FST cascade’s 100,000-best list, or by hand. To generate each subsequent line, we modify the cascade and run it again. The first modification is to incorporate a discourse model. From our poetry corpus, we estimate a word’s unigram probability given the words on the previous line, via IBM Model 1 (Brown et al., 1993). We modify P(e) by interpolating in these probabilities. Second, we check if any previous line The women of the night Again and all the way Like a mouse in the white Not the heart of the day. - - Of the bed to trust me Around her twists the string But i will not tell thee Fire changes everything. - - A son of the right hand confines His uncle could have broken in Towards the high bank and the pines Upon the eyes and i have been - - Into one of her hundred year old Or the house in a house in a cold The first time she met him Like a mouse in the dim For me to the moon and when i told - - Into o"
D10-1051,W10-0304,0,0.01695,", w2 , ...wn needs to be rhymed with, according to the user-supplied scheme. If so, we build an additional FST that accepts only strings whose final word rhymes with wn . This is a reasonable approach, though it will not, for example, rhyme ...tar me with ...army. We say two non-identical words rhyme if their phoneme strings share a common suffix that includes the last stressed vowel. Figure 9 shows several poems that we automatically generate with this scheme. 5 Translation Automatically generated poetry can sound good when read aloud, but it often has a “nonsense” feel to it. According to (Gervas, 2010), creative-language researchers interested in realization and surface language statistics (“how to say”) have tended to gravitate to poetry generation, while researchers interested in characters, goals, and story-line (“what to say”) have tended to gravitate to prose story generation. Translation provides one way to tie things to529 DC is a long sequence of such three-line stanzas (tercets). The meter in Italian is hendecasyllabic, which has ten syllables and ensures three beats. Dante’s Italian rhyme scheme is: ABA, BCB, CDC, etc, meaning that lines 2, 4, and 6 rhyme with each other; lines 5,"
D10-1051,C08-1048,0,0.360218,"ne text repositories of human works and maintain these in memory. In this paper we concentrate on statistical methods applied to the analysis, generation, and translation of poetry. By analysis, we mean extracting patterns from existing online poetry corpora. We use these patterns to generate new poems and translate existing poems. When translating, we render target text in a rhythmic scheme determined by the user. Poetry generation has received research attention in the past (Manurung et al., 2000; Gervas, 2001; Diaz-Agudo et al., 2002; Manurung, 2003; Wong and Chun, 2008; Tosa et al., 2008; Jiang and Zhou, 2008; Netzer et al., 2009), including the use of statistical methods, although there is a long way to go. One difficulty has been the evaluation of machine-generated poetry—this continues to be a difficulty in the present paper. Less research effort has been spent on poetry analysis and poetry translation, which we tackle here. 2 Terms Meter refers to the rhythmic beat of poetic text when read aloud. Iambic is a common meter that sounds like da-DUM da-DUM da-DUM, etc. Each daDUM is called a foot. Anapest meter sounds like da-da-DUM da-da-DUM da-da-DUM, etc. Trimeter refers to a line with three fee"
D10-1051,P07-2045,0,0.00245581,"has been translated many times into English, we have examples of good outputs. Some translations target iambic pentameter, but even the most respected translations give up on rhyme, since English is much harder to rhyme than Italian. Longfellow’s translation begins: midway upon the journey of our life i found myself within a forest dark for the straightforward pathway had been lost. We arrange the translation problem as a cascade of WFSTs, as shown in Figure 10. We call our Italian input i. In lieu of the first WFST, we use the statistical phrase-based machine translation (PBMT) system Moses (Koehn et al., 2007), which generates a target-language lattice with paths scored by P(e|i). We send this lattice through the same P(m|e) device we trained in Section 3. Finally, we filter the resulting syllable sequences with a strict, single-path, deterministic iambic pentameter acceptor, P(m).3 Our 3 It is also possible to use a looser iambic P(m) model, as described in Section 3. Parallel Italian/English Data Collection Word count (English) DC-train 400,670 Il Fiore 25,995 Detto Damare 2,483 Egloghe 3,120 Misc. 557 Europarl 32,780,960 Original: nel mezzo del cammin di nostra vita mi ritrovai per una selva osc"
D10-1051,W09-2005,0,0.175107,"f human works and maintain these in memory. In this paper we concentrate on statistical methods applied to the analysis, generation, and translation of poetry. By analysis, we mean extracting patterns from existing online poetry corpora. We use these patterns to generate new poems and translate existing poems. When translating, we render target text in a rhythmic scheme determined by the user. Poetry generation has received research attention in the past (Manurung et al., 2000; Gervas, 2001; Diaz-Agudo et al., 2002; Manurung, 2003; Wong and Chun, 2008; Tosa et al., 2008; Jiang and Zhou, 2008; Netzer et al., 2009), including the use of statistical methods, although there is a long way to go. One difficulty has been the evaluation of machine-generated poetry—this continues to be a difficulty in the present paper. Less research effort has been spent on poetry analysis and poetry translation, which we tackle here. 2 Terms Meter refers to the rhythmic beat of poetic text when read aloud. Iambic is a common meter that sounds like da-DUM da-DUM da-DUM, etc. Each daDUM is called a foot. Anapest meter sounds like da-da-DUM da-da-DUM da-da-DUM, etc. Trimeter refers to a line with three feet, pentameter to a lin"
D12-1025,P09-1088,0,0.0125935,"for decipherment. The new improvements allow us to solve a word substitution cipher with billions of tokens and hundreds of thousands of word types. Through better approximation, we achieve a significant increase in deciphering accuracy. In the following section, we present details of our new approach. 3 Slice Sampling for Bayesian Decipherment In this section, we first give an introduction to Bayesian decipherment and then describe how to use slice sampling for it. 3.1 Bayesian Decipherment Bayesian inference has been widely used in natural language processing (Goldwater and Griffiths, 2007; Blunsom et al., 2009; Ravi and Knight, 2011b). It is very attractive for problems like word substitution ciphers for the following reasons. First, there are no memory bottlenecks as compared to EM, which has an O(N · V 2 ) space complexity. Second, priors encourage the model to learn a sparse distribution. The inference is usually performed using Gibbs sampling. For decipherment, a Gibbs sampler keeps drawing samples from plaintext sequences according to derivation probability P (d): P (d) = P (e) · n ∏ P (ci |ei ) (2) i In Bayesian inference, P (e) is still given by an ngram language model, while the channel pro"
D12-1025,W08-0309,0,0.0167082,"Missing"
D12-1025,P11-2071,0,0.439865,"Missing"
D12-1025,P07-1094,0,0.0160372,"the bigrams with their counts for decipherment. The new improvements allow us to solve a word substitution cipher with billions of tokens and hundreds of thousands of word types. Through better approximation, we achieve a significant increase in deciphering accuracy. In the following section, we present details of our new approach. 3 Slice Sampling for Bayesian Decipherment In this section, we first give an introduction to Bayesian decipherment and then describe how to use slice sampling for it. 3.1 Bayesian Decipherment Bayesian inference has been widely used in natural language processing (Goldwater and Griffiths, 2007; Blunsom et al., 2009; Ravi and Knight, 2011b). It is very attractive for problems like word substitution ciphers for the following reasons. First, there are no memory bottlenecks as compared to EM, which has an O(N · V 2 ) space complexity. Second, priors encourage the model to learn a sparse distribution. The inference is usually performed using Gibbs sampling. For decipherment, a Gibbs sampler keeps drawing samples from plaintext sequences according to derivation probability P (d): P (d) = P (e) · n ∏ P (ci |ei ) (2) i In Bayesian inference, P (e) is still given by an ngram language model,"
D12-1025,P08-1088,0,0.789045,"Missing"
D12-1025,P06-2065,1,0.513662,"intext token ei with a cipher token fi with probability P (fi |ei ). Based on the above generative story, we write the probability of the cipher string f as: P (f )θ = ∑ e P (e) · n ∏ Pθ (fi |ei ) (1) i We use this equation as an objective function for maximum likelihood training. In the equation, P (e) is given by an ngram language model, which is trained using a large amount of monolingual texts. The rest of the task is to manipulate channel probabilities Pθ (fi |ei ) so that the probability of the observed texts P (f )θ is maximized. Theoretically, we can directly apply EM, as proposed in (Knight et al., 2006), or Bayesian decipherment (Ravi and Knight, 2011a) to solve the problem. However, unlike letter substitution ciphers, word substitution ciphers pose much greater challenges to algorithm scalability. To solve a word substitution cipher, the EM algorithm has a computational complexity of O(N · V 2 · R) and the complexity of Bayesian method is O(N · V · R), where V is the size of plaintext vocabulary, N is the length of ciphertext, and R is the number of iterations. In the world of word substitution ciphers, both V and N are very large, making these approaches impractical. (Ravi and Knight, 2011"
D12-1025,W02-0902,1,0.658388,"Missing"
D12-1025,2005.mtsummit-papers.11,0,0.0467337,"million tokens French: 28k tokens Spanish: 26k tokens French: 30k tokens Spanish: 28k tokens Table 4: Europarl Training, Tuning, and Testing Data that table’s translation will be used. If a source word exists in both tables, Moses will create two separate decoding paths and choose the best one after taking other features into account. If a word is not seen in either of the tables, it is copied literally to the output. 6 MT Experiments and Results 6.1 Data In our MT experiments, we translate French into Spanish and use the following corpora to learn our translation systems: • Europarl Corpus (Koehn, 2005): The Europarl parallel corpus is extracted from the proceedings of the European Parliament and includes versions in 11 European languages. The corpus contains articles from the political domain and is used to train our baseline system. We use the 6th version of the corpus. After cleaning, there are 1.3 million lines left for training. We use the last 2k lines for tuning and testing (1k for each), and the rest for training. Details of training, tuning, and testing data are listed in Table 4. • EMEA Corpus (Tiedemann, 2009): EMEA is a parallel corpus made out of PDF documents from the European"
D12-1025,P03-1021,0,0.0289358,"ed SMT system using Moses (Koehn et al., 2007). The baseline system has 3 models: a translation model, a reordering model, and a language model. The language model can be trained on monolingual data, and the rest are trained on parallel data. By default, Moses uses the following 8 features to score a candidate translation: • direct and inverse translation probabilities • direct and inverse lexical weighting • phrase penalty • a language model • a re-ordering model • word penalty Each of the 8 features has its own weight, which can be tuned on a held-out set using minimum error rate training. (Och, 2003). In the following sections, we describe how to use decipherment to learn domain specific translation probabilities, and use the new features to improve the baseline. Figure 2: Learning curve for a very large word substitution cipher: Both token and type accuracy rise as more and more ciphertext becomes available. 5 Before building the language model, we replace low frequency word types with an ”UNK” symbol, leaving 129k unique word types. 271 5.2 Learning a New Translation Table with Decipherment From a decipherment perspective, machine translation is a much more complex task than solving a w"
D12-1025,P02-1040,0,0.0909775,"Missing"
D12-1025,P95-1050,0,0.614283,"Missing"
D12-1025,D08-1085,1,0.797669,"ous works, the translation table we build from monolingual data do not only contain unseen words but also words seen in parallel data. 266 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 266–275, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics 2 Word Substitution Ciphers Before we present our new decipherment framework, we quickly review word substitution decipherment. Recently, there has been an increasing interest in decipherment work (Ravi and Knight, 2011a; Ravi and Knight, 2008). While letter substitution ciphers can be solved easily, nobody has been able to solve a word substitution cipher with high accuracy. As shown in Figure 1, a word substitution cipher is generated by replacing each word in a natural language (plaintext) sequence with a cipher token according to a substitution table. The mapping in the table is deterministic – each plaintext word type is only encoded with one unique cipher token. Solving a word substitution cipher means recovering the original plaintext from the ciphertext without knowing the substitution table. The only thing we rely on is kno"
D12-1025,P11-1025,1,0.81354,"is only encoded with one unique cipher token. Solving a word substitution cipher means recovering the original plaintext from the ciphertext without knowing the substitution table. The only thing we rely on is knowledge about the underlying language. Figure 1: Encoding and Decipherment of a Word Substitution Cipher How can we solve a word substitution cipher? The approach is similar to those taken by cryptanalysts who try to recover keys that convert encrypted texts to readable texts. Suppose we observe a large cipher string f and want to decipher it into English e. We can follow the work in (Ravi and Knight, 2011b) and assume that the cipher string f is generated in the following way: 267 • Generate English plaintext sequence e e1 , e2 ...en with probability P(e). = • Replace each English plaintext token ei with a cipher token fi with probability P (fi |ei ). Based on the above generative story, we write the probability of the cipher string f as: P (f )θ = ∑ e P (e) · n ∏ Pθ (fi |ei ) (1) i We use this equation as an objective function for maximum likelihood training. In the equation, P (e) is given by an ngram language model, which is trained using a large amount of monolingual texts. The rest of the"
D12-1025,P11-1002,1,0.879056,"is only encoded with one unique cipher token. Solving a word substitution cipher means recovering the original plaintext from the ciphertext without knowing the substitution table. The only thing we rely on is knowledge about the underlying language. Figure 1: Encoding and Decipherment of a Word Substitution Cipher How can we solve a word substitution cipher? The approach is similar to those taken by cryptanalysts who try to recover keys that convert encrypted texts to readable texts. Suppose we observe a large cipher string f and want to decipher it into English e. We can follow the work in (Ravi and Knight, 2011b) and assume that the cipher string f is generated in the following way: 267 • Generate English plaintext sequence e e1 , e2 ...en with probability P(e). = • Replace each English plaintext token ei with a cipher token fi with probability P (fi |ei ). Based on the above generative story, we write the probability of the cipher string f as: P (f )θ = ∑ e P (e) · n ∏ Pθ (fi |ei ) (1) i We use this equation as an objective function for maximum likelihood training. In the equation, P (e) is given by an ngram language model, which is trained using a large amount of monolingual texts. The rest of the"
D12-1025,C08-1125,0,0.0239207,"Missing"
D12-1025,P07-2045,0,\N,Missing
D12-1025,P08-1000,0,\N,Missing
D13-1173,C10-1011,0,0.0429152,"aword corpus for our decipherment experiments. The corpus contains news articles from different news agencies and is available in Spanish and English. We use only the AFP (Agence FrancePresse) section of the corpus in decipherment experiments. We tokenize the corpus using tools that come with the Europarl corpus (Koehn, 2005). To shorten the time required for running different systems on large amounts of data, we keep only the top 5000 most frequent word types in both languages and replace all other word types with UNK. We also throw away lines with more than 40 tokens, as the Spanish parser (Bohnet, 2010) we use is slow when processing long sentences. After preprocessing, the corpus contains approximately 440 million tokens in Spanish and 350 million tokens in English. To obtain dependency bigrams, we use the Bohnet parsers (Bohnet, 2010) to parse both the Spanish and English version of the corpus. 4.2 Systems Three systems are evaluated in the experiments. We implement a baseline system, Adjacent, based on Dou and Knight (2012). The baseline system collects adjacent bigrams and their counts from Spanish and English texts. It then builds an English bigram language model using the English adjac"
D13-1173,P11-2071,0,0.333324,"Missing"
D13-1173,D12-1025,1,0.390422,"ers for a large scale MT system. Other work tries to learn full MT systems using only non parallel data through decipherment (Ravi and Knight, 2011; Ravi, 2013). However, the performance of such systems is poor compared with those trained with parallel data. Given that we often have some parallel data, it is more practical to improve a translation system trained on parallel corpora with non parallel Figure 1: Improving machine translation with decipherment (Grey boxes represent new data and process). Mono: monolingual; LM: language model; LEX: translation lexicon; TM: translation model. data. Dou and Knight (2012) successfully apply decipherment to learn a domain specific translation lexicon from monolingual data to improve out-ofdomain machine translation. Although their approach works well for Spanish/French, they do not show whether their approach works for other language pairs. Moreover, the non parallel data used in their experiments is created from a parallel corpus. Such highly comparable data is difficult to obtain in reality. In this work, we improve previous work by Dou and Knight (2012) using genuinely non parallel data, 1668 Proceedings of the 2013 Conference on Empirical Methods in Natural"
D13-1173,P98-1069,0,0.166702,"ens e1 e2 with probability P (e1 e2 ) given by a language model built from large numbers of plaintext bigrams. • Substitute e1 with f1 and e2 with f2 with probability P (f1 |e1 ) · P (f2 |e2 ). The probability of any cipher bigram F is: P (F ) = X P (e1 e2 ) e1 e2 2 Y P (fi |ei ) i=1 Given a corpus of N cipher bigrams F1 ...FN , the probability of the corpus is: Previous Work Motivated by the idea that a translation lexicon induced from non parallel data can be applied to MT, a variety of prior research has tried to build a translation lexicon from non parallel or comparable data (Rapp, 1995; Fung and Yee, 1998; Koehn and Knight, 2002; Haghighi et al., 2008; Garera et al., 2009; Bergsma and Van Durme, 2011; Daum´e and Jagarlamudi, 2011; Irvine and Callison-Burch, 2013b; Irvine and Callison-Burch, 2013a). Although previous work is able to build a translation lexicon without parallel data, little has used the lexicon to improve machine translation. There has been increasing interest in learning translation lexicons from non parallel data with decipherment techniques (Ravi and Knight, 2011; Dou and Knight, 2012; Nuhn et al., 2012). Decipherment views one language as a cipher for another and learns a tr"
D13-1173,W09-1117,0,0.287067,"lt from large numbers of plaintext bigrams. • Substitute e1 with f1 and e2 with f2 with probability P (f1 |e1 ) · P (f2 |e2 ). The probability of any cipher bigram F is: P (F ) = X P (e1 e2 ) e1 e2 2 Y P (fi |ei ) i=1 Given a corpus of N cipher bigrams F1 ...FN , the probability of the corpus is: Previous Work Motivated by the idea that a translation lexicon induced from non parallel data can be applied to MT, a variety of prior research has tried to build a translation lexicon from non parallel or comparable data (Rapp, 1995; Fung and Yee, 1998; Koehn and Knight, 2002; Haghighi et al., 2008; Garera et al., 2009; Bergsma and Van Durme, 2011; Daum´e and Jagarlamudi, 2011; Irvine and Callison-Burch, 2013b; Irvine and Callison-Burch, 2013a). Although previous work is able to build a translation lexicon without parallel data, little has used the lexicon to improve machine translation. There has been increasing interest in learning translation lexicons from non parallel data with decipherment techniques (Ravi and Knight, 2011; Dou and Knight, 2012; Nuhn et al., 2012). Decipherment views one language as a cipher for another and learns a translation lexicon that produces a good decipherment. In an effort to"
D13-1173,P08-1088,0,0.146464,"by a language model built from large numbers of plaintext bigrams. • Substitute e1 with f1 and e2 with f2 with probability P (f1 |e1 ) · P (f2 |e2 ). The probability of any cipher bigram F is: P (F ) = X P (e1 e2 ) e1 e2 2 Y P (fi |ei ) i=1 Given a corpus of N cipher bigrams F1 ...FN , the probability of the corpus is: Previous Work Motivated by the idea that a translation lexicon induced from non parallel data can be applied to MT, a variety of prior research has tried to build a translation lexicon from non parallel or comparable data (Rapp, 1995; Fung and Yee, 1998; Koehn and Knight, 2002; Haghighi et al., 2008; Garera et al., 2009; Bergsma and Van Durme, 2011; Daum´e and Jagarlamudi, 2011; Irvine and Callison-Burch, 2013b; Irvine and Callison-Burch, 2013a). Although previous work is able to build a translation lexicon without parallel data, little has used the lexicon to improve machine translation. There has been increasing interest in learning translation lexicons from non parallel data with decipherment techniques (Ravi and Knight, 2011; Dou and Knight, 2012; Nuhn et al., 2012). Decipherment views one language as a cipher for another and learns a translation lexicon that produces a good decipher"
D13-1173,W13-2233,0,0.303185,"f2 with probability P (f1 |e1 ) · P (f2 |e2 ). The probability of any cipher bigram F is: P (F ) = X P (e1 e2 ) e1 e2 2 Y P (fi |ei ) i=1 Given a corpus of N cipher bigrams F1 ...FN , the probability of the corpus is: Previous Work Motivated by the idea that a translation lexicon induced from non parallel data can be applied to MT, a variety of prior research has tried to build a translation lexicon from non parallel or comparable data (Rapp, 1995; Fung and Yee, 1998; Koehn and Knight, 2002; Haghighi et al., 2008; Garera et al., 2009; Bergsma and Van Durme, 2011; Daum´e and Jagarlamudi, 2011; Irvine and Callison-Burch, 2013b; Irvine and Callison-Burch, 2013a). Although previous work is able to build a translation lexicon without parallel data, little has used the lexicon to improve machine translation. There has been increasing interest in learning translation lexicons from non parallel data with decipherment techniques (Ravi and Knight, 2011; Dou and Knight, 2012; Nuhn et al., 2012). Decipherment views one language as a cipher for another and learns a translation lexicon that produces a good decipherment. In an effort to build a MT system without a parallel corpus, Ravi and Knight (2011) view Spanish as a 1669"
D13-1173,N13-1056,0,0.32706,"f2 with probability P (f1 |e1 ) · P (f2 |e2 ). The probability of any cipher bigram F is: P (F ) = X P (e1 e2 ) e1 e2 2 Y P (fi |ei ) i=1 Given a corpus of N cipher bigrams F1 ...FN , the probability of the corpus is: Previous Work Motivated by the idea that a translation lexicon induced from non parallel data can be applied to MT, a variety of prior research has tried to build a translation lexicon from non parallel or comparable data (Rapp, 1995; Fung and Yee, 1998; Koehn and Knight, 2002; Haghighi et al., 2008; Garera et al., 2009; Bergsma and Van Durme, 2011; Daum´e and Jagarlamudi, 2011; Irvine and Callison-Burch, 2013b; Irvine and Callison-Burch, 2013a). Although previous work is able to build a translation lexicon without parallel data, little has used the lexicon to improve machine translation. There has been increasing interest in learning translation lexicons from non parallel data with decipherment techniques (Ravi and Knight, 2011; Dou and Knight, 2012; Nuhn et al., 2012). Decipherment views one language as a cipher for another and learns a translation lexicon that produces a good decipherment. In an effort to build a MT system without a parallel corpus, Ravi and Knight (2011) view Spanish as a 1669"
D13-1173,E12-1014,0,0.11134,"imited parallel data. In experiments, we observe BLEU gains of 1.2 to 1.8 across three different test sets. 1 Introduction State-of-the-art machine translation (MT) systems apply statistical techniques to learn translation rules from large amounts of parallel data. However, parallel data is limited for many language pairs and domains. In general, it is easier to obtain non parallel data. The ability to build a machine translation system using monolingual data could alleviate problems caused by insufficient parallel data. Towards building a machine translation system without a parallel corpus, Klementiev et al. (2012) use non parallel data to estimate parameters for a large scale MT system. Other work tries to learn full MT systems using only non parallel data through decipherment (Ravi and Knight, 2011; Ravi, 2013). However, the performance of such systems is poor compared with those trained with parallel data. Given that we often have some parallel data, it is more practical to improve a translation system trained on parallel corpora with non parallel Figure 1: Improving machine translation with decipherment (Grey boxes represent new data and process). Mono: monolingual; LM: language model; LEX: translat"
D13-1173,P06-2065,1,0.708923,"been increasing interest in learning translation lexicons from non parallel data with decipherment techniques (Ravi and Knight, 2011; Dou and Knight, 2012; Nuhn et al., 2012). Decipherment views one language as a cipher for another and learns a translation lexicon that produces a good decipherment. In an effort to build a MT system without a parallel corpus, Ravi and Knight (2011) view Spanish as a 1669 P (corpus) = N Y P (Fj ) j=1 Given a plaintext bigram language model, the goal is to manipulate P (f |e) to maximize P (corpus). Theoretically, one can directly apply EM to solve the problem (Knight et al., 2006). However, EM has time complexity O(N · Ve2 ) and space complexity O(Vf · Ve ), where Vf , Ve are the sizes of ciphertext and plaintext vocabularies respectively, and N is the number of cipher bigrams. Ravi and Knight (2011) apply Bayesian learning to reduce the space complexity. Instead of estimating probabilities P (f |e), Bayesian learning tries to draw samples from plaintext sequences given ciphertext bigrams. During sampling, the probability of any possible plaintext sample e1 e2 is given as: Psample (e1 e2 ) = P (e1 e2 ) 2 Y i=1 Pbayes (fi |ei ) misi´on de naciones unidas en oriente medi"
D13-1173,W02-0902,1,0.78764,"bility P (e1 e2 ) given by a language model built from large numbers of plaintext bigrams. • Substitute e1 with f1 and e2 with f2 with probability P (f1 |e1 ) · P (f2 |e2 ). The probability of any cipher bigram F is: P (F ) = X P (e1 e2 ) e1 e2 2 Y P (fi |ei ) i=1 Given a corpus of N cipher bigrams F1 ...FN , the probability of the corpus is: Previous Work Motivated by the idea that a translation lexicon induced from non parallel data can be applied to MT, a variety of prior research has tried to build a translation lexicon from non parallel or comparable data (Rapp, 1995; Fung and Yee, 1998; Koehn and Knight, 2002; Haghighi et al., 2008; Garera et al., 2009; Bergsma and Van Durme, 2011; Daum´e and Jagarlamudi, 2011; Irvine and Callison-Burch, 2013b; Irvine and Callison-Burch, 2013a). Although previous work is able to build a translation lexicon without parallel data, little has used the lexicon to improve machine translation. There has been increasing interest in learning translation lexicons from non parallel data with decipherment techniques (Ravi and Knight, 2011; Dou and Knight, 2012; Nuhn et al., 2012). Decipherment views one language as a cipher for another and learns a translation lexicon that p"
D13-1173,2005.mtsummit-papers.11,0,0.0221485,"t is “accepted(verb) request(object)”. If we limit the search space, a system is more likely to find a better decipherment. 4 Deciphering Spanish Gigaword In this section, we compare dependency bigrams with adjacent bigrams for deciphering Spanish into English. 4.1 Data We use the Gigaword corpus for our decipherment experiments. The corpus contains news articles from different news agencies and is available in Spanish and English. We use only the AFP (Agence FrancePresse) section of the corpus in decipherment experiments. We tokenize the corpus using tools that come with the Europarl corpus (Koehn, 2005). To shorten the time required for running different systems on large amounts of data, we keep only the top 5000 most frequent word types in both languages and replace all other word types with UNK. We also throw away lines with more than 40 tokens, as the Spanish parser (Bohnet, 2010) we use is slow when processing long sentences. After preprocessing, the corpus contains approximately 440 million tokens in Spanish and 350 million tokens in English. To obtain dependency bigrams, we use the Bohnet parsers (Bohnet, 2010) to parse both the Spanish and English version of the corpus. 4.2 Systems Th"
D13-1173,P12-1017,0,0.236919,"a translation lexicon from non parallel or comparable data (Rapp, 1995; Fung and Yee, 1998; Koehn and Knight, 2002; Haghighi et al., 2008; Garera et al., 2009; Bergsma and Van Durme, 2011; Daum´e and Jagarlamudi, 2011; Irvine and Callison-Burch, 2013b; Irvine and Callison-Burch, 2013a). Although previous work is able to build a translation lexicon without parallel data, little has used the lexicon to improve machine translation. There has been increasing interest in learning translation lexicons from non parallel data with decipherment techniques (Ravi and Knight, 2011; Dou and Knight, 2012; Nuhn et al., 2012). Decipherment views one language as a cipher for another and learns a translation lexicon that produces a good decipherment. In an effort to build a MT system without a parallel corpus, Ravi and Knight (2011) view Spanish as a 1669 P (corpus) = N Y P (Fj ) j=1 Given a plaintext bigram language model, the goal is to manipulate P (f |e) to maximize P (corpus). Theoretically, one can directly apply EM to solve the problem (Knight et al., 2006). However, EM has time complexity O(N · Ve2 ) and space complexity O(Vf · Ve ), where Vf , Ve are the sizes of ciphertext and plaintext vocabularies respec"
D13-1173,J03-1002,0,0.00563571,"sh texts as our test data. The data contains 37,505 tokens and 6556 word types. We use type accuracy as our evaluation metric: Given a word type f in Spanish, we find a translation pair (f, e) with the highest average P (e|f ) from the translation table learned through decipherment. If the translation pair (f, e) can also be found in a gold translation lexicon Tgold , we treat the word type f as correctly deciphered. Let |C |be the number of word types correctly deciphered, and |V |be the total number of word types evaluated. We define type accuracy as |C| |V |. To create Tgold , we use GIZA (Och and Ney, 2003) to align a small amount of Spanish-English parallel text (1 million tokens for each language), and use the lexicon derived from the alignment as our gold translation lexicon. Tgold contains a subset of 4408 types seen in the test data, among which, 2878 are also top 5000 frequent word types. 4.5 Results During decipherment, we gradually increase the size of Spanish texts and compare the learning curves of three deciphering systems in Figure 2. 1672 Figure 2: Learning curves for three decipherment systems. Compared with Adjacent (previous state of the art), systems that use dependency bigrams"
D13-1173,P03-1021,0,0.041946,"e English word e, where P (e|f ) is the highest as the initial sample; for any f that are not seen in the seed lexicon, we do random initialization. We perform 20 random restarts with 10k iterations on each and build a word-to-word translation lexicon Tdecipher by collecting translation pairs seen in at least 3 final decipherments with either P (f |e) ≥ 0.2 or P (e|f ) ≥ 0.2. 5.2.3 • direct and inverse lexical weighting • a language model score • a distortion score • phrase penalty • word penalty The 8 features have weights adjusted on the tuning data using minimum error rate training (MERT) (Och, 2003). PBMT has a phrase table Tphrase . During decoding, Moses copies out-of-vocabulary (OOV) words, which can not be found in Tphrase , 1673 Improving Translation of Observed Words with Decipherment To improve translation of words observed in our parallel corpus, we simply use Tdecipher as an additional parallel corpus. First, we filter Tdecipher by keeping only translation pairs (f, e), where f is observed in the Spanish part and e is observed in the English part of the parallel corpus. Then we append all the Spanish and English words in the filtered Tdecipher to the end of Spanish part and Engl"
D13-1173,P95-1050,0,0.562272,"laintext tokens e1 e2 with probability P (e1 e2 ) given by a language model built from large numbers of plaintext bigrams. • Substitute e1 with f1 and e2 with f2 with probability P (f1 |e1 ) · P (f2 |e2 ). The probability of any cipher bigram F is: P (F ) = X P (e1 e2 ) e1 e2 2 Y P (fi |ei ) i=1 Given a corpus of N cipher bigrams F1 ...FN , the probability of the corpus is: Previous Work Motivated by the idea that a translation lexicon induced from non parallel data can be applied to MT, a variety of prior research has tried to build a translation lexicon from non parallel or comparable data (Rapp, 1995; Fung and Yee, 1998; Koehn and Knight, 2002; Haghighi et al., 2008; Garera et al., 2009; Bergsma and Van Durme, 2011; Daum´e and Jagarlamudi, 2011; Irvine and Callison-Burch, 2013b; Irvine and Callison-Burch, 2013a). Although previous work is able to build a translation lexicon without parallel data, little has used the lexicon to improve machine translation. There has been increasing interest in learning translation lexicons from non parallel data with decipherment techniques (Ravi and Knight, 2011; Dou and Knight, 2012; Nuhn et al., 2012). Decipherment views one language as a cipher for ano"
D13-1173,P11-1002,1,0.877948,"echniques to learn translation rules from large amounts of parallel data. However, parallel data is limited for many language pairs and domains. In general, it is easier to obtain non parallel data. The ability to build a machine translation system using monolingual data could alleviate problems caused by insufficient parallel data. Towards building a machine translation system without a parallel corpus, Klementiev et al. (2012) use non parallel data to estimate parameters for a large scale MT system. Other work tries to learn full MT systems using only non parallel data through decipherment (Ravi and Knight, 2011; Ravi, 2013). However, the performance of such systems is poor compared with those trained with parallel data. Given that we often have some parallel data, it is more practical to improve a translation system trained on parallel corpora with non parallel Figure 1: Improving machine translation with decipherment (Grey boxes represent new data and process). Mono: monolingual; LM: language model; LEX: translation lexicon; TM: translation model. data. Dou and Knight (2012) successfully apply decipherment to learn a domain specific translation lexicon from monolingual data to improve out-ofdomain"
D13-1173,P13-1036,0,0.574259,"slation rules from large amounts of parallel data. However, parallel data is limited for many language pairs and domains. In general, it is easier to obtain non parallel data. The ability to build a machine translation system using monolingual data could alleviate problems caused by insufficient parallel data. Towards building a machine translation system without a parallel corpus, Klementiev et al. (2012) use non parallel data to estimate parameters for a large scale MT system. Other work tries to learn full MT systems using only non parallel data through decipherment (Ravi and Knight, 2011; Ravi, 2013). However, the performance of such systems is poor compared with those trained with parallel data. Given that we often have some parallel data, it is more practical to improve a translation system trained on parallel corpora with non parallel Figure 1: Improving machine translation with decipherment (Grey boxes represent new data and process). Mono: monolingual; LM: language model; LEX: translation lexicon; TM: translation model. data. Dou and Knight (2012) successfully apply decipherment to learn a domain specific translation lexicon from monolingual data to improve out-ofdomain machine trans"
D13-1173,C98-1066,0,\N,Missing
D13-1173,P07-2045,0,\N,Missing
D13-1173,P08-1000,0,\N,Missing
D14-1030,D13-1140,1,0.840612,"ler than the entire vocabulary V . The probability of an output word l at position t + 1 given that its class is m is defined as: P (yl (t + 1) |cm (t), s(t)) = exp (s(t)D0 vl ) , PV 0 0 v )) (exp (s(t)D k k=1 where D0 is a matrix of output word embeddings and vl is a one hot vector representing the word with index l. The probability of the word w(t + 1) given its class ci can now be computed as: P (w(t + 1) |s(t)) =P (w(t + 1) |ci , s(t)) P (ci |s(t)). s(t) = φ (Dw(t) + Ws(t − 1)) , Neural Probabilistic Language Model We use the feedforward neural probabilistic language model architecture of Vaswani et al. (2013), as shown in Figure 4. Each context u comprises a sequence of words uj (1 ≤ j ≤ n − 1) represented as one-hot vectors, which are fed as input to the neural network. At the output layer, the neural network computes the probability P (w |u) for each word w, as follows. The output of the first hidden layer h1 is   n−1 X h1 = φ  Cj Duj + b1  , where D is the matrix of input word embeddings, W is a matrix that transforms the activations from the hidden layer in position t − 1, and φ is a 1 sigmoid function, defined as φ(x) = 1+exp(−x) , that is applied elementwise. We need to compute the proba"
D14-1030,P13-2152,0,0.0401615,"s the word with inversely proportional eftheir representations in terms of how well they fort to how predictable the word is (Frank et al., can be used to decode the word being read from 2013). There is a well studied response known as MEG data. We obtain correspondences between the N400 that is an increase of the activity in the the models and the brain data that are consistent temporal cortex that has been recently shown to be with a model of language processing in which graded by the amount of surprisal of the incoming brain activity encodes story context, and where word given the context (Frank et al., 2013). This is 3.2. Pre-lexical analysis each new word generates additional brain activity, 3.3. Lexical-semantic analysis analogous to the output probability of the incomIn order to tease apart early pre-lexical processes in flowing generally from visual processing areas to To identify cortical dynamics of reading comprehension, reading, Tarkiainen and colleagues (Tarkiainen et al., ing word from the neural network. Helenius and colleagues et al., 1998) employed a 1999) used words, syllables, and single letters,areas, imbeddedculminating more high level in an (Helenius updated Fig. 2 shows a hypot"
D14-1048,W13-2322,1,0.652887,"tax trees into strings data (by taking yields), it is not obvious how to do this for unordered AMR graph elements. The example above also shows that gold alignment links reach into the internal nodes of AMR. Prior SMT work (Jones et al., 2012) describes alignment of semantic graphs and strings, though their experiments are limited to the GeoQuery domain, and their methods are not described in detail. Flanigan et al (2014) describe a heuristic AMR/English aligner. While heuristic aligners can achieve good accuracy, they will not automatically improve as more AMR/English data comes Introduction Banarescu et al. (2013) describe a semantics bank of English sentences paired with their logical meanings, written in Abstract Meaning Representation (AMR). The designers of AMR leave open the question of how meanings are derived from English sentences (and vice-versa), so there are no manually-annotated alignment links between English words and AMR concepts. This paper studies how to build such links automatically, using co-occurrence and other information. Automatic alignments may be useful for downstream extraction of semantic interpretation and generation rules. AMRs are directed, acyclic graphs with labeled edg"
D14-1048,J93-2003,0,0.0560458,"the nodes that are seen previously. For example the AMR: (w / want-01 :arg0 (b / boy) :arg1 (g / go-01 :arg0 b)) is linearized into this order: w / want-01 :arg0 b / boy :arg1 g / go-01 :arg0 b. Note that semantically related nodes often stay close together after linearization. After linearizing the AMR graph into a string, we perform a series of preprocessing steps including lowercasing the letters, removing stop words, and stemming. The AMR and English stop word lists are generated based on our knowledge of AMR design. 2.2 Training Our training method is based on IBM word alignment models (Brown et al., 1993). We modify the objective functions of the IBM models to en426 courage agreement between learning parameters in English-to-AMR and AMR-to-English directions of EM. The solution of this objective function can be approximated in an extremely simple way that requires almost no extra coding effort. Assume that we have a set of sentence pairs {(E, A)}, where each E is an English sentence and each A is a linearized AMR. According to IBM models, A is generated from E through a generative story based on some parameters. For example, in IBM Model 2, given E we first decide the length of A based on some"
D14-1048,P14-1134,0,0.357526,"to efficiently consider. We therefore follow syntax-based SMT custom and use string/string alignment models in aligning our graph/string pairs. However, while it is straightforward to convert syntax trees into strings data (by taking yields), it is not obvious how to do this for unordered AMR graph elements. The example above also shows that gold alignment links reach into the internal nodes of AMR. Prior SMT work (Jones et al., 2012) describes alignment of semantic graphs and strings, though their experiments are limited to the GeoQuery domain, and their methods are not described in detail. Flanigan et al (2014) describe a heuristic AMR/English aligner. While heuristic aligners can achieve good accuracy, they will not automatically improve as more AMR/English data comes Introduction Banarescu et al. (2013) describe a semantics bank of English sentences paired with their logical meanings, written in Abstract Meaning Representation (AMR). The designers of AMR leave open the question of how meanings are derived from English sentences (and vice-versa), so there are no manually-annotated alignment links between English words and AMR concepts. This paper studies how to build such links automatically, using"
D14-1048,N04-1035,1,0.446703,"s, because AMR and English are highly cognate. It is harder in other ways, as AMR is graphstructured, and children of an AMR node are unordered. There are also fewer available training pairs than in SMT. One approach is to define a generative model from AMR graphs to strings. We can then use EM to uncover hidden derivations, which alignments weakly reflect. This approach is used in string/string SMT (Brown et al., 1993). However, we do not yet have such a generative graphto-string model, and even if we did, there might not be an efficient EM solution. For example, in syntax-based SMT systems (Galley et al., 2004), the generative tree/string transduction story is clear, but in the absence of alignment constraints, there are too many derivations and rules for EM to efficiently consider. We therefore follow syntax-based SMT custom and use string/string alignment models in aligning our graph/string pairs. However, while it is straightforward to convert syntax trees into strings data (by taking yields), it is not obvious how to do this for unordered AMR graph elements. The example above also shows that gold alignment links reach into the internal nodes of AMR. Prior SMT work (Jones et al., 2012) describes"
D14-1048,W08-0509,0,0.0401697,"Missing"
D14-1048,C12-1083,1,\N,Missing
D14-1048,N06-1014,0,\N,Missing
D14-1061,W09-1117,0,0.141976,"Missing"
D14-1061,J93-2003,0,0.0752613,"Missing"
D14-1061,P13-1057,0,0.0206327,"Missing"
D14-1061,P11-2071,0,0.0854123,"Missing"
D14-1061,P08-1088,0,0.368446,"ystem from non parallel data. Given that we often have some parallel data, it is more practical to improve a translation system trained on parallel data by using additional non parallel data. Rapp (1995) shows that with a seed lexicon, it is possible to induce new word level translations from non parallel data. Motivated by the idea that a translation lexicon induced from non parallel data can be used to translate out of vocabulary words (OOV), a variety of prior research has tried to build a translation lexicon from non parallel or comparable data (Fung and Yee, 1998; Koehn and Knight, 2002; Haghighi et al., 2008; Garera Figure 1: Combine word alignment and decipherment into a single learning process. et al., 2009; Bergsma and Van Durme, 2011; Daum´e and Jagarlamudi, 2011; Irvine and Callison-Burch, 2013b; Irvine and Callison-Burch, 2013a; Irvine et al., 2013). Lately, there has been increasing interest in learning translation lexicons from non parallel data with decipherment techniques (Ravi and Knight, 2011; Dou and Knight, 2012; Nuhn et al., 2012; Dou and Knight, 2013). Decipherment views one language as a cipher for another and learns a translation lexicon that produces fluent text in the target ("
D14-1061,D12-1025,1,0.887248,"cabulary words (OOV), a variety of prior research has tried to build a translation lexicon from non parallel or comparable data (Fung and Yee, 1998; Koehn and Knight, 2002; Haghighi et al., 2008; Garera Figure 1: Combine word alignment and decipherment into a single learning process. et al., 2009; Bergsma and Van Durme, 2011; Daum´e and Jagarlamudi, 2011; Irvine and Callison-Burch, 2013b; Irvine and Callison-Burch, 2013a; Irvine et al., 2013). Lately, there has been increasing interest in learning translation lexicons from non parallel data with decipherment techniques (Ravi and Knight, 2011; Dou and Knight, 2012; Nuhn et al., 2012; Dou and Knight, 2013). Decipherment views one language as a cipher for another and learns a translation lexicon that produces fluent text in the target (plaintext) language. Previous work has shown that decipherment not only helps find translations for OOVs (Dou and Knight, 2012), but also improves translations of observed words (Dou and Knight, 2013). We find that previous work using monolingual or comparable data to improve quality of machine translation separates two learning tasks: first, translation rules are learned from parallel data, and then the information learne"
D14-1061,W13-2233,0,0.0730801,"data. Rapp (1995) shows that with a seed lexicon, it is possible to induce new word level translations from non parallel data. Motivated by the idea that a translation lexicon induced from non parallel data can be used to translate out of vocabulary words (OOV), a variety of prior research has tried to build a translation lexicon from non parallel or comparable data (Fung and Yee, 1998; Koehn and Knight, 2002; Haghighi et al., 2008; Garera Figure 1: Combine word alignment and decipherment into a single learning process. et al., 2009; Bergsma and Van Durme, 2011; Daum´e and Jagarlamudi, 2011; Irvine and Callison-Burch, 2013b; Irvine and Callison-Burch, 2013a; Irvine et al., 2013). Lately, there has been increasing interest in learning translation lexicons from non parallel data with decipherment techniques (Ravi and Knight, 2011; Dou and Knight, 2012; Nuhn et al., 2012; Dou and Knight, 2013). Decipherment views one language as a cipher for another and learns a translation lexicon that produces fluent text in the target (plaintext) language. Previous work has shown that decipherment not only helps find translations for OOVs (Dou and Knight, 2012), but also improves translations of observed words (Dou and Knight,"
D14-1061,D13-1173,1,0.633919,"research has tried to build a translation lexicon from non parallel or comparable data (Fung and Yee, 1998; Koehn and Knight, 2002; Haghighi et al., 2008; Garera Figure 1: Combine word alignment and decipherment into a single learning process. et al., 2009; Bergsma and Van Durme, 2011; Daum´e and Jagarlamudi, 2011; Irvine and Callison-Burch, 2013b; Irvine and Callison-Burch, 2013a; Irvine et al., 2013). Lately, there has been increasing interest in learning translation lexicons from non parallel data with decipherment techniques (Ravi and Knight, 2011; Dou and Knight, 2012; Nuhn et al., 2012; Dou and Knight, 2013). Decipherment views one language as a cipher for another and learns a translation lexicon that produces fluent text in the target (plaintext) language. Previous work has shown that decipherment not only helps find translations for OOVs (Dou and Knight, 2012), but also improves translations of observed words (Dou and Knight, 2013). We find that previous work using monolingual or comparable data to improve quality of machine translation separates two learning tasks: first, translation rules are learned from parallel data, and then the information learned from parallel data is used to bootstrap"
D14-1061,N13-1056,0,0.0790273,"data. Rapp (1995) shows that with a seed lexicon, it is possible to induce new word level translations from non parallel data. Motivated by the idea that a translation lexicon induced from non parallel data can be used to translate out of vocabulary words (OOV), a variety of prior research has tried to build a translation lexicon from non parallel or comparable data (Fung and Yee, 1998; Koehn and Knight, 2002; Haghighi et al., 2008; Garera Figure 1: Combine word alignment and decipherment into a single learning process. et al., 2009; Bergsma and Van Durme, 2011; Daum´e and Jagarlamudi, 2011; Irvine and Callison-Burch, 2013b; Irvine and Callison-Burch, 2013a; Irvine et al., 2013). Lately, there has been increasing interest in learning translation lexicons from non parallel data with decipherment techniques (Ravi and Knight, 2011; Dou and Knight, 2012; Nuhn et al., 2012; Dou and Knight, 2013). Decipherment views one language as a cipher for another and learns a translation lexicon that produces fluent text in the target (plaintext) language. Previous work has shown that decipherment not only helps find translations for OOVs (Dou and Knight, 2012), but also improves translations of observed words (Dou and Knight,"
D14-1061,D13-1109,0,0.0586598,"induce new word level translations from non parallel data. Motivated by the idea that a translation lexicon induced from non parallel data can be used to translate out of vocabulary words (OOV), a variety of prior research has tried to build a translation lexicon from non parallel or comparable data (Fung and Yee, 1998; Koehn and Knight, 2002; Haghighi et al., 2008; Garera Figure 1: Combine word alignment and decipherment into a single learning process. et al., 2009; Bergsma and Van Durme, 2011; Daum´e and Jagarlamudi, 2011; Irvine and Callison-Burch, 2013b; Irvine and Callison-Burch, 2013a; Irvine et al., 2013). Lately, there has been increasing interest in learning translation lexicons from non parallel data with decipherment techniques (Ravi and Knight, 2011; Dou and Knight, 2012; Nuhn et al., 2012; Dou and Knight, 2013). Decipherment views one language as a cipher for another and learns a translation lexicon that produces fluent text in the target (plaintext) language. Previous work has shown that decipherment not only helps find translations for OOVs (Dou and Knight, 2012), but also improves translations of observed words (Dou and Knight, 2013). We find that previous work using monolingual or co"
D14-1061,P13-1154,0,0.177029,". . , Fnmono , . . . , Fmono , decipherment finds word-to-word translations that best describe the ciphertext. Knight et al. (2006) are the first to study several natural language decipherment problems with unsupervised learning. Since then, there has been increasing interest in improving decipherment techniques and its application to machine translation (Ravi and Knight, 2011; (2) In the next two sections, we describe the word alignment and decipherment models, and present how they are combined to perform joint optimization. 558 Dou and Knight, 2012; Nuhn et al., 2012; Dou and Knight, 2013; Nuhn et al., 2013). In order to speed up decipherment, Dou and Knight (2012) suggest that a frequency list of bigrams might contain enough information for decipherment. According to them, a monolingual ciphertext bigram Fmono is generated through the following generative story: • Generate a sequence of two plaintext tokens e1 e2 with probability P(e1 e2 ) given by a language model built from large numbers of plaintext bigrams. • Substitute e1 with f1 and e2 with f2 with probability t( f1 |e1 ) · t( f2 |e2 ). The probability of any cipher bigram F is: X P(Fmono ) = P(e1 e2 ) · t( f1 |e1 ) · t( f2 |e2 ) (4) e1 e2"
D14-1061,P08-1102,0,0.0723206,"Missing"
D14-1061,P03-1021,0,0.0059482,"gasy. 3 an online dictionary from malagasyword.org, as well as a lexicon learned from the parallel data To further expand our Malagasy training data, we 562 alignment in two directions and use grow-diag-finaland heuristic to obtain final alignment. During decoding, we use 8 standard features in Moses to score a candidate translation: direct and inverse translation probabilities, direct and inverse lexical weighting, a language model score, a distortion score, phrase penalty, and word penalty. The weights for the features are learned on the tuning data using minimum error rate training (MERT) (Och, 2003). To compare with previous decipherment approach to improve machine translation, we build a second baseline system. We follow the work by Dou and Knight (2013) to decipher Malagasy into English, and build a translation lexicon T decipher from decipherment. To improve machine translation, we simply use T decipher as an additional parallel corpus. First, we filter T decipher by keeping only translation pairs ( f, e), where f is observed in the Spanish part and e is observed in the English part of the parallel corpus. Then we append all the Spanish and English words in the filtered T decipher to"
D14-1061,E12-1014,0,0.0335517,"for many languages and domains, there is not enough parallel data to train a decent quality MT system. However, compared with parallel data, there are much larger amounts of non parallel data. The ability to learn a translation lexicon or even build a machine translation system using monolingual data helps address the problems of insufficient parallel data. Ravi and Knight (2011) are among the first to learn a full MT system using only non parallel data through decipherment. However, the performance of such systems is much lower compared with those trained with parallel data. In another work, Klementiev et al. (2012) show that, given a phrase table, it is possible to estimate parameters for a phrase-based MT system from non parallel data. Given that we often have some parallel data, it is more practical to improve a translation system trained on parallel data by using additional non parallel data. Rapp (1995) shows that with a seed lexicon, it is possible to induce new word level translations from non parallel data. Motivated by the idea that a translation lexicon induced from non parallel data can be used to translate out of vocabulary words (OOV), a variety of prior research has tried to build a transla"
D14-1061,P95-1050,0,0.452858,"ress the problems of insufficient parallel data. Ravi and Knight (2011) are among the first to learn a full MT system using only non parallel data through decipherment. However, the performance of such systems is much lower compared with those trained with parallel data. In another work, Klementiev et al. (2012) show that, given a phrase table, it is possible to estimate parameters for a phrase-based MT system from non parallel data. Given that we often have some parallel data, it is more practical to improve a translation system trained on parallel data by using additional non parallel data. Rapp (1995) shows that with a seed lexicon, it is possible to induce new word level translations from non parallel data. Motivated by the idea that a translation lexicon induced from non parallel data can be used to translate out of vocabulary words (OOV), a variety of prior research has tried to build a translation lexicon from non parallel or comparable data (Fung and Yee, 1998; Koehn and Knight, 2002; Haghighi et al., 2008; Garera Figure 1: Combine word alignment and decipherment into a single learning process. et al., 2009; Bergsma and Van Durme, 2011; Daum´e and Jagarlamudi, 2011; Irvine and Calliso"
D14-1061,P11-1002,1,0.887972,"f-the-art machine translation (MT) systems apply statistical techniques to learn translation rules automatically from parallel data. However, this reliance on parallel data seriously limits the scope of MT application in the real world, as for many languages and domains, there is not enough parallel data to train a decent quality MT system. However, compared with parallel data, there are much larger amounts of non parallel data. The ability to learn a translation lexicon or even build a machine translation system using monolingual data helps address the problems of insufficient parallel data. Ravi and Knight (2011) are among the first to learn a full MT system using only non parallel data through decipherment. However, the performance of such systems is much lower compared with those trained with parallel data. In another work, Klementiev et al. (2012) show that, given a phrase table, it is possible to estimate parameters for a phrase-based MT system from non parallel data. Given that we often have some parallel data, it is more practical to improve a translation system trained on parallel data by using additional non parallel data. Rapp (1995) shows that with a seed lexicon, it is possible to induce ne"
D14-1061,P06-2065,1,0.938931,"e advanced models for word alignment, such as Model 3 and Model 4, which use more parameters to describe the generative process. We do not go into details of those models here and the reader is referred to the paper describing them. Under the Model 1-2 and HMM alignment models, the probability of target sentence given source sentence is: A New Objective Function M X d(a j |a j−1 , j)t( f j |ea j ). j=1 Decipherment Given a corpus of N foreign text sequences (cipherN text), F1mono , . . . , Fnmono , . . . , Fmono , decipherment finds word-to-word translations that best describe the ciphertext. Knight et al. (2006) are the first to study several natural language decipherment problems with unsupervised learning. Since then, there has been increasing interest in improving decipherment techniques and its application to machine translation (Ravi and Knight, 2011; (2) In the next two sections, we describe the word alignment and decipherment models, and present how they are combined to perform joint optimization. 558 Dou and Knight, 2012; Nuhn et al., 2012; Dou and Knight, 2013; Nuhn et al., 2013). In order to speed up decipherment, Dou and Knight (2012) suggest that a frequency list of bigrams might contain"
D14-1061,P13-1036,0,0.129561,"counts for decipherment in the EM algorithm. 2.4 renormalize the translation table and distortion table to update parameters in the new M step. The E step for parallel part can be computed efficiently using the forward-backward algorithm (Vogel et al., 1996). However, as we pointed out in Section 2.3, the E step for the non parallel part has a time complexity of O(V 2 ) with the forward-backward algorithm, where V is the size of English vocabulary, and is usually very large. Previous work has tried to make decipherment scalable (Ravi and Knight, 2011; Dou and Knight, 2012; Nuhn et al., 2013; Ravi, 2013). However, all of them are designed for decipherment with either Bayesian inference or beam search. In contrast, we need an algorithm to make EM decipherment scalable. To overcome this problem, we modify the slice sampling (Neal, 2000) approach used by Dou and Knight (2012) to compute expected counts from non parallel data needed for the EM algorithm. 2.4.1 Draw Samples with Slice Sampling To start the sampling process, we initialize the first sample by performing approximate Viterbi decoding using results from the last EM iteration. For each foreign dependency bigram f1 , f2 , we find the top"
D14-1061,W02-0902,1,0.689544,"for a phrase-based MT system from non parallel data. Given that we often have some parallel data, it is more practical to improve a translation system trained on parallel data by using additional non parallel data. Rapp (1995) shows that with a seed lexicon, it is possible to induce new word level translations from non parallel data. Motivated by the idea that a translation lexicon induced from non parallel data can be used to translate out of vocabulary words (OOV), a variety of prior research has tried to build a translation lexicon from non parallel or comparable data (Fung and Yee, 1998; Koehn and Knight, 2002; Haghighi et al., 2008; Garera Figure 1: Combine word alignment and decipherment into a single learning process. et al., 2009; Bergsma and Van Durme, 2011; Daum´e and Jagarlamudi, 2011; Irvine and Callison-Burch, 2013b; Irvine and Callison-Burch, 2013a; Irvine et al., 2013). Lately, there has been increasing interest in learning translation lexicons from non parallel data with decipherment techniques (Ravi and Knight, 2011; Dou and Knight, 2012; Nuhn et al., 2012; Dou and Knight, 2013). Decipherment views one language as a cipher for another and learns a translation lexicon that produces flue"
D14-1061,P12-1033,1,0.890716,"Missing"
D14-1061,C96-2141,0,0.835398,"65, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics • We propose a new objective function for word alignment that combines the process of word alignment and decipherment into a single learning task. 2.2 Given a source sentence F = f1 , . . . , fj , . . . , fJ and a target sentence E = e1 , . . . , ei , . . . , eI , word alignment models describe the generative process employed to produce the French sentence from the English sentence through alignments a = a1 , . . . , aj , . . . , aJ . The IBM models 1-2 (Brown et al., 1993) and the HMM word alignment model (Vogel et al., 1996) use two sets of parameters, distortion probabilities and translation probabilities, to define the joint probability of a target sentence and alignment given a source sentence. • In experiments, we find that the joint process outperforms the previous pipeline approach, and observe Bleu gains of 0.9 and 2.1 on two different test sets. • We release 15.3 million tokens of monolingual Malagasy data from the web, as well as a small Malagasy dependency tree bank containing 20k tokens. 2 P(F, a |E) = Joint Word Alignment and Decipherment 2.1 Word Alignment J Y In previous work that uses monolingual d"
D14-1061,N01-1026,0,0.0835409,"ee no improvements in Bleu scores from running those models. We do word It is natural to think that creation of annotated data for training a POS tagger and a parser requires large amounts of efforts from annotators who understand the language well. However, we find that through the help of parallel data and dictionaries, we are able to create more annotated data by ourselves to improve tagging and parsing accuracy. This idea is inspired by previous work that tries to learn a semi-supervised parser by projecting dependency relations from one language (with good dependency parsers) to another (Yarowsky and Ngai, 2001; Ganchev et al., 2009). However, we find those automatic approaches do not work well for Malagasy. 3 an online dictionary from malagasyword.org, as well as a lexicon learned from the parallel data To further expand our Malagasy training data, we 562 alignment in two directions and use grow-diag-finaland heuristic to obtain final alignment. During decoding, we use 8 standard features in Moses to score a candidate translation: direct and inverse translation probabilities, direct and inverse lexical weighting, a language model score, a distortion score, phrase penalty, and word penalty. The weig"
D14-1061,J93-2004,0,0.0453077,"nt, and show that the joint learning process improves Bleu scores by up to 2.1 points over a phrase-based MT baseline. 4.1 English 4.3 Building A Dependency Parser for Malagasy Since Malagasy and English have very different word orders, we decide to apply dependency based decipherment for the two languages as suggested by Dou and Knight (2013). To extract dependency relations, we need to parse monolingual data in Malagasy and English. For English, there are already many good parsers available. In our experiments, we use Turbo parser (Martins et al., 2013) trained on the English Penn Treebank (Marcus et al., 1993) to parse all our English monolingual data. However, there is no existing good parser for Malagasy. The quality of a dependency parser depends on the amount of training data available. State-of-the-art English parsers are built from Penn Treebank, which contains over 1 million tokens of annotated syntactical The Malagasy Language Malagasy is the official language of Madagascar. It has around 18 million native speakers. Although Madagascar is an African country, Malagasy belongs to the Malayo-Polynesian branch of the Austronesian language family. Malagasy and English have very different word or"
D14-1061,P13-2109,0,0.039141,". In the end, we perform joint word alignment and decipherment, and show that the joint learning process improves Bleu scores by up to 2.1 points over a phrase-based MT baseline. 4.1 English 4.3 Building A Dependency Parser for Malagasy Since Malagasy and English have very different word orders, we decide to apply dependency based decipherment for the two languages as suggested by Dou and Knight (2013). To extract dependency relations, we need to parse monolingual data in Malagasy and English. For English, there are already many good parsers available. In our experiments, we use Turbo parser (Martins et al., 2013) trained on the English Penn Treebank (Marcus et al., 1993) to parse all our English monolingual data. However, there is no existing good parser for Malagasy. The quality of a dependency parser depends on the amount of training data available. State-of-the-art English parsers are built from Penn Treebank, which contains over 1 million tokens of annotated syntactical The Malagasy Language Malagasy is the official language of Madagascar. It has around 18 million native speakers. Although Madagascar is an African country, Malagasy belongs to the Malayo-Polynesian branch of the Austronesian langua"
D14-1061,P08-1101,0,0.0282499,"bank containing 20k tokens. 2 P(F, a |E) = Joint Word Alignment and Decipherment 2.1 Word Alignment J Y In previous work that uses monolingual data to improve machine translation, a seed translation lexicon learned from parallel data is used to find new translations through either word vector based approaches or decipherment. In return, selection of a seed lexicon needs to be careful as using a poor quality seed lexicon could hurt the downstream process. Evidence from a number of previous work shows that a joint inference process leads to better performance in both tasks (Jiang et al., 2008; Zhang and Clark, 2008). In the presence of parallel and monolingual data, we would like the alignment and decipherment models to benefit from each other. Since the decipherment and word alignment models contain word-to-word translation probabilities t( f |e), having them share these parameters during learning will allow us to pool information from both data types. This leads us to develop a new objective function that takes both learning processes into account. Given our parallel data, (E1 , F1 ), . . . , (Em , Fm ), . . . , (E M , F M ), and monolingual N data F1mono , . . . , Fnmono , . . . , Fmono , we seek to m"
D14-1061,P98-1069,0,\N,Missing
D14-1061,C98-1066,0,\N,Missing
D14-1061,P07-2045,0,\N,Missing
D14-1061,P09-1042,0,\N,Missing
D14-1061,P12-1017,0,\N,Missing
D14-1061,P08-1000,0,\N,Missing
D14-1185,P13-1154,1,0.841557,"Figure 3 lists these methods. We compare our work to the only previously published cipher type classifier for classical ciphers2 . This classifier is trained on 16, 800 ciphertexts and is implemented in javascript to run in the web browser: The user can provide the ciphertext as input to a web page that returns the classifier’s predictions. The source code of the classifier is available online. Our work includes a reimplementation of the features used in that classifier. As examples for work that deals with the automated decipherment of cipher texts, we point to (Ravi and Knight, 2011), and (Nuhn et al., 2013). These publications develop specialized algorithms for solving simple and homophonic substitution ciphers, which are just two out of the 56 cipher types defined by the ACA. We also want to mention (de Souza et al., 2013), which presents a cipher type classifier for the finalist algorithms of the Advanced Encryption Standard (AES) contest. 1 2 http://cryptogram.org/cipher_types.html See http://bionsgadgets.appspot.com/gadget_forms/ refscore_extended.html bionspot/cipher-id-tests and type encipher Figure 1: Example “CMBIFID” cipher: Text is grouped in five character chunks for readability. 2 ke"
D14-1185,P11-1025,1,0.844937,"llowing us to implement them. Figure 3 lists these methods. We compare our work to the only previously published cipher type classifier for classical ciphers2 . This classifier is trained on 16, 800 ciphertexts and is implemented in javascript to run in the web browser: The user can provide the ciphertext as input to a web page that returns the classifier’s predictions. The source code of the classifier is available online. Our work includes a reimplementation of the features used in that classifier. As examples for work that deals with the automated decipherment of cipher texts, we point to (Ravi and Knight, 2011), and (Nuhn et al., 2013). These publications develop specialized algorithms for solving simple and homophonic substitution ciphers, which are just two out of the 56 cipher types defined by the ACA. We also want to mention (de Souza et al., 2013), which presents a cipher type classifier for the finalist algorithms of the Advanced Encryption Standard (AES) contest. 1 2 http://cryptogram.org/cipher_types.html See http://bionsgadgets.appspot.com/gadget_forms/ refscore_extended.html bionspot/cipher-id-tests and type encipher Figure 1: Example “CMBIFID” cipher: Text is grouped in five character chu"
D15-1105,W98-1210,0,0.0916248,"Missing"
D15-1105,N03-1017,0,0.0342173,"Missing"
D15-1105,P07-2045,0,0.0157999,"Missing"
D15-1105,2005.mtsummit-papers.11,0,0.0616076,"e1. The contributions of this paper are: 1. We provide a new quantitative bound for how much information a translator adds to an original text. 2. We present practical software to compress bilingual text with compression rates that exceed the previous state-of-the-art. 3. We set up a public benchmark bilingual text compression challenge to stimulate new researchers to find and exploit patterns in bilingual text. Ultimately, we want to feed those ideas into practical machine translation systems. 2 Monolingual compression Data We propose the widely accessible Spanish/English Europarl corpus v7 (Koehn, 2005) as a benchmark for bilingual text compression (Figure 2). Portions of this large corpus have been used in previous compression work (S´anchez-Mart´ınez et al., 2012). The Spanish side is in UTF-8. For English, we have removed accent marks and further eliminated all but the 95 printable ASCII characters (Brown et al., 1992), plus newline. Our task is to compress the data “as is”: un1 File size has advantages, as perplexity computations are often buggy, and they usually gloss over how probability is apportioned to out-of-vocabulary items. 2 Or other symbols, such as words, bytes, or unicode seq"
D15-1105,N10-1062,0,0.0145992,"l text using EM (Brown et al., 1993). However, the t-table is still too large to prepend to the compressed English file. 5.3 1. Choose English length m w/prob (m|l) 2. For j = 1..m: Adaptive translation modeling 2a. set aj to null w/prob p1 , or Instead, inspired by PPM, we build up translation tables in RAM, during a single pass of our compressor. Our decompressor then rebuilds these same tables, in the same way, in order to interpret the compressed bit string. Neal and Hinton (1998) describe online EM, which updates probability tables after each training example. Liang and Klein (2009) and Levenberg et al. (2010) apply online EM to a number of language tasks, including word alignment. Here we concentrate on the single-pass case. We initialize a uniform translation model, use it to collect fractional counts from the first segment 2b. choose non-null aj w/prob (1 − p1 )o(aj − ak ) 3. For j = 1..m, choose translation ej w/prob t(ej |faj ) In the expression o(aj − ak ), k is the maximum English index (k &lt; j) such that ak 6= 0. The relative offset o-table learns to encourage adjacent English words to align to adjacent Spanish words. Batch HMM performs poorly under uniform initialization, with two causes of"
D15-1105,W13-1724,0,0.0329481,"Missing"
D15-1105,J92-1002,0,0.435777,"ompression challenge to stimulate new researchers to find and exploit patterns in bilingual text. Ultimately, we want to feed those ideas into practical machine translation systems. 2 Monolingual compression Data We propose the widely accessible Spanish/English Europarl corpus v7 (Koehn, 2005) as a benchmark for bilingual text compression (Figure 2). Portions of this large corpus have been used in previous compression work (S´anchez-Mart´ınez et al., 2012). The Spanish side is in UTF-8. For English, we have removed accent marks and further eliminated all but the 95 printable ASCII characters (Brown et al., 1992), plus newline. Our task is to compress the data “as is”: un1 File size has advantages, as perplexity computations are often buggy, and they usually gloss over how probability is apportioned to out-of-vocabulary items. 2 Or other symbols, such as words, bytes, or unicode sequences. 890 an escape (ESC), after which it executes a hard backoff to the (n-1)-gram table. In PPMA, P(ESC) is 1/(1+D), where D is the number of times the context has been seen. PPMB uses q/D, where q is the number of distinct character types seen in the context. PPMC uses q/(q+D), aka Witten-Bell. PPMD uses q/2D. PPM* use"
D15-1105,N09-1069,0,0.0111983,"t, a, and  on our bilingual text using EM (Brown et al., 1993). However, the t-table is still too large to prepend to the compressed English file. 5.3 1. Choose English length m w/prob (m|l) 2. For j = 1..m: Adaptive translation modeling 2a. set aj to null w/prob p1 , or Instead, inspired by PPM, we build up translation tables in RAM, during a single pass of our compressor. Our decompressor then rebuilds these same tables, in the same way, in order to interpret the compressed bit string. Neal and Hinton (1998) describe online EM, which updates probability tables after each training example. Liang and Klein (2009) and Levenberg et al. (2010) apply online EM to a number of language tasks, including word alignment. Here we concentrate on the single-pass case. We initialize a uniform translation model, use it to collect fractional counts from the first segment 2b. choose non-null aj w/prob (1 − p1 )o(aj − ak ) 3. For j = 1..m, choose translation ej w/prob t(ej |faj ) In the expression o(aj − ak ), k is the maximum English index (k &lt; j) such that ak 6= 0. The relative offset o-table learns to encourage adjacent English words to align to adjacent Spanish words. Batch HMM performs poorly under uniform initia"
D15-1105,J93-2003,0,0.171597,"st work at the word level instead of the character level. If we are predicting the jth English word, and we know that it translates fi (“aligns to fi ”), and if fi has only a handful of translations, then we may be able to specify ej with just a few bits. We may therefore suppose that a set of Viterbi word alignments may be useful for compression (Conley and Klein, 2008; S´anchezMart´ınez et al., 2012). We consider unidirectional alignments that link each target position j to a single source position i (including the null word at i = 0). Such alignments can be computed automatically using EM (Brown et al., 1993), and stored in one of two formats: Absolute: 1 2 5 5 7 0 3 6 . . . Relative: +1 +1 +3 0 +2 null -4 +3 . . . In order to interpret the bits produced by the compressor, our decompressor must also have access to the same Viterbi alignments. Therefore, we must include those alignments at the beginning of the compressed file. So let’s compress them too. How compressible are alignment sequences? Figure 5 gives results for Viterbi alignments derived from our large parallel Spanish/English corpus. First, some interesting facts: • Huffman works better on relative offsets, because the common “+1” gets"
D15-1105,W02-1020,0,0.0469889,"Missing"
D15-1105,P02-1040,0,0.0943242,"Missing"
D15-1105,W04-1104,0,0.0902698,"Missing"
D15-1105,C04-1175,0,0.0282097,"Missing"
D15-1105,J00-3004,0,0.224582,"Missing"
D15-1105,C96-2141,0,0.175161,") where |VE |is the size of the English vocabulary. We determine |VE |via a quick initial pass through the data, then include it at the top of our compressed file. In batch EM, we usually run IBM Model 1 for a few iterations before Model 2, gripped by an atavistic fear that the a probabilities will enforce rigid alignments before word co-occurrences have a chance to settle in. It turns out this fear is justified in online EM! Because the a table initially learns to align most words to null, we smooth it more heavily (λa = 102 , λt = 10−4 ). We also implement a single-pass HMM alignment model (Vogel et al., 1996). In the IBM models, we can either collect fractional counts after we have compressed a whole sentence, or we can do it word-by-word. In the HMM model, alignment choices are no longer independent of one another: Given f1 . . . fl : P(ej |f1 . . . fl , e1 . . . ej−1 ) ∼ P(ej |f1 . . . fl ) = P [1 − P(ST OP |j, l)] li=0 a(i|j, l)t(ej |fi ) We can train t, a, and  on our bilingual text using EM (Brown et al., 1993). However, the t-table is still too large to prepend to the compressed English file. 5.3 1. Choose English length m w/prob (m|l) 2. For j = 1..m: Adaptive translation modeling 2a. set"
D15-1136,P13-2009,0,0.017486,"ta that is much narrower and an order of magnitude smaller than that of AMR, primarily the Geoquery corpus (Zelle and Mooney, 1996). The WASP system of Wong and Mooney (2006) uses hierarchical SMT techniques and does not apply semanticspecific improvements; its extension (Wong and Mooney, 2007) incorporates a target-side reordering component much like the one presented in Section 4.4. Jones et al. (2012a) cast semantic parsing as an instance of hyperedge replacement grammar transduction; like this work they use an IBM model-influenced alignment algorithm and a GHKM-based extraction algorithm. Andreas et al. (2013) use phrase-based and hierarchical SMT techniques on Geoquery. Like this work, they perform a transformation of the input semantic representation so that it is amenable to use in an existing machine translation system. However, they are unable to reach the state of the art in performance. Li et al. (2013) directly address GHKM’s word-toterminal alignment requirement by extending that algorithm to handle word-to-node alignment. Our SBMT system is grounded in the theory of tree transducers, which were applied to the task of semantic parsing by Jones et al. (2011; 2012b). Semantic parsing in gene"
D15-1136,W13-2322,1,0.831018,"ng Representation Using Syntax-Based Machine Translation Michael Pust, Ulf Hermjakob, Kevin Knight, Daniel Marcu, Jonathan May Information Sciences Institute Computer Science Department University of Southern California {pust, ulf, knight, marcu, jonmay}@isi.edu Abstract polarit soldier 0 ARG die-01 The soldier was not afraid of dying. The soldier was not afraid to die. The soldier did not fear death. Figure 1: An Abstract Meaning Representation (AMR) with several English renderings. Introduction Abstract Meaning Representation (AMR) is a compact, readable, whole-sentence semantic annotation (Banarescu et al., 2013). It includes entity identification and typing, PropBank semantic roles (Kingsbury and Palmer, 2002), individual entities playing multiple roles, as well as treatments of modality, negation, etc. AMR abstracts in numerous ways, e.g., by assigning the same conceptual structure to fear (v), fear (n), and afraid (adj). Figure 1 gives an example. AMR parsing is a new research problem, with only a few papers published to date (Flanigan et al., 2014; Wang et al., 2015) and a publicly available corpus of more than 10,000 English/AMR pairs.1 New research problems can be tackled either by developing ne"
D15-1136,J93-2003,0,0.0479423,"training, development, and test splits specified in the AMR corpus (Table 1). The training set is used for rule extraction, language modeling, and statistical rule feature calculation. The development set is used both for parameter optimization and qualitatively for hill-climbing. The test set is held out blind for evaluation. We preprocess the English with a simple rule-based tokenizer and, except where noted, lowercase all data. We obtain English–AMR alignments by using the unsupervised alignment approach of Pourdamghani et al. (2014), which linearizes the AMR and then applies the models of Brown et al. (1993) with an additional symmetrization constraint. All parsing results reported in this work are obtained with the Smatch 1.0 software (Cai and Knight, 2013). We compare our results to those of Flanigan et al. (2014) on the AMR 1.0 data splits; we run that work’s JAMR software according to the provided instructions.3 We also compare our results to published scores in the recent work of Wang et al. (2015). Their work uses slightly different data than that used here 4 but in practice we have not seen significant variation in results. 3 https://github.com/jflanigan/jamr LDC2013E117, a pre-released ve"
D15-1136,P13-2131,1,0.81492,"tion that is suitable for string-to-tree SBMT rule extraction and decoding (Section 4.1). 2 LDC Catalog number 2014T12 See e.g. the related work section of Huck et al. (2014). 1143 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1143–1154, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. Target Nodes Edges Alignments SBMT tree labeled unlabeled words to leaves Children Accuracy Metric ordered B LEU (Papineni et al., 2002) AMR parsing graph unlabeled labeled words to leaves + words to edges unordered Smatch (Cai and Knight, 2013) Corpus Training Development Test Tokens 218,021 29,484 30,263 Table 1: Data splits of AMR 1.0, used in this work. Tokens are English, after tokenization. Figure 2: Differences between AMR parsing and syntax-based machine translation (SBMT). 2. Proposing a target-side reordering technique that leverages the fact that child nodes in AMR are unordered (Section 4.4). 3. Introducing a hierarchical AMR-specific language model to ensure generation of likely parent-child relationships (Section 5). 4. Integrating several semantic knowledge sources into the task (Section 6). 5. Developing tuning method"
D15-1136,A00-2018,0,0.379684,"Missing"
D15-1136,P96-1041,0,0.0575384,"e SBMT system proceeds as follows. Given a corpus of (source string, target tree, source-target word alignment) sentence translation training tuples and a corpus of (source, target, score) sentence translation tuning tuples: 1. Rule extraction: A grammar of string-totree rules is induced from training tuples using the GHKM algorithm (Galley et al., 2004; Galley et al., 2006). 2. Local feature calculation: Statistical and indicator features, as described by Chiang et al. (2009), are calculated over the rule grammar. 3. Language model calculation: A KneserNey-interpolated 5-gram language model (Chen and Goodman, 1996) is learned from the yield of the target training trees. 4. Decoding: A beamed bottom-up chart decoder (Pust and Knight, 2009; Hopkins and Langmead, 2010) calculates the optimal derivations given a source string and feature parameter set. 5. Tuning: Feature parameters are optimized using the MIRA learning approach (Chiang et al., 2009) to maximize the objective, typically B LEU (Papineni et al., 2002), associated with a tuning corpus. We initially use this system with no modifications and pretend that English–AMR is a language pair indistinct from any other. 3 Data and Comparisons We use Engli"
D15-1136,N09-1025,1,0.453031,"Missing"
D15-1136,P97-1003,0,0.0336152,"acceptable to GHKM, and hence an entire end-to-end AMR parser may now be built with SBMT tools, the resulting parser does not exhibit very good performance (Table 3, first line). The trees we are learning on are exceedingly flat, and thus yield rules that do not generalize sufficiently. Rules produced from the top of the tree in Figure 3c, such as that in Figure 4a, are only appropriate for cases where fear-01 has exactly three roles: ARG0 (agent), ARG1 (patient), and polarity. We follow the lead of Wang et al. (2010), who in turn were influenced by similar approaches in monolingual parsing (Collins, 1997; Charniak, 2000), and re-structure trees at nodes with more 5 In Figure 3 the negative polarity marker ‘-’ is a string. Disconnected referents labeled ‘*’ are treated as AMR instances with no roles. than three children (i.e. instances with more than one role), to allow generalization of flat structures. However, our trees are unlike syntactic constituent trees in that they do not have labeled nonterminal nodes, so we have no natural choice of an intermediate (“bar”) label. We must choose a meaningful label to characterize an instance and its roles. We initially choose the concept label, resul"
D15-1136,P05-1045,0,0.00345268,"semantic parsing, we have not yet discussed the use of any semantic resources. In this section we rectify that omission. 6.1 Rules from Numerical Quantities and Named Entities While the majority of string-to-tree rules in SBMT systems are extracted from aligned parallel data, it is common practice to dynamically generate additional rules to handle the translation of dates and numerical quantities, as these follow common patterns and are easily detected at decode-time. We follow this practice here, and additionally detect person names at decode-time using the Stanford Named Entity Recognizer (Finkel et al., 2005). We use cased, tokenized source data to build the decode-time rules. We add indicator features to these rules so that our tuning methods can decide how favorable the resources are. We leave as future work the incorporation of named-entity rules for other classes, since most available namedentity recognition beyond person names is at a granularity level that is incompatible with AMR (e.g. we can recognize ‘Location’ but not distinguish between ‘City’ and ‘Country’). 6.2 Hierarchical Semantic Categories fear-01 die-01 ARG1 * Figure 5: Final modification of the AMR data; semantically clustered p"
D15-1136,P14-1134,0,0.298361,"(AMR) with several English renderings. Introduction Abstract Meaning Representation (AMR) is a compact, readable, whole-sentence semantic annotation (Banarescu et al., 2013). It includes entity identification and typing, PropBank semantic roles (Kingsbury and Palmer, 2002), individual entities playing multiple roles, as well as treatments of modality, negation, etc. AMR abstracts in numerous ways, e.g., by assigning the same conceptual structure to fear (v), fear (n), and afraid (adj). Figure 1 gives an example. AMR parsing is a new research problem, with only a few papers published to date (Flanigan et al., 2014; Wang et al., 2015) and a publicly available corpus of more than 10,000 English/AMR pairs.1 New research problems can be tackled either by developing new algorithms/techniques (Flanigan et al., 2014; Wang et al., 2015) or by adapting existing algorithms/techniques to the problem at hand. In this paper, we investigate the second approach. The AMR parsing problem bears a strong formal resemblance to syntax-based machine translation (SBMT) of the string-to-tree variety, in that 1 ARG1 ins t t ins - 1 fear-01 y G 1 st in AR We present a parser for Abstract Meaning Representation (AMR). We treat E"
D15-1136,N04-1035,1,0.67464,"constitute lightweight changes on a baseline SBMT system, we achieve state-of-the-art AMR parsing results. We next describe our baseline, and then describe how we adapt it to AMR parsing. 2 Sentences 10,313 1,368 1,371 Syntax-Based Machine Translation Our baseline SBMT system proceeds as follows. Given a corpus of (source string, target tree, source-target word alignment) sentence translation training tuples and a corpus of (source, target, score) sentence translation tuning tuples: 1. Rule extraction: A grammar of string-totree rules is induced from training tuples using the GHKM algorithm (Galley et al., 2004; Galley et al., 2006). 2. Local feature calculation: Statistical and indicator features, as described by Chiang et al. (2009), are calculated over the rule grammar. 3. Language model calculation: A KneserNey-interpolated 5-gram language model (Chen and Goodman, 1996) is learned from the yield of the target training trees. 4. Decoding: A beamed bottom-up chart decoder (Pust and Knight, 2009; Hopkins and Langmead, 2010) calculates the optimal derivations given a source string and feature parameter set. 5. Tuning: Feature parameters are optimized using the MIRA learning approach (Chiang et al.,"
D15-1136,P06-1121,1,0.857425,"ht changes on a baseline SBMT system, we achieve state-of-the-art AMR parsing results. We next describe our baseline, and then describe how we adapt it to AMR parsing. 2 Sentences 10,313 1,368 1,371 Syntax-Based Machine Translation Our baseline SBMT system proceeds as follows. Given a corpus of (source string, target tree, source-target word alignment) sentence translation training tuples and a corpus of (source, target, score) sentence translation tuning tuples: 1. Rule extraction: A grammar of string-totree rules is induced from training tuples using the GHKM algorithm (Galley et al., 2004; Galley et al., 2006). 2. Local feature calculation: Statistical and indicator features, as described by Chiang et al. (2009), are calculated over the rule grammar. 3. Language model calculation: A KneserNey-interpolated 5-gram language model (Chen and Goodman, 1996) is learned from the yield of the target training trees. 4. Decoding: A beamed bottom-up chart decoder (Pust and Knight, 2009; Hopkins and Langmead, 2010) calculates the optimal derivations given a source string and feature parameter set. 5. Tuning: Feature parameters are optimized using the MIRA learning approach (Chiang et al., 2009) to maximize the"
D15-1136,J02-3001,0,0.0353382,"sting machine translation system. However, they are unable to reach the state of the art in performance. Li et al. (2013) directly address GHKM’s word-toterminal alignment requirement by extending that algorithm to handle word-to-node alignment. Our SBMT system is grounded in the theory of tree transducers, which were applied to the task of semantic parsing by Jones et al. (2011; 2012b). Semantic parsing in general and AMR parsing specifically can be considered a subsumption of many semantic resolution sub-tasks, e.g. named entity recognition (Nadeau and Sekine, 2007), semantic role labeling (Gildea and Jurafsky, 2002), word sense disambiguation (Navigli, 2009) and relation finding (Bach and Badaskar, 2007). 10 Conclusion By restructuring our AMRs we are able to convert a sophisticated SBMT engine into a baseline semantic parser with little additional effort. By further restructuring our data to appropriately model the behavior we want to capture, we are able to rapidly achieve state-of-the-art results. Finally, by incorporating novel language models and external semantic resources, we are able to increase quality even more. This is not the last word on AMR parsing, as fortunately, machine translation techn"
D15-1136,D10-1063,0,0.0114874,"nd a corpus of (source, target, score) sentence translation tuning tuples: 1. Rule extraction: A grammar of string-totree rules is induced from training tuples using the GHKM algorithm (Galley et al., 2004; Galley et al., 2006). 2. Local feature calculation: Statistical and indicator features, as described by Chiang et al. (2009), are calculated over the rule grammar. 3. Language model calculation: A KneserNey-interpolated 5-gram language model (Chen and Goodman, 1996) is learned from the yield of the target training trees. 4. Decoding: A beamed bottom-up chart decoder (Pust and Knight, 2009; Hopkins and Langmead, 2010) calculates the optimal derivations given a source string and feature parameter set. 5. Tuning: Feature parameters are optimized using the MIRA learning approach (Chiang et al., 2009) to maximize the objective, typically B LEU (Papineni et al., 2002), associated with a tuning corpus. We initially use this system with no modifications and pretend that English–AMR is a language pair indistinct from any other. 3 Data and Comparisons We use English–AMR data from the AMR 1.0 corpus, LDC Catalog number 2014T12. In contrast to narrow-domain data sources that are often used in work related to semantic"
D15-1136,W14-3362,0,0.0118261,"re widely available, anyone wishing to generate AMR from text need only follow our recipe and retrain an existing framework with relevant data to quickly obtain state-of-the-art results. Since SBMT and AMR parsing are, in fact, distinct tasks, as outlined in Figure 2, to adapt the SBMT parsing framework to AMR parsing, we develop novel representations and techniques. Some of our key ideas include: 1. Introducing an AMR-equivalent representation that is suitable for string-to-tree SBMT rule extraction and decoding (Section 4.1). 2 LDC Catalog number 2014T12 See e.g. the related work section of Huck et al. (2014). 1143 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1143–1154, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. Target Nodes Edges Alignments SBMT tree labeled unlabeled words to leaves Children Accuracy Metric ordered B LEU (Papineni et al., 2002) AMR parsing graph unlabeled labeled words to leaves + words to edges unordered Smatch (Cai and Knight, 2013) Corpus Training Development Test Tokens 218,021 29,484 30,263 Table 1: Data splits of AMR 1.0, used in this work. Tokens are English, after tokenization"
D15-1136,U11-1005,0,0.0154014,"KM-based extraction algorithm. Andreas et al. (2013) use phrase-based and hierarchical SMT techniques on Geoquery. Like this work, they perform a transformation of the input semantic representation so that it is amenable to use in an existing machine translation system. However, they are unable to reach the state of the art in performance. Li et al. (2013) directly address GHKM’s word-toterminal alignment requirement by extending that algorithm to handle word-to-node alignment. Our SBMT system is grounded in the theory of tree transducers, which were applied to the task of semantic parsing by Jones et al. (2011; 2012b). Semantic parsing in general and AMR parsing specifically can be considered a subsumption of many semantic resolution sub-tasks, e.g. named entity recognition (Nadeau and Sekine, 2007), semantic role labeling (Gildea and Jurafsky, 2002), word sense disambiguation (Navigli, 2009) and relation finding (Bach and Badaskar, 2007). 10 Conclusion By restructuring our AMRs we are able to convert a sophisticated SBMT engine into a baseline semantic parser with little additional effort. By further restructuring our data to appropriately model the behavior we want to capture, we are able to rapi"
D15-1136,C12-1083,1,0.768754,"Missing"
D15-1136,H90-1020,0,0.139969,"s the optimal derivations given a source string and feature parameter set. 5. Tuning: Feature parameters are optimized using the MIRA learning approach (Chiang et al., 2009) to maximize the objective, typically B LEU (Papineni et al., 2002), associated with a tuning corpus. We initially use this system with no modifications and pretend that English–AMR is a language pair indistinct from any other. 3 Data and Comparisons We use English–AMR data from the AMR 1.0 corpus, LDC Catalog number 2014T12. In contrast to narrow-domain data sources that are often used in work related to semantic parsing (Price, 1990; Zelle, 1995; Kuhlmann et al., 2004), the AMR corpus covers a broad range of news and web forum data. We use the training, development, and test splits specified in the AMR corpus (Table 1). The training set is used for rule extraction, language modeling, and statistical rule feature calculation. The development set is used both for parameter optimization and qualitatively for hill-climbing. The test set is held out blind for evaluation. We preprocess the English with a simple rule-based tokenizer and, except where noted, lowercase all data. We obtain English–AMR alignments by using the unsup"
D15-1136,P12-1051,0,0.0106212,"parsing task by modifying training data and adding lightweight AMRspecific features. Several other recent works have used a machine translation approach to semantic parsing, but all have been applied to domain data that is much narrower and an order of magnitude smaller than that of AMR, primarily the Geoquery corpus (Zelle and Mooney, 1996). The WASP system of Wong and Mooney (2006) uses hierarchical SMT techniques and does not apply semanticspecific improvements; its extension (Wong and Mooney, 2007) incorporates a target-side reordering component much like the one presented in Section 4.4. Jones et al. (2012a) cast semantic parsing as an instance of hyperedge replacement grammar transduction; like this work they use an IBM model-influenced alignment algorithm and a GHKM-based extraction algorithm. Andreas et al. (2013) use phrase-based and hierarchical SMT techniques on Geoquery. Like this work, they perform a transformation of the input semantic representation so that it is amenable to use in an existing machine translation system. However, they are unable to reach the state of the art in performance. Li et al. (2013) directly address GHKM’s word-toterminal alignment requirement by extending tha"
D15-1136,N09-2036,1,0.774328,"ation training tuples and a corpus of (source, target, score) sentence translation tuning tuples: 1. Rule extraction: A grammar of string-totree rules is induced from training tuples using the GHKM algorithm (Galley et al., 2004; Galley et al., 2006). 2. Local feature calculation: Statistical and indicator features, as described by Chiang et al. (2009), are calculated over the rule grammar. 3. Language model calculation: A KneserNey-interpolated 5-gram language model (Chen and Goodman, 1996) is learned from the yield of the target training trees. 4. Decoding: A beamed bottom-up chart decoder (Pust and Knight, 2009; Hopkins and Langmead, 2010) calculates the optimal derivations given a source string and feature parameter set. 5. Tuning: Feature parameters are optimized using the MIRA learning approach (Chiang et al., 2009) to maximize the objective, typically B LEU (Papineni et al., 2002), associated with a tuning corpus. We initially use this system with no modifications and pretend that English–AMR is a language pair indistinct from any other. 3 Data and Comparisons We use English–AMR data from the AMR 1.0 corpus, LDC Catalog number 2014T12. In contrast to narrow-domain data sources that are often use"
D15-1136,kingsbury-palmer-2002-treebank,0,0.217756,"ht, Daniel Marcu, Jonathan May Information Sciences Institute Computer Science Department University of Southern California {pust, ulf, knight, marcu, jonmay}@isi.edu Abstract polarit soldier 0 ARG die-01 The soldier was not afraid of dying. The soldier was not afraid to die. The soldier did not fear death. Figure 1: An Abstract Meaning Representation (AMR) with several English renderings. Introduction Abstract Meaning Representation (AMR) is a compact, readable, whole-sentence semantic annotation (Banarescu et al., 2013). It includes entity identification and typing, PropBank semantic roles (Kingsbury and Palmer, 2002), individual entities playing multiple roles, as well as treatments of modality, negation, etc. AMR abstracts in numerous ways, e.g., by assigning the same conceptual structure to fear (v), fear (n), and afraid (adj). Figure 1 gives an example. AMR parsing is a new research problem, with only a few papers published to date (Flanigan et al., 2014; Wang et al., 2015) and a publicly available corpus of more than 10,000 English/AMR pairs.1 New research problems can be tackled either by developing new algorithms/techniques (Flanigan et al., 2014; Wang et al., 2015) or by adapting existing algorithm"
D15-1136,D07-1038,1,0.835975,"Missing"
D15-1136,P02-1040,0,0.0972587,"vel representations and techniques. Some of our key ideas include: 1. Introducing an AMR-equivalent representation that is suitable for string-to-tree SBMT rule extraction and decoding (Section 4.1). 2 LDC Catalog number 2014T12 See e.g. the related work section of Huck et al. (2014). 1143 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1143–1154, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. Target Nodes Edges Alignments SBMT tree labeled unlabeled words to leaves Children Accuracy Metric ordered B LEU (Papineni et al., 2002) AMR parsing graph unlabeled labeled words to leaves + words to edges unordered Smatch (Cai and Knight, 2013) Corpus Training Development Test Tokens 218,021 29,484 30,263 Table 1: Data splits of AMR 1.0, used in this work. Tokens are English, after tokenization. Figure 2: Differences between AMR parsing and syntax-based machine translation (SBMT). 2. Proposing a target-side reordering technique that leverages the fact that child nodes in AMR are unordered (Section 4.4). 3. Introducing a hierarchical AMR-specific language model to ensure generation of likely parent-child relationships (Section"
D15-1136,D14-1048,1,0.817579,", 2004), the AMR corpus covers a broad range of news and web forum data. We use the training, development, and test splits specified in the AMR corpus (Table 1). The training set is used for rule extraction, language modeling, and statistical rule feature calculation. The development set is used both for parameter optimization and qualitatively for hill-climbing. The test set is held out blind for evaluation. We preprocess the English with a simple rule-based tokenizer and, except where noted, lowercase all data. We obtain English–AMR alignments by using the unsupervised alignment approach of Pourdamghani et al. (2014), which linearizes the AMR and then applies the models of Brown et al. (1993) with an additional symmetrization constraint. All parsing results reported in this work are obtained with the Smatch 1.0 software (Cai and Knight, 2013). We compare our results to those of Flanigan et al. (2014) on the AMR 1.0 data splits; we run that work’s JAMR software according to the provided instructions.3 We also compare our results to published scores in the recent work of Wang et al. (2015). Their work uses slightly different data than that used here 4 but in practice we have not seen significant variation i"
D15-1136,J10-2004,1,0.846218,"m this SBMT-compliant rewrite. 4.2 Tree Restructuring While the transformation in Figure 3c is acceptable to GHKM, and hence an entire end-to-end AMR parser may now be built with SBMT tools, the resulting parser does not exhibit very good performance (Table 3, first line). The trees we are learning on are exceedingly flat, and thus yield rules that do not generalize sufficiently. Rules produced from the top of the tree in Figure 3c, such as that in Figure 4a, are only appropriate for cases where fear-01 has exactly three roles: ARG0 (agent), ARG1 (patient), and polarity. We follow the lead of Wang et al. (2010), who in turn were influenced by similar approaches in monolingual parsing (Collins, 1997; Charniak, 2000), and re-structure trees at nodes with more 5 In Figure 3 the negative polarity marker ‘-’ is a string. Disconnected referents labeled ‘*’ are treated as AMR instances with no roles. than three children (i.e. instances with more than one role), to allow generalization of flat structures. However, our trees are unlike syntactic constituent trees in that they do not have labeled nonterminal nodes, so we have no natural choice of an intermediate (“bar”) label. We must choose a meaningful labe"
D15-1136,N15-1040,0,0.404884,"lish renderings. Introduction Abstract Meaning Representation (AMR) is a compact, readable, whole-sentence semantic annotation (Banarescu et al., 2013). It includes entity identification and typing, PropBank semantic roles (Kingsbury and Palmer, 2002), individual entities playing multiple roles, as well as treatments of modality, negation, etc. AMR abstracts in numerous ways, e.g., by assigning the same conceptual structure to fear (v), fear (n), and afraid (adj). Figure 1 gives an example. AMR parsing is a new research problem, with only a few papers published to date (Flanigan et al., 2014; Wang et al., 2015) and a publicly available corpus of more than 10,000 English/AMR pairs.1 New research problems can be tackled either by developing new algorithms/techniques (Flanigan et al., 2014; Wang et al., 2015) or by adapting existing algorithms/techniques to the problem at hand. In this paper, we investigate the second approach. The AMR parsing problem bears a strong formal resemblance to syntax-based machine translation (SBMT) of the string-to-tree variety, in that 1 ARG1 ins t t ins - 1 fear-01 y G 1 st in AR We present a parser for Abstract Meaning Representation (AMR). We treat Englishto-AMR convers"
D15-1136,N10-1000,0,0.0447018,"Missing"
D15-1136,P15-1095,0,0.503983,"Missing"
D15-1136,N06-1056,0,0.0542131,"labels and restructures to resolve discrepancies between dependency standards and AMR’s specification. In contrast to these works, we use a single-pass approach and re-use existing machine translation architecture, adapting to the AMR parsing task by modifying training data and adding lightweight AMRspecific features. Several other recent works have used a machine translation approach to semantic parsing, but all have been applied to domain data that is much narrower and an order of magnitude smaller than that of AMR, primarily the Geoquery corpus (Zelle and Mooney, 1996). The WASP system of Wong and Mooney (2006) uses hierarchical SMT techniques and does not apply semanticspecific improvements; its extension (Wong and Mooney, 2007) incorporates a target-side reordering component much like the one presented in Section 4.4. Jones et al. (2012a) cast semantic parsing as an instance of hyperedge replacement grammar transduction; like this work they use an IBM model-influenced alignment algorithm and a GHKM-based extraction algorithm. Andreas et al. (2013) use phrase-based and hierarchical SMT techniques on Geoquery. Like this work, they perform a transformation of the input semantic representation so that"
D15-1136,P07-1121,0,0.024563,"ese works, we use a single-pass approach and re-use existing machine translation architecture, adapting to the AMR parsing task by modifying training data and adding lightweight AMRspecific features. Several other recent works have used a machine translation approach to semantic parsing, but all have been applied to domain data that is much narrower and an order of magnitude smaller than that of AMR, primarily the Geoquery corpus (Zelle and Mooney, 1996). The WASP system of Wong and Mooney (2006) uses hierarchical SMT techniques and does not apply semanticspecific improvements; its extension (Wong and Mooney, 2007) incorporates a target-side reordering component much like the one presented in Section 4.4. Jones et al. (2012a) cast semantic parsing as an instance of hyperedge replacement grammar transduction; like this work they use an IBM model-influenced alignment algorithm and a GHKM-based extraction algorithm. Andreas et al. (2013) use phrase-based and hierarchical SMT techniques on Geoquery. Like this work, they perform a transformation of the input semantic representation so that it is amenable to use in an existing machine translation system. However, they are unable to reach the state of the art"
D15-1136,N03-1031,0,\N,Missing
D16-1126,W13-3503,0,0.0236824,"r hypernyms in WordNet (Miller, 1995). For example, while Banerjee and Pedersen (2003) use WordNet to assign a 1.0 similarity score between car and automobile, they only give a 0.3 similarity between car and gasoline. A second method is to use pointwise mutual information (PMI). Let t be the topic/phrase, and let w be a candidate related word. We collect a set of sentences S that contain t, and sort candidates by Proportion of sentences in S containing w P(w) in general text Table 2 shows that PMI has a tendency to assign a high score to low frequency words (Bouma, 2009; Role and Nadif, 2011; Damani, 2013). 1185 A third method is word2vec (Mikolov et al., 2013a), which provides distributed word representations. We train a continuous-bag-of-words model3 with window size 8 and 40 and word vector dimension 200. We score candidate related words/phrases with cosine to topic-word vector. We find that a larger window size works best (Pennington et al., 2014; Levy and Goldberg, 2014). Table 2 shows examples. The training corpus for word2vec has a crucial effect on the quality of the related words. We train word2vec models on the English Gigaword corpus,4 a song lyrics corpus, and the first billion char"
D16-1126,N15-1180,1,0.838641,"e machinery with deep learning, guaranteeing formal correctness of our poems, while gaining coherence of long1184 distance RNNs. • By using words related to the user’s topic as rhyme words, we design a system that can generate poems with topical coherence. This allows us to generate longer topical poems. • We extend our method to other poetry formats and languages. 3 Vocabulary To generate a line of iambic pentameter poetry, we arrange words to form a sequence of ten syllables alternating between stressed and unstressed. For example: 010 1 0 10 101 Attending on his golden pilgramage Following Ghazvininejad and Knight (2015), we refer to unstressed syllables with 0 and stressed syllables with 1, so that the form of a Shakespearean sonnet is ((01)5 )14 . To get stress patterns for individual words, we use CMU pronunciation dictionary,2 collapsing primary and secondary stresses. For example: CAFETERIA K AE2 F AH0 T IH1 R IY0 AH0 becomes CAFETERIA 10100 The first two columns of Table 1 show other examples. From the 125,074 CMU dictionary word types, we can actually only use words whose stress pattern matches the iambic pattern (alternating 1s and 0s). However, we make an exception for words that end in ...100 (such"
D16-1126,D10-1051,1,0.956494,"d Chun, 2008; Jiang and Zhou, 2008; Netzer et al., 2009). Recent work attempts to solve this problem by applying grammatical and semantic templates (Oliveira, 2009; Oliveira, 2012), or by modeling the task as statistical machine translation, in which each line is a “translation” of the previous line (Zhou et al., 2009; He et al., 2012). Yan et al. (2013) proposes a method based on summarization techniques for poem generation, retrieving candidate sentences from a large corpus of poems based on a user’s query and clustering the constituent terms, summarizing each cluster into a line of a poem. Greene et al. (2010) use unsupervised learning to estimate the stress patterns of words in a poetry corpus, then use these in a finite-state network to generate short English love poems. Several deep learning methods have recently been proposed for generating poems. Zhang and Lapata (2014) use an RNN model to generate 4-line Chinese poems. They force the decoder to rhyme the second and fourth lines, trusting the RNN to control rhythm. Yi et al. (2016) also propose an attentionbased bidirectional RNN model for generating 4line Chinese poems. The only such work which tries to generate longer poems is from Wang et a"
D16-1126,C08-1048,0,0.296685,"tions 3-7 describe how we address these tasks. After this, we show results of Hafez generating 14line classical sonnets with rhyme scheme ABAB CDCD EFEF GG, written in iambic pentameter (ten syllables per line with alternating stress: “da-DUM da-DUM da-DUM . . . ”). We then show experiments on Hafez’s parameters and conclude by showing the generality of the approach with respect to language and poetic form. 2 Prior Work Automated poem generation has been a popular but challenging research topic (Manurung et al., 2000; Gervas, 2001; Diaz-Agudo et al., 2002; Manurung, 2003; Wong and Chun, 2008; Jiang and Zhou, 2008; Netzer et al., 2009). Recent work attempts to solve this problem by applying grammatical and semantic templates (Oliveira, 2009; Oliveira, 2012), or by modeling the task as statistical machine translation, in which each line is a “translation” of the previous line (Zhou et al., 2009; He et al., 2012). Yan et al. (2013) proposes a method based on summarization techniques for poem generation, retrieving candidate sentences from a large corpus of poems based on a user’s query and clustering the constituent terms, summarizing each cluster into a line of a poem. Greene et al. (2010) use unsupervi"
D16-1126,P14-2050,0,0.0319958,"sentences S that contain t, and sort candidates by Proportion of sentences in S containing w P(w) in general text Table 2 shows that PMI has a tendency to assign a high score to low frequency words (Bouma, 2009; Role and Nadif, 2011; Damani, 2013). 1185 A third method is word2vec (Mikolov et al., 2013a), which provides distributed word representations. We train a continuous-bag-of-words model3 with window size 8 and 40 and word vector dimension 200. We score candidate related words/phrases with cosine to topic-word vector. We find that a larger window size works best (Pennington et al., 2014; Levy and Goldberg, 2014). Table 2 shows examples. The training corpus for word2vec has a crucial effect on the quality of the related words. We train word2vec models on the English Gigaword corpus,4 a song lyrics corpus, and the first billion characters from Wikipedia.5 The Gigaword corpus produces related words that are too newsy, while the song lyrics corpus does not cover enough topics. Hence, we train on Wikipedia. To obtain related phrases as well as words, we apply the method of Mikolov et al. (2013b) to the Wikipedia corpus, which replaces collocations like Los Angeles with single tokens like Los Angeles. Word"
D16-1126,W09-2005,0,0.0271403,"we address these tasks. After this, we show results of Hafez generating 14line classical sonnets with rhyme scheme ABAB CDCD EFEF GG, written in iambic pentameter (ten syllables per line with alternating stress: “da-DUM da-DUM da-DUM . . . ”). We then show experiments on Hafez’s parameters and conclude by showing the generality of the approach with respect to language and poetic form. 2 Prior Work Automated poem generation has been a popular but challenging research topic (Manurung et al., 2000; Gervas, 2001; Diaz-Agudo et al., 2002; Manurung, 2003; Wong and Chun, 2008; Jiang and Zhou, 2008; Netzer et al., 2009). Recent work attempts to solve this problem by applying grammatical and semantic templates (Oliveira, 2009; Oliveira, 2012), or by modeling the task as statistical machine translation, in which each line is a “translation” of the previous line (Zhou et al., 2009; He et al., 2012). Yan et al. (2013) proposes a method based on summarization techniques for poem generation, retrieving candidate sentences from a large corpus of poems based on a user’s query and clustering the constituent terms, summarizing each cluster into a line of a poem. Greene et al. (2010) use unsupervised learning to estima"
D16-1126,D14-1162,0,0.114004,"ord. We collect a set of sentences S that contain t, and sort candidates by Proportion of sentences in S containing w P(w) in general text Table 2 shows that PMI has a tendency to assign a high score to low frequency words (Bouma, 2009; Role and Nadif, 2011; Damani, 2013). 1185 A third method is word2vec (Mikolov et al., 2013a), which provides distributed word representations. We train a continuous-bag-of-words model3 with window size 8 and 40 and word vector dimension 200. We score candidate related words/phrases with cosine to topic-word vector. We find that a larger window size works best (Pennington et al., 2014; Levy and Goldberg, 2014). Table 2 shows examples. The training corpus for word2vec has a crucial effect on the quality of the related words. We train word2vec models on the English Gigaword corpus,4 a song lyrics corpus, and the first billion characters from Wikipedia.5 The Gigaword corpus produces related words that are too newsy, while the song lyrics corpus does not cover enough topics. Hence, we train on Wikipedia. To obtain related phrases as well as words, we apply the method of Mikolov et al. (2013b) to the Wikipedia corpus, which replaces collocations like Los Angeles with single tok"
D16-1126,D14-1074,0,0.551377,"translation” of the previous line (Zhou et al., 2009; He et al., 2012). Yan et al. (2013) proposes a method based on summarization techniques for poem generation, retrieving candidate sentences from a large corpus of poems based on a user’s query and clustering the constituent terms, summarizing each cluster into a line of a poem. Greene et al. (2010) use unsupervised learning to estimate the stress patterns of words in a poetry corpus, then use these in a finite-state network to generate short English love poems. Several deep learning methods have recently been proposed for generating poems. Zhang and Lapata (2014) use an RNN model to generate 4-line Chinese poems. They force the decoder to rhyme the second and fourth lines, trusting the RNN to control rhythm. Yi et al. (2016) also propose an attentionbased bidirectional RNN model for generating 4line Chinese poems. The only such work which tries to generate longer poems is from Wang et al. (2016), who use an attention-based LSTM model for generation iambic poems. They train on a small dataset and do not use an explicit system for constraining rhythm and rhyme in the poem. Novel contributions of our work are: • We combine finite-state machinery with dee"
D16-1126,Y09-1006,0,0.0624571,"nts on Hafez’s parameters and conclude by showing the generality of the approach with respect to language and poetic form. 2 Prior Work Automated poem generation has been a popular but challenging research topic (Manurung et al., 2000; Gervas, 2001; Diaz-Agudo et al., 2002; Manurung, 2003; Wong and Chun, 2008; Jiang and Zhou, 2008; Netzer et al., 2009). Recent work attempts to solve this problem by applying grammatical and semantic templates (Oliveira, 2009; Oliveira, 2012), or by modeling the task as statistical machine translation, in which each line is a “translation” of the previous line (Zhou et al., 2009; He et al., 2012). Yan et al. (2013) proposes a method based on summarization techniques for poem generation, retrieving candidate sentences from a large corpus of poems based on a user’s query and clustering the constituent terms, summarizing each cluster into a line of a poem. Greene et al. (2010) use unsupervised learning to estimate the stress patterns of words in a poetry corpus, then use these in a finite-state network to generate short English love poems. Several deep learning methods have recently been proposed for generating poems. Zhang and Lapata (2014) use an RNN model to generate"
D16-1159,P05-1022,0,0.0188937,"er CJ Parser E2P (Vinyals et al., 2015) WSJ 23 F1-score 92.1 89.6 90.5 # valid trees (out of 2416) 2416 2362 unk Table 3: Labeled F1-scores of different parsers on WSJ Section 23. The F1-score is calculated on valid trees only. Figure 1: Sample inputs and outputs of the E2E, PE2PE, E2F, E2G, and E2P models. (Pradhan and Xue, 2009) and the English Web Treebank (Petrov and McDonald, 2012). In addition to these gold treebanks, we take 4M English sentences from English-German data and 4M English sentences from English-French data, and we parse these 8M sentences with the Charniak-Johnson parser1 (Charniak and Johnson, 2005). We call these 8,162K pairs the CJ corpus. We use WSJ Section 22 as our development set and section 23 as the test set, where we obtain an F1-score of 89.6, competitive with the previously-published 90.5 (Table 4). Model Architecture. For all experiments2 , we use a two-layer encoder-decoder with long short-term memory (LSTM) units (Hochreiter and Schmidhuber, 1997). We use a minibatch of 128, a hidden state size of 1000, and a dropout rate of 0.2. 1 The CJ parser is here https://github.com/BLLIP/bllipparser and we used the pretrained model ”WSJ+Gigaword-v2”. 2 We use the toolkit: https://git"
D16-1159,W06-1628,0,0.155724,"Missing"
D16-1159,D07-1079,1,0.687944,"two methods to detect whether the encoder has learned local and global source syntax. A fine-grained analysis of the syntactic structure learned by the encoder reveals which kinds of syntax are learned and which are missing. 1 Introduction The sequence to sequence model (seq2seq) has been successfully applied to neural machine translation (NMT) (Sutskever et al., 2014; Cho et al., 2014) and can match or surpass MT state-of-art. Nonneural machine translation systems consist chiefly of phrase-based systems (Koehn et al., 2003) and syntax-based systems (Galley et al., 2004; Galley et al., 2006; DeNeefe et al., 2007; Liu et al., 2011; Cowan et al., 2006), the latter of which adds syntactic information to source side (tree-to-string), target side (string-to-tree) or both sides (tree-to-tree). As the seq2seq model first encodes the source sentence into a high-dimensional vector, then decodes into a target sentence, it is hard to understand and interpret what is going on inside such a procedure. Considering the evolution of non-neural translation systems, it is natural to ask: 1. Does the encoder learn syntactic information about the source sentence? 2. What kind of syntactic information is learned, and how"
D16-1159,N04-1035,1,0.174993,"de as a by-product of training. We propose two methods to detect whether the encoder has learned local and global source syntax. A fine-grained analysis of the syntactic structure learned by the encoder reveals which kinds of syntax are learned and which are missing. 1 Introduction The sequence to sequence model (seq2seq) has been successfully applied to neural machine translation (NMT) (Sutskever et al., 2014; Cho et al., 2014) and can match or surpass MT state-of-art. Nonneural machine translation systems consist chiefly of phrase-based systems (Koehn et al., 2003) and syntax-based systems (Galley et al., 2004; Galley et al., 2006; DeNeefe et al., 2007; Liu et al., 2011; Cowan et al., 2006), the latter of which adds syntactic information to source side (tree-to-string), target side (string-to-tree) or both sides (tree-to-tree). As the seq2seq model first encodes the source sentence into a high-dimensional vector, then decodes into a target sentence, it is hard to understand and interpret what is going on inside such a procedure. Considering the evolution of non-neural translation systems, it is natural to ask: 1. Does the encoder learn syntactic information about the source sentence? 2. What kind o"
D16-1159,P06-1121,1,0.154733,"training. We propose two methods to detect whether the encoder has learned local and global source syntax. A fine-grained analysis of the syntactic structure learned by the encoder reveals which kinds of syntax are learned and which are missing. 1 Introduction The sequence to sequence model (seq2seq) has been successfully applied to neural machine translation (NMT) (Sutskever et al., 2014; Cho et al., 2014) and can match or surpass MT state-of-art. Nonneural machine translation systems consist chiefly of phrase-based systems (Koehn et al., 2003) and syntax-based systems (Galley et al., 2004; Galley et al., 2006; DeNeefe et al., 2007; Liu et al., 2011; Cowan et al., 2006), the latter of which adds syntactic information to source side (tree-to-string), target side (string-to-tree) or both sides (tree-to-tree). As the seq2seq model first encodes the source sentence into a high-dimensional vector, then decodes into a target sentence, it is hard to understand and interpret what is going on inside such a procedure. Considering the evolution of non-neural translation systems, it is natural to ask: 1. Does the encoder learn syntactic information about the source sentence? 2. What kind of syntactic informati"
D16-1159,P14-1062,0,0.0041351,"endencies, such as line lengths and quotes. They also conduct an error analysis of the predictions. Li et al. (2016) explore the syntactic behavior of an RNN-based sentiment analyzer, including the compositionality of negation, intensification, and concessive clauses, by plotting a 60-dimensional heat map of hidden unit values. They also introduce a first-order derivative based method to measure each unit’s contribution to the final decision. Verifying syntactic/semantic properties. Several works try to build a good distributional representation of sentences or paragraph (Socher et al., 2013; Kalchbrenner et al., 2014; Kim, 2014; Zhao et al., 2015; Le and Mikolov, 2014; Kiros et al., 2015). They implicitly verify the claimed syntactic/semantic properties of learned representations by applying them to downstream classification tasks 1527 such as sentiment analysis, sentence classification, semantic relatedness, paraphrase detection, imagesentence ranking, question-type classification, etc. Novel contributions of our work include: • We locate a subset of activation cells that are responsible for certain syntactic labels. We explore the concentration and layer distribution of different syntactic labels. • We"
D16-1159,D14-1181,0,0.00451412,"gths and quotes. They also conduct an error analysis of the predictions. Li et al. (2016) explore the syntactic behavior of an RNN-based sentiment analyzer, including the compositionality of negation, intensification, and concessive clauses, by plotting a 60-dimensional heat map of hidden unit values. They also introduce a first-order derivative based method to measure each unit’s contribution to the final decision. Verifying syntactic/semantic properties. Several works try to build a good distributional representation of sentences or paragraph (Socher et al., 2013; Kalchbrenner et al., 2014; Kim, 2014; Zhao et al., 2015; Le and Mikolov, 2014; Kiros et al., 2015). They implicitly verify the claimed syntactic/semantic properties of learned representations by applying them to downstream classification tasks 1527 such as sentiment analysis, sentence classification, semantic relatedness, paraphrase detection, imagesentence ranking, question-type classification, etc. Novel contributions of our work include: • We locate a subset of activation cells that are responsible for certain syntactic labels. We explore the concentration and layer distribution of different syntactic labels. • We extract who"
D16-1159,N03-1017,0,0.0443415,"learns syntactic information on the source side as a by-product of training. We propose two methods to detect whether the encoder has learned local and global source syntax. A fine-grained analysis of the syntactic structure learned by the encoder reveals which kinds of syntax are learned and which are missing. 1 Introduction The sequence to sequence model (seq2seq) has been successfully applied to neural machine translation (NMT) (Sutskever et al., 2014; Cho et al., 2014) and can match or surpass MT state-of-art. Nonneural machine translation systems consist chiefly of phrase-based systems (Koehn et al., 2003) and syntax-based systems (Galley et al., 2004; Galley et al., 2006; DeNeefe et al., 2007; Liu et al., 2011; Cowan et al., 2006), the latter of which adds syntactic information to source side (tree-to-string), target side (string-to-tree) or both sides (tree-to-tree). As the seq2seq model first encodes the source sentence into a high-dimensional vector, then decodes into a target sentence, it is hard to understand and interpret what is going on inside such a procedure. Considering the evolution of non-neural translation systems, it is natural to ask: 1. Does the encoder learn syntactic informa"
D16-1159,D12-1096,0,0.0221144,"ed), using the CJ corpus, the same corpus used to train E2P . Figure 4 shows how we construct model E2F2P from model E2F. For fine-tuning, we use the same dropout rate and learning rate updating configuration for E2P as described in Section 4. 6.2 Evaluation We train four new neural parsers using the encoders of the two auto-encoders and the two NMT models respectively. We use three tools to evaluate and analyze: 1. The EVALB tool3 to calculate the labeled bracketing F1-score. 2. The zxx package4 to calculate Tree edit distance (TED) (Zhang and Shasha, 1989). 3. The Berkeley Parser Analyser5 (Kummerfeld et al., 2012) to analyze parsing error types. The linearized parse trees generated by these neural parsers are not always well-formed. They can be split into the following categories: • Malformed trees: The linearized sequence can not be converted back into a tree, due to missing or mismatched brackets. • Well-formed trees: The sequence can be converted back into a tree. Tree edit distance can be calculated on this category. 3 http://nlp.cs.nyu.edu/evalb/ https://github.com/timtadh/zhang-shasha 5 https://github.com/jkkummerfeld/berkeley-parser-analyser 4 1531 – Wrong length trees: The number of tree leaves"
D16-1159,P13-2018,0,0.0101337,"r Analyzer (Kummerfeld et al., 2012) to gain a more linguistic understanding of predicted parses. Like TED, the Berkeley Parser Analyzer is based on tree transformation. It repairs the parse tree via a sequence of sub-tree movements, node insertions and deletions. During this process, multiple bracket errors are fixed, and it associates this group of node errors with a linguistically meaningful error type. The first column of Figure 5 shows the average number of bracket errors per sentence for model E2P on the intersection set. For other models, we report the ratio of each model to model E2P. Kummerfeld et al. (2013) and Kummerfeld et al. (2012) give descriptions of different error types. The NMT-based predicted parses introduce around twice the bracketing errors for the first 10 error types, whereas for “Sense Confusion”, they bring more than 16 times bracket errors. “Sense confusion” is the case where the head word of a phrase receives the wrong POS, E2P (Ave. Bracket Err) Co-ordination VP Attach PP Attach Verb taking wrong arguments Modifier Attach Unary NP Internal Noun boundary error Different label Single Word Phrase Sense Confusion 0.081 0.024 0.242 0.035 0.205 0.123 0.053 0.022 0.137 0.150 0.057 E"
D16-1159,N16-1082,0,0.0168365,"use the fixedlength encoding vector for other purposes. 3 Related work Interpreting Recurrent Neural Networks. The most popular method to visualize high-dimensional vectors, such as word embeddings, is to project them into two-dimensional space using t-SNE (van der Maaten and Hinton, 2008). Very few works try to interpret recurrent neural networks in NLP. Karpathy et al. (2016) use a character-level LSTM language model as a test-bed and find several activation cells that track long-distance dependencies, such as line lengths and quotes. They also conduct an error analysis of the predictions. Li et al. (2016) explore the syntactic behavior of an RNN-based sentiment analyzer, including the compositionality of negation, intensification, and concessive clauses, by plotting a 60-dimensional heat map of hidden unit values. They also introduce a first-order derivative based method to measure each unit’s contribution to the final decision. Verifying syntactic/semantic properties. Several works try to build a good distributional representation of sentences or paragraph (Socher et al., 2013; Kalchbrenner et al., 2014; Kim, 2014; Zhao et al., 2015; Le and Mikolov, 2014; Kiros et al., 2015). They implicitly"
D16-1159,P11-1128,0,0.049239,"Missing"
D16-1159,J93-2004,0,0.0553192,"Missing"
D16-1159,P02-1040,0,0.121109,"Missing"
D16-1159,N09-4006,0,0.00920115,"K 200K 200K Output vocabulary size 40K 40K 40K 40K Train/Dev/Test Corpora Sizes (sentence pairs) 4M/3000/2737 4M/3000/2737 4M/6003/3003 4M/3000/2737 200K 121 8162K/1700/2416 BLEU 89.11 88.84 24.59 12.60 n/a Table 2: Model settings and test-set BLEU-n4r1 scores (Papineni et al., 2002). Parser CJ Parser E2P (Vinyals et al., 2015) WSJ 23 F1-score 92.1 89.6 90.5 # valid trees (out of 2416) 2416 2362 unk Table 3: Labeled F1-scores of different parsers on WSJ Section 23. The F1-score is calculated on valid trees only. Figure 1: Sample inputs and outputs of the E2E, PE2PE, E2F, E2G, and E2P models. (Pradhan and Xue, 2009) and the English Web Treebank (Petrov and McDonald, 2012). In addition to these gold treebanks, we take 4M English sentences from English-German data and 4M English sentences from English-French data, and we parse these 8M sentences with the Charniak-Johnson parser1 (Charniak and Johnson, 2005). We call these 8,162K pairs the CJ corpus. We use WSJ Section 22 as our development set and section 23 as the test set, where we obtain an F1-score of 89.6, competitive with the previously-published 90.5 (Table 4). Model Architecture. For all experiments2 , we use a two-layer encoder-decoder with long s"
D16-1159,D13-1170,0,0.0021895,"ack long-distance dependencies, such as line lengths and quotes. They also conduct an error analysis of the predictions. Li et al. (2016) explore the syntactic behavior of an RNN-based sentiment analyzer, including the compositionality of negation, intensification, and concessive clauses, by plotting a 60-dimensional heat map of hidden unit values. They also introduce a first-order derivative based method to measure each unit’s contribution to the final decision. Verifying syntactic/semantic properties. Several works try to build a good distributional representation of sentences or paragraph (Socher et al., 2013; Kalchbrenner et al., 2014; Kim, 2014; Zhao et al., 2015; Le and Mikolov, 2014; Kiros et al., 2015). They implicitly verify the claimed syntactic/semantic properties of learned representations by applying them to downstream classification tasks 1527 such as sentiment analysis, sentence classification, semantic relatedness, paraphrase detection, imagesentence ranking, question-type classification, etc. Novel contributions of our work include: • We locate a subset of activation cells that are responsible for certain syntactic labels. We explore the concentration and layer distribution of differ"
D16-1163,W15-3001,0,0.0163026,"ent models is 0.5 with a decay rate of 0.9 when the development perplexity does not improve. The child models are all trained for 100 epochs. We re-scale the gradient when the gradient norm of all parameters is greater than 5. The initial parameter range is [-0.08, +0.08]. We also initialize our forget-gate biases to 1 as specified by J´ozefowicz et al. (2015) and Gers et al. (2000). For decoding we use a beam search of width 12. 4.1 Transfer Results The results for our transfer learning method applied to the four languages above are in Table 2. The parent models were trained on the WMT 2015 (Bojar et al., 2015) French–English corpus for 5 epochs. Our baseline NMT systems (‘NMT’ row) all receive a large B LEU improvement when using the transfer method (the ‘Xfer’ row) with an average B LEU improvement of 5.6. Additionally, when we use unknown word replacement from Luong et al. (2015b) and ensemble together 8 models (the ‘Final’ row) we further improve upon our B LEU scores, bringing the average B LEU improvement to 7.5. Overall our method allows the NMT system to reach competitive scores and outperform the SBMT system in one of the four language pairs. Figure 2: Our NMT model architecture, showing si"
D16-1163,1997.mtsummit-plenaries.5,0,0.630738,"Missing"
D16-1163,N09-1025,1,0.225735,"ores along with the NMT baselines that do not use transfer. There is a large gap between the SBMT and NMT systems when our transfer method is not used. The SBMT system used in this paper is a stringto-tree statistical machine translation system (Galley et al., 2006; Galley et al., 2004). In this system there are two count-based 5-gram language models. One is trained on the English side of the WMT 2015 English–French dataset and the other is trained on the English side of the low-resource bitext. Additionally, the SBMT models use thousands of sparsely-occurring, lexicalized syntactic features (Chiang et al., 2009). For our NMT system, we use development sets for Hausa, Turkish, Uzbek, and Urdu to tune the learn1571 ing rate, parameter initialization range, dropout rate, and hidden state size for all the experiments. For training we use a minibatch size of 128, hidden state size of 1000, a target vocabulary size of 15K, and a source vocabulary size of 30K. The child models are trained with a dropout probability of 0.5, as in Zaremba et al. (2014). The common parent model is trained with a dropout probability of 0.2. The learning rate used for both child and parent models is 0.5 with a decay rate of 0.9"
D16-1163,P15-1166,0,0.261996,"analysis (Wang and Zheng, 2015). Deep learning models discover multiple levels of representation, some of which may be useful across tasks, which makes them particularly suited to transfer learning (Bengio, 2012). For example, Cires¸an et al. (2012) use a convolutional neural network to recognize handwritten characters and show positive effects of transfer between models for Latin and Chinese characters. Ours is the first study to apply transfer learning to neural machine translation. There has also been work on using data from multiple language pairs in NMT to improve performance. Recently, Dong et al. (2015) showed that sharing a source encoder for one language helps performance when using different target decoders Decoder NMT Xfer Final SBMT Hausa 16.8 21.3 24.0 23.7 Turkish 11.4 17.0 18.7 20.4 Uzbek 10.7 14.4 16.8 17.9 Urdu 5.2 13.8 14.5 17.9 Re-scorer Table 2: Our method significantly improves NMT results for None NMT Xfer LM Hausa 23.7 24.5 24.8 23.6 SBMT Decoder Turkish Uzbek 20.4 17.9 21.4 19.5 21.8 19.5 21.1 17.9 Urdu 17.9 18.2 19.1 18.2 the translation of low-resource languages into English. Results Table 3: Our transfer method applied to re-scoring output nshow test-set B LEU scores. The"
D16-1163,N16-1101,0,0.284449,"irst row shows the out transfer, and the ‘Xfer’ row shows results with transfer. The SBMT performance with no re-scoring and the other 3 rows ‘Final’ row shows B LEU after we ensemble 8 models and use show the performance after re-scoring with the selected model. unknown word replacement. Note: the ‘LM’ row shows the results when an RNN LM trained on the large English corpus was used to re-score. for different languages. In that paper the authors showed that using this framework improves performance for low-resource languages by incorporating a mix of low-resource and high-resource languages. Firat et al. (2016) used a similar approach, employing a separate encoder for each source language, a separate decoder for each target language, and a shared attention mechanism across all languages. They then trained these components jointly across multiple different language pairs to show improvements in a lower-resource setting. There are a few key differences between our work and theirs. One is that we are working with truly small amounts of training data. Dong et al. (2015) used a training corpus of about 8m English words for the low-resource experiments, and Firat et al. (2016) used from 2m to 4m words, wh"
D16-1163,N04-1035,1,0.184111,"g translation knowledge from parallel text. NMT systems have achieved competitive accuracy rates under large-data training conditions for language pairs This work was carried out while all authors were at USC’s Information Sciences Institute. *This author is currently at Google Brain. such as English–French. However, neural methods are data-hungry and learn poorly from low-count events. This behavior makes vanilla NMT a poor choice for low-resource languages, where parallel data is scarce. Table 1 shows that for 4 low-resource languages, a standard string-to-tree statistical MT system (SBMT) (Galley et al., 2004; Galley et al., 2006) strongly outperforms NMT, even when NMT uses the state-of-the-art local attention plus feedinput techniques from Luong et al. (2015a). In this paper, we describe a method for substantially improving NMT results on these languages. Our key idea is to first train a high-resource language pair, then use the resulting trained network (the parent model) to initialize and constrain training for our low-resource language pair (the child model). We find that we can optimize our results by fixing certain parameters of the parent model and letting the rest be fine-tuned by the chi"
D16-1163,P06-1121,1,0.286724,"ge from parallel text. NMT systems have achieved competitive accuracy rates under large-data training conditions for language pairs This work was carried out while all authors were at USC’s Information Sciences Institute. *This author is currently at Google Brain. such as English–French. However, neural methods are data-hungry and learn poorly from low-count events. This behavior makes vanilla NMT a poor choice for low-resource languages, where parallel data is scarce. Table 1 shows that for 4 low-resource languages, a standard string-to-tree statistical MT system (SBMT) (Galley et al., 2004; Galley et al., 2006) strongly outperforms NMT, even when NMT uses the state-of-the-art local attention plus feedinput techniques from Luong et al. (2015a). In this paper, we describe a method for substantially improving NMT results on these languages. Our key idea is to first train a high-resource language pair, then use the resulting trained network (the parent model) to initialize and constrain training for our low-resource language pair (the child model). We find that we can optimize our results by fixing certain parameters of the parent model and letting the rest be fine-tuned by the child model. We report NM"
D16-1163,N06-1014,0,0.0428891,"d the EngPerm–English learns to un-permute scrambled English sentences. The LM is a 2-layer LSTM RNN language model. Dictionary Initialization Using the transfer method, we always initialize input language embeddings for the child model with randomly-assigned embeddings from the parent (which has a different input language). A smarter method might be to initialize child embeddings with similar parent embeddings, where similarity is measured by word-to-word t-table probabilities. To get these probabilities we compose Uzbek–English and English–French t-tables obtained from the Berkeley Aligner (Liang et al., 2006). We see from Figure 4 that this dictionary-based assignment results in faster improvement in the early part of the training. However the final performance is similar to our standard model, indicating that the training is able to untangle the dictionary permutation introduced by randomly-assigned embeddings. 5.6 Transfer Model None French–English Parent English–English Parent EngPerm–English Parent LM Xfer Different Parent Models In the above experiments, we use a parent model trained on a large French–English corpus. One might hypothesize that our gains come from exploit1574 ing the English h"
D16-1163,D15-1166,0,0.858229,"is work was carried out while all authors were at USC’s Information Sciences Institute. *This author is currently at Google Brain. such as English–French. However, neural methods are data-hungry and learn poorly from low-count events. This behavior makes vanilla NMT a poor choice for low-resource languages, where parallel data is scarce. Table 1 shows that for 4 low-resource languages, a standard string-to-tree statistical MT system (SBMT) (Galley et al., 2004; Galley et al., 2006) strongly outperforms NMT, even when NMT uses the state-of-the-art local attention plus feedinput techniques from Luong et al. (2015a). In this paper, we describe a method for substantially improving NMT results on these languages. Our key idea is to first train a high-resource language pair, then use the resulting trained network (the parent model) to initialize and constrain training for our low-resource language pair (the child model). We find that we can optimize our results by fixing certain parameters of the parent model and letting the rest be fine-tuned by the child model. We report NMT improvements from transfer learning of 5.6 B LEU on average, and we provide an analysis of why the method works. The final NMT sys"
D16-1163,P15-1002,0,0.348052,"is work was carried out while all authors were at USC’s Information Sciences Institute. *This author is currently at Google Brain. such as English–French. However, neural methods are data-hungry and learn poorly from low-count events. This behavior makes vanilla NMT a poor choice for low-resource languages, where parallel data is scarce. Table 1 shows that for 4 low-resource languages, a standard string-to-tree statistical MT system (SBMT) (Galley et al., 2004; Galley et al., 2006) strongly outperforms NMT, even when NMT uses the state-of-the-art local attention plus feedinput techniques from Luong et al. (2015a). In this paper, we describe a method for substantially improving NMT results on these languages. Our key idea is to first train a high-resource language pair, then use the resulting trained network (the parent model) to initialize and constrain training for our low-resource language pair (the child model). We find that we can optimize our results by fixing certain parameters of the parent model and letting the rest be fine-tuned by the child model. We report NMT improvements from transfer learning of 5.6 B LEU on average, and we provide an analysis of why the method works. The final NMT sys"
D16-1163,D13-1140,0,0.0533372,"sfer learning helps French0 (13.3→20.0) more than it helps Uzbek (10.7→15.0). 4.2 5 Re-scoring Results We also use the NMT model with transfer learning as a feature when re-scoring output n-best lists (n = 1000) from the SBMT system. Table 3 shows the results of re-scoring. We compare re-scoring with transfer NMT to re-scoring with baseline (i.e. non-transfer) NMT and to re-scoring with a neural language model. The neural language model is an LSTM RNN with 2 layers and 1000 hidden states. It has a target vocabulary of 100K and is trained using noise-contrastive estimation (Mnih and Teh, 2012; Vaswani et al., 2013; Baltescu and Blunsom, 2015; Williams et al., 2015). Additionally, it is trained using dropout with a dropout probability of 0.2 as suggested by Zaremba et al. (2014). Re-scoring with the transfer NMT model yields an improvement of 1.1– 1.6 B LEU points above the strong SBMT system; we find that transfer NMT is a better re-scoring feature than baseline NMT or neural language models. In the next section, we describe a number of additional experiments designed to help us understand the contribution of the various components of our transfer model. 1572 Analysis We analyze the effects of using di"
D16-1163,2020.wmt-1.63,0,\N,Missing
D16-1163,2020.coling-tutorials.3,0,\N,Missing
D16-1248,J93-2003,0,0.0983254,"ht length. When we evaluate the system on previously unseen test data, using BLEU (Papineni et al., 2002), we consistently find the length ratio between MT outputs and human references translations to be very close to 1.0. Thus, no brevity penalty is incurred. This behavior seems to come for free, without special design. By contrast, builders of standard statistical MT (SMT) systems must work hard to ensure correct length. The original mechanism comes from the IBM SMT group, whose famous Models 1-5 included a learned table (y|x), with x and y being the lengths of source and target sentences (Brown et al., 1993). But they did not deploy this table when decoding a foreign sentence f into an English sentence e; it did not participate in incremental scoring and pruning of candidate translations. As a result (Brown et al., 1995): “However, for a given f, if the goal is to discover the most probable e, then the product P(e) P(f|e) is too small for long English strings as compared with short ones. As a result, short English strings are improperly favored over longer English strings. This tendency is counteracted in part by the following modification: Replace P(f|e) with clength(e) · P(f|e) for some empiric"
D16-1248,1997.mtsummit-plenaries.5,0,0.364389,"Missing"
D16-1248,D15-1166,0,0.0106307,"evin Knight1 , and Deniz Yuret2 Information Sciences Institute & Computer Science Department University of Southern California {xingshi,knight}@isi.edu 2 Computer Engineering, Koc¸ University dyuret@ku.edu.tr Abstract We investigate how neural, encoder-decoder translation systems output target strings of appropriate lengths, finding that a collection of hidden units learns to explicitly implement this functionality. 1 Introduction The neural encoder-decoder framework for machine translation (Neco and Forcada, 1997; Casta˜no and Casacuberta, 1997; Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015) provides new tools for addressing the field’s difficult challenges. In this framework (Figure 1), we use a recurrent neural network (encoder) to convert a source sentence into a dense, fixed-length vector. We then use another recurrent network (decoder) to convert that vector into a target sentence. In this paper, we train long shortterm memory (LSTM) neural units (Hochreiter and Schmidhuber, 1997) trained with back-propagation through time (Werbos, 1990). A remarkable feature of this simple neural MT (NMT) model is that it produces translations of the right length. When we evaluate the syste"
D16-1248,P03-1021,0,0.0481073,"is to discover the most probable e, then the product P(e) P(f|e) is too small for long English strings as compared with short ones. As a result, short English strings are improperly favored over longer English strings. This tendency is counteracted in part by the following modification: Replace P(f|e) with clength(e) · P(f|e) for some empirically chosen constant c. This modification is treatment of the symptom rather than treatment of the disease itself, but it offers some temporary relief. The cure lies in better modeling.” More temporary relief came from Minimum Error-Rate Training (MERT) (Och, 2003), which automatically sets c to maximize BLEU score. MERT also sets weights for the language model P(e), translation model P(f|e), and other features. The length feature combines so sensitively with other features that MERT frequently returns to it as it revises one weight at a time. NMT’s ability to correctly model length is remarkable for these reasons: • SMT relies on maximum BLEU training to obtain a length ratio that is prized by BLEU, while NMT obtains the same result through generic maximum likelihood training. • Standard SMT models explicitly “cross off” 2278 Proceedings of the 2016 Co"
D16-1248,P02-1040,0,0.120643,"difficult challenges. In this framework (Figure 1), we use a recurrent neural network (encoder) to convert a source sentence into a dense, fixed-length vector. We then use another recurrent network (decoder) to convert that vector into a target sentence. In this paper, we train long shortterm memory (LSTM) neural units (Hochreiter and Schmidhuber, 1997) trained with back-propagation through time (Werbos, 1990). A remarkable feature of this simple neural MT (NMT) model is that it produces translations of the right length. When we evaluate the system on previously unseen test data, using BLEU (Papineni et al., 2002), we consistently find the length ratio between MT outputs and human references translations to be very close to 1.0. Thus, no brevity penalty is incurred. This behavior seems to come for free, without special design. By contrast, builders of standard statistical MT (SMT) systems must work hard to ensure correct length. The original mechanism comes from the IBM SMT group, whose famous Models 1-5 included a learned table (y|x), with x and y being the lengths of source and target sentences (Brown et al., 1993). But they did not deploy this table when decoding a foreign sentence f into an Englis"
D17-1266,N16-2017,0,0.0238912,"ne or more higher-resource, closely Related Languages (RL). Examples of such IL/RL pairs are Afrikaans/Dutch and Bosnian/Serbian. A natural idea is to use RL resources to improve the task for IL. But this requires some kind of conversion between RL and IL. Assume the required NLP capability is named entity tagging. If we can convert RL to IL, we can convert all RL training data along with annotations into IL and train the tagger for IL. Or, if we can convert IL to RL we can use the potentially existing RL named entity tagger on converted IL data and project back the tags. Following this idea, Currey et al. (2016) use a rule-based translation system to convert Italian and Portuguese into Spanish, to improve Spanish (here, IL) language modeling, Nakov and Ng (2009) convert RL/English parallel data to IL/English where both RL and IL have Latin orthography to improve IL/English machine translation. Hana et al. (2006) use cognates to adapt Spanish resources to Brazilian Portuguese to train a part-of-speech tagger. Mann and Yarowsky (2001) use Spanish/Portuguese cognates to convert an English/Spanish lexicon to English/Portuguese. These works prove the usefulness of RL data to improve NLP for IL, but they a"
D17-1266,D12-1025,1,0.505584,"n (Nakov and Tiedemann, 2012) between related languages. Use of non-parallel data: Cognates can be extracted from monolingual data and used as a parallel lexicon (Hana et al., 2006; Mann and Yarowsky, 2001; Kondrak et al., 2003). However, our task is whole-text transformation, not just cognate extraction. Unsupervised deciphering methods, which require no parallel data, have been used for bilingual lexicon extraction and machine translation. Word-based deciphering systems ignore sub-word similarities between related languages (Koehn and Knight, 2002; Ravi and Knight, 2011b; Nuhn et al., 2012; Dou and Knight, 2012; Ravi, 2013). Haghighi et al. (2008) and Naim and Gildea (2015) propose models that can use orthographic similarities. However, the model proposed by (Naim and Gildea, 2015) is only capable of producing a parallel lexicon and not translation. Furthermore, both systems require the languages to have the same orthography and their vocabulary is limited to what they see during training. Character-based decipherment is the model we use for solving this problem. Character-based decipherment has been previously applied to problems like solving letter substitution ciphers (Knight et al., 2006; Ravi a"
D17-1266,goldhahn-etal-2012-building,0,0.00528954,"nough score the model outputs the corresponding IL token. Otherwise it outputs the highest scored character sequence produced by the cipher model as and OOV. In our experiments we use 1-gram and 2-gram language models trained on all the existing IL monolingual data (Table 1). 4 Data We collect data for six pairs of related languages: Afrikaans(afr) / Dutch(dut), Bosnian(bos) / Serbian(srb), Danish(dan) / Swedish(swe), Macedonian(mkd) / Bulgarian(bul), Malaysian(mal) / Indonesian(ind), and Polish(pol) / Belorussian(bel). For each language, we download the monolingual data from Leipzig corpora (Goldhahn et al., 2012). The domain of the data is news, web, and Wikipedia. We consider the language with more data as RL and the one with less data as IL. Table 1 shows the size of available data for each language. We also extract the list of alphabets for each language from Wikipedia, and collect the Universal Declaration of Human Rights (UDHR) for each IL and RL. We manually sentence align these documents and get 104 sentences and about 1.5K tokens per language. We use these documents for testing the conversion accuracy. We tokenize and lowercase all the monolingual, parallel and UDHR data with Moses scripts. We"
D17-1266,P08-1088,0,0.0563625,"en related languages. Use of non-parallel data: Cognates can be extracted from monolingual data and used as a parallel lexicon (Hana et al., 2006; Mann and Yarowsky, 2001; Kondrak et al., 2003). However, our task is whole-text transformation, not just cognate extraction. Unsupervised deciphering methods, which require no parallel data, have been used for bilingual lexicon extraction and machine translation. Word-based deciphering systems ignore sub-word similarities between related languages (Koehn and Knight, 2002; Ravi and Knight, 2011b; Nuhn et al., 2012; Dou and Knight, 2012; Ravi, 2013). Haghighi et al. (2008) and Naim and Gildea (2015) propose models that can use orthographic similarities. However, the model proposed by (Naim and Gildea, 2015) is only capable of producing a parallel lexicon and not translation. Furthermore, both systems require the languages to have the same orthography and their vocabulary is limited to what they see during training. Character-based decipherment is the model we use for solving this problem. Character-based decipherment has been previously applied to problems like solving letter substitution ciphers (Knight et al., 2006; Ravi and Knight, 2011a) or transliterating"
D17-1266,D09-1141,0,0.0214534,"RL resources to improve the task for IL. But this requires some kind of conversion between RL and IL. Assume the required NLP capability is named entity tagging. If we can convert RL to IL, we can convert all RL training data along with annotations into IL and train the tagger for IL. Or, if we can convert IL to RL we can use the potentially existing RL named entity tagger on converted IL data and project back the tags. Following this idea, Currey et al. (2016) use a rule-based translation system to convert Italian and Portuguese into Spanish, to improve Spanish (here, IL) language modeling, Nakov and Ng (2009) convert RL/English parallel data to IL/English where both RL and IL have Latin orthography to improve IL/English machine translation. Hana et al. (2006) use cognates to adapt Spanish resources to Brazilian Portuguese to train a part-of-speech tagger. Mann and Yarowsky (2001) use Spanish/Portuguese cognates to convert an English/Spanish lexicon to English/Portuguese. These works prove the usefulness of RL data to improve NLP for IL, but they are designed for specific tasks and IL/RL pairs. In this paper we propose a universal method for translating texts between closely related languages. We a"
D17-1266,P12-2059,0,0.0197622,"nslating back the Romanized version of languages like Greeklish to Greek (Chalamandaris et al., 2006) and Arabizi to Arabic (May et al., 2014). These methods cannot be applied to our problem because time and resources are limited to build a translation system for the specific language pair. Machine learning systems that use parallel data: These methods cover a broader range of languages but require parallel text between related languages. They include character-level machine translation (Vilar et al., 2007; Tiedemann, 2009) or combination of word-level and character-level machine translation (Nakov and Tiedemann, 2012) between related languages. Use of non-parallel data: Cognates can be extracted from monolingual data and used as a parallel lexicon (Hana et al., 2006; Mann and Yarowsky, 2001; Kondrak et al., 2003). However, our task is whole-text transformation, not just cognate extraction. Unsupervised deciphering methods, which require no parallel data, have been used for bilingual lexicon extraction and machine translation. Word-based deciphering systems ignore sub-word similarities between related languages (Koehn and Knight, 2002; Ravi and Knight, 2011b; Nuhn et al., 2012; Dou and Knight, 2012; Ravi, 2"
D17-1266,P12-1017,0,0.0190837,"machine translation (Nakov and Tiedemann, 2012) between related languages. Use of non-parallel data: Cognates can be extracted from monolingual data and used as a parallel lexicon (Hana et al., 2006; Mann and Yarowsky, 2001; Kondrak et al., 2003). However, our task is whole-text transformation, not just cognate extraction. Unsupervised deciphering methods, which require no parallel data, have been used for bilingual lexicon extraction and machine translation. Word-based deciphering systems ignore sub-word similarities between related languages (Koehn and Knight, 2002; Ravi and Knight, 2011b; Nuhn et al., 2012; Dou and Knight, 2012; Ravi, 2013). Haghighi et al. (2008) and Naim and Gildea (2015) propose models that can use orthographic similarities. However, the model proposed by (Naim and Gildea, 2015) is only capable of producing a parallel lexicon and not translation. Furthermore, both systems require the languages to have the same orthography and their vocabulary is limited to what they see during training. Character-based decipherment is the model we use for solving this problem. Character-based decipherment has been previously applied to problems like solving letter substitution ciphers (Knigh"
D17-1266,P13-1036,0,0.0157416,", 2012) between related languages. Use of non-parallel data: Cognates can be extracted from monolingual data and used as a parallel lexicon (Hana et al., 2006; Mann and Yarowsky, 2001; Kondrak et al., 2003). However, our task is whole-text transformation, not just cognate extraction. Unsupervised deciphering methods, which require no parallel data, have been used for bilingual lexicon extraction and machine translation. Word-based deciphering systems ignore sub-word similarities between related languages (Koehn and Knight, 2002; Ravi and Knight, 2011b; Nuhn et al., 2012; Dou and Knight, 2012; Ravi, 2013). Haghighi et al. (2008) and Naim and Gildea (2015) propose models that can use orthographic similarities. However, the model proposed by (Naim and Gildea, 2015) is only capable of producing a parallel lexicon and not translation. Furthermore, both systems require the languages to have the same orthography and their vocabulary is limited to what they see during training. Character-based decipherment is the model we use for solving this problem. Character-based decipherment has been previously applied to problems like solving letter substitution ciphers (Knight et al., 2006; Ravi and Knight, 20"
D17-1266,N09-1005,1,0.763307,"ose models that can use orthographic similarities. However, the model proposed by (Naim and Gildea, 2015) is only capable of producing a parallel lexicon and not translation. Furthermore, both systems require the languages to have the same orthography and their vocabulary is limited to what they see during training. Character-based decipherment is the model we use for solving this problem. Character-based decipherment has been previously applied to problems like solving letter substitution ciphers (Knight et al., 2006; Ravi and Knight, 2011a) or transliterating Japanese katakana into English (Ravi and Knight, 2009), but not for translating full texts between related languages. 3 Translating RL to IL We learn a character-based cipher model for translating RL to IL. At decoding, this model is combined with a word based IL language model to produce IL text from RL. 3.1 Cipher Model Our noisy-channel cipher model converts a sequence of IL characters s1 , ..., sn to a sequence of RL characters t1 , ..., tm . It is a WFST composed of three components (Figure 2): WFST1 is a one-to-one letter substitution model. For each IL character s it writes one RL character t with probability p1 (t|s). 2514 s′ ,ϵ,1 goes to"
D17-1266,P11-1025,1,0.834809,"evel and character-level machine translation (Nakov and Tiedemann, 2012) between related languages. Use of non-parallel data: Cognates can be extracted from monolingual data and used as a parallel lexicon (Hana et al., 2006; Mann and Yarowsky, 2001; Kondrak et al., 2003). However, our task is whole-text transformation, not just cognate extraction. Unsupervised deciphering methods, which require no parallel data, have been used for bilingual lexicon extraction and machine translation. Word-based deciphering systems ignore sub-word similarities between related languages (Koehn and Knight, 2002; Ravi and Knight, 2011b; Nuhn et al., 2012; Dou and Knight, 2012; Ravi, 2013). Haghighi et al. (2008) and Naim and Gildea (2015) propose models that can use orthographic similarities. However, the model proposed by (Naim and Gildea, 2015) is only capable of producing a parallel lexicon and not translation. Furthermore, both systems require the languages to have the same orthography and their vocabulary is limited to what they see during training. Character-based decipherment is the model we use for solving this problem. Character-based decipherment has been previously applied to problems like solving letter substit"
D17-1266,A00-1002,0,0.174189,"ords. In other words, the vocabulary is limited by the decoding LM, not the cipher model. Separation of training and decoding language models enables us to train the decoding LM on as much data as is available without worrying about training speed or memory issues. We can also transliterate out of vocabulary words by spelling out the best path produced by cipher model in case no good match is found for a token in the decoding LM. 2 Related Work Previous work on translation between related languages can be categorized into three groups: Systems for specific language pairs such as Czech-Slovak (Haji et al., 2000), Turkish-Crimean Tatar (Cicekli, 2002), Irish-Scottish Gaelic (Scannell, 2006), and Indonesian-Malaysian (Larasati and Kubo, 2010). Another similar trend is translation between dialects of the same language like Arabic dialects to standard Arabic (Hitham et al., 2008; Sawaf, 2010; Salloum and Habash, 2010). Also, work has been done on translating back the Romanized version of languages like Greeklish to Greek (Chalamandaris et al., 2006) and Arabizi to Arabic (May et al., 2014). These methods cannot be applied to our problem because time and resources are limited to build a translation system"
D17-1266,P11-1002,1,0.179585,"evel and character-level machine translation (Nakov and Tiedemann, 2012) between related languages. Use of non-parallel data: Cognates can be extracted from monolingual data and used as a parallel lexicon (Hana et al., 2006; Mann and Yarowsky, 2001; Kondrak et al., 2003). However, our task is whole-text transformation, not just cognate extraction. Unsupervised deciphering methods, which require no parallel data, have been used for bilingual lexicon extraction and machine translation. Word-based deciphering systems ignore sub-word similarities between related languages (Koehn and Knight, 2002; Ravi and Knight, 2011b; Nuhn et al., 2012; Dou and Knight, 2012; Ravi, 2013). Haghighi et al. (2008) and Naim and Gildea (2015) propose models that can use orthographic similarities. However, the model proposed by (Naim and Gildea, 2015) is only capable of producing a parallel lexicon and not translation. Furthermore, both systems require the languages to have the same orthography and their vocabulary is limited to what they see during training. Character-based decipherment is the model we use for solving this problem. Character-based decipherment has been previously applied to problems like solving letter substit"
D17-1266,W06-2005,0,0.166639,"tagging. If we can convert RL to IL, we can convert all RL training data along with annotations into IL and train the tagger for IL. Or, if we can convert IL to RL we can use the potentially existing RL named entity tagger on converted IL data and project back the tags. Following this idea, Currey et al. (2016) use a rule-based translation system to convert Italian and Portuguese into Spanish, to improve Spanish (here, IL) language modeling, Nakov and Ng (2009) convert RL/English parallel data to IL/English where both RL and IL have Latin orthography to improve IL/English machine translation. Hana et al. (2006) use cognates to adapt Spanish resources to Brazilian Portuguese to train a part-of-speech tagger. Mann and Yarowsky (2001) use Spanish/Portuguese cognates to convert an English/Spanish lexicon to English/Portuguese. These works prove the usefulness of RL data to improve NLP for IL, but they are designed for specific tasks and IL/RL pairs. In this paper we propose a universal method for translating texts between closely related languages. We assume that IL and RL are mostly cognates, having roughly the same word order. Our method is orthography-agnostic for alphabetic systems, and crucially, i"
D17-1266,P06-2065,1,0.0558828,"2012; Dou and Knight, 2012; Ravi, 2013). Haghighi et al. (2008) and Naim and Gildea (2015) propose models that can use orthographic similarities. However, the model proposed by (Naim and Gildea, 2015) is only capable of producing a parallel lexicon and not translation. Furthermore, both systems require the languages to have the same orthography and their vocabulary is limited to what they see during training. Character-based decipherment is the model we use for solving this problem. Character-based decipherment has been previously applied to problems like solving letter substitution ciphers (Knight et al., 2006; Ravi and Knight, 2011a) or transliterating Japanese katakana into English (Ravi and Knight, 2009), but not for translating full texts between related languages. 3 Translating RL to IL We learn a character-based cipher model for translating RL to IL. At decoding, this model is combined with a word based IL language model to produce IL text from RL. 3.1 Cipher Model Our noisy-channel cipher model converts a sequence of IL characters s1 , ..., sn to a sequence of RL characters t1 , ..., tm . It is a WFST composed of three components (Figure 2): WFST1 is a one-to-one letter substitution model. F"
D17-1266,W02-0902,1,0.399813,"or combination of word-level and character-level machine translation (Nakov and Tiedemann, 2012) between related languages. Use of non-parallel data: Cognates can be extracted from monolingual data and used as a parallel lexicon (Hana et al., 2006; Mann and Yarowsky, 2001; Kondrak et al., 2003). However, our task is whole-text transformation, not just cognate extraction. Unsupervised deciphering methods, which require no parallel data, have been used for bilingual lexicon extraction and machine translation. Word-based deciphering systems ignore sub-word similarities between related languages (Koehn and Knight, 2002; Ravi and Knight, 2011b; Nuhn et al., 2012; Dou and Knight, 2012; Ravi, 2013). Haghighi et al. (2008) and Naim and Gildea (2015) propose models that can use orthographic similarities. However, the model proposed by (Naim and Gildea, 2015) is only capable of producing a parallel lexicon and not translation. Furthermore, both systems require the languages to have the same orthography and their vocabulary is limited to what they see during training. Character-based decipherment is the model we use for solving this problem. Character-based decipherment has been previously applied to problems like"
D17-1266,N03-2016,1,0.145519,"and resources are limited to build a translation system for the specific language pair. Machine learning systems that use parallel data: These methods cover a broader range of languages but require parallel text between related languages. They include character-level machine translation (Vilar et al., 2007; Tiedemann, 2009) or combination of word-level and character-level machine translation (Nakov and Tiedemann, 2012) between related languages. Use of non-parallel data: Cognates can be extracted from monolingual data and used as a parallel lexicon (Hana et al., 2006; Mann and Yarowsky, 2001; Kondrak et al., 2003). However, our task is whole-text transformation, not just cognate extraction. Unsupervised deciphering methods, which require no parallel data, have been used for bilingual lexicon extraction and machine translation. Word-based deciphering systems ignore sub-word similarities between related languages (Koehn and Knight, 2002; Ravi and Knight, 2011b; Nuhn et al., 2012; Dou and Knight, 2012; Ravi, 2013). Haghighi et al. (2008) and Naim and Gildea (2015) propose models that can use orthographic similarities. However, the model proposed by (Naim and Gildea, 2015) is only capable of producing a pa"
D17-1266,2010.amta-papers.5,0,0.0479092,"ate out of vocabulary words by spelling out the best path produced by cipher model in case no good match is found for a token in the decoding LM. 2 Related Work Previous work on translation between related languages can be categorized into three groups: Systems for specific language pairs such as Czech-Slovak (Haji et al., 2000), Turkish-Crimean Tatar (Cicekli, 2002), Irish-Scottish Gaelic (Scannell, 2006), and Indonesian-Malaysian (Larasati and Kubo, 2010). Another similar trend is translation between dialects of the same language like Arabic dialects to standard Arabic (Hitham et al., 2008; Sawaf, 2010; Salloum and Habash, 2010). Also, work has been done on translating back the Romanized version of languages like Greeklish to Greek (Chalamandaris et al., 2006) and Arabizi to Arabic (May et al., 2014). These methods cannot be applied to our problem because time and resources are limited to build a translation system for the specific language pair. Machine learning systems that use parallel data: These methods cover a broader range of languages but require parallel text between related languages. They include character-level machine translation (Vilar et al., 2007; Tiedemann, 2009) or combina"
D17-1266,2009.eamt-1.3,0,0.0675333,"m et al., 2008; Sawaf, 2010; Salloum and Habash, 2010). Also, work has been done on translating back the Romanized version of languages like Greeklish to Greek (Chalamandaris et al., 2006) and Arabizi to Arabic (May et al., 2014). These methods cannot be applied to our problem because time and resources are limited to build a translation system for the specific language pair. Machine learning systems that use parallel data: These methods cover a broader range of languages but require parallel text between related languages. They include character-level machine translation (Vilar et al., 2007; Tiedemann, 2009) or combination of word-level and character-level machine translation (Nakov and Tiedemann, 2012) between related languages. Use of non-parallel data: Cognates can be extracted from monolingual data and used as a parallel lexicon (Hana et al., 2006; Mann and Yarowsky, 2001; Kondrak et al., 2003). However, our task is whole-text transformation, not just cognate extraction. Unsupervised deciphering methods, which require no parallel data, have been used for bilingual lexicon extraction and machine translation. Word-based deciphering systems ignore sub-word similarities between related languages"
D17-1266,W07-0705,0,0.0686303,"andard Arabic (Hitham et al., 2008; Sawaf, 2010; Salloum and Habash, 2010). Also, work has been done on translating back the Romanized version of languages like Greeklish to Greek (Chalamandaris et al., 2006) and Arabizi to Arabic (May et al., 2014). These methods cannot be applied to our problem because time and resources are limited to build a translation system for the specific language pair. Machine learning systems that use parallel data: These methods cover a broader range of languages but require parallel text between related languages. They include character-level machine translation (Vilar et al., 2007; Tiedemann, 2009) or combination of word-level and character-level machine translation (Nakov and Tiedemann, 2012) between related languages. Use of non-parallel data: Cognates can be extracted from monolingual data and used as a parallel lexicon (Hana et al., 2006; Mann and Yarowsky, 2001; Kondrak et al., 2003). However, our task is whole-text transformation, not just cognate extraction. Unsupervised deciphering methods, which require no parallel data, have been used for bilingual lexicon extraction and machine translation. Word-based deciphering systems ignore sub-word similarities between"
D17-1266,N01-1020,0,0.423394,"tagger for IL. Or, if we can convert IL to RL we can use the potentially existing RL named entity tagger on converted IL data and project back the tags. Following this idea, Currey et al. (2016) use a rule-based translation system to convert Italian and Portuguese into Spanish, to improve Spanish (here, IL) language modeling, Nakov and Ng (2009) convert RL/English parallel data to IL/English where both RL and IL have Latin orthography to improve IL/English machine translation. Hana et al. (2006) use cognates to adapt Spanish resources to Brazilian Portuguese to train a part-of-speech tagger. Mann and Yarowsky (2001) use Spanish/Portuguese cognates to convert an English/Spanish lexicon to English/Portuguese. These works prove the usefulness of RL data to improve NLP for IL, but they are designed for specific tasks and IL/RL pairs. In this paper we propose a universal method for translating texts between closely related languages. We assume that IL and RL are mostly cognates, having roughly the same word order. Our method is orthography-agnostic for alphabetic systems, and crucially, it does not need any parallel data. From now on, we talk about converting RL to IL, but the method does not distinguish betw"
D17-1266,2014.amta-researchers.25,0,0.04133,"n related languages can be categorized into three groups: Systems for specific language pairs such as Czech-Slovak (Haji et al., 2000), Turkish-Crimean Tatar (Cicekli, 2002), Irish-Scottish Gaelic (Scannell, 2006), and Indonesian-Malaysian (Larasati and Kubo, 2010). Another similar trend is translation between dialects of the same language like Arabic dialects to standard Arabic (Hitham et al., 2008; Sawaf, 2010; Salloum and Habash, 2010). Also, work has been done on translating back the Romanized version of languages like Greeklish to Greek (Chalamandaris et al., 2006) and Arabizi to Arabic (May et al., 2014). These methods cannot be applied to our problem because time and resources are limited to build a translation system for the specific language pair. Machine learning systems that use parallel data: These methods cover a broader range of languages but require parallel text between related languages. They include character-level machine translation (Vilar et al., 2007; Tiedemann, 2009) or combination of word-level and character-level machine translation (Nakov and Tiedemann, 2012) between related languages. Use of non-parallel data: Cognates can be extracted from monolingual data and used as a"
D17-1266,chalamandaris-etal-2006-greek,0,\N,Missing
D18-1023,P17-1042,0,0.0533221,"versity, CIFAR Global Scholar kyunghyun.cho@nyu.edu 3 Didi Labs and University of Southern California knight@isi.edu Abstract (MT) is only available for dozens of dominant languages. In this paper we aim to construct a multilingual common semantic space where words in multiple languages are mapped into a distributed, language-agnostic semantic continuous space, so that resources and knowledge can be shared across languages. Previous multilingual embedding methods align the semantic distributions of words from multiple languages within the common semantic space. Though several recent attempts (Artetxe et al., 2017, 2018; Conneau et al., 2017) have shown that it is possible to extract multilingual word embedding from a pair of potentially unaligned corpora in multiple languages, we claim that it is necessary to impose more constraints to preserve linguistic properties and facilitate downstream NLP tasks, such as cross-lingual IE, and MT. We find that words also can be clustered through explicit (e.g., sharing affixes of certain linguistic functions) or implicit clues (e.g., sharing neighbors from monolingual word embedding) and such clusters should also be consistent across multiple languages. To do so,"
D18-1023,P18-1073,0,0.02517,"Missing"
D18-1023,P15-1027,0,0.0335093,"ow-resource languages. Experiments demonstrate that our framework is effective at capturing linguistic properties and significantly outperforms state-of-the-art multi-lingual embedding learning methods. pendency parsing (Guo et al., 2015; Ammar et al., 2016a), and name tagging (Zhang et al., 2017a; Tsai and Roth, 2016; Zhang et al., 2018; Cheung et al.; Zhang et al., 2017b; Feng et al., 2017). Using bilingual aligned words, previous methods project multiple monolingual embeddings into a shared semantic space using linear mappings (Mikolov et al., 2013b; Rothe et al., 2016; Zhang et al., 2016; Baroni et al., 2015; Xing et al., 2015; Smith et al., 2017) or canonical correlation analysis (CCA) (Ammar et al., 2016b; Faruqui and Dyer, 2014; Lu et al., 2015). Compared with CCA, which only optimizes the correlation for each individual pair of languages, linear mapping based methods can jointly optimize all the languages in the common semantic space. We focus on learning linear mappings to construct the common semantic space and adopt correlational neural networks (CorrNet) (Chandar et al., 2016; Rajendran et al., 2015) as the basic model. In contrast to previous work which only exploited monolingual word se"
D18-1023,N16-1030,0,0.0149937,"n (PER), Location (LOC), Organization (ORG), and Geo-Political Entities (GPE). We experiment with two sets of languages. The first set Amh+Tig consists of Amharic and Tigrinya. Both languages share the same Ge’ez script and descend from the proto-Semitic language family. The other set Eng+Uig+Tur consists of one high-resource language (English), one mediumresource language (Turkish) and one low-resource language (Uighur). It also consists of two distinct language scripts: English and Turkish use Latin script while Uighur uses Arabic script. We use an LSTM-CRF architecture (Huang et al., 2015; Lample et al., 2016; Ma and Hovy, 2016) for name tagging. It takes only word embedding as input and predict a tag for each word. Table 5 shows the statistics of training, development, and test sets for each language released by Linguistic Data Consortium (LDC).15 For each language pair, we combine the bilingual aligned words extracted from Wiktionary and monolingual dictionaries based on identical strings.16 We evaluate the quality from several aspects: Impact of Bilingual Dictionary Size In order to show the impact of the size of bilingual lexicons, we use three languages as a case study, and gradually reduce t"
D18-1023,C16-1171,0,0.0188394,"ers. Another branch of approaches for multilingual word embeddings are based on parallel or comparable data, such as parallel sentences (AP Chandar et al., 2014; Gouws et al., 2015; Luong et al., 2015; Hermann and Blunsom, 2014; Schwenk et al., 2017), phrase translations (Duong et al., 2016) and comparable documents (Vulic and Moens, 2015). Moreover, to reduce the need of bilingual alignment, several approaches have been designed to learn cross-lingual embeddings based on a small seed dictionary (Vulic and Korhonen, 2016; Zhang et al., 2016; Artetxe et al., 2017), or even with no supervision (Cao et al., 2016; Zhang et al., 2017d,c; Conneau et al., 2017; Artetxe et al., 2018). However, such methods are still limited to bilingual word embedding learning and remaining to be explored for common semantic space construction. 3 2 Related Work 3.1 Multilingual word embeddings have advanced many multilingual NLP tasks, such as machine translation (Zou et al., 2013; Mikolov et al., 2013b; Madhyastha and Espa˜na-Bonet, 2017), de2 Approach Overview Figure 1 shows the overview of our neural architecture. We project all monolingual word embeddings into a common semantic space based on word-level as well as clu"
D18-1023,N15-1028,0,0.0226675,"ate-of-the-art multi-lingual embedding learning methods. pendency parsing (Guo et al., 2015; Ammar et al., 2016a), and name tagging (Zhang et al., 2017a; Tsai and Roth, 2016; Zhang et al., 2018; Cheung et al.; Zhang et al., 2017b; Feng et al., 2017). Using bilingual aligned words, previous methods project multiple monolingual embeddings into a shared semantic space using linear mappings (Mikolov et al., 2013b; Rothe et al., 2016; Zhang et al., 2016; Baroni et al., 2015; Xing et al., 2015; Smith et al., 2017) or canonical correlation analysis (CCA) (Ammar et al., 2016b; Faruqui and Dyer, 2014; Lu et al., 2015). Compared with CCA, which only optimizes the correlation for each individual pair of languages, linear mapping based methods can jointly optimize all the languages in the common semantic space. We focus on learning linear mappings to construct the common semantic space and adopt correlational neural networks (CorrNet) (Chandar et al., 2016; Rajendran et al., 2015) as the basic model. In contrast to previous work which only exploited monolingual word semantics, we introduce multiple cluster-level alignments and design a new cluster consistent CorrNet to align both words and clusters. Another b"
D18-1023,W15-1521,0,0.173801,"the common semantic space. We focus on learning linear mappings to construct the common semantic space and adopt correlational neural networks (CorrNet) (Chandar et al., 2016; Rajendran et al., 2015) as the basic model. In contrast to previous work which only exploited monolingual word semantics, we introduce multiple cluster-level alignments and design a new cluster consistent CorrNet to align both words and clusters. Another branch of approaches for multilingual word embeddings are based on parallel or comparable data, such as parallel sentences (AP Chandar et al., 2014; Gouws et al., 2015; Luong et al., 2015; Hermann and Blunsom, 2014; Schwenk et al., 2017), phrase translations (Duong et al., 2016) and comparable documents (Vulic and Moens, 2015). Moreover, to reduce the need of bilingual alignment, several approaches have been designed to learn cross-lingual embeddings based on a small seed dictionary (Vulic and Korhonen, 2016; Zhang et al., 2016; Artetxe et al., 2017), or even with no supervision (Cao et al., 2016; Zhang et al., 2017d,c; Conneau et al., 2017; Artetxe et al., 2018). However, such methods are still limited to bilingual word embedding learning and remaining to be explored for comm"
D18-1023,P16-1101,0,0.0607558,"C), Organization (ORG), and Geo-Political Entities (GPE). We experiment with two sets of languages. The first set Amh+Tig consists of Amharic and Tigrinya. Both languages share the same Ge’ez script and descend from the proto-Semitic language family. The other set Eng+Uig+Tur consists of one high-resource language (English), one mediumresource language (Turkish) and one low-resource language (Uighur). It also consists of two distinct language scripts: English and Turkish use Latin script while Uighur uses Arabic script. We use an LSTM-CRF architecture (Huang et al., 2015; Lample et al., 2016; Ma and Hovy, 2016) for name tagging. It takes only word embedding as input and predict a tag for each word. Table 5 shows the statistics of training, development, and test sets for each language released by Linguistic Data Consortium (LDC).15 For each language pair, we combine the bilingual aligned words extracted from Wiktionary and monolingual dictionaries based on identical strings.16 We evaluate the quality from several aspects: Impact of Bilingual Dictionary Size In order to show the impact of the size of bilingual lexicons, we use three languages as a case study, and gradually reduce the size of the lexic"
D18-1023,W17-2617,0,0.0411181,"Missing"
D18-1023,D16-1136,0,0.0372422,"antic space and adopt correlational neural networks (CorrNet) (Chandar et al., 2016; Rajendran et al., 2015) as the basic model. In contrast to previous work which only exploited monolingual word semantics, we introduce multiple cluster-level alignments and design a new cluster consistent CorrNet to align both words and clusters. Another branch of approaches for multilingual word embeddings are based on parallel or comparable data, such as parallel sentences (AP Chandar et al., 2014; Gouws et al., 2015; Luong et al., 2015; Hermann and Blunsom, 2014; Schwenk et al., 2017), phrase translations (Duong et al., 2016) and comparable documents (Vulic and Moens, 2015). Moreover, to reduce the need of bilingual alignment, several approaches have been designed to learn cross-lingual embeddings based on a small seed dictionary (Vulic and Korhonen, 2016; Zhang et al., 2016; Artetxe et al., 2017), or even with no supervision (Cao et al., 2016; Zhang et al., 2017d,c; Conneau et al., 2017; Artetxe et al., 2018). However, such methods are still limited to bilingual word embedding learning and remaining to be explored for common semantic space construction. 3 2 Related Work 3.1 Multilingual word embeddings have advan"
D18-1023,E17-1084,0,0.0871444,"A, MultiSkip, and MultiCross)12 . MultiCluster (Ammar et al., 2016b) groups multilingual words into clusters based on bilingual dictionaries and forces all the words from various languages within one cluster share the same embedding. MultiCCA (Ammar et al., 2016b; Faruqui and Dyer, 2014) uses CCA to estimate linear projections for each pair of languages. MultiSkip is an extension of the multilingual skip-gram model (Luong et al., 2015), which requires parallel data. MultiCross is an approach to unify bilingual word embeddings into a shared semantic space using post hoc linear transformations (Duong et al., 2017). Table 2 lists the hyper-parameters used in the experiments. 512 512 20 1, 2, 3 500 0.5 Adadelta Table 2: Hyper-parameters. X OR = L(HlRi , HlRj ) , {li ,lj }∈A where W is the same as the W used in Section 3.3 for each language. We finally optimize the sum of the losses by finding the parameters 0 θ = {Wl , bl , bl , Ul , b∗l , CNNl , bR l }, where l denotes a specific language: Oθ = OW + ON + Ochar + OR 4 Experiments 4.1 Experiment Setup 4.2 Previous work (Ammar et al., 2016b; Duong et al., 2017) evaluated multilingual word embeddings on a series of intrinsic (e.g., monolingual and crossling"
D18-1023,E14-1049,0,0.1096,"ificantly outperforms state-of-the-art multi-lingual embedding learning methods. pendency parsing (Guo et al., 2015; Ammar et al., 2016a), and name tagging (Zhang et al., 2017a; Tsai and Roth, 2016; Zhang et al., 2018; Cheung et al.; Zhang et al., 2017b; Feng et al., 2017). Using bilingual aligned words, previous methods project multiple monolingual embeddings into a shared semantic space using linear mappings (Mikolov et al., 2013b; Rothe et al., 2016; Zhang et al., 2016; Baroni et al., 2015; Xing et al., 2015; Smith et al., 2017) or canonical correlation analysis (CCA) (Ammar et al., 2016b; Faruqui and Dyer, 2014; Lu et al., 2015). Compared with CCA, which only optimizes the correlation for each individual pair of languages, linear mapping based methods can jointly optimize all the languages in the common semantic space. We focus on learning linear mappings to construct the common semantic space and adopt correlational neural networks (CorrNet) (Chandar et al., 2016; Rajendran et al., 2015) as the basic model. In contrast to previous work which only exploited monolingual word semantics, we introduce multiple cluster-level alignments and design a new cluster consistent CorrNet to align both words and c"
D18-1023,H93-1061,0,0.177294,"h, French, Hungarian, Italian, Swedish) respectively. The monolingual data for each language is the combination of the Leipzig Corpora Collection9 and Europarl.10 The bilingual dictionaries are the same as those used in Ammar et al. (2016b).11 Intrinsic Evaluation: QVEC In order to evaluate the quality of multilingual embeddings, we adopt QVEC (Tsvetkov et al., 2015) as the intrinsic evaluation measure. It evaluates the quality of word embeddings based on the alignment of distributional word vectors to linguistic feature vectors extracted from manually crafted lexical resources, e.g., SemCor (Miller et al., 1993). For each word, each dimension of its linguistic feature vector defines the probability of that word belongs to a supersense (e.g., NN.FOOD) which is summarized from WordNet (Fellbaum, 1998). QVEC is computed as QVEC = Pmax j aij ≤1 D X P X r(xi , sj ) × aij , i=1 j=1 where x ∈ RD×1 denotes a distributional word vector and s ∈ RP ×1 denotes a linguistic word vector. D and P denote the sizes of vectors respectively. aij = 1 iff xi is aligned to sj , otherwise aij = 0. r(xi , sj ) is the Pearson’s correlation between xi and sj . QVEC-CCA (Ammar et al., 2016b) is extended from QVEC by using CCA"
D18-1023,N16-1091,0,0.0625978,"Missing"
D18-1023,P15-1119,0,0.0308306,"nt between clusters for common semantic space construction. We evaluate our approach on monolingual and multilingual QVEC (Tsvetkov et al., 2015) tasks, which measure the quality of word embeddings based on the alignment of the embeddings to linguistic feature vectors extracted from manually crafted linguistic resources, as well as an extrinsic evaluation on name tagging for low-resource languages. Experiments demonstrate that our framework is effective at capturing linguistic properties and significantly outperforms state-of-the-art multi-lingual embedding learning methods. pendency parsing (Guo et al., 2015; Ammar et al., 2016a), and name tagging (Zhang et al., 2017a; Tsai and Roth, 2016; Zhang et al., 2018; Cheung et al.; Zhang et al., 2017b; Feng et al., 2017). Using bilingual aligned words, previous methods project multiple monolingual embeddings into a shared semantic space using linear mappings (Mikolov et al., 2013b; Rothe et al., 2016; Zhang et al., 2016; Baroni et al., 2015; Xing et al., 2015; Smith et al., 2017) or canonical correlation analysis (CCA) (Ammar et al., 2016b; Faruqui and Dyer, 2014; Lu et al., 2015). Compared with CCA, which only optimizes the correlation for each individu"
D18-1023,W17-2619,0,0.0200454,"linear mappings to construct the common semantic space and adopt correlational neural networks (CorrNet) (Chandar et al., 2016; Rajendran et al., 2015) as the basic model. In contrast to previous work which only exploited monolingual word semantics, we introduce multiple cluster-level alignments and design a new cluster consistent CorrNet to align both words and clusters. Another branch of approaches for multilingual word embeddings are based on parallel or comparable data, such as parallel sentences (AP Chandar et al., 2014; Gouws et al., 2015; Luong et al., 2015; Hermann and Blunsom, 2014; Schwenk et al., 2017), phrase translations (Duong et al., 2016) and comparable documents (Vulic and Moens, 2015). Moreover, to reduce the need of bilingual alignment, several approaches have been designed to learn cross-lingual embeddings based on a small seed dictionary (Vulic and Korhonen, 2016; Zhang et al., 2016; Artetxe et al., 2017), or even with no supervision (Cao et al., 2016; Zhang et al., 2017d,c; Conneau et al., 2017; Artetxe et al., 2018). However, such methods are still limited to bilingual word embedding learning and remaining to be explored for common semantic space construction. 3 2 Related Work 3"
D18-1023,N18-5009,1,0.881187,"Missing"
D18-1023,N16-1072,0,0.0330789,"roach on monolingual and multilingual QVEC (Tsvetkov et al., 2015) tasks, which measure the quality of word embeddings based on the alignment of the embeddings to linguistic feature vectors extracted from manually crafted linguistic resources, as well as an extrinsic evaluation on name tagging for low-resource languages. Experiments demonstrate that our framework is effective at capturing linguistic properties and significantly outperforms state-of-the-art multi-lingual embedding learning methods. pendency parsing (Guo et al., 2015; Ammar et al., 2016a), and name tagging (Zhang et al., 2017a; Tsai and Roth, 2016; Zhang et al., 2018; Cheung et al.; Zhang et al., 2017b; Feng et al., 2017). Using bilingual aligned words, previous methods project multiple monolingual embeddings into a shared semantic space using linear mappings (Mikolov et al., 2013b; Rothe et al., 2016; Zhang et al., 2016; Baroni et al., 2015; Xing et al., 2015; Smith et al., 2017) or canonical correlation analysis (CCA) (Ammar et al., 2016b; Faruqui and Dyer, 2014; Lu et al., 2015). Compared with CCA, which only optimizes the correlation for each individual pair of languages, linear mapping based methods can jointly optimize all the la"
D18-1023,D15-1243,0,0.120914,"/ -ler), Somali (-o)). Linguists have created a wide variety of linguistic property knowledge bases, which are readily available for thousands of languages. For example, the CLDR (Unicode Common Locale Data Repository)2 includes closed word classes and affixes indicating various linguistic properties. We propose to take advantage of these languageuniversal resources to create clusters, where the words within one cluster share the same linguistic property, and build alignment between clusters for common semantic space construction. We evaluate our approach on monolingual and multilingual QVEC (Tsvetkov et al., 2015) tasks, which measure the quality of word embeddings based on the alignment of the embeddings to linguistic feature vectors extracted from manually crafted linguistic resources, as well as an extrinsic evaluation on name tagging for low-resource languages. Experiments demonstrate that our framework is effective at capturing linguistic properties and significantly outperforms state-of-the-art multi-lingual embedding learning methods. pendency parsing (Guo et al., 2015; Ammar et al., 2016a), and name tagging (Zhang et al., 2017a; Tsai and Roth, 2016; Zhang et al., 2018; Cheung et al.; Zhang et a"
D18-1023,P17-1179,0,0.0840215,"Missing"
D18-1023,P16-1024,0,0.0317905,"cluster-level alignments and design a new cluster consistent CorrNet to align both words and clusters. Another branch of approaches for multilingual word embeddings are based on parallel or comparable data, such as parallel sentences (AP Chandar et al., 2014; Gouws et al., 2015; Luong et al., 2015; Hermann and Blunsom, 2014; Schwenk et al., 2017), phrase translations (Duong et al., 2016) and comparable documents (Vulic and Moens, 2015). Moreover, to reduce the need of bilingual alignment, several approaches have been designed to learn cross-lingual embeddings based on a small seed dictionary (Vulic and Korhonen, 2016; Zhang et al., 2016; Artetxe et al., 2017), or even with no supervision (Cao et al., 2016; Zhang et al., 2017d,c; Conneau et al., 2017; Artetxe et al., 2018). However, such methods are still limited to bilingual word embedding learning and remaining to be explored for common semantic space construction. 3 2 Related Work 3.1 Multilingual word embeddings have advanced many multilingual NLP tasks, such as machine translation (Zou et al., 2013; Mikolov et al., 2013b; Madhyastha and Espa˜na-Bonet, 2017), de2 Approach Overview Figure 1 shows the overview of our neural architecture. We project all m"
D18-1023,D17-1207,0,0.120181,"Missing"
D18-1023,P15-2118,0,0.0245502,"works (CorrNet) (Chandar et al., 2016; Rajendran et al., 2015) as the basic model. In contrast to previous work which only exploited monolingual word semantics, we introduce multiple cluster-level alignments and design a new cluster consistent CorrNet to align both words and clusters. Another branch of approaches for multilingual word embeddings are based on parallel or comparable data, such as parallel sentences (AP Chandar et al., 2014; Gouws et al., 2015; Luong et al., 2015; Hermann and Blunsom, 2014; Schwenk et al., 2017), phrase translations (Duong et al., 2016) and comparable documents (Vulic and Moens, 2015). Moreover, to reduce the need of bilingual alignment, several approaches have been designed to learn cross-lingual embeddings based on a small seed dictionary (Vulic and Korhonen, 2016; Zhang et al., 2016; Artetxe et al., 2017), or even with no supervision (Cao et al., 2016; Zhang et al., 2017d,c; Conneau et al., 2017; Artetxe et al., 2018). However, such methods are still limited to bilingual word embedding learning and remaining to be explored for common semantic space construction. 3 2 Related Work 3.1 Multilingual word embeddings have advanced many multilingual NLP tasks, such as machine"
D18-1023,N16-1156,0,0.121959,"n name tagging for low-resource languages. Experiments demonstrate that our framework is effective at capturing linguistic properties and significantly outperforms state-of-the-art multi-lingual embedding learning methods. pendency parsing (Guo et al., 2015; Ammar et al., 2016a), and name tagging (Zhang et al., 2017a; Tsai and Roth, 2016; Zhang et al., 2018; Cheung et al.; Zhang et al., 2017b; Feng et al., 2017). Using bilingual aligned words, previous methods project multiple monolingual embeddings into a shared semantic space using linear mappings (Mikolov et al., 2013b; Rothe et al., 2016; Zhang et al., 2016; Baroni et al., 2015; Xing et al., 2015; Smith et al., 2017) or canonical correlation analysis (CCA) (Ammar et al., 2016b; Faruqui and Dyer, 2014; Lu et al., 2015). Compared with CCA, which only optimizes the correlation for each individual pair of languages, linear mapping based methods can jointly optimize all the languages in the common semantic space. We focus on learning linear mappings to construct the common semantic space and adopt correlational neural networks (CorrNet) (Chandar et al., 2016; Rajendran et al., 2015) as the basic model. In contrast to previous work which only exploite"
D18-1023,D13-1141,0,0.0354099,"o reduce the need of bilingual alignment, several approaches have been designed to learn cross-lingual embeddings based on a small seed dictionary (Vulic and Korhonen, 2016; Zhang et al., 2016; Artetxe et al., 2017), or even with no supervision (Cao et al., 2016; Zhang et al., 2017d,c; Conneau et al., 2017; Artetxe et al., 2018). However, such methods are still limited to bilingual word embedding learning and remaining to be explored for common semantic space construction. 3 2 Related Work 3.1 Multilingual word embeddings have advanced many multilingual NLP tasks, such as machine translation (Zou et al., 2013; Mikolov et al., 2013b; Madhyastha and Espa˜na-Bonet, 2017), de2 Approach Overview Figure 1 shows the overview of our neural architecture. We project all monolingual word embeddings into a common semantic space based on word-level as well as cluster-level alignments and learn the transformation functions. First, on cldr.unicode.org 251 Figure 1: Architecture Overview. In each monolingual semantic space, the words within solid rectangle denote a neighbor based cluster and the words within dotted rectangle denote a linguistic property based cluster. where Hl1 ∈ R|Vl1 |×h and Hl2 ∈ R|Vl2 |×h are"
D18-1023,N15-1104,0,0.0397766,". Experiments demonstrate that our framework is effective at capturing linguistic properties and significantly outperforms state-of-the-art multi-lingual embedding learning methods. pendency parsing (Guo et al., 2015; Ammar et al., 2016a), and name tagging (Zhang et al., 2017a; Tsai and Roth, 2016; Zhang et al., 2018; Cheung et al.; Zhang et al., 2017b; Feng et al., 2017). Using bilingual aligned words, previous methods project multiple monolingual embeddings into a shared semantic space using linear mappings (Mikolov et al., 2013b; Rothe et al., 2016; Zhang et al., 2016; Baroni et al., 2015; Xing et al., 2015; Smith et al., 2017) or canonical correlation analysis (CCA) (Ammar et al., 2016b; Faruqui and Dyer, 2014; Lu et al., 2015). Compared with CCA, which only optimizes the correlation for each individual pair of languages, linear mapping based methods can jointly optimize all the languages in the common semantic space. We focus on learning linear mappings to construct the common semantic space and adopt correlational neural networks (CorrNet) (Chandar et al., 2016; Rajendran et al., 2015) as the basic model. In contrast to previous work which only exploited monolingual word semantics, we introdu"
D18-1023,Q16-1031,0,\N,Missing
E03-1076,A00-1031,0,0.00604947,"ences, and therefore taken as evidence for the existence of a translation for den. Another example for this is the word Voraussetzung (English: condition), which is split into vor and aussetzung. The word vor translates to many different prepositions, which frequently occur in English. To exclude these mistakes, we use information about the parts-of-speech of words. We do not want to break up a compound into parts that are prepositions or determiners, but only content words: nouns, adverbs, adjectives, and verbs. To accomplish this, we tag the German corpus with POS tags using the TnT tagger [Brants, 2000]. We then obtain statistics on the parts-ofspeech of words in the corpus. This allows us to exclude words based on their POS as possible parts of compounds. We limit possible parts of compounds to words that occur most of the time as one of following POS: ADJA, ADJD, ADV, NN, NE, PTKNEG, VVFIN, VVIMP, VVINF, VVIZU, VVPP, VAFIN, VAIMP, VAINF, VAPP, VMFIN, VMINF, VMPP. 7 Evaluation The training set for the experiments is a corpus of 650,000 noun phrases and prepositional phrases (NP/PP). For each German NP/PP, we have a English translation. This data was extracted from the Europarl corpus [Koeh"
E03-1076,J90-2002,0,0.292355,"not 191 Method raw eager frequency based using parallel using parallel and POS BLEU 0.291 0.222 0.317 0.294 0.306 Table 2: Evaluation of the methods with a word based statistical machine translation system (IBM Model 4). Frequency based splitting is best, the methods using splitting knowledge from a parallel corpus also improve over unsplit (raw) data. systems. Hence, we use the splitting methods to prepare training and testing data to optimize the performance of such systems. First, we measured the impact on a word based statistical machine translation system, the widely studied IBM Model 4 [Brown et al., 1990], for which training tools [Al-Onaizan et al., 19991 and decoders [Germann et al., 2001] are freely available. We trained the system on the 650,000 NP/PPs with the Giza toolkit, and evaluated the translation quality on the same 1000 NP/PP test set as in the previous section. Training and testing data was split consistently in the same way. The translation accuracy is measured against reference translations using the BLEU score [Papineni et al., 2002]. Table 2 displays the results. Somewhat surprisingly, the frequency based method leads to better translation quality than the more accurate meth"
E03-1076,2002.tmi-papers.3,0,0.291402,"Missing"
E03-1076,W02-1018,0,0.0104247,"o that a one-to-one correspondence to English can be established. Note that we are looking for a one-to-one correspondence to English content words: Say, the preferred translation of Aktionsplan is plan for action. The lack of correspondence for the English word for does not detract from the definition of the task: We would still like to break up the German compound into the two parts Aktion and Plan. The insertion of function words is not our concern. Ultimately, the purpose of this work is to improve the quality of machine translation systems. For instance, phrase-based translation systems [Marcu and Wong, 2002] may recover more easily from splitting regimes that do not create a one-to-one translation correspondence. One splitting method may mistakenly break up the word Aktionsplan into the three words Akt, Ion, and Plan. But if we consistently break up the word Aktion into Akt and Ion in our training data, such a 187 system will likely learn the translation of the word pair Akt Ion into the single English word action. These considerations lead us to three different objectives and therefore three different evaluation metrics for the task of compound splitting: • One-to-One correspondence • Translati"
E03-1076,P02-1040,0,0.103409,"e performance of such systems. First, we measured the impact on a word based statistical machine translation system, the widely studied IBM Model 4 [Brown et al., 1990], for which training tools [Al-Onaizan et al., 19991 and decoders [Germann et al., 2001] are freely available. We trained the system on the 650,000 NP/PPs with the Giza toolkit, and evaluated the translation quality on the same 1000 NP/PP test set as in the previous section. Training and testing data was split consistently in the same way. The translation accuracy is measured against reference translations using the BLEU score [Papineni et al., 2002]. Table 2 displays the results. Somewhat surprisingly, the frequency based method leads to better translation quality than the more accurate methods that take advantage from knowledge from the parallel corpus. One reason for this is that the system recovers more easily from words that are split too much than from words that are not split up sufficiently. Of course, this has limitations: Eager splitting into as many parts as possible fares abysmally. 7.3 Translation Quality with Phrase Based Machine Translation Compound words violate the bias for one-to-one word correspondences of word based S"
E03-1076,P01-1030,1,0.26186,"291 0.222 0.317 0.294 0.306 Table 2: Evaluation of the methods with a word based statistical machine translation system (IBM Model 4). Frequency based splitting is best, the methods using splitting knowledge from a parallel corpus also improve over unsplit (raw) data. systems. Hence, we use the splitting methods to prepare training and testing data to optimize the performance of such systems. First, we measured the impact on a word based statistical machine translation system, the widely studied IBM Model 4 [Brown et al., 1990], for which training tools [Al-Onaizan et al., 19991 and decoders [Germann et al., 2001] are freely available. We trained the system on the 650,000 NP/PPs with the Giza toolkit, and evaluated the translation quality on the same 1000 NP/PP test set as in the previous section. Training and testing data was split consistently in the same way. The translation accuracy is measured against reference translations using the BLEU score [Papineni et al., 2002]. Table 2 displays the results. Somewhat surprisingly, the frequency based method leads to better translation quality than the more accurate methods that take advantage from knowledge from the parallel corpus. One reason for this is"
E03-1076,W01-0504,1,0.432753,"akt—ion—plan We arrive at these splitting options, since all the parts — aktionsplan, aktions, aktion, akt, ion, and plan — have been observed as whole words in the training corpus. These splitting options are the basis of our work. In the following we discuss methods that pick one of them as the correct splitting of the compound. 4 Frequency Based Metric 5 Guidance from a Parallel Corpus The more frequent a word occurs in a training corpus, the bigger the statistical basis to estimate translation probabilities, and the more likely the correct translation probability distribution is learned [Koehn and Knight, 20011. This insight leads us to define a splitting metric based on word frequency. Given the count of words in the corpus, we pick the split S with the highest geometric mean of word frequencies of its parts pi (n being the number of parts): As stated earlier, one of our objectives is the splitting of compounds into parts that have one-to-one correspondence to English. One source of information about word correspondence is a parallel corpus: text in a foreign language, accompanied by translations into English. Usually, such a corpus is provided in form of sentence translation pairs. Going through"
H93-1036,J92-4003,0,0.00942069,"Missing"
H93-1036,C92-4189,0,0.0409406,"Missing"
H93-1036,1991.mtsummit-papers.3,0,0.0924546,"e translation system. Because we axe aiming at broad sem~tnticcoverage, we are focusing on automatic and semi-automatic methods of knowledge acquisition. Here we report on algorithms for merging complementary online resources, in particular the LDOCE and WordNet dictionaries. We discuss empirical results, and how these results have been incorporated into the PANGLOSS ontology. 1. Introduction The PANGLOSS project is a three-site collaborative effort to build a large-scale knowledge-based machine translation system. Key components of PANGLOSS include New Mexico State University&apos;s ULTRA parser [Farwell and Wilks, 1991], Carnegie Mellon&apos;s interlingua representation format [Nirenburg and Defrise, 1991], and USC/ISI&apos;s PENMAN English generation system [Penman, 1989]. Another key component currently under construction at ISI is the PANGLOSS ontology, a largescale conceptual network intended to support semantic processing in other PANGLOSS modules. This network will contain 50,000 nodes representing commonly encountered objects, entities, qualities, and relations. The upper (more abstract) region of the ontology is called the Ontology Base (OB) and contains approximately 400 items that represent generalizations"
H93-1036,W90-0108,0,0.15641,"Missing"
I17-1037,N13-1006,0,0.0596371,"Missing"
I17-1037,P15-1017,0,0.0455657,"Missing"
I17-1037,Q16-1026,0,0.103444,"Missing"
I17-1037,2005.mtsummit-posters.10,0,0.00943266,", title, nationalities, topical keywords, organization suffixes, temporal words, locations and people, and stop words which are unlikely to be names. Elicitation Corpus. An elicitation corpus is a controlled corpus translated by a bilingual consultant in order to produce high quality word aligned sentence pairs. During the elicitation process, the user will translate a subset of these sentences that is dynamically determined to be sufficient for learning the desired grammar rules. We extracted word and phrase translation pairs from the Elicitation corpus developed by CMU (Probst et al., 2001; Alvarez et al., 2005) 11 for the DARPA LORELEI which contains pairs of sentences in a low-resource language and English. 3.5 Encoding Linguistic Features We merged the linguistic resources collected above into three types of features: (1) name gazetteers; (2) list of suffixes and contextual words (e.g., titles) that indicate names; and (3) list of words that indicate non-names (e.g., time expressions). Ultimately we obtained 30 explicit linguistic feature categories. Table 5 shows the statistics of the encoded features. For each token wi in a sentence, we check whether wi , its previous token wi−1 and its next tok"
I17-1037,P03-2031,0,0.0451114,"Missing"
I17-1037,C10-3010,0,0.0268383,"Missing"
I17-1037,W13-2322,1,0.738035,"2015) for linking: (1) Popularity: we prefer popular entities in the KB; (2) Coherence: we link a pair of a foreign name and its English translation simultaneously and favor their candidate entities that are also strongly connected in the KB through a direct cross-lingual page link, a common neighbor, or sharing similar properties. After linking, we assign an entity type to each pair based on their properties in the KB (e.g., an entity with a birthdate and a death-date is likely to be a person). The typing component is a Maximum Entropy model learned from the Abstract Meaning Representation (Banarescu et al., 2013) corpus that includes both entity type and Wikipedia link for each entity mention, using KB properties as features. 3.4 Language Hausa Turkish Uzbek 9 PER 1,174 2,819 1,771 LOC 5,123 7,271 5,331 ORG 199 262 103 Title Non-Name Suffix 42 231 178 391 411 271 21 181 209 Table 5: Name Related List Statistics (# of entries). related words and phrases. For each language, we first extracted 2, 000 to 3, 000 parallel sentence/phrase pairs. Then we ran GIZA++ over these pairs and combined structure rules from WALS to obtain word translation pairs. We also extracted translations of the following English"
I17-1037,P16-2011,1,0.90027,"Missing"
I17-1037,P09-1113,0,0.155476,"Missing"
I17-1037,kamholz-etal-2014-panlex,0,0.0231862,"Missing"
I17-1037,P15-2060,0,0.0610169,"Missing"
I17-1037,P12-1073,0,0.122701,"Missing"
I17-1037,W15-1506,0,0.0387008,"Missing"
I17-1037,N16-1030,0,0.10763,"Missing"
I17-1037,N16-1034,0,0.0528109,"Missing"
I17-1037,U08-1016,0,0.0884468,"Missing"
I17-1037,C16-1095,0,0.0204274,"eature design. Our work argues that data-driven implicit knowledge like word embeddings cannot cover all linguistic phenomena in low-resource settings. We propose to embrace the readily available universal resources for many languages, and proved this process of making them actionable is not costly and does not require a system developer to “know” the language. Many more non-traditional linguistic resources remain to explore in the future, including Lexvo (de Melo, 2015), Multilingual Entity Taxonomy (de Melo and Weikum, 2010), EZGlot, URIEL knowledge Some recent studies (Zhang et al., 2016a; Littell et al., 2016a; Tsai et al., 2016; Pan et al., 2017) under the DARPA LORELEI program focused on name tagging for low-resource languages. Most noise tolerant supervised learning algorithms (Bylander, 1994; Dredze et al., 2008; Crammer et al., 2009; Kalapanidas et al., 2003; Scott et al., 2013) have been applied for improving image classifi1 cation (Mnih and Hinton, 2012; Natarajan et al., 2013; Sukhbaatar et al., 2014; Xiao et al., 2015). Coupling our idea with these algorithms is also likely to yield further improvement. 369 base (Littell et al., 2016b), travel phrase books and yellow phone books. We will"
I17-1037,J03-1002,0,0.00957413,"Missing"
I17-1037,C16-1123,0,0.0386226,"Missing"
I17-1037,P15-2047,1,0.903517,"Missing"
I17-1037,P15-2034,0,0.0143569,"stical NLP research, because they were not specifically designed for NLP purpose at the first place and they are often far from complete. Thus they are not immediately actionable - converted into features, rules or patterns for a target NLP application. In this paper we design various methods to convert them into machine readable features for a new DNN architecture. Very little work has used non-traditional resources mentioned in this paper for practical downstream NLP applications. Limited work only used them for resource building (e.g., (Sarma et al., 2012)) or studying word order typology (Ostling, 2015). To the best of our knowledge, our work is the first to encode them as actionable knowledge for IE. We aim to answer the following research questions: How to effectively acquire linguistic knowledge from non-traditional resources, and represent them for computational models? How much further gain can be obtained in addition to traditional resources? 2 2.1 Languages Hausa Turkish Uzbek # of Documents # of Names # of Sentences Train 137 128 127 Test 100 100 100 Train 3,414 2,341 3,577 Test 1,320 2,173 3,137 Train Test 3,156 1,130 1,973 2,119 3,588 3,037 Table 1: Data Statistics. in the entire s"
I17-1037,P14-5010,0,0.00635514,"Missing"
I17-1037,N15-1119,1,0.847033,"titude, longitude, feature class, feature code, country code, administrative code, population, elevation and time zone. JRC Names. Finally we include the JRC Names (Steinberger et al., 20011), a large list of person and organization names (about 205,000 entries) in over 20 different scripts. Some entries include additional information such as frequency, title and date ranges. Grounding to KB and Typing. For names that we are able to acquire English translations, we further ground (“wikify”) them to an external knowledge base (KB, DBpedia in our work) if they are linkable. We use two measures (Pan et al., 2015) for linking: (1) Popularity: we prefer popular entities in the KB; (2) Coherence: we link a pair of a foreign name and its English translation simultaneously and favor their candidate entities that are also strongly connected in the KB through a direct cross-lingual page link, a common neighbor, or sharing similar properties. After linking, we assign an entity type to each pair based on their properties in the KB (e.g., an entity with a birthdate and a death-date is likely to be a person). The typing component is a Maximum Entropy model learned from the Abstract Meaning Representation (Banare"
I17-1037,P17-1178,1,0.840946,"riven implicit knowledge like word embeddings cannot cover all linguistic phenomena in low-resource settings. We propose to embrace the readily available universal resources for many languages, and proved this process of making them actionable is not costly and does not require a system developer to “know” the language. Many more non-traditional linguistic resources remain to explore in the future, including Lexvo (de Melo, 2015), Multilingual Entity Taxonomy (de Melo and Weikum, 2010), EZGlot, URIEL knowledge Some recent studies (Zhang et al., 2016a; Littell et al., 2016a; Tsai et al., 2016; Pan et al., 2017) under the DARPA LORELEI program focused on name tagging for low-resource languages. Most noise tolerant supervised learning algorithms (Bylander, 1994; Dredze et al., 2008; Crammer et al., 2009; Kalapanidas et al., 2003; Scott et al., 2013) have been applied for improving image classifi1 cation (Mnih and Hinton, 2012; Natarajan et al., 2013; Sukhbaatar et al., 2014; Xiao et al., 2015). Coupling our idea with these algorithms is also likely to yield further improvement. 369 base (Littell et al., 2016b), travel phrase books and yellow phone books. We will also investigate whether these linguist"
I17-1037,K16-1022,0,0.0417752,"argues that data-driven implicit knowledge like word embeddings cannot cover all linguistic phenomena in low-resource settings. We propose to embrace the readily available universal resources for many languages, and proved this process of making them actionable is not costly and does not require a system developer to “know” the language. Many more non-traditional linguistic resources remain to explore in the future, including Lexvo (de Melo, 2015), Multilingual Entity Taxonomy (de Melo and Weikum, 2010), EZGlot, URIEL knowledge Some recent studies (Zhang et al., 2016a; Littell et al., 2016a; Tsai et al., 2016; Pan et al., 2017) under the DARPA LORELEI program focused on name tagging for low-resource languages. Most noise tolerant supervised learning algorithms (Bylander, 1994; Dredze et al., 2008; Crammer et al., 2009; Kalapanidas et al., 2003; Scott et al., 2013) have been applied for improving image classifi1 cation (Mnih and Hinton, 2012; Natarajan et al., 2013; Sukhbaatar et al., 2014; Xiao et al., 2015). Coupling our idea with these algorithms is also likely to yield further improvement. 369 base (Littell et al., 2016b), travel phrase books and yellow phone books. We will also investigate whe"
I17-1037,P13-1106,0,0.2057,"Missing"
I17-1037,2001.mtsummit-road.7,0,0.034043,"ase, location affixes, title, nationalities, topical keywords, organization suffixes, temporal words, locations and people, and stop words which are unlikely to be names. Elicitation Corpus. An elicitation corpus is a controlled corpus translated by a bilingual consultant in order to produce high quality word aligned sentence pairs. During the elicitation process, the user will translate a subset of these sentences that is dynamically determined to be sufficient for learning the desired grammar rules. We extracted word and phrase translation pairs from the Elicitation corpus developed by CMU (Probst et al., 2001; Alvarez et al., 2005) 11 for the DARPA LORELEI which contains pairs of sentences in a low-resource language and English. 3.5 Encoding Linguistic Features We merged the linguistic resources collected above into three types of features: (1) name gazetteers; (2) list of suffixes and contextual words (e.g., titles) that indicate names; and (3) list of words that indicate non-names (e.g., time expressions). Ultimately we obtained 30 explicit linguistic feature categories. Table 5 shows the statistics of the encoded features. For each token wi in a sentence, we check whether wi , its previous toke"
I17-1037,Q14-1005,0,0.0590155,"Missing"
I17-1037,C12-2095,0,0.0450664,"Missing"
I17-1037,C16-1080,0,0.055237,"Missing"
I17-1037,U09-1004,0,0.0672211,"Missing"
I17-1037,D16-1007,0,0.0239341,"Missing"
I17-1037,W12-5113,0,0.0137604,"rces have been largely ignored by the mainstream statistical NLP research, because they were not specifically designed for NLP purpose at the first place and they are often far from complete. Thus they are not immediately actionable - converted into features, rules or patterns for a target NLP application. In this paper we design various methods to convert them into machine readable features for a new DNN architecture. Very little work has used non-traditional resources mentioned in this paper for practical downstream NLP applications. Limited work only used them for resource building (e.g., (Sarma et al., 2012)) or studying word order typology (Ostling, 2015). To the best of our knowledge, our work is the first to encode them as actionable knowledge for IE. We aim to answer the following research questions: How to effectively acquire linguistic knowledge from non-traditional resources, and represent them for computational models? How much further gain can be obtained in addition to traditional resources? 2 2.1 Languages Hausa Turkish Uzbek # of Documents # of Names # of Sentences Train 137 128 127 Test 100 100 100 Train 3,414 2,341 3,577 Test 1,320 2,173 3,137 Train Test 3,156 1,130 1,973 2,119 3,58"
I17-1037,C14-1220,0,0.0708708,"Missing"
I17-1037,N16-1029,1,0.884103,"challenge, especially because we don’t have enough resources and tools to perform a deep understanding of the contexts. For example, when a journal name “New England” appears in Hausa texts, all of its mentions are mistakenly labeled as location instead of organization, because the dominant type label of “New England” is location in all of our resources. Uzbek 64.1 67.4 67.2 68.4 Table 6: Feature Integration Methods Comparison. We compare the following models: a baseline model that uses only character and word embedding features, a model adding traditional linguistic features as described in (Zhang et al., 2016a), and a model further adding non-traditional linguistic features using the third integration method. Figure 4 presents the results. Clearly models trained with linguistic features substantially outperform the baseline models on all noise levels for all languages. As the noise level increases, the performance of the baseline model drops drastically while the model trained with linguistic features successfully curbs the downward trend and forms a relatively flat curve at last. Adding non-traditional linguistic features provides further gains in almost all settings. Notably for Turkish, adding"
I17-1037,E17-1119,0,0.026404,"e 5 Related Work The major novel contribution of this paper is to systematically explore many non-traditional linguistic resources which have been largely neglected by the mainstream NLP community. Some previous efforts used WALS to study the typological relations across languages (Rama and Prasanth, 2012; O’Horan et al., 2016; Yamauchi and Murawaki, 2016) but very little work used it for practical NLP applications. Most DNN methods solely relied on character embeddings and word embeddings as features for name tagging (e.g., (Huang et al., 2015; Lample et al., 2016; Chiu and Nichols, 2016)). (Shimaoka et al., 2017) used hand-crafted features to improve the performance of DNN on fine-grained entity typing. (Chiu and Nichols, 2016) attempted to incorporate gazetteers as ex368 Hausa 72.2 65 64 60 65.7 60 53.8 56 45.8 49 0 9.5 19.9 32.8 66 55 50 39.5 35 59 64.1 57.8 55 51 48 43.3 44 0 12.7 19.9 27.1 39.9 40 0 8.4 Noise Level Noise Level Embedding Features 63 60.0 65.9 40 44.7 Uzbek 68.4 70 45 53 45 Turkish 74.3 70 68 F-score F-score 71 75 F-score 75 Embedding+Traditional Linguistic Features 18.7 30 40.06 Noise Level Embedding+Traditional+Non-traditional Linguistic Features Figure 4: Name Tagging Performance"
I17-1037,C16-1045,1,0.922682,"challenge, especially because we don’t have enough resources and tools to perform a deep understanding of the contexts. For example, when a journal name “New England” appears in Hausa texts, all of its mentions are mistakenly labeled as location instead of organization, because the dominant type label of “New England” is location in all of our resources. Uzbek 64.1 67.4 67.2 68.4 Table 6: Feature Integration Methods Comparison. We compare the following models: a baseline model that uses only character and word embedding features, a model adding traditional linguistic features as described in (Zhang et al., 2016a), and a model further adding non-traditional linguistic features using the third integration method. Figure 4 presents the results. Clearly models trained with linguistic features substantially outperform the baseline models on all noise levels for all languages. As the noise level increases, the performance of the baseline model drops drastically while the model trained with linguistic features successfully curbs the downward trend and forms a relatively flat curve at last. Adding non-traditional linguistic features provides further gains in almost all settings. Notably for Turkish, adding"
J08-3004,J00-1004,0,0.197688,"Missing"
J08-3004,C00-1007,0,0.0182227,"conversion to name transliteration. However, language problems like machine translation break this mold, because they involve massive re-ordering of symbols, and because the transformation processes seem sensitive to hierarchical tree structure. Recently, speciﬁc probabilistic tree-based models have been proposed not only for machine translation (Wu 1997; Alshawi, Bangalore, and Douglas 2000; Yamada and Knight 2001; Eisner 2003; Gildea 2003), but also for summarization (Knight and Marcu 2002), paraphrasing (Pang, Knight, and Marcu 2003), natural language generation (Langkilde and Knight 1998; Bangalore and Rambow 2000; Corston-Oliver et al. 2002), parsing, and language modeling (Baker 1979; Lari and Young 1990; Collins 1997; Chelba and Jelinek 2000; Charniak 2001; Klein ∗ ∗∗ † 1 Information Sciences Institute, 4676 Admiralty Way, Marina del Rey, CA 90292. E-mail: graehl@isi.edu. Information Sciences Institute, 4676 Admiralty Way, Marina del Rey, CA 90292. E-mail: knight@isi.edu. Information Sciences Institute, 4676 Admiralty Way, Marina del Rey, CA 90292. E-mail: jonmay@isi.edu. www.research.att.com/sw/tools/fsm and www.isi.edu/licensed-sw/carmel. Submission received: 30 October 2003; revised submission re"
J08-3004,P01-1017,0,0.0342892,"d because the transformation processes seem sensitive to hierarchical tree structure. Recently, speciﬁc probabilistic tree-based models have been proposed not only for machine translation (Wu 1997; Alshawi, Bangalore, and Douglas 2000; Yamada and Knight 2001; Eisner 2003; Gildea 2003), but also for summarization (Knight and Marcu 2002), paraphrasing (Pang, Knight, and Marcu 2003), natural language generation (Langkilde and Knight 1998; Bangalore and Rambow 2000; Corston-Oliver et al. 2002), parsing, and language modeling (Baker 1979; Lari and Young 1990; Collins 1997; Chelba and Jelinek 2000; Charniak 2001; Klein ∗ ∗∗ † 1 Information Sciences Institute, 4676 Admiralty Way, Marina del Rey, CA 90292. E-mail: graehl@isi.edu. Information Sciences Institute, 4676 Admiralty Way, Marina del Rey, CA 90292. E-mail: knight@isi.edu. Information Sciences Institute, 4676 Admiralty Way, Marina del Rey, CA 90292. E-mail: jonmay@isi.edu. www.research.att.com/sw/tools/fsm and www.isi.edu/licensed-sw/carmel. Submission received: 30 October 2003; revised submission received: 30 August 2007; accepted for publication: 20 October 2007. © 2008 Association for Computational Linguistics Computational Linguistics Volume"
J08-3004,P97-1003,0,0.209903,"olve massive re-ordering of symbols, and because the transformation processes seem sensitive to hierarchical tree structure. Recently, speciﬁc probabilistic tree-based models have been proposed not only for machine translation (Wu 1997; Alshawi, Bangalore, and Douglas 2000; Yamada and Knight 2001; Eisner 2003; Gildea 2003), but also for summarization (Knight and Marcu 2002), paraphrasing (Pang, Knight, and Marcu 2003), natural language generation (Langkilde and Knight 1998; Bangalore and Rambow 2000; Corston-Oliver et al. 2002), parsing, and language modeling (Baker 1979; Lari and Young 1990; Collins 1997; Chelba and Jelinek 2000; Charniak 2001; Klein ∗ ∗∗ † 1 Information Sciences Institute, 4676 Admiralty Way, Marina del Rey, CA 90292. E-mail: graehl@isi.edu. Information Sciences Institute, 4676 Admiralty Way, Marina del Rey, CA 90292. E-mail: knight@isi.edu. Information Sciences Institute, 4676 Admiralty Way, Marina del Rey, CA 90292. E-mail: jonmay@isi.edu. www.research.att.com/sw/tools/fsm and www.isi.edu/licensed-sw/carmel. Submission received: 30 October 2003; revised submission received: 30 August 2007; accepted for publication: 20 October 2007. © 2008 Association for Computational Ling"
J08-3004,W02-2105,0,0.00505275,"eration. However, language problems like machine translation break this mold, because they involve massive re-ordering of symbols, and because the transformation processes seem sensitive to hierarchical tree structure. Recently, speciﬁc probabilistic tree-based models have been proposed not only for machine translation (Wu 1997; Alshawi, Bangalore, and Douglas 2000; Yamada and Knight 2001; Eisner 2003; Gildea 2003), but also for summarization (Knight and Marcu 2002), paraphrasing (Pang, Knight, and Marcu 2003), natural language generation (Langkilde and Knight 1998; Bangalore and Rambow 2000; Corston-Oliver et al. 2002), parsing, and language modeling (Baker 1979; Lari and Young 1990; Collins 1997; Chelba and Jelinek 2000; Charniak 2001; Klein ∗ ∗∗ † 1 Information Sciences Institute, 4676 Admiralty Way, Marina del Rey, CA 90292. E-mail: graehl@isi.edu. Information Sciences Institute, 4676 Admiralty Way, Marina del Rey, CA 90292. E-mail: knight@isi.edu. Information Sciences Institute, 4676 Admiralty Way, Marina del Rey, CA 90292. E-mail: jonmay@isi.edu. www.research.att.com/sw/tools/fsm and www.isi.edu/licensed-sw/carmel. Submission received: 30 October 2003; revised submission received: 30 August 2007; accep"
J08-3004,P79-1022,0,0.810927,"simply the ﬁrst projection). For every weighted context-free grammar, there is an equivalent wRTG that generates its weighted derivation trees (whose yield is a string in the context-free language), and the yield of any regular tree language is a context-free string language (Gécseg and Steinby 1984). We can also interpret a regular tree grammar as a context-free string grammar with alphabet Σ ∪ {(, )}. wRTGs generate (ignoring weights) exactly the recognizable tree languages, which are sets of trees accepted by a non-transducing automaton version of T. This acceptor automaton is described in Doner (1970) and is actually a closer mechanical analogue to an FSA than is the rewrite-rule-based wRTG. RTGs are closed under intersection (Gécseg and Steinby 1984), and the constructive proof also applies to weighted wRTG intersection. There is a normal form for wRTGs analogous to that of regular grammars: Right-hand sides are a single terminal root with (optional) nonterminal children. What is sometimes called a forest in natural language generation (Langkilde 2000; Nederhof and Satta 2002) is a ﬁnite wRTG without loops—for all valid derivation trees, each nonterminal may only occur once in any path fr"
J08-3004,P02-1001,0,0.0718822,"Missing"
J08-3004,P03-2041,0,0.890805,"tware toolkits like the AT&T FSM Library and USC/ISI’s Carmel.1 Moreover, a surprising variety of problems are attackable with FSTs, from part-of-speech tagging to letter-to-sound conversion to name transliteration. However, language problems like machine translation break this mold, because they involve massive re-ordering of symbols, and because the transformation processes seem sensitive to hierarchical tree structure. Recently, speciﬁc probabilistic tree-based models have been proposed not only for machine translation (Wu 1997; Alshawi, Bangalore, and Douglas 2000; Yamada and Knight 2001; Eisner 2003; Gildea 2003), but also for summarization (Knight and Marcu 2002), paraphrasing (Pang, Knight, and Marcu 2003), natural language generation (Langkilde and Knight 1998; Bangalore and Rambow 2000; Corston-Oliver et al. 2002), parsing, and language modeling (Baker 1979; Lari and Young 1990; Collins 1997; Chelba and Jelinek 2000; Charniak 2001; Klein ∗ ∗∗ † 1 Information Sciences Institute, 4676 Admiralty Way, Marina del Rey, CA 90292. E-mail: graehl@isi.edu. Information Sciences Institute, 4676 Admiralty Way, Marina del Rey, CA 90292. E-mail: knight@isi.edu. Information Sciences Institute, 4676"
J08-3004,P03-1011,0,0.0923722,"s like the AT&T FSM Library and USC/ISI’s Carmel.1 Moreover, a surprising variety of problems are attackable with FSTs, from part-of-speech tagging to letter-to-sound conversion to name transliteration. However, language problems like machine translation break this mold, because they involve massive re-ordering of symbols, and because the transformation processes seem sensitive to hierarchical tree structure. Recently, speciﬁc probabilistic tree-based models have been proposed not only for machine translation (Wu 1997; Alshawi, Bangalore, and Douglas 2000; Yamada and Knight 2001; Eisner 2003; Gildea 2003), but also for summarization (Knight and Marcu 2002), paraphrasing (Pang, Knight, and Marcu 2003), natural language generation (Langkilde and Knight 1998; Bangalore and Rambow 2000; Corston-Oliver et al. 2002), parsing, and language modeling (Baker 1979; Lari and Young 1990; Collins 1997; Chelba and Jelinek 2000; Charniak 2001; Klein ∗ ∗∗ † 1 Information Sciences Institute, 4676 Admiralty Way, Marina del Rey, CA 90292. E-mail: graehl@isi.edu. Information Sciences Institute, 4676 Admiralty Way, Marina del Rey, CA 90292. E-mail: knight@isi.edu. Information Sciences Institute, 4676 Admiralty Way,"
J08-3004,N04-1014,1,0.39167,"Missing"
J08-3004,P03-1054,0,0.0551946,"Missing"
J08-3004,knight-al-onaizan-1998-translation,1,0.794918,"Missing"
J08-3004,N03-1017,0,0.11059,"Missing"
J08-3004,N03-1019,0,0.035142,"Missing"
J08-3004,A00-2023,0,0.0140971,"nizable tree languages, which are sets of trees accepted by a non-transducing automaton version of T. This acceptor automaton is described in Doner (1970) and is actually a closer mechanical analogue to an FSA than is the rewrite-rule-based wRTG. RTGs are closed under intersection (Gécseg and Steinby 1984), and the constructive proof also applies to weighted wRTG intersection. There is a normal form for wRTGs analogous to that of regular grammars: Right-hand sides are a single terminal root with (optional) nonterminal children. What is sometimes called a forest in natural language generation (Langkilde 2000; Nederhof and Satta 2002) is a ﬁnite wRTG without loops—for all valid derivation trees, each nonterminal may only occur once in any path from root to a leaf: ∀n ∈ N, t ∈ TΣ (N), h ∈ (paths × P)∗ : (n, ()) ⇒∗G (t, h) =⇒ pathst ({n} ) = ∅ RTGs produce tree sets equivalent to those produced by tree substitution grammars (TSGs) (Schabes 1990) up to relabeling. The relabeling is necessary because RTGs distinguish states and tree symbols, which are conﬂated in TSGs at the elementary tree root. Regular tree languages are strictly contained in tree sets of tree adjoining grammars (TAG; Joshi and Scha"
J08-3004,P98-1116,1,0.462175,"tagging to letter-to-sound conversion to name transliteration. However, language problems like machine translation break this mold, because they involve massive re-ordering of symbols, and because the transformation processes seem sensitive to hierarchical tree structure. Recently, speciﬁc probabilistic tree-based models have been proposed not only for machine translation (Wu 1997; Alshawi, Bangalore, and Douglas 2000; Yamada and Knight 2001; Eisner 2003; Gildea 2003), but also for summarization (Knight and Marcu 2002), paraphrasing (Pang, Knight, and Marcu 2003), natural language generation (Langkilde and Knight 1998; Bangalore and Rambow 2000; Corston-Oliver et al. 2002), parsing, and language modeling (Baker 1979; Lari and Young 1990; Collins 1997; Chelba and Jelinek 2000; Charniak 2001; Klein ∗ ∗∗ † 1 Information Sciences Institute, 4676 Admiralty Way, Marina del Rey, CA 90292. E-mail: graehl@isi.edu. Information Sciences Institute, 4676 Admiralty Way, Marina del Rey, CA 90292. E-mail: knight@isi.edu. Information Sciences Institute, 4676 Admiralty Way, Marina del Rey, CA 90292. E-mail: jonmay@isi.edu. www.research.att.com/sw/tools/fsm and www.isi.edu/licensed-sw/carmel. Submission received: 30 October"
J08-3004,W02-1018,0,0.0337006,"Missing"
J08-3004,P02-1015,0,0.0176924,"guages, which are sets of trees accepted by a non-transducing automaton version of T. This acceptor automaton is described in Doner (1970) and is actually a closer mechanical analogue to an FSA than is the rewrite-rule-based wRTG. RTGs are closed under intersection (Gécseg and Steinby 1984), and the constructive proof also applies to weighted wRTG intersection. There is a normal form for wRTGs analogous to that of regular grammars: Right-hand sides are a single terminal root with (optional) nonterminal children. What is sometimes called a forest in natural language generation (Langkilde 2000; Nederhof and Satta 2002) is a ﬁnite wRTG without loops—for all valid derivation trees, each nonterminal may only occur once in any path from root to a leaf: ∀n ∈ N, t ∈ TΣ (N), h ∈ (paths × P)∗ : (n, ()) ⇒∗G (t, h) =⇒ pathst ({n} ) = ∅ RTGs produce tree sets equivalent to those produced by tree substitution grammars (TSGs) (Schabes 1990) up to relabeling. The relabeling is necessary because RTGs distinguish states and tree symbols, which are conﬂated in TSGs at the elementary tree root. Regular tree languages are strictly contained in tree sets of tree adjoining grammars (TAG; Joshi and Schabes 1997), which generate"
J08-3004,W99-0604,0,0.296338,"Missing"
J08-3004,N03-1024,1,0.2473,"Missing"
J08-3004,C69-0101,0,0.730689,"ormation Sciences Institute, 4676 Admiralty Way, Marina del Rey, CA 90292. E-mail: knight@isi.edu. Information Sciences Institute, 4676 Admiralty Way, Marina del Rey, CA 90292. E-mail: jonmay@isi.edu. www.research.att.com/sw/tools/fsm and www.isi.edu/licensed-sw/carmel. Submission received: 30 October 2003; revised submission received: 30 August 2007; accepted for publication: 20 October 2007. © 2008 Association for Computational Linguistics Computational Linguistics Volume 34, Number 3 and Manning 2003). It is useful to understand generic algorithms that may support all these tasks and more. Rounds (1970) and Thatcher (1970) independently introduced tree transducers as a generalization of FSTs. Rounds was motivated by natural language: Recent developments in the theory of automata have pointed to an extension of the domain of deﬁnition of automata from strings to trees . . . parts of mathematical linguistics can be formalized easily in a tree-automaton setting . . . We investigate decision problems and closure properties. Our results should clarify the nature of syntax-directed translations and transformational grammars . . . (Rounds 1970) The Rounds/Thatcher tree transducer is very similar to"
J08-3004,J97-3002,0,0.0722114,"67) and forward–backward training (Baum and Eagon 1967), as well as software toolkits like the AT&T FSM Library and USC/ISI’s Carmel.1 Moreover, a surprising variety of problems are attackable with FSTs, from part-of-speech tagging to letter-to-sound conversion to name transliteration. However, language problems like machine translation break this mold, because they involve massive re-ordering of symbols, and because the transformation processes seem sensitive to hierarchical tree structure. Recently, speciﬁc probabilistic tree-based models have been proposed not only for machine translation (Wu 1997; Alshawi, Bangalore, and Douglas 2000; Yamada and Knight 2001; Eisner 2003; Gildea 2003), but also for summarization (Knight and Marcu 2002), paraphrasing (Pang, Knight, and Marcu 2003), natural language generation (Langkilde and Knight 1998; Bangalore and Rambow 2000; Corston-Oliver et al. 2002), parsing, and language modeling (Baker 1979; Lari and Young 1990; Collins 1997; Chelba and Jelinek 2000; Charniak 2001; Klein ∗ ∗∗ † 1 Information Sciences Institute, 4676 Admiralty Way, Marina del Rey, CA 90292. E-mail: graehl@isi.edu. Information Sciences Institute, 4676 Admiralty Way, Marina del R"
J08-3004,P01-1067,1,0.0800498,"on 1967), as well as software toolkits like the AT&T FSM Library and USC/ISI’s Carmel.1 Moreover, a surprising variety of problems are attackable with FSTs, from part-of-speech tagging to letter-to-sound conversion to name transliteration. However, language problems like machine translation break this mold, because they involve massive re-ordering of symbols, and because the transformation processes seem sensitive to hierarchical tree structure. Recently, speciﬁc probabilistic tree-based models have been proposed not only for machine translation (Wu 1997; Alshawi, Bangalore, and Douglas 2000; Yamada and Knight 2001; Eisner 2003; Gildea 2003), but also for summarization (Knight and Marcu 2002), paraphrasing (Pang, Knight, and Marcu 2003), natural language generation (Langkilde and Knight 1998; Bangalore and Rambow 2000; Corston-Oliver et al. 2002), parsing, and language modeling (Baker 1979; Lari and Young 1990; Collins 1997; Chelba and Jelinek 2000; Charniak 2001; Klein ∗ ∗∗ † 1 Information Sciences Institute, 4676 Admiralty Way, Marina del Rey, CA 90292. E-mail: graehl@isi.edu. Information Sciences Institute, 4676 Admiralty Way, Marina del Rey, CA 90292. E-mail: knight@isi.edu. Information Sciences Ins"
J08-3004,P02-1039,1,0.67228,"Missing"
J08-3004,J91-1004,0,\N,Missing
J08-3004,J97-4005,0,\N,Missing
J08-3004,1987.mtsummit-1.11,0,\N,Missing
J08-3004,C98-1112,1,\N,Missing
J08-3004,P96-1029,0,\N,Missing
J09-4009,P05-1033,0,0.869126,"mars and present a linear-time algorithm for binarizing synchronous rules when possible. In our large-scale experiments, we found that almost all rules are binarizable and the resulting binarized rule set significantly improves the speed and accuracy of a state-of-the-art syntaxbased machine translation system. We also discuss the more general, and computationally more difficult, problem of finding good parsing strategies for non-binarizable rules, and present an approximate polynomial-time algorithm for this problem. 1. Introduction Several recent syntax-based models for machine translation (Chiang 2005; Galley et al. 2004) can be seen as instances of the general framework of synchronous grammars and tree transducers. In this framework, both alignment (synchronous parsing) and decoding can be thought of as parsing problems, whose complexity is in general exponential in the number of nonterminals on the right-hand side of a grammar rule. To alleviate this problem, we investigate bilingual binarization as a technique to factor each synchronous grammar rule into a series of binary rules. Although monolingual context-free grammars (CFGs) can always be binarized, this is not the case ∗ Informatio"
J09-4009,P03-2041,0,0.106836,"reorderings of nonterminals are denoted by variables xi . In the treetransducer formalism of Rounds (1970), the right-hand (target) side subtree can have multiple levels, as in the first rule above. This system can model non-isomorphic transformations on English parse trees to “fit” another language, learning, for example, that the (V S O) structure in Arabic should be transformed into a (S (V O)) structure in English, by looking at two-level tree fragments (Knight and Graehl 2005). From a synchronous rewriting point of view, this is more akin to synchronous tree substitution grammar (STSG) (Eisner 2003; Shieber 2004) (see Figure 3). This larger locality captures more linguistic phenomena and leads to better parameter estimation. By creating a 563 Computational Linguistics Volume 35, Number 4 Figure 3 Two equivalent representations of the first rule in Example (5): (a) tree transducer; (b) Synchronous Tree Subsitution Grammar (STSG). The ↓ arrows denote substitution sites, which correspond to variables in tree transducers. nonterminal for each right-hand-side tree, we can convert the transducer representation to an SCFG with the same generative capacity. We can again create a projected CFG w"
J09-4009,N04-1035,1,0.843738,"ent a linear-time algorithm for binarizing synchronous rules when possible. In our large-scale experiments, we found that almost all rules are binarizable and the resulting binarized rule set significantly improves the speed and accuracy of a state-of-the-art syntaxbased machine translation system. We also discuss the more general, and computationally more difficult, problem of finding good parsing strategies for non-binarizable rules, and present an approximate polynomial-time algorithm for this problem. 1. Introduction Several recent syntax-based models for machine translation (Chiang 2005; Galley et al. 2004) can be seen as instances of the general framework of synchronous grammars and tree transducers. In this framework, both alignment (synchronous parsing) and decoding can be thought of as parsing problems, whose complexity is in general exponential in the number of nonterminals on the right-hand side of a grammar rule. To alleviate this problem, we investigate bilingual binarization as a technique to factor each synchronous grammar rule into a series of binary rules. Although monolingual context-free grammars (CFGs) can always be binarized, this is not the case ∗ Information Science Institute,"
J09-4009,N07-1019,1,0.850542,"Missing"
J09-4009,W05-1506,1,0.824427,"ecting the constraints from the other side. This scheme generalizes to the case where we have n nonterminals in a SCFG rule, and the decoder conservatively assumes nothing can be done on language model scoring (because target-language spans are non-contiguous in general) until the real nonterminal has been recognized. In other words, target-language boundary words 2 An alternative to integrated decoding is rescoring, where one first computes the k-best translations according to the TM only, and then reranks the k-best list with the language model costs. This method runs very fast in practice (Huang and Chiang 2005), but often produces a considerable number of search errors because the true best translation is often outside of the k-best list, especially for longer sentences. 562 Huang et al. Binarization of Synchronous Context-Free Grammars from each child nonterminal of the rule will be cached in all virtual nonterminals derived from this rule. In the case of m-gram integrated decoding, we have to maintain 2(m − 1) boundary words for each child nonterminal, which leads to a prohibitive overall complexity of O(|w|3+2n(m−1) ), which is exponential in rule size (Huang, Zhang, and Gildea 2005). Aggressive"
J09-4009,W05-1507,1,0.919163,"Missing"
J09-4009,P05-1057,0,0.0471912,"Missing"
J09-4009,N03-1021,0,0.294945,"real examples of non-binarizable cases verified by native speakers. In the final, theoretical, sections of this article, we investigate the general problem of finding the most efficient synchronous parsing or decoding strategy for arbitrary synchronous context-free grammar (SCFG) rules, including non-binarizable cases. Although this problem is believed to be NP-complete, we prove two results that substantially reduce the search space over strategies. We also present an optimal algorithm that runs tractably in practice and a polynomial-time algorithm that is a good approximation of the former. Melamed (2003) discusses binarization of multi-text grammars on a theoretical level, showing the importance and difficulty of binarization for efficient synchronous parsing. One way around this difficulty is to stipulate that all rules must be binary from the outset, as in Inversion Transduction Grammar (ITG) (Wu 1997) and the binary SCFG employed by the Hiero system (Chiang 2005) to model the hierarchical phrases. In contrast, the rule extraction method of Galley et al. (2004) aims to incorporate more syntactic information by providing parse trees for the target language and extracting tree transducer rule"
J09-4009,W03-0301,0,0.0464804,"Missing"
J09-4009,J03-1006,0,0.1198,"Missing"
J09-4009,J04-4002,0,0.117917,"e 13 Comparing the two binarization methods in terms of translation quality against search effort. Table 2 Machine translation results for syntax-based systems vs. the phrase-based Alignment Template System. System BLEU monolingual binarization synchronous binarization alignment-template system 36.25 38.44 37.00 decoding is used as a measure of the size of search space, or time efficiency. Our system is consistently faster and more accurate than the baseline system. We also compare the top result of our synchronous binarization system with the state-of-the-art alignment-template system (ATS) (Och and Ney 2004). The results are shown in Table 2. Our system has a promising improvement over the ATS system, which is trained on a larger data set but tuned independently. A larger-scale system based on our best result performs very well in the 2006 NIST MT Evaluation (ISI Machine Translation Team 2006), achieving the best overall BLEU scores in the Chineseto-English track among all participants.4 The readers are referred to Galley et al. (2004) for details of the decoder and the overall system. 6. One-Sided Binarization In this section and the following section, we discuss techniques for handling rules th"
J09-4009,C69-0101,0,0.65771,"th source- and target-sides, so that we can generate a binary-branching SCFG: (4) S PP-VP → → NP 1 PP-VP 2 , VP 1 PP 2 , NP 1 PP-VP 2 PP 2 VP 1 In this case m-gram integrated decoding can be done in O(|w|3+4(m−1) ) time, which is a much lower-order polynomial and no longer depends on rule size (Wu 1996), allowing the search to be much faster and more accurate, as is evidenced in the Hiero system of Chiang (2005), which restricts the hierarchical phrases to form binary-branching SCFG rules. Some recent syntax-based MT systems (Galley et al. 2004) have adopted the formalism of tree transducers (Rounds 1970), modeling translation as a set of rules for a transducer that takes a syntax tree in one language as input and transforms it into a tree (or string) in the other language. The same decoding algorithms are used for machine translation in this formalism, and the following example shows that the same issues of binarization arise. Suppose we have the following transducer rules: (5) S(x1 :NP x2 :PP x3 :VP) NP( / B`aow¯eier) VP( / jux´ ˇ ıng le hu`ıt´an) PP( / yuˇ Sh¯al´ong)  >L     → → → → S(x1 VP(x3 x2 )) NP(NNP(Powell)) VP(VBD(held) NP(DT(a) NPS(meeting))) PP(TO(with) NP(NNP(Sharon))) w"
J09-4009,H05-1101,0,0.806039,"binarizing a tree-transducer rule, and consider only the alignment (or permutation) of the nonterminal variables. Again, rightmost binarization is preferable for the first rule. In SCFG-based frameworks, the problem of finding a word-level alignment between two sentences is an instance of the synchronous parsing problem: Given two strings and a synchronous grammar, find a parse tree that generates both input strings. The benefit of binary grammars also applies in this case. Wu (1997) shows that parsing a binary-branching SCFG is in O(|w|6 ), while parsing SCFG with arbitrary rules is NP-hard (Satta and Peserico 2005). For example, in Figure 2, the complexity of synchronous parsing for the original grammar (a) is O(|w|8 ), because we have to maintain four indices on either side, giving a total of eight; parsing the monolingually binarized grammar (b) involves seven indices, three on the Chinese side and four on the English side. In contrast, the synchronously binarized version (c) requires only 3 + 3 = 6 indices, which can be thought of as “CKY in two dimensions.” An efficient alignment algorithm is guaranteed if a binarization is found, and the same binarization can be used for decoding and alignment. We"
J09-4009,W04-3312,0,0.0458485,"of nonterminals are denoted by variables xi . In the treetransducer formalism of Rounds (1970), the right-hand (target) side subtree can have multiple levels, as in the first rule above. This system can model non-isomorphic transformations on English parse trees to “fit” another language, learning, for example, that the (V S O) structure in Arabic should be transformed into a (S (V O)) structure in English, by looking at two-level tree fragments (Knight and Graehl 2005). From a synchronous rewriting point of view, this is more akin to synchronous tree substitution grammar (STSG) (Eisner 2003; Shieber 2004) (see Figure 3). This larger locality captures more linguistic phenomena and leads to better parameter estimation. By creating a 563 Computational Linguistics Volume 35, Number 4 Figure 3 Two equivalent representations of the first rule in Example (5): (a) tree transducer; (b) Synchronous Tree Subsitution Grammar (STSG). The ↓ arrows denote substitution sites, which correspond to variables in tree transducers. nonterminal for each right-hand-side tree, we can convert the transducer representation to an SCFG with the same generative capacity. We can again create a projected CFG which will be ex"
J09-4009,C90-3045,0,0.379979,"arizable SCFGs, and are mainly of theoretical interest. Algorithms 1–3 make fewer and fewer assumptions on the strategy space, and produce parsing strategies closer and closer to the optimal. Algorithm 4 further improves Algorithm 3. Section Algorithm Assumptions of Strategy Space Complexity 3–4 6 Alg. 1 (synchronous) Alg. 2 (one-sided, CKY) Alg. 3 (optimal) ⇒ Alg. 4 (best-first) Contiguous on both sides Contiguous on one side O(n) O(n3 ) O(3n ) O(9k n2k ) 7.2 No assumptions systems improve. Synchronous grammars that go beyond the power of SCFG (and therefore binary SCFG) have been defined by Shieber and Schabes (1990) and Rambow and Satta (1999), and motivated for machine translation by Melamed (2003), although previous work has not given algorithms for finding efficient and optimal parsing strategies for general SCFGs, which we believe is an important problem. In the remainder of this section and the next section, we will present a series of algorithms that produce increasingly faster parsing strategies, by gradually relaxing the strong “continuity” constraint made by the synchronous binarization technique. As that technique requires continuity on both languages, we will first study a relaxation where bin"
J09-4009,P06-1123,0,0.0494632,"Missing"
J09-4009,P96-1021,0,0.347375,"NP and PP into an intermediate state which contains a gap on the English side. (c) This scheme groups PP and VP into an intermediate state which is contiguous on both sides. These two binarizations are no different in the translation-model-only decoding described previously, just as in monolingual parsing. However, in the source-channel approach to machine translation, we need to combine probabilities from the translation model (TM) (an SCFG) with the language model (an n-gram), which has been shown to be very important for translation quality (Chiang 2005). To do bigram-integrated decoding (Wu 1996), we need to augment each chart item (X, i, j) with twoÃ target! u ··· v language boundary words u and v to produce a bigram-item which we denote i X j .2 Now the two binarizations have very different effects. In the first case, we first combine NP with PP. This step is written as follows in the weighted deduction notation of Nederhof (2003): ¶ µ ¶ µ Powell ··· Powell with ··· Sharon :q :p NP PP 2 4 2 µ1 ¶ Powell ··· Powell ··· with ··· Sharon : pq NP-PP 1 4 where p and q are the scores of antecedent items. This situation is unpleasant because in the target language NP and PP are not contiguou"
J09-4009,J97-3002,0,0.790505,"e cases. Although this problem is believed to be NP-complete, we prove two results that substantially reduce the search space over strategies. We also present an optimal algorithm that runs tractably in practice and a polynomial-time algorithm that is a good approximation of the former. Melamed (2003) discusses binarization of multi-text grammars on a theoretical level, showing the importance and difficulty of binarization for efficient synchronous parsing. One way around this difficulty is to stipulate that all rules must be binary from the outset, as in Inversion Transduction Grammar (ITG) (Wu 1997) and the binary SCFG employed by the Hiero system (Chiang 2005) to model the hierarchical phrases. In contrast, the rule extraction method of Galley et al. (2004) aims to incorporate more syntactic information by providing parse trees for the target language and extracting tree transducer rules that apply to the parses. This approach results in rules with many nonterminals, making good binarization techniques critical. We explain how synchronous rule binarization interacts with n-gram language models and affects decoding for machine translation in Section 2. We define binarization formally in"
J09-4009,W07-0404,1,0.868562,"g one nonterminal at a time. The optimal grouping of nonterminals is shown on the right. time O(|w|10 ) by adding one nonterminal at a time. All permutations of less than eight elements can be optimally parsed by adding one element at a time. 7.4 Discontinuous Parsing Is Necessary Only for Non-Decomposable Permutations In this subsection, we show that an optimal parsing strategy can be found by first factoring an SCFG rule into a sequence of shorter SCFG rules, if possible, and then considering each of the new rules independently. The first step can be done efficiently using the algorithms of Zhang and Gildea (2007). The second step can be done in time O(9kc · n2kc ) using Algorithm 4, where kc is the complexity of the longest SCFG rule after factorizations, implying that kc ≤ (n + 4). We show that this two-step process is optimal, by proving that the optimal parsing strategy for the initial rule will not need to build subsets of children that cross the boundaries of the factorization into shorter SCFG rules. Figure 19 shows a permutation that contains permutations of fewer numbers within itself so that the entire permutation can be decomposed hierarchically. We prove that if there is a contiguous block"
J09-4009,N06-1033,1,0.518184,"present a decoding strategy for these rules in Section 6. Section 7 gives a solution to the general theoretical problem of finding optimal decoding and synchronous parsing strategies for arbitrary SCFGs, and presents complexity results on the nonbinarizable rules from our Chinese–English data. These final two sections are of primarily theoretical interest, as nonbinarizable rules have not been shown to benefit real-world machine translation systems. However, the algorithms presented may become relevant as machine translation systems improve. 1 A preliminary version of Section 1–5 appeared in Zhang et al. (2006). 560 Huang et al. Binarization of Synchronous Context-Free Grammars 2. Motivation Consider the following Chinese sentence and its English translation: (1)    >L  B`aow¯eier yuˇ Sh¯al´ong jux´ ˇ ıng le Powell with Sharon hold [past.] “Powell held a meeting with Sharon”  hu`ıt´an meeting Suppose we have the following SCFG, where superscripts indicate reorderings (formal definitions of SCFGs with a more flexible notation can be found in Section 3): (2) S NP VP PP → → → → NP 1 PP 2 VP 3 , / B`aow¯eier, / jux´ ˇ ıng le hu`ıt´an, / yuˇ Sh¯al´ong,  >L     NP 1 VP 3 PP 2 Powell held"
J09-4009,W07-0405,1,\N,Missing
J09-4009,P06-1121,1,\N,Missing
J09-4009,W90-0102,0,\N,Missing
J10-2004,P98-1006,0,0.0838333,"Missing"
J10-2004,J93-2003,0,0.0129373,"Missing"
J10-2004,A00-2018,0,0.126804,"yle trees. One striking fact about these trees is that they contain many ﬂat structures. For example, base noun phrases frequently have ﬁve or more direct children. It is well known in monolingual parsing research that these ﬂat structures cause problems. Although thousands of rewrite rules can be learned from the Penn Treebank, these rules still do not cover the new rewrites observed in held-out test data. For this reason, and to extract more general knowledge, many monolingual parsing models are markovized so that they can produce ﬂat structures incrementally and horizontally (Collins 1997; Charniak 2000). Other parsing systems binarize the training trees in a pre-processing step, then learn to model the binarized corpus (Petrov et al. 2006); after parsing, their results are ﬂattened back in a post-processing step. In addition, Johnson (1998b) shows that different types of tree structuring (e.g., the Chomsky adjunction representation vs. the Penn Treebank II representation) can have a large effect on the parsing performance of a PCFG estimated from these trees. We ﬁnd that ﬂat structures are also problematic for syntax-based machine translation. The rules we learn from tree/string/alignment tr"
J10-2004,J07-2003,0,0.240062,"putational Linguistics Volume 36, Number 2 Figure 3 Additional rules extracted from the learning case in Figure 1. store English words that appear at the left and right corners of the tree, as these are needed for computing the P(e) score when cells are combined. For CKY to work, all transducer rules must be broken down, or binarized, into rules that contain at most two variables—more efﬁcient search can be gained if this binarization produces rules that can be incrementally scored by the language model (Melamed, Satta, and Wellington 2004; Zhang et al. 2006). Finally, we employ cube pruning (Chiang 2007) for further efﬁciency in the search. When scoring translation candidates, we add several smaller models. One model rewards longer translation candidates, off-setting the language model’s desire for short output. Other models punish rules that drop Chinese content words or introduce spurious English content words. We also include lexical smoothing models (Gale and Sampson 1996; Good 1953) to help distinguish good low-count rules from bad lowcount rules. The ﬁnal score of a translation candidate is a weighted linear combination of log P(e), log P(e, c), and the scores from these additional smal"
J10-2004,D09-1037,0,0.100934,"Missing"
J10-2004,P97-1003,0,0.0914444,"ng, but in this work we concentrate only on target-language syntax. The target-language generation problem presents a difﬁcult challenge, whereas the source sentence is ﬁxed and usually already grammatical. To prepare training data for such a system, we begin with a bilingual text that has been automatically processed into segment pairs. We require that the segments be single sentences on the English side, whereas the corresponding Chinese segments may be sentences, sentence fragments, or multiple sentences. We then parse the English side of the bilingual text using a re-implementation of the Collins (1997) parsing model, which we train on the Penn English Treebank (Marcus, Santorini, and Marcinkiewicz 1993). Finally, we word-align the segment pairs according to IBM Model 4 (Brown et al. 1993). Figure 1 shows a sample (tree, string, alignment) triple. We build two generative statistical models from this data. First, we construct a smoothed n-gram language model (Kneser and Ney 1995; Stolcke 2002) out of the English side of the bilingual data. This model assigns a probability P(e) to any candidate translation, rewarding translations whose subsequences have been observed frequently in the training"
J10-2004,D07-1079,1,0.88608,"traction method assigns each unaligned Chinese word to a default rule in the derivation tree. We next follow Galley et al. (2006) in allowing unaligned Chinese words to participate in multiple translation rules. In this case, we obtain a derivation forest of minimal rules. Galley et al. show how to use EM to count rules over derivation forests and obtain Viterbi derivation trees of minimal rules. We also follow Galley et al. in collecting composed rules, namely, compositions of minimal rules. These larger rules have been shown to substantially improve translation accuracy (Galley et al. 2006; DeNeefe et al. 2007). Figure 3 shows some of the additional rules. With these models, we can decode a new Chinese sentence by enumerating and scoring all of the English trees that can be derived from it by rule. The score is a weighted product of P(e) and P(e, c). To search efﬁciently, we employ the CKY dynamicprogramming parsing algorithm (Yamada and Knight 2002; Galley et al. 2006). This algorithm builds English trees on top of Chinese spans. In each cell of the CKY matrix, we store the non-terminal symbol at the root of the English tree being built up. We also Figure 2 Minimal rules extracted from the learning"
J10-2004,P06-1121,1,0.821544,"and deletion (R2, R4), and tree traversal (R9, R7). The extracted rules for a given example form a derivation tree. We collect all rules over the entire bilingual corpus, and we normalize rule counts count(rule) in this way: P(rule) = count(LHS -root(rule)) . When we apply these probabilities to derive an English sentence e and a corresponding Chinese sentence c, we wind up computing the joint probability P(e, c). We smooth the rule counts with Good–Turing smoothing (Good 1953). This extraction method assigns each unaligned Chinese word to a default rule in the derivation tree. We next follow Galley et al. (2006) in allowing unaligned Chinese words to participate in multiple translation rules. In this case, we obtain a derivation forest of minimal rules. Galley et al. show how to use EM to count rules over derivation forests and obtain Viterbi derivation trees of minimal rules. We also follow Galley et al. in collecting composed rules, namely, compositions of minimal rules. These larger rules have been shown to substantially improve translation accuracy (Galley et al. 2006; DeNeefe et al. 2007). Figure 3 shows some of the additional rules. With these models, we can decode a new Chinese sentence by enu"
J10-2004,N04-1035,1,0.876041,"re 1 A sample learning case for the syntax-based machine translation system described in this article. 248 Wang et al. Re-structuring, Re-labeling, and Re-aligning SMT (Brown et al. 1993), our model operates in the English-to-Chinese direction— we envision a generative top–down process by which an English tree is gradually transformed (by probabilistic rules) into an observed Chinese string. We represent a collection of such rules as a tree transducer (Knight and Graehl 2005). In order to construct this transducer from parsed and word-aligned data, we use the GHKM rule extraction algorithm of Galley et al. (2004). This algorithm computes the unique set of minimal rules needed to explain any sentence pair in the data. Figure 2 shows all the minimal rules extracted from the example (tree, string, alignment) triple in Figure 1. Note that rules specify rotation (e.g., R1, R5), direct translation (R3, R10), insertion and deletion (R2, R4), and tree traversal (R9, R7). The extracted rules for a given example form a derivation tree. We collect all rules over the entire bilingual corpus, and we normalize rule counts count(rule) in this way: P(rule) = count(LHS -root(rule)) . When we apply these probabilities"
J10-2004,J99-4004,0,0.0216439,"ubstructure that yields NNP2 , NNP3 , and their translational equivalences. To obtain more factorizable sub-phrases, we need to parallel-binarize in both directions. 254 Wang et al. Re-structuring, Re-labeling, and Re-aligning Figure 7 Packed forest obtained by packing trees (3) and (6) in Figure 6. 3.2.2 Parallel Binarization. Simple binarizations transform a parse tree into another single parse tree. Parallel binarization transforms a parse tree into a binarization forest, packed to enable dynamic programming when we extract translation rules from it. Borrowing terms from parsing semirings (Goodman 1999), a packed forest is composed of additive forest nodes (⊕-nodes) and multiplicative forest nodes (⊗-nodes). In the binarization forest, a ⊗-node corresponds to a tree node in the unbinarized tree or a new tree node introduced during tree binarization; and this ⊗-node composes several ⊕-nodes, forming a one-level substructure that is observed in the unbinarized tree or in one of its binarized tree. A ⊕-node corresponds to alternative ways of binarizing the same tree node and it contains one or more ⊗-nodes. The same ⊕-node can appear in more than one place in the packed forest, enabling sharing"
J10-2004,N06-1031,1,0.897685,"a translation candidate is a weighted linear combination of log P(e), log P(e, c), and the scores from these additional smaller models. We obtain weights through minimum error-rate training (Och 2003). The system thus constructed performs fairly well at Chinese-to-English translation, as reﬂected in the NIST06 common evaluation of machine translation quality.1 However, it would be surprising if the parse structures and word alignments in our bilingual data were somehow perfectly suited to syntax-based SMT—we have so far used out-of-the-box tools like IBM Model 4 and a Treebank-trained parser. Huang and Knight (2006) already investigated whether different syntactic labels would be more appropriate for SMT, though their study was carried out on a weak baseline translation system. In this article, we take a broad view and investigate how changes to syntactic structures, syntactic labels, and word alignments can lead to substantial improvements in translation quality on top of a strong baseline. We design our methods around problems that arise in MT data whose parses and alignments use some Penn Treebankstyle annotations. We believe that some of the techniques will apply to other annotation schemes, but conc"
J10-2004,J98-4004,0,0.110894,"use problems. Although thousands of rewrite rules can be learned from the Penn Treebank, these rules still do not cover the new rewrites observed in held-out test data. For this reason, and to extract more general knowledge, many monolingual parsing models are markovized so that they can produce ﬂat structures incrementally and horizontally (Collins 1997; Charniak 2000). Other parsing systems binarize the training trees in a pre-processing step, then learn to model the binarized corpus (Petrov et al. 2006); after parsing, their results are ﬂattened back in a post-processing step. In addition, Johnson (1998b) shows that different types of tree structuring (e.g., the Chomsky adjunction representation vs. the Penn Treebank II representation) can have a large effect on the parsing performance of a PCFG estimated from these trees. We ﬁnd that ﬂat structures are also problematic for syntax-based machine translation. The rules we learn from tree/string/alignment triples often lack sufﬁcient generalization power. For example, consider the training samples in Figure 4. We should be able to learn enough from these two samples to translate the new phrase .W #L Z Æ{ 3ä VIKTOR CHERNOMYRDIN AND HIS COLL"
J10-2004,P03-1054,0,0.00631755,"in Figure 11(b) yields ungrammatical translations like he likes reading she does not like reading. Tree binarization enables the reuse of substructures, but causes over-generation of trees at the same time. We solve the coarse-nonterminal problem by reﬁning/re-labeling the training tree labels. Re-labeling is done by enriching the nonterminal label of each tree node based on its context information. Re-labeling has already been used in monolingual parsing research to improve parsing accuracy of PCFGs. We are interested in two types of re-labeling methods: Linguistically motivated re-labeling (Klein and Manning 2003; Johnson 1998b) enriches the labels of parser training trees using parent labels, head word tag labels, and/or sibling labels. Automatic category splitting (Petrov et al. 2006) reﬁnes a nonterminal Figure 10 MT output errors due to coarse Penn Treebank annotations. Oval nodes in (b) are rule overlapping nodes. Subtree (b) is formed by composing the LHSs of R20, R21, and R22. 262 Wang et al. Re-structuring, Re-labeling, and Re-aligning Figure 11 Tree binarization over-generalizes the parse tree. Translation rules R23 and R24 are acquired from binarized training trees, aiming for reuse of subst"
J10-2004,J08-3004,1,0.883026,"Missing"
J10-2004,W04-3250,0,0.0206993,"ecause the NIST08 evaluation set is a mix of newswire text and Web text, we also report the BLEU scores on the newswire portion. We use two 5-gram language models. One is trained on the English half of the bitext. The other is trained on one billion words of monolingual data. Kneser–Ney smoothing (Kneser and Ney 1995) is applied to both language models. Language models are represented using randomized data structures similar to those of Talbot and Osborne (2007) in decoding for efﬁcient RAM usage. To test the signiﬁcance of improvements over the baseline, we compute paired bootstrap p-values (Koehn 2004) for BLEU between the baseline system and each improved system. 3. Re-structuring Trees for Training Our translation system is trained on Chinese/English data, where the English side has been automatically parsed into Penn Treebank-style trees. One striking fact about these trees is that they contain many ﬂat structures. For example, base noun phrases frequently have ﬁve or more direct children. It is well known in monolingual parsing research that these ﬂat structures cause problems. Although thousands of rewrite rules can be learned from the Penn Treebank, these rules still do not cover the"
J10-2004,W02-1018,1,0.803481,"Missing"
J10-2004,J93-2004,0,0.0464715,"Missing"
J10-2004,D07-1038,1,0.870717,"Missing"
J10-2004,P04-1084,0,0.0245554,"Missing"
J10-2004,D08-1022,0,0.0550391,"t rules from the admissible nodes in the packed forest. Rules that can be extracted from the original unrestructured tree can be extracted from the packed forest as well. Parallel binarization results in parse forests. Thus translation rules need to be extracted from training data consisting of (e-forest, f, a)-tuples. 3.3 Extracting Translation Rules from (e-forest, f, a)-tuples Our algorithm to extract rules from (e-forest, f, a)-tuples is a natural generalization of the (e-parse, f, a)-based rule extraction algorithm in Galley et al. (2006). A similar problem is also elegantly addressed in Mi and Huang (2008) in detail. The forest-based rule extraction algorithm takes as input a (e-forest, f, a)-triple, and outputs a derivation forest (Galley et al. 2006), which consists of overlapping translation rules. The algorithm recursively traverses the e-forest top–down, extracts rules only at admissible e-forest 256 Wang et al. Re-structuring, Re-labeling, and Re-aligning nodes, and transforms e-forest nodes into synchronous derivation-forest nodes via the following two procedures, depending on which condition is met. r r Condition 1: If we reach an additive e-forest node, for each of its children, which"
J10-2004,J04-4002,0,0.187296,"Missing"
J10-2004,P03-1021,0,0.0115692,"idates, we add several smaller models. One model rewards longer translation candidates, off-setting the language model’s desire for short output. Other models punish rules that drop Chinese content words or introduce spurious English content words. We also include lexical smoothing models (Gale and Sampson 1996; Good 1953) to help distinguish good low-count rules from bad lowcount rules. The ﬁnal score of a translation candidate is a weighted linear combination of log P(e), log P(e, c), and the scores from these additional smaller models. We obtain weights through minimum error-rate training (Och 2003). The system thus constructed performs fairly well at Chinese-to-English translation, as reﬂected in the NIST06 common evaluation of machine translation quality.1 However, it would be surprising if the parse structures and word alignments in our bilingual data were somehow perfectly suited to syntax-based SMT—we have so far used out-of-the-box tools like IBM Model 4 and a Treebank-trained parser. Huang and Knight (2006) already investigated whether different syntactic labels would be more appropriate for SMT, though their study was carried out on a weak baseline translation system. In this art"
J10-2004,P06-1055,0,0.704636,"e ﬁve or more direct children. It is well known in monolingual parsing research that these ﬂat structures cause problems. Although thousands of rewrite rules can be learned from the Penn Treebank, these rules still do not cover the new rewrites observed in held-out test data. For this reason, and to extract more general knowledge, many monolingual parsing models are markovized so that they can produce ﬂat structures incrementally and horizontally (Collins 1997; Charniak 2000). Other parsing systems binarize the training trees in a pre-processing step, then learn to model the binarized corpus (Petrov et al. 2006); after parsing, their results are ﬂattened back in a post-processing step. In addition, Johnson (1998b) shows that different types of tree structuring (e.g., the Chomsky adjunction representation vs. the Penn Treebank II representation) can have a large effect on the parsing performance of a PCFG estimated from these trees. We ﬁnd that ﬂat structures are also problematic for syntax-based machine translation. The rules we learn from tree/string/alignment triples often lack sufﬁcient generalization power. For example, consider the training samples in Figure 4. We should be able to learn enough"
J10-2004,P07-1065,0,0.0152868,"is from the newswire domain, and we chose it to represent a wide period of time rather than a single year. We use the NIST08 evaluation set as our test set. Because the NIST08 evaluation set is a mix of newswire text and Web text, we also report the BLEU scores on the newswire portion. We use two 5-gram language models. One is trained on the English half of the bitext. The other is trained on one billion words of monolingual data. Kneser–Ney smoothing (Kneser and Ney 1995) is applied to both language models. Language models are represented using randomized data structures similar to those of Talbot and Osborne (2007) in decoding for efﬁcient RAM usage. To test the signiﬁcance of improvements over the baseline, we compute paired bootstrap p-values (Koehn 2004) for BLEU between the baseline system and each improved system. 3. Re-structuring Trees for Training Our translation system is trained on Chinese/English data, where the English side has been automatically parsed into Penn Treebank-style trees. One striking fact about these trees is that they contain many ﬂat structures. For example, base noun phrases frequently have ﬁve or more direct children. It is well known in monolingual parsing research that th"
J10-2004,C96-2141,0,0.0804986,"Missing"
J10-2004,D07-1078,1,0.89473,"Missing"
J10-2004,J97-3002,0,0.829465,"we notice that re-structuring tends to help MT accuracy more than re-labeling. We mentioned earlier that re-structuring overgeneralizes structures, but enables reuse of substructures. Results in Table 5 and Table 1 show substructure reuse mitigates structure over-generalization in our tree restructuring method. 5. Re-aligning (Tree, String) Pairs for Training So far, we have improved the English structures in our parsed, aligned training corpus. We now turn to improving the word alignments. Some MT systems use the same model for alignment and translation—examples include Brown et al. (1993), Wu (1997), Alshawi, Bangalore, and Douglas (1998), Yamada and Knight (2001, 2002), and Cohn and Blunsom (2009). Other systems use Brown et al. for alignment, then collect counts for a completely different model, such as Och and Ney Table 5 Impact of re-labeling methods on MT accuracy as measured by BLEU. Four-way splitting was carried out on EM-binarized trees; thus, it already beneﬁts from tree re-structuring. All p-values are computed against Baseline1. E XPERIMENT NIST08 BLEU p NIST08-NW BLEU p Baseline1 (no re-structuring and no re-labeling) Linguistically motivated re-labeling 29.12 29.57 — 0.029"
J10-2004,P01-1067,1,0.646868,"uracy more than re-labeling. We mentioned earlier that re-structuring overgeneralizes structures, but enables reuse of substructures. Results in Table 5 and Table 1 show substructure reuse mitigates structure over-generalization in our tree restructuring method. 5. Re-aligning (Tree, String) Pairs for Training So far, we have improved the English structures in our parsed, aligned training corpus. We now turn to improving the word alignments. Some MT systems use the same model for alignment and translation—examples include Brown et al. (1993), Wu (1997), Alshawi, Bangalore, and Douglas (1998), Yamada and Knight (2001, 2002), and Cohn and Blunsom (2009). Other systems use Brown et al. for alignment, then collect counts for a completely different model, such as Och and Ney Table 5 Impact of re-labeling methods on MT accuracy as measured by BLEU. Four-way splitting was carried out on EM-binarized trees; thus, it already beneﬁts from tree re-structuring. All p-values are computed against Baseline1. E XPERIMENT NIST08 BLEU p NIST08-NW BLEU p Baseline1 (no re-structuring and no re-labeling) Linguistically motivated re-labeling 29.12 29.57 — 0.029 35.33 35.85 — 0.050 Baseline2 (EM re-structuring but no re-labeli"
J10-2004,P02-1039,1,0.437148,"ts and obtain Viterbi derivation trees of minimal rules. We also follow Galley et al. in collecting composed rules, namely, compositions of minimal rules. These larger rules have been shown to substantially improve translation accuracy (Galley et al. 2006; DeNeefe et al. 2007). Figure 3 shows some of the additional rules. With these models, we can decode a new Chinese sentence by enumerating and scoring all of the English trees that can be derived from it by rule. The score is a weighted product of P(e) and P(e, c). To search efﬁciently, we employ the CKY dynamicprogramming parsing algorithm (Yamada and Knight 2002; Galley et al. 2006). This algorithm builds English trees on top of Chinese spans. In each cell of the CKY matrix, we store the non-terminal symbol at the root of the English tree being built up. We also Figure 2 Minimal rules extracted from the learning case in Figure 1 using the GHKM procedure. 249 Computational Linguistics Volume 36, Number 2 Figure 3 Additional rules extracted from the learning case in Figure 1. store English words that appear at the left and right corners of the tree, as these are needed for computing the P(e) score when cells are combined. For CKY to work, all transduce"
J10-2004,J02-1005,0,\N,Missing
J10-2004,D08-1033,0,\N,Missing
J10-2004,J12-2006,0,\N,Missing
J10-2004,C98-1006,0,\N,Missing
J10-2004,N06-1033,1,\N,Missing
J10-3001,J93-2003,0,0.134265,"stical machine translation (SMT). Brown et al. (1993) have provided the most popular word alignment algorithm to date, one that has been implemented in the GIZA (Al-Onaizan et al. 1999) and GIZA++ (Och and Ney 2003) software and adopted by nearly every SMT project. In this article, we investigate whether this algorithm makes search errors when it computes Viterbi alignments, that is, whether it returns alignments that are sub-optimal according to a trained model. 1. Background Word alignment is the problem of annotating a bilingual text with links connecting words that have the same meanings. Brown et al. (1993) align an English/French sentence pair by positing a probabilistic model by which an English sentence is translated into French.1 The model provides a set of non-deterministic choices. When a particular sequence of choices is applied to an English input sentence e1 ...el , the result is a particular French output sentence f1 ...fm . In the Brown et al. models, a decision sequence also implies a speciﬁc word alignment vector a1 ...am . We say aj = i when French word fj was produced by English word ei during the translation. Here is a sample sentence pair (e, f) and word alignment a: e: NULL0 Ma"
J10-3001,P01-1030,1,0.849898,"r and GIZA++ makes more errors. Finally, Figure 5 plots the time taken for ILP alignment at different sentence lengths showing a positive correlation as well. 5. Discussion We have determined that GIZA++ makes few search errors, despite the heuristic nature of the algorithm. These search errors do not materially affect overall alignment accuracy. In practice, this means that researchers should not spend time optimizing this particular aspect of SMT systems. Search errors can occur in many areas of SMT. The area that has received the most attention is runtime decoding/translation. For example, Germann et al. (2001) devise an optimal ILP decoder to identify types of search errors made by other decoders. A second area (this article) is ﬁnding Viterbi alignments, given a set of alignment parameter values. A third area is actually learning those parameter values. Brown et al.’s (1993) EM learning algorithm aims to optimize the probability of the French side of the parallel corpus given the English side. For Model 3 and above, Brown et al. collect parameter counts over subsets of alignments, instead of over all alignments. These subsets, like Viterbi alignments, are generated heuristically, and it may be tha"
J10-3001,J03-1002,0,0.0268533,"Missing"
J10-3001,E06-1004,0,0.228718,"φ0 ...φl are only shorthand, as their values are completely determined by the alignment a1 ...am . 296 Ravi and Knight Does GIZA++ Make Search Errors? 3. Finding the Viterbi Alignment We assume that probability tables n, t, d, and p have already been learned from data, using the EM method described by Brown et al. (1993). We are concerned solely with the problem of then ﬁnding the best alignment for a given sentence pair (e, f) as described in Equation 1. Brown et al. were unable to discover a polynomial time algorithm for this problem, which was in fact subsequently shown to be NP-complete (Udupa and Maji 2006). Brown et al. therefore devise a hill-climbing algorithm. This algorithm starts with a reasonably good alignment (Viterbi IBM Model 2, computable in quadratic time), after which it greedily executes small changes to the alignment structure, gradually increasing P(a, f|e). The small changes consist of moves, in which the value of some aj is changed, and swaps, in which a pair aj and ak exchange values. At each step in the greedy search, all possible moves and swaps are considered, and the one which increases P(a, f|e) the most is executed. When P(a, f|e) can no longer be improved, the search h"
J98-4003,J93-2003,0,0.0100091,"Missing"
J98-4003,1983.tc-1.13,0,0.109971,"Missing"
J98-4003,H94-1029,0,0.0113812,"ice cream and I scream. Transliteration is not trivial to automate, but we will be concerned with an even more challenging problem--going from katakana back to English, i.e., back-transliteration. Human translators can often ""sound out"" a katakana phrase to guess an appropriate translation. Automating this process has great practical importance in Japanese/English machine translation. Katakana phrases are the largest source of text phrases that do not appear in bilingual dictionaries or training corpora (a.k.a. ""notfound words""), but very little computational work has been done in this area. Yamron et al. (1994) briefly mention a pattern-matching approach, while Arbabi et al. (1994) discuss a hybrid neural-net/expert-system approach to (forward) transliteration. The information-losing aspect of transliteration makes it hard to invert. Here are some problem instances, taken from actual newspaper articles: 600 ? ? ? (aasudee) (robaato shyoon renaado) (masutaazutoonamento) Knight and Graehl Machine Transliteration English translations appear later in this article. Here are a few observations about back-transliteration that give an idea of the difficulty of the task: • Back-transliteration is less forgiv"
J98-4003,J97-4001,0,\N,Missing
J98-4003,H94-1096,0,\N,Missing
J99-4005,C92-2092,0,0.0226881,"s no, then we k n o w that every string e includes at least one zero value in the computation of either P(e) or P(fle). From any proposed Hamilton Circuit--i.e., some ordering of vertices in G - - w e can construct a string e using the same ordering. This e will have P(f]e) = 1 according to the channel model. Therefore, P(e) = 0. By the source model, this can only h a p p e n if the proposed &quot;circuit&quot; is actually broken somewhere. So no Hamilton Circuit exists. Figure 3 illustrates the intuitive correspondence between selecting a good w o r d order and finding a Hamilton Circuit. We note that Brew (1992) discusses the NPcompleteness of a related problem, that of finding some permutation of a string that is acceptable to a given context-free grammar. Both of these results deal with decision problems. Returning to optimization, we recall another circuit task called the Traveling 612 Knight Decoding Complexity my b°uid~N~ falls r~ &apos; / ~ ~ Thursday Figure 3 Selecting a good source word order is like solving the Hamilton Circuit Problem. If we assume that the channel model offers deterministic, word-for-word translations, then the bigram source model takes responsibility for ordering them. Some wo"
J99-4005,J93-2003,0,0.0625353,"this complexity to factors not present in other decoding problems. 1. I n t r o d u c t i o n Statistical models are widely used in attacking natural language problems. The s o u r c e c h a n n e l framework is especially popular, finding applications in part-of-speech tagging, accent restoration, transliteration, speech recognition, and many other areas. In this framework, we build an underspecified model of how certain structures (such as strings) are generated and transformed. We then instantiate the model through training on a database of sample structures and transformations. Recently, Brown et al. (1993) built a source-channel model of translation between English and French. They assumed that English strings are produced according to some stochastic process (source model) and transformed stochastically into French strings (channel model). To translate French to English, it is necessary to find an English source string that is likely according to the models. With a nod to its cryptographic antecedents, this kind of translation is called decoding. This paper looks at decoding complexity. 2. P a r t - o f - S p e e c h T a g g i n g The prototype source-channel application in natural language is"
J99-4005,A88-1019,0,0.0107326,"odel of translation between English and French. They assumed that English strings are produced according to some stochastic process (source model) and transformed stochastically into French strings (channel model). To translate French to English, it is necessary to find an English source string that is likely according to the models. With a nod to its cryptographic antecedents, this kind of translation is called decoding. This paper looks at decoding complexity. 2. P a r t - o f - S p e e c h T a g g i n g The prototype source-channel application in natural language is part-of-speech tagging (Church 1988). We review it here for purposes of comparison with machine translation. Source strings comprise sequences of part-of-speech tags like noun, verb, etc. A simple source model assigns a probability to a tag sequence tl .. •tm based on the probabilities of the tag pairs inside it. Target strings are English sentences, e.g., wl ... win. The channel model assumes each tag is probabilistically replaced by a word (e.g., noun by dog) without considering context. More concretely, we have: • v total tags • A bigram source model with v2 parameters of the form b(t]t), where P(tl... tin) &quot;&quot; b(tllboundary)"
knight-al-onaizan-1998-translation,J93-2003,0,\N,Missing
knight-al-onaizan-1998-translation,P96-1021,0,\N,Missing
knight-al-onaizan-1998-translation,P97-1037,0,\N,Missing
knight-al-onaizan-1998-translation,J97-3002,0,\N,Missing
knight-al-onaizan-1998-translation,P97-1046,0,\N,Missing
knight-al-onaizan-1998-translation,P97-1047,0,\N,Missing
L16-1067,P13-2131,1,0.824032,"9 68.7 23.4 R∗ 100.0 100.0 67.1 41.8 92.2 92.4 46.7 Figure 3: System name annotated as ‘PBMT base PRO’. Table 2: Performance of atom detector in terms of Precision, Recall, and F1 score, and reconstruction from survey response (R∗ ). 5. Baseline Evaluation Data From the collected data, five papers are used for development, and 62 are used for evaluation. Evaluation Metrics We evaluate system performance of atom detection with precision and recall. We approach the evaluation of the linked structured representation by transforming it into a directed acyclic graph and computing the Smatch score (Cai and Knight, 2013), previously used to evaluate the similarity between Abstract Meaning Representation (AMR) structures. 5.1. Atom Detection Evaluation Table 2 shows the performance of atom detection. As annotators do not tell us where the information is located, we match the annotated atoms to every substring in the structured text and present annotation recall from text as R∗ in Table 2. This presents a soft ceiling for our baseline approach. Finding annotated dataset name, size, and system name atoms was challenging due to abbreviations, PDF-to-text conversion errors, lexical diversity and name expansion, as"
L16-1067,councill-etal-2008-parscit,0,0.0238814,"3 4. St. Dev. 1.38 1.78 89.65 9.85 4.14 1.52 5.01 0.34 We present a pipelined pattern-based system that extracts individual atoms from a plain text logical representation of a machine translation paper and selects and links them into a structured representation. 4.1. Table 1: Structured text and survey response mean and standard deviation. Figure 2: Survey form to collect annotations. commercial TET system.2 As tables are often used to report experimental results, we pay special attention to their extraction. We extract tabular information using TableSeer3 (Liu et al., 2007). We use ParsCit4 (Councill et al., 2008) to derive the hierarchical structure of sections and subsections. We produce the final representation of papers with a system that combines the inputs of all three components. The process produces structured text, split into sections and subsections with parsed tables accompanied by captions, but does not include figures. 3.2. Structured Representation Annotation Annotators are presented with the papers in PDF format. Ideally, annotators would highlight relevant information in the text and link it to the structured representation. However, such linking is very time consuming. As an alternativ"
L16-1067,I11-1001,0,0.0369291,"Missing"
L16-1067,W06-1606,1,0.770132,"Missing"
L16-1067,W09-3607,0,0.0130031,"a structured representation of the experimental data (bottom). Two result values are associated with the PBMT system and two with the SPMT-Comb system. The RESULT values are connected to DATASETS via the test or train relation, and to an EXPERIMENT TYPE. We refer to the individual pieces of information that comprise the structured representation as atoms. In the example, there are 15 atoms, including Chinese-English, BLEU, 31.46 and 2002 NIST. 3. Data and Annotation In order to construct the dataset, we started by selecting papers related to Machine Translation from the ACL Anthology corpus (Radev et al., 2009) using keyword search and targeting of MT related workshops. For a random sample of 67 papers, we asked the annotators to provide a structured representation of experimental results as defined in the previous section. In order to aid structured information extraction, we automatically produced a structured text representation of each paper. The representation consists of plain text split into sections and subsections, as well as parsed tables. The annotated dataset is released alongside the paper1 to promote future research. In total, 1063 atoms were annotated. Additional dataset statistics ar"
L16-1067,P11-4002,0,0.394826,"Missing"
L18-1266,K16-1007,0,0.0145068,"termed the Standard (the compared-to entity) in the Comparison frame is replaced by a Comparison Set. Our treatment generalizes somewhat over the FrameNet treatment by exploiting a single roleset 3 For ARG0 and ARG1 only, an effort is made to map to Dowty’s prototypical agent and patient (Dowty, 1991), respectively. 1678 for all of these constructions (see Section 5.1). Our general roleset is very similar (in definition, albeit distinct in labeling) to that of Bakhshandeh and Allen (2015), who aim to predict the predicate-argument structure of comparison sentences to support semantic parsing (Bakhshandeh et al., 2016). Also related is the constructional annotation scheme of Dunietz et al. (2017), which targets causal language: in its current form their scheme includes the DegreeConsequence and The X-er, The Y-er constructions but excludes argument structure constructions like Caused Motion. PropBank has always included multi-word verbal predicates (e.g., eat up), and has recently been expanding its lexicon of rolesets and annotations to include light verb constructions and the degree and quantity-related constructions that will be discussed here (Hwang et al., 2010; Bonial et al., 2014; Bonial and Palmer,"
L18-1266,W13-2322,1,0.895126,"Missing"
L18-1266,L16-1628,1,0.836428,"andeh et al., 2016). Also related is the constructional annotation scheme of Dunietz et al. (2017), which targets causal language: in its current form their scheme includes the DegreeConsequence and The X-er, The Y-er constructions but excludes argument structure constructions like Caused Motion. PropBank has always included multi-word verbal predicates (e.g., eat up), and has recently been expanding its lexicon of rolesets and annotations to include light verb constructions and the degree and quantity-related constructions that will be discussed here (Hwang et al., 2010; Bonial et al., 2014; Bonial and Palmer, 2016). The expansion of the roleset lexicon was necessary not only for PropBank to provide construction-based annotations, but also because PropBank and AMR maintain a shared lexicon of rolesets that ensures symmetry between the two projects. 3. AMR Approach to Constructions In building the AMR corpus, we have run across a variety of cases where a predicate appeared to be in an atypical context: none of the senses listed in the lexicon of rolesets provided the appropriate role label choices for novel arguments encountered. For example, none of the PropBank rolesets for blink list appropriate semant"
L18-1266,bonial-etal-2014-propbank,1,0.924991,"antic parsing (Bakhshandeh et al., 2016). Also related is the constructional annotation scheme of Dunietz et al. (2017), which targets causal language: in its current form their scheme includes the DegreeConsequence and The X-er, The Y-er constructions but excludes argument structure constructions like Caused Motion. PropBank has always included multi-word verbal predicates (e.g., eat up), and has recently been expanding its lexicon of rolesets and annotations to include light verb constructions and the degree and quantity-related constructions that will be discussed here (Hwang et al., 2010; Bonial et al., 2014; Bonial and Palmer, 2016). The expansion of the roleset lexicon was necessary not only for PropBank to provide construction-based annotations, but also because PropBank and AMR maintain a shared lexicon of rolesets that ensures symmetry between the two projects. 3. AMR Approach to Constructions In building the AMR corpus, we have run across a variety of cases where a predicate appeared to be in an atypical context: none of the senses listed in the lexicon of rolesets provided the appropriate role label choices for novel arguments encountered. For example, none of the PropBank rolesets for bli"
L18-1266,P13-2131,1,0.826219,"Missing"
L18-1266,W17-0812,0,0.0128119,"a Comparison Set. Our treatment generalizes somewhat over the FrameNet treatment by exploiting a single roleset 3 For ARG0 and ARG1 only, an effort is made to map to Dowty’s prototypical agent and patient (Dowty, 1991), respectively. 1678 for all of these constructions (see Section 5.1). Our general roleset is very similar (in definition, albeit distinct in labeling) to that of Bakhshandeh and Allen (2015), who aim to predict the predicate-argument structure of comparison sentences to support semantic parsing (Bakhshandeh et al., 2016). Also related is the constructional annotation scheme of Dunietz et al. (2017), which targets causal language: in its current form their scheme includes the DegreeConsequence and The X-er, The Y-er constructions but excludes argument structure constructions like Caused Motion. PropBank has always included multi-word verbal predicates (e.g., eat up), and has recently been expanding its lexicon of rolesets and annotations to include light verb constructions and the degree and quantity-related constructions that will be discussed here (Hwang et al., 2010; Bonial et al., 2014; Bonial and Palmer, 2016). The expansion of the roleset lexicon was necessary not only for PropBank"
L18-1266,W10-1810,1,0.796285,"ences to support semantic parsing (Bakhshandeh et al., 2016). Also related is the constructional annotation scheme of Dunietz et al. (2017), which targets causal language: in its current form their scheme includes the DegreeConsequence and The X-er, The Y-er constructions but excludes argument structure constructions like Caused Motion. PropBank has always included multi-word verbal predicates (e.g., eat up), and has recently been expanding its lexicon of rolesets and annotations to include light verb constructions and the degree and quantity-related constructions that will be discussed here (Hwang et al., 2010; Bonial et al., 2014; Bonial and Palmer, 2016). The expansion of the roleset lexicon was necessary not only for PropBank to provide construction-based annotations, but also because PropBank and AMR maintain a shared lexicon of rolesets that ensures symmetry between the two projects. 3. AMR Approach to Constructions In building the AMR corpus, we have run across a variety of cases where a predicate appeared to be in an atypical context: none of the senses listed in the lexicon of rolesets provided the appropriate role label choices for novel arguments encountered. For example, none of the Prop"
L18-1266,N15-1114,0,0.0551831,"meaning. Keywords: Semantics, Constructions, Meaning Representation 1. Introduction The Abstract Meaning Representation (AMR) project (Banarescu et al., 2013) has created a manually annotated “semantics bank” of text. A goal of AMR is to capture core facets of meaning while abstracting away from idiosyncratic syntactic facts; thus, for example, She adjusted the machine and She made an adjustment to the machine share the same AMR. The purpose of these annotations is to support natural language processing (NLP) applications such as natural language understanding, generation, and summarization (Liu et al., 2015; Pourdamghani et al., 2016), machine translation, question answering (Mitra and Baral, 2016), information extraction (Pan et al., 2015), and biomedical text mining (Garg et al., 2016; Rao et al., 2017; Wang et al., 2017). With a growing body of over 70 research papers using AMR,1 the corpus is becoming a benchmark dataset. As a practical NLP resource, AMR annotation has focused on providing coverage for the more frequent and predictably patterned linguistic phenomena, and therefore has not necessarily provided adequate representations for some of the rarer structures found in the long tail of"
L18-1266,W16-5706,1,0.830623,"Missing"
L18-1266,J05-1004,1,0.31696,"grammar approach, the construction itself can license the arguments. This distinction also affects what is thought to be stored in the lexicon and, perhaps most relevant to AMR, what would need to be represented in a computational lexicon: additional senses tied to a lexical predicate, or constructional entries. 2.2. Abstract Meaning Representation Annotation The AMR project annotations are completed on a sentenceby-sentence basis, where each sentence is represented by a rooted directed acyclic graph (DAG). See Figure 1. PENMAN notation. AMR concepts are either English words (boy), PropBank (Palmer et al., 2005) rolesets (want-01), or special keywords indicating generic entity types: date-entity, world-region, distance-quantity, etc. In addition to the PropBank lexicon of rolesets, which associate argument numbers (ARG 0–6) with predicate-specific3 semantic roles (e.g., ARG0=wanter in ex. 1), AMR uses approximately 100 relations of its own (e.g., :time, :age, :quantity, :destination, etc.). These AMR-specific relations can be thought of as a fine-grained inventory of modifier role labels. AMR abstracts away from language-specific, idiosyncratic facts, such that distinct syntactic realizations of the"
L18-1266,N15-1119,1,0.802771,"(Banarescu et al., 2013) has created a manually annotated “semantics bank” of text. A goal of AMR is to capture core facets of meaning while abstracting away from idiosyncratic syntactic facts; thus, for example, She adjusted the machine and She made an adjustment to the machine share the same AMR. The purpose of these annotations is to support natural language processing (NLP) applications such as natural language understanding, generation, and summarization (Liu et al., 2015; Pourdamghani et al., 2016), machine translation, question answering (Mitra and Baral, 2016), information extraction (Pan et al., 2015), and biomedical text mining (Garg et al., 2016; Rao et al., 2017; Wang et al., 2017). With a growing body of over 70 research papers using AMR,1 the corpus is becoming a benchmark dataset. As a practical NLP resource, AMR annotation has focused on providing coverage for the more frequent and predictably patterned linguistic phenomena, and therefore has not necessarily provided adequate representations for some of the rarer structures found in the long tail of language (Zipf, 1949). However, as the project matures, we are aiming to expand the representation to go beyond capturing the semantics"
L18-1266,W16-6603,1,0.775868,": Semantics, Constructions, Meaning Representation 1. Introduction The Abstract Meaning Representation (AMR) project (Banarescu et al., 2013) has created a manually annotated “semantics bank” of text. A goal of AMR is to capture core facets of meaning while abstracting away from idiosyncratic syntactic facts; thus, for example, She adjusted the machine and She made an adjustment to the machine share the same AMR. The purpose of these annotations is to support natural language processing (NLP) applications such as natural language understanding, generation, and summarization (Liu et al., 2015; Pourdamghani et al., 2016), machine translation, question answering (Mitra and Baral, 2016), information extraction (Pan et al., 2015), and biomedical text mining (Garg et al., 2016; Rao et al., 2017; Wang et al., 2017). With a growing body of over 70 research papers using AMR,1 the corpus is becoming a benchmark dataset. As a practical NLP resource, AMR annotation has focused on providing coverage for the more frequent and predictably patterned linguistic phenomena, and therefore has not necessarily provided adequate representations for some of the rarer structures found in the long tail of language (Zipf, 1949). Howe"
L18-1266,W17-2315,1,0.813627,"cs bank” of text. A goal of AMR is to capture core facets of meaning while abstracting away from idiosyncratic syntactic facts; thus, for example, She adjusted the machine and She made an adjustment to the machine share the same AMR. The purpose of these annotations is to support natural language processing (NLP) applications such as natural language understanding, generation, and summarization (Liu et al., 2015; Pourdamghani et al., 2016), machine translation, question answering (Mitra and Baral, 2016), information extraction (Pan et al., 2015), and biomedical text mining (Garg et al., 2016; Rao et al., 2017; Wang et al., 2017). With a growing body of over 70 research papers using AMR,1 the corpus is becoming a benchmark dataset. As a practical NLP resource, AMR annotation has focused on providing coverage for the more frequent and predictably patterned linguistic phenomena, and therefore has not necessarily provided adequate representations for some of the rarer structures found in the long tail of language (Zipf, 1949). However, as the project matures, we are aiming to expand the representation to go beyond capturing the semantics of purely compositional language to better capture the semantics"
N03-1024,W02-1022,0,0.0515429,"ions in Figure 1, our algorithm automatically induces the FSA in Figure 2, which represents compactly 49 distinct renderings of the same semantic meaning. Our FSAs capture both lexical paraphrases, such as {fighting, battle}, {died, were killed} and structural paraphrases such as {last week’s fighting, the battle of last week}. The contexts in which these are correct paraphrases are also conveniently captured in the representation. In previous work, Langkilde and Knight (1998) used word lattices for language generation, but their method involved hand-crafted rules. Bangalore et al. (2001) and Barzilay and Lee (2002) both applied the technique of multi-sequence alignment (MSA) to align parallel corpora and produced similar FSAs. For their purposes, they mainly need to ensure the correctness of consensus among different translations, so that different constituent orderings in input sentences do not pose a serious prob1. 3. 5. 7. 9. 11. At least 12 people were killed in the battle last week. Last week’s fight took at least 12 lives. The battle of last week killed at least 12 persons. At least 12 died in the battle last week. During last week’s fighting, at least 12 people died. Last week’s fighting took the"
N03-1024,N03-1003,0,0.711823,"f all paths represented by the FSAs, and direct application of MSA in the presence of different constituent orderings can be problematic. For example, when given as input the same sentences in Figure 1, one instantiation of the MSA algorithm produces the FSA in Figure 3, which contains many “bad” paths such as the battle of last week’s fighting took at least 12 people lost their people died in the fighting last week’s fighting (See Section 4.2.2 for a more quantitative analysis.). It’s still possible to use MSA if, for example, the input is pre-clustered to have the same constituent ordering (Barzilay and Lee (2003)). But we chose to approach this problem from another direction. As a result, we propose a new syntax-based algorithm to produce FSAs. In this paper, we first introduce the multiple translation corpus that we use in our experiments (see Section 2). We then present the algorithms that we developed to induce finite-state paraphrase representations from such data (see Section 3). An important part of the paper is dedicated to evaluating the quality of the finite-state representations that we derive (see Section 4). Since our representations encode thousands and sometimes millions of equivalent ve"
N03-1024,P01-1008,0,0.902573,"1984). Natural language generation researchers have used paraphrasing to increase the expressive power of generation systems (Iordanskaja et al., 1991; Lenke, 1994; Stede, 1999). And researchers in multi-document text summarization (Barzilay et al., 1999), information extraction (Shinyama et al., 2002), and question answering (Lin and Pantel, 2001; Hermjakob et al., 2002) have focused on identifying and exploiting paraphrases in the context of recognizing redundancies, alternative formulations of the same meaning, and improving the performance of question answering systems. In previous work (Barzilay and McKeown, 2001; Lin and Pantel, 2001; Shinyama et al., 2002), paraphrases are represented as sets or pairs of semantically equivalent words, phrases, and patterns. Although this is adequate in the context of some applications, it is clearly too weak from a generative perspective. Assume, for example, that we know that text pairs (stock market rose, stock Kevin Knight and Daniel Marcu Information Sciences Institute University of Southern California Marina Del Rey, CA 90292 USA {knight,marcu}@isi.edu market gained) and (stock market rose, stock prices rose) have the same meaning. If we memorized only these tw"
N03-1024,P99-1071,0,0.0539953,"native semantic renderings, which may be used to evaluate the quality of translations. 1 Introduction In the past, paraphrases have come under the scrutiny of many research communities. Information retrieval researchers have used paraphrasing techniques for query reformulation in order to increase the recall of information retrieval engines (Sparck Jones and Tait, 1984). Natural language generation researchers have used paraphrasing to increase the expressive power of generation systems (Iordanskaja et al., 1991; Lenke, 1994; Stede, 1999). And researchers in multi-document text summarization (Barzilay et al., 1999), information extraction (Shinyama et al., 2002), and question answering (Lin and Pantel, 2001; Hermjakob et al., 2002) have focused on identifying and exploiting paraphrases in the context of recognizing redundancies, alternative formulations of the same meaning, and improving the performance of question answering systems. In previous work (Barzilay and McKeown, 2001; Lin and Pantel, 2001; Shinyama et al., 2002), paraphrases are represented as sets or pairs of semantically equivalent words, phrases, and patterns. Although this is adequate in the context of some applications, it is clearly too"
N03-1024,A00-2018,0,0.0441775,"Missing"
N03-1024,P98-1116,1,0.637441,"): multiple English translations of many foreign language texts. For instance, when given as input the 11 semantically equivalent English translations in Figure 1, our algorithm automatically induces the FSA in Figure 2, which represents compactly 49 distinct renderings of the same semantic meaning. Our FSAs capture both lexical paraphrases, such as {fighting, battle}, {died, were killed} and structural paraphrases such as {last week’s fighting, the battle of last week}. The contexts in which these are correct paraphrases are also conveniently captured in the representation. In previous work, Langkilde and Knight (1998) used word lattices for language generation, but their method involved hand-crafted rules. Bangalore et al. (2001) and Barzilay and Lee (2002) both applied the technique of multi-sequence alignment (MSA) to align parallel corpora and produced similar FSAs. For their purposes, they mainly need to ensure the correctness of consensus among different translations, so that different constituent orderings in input sentences do not pose a serious prob1. 3. 5. 7. 9. 11. At least 12 people were killed in the battle last week. Last week’s fight took at least 12 lives. The battle of last week killed at l"
N03-1024,C94-1051,0,0.0496961,"e sentences in the input sets. Our FSAs can also predict the correctness of alternative semantic renderings, which may be used to evaluate the quality of translations. 1 Introduction In the past, paraphrases have come under the scrutiny of many research communities. Information retrieval researchers have used paraphrasing techniques for query reformulation in order to increase the recall of information retrieval engines (Sparck Jones and Tait, 1984). Natural language generation researchers have used paraphrasing to increase the expressive power of generation systems (Iordanskaja et al., 1991; Lenke, 1994; Stede, 1999). And researchers in multi-document text summarization (Barzilay et al., 1999), information extraction (Shinyama et al., 2002), and question answering (Lin and Pantel, 2001; Hermjakob et al., 2002) have focused on identifying and exploiting paraphrases in the context of recognizing redundancies, alternative formulations of the same meaning, and improving the performance of question answering systems. In previous work (Barzilay and McKeown, 2001; Lin and Pantel, 2001; Shinyama et al., 2002), paraphrases are represented as sets or pairs of semantically equivalent words, phrases, an"
N03-1024,C98-1112,1,\N,Missing
N03-2016,P01-1030,1,0.457519,"s split into words, and all possible word pairings are stored in a file. Numbers and punctuation are not considered, since we feel that they warrant a more specific approach. After sorting and removing duplicates, the file represents all possible one-to-one word alignments of the bitext. Also removed are the pairs that include English 3 Experiments We induced translation models using IBM Model 4 (Brown et al., 1990) with the GIZA toolkit (Al-Onaizan et al., 1999). The maximum sentence length in the training data was set at 30 words. The actual translations were produced with a greedy decoder (Germann et al., 2001). For the evaluation of translation quality, we used the BLEU metric (Papineni et al., 2002), which measures the n-gram overlap between the translated output and one or more reference translations. In our experiments, we used only one reference translation. 3.1 Word alignment quality In order to directly measure the influence of the added cognate information on the word alignment quality, we performed a single experiment using a set of 500 manually aligned sentences from Hansards (Och and Ney, 2000). Giza was first trained on 50,000 sentences from Hansards, and then on the same training set au"
N03-2016,N01-1020,0,0.323315,"uage to another (e.g. English sprint and Japanese supurinto). In a broad sense, cognates include not only genetically related words and borrowings but also names, numbers, and punctuation. Practically all bitexts (bilingual parallel corpora) contain some kind of cognates. If the languages are represented in different scripts, a phonetic transcription or transliteration of one or both parts of the bitext is a pre-requisite for identifying cognates. Cognates have been employed for a number of bitextrelated tasks, including sentence alignment (Simard et al., 1992), inducing translation lexicons (Mann and Yarowsky, 2001), and improving statistical machine translation models (Al-Onaizan et al., 1999). Cognates are particularly useful when machine-readable bilingual dictionaries are not available. Al-Onaizan et al. (1999) experimented with using bilingual dictionaries and cognates in the training of Czech–English translation models. They found that appending probable cognates to the training bitext significantly lowered the perplexity score on the test bitext (in some cases more than when using a bilingual dictionary), and observed improvement in word alignments of test sentences. In this paper, we investigate"
N03-2016,P00-1056,0,0.17161,"Missing"
N03-2016,P02-1040,0,0.0821725,"ation are not considered, since we feel that they warrant a more specific approach. After sorting and removing duplicates, the file represents all possible one-to-one word alignments of the bitext. Also removed are the pairs that include English 3 Experiments We induced translation models using IBM Model 4 (Brown et al., 1990) with the GIZA toolkit (Al-Onaizan et al., 1999). The maximum sentence length in the training data was set at 30 words. The actual translations were produced with a greedy decoder (Germann et al., 2001). For the evaluation of translation quality, we used the BLEU metric (Papineni et al., 2002), which measures the n-gram overlap between the translated output and one or more reference translations. In our experiments, we used only one reference translation. 3.1 Word alignment quality In order to directly measure the influence of the added cognate information on the word alignment quality, we performed a single experiment using a set of 500 manually aligned sentences from Hansards (Och and Ney, 2000). Giza was first trained on 50,000 sentences from Hansards, and then on the same training set augmented with a set of cognates. The set consisted of two copies of a list produced by applyi"
N03-2016,1992.tmi-1.7,0,0.578863,"sh night and German nacht) or borrowing from one language to another (e.g. English sprint and Japanese supurinto). In a broad sense, cognates include not only genetically related words and borrowings but also names, numbers, and punctuation. Practically all bitexts (bilingual parallel corpora) contain some kind of cognates. If the languages are represented in different scripts, a phonetic transcription or transliteration of one or both parts of the bitext is a pre-requisite for identifying cognates. Cognates have been employed for a number of bitextrelated tasks, including sentence alignment (Simard et al., 1992), inducing translation lexicons (Mann and Yarowsky, 2001), and improving statistical machine translation models (Al-Onaizan et al., 1999). Cognates are particularly useful when machine-readable bilingual dictionaries are not available. Al-Onaizan et al. (1999) experimented with using bilingual dictionaries and cognates in the training of Czech–English translation models. They found that appending probable cognates to the training bitext significantly lowered the perplexity score on the test bitext (in some cases more than when using a bilingual dictionary), and observed improvement in word ali"
N03-2016,J93-2003,0,\N,Missing
N03-2026,H01-1033,1,\N,Missing
N04-1014,J00-1004,0,0.0760838,"Viterbi decoding (Viterbi, 1967) and forward-backward training (Baum and Eagon, 1967), as well as generic software toolkits. Moreover, a surprising variety of problems are attackable with FSTs, from partof-speech tagging to letter-to-sound conversion to name transliteration. However, language problems like machine translation break this mold, because they involve massive reordering of symbols, and because the transformation processes seem sensitive to hierarchical tree structure. Recently, specific probabilistic tree-based models have been proposed not only for machine translation (Wu, 1997; Alshawi, Bangalore, and Douglas, 2000; Yamada and Knight, 2001; Gildea, 2003; Eisner, 2003), but also for This work was supported by DARPA contract F49620-001-0337 and ARDA contract MDA904-02-C-0450. Kevin Knight Information Sciences Institute University of Southern California 4676 Admiralty Way Marina del Rey, CA 90292 knight@isi.edu summarization (Knight and Marcu, 2002), paraphrasing (Pang, Knight, and Marcu, 2003), natural language generation (Langkilde and Knight, 1998; Bangalore and Rambow, 2000; Corston-Oliver et al., 2002), and language modeling (Baker, 1979; Lari and Young, 1990; Collins, 1997; Chelba and Jelinek, 2000;"
N04-1014,C00-1007,0,0.0127864,"ture. Recently, specific probabilistic tree-based models have been proposed not only for machine translation (Wu, 1997; Alshawi, Bangalore, and Douglas, 2000; Yamada and Knight, 2001; Gildea, 2003; Eisner, 2003), but also for This work was supported by DARPA contract F49620-001-0337 and ARDA contract MDA904-02-C-0450. Kevin Knight Information Sciences Institute University of Southern California 4676 Admiralty Way Marina del Rey, CA 90292 knight@isi.edu summarization (Knight and Marcu, 2002), paraphrasing (Pang, Knight, and Marcu, 2003), natural language generation (Langkilde and Knight, 1998; Bangalore and Rambow, 2000; Corston-Oliver et al., 2002), and language modeling (Baker, 1979; Lari and Young, 1990; Collins, 1997; Chelba and Jelinek, 2000; Charniak, 2001; Klein and Manning, 2003). It is useful to understand generic algorithms that may support all these tasks and more. (Rounds, 1970) and (Thatcher, 1970) independently introduced tree transducers as a generalization of FSTs. Rounds was motivated by natural language. The Rounds tree transducer is very similar to a left-to-right FST, except that it works top-down, pursuing subtrees in parallel, with each subtree transformed depending only on its own pass"
N04-1014,P01-1017,0,0.0109996,"; Yamada and Knight, 2001; Gildea, 2003; Eisner, 2003), but also for This work was supported by DARPA contract F49620-001-0337 and ARDA contract MDA904-02-C-0450. Kevin Knight Information Sciences Institute University of Southern California 4676 Admiralty Way Marina del Rey, CA 90292 knight@isi.edu summarization (Knight and Marcu, 2002), paraphrasing (Pang, Knight, and Marcu, 2003), natural language generation (Langkilde and Knight, 1998; Bangalore and Rambow, 2000; Corston-Oliver et al., 2002), and language modeling (Baker, 1979; Lari and Young, 1990; Collins, 1997; Chelba and Jelinek, 2000; Charniak, 2001; Klein and Manning, 2003). It is useful to understand generic algorithms that may support all these tasks and more. (Rounds, 1970) and (Thatcher, 1970) independently introduced tree transducers as a generalization of FSTs. Rounds was motivated by natural language. The Rounds tree transducer is very similar to a left-to-right FST, except that it works top-down, pursuing subtrees in parallel, with each subtree transformed depending only on its own passed-down state. This class of transducer is often nowadays called R, for “Root-to-frontier” (Gécseg and Steinby, 1984). Rounds uses a mathematics-"
N04-1014,P97-1003,0,0.0554639,"97; Alshawi, Bangalore, and Douglas, 2000; Yamada and Knight, 2001; Gildea, 2003; Eisner, 2003), but also for This work was supported by DARPA contract F49620-001-0337 and ARDA contract MDA904-02-C-0450. Kevin Knight Information Sciences Institute University of Southern California 4676 Admiralty Way Marina del Rey, CA 90292 knight@isi.edu summarization (Knight and Marcu, 2002), paraphrasing (Pang, Knight, and Marcu, 2003), natural language generation (Langkilde and Knight, 1998; Bangalore and Rambow, 2000; Corston-Oliver et al., 2002), and language modeling (Baker, 1979; Lari and Young, 1990; Collins, 1997; Chelba and Jelinek, 2000; Charniak, 2001; Klein and Manning, 2003). It is useful to understand generic algorithms that may support all these tasks and more. (Rounds, 1970) and (Thatcher, 1970) independently introduced tree transducers as a generalization of FSTs. Rounds was motivated by natural language. The Rounds tree transducer is very similar to a left-to-right FST, except that it works top-down, pursuing subtrees in parallel, with each subtree transformed depending only on its own passed-down state. This class of transducer is often nowadays called R, for “Root-to-frontier” (Gécseg and"
N04-1014,W02-2105,0,0.0166659,"babilistic tree-based models have been proposed not only for machine translation (Wu, 1997; Alshawi, Bangalore, and Douglas, 2000; Yamada and Knight, 2001; Gildea, 2003; Eisner, 2003), but also for This work was supported by DARPA contract F49620-001-0337 and ARDA contract MDA904-02-C-0450. Kevin Knight Information Sciences Institute University of Southern California 4676 Admiralty Way Marina del Rey, CA 90292 knight@isi.edu summarization (Knight and Marcu, 2002), paraphrasing (Pang, Knight, and Marcu, 2003), natural language generation (Langkilde and Knight, 1998; Bangalore and Rambow, 2000; Corston-Oliver et al., 2002), and language modeling (Baker, 1979; Lari and Young, 1990; Collins, 1997; Chelba and Jelinek, 2000; Charniak, 2001; Klein and Manning, 2003). It is useful to understand generic algorithms that may support all these tasks and more. (Rounds, 1970) and (Thatcher, 1970) independently introduced tree transducers as a generalization of FSTs. Rounds was motivated by natural language. The Rounds tree transducer is very similar to a left-to-right FST, except that it works top-down, pursuing subtrees in parallel, with each subtree transformed depending only on its own passed-down state. This class of t"
N04-1014,P03-2041,0,0.68286,"n, 1967), as well as generic software toolkits. Moreover, a surprising variety of problems are attackable with FSTs, from partof-speech tagging to letter-to-sound conversion to name transliteration. However, language problems like machine translation break this mold, because they involve massive reordering of symbols, and because the transformation processes seem sensitive to hierarchical tree structure. Recently, specific probabilistic tree-based models have been proposed not only for machine translation (Wu, 1997; Alshawi, Bangalore, and Douglas, 2000; Yamada and Knight, 2001; Gildea, 2003; Eisner, 2003), but also for This work was supported by DARPA contract F49620-001-0337 and ARDA contract MDA904-02-C-0450. Kevin Knight Information Sciences Institute University of Southern California 4676 Admiralty Way Marina del Rey, CA 90292 knight@isi.edu summarization (Knight and Marcu, 2002), paraphrasing (Pang, Knight, and Marcu, 2003), natural language generation (Langkilde and Knight, 1998; Bangalore and Rambow, 2000; Corston-Oliver et al., 2002), and language modeling (Baker, 1979; Lari and Young, 1990; Collins, 1997; Chelba and Jelinek, 2000; Charniak, 2001; Klein and Manning, 2003). It is useful"
N04-1014,P03-1011,0,0.15319,"(Baum and Eagon, 1967), as well as generic software toolkits. Moreover, a surprising variety of problems are attackable with FSTs, from partof-speech tagging to letter-to-sound conversion to name transliteration. However, language problems like machine translation break this mold, because they involve massive reordering of symbols, and because the transformation processes seem sensitive to hierarchical tree structure. Recently, specific probabilistic tree-based models have been proposed not only for machine translation (Wu, 1997; Alshawi, Bangalore, and Douglas, 2000; Yamada and Knight, 2001; Gildea, 2003; Eisner, 2003), but also for This work was supported by DARPA contract F49620-001-0337 and ARDA contract MDA904-02-C-0450. Kevin Knight Information Sciences Institute University of Southern California 4676 Admiralty Way Marina del Rey, CA 90292 knight@isi.edu summarization (Knight and Marcu, 2002), paraphrasing (Pang, Knight, and Marcu, 2003), natural language generation (Langkilde and Knight, 1998; Bangalore and Rambow, 2000; Corston-Oliver et al., 2002), and language modeling (Baker, 1979; Lari and Young, 1990; Collins, 1997; Chelba and Jelinek, 2000; Charniak, 2001; Klein and Manning, 2003"
N04-1014,P03-1054,0,0.0384115,"ght, 2001; Gildea, 2003; Eisner, 2003), but also for This work was supported by DARPA contract F49620-001-0337 and ARDA contract MDA904-02-C-0450. Kevin Knight Information Sciences Institute University of Southern California 4676 Admiralty Way Marina del Rey, CA 90292 knight@isi.edu summarization (Knight and Marcu, 2002), paraphrasing (Pang, Knight, and Marcu, 2003), natural language generation (Langkilde and Knight, 1998; Bangalore and Rambow, 2000; Corston-Oliver et al., 2002), and language modeling (Baker, 1979; Lari and Young, 1990; Collins, 1997; Chelba and Jelinek, 2000; Charniak, 2001; Klein and Manning, 2003). It is useful to understand generic algorithms that may support all these tasks and more. (Rounds, 1970) and (Thatcher, 1970) independently introduced tree transducers as a generalization of FSTs. Rounds was motivated by natural language. The Rounds tree transducer is very similar to a left-to-right FST, except that it works top-down, pursuing subtrees in parallel, with each subtree transformed depending only on its own passed-down state. This class of transducer is often nowadays called R, for “Root-to-frontier” (Gécseg and Steinby, 1984). Rounds uses a mathematics-oriented example of an R t"
N04-1014,knight-al-onaizan-1998-translation,1,0.142657,"rohibit any subsequent Japanese function word insertion: - r NN(x0:car) → t x0 - r CC(x0:and) → t x0 State t means “translate this word,” and we have a production for every pair of co-occurring English and Japanese words: - t car → kuruma - t car → wa - t car → *e* This follows (Yamada and Knight, 2001) in also allowing English words to disappear, or translate to epsilon. Every production in the xRS transducer has an associated weight and corresponds to exactly one of the model parameters. There are several benefits to this xRS formulation. First, it clarifies the model, in the same way that (Knight and Al-Onaizan, 1998; Kumar and Byrne, 2003) elucidate other machine translation models in easily-grasped FST terms. Second, the model can be trained with generic, off-the-shelf tools—versus the alternative of working out model-specific re-estimation formulae and implementing custom training software. Third, we can easily extend the model in interesting ways. For example, we can add productions for multi-level and lexical re-ordering: - r NP(x0:NP, PP(IN(of), x1:NP)) → q x1, no, q x0 We can add productions for phrasal translations: - r NP(JJ(big), NN(cars)) → ooki, kuruma This can now include crucial non-constitu"
N04-1014,N03-1019,0,0.0260463,"se function word insertion: - r NN(x0:car) → t x0 - r CC(x0:and) → t x0 State t means “translate this word,” and we have a production for every pair of co-occurring English and Japanese words: - t car → kuruma - t car → wa - t car → *e* This follows (Yamada and Knight, 2001) in also allowing English words to disappear, or translate to epsilon. Every production in the xRS transducer has an associated weight and corresponds to exactly one of the model parameters. There are several benefits to this xRS formulation. First, it clarifies the model, in the same way that (Knight and Al-Onaizan, 1998; Kumar and Byrne, 2003) elucidate other machine translation models in easily-grasped FST terms. Second, the model can be trained with generic, off-the-shelf tools—versus the alternative of working out model-specific re-estimation formulae and implementing custom training software. Third, we can easily extend the model in interesting ways. For example, we can add productions for multi-level and lexical re-ordering: - r NP(x0:NP, PP(IN(of), x1:NP)) → q x1, no, q x0 We can add productions for phrasal translations: - r NP(JJ(big), NN(cars)) → ooki, kuruma This can now include crucial non-constituent phrasal translations"
N04-1014,A00-2023,0,0.013539,"t of a becoming b in G is wG (a, b) ≡ P w(h), the sum of weights of all unique L∗ h:(a,())⇒G (b,h) (leftmost) derivations transforming a to b, and the weight of t in G is WG (t) = wG (S, t). The weighted regular tree language produced by G is LG ≡ {(t, w) ∈ TΣ × R+ |WG (t) = w}. For every weighted context-free grammar, there is an equivalent wRTG that produces its weighted derivation trees with yields being the string produced, and the yields of regular tree grammars are context free string languages (Gécseg and Steinby, 1984). What is sometimes called a forest in natural language generation (Langkilde, 2000; Nederhof and Satta, 2002) is a finite wRTG without loops, i.e., ∀n ∈ N (n, ()) ⇒∗G (t, h) =⇒ pathst ({n}) = ∅. Regular tree languages are strictly contained in tree sets of tree adjoining grammars (Joshi and Schabes, 1997). Rules whose rhs are a pure T∆ with no states/paths for further expansion are called terminal rules. Rules of the form (q, pat) →w (q ′ , ()) are ǫ-rules, or epsilon rules, which substitute state q ′ for state q without producing output, and stay at the current input subtree. Multiple initial states are not needed: we can use a single start state Qi , and instead of each i"
N04-1014,P98-1116,1,0.559227,"e to hierarchical tree structure. Recently, specific probabilistic tree-based models have been proposed not only for machine translation (Wu, 1997; Alshawi, Bangalore, and Douglas, 2000; Yamada and Knight, 2001; Gildea, 2003; Eisner, 2003), but also for This work was supported by DARPA contract F49620-001-0337 and ARDA contract MDA904-02-C-0450. Kevin Knight Information Sciences Institute University of Southern California 4676 Admiralty Way Marina del Rey, CA 90292 knight@isi.edu summarization (Knight and Marcu, 2002), paraphrasing (Pang, Knight, and Marcu, 2003), natural language generation (Langkilde and Knight, 1998; Bangalore and Rambow, 2000; Corston-Oliver et al., 2002), and language modeling (Baker, 1979; Lari and Young, 1990; Collins, 1997; Chelba and Jelinek, 2000; Charniak, 2001; Klein and Manning, 2003). It is useful to understand generic algorithms that may support all these tasks and more. (Rounds, 1970) and (Thatcher, 1970) independently introduced tree transducers as a generalization of FSTs. Rounds was motivated by natural language. The Rounds tree transducer is very similar to a left-to-right FST, except that it works top-down, pursuing subtrees in parallel, with each subtree transformed de"
N04-1014,P02-1015,0,0.0266505,"in G is wG (a, b) ≡ P w(h), the sum of weights of all unique L∗ h:(a,())⇒G (b,h) (leftmost) derivations transforming a to b, and the weight of t in G is WG (t) = wG (S, t). The weighted regular tree language produced by G is LG ≡ {(t, w) ∈ TΣ × R+ |WG (t) = w}. For every weighted context-free grammar, there is an equivalent wRTG that produces its weighted derivation trees with yields being the string produced, and the yields of regular tree grammars are context free string languages (Gécseg and Steinby, 1984). What is sometimes called a forest in natural language generation (Langkilde, 2000; Nederhof and Satta, 2002) is a finite wRTG without loops, i.e., ∀n ∈ N (n, ()) ⇒∗G (t, h) =⇒ pathst ({n}) = ∅. Regular tree languages are strictly contained in tree sets of tree adjoining grammars (Joshi and Schabes, 1997). Rules whose rhs are a pure T∆ with no states/paths for further expansion are called terminal rules. Rules of the form (q, pat) →w (q ′ , ()) are ǫ-rules, or epsilon rules, which substitute state q ′ for state q without producing output, and stay at the current input subtree. Multiple initial states are not needed: we can use a single start state Qi , and instead of each initial state q with startin"
N04-1014,N03-1024,1,0.081724,"mbols, and because the transformation processes seem sensitive to hierarchical tree structure. Recently, specific probabilistic tree-based models have been proposed not only for machine translation (Wu, 1997; Alshawi, Bangalore, and Douglas, 2000; Yamada and Knight, 2001; Gildea, 2003; Eisner, 2003), but also for This work was supported by DARPA contract F49620-001-0337 and ARDA contract MDA904-02-C-0450. Kevin Knight Information Sciences Institute University of Southern California 4676 Admiralty Way Marina del Rey, CA 90292 knight@isi.edu summarization (Knight and Marcu, 2002), paraphrasing (Pang, Knight, and Marcu, 2003), natural language generation (Langkilde and Knight, 1998; Bangalore and Rambow, 2000; Corston-Oliver et al., 2002), and language modeling (Baker, 1979; Lari and Young, 1990; Collins, 1997; Chelba and Jelinek, 2000; Charniak, 2001; Klein and Manning, 2003). It is useful to understand generic algorithms that may support all these tasks and more. (Rounds, 1970) and (Thatcher, 1970) independently introduced tree transducers as a generalization of FSTs. Rounds was motivated by natural language. The Rounds tree transducer is very similar to a left-to-right FST, except that it works top-down, pursu"
N04-1014,C69-0101,0,0.741342,"RDA contract MDA904-02-C-0450. Kevin Knight Information Sciences Institute University of Southern California 4676 Admiralty Way Marina del Rey, CA 90292 knight@isi.edu summarization (Knight and Marcu, 2002), paraphrasing (Pang, Knight, and Marcu, 2003), natural language generation (Langkilde and Knight, 1998; Bangalore and Rambow, 2000; Corston-Oliver et al., 2002), and language modeling (Baker, 1979; Lari and Young, 1990; Collins, 1997; Chelba and Jelinek, 2000; Charniak, 2001; Klein and Manning, 2003). It is useful to understand generic algorithms that may support all these tasks and more. (Rounds, 1970) and (Thatcher, 1970) independently introduced tree transducers as a generalization of FSTs. Rounds was motivated by natural language. The Rounds tree transducer is very similar to a left-to-right FST, except that it works top-down, pursuing subtrees in parallel, with each subtree transformed depending only on its own passed-down state. This class of transducer is often nowadays called R, for “Root-to-frontier” (Gécseg and Steinby, 1984). Rounds uses a mathematics-oriented example of an R transducer, which we summarize in Figure 1. At each point in the top-down traversal, the transducer choose"
N04-1014,J97-3002,0,0.240874,"t, such as Viterbi decoding (Viterbi, 1967) and forward-backward training (Baum and Eagon, 1967), as well as generic software toolkits. Moreover, a surprising variety of problems are attackable with FSTs, from partof-speech tagging to letter-to-sound conversion to name transliteration. However, language problems like machine translation break this mold, because they involve massive reordering of symbols, and because the transformation processes seem sensitive to hierarchical tree structure. Recently, specific probabilistic tree-based models have been proposed not only for machine translation (Wu, 1997; Alshawi, Bangalore, and Douglas, 2000; Yamada and Knight, 2001; Gildea, 2003; Eisner, 2003), but also for This work was supported by DARPA contract F49620-001-0337 and ARDA contract MDA904-02-C-0450. Kevin Knight Information Sciences Institute University of Southern California 4676 Admiralty Way Marina del Rey, CA 90292 knight@isi.edu summarization (Knight and Marcu, 2002), paraphrasing (Pang, Knight, and Marcu, 2003), natural language generation (Langkilde and Knight, 1998; Bangalore and Rambow, 2000; Corston-Oliver et al., 2002), and language modeling (Baker, 1979; Lari and Young, 1990; Co"
N04-1014,P01-1067,1,0.780567,"orward-backward training (Baum and Eagon, 1967), as well as generic software toolkits. Moreover, a surprising variety of problems are attackable with FSTs, from partof-speech tagging to letter-to-sound conversion to name transliteration. However, language problems like machine translation break this mold, because they involve massive reordering of symbols, and because the transformation processes seem sensitive to hierarchical tree structure. Recently, specific probabilistic tree-based models have been proposed not only for machine translation (Wu, 1997; Alshawi, Bangalore, and Douglas, 2000; Yamada and Knight, 2001; Gildea, 2003; Eisner, 2003), but also for This work was supported by DARPA contract F49620-001-0337 and ARDA contract MDA904-02-C-0450. Kevin Knight Information Sciences Institute University of Southern California 4676 Admiralty Way Marina del Rey, CA 90292 knight@isi.edu summarization (Knight and Marcu, 2002), paraphrasing (Pang, Knight, and Marcu, 2003), natural language generation (Langkilde and Knight, 1998; Bangalore and Rambow, 2000; Corston-Oliver et al., 2002), and language modeling (Baker, 1979; Lari and Young, 1990; Collins, 1997; Chelba and Jelinek, 2000; Charniak, 2001; Klein and"
N04-1014,C98-1112,1,\N,Missing
N04-1035,2003.mtsummit-papers.6,1,0.303093,"tistical MT. We take this approach in our paper. Of course, the broad statistical MT program is aimed at a wider goal than the conventional rule-based program – it seeks to understand and explain human translation data, and automatically learn from it. For this reason, we think it is important to learn from the model/data explainability studies of Fox (2002) and to extend her results. In addition to being motivated by rule-based systems, we also see advantages to English syntax within the statistical framework, such as marrying syntax-based translation models with syntaxbased language models (Charniak et al., 2003) and other potential benefits described by Eisner (2003). Our basic idea is to create transformation rules that condition on larger fragments of tree structure. It is certainly possible to build such rules by hand, and we have done this to formally explain a number of humantranslation examples. But our main interest is in collecting a large set of such rules automatically through corpus analysis. The search for these rules is driven exactly by the problems raised by Fox (2002) – cases of crossing and divergence motivate the algorithms to come up with better explanations of the data and better"
N04-1035,P03-2041,0,0.926149,"road statistical MT program is aimed at a wider goal than the conventional rule-based program – it seeks to understand and explain human translation data, and automatically learn from it. For this reason, we think it is important to learn from the model/data explainability studies of Fox (2002) and to extend her results. In addition to being motivated by rule-based systems, we also see advantages to English syntax within the statistical framework, such as marrying syntax-based translation models with syntaxbased language models (Charniak et al., 2003) and other potential benefits described by Eisner (2003). Our basic idea is to create transformation rules that condition on larger fragments of tree structure. It is certainly possible to build such rules by hand, and we have done this to formally explain a number of humantranslation examples. But our main interest is in collecting a large set of such rules automatically through corpus analysis. The search for these rules is driven exactly by the problems raised by Fox (2002) – cases of crossing and divergence motivate the algorithms to come up with better explanations of the data and better rules. Section 2 of this paper describes algorithms for"
N04-1035,W02-1039,0,0.537809,"Dept. of Computer Science Information Sciences Institute University of California University of Southern California Los Angeles, CA 90024 Marina Del Rey, CA 90292 mhopkins@cs.ucla.edu Abstract We propose a theory that gives formal semantics to word-level alignments defined over parallel corpora. We use our theory to introduce a linear algorithm that can be used to derive from word-aligned, parallel corpora the minimal set of syntactically motivated transformation rules that explain human translation data. 1 Introduction In a very interesting study of syntax in statistical machine translation, Fox (2002) looks at how well proposed translation models fit actual translation data. One such model embodies a restricted, linguistically-motivated notion of word re-ordering. Given an English parse tree, children at any node may be reordered prior to translation. Nodes are processed independently. Previous to Fox (2002), it had been observed that this model would prohibit certain re-orderings in certain language pairs (such as subjectVP(verb-object) into verb-subject-object), but Fox carried out the first careful empirical study, showing that many other common translation patterns fall outside the sco"
N04-1035,P03-1011,0,0.0428361,"e focused on providing a well-founded mathematical theory and efficient, linear algorithms for learning syntactically motivated transformation rules from parallel corpora. One can easily imagine a range of techniques for defining probability distributions over the rules that we learn. We suspect that such probabilistic rules could be also used in conjunction with statistical decoders, to increase the accuracy of statistical machine translation systems. 5 Conclusion The fundamental assumption underlying much recent work in statistical machine translation (Yamada and Knight, 2001; Eisner, 2003; Gildea, 2003) is that local transformations (primarily child-node re-orderings) of one-level parent-children substructures are an adequate model for parallel corpora. Our empirical results suggest that this may be too strong of an assumption. To explain the data in two parallel corpora, one English-French, and one English-Chinese, we are often forced to learn rules involving much larger tree fragments. The theory, algorithms, and transformation rules we learn automatically from data have several interesting aspects. 1. Our rules provide a good, realistic indicator of the complexities inherent in translatio"
N04-1035,N03-1017,1,0.077612,"parsing errors also cause trouble, as a normally well-behaved re-ordering environment can be disrupted by wrong phrase attachment. For other language pairs, the divergence is expected to be greater. In the face of these problems, we may choose among several alternatives. The first is to abandon syntax in statistical machine translation, on the grounds that syntactic models are a poor fit for the data. On this view, adding syntax yields no improvement over robust phrasesubstitution models, and the only question is how much {knight,marcu}@isi.edu does syntax hurt performance. Along this line, (Koehn et al., 2003) present convincing evidence that restricting phrasal translation to syntactic constituents yields poor translation performance – the ability to translate nonconstituent phrases (such as “there are”, “note that”, and “according to”) turns out to be critical and pervasive. Another direction is to abandon conventional English syntax and move to more robust grammars that adapt to the parallel training corpus. One approach here is that of Wu (1997), in which word-movement is modeled by rotations at unlabeled, binary-branching nodes. At each sentence pair, the parse adapts to explain the translatio"
N04-1035,P00-1056,0,0.251997,"to explain the data well. 3.2 Data We performed experiments with two corpora, the FBIS English-Chinese Parallel Text and the Hansard FrenchEnglish corpus.We parsed the English sentences with a state-of-the-art statistical parser (Collins, 1999). For the FBIS corpus (representing eight million English words), we automatically generated word-alignments using GIZA++ (Och and Ney, 2003), which we trained on a much larger data set (150 million words). Cases other than one-to-one sentence mappings were eliminated. For the Hansard corpus, we took the human annotation of word alignment described in (Och and Ney, 2000). The corpus contains two kinds of alignments: S (sure) for unambiguous cases and P (possible) for unclear cases, e.g. idiomatic expressions and missing function words (S ⊆ P ). In order to be able to make legitimate comparisons between the two language pairs, we also used GIZA++ to obtain machine-generated word alignments for Hansard: we trained it with the 500 sentences and additional data representing 13.7 million English words (taken from the Hansard and European parliament corpora). 3.3 Results From a theoretical point of view, we have shown that our model can fully explain the transforma"
N04-1035,J03-1002,0,0.0261537,"study with that of Fox (2002). The additional language pair provides a good means of evaluating how our transformation rule extraction method scales to more problematic language pairs for which child-reordering models are shown not to explain the data well. 3.2 Data We performed experiments with two corpora, the FBIS English-Chinese Parallel Text and the Hansard FrenchEnglish corpus.We parsed the English sentences with a state-of-the-art statistical parser (Collins, 1999). For the FBIS corpus (representing eight million English words), we automatically generated word-alignments using GIZA++ (Och and Ney, 2003), which we trained on a much larger data set (150 million words). Cases other than one-to-one sentence mappings were eliminated. For the Hansard corpus, we took the human annotation of word alignment described in (Och and Ney, 2000). The corpus contains two kinds of alignments: S (sure) for unambiguous cases and P (possible) for unclear cases, e.g. idiomatic expressions and missing function words (S ⊆ P ). In order to be able to make legitimate comparisons between the two language pairs, we also used GIZA++ to obtain machine-generated word alignments for Hansard: we trained it with the 500 sen"
N04-1035,J97-3002,0,0.857464,"er robust phrasesubstitution models, and the only question is how much {knight,marcu}@isi.edu does syntax hurt performance. Along this line, (Koehn et al., 2003) present convincing evidence that restricting phrasal translation to syntactic constituents yields poor translation performance – the ability to translate nonconstituent phrases (such as “there are”, “note that”, and “according to”) turns out to be critical and pervasive. Another direction is to abandon conventional English syntax and move to more robust grammars that adapt to the parallel training corpus. One approach here is that of Wu (1997), in which word-movement is modeled by rotations at unlabeled, binary-branching nodes. At each sentence pair, the parse adapts to explain the translation pattern. If the same unambiguous English sentence were to appear twice in the corpus, with different Chinese translations, then it could have different learned parses. A third direction is to maintain English syntax and investigate alternate transformation models. After all, many conventional translation systems are indeed based on syntactic transformations far more expressive than what has been proposed in syntax-based statistical MT. We tak"
N04-1035,P01-1067,1,0.719731,"ossing due to a modal. In this paper, we focused on providing a well-founded mathematical theory and efficient, linear algorithms for learning syntactically motivated transformation rules from parallel corpora. One can easily imagine a range of techniques for defining probability distributions over the rules that we learn. We suspect that such probabilistic rules could be also used in conjunction with statistical decoders, to increase the accuracy of statistical machine translation systems. 5 Conclusion The fundamental assumption underlying much recent work in statistical machine translation (Yamada and Knight, 2001; Eisner, 2003; Gildea, 2003) is that local transformations (primarily child-node re-orderings) of one-level parent-children substructures are an adequate model for parallel corpora. Our empirical results suggest that this may be too strong of an assumption. To explain the data in two parallel corpora, one English-French, and one English-Chinese, we are often forced to learn rules involving much larger tree fragments. The theory, algorithms, and transformation rules we learn automatically from data have several interesting aspects. 1. Our rules provide a good, realistic indicator of the comple"
N04-1035,J03-4003,0,\N,Missing
N06-1001,W04-3237,0,0.0273301,"capitalization tag sequence. Associating a tag in the output with the corresponding A capitalizer is a tagger that recovers the capitalization tag for each input lowercased word, outputting a well-capitalized sentence. Since each lowercased word can have more than one tag, and associating a tag with a lowercased word can result in more than one surface form (e.g., /home/doc MX can be either /home/DOC or /home/Doc), we need a capitalization model to solve the capitalization ambiguities. For example, Lita et al. (2003) use a trigram language model estimated from a corpus with case information; Chelba and Acero (2004) use a maximum entropy Markov model (MEMM) combining features involving words and their cases. Capitalization models presented in most previous approaches are monolingual because the models are estimated only from monolingual texts. However, for capitalizing machine translation outputs, using only monolingual capitalization models is not enough. For example, if the sentence “click ok to save your changes to /home/doc .” in the above example is the translation of the French sentence “CLIQUEZ SUR OK POUR ENREGISTRER VOS MODIFICATIONS DANS /HOME/DOC .”, the correct capitalization result should pr"
N06-1001,N04-1035,1,0.501754,"wo words in E represents the dependency between them captured by monolingual n-gram language models. We also assume that both E and F have phrase boundaries available (denoted by the square brackets), and that A is the phrase alignment. ˜i is the i-th In Figure 3, F˜j is the j-th phrase of F , E phrase of E, and they align to each other. We do not require a word alignment; instead we find it reason˜i can be aligned to any able to think that a word in E adapted to syntax-based machine translation, too. To this end, the translational correspondence is described within a translation rule, i.e., (Galley et al., 2004) (or a synchronous production), rather than a translational phrase pair; and the training data will be derivation forests, instead of the phrase-aligned bilingual corpus. 2 The capitalization model p(E|F, A) itself does not require the existence of e. This means that in principle this model can also be viewed as a capitalized translation model that performs translation and capitalization in an integrated step. In our paper, however, we consider the case where the machine translation output e is given, which is reflected by the the fact that GEN(e) takes e as input in Formula 1. word in F˜j . A"
N06-1001,P03-1020,0,0.768398,"getting the surface form “Click OK to save your changes to /home/DOC .”. We present a probabilistic bilingual capitalization model for capitalizing machine translation outputs using conditional random fields. Experiments carried out on three language pairs and a variety of experiment conditions show that our model significantly outperforms a strong monolingual capitalization model baseline, especially when working with small datasets and/or European language pairs. 1 Introduction Capitalization is the process of recovering case information for texts in lowercase. It is also called truecasing (Lita et al., 2003). Usually, capitalization itself tries to improve the legibility of texts. It, however, can affect the word choice or order when interacting with other models. In natural language processing, a good capitalization model has been shown useful for tasks like name entity recognition, automatic content extraction, speech recognition, modern word processors, and machine translation (MT). Capitalization can be viewed as a sequence labeling process. The input to this process is a sentence in lowercase. For each lowercased word in the input sentence, we have several available capitalization tags: init"
N06-1001,W02-1018,1,0.822044,"d bilingual capitalization model using the development set. Since estimation of the feature weights requires the phrase alignment information, we efficiently applied the NPA on the development set. We employed two LM-based capitalizers as baselines for performance comparison: a unigram-based capitalizer and a strong trigram-based one. The unigram-based capitalizer is the usual baseline for capitalization experiments in previous work. The trigram-based baseline is similar to the one in (Lita et al., 2003) except that we used Kneser-Ney smoothing instead of a mixture. A phrase-based SMT system (Marcu and Wong, 2002) was trained on the bitext. The capitalizer was incorporated into the MT system as a postprocessing module — it capitalizes the lowercased MT output. The phrase boundaries and alignments needed by the capitalizer were automatically inferred as part of the decoding process. 6 Experiments 6.2 6.1 Settings We conducted capitalization experiments on three language pairs: English-to-French (E→F) with a bilingual corpus from the Information Technology (IT) domain; French-to-English (F→E) with a bilingual corpus from the general news domain; and Chinese-to-English (C→E) with a bilingual corpus from t"
N06-1001,P99-1021,0,0.0214936,"for each low2 Finput Lower Case {F } Lower Case {f } Train Translation Model Lower Case {e} Train Language Model f Translation Model MT Decoder {E} Train Monolingual Capitalization Model Languagel Model Monolingual Cap Model e Capitalization Eoutput Figure 1: The monolingual capitalization scheme employed by most statistical MT systems. ercased sentence e, they find the label sequence T that maximizes p(T |e). They use a maximum entropy Markov model (MEMM) to combine features of words, cases and context (i.e., tag transitions). Gale et al. (1994) report good results on capitalizing 100 words. Mikheev (1999) performs capitalization using simple positional heuristics. 3 Monolingual Capitalization Scheme Translation and capitalization are usually performed in two successive steps because removing case information from the training of translation models substantially reduces both the source and target vocabulary sizes. Smaller vocabularies lead to a smaller translation model with fewer parameters to learn. For example, if we do not remove the case information, we will have to deal with at least nine probabilities for the English-French word pair (click, cliquez). This is because either “click” or “c"
N06-1001,J04-4002,0,0.0154102,"alignment can be quite computationally expensive as it requires to translate the entire training corpus; also a phrase aligner is not always available. We therefore generate the training data using a na¨ıve phrase aligner (NPA) instead of resorting to a real one. The input to the NPA is a word-aligned bilingual corpus. The NPA stochastically chooses for each sentence pair one segmentation and phrase alignment that is consistent with the word alignment. An aligned phrase pair is consistent with the word alignment if neither phrase contains any word aligning to a word outside the other phrase (Och and Ney, 2004). The NPA chunks the source sentence into phrases according to a probabilistic distribution over source phrase lengths. This distribution can be obtained from the trace output of a phrase-based MT Languages E→F (IT) F→E (news) C→E (news) Entire Corpus (#W) Training Dev Test-Prec. 62M 13K 15K 144M 11K 22K 50M 8K 17K Test-BLEU (#sents) 763 241 919 Table 2: Corpora used in experiments. decoder on a small development set. The NPA has to retry if the current source phrase cannot find any consistent target phrase. Unaligned target words are attached to the left phrase. Heuristics are employed to pre"
N06-1001,2001.mtsummit-papers.68,0,0.0158198,"assess the impact of our capitalizer on end-to-end translation performance; in this case, the capitalizer may operate on ungrammatical sentences. We chose to work with these three language pairs because we wanted to test our capitalization model on both English and French target MT systems and in cases where the source language has no case information (such as in Chinese). We estimated the feature functions, such as the log probabilities in the language model, from the 6 BLEU and Precision We measured the impact of our capitalization model in the context of an end-to-end MT system using BLEU (Papineni et al., 2001). In this context, the capitalizer operates on potentially ill-formed, MTproduced outputs. To this end, we first integrated our bilingual capitalizer into the phrase-based SMT system as a postprocessing module. The decoder of the MT system was modified to provide the capitalizer with the case-preserved source sentence, the lowercased translation, and the phrase boundaries and their alignments. Based on this information, our bilingual capitalizer recovers the case information of the lowercased translation, outputting a capitalized target sentence. The case-restored machine translations were eva"
N06-1001,P04-1007,0,0.356563,"in Formula 1. word in F˜j . A probabilistic model defined on this graph is a Conditional Random Field. Therefore, it is natural to formulate the bilingual capitalization model using CRFs:3 ! (2) ! (3) I X 1 pλ (E|F, A) = exp λi fi (E, F, A) Z(F, A, λ) i=1 where Z(F, A, λ) = X E∈GEN(e) exp I X λi fi (E, F, A) i=1 fi (E, F, A), i = 1...I are the I features, and λ = (λ1 , ..., λI ) is the feature weight vector. Based on this capitalization model, the decoder in the capitalizer looks for the best E ∗ such that ∗ E = arg maxE∈GEN(e,F ) I X λi fi (E, F, A) (4) i=1 4.2 Parameter Estimation Following Roark et al. (2004), Lafferty et al. (2001) and Chen and Rosenfeld (1999), we are looking for the set of feature weights λ maximizing the regularized log-likelihood LLR (λ) of the training data {E (n) , F (n) , A(n) , n = 1, ..., N }. LLR (λ) = N X “ ” ||λ||2 log p E (n) |F (n) , A(n) − 2σ 2 n=1 (5) 4.3 Feature Functions We define features based on the alignment graph in Figure 3. Each feature function is defined on a word. Monolingual language model feature. The monolingual LM feature of word Ei is the logarithm of the probability of the n-gram ending at Ei : fLM (Ei , F, A) = log p(Ei |Ei−1 , ..., Ei−n+1 ) (6)"
N06-1001,P02-1040,0,\N,Missing
N06-1031,J00-1004,0,0.0114045,"orating a handful of relabeling strategies that yields a statistically significant improvement of 2.3 BLEU points over a baseline syntax-based system.  1 Introduction Recent work in statistical machine translation (MT) has sought to overcome the limitations of phrasebased models (Marcu and Wong, 2002; Koehn et al., 2003; Och and Ney, 2004) by making use of syntactic information. Syntax-based MT offers the potential advantages of enforcing syntaxmotivated constraints in translation and capturing long-distance/non-contiguous dependencies. Some approaches have used syntax at the core (Wu, 1997; Alshawi et al., 2000; Yamada and Knight, 2001; Gildea, 2003; Eisner, 2003; Hearne and Way, 2003; Melamed, 2004) while others have integrated syntax into existing phrase-based frameworks (Xia and McCord, 2004; Chiang, 2005; Collins et al., 2005; Quirk et al., 2005). In this work, we employ a syntax-based model that applies a series of tree/string (xRS) rules (Galley et al., 2004; Graehl and Knight, 2004) to a source language string to produce a target language phrase structure tree. Figure 1 exemplifies the translation  process, which is called a derivation, from Chinese into English. The source string to trans"
N06-1031,P05-1033,0,0.0374814,"ation (MT) has sought to overcome the limitations of phrasebased models (Marcu and Wong, 2002; Koehn et al., 2003; Och and Ney, 2004) by making use of syntactic information. Syntax-based MT offers the potential advantages of enforcing syntaxmotivated constraints in translation and capturing long-distance/non-contiguous dependencies. Some approaches have used syntax at the core (Wu, 1997; Alshawi et al., 2000; Yamada and Knight, 2001; Gildea, 2003; Eisner, 2003; Hearne and Way, 2003; Melamed, 2004) while others have integrated syntax into existing phrase-based frameworks (Xia and McCord, 2004; Chiang, 2005; Collins et al., 2005; Quirk et al., 2005). In this work, we employ a syntax-based model that applies a series of tree/string (xRS) rules (Galley et al., 2004; Graehl and Knight, 2004) to a source language string to produce a target language phrase structure tree. Figure 1 exemplifies the translation  process, which is called a derivation, from Chinese into English. The source string to translate ( 1 .) is shown at the top left. Rule replaces the Chinese word (shaded) with the 2 then builds a VP over English NP-C police. Rule the NP-C sequence. Next, is translated 3 Finally, rule 4 as the"
N06-1031,P05-1066,0,0.0210928,"Missing"
N06-1031,P03-2041,0,0.0148112,"istically significant improvement of 2.3 BLEU points over a baseline syntax-based system.  1 Introduction Recent work in statistical machine translation (MT) has sought to overcome the limitations of phrasebased models (Marcu and Wong, 2002; Koehn et al., 2003; Och and Ney, 2004) by making use of syntactic information. Syntax-based MT offers the potential advantages of enforcing syntaxmotivated constraints in translation and capturing long-distance/non-contiguous dependencies. Some approaches have used syntax at the core (Wu, 1997; Alshawi et al., 2000; Yamada and Knight, 2001; Gildea, 2003; Eisner, 2003; Hearne and Way, 2003; Melamed, 2004) while others have integrated syntax into existing phrase-based frameworks (Xia and McCord, 2004; Chiang, 2005; Collins et al., 2005; Quirk et al., 2005). In this work, we employ a syntax-based model that applies a series of tree/string (xRS) rules (Galley et al., 2004; Graehl and Knight, 2004) to a source language string to produce a target language phrase structure tree. Figure 1 exemplifies the translation  process, which is called a derivation, from Chinese into English. The source string to translate ( 1 .) is shown at the top left. Rule replaces t"
N06-1031,N04-1035,1,0.623924,"yntactic information. Syntax-based MT offers the potential advantages of enforcing syntaxmotivated constraints in translation and capturing long-distance/non-contiguous dependencies. Some approaches have used syntax at the core (Wu, 1997; Alshawi et al., 2000; Yamada and Knight, 2001; Gildea, 2003; Eisner, 2003; Hearne and Way, 2003; Melamed, 2004) while others have integrated syntax into existing phrase-based frameworks (Xia and McCord, 2004; Chiang, 2005; Collins et al., 2005; Quirk et al., 2005). In this work, we employ a syntax-based model that applies a series of tree/string (xRS) rules (Galley et al., 2004; Graehl and Knight, 2004) to a source language string to produce a target language phrase structure tree. Figure 1 exemplifies the translation  process, which is called a derivation, from Chinese into English. The source string to translate ( 1 .) is shown at the top left. Rule replaces the Chinese word (shaded) with the 2 then builds a VP over English NP-C police. Rule the NP-C sequence. Next, is translated 3 Finally, rule 4 as the NP-C the gunman by rule . combines the sequence of NP-C VP . into an S, denoting a complete tree. The yield of this tree gives the target translation: the gunm"
N06-1031,P03-1011,0,0.00749152,"yields a statistically significant improvement of 2.3 BLEU points over a baseline syntax-based system.  1 Introduction Recent work in statistical machine translation (MT) has sought to overcome the limitations of phrasebased models (Marcu and Wong, 2002; Koehn et al., 2003; Och and Ney, 2004) by making use of syntactic information. Syntax-based MT offers the potential advantages of enforcing syntaxmotivated constraints in translation and capturing long-distance/non-contiguous dependencies. Some approaches have used syntax at the core (Wu, 1997; Alshawi et al., 2000; Yamada and Knight, 2001; Gildea, 2003; Eisner, 2003; Hearne and Way, 2003; Melamed, 2004) while others have integrated syntax into existing phrase-based frameworks (Xia and McCord, 2004; Chiang, 2005; Collins et al., 2005; Quirk et al., 2005). In this work, we employ a syntax-based model that applies a series of tree/string (xRS) rules (Galley et al., 2004; Graehl and Knight, 2004) to a source language string to produce a target language phrase structure tree. Figure 1 exemplifies the translation  process, which is called a derivation, from Chinese into English. The source string to translate ( 1 .) is shown at the top left. R"
N06-1031,N04-1014,1,0.124067,"Syntax-based MT offers the potential advantages of enforcing syntaxmotivated constraints in translation and capturing long-distance/non-contiguous dependencies. Some approaches have used syntax at the core (Wu, 1997; Alshawi et al., 2000; Yamada and Knight, 2001; Gildea, 2003; Eisner, 2003; Hearne and Way, 2003; Melamed, 2004) while others have integrated syntax into existing phrase-based frameworks (Xia and McCord, 2004; Chiang, 2005; Collins et al., 2005; Quirk et al., 2005). In this work, we employ a syntax-based model that applies a series of tree/string (xRS) rules (Galley et al., 2004; Graehl and Knight, 2004) to a source language string to produce a target language phrase structure tree. Figure 1 exemplifies the translation  process, which is called a derivation, from Chinese into English. The source string to translate ( 1 .) is shown at the top left. Rule replaces the Chinese word (shaded) with the 2 then builds a VP over English NP-C police. Rule the NP-C sequence. Next, is translated 3 Finally, rule 4 as the NP-C the gunman by rule . combines the sequence of NP-C VP . into an S, denoting a complete tree. The yield of this tree gives the target translation: the gunman was killed by police ."
N06-1031,2003.mtsummit-papers.22,0,0.00982856,"ificant improvement of 2.3 BLEU points over a baseline syntax-based system.  1 Introduction Recent work in statistical machine translation (MT) has sought to overcome the limitations of phrasebased models (Marcu and Wong, 2002; Koehn et al., 2003; Och and Ney, 2004) by making use of syntactic information. Syntax-based MT offers the potential advantages of enforcing syntaxmotivated constraints in translation and capturing long-distance/non-contiguous dependencies. Some approaches have used syntax at the core (Wu, 1997; Alshawi et al., 2000; Yamada and Knight, 2001; Gildea, 2003; Eisner, 2003; Hearne and Way, 2003; Melamed, 2004) while others have integrated syntax into existing phrase-based frameworks (Xia and McCord, 2004; Chiang, 2005; Collins et al., 2005; Quirk et al., 2005). In this work, we employ a syntax-based model that applies a series of tree/string (xRS) rules (Galley et al., 2004; Graehl and Knight, 2004) to a source language string to produce a target language phrase structure tree. Figure 1 exemplifies the translation  process, which is called a derivation, from Chinese into English. The source string to translate ( 1 .) is shown at the top left. Rule replaces the Chinese word (shade"
N06-1031,J98-4004,0,0.0138072,"this rule with rule deeply love her. We experimented with four sisterhood annotation (SISTERHOOD) variants of decreasing complexity. The first was described above, which includes rightmost (#L), leftmost (#R), middle (#LR), and alone (no annotation). Variant 2 omitted #LR, variant 3 kept only #LR, and variant 4 only annotated nodes without sisters. Variants 1 and 2 produced the largest gains from relabeling: 1.27 and 0.85 BLEU points, respectively. 245 Figure 8: Rules before and after parent annotation. 3.2.2 Parent Annotation Another common relabeling method in parsing is parent annotation (Johnson, 1998), in which a node is annotated with its parent’s label. Typically, this is done only to nonterminals, but Klein and Manning (2003) found that annotating preterminals as well was highly effective. It seemed likely that such contextual information could also benefit MT. Let us tackle the bad output from Figure 6 with 4 is relabeled as parent annotation. In Figure 8, rule 40 and expects an NP-CˆVP, i.e., an NP-C with a rule VP parent. In the PTB, we observe that the NP-C she never has a VP parent, while her does. In fact, the most popular parent for the NP-C her is VP, while the 3 is relabeled mo"
N06-1031,P03-1054,0,0.172284,".0K 3 +2.9M +17.4K –3.5K –143.4K –9.4K BLEU 20.06 20.2 20.36 20.14 20.15 20.18 20.09 20.09 20.11 20.07 20.03 20.14 20.28 21.33 20.91 20.36 20.59 19.77 20.01 15.63 20.36 19.93 19.3 20.01 ∆ — ▲ ▲ ▲ ▲ ▲ ▲ ? ▲ ? ▼ ▲ ▲ ▲ ▲ ▲ ▲ ▼ ▼ ▼ ▲ ▼ ▼ ▼ Figure 4: Rules before and after lexicalization. calization and tag annotation. 3.1.1 Figure 3: For each relabeling method and variant, the impact on ruleset size and BLEU score over the baseline. 3 Relabeling The small tagset of the PTB has the advantage of being simple to annotate and to parse. On the other hand, this can lead to tags that are overly generic. Klein and Manning (2003) discuss this as a problem in parsing and demonstrate that annotating additional information onto the PTB tags leads to improved parsing performance. We similarly propose methods of relabeling PTB trees that notably improve MT quality. In the next two subsections, we explore relabeling strategies that fall under two categories introduced by Klein and Manning — internal annotation and external annotation. 3.1 Internal Annotation Internal annotation reveals information about a node and its descendants to its surrounding nodes (ancestors, sisters, and other relatives) that is otherwise hidden. Th"
N06-1031,N03-1017,0,0.00768258,"arina del Rey, CA 90292 bhuang@languageweaver.com knight@isi.edu Abstract   We identify problems with the Penn Treebank that render it imperfect for syntaxbased machine translation and propose methods of relabeling the syntax trees to improve translation quality. We develop a system incorporating a handful of relabeling strategies that yields a statistically significant improvement of 2.3 BLEU points over a baseline syntax-based system.  1 Introduction Recent work in statistical machine translation (MT) has sought to overcome the limitations of phrasebased models (Marcu and Wong, 2002; Koehn et al., 2003; Och and Ney, 2004) by making use of syntactic information. Syntax-based MT offers the potential advantages of enforcing syntaxmotivated constraints in translation and capturing long-distance/non-contiguous dependencies. Some approaches have used syntax at the core (Wu, 1997; Alshawi et al., 2000; Yamada and Knight, 2001; Gildea, 2003; Eisner, 2003; Hearne and Way, 2003; Melamed, 2004) while others have integrated syntax into existing phrase-based frameworks (Xia and McCord, 2004; Chiang, 2005; Collins et al., 2005; Quirk et al., 2005). In this work, we employ a syntax-based model that applie"
N06-1031,W04-3250,0,0.0696197,"Missing"
N06-1031,W02-1018,0,0.019634,"a 4676 Admiralty Way Marina del Rey, CA 90292 bhuang@languageweaver.com knight@isi.edu Abstract   We identify problems with the Penn Treebank that render it imperfect for syntaxbased machine translation and propose methods of relabeling the syntax trees to improve translation quality. We develop a system incorporating a handful of relabeling strategies that yields a statistically significant improvement of 2.3 BLEU points over a baseline syntax-based system.  1 Introduction Recent work in statistical machine translation (MT) has sought to overcome the limitations of phrasebased models (Marcu and Wong, 2002; Koehn et al., 2003; Och and Ney, 2004) by making use of syntactic information. Syntax-based MT offers the potential advantages of enforcing syntaxmotivated constraints in translation and capturing long-distance/non-contiguous dependencies. Some approaches have used syntax at the core (Wu, 1997; Alshawi et al., 2000; Yamada and Knight, 2001; Gildea, 2003; Eisner, 2003; Hearne and Way, 2003; Melamed, 2004) while others have integrated syntax into existing phrase-based frameworks (Xia and McCord, 2004; Chiang, 2005; Collins et al., 2005; Quirk et al., 2005). In this work, we employ a syntax-bas"
N06-1031,J93-2004,0,0.0371656,"Missing"
N06-1031,P04-1083,0,0.00641834,"2.3 BLEU points over a baseline syntax-based system.  1 Introduction Recent work in statistical machine translation (MT) has sought to overcome the limitations of phrasebased models (Marcu and Wong, 2002; Koehn et al., 2003; Och and Ney, 2004) by making use of syntactic information. Syntax-based MT offers the potential advantages of enforcing syntaxmotivated constraints in translation and capturing long-distance/non-contiguous dependencies. Some approaches have used syntax at the core (Wu, 1997; Alshawi et al., 2000; Yamada and Knight, 2001; Gildea, 2003; Eisner, 2003; Hearne and Way, 2003; Melamed, 2004) while others have integrated syntax into existing phrase-based frameworks (Xia and McCord, 2004; Chiang, 2005; Collins et al., 2005; Quirk et al., 2005). In this work, we employ a syntax-based model that applies a series of tree/string (xRS) rules (Galley et al., 2004; Graehl and Knight, 2004) to a source language string to produce a target language phrase structure tree. Figure 1 exemplifies the translation  process, which is called a derivation, from Chinese into English. The source string to translate ( 1 .) is shown at the top left. Rule replaces the Chinese word (shaded) with the 2 th"
N06-1031,P00-1056,0,0.0119096,"Missing"
N06-1031,J04-4002,0,0.00539745,"292 bhuang@languageweaver.com knight@isi.edu Abstract   We identify problems with the Penn Treebank that render it imperfect for syntaxbased machine translation and propose methods of relabeling the syntax trees to improve translation quality. We develop a system incorporating a handful of relabeling strategies that yields a statistically significant improvement of 2.3 BLEU points over a baseline syntax-based system.  1 Introduction Recent work in statistical machine translation (MT) has sought to overcome the limitations of phrasebased models (Marcu and Wong, 2002; Koehn et al., 2003; Och and Ney, 2004) by making use of syntactic information. Syntax-based MT offers the potential advantages of enforcing syntaxmotivated constraints in translation and capturing long-distance/non-contiguous dependencies. Some approaches have used syntax at the core (Wu, 1997; Alshawi et al., 2000; Yamada and Knight, 2001; Gildea, 2003; Eisner, 2003; Hearne and Way, 2003; Melamed, 2004) while others have integrated syntax into existing phrase-based frameworks (Xia and McCord, 2004; Chiang, 2005; Collins et al., 2005; Quirk et al., 2005). In this work, we employ a syntax-based model that applies a series of tree/s"
N06-1031,P02-1040,0,0.107041,"Missing"
N06-1031,P05-1034,0,0.00830705,"e limitations of phrasebased models (Marcu and Wong, 2002; Koehn et al., 2003; Och and Ney, 2004) by making use of syntactic information. Syntax-based MT offers the potential advantages of enforcing syntaxmotivated constraints in translation and capturing long-distance/non-contiguous dependencies. Some approaches have used syntax at the core (Wu, 1997; Alshawi et al., 2000; Yamada and Knight, 2001; Gildea, 2003; Eisner, 2003; Hearne and Way, 2003; Melamed, 2004) while others have integrated syntax into existing phrase-based frameworks (Xia and McCord, 2004; Chiang, 2005; Collins et al., 2005; Quirk et al., 2005). In this work, we employ a syntax-based model that applies a series of tree/string (xRS) rules (Galley et al., 2004; Graehl and Knight, 2004) to a source language string to produce a target language phrase structure tree. Figure 1 exemplifies the translation  process, which is called a derivation, from Chinese into English. The source string to translate ( 1 .) is shown at the top left. Rule replaces the Chinese word (shaded) with the 2 then builds a VP over English NP-C police. Rule the NP-C sequence. Next, is translated 3 Finally, rule 4 as the NP-C the gunman by rule . combines the sequ"
N06-1031,J97-3002,0,0.019106,"tem incorporating a handful of relabeling strategies that yields a statistically significant improvement of 2.3 BLEU points over a baseline syntax-based system.  1 Introduction Recent work in statistical machine translation (MT) has sought to overcome the limitations of phrasebased models (Marcu and Wong, 2002; Koehn et al., 2003; Och and Ney, 2004) by making use of syntactic information. Syntax-based MT offers the potential advantages of enforcing syntaxmotivated constraints in translation and capturing long-distance/non-contiguous dependencies. Some approaches have used syntax at the core (Wu, 1997; Alshawi et al., 2000; Yamada and Knight, 2001; Gildea, 2003; Eisner, 2003; Hearne and Way, 2003; Melamed, 2004) while others have integrated syntax into existing phrase-based frameworks (Xia and McCord, 2004; Chiang, 2005; Collins et al., 2005; Quirk et al., 2005). In this work, we employ a syntax-based model that applies a series of tree/string (xRS) rules (Galley et al., 2004; Graehl and Knight, 2004) to a source language string to produce a target language phrase structure tree. Figure 1 exemplifies the translation  process, which is called a derivation, from Chinese into English. The"
N06-1031,C04-1073,0,0.015147,"istical machine translation (MT) has sought to overcome the limitations of phrasebased models (Marcu and Wong, 2002; Koehn et al., 2003; Och and Ney, 2004) by making use of syntactic information. Syntax-based MT offers the potential advantages of enforcing syntaxmotivated constraints in translation and capturing long-distance/non-contiguous dependencies. Some approaches have used syntax at the core (Wu, 1997; Alshawi et al., 2000; Yamada and Knight, 2001; Gildea, 2003; Eisner, 2003; Hearne and Way, 2003; Melamed, 2004) while others have integrated syntax into existing phrase-based frameworks (Xia and McCord, 2004; Chiang, 2005; Collins et al., 2005; Quirk et al., 2005). In this work, we employ a syntax-based model that applies a series of tree/string (xRS) rules (Galley et al., 2004; Graehl and Knight, 2004) to a source language string to produce a target language phrase structure tree. Figure 1 exemplifies the translation  process, which is called a derivation, from Chinese into English. The source string to translate ( 1 .) is shown at the top left. Rule replaces the Chinese word (shaded) with the 2 then builds a VP over English NP-C police. Rule the NP-C sequence. Next, is translated 3 Finally,"
N06-1031,P01-1067,1,0.0529636,"elabeling strategies that yields a statistically significant improvement of 2.3 BLEU points over a baseline syntax-based system.  1 Introduction Recent work in statistical machine translation (MT) has sought to overcome the limitations of phrasebased models (Marcu and Wong, 2002; Koehn et al., 2003; Och and Ney, 2004) by making use of syntactic information. Syntax-based MT offers the potential advantages of enforcing syntaxmotivated constraints in translation and capturing long-distance/non-contiguous dependencies. Some approaches have used syntax at the core (Wu, 1997; Alshawi et al., 2000; Yamada and Knight, 2001; Gildea, 2003; Eisner, 2003; Hearne and Way, 2003; Melamed, 2004) while others have integrated syntax into existing phrase-based frameworks (Xia and McCord, 2004; Chiang, 2005; Collins et al., 2005; Quirk et al., 2005). In this work, we employ a syntax-based model that applies a series of tree/string (xRS) rules (Galley et al., 2004; Graehl and Knight, 2004) to a source language string to produce a target language phrase structure tree. Figure 1 exemplifies the translation  process, which is called a derivation, from Chinese into English. The source string to translate ( 1 .) is shown at t"
N06-1031,zhang-etal-2004-interpreting,0,0.00794036,"Missing"
N06-1031,J03-4003,0,\N,Missing
N06-1031,J08-3004,1,\N,Missing
N06-1033,P03-2041,0,0.286386,"juxing le huitan PP(TO(with), NP(NNP(Sharon))) → yu Shalong where the reorderings of nonterminals are denoted by variables xi . Notice that the first rule has a multi-level lefthand side subtree. This system can model nonisomorphic transformations on English parse trees to “fit” another language, for example, learning that 258 the (S (V O)) structure in English should be transformed into a (V S O) structure in Arabic, by looking at two-level tree fragments (Knight and Graehl, 2005). From a synchronous rewriting point of view, this is more akin to synchronous tree substitution grammar (STSG) (Eisner, 2003). This larger locality is linguistically motivated and leads to a better parameter estimation. By imagining the left-hand-side trees as special nonterminals, we can virtually create an SCFG with the same generative capacity. The technical details will be explained in Section 3.2. In general, if we are given an arbitrary synchronous rule with many nonterminals, what are the good decompositions that lead to a binary grammar? Figure 2 suggests that a binarization is good if every virtual nonterminal has contiguous spans on both sides. We formalize this idea in the next section. 2 Synchronous Bina"
N06-1033,N04-1035,1,0.655736,"re-orderings by binarizing synchronous rules when possible and show that the resulting rule set significantly improves the speed and accuracy of a state-of-the-art syntax-based machine translation system. 1 • We examine the effect of this binarization method on end-to-end machine translation quality, compared to a more typical baseline method. • We examine cases of non-binarizable rules in a large, empirically-derived rule set, and we investigate the effect on translation quality when excluding such rules. Introduction Several recent syntax-based models for machine translation (Chiang, 2005; Galley et al., 2004) can be seen as instances of the general framework of synchronous grammars and tree transducers. In this framework, both alignment (synchronous parsing) and decoding can be thought of as parsing problems, whose complexity is in general exponential in the number of nonterminals on the right hand side of a grammar rule. To alleviate this problem, we investigate bilingual binarization to factor the synchronous grammar to a smaller branching factor, although it is not guaranteed to be successful for any synchronous rule with arbitrary permutation. In particular: Melamed (2003) discusses binarizati"
N06-1033,W05-1507,1,0.837659,"e n nonterminals in a SCFG rule, and the decoder conservatively assumes nothing can be done on language model scoring (because target-language spans are non-contiguous in general) until the real nonterminal has been recognized. In other words, targetlanguage boundary words from each child nonterminal of the rule will be cached in all virtual nonterminals derived from this rule. In the case of m-gram integrated decoding, we have to maintain 2(m − 1) boundary words for each child nonterminal, which leads to a prohibitive overall complexity of O(|w|3+2n(m−1) ), which is exponential in rule size (Huang et al., 2005). Aggressive pruning must be used to make it tractable in practice, which in general introduces many search errors and adversely affects translation quality. In the second case, however:  with ··· Sharon 2  PP held 2 4 ··· VPP-VP  :r Sharon 7   held ··· meeting 4 VP 7  :s : rs · Pr(with |meeting) Here since PP and VP are contiguous (but swapped) in the target-language, we can include the source (Chinese) NP NP PP VP VP PP target (English) English boundary words VPP-VP VPP-VP Sharon PP with meeting held Powell Powell VP NP 1 2 4 7 Chinese indices Figure 2: The alignment pattern (left) and"
N06-1033,N03-1021,0,0.0662649,"(Chiang, 2005; Galley et al., 2004) can be seen as instances of the general framework of synchronous grammars and tree transducers. In this framework, both alignment (synchronous parsing) and decoding can be thought of as parsing problems, whose complexity is in general exponential in the number of nonterminals on the right hand side of a grammar rule. To alleviate this problem, we investigate bilingual binarization to factor the synchronous grammar to a smaller branching factor, although it is not guaranteed to be successful for any synchronous rule with arbitrary permutation. In particular: Melamed (2003) discusses binarization of multitext grammars on a theoretical level, showing the importance and difficulty of binarization for efficient synchronous parsing. One way around this difficulty is to stipulate that all rules must be binary from the outset, as in inversion-transduction grammar (ITG) (Wu, 1997) and the binary synchronous context-free grammar (SCFG) employed by the Hiero system (Chiang, 2005) to model the hierarchical phrases. In contrast, the rule extraction method of Galley et al. (2004) aims to incorporate more syntactic information by providing parse trees for the target language"
N06-1033,J04-4002,0,0.236624,"Missing"
N06-1033,H05-1101,0,0.681458,"VPP-VP → NP(1) VP(1) (2) , NP(1) VPP-VP VPP-VP (2) (2) PP , PP VP(1) (2) In this case m-gram integrated decoding can be done in O(|w|3+4(m−1) ) time which is much lowerorder polynomial and no longer depends on rule size (Wu, 1996), allowing the search to be much faster and more accurate facing pruning, as is evidenced in the Hiero system of Chiang (2005) where he restricts the hierarchical phrases to be a binary SCFG. The benefit of binary grammars also lies in synchronous parsing (alignment). Wu (1997) shows that parsing a binary SCFG is in O(|w|6 ) while parsing SCFG is NP-hard in general (Satta and Peserico, 2005). The same reasoning applies to tree transducer rules. Suppose we have the following tree-to-string rules, following Galley et al. (2004): (3) S(x0 :NP, VP(x2 :VP, x1 :PP)) → x0 x1 x2 NP(NNP(Powell)) → Baoweier VP(VBD(held), NP(DT(a) NPS(meeting))) → juxing le huitan PP(TO(with), NP(NNP(Sharon))) → yu Shalong where the reorderings of nonterminals are denoted by variables xi . Notice that the first rule has a multi-level lefthand side subtree. This system can model nonisomorphic transformations on English parse trees to “fit” another language, for example, learning that 258 the (S (V O)) struct"
N06-1033,C90-3045,0,0.251755,"Missing"
N06-1033,P96-1021,0,0.41387,"s production. language model score by adding Pr(with |meeting), and the resulting item again has two boundary words. Later we add Pr(held | Powell) whenthe Powell ··· Powell resulting item is combined with to NP 1 2 form an S item. As illustrated in Figure 2, VPP-VP has contiguous spans on both source and target sides, so that we can generate a binary-branching SCFG: (2) S→ VPP-VP → NP(1) VP(1) (2) , NP(1) VPP-VP VPP-VP (2) (2) PP , PP VP(1) (2) In this case m-gram integrated decoding can be done in O(|w|3+4(m−1) ) time which is much lowerorder polynomial and no longer depends on rule size (Wu, 1996), allowing the search to be much faster and more accurate facing pruning, as is evidenced in the Hiero system of Chiang (2005) where he restricts the hierarchical phrases to be a binary SCFG. The benefit of binary grammars also lies in synchronous parsing (alignment). Wu (1997) shows that parsing a binary SCFG is in O(|w|6 ) while parsing SCFG is NP-hard in general (Satta and Peserico, 2005). The same reasoning applies to tree transducer rules. Suppose we have the following tree-to-string rules, following Galley et al. (2004): (3) S(x0 :NP, VP(x2 :VP, x1 :PP)) → x0 x1 x2 NP(NNP(Powell)) → Baow"
N06-1033,J97-3002,0,0.851118,"s on the right hand side of a grammar rule. To alleviate this problem, we investigate bilingual binarization to factor the synchronous grammar to a smaller branching factor, although it is not guaranteed to be successful for any synchronous rule with arbitrary permutation. In particular: Melamed (2003) discusses binarization of multitext grammars on a theoretical level, showing the importance and difficulty of binarization for efficient synchronous parsing. One way around this difficulty is to stipulate that all rules must be binary from the outset, as in inversion-transduction grammar (ITG) (Wu, 1997) and the binary synchronous context-free grammar (SCFG) employed by the Hiero system (Chiang, 2005) to model the hierarchical phrases. In contrast, the rule extraction method of Galley et al. (2004) aims to incorporate more syntactic information by providing parse trees for the target language and extracting tree transducer rules that apply to the parses. This approach results in rules with many nonterminals, making good binarization techniques critical. Suppose we have the following SCFG, where superscripts indicate reorderings (formal definitions of 256 Proceedings of the Human Language Tech"
N06-1033,W90-0102,0,\N,Missing
N06-1033,P05-1033,0,\N,Missing
N06-1045,W98-1426,1,\N,Missing
N06-1045,N04-1035,1,\N,Missing
N06-1045,W05-1506,0,\N,Missing
N06-1045,C96-2215,0,\N,Missing
N06-1045,P04-1015,0,\N,Missing
N06-1045,P05-3025,1,\N,Missing
N06-1045,E03-1005,0,\N,Missing
N06-1045,C92-3126,0,\N,Missing
N09-1005,P02-1051,1,0.825471,"Missing"
N09-1005,P08-2014,0,0.0227015,"We frame the transliteration task as a decipherment problem and show that it is possible to learn cross-language phoneme mapping tables using only monolingual resources. We compare various methods and evaluate their accuracies on a standard name transliteration task. 1 • Manually-constructed transliteration models, e.g., (Hermjakob et al., 2008). • Models constructed from bilingual dictionaries of terms and names, e.g., (Knight and Graehl, 1998; Huang et al., 2004; Haizhou et al., 2004; Zelenko and Aone, 2006; Yoon et al., 2007; Li et al., 2007; Karimi et al., 2007; Sherif and Kondrak, 2007b; Goldwasser and Roth, 2008b). Introduction Transliteration refers to the transport of names and terms between languages with different writing systems and phoneme inventories. Recently there has been a large amount of interesting work in this area, and the literature has outgrown being citable in its entirety. Much of this work focuses on backtransliteration, which tries to restore a name or term that has been transported into a foreign language. Here, there is often only one correct target spelling—for example, given jyon.kairu (the name of a U.S. Senator transported to Japanese), we must output “Jon Kyl”, not “John K"
N09-1005,D08-1037,0,0.0315634,"We frame the transliteration task as a decipherment problem and show that it is possible to learn cross-language phoneme mapping tables using only monolingual resources. We compare various methods and evaluate their accuracies on a standard name transliteration task. 1 • Manually-constructed transliteration models, e.g., (Hermjakob et al., 2008). • Models constructed from bilingual dictionaries of terms and names, e.g., (Knight and Graehl, 1998; Huang et al., 2004; Haizhou et al., 2004; Zelenko and Aone, 2006; Yoon et al., 2007; Li et al., 2007; Karimi et al., 2007; Sherif and Kondrak, 2007b; Goldwasser and Roth, 2008b). Introduction Transliteration refers to the transport of names and terms between languages with different writing systems and phoneme inventories. Recently there has been a large amount of interesting work in this area, and the literature has outgrown being citable in its entirety. Much of this work focuses on backtransliteration, which tries to restore a name or term that has been transported into a foreign language. Here, there is often only one correct target spelling—for example, given jyon.kairu (the name of a U.S. Senator transported to Japanese), we must output “Jon Kyl”, not “John K"
N09-1005,P07-1094,0,0.0559287,"Missing"
N09-1005,P04-1021,0,0.0389268,"here we see several techniques in use: We present a method for performing machine transliteration without any parallel resources. We frame the transliteration task as a decipherment problem and show that it is possible to learn cross-language phoneme mapping tables using only monolingual resources. We compare various methods and evaluate their accuracies on a standard name transliteration task. 1 • Manually-constructed transliteration models, e.g., (Hermjakob et al., 2008). • Models constructed from bilingual dictionaries of terms and names, e.g., (Knight and Graehl, 1998; Huang et al., 2004; Haizhou et al., 2004; Zelenko and Aone, 2006; Yoon et al., 2007; Li et al., 2007; Karimi et al., 2007; Sherif and Kondrak, 2007b; Goldwasser and Roth, 2008b). Introduction Transliteration refers to the transport of names and terms between languages with different writing systems and phoneme inventories. Recently there has been a large amount of interesting work in this area, and the literature has outgrown being citable in its entirety. Much of this work focuses on backtransliteration, which tries to restore a name or term that has been transported into a foreign language. Here, there is often only one correct ta"
N09-1005,P08-1045,1,0.864813,"thern California Information Sciences Institute Marina del Rey, California 90292 {sravi,knight}@isi.edu Abstract We explore the third dimension, where we see several techniques in use: We present a method for performing machine transliteration without any parallel resources. We frame the transliteration task as a decipherment problem and show that it is possible to learn cross-language phoneme mapping tables using only monolingual resources. We compare various methods and evaluate their accuracies on a standard name transliteration task. 1 • Manually-constructed transliteration models, e.g., (Hermjakob et al., 2008). • Models constructed from bilingual dictionaries of terms and names, e.g., (Knight and Graehl, 1998; Huang et al., 2004; Haizhou et al., 2004; Zelenko and Aone, 2006; Yoon et al., 2007; Li et al., 2007; Karimi et al., 2007; Sherif and Kondrak, 2007b; Goldwasser and Roth, 2008b). Introduction Transliteration refers to the transport of names and terms between languages with different writing systems and phoneme inventories. Recently there has been a large amount of interesting work in this area, and the literature has outgrown being citable in its entirety. Much of this work focuses on backtra"
N09-1005,N04-1036,0,0.0272817,"e third dimension, where we see several techniques in use: We present a method for performing machine transliteration without any parallel resources. We frame the transliteration task as a decipherment problem and show that it is possible to learn cross-language phoneme mapping tables using only monolingual resources. We compare various methods and evaluate their accuracies on a standard name transliteration task. 1 • Manually-constructed transliteration models, e.g., (Hermjakob et al., 2008). • Models constructed from bilingual dictionaries of terms and names, e.g., (Knight and Graehl, 1998; Huang et al., 2004; Haizhou et al., 2004; Zelenko and Aone, 2006; Yoon et al., 2007; Li et al., 2007; Karimi et al., 2007; Sherif and Kondrak, 2007b; Goldwasser and Roth, 2008b). Introduction Transliteration refers to the transport of names and terms between languages with different writing systems and phoneme inventories. Recently there has been a large amount of interesting work in this area, and the literature has outgrown being citable in its entirety. Much of this work focuses on backtransliteration, which tries to restore a name or term that has been transported into a foreign language. Here, there is oft"
N09-1005,P07-1082,0,0.021348,"transliteration without any parallel resources. We frame the transliteration task as a decipherment problem and show that it is possible to learn cross-language phoneme mapping tables using only monolingual resources. We compare various methods and evaluate their accuracies on a standard name transliteration task. 1 • Manually-constructed transliteration models, e.g., (Hermjakob et al., 2008). • Models constructed from bilingual dictionaries of terms and names, e.g., (Knight and Graehl, 1998; Huang et al., 2004; Haizhou et al., 2004; Zelenko and Aone, 2006; Yoon et al., 2007; Li et al., 2007; Karimi et al., 2007; Sherif and Kondrak, 2007b; Goldwasser and Roth, 2008b). Introduction Transliteration refers to the transport of names and terms between languages with different writing systems and phoneme inventories. Recently there has been a large amount of interesting work in this area, and the literature has outgrown being citable in its entirety. Much of this work focuses on backtransliteration, which tries to restore a name or term that has been transported into a foreign language. Here, there is often only one correct target spelling—for example, given jyon.kairu (the name of a U.S. Senator transport"
N09-1005,W99-0906,1,0.87206,"w EM to run until the P(j) likelihood ratio between subsequent training iterations reaches 0.9999, and we terminate early if 200 iterations are reached. Finally, we decode our test set of U.S. Senator names. Following Knight et al (2006), we stretch out the P(j|e) model probabilities after decipherment training and prior to decoding our test set, by cubing their values. Decipherment under the conditions of transliteration is substantially more difficult than solving letter-substitution ciphers (Knight et al., 2006; Ravi and Knight, 2008; Ravi and Knight, 2009) or phoneme-substitution ciphers (Knight and Yamada, 1999). This is because the target table contains significant non-determinism, and because each symbol has multiple possible fertilities, which introduces uncertainty about the length of the target string. 4.1 Baseline P(e) Model Clearly, we can design P(e) in a number of ways. We might expect that the more the system knows about English, the better it will be able to decipher the Japanese. Our baseline P(e) is a 2-gram phoneme model trained on phoneme sequences from the CMU dictionary. The second row (2a) in Figure 4 shows results when we decipher with this fixed P(e). This approach performs poorly"
N09-1005,P06-2065,1,0.901678,"apply it to the name transliteration task—decoding 100 U.S. Senator names from Japanese to English using the automata shown in Figure 1. For all experiments, we keep the rest of the models in the cascade (WFSA A, WFST B, and WFST D) unchanged. We evaluate on whole-name error-rate (maximum of 100/100) as well as normalized word edit distance, which gives partial credit for getting the first or last name correct. 4 Acquiring Phoneme Mappings from Non-Parallel Data Our main data consists of 9350 unique Japanese phoneme sequences, which we can consider as a single long sequence j. As suggested by Knight et al (2006), we explain the existence of j as the result of someone initially producing a long English phoneme sequence e, according to P(e), then transforming it into j, according to P(j|e). The probability of our observed data P(j) can be written as: P (j) = X e P (e) · P (j|e) We take P(e) to be some fixed model of monolingual English phoneme production, represented as a weighted finite-state acceptor (WFSA). P(j|e) is implemented as the initial, uniformly-weighted WFST C described in Section 2, with 15320 phonemic connections. We next maximize P(j) by manipulating the substitution table P(j|e), aimin"
N09-1005,P06-1142,0,0.0251455,"itution vs. character substitution • heuristic vs. generative vs. discriminative models • manual vs. automatic knowledge acquisition 37 • Extraction of parallel examples from bilingual corpora, using bootstrap dictionaries e.g., (Sherif and Kondrak, 2007a; Goldwasser and Roth, 2008a). • Extraction of parallel examples from comparable corpora, using bootstrap dictionaries, and temporal and word co-occurrence, e.g., (Sproat et al., 2006; Klementiev and Roth, 2008). • Extraction of parallel examples from web queries, using bootstrap dictionaries, e.g., (Nagata et al., 2001; Oh and Isahara, 2006; Kuo et al., 2006; Wu and Chang, 2007). • Comparing terms from different languages in phonetic space, e.g., (Tao et al., 2006; Goldberg and Elhadad, 2008). In this paper, we investigate methods to acquire transliteration mappings from non-parallel sources. We are inspired by previous work in unsupervised learning for natural language, e.g. (Yarowsky, 1995; Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 37–45, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics WFSA - A English word sequence WFST - B ( SPENCER ABRAHAM ) English"
N09-1005,P07-1016,0,0.0259859,"rforming machine transliteration without any parallel resources. We frame the transliteration task as a decipherment problem and show that it is possible to learn cross-language phoneme mapping tables using only monolingual resources. We compare various methods and evaluate their accuracies on a standard name transliteration task. 1 • Manually-constructed transliteration models, e.g., (Hermjakob et al., 2008). • Models constructed from bilingual dictionaries of terms and names, e.g., (Knight and Graehl, 1998; Huang et al., 2004; Haizhou et al., 2004; Zelenko and Aone, 2006; Yoon et al., 2007; Li et al., 2007; Karimi et al., 2007; Sherif and Kondrak, 2007b; Goldwasser and Roth, 2008b). Introduction Transliteration refers to the transport of names and terms between languages with different writing systems and phoneme inventories. Recently there has been a large amount of interesting work in this area, and the literature has outgrown being citable in its entirety. Much of this work focuses on backtransliteration, which tries to restore a name or term that has been transported into a foreign language. Here, there is often only one correct target spelling—for example, given jyon.kairu (the name of a U"
N09-1005,W01-1413,0,0.0263405,"ong a number of dimensions: • phoneme substitution vs. character substitution • heuristic vs. generative vs. discriminative models • manual vs. automatic knowledge acquisition 37 • Extraction of parallel examples from bilingual corpora, using bootstrap dictionaries e.g., (Sherif and Kondrak, 2007a; Goldwasser and Roth, 2008a). • Extraction of parallel examples from comparable corpora, using bootstrap dictionaries, and temporal and word co-occurrence, e.g., (Sproat et al., 2006; Klementiev and Roth, 2008). • Extraction of parallel examples from web queries, using bootstrap dictionaries, e.g., (Nagata et al., 2001; Oh and Isahara, 2006; Kuo et al., 2006; Wu and Chang, 2007). • Comparing terms from different languages in phonetic space, e.g., (Tao et al., 2006; Goldberg and Elhadad, 2008). In this paper, we investigate methods to acquire transliteration mappings from non-parallel sources. We are inspired by previous work in unsupervised learning for natural language, e.g. (Yarowsky, 1995; Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 37–45, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics WFSA - A English word sequen"
N09-1005,D08-1085,1,0.791913,"h an algorithm for EM training of weighted finite-state transducers. 40 We allow EM to run until the P(j) likelihood ratio between subsequent training iterations reaches 0.9999, and we terminate early if 200 iterations are reached. Finally, we decode our test set of U.S. Senator names. Following Knight et al (2006), we stretch out the P(j|e) model probabilities after decipherment training and prior to decoding our test set, by cubing their values. Decipherment under the conditions of transliteration is substantially more difficult than solving letter-substitution ciphers (Knight et al., 2006; Ravi and Knight, 2008; Ravi and Knight, 2009) or phoneme-substitution ciphers (Knight and Yamada, 1999). This is because the target table contains significant non-determinism, and because each symbol has multiple possible fertilities, which introduces uncertainty about the length of the target string. 4.1 Baseline P(e) Model Clearly, we can design P(e) in a number of ways. We might expect that the more the system knows about English, the better it will be able to decipher the Japanese. Our baseline P(e) is a 2-gram phoneme model trained on phoneme sequences from the CMU dictionary. The second row (2a) in Figure 4"
N09-1005,P07-1109,0,0.0192972,"ut any parallel resources. We frame the transliteration task as a decipherment problem and show that it is possible to learn cross-language phoneme mapping tables using only monolingual resources. We compare various methods and evaluate their accuracies on a standard name transliteration task. 1 • Manually-constructed transliteration models, e.g., (Hermjakob et al., 2008). • Models constructed from bilingual dictionaries of terms and names, e.g., (Knight and Graehl, 1998; Huang et al., 2004; Haizhou et al., 2004; Zelenko and Aone, 2006; Yoon et al., 2007; Li et al., 2007; Karimi et al., 2007; Sherif and Kondrak, 2007b; Goldwasser and Roth, 2008b). Introduction Transliteration refers to the transport of names and terms between languages with different writing systems and phoneme inventories. Recently there has been a large amount of interesting work in this area, and the literature has outgrown being citable in its entirety. Much of this work focuses on backtransliteration, which tries to restore a name or term that has been transported into a foreign language. Here, there is often only one correct target spelling—for example, given jyon.kairu (the name of a U.S. Senator transported to Japanese), we must o"
N09-1005,P07-1119,0,0.0361491,"ut any parallel resources. We frame the transliteration task as a decipherment problem and show that it is possible to learn cross-language phoneme mapping tables using only monolingual resources. We compare various methods and evaluate their accuracies on a standard name transliteration task. 1 • Manually-constructed transliteration models, e.g., (Hermjakob et al., 2008). • Models constructed from bilingual dictionaries of terms and names, e.g., (Knight and Graehl, 1998; Huang et al., 2004; Haizhou et al., 2004; Zelenko and Aone, 2006; Yoon et al., 2007; Li et al., 2007; Karimi et al., 2007; Sherif and Kondrak, 2007b; Goldwasser and Roth, 2008b). Introduction Transliteration refers to the transport of names and terms between languages with different writing systems and phoneme inventories. Recently there has been a large amount of interesting work in this area, and the literature has outgrown being citable in its entirety. Much of this work focuses on backtransliteration, which tries to restore a name or term that has been transported into a foreign language. Here, there is often only one correct target spelling—for example, given jyon.kairu (the name of a U.S. Senator transported to Japanese), we must o"
N09-1005,P06-1010,0,0.121479,"Jon Kyl”, not “John Kyre” or any other variation. There are many techniques for transliteration and back-transliteration, and they vary along a number of dimensions: • phoneme substitution vs. character substitution • heuristic vs. generative vs. discriminative models • manual vs. automatic knowledge acquisition 37 • Extraction of parallel examples from bilingual corpora, using bootstrap dictionaries e.g., (Sherif and Kondrak, 2007a; Goldwasser and Roth, 2008a). • Extraction of parallel examples from comparable corpora, using bootstrap dictionaries, and temporal and word co-occurrence, e.g., (Sproat et al., 2006; Klementiev and Roth, 2008). • Extraction of parallel examples from web queries, using bootstrap dictionaries, e.g., (Nagata et al., 2001; Oh and Isahara, 2006; Kuo et al., 2006; Wu and Chang, 2007). • Comparing terms from different languages in phonetic space, e.g., (Tao et al., 2006; Goldberg and Elhadad, 2008). In this paper, we investigate methods to acquire transliteration mappings from non-parallel sources. We are inspired by previous work in unsupervised learning for natural language, e.g. (Yarowsky, 1995; Human Language Technologies: The 2009 Annual Conference of the North American Ch"
N09-1005,W06-1630,0,0.0870334,"ic knowledge acquisition 37 • Extraction of parallel examples from bilingual corpora, using bootstrap dictionaries e.g., (Sherif and Kondrak, 2007a; Goldwasser and Roth, 2008a). • Extraction of parallel examples from comparable corpora, using bootstrap dictionaries, and temporal and word co-occurrence, e.g., (Sproat et al., 2006; Klementiev and Roth, 2008). • Extraction of parallel examples from web queries, using bootstrap dictionaries, e.g., (Nagata et al., 2001; Oh and Isahara, 2006; Kuo et al., 2006; Wu and Chang, 2007). • Comparing terms from different languages in phonetic space, e.g., (Tao et al., 2006; Goldberg and Elhadad, 2008). In this paper, we investigate methods to acquire transliteration mappings from non-parallel sources. We are inspired by previous work in unsupervised learning for natural language, e.g. (Yarowsky, 1995; Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 37–45, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics WFSA - A English word sequence WFST - B ( SPENCER ABRAHAM ) English sound sequence Japanese sound sequence WFST - C ( S P EH N S ER EY B R AH HH AE M ) WFST - D ( S U P E N S A"
N09-1005,D07-1106,0,0.0386338,"Missing"
N09-1005,P95-1026,0,0.165751,"a, using bootstrap dictionaries, and temporal and word co-occurrence, e.g., (Sproat et al., 2006; Klementiev and Roth, 2008). • Extraction of parallel examples from web queries, using bootstrap dictionaries, e.g., (Nagata et al., 2001; Oh and Isahara, 2006; Kuo et al., 2006; Wu and Chang, 2007). • Comparing terms from different languages in phonetic space, e.g., (Tao et al., 2006; Goldberg and Elhadad, 2008). In this paper, we investigate methods to acquire transliteration mappings from non-parallel sources. We are inspired by previous work in unsupervised learning for natural language, e.g. (Yarowsky, 1995; Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 37–45, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics WFSA - A English word sequence WFST - B ( SPENCER ABRAHAM ) English sound sequence Japanese sound sequence WFST - C ( S P EH N S ER EY B R AH HH AE M ) WFST - D ( S U P E N S A A E E B U R A H A M U ) Japanese katakana sequence (スペンサー・エーブラハム) Figure 1: Model used for back-transliteration of Japanese katakana names and terms into English. The model employs a four-stage cascade of weighted finite-state tran"
N09-1005,P07-1015,0,0.041991,"ent a method for performing machine transliteration without any parallel resources. We frame the transliteration task as a decipherment problem and show that it is possible to learn cross-language phoneme mapping tables using only monolingual resources. We compare various methods and evaluate their accuracies on a standard name transliteration task. 1 • Manually-constructed transliteration models, e.g., (Hermjakob et al., 2008). • Models constructed from bilingual dictionaries of terms and names, e.g., (Knight and Graehl, 1998; Huang et al., 2004; Haizhou et al., 2004; Zelenko and Aone, 2006; Yoon et al., 2007; Li et al., 2007; Karimi et al., 2007; Sherif and Kondrak, 2007b; Goldwasser and Roth, 2008b). Introduction Transliteration refers to the transport of names and terms between languages with different writing systems and phoneme inventories. Recently there has been a large amount of interesting work in this area, and the literature has outgrown being citable in its entirety. Much of this work focuses on backtransliteration, which tries to restore a name or term that has been transported into a foreign language. Here, there is often only one correct target spelling—for example, given jyon.kairu"
N09-1005,W06-1672,0,0.0193428,"chniques in use: We present a method for performing machine transliteration without any parallel resources. We frame the transliteration task as a decipherment problem and show that it is possible to learn cross-language phoneme mapping tables using only monolingual resources. We compare various methods and evaluate their accuracies on a standard name transliteration task. 1 • Manually-constructed transliteration models, e.g., (Hermjakob et al., 2008). • Models constructed from bilingual dictionaries of terms and names, e.g., (Knight and Graehl, 1998; Huang et al., 2004; Haizhou et al., 2004; Zelenko and Aone, 2006; Yoon et al., 2007; Li et al., 2007; Karimi et al., 2007; Sherif and Kondrak, 2007b; Goldwasser and Roth, 2008b). Introduction Transliteration refers to the transport of names and terms between languages with different writing systems and phoneme inventories. Recently there has been a large amount of interesting work in this area, and the literature has outgrown being citable in its entirety. Much of this work focuses on backtransliteration, which tries to restore a name or term that has been transported into a foreign language. Here, there is often only one correct target spelling—for exampl"
N09-1005,J98-4003,1,\N,Missing
N09-1025,P08-1024,0,0.207216,"reranking (Huang, 2008). Second, it attempted to incorporate syntax by applying off-the-shelf part-ofspeech taggers and parsers to MT output, a task these tools were never designed for. By contrast, we incorporate features directly into hierarchical and syntaxbased decoders. A third difficulty with Och et al.’s study was that it used MERT, which is not an ideal vehicle for feature exploration because it is observed not to perform well with large feature sets. Others have introduced alternative discriminative training methods (Tillmann and Zhang, 2006; Liang et al., 2006; Turian et al., 2007; Blunsom et al., 2008; Macherey et al., 2008), in which a recurring challenge is scalability: to train many features, we need many trainHuman Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 218–226, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics ing examples, and to train discriminatively, we need to search through all possible translations of each training example. Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset. We follow this approa"
N09-1025,J93-2003,0,0.0195041,"Missing"
N09-1025,P07-1005,1,0.840775,"xtracted rules. (If a rule is observed with more than one set of word alignments, we keep only the most frequent one.) We then define, for each triple ( f, e, f+1 ), a feature that counts the number of times that f is aligned to e and f+1 occurs to the right of f ; and similarly for triples ( f, e, f−1 ) with f−1 occurring to the left of f . In order to limit the size of the model, we restrict words to be among the 100 most frequently occurring words from the training data; all other words are replaced with a token <unk>. These features are somewhat similar to features used by Watanabe et al. (2007), but more in the spirit of features used in the word sense disambiguation model introduced by Lee and Ng (2002) and incorporated as a submodel of a translation system by Chan et al. (2007); here, we are incorporating some of its features directly into the translation model. 5 Experiments For our experiments, we used a 260 million word Chinese/English bitext. We ran GIZA++ on the entire bitext to produce IBM Model 4 word alignments, and then the link deletion algorithm (Fossum et al., 2008) to yield better-quality alignments. For System Hiero Training MERT MIRA Syntax MERT MIRA Features baseli"
N09-1025,D08-1024,1,0.651902,"st a set of hypothesis translations ei1 , . . . , ein , which are the 4.1 10-best translations according to each of: h(e) · w B(e) + h(e) · w (1) −B(e) + h(e) · w • For each i, select an oracle translation: e∗ = arg max (B(e) + h(e) · w) Let ∆hi j = e ∗ h(ei ) (2) − h(ei j ). • For each ei j , compute the loss `i j = B(e∗i ) − B(ei j ) (3) • Update w to the value of w0 that minimizes: m X 1 0 kw − wk2 + C max (`i j − ∆hi j · w0 ) (4) 1≤ j≤n 2 i=1 where C = 0.01. This minimization is performed by a variant of sequential minimal optimization (Platt, 1998). Following Chiang et al. (2008), we calculate the sentence B scores in (1), (2), and (3) in the context of some previous 1-best translations. We run 20 of these learners in parallel, and when training is finished, the weight vectors from all iterations of all learners are averaged together. Since the interface between the trainer and the decoder is fairly simple—for each sentence, the decoder sends the trainer a forest, and the trainer returns a weight update—it is easy to use this algorithm with a variety of CKY-based decoders: here, we are using it in conjunction with both the Hiero decoder and our syntax-based decoder"
N09-1025,P05-1033,1,0.617392,"particularly as it pertains to improving the best systems we have. Further: • Do syntax-based translation systems have unique and effective levers to pull when designing new features? • Can large numbers of feature weights be learned efficiently and stably on modest amounts of data? In this paper, we address these questions by experimenting with a large number of new features. We add more than 250 features to improve a syntaxbased MT system—already the highest-scoring single system in the NIST 2008 Chinese-English common-data track—by +1.1 B. We also add more than 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 B improvement. This research was supported in part by DARPA contract HR0011-06-C-0022 under subcontract to BBN Technologies. ∗ 218 2 Related Work The work of Och et al (2004) is perhaps the bestknown study of new features and their impact on translation quality. However, it had a few shortcomings. First, it used the features for reranking n-best lists of translations, rather than for decoding or forest reranking (Huang, 2008). Second, it attempted to incorporate syntax by applying off-the-shelf part-ofspeech taggers and parsers to MT output, a task these tools were never"
N09-1025,J07-2003,1,0.210507,"string-tostring translation system. Its rules, which are extracted from unparsed, word-aligned parallel text, are synchronous CFG productions, for example: X → X1 de X2 , X2 of X1 As the number of nonterminals is limited to two, the grammar is equivalent to an inversion transduction grammar (Wu, 1997). The baseline model includes 12 features whose weights are optimized using MERT. Two of the features are n-gram language models, which require intersecting the synchronous CFG with finite-state automata representing the language models. This grammar can be parsed efficiently using cube pruning (Chiang, 2007). 3.2 Syntax-based system Our syntax-based system transforms source Chinese strings into target English syntax trees. Following previous work in statistical MT (Brown et al., 1993), we envision a noisy-channel model in which a language model generates English, and then a translation model transforms English trees into Chinese. We represent the translation model as a tree transducer (Knight and Graehl, 2005). It is obtained from bilingual text that has been word-aligned and whose English side has been syntactically parsed. From this data, we use the the GHKM minimal-rule extraction algorithm of"
N09-1025,P97-1003,0,0.373709,"x MERT MIRA Features baseline syntax, distortion syntax, distortion, discount all source-side, discount baseline baseline overlap node count all target-side, discount # 11 56 61 10990 25 25 132 136 283 Tune 35.4 35.9 36.6 38.4 38.6 38.5 38.7 38.7 39.6 Test 36.1 36.9∗ 37.3∗∗ 37.6∗∗ 39.5 39.8∗ 39.9∗ 40.0∗∗ 40.6∗∗ Table 1: Adding new features with MIRA significantly improves translation accuracy. Scores are case-insensitive IBM B scores. ∗ or ∗∗ = significantly better than MERT baseline (p < 0.05 or 0.01, respectively). the syntax-based system, we ran a reimplementation of the Collins parser (Collins, 1997) on the English half of the bitext to produce parse trees, then restructured and relabeled them as described in Section 3.2. Syntax-based rule extraction was performed on a 65 million word subset of the training data. For Hiero, rules with up to two nonterminals were extracted from a 38 million word subset and phrasal rules were extracted from the remainder of the training data. We trained three 5-gram language models: one on the English half of the bitext, used by both systems, one on one billion words of English, used by the syntax-based system, and one on two billion words of English, used"
N09-1025,D07-1079,1,0.641497,"Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 218–226, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics ing examples, and to train discriminatively, we need to search through all possible translations of each training example. Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset. We follow this approach here. 3 minimal rules. These larger rules have been shown to substantially improve translation accuracy (Galley et al., 2006; DeNeefe et al., 2007). We apply Good-Turing discounting to the transducer rule counts and obtain probability estimates: P(rule) = Systems Used 3.1 Hiero Hiero (Chiang, 2005) is a hierarchical, string-tostring translation system. Its rules, which are extracted from unparsed, word-aligned parallel text, are synchronous CFG productions, for example: X → X1 de X2 , X2 of X1 As the number of nonterminals is limited to two, the grammar is equivalent to an inversion transduction grammar (Wu, 1997). The baseline model includes 12 features whose weights are optimized using MERT. Two of the features are n-gram language mode"
N09-1025,W08-0306,1,0.527338,"l other words are replaced with a token <unk>. These features are somewhat similar to features used by Watanabe et al. (2007), but more in the spirit of features used in the word sense disambiguation model introduced by Lee and Ng (2002) and incorporated as a submodel of a translation system by Chan et al. (2007); here, we are incorporating some of its features directly into the translation model. 5 Experiments For our experiments, we used a 260 million word Chinese/English bitext. We ran GIZA++ on the entire bitext to produce IBM Model 4 word alignments, and then the link deletion algorithm (Fossum et al., 2008) to yield better-quality alignments. For System Hiero Training MERT MIRA Syntax MERT MIRA Features baseline syntax, distortion syntax, distortion, discount all source-side, discount baseline baseline overlap node count all target-side, discount # 11 56 61 10990 25 25 132 136 283 Tune 35.4 35.9 36.6 38.4 38.6 38.5 38.7 38.7 39.6 Test 36.1 36.9∗ 37.3∗∗ 37.6∗∗ 39.5 39.8∗ 39.9∗ 40.0∗∗ 40.6∗∗ Table 1: Adding new features with MIRA significantly improves translation accuracy. Scores are case-insensitive IBM B scores. ∗ or ∗∗ = significantly better than MERT baseline (p < 0.05 or 0.01, respectivel"
N09-1025,N04-1035,1,0.501726,"3.2 Syntax-based system Our syntax-based system transforms source Chinese strings into target English syntax trees. Following previous work in statistical MT (Brown et al., 1993), we envision a noisy-channel model in which a language model generates English, and then a translation model transforms English trees into Chinese. We represent the translation model as a tree transducer (Knight and Graehl, 2005). It is obtained from bilingual text that has been word-aligned and whose English side has been syntactically parsed. From this data, we use the the GHKM minimal-rule extraction algorithm of (Galley et al., 2004) to yield rules like: NP-C(x0 :NPB PP(IN(of x1 :NPB)) ↔ x1 de x0 Though this rule can be used in either direction, here we use it right-to-left (Chinese to English). We follow Galley et al. (2006) in allowing unaligned Chinese words to participate in multiple translation rules, and in collecting larger rules composed of 219 count(rule) count(LHS-root(rule)) When we apply these probabilities to derive an English sentence e and a corresponding Chinese sentence c, we wind up with the joint probability P(e, c). The baseline model includes log P(e, c), the two n-gram language models log P(e), and o"
N09-1025,P06-1121,1,0.630882,"y trainHuman Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 218–226, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics ing examples, and to train discriminatively, we need to search through all possible translations of each training example. Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset. We follow this approach here. 3 minimal rules. These larger rules have been shown to substantially improve translation accuracy (Galley et al., 2006; DeNeefe et al., 2007). We apply Good-Turing discounting to the transducer rule counts and obtain probability estimates: P(rule) = Systems Used 3.1 Hiero Hiero (Chiang, 2005) is a hierarchical, string-tostring translation system. Its rules, which are extracted from unparsed, word-aligned parallel text, are synchronous CFG productions, for example: X → X1 de X2 , X2 of X1 As the number of nonterminals is limited to two, the grammar is equivalent to an inversion transduction grammar (Wu, 1997). The baseline model includes 12 features whose weights are optimized using MERT. Two of the features a"
N09-1025,P08-1067,0,0.0419096,"the highest-scoring single system in the NIST 2008 Chinese-English common-data track—by +1.1 B. We also add more than 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 B improvement. This research was supported in part by DARPA contract HR0011-06-C-0022 under subcontract to BBN Technologies. ∗ 218 2 Related Work The work of Och et al (2004) is perhaps the bestknown study of new features and their impact on translation quality. However, it had a few shortcomings. First, it used the features for reranking n-best lists of translations, rather than for decoding or forest reranking (Huang, 2008). Second, it attempted to incorporate syntax by applying off-the-shelf part-ofspeech taggers and parsers to MT output, a task these tools were never designed for. By contrast, we incorporate features directly into hierarchical and syntaxbased decoders. A third difficulty with Och et al.’s study was that it used MERT, which is not an ideal vehicle for feature exploration because it is observed not to perform well with large feature sets. Others have introduced alternative discriminative training methods (Tillmann and Zhang, 2006; Liang et al., 2006; Turian et al., 2007; Blunsom et al., 2008; Ma"
N09-1025,W02-1006,0,0.0271944,"st frequent one.) We then define, for each triple ( f, e, f+1 ), a feature that counts the number of times that f is aligned to e and f+1 occurs to the right of f ; and similarly for triples ( f, e, f−1 ) with f−1 occurring to the left of f . In order to limit the size of the model, we restrict words to be among the 100 most frequently occurring words from the training data; all other words are replaced with a token <unk>. These features are somewhat similar to features used by Watanabe et al. (2007), but more in the spirit of features used in the word sense disambiguation model introduced by Lee and Ng (2002) and incorporated as a submodel of a translation system by Chan et al. (2007); here, we are incorporating some of its features directly into the translation model. 5 Experiments For our experiments, we used a 260 million word Chinese/English bitext. We ran GIZA++ on the entire bitext to produce IBM Model 4 word alignments, and then the link deletion algorithm (Fossum et al., 2008) to yield better-quality alignments. For System Hiero Training MERT MIRA Syntax MERT MIRA Features baseline syntax, distortion syntax, distortion, discount all source-side, discount baseline baseline overlap node coun"
N09-1025,P06-1096,0,0.752695,"Missing"
N09-1025,D08-1076,0,0.134772,"8). Second, it attempted to incorporate syntax by applying off-the-shelf part-ofspeech taggers and parsers to MT output, a task these tools were never designed for. By contrast, we incorporate features directly into hierarchical and syntaxbased decoders. A third difficulty with Och et al.’s study was that it used MERT, which is not an ideal vehicle for feature exploration because it is observed not to perform well with large feature sets. Others have introduced alternative discriminative training methods (Tillmann and Zhang, 2006; Liang et al., 2006; Turian et al., 2007; Blunsom et al., 2008; Macherey et al., 2008), in which a recurring challenge is scalability: to train many features, we need many trainHuman Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 218–226, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics ing examples, and to train discriminatively, we need to search through all possible translations of each training example. Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset. We follow this approach here. 3 minimal rules"
N09-1025,P08-1114,0,0.131782,"on rules, e.g., insert-the and insert-is. There are 35 such features. 4.2 Source-side features We now turn to features that make use of source-side context. Although these features capture dependencies that cross boundaries between rules, they are still local in the sense that no new states need to be added to the decoder. This is because the entire source sentence, being fixed, is always available to every feature. 221 Soft syntactic constraints Neither of our systems uses source-side syntactic information; hence, both could potentially benefit from soft syntactic constraints as described by Marton and Resnik (2008). In brief, these features use the output of an independent syntactic parser on the source sentence, rewarding decoder constituents that match syntactic constituents and punishing decoder constituents that cross syntactic constituents. We use separatelytunable features for each syntactic category. Structural distortion features Both of our systems have rules with variables that generalize over possible fillers, but neither system’s basic model conditions a rule application on the size of a filler, making it difficult to distinguish long-distance reorderings from short-distance reorderings. To"
N09-1025,P02-1038,0,0.52616,"6). Then we use a CKY-style parser (Yamada and Knight, 2002; Galley et al., 2006) with cube pruning to decode new sentences. We include two other techniques in our baseline. To get more general translation rules, we restructure our English training trees using expectationmaximization (Wang et al., 2007), and to get more specific translation rules, we relabel the trees with up to 4 specialized versions of each nonterminal symbol, again using expectation-maximization and the split/merge technique of Petrov et al. (2006). 3.3 MIRA training We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008). Let e stand for output strings or their derivations, and let h(e) stand for the feature vector for e. Initialize the feature weights w. Then, repeatedly: • Select a batch of input sentences f1 , . . . , fm and decode each fi to obtain a forest of translations. • For each i, select from the forest a set of hypothesis translations ei1 , . . . , ein , which are the 4.1 10-best translations according to each of: h(e) · w B(e) + h(e) · w (1) −B(e) + h(e) · w • For each i, s"
N09-1025,P03-1021,0,0.12973,",001 New Features for Statistical Machine Translation∗ David Chiang and Kevin Knight USC Information Sciences Institute 4676 Admiralty Way, Suite 1001 Marina del Rey, CA 90292 USA Abstract Many of the new features use syntactic information, and in particular depend on information that is available only inside a syntax-based translation model. Thus they widen the advantage that syntaxbased models have over other types of models. The models are trained using the Margin Infused Relaxed Algorithm or MIRA (Crammer et al., 2006) instead of the standard minimum-error-rate training or MERT algorithm (Och, 2003). Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks. We use the Margin Infused Relaxed Algorithm of Crammer et al. to add a large number of new features to two machine translation systems: the Hiero hierarchical phrasebased translation system and our syntax-based translation system. On a large-scale ChineseEnglish translation task, we obtain statistically significant improvements of +1.5 B and +1.1 B, respectively. We analyze the impact of the new feature"
N09-1025,P06-1055,0,0.128993,"in at most two variables and can be incrementally scored by the language model (Zhang et al., 2006). Then we use a CKY-style parser (Yamada and Knight, 2002; Galley et al., 2006) with cube pruning to decode new sentences. We include two other techniques in our baseline. To get more general translation rules, we restructure our English training trees using expectationmaximization (Wang et al., 2007), and to get more specific translation rules, we relabel the trees with up to 4 specialized versions of each nonterminal symbol, again using expectation-maximization and the split/merge technique of Petrov et al. (2006). 3.3 MIRA training We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008). Let e stand for output strings or their derivations, and let h(e) stand for the feature vector for e. Initialize the feature weights w. Then, repeatedly: • Select a batch of input sentences f1 , . . . , fm and decode each fi to obtain a forest of translations. • For each i, select from the forest a set of hypothesis translations ei1 , . . . , ein , which are the 4.1 10-best transla"
N09-1025,P07-1065,0,0.0152988,"Missing"
N09-1025,P06-1091,0,0.161178,"ng n-best lists of translations, rather than for decoding or forest reranking (Huang, 2008). Second, it attempted to incorporate syntax by applying off-the-shelf part-ofspeech taggers and parsers to MT output, a task these tools were never designed for. By contrast, we incorporate features directly into hierarchical and syntaxbased decoders. A third difficulty with Och et al.’s study was that it used MERT, which is not an ideal vehicle for feature exploration because it is observed not to perform well with large feature sets. Others have introduced alternative discriminative training methods (Tillmann and Zhang, 2006; Liang et al., 2006; Turian et al., 2007; Blunsom et al., 2008; Macherey et al., 2008), in which a recurring challenge is scalability: to train many features, we need many trainHuman Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 218–226, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics ing examples, and to train discriminatively, we need to search through all possible translations of each training example. Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as"
N09-1025,D07-1078,1,0.613116,"content words. All features are linearly combined and their weights are optimized using MERT. For efficient decoding with integrated n-gram language models, all transducer rules must be binarized into rules that contain at most two variables and can be incrementally scored by the language model (Zhang et al., 2006). Then we use a CKY-style parser (Yamada and Knight, 2002; Galley et al., 2006) with cube pruning to decode new sentences. We include two other techniques in our baseline. To get more general translation rules, we restructure our English training trees using expectationmaximization (Wang et al., 2007), and to get more specific translation rules, we relabel the trees with up to 4 specialized versions of each nonterminal symbol, again using expectation-maximization and the split/merge technique of Petrov et al. (2006). 3.3 MIRA training We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008). Let e stand for output strings or their derivations, and let h(e) stand for the feature vector for e. Initialize the feature weights w. Then, repeatedly: • Select a"
N09-1025,D07-1080,0,0.64867,"d Chiang and Kevin Knight USC Information Sciences Institute 4676 Admiralty Way, Suite 1001 Marina del Rey, CA 90292 USA Abstract Many of the new features use syntactic information, and in particular depend on information that is available only inside a syntax-based translation model. Thus they widen the advantage that syntaxbased models have over other types of models. The models are trained using the Margin Infused Relaxed Algorithm or MIRA (Crammer et al., 2006) instead of the standard minimum-error-rate training or MERT algorithm (Och, 2003). Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks. We use the Margin Infused Relaxed Algorithm of Crammer et al. to add a large number of new features to two machine translation systems: the Hiero hierarchical phrasebased translation system and our syntax-based translation system. On a large-scale ChineseEnglish translation task, we obtain statistically significant improvements of +1.5 B and +1.1 B, respectively. We analyze the impact of the new features and the performance of the learning algorithm. 1 Wei Wang Language W"
N09-1025,J97-3002,0,0.217863,"mal rules. These larger rules have been shown to substantially improve translation accuracy (Galley et al., 2006; DeNeefe et al., 2007). We apply Good-Turing discounting to the transducer rule counts and obtain probability estimates: P(rule) = Systems Used 3.1 Hiero Hiero (Chiang, 2005) is a hierarchical, string-tostring translation system. Its rules, which are extracted from unparsed, word-aligned parallel text, are synchronous CFG productions, for example: X → X1 de X2 , X2 of X1 As the number of nonterminals is limited to two, the grammar is equivalent to an inversion transduction grammar (Wu, 1997). The baseline model includes 12 features whose weights are optimized using MERT. Two of the features are n-gram language models, which require intersecting the synchronous CFG with finite-state automata representing the language models. This grammar can be parsed efficiently using cube pruning (Chiang, 2007). 3.2 Syntax-based system Our syntax-based system transforms source Chinese strings into target English syntax trees. Following previous work in statistical MT (Brown et al., 1993), we envision a noisy-channel model in which a language model generates English, and then a translation model"
N09-1025,P02-1039,1,0.475396,"y P(e, c). The baseline model includes log P(e, c), the two n-gram language models log P(e), and other features for a total of 25. For example, there is a pair of features to punish rules that drop Chinese content words or introduce spurious English content words. All features are linearly combined and their weights are optimized using MERT. For efficient decoding with integrated n-gram language models, all transducer rules must be binarized into rules that contain at most two variables and can be incrementally scored by the language model (Zhang et al., 2006). Then we use a CKY-style parser (Yamada and Knight, 2002; Galley et al., 2006) with cube pruning to decode new sentences. We include two other techniques in our baseline. To get more general translation rules, we restructure our English training trees using expectationmaximization (Wang et al., 2007), and to get more specific translation rules, we relabel the trees with up to 4 specialized versions of each nonterminal symbol, again using expectation-maximization and the split/merge technique of Petrov et al. (2006). 3.3 MIRA training We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al"
N09-1025,N06-1033,1,0.759347,"nese sentence c, we wind up with the joint probability P(e, c). The baseline model includes log P(e, c), the two n-gram language models log P(e), and other features for a total of 25. For example, there is a pair of features to punish rules that drop Chinese content words or introduce spurious English content words. All features are linearly combined and their weights are optimized using MERT. For efficient decoding with integrated n-gram language models, all transducer rules must be binarized into rules that contain at most two variables and can be incrementally scored by the language model (Zhang et al., 2006). Then we use a CKY-style parser (Yamada and Knight, 2002; Galley et al., 2006) with cube pruning to decode new sentences. We include two other techniques in our baseline. To get more general translation rules, we restructure our English training trees using expectationmaximization (Wang et al., 2007), and to get more specific translation rules, we relabel the trees with up to 4 specialized versions of each nonterminal symbol, again using expectation-maximization and the split/merge technique of Petrov et al. (2006). 3.3 MIRA training We incorporate all our new features into a linear model (Oc"
N09-1025,N04-1021,0,\N,Missing
N09-2036,J07-2003,0,0.813388,"ts involve heavy parameter tuning, which involves heavy decoding. In this paper, we present a new method to improve decoding performance, obtaining a significant speedup over a strong baseline with no loss in Bleu. In scenarios where fast decoding is more important than optimal Bleu, we obtain better Bleu for the same time investment. Our baseline is a full-scale syntax-based MT system with 245m tree-transducer rules of the kind described in (Galley et al., 2004), 192 English non-terminal symbols, an integrated 5-gram language model (LM), and a decoder that uses state-of-the-art cube pruning (Chiang, 2007). A sample translation rule is: S(x0:NP x1:VP) ↔ x1:VP x0:NP In CKY string-to-tree decoding, we attack spans of the input string from shortest to longest. We populate each span with a set of edges. An edge contains a English non-terminal (NT) symbol (NP, VP, etc), border words for LM combination, pointers to child edges, and a score. The score is a sum of (1) the left-child edge score, (2) the right-child edge score, (3) the score of the translation rule that combined them, and (4) the target-string LM score. In this paper, we are only concerned with what happens when constructing edges for a"
N09-2036,N04-1035,1,0.805303,"MT systems have proven effective—the models are compelling and show good room for improvement. However, slow decoding hinders research, as most experiments involve heavy parameter tuning, which involves heavy decoding. In this paper, we present a new method to improve decoding performance, obtaining a significant speedup over a strong baseline with no loss in Bleu. In scenarios where fast decoding is more important than optimal Bleu, we obtain better Bleu for the same time investment. Our baseline is a full-scale syntax-based MT system with 245m tree-transducer rules of the kind described in (Galley et al., 2004), 192 English non-terminal symbols, an integrated 5-gram language model (LM), and a decoder that uses state-of-the-art cube pruning (Chiang, 2007). A sample translation rule is: S(x0:NP x1:VP) ↔ x1:VP x0:NP In CKY string-to-tree decoding, we attack spans of the input string from shortest to longest. We populate each span with a set of edges. An edge contains a English non-terminal (NT) symbol (NP, VP, etc), border words for LM combination, pointers to child edges, and a score. The score is a sum of (1) the left-child edge score, (2) the right-child edge score, (3) the score of the translation"
N09-2036,W05-1506,0,0.139659,"Missing"
N09-2036,P07-1019,0,0.11966,"Figure 2 shows how our lists are organized. The quality of our 1000-best edges can be improved. When we organize the higher-level lists by left edge-sets, we give prominence to the best left edge-set (eg, NP) over others (eg, VP). If the left span is relatively short, the contribution of the left NP to the total score of the new edge is small, so this prominence is misplaced. Therefore, we repeat the above process with the higher-level lists organized by right span instead of left. We merge the right-oriented and left-oriented structures, making sure that duplicates are avoided. Related Work. Huang and Chiang (2007) de45000 53 52.8 lazy cube generation exhaustive cube generation 52.6 52.4 bleu model cost 44000 43000 52.2 lazy cube generation exhaustive cube generation 52 51.8 42000 51.6 51.4 51.2 5x108 1x109 1.5x109 2x109 2.5x109 20000 3x109 40000 60000 80000 decode time (seconds) edges created Figure 3: Number of edges produced by the decoder, versus model cost of 1-best decodings. scribe a variation of cube pruning called cube growing, and they apply it to a source-tree to targetstring translator. It is a two pass approach, where a context-free parser is used to build a source forest, and a top down la"
N09-2064,P05-1022,0,0.533885,"Missing"
N09-2064,J05-1003,0,0.129041,"Missing"
N09-2064,P03-1054,0,0.071411,"Missing"
N09-2064,N07-1051,0,0.287213,"Missing"
N09-2064,W06-1608,0,0.0271767,"and Brill, 1999; Sagae and Lavie, 2006) gives a significant improvement in f-score, it tends to flatten the structure of the individual parses. To illustrate, Figures 1 and 2 contrast the output of the Charniak parser with the output of constituent recombination on a sentence from WSJ section 24. We recombine context-free productions instead of constituents, producing trees containing only context-free productions that have been seen in the individual parsers’ output (Figure 3). Introduction Parse quality impacts the quality of downstream applications such as syntax-based machine translation (Quirk and Corston-Oliver, 2006). Combining the output of multiple parsers can boost the accuracy of such applications. Parses can be combined in two ways: parse selection (selecting the best parse from the output of the individual parsers) or parse hybridization (constructing the best parse by recombining sub-sentential components from the output of the individual parsers). Second, the parse selection method of (Henderson and Brill, 1999) selects the parse with maximum expected precision; here, we present an efficient, linear-time algorithm for selecting the parse with maximum expected f-score within the Minimum Bayes Risk"
N09-2064,N06-2033,0,0.360157,"u Kevin Knight Information Sciences Institute University of Southern California Marina del Rey, CA 90292 knight@isi.edu Abstract 1.1 Related Work (Henderson and Brill, 1999) perform parse selection by maximizing the expected precision of the selected parse with respect to the set of parses being combined. (Henderson and Brill, 1999) and (Sagae and Lavie, 2006) propose methods for parse hybridization by recombining constituents. Combining the 1-best output of multiple parsers via parse selection or parse hybridization improves f-score over the best individual parser (Henderson and Brill, 1999; Sagae and Lavie, 2006). We propose three ways to improve upon existing methods for parser combination. First, we propose a method of parse hybridization that recombines context-free productions instead of constituents, thereby preserving the structure of the output of the individual parsers to a greater extent. Second, we propose an efficient lineartime algorithm for computing expected f-score using Minimum Bayes Risk parse selection. Third, we extend these parser combination methods from multiple 1-best outputs to multiple n-best outputs. We present results on WSJ section 23 and also on the English side of a Chine"
N09-2064,W99-0623,0,\N,Missing
N10-1014,J93-2003,0,0.0703269,"the sequence fl . The parameter pl is a left length distribution. The probabilities pmid , pright , decompose in the same way, except substituting a separate length distribution pm and pr for pl . For the T ERMINAL rule, we emit ft with a similarly decomposed distribution pterm using length distribution pw . We define the probability of generating a foreign word fj as p(fj |A, eA ) = X i∈eA 1 pt (fj |ei ) |eA | with i ∈ eA denoting an index ranging over the indices of the English words contained in eA . The reader may recognize the above expressions as the probability assigned by IBM Model 1 (Brown et al., 1993) of generating the words fl given the words eA , with one important difference – the length m of the foreign sentence is often not modeled, so the term pl (|fl |= m |A) is set to a constant and ignored. Parameterizing this length allows our model to effectively control the number of words produced at different levels of the derivation. It is worth noting how each parameter affects the model’s behavior. The pt distribution is a standard “translation” table, familiar from the IBM Models. The pinv distribution is a “distortion” parameter, and models the likelihood of inverting non-terminals B and"
N10-1014,N10-1015,1,0.831836,"sis is “correct”, but “good enough” without resorting to more computationally complicated models. In general, our model follows an “extract as much as possible” approach. We hypothesize that this approach will capture important syntactic generalizations, but it also risks including low-quality rules. It is an empirical question whether this approach is effective, and we investigate this issue further in Section 5.3. There are possibilities for improving our model’s treatment of syntactic divergence. One option is to allow the model to select trees which are more consistent with the alignment (Burkett et al., 2010), which our model can do since it permits efficient inference over forests. The second is to modify the generative process slightly, perhaps by including the “clone” operator of Gildea (2003). 123 Parameter Estimation The parameters of our model can be efficiently estimated in an unsupervised fashion using the Expectation-Maximization (EM) algorithm. The Estep requires the computation of expected counts under our model for each multinomial parameter. We omit the details of obtaining expected counts for each distribution, since they can be obtained using simple arithmetic from a single quantity"
N10-1014,P06-2014,0,0.621519,"e and target languages through word alignment patterns, sometimes in combination with constraints from parser outputs. However, word alignments are not perfect indicators of syntactic alignment, and syntactic systems are very sensitive to word alignment behavior. Even a single spurious word alignment can invalidate a large number of otherwise extractable rules, while unaligned words can result in an exponentially large set of extractable rules to choose from. Researchers have worked to incorporate syntactic information into word alignments, resulting in improvements to both alignment quality (Cherry and Lin, 2006; DeNero and Klein, 2007), and translation quality (May and Knight, 2007; Fossum et al., 2008). In this paper, we remove the dependence on word alignments and instead directly model the syntactic correspondences in the data, in a manner broadly similar to Yamada and Knight (2001). In particular, we propose an unsupervised model that aligns nodes of a parse tree (or forest) in one language to spans of a sentence in another. Our model is an instance of the inversion transduction grammar (ITG) formalism (Wu, 1997), constrained in such a way that one side of the synchronous derivation respects a s"
N10-1014,W07-0403,0,0.0769298,"ngle n-ary English tree obtained from a monolingual parser. However, it is worth noting that the English side of the ITG derivation is not completely fixed. Where our English trees are more than binary branching, we permit any binarization in our dynamic program. For efficiency, we also ruled out span alignments that are extremely lopsided, for example, a 1-word English span aligned to a 20-word foreign span. Specifically, we pruned any span alignment in which one side is more than 5 times larger than the other. Finally, we employ pruning based on highprecision alignments from simpler models (Cherry and Lin, 2007; Haghighi et al., 2009). We compute word-to-word alignments by finding all word pairs which have a posterior of at least 0.7 according to both forward and reverse IBM Model 1 parameters, and prune any span pairs which invalidate more than 3 of these alignments. In total, this pruning reSpan Syntactic Alignment GIZA++ Rule Syntactic Alignment GIZA++ P 50.9 56.1 P 39.6 46.2 R 83.0 67.3 R 40.3 34.7 F1 63.1 61.2 F1 39.9 39.6 Table 3: Alignment quality results for our syntactic aligner and our GIZA++ baseline. duced computation from approximately 1.5 seconds per sentence to about 0.3 seconds per s"
N10-1014,P05-1033,1,0.713885,"e being robust in the face of syntactic divergence. Alignment quality and end-to-end translation experiments demonstrate that this consistency yields higher quality alignments than our baseline. 1 Introduction Syntactic machine translation has advanced significantly in recent years, and multiple variants currently achieve state-of-the-art translation quality. Many of these systems exploit linguistically-derived syntactic information either on the target side (Galley et al., 2006), the source side (Huang et al., 2006), or both (Liu et al., 2009). Still others induce their syntax from the data (Chiang, 2005). Despite differences in detail, the vast majority of syntactic methods share a critical dependence on word alignments. In particular, they infer syntactic correspondences between the source and target languages through word alignment patterns, sometimes in combination with constraints from parser outputs. However, word alignments are not perfect indicators of syntactic alignment, and syntactic systems are very sensitive to word alignment behavior. Even a single spurious word alignment can invalidate a large number of otherwise extractable rules, while unaligned words can result in an exponent"
N10-1014,D09-1037,0,0.0675543,"s permits the extraction of the general rule (PP → IN1 NP2 ; 在 NP2 IN1 ), and the more lexicalized (PP → before NP ; 在 NP 之前). 3.1 Parameterization In principle, our model could have one parameter for each instantiation r of a rule type. This model would have an unmanageable number of parameters, producing both computational and modeling issues – it is well known that unsupervised models with large numbers of parameters are prone to degenerate analyses of the data (DeNero et al., 2006). One solution might be to apply an informed prior with a computationally tractable inference procedure (e.g. Cohn and Blunsom (2009) or Liu and Gildea (2009)). We opt here for the simpler, statistically more robust solution of making independence assumptions to keep the number of parameters at a reasonable level. Concretely, we define the probability of the B I NARY M ONO rule,4 p(r = A → B C; fl B fm C fr |A, eA ) which conditions on the root of the rule A and the English yield eA , as pg (A → B C |A, eA ) · pinv (I |B, C)· plef t (fl |A, eA )·pmid (fm |A, eA )·pright (fr |A, eA ) In words, we assume that the rule probability decomposes into a monolingual PCFG grammar probability pg , an inversion probability pinv , and a"
N10-1014,P07-1003,1,0.954134,"through word alignment patterns, sometimes in combination with constraints from parser outputs. However, word alignments are not perfect indicators of syntactic alignment, and syntactic systems are very sensitive to word alignment behavior. Even a single spurious word alignment can invalidate a large number of otherwise extractable rules, while unaligned words can result in an exponentially large set of extractable rules to choose from. Researchers have worked to incorporate syntactic information into word alignments, resulting in improvements to both alignment quality (Cherry and Lin, 2006; DeNero and Klein, 2007), and translation quality (May and Knight, 2007; Fossum et al., 2008). In this paper, we remove the dependence on word alignments and instead directly model the syntactic correspondences in the data, in a manner broadly similar to Yamada and Knight (2001). In particular, we propose an unsupervised model that aligns nodes of a parse tree (or forest) in one language to spans of a sentence in another. Our model is an instance of the inversion transduction grammar (ITG) formalism (Wu, 1997), constrained in such a way that one side of the synchronous derivation respects a syntactic parse. Our model"
N10-1014,W06-3105,1,0.843418,"ctly aligns to before, while 在 functions as a generic preposition, which our model handles by attaching it to the PP. This analysis permits the extraction of the general rule (PP → IN1 NP2 ; 在 NP2 IN1 ), and the more lexicalized (PP → before NP ; 在 NP 之前). 3.1 Parameterization In principle, our model could have one parameter for each instantiation r of a rule type. This model would have an unmanageable number of parameters, producing both computational and modeling issues – it is well known that unsupervised models with large numbers of parameters are prone to degenerate analyses of the data (DeNero et al., 2006). One solution might be to apply an informed prior with a computationally tractable inference procedure (e.g. Cohn and Blunsom (2009) or Liu and Gildea (2009)). We opt here for the simpler, statistically more robust solution of making independence assumptions to keep the number of parameters at a reasonable level. Concretely, we define the probability of the B I NARY M ONO rule,4 p(r = A → B C; fl B fm C fr |A, eA ) which conditions on the root of the rule A and the English yield eA , as pg (A → B C |A, eA ) · pinv (I |B, C)· plef t (fl |A, eA )·pmid (fm |A, eA )·pright (fr |A, eA ) In words,"
N10-1014,N09-1026,1,0.844954,"ic system of Galley et al. (2006). For the translation rules extracted from our data, we computed standard features based on relative frequency counts, and tuned their weights using MERT (Och, 2003). We also included a language model feature, using a 5-gram language model trained on 220 million words of English text using the SRILM Toolkit (Stolcke, 2002). For tuning and test data, we used a subset of the NIST MT04 and MT05 with sentences of length at most 40. We used the first 1000 sentences of this set for tuning and the remaining 642 sentences as test data. We used the decoder described in DeNero et al. (2009) during both tuning and testing. We provide final tune and test set results in Table 4. Our alignments produce a 1.0 BLEU improvement over the baseline. Our reported syntactic results were obtained when rules were thresholded by count; we discuss this in the next section. 5.3 Analysis As discussed in Section 3.4, our aligner is designed to extract many rules, which risks inadvertently extracting low-quality rules. To quantify this, we 8 http://code.google.com/p/berkeleyparser/ 9 first examined the number of rules extracted by our 5 iterations of model 1, 5 iterations of HMM, 3 iterations align"
N10-1014,W08-0306,1,0.944423,"nts from parser outputs. However, word alignments are not perfect indicators of syntactic alignment, and syntactic systems are very sensitive to word alignment behavior. Even a single spurious word alignment can invalidate a large number of otherwise extractable rules, while unaligned words can result in an exponentially large set of extractable rules to choose from. Researchers have worked to incorporate syntactic information into word alignments, resulting in improvements to both alignment quality (Cherry and Lin, 2006; DeNero and Klein, 2007), and translation quality (May and Knight, 2007; Fossum et al., 2008). In this paper, we remove the dependence on word alignments and instead directly model the syntactic correspondences in the data, in a manner broadly similar to Yamada and Knight (2001). In particular, we propose an unsupervised model that aligns nodes of a parse tree (or forest) in one language to spans of a sentence in another. Our model is an instance of the inversion transduction grammar (ITG) formalism (Wu, 1997), constrained in such a way that one side of the synchronous derivation respects a syntactic parse. Our model is best suited to systems which use source- or target-side trees onl"
N10-1014,N04-1035,1,0.906585,"RB VBN drastically fallen !易 trade ""差 surplus 大幅度 drastically 少 fall 了 (past) Figure 1: A single incorrect alignment removes an extractable node, and hence several desirable rules. We represent correct extractable nodes in bold, spurious extractable nodes with a *, and incorrectly blocked extractable nodes in bold strikethrough. chief purpose is to align nodes in the syntactic parse in one language to spans in the other – an alignment we will refer to as a “syntactic” alignment. These alignments are employed by standard syntactic rule extraction algorithms, for example, the GHKM algorithm of Galley et al. (2004). Following that work, we will assume parses are present in the target language, though our model applies in either direction. Currently, although syntactic systems make use of syntactic alignments, these alignments must be induced indirectly from word-level alignments. Previous work has discussed at length the poor interaction of word-alignments with syntactic rule extraction (DeNero and Klein, 2007; Fossum et al., 2008). For completeness, we provide a brief example of this interaction, but for a more detailed discussion we refer the reader to these presentations. 2.1 Interaction with Word Al"
N10-1014,P06-1121,1,0.926158,"source sentence to nodes in a target parse tree. We show that our model produces syntactically consistent analyses where possible, while being robust in the face of syntactic divergence. Alignment quality and end-to-end translation experiments demonstrate that this consistency yields higher quality alignments than our baseline. 1 Introduction Syntactic machine translation has advanced significantly in recent years, and multiple variants currently achieve state-of-the-art translation quality. Many of these systems exploit linguistically-derived syntactic information either on the target side (Galley et al., 2006), the source side (Huang et al., 2006), or both (Liu et al., 2009). Still others induce their syntax from the data (Chiang, 2005). Despite differences in detail, the vast majority of syntactic methods share a critical dependence on word alignments. In particular, they infer syntactic correspondences between the source and target languages through word alignment patterns, sometimes in combination with constraints from parser outputs. However, word alignments are not perfect indicators of syntactic alignment, and syntactic systems are very sensitive to word alignment behavior. Even a single spur"
N10-1014,P03-1011,0,0.487993,"use of in the next section. Although several binarizations are possible, we give one such binarization and its associated probabilities in Table 2. 3.4 Robustness to Syntactic Divergence Generally speaking, ITG grammars have proven more useful without the monolingual syntactic constraints imposed by a target parse tree. When derivations are restricted to respect a target-side parse tree, many desirable alignments are ruled out when the syntax of the two languages diverges, and alignment quality drops precipitously (Zhang and Gildea, 2004), though attempts have been made to address this issue (Gildea, 2003). Our model is designed to degrade gracefully in the case of syntactic divergence. Because it can produce foreign words at any level of the derivation, our model can effectively back off to a variant of Model 1 in the case where an ITG derivation that both respects the target parse tree and the desired word-level alignments cannot be found. For example, consider the sentence pair fragment in Figure 3. It is not possible to produce an ITG derivation of this fragment that both respects the English tree and also aligns all foreign words to their obvious English counterparts. Our model handles thi"
N10-1014,P96-1024,0,0.0818471,"er sentence to about 0.3 seconds per sentence, a speed-up of a factor of 5. 4.3 Decoding Given a trained model, we extract a tree-to-string alignment as follows: we compute, for each node in the English tree, the posterior probability of a particular foreign span assignment using the same dynamic program needed for EM. We then compute the set of span assignments which maximizes the sum of these posteriors, constrained such that the foreign span assignments nest in the obvious way. This algorithm is a natural synchronous generalization of the monolingual Maximum Constituents Parse algorithm of Goodman (1996). 5 5.1 Experiments Alignment Quality We first evaluated our alignments against gold standard annotations. Our training data consisted of the 2261 manually aligned and translated sentences of the Chinese Treebank (Bies et al., 2007) and approximately half a million unlabeled sentences of parallel Chinese-English newswire. The unlabeled data was subsampled (Li et al., 2009) from a larger corpus by selecting sentences which have good tune and test set coverage, and limited to sentences of length at most 40. We parsed the English side of the training data with the Berkeley parser.8 For our baseli"
N10-1014,P09-1104,1,0.872906,"e obtained from a monolingual parser. However, it is worth noting that the English side of the ITG derivation is not completely fixed. Where our English trees are more than binary branching, we permit any binarization in our dynamic program. For efficiency, we also ruled out span alignments that are extremely lopsided, for example, a 1-word English span aligned to a 20-word foreign span. Specifically, we pruned any span alignment in which one side is more than 5 times larger than the other. Finally, we employ pruning based on highprecision alignments from simpler models (Cherry and Lin, 2007; Haghighi et al., 2009). We compute word-to-word alignments by finding all word pairs which have a posterior of at least 0.7 according to both forward and reverse IBM Model 1 parameters, and prune any span pairs which invalidate more than 3 of these alignments. In total, this pruning reSpan Syntactic Alignment GIZA++ Rule Syntactic Alignment GIZA++ P 50.9 56.1 P 39.6 46.2 R 83.0 67.3 R 40.3 34.7 F1 63.1 61.2 F1 39.9 39.6 Table 3: Alignment quality results for our syntactic aligner and our GIZA++ baseline. duced computation from approximately 1.5 seconds per sentence to about 0.3 seconds per sentence, a speed-up of a"
N10-1014,W06-3601,1,0.930085,"arse tree. We show that our model produces syntactically consistent analyses where possible, while being robust in the face of syntactic divergence. Alignment quality and end-to-end translation experiments demonstrate that this consistency yields higher quality alignments than our baseline. 1 Introduction Syntactic machine translation has advanced significantly in recent years, and multiple variants currently achieve state-of-the-art translation quality. Many of these systems exploit linguistically-derived syntactic information either on the target side (Galley et al., 2006), the source side (Huang et al., 2006), or both (Liu et al., 2009). Still others induce their syntax from the data (Chiang, 2005). Despite differences in detail, the vast majority of syntactic methods share a critical dependence on word alignments. In particular, they infer syntactic correspondences between the source and target languages through word alignment patterns, sometimes in combination with constraints from parser outputs. However, word alignments are not perfect indicators of syntactic alignment, and syntactic systems are very sensitive to word alignment behavior. Even a single spurious word alignment can invalidate a l"
N10-1014,W09-0424,0,0.0225108,"ximizes the sum of these posteriors, constrained such that the foreign span assignments nest in the obvious way. This algorithm is a natural synchronous generalization of the monolingual Maximum Constituents Parse algorithm of Goodman (1996). 5 5.1 Experiments Alignment Quality We first evaluated our alignments against gold standard annotations. Our training data consisted of the 2261 manually aligned and translated sentences of the Chinese Treebank (Bies et al., 2007) and approximately half a million unlabeled sentences of parallel Chinese-English newswire. The unlabeled data was subsampled (Li et al., 2009) from a larger corpus by selecting sentences which have good tune and test set coverage, and limited to sentences of length at most 40. We parsed the English side of the training data with the Berkeley parser.8 For our baseline alignments, we used GIZA++, trained in the standard way.9 We used the grow-diag-final alignment heuristic, as we found it outperformed union in early experiments. We trained our unsupervised syntactic aligner on the concatenation of the labelled and unlabelled data. As is standard in unsupervised alignment models, we initialized the translation parameters pt by first tr"
N10-1014,N06-1014,1,0.807301,"t set coverage, and limited to sentences of length at most 40. We parsed the English side of the training data with the Berkeley parser.8 For our baseline alignments, we used GIZA++, trained in the standard way.9 We used the grow-diag-final alignment heuristic, as we found it outperformed union in early experiments. We trained our unsupervised syntactic aligner on the concatenation of the labelled and unlabelled data. As is standard in unsupervised alignment models, we initialized the translation parameters pt by first training 5 iterations of IBM Model 1 using the joint training algorithm of Liang et al. (2006), and then trained our model for 5 EM iterations. We extracted syntactic rules using a re-implementation of the Galley et al. (2006) algorithm from both our syntactic alignments and the GIZA++ alignments. We handle null-aligned words by extracting every consistent derivation, and extracted composed rules consisting of at most 3 minimal rules. We evaluate our alignments against the gold standard in two ways. We calculated Span F-score, which compares the set of extractable nodes paired with a foreign span, and Rule F-score (Fossum et al., 2008) over minimal rules. The results are shown in Table"
N10-1014,D09-1136,0,0.010795,"the general rule (PP → IN1 NP2 ; 在 NP2 IN1 ), and the more lexicalized (PP → before NP ; 在 NP 之前). 3.1 Parameterization In principle, our model could have one parameter for each instantiation r of a rule type. This model would have an unmanageable number of parameters, producing both computational and modeling issues – it is well known that unsupervised models with large numbers of parameters are prone to degenerate analyses of the data (DeNero et al., 2006). One solution might be to apply an informed prior with a computationally tractable inference procedure (e.g. Cohn and Blunsom (2009) or Liu and Gildea (2009)). We opt here for the simpler, statistically more robust solution of making independence assumptions to keep the number of parameters at a reasonable level. Concretely, we define the probability of the B I NARY M ONO rule,4 p(r = A → B C; fl B fm C fr |A, eA ) which conditions on the root of the rule A and the English yield eA , as pg (A → B C |A, eA ) · pinv (I |B, C)· plef t (fl |A, eA )·pmid (fm |A, eA )·pright (fr |A, eA ) In words, we assume that the rule probability decomposes into a monolingual PCFG grammar probability pg , an inversion probability pinv , and a probability of left, mid"
N10-1014,P06-1077,0,0.0477303,"r model can be efficiently trained on large parallel corpora. When compared to standard word-alignmentbacked baselines, our model produces more consistent analyses of parallel sentences, leading to high-count, high-quality transfer rules. End-toend translation experiments demonstrate that these higher quality rules improve translation quality by 1.0 BLEU over a word-alignment-backed baseline. 2 Syntactic Rule Extraction Our model is intended for use in syntactic translation models which make use of syntactic parses on either the target (Galley et al., 2006) or source side (Huang et al., 2006; Liu et al., 2006). Our model’s 118 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 118–126, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics S NP VP DT* NN NN VBZ the trade surplus has ADVP RB VBN drastically fallen !易 trade ""差 surplus 大幅度 drastically 少 fall 了 (past) Figure 1: A single incorrect alignment removes an extractable node, and hence several desirable rules. We represent correct extractable nodes in bold, spurious extractable nodes with a *, and incorrectly blocked extractable nodes in bold strikethrough. chie"
N10-1014,P09-1063,0,0.0941769,"Missing"
N10-1014,J93-2004,0,0.0347448,"rminal symbol A. We modify p(fj |eA ) from the previous section to allow production of fj directly from the non-terminal7 A: p(fj |eA ) = pnt · p(fj |A) X 1 + (1 − pnt ) · pt (fj |ei ) |eA | i∈e A where pnt is a global binomial parameter which controls how often such alignments are made. This necessitates the inclusion of parameters like pt ( 的 |NP) into our translation table. Generally, these parameters do not contain much information, but rather function like a traditional N ULL rooted at some position in the tree. However, in some cases, the particular annotation used by the Penn Treebank (Marcus et al., 1993) (and hence most parsers) allows for some interesting parameters to be learned. For example, we found that our aligner often matched the Chinese word 了, which marks the past tense (among other things), to the preterminals VBD and VBN, which denote the English simple past and perfect tense. Additionally, Chinese measure words like 个 and 名 often align to the CD (numeral) preterminal. These generalizations can be quite useful – where a particular number might predict a measure word quite poorly, the generalization that measure words co-occur with the CD tag is very robust. 7 For terminal symbols"
N10-1014,D07-1038,1,0.932126,"bination with constraints from parser outputs. However, word alignments are not perfect indicators of syntactic alignment, and syntactic systems are very sensitive to word alignment behavior. Even a single spurious word alignment can invalidate a large number of otherwise extractable rules, while unaligned words can result in an exponentially large set of extractable rules to choose from. Researchers have worked to incorporate syntactic information into word alignments, resulting in improvements to both alignment quality (Cherry and Lin, 2006; DeNero and Klein, 2007), and translation quality (May and Knight, 2007; Fossum et al., 2008). In this paper, we remove the dependence on word alignments and instead directly model the syntactic correspondences in the data, in a manner broadly similar to Yamada and Knight (2001). In particular, we propose an unsupervised model that aligns nodes of a parse tree (or forest) in one language to spans of a sentence in another. Our model is an instance of the inversion transduction grammar (ITG) formalism (Wu, 1997), constrained in such a way that one side of the synchronous derivation respects a syntactic parse. Our model is best suited to systems which use source- or"
N10-1014,P04-1066,0,0.0389842,"pw parameterizes the number of words produced for a particular English word e, functioning similarly to the “fertilities” employed by IBM Models 3 and 4 (Brown et al., 1993). This allows us to model, for example, the tendency of English determiners the and a translate to nothing in the Chinese, and of English names to align to multiple Chinese words. In general, we expect an English word to usually align to one Chinese word, and so we place a weak Dirichlet prior on on the pe distribution which puts extra mass on 1-length word sequences. This is helpful for avoiding the “garbage collection” (Moore, 2004) problem for rare words. 3.2 Exploiting Non-Terminal Labels There are often foreign words that do not correspond well to any English word, which our model must also handle. We elected for a simple augmentation to our model to account for these words. When generating foreign word sequences f at a non-terminal (i.e. via the U NARY or B INARY productions), we also allow for the production of foreign words from the non-terminal symbol A. We modify p(fj |eA ) from the previous section to allow production of fj directly from the non-terminal7 A: p(fj |eA ) = pnt · p(fj |A) X 1 + (1 − pnt ) · pt (fj"
N10-1014,J03-1002,0,0.0124084,"a span of the foreign sentence which (1) contains every source word that aligns to a target word in the yield of the node and (2) contains no source words that align outside that yield. Only nodes for which a non-empty span satisfying (1) and (2) exists may form the root or leaf of a translation rule; for that reason, we will refer to these nodes as extractable nodes. Since extractable nodes are inferred based on word alignments, spurious word alignments can rule out otherwise desirable extraction points. For exam119 ple, consider the alignment in Figure 1. This alignment, produced by GIZA++ (Och and Ney, 2003), contains 4 correct alignments (the filled circles), but incorrectly aligns the to the Chinese past tense marker 了 (the hollow circle). This mistaken alignment produces the incorrect rule (DT → the ; 了), and also blocks the extraction of (VBN → fallen ; 减少 了). More high-level syntactic transfer rules are also ruled out, for example, the “the insertion rule” (NP → the NN1 NN2 ; NN1 NN2 ) and the high-level (S → NP1 VP2 ; NP1 VP2 ). 3 A Syntactic Alignment Model The most common approach to avoiding these problems is to inject knowledge about syntactic constraints into a word alignment model (Ch"
N10-1014,P03-1021,0,0.0378323,"core, which compares the set of extractable nodes paired with a foreign span, and Rule F-score (Fossum et al., 2008) over minimal rules. The results are shown in Table 3. By both measures, our syntactic aligner effectively trades recall for precision when compared to our baseline, slightly increasing overall F-score. 5.2 Translation Quality For our translation system, we used a reimplementation of the syntactic system of Galley et al. (2006). For the translation rules extracted from our data, we computed standard features based on relative frequency counts, and tuned their weights using MERT (Och, 2003). We also included a language model feature, using a 5-gram language model trained on 220 million words of English text using the SRILM Toolkit (Stolcke, 2002). For tuning and test data, we used a subset of the NIST MT04 and MT05 with sentences of length at most 40. We used the first 1000 sentences of this set for tuning and the remaining 642 sentences as test data. We used the decoder described in DeNero et al. (2009) during both tuning and testing. We provide final tune and test set results in Table 4. Our alignments produce a 1.0 BLEU improvement over the baseline. Our reported syntactic re"
N10-1014,J97-3002,0,0.936111,"to word alignments, resulting in improvements to both alignment quality (Cherry and Lin, 2006; DeNero and Klein, 2007), and translation quality (May and Knight, 2007; Fossum et al., 2008). In this paper, we remove the dependence on word alignments and instead directly model the syntactic correspondences in the data, in a manner broadly similar to Yamada and Knight (2001). In particular, we propose an unsupervised model that aligns nodes of a parse tree (or forest) in one language to spans of a sentence in another. Our model is an instance of the inversion transduction grammar (ITG) formalism (Wu, 1997), constrained in such a way that one side of the synchronous derivation respects a syntactic parse. Our model is best suited to systems which use source- or target-side trees only. The design of our model is such that, for divergent structures, a structurally integrated backoff to flatter word-level (or null) analyses is available. Therefore, our model is empirically robust to the case where syntactic divergence between languages prevents syntactically accurate ITG derivations. We show that, with appropriate pruning, our model can be efficiently trained on large parallel corpora. When compared"
N10-1014,P01-1067,1,0.669279,"e spurious word alignment can invalidate a large number of otherwise extractable rules, while unaligned words can result in an exponentially large set of extractable rules to choose from. Researchers have worked to incorporate syntactic information into word alignments, resulting in improvements to both alignment quality (Cherry and Lin, 2006; DeNero and Klein, 2007), and translation quality (May and Knight, 2007; Fossum et al., 2008). In this paper, we remove the dependence on word alignments and instead directly model the syntactic correspondences in the data, in a manner broadly similar to Yamada and Knight (2001). In particular, we propose an unsupervised model that aligns nodes of a parse tree (or forest) in one language to spans of a sentence in another. Our model is an instance of the inversion transduction grammar (ITG) formalism (Wu, 1997), constrained in such a way that one side of the synchronous derivation respects a syntactic parse. Our model is best suited to systems which use source- or target-side trees only. The design of our model is such that, for divergent structures, a structurally integrated backoff to flatter word-level (or null) analyses is available. Therefore, our model is empiri"
N10-1014,C04-1060,0,0.0186119,"our parameterization permits efficient inference algorithms which we will make use of in the next section. Although several binarizations are possible, we give one such binarization and its associated probabilities in Table 2. 3.4 Robustness to Syntactic Divergence Generally speaking, ITG grammars have proven more useful without the monolingual syntactic constraints imposed by a target parse tree. When derivations are restricted to respect a target-side parse tree, many desirable alignments are ruled out when the syntax of the two languages diverges, and alignment quality drops precipitously (Zhang and Gildea, 2004), though attempts have been made to address this issue (Gildea, 2003). Our model is designed to degrade gracefully in the case of syntactic divergence. Because it can produce foreign words at any level of the derivation, our model can effectively back off to a variant of Model 1 in the case where an ITG derivation that both respects the target parse tree and the desired word-level alignments cannot be found. For example, consider the sentence pair fragment in Figure 3. It is not possible to produce an ITG derivation of this fragment that both respects the English tree and also aligns all forei"
N10-1014,N06-1033,1,0.856635,"ns can be quite useful – where a particular number might predict a measure word quite poorly, the generalization that measure words co-occur with the CD tag is very robust. 7 For terminal symbols E, this production is not possible. 122 3.3 Membership in ITG The generative process which describes our model contains a class of grammars larger than the computationally efficient class of ITG grammars. Fortunately, the parameterization described above not only reduces the number of parameters to a manageable level, but also introduces independence assumptions which permit synchronous binarization (Zhang et al., 2006) of our grammar. Any SCFG that can be synchronously binarized is an ITG, meaning that our parameterization permits efficient inference algorithms which we will make use of in the next section. Although several binarizations are possible, we give one such binarization and its associated probabilities in Table 2. 3.4 Robustness to Syntactic Divergence Generally speaking, ITG grammars have proven more useful without the monolingual syntactic constraints imposed by a target parse tree. When derivations are restricted to respect a target-side parse tree, many desirable alignments are ruled out when"
N10-1068,P09-1088,0,0.0973336,"trained on observed strings rather than string pairs. By again treating the first FSA as an FST with empty input, we can train using the FST-cascade training algorithm described in the previous paragraph. Once we have our trained cascade, we can apply it to new data, obtaining (for example) the k-best output strings for an input string. 3 Generic Bayesian Training Bayesian learning is a wide-ranging field. We focus on training using Gibbs sampling (Geman and Geman, 1984), because it has been popularly applied in the natural language literature, e.g., (Finkel et al., 2005; DeNero et al., 2008; Blunsom et al., 2009). Our overall plan is to give a generic algorithm for Bayesian training that is a “drop-in replacement” for EM training. That is, we input an FST cascade and data and output the same FST cascade with trained weights. This is an approximation to a purely Bayesian setup (where one would always integrate over all possible weightings), but one which makes it easy to deploy FSTs to efficiently decode new data. Likewise, we do not yet support nonparametric approaches—to create a drop-in replacement for EM, we require that all parameters be specified in the initial FST cascade. We return to this issu"
N10-1068,P02-1065,0,0.0314681,"multiple training runs. Our experiments on four different tasks demonstrate the genericity of this framework, and, where applicable, large improvements in performance over EM. We also show, for unsupervised part-of-speech tagging, that automatic run selection gives a large improvement over previous Bayesian approaches. 1 Adam Pauls2 Introduction In this paper, we investigate Bayesian inference for weighted finite-state transducers (WFSTs). Many natural language models can be captured by weighted finite-state transducers (Pereira et al., 1994; Sproat et al., 1996; Knight and Al-Onaizan, 1998; Clark, 2002; Kolak et al., 2003; Mathias and Byrne, 2006), which offer several benefits: • WFSTs provide a uniform knowledge representation. • Complex problems can be broken down into a cascade of simple WFSTs. • Input- and output-epsilon transitions allow compact designs. • Generic algorithms exist for doing inferences with WFSTs. These include best-path decoding, k-best path extraction, composition, ∗ The authors are listed in alphabetical order. Please direct correspondence to Sujith Ravi (sravi@isi.edu). This work was supported by NSF grant IIS-0904684 and DARPA contract HR0011-06-C0022. Weighted tre"
N10-1068,D08-1033,0,0.0722013,"Missing"
N10-1068,P05-1045,0,0.0169169,"transformer of strings. Such a cascade is trained on observed strings rather than string pairs. By again treating the first FSA as an FST with empty input, we can train using the FST-cascade training algorithm described in the previous paragraph. Once we have our trained cascade, we can apply it to new data, obtaining (for example) the k-best output strings for an input string. 3 Generic Bayesian Training Bayesian learning is a wide-ranging field. We focus on training using Gibbs sampling (Geman and Geman, 1984), because it has been popularly applied in the natural language literature, e.g., (Finkel et al., 2005; DeNero et al., 2008; Blunsom et al., 2009). Our overall plan is to give a generic algorithm for Bayesian training that is a “drop-in replacement” for EM training. That is, we input an FST cascade and data and output the same FST cascade with trained weights. This is an approximation to a purely Bayesian setup (where one would always integrate over all possible weightings), but one which makes it easy to deploy FSTs to efficiently decode new data. Likewise, we do not yet support nonparametric approaches—to create a drop-in replacement for EM, we require that all parameters be specified in the"
N10-1068,D08-1036,0,0.0242846,"ation after the sidetracking transition is selected? Should the path attempt to re-join the old derivation as soon as possible, and if so, how is this efficiently done? Then, how can we compute new derivation scores for all possible sidetracks, so that we can choose a new sample by an appropriate weighted coin flip? Finally, would such a sampler be reversible? In order to satisfy theoretical conditions for Gibbs sampling, if we move from sample A to sample B, we must be able to immediately get back to sample A. We take a different tack here, moving from pointwise sampling to blocked sampling. Gao and Johnson (2008) employed blocked sampling for POS tagging, and the approach works nicely for arbitrary derivation lattices. We again start with a random derivation for each example in the corpus. We then choose a training example and exchange its entire derivation lattice to the end of the corpus. We create a weighted version of this lattice, called the proposal lattice, such that we can approximately sample whole paths by stochastic generation. The probabilities are based on the event counts from the rest of the sample (the cache), and on the base distribution, 451 αP0 (r |q) + c(q, r) α + c(q) (3) where q"
N10-1068,P07-1094,0,0.598979,"revious decisions inside the cache. We use Gibbs sampling to estimate the distribution of tags given words. The key to efficient sampling is to define a sampling operator that makes some small change to the overall corpus derivation. With such an operator, we derive an incremental formula for re-scoring the probability of an entire new derivation based on the probability of the old derivation. Exchangeability makes this efficient— we pretend like the area around the small change occurs at the end of the corpus, so that both old and new derivations share the same cache. Goldwater and Griffiths (2007) choose the re-sampling operator “change the tag of a single word,” and they derive the corresponding incremental scoring formula for unsupervised tagging. For other problems, designers develop different sampling operators and derive different incremental scoring formulas. 3.2 Generic Case In order to develop a generic algorithm, we need to abstract away from these problem-specific design choices. In general, hidden derivations correspond to paths through derivation lattices, so we first and are computed in this way: P(r |q) = Figure 3: Changing a decision in the derivation lattice. All paths"
N10-1068,J08-3004,1,0.847533,"ribed general training algorithms for FST cascades and their implementation, and examined the problem of run selection for both EM and Bayesian training. This work raises several interesting points for future study. First, is there an efficient method for performing pointwise sampling on general FSTs, and would pointwise sampling deliver better empirical results than blocked sampling across a range of tasks? Second, can generic methods similar to the ones described here be developed for cascades of tree transducers? It is straightforward to adapt our methods to train a single tree transducer (Graehl et al., 2008), but as most types of tree transducers are not closed under composition (G´ecseg and Steinby, 1984), the compose/de-compose method cannot be directly applied to train cascades. Third, what is the best way to extend the FST formalism to represent non-parametric Bayesian models? Consider the English re-spacing application. We currently take observed (un-spaced) data and build a giant unigram FSA that models every letter sequence seen in the data of up to 10 letters, both words and non-words. This FSA has 207,253 transitions. We also define P0 for each individual transition, which allows a prefe"
N10-1068,knight-al-onaizan-1998-translation,1,0.78918,"atically selecting from among multiple training runs. Our experiments on four different tasks demonstrate the genericity of this framework, and, where applicable, large improvements in performance over EM. We also show, for unsupervised part-of-speech tagging, that automatic run selection gives a large improvement over previous Bayesian approaches. 1 Adam Pauls2 Introduction In this paper, we investigate Bayesian inference for weighted finite-state transducers (WFSTs). Many natural language models can be captured by weighted finite-state transducers (Pereira et al., 1994; Sproat et al., 1996; Knight and Al-Onaizan, 1998; Clark, 2002; Kolak et al., 2003; Mathias and Byrne, 2006), which offer several benefits: • WFSTs provide a uniform knowledge representation. • Complex problems can be broken down into a cascade of simple WFSTs. • Input- and output-epsilon transitions allow compact designs. • Generic algorithms exist for doing inferences with WFSTs. These include best-path decoding, k-best path extraction, composition, ∗ The authors are listed in alphabetical order. Please direct correspondence to Sujith Ravi (sravi@isi.edu). This work was supported by NSF grant IIS-0904684 and DARPA contract HR0011-06-C0022."
N10-1068,P06-2065,1,0.854535,"data. • We propose a method for automatic run selec447 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 447–455, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics tion, i.e., how to automatically select among multiple training runs in order to achieve the best possible task accuracy. The natural language applications we consider in this paper are: (1) unsupervised part-of-speech (POS) tagging (Merialdo, 1994; Goldwater and Griffiths, 2007), (2) letter substitution decipherment (Peleg and Rosenfeld, 1979; Knight et al., 2006; Ravi and Knight, 2008), (3) segmentation of space-free English (Goldwater et al., 2009), and (4) Japanese/English phoneme alignment (Knight and Graehl, 1998; Ravi and Knight, 2009a). Figure 1 shows how each of these problems can be represented as a cascade of finite-state acceptors (FSAs) and finite-state transducers (FSTs). 2 Generic EM Training We first describe forward-backward EM training for a single FST M. Given a string pair (v, w) from our training data, we transform v into an FST Mv that just maps v to itself, and likewise transform w into an FST Mw . Then we compose Mv with M, and"
N10-1068,N03-1018,0,0.0182333,"ining runs. Our experiments on four different tasks demonstrate the genericity of this framework, and, where applicable, large improvements in performance over EM. We also show, for unsupervised part-of-speech tagging, that automatic run selection gives a large improvement over previous Bayesian approaches. 1 Adam Pauls2 Introduction In this paper, we investigate Bayesian inference for weighted finite-state transducers (WFSTs). Many natural language models can be captured by weighted finite-state transducers (Pereira et al., 1994; Sproat et al., 1996; Knight and Al-Onaizan, 1998; Clark, 2002; Kolak et al., 2003; Mathias and Byrne, 2006), which offer several benefits: • WFSTs provide a uniform knowledge representation. • Complex problems can be broken down into a cascade of simple WFSTs. • Input- and output-epsilon transitions allow compact designs. • Generic algorithms exist for doing inferences with WFSTs. These include best-path decoding, k-best path extraction, composition, ∗ The authors are listed in alphabetical order. Please direct correspondence to Sujith Ravi (sravi@isi.edu). This work was supported by NSF grant IIS-0904684 and DARPA contract HR0011-06-C0022. Weighted tree transducers play t"
N10-1068,J94-2001,0,0.720149,"s are: • We describe a Bayesian inference algorithm that can be used to train any cascade of WFSTs on end-to-end data. • We propose a method for automatic run selec447 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 447–455, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics tion, i.e., how to automatically select among multiple training runs in order to achieve the best possible task accuracy. The natural language applications we consider in this paper are: (1) unsupervised part-of-speech (POS) tagging (Merialdo, 1994; Goldwater and Griffiths, 2007), (2) letter substitution decipherment (Peleg and Rosenfeld, 1979; Knight et al., 2006; Ravi and Knight, 2008), (3) segmentation of space-free English (Goldwater et al., 2009), and (4) Japanese/English phoneme alignment (Knight and Graehl, 1998; Ravi and Knight, 2009a). Figure 1 shows how each of these problems can be represented as a cascade of finite-state acceptors (FSAs) and finite-state transducers (FSTs). 2 Generic EM Training We first describe forward-backward EM training for a single FST M. Given a string pair (v, w) from our training data, we transform"
N10-1068,H94-1050,0,0.0137433,"a. We also investigate the problem of automatically selecting from among multiple training runs. Our experiments on four different tasks demonstrate the genericity of this framework, and, where applicable, large improvements in performance over EM. We also show, for unsupervised part-of-speech tagging, that automatic run selection gives a large improvement over previous Bayesian approaches. 1 Adam Pauls2 Introduction In this paper, we investigate Bayesian inference for weighted finite-state transducers (WFSTs). Many natural language models can be captured by weighted finite-state transducers (Pereira et al., 1994; Sproat et al., 1996; Knight and Al-Onaizan, 1998; Clark, 2002; Kolak et al., 2003; Mathias and Byrne, 2006), which offer several benefits: • WFSTs provide a uniform knowledge representation. • Complex problems can be broken down into a cascade of simple WFSTs. • Input- and output-epsilon transitions allow compact designs. • Generic algorithms exist for doing inferences with WFSTs. These include best-path decoding, k-best path extraction, composition, ∗ The authors are listed in alphabetical order. Please direct correspondence to Sujith Ravi (sravi@isi.edu). This work was supported by NSF gra"
N10-1068,D08-1085,1,0.85282,"method for automatic run selec447 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 447–455, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics tion, i.e., how to automatically select among multiple training runs in order to achieve the best possible task accuracy. The natural language applications we consider in this paper are: (1) unsupervised part-of-speech (POS) tagging (Merialdo, 1994; Goldwater and Griffiths, 2007), (2) letter substitution decipherment (Peleg and Rosenfeld, 1979; Knight et al., 2006; Ravi and Knight, 2008), (3) segmentation of space-free English (Goldwater et al., 2009), and (4) Japanese/English phoneme alignment (Knight and Graehl, 1998; Ravi and Knight, 2009a). Figure 1 shows how each of these problems can be represented as a cascade of finite-state acceptors (FSAs) and finite-state transducers (FSTs). 2 Generic EM Training We first describe forward-backward EM training for a single FST M. Given a string pair (v, w) from our training data, we transform v into an FST Mv that just maps v to itself, and likewise transform w into an FST Mw . Then we compose Mv with M, and compose the result with"
N10-1068,N09-1005,1,0.84273,"es, California, June 2010. 2010 Association for Computational Linguistics tion, i.e., how to automatically select among multiple training runs in order to achieve the best possible task accuracy. The natural language applications we consider in this paper are: (1) unsupervised part-of-speech (POS) tagging (Merialdo, 1994; Goldwater and Griffiths, 2007), (2) letter substitution decipherment (Peleg and Rosenfeld, 1979; Knight et al., 2006; Ravi and Knight, 2008), (3) segmentation of space-free English (Goldwater et al., 2009), and (4) Japanese/English phoneme alignment (Knight and Graehl, 1998; Ravi and Knight, 2009a). Figure 1 shows how each of these problems can be represented as a cascade of finite-state acceptors (FSAs) and finite-state transducers (FSTs). 2 Generic EM Training We first describe forward-backward EM training for a single FST M. Given a string pair (v, w) from our training data, we transform v into an FST Mv that just maps v to itself, and likewise transform w into an FST Mw . Then we compose Mv with M, and compose the result with Mw . This composition follows Pereira and Riley (1996), treating epsilon input and output transitions correctly, especially with regards to their weighted in"
N10-1068,P09-1057,1,0.812237,"es, California, June 2010. 2010 Association for Computational Linguistics tion, i.e., how to automatically select among multiple training runs in order to achieve the best possible task accuracy. The natural language applications we consider in this paper are: (1) unsupervised part-of-speech (POS) tagging (Merialdo, 1994; Goldwater and Griffiths, 2007), (2) letter substitution decipherment (Peleg and Rosenfeld, 1979; Knight et al., 2006; Ravi and Knight, 2008), (3) segmentation of space-free English (Goldwater et al., 2009), and (4) Japanese/English phoneme alignment (Knight and Graehl, 1998; Ravi and Knight, 2009a). Figure 1 shows how each of these problems can be represented as a cascade of finite-state acceptors (FSAs) and finite-state transducers (FSTs). 2 Generic EM Training We first describe forward-backward EM training for a single FST M. Given a string pair (v, w) from our training data, we transform v into an FST Mv that just maps v to itself, and likewise transform w into an FST Mw . Then we compose Mv with M, and compose the result with Mw . This composition follows Pereira and Riley (1996), treating epsilon input and output transitions correctly, especially with regards to their weighted in"
N10-1068,J96-3004,0,0.091326,"the problem of automatically selecting from among multiple training runs. Our experiments on four different tasks demonstrate the genericity of this framework, and, where applicable, large improvements in performance over EM. We also show, for unsupervised part-of-speech tagging, that automatic run selection gives a large improvement over previous Bayesian approaches. 1 Adam Pauls2 Introduction In this paper, we investigate Bayesian inference for weighted finite-state transducers (WFSTs). Many natural language models can be captured by weighted finite-state transducers (Pereira et al., 1994; Sproat et al., 1996; Knight and Al-Onaizan, 1998; Clark, 2002; Kolak et al., 2003; Mathias and Byrne, 2006), which offer several benefits: • WFSTs provide a uniform knowledge representation. • Complex problems can be broken down into a cascade of simple WFSTs. • Input- and output-epsilon transitions allow compact designs. • Generic algorithms exist for doing inferences with WFSTs. These include best-path decoding, k-best path extraction, composition, ∗ The authors are listed in alphabetical order. Please direct correspondence to Sujith Ravi (sravi@isi.edu). This work was supported by NSF grant IIS-0904684 and DA"
N10-1068,J98-4003,1,\N,Missing
N15-1021,P12-1074,0,0.245727,"Missing"
N15-1119,C12-1028,1,0.775921,"Cucerzan, 2011; Guo et al., 2011; Han and Sun, 2011; Ratinov et al., 2011; Chen and Ji, 2011; Kozareva et al., 2011; Dalton and Dietz, 2013)), the target entity mention’s “collaborators” may simply include all mentions which co-occur in the same discourse (sentence, paragraph or document) (Ratinov et al., 2011; Nguyen et al., 2012). But this approach usually introduces many irrelevant mentions, and it’s very difficult to automatically determine the scope of discourse. In contrast, some recent work exploited more restricted measures by only choosing those mentions which are topically related (Cassidy et al., 2012; Xu et al., 2012), bear a relation from a fixed set (Cheng and Roth, 2013), coreferential (Nguyen et al., 2012; Huang et al., 2014), socially related (Cassidy et al., 2012; Huang et al., 3 The mapping from AMR entity types to these three main types is at: amr.isi.edu/lib/ne-type-sc.txt 1131 2014), dependent (Ling et al., 2014), or a combination of these through meta-paths (Huang et al., 2014). These measures can collect more precise collaborators but suffer from low coverage of predefined information templates and the unsatisfying quality of state-of-the-art coreference resolution, relation a"
N15-1119,D11-1071,1,0.898714,"coherent set. We show preliminary results suggesting that AMR is effective for the partitioning of all mentions in a document into coherent sets for collective linking. We evaluate our approach using both human and automatic AMR annotation, limiting target named entity types to person (PER), organization (ORG), and geo-political entities (GPE) 3 . 2 Related Work In most recent collective inference methods for EL (e.g., (Kulkarni et al., 2009; Pennacchiotti and Pantel, 2009; Fernandez et al., 2010; Radford et al., 2010; Cucerzan, 2011; Guo et al., 2011; Han and Sun, 2011; Ratinov et al., 2011; Chen and Ji, 2011; Kozareva et al., 2011; Dalton and Dietz, 2013)), the target entity mention’s “collaborators” may simply include all mentions which co-occur in the same discourse (sentence, paragraph or document) (Ratinov et al., 2011; Nguyen et al., 2012). But this approach usually introduces many irrelevant mentions, and it’s very difficult to automatically determine the scope of discourse. In contrast, some recent work exploited more restricted measures by only choosing those mentions which are topically related (Cassidy et al., 2012; Xu et al., 2012), bear a relation from a fixed set (Cheng and Roth, 201"
N15-1119,D13-1184,0,0.069909,"; Chen and Ji, 2011; Kozareva et al., 2011; Dalton and Dietz, 2013)), the target entity mention’s “collaborators” may simply include all mentions which co-occur in the same discourse (sentence, paragraph or document) (Ratinov et al., 2011; Nguyen et al., 2012). But this approach usually introduces many irrelevant mentions, and it’s very difficult to automatically determine the scope of discourse. In contrast, some recent work exploited more restricted measures by only choosing those mentions which are topically related (Cassidy et al., 2012; Xu et al., 2012), bear a relation from a fixed set (Cheng and Roth, 2013), coreferential (Nguyen et al., 2012; Huang et al., 2014), socially related (Cassidy et al., 2012; Huang et al., 3 The mapping from AMR entity types to these three main types is at: amr.isi.edu/lib/ne-type-sc.txt 1131 2014), dependent (Ling et al., 2014), or a combination of these through meta-paths (Huang et al., 2014). These measures can collect more precise collaborators but suffer from low coverage of predefined information templates and the unsatisfying quality of state-of-the-art coreference resolution, relation and event extraction. In this paper, we demonstrate that AMR is an appropria"
N15-1119,P14-1134,0,0.0471633,"Missing"
N15-1119,I11-1113,0,0.0155229,"entity targets - one target entity for each mention in the coherent set. We show preliminary results suggesting that AMR is effective for the partitioning of all mentions in a document into coherent sets for collective linking. We evaluate our approach using both human and automatic AMR annotation, limiting target named entity types to person (PER), organization (ORG), and geo-political entities (GPE) 3 . 2 Related Work In most recent collective inference methods for EL (e.g., (Kulkarni et al., 2009; Pennacchiotti and Pantel, 2009; Fernandez et al., 2010; Radford et al., 2010; Cucerzan, 2011; Guo et al., 2011; Han and Sun, 2011; Ratinov et al., 2011; Chen and Ji, 2011; Kozareva et al., 2011; Dalton and Dietz, 2013)), the target entity mention’s “collaborators” may simply include all mentions which co-occur in the same discourse (sentence, paragraph or document) (Ratinov et al., 2011; Nguyen et al., 2012). But this approach usually introduces many irrelevant mentions, and it’s very difficult to automatically determine the scope of discourse. In contrast, some recent work exploited more restricted measures by only choosing those mentions which are topically related (Cassidy et al., 2012; Xu et al.,"
N15-1119,P11-1095,0,0.00935942,"ne target entity for each mention in the coherent set. We show preliminary results suggesting that AMR is effective for the partitioning of all mentions in a document into coherent sets for collective linking. We evaluate our approach using both human and automatic AMR annotation, limiting target named entity types to person (PER), organization (ORG), and geo-political entities (GPE) 3 . 2 Related Work In most recent collective inference methods for EL (e.g., (Kulkarni et al., 2009; Pennacchiotti and Pantel, 2009; Fernandez et al., 2010; Radford et al., 2010; Cucerzan, 2011; Guo et al., 2011; Han and Sun, 2011; Ratinov et al., 2011; Chen and Ji, 2011; Kozareva et al., 2011; Dalton and Dietz, 2013)), the target entity mention’s “collaborators” may simply include all mentions which co-occur in the same discourse (sentence, paragraph or document) (Ratinov et al., 2011; Nguyen et al., 2012). But this approach usually introduces many irrelevant mentions, and it’s very difficult to automatically determine the scope of discourse. In contrast, some recent work exploited more restricted measures by only choosing those mentions which are topically related (Cassidy et al., 2012; Xu et al., 2012), bear a relat"
N15-1119,N06-2015,0,0.0411608,"ailed tests and the deployment prospects are uncertain.”, AMR labels “Yuri dolgoruky” as a product instead of a person. We manually mapped AMR entity types to equivalent DBpedia types to inform type matching restrictions 5 . However, to make our context comparison algorithm less dependent on the quality of this mapping, and on automatic AMR name type assignment, we add a mention’s type to its collaborators 6 . In future work we plan to investigate the effects of different type matching techniques, varying degrees of strictness. 3.2 Semantic Roles AMR defines core roles based on the OntoNotes (Hovy et al., 2006) semantic role layer. Each predicate is associated with a sense and frame description. If a target entity mention m and a context entity mention n are both playing core roles for the same predicate, we consider n as a collaborator of m. Consider the following post: “Did Palin apologize to Giffords? He needs to conduct a beer summit between Palin and NBC.”. We add “Giffords” and “NBC” as collaborators of “Palin”, because they play core roles in both the “apologize-01” and “meet-03” events. AMR defines new core semantic roles which did not exist in PropBank (Palmer et al., 2005), NomBank (Meyers"
N15-1119,P14-1036,1,0.792998,"z, 2013)), the target entity mention’s “collaborators” may simply include all mentions which co-occur in the same discourse (sentence, paragraph or document) (Ratinov et al., 2011; Nguyen et al., 2012). But this approach usually introduces many irrelevant mentions, and it’s very difficult to automatically determine the scope of discourse. In contrast, some recent work exploited more restricted measures by only choosing those mentions which are topically related (Cassidy et al., 2012; Xu et al., 2012), bear a relation from a fixed set (Cheng and Roth, 2013), coreferential (Nguyen et al., 2012; Huang et al., 2014), socially related (Cassidy et al., 2012; Huang et al., 3 The mapping from AMR entity types to these three main types is at: amr.isi.edu/lib/ne-type-sc.txt 1131 2014), dependent (Ling et al., 2014), or a combination of these through meta-paths (Huang et al., 2014). These measures can collect more precise collaborators but suffer from low coverage of predefined information templates and the unsatisfying quality of state-of-the-art coreference resolution, relation and event extraction. In this paper, we demonstrate that AMR is an appropriate and elegant way to acquire, select, represent and orga"
N15-1119,R09-1032,1,0.796787,"nce, to the correct target entity Indonesian Agency for Meteorology, Climatology and Geophysics, whose headquarters are listed as Jakarta in the KB: “It keeps on shaking. Jakarta BMKG spokesman Mujuhidin said”. Here, “Jakarta” is added as a collaborator of “BMKG” since AMR labels it as the location of the organization, which facilitates the correct link because in DBpedia Jakarta is listed as its headquarter. Authors often assume that readers will infer implicit temporal information about events. In fact, half of the events extracted by information extraction (IE) systems lack time arguments (Ji et al., 2009). Therefore if an AMR parse includes no time information, we use the document creation time as an additional collaborator for mention in question. For example, knowing the document creation time “2005-06-05” can help us link “Hsiung Feng” in the following sentence “The BBC reported that Taiwan has successfully test fired the Hsiung Feng, its first cruise missile.” to Hsiung Feng IIE, which was deployed in 2005. Similarly, we include document creation location as a global collaborator. 3.4 Coreference For linking purposes, we treat a coreferential chain of mentions as a single “mention”. In doi"
N15-1119,D11-1011,0,0.010915,"ow preliminary results suggesting that AMR is effective for the partitioning of all mentions in a document into coherent sets for collective linking. We evaluate our approach using both human and automatic AMR annotation, limiting target named entity types to person (PER), organization (ORG), and geo-political entities (GPE) 3 . 2 Related Work In most recent collective inference methods for EL (e.g., (Kulkarni et al., 2009; Pennacchiotti and Pantel, 2009; Fernandez et al., 2010; Radford et al., 2010; Cucerzan, 2011; Guo et al., 2011; Han and Sun, 2011; Ratinov et al., 2011; Chen and Ji, 2011; Kozareva et al., 2011; Dalton and Dietz, 2013)), the target entity mention’s “collaborators” may simply include all mentions which co-occur in the same discourse (sentence, paragraph or document) (Ratinov et al., 2011; Nguyen et al., 2012). But this approach usually introduces many irrelevant mentions, and it’s very difficult to automatically determine the scope of discourse. In contrast, some recent work exploited more restricted measures by only choosing those mentions which are topically related (Cassidy et al., 2012; Xu et al., 2012), bear a relation from a fixed set (Cheng and Roth, 2013), coreferential (Nguy"
N15-1119,W04-2705,0,0.0509936,"2006) semantic role layer. Each predicate is associated with a sense and frame description. If a target entity mention m and a context entity mention n are both playing core roles for the same predicate, we consider n as a collaborator of m. Consider the following post: “Did Palin apologize to Giffords? He needs to conduct a beer summit between Palin and NBC.”. We add “Giffords” and “NBC” as collaborators of “Palin”, because they play core roles in both the “apologize-01” and “meet-03” events. AMR defines new core semantic roles which did not exist in PropBank (Palmer et al., 2005), NomBank (Meyers et al., 2004), or Ontonotes (Hovy et al., 2006). Intuitively, the following special roles should provide discriminative collaborators: • The ARG2 role of the have-org-role-91 frame indicates the title held by an entity (ARG0), such as President and Governor, within a particular organization (ARG1). • ARG2 and ARG3 of have-rel-role-91 are used to describe two related entities of the same type, such as family members. AMR defines a rich set of general semantic relations through non-core semantic roles. We choose the following subset of non-core roles to provide collaborators for entity mentions: domain, mod,"
N15-1119,J05-1004,0,0.0263293,"on the OntoNotes (Hovy et al., 2006) semantic role layer. Each predicate is associated with a sense and frame description. If a target entity mention m and a context entity mention n are both playing core roles for the same predicate, we consider n as a collaborator of m. Consider the following post: “Did Palin apologize to Giffords? He needs to conduct a beer summit between Palin and NBC.”. We add “Giffords” and “NBC” as collaborators of “Palin”, because they play core roles in both the “apologize-01” and “meet-03” events. AMR defines new core semantic roles which did not exist in PropBank (Palmer et al., 2005), NomBank (Meyers et al., 2004), or Ontonotes (Hovy et al., 2006). Intuitively, the following special roles should provide discriminative collaborators: • The ARG2 role of the have-org-role-91 frame indicates the title held by an entity (ARG0), such as President and Governor, within a particular organization (ARG1). • ARG2 and ARG3 of have-rel-role-91 are used to describe two related entities of the same type, such as family members. AMR defines a rich set of general semantic relations through non-core semantic roles. We choose the following subset of non-core roles to provide collaborators fo"
N15-1119,D09-1025,0,0.0156305,"tions are linked simultaneously by choosing an “optimal” or maximally “coherent” set of named entity targets - one target entity for each mention in the coherent set. We show preliminary results suggesting that AMR is effective for the partitioning of all mentions in a document into coherent sets for collective linking. We evaluate our approach using both human and automatic AMR annotation, limiting target named entity types to person (PER), organization (ORG), and geo-political entities (GPE) 3 . 2 Related Work In most recent collective inference methods for EL (e.g., (Kulkarni et al., 2009; Pennacchiotti and Pantel, 2009; Fernandez et al., 2010; Radford et al., 2010; Cucerzan, 2011; Guo et al., 2011; Han and Sun, 2011; Ratinov et al., 2011; Chen and Ji, 2011; Kozareva et al., 2011; Dalton and Dietz, 2013)), the target entity mention’s “collaborators” may simply include all mentions which co-occur in the same discourse (sentence, paragraph or document) (Ratinov et al., 2011; Nguyen et al., 2012). But this approach usually introduces many irrelevant mentions, and it’s very difficult to automatically determine the scope of discourse. In contrast, some recent work exploited more restricted measures by only choosi"
N15-1119,P11-1138,0,0.0329246,"r each mention in the coherent set. We show preliminary results suggesting that AMR is effective for the partitioning of all mentions in a document into coherent sets for collective linking. We evaluate our approach using both human and automatic AMR annotation, limiting target named entity types to person (PER), organization (ORG), and geo-political entities (GPE) 3 . 2 Related Work In most recent collective inference methods for EL (e.g., (Kulkarni et al., 2009; Pennacchiotti and Pantel, 2009; Fernandez et al., 2010; Radford et al., 2010; Cucerzan, 2011; Guo et al., 2011; Han and Sun, 2011; Ratinov et al., 2011; Chen and Ji, 2011; Kozareva et al., 2011; Dalton and Dietz, 2013)), the target entity mention’s “collaborators” may simply include all mentions which co-occur in the same discourse (sentence, paragraph or document) (Ratinov et al., 2011; Nguyen et al., 2012). But this approach usually introduces many irrelevant mentions, and it’s very difficult to automatically determine the scope of discourse. In contrast, some recent work exploited more restricted measures by only choosing those mentions which are topically related (Cassidy et al., 2012; Xu et al., 2012), bear a relation from a fixed set ("
N15-1119,W13-2322,1,\N,Missing
N15-1180,D10-1016,0,0.0469063,"Missing"
N15-1180,D10-1051,1,0.917475,"ncluding space) while maintaining normal English fluency. 1572 • Lines are in iambic meter, i.e., their syllables have the stress pattern 01010101, where 0 represents an unstressed syllable, and 1 represents a stressed syllable. We also allow 01010100, to allow a line to end in a word like Angela. • The two lines end in a pair of rhyming words. Words rhyme if their phoneme sequences match from the final stressed vowel onwards. We obtain stress patterns and phoneme sequences from the CMU pronunciation dictionary.4 Monosyllabic words cause trouble, because their stress often depends on context (Greene et al., 2010). For example, eighth is stressed in eighth street, but not in eighth avenue. This makes it hard to guarantee that automatically-generated lines will scan as intended. We therefore eject all monosyllabic words 4 http://www.speech.cs.cmu.edu/cgi-bin/cmudict Sophisticated potentates misrepresenting Emirates. from the vocabulary, except for six unstressed ones (a, an, and, the, of, or). Here is a sample poem password: The supervisor notified the transportation nationwide. The le-gen-da-ry Ja-pan-ese ↓ ↑ ↓ ↑ ↓ ↑ ↓ ↑ Sub-si-di-ar-ies ov-er-seas ↓ ↑ ↓ ↑ ↓ ↑ ↓ ↑ Afghanistan, Afghanistan, Afghanistan,"
N15-1180,P07-2045,0,0.00278281,"les α and β we can vary between smooth but long sentences (α = 1 and β = 0) to XKCD-style phrases (α = 0 and β = 15). Table 1 shows example sentences we obtain with α = 2.5 and β = −2.5, yielding sentences of 9.7 words on average. 2.5 Poetry In ancient times, people recorded long, historical epics using poetry, to enhance memorability. We follow this idea by turning each system-assigned 60-bit string into a short, distinct English poem. Our format is the rhyming iambic tetrameter couplet: • The poem contains two lines of eight syllables each. We then use the Moses machine translation toolkit (Koehn et al., 2007) to search for the 1-best translation of our input 60-bit string, using the phrase table and a 5-gram English LM, disallowing re-ordering. Table 1 shows that these sentences are shorter than the mnemonic method (11.8 words versus 15 words), without losing fluency. Given a generated English sequence, we can deterministically reconstruct the original 60-bit input string, using the above phrase table in reverse. 2.4 Our next technique (Frequency Method) modifies the phrase table by assigning short bit codes to frequent words, and long bit codes to infrequent words. For example: Frequency Method S"
N16-1004,2002.tmi-tutorials.2,1,0.639973,"f a second, German input string containing the word “Flussufer” (river bank). Och and Ney (2001) describe such a multi-source MT system. They first train separate bilingual MT systems F →E, G→E, etc. At runtime, they separately translate input strings f and g into candidate target strings e1 and e2 , then select the best one of the two. A typical selection factor is the product of the system scores. Schwartz (2008) revisits such factors in the context of log-linear models and Bleu score, while Max et al. (2010) re-rank F →E n-best lists using n-gram precision with respect to G→E translations. Callison-Burch (2002) exploits hypothesis selection in multi-source MT to expand available corpora, via co-training. Others use system combination techniques to merge hypotheses at the word level, creating the ability to synthesize new translations outside those proposed by the single-source translators. These methods include confusion networks (Matusov et al., 2006; Schroeder et al., 2009), source-side string combination (Schroeder et al., 2009), and median strings (Gonz´alez-Rubio and Casacuberta, 2010). The above work all relies on base MT systems trained on bilingual data, using traditional methods. This follo"
N16-1004,1997.mtsummit-plenaries.5,0,0.484702,"Missing"
N16-1004,N15-1021,1,0.839461,"lies on base MT systems trained on bilingual data, using traditional methods. This follows early work in sentence alignment (Gale and Church, 1993) and word alignment (Simard, 1999), which exploited trilingual text, but did not build trilingual models. Previous authors possibly considered a three-dimensional translation table t(e|f, g) to be prohibitive. In this paper, by contrast, we train a P(e|f, g) model directly on trilingual data, and we use that model to decode an (f, g) pair simultaneously. We view this as a kind of multi-tape transduction (Elgot and Mezei, 1965; Kaplan and Kay, 1994; Deri and Knight, 2015) with two input tapes and one output tape. Our contributions are as follows: • We train a P(e|f, g) model directly on trilingual data, and we use it to decode a new source string pair (f, g) into target string e. • We show positive Bleu improvements over strong single-source baselines. • We show that improvements are best when the two source languages are more distant from each other. We are able to achieve these results using 30 Proceedings of NAACL-HLT 2016, pages 30–34, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics W X Y Z <EOS&gt; den state h and ce"
N16-1004,P15-1166,0,0.245033,"h and cell state c. We propose two combination methods. 2.1 A B C <EOS&gt; W X Y Z Figure 1: The encoder-decoder framework for neural machine translation (NMT) (Sutskever et al., 2014). Here, a source senThe Basic method works by concatenating the two hidden states from the source encoders, applying a linear transformation Wc (size 2000 x 1000), then sending its output through a tanh non-linearity. This operation is represented by the equation:  tence C B A (presented in reverse order as A B C) is translated decoder (gray). the framework of neural encoder-decoder models, where multi-target MT (Dong et al., 2015) and multi-source, cross-modal mappings have been explored (Luong et al., 2015a). 2 Multi-Source Neural MT In the neural encoder-decoder framework for MT (Neco and Forcada, 1997; Casta˜no and Casacuberta, 1997; Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015b), we use a recurrent neural network (encoder) to convert a source sentence into a dense, fixed-length vector. We then use another recurrent network (decoder) to convert that vector in a target sentence.1 In this paper, we use a four-layer encoder-decoder system (Figure 1) with long short-term memory (LSTM) units (Hochrei"
N16-1004,J93-1004,0,0.0470832,"ti-source MT to expand available corpora, via co-training. Others use system combination techniques to merge hypotheses at the word level, creating the ability to synthesize new translations outside those proposed by the single-source translators. These methods include confusion networks (Matusov et al., 2006; Schroeder et al., 2009), source-side string combination (Schroeder et al., 2009), and median strings (Gonz´alez-Rubio and Casacuberta, 2010). The above work all relies on base MT systems trained on bilingual data, using traditional methods. This follows early work in sentence alignment (Gale and Church, 1993) and word alignment (Simard, 1999), which exploited trilingual text, but did not build trilingual models. Previous authors possibly considered a three-dimensional translation table t(e|f, g) to be prohibitive. In this paper, by contrast, we train a P(e|f, g) model directly on trilingual data, and we use that model to decode an (f, g) pair simultaneously. We view this as a kind of multi-tape transduction (Elgot and Mezei, 1965; Kaplan and Kay, 1994; Deri and Knight, 2015) with two input tapes and one output tape. Our contributions are as follows: • We train a P(e|f, g) model directly on triling"
N16-1004,J94-3001,0,0.106105,"The above work all relies on base MT systems trained on bilingual data, using traditional methods. This follows early work in sentence alignment (Gale and Church, 1993) and word alignment (Simard, 1999), which exploited trilingual text, but did not build trilingual models. Previous authors possibly considered a three-dimensional translation table t(e|f, g) to be prohibitive. In this paper, by contrast, we train a P(e|f, g) model directly on trilingual data, and we use that model to decode an (f, g) pair simultaneously. We view this as a kind of multi-tape transduction (Elgot and Mezei, 1965; Kaplan and Kay, 1994; Deri and Knight, 2015) with two input tapes and one output tape. Our contributions are as follows: • We train a P(e|f, g) model directly on trilingual data, and we use it to decode a new source string pair (f, g) into target string e. • We show positive Bleu improvements over strong single-source baselines. • We show that improvements are best when the two source languages are more distant from each other. We are able to achieve these results using 30 Proceedings of NAACL-HLT 2016, pages 30–34, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics W X Y Z"
N16-1004,D15-1166,0,0.292288,"Z Figure 1: The encoder-decoder framework for neural machine translation (NMT) (Sutskever et al., 2014). Here, a source senThe Basic method works by concatenating the two hidden states from the source encoders, applying a linear transformation Wc (size 2000 x 1000), then sending its output through a tanh non-linearity. This operation is represented by the equation:  tence C B A (presented in reverse order as A B C) is translated decoder (gray). the framework of neural encoder-decoder models, where multi-target MT (Dong et al., 2015) and multi-source, cross-modal mappings have been explored (Luong et al., 2015a). 2 Multi-Source Neural MT In the neural encoder-decoder framework for MT (Neco and Forcada, 1997; Casta˜no and Casacuberta, 1997; Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015b), we use a recurrent neural network (encoder) to convert a source sentence into a dense, fixed-length vector. We then use another recurrent network (decoder) to convert that vector in a target sentence.1 In this paper, we use a four-layer encoder-decoder system (Figure 1) with long short-term memory (LSTM) units (Hochreiter and Schmidhuber, 1997) trained for maximum likelihood (via a softmax layer"
N16-1004,E06-1005,0,0.0126952,"selection factor is the product of the system scores. Schwartz (2008) revisits such factors in the context of log-linear models and Bleu score, while Max et al. (2010) re-rank F →E n-best lists using n-gram precision with respect to G→E translations. Callison-Burch (2002) exploits hypothesis selection in multi-source MT to expand available corpora, via co-training. Others use system combination techniques to merge hypotheses at the word level, creating the ability to synthesize new translations outside those proposed by the single-source translators. These methods include confusion networks (Matusov et al., 2006; Schroeder et al., 2009), source-side string combination (Schroeder et al., 2009), and median strings (Gonz´alez-Rubio and Casacuberta, 2010). The above work all relies on base MT systems trained on bilingual data, using traditional methods. This follows early work in sentence alignment (Gale and Church, 1993) and word alignment (Simard, 1999), which exploited trilingual text, but did not build trilingual models. Previous authors possibly considered a three-dimensional translation table t(e|f, g) to be prohibitive. In this paper, by contrast, we train a P(e|f, g) model directly on trilingual"
N16-1004,max-etal-2010-contrastive,0,0.0214526,"he normally ambiguous English word “bank” may be more easily translated into French in the presence of a second, German input string containing the word “Flussufer” (river bank). Och and Ney (2001) describe such a multi-source MT system. They first train separate bilingual MT systems F →E, G→E, etc. At runtime, they separately translate input strings f and g into candidate target strings e1 and e2 , then select the best one of the two. A typical selection factor is the product of the system scores. Schwartz (2008) revisits such factors in the context of log-linear models and Bleu score, while Max et al. (2010) re-rank F →E n-best lists using n-gram precision with respect to G→E translations. Callison-Burch (2002) exploits hypothesis selection in multi-source MT to expand available corpora, via co-training. Others use system combination techniques to merge hypotheses at the word level, creating the ability to synthesize new translations outside those proposed by the single-source translators. These methods include confusion networks (Matusov et al., 2006; Schroeder et al., 2009), source-side string combination (Schroeder et al., 2009), and median strings (Gonz´alez-Rubio and Casacuberta, 2010). The"
N16-1004,2001.mtsummit-papers.46,0,0.401378,"ion model. 1 Introduction Kay (2000) points out that if a document is translated once, it is likely to be translated again and again into other languages. This gives rise to an interesting idea: a human does the first translation by hand, then turns the rest over to machine translation (MT). The translation system now has two strings as input, which can reduce ambiguity via “triangulation” (Kay’s term). For example, the normally ambiguous English word “bank” may be more easily translated into French in the presence of a second, German input string containing the word “Flussufer” (river bank). Och and Ney (2001) describe such a multi-source MT system. They first train separate bilingual MT systems F →E, G→E, etc. At runtime, they separately translate input strings f and g into candidate target strings e1 and e2 , then select the best one of the two. A typical selection factor is the product of the system scores. Schwartz (2008) revisits such factors in the context of log-linear models and Bleu score, while Max et al. (2010) re-rank F →E n-best lists using n-gram precision with respect to G→E translations. Callison-Burch (2002) exploits hypothesis selection in multi-source MT to expand available corpo"
N16-1004,E09-1082,0,0.0249812,"he product of the system scores. Schwartz (2008) revisits such factors in the context of log-linear models and Bleu score, while Max et al. (2010) re-rank F →E n-best lists using n-gram precision with respect to G→E translations. Callison-Burch (2002) exploits hypothesis selection in multi-source MT to expand available corpora, via co-training. Others use system combination techniques to merge hypotheses at the word level, creating the ability to synthesize new translations outside those proposed by the single-source translators. These methods include confusion networks (Matusov et al., 2006; Schroeder et al., 2009), source-side string combination (Schroeder et al., 2009), and median strings (Gonz´alez-Rubio and Casacuberta, 2010). The above work all relies on base MT systems trained on bilingual data, using traditional methods. This follows early work in sentence alignment (Gale and Church, 1993) and word alignment (Simard, 1999), which exploited trilingual text, but did not build trilingual models. Previous authors possibly considered a three-dimensional translation table t(e|f, g) to be prohibitive. In this paper, by contrast, we train a P(e|f, g) model directly on trilingual data, and we use that mod"
N16-1004,2008.amta-srw.6,0,0.0393902,"wo strings as input, which can reduce ambiguity via “triangulation” (Kay’s term). For example, the normally ambiguous English word “bank” may be more easily translated into French in the presence of a second, German input string containing the word “Flussufer” (river bank). Och and Ney (2001) describe such a multi-source MT system. They first train separate bilingual MT systems F →E, G→E, etc. At runtime, they separately translate input strings f and g into candidate target strings e1 and e2 , then select the best one of the two. A typical selection factor is the product of the system scores. Schwartz (2008) revisits such factors in the context of log-linear models and Bleu score, while Max et al. (2010) re-rank F →E n-best lists using n-gram precision with respect to G→E translations. Callison-Burch (2002) exploits hypothesis selection in multi-source MT to expand available corpora, via co-training. Others use system combination techniques to merge hypotheses at the word level, creating the ability to synthesize new translations outside those proposed by the single-source translators. These methods include confusion networks (Matusov et al., 2006; Schroeder et al., 2009), source-side string comb"
N16-1004,W99-0602,0,0.0393791,"ia co-training. Others use system combination techniques to merge hypotheses at the word level, creating the ability to synthesize new translations outside those proposed by the single-source translators. These methods include confusion networks (Matusov et al., 2006; Schroeder et al., 2009), source-side string combination (Schroeder et al., 2009), and median strings (Gonz´alez-Rubio and Casacuberta, 2010). The above work all relies on base MT systems trained on bilingual data, using traditional methods. This follows early work in sentence alignment (Gale and Church, 1993) and word alignment (Simard, 1999), which exploited trilingual text, but did not build trilingual models. Previous authors possibly considered a three-dimensional translation table t(e|f, g) to be prohibitive. In this paper, by contrast, we train a P(e|f, g) model directly on trilingual data, and we use that model to decode an (f, g) pair simultaneously. We view this as a kind of multi-tape transduction (Elgot and Mezei, 1965; Kaplan and Kay, 1994; Deri and Knight, 2015) with two input tapes and one output tape. Our contributions are as follows: • We train a P(e|f, g) model directly on trilingual data, and we use it to decode"
N16-1004,P15-1150,0,0.00345727,"1 ; h2 ] into a target sentence W X Y Z. At each step, an evolving realvalued vector summarizes the state of the encoder (white) and Basic Combination Method (1) Wc and all other weights in the network are learned from example string triples drawn from a trilingual training corpus. The new cell state is simply the sum of the two cell states from the encoders. c = c1 + c2 (2) We also attempted to concatenate cell states and apply a linear transformation, but training diverges due to large cell values. 2.2 Child-Sum Method Our second combination method is inspired by the Child-Sum Tree-LSTMs of Tai et al. (2015). Here, we use an LSTM variant to combine the two hidden states and cells. The standard LSTM input, output, and new cell value are all calculated. Then cell states from each encoder get their own forget gates. The final cell state and hidden state are calculated as in a normal LSTM. More precisely:  i = sigmoid W1i h1 + W2i h2  f = sigmoid Wif hi   (4)  o = sigmoid W1o h1 + W2o h2  u = tanh W1u h1 + W2u h2 (3)   (5) (6) c = if uf + f1 c1 + f2 c2 (7) h = of tanh(cf ) (8) This method employs eight new matrices (the W ’s in the above equations), each of size 1000 x 1000. The symbol repres"
N16-1029,N03-1002,0,0.00885673,"2003), bilingual labeled data (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013) or naturally partially annotated data such as Wikipedia (Nothman et al., 2013), bootstrapping (Agichtein and Gravano, 2000; Niu et al., 2003; Becker et al., 2005; Wu et al., 2009; Chiticariu et al., 2010), and unsupervised learning (Mikheev et al., 1999; McCallum and Li, 2003; Etzioni et al., 2005; Nadeau et al., 2006; Nadeau and Sekine, 2007; Ji and Lin, 2009). Name tagging has been explored for many nonEnglish languages such as in Chinese (Ji and Grishman, 2005; Li et al., 2014), Japanese (Asahara and Matsumoto, 2003; Li et al., 2014), Arabic (Maloney and Niv, 1998), Catalan (Carreras et al., 2003), Bulgarian (Osenova and Kolkovska, 2002), Dutch (De Meulder et al., 2002), French (Béchet et al., 2000), German (Thielen, 1995), Italian (Cucchiarelli et al., 1998), Greek (Karkaletsis et al., 1999), Spanish (Arévalo et al., 2002), Portuguese (Hana et al., 2006), Serbo-croatian (Nenadić and Spasić, 2000), Swedish (Dalianis and Åström, 2001) and Turkish (Tür et al., 2003). However, most of previous work relied on substantial amount of resources such as language-specific rules, basic tools such as part-of-speech"
N16-1029,W13-2322,1,0.0318382,", and then apply Soundex code (Mortimer and Salathiel, 1995; Raghavan and Allan, 2004) to find the IL name equivalent that shares the most similar pronunciation as each English name. For example, the Bengali name “টিন ে য়ার” and “Tony Blair” have the same Soundex code “T500 B460”. 3.5 4 Supervised Active Learning Mining Expectations from KB In addition to unstructured documents, we also try to leverage structured English knowledge bases (KBs) such as DBpedia5 . Each entry is associated with a set of types such as Company, Actor and Agent. We utilize the Abstract Meaning Representation corpus (Banarescu et al., 2013) which contains both entity type and linked KB title annotations, to automatically map 9, 514 entity types in DBPedia to three main entity types of interest: Person (PER), Location (LOC) and Organization (ORG). Then we adopt a language-independent crosslingual entity linking system (Wang et al., 2015) 4 5 to link each IL name mention to English DBPedia. This linker is based on an unsupervised quantified collective inference approach. It constructs knowledge networks from the IL source documents based on entity mention co-occurrence, and knowledge networks from KB. Each IL name is matched with"
N16-1029,P00-1011,0,0.050254,"ng (Agichtein and Gravano, 2000; Niu et al., 2003; Becker et al., 2005; Wu et al., 2009; Chiticariu et al., 2010), and unsupervised learning (Mikheev et al., 1999; McCallum and Li, 2003; Etzioni et al., 2005; Nadeau et al., 2006; Nadeau and Sekine, 2007; Ji and Lin, 2009). Name tagging has been explored for many nonEnglish languages such as in Chinese (Ji and Grishman, 2005; Li et al., 2014), Japanese (Asahara and Matsumoto, 2003; Li et al., 2014), Arabic (Maloney and Niv, 1998), Catalan (Carreras et al., 2003), Bulgarian (Osenova and Kolkovska, 2002), Dutch (De Meulder et al., 2002), French (Béchet et al., 2000), German (Thielen, 1995), Italian (Cucchiarelli et al., 1998), Greek (Karkaletsis et al., 1999), Spanish (Arévalo et al., 2002), Portuguese (Hana et al., 2006), Serbo-croatian (Nenadić and Spasić, 2000), Swedish (Dalianis and Åström, 2001) and Turkish (Tür et al., 2003). However, most of previous work relied on substantial amount of resources such as language-specific rules, basic tools such as part-of-speech taggers, a large amount of labeled data, or a huge amount of Web ngram data, which are usually unavailable for low-resource ILs. In contrast, in this paper we put the name tagging task in"
N16-1029,E03-1038,0,0.0920325,"et al., 2013) or naturally partially annotated data such as Wikipedia (Nothman et al., 2013), bootstrapping (Agichtein and Gravano, 2000; Niu et al., 2003; Becker et al., 2005; Wu et al., 2009; Chiticariu et al., 2010), and unsupervised learning (Mikheev et al., 1999; McCallum and Li, 2003; Etzioni et al., 2005; Nadeau et al., 2006; Nadeau and Sekine, 2007; Ji and Lin, 2009). Name tagging has been explored for many nonEnglish languages such as in Chinese (Ji and Grishman, 2005; Li et al., 2014), Japanese (Asahara and Matsumoto, 2003; Li et al., 2014), Arabic (Maloney and Niv, 1998), Catalan (Carreras et al., 2003), Bulgarian (Osenova and Kolkovska, 2002), Dutch (De Meulder et al., 2002), French (Béchet et al., 2000), German (Thielen, 1995), Italian (Cucchiarelli et al., 1998), Greek (Karkaletsis et al., 1999), Spanish (Arévalo et al., 2002), Portuguese (Hana et al., 2006), Serbo-croatian (Nenadić and Spasić, 2000), Swedish (Dalianis and Åström, 2001) and Turkish (Tür et al., 2003). However, most of previous work relied on substantial amount of resources such as language-specific rules, basic tools such as part-of-speech taggers, a large amount of labeled data, or a huge amount of Web ngram data, which"
N16-1029,N13-1006,0,0.0156684,"We can see that name identification, especially organization identification is the main bottleneck for all languages. For example, many organization names in Hausa are often very long, nested or all low-cased, such as “makaran256 Name Tagging is a well-studied problem. Many types of frameworks have been used, including rules (Farmakiotou et al., 2000; Nadeau and Sekine, 2007), supervised models using monolingual labeled data (Zhou and Su, 2002; Chieu and Ng, 2002; Rizzo and Troncy, 2012; McCallum and Li, 2003; Li and McCallum, 2003), bilingual labeled data (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013) or naturally partially annotated data such as Wikipedia (Nothman et al., 2013), bootstrapping (Agichtein and Gravano, 2000; Niu et al., 2003; Becker et al., 2005; Wu et al., 2009; Chiticariu et al., 2010), and unsupervised learning (Mikheev et al., 1999; McCallum and Li, 2003; Etzioni et al., 2005; Nadeau et al., 2006; Nadeau and Sekine, 2007; Ji and Lin, 2009). Name tagging has been explored for many nonEnglish languages such as in Chinese (Ji and Grishman, 2005; Li et al., 2014), Japanese (Asahara and Matsumoto, 2003; Li et al., 2014), Arabic (Maloney and Niv, 1998), Cat"
N16-1029,C02-1025,0,0.0358446,"typing accuracy is computed on correctly identified names Table 5: Breakdown Scores Table 5 presents the detailed break-down scores for all languages. We can see that name identification, especially organization identification is the main bottleneck for all languages. For example, many organization names in Hausa are often very long, nested or all low-cased, such as “makaran256 Name Tagging is a well-studied problem. Many types of frameworks have been used, including rules (Farmakiotou et al., 2000; Nadeau and Sekine, 2007), supervised models using monolingual labeled data (Zhou and Su, 2002; Chieu and Ng, 2002; Rizzo and Troncy, 2012; McCallum and Li, 2003; Li and McCallum, 2003), bilingual labeled data (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013) or naturally partially annotated data such as Wikipedia (Nothman et al., 2013), bootstrapping (Agichtein and Gravano, 2000; Niu et al., 2003; Becker et al., 2005; Wu et al., 2009; Chiticariu et al., 2010), and unsupervised learning (Mikheev et al., 1999; McCallum and Li, 2003; Etzioni et al., 2005; Nadeau et al., 2006; Nadeau and Sekine, 2007; Ji and Lin, 2009). Name tagging has been explored for many nonEnglish languages such"
N16-1029,D10-1098,0,0.00551045,"s “makaran256 Name Tagging is a well-studied problem. Many types of frameworks have been used, including rules (Farmakiotou et al., 2000; Nadeau and Sekine, 2007), supervised models using monolingual labeled data (Zhou and Su, 2002; Chieu and Ng, 2002; Rizzo and Troncy, 2012; McCallum and Li, 2003; Li and McCallum, 2003), bilingual labeled data (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013) or naturally partially annotated data such as Wikipedia (Nothman et al., 2013), bootstrapping (Agichtein and Gravano, 2000; Niu et al., 2003; Becker et al., 2005; Wu et al., 2009; Chiticariu et al., 2010), and unsupervised learning (Mikheev et al., 1999; McCallum and Li, 2003; Etzioni et al., 2005; Nadeau et al., 2006; Nadeau and Sekine, 2007; Ji and Lin, 2009). Name tagging has been explored for many nonEnglish languages such as in Chinese (Ji and Grishman, 2005; Li et al., 2014), Japanese (Asahara and Matsumoto, 2003; Li et al., 2014), Arabic (Maloney and Niv, 1998), Catalan (Carreras et al., 2003), Bulgarian (Osenova and Kolkovska, 2002), Dutch (De Meulder et al., 2002), French (Béchet et al., 2000), German (Thielen, 1995), Italian (Cucchiarelli et al., 1998), Greek (Karkaletsis et al., 199"
N16-1029,W99-0613,0,0.0761662,"ey that aims to acquire a wide-range of IL-specific knowledge from native speakers in an efficient way. The survey categorizes questions and organizes them into a tree structure, so that the order of questions is chosen based on the answers of previous questions. The survey answers are then automatically translated into rules, patterns or gazetteers in the tagger. Some example questions are shown in Table 2. 3.3 Mono-lingual Expectation Mining We use a bootstrapping method to acquire IL patterns from unlabeled mono-lingual IL documents. Following the same idea in (Agichtein and Gravano, 2000; Collins and Singer, 1999), we first use names identified by high-confident rules as seeds, and generalize patterns from the contexts of these seeds. Then we evaluate the patterns and apply high-quality ones to find more names as new seeds. This process is repeated iteratively 2 . We define a pattern as a triple ⟨lef t, name, right⟩, where name is a name, left and right3 are context vectors with weighted terms (the weight is computed based on each token’s tf-idf score). For example, from a Hausa sentence “gwamnatin kasar Sin ta samar wa kasashen yammacin Afirka ... (the Government of China has given ... products to the"
N16-1029,P98-1045,0,0.125064,"er et al., 2005; Wu et al., 2009; Chiticariu et al., 2010), and unsupervised learning (Mikheev et al., 1999; McCallum and Li, 2003; Etzioni et al., 2005; Nadeau et al., 2006; Nadeau and Sekine, 2007; Ji and Lin, 2009). Name tagging has been explored for many nonEnglish languages such as in Chinese (Ji and Grishman, 2005; Li et al., 2014), Japanese (Asahara and Matsumoto, 2003; Li et al., 2014), Arabic (Maloney and Niv, 1998), Catalan (Carreras et al., 2003), Bulgarian (Osenova and Kolkovska, 2002), Dutch (De Meulder et al., 2002), French (Béchet et al., 2000), German (Thielen, 1995), Italian (Cucchiarelli et al., 1998), Greek (Karkaletsis et al., 1999), Spanish (Arévalo et al., 2002), Portuguese (Hana et al., 2006), Serbo-croatian (Nenadić and Spasić, 2000), Swedish (Dalianis and Åström, 2001) and Turkish (Tür et al., 2003). However, most of previous work relied on substantial amount of resources such as language-specific rules, basic tools such as part-of-speech taggers, a large amount of labeled data, or a huge amount of Web ngram data, which are usually unavailable for low-resource ILs. In contrast, in this paper we put the name tagging task in a new emergent setting where we need to process a surprise I"
N16-1029,demiros-etal-2000-named,0,0.0375117,"stic phenomena. Typing Accuracy* Overall F-score 84.1 93.6 86.2 92.8 72.0 69.1 82.3 40.7 51.6 33.8 65.1 35.0 43.6 47.1 * typing accuracy is computed on correctly identified names Table 5: Breakdown Scores Table 5 presents the detailed break-down scores for all languages. We can see that name identification, especially organization identification is the main bottleneck for all languages. For example, many organization names in Hausa are often very long, nested or all low-cased, such as “makaran256 Name Tagging is a well-studied problem. Many types of frameworks have been used, including rules (Farmakiotou et al., 2000; Nadeau and Sekine, 2007), supervised models using monolingual labeled data (Zhou and Su, 2002; Chieu and Ng, 2002; Rizzo and Troncy, 2012; McCallum and Li, 2003; Li and McCallum, 2003), bilingual labeled data (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013) or naturally partially annotated data such as Wikipedia (Nothman et al., 2013), bootstrapping (Agichtein and Gravano, 2000; Niu et al., 2003; Becker et al., 2005; Wu et al., 2009; Chiticariu et al., 2010), and unsupervised learning (Mikheev et al., 1999; McCallum and Li, 2003; Etzioni et al., 2005; Nadeau et al., 2"
N16-1029,P05-1045,0,0.0179112,"preposition suffixes as many as you can (e.g. “’da” in “Ankara’da yaşıyorum (I live in Ankara)” is a preposition suffix which means “in”). Translation 1. Please translate the following English words and phrases: - organization suffix: agency, group, council, party, school, hospital, company, office, ... - time expression: January, ..., December; Monday, ..., Sunday; ... Table 2: Survey Question Examples Besides the static knowledge like patterns, we can also dynamically acquire expected names from topically-related English documents for a given IL document. We apply the Stanford name tagger (Finkel et al., 2005) to the English documents to obtain a list of expected names. Then we translate the English patterns and expected names to IL. When there is no human constructed English-to-IL lexicon available, we derive a word-for-word translation table from a small parallel data set using the GIZA++ word alignment tool (Och and Ney, 2003). We also convert IL text to Latin characters based on Unicode mapping4 , and then apply Soundex code (Mortimer and Salathiel, 1995; Raghavan and Allan, 2004) to find the IL name equivalent that shares the most similar pronunciation as each English name. For example, the Be"
N16-1029,W06-2005,0,0.0333424,"9; McCallum and Li, 2003; Etzioni et al., 2005; Nadeau et al., 2006; Nadeau and Sekine, 2007; Ji and Lin, 2009). Name tagging has been explored for many nonEnglish languages such as in Chinese (Ji and Grishman, 2005; Li et al., 2014), Japanese (Asahara and Matsumoto, 2003; Li et al., 2014), Arabic (Maloney and Niv, 1998), Catalan (Carreras et al., 2003), Bulgarian (Osenova and Kolkovska, 2002), Dutch (De Meulder et al., 2002), French (Béchet et al., 2000), German (Thielen, 1995), Italian (Cucchiarelli et al., 1998), Greek (Karkaletsis et al., 1999), Spanish (Arévalo et al., 2002), Portuguese (Hana et al., 2006), Serbo-croatian (Nenadić and Spasić, 2000), Swedish (Dalianis and Åström, 2001) and Turkish (Tür et al., 2003). However, most of previous work relied on substantial amount of resources such as language-specific rules, basic tools such as part-of-speech taggers, a large amount of labeled data, or a huge amount of Web ngram data, which are usually unavailable for low-resource ILs. In contrast, in this paper we put the name tagging task in a new emergent setting where we need to process a surprise IL within very short time using very few resources. The TIDES 2003 Surprise Language Hindi Named En"
N16-1029,P05-1051,1,0.726173,"ncy, 2012; McCallum and Li, 2003; Li and McCallum, 2003), bilingual labeled data (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013) or naturally partially annotated data such as Wikipedia (Nothman et al., 2013), bootstrapping (Agichtein and Gravano, 2000; Niu et al., 2003; Becker et al., 2005; Wu et al., 2009; Chiticariu et al., 2010), and unsupervised learning (Mikheev et al., 1999; McCallum and Li, 2003; Etzioni et al., 2005; Nadeau et al., 2006; Nadeau and Sekine, 2007; Ji and Lin, 2009). Name tagging has been explored for many nonEnglish languages such as in Chinese (Ji and Grishman, 2005; Li et al., 2014), Japanese (Asahara and Matsumoto, 2003; Li et al., 2014), Arabic (Maloney and Niv, 1998), Catalan (Carreras et al., 2003), Bulgarian (Osenova and Kolkovska, 2002), Dutch (De Meulder et al., 2002), French (Béchet et al., 2000), German (Thielen, 1995), Italian (Cucchiarelli et al., 1998), Greek (Karkaletsis et al., 1999), Spanish (Arévalo et al., 2002), Portuguese (Hana et al., 2006), Serbo-croatian (Nenadić and Spasić, 2000), Swedish (Dalianis and Åström, 2001) and Turkish (Tür et al., 2003). However, most of previous work relied on substantial amount of resources such as lan"
N16-1029,Y09-1024,1,0.804456,"pervised models using monolingual labeled data (Zhou and Su, 2002; Chieu and Ng, 2002; Rizzo and Troncy, 2012; McCallum and Li, 2003; Li and McCallum, 2003), bilingual labeled data (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013) or naturally partially annotated data such as Wikipedia (Nothman et al., 2013), bootstrapping (Agichtein and Gravano, 2000; Niu et al., 2003; Becker et al., 2005; Wu et al., 2009; Chiticariu et al., 2010), and unsupervised learning (Mikheev et al., 1999; McCallum and Li, 2003; Etzioni et al., 2005; Nadeau et al., 2006; Nadeau and Sekine, 2007; Ji and Lin, 2009). Name tagging has been explored for many nonEnglish languages such as in Chinese (Ji and Grishman, 2005; Li et al., 2014), Japanese (Asahara and Matsumoto, 2003; Li et al., 2014), Arabic (Maloney and Niv, 1998), Catalan (Carreras et al., 2003), Bulgarian (Osenova and Kolkovska, 2002), Dutch (De Meulder et al., 2002), French (Béchet et al., 2000), German (Thielen, 1995), Italian (Cucchiarelli et al., 1998), Greek (Karkaletsis et al., 1999), Spanish (Arévalo et al., 2002), Portuguese (Hana et al., 2006), Serbo-croatian (Nenadić and Spasić, 2000), Swedish (Dalianis and Åström, 2001) and Turkish"
N16-1029,P12-1073,0,0.0266001,"for all languages. We can see that name identification, especially organization identification is the main bottleneck for all languages. For example, many organization names in Hausa are often very long, nested or all low-cased, such as “makaran256 Name Tagging is a well-studied problem. Many types of frameworks have been used, including rules (Farmakiotou et al., 2000; Nadeau and Sekine, 2007), supervised models using monolingual labeled data (Zhou and Su, 2002; Chieu and Ng, 2002; Rizzo and Troncy, 2012; McCallum and Li, 2003; Li and McCallum, 2003), bilingual labeled data (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013) or naturally partially annotated data such as Wikipedia (Nothman et al., 2013), bootstrapping (Agichtein and Gravano, 2000; Niu et al., 2003; Becker et al., 2005; Wu et al., 2009; Chiticariu et al., 2010), and unsupervised learning (Mikheev et al., 1999; McCallum and Li, 2003; Etzioni et al., 2005; Nadeau et al., 2006; Nadeau and Sekine, 2007; Ji and Lin, 2009). Name tagging has been explored for many nonEnglish languages such as in Chinese (Ji and Grishman, 2005; Li et al., 2014), Japanese (Asahara and Matsumoto, 2003; Li et al., 2014), Arabic (Maloney a"
N16-1029,li-etal-2014-comparison,1,0.597073,"Li, 2003; Li and McCallum, 2003), bilingual labeled data (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013) or naturally partially annotated data such as Wikipedia (Nothman et al., 2013), bootstrapping (Agichtein and Gravano, 2000; Niu et al., 2003; Becker et al., 2005; Wu et al., 2009; Chiticariu et al., 2010), and unsupervised learning (Mikheev et al., 1999; McCallum and Li, 2003; Etzioni et al., 2005; Nadeau et al., 2006; Nadeau and Sekine, 2007; Ji and Lin, 2009). Name tagging has been explored for many nonEnglish languages such as in Chinese (Ji and Grishman, 2005; Li et al., 2014), Japanese (Asahara and Matsumoto, 2003; Li et al., 2014), Arabic (Maloney and Niv, 1998), Catalan (Carreras et al., 2003), Bulgarian (Osenova and Kolkovska, 2002), Dutch (De Meulder et al., 2002), French (Béchet et al., 2000), German (Thielen, 1995), Italian (Cucchiarelli et al., 1998), Greek (Karkaletsis et al., 1999), Spanish (Arévalo et al., 2002), Portuguese (Hana et al., 2006), Serbo-croatian (Nenadić and Spasić, 2000), Swedish (Dalianis and Åström, 2001) and Turkish (Tür et al., 2003). However, most of previous work relied on substantial amount of resources such as language-specific rul"
N16-1029,W98-1002,0,0.0349112,"al., 2012; Che et al., 2013; Wang et al., 2013) or naturally partially annotated data such as Wikipedia (Nothman et al., 2013), bootstrapping (Agichtein and Gravano, 2000; Niu et al., 2003; Becker et al., 2005; Wu et al., 2009; Chiticariu et al., 2010), and unsupervised learning (Mikheev et al., 1999; McCallum and Li, 2003; Etzioni et al., 2005; Nadeau et al., 2006; Nadeau and Sekine, 2007; Ji and Lin, 2009). Name tagging has been explored for many nonEnglish languages such as in Chinese (Ji and Grishman, 2005; Li et al., 2014), Japanese (Asahara and Matsumoto, 2003; Li et al., 2014), Arabic (Maloney and Niv, 1998), Catalan (Carreras et al., 2003), Bulgarian (Osenova and Kolkovska, 2002), Dutch (De Meulder et al., 2002), French (Béchet et al., 2000), German (Thielen, 1995), Italian (Cucchiarelli et al., 1998), Greek (Karkaletsis et al., 1999), Spanish (Arévalo et al., 2002), Portuguese (Hana et al., 2006), Serbo-croatian (Nenadić and Spasić, 2000), Swedish (Dalianis and Åström, 2001) and Turkish (Tür et al., 2003). However, most of previous work relied on substantial amount of resources such as language-specific rules, basic tools such as part-of-speech taggers, a large amount of labeled data, or a huge"
N16-1029,W03-0430,0,0.0347683,"ntified names Table 5: Breakdown Scores Table 5 presents the detailed break-down scores for all languages. We can see that name identification, especially organization identification is the main bottleneck for all languages. For example, many organization names in Hausa are often very long, nested or all low-cased, such as “makaran256 Name Tagging is a well-studied problem. Many types of frameworks have been used, including rules (Farmakiotou et al., 2000; Nadeau and Sekine, 2007), supervised models using monolingual labeled data (Zhou and Su, 2002; Chieu and Ng, 2002; Rizzo and Troncy, 2012; McCallum and Li, 2003; Li and McCallum, 2003), bilingual labeled data (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013) or naturally partially annotated data such as Wikipedia (Nothman et al., 2013), bootstrapping (Agichtein and Gravano, 2000; Niu et al., 2003; Becker et al., 2005; Wu et al., 2009; Chiticariu et al., 2010), and unsupervised learning (Mikheev et al., 1999; McCallum and Li, 2003; Etzioni et al., 2005; Nadeau et al., 2006; Nadeau and Sekine, 2007; Ji and Lin, 2009). Name tagging has been explored for many nonEnglish languages such as in Chinese (Ji and Grishman, 2005; Li et al."
N16-1029,E99-1001,0,0.0394154,"Many types of frameworks have been used, including rules (Farmakiotou et al., 2000; Nadeau and Sekine, 2007), supervised models using monolingual labeled data (Zhou and Su, 2002; Chieu and Ng, 2002; Rizzo and Troncy, 2012; McCallum and Li, 2003; Li and McCallum, 2003), bilingual labeled data (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013) or naturally partially annotated data such as Wikipedia (Nothman et al., 2013), bootstrapping (Agichtein and Gravano, 2000; Niu et al., 2003; Becker et al., 2005; Wu et al., 2009; Chiticariu et al., 2010), and unsupervised learning (Mikheev et al., 1999; McCallum and Li, 2003; Etzioni et al., 2005; Nadeau et al., 2006; Nadeau and Sekine, 2007; Ji and Lin, 2009). Name tagging has been explored for many nonEnglish languages such as in Chinese (Ji and Grishman, 2005; Li et al., 2014), Japanese (Asahara and Matsumoto, 2003; Li et al., 2014), Arabic (Maloney and Niv, 1998), Catalan (Carreras et al., 2003), Bulgarian (Osenova and Kolkovska, 2002), Dutch (De Meulder et al., 2002), French (Béchet et al., 2000), German (Thielen, 1995), Italian (Cucchiarelli et al., 1998), Greek (Karkaletsis et al., 1999), Spanish (Arévalo et al., 2002), Portuguese (H"
N16-1029,N03-2025,0,0.0601139,"usa are often very long, nested or all low-cased, such as “makaran256 Name Tagging is a well-studied problem. Many types of frameworks have been used, including rules (Farmakiotou et al., 2000; Nadeau and Sekine, 2007), supervised models using monolingual labeled data (Zhou and Su, 2002; Chieu and Ng, 2002; Rizzo and Troncy, 2012; McCallum and Li, 2003; Li and McCallum, 2003), bilingual labeled data (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013) or naturally partially annotated data such as Wikipedia (Nothman et al., 2013), bootstrapping (Agichtein and Gravano, 2000; Niu et al., 2003; Becker et al., 2005; Wu et al., 2009; Chiticariu et al., 2010), and unsupervised learning (Mikheev et al., 1999; McCallum and Li, 2003; Etzioni et al., 2005; Nadeau et al., 2006; Nadeau and Sekine, 2007; Ji and Lin, 2009). Name tagging has been explored for many nonEnglish languages such as in Chinese (Ji and Grishman, 2005; Li et al., 2014), Japanese (Asahara and Matsumoto, 2003; Li et al., 2014), Arabic (Maloney and Niv, 1998), Catalan (Carreras et al., 2003), Bulgarian (Osenova and Kolkovska, 2002), Dutch (De Meulder et al., 2002), French (Béchet et al., 2000), German (Thielen, 1995), Ita"
N16-1029,J03-1002,0,0.00415466,"uary, ..., December; Monday, ..., Sunday; ... Table 2: Survey Question Examples Besides the static knowledge like patterns, we can also dynamically acquire expected names from topically-related English documents for a given IL document. We apply the Stanford name tagger (Finkel et al., 2005) to the English documents to obtain a list of expected names. Then we translate the English patterns and expected names to IL. When there is no human constructed English-to-IL lexicon available, we derive a word-for-word translation table from a small parallel data set using the GIZA++ word alignment tool (Och and Ney, 2003). We also convert IL text to Latin characters based on Unicode mapping4 , and then apply Soundex code (Mortimer and Salathiel, 1995; Raghavan and Allan, 2004) to find the IL name equivalent that shares the most similar pronunciation as each English name. For example, the Bengali name “টিন ে য়ার” and “Tony Blair” have the same Soundex code “T500 B460”. 3.5 4 Supervised Active Learning Mining Expectations from KB In addition to unstructured documents, we also try to leverage structured English knowledge bases (KBs) such as DBpedia5 . Each entry is associated with a set of types such as Company,"
N16-1029,W04-2905,0,0.0351028,"uire expected names from topically-related English documents for a given IL document. We apply the Stanford name tagger (Finkel et al., 2005) to the English documents to obtain a list of expected names. Then we translate the English patterns and expected names to IL. When there is no human constructed English-to-IL lexicon available, we derive a word-for-word translation table from a small parallel data set using the GIZA++ word alignment tool (Och and Ney, 2003). We also convert IL text to Latin characters based on Unicode mapping4 , and then apply Soundex code (Mortimer and Salathiel, 1995; Raghavan and Allan, 2004) to find the IL name equivalent that shares the most similar pronunciation as each English name. For example, the Bengali name “টিন ে য়ার” and “Tony Blair” have the same Soundex code “T500 B460”. 3.5 4 Supervised Active Learning Mining Expectations from KB In addition to unstructured documents, we also try to leverage structured English knowledge bases (KBs) such as DBpedia5 . Each entry is associated with a set of types such as Company, Actor and Agent. We utilize the Abstract Meaning Representation corpus (Banarescu et al., 2013) which contains both entity type and linked KB title annotation"
N16-1029,E12-2015,0,0.00508774,"omputed on correctly identified names Table 5: Breakdown Scores Table 5 presents the detailed break-down scores for all languages. We can see that name identification, especially organization identification is the main bottleneck for all languages. For example, many organization names in Hausa are often very long, nested or all low-cased, such as “makaran256 Name Tagging is a well-studied problem. Many types of frameworks have been used, including rules (Farmakiotou et al., 2000; Nadeau and Sekine, 2007), supervised models using monolingual labeled data (Zhou and Su, 2002; Chieu and Ng, 2002; Rizzo and Troncy, 2012; McCallum and Li, 2003; Li and McCallum, 2003), bilingual labeled data (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013) or naturally partially annotated data such as Wikipedia (Nothman et al., 2013), bootstrapping (Agichtein and Gravano, 2000; Niu et al., 2003; Becker et al., 2005; Wu et al., 2009; Chiticariu et al., 2010), and unsupervised learning (Mikheev et al., 1999; McCallum and Li, 2003; Etzioni et al., 2005; Nadeau et al., 2006; Nadeau and Sekine, 2007; Ji and Lin, 2009). Name tagging has been explored for many nonEnglish languages such as in Chinese (Ji and Gr"
N16-1029,D08-1112,0,0.0103785,"abeled pool ϕ(·) ← query strategy, B ← query batch size M ← maximum number of tokens while Length(L)&lt; M do θ = train(L); for b ∈ {1, 2, ..., B} do x∗b = arg maxx∈U ϕ(x) L = L ∪ {x∗b , label(x∗b )} U = U − x∗b end for end while T ∑ M ∑ Name 4,713 1,619 6,119 4120 4,954 2,694 3,745 Unique Name 2,820 950 3,375 2,871 3,314 1,323 2,337 IL Dev. Docs 12,495 13,652 1,616 4,597 10,000 10,000 427 IL-English Docs 169 645 145 166 191 484 252 Table 3: Data Statistics Jing et al. (2004) proposed an entropy measure for active learning for image retrieval task. We compared it with other measures proposed by (Settles and Craven, 2008) and found that sequence entropy (SE) is most effective for our name tagging task. We use ϕSE to represent how informative a sentence is: ϕSE (x) = − Language IL Test Docs Bengali 100 Hausa 100 Tagalog 100 Tamil 100 Thai 100 Turkish 100 Yoruba 100 Pθ (yt = m)logPθ (yt = m) each expectation-driven rule based on its precision score on a small development set of ten documents. Then we apply these rules in the priority order of their confidence values. When the results of two taggers are conflicting on either mention boundary or type, if the applied rule has high confidence we will trust its outpu"
N16-1029,P13-1106,0,0.256647,"ame identification, especially organization identification is the main bottleneck for all languages. For example, many organization names in Hausa are often very long, nested or all low-cased, such as “makaran256 Name Tagging is a well-studied problem. Many types of frameworks have been used, including rules (Farmakiotou et al., 2000; Nadeau and Sekine, 2007), supervised models using monolingual labeled data (Zhou and Su, 2002; Chieu and Ng, 2002; Rizzo and Troncy, 2012; McCallum and Li, 2003; Li and McCallum, 2003), bilingual labeled data (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013) or naturally partially annotated data such as Wikipedia (Nothman et al., 2013), bootstrapping (Agichtein and Gravano, 2000; Niu et al., 2003; Becker et al., 2005; Wu et al., 2009; Chiticariu et al., 2010), and unsupervised learning (Mikheev et al., 1999; McCallum and Li, 2003; Etzioni et al., 2005; Nadeau et al., 2006; Nadeau and Sekine, 2007; Ji and Lin, 2009). Name tagging has been explored for many nonEnglish languages such as in Chinese (Ji and Grishman, 2005; Li et al., 2014), Japanese (Asahara and Matsumoto, 2003; Li et al., 2014), Arabic (Maloney and Niv, 1998), Catalan (Carreras et al"
N16-1029,D15-1081,1,0.782214,"earning Mining Expectations from KB In addition to unstructured documents, we also try to leverage structured English knowledge bases (KBs) such as DBpedia5 . Each entry is associated with a set of types such as Company, Actor and Agent. We utilize the Abstract Meaning Representation corpus (Banarescu et al., 2013) which contains both entity type and linked KB title annotations, to automatically map 9, 514 entity types in DBPedia to three main entity types of interest: Person (PER), Location (LOC) and Organization (ORG). Then we adopt a language-independent crosslingual entity linking system (Wang et al., 2015) 4 5 to link each IL name mention to English DBPedia. This linker is based on an unsupervised quantified collective inference approach. It constructs knowledge networks from the IL source documents based on entity mention co-occurrence, and knowledge networks from KB. Each IL name is matched with candidate entities in English KB using name translation pairs derived from inter-lingual KB links in Wikipedia and DBPedia. We also apply the wordfor-word translation tables constructed from parallel data as described in Section 3.4 to translate some uncommon names. Then it performs semantic compariso"
N16-1029,D09-1158,0,0.00820629,"low-cased, such as “makaran256 Name Tagging is a well-studied problem. Many types of frameworks have been used, including rules (Farmakiotou et al., 2000; Nadeau and Sekine, 2007), supervised models using monolingual labeled data (Zhou and Su, 2002; Chieu and Ng, 2002; Rizzo and Troncy, 2012; McCallum and Li, 2003; Li and McCallum, 2003), bilingual labeled data (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013) or naturally partially annotated data such as Wikipedia (Nothman et al., 2013), bootstrapping (Agichtein and Gravano, 2000; Niu et al., 2003; Becker et al., 2005; Wu et al., 2009; Chiticariu et al., 2010), and unsupervised learning (Mikheev et al., 1999; McCallum and Li, 2003; Etzioni et al., 2005; Nadeau et al., 2006; Nadeau and Sekine, 2007; Ji and Lin, 2009). Name tagging has been explored for many nonEnglish languages such as in Chinese (Ji and Grishman, 2005; Li et al., 2014), Japanese (Asahara and Matsumoto, 2003; Li et al., 2014), Arabic (Maloney and Niv, 1998), Catalan (Carreras et al., 2003), Bulgarian (Osenova and Kolkovska, 2002), Dutch (De Meulder et al., 2002), French (Béchet et al., 2000), German (Thielen, 1995), Italian (Cucchiarelli et al., 1998), Gree"
N16-1029,P02-1060,0,0.0248791,"1 35.0 43.6 47.1 * typing accuracy is computed on correctly identified names Table 5: Breakdown Scores Table 5 presents the detailed break-down scores for all languages. We can see that name identification, especially organization identification is the main bottleneck for all languages. For example, many organization names in Hausa are often very long, nested or all low-cased, such as “makaran256 Name Tagging is a well-studied problem. Many types of frameworks have been used, including rules (Farmakiotou et al., 2000; Nadeau and Sekine, 2007), supervised models using monolingual labeled data (Zhou and Su, 2002; Chieu and Ng, 2002; Rizzo and Troncy, 2012; McCallum and Li, 2003; Li and McCallum, 2003), bilingual labeled data (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013) or naturally partially annotated data such as Wikipedia (Nothman et al., 2013), bootstrapping (Agichtein and Gravano, 2000; Niu et al., 2003; Becker et al., 2005; Wu et al., 2009; Chiticariu et al., 2010), and unsupervised learning (Mikheev et al., 1999; McCallum and Li, 2003; Etzioni et al., 2005; Nadeau et al., 2006; Nadeau and Sekine, 2007; Ji and Lin, 2009). Name tagging has been explored for many nonEng"
N16-1029,C98-1045,0,\N,Missing
N16-1145,N09-1025,1,0.743713,"rate our LSTM as a rescoring feature on top of the output of a strong Arabic-English syntax-based string-to-tree statistical machine translation system (Galley et al., 2006; Galley et al., 2004). That system is trained on 208 million words of parallel Arabic-English data from a variety of sources, much of which is Egyptian discussion forum content. The Arabic side is morphologically segmented with MADA-ARZ (Habash et al., 2013). We use a standard set of features, including two conventional count-based language models, as well as thousands of sparsely-occurring, lexicalized syntactic features (Chiang et al., 2009). The system additionally uses a source-to-target feed-forward neural network translation model during decoding, analogous to the model of (Devlin et al., 2014). These features are tuned with MIRA (Chiang et al., 2009) on approximately 63,000 words of Arabic discussion forum text, along with three references. We evaluate this baseline on two test sets, each with approximately 34,000 words from the same genre used in tuning. We generate n-best lists (n = 1000) of unique translations for each sentence in the tuning set and re-rank the translations using an approach based on MERT (Och, 2003). We"
N16-1145,P14-1129,0,0.0646684,"et al., 2006; Galley et al., 2004). That system is trained on 208 million words of parallel Arabic-English data from a variety of sources, much of which is Egyptian discussion forum content. The Arabic side is morphologically segmented with MADA-ARZ (Habash et al., 2013). We use a standard set of features, including two conventional count-based language models, as well as thousands of sparsely-occurring, lexicalized syntactic features (Chiang et al., 2009). The system additionally uses a source-to-target feed-forward neural network translation model during decoding, analogous to the model of (Devlin et al., 2014). These features are tuned with MIRA (Chiang et al., 2009) on approximately 63,000 words of Arabic discussion forum text, along with three references. We evaluate this baseline on two test sets, each with approximately 34,000 words from the same genre used in tuning. We generate n-best lists (n = 1000) of unique translations for each sentence in the tuning set and re-rank the translations using an approach based on MERT (Och, 2003). We collapse all features other than language models, text length, and derivation size into a single feature, formed by taking the dot product of the previously lea"
N16-1145,N04-1035,1,0.464603,"s 2048 hidden units, with a target vocabulary size of 793,471. For training, we also use dropout to prevent overfitting. We follow Zaremba et al. (2014) for dropout locations, and we use a dropout rate of 0.2. The training is parallelized across 4 GPUs, such that each layer lies on its own GPU and communicates its activations to the next layer once it finishes its computation. 5.2 Statistical Machine Translation We incorporate our LSTM as a rescoring feature on top of the output of a strong Arabic-English syntax-based string-to-tree statistical machine translation system (Galley et al., 2006; Galley et al., 2004). That system is trained on 208 million words of parallel Arabic-English data from a variety of sources, much of which is Egyptian discussion forum content. The Arabic side is morphologically segmented with MADA-ARZ (Habash et al., 2013). We use a standard set of features, including two conventional count-based language models, as well as thousands of sparsely-occurring, lexicalized syntactic features (Chiang et al., 2009). The system additionally uses a source-to-target feed-forward neural network translation model during decoding, analogous to the model of (Devlin et al., 2014). These featur"
N16-1145,P06-1121,1,0.72422,", where each layer has 2048 hidden units, with a target vocabulary size of 793,471. For training, we also use dropout to prevent overfitting. We follow Zaremba et al. (2014) for dropout locations, and we use a dropout rate of 0.2. The training is parallelized across 4 GPUs, such that each layer lies on its own GPU and communicates its activations to the next layer once it finishes its computation. 5.2 Statistical Machine Translation We incorporate our LSTM as a rescoring feature on top of the output of a strong Arabic-English syntax-based string-to-tree statistical machine translation system (Galley et al., 2006; Galley et al., 2004). That system is trained on 208 million words of parallel Arabic-English data from a variety of sources, much of which is Egyptian discussion forum content. The Arabic side is morphologically segmented with MADA-ARZ (Habash et al., 2013). We use a standard set of features, including two conventional count-based language models, as well as thousands of sparsely-occurring, lexicalized syntactic features (Chiang et al., 2009). The system additionally uses a source-to-target feed-forward neural network translation model during decoding, analogous to the model of (Devlin et al"
N16-1145,N13-1044,0,0.0233637,"d across 4 GPUs, such that each layer lies on its own GPU and communicates its activations to the next layer once it finishes its computation. 5.2 Statistical Machine Translation We incorporate our LSTM as a rescoring feature on top of the output of a strong Arabic-English syntax-based string-to-tree statistical machine translation system (Galley et al., 2006; Galley et al., 2004). That system is trained on 208 million words of parallel Arabic-English data from a variety of sources, much of which is Egyptian discussion forum content. The Arabic side is morphologically segmented with MADA-ARZ (Habash et al., 2013). We use a standard set of features, including two conventional count-based language models, as well as thousands of sparsely-occurring, lexicalized syntactic features (Chiang et al., 2009). The system additionally uses a source-to-target feed-forward neural network translation model during decoding, analogous to the model of (Devlin et al., 2014). These features are tuned with MIRA (Chiang et al., 2009) on approximately 63,000 words of Arabic discussion forum text, along with three references. We evaluate this baseline on two test sets, each with approximately 34,000 words from the same genre"
N16-1145,D15-1166,0,0.0268234,"imple approach for handling large vocabularies effectively on the GPU. • Significantly improved perplexities (43.2) on the One Billion Word benchmark over Chelba et al. (2013) • Extrinsic machine translation improvement over a strong baseline. • Fast decoding times because in practice there is no need to normalize. 2 Long Short Term Memory Language Models In recent years, LSTMs (Hochreiter and Schmidhuber, 1997) have achieved state-of-the-art performance in many natural language tasks such as language modeling (Zaremba et al., 2014) and statistical machine translation (Sutskever et al., 2014; Luong et al., 2015). LSTMs were designed to have longer memories than standard RNNs, allowing them to exploit more context to make predictions. To compute word probabilities, the LSTM reads words left-to-right, updating its memory after each word and producing a hidden state h, which summarizes all of the history. For details on the architecture and computations of the LSTM, the reader can refer to (Zaremba et al., 2014). In this model the probability of word w given history u is P (w |u) = p(w |u) , Z(u) (1) where p(w |u) = exp Dw hT + bw is an unnormalized probability. Dw and bw are the output word embedding a"
N16-1145,P03-1021,0,0.0196923,"g et al., 2009). The system additionally uses a source-to-target feed-forward neural network translation model during decoding, analogous to the model of (Devlin et al., 2014). These features are tuned with MIRA (Chiang et al., 2009) on approximately 63,000 words of Arabic discussion forum text, along with three references. We evaluate this baseline on two test sets, each with approximately 34,000 words from the same genre used in tuning. We generate n-best lists (n = 1000) of unique translations for each sentence in the tuning set and re-rank the translations using an approach based on MERT (Och, 2003). We collapse all features other than language models, text length, and derivation size into a single feature, formed by taking the dot product of the previously learned feature and weight vectors. We then run a single iteration of MERT on the n-best lists to determine optimal weights for the collapsed feature, the uncollapsed features, and an LSTM feature formed by taking the score of the hypothesis according to the LSTM described in Section 5.1. We use the weights to rerank hypotheses from the n-best lists of the two test sets. We repeated this experiment, substituting instead a twolayer LST"
N16-1145,D13-1140,1,0.928366,"language models (LSTMs) are a class of RNNs that have been designed to model long histories and are easier to train than standard RNNs. LSTMs are currently the best performing language models on the Penn Treebank (PTB) dataset (Zaremba et al., 2014). The most common method for training LSTMs, maximum likelihood estimation (MLE), is prohibitively expensive for large vocabularies, as it involves time-intensive matrix-matrix multiplications. Noise-contrastive estimation (NCE) has been a successful alternative to train continuous space language models with large vocabularies (Mnih and Teh, 2012; Vaswani et al., 2013). However, NCE in its standard form is not suitable for GPUs, as the computations are not amenable to dense matrix operations. In this paper, we present a natural modification to the NCE objective function for language modeling that allows a very efficient GPU implementation. Using our new objective, we train large multi-layer LSTMs on the One Billion Word benchmark (Chelba et al., 2013), with its full 780k word vocabulary. We achieve significantly lower perplexities with a single model, while using only a sixth of the parameters of a very strong baseline model (Chelba et al., 2013). We releas"
N16-1145,N15-1083,0,\N,Missing
N18-1205,N06-1044,0,0.118345,"Missing"
N18-1205,C96-2215,0,0.603625,"Missing"
N18-2011,D16-1126,1,0.882175,"Missing"
N18-2011,D10-1016,0,0.271511,"Missing"
N18-2011,P17-4008,1,0.906803,"Missing"
N18-2011,D10-1051,1,0.875698,"Missing"
N18-2011,P17-1016,0,0.244481,"Missing"
N18-2011,J03-1002,0,0.0101892,"arch. Figure 1 shows how this technique addresses the problem. the FSA does not accept &lt;UNK>, as it is not pronounceable. We can let the system produce its next guess instead, but &lt;UNK> is a sign that the translation system is not sure about the source meaning. To overcome this problem, we use an idea similar to model B. This time, in addition to encouraging the unconstrained translated words, we encourage all potential translations of the foreign words. To get the potential translations, we use the translation table (t-table) extracted from parallel French-English training data using Giza++ (Och and Ney, 2003). This way, the system receives an external signal that guides it toward selecting better translations for the rare foreign word. We run five iterations of each of IBM models 1, 2, HMM, and 4 to get the t-table. An example of how this method improves the poem quality over model B can be observed in the fifth line of the poems in Figure 2. French poem: Sans mains tordues, comme ces hommes, Ces pauvres hommes sans espoir, Qui osent nourrir lesp´erance Dans le caveau du d´esespoir: Il regardait vers le soleil Et buvait lair frais jusquau soir. Human reference: He did not wring his hands, as do Th"
N18-2011,W17-3502,0,0.106636,"Missing"
N18-2011,D14-1074,0,0.35197,"Missing"
N18-2083,J93-2003,0,0.118202,"Missing"
N18-2083,J03-1002,0,0.0427943,"lingual data and substitute each word with its cluster representative to get alignments. They then duplicate their parallel data and use both regular alignments and alignments on word classes for training MT. Koˇcisk`y et al. (2014) simultaneously learn alignments and word representations from bilingual data. Their method does not benefit from monolingual data and requires large parallel data for training. Songyot and Chiang (2014) define a word-similarity model that can be trained from monolingual data using a feed-forward neural network, and alter the implementation of IBM models in Giza++ (Och and Ney, 2003) to use the word similarity inside their EM. They require large monolingual data for both source language and English. While English monolingual data is abundant, availability of large and reliable monolingual data for many low resource languages is not guaranteed. All these previous works define their own word similarity models, which similar to the more widely used distributed word representation methods (Mikolov et al., 2013; Pennington et al., 2014), assign high similarity to substitutable words in a given context; however, substitutability does not We present a method for improving word a"
N18-2083,D14-1162,0,0.0972197,"imilarity model that can be trained from monolingual data using a feed-forward neural network, and alter the implementation of IBM models in Giza++ (Och and Ney, 2003) to use the word similarity inside their EM. They require large monolingual data for both source language and English. While English monolingual data is abundant, availability of large and reliable monolingual data for many low resource languages is not guaranteed. All these previous works define their own word similarity models, which similar to the more widely used distributed word representation methods (Mikolov et al., 2013; Pennington et al., 2014), assign high similarity to substitutable words in a given context; however, substitutability does not We present a method for improving word alignments using word similarities. This method is based on encouraging common alignment links between semantically similar words. We use word vectors trained on monolingual data to estimate similarity. Our experiments on translating fifteen languages into English show consistent BLEU score improvements across the languages. 1 Introduction Word alignments are essential for statistical machine translation (MT), especially in low-resource settings where ne"
N18-2083,P06-2014,0,0.11213,"Missing"
N18-2083,D14-1048,1,0.859755,"ttings where a large portion of word types appear infrequently in the parallel data. In this paper we improve word alignments and consequently machine translation in low resource settings by improving the alignments of infrequent tokens. Works that deal with the rare-word problem in word alignment include those that alter the probability distribution of IBM models’ parameters by adding prior distributions (Vaswani et al., 2012; Mermer and Sarac¸lar, 2011), smoothing the probabilities (Moore, 2004; Zhang and Chiang, 2014; Van Bui and Le, 2016) or introducing symmetrization (Liang et al., 2006; Pourdamghani et al., 2014). These works, although effective, merely rely on the information extracted from the paral524 Proceedings of NAACL-HLT 2018, pages 524–528 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics always imply synonymy. For instance tea and coffee, or Pakistan and Afghanistan will be similar in these models but do not share translations. In this paper we propose a simple method to use off-the-shelf distributed representation methods to improve word alignments for low-resource machine translation (Section 2). Our model is based on encouraging common alignment li"
N18-2083,D14-1197,0,0.0454204,"adding semantic information. The motivation behind this branch is simple: Words with similar meanings should have similar translations. Previously, Ma et al. (2011) cluster words using monolingual data and substitute each word with its cluster representative to get alignments. They then duplicate their parallel data and use both regular alignments and alignments on word classes for training MT. Koˇcisk`y et al. (2014) simultaneously learn alignments and word representations from bilingual data. Their method does not benefit from monolingual data and requires large parallel data for training. Songyot and Chiang (2014) define a word-similarity model that can be trained from monolingual data using a feed-forward neural network, and alter the implementation of IBM models in Giza++ (Och and Ney, 2003) to use the word similarity inside their EM. They require large monolingual data for both source language and English. While English monolingual data is abundant, availability of large and reliable monolingual data for many low resource languages is not guaranteed. All these previous works define their own word similarity models, which similar to the more widely used distributed word representation methods (Mikolo"
N18-2083,W08-0306,1,0.831496,"Missing"
N18-2083,E03-1026,0,0.149716,"Missing"
N18-2083,W02-1012,0,0.247052,"Missing"
N18-2083,D09-1024,0,0.0607169,"Missing"
N18-2083,P14-2037,0,0.0359161,"Missing"
N18-2083,P12-1033,0,0.0221363,"lgorithm. However, EM works poorly for low-frequency words as they do not appear enough in the training data for confident parameter estimation. This problem is even worse in low-resource settings where a large portion of word types appear infrequently in the parallel data. In this paper we improve word alignments and consequently machine translation in low resource settings by improving the alignments of infrequent tokens. Works that deal with the rare-word problem in word alignment include those that alter the probability distribution of IBM models’ parameters by adding prior distributions (Vaswani et al., 2012; Mermer and Sarac¸lar, 2011), smoothing the probabilities (Moore, 2004; Zhang and Chiang, 2014; Van Bui and Le, 2016) or introducing symmetrization (Liang et al., 2006; Pourdamghani et al., 2014). These works, although effective, merely rely on the information extracted from the paral524 Proceedings of NAACL-HLT 2018, pages 524–528 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics always imply synonymy. For instance tea and coffee, or Pakistan and Afghanistan will be similar in these models but do not share translations. In this paper we propose a simp"
N18-2083,P07-2045,0,0.0126136,"10 million tokens of monolingual data. 2.2 3 We use data from fifteen languages for our machine translation experiments.3 These languages include Amheric, Arabic, Bengali, Mandarin, Farsi, Hausa, Somali, Spanish, Tamil, Thai, Turkish, Uighur, Urdu, Uzbek and Yoruba. Table 1 shows the size of training, development, test and monolingual data for each language. In addition, we use hand aligned data4 for Arabic/English (77.3K+119.5K tokens), Chinese/English (240.2K+305.2K tokens), and Farsi/English (0.9K+0.8K tokens) for word alignment experiments. We lowercase and tokenize all data using Moses (Koehn et al., 2007) scripts. train dev. test mono. amh 2.1M 39.8K 19.5K 4.3M ara 3.8M 39.1K 19.8K 230.4M ben 0.9M 41.9K 21.0K 2.5M cmn 10.6M 41.7K 20.5K 33.2M fas 4.3M 47.7K 24.2K 271.2M hau 2.1M 48.0K 24.1K 3.9M som 2.8M 46.8K 23.5K 13.5M spa 24.1M 49.4K 24.3K 14.7M tam 0.5M 39.0K 11.4K 1.0M tha 0.7M 39.1K 23.1K 39.7M tur 4.1M 40.2K 19.9K 483.0M uig 5.2M 8.6K 4.3K 33.8M urd 1.1M 46.7K 23.2K 14.4M uzb 4.2M 42.5K 21.7K 60.3M yor 2.1M 47.9K 24.5K 7.0M Extracting Transliterations For any infrequent English token e (w.r.t. the parallel data) and its translation table entry f , if f is a transliteration of e we add t"
N18-2083,P14-1072,0,0.013006,"e training data for confident parameter estimation. This problem is even worse in low-resource settings where a large portion of word types appear infrequently in the parallel data. In this paper we improve word alignments and consequently machine translation in low resource settings by improving the alignments of infrequent tokens. Works that deal with the rare-word problem in word alignment include those that alter the probability distribution of IBM models’ parameters by adding prior distributions (Vaswani et al., 2012; Mermer and Sarac¸lar, 2011), smoothing the probabilities (Moore, 2004; Zhang and Chiang, 2014; Van Bui and Le, 2016) or introducing symmetrization (Liang et al., 2006; Pourdamghani et al., 2014). These works, although effective, merely rely on the information extracted from the paral524 Proceedings of NAACL-HLT 2018, pages 524–528 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics always imply synonymy. For instance tea and coffee, or Pakistan and Afghanistan will be similar in these models but do not share translations. In this paper we propose a simple method to use off-the-shelf distributed representation methods to improve word alignments fo"
N18-2083,N04-4015,0,0.0411384,"Missing"
N18-2083,N06-1014,0,0.0714725,"e in low-resource settings where a large portion of word types appear infrequently in the parallel data. In this paper we improve word alignments and consequently machine translation in low resource settings by improving the alignments of infrequent tokens. Works that deal with the rare-word problem in word alignment include those that alter the probability distribution of IBM models’ parameters by adding prior distributions (Vaswani et al., 2012; Mermer and Sarac¸lar, 2011), smoothing the probabilities (Moore, 2004; Zhang and Chiang, 2014; Van Bui and Le, 2016) or introducing symmetrization (Liang et al., 2006; Pourdamghani et al., 2014). These works, although effective, merely rely on the information extracted from the paral524 Proceedings of NAACL-HLT 2018, pages 524–528 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics always imply synonymy. For instance tea and coffee, or Pakistan and Afghanistan will be similar in these models but do not share translations. In this paper we propose a simple method to use off-the-shelf distributed representation methods to improve word alignments for low-resource machine translation (Section 2). Our model is based on enc"
N18-2083,2011.mtsummit-papers.41,0,0.050885,"graphy (Hermjakob, 2009) morphological analysis (De Gispert et al., 2006; Lee, 2004), syntactic constraints (Fossum et al., 2008; Cherry and Lin, 2006; Toutanova et al., 2002) or a mixture of such clues (Tiedemann, 2003). These methods need languagespecific knowledge or tools like morphological analyzers or syntax parsers that is costly and time consuming to obtain for any given language. A less explored branch that can help aligning rare words is adding semantic information. The motivation behind this branch is simple: Words with similar meanings should have similar translations. Previously, Ma et al. (2011) cluster words using monolingual data and substitute each word with its cluster representative to get alignments. They then duplicate their parallel data and use both regular alignments and alignments on word classes for training MT. Koˇcisk`y et al. (2014) simultaneously learn alignments and word representations from bilingual data. Their method does not benefit from monolingual data and requires large parallel data for training. Songyot and Chiang (2014) define a word-similarity model that can be trained from monolingual data using a feed-forward neural network, and alter the implementation"
N18-2083,P11-2032,0,0.0467505,"Missing"
N18-5009,P16-1213,0,0.0310293,". In addition to the entity extraction and linking results, we also display the top 5 images for each entity retrieved from Google Image Search11 . In this way even when a user cannot read a document in a lowresource language, s/he will obtain a high-level summary of entities involved in the document. 6 Figure 4: Different Map Styles 7 Heatmap Visualization Related Work Some recent work has also focused on lowresource name tagging (Tsai et al., 2016; Littell et al., 2016; Zhang et al., 2016; Yang et al., 2017) and cross-lingual entity linking (McNamee et al., 2011; Spitkovsky and Chang, 2011; Sil and Florian, 2016), but the system demonstrated in this paper is the first publicly available end-to-end system to perform both tasks and all of the 282 Wikipedia languages. Using disaster monitoring as a use case, we detect the following ten topics from the input multilingual data based on translating 117 English disaster keywords via PanLex12 : (1) water supply, (2) food supply, (3) medical assistance, (4) terrorism or other extreme violence, (5) utilities, energy or sanitation, (6) evacuation, (7) shelter, (8) search and rescue, (9) civil unrest or widespread crime, and (10) infrastructure, as defined in the"
N18-5009,W13-2322,1,0.753485,"ifier} /entity linking amr /localize/{identifier} Description Retrieve the current server status, including supported languages, language identifiers, and the state (offline, online, or pending) of each model. Retrieve the current status of a given language. Main entry of the EDL system. Take input in either plain text or *.ltf format, tag names that are PER, ORG or LOC/GPE, and link them to Wikipedia. Transliterate a name to Latin script. Query based entity linking. Link each mention to KBs. English entity linking for Abstract Meaning Representation (AMR) style input (Pan et al., 2015). AMR (Banarescu et al., 2013) is a structured semantic representation scheme. The rich semantic knowledge in AMR boosts linking performance. Localize a LOC/GPE name based on GeoNames database. Table 1: RUN APIs description. APIs /status /status/{identifier} /train/{identifier} Description An alias of /status Query the current status of a model being trained. Train a new name tagging model for a language. A model id is automatically generated and returned based on model name, and time stamp. Table 2: TRAIN APIs description. Figure 1: Cross-lingual Entity Extraction and Linking Interface Figure 2: Cross-lingual Entity Extra"
N18-5009,K16-1022,0,0.0371881,"e test interface, where a user can select one of the 282 languages, enter a text or select an example document, and run the system. Figure 2 shows an output example. In addition to the entity extraction and linking results, we also display the top 5 images for each entity retrieved from Google Image Search11 . In this way even when a user cannot read a document in a lowresource language, s/he will obtain a high-level summary of entities involved in the document. 6 Figure 4: Different Map Styles 7 Heatmap Visualization Related Work Some recent work has also focused on lowresource name tagging (Tsai et al., 2016; Littell et al., 2016; Zhang et al., 2016; Yang et al., 2017) and cross-lingual entity linking (McNamee et al., 2011; Spitkovsky and Chang, 2011; Sil and Florian, 2016), but the system demonstrated in this paper is the first publicly available end-to-end system to perform both tasks and all of the 282 Wikipedia languages. Using disaster monitoring as a use case, we detect the following ten topics from the input multilingual data based on translating 117 English disaster keywords via PanLex12 : (1) water supply, (2) food supply, (3) medical assistance, (4) terrorism or other extreme violence,"
N18-5009,Q16-1026,0,0.0419127,"Missing"
N18-5009,N16-1029,1,0.928423,"on Sciences Institute {jonmay,knight}@isi.edu Abstract tity mention with a certain type. Our model is based on a bi-directional long short-term memory (LSTM) networks with a Conditional Random Fields (CRFs) layer (Chiu and Nichols, 2016). It is challenging to perform entity extraction across a massive variety of languages because most languages don’t have sufficient data to train a machine learning model. To tackle the low-resource challenge, we developed creative methods of deriving noisy training data from Wikipedia (Pan et al., 2017), exploiting non-traditional languageuniversal resources (Zhang et al., 2016) and crosslingual transfer learning (Cheung et al., 2017). We demonstrate E LISA -E DL, a state-of-the-art re-trainable system to extract entity mentions from low-resource languages, link them to external English knowledge bases, and visualize locations related to disaster topics on a world heatmap. We make all of our data sets1 , resources and system training and testing APIs2 publicly available for research purpose. 1 Introduction Our cross-lingual entity extraction, linking and localization system is capable of extracting named entities from unstructured text in any of 282 Wikipedia languag"
N18-5009,W16-2701,1,0.830496,"ages, and the TRAIN section provides a re-training function for users who want to train their own customized name tagging models using their own datasets. We also published our training and test data sets, as well as resources related to at morphology analysis and name translation at: https://elisa-ie.github.io/wikiann. Table 1 and Table 2 present the detailed functionality and usages of the APIs of these two sections. Besides the core components as described in Section 2 and Section 3, we also provide the APIs of additional components, including a re-trainable name transliteration component (Lin et al., 2016) and a universal name and word translation component based on word alignment derived from crossTable 3: Name Tagging Performance on Low-Resource Languages GeoNames and Wikipedia as their salience scores. Then we construct knowledge networks from source language texts, where each node represents a entity mention, and each link represents a sentence-level co-occurrence relation. If two mentions cooccur in the same sentence, we prefer their entity candidates in the GeoNames to share an administrative code and type, or be geographically close in the world, as measured in terms of latitude and long"
N18-5009,C16-1095,0,0.0142781,"here a user can select one of the 282 languages, enter a text or select an example document, and run the system. Figure 2 shows an output example. In addition to the entity extraction and linking results, we also display the top 5 images for each entity retrieved from Google Image Search11 . In this way even when a user cannot read a document in a lowresource language, s/he will obtain a high-level summary of entities involved in the document. 6 Figure 4: Different Map Styles 7 Heatmap Visualization Related Work Some recent work has also focused on lowresource name tagging (Tsai et al., 2016; Littell et al., 2016; Zhang et al., 2016; Yang et al., 2017) and cross-lingual entity linking (McNamee et al., 2011; Spitkovsky and Chang, 2011; Sil and Florian, 2016), but the system demonstrated in this paper is the first publicly available end-to-end system to perform both tasks and all of the 282 Wikipedia languages. Using disaster monitoring as a use case, we detect the following ten topics from the input multilingual data based on translating 117 English disaster keywords via PanLex12 : (1) water supply, (2) food supply, (3) medical assistance, (4) terrorism or other extreme violence, (5) utilities, energy"
N18-5009,I11-1029,0,0.0340278,"d run the system. Figure 2 shows an output example. In addition to the entity extraction and linking results, we also display the top 5 images for each entity retrieved from Google Image Search11 . In this way even when a user cannot read a document in a lowresource language, s/he will obtain a high-level summary of entities involved in the document. 6 Figure 4: Different Map Styles 7 Heatmap Visualization Related Work Some recent work has also focused on lowresource name tagging (Tsai et al., 2016; Littell et al., 2016; Zhang et al., 2016; Yang et al., 2017) and cross-lingual entity linking (McNamee et al., 2011; Spitkovsky and Chang, 2011; Sil and Florian, 2016), but the system demonstrated in this paper is the first publicly available end-to-end system to perform both tasks and all of the 282 Wikipedia languages. Using disaster monitoring as a use case, we detect the following ten topics from the input multilingual data based on translating 117 English disaster keywords via PanLex12 : (1) water supply, (2) food supply, (3) medical assistance, (4) terrorism or other extreme violence, (5) utilities, energy or sanitation, (6) evacuation, (7) shelter, (8) search and rescue, (9) civil unrest or widespre"
N18-5009,N15-1119,1,0.949944,"efforts reported from incident regions in low-resource languages. In the rest of the paper, we will present a comprehensive overview of the system components (Section 2 and Section 3), APIs (Section 4), interface3 (Section 5), and visualization4 (Section 6). 2 3 Entity Linking and Localization After we extract entity mentions, we link GPE and LOC mentions to GeoNames5 , and PER and ORG mentions to Wikipedia6 . We adopt the name translation approach described in (Pan et al., 2017) to translate each tagged entity mention into English, then we apply an unsupervised collective inference approach (Pan et al., 2015) to link each translated mention to the target KB. Figure 2 shows an example output of a Hausa document. The extracted entity mentions “Stephane Dujarric” and “birnin Bentiu” are linked to their corresponding entries in Wikipedia and GeoNames respectively. Compared to traditional entity linking, the unique challenge of linking to GeoNames is that it is very scarce, without rich linked structures or text descriptions. Only 500k out of 4.7 million entities in Wikipedia are linked to GeoNames. Therefore, we associate mentions with entities in the KBs in a collective manner, based on salience, sim"
N18-5009,P17-1178,1,0.947324,"er Polytechnic Institute {zhangb8,liny9,panx2,lud2,jih}@rpi.edu 2 Information Sciences Institute {jonmay,knight}@isi.edu Abstract tity mention with a certain type. Our model is based on a bi-directional long short-term memory (LSTM) networks with a Conditional Random Fields (CRFs) layer (Chiu and Nichols, 2016). It is challenging to perform entity extraction across a massive variety of languages because most languages don’t have sufficient data to train a machine learning model. To tackle the low-resource challenge, we developed creative methods of deriving noisy training data from Wikipedia (Pan et al., 2017), exploiting non-traditional languageuniversal resources (Zhang et al., 2016) and crosslingual transfer learning (Cheung et al., 2017). We demonstrate E LISA -E DL, a state-of-the-art re-trainable system to extract entity mentions from low-resource languages, link them to external English knowledge bases, and visualize locations related to disaster topics on a world heatmap. We make all of our data sets1 , resources and system training and testing APIs2 publicly available for research purpose. 1 Introduction Our cross-lingual entity extraction, linking and localization system is capable of ext"
P01-1030,J99-4005,1,\N,Missing
P01-1030,J93-2003,0,\N,Missing
P01-1030,P97-1047,0,\N,Missing
P01-1030,P96-1021,0,\N,Missing
P01-1030,P97-1037,0,\N,Missing
P01-1067,W98-1426,1,\N,Missing
P01-1067,soricut-etal-2002-using,1,\N,Missing
P01-1067,J93-1004,0,\N,Missing
P01-1067,W02-2103,0,\N,Missing
P01-1067,J91-1004,0,\N,Missing
P01-1067,W95-0115,0,\N,Missing
P01-1067,W99-0604,0,\N,Missing
P01-1067,W99-0906,1,\N,Missing
P01-1067,J97-2001,0,\N,Missing
P01-1067,J99-4005,1,\N,Missing
P01-1067,1994.amta-1.18,1,\N,Missing
P01-1067,1999.tc-1.8,0,\N,Missing
P01-1067,W99-0626,0,\N,Missing
P01-1067,jones-havrilla-1998-twisted,0,\N,Missing
P01-1067,W96-0501,0,\N,Missing
P01-1067,W95-0114,0,\N,Missing
P01-1067,A00-2023,0,\N,Missing
P01-1067,J98-1001,0,\N,Missing
P01-1067,J97-4005,0,\N,Missing
P01-1067,W96-0208,0,\N,Missing
P01-1067,C00-2172,0,\N,Missing
P01-1067,A97-1050,0,\N,Missing
P01-1067,C00-2135,0,\N,Missing
P01-1067,C00-2105,0,\N,Missing
P01-1067,C00-2163,0,\N,Missing
P01-1067,C94-1007,0,\N,Missing
P01-1067,E99-1001,0,\N,Missing
P01-1067,W00-0801,0,\N,Missing
P01-1067,C94-2183,0,\N,Missing
P01-1067,C00-2089,0,\N,Missing
P01-1067,W98-1230,0,\N,Missing
P01-1067,J93-2003,0,\N,Missing
P01-1067,2002.tmi-papers.20,0,\N,Missing
P01-1067,W02-1021,0,\N,Missing
P01-1067,W01-1407,0,\N,Missing
P01-1067,P98-1069,0,\N,Missing
P01-1067,C98-1066,0,\N,Missing
P01-1067,W00-0507,0,\N,Missing
P01-1067,J03-3002,0,\N,Missing
P01-1067,C88-1016,0,\N,Missing
P01-1067,P00-1059,0,\N,Missing
P01-1067,W98-1005,1,\N,Missing
P01-1067,C00-1007,0,\N,Missing
P01-1067,C02-1012,0,\N,Missing
P01-1067,C96-2098,0,\N,Missing
P01-1067,W01-1412,0,\N,Missing
P01-1067,N01-1020,0,\N,Missing
P01-1067,J03-4003,0,\N,Missing
P01-1067,J90-2002,0,\N,Missing
P01-1067,P02-1040,0,\N,Missing
P01-1067,P95-1026,0,\N,Missing
P01-1067,P95-1034,1,\N,Missing
P01-1067,W01-1409,0,\N,Missing
P01-1067,P99-1067,0,\N,Missing
P01-1067,J94-4003,0,\N,Missing
P01-1067,P98-1116,1,\N,Missing
P01-1067,C98-1112,1,\N,Missing
P01-1067,P02-1039,1,\N,Missing
P01-1067,1995.tmi-1.21,0,\N,Missing
P01-1067,J95-4004,0,\N,Missing
P01-1067,P01-1050,0,\N,Missing
P01-1067,P93-1003,0,\N,Missing
P01-1067,P91-1034,0,\N,Missing
P01-1067,J98-1004,0,\N,Missing
P01-1067,knight-al-onaizan-1998-translation,1,\N,Missing
P01-1067,J96-4002,0,\N,Missing
P01-1067,A00-1031,0,\N,Missing
P01-1067,J00-2004,0,\N,Missing
P01-1067,P99-1068,0,\N,Missing
P01-1067,P96-1021,0,\N,Missing
P01-1067,P02-1051,1,\N,Missing
P01-1067,P02-1038,0,\N,Missing
P01-1067,P97-1037,0,\N,Missing
P01-1067,J97-3002,0,\N,Missing
P01-1067,W02-1039,0,\N,Missing
P01-1067,J03-1005,0,\N,Missing
P01-1067,P97-1017,1,\N,Missing
P01-1067,W01-0504,1,\N,Missing
P01-1067,W00-2004,0,\N,Missing
P01-1067,fleming-cohen-2000-mixed,0,\N,Missing
P01-1067,P00-1056,0,\N,Missing
P01-1067,P97-1047,0,\N,Missing
P01-1067,W00-1401,0,\N,Missing
P01-1067,P96-1029,0,\N,Missing
P01-1067,P01-1030,1,\N,Missing
P02-1039,A00-2023,0,\N,Missing
P02-1039,J93-2003,0,\N,Missing
P02-1039,J03-4003,0,\N,Missing
P02-1039,P02-1040,0,\N,Missing
P02-1039,P01-1067,1,\N,Missing
P02-1039,P01-1017,0,\N,Missing
P02-1039,J97-3002,0,\N,Missing
P02-1039,P00-1056,0,\N,Missing
P02-1051,E99-1001,0,\N,Missing
P02-1051,J93-2003,0,\N,Missing
P02-1051,W98-1005,1,\N,Missing
P02-1051,W02-0505,1,\N,Missing
P03-1040,P02-1051,1,0.0974785,"mputationally more expensive methods. We go on to tackle the task of noun phrase translation in a maximum entropy reranking framework. Treating translation as a reranking problem instead of as a search problem enables us to use features over the full translation pair. We integrate both empirical and symbolic knowledge sources as features into our system which outperforms the best known methods in statistical machine translation. Previous work on defining subtasks within statistical machine translation has been performed on, e.g., noun-noun pair (Cao and Li, 2002) and named entity translation (Al-Onaizan and Knight, 2002). 2 Noun Phrase Translation as a Subtask In this work, we consider both noun phrases and prepositional phrases, which we will refer to as NP/PPs. We include prepositional phrases for a number of reasons. Both are attached at the clause level. Also, the translation of the preposition often depends heavily on the noun phrase (in the morning). Moreover, the distinction between noun phrases and prepositional phrases is not always clear (note the Japanese bunsetsu) or hard to separate (German joining of preposition and determiner into  in the). one lexical unit, e.g., ins in das 2.1 Definition We"
P03-1040,C02-1011,0,0.163175,"more dedicated modeling, but also the use of computationally more expensive methods. We go on to tackle the task of noun phrase translation in a maximum entropy reranking framework. Treating translation as a reranking problem instead of as a search problem enables us to use features over the full translation pair. We integrate both empirical and symbolic knowledge sources as features into our system which outperforms the best known methods in statistical machine translation. Previous work on defining subtasks within statistical machine translation has been performed on, e.g., noun-noun pair (Cao and Li, 2002) and named entity translation (Al-Onaizan and Knight, 2002). 2 Noun Phrase Translation as a Subtask In this work, we consider both noun phrases and prepositional phrases, which we will refer to as NP/PPs. We include prepositional phrases for a number of reasons. Both are attached at the clause level. Also, the translation of the preposition often depends heavily on the noun phrase (in the morning). Moreover, the distinction between noun phrases and prepositional phrases is not always clear (note the Japanese bunsetsu) or hard to separate (German joining of preposition and determiner into  in"
P03-1040,P97-1003,0,0.118298,"Missing"
P03-1040,P01-1030,1,0.617885,"best list for different sizes  A total of 737,388 NP/PP pairs are collected from the German-English Europarl corpus as training data. Certain German NP/PPs consistently do not align to NP/PPs in English (see the example in Section 2.2). These are detected at this point. The obtained data of unaligned NP/PPs can be used for dealing with these special cases. 3.3 Base Model Given the NP/PP corpus, we can use any general statistical machine translation method to train a translation system for noun phrases. As a baseline, we use an IBM Model 4 (Brown et al., 1993) system 3 with a greedy decoder4 (Germann et al., 2001). We found that phrase based models achieve better translation quality than IBM Model 4. Such models segment the input sequence into a number of (non-linguistic) phrases, translate each phrase using a phrase translation table, and allow for reordering of phrases in the output. No phrases may be dropped or added. We use a phrase translation model that extracts its phrase translation table from word alignments generated by the Giza++ toolkit. Details of this model are described by Koehn et al. (2003). To obtain an n-best list of candidate translations, we developed a beam search decoder. This de"
P03-1040,E03-1076,1,0.635786,"is common in a number of languages (German, Dutch, Finnish, Greek), and poses a serious problem for machine translation: The word Aktionsplan may not be known to the system, but if the word were broken up into Aktion and Plan, the system could easily translate it into action plan, or plan for action. The issues for breaking up compounds are: Knowing the morphological rules for joining words, resolving ambiguities of breaking up a word (Haupt Haupt-Turm or Haupt-Sturm), and finding sturm the right level of splitting granularity (Frei-Tag or Freitag). Here, we follow an approach introduced by Koehn and Knight (2003): First, we collect frequency statistics over words in our training corpus. Compounds may be broken up only into known words in the corpus. For each potential compound we check if morphological splitting rules allow us to break it up into such known words. Finally, we pick a splitting option (perhaps not breaking up the compound at all). This decision is based on the frequency of the words involved. 7 Available at http://www-rohan.sdsu.edu/ mal ouf/pubs.html  Specifically, we pick the splitting option with highest geometric mean of word frequencies of its parts  : best argmaxS  -  co"
P03-1040,N03-1017,1,0.0620163,"es. As a baseline, we use an IBM Model 4 (Brown et al., 1993) system 3 with a greedy decoder4 (Germann et al., 2001). We found that phrase based models achieve better translation quality than IBM Model 4. Such models segment the input sequence into a number of (non-linguistic) phrases, translate each phrase using a phrase translation table, and allow for reordering of phrases in the output. No phrases may be dropped or added. We use a phrase translation model that extracts its phrase translation table from word alignments generated by the Giza++ toolkit. Details of this model are described by Koehn et al. (2003). To obtain an n-best list of candidate translations, we developed a beam search decoder. This decoder employs hypothesis recombination and stores the search states in a search graph – similar to work by Ueffing et al. (2002) – which can be mined with standard finite state machine methods5 for n-best lists. 3 Available at http://www-i6.informatik.rwthaachen.de/ och/software/GIZA++.html 4 Available at http://www.isi.edu/licensed-sw /rewrite-decoder/ 5 We use the Carmel toolkit available at http://www. isi.edu/licensed-sw/carmel/  3.4 Acceptable Translations in the n-Best List One key question"
P03-1040,W02-2018,0,0.00821825,"so that #  !&quot;    *) for our sample  + of candidate translations. Maximum entropy learning finds a set of feature values $  so that ,.-/102& 43 ,6&quot;5- 02& 73 for each feature &  . These expectations are computed as sums over all candidate translations   for all inputs  : #98;:=&lt; &gt; ? @!A  & (&apos;   # 8B:=&lt; &gt;     & (&apos;C . A nice property of maximum entropy training is that it converges to a global optimum. There are a number of methods and tools available to carry out this training of feature values. We use the toolkit 7 developed by Malouf (2002). Berger et al. (1996) and Manning and Sch¨utze (1999) provide good introductions to maximum entropy learning. Note that any other machine learning, such as support vector machines, could be used as well. We chose maximum entropy for its ability to deal with both real-valued and binary features. This method is also similar to work by Och and Ney (2002), who use maximum entropy to tune model parameters. 4 Properties of NP/PP Translation We will now discuss the properties of NP/PP translation that we exploit in order to improve our NP/PP translation subsystem. The first of these (compounding of"
P03-1040,P00-1056,0,0.0124917,"Missing"
P03-1040,P02-1038,0,0.00793707,"&gt;     & (&apos;C . A nice property of maximum entropy training is that it converges to a global optimum. There are a number of methods and tools available to carry out this training of feature values. We use the toolkit 7 developed by Malouf (2002). Berger et al. (1996) and Manning and Sch¨utze (1999) provide good introductions to maximum entropy learning. Note that any other machine learning, such as support vector machines, could be used as well. We chose maximum entropy for its ability to deal with both real-valued and binary features. This method is also similar to work by Och and Ney (2002), who use maximum entropy to tune model parameters. 4 Properties of NP/PP Translation We will now discuss the properties of NP/PP translation that we exploit in order to improve our NP/PP translation subsystem. The first of these (compounding of words) is addressed by preprocessing, while the others motivate features which are used in n-best list reranking. 4.1 Compound Splitting Compounding of words, especially nouns, is common in a number of languages (German, Dutch, Finnish, Greek), and poses a serious problem for machine translation: The word Aktionsplan may not be known to the system, but"
P03-1040,P02-1040,0,0.108367,"d translating them in iso3.1 Model features features n-best list features features Reranker translation Figure 2: Design of the noun phrase translation subsystem: The base model generates an n-best list that is rescored using additional features lation with the same methods as the overall system has little impact on overall translation quality. In fact, we achieved a slight improvement in results due to the fact that NP/PPs are consistently translated as NP/PPs. A perfect NP/PP subsystem would triple the number of correctly translated sentences. Performance is also measured by the BLEU score (Papineni et al., 2002), which measures similarity to the reference translation taken from the English side of the parallel corpus. These findings indicate that solving the NP/PP translation problem would be a significant step toward improving overall translation quality, even if the overall system is not changed in any way. The findings also indicate that isolating the NP/PP translation task as a subtask does not harm performance. 3 Framework When translating a foreign input sentence, we detect its NP/PPs and translate them with an NP/PP translation subsystem. The best translation (or multiple best translations) is"
P03-1040,C00-2105,0,0.0586536,"Missing"
P03-1040,W02-1021,0,0.0178599,"Missing"
P03-1040,J93-2003,0,\N,Missing
P03-1040,J96-1002,0,\N,Missing
P05-3023,melvin-etal-2004-creation,1,0.752599,"ive Poster and Demonstration Sessions, c pages 89–92, Ann Arbor, June 2005. 2005 Association for Computational Linguistics ASR English Prompts or TTS English GUI: prompts, confirmations, ASR switch Prompts or TTS Farsi Dialog Manager ASR Farsi MT SMT English to Farsi Farsi to English English to Farsi Farsi to English Figure 1: Architecture of the Transonics system. The Dialogue Manager acts as the hub through which the individual components interact. our own large-scale simulated doctor-patient dialogue corpus based on recordings of medical students examining standardized patients (details in Belvin et al. 2004).1 The Farsi acoustic models r equired an eclectic approach due to the lack of existing labeled speech corpora. The approach included borrowing acoustic data from English by means of developing a sub-phonetic mapping between the two languages, as detailed in (Srinivasamurthy & Narayanan 2003), as well as use of a small existing Farsi speech corpus (FARSDAT), and our own team-internally generated acoustic data. Language modeling data was also obtained from multiple sources. The Defense Language Institute translated approximately 600,000 words of English medical dialogue data (including our stan"
P05-3025,copestake-flickinger-2000-open,0,0.0163909,"to the decoder at each point in the process. DerivTool simplifies the user’s experience of exploring these choices by presenting only the decisions relevant to the context in which the user is working, and allowing the user to search for choices that fit a particular set of conditions. Some previous tools have allowed the user to visualize word alignment information (Callison-Burch et al., 2004; Smith and Jahr, 2000), but there has been no corresponding deep effort into visualizing the decoding experience itself. Other tools use visualization to aid the user in manually developing a grammar (Copestake and Flickinger, 2000), while our tool visualizes 97 Proceedings of the ACL Interactive Poster and Demonstration Sessions, c pages 97–100, Ann Arbor, June 2005. 2005 Association for Computational Linguistics Starting with: and applying the rule: we get: If we then apply the rule: we get: Applying the next rule: results in: Finally, applying the rule: results in the final phrase: ú ´0 â NPB(DT(the) NNS(police)) ↔ ´ 0 ú NPB(DT(the) NNS(police)) â VBN(killed) ↔ â ú NPB(DT(the) NNS(police)) VBN(killed) NP-C(x0:NPB) ↔ x0 ú NP-C(NPB(DT(the) NNS(police))) VBN(killed) VP(VBD(was) VP-C(x0:VBN PP(IN(by) x1:NP-C))) ↔ ú x1 x0"
P05-3025,N04-1035,1,0.915748,"BD(was) VP-C(VBN(killed) PP(IN(by) NP-C(NPB(DT(the) NNS(police)))))) Table 1: By applying applying four rules, a Chinese verb phrase is translated to English. the translation process itself, using rules from very large, automatically learned rule sets. DerivTool can be adapted to visualize other syntax-based MT models, other tree-to-tree or tree-to-string MT models, or models for paraphrasing. 2 Translation Framework It is useful at this point to give a brief description of the syntax-based framework that we work with, which is based on translating Chinese sentences into English syntax trees. Galley et al. (2004) describe how to learn hundreds of millions of treetransformation rules from a parsed, aligned Chinese/English corpus, and Galley et al. (submitted) describe probability estimators for those rules. We decode a new Chinese sentence with a method similar to parsing, where we apply learned rules to build up a complete English tree hypothesis from the Chinese string. The rule extractor learns rules for many situations. Some are simple phrase-to-phrase rules such as: NPB(DT(the) NNS(police)) ↔ ´ 0 This rule should be read as follows: replace the Chinese word ´ 0 with the noun phrase “the police”. O"
P05-3025,J04-4002,0,0.207922,"method allows a user to explore a model of syntax-based statistical machine translation (MT), to understand the model’s strengths and weaknesses, and to compare it to other MT systems. Using this visualization method, we can find and address conceptual and practical problems in an MT system. In our demonstration at ACL, new users of our tool will drive a syntaxbased decoder for themselves. 1 1. Does our framework allow good translations for real data, and if not, where does it get stuck? 2. How does our framework compare to existing state-of-the-art phrase-based statistical MT systems such as Och and Ney (2004)? Introduction There are many new approaches to statistical machine translation, and more ideas are being suggested all the time. However, it is difficult to determine how well a model will actually perform. Experienced researchers have been surprised by the capability of unintuitive word-for-word models; at the same time, seemingly capable models often have serious hidden problems — intuition is no substitute for experimentation. With translation ideas growing more complex, capturing aspects of linguistic structure in different ways, it becomes difficult to try out a new idea without a large-"
P05-3025,smith-jahr-2000-cairo,0,0.0843385,"erivTool, an interactive translation visualization tool. It allows a user to build up a translation from one language to another, step by step, presenting the user with the myriad of choices available to the decoder at each point in the process. DerivTool simplifies the user’s experience of exploring these choices by presenting only the decisions relevant to the context in which the user is working, and allowing the user to search for choices that fit a particular set of conditions. Some previous tools have allowed the user to visualize word alignment information (Callison-Burch et al., 2004; Smith and Jahr, 2000), but there has been no corresponding deep effort into visualizing the decoding experience itself. Other tools use visualization to aid the user in manually developing a grammar (Copestake and Flickinger, 2000), while our tool visualizes 97 Proceedings of the ACL Interactive Poster and Demonstration Sessions, c pages 97–100, Ann Arbor, June 2005. 2005 Association for Computational Linguistics Starting with: and applying the rule: we get: If we then apply the rule: we get: Applying the next rule: results in: Finally, applying the rule: results in the final phrase: ú ´0 â NPB(DT(the) NNS(police)"
P05-3025,P96-1021,0,0.0375929,"can tie several of these concepts together; the rule VP(VBD(was) VP-C(x0:VBN PP(IN(by) x1:NP-C))) ↔ ú x1 x0 takes a Chinese word ú and two English constituents — x1, a noun phrase, and x0, a pastparticiple verb — and translates them into a phrase of the form “was [verb] by [noun-phrase]”. Notice that the order of the constituents has been reversed in the resulting English phrase, and that English function words have been generated. The decoder builds up a translation from the Chinese sentence into an English tree by applying these rules. It follows the decoding-as-parsing idea exemplified by Wu (1996) and Yamada and Knight (2002). For example, the Chinese verb phrase ú ´ 0 â (literally, “[passive] police kill”) can be translated to English via four rules (see Table 1). 3 DerivTool In order to test whether good translations can be generated with rules learned by Galley et al. (2004), we created DerivTool as an environment for interactively using these rules as a decoder would. A user starts with a Chinese sentence and applies rules one after another, building up a translation from Chinese to English. After finishing the translation, the user can save the trace of rule-applications (the deri"
P05-3025,P01-1067,1,0.774403,"Missing"
P05-3025,P02-1039,1,0.84412,"al of these concepts together; the rule VP(VBD(was) VP-C(x0:VBN PP(IN(by) x1:NP-C))) ↔ ú x1 x0 takes a Chinese word ú and two English constituents — x1, a noun phrase, and x0, a pastparticiple verb — and translates them into a phrase of the form “was [verb] by [noun-phrase]”. Notice that the order of the constituents has been reversed in the resulting English phrase, and that English function words have been generated. The decoder builds up a translation from the Chinese sentence into an English tree by applying these rules. It follows the decoding-as-parsing idea exemplified by Wu (1996) and Yamada and Knight (2002). For example, the Chinese verb phrase ú ´ 0 â (literally, “[passive] police kill”) can be translated to English via four rules (see Table 1). 3 DerivTool In order to test whether good translations can be generated with rules learned by Galley et al. (2004), we created DerivTool as an environment for interactively using these rules as a decoder would. A user starts with a Chinese sentence and applies rules one after another, building up a translation from Chinese to English. After finishing the translation, the user can save the trace of rule-applications (the derivation tree) for later analys"
P06-1121,N04-1014,1,\N,Missing
P06-1121,N04-1035,1,\N,Missing
P06-1121,C00-2092,0,\N,Missing
P06-1121,P01-1067,1,\N,Missing
P06-1121,J04-4002,0,\N,Missing
P06-1121,P05-1033,0,\N,Missing
P06-1121,J97-3002,0,\N,Missing
P06-1121,W02-1039,0,\N,Missing
P06-1121,J08-3004,1,\N,Missing
P06-1121,N06-1033,1,\N,Missing
P06-2065,J99-4005,1,0.589052,"t sequences (like c). Some of the parameters in these models can be estimated with supervised training, but most cannot. When we face a new ciphertext sequence c, we first use expectation-maximization (EM) (Dempster, Laird, and Rubin, 1977) to set all free parameters to maximize P(c), which is the same (by Bayes Rule) as maximizing the sum over all p of P(p)  P(c p). We then use the Viterbi algorithm to choose the p maximizing P(p)  P(c p), which is the same (by Bayes Rule) as our original goal of maximizing P(p c), or plaintext given ciphertext. Figures 1 and 2 show standard EM algorithms (Knight, 1999) for the case in which we have a bigram P(p) model (driven by a two-dimensional b table of bigram probabilities) and a one-for-one P(c p) model (driven by a two-dimensional s table of substitution probabilities). This case covers Section 3, while more complex models are employed in later sections. We study a number of natural language decipherment problems using unsupervised learning. These include letter substitution ciphers, character code conversion, phonetic decipherment, and word-based ciphers with relevance to machine translation. Straightforward unsupervised learning techniques most oft"
P06-2065,W99-0906,1,0.190652,"= = 0.48 0.33 * 0.14 0.04 for some regions where re-ordering indeed happens. We are able to move back to our chunk-based model in semi-supervised mode, which avoids the re-ordering problem, and we obtain near-perfect decipherment tables when we asked a human to re-type a few hundred words of mystery-encoded text in a UTF8 editor. P(16|12) = 0.58 * P( 2|12) = 0.32 * P(31|12) = 0.03 5 Phonetic Decipherment Figure 6: A portion of the learned P(c |p) substitution probabilities for Hindi decipherment. Correct mappings are marked with *. This section expands previous work on phonetic decipherment (Knight and Yamada, 1999). Archaeologists are often faced with an unknown writing system that is believed to represent a known spoken language. That is, the written characters encode phonetic sequences (sometimes individual phonemes, and sometimes whole words), and the relationship between text and sound is to be discovered, followed by the meaning. Viewing text as a code for speech was radical some years ago. It is now the standard view of writing systems, and many even view written Chinese as a straightforward syllabary, albeit one that is much larger and complex than, say, Japanese kana. Both Linear B and Mayan wri"
P06-2065,J93-2003,0,0.0545427,"d unsupervised learning techniques most often fail on the first try, so we describe techniques for understanding errors and significantly increasing performance. 1 Introduction Unsupervised learning holds great promise for breakthroughs in natural language processing. In cases like (Yarowsky, 1995), unsupervised methods offer accuracy results than rival supervised methods (Yarowsky, 1994) while requiring only a fraction of the data preparation effort. Such methods have also been a key driver of progress in statistical machine translation, which depends heavily on unsupervised word alignments (Brown et al., 1993). There are also interesting problems for which supervised learning is not an option. These include deciphering unknown writing systems, such as the Easter Island rongorongo script and the 20,000-word Voynich manuscript. Deciphering animal language is another case. Machine translation of human languages is another, when we consider language pairs where little or no parallel text is available. Ultimately, unsupervised learning also holds promise for scientific discovery in linguistics. At some point, our programs will begin finding novel, publishable regularities in vast amounts of linguistic d"
P06-2065,P95-1026,0,\N,Missing
P06-2065,P94-1013,0,\N,Missing
P08-1045,P98-1036,0,0.0193253,"aum´e III University of Utah School of Computing 50 S Central Campus Drive Salt Lake City, UT 84112, USA me@hal3.name taken from NIST02-05 corpora Ref2 musicians such as Bach, Mozart, Chopin, Bethoven, Shuman, Rachmaninoff, Rafael and Brokoviev Ref3 composers including Bach, Mozart, Schopen, Beethoven, missing name Raphael, Rahmaniev and Brokofien Ref4 composers such as Bach, Mozart, missing name Beethoven, Schumann, Rachmaninov, Raphael and Prokofiev The task of transliterating names (independent of end-to-end MT) has received a significant amount of research, e.g., (Knight and Graehl, 1997; Chen et al., 1998; Al-Onaizan, 2002). One approach is to “sound out” words and create new, plausible targetlanguage spellings that preserve the sounds of the source-language name as much as possible. Another approach is to phonetically match source-language names against a large list of target-language words 389 Proceedings of ACL-08: HLT, pages 389–397, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics and phrases. Most of this work has been disconnected from end-to-end MT, a problem which we address head-on in this paper. The simplest way to integrate name handling into SMT is:"
P08-1045,kang-choi-2000-automatic,0,0.123058,"Missing"
P08-1045,N06-1011,0,0.130978,"Missing"
P08-1045,P97-1017,1,0.733098,"e-art is poor for 1 Hal Daum´e III University of Utah School of Computing 50 S Central Campus Drive Salt Lake City, UT 84112, USA me@hal3.name taken from NIST02-05 corpora Ref2 musicians such as Bach, Mozart, Chopin, Bethoven, Shuman, Rachmaninoff, Rafael and Brokoviev Ref3 composers including Bach, Mozart, Schopen, Beethoven, missing name Raphael, Rahmaniev and Brokofien Ref4 composers such as Bach, Mozart, missing name Beethoven, Schumann, Rachmaninov, Raphael and Prokofiev The task of transliterating names (independent of end-to-end MT) has received a significant amount of research, e.g., (Knight and Graehl, 1997; Chen et al., 1998; Al-Onaizan, 2002). One approach is to “sound out” words and create new, plausible targetlanguage spellings that preserve the sounds of the source-language name as much as possible. Another approach is to phonetically match source-language names against a large list of target-language words 389 Proceedings of ACL-08: HLT, pages 389–397, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics and phrases. Most of this work has been disconnected from end-to-end MT, a problem which we address head-on in this paper. The simplest way to integrate name ha"
P08-1045,P04-1021,0,0.537393,"Missing"
P08-1045,W01-1413,0,0.0606696,"Missing"
P08-1045,P07-1119,0,0.281956,"ortran. In 391 Transliterator This section describes how we transliterate Arabic words or phrases. Given a word such as ¬ñJJ K AÒkP or a phrase such as É J ¯@P  K P ñ Ó , we want to find the English transliteration for it. This is not just a romanization like rHmanynuf and murys rafyl for the examples above, but a properly spelled English name such as Rachmaninoff and Maurice Ravel. The transliteration result can contain several alternatives, e.g. RachmaninoffjRachmaninov. Unlike various generative approaches (Knight and Graehl, 1997; Stalls and Knight, 1998; Li et al., 2004; Matthews, 2007; Sherif and Kondrak, 2007; Kashani et al., 2007), we do not synthesize an English spelling from scratch, but rather find a translation in very large lists of English words (3.4 million) and phrases (47 million). We develop a similarity metric for Arabic and English words. Since matching against millions of candidates is computationally prohibitive, we store the English words and phrases in an index, such that given an Arabic word or phrase, we quickly retrieve a much smaller set of likely candidates and apply our similarity metric to that smaller list. We divide the task of transliteration into two steps: given an Ara"
P08-1045,P06-1010,0,0.284514,"Missing"
P08-1045,W98-1005,1,0.872525,"the correct translation Termoli, but also the incorrect Portran. In 391 Transliterator This section describes how we transliterate Arabic words or phrases. Given a word such as ¬ñJJ K AÒkP or a phrase such as É J ¯@P  K P ñ Ó , we want to find the English transliteration for it. This is not just a romanization like rHmanynuf and murys rafyl for the examples above, but a properly spelled English name such as Rachmaninoff and Maurice Ravel. The transliteration result can contain several alternatives, e.g. RachmaninoffjRachmaninov. Unlike various generative approaches (Knight and Graehl, 1997; Stalls and Knight, 1998; Li et al., 2004; Matthews, 2007; Sherif and Kondrak, 2007; Kashani et al., 2007), we do not synthesize an English spelling from scratch, but rather find a translation in very large lists of English words (3.4 million) and phrases (47 million). We develop a similarity metric for Arabic and English words. Since matching against millions of candidates is computationally prohibitive, we store the English words and phrases in an index, such that given an Arabic word or phrase, we quickly retrieve a much smaller set of likely candidates and apply our similarity metric to that smaller list. We divi"
P08-1045,P98-2220,0,0.34804,"Missing"
P08-1045,W02-2017,0,\N,Missing
P08-1045,W02-0505,1,\N,Missing
P08-1045,C98-1036,0,\N,Missing
P09-1057,C04-1080,0,0.533736,"Missing"
P09-1057,W02-0603,0,0.0430739,"Missing"
P09-1057,P08-1085,0,0.518174,"nknown Words The literature also includes results reported in a different setting for the tagging problem. In some scenarios, a complete dictionary with entries for all word types may not be readily available to us and instead, we might be provided with an incomplete dictionary that contains entries for only frequent word types. In such cases, any word not appearing in the dictionary will be treated as an unknown word, and can be labeled with any of the tags from given tagset (i.e., for every unknown word, there are 17 tag possibilities). Some previous approaches (Toutanova and Johnson, 2008; Goldberg et al., 2008) handle unknown words explicitly using ambiguity class components conditioned on various morphological features, and this has shown to produce good tagging results, especially when dealing with incomplete dictionaries. We follow a simple approach using just one of the features used in (Toutanova and Johnson, 2008) for assigning tag possibilities to every unknown word. We first identify the top-100 suffixes (up to 3 characters) for words in the dictionary. Using the word/tag pairs from the dictionary, we train a simple probabilistic model that predicts the 7 Discussion The method proposed in th"
P09-1057,J01-2001,0,0.0542802,"Missing"
P09-1057,P07-1094,0,0.736423,"Missing"
P09-1057,D07-1031,0,0.51444,"in the EM tagging output. We also look at things more globally. We investigate the Viterbi tag sequence generated by EM training and count how many distinct tag bigrams there are in that sequence. We call this the observed grammar size, and it is 915. That is, in tagging the 24,115 test tokens, EM uses 915 of the available 45 × 45 = 2025 tag bigrams.2 The advantage of the observed grammar size is that we What goes wrong with EM? We analyze the tag sequence output produced by EM and try to see where EM goes wrong. The overall POS tag distribution learnt by EM is relatively uniform, as noted by Johnson (2007), and it tends to assign equal number of tokens to each 2 We contrast observed size with the model size for the grammar, which we define as the number of P(t2 |t1 ) entries in EM’s trained tag model that exceed 0.0001 probability. 505 training text IP formulation they START g1 PRO-AUX g2 PRO-V g3 AUX-N g4 AUX-V g5 V-N g6 V-V g7 N-PUNC g8 V-PUNC g9 PUNC-PRO g10 PRO-N fish . fish L1 L10 L2 AUX L3 L9 L4 dictionary variables I L0 PRO d1 PRO-they d2 AUX-can d3 V-can d4 N-fish d5 V-fish d6 PUNC-. d7 PRO-I can V L5 L7 L11 link variables L6 N L8 PUNC Integer Program Minimize: ∑i=1…10 gi Constraints: g"
P09-1057,J94-2001,0,0.86645,"ctionary entries are relevant to the 5,878 word types in the test set. Per-token ambiguity is about 1.5 tags/token, yielding approximately 106425 possible ways to tag the data. There are 45 distinct grammatical tags. In this set-up, there are no unknown words. Figure 1 shows prior results for this problem. While the methods are quite different, they all make use of two common model elements. One is a probabilistic n-gram tag model P(ti |ti−n+1 ...ti−1 ), which we call the grammar. The other is a probabilistic word-given-tag model P(wi |ti ), which we call the dictionary. The classic approach (Merialdo, 1994) is expectation-maximization (EM), where we estimate grammar and dictionary probabilities in order to maximize the probability of the observed word sequence: We describe a novel method for the task of unsupervised POS tagging with a dictionary, one that uses integer programming to explicitly search for the smallest model that explains the data, and then uses EM to set parameter values. We evaluate our method on a standard test corpus using different standard tagsets (a 45-tagset as well as a smaller 17-tagset), and show that our approach performs better than existing state-of-the-art systems i"
P09-1057,P05-1044,0,0.72365,"Missing"
P09-1064,N04-1022,0,0.861309,"n statistical machine translation, output translations are evaluated by their similarity to human reference translations, where similarity is most often measured by BLEU (Papineni et al., 2002). A decoding objective specifies how to derive final translations from a system’s underlying statistical model. The Bayes optimal decoding objective is to minimize risk based on the similarity measure used for evaluation. The corresponding minimum Bayes risk (MBR) procedure maximizes the expected similarity score of a system’s translations relative to the model’s distribution over possible translations (Kumar and Byrne, 2004). Unfortunately, with a non-linear similarity measure like BLEU, we must resort to approximating the expected loss using a k-best list, which accounts for only a tiny fraction of a model’s full posterior distribution. In this paper, we introduce a variant of the MBR decoding procedure that applies efficiently to translation forests. Instead of maximizing expected similarity, we express similarity in terms of features of sentences, and choose translations that are similar to expected feature values. 567 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 567"
P09-1064,W07-0734,0,0.0159683,"s original intent. One could imagine different feature-based expressions that also produce BLEU scores for real sentences, but produce different values for fractional features. Some care must be taken to define S(e; φ(e0 )) to extend naturally from integer-valued to real-valued features. Second, while any similarity measure can in principle be expressed as S(e; φ(e0 )) for a sufficiently rich feature space, fast consensus decoding will not apply effectively to all functions. For instance, we cannot naturally use functions that include alignments or matchings between e and e0 , such as METEOR (Agarwal and Lavie, 2007) and TER (Snover et al., 2006). Though these functions can in principle be expressed in terms of features of e0 (for instance with indicator features for whole sentences), fast consensus decoding will only be effective if different sentences share many features, so that the feature expectations effectively capture trends in the underlying distribution. 3 the ... telescope 0.6 “saw the” h = 0.4 · 1 + (0.6 · 1.0) · 1 Figure 2: This translation forest for a Spanish sentence encodes two English parse trees. Hyper-edges (boxes) are annotated with normalized transition probabilities, as well as the"
P09-1064,P09-1067,0,0.302909,"ttice or forest, whereas fast consensus decoding restricts this search to a k-best list. However, Tromble et al. (2008) showed that most of the improvement from lattice-based consensus decoding comes from lattice-based expectations, not search: searching over lattices instead of k-best lists did not change results for two language pairs, and improved a third language pair by 0.3 BLEU. Thus, we do not consider our use of k-best lists to be a substantial liability of our approach. Fast consensus decoding is also similar in character to the concurrently developed variational decoding approach of Li et al. (2009). Using BLEU, both approaches choose outputs that match expected n-gram counts from forests, though differ in the details. It is possible to define a similarity measure under which the two approaches are equivalent.5 Experimental Results We evaluate these consensus decoding techniques on two different full-scale state-of-the-art hierarchical machine translation systems. Both systems were trained for 2008 GALE evaluations, in which they outperformed a phrase-based system trained on identical data. 4.1 Hiero: a Hierarchical MT Pipeline Hiero is a hierarchical system that expresses its translatio"
P09-1064,D08-1024,1,0.377021,"r word-aligned sentence pairs provides rule frequency counts, which are normalized to estimate features on rules. The grammar rules of Hiero all share a single non-terminal symbol X, and have at most two non-terminals and six total items (non-terminals and lexical items), for example: my X2 ’s X1 → X1 de mi X2 We extracted the grammar from training data using standard parameters. Rules were allowed to span at most 15 words in the training data. The log-linear model weights were trained using MIRA, a margin-based optimization procedure that accommodates many features (Crammer and Singer, 2003; Chiang et al., 2008). In addition to standard rule frequency features, we included the distortion and syntactic features described in Chiang et al. (2008). 4.2 SBMT: a Syntax-Based MT Pipeline SBMT is a string-to-tree translation system with rich target-side syntactic information encoded in the translation model. The synchronous grammar rules are extracted from word aligned sentence pairs where the target sentence is annotated with a syntactic parse (Galley et al., 2004). Rules map source-side strings to target-side parse tree fragments, and non-terminal symbols correspond to target-side grammatical categories: ("
P09-1064,W06-1606,1,0.591075,"yntax-Based MT Pipeline SBMT is a string-to-tree translation system with rich target-side syntactic information encoded in the translation model. The synchronous grammar rules are extracted from word aligned sentence pairs where the target sentence is annotated with a syntactic parse (Galley et al., 2004). Rules map source-side strings to target-side parse tree fragments, and non-terminal symbols correspond to target-side grammatical categories: (NP (NP (PRP$ my) NN2 (POS ’s)) NNS1 ) → NNS1 de mi NN2 We extracted the grammar via an array of criteria (Galley et al., 2006; DeNeefe et al., 2007; Marcu et al., 2006). The model was trained using minimum error rate training for Arabic (Och, 2003) and MIRA for Chinese (Chiang et al., 2008). 5 For example, decoding under a variational approximation to the model’s posterior that decomposes over bigram probabilities is equivalent to fast consensus decoding with h ic(e,t) Q c(e0 ,t) the similarity measure B(e; e0 ) = t∈T2 c(e , 0 ,h(t)) where h(t) is the unigram prefix of bigram t. 572 Arabic-English Objective Hiero Min. Bayes Risk (Alg 1) 2h 47m 5m 49s Fast Consensus (Alg 3) Speed Ratio 29 Chinese-English Objective Hiero Min. Bayes Risk (Alg 1) 10h 24m Fast Co"
P09-1064,P08-1023,0,0.0827768,"r this reason, we propose a new objective that retains the benefits of MBR, but can be optimized efficiently, even for non-linear similarity measures. In experiments using BLEU over 1000best lists, we found that our objective provided benefits very similar to MBR, only much faster. This same decoding objective can also be computed efficiently from forest-based expectations. Translation forests compactly encode distributions over much larger sets of derivations and arise naturally in chart-based decoding for a wide variety of hierarchical translation systems (Chiang, 2007; Galley et al., 2006; Mi et al., 2008; Venugopal et al., 2007). The resulting forest-based decoding procedure compares favorably in both complexity and performance to the recently proposed latticebased MBR (Tromble et al., 2008). The contributions of this paper include a lineartime algorithm for MBR using linear similarities, a linear-time alternative to MBR using non-linear similarity measures, and a forest-based extension to this procedure for similarities based on n-gram counts. In experiments, we show that our fast procedure is on average 80 times faster than MBR using 1000-best lists. We also show that using forests outperfo"
P09-1064,P03-1021,0,0.0977732,"syntactic information encoded in the translation model. The synchronous grammar rules are extracted from word aligned sentence pairs where the target sentence is annotated with a syntactic parse (Galley et al., 2004). Rules map source-side strings to target-side parse tree fragments, and non-terminal symbols correspond to target-side grammatical categories: (NP (NP (PRP$ my) NN2 (POS ’s)) NNS1 ) → NNS1 de mi NN2 We extracted the grammar via an array of criteria (Galley et al., 2006; DeNeefe et al., 2007; Marcu et al., 2006). The model was trained using minimum error rate training for Arabic (Och, 2003) and MIRA for Chinese (Chiang et al., 2008). 5 For example, decoding under a variational approximation to the model’s posterior that decomposes over bigram probabilities is equivalent to fast consensus decoding with h ic(e,t) Q c(e0 ,t) the similarity measure B(e; e0 ) = t∈T2 c(e , 0 ,h(t)) where h(t) is the unigram prefix of bigram t. 572 Arabic-English Objective Hiero Min. Bayes Risk (Alg 1) 2h 47m 5m 49s Fast Consensus (Alg 3) Speed Ratio 29 Chinese-English Objective Hiero Min. Bayes Risk (Alg 1) 10h 24m Fast Consensus (Alg 3) 4m 52s Speed Ratio 128 Arabic-English Similarity Hiero 52.0 BLEU"
P09-1064,D07-1079,1,0.132411,"(2008). 4.2 SBMT: a Syntax-Based MT Pipeline SBMT is a string-to-tree translation system with rich target-side syntactic information encoded in the translation model. The synchronous grammar rules are extracted from word aligned sentence pairs where the target sentence is annotated with a syntactic parse (Galley et al., 2004). Rules map source-side strings to target-side parse tree fragments, and non-terminal symbols correspond to target-side grammatical categories: (NP (NP (PRP$ my) NN2 (POS ’s)) NNS1 ) → NNS1 de mi NN2 We extracted the grammar via an array of criteria (Galley et al., 2006; DeNeefe et al., 2007; Marcu et al., 2006). The model was trained using minimum error rate training for Arabic (Och, 2003) and MIRA for Chinese (Chiang et al., 2008). 5 For example, decoding under a variational approximation to the model’s posterior that decomposes over bigram probabilities is equivalent to fast consensus decoding with h ic(e,t) Q c(e0 ,t) the similarity measure B(e; e0 ) = t∈T2 c(e , 0 ,h(t)) where h(t) is the unigram prefix of bigram t. 572 Arabic-English Objective Hiero Min. Bayes Risk (Alg 1) 2h 47m 5m 49s Fast Consensus (Alg 3) Speed Ratio 29 Chinese-English Objective Hiero Min. Bayes Risk (A"
P09-1064,P02-1040,0,0.12146,"lists. Furthermore, our fast decoding procedure can select output sentences based on distributions over entire forests of translations, in addition to k-best lists. We evaluate our procedure on translation forests from two large-scale, state-of-the-art hierarchical machine translation systems. Our forest-based decoding objective consistently outperforms k-best list MBR, giving improvements of up to 1.0 BLEU. 1 Introduction In statistical machine translation, output translations are evaluated by their similarity to human reference translations, where similarity is most often measured by BLEU (Papineni et al., 2002). A decoding objective specifies how to derive final translations from a system’s underlying statistical model. The Bayes optimal decoding objective is to minimize risk based on the similarity measure used for evaluation. The corresponding minimum Bayes risk (MBR) procedure maximizes the expected similarity score of a system’s translations relative to the model’s distribution over possible translations (Kumar and Byrne, 2004). Unfortunately, with a non-linear similarity measure like BLEU, we must resort to approximating the expected loss using a k-best list, which accounts for only a tiny frac"
P09-1064,P07-2026,0,0.185314,"ind e∗ = arg maxe λ · θ(f, e). For MBR decoding, we instead leverage a similarity measure S(e; e0 ) to choose a translation using the model’s probability distribution P(e|f ), which has support over a set of possible translations E. The Viterbi derivation e∗ is the mode of this distribution. MBR is meant to choose a translation that will be similar, on expectation, to any possible reference translation. To this end, MBR chooses e˜ that maximizes expected similarity to the sentences in E under P(e|f ):1 We can sometimes exit the inner for loop early, whenever Ae can never become larger than A (Ehling et al., 2007). Even with this shortcut, the running time of Algorithm 1 is O(k 2 · n), where n is the maximum sentence length, assuming that S(e; e0 ) can be computed in O(n) time. 2.2 Minimum Bayes Risk over Features We now consider the case when S(e; e0 ) is a linear function of sentenceP features. Let S(e; e0 ) be a function of the form j ωj (e) · φj (e0 ), where φj (e0 ) are real-valued features of e0 , and ωj (e) are sentence-specific weights on those features. Then, the MBR objective can be re-written as   e˜ = arg maxe EP(e0 |f ) S(e; e0 ) X = arg maxe P(e0 |f ) · S(e; e0 ) e0 ∈E MBR can also be i"
P09-1064,D07-1014,0,0.0079072,"(e) · φj (e0 ), where φj (e0 ) are real-valued features of e0 , and ωj (e) are sentence-specific weights on those features. Then, the MBR objective can be re-written as   e˜ = arg maxe EP(e0 |f ) S(e; e0 ) X = arg maxe P(e0 |f ) · S(e; e0 ) e0 ∈E MBR can also be interpreted as a consensus decoding procedure: it chooses a translation similar to other high-posterior translations. Minimizing risk has been shown to improve performance for MT (Kumar and Byrne, 2004), as well as other language processing tasks (Goodman, 1996; Goel and Byrne, 2000; Kumar and Byrne, 2002; Titov and Henderson, 2006; Smith and Smith, 2007). The distribution P(e|f ) can be induced from a translation system’s features and weights by exponentiating with base b to form a log-linear model:   arg maxe∈E EP(e0 |f ) S(e; e0 ) = arg maxe X P (e0 |f ) · X e0 ∈E ωj (e) · φj (e0 ) j "" = arg maxe X j = arg maxe X ωj (e) # X 0 0 P (e |f ) · φj (e ) e0 ∈E   ωj (e) · EP(e0 |f ) φj (e0 ) . (1) j bλ·θ(f,e) λ·θ(f,e0 ) e0 ∈E b Equation 1 implies that we can find MBR translations by first computing all feature expectations, then applying S only once for each e. Algorithm 2 proceduralizes this idea: lines 1-4 compute feature expectations, and li"
P09-1064,N04-1035,1,0.103795,"g-linear model weights were trained using MIRA, a margin-based optimization procedure that accommodates many features (Crammer and Singer, 2003; Chiang et al., 2008). In addition to standard rule frequency features, we included the distortion and syntactic features described in Chiang et al. (2008). 4.2 SBMT: a Syntax-Based MT Pipeline SBMT is a string-to-tree translation system with rich target-side syntactic information encoded in the translation model. The synchronous grammar rules are extracted from word aligned sentence pairs where the target sentence is annotated with a syntactic parse (Galley et al., 2004). Rules map source-side strings to target-side parse tree fragments, and non-terminal symbols correspond to target-side grammatical categories: (NP (NP (PRP$ my) NN2 (POS ’s)) NNS1 ) → NNS1 de mi NN2 We extracted the grammar via an array of criteria (Galley et al., 2006; DeNeefe et al., 2007; Marcu et al., 2006). The model was trained using minimum error rate training for Arabic (Och, 2003) and MIRA for Chinese (Chiang et al., 2008). 5 For example, decoding under a variational approximation to the model’s posterior that decomposes over bigram probabilities is equivalent to fast consensus decod"
P09-1064,2006.amta-papers.25,0,0.0235326,"ne different feature-based expressions that also produce BLEU scores for real sentences, but produce different values for fractional features. Some care must be taken to define S(e; φ(e0 )) to extend naturally from integer-valued to real-valued features. Second, while any similarity measure can in principle be expressed as S(e; φ(e0 )) for a sufficiently rich feature space, fast consensus decoding will not apply effectively to all functions. For instance, we cannot naturally use functions that include alignments or matchings between e and e0 , such as METEOR (Agarwal and Lavie, 2007) and TER (Snover et al., 2006). Though these functions can in principle be expressed in terms of features of e0 (for instance with indicator features for whole sentences), fast consensus decoding will only be effective if different sentences share many features, so that the feature expectations effectively capture trends in the underlying distribution. 3 the ... telescope 0.6 “saw the” h = 0.4 · 1 + (0.6 · 1.0) · 1 Figure 2: This translation forest for a Spanish sentence encodes two English parse trees. Hyper-edges (boxes) are annotated with normalized transition probabilities, as well as the bigrams produced by each rule"
P09-1064,P06-1121,1,0.537803,"BR does not apply. For this reason, we propose a new objective that retains the benefits of MBR, but can be optimized efficiently, even for non-linear similarity measures. In experiments using BLEU over 1000best lists, we found that our objective provided benefits very similar to MBR, only much faster. This same decoding objective can also be computed efficiently from forest-based expectations. Translation forests compactly encode distributions over much larger sets of derivations and arise naturally in chart-based decoding for a wide variety of hierarchical translation systems (Chiang, 2007; Galley et al., 2006; Mi et al., 2008; Venugopal et al., 2007). The resulting forest-based decoding procedure compares favorably in both complexity and performance to the recently proposed latticebased MBR (Tromble et al., 2008). The contributions of this paper include a lineartime algorithm for MBR using linear similarities, a linear-time alternative to MBR using non-linear similarity measures, and a forest-based extension to this procedure for similarities based on n-gram counts. In experiments, we show that our fast procedure is on average 80 times faster than MBR using 1000-best lists. We also show that using"
P09-1064,W06-1666,0,0.0163604,"function of the form j ωj (e) · φj (e0 ), where φj (e0 ) are real-valued features of e0 , and ωj (e) are sentence-specific weights on those features. Then, the MBR objective can be re-written as   e˜ = arg maxe EP(e0 |f ) S(e; e0 ) X = arg maxe P(e0 |f ) · S(e; e0 ) e0 ∈E MBR can also be interpreted as a consensus decoding procedure: it chooses a translation similar to other high-posterior translations. Minimizing risk has been shown to improve performance for MT (Kumar and Byrne, 2004), as well as other language processing tasks (Goodman, 1996; Goel and Byrne, 2000; Kumar and Byrne, 2002; Titov and Henderson, 2006; Smith and Smith, 2007). The distribution P(e|f ) can be induced from a translation system’s features and weights by exponentiating with base b to form a log-linear model:   arg maxe∈E EP(e0 |f ) S(e; e0 ) = arg maxe X P (e0 |f ) · X e0 ∈E ωj (e) · φj (e0 ) j "" = arg maxe X j = arg maxe X ωj (e) # X 0 0 P (e |f ) · φj (e ) e0 ∈E   ωj (e) · EP(e0 |f ) φj (e0 ) . (1) j bλ·θ(f,e) λ·θ(f,e0 ) e0 ∈E b Equation 1 implies that we can find MBR translations by first computing all feature expectations, then applying S only once for each e. Algorithm 2 proceduralizes this idea: lines 1-4 compute feat"
P09-1064,D08-1065,0,0.317373,"University of California, Berkeley University of Southern California denero@cs.berkeley.edu {chiang, knight}@isi.edu Abstract Our exposition begins with algorithms over kbest lists. A na¨ıve algorithm for finding MBR translations computes the similarity between every pair of k sentences, entailing O(k 2 ) comparisons. We show that if the similarity measure is linear in features of a sentence, then computing expected similarity for all k sentences requires only k similarity evaluations. Specific instances of this general algorithm have recently been proposed for two linear similarity measures (Tromble et al., 2008; Zhang and Gildea, 2008). However, the sentence similarity measures we want to optimize in MT are not linear functions, and so this fast algorithm for MBR does not apply. For this reason, we propose a new objective that retains the benefits of MBR, but can be optimized efficiently, even for non-linear similarity measures. In experiments using BLEU over 1000best lists, we found that our objective provided benefits very similar to MBR, only much faster. This same decoding objective can also be computed efficiently from forest-based expectations. Translation forests compactly encode distribution"
P09-1064,P96-1024,0,0.036875,"a linear function of sentenceP features. Let S(e; e0 ) be a function of the form j ωj (e) · φj (e0 ), where φj (e0 ) are real-valued features of e0 , and ωj (e) are sentence-specific weights on those features. Then, the MBR objective can be re-written as   e˜ = arg maxe EP(e0 |f ) S(e; e0 ) X = arg maxe P(e0 |f ) · S(e; e0 ) e0 ∈E MBR can also be interpreted as a consensus decoding procedure: it chooses a translation similar to other high-posterior translations. Minimizing risk has been shown to improve performance for MT (Kumar and Byrne, 2004), as well as other language processing tasks (Goodman, 1996; Goel and Byrne, 2000; Kumar and Byrne, 2002; Titov and Henderson, 2006; Smith and Smith, 2007). The distribution P(e|f ) can be induced from a translation system’s features and weights by exponentiating with base b to form a log-linear model:   arg maxe∈E EP(e0 |f ) S(e; e0 ) = arg maxe X P (e0 |f ) · X e0 ∈E ωj (e) · φj (e0 ) j "" = arg maxe X j = arg maxe X ωj (e) # X 0 0 P (e |f ) · φj (e ) e0 ∈E   ωj (e) · EP(e0 |f ) φj (e0 ) . (1) j bλ·θ(f,e) λ·θ(f,e0 ) e0 ∈E b Equation 1 implies that we can find MBR translations by first computing all feature expectations, then applying S only once"
P09-1064,N07-1063,0,0.0177076,"propose a new objective that retains the benefits of MBR, but can be optimized efficiently, even for non-linear similarity measures. In experiments using BLEU over 1000best lists, we found that our objective provided benefits very similar to MBR, only much faster. This same decoding objective can also be computed efficiently from forest-based expectations. Translation forests compactly encode distributions over much larger sets of derivations and arise naturally in chart-based decoding for a wide variety of hierarchical translation systems (Chiang, 2007; Galley et al., 2006; Mi et al., 2008; Venugopal et al., 2007). The resulting forest-based decoding procedure compares favorably in both complexity and performance to the recently proposed latticebased MBR (Tromble et al., 2008). The contributions of this paper include a lineartime algorithm for MBR using linear similarities, a linear-time alternative to MBR using non-linear similarity measures, and a forest-based extension to this procedure for similarities based on n-gram counts. In experiments, we show that our fast procedure is on average 80 times faster than MBR using 1000-best lists. We also show that using forests outperforms using k-best lists co"
P09-1064,2006.amta-papers.8,1,0.218007,"tside algorithm, while the denominator is the inside score of the root node. Note that many possible derivations of f are pruned from the forest during decoding, and so this posterior is approximate. The expected n-gram count vector for a hyperedge is E[φ(h)] = P(h|f ) · φ(h). Hence, after computing P (h|f ) for every h, we need only sum P(h|f ) · φ(h) for all h to compute E[φ(e)]. This entire procedure is a linear-time computation in the number of hyper-edges in the forest. To complete forest-based fast consensus decoding, we then extract a k-best list of unique translations from the forest (Huang et al., 2006) and continue Algorithm 3 from line 5, which chooses the e˜ from the k-best list that maximizes BLEU(e; E[φ(e0 )]). 4 The log-BLEU function must be modified slightly to yield a linear Taylor approximation: Tromble et al. (2008) replace the clipped n-gram count with the product of an ngram count and an n-gram indicator function. 571 4 generated already by the decoder of a syntactic translation system. Second, rather than use BLEU as a sentencelevel similarity measure directly, Tromble et al. (2008) approximate corpus BLEU with G above. The parameters θ of the approximation must be estimated on"
P09-1064,P08-1025,0,0.390763,"ia, Berkeley University of Southern California denero@cs.berkeley.edu {chiang, knight}@isi.edu Abstract Our exposition begins with algorithms over kbest lists. A na¨ıve algorithm for finding MBR translations computes the similarity between every pair of k sentences, entailing O(k 2 ) comparisons. We show that if the similarity measure is linear in features of a sentence, then computing expected similarity for all k sentences requires only k similarity evaluations. Specific instances of this general algorithm have recently been proposed for two linear similarity measures (Tromble et al., 2008; Zhang and Gildea, 2008). However, the sentence similarity measures we want to optimize in MT are not linear functions, and so this fast algorithm for MBR does not apply. For this reason, we propose a new objective that retains the benefits of MBR, but can be optimized efficiently, even for non-linear similarity measures. In experiments using BLEU over 1000best lists, we found that our objective provided benefits very similar to MBR, only much faster. This same decoding objective can also be computed efficiently from forest-based expectations. Translation forests compactly encode distributions over much larger sets o"
P09-1064,P08-1067,0,0.0158513,"a, 2008; Tromble et al., 2008). In this section, we consider BLEU in particular, for which the relevant features φ(e) are n-gram counts up to length n = 4. We show how to compute expectations of these counts efficiently from translation forests. 3.1 Translation Forests Translation forests compactly encode an exponential number of output translations for an input sentence, along with their model scores. Forests arise naturally in chart-based decoding procedures for many hierarchical translation systems (Chiang, 2007). Exploiting forests has proven a fruitful avenue of research in both parsing (Huang, 2008) and machine translation (Mi et al., 2008). Formally, translation forests are weighted acyclic hyper-graphs. The nodes are states in the decoding process that include the span (i, j) of the sentence to be translated, the grammar symbol s over that span, and the left and right context words of the translation relevant for computing n-gram language model scores.3 Each hyper-edge h represents the application of a synchronous rule r that combines nodes corresponding to non-terminals in Computing Feature Expectations We now turn our focus to efficiently computing feature expectations, in service of"
P09-1064,W02-1019,0,0.0552059,"res. Let S(e; e0 ) be a function of the form j ωj (e) · φj (e0 ), where φj (e0 ) are real-valued features of e0 , and ωj (e) are sentence-specific weights on those features. Then, the MBR objective can be re-written as   e˜ = arg maxe EP(e0 |f ) S(e; e0 ) X = arg maxe P(e0 |f ) · S(e; e0 ) e0 ∈E MBR can also be interpreted as a consensus decoding procedure: it chooses a translation similar to other high-posterior translations. Minimizing risk has been shown to improve performance for MT (Kumar and Byrne, 2004), as well as other language processing tasks (Goodman, 1996; Goel and Byrne, 2000; Kumar and Byrne, 2002; Titov and Henderson, 2006; Smith and Smith, 2007). The distribution P(e|f ) can be induced from a translation system’s features and weights by exponentiating with base b to form a log-linear model:   arg maxe∈E EP(e0 |f ) S(e; e0 ) = arg maxe X P (e0 |f ) · X e0 ∈E ωj (e) · φj (e0 ) j "" = arg maxe X j = arg maxe X ωj (e) # X 0 0 P (e |f ) · φj (e ) e0 ∈E   ωj (e) · EP(e0 |f ) φj (e0 ) . (1) j bλ·θ(f,e) λ·θ(f,e0 ) e0 ∈E b Equation 1 implies that we can find MBR translations by first computing all feature expectations, then applying S only once for each e. Algorithm 2 proceduralizes this i"
P09-1064,D08-1022,0,\N,Missing
P09-1064,J07-2003,1,\N,Missing
P10-1051,W02-0603,0,0.0336295,"smallest grammar Gmin1 ⊂ G that explains the set of word bigram types observed in the data rather than the word sequence itself, and the second (Minimization 2) finds the smallest augmentation of Gmin1 that explains the full word sequence. Minimized models for supertagging The idea of searching for minimized models is related to classic Minimum Description Length (MDL) (Barron et al., 1998), which seeks to select a small model that captures the most regularity in the observed data. This modeling strategy has been shown to produce good results for many natural language tasks (Goldsmith, 2001; Creutz and Lagus, 2002; Ravi and Knight, 2009). For tagging, the idea has been implemented using Bayesian models with priors that indirectly induce sparsity in the learned models (Goldwater and Griffiths, 2007); however, Ravi and Knight (2009) show a better approach is to directly minimize the model using an integer programming (IP) formulation. Here, we build on this idea for supertagging. There are many challenges involved in using IP minimization for supertagging. The 1241 distinct supertags in the tagset result in 1.5 million tag bigram entries in the model and the dictionary contains almost 3.5 million word/ta"
P10-1051,D09-1123,0,0.0345392,"Missing"
P10-1051,P08-1085,0,0.0980955,"ctical applications. It has been explored at length in the literature since Merialdo (1994), though the task setting as usually defined in such experiments is somewhat artificial since the tag dictionaries are derived from tagged corpora. Nonetheless, the methods proposed apply to realistic scenarios in which one has an electronic part-of-speech tag dictionary or a hand-crafted grammar with limited coverage. Most work has focused on POS-tagging for English using the Penn Treebank (Marcus et al., 1993), such as (Banko and Moore, 2004; Goldwater and Griffiths, 2007; Toutanova and Johnson, 2008; Goldberg et al., 2008; Ravi and Knight, 2009). This generally involves working with the standard set of 45 POS-tags employed in the Penn Treebank. The most ambiguous word has 7 different POS tags associated with it. Most methods have employed some variant of Expectation Maximization (EM) to learn parameters for a bigram 1 See Banko and Moore (2004) for a description of how many early POS-tagging papers in fact used a number of heuristic cutoffs that greatly simplify the problem. 495 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 495–503, c Uppsala, Sweden, 11-16 July"
P10-1051,J01-2001,0,0.0134756,"tion 1) finds the smallest grammar Gmin1 ⊂ G that explains the set of word bigram types observed in the data rather than the word sequence itself, and the second (Minimization 2) finds the smallest augmentation of Gmin1 that explains the full word sequence. Minimized models for supertagging The idea of searching for minimized models is related to classic Minimum Description Length (MDL) (Barron et al., 1998), which seeks to select a small model that captures the most regularity in the observed data. This modeling strategy has been shown to produce good results for many natural language tasks (Goldsmith, 2001; Creutz and Lagus, 2002; Ravi and Knight, 2009). For tagging, the idea has been implemented using Bayesian models with priors that indirectly induce sparsity in the learned models (Goldwater and Griffiths, 2007); however, Ravi and Knight (2009) show a better approach is to directly minimize the model using an integer programming (IP) formulation. Here, we build on this idea for supertagging. There are many challenges involved in using IP minimization for supertagging. The 1241 distinct supertags in the tagset result in 1.5 million tag bigram entries in the model and the dictionary contains al"
P10-1051,P07-1094,0,0.338937,"ictionary and unlabeled data is an interesting task with practical applications. It has been explored at length in the literature since Merialdo (1994), though the task setting as usually defined in such experiments is somewhat artificial since the tag dictionaries are derived from tagged corpora. Nonetheless, the methods proposed apply to realistic scenarios in which one has an electronic part-of-speech tag dictionary or a hand-crafted grammar with limited coverage. Most work has focused on POS-tagging for English using the Penn Treebank (Marcus et al., 1993), such as (Banko and Moore, 2004; Goldwater and Griffiths, 2007; Toutanova and Johnson, 2008; Goldberg et al., 2008; Ravi and Knight, 2009). This generally involves working with the standard set of 45 POS-tags employed in the Penn Treebank. The most ambiguous word has 7 different POS tags associated with it. Most methods have employed some variant of Expectation Maximization (EM) to learn parameters for a bigram 1 See Banko and Moore (2004) for a description of how many early POS-tagging papers in fact used a number of heuristic cutoffs that greatly simplify the problem. 495 Proceedings of the 48th Annual Meeting of the Association for Computational Lingu"
P10-1051,J07-3004,0,0.125826,"e integer programming formulation that identifies minimal grammars efficiently and effectively. 2 NPAPER+CIVIL NPAPER CIVIL Max 126 Type ambig 1.69 Tok ambig 18.71 849 644 486 64 48 39 1.48 1.42 1.52 11.76 12.17 11.33 Table 1: Statistics for the training data used to extract lexicons for CCGbank and CCG-TUT. Distinct: # of distinct lexical categories; Max: # of categories for the most ambiguous word; Type ambig: per word type category ambiguity; Tok ambig: per word token category ambiguity. Data CCGbank. CCGbank was created by semiautomatically converting the Penn Treebank to CCG derivations (Hockenmaier and Steedman, 2007). We use the standard splits of the data used in semi-supervised tagging experiments (e.g. Banko and Moore (2004)): sections 0-18 for training, 19-21 for development, and 22-24 for test. trast, supertags are detailed, structured labels; a universal set of grammatical rules defines how categories may combine with one another to project syntactic structure.2 Because of this, properties of the CCG formalism itself can be used to constrain learning—prior to considering any particular language, grammar or data set. Baldridge (2008) uses this observation to create grammar-informed tag transitions fo"
P10-1051,J93-2004,0,0.0402447,"ng accurate part-of-speech (POS) taggers using a tag dictionary and unlabeled data is an interesting task with practical applications. It has been explored at length in the literature since Merialdo (1994), though the task setting as usually defined in such experiments is somewhat artificial since the tag dictionaries are derived from tagged corpora. Nonetheless, the methods proposed apply to realistic scenarios in which one has an electronic part-of-speech tag dictionary or a hand-crafted grammar with limited coverage. Most work has focused on POS-tagging for English using the Penn Treebank (Marcus et al., 1993), such as (Banko and Moore, 2004; Goldwater and Griffiths, 2007; Toutanova and Johnson, 2008; Goldberg et al., 2008; Ravi and Knight, 2009). This generally involves working with the standard set of 45 POS-tags employed in the Penn Treebank. The most ambiguous word has 7 different POS tags associated with it. Most methods have employed some variant of Expectation Maximization (EM) to learn parameters for a bigram 1 See Banko and Moore (2004) for a description of how many early POS-tagging papers in fact used a number of heuristic cutoffs that greatly simplify the problem. 495 Proceedings of the"
P10-1051,C08-1008,1,0.801717,"lly converting the Penn Treebank to CCG derivations (Hockenmaier and Steedman, 2007). We use the standard splits of the data used in semi-supervised tagging experiments (e.g. Banko and Moore (2004)): sections 0-18 for training, 19-21 for development, and 22-24 for test. trast, supertags are detailed, structured labels; a universal set of grammatical rules defines how categories may combine with one another to project syntactic structure.2 Because of this, properties of the CCG formalism itself can be used to constrain learning—prior to considering any particular language, grammar or data set. Baldridge (2008) uses this observation to create grammar-informed tag transitions for a bitag HMM supertagger based on two main properties. First, categories differ in their complexity and less complex categories tend to be used more frequently. For example, two categories for buy in CCGbank are (S[dcl]NP)/NP and ((((S[b]NP)/PP)/PP)/(S[adj]NP))/NP; the former occurs 33 times, the latter once. Second, categories indicate the form of categories found adjacent to them; for example, the category for sentential complement verbs ((SNP)/S) expects an NP to its left and an S to its right. Categories combine via r"
P10-1051,J94-2001,0,0.4331,"basic expectation-maximization training with a bitag Hidden Markov Model, which we show on the CCGbank and CCG-TUT corpora. The strategies provide further error reductions when combined. We describe a new two-stage integer programming strategy that efficiently deals with the high degree of ambiguity on these datasets while obtaining the full effect of model minimization. 1 Kevin Knight1 Introduction Creating accurate part-of-speech (POS) taggers using a tag dictionary and unlabeled data is an interesting task with practical applications. It has been explored at length in the literature since Merialdo (1994), though the task setting as usually defined in such experiments is somewhat artificial since the tag dictionaries are derived from tagged corpora. Nonetheless, the methods proposed apply to realistic scenarios in which one has an electronic part-of-speech tag dictionary or a hand-crafted grammar with limited coverage. Most work has focused on POS-tagging for English using the Penn Treebank (Marcus et al., 1993), such as (Banko and Moore, 2004; Goldwater and Griffiths, 2007; Toutanova and Johnson, 2008; Goldberg et al., 2008; Ravi and Knight, 2009). This generally involves working with the sta"
P10-1051,C04-1080,0,0.134526,") taggers using a tag dictionary and unlabeled data is an interesting task with practical applications. It has been explored at length in the literature since Merialdo (1994), though the task setting as usually defined in such experiments is somewhat artificial since the tag dictionaries are derived from tagged corpora. Nonetheless, the methods proposed apply to realistic scenarios in which one has an electronic part-of-speech tag dictionary or a hand-crafted grammar with limited coverage. Most work has focused on POS-tagging for English using the Penn Treebank (Marcus et al., 1993), such as (Banko and Moore, 2004; Goldwater and Griffiths, 2007; Toutanova and Johnson, 2008; Goldberg et al., 2008; Ravi and Knight, 2009). This generally involves working with the standard set of 45 POS-tags employed in the Penn Treebank. The most ambiguous word has 7 different POS tags associated with it. Most methods have employed some variant of Expectation Maximization (EM) to learn parameters for a bigram 1 See Banko and Moore (2004) for a description of how many early POS-tagging papers in fact used a number of heuristic cutoffs that greatly simplify the problem. 495 Proceedings of the 48th Annual Meeting of the Asso"
P10-1051,P09-1057,1,0.867693,"has been explored at length in the literature since Merialdo (1994), though the task setting as usually defined in such experiments is somewhat artificial since the tag dictionaries are derived from tagged corpora. Nonetheless, the methods proposed apply to realistic scenarios in which one has an electronic part-of-speech tag dictionary or a hand-crafted grammar with limited coverage. Most work has focused on POS-tagging for English using the Penn Treebank (Marcus et al., 1993), such as (Banko and Moore, 2004; Goldwater and Griffiths, 2007; Toutanova and Johnson, 2008; Goldberg et al., 2008; Ravi and Knight, 2009). This generally involves working with the standard set of 45 POS-tags employed in the Penn Treebank. The most ambiguous word has 7 different POS tags associated with it. Most methods have employed some variant of Expectation Maximization (EM) to learn parameters for a bigram 1 See Banko and Moore (2004) for a description of how many early POS-tagging papers in fact used a number of heuristic cutoffs that greatly simplify the problem. 495 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 495–503, c Uppsala, Sweden, 11-16 July 2010. 2010 Association"
P10-1051,N06-1019,0,0.249753,"Missing"
P10-1051,J07-4004,0,0.0952989,"Missing"
P10-1107,D07-1093,0,0.0965076,"Missing"
P10-1107,W97-0119,0,0.0342961,"on that is unknown in a typical deciphering scenario (while being readily available for living languages). For instance, some methods employ a hand-coded similarity function (Kondrak, 2001), while others assume knowledge of the phonetic mapping or require parallel cognate pairs to learn a similarity function (Bouchard et al., 2007). A second related line of work is lexicon induction from non-parallel corpora. While this research has similar goals, it typically builds on information or resources unavailable for ancient texts, such as comparable corpora, a seed lexicon, and cognate information (Fung and McKeown, 1997; Rapp, 1999; Koehn and Knight, 2002; Haghighi et al., 2008). Moreover, distributional methods that rely on co-occurrence analysis operate over large corpora, which are typically unavailable for a lost language. Finally, Knight and Yamada (1999) and Knight et al. (2006) describe a computational HMMbased method for deciphering an unknown script that represents a known spoken language. This method “makes the text speak” by gleaning character-to-sound mappings from non-parallel character and sound sequences. It does not relate words in different languages, thus it cannot encode deciphering constr"
P10-1107,P08-1088,0,0.136172,"being readily available for living languages). For instance, some methods employ a hand-coded similarity function (Kondrak, 2001), while others assume knowledge of the phonetic mapping or require parallel cognate pairs to learn a similarity function (Bouchard et al., 2007). A second related line of work is lexicon induction from non-parallel corpora. While this research has similar goals, it typically builds on information or resources unavailable for ancient texts, such as comparable corpora, a seed lexicon, and cognate information (Fung and McKeown, 1997; Rapp, 1999; Koehn and Knight, 2002; Haghighi et al., 2008). Moreover, distributional methods that rely on co-occurrence analysis operate over large corpora, which are typically unavailable for a lost language. Finally, Knight and Yamada (1999) and Knight et al. (2006) describe a computational HMMbased method for deciphering an unknown script that represents a known spoken language. This method “makes the text speak” by gleaning character-to-sound mappings from non-parallel character and sound sequences. It does not relate words in different languages, thus it cannot encode deciphering constraints similar to the ones considered in this paper. More imp"
P10-1107,P06-2065,1,0.810742,"pairs to learn a similarity function (Bouchard et al., 2007). A second related line of work is lexicon induction from non-parallel corpora. While this research has similar goals, it typically builds on information or resources unavailable for ancient texts, such as comparable corpora, a seed lexicon, and cognate information (Fung and McKeown, 1997; Rapp, 1999; Koehn and Knight, 2002; Haghighi et al., 2008). Moreover, distributional methods that rely on co-occurrence analysis operate over large corpora, which are typically unavailable for a lost language. Finally, Knight and Yamada (1999) and Knight et al. (2006) describe a computational HMMbased method for deciphering an unknown script that represents a known spoken language. This method “makes the text speak” by gleaning character-to-sound mappings from non-parallel character and sound sequences. It does not relate words in different languages, thus it cannot encode deciphering constraints similar to the ones considered in this paper. More importantly, this method had not been applied to archaeological data. While lost languages are gaining increasing interest in the NLP community (Knight and Sproat, 2009), there have been no successful attempts of"
P10-1107,W02-0902,1,0.558414,"phering scenario (while being readily available for living languages). For instance, some methods employ a hand-coded similarity function (Kondrak, 2001), while others assume knowledge of the phonetic mapping or require parallel cognate pairs to learn a similarity function (Bouchard et al., 2007). A second related line of work is lexicon induction from non-parallel corpora. While this research has similar goals, it typically builds on information or resources unavailable for ancient texts, such as comparable corpora, a seed lexicon, and cognate information (Fung and McKeown, 1997; Rapp, 1999; Koehn and Knight, 2002; Haghighi et al., 2008). Moreover, distributional methods that rely on co-occurrence analysis operate over large corpora, which are typically unavailable for a lost language. Finally, Knight and Yamada (1999) and Knight et al. (2006) describe a computational HMMbased method for deciphering an unknown script that represents a known spoken language. This method “makes the text speak” by gleaning character-to-sound mappings from non-parallel character and sound sequences. It does not relate words in different languages, thus it cannot encode deciphering constraints similar to the ones considered"
P10-1107,N01-1014,0,0.147769,"Missing"
P10-1107,J94-3004,0,0.226293,"Missing"
P10-1107,P99-1067,0,0.0646936,"typical deciphering scenario (while being readily available for living languages). For instance, some methods employ a hand-coded similarity function (Kondrak, 2001), while others assume knowledge of the phonetic mapping or require parallel cognate pairs to learn a similarity function (Bouchard et al., 2007). A second related line of work is lexicon induction from non-parallel corpora. While this research has similar goals, it typically builds on information or resources unavailable for ancient texts, such as comparable corpora, a seed lexicon, and cognate information (Fung and McKeown, 1997; Rapp, 1999; Koehn and Knight, 2002; Haghighi et al., 2008). Moreover, distributional methods that rely on co-occurrence analysis operate over large corpora, which are typically unavailable for a lost language. Finally, Knight and Yamada (1999) and Knight et al. (2006) describe a computational HMMbased method for deciphering an unknown script that represents a known spoken language. This method “makes the text speak” by gleaning character-to-sound mappings from non-parallel character and sound sequences. It does not relate words in different languages, thus it cannot encode deciphering constraints simila"
P10-1107,N09-4008,1,0.804974,"language. Finally, Knight and Yamada (1999) and Knight et al. (2006) describe a computational HMMbased method for deciphering an unknown script that represents a known spoken language. This method “makes the text speak” by gleaning character-to-sound mappings from non-parallel character and sound sequences. It does not relate words in different languages, thus it cannot encode deciphering constraints similar to the ones considered in this paper. More importantly, this method had not been applied to archaeological data. While lost languages are gaining increasing interest in the NLP community (Knight and Sproat, 2009), there have been no successful attempts of their automatic decipherment. 3 Background on Ugaritic Manual Decipherment of Ugaritic Ugaritic tablets were first found in Syria in 1929 (Smith, 1955; Watson and Wyatt, 1999). At the time, the cuneiform writing on the tablets was of an unknown type. Charles Virolleaud, who lead the initial decipherment effort, recognized that the script was likely alphabetic, since the inscribed words consisted of only thirty distinct symbols. The location of the tablets discovery further suggested that Ugaritic was likely to have been a Semitic language from the We"
P10-1107,W99-0906,1,0.648885,"g or require parallel cognate pairs to learn a similarity function (Bouchard et al., 2007). A second related line of work is lexicon induction from non-parallel corpora. While this research has similar goals, it typically builds on information or resources unavailable for ancient texts, such as comparable corpora, a seed lexicon, and cognate information (Fung and McKeown, 1997; Rapp, 1999; Koehn and Knight, 2002; Haghighi et al., 2008). Moreover, distributional methods that rely on co-occurrence analysis operate over large corpora, which are typically unavailable for a lost language. Finally, Knight and Yamada (1999) and Knight et al. (2006) describe a computational HMMbased method for deciphering an unknown script that represents a known spoken language. This method “makes the text speak” by gleaning character-to-sound mappings from non-parallel character and sound sequences. It does not relate words in different languages, thus it cannot encode deciphering constraints similar to the ones considered in this paper. More importantly, this method had not been applied to archaeological data. While lost languages are gaining increasing interest in the NLP community (Knight and Sproat, 2009), there have been n"
P10-1107,H01-1035,0,\N,Missing
P10-1108,J97-2003,0,0.260836,"hms for application of cascades of weighted tree transducers to weighted tree acceptors, connecting formal theory with actual practice. Additionally, we present novel on-the-fly variants of these algorithms, and compare their performance on a syntax machine translation cascade based on (Yamada and Knight, 2001). 1 • explicit algorithms for application of WTT cascades • novel algorithms for on-the-fly application of WTT cascades, and • experiments comparing the performance of these algorithms. 2 Motivation Weighted finite-state transducers have found recent favor as models of natural language (Mohri, 1997). In order to make actual use of systems built with these formalisms we must first calculate the set of possible weighted outputs allowed by the transducer given some input, which we call forward application, or the set of possible weighted inputs given some output, which we call backward application. After application we can do some inference on this result, such as determining its k highest weighted elements. We may also want to divide up our problems into manageable chunks, each represented by a transducer. As noted by Woods (1980), it is easier for designers to write several small transduc"
P10-1108,P09-1108,0,0.0890181,"t graph need not be built. A graphical representation of all three methods is presented in Figure 1. 3 Application of tree transducers Now let us revisit these strategies in the setting of trees and tree transducers. Imagine we have a tree or set of trees as input that can be represented as a weighted regular tree grammar3 (WRTG) and a WTT that can transform that input with some weight. We would like to know the k-best trees the WTT can produce as output for that input, along with their weights. We already know of several methods for acquiring k-best trees from a WRTG (Huang and Chiang, 2005; Pauls and Klein, 2009), so we then must ask if, analogously to the string case, WTTs preserve recognizability4 and we can form an application WRTG. Before we begin, however, we must define WTTs and WRTGs. 3.1 Preliminaries5 A ranked alphabet is a finite set Σ such that every member σ ∈ Σ has a rank rk(σ) ∈ N. We call Σ(k) ⊆ Σ, k ∈ N the set of those σ ∈ Σ such that rk(σ) = k. The set of variables is denoted X = {x1 , x2 , . . .} and is assumed to be disjoint from any ranked alphabet used in this paper. We use ⊥ to denote a symbol of rank 0 that is not in any ranked alphabet used in this paper. A tree t ∈ TΣ is deno"
P10-1108,J80-1001,0,0.74103,"cers have found recent favor as models of natural language (Mohri, 1997). In order to make actual use of systems built with these formalisms we must first calculate the set of possible weighted outputs allowed by the transducer given some input, which we call forward application, or the set of possible weighted inputs given some output, which we call backward application. After application we can do some inference on this result, such as determining its k highest weighted elements. We may also want to divide up our problems into manageable chunks, each represented by a transducer. As noted by Woods (1980), it is easier for designers to write several small transducers where each performs a simple transformation, rather than painstakingly construct a single complicated device. We would like to know, then, the result of transformation of input or output by a cascade of transducers, one operating after the other. As we will see, there are various strategies for approaching this problem. We will consider offline composition, bucket brigade application, and on-the-fly application. Application of cascades of weighted string transducers (WSTs) has been well-studied (Mohri, Strategies for the string ca"
P10-1108,W05-1506,0,0.0396786,"in that the entire result graph need not be built. A graphical representation of all three methods is presented in Figure 1. 3 Application of tree transducers Now let us revisit these strategies in the setting of trees and tree transducers. Imagine we have a tree or set of trees as input that can be represented as a weighted regular tree grammar3 (WRTG) and a WTT that can transform that input with some weight. We would like to know the k-best trees the WTT can produce as output for that input, along with their weights. We already know of several methods for acquiring k-best trees from a WRTG (Huang and Chiang, 2005; Pauls and Klein, 2009), so we then must ask if, analogously to the string case, WTTs preserve recognizability4 and we can form an application WRTG. Before we begin, however, we must define WTTs and WRTGs. 3.1 Preliminaries5 A ranked alphabet is a finite set Σ such that every member σ ∈ Σ has a rank rk(σ) ∈ N. We call Σ(k) ⊆ Σ, k ∈ N the set of those σ ∈ Σ such that rk(σ) = k. The set of variables is denoted X = {x1 , x2 , . . .} and is assumed to be disjoint from any ranked alphabet used in this paper. We use ⊥ to denote a symbol of rank 0 that is not in any ranked alphabet used in this pape"
P10-1108,P01-1067,1,0.821707,"n this work, presenting: Weighted tree transducers have been proposed as useful formal models for representing syntactic natural language processing applications, but there has been little description of inference algorithms for these automata beyond formal foundations. We give a detailed description of algorithms for application of cascades of weighted tree transducers to weighted tree acceptors, connecting formal theory with actual practice. Additionally, we present novel on-the-fly variants of these algorithms, and compare their performance on a syntax machine translation cascade based on (Yamada and Knight, 2001). 1 • explicit algorithms for application of WTT cascades • novel algorithms for on-the-fly application of WTT cascades, and • experiments comparing the performance of these algorithms. 2 Motivation Weighted finite-state transducers have found recent favor as models of natural language (Mohri, 1997). In order to make actual use of systems built with these formalisms we must first calculate the set of possible weighted outputs allowed by the transducer given some input, which we call forward application, or the set of possible weighted inputs given some output, which we call backward applicatio"
P11-1002,P09-1088,0,0.0105762,"ntext word type e (where cj 6= c). This yields a much smaller channel with size &lt; (2K + 1)2 . Retrain the new channel using EM algorithm. 4. Goto Step 2 and repeat the procedure, extending the channel size iteratively in each stage. Finally, we decode the given ciphertext c by using the Viterbi algorithm to choose the plaintext decoding e that maximizes P (e) · Pθtrained (c|e)3 , stretching the channel probabilities (Knight et al., 2006). 2.2 Bayesian Decipherment Bayesian inference methods have become popular in natural language processing (Goldwater and Griffiths, 2007; Finkel et al., 2005; Blunsom et al., 2009; Chiang et al., 2010; Snyder et al., 2010). These methods are attractive for their ability to manage uncertainty about model parameters and allow one to incorporate prior knowledge during inference. Here, we propose a novel decipherment approach using Bayesian learning. Our method holds several other advantages over the EM approach—(1) inference using smart sampling strategies permits efficient training, allowing us to scale to large data/vocabulary sizes, (2) incremental scoring of derivations during sampling allows efficient inference even when we use higher-order n-gram LMs, (3) there are"
P11-1002,J93-2003,0,0.110375,"@isi.edu Abstract In this work, we tackle the task of machine translation (MT) without parallel training data. We frame the MT problem as a decipherment task, treating the foreign text as a cipher for English and present novel methods for training translation models from nonparallel text. 1 Introduction Bilingual corpora are a staple of statistical machine translation (SMT) research. From these corpora, we estimate translation model parameters: wordto-word translation tables, fertilities, distortion parameters, phrase tables, syntactic transformations, etc. Starting with the classic IBM work (Brown et al., 1993), training has been viewed as a maximization problem involving hidden word alignments (a) that are assumed to underlie observed sentence pairs (e, f ): learned model to translate new foreign strings. As successful work develops along this line, we expect more domains and language pairs to be conquered by SMT. How can we learn a translation model from nonparallel data? Intuitively, we try to construct translation model tables which, when applied to observed foreign text, consistently yield sensible English. This is essentially the same approach taken by cryptanalysts and epigraphers when they d"
P11-1002,N10-1068,1,0.884511,"re cj 6= c). This yields a much smaller channel with size &lt; (2K + 1)2 . Retrain the new channel using EM algorithm. 4. Goto Step 2 and repeat the procedure, extending the channel size iteratively in each stage. Finally, we decode the given ciphertext c by using the Viterbi algorithm to choose the plaintext decoding e that maximizes P (e) · Pθtrained (c|e)3 , stretching the channel probabilities (Knight et al., 2006). 2.2 Bayesian Decipherment Bayesian inference methods have become popular in natural language processing (Goldwater and Griffiths, 2007; Finkel et al., 2005; Blunsom et al., 2009; Chiang et al., 2010; Snyder et al., 2010). These methods are attractive for their ability to manage uncertainty about model parameters and allow one to incorporate prior knowledge during inference. Here, we propose a novel decipherment approach using Bayesian learning. Our method holds several other advantages over the EM approach—(1) inference using smart sampling strategies permits efficient training, allowing us to scale to large data/vocabulary sizes, (2) incremental scoring of derivations during sampling allows efficient inference even when we use higher-order n-gram LMs, (3) there are no memory bottlenecks"
P11-1002,P05-1045,0,0.0019567,"ociated with the plaintext word type e (where cj 6= c). This yields a much smaller channel with size &lt; (2K + 1)2 . Retrain the new channel using EM algorithm. 4. Goto Step 2 and repeat the procedure, extending the channel size iteratively in each stage. Finally, we decode the given ciphertext c by using the Viterbi algorithm to choose the plaintext decoding e that maximizes P (e) · Pθtrained (c|e)3 , stretching the channel probabilities (Knight et al., 2006). 2.2 Bayesian Decipherment Bayesian inference methods have become popular in natural language processing (Goldwater and Griffiths, 2007; Finkel et al., 2005; Blunsom et al., 2009; Chiang et al., 2010; Snyder et al., 2010). These methods are attractive for their ability to manage uncertainty about model parameters and allow one to incorporate prior knowledge during inference. Here, we propose a novel decipherment approach using Bayesian learning. Our method holds several other advantages over the EM approach—(1) inference using smart sampling strategies permits efficient training, allowing us to scale to large data/vocabulary sizes, (2) incremental scoring of derivations during sampling allows efficient inference even when we use higher-order n-gr"
P11-1002,W97-0119,0,0.0183403,"s decipherment work for solving simpler substitution/transposition ciphers (Bauer, 2006; Knight et al., 2006). We must keep in mind, however, that foreign language is a much more demanding code, involving highly nondeterministic mappings and very large substitution tables. The contributions of this paper are therefore: • We give first results for training a full translation model from non-parallel text, and we apply the model to translate previously-unseen text. This work is thus distinguished from prior work on extracting or augmenting partial lexicons using non-parallel corpora (Rapp, 1995; Fung and McKeown, 1997; Koehn and Knight, 2000; Haghighi et al., 2008). It also contrasts with self-training (McClosky et al., 2006), which requires a parallel seed and often does not engage in iterative maximization. • We develop novel methods to deal with largescale vocabularies inherent in MT problems. 2 Word Substitution Decipherment Before we tackle machine translation without parallel data, we first solve a simpler problem—word substitution decipherment. Here, we do not have to worry about hidden alignments since there is only one alignment. In a word substitution cipher, every word in the natural language (p"
P11-1002,P01-1030,1,0.22055,"Missing"
P11-1002,P07-1094,0,0.00959101,"all other parameters e → cj associated with the plaintext word type e (where cj 6= c). This yields a much smaller channel with size &lt; (2K + 1)2 . Retrain the new channel using EM algorithm. 4. Goto Step 2 and repeat the procedure, extending the channel size iteratively in each stage. Finally, we decode the given ciphertext c by using the Viterbi algorithm to choose the plaintext decoding e that maximizes P (e) · Pθtrained (c|e)3 , stretching the channel probabilities (Knight et al., 2006). 2.2 Bayesian Decipherment Bayesian inference methods have become popular in natural language processing (Goldwater and Griffiths, 2007; Finkel et al., 2005; Blunsom et al., 2009; Chiang et al., 2010; Snyder et al., 2010). These methods are attractive for their ability to manage uncertainty about model parameters and allow one to incorporate prior knowledge during inference. Here, we propose a novel decipherment approach using Bayesian learning. Our method holds several other advantages over the EM approach—(1) inference using smart sampling strategies permits efficient training, allowing us to scale to large data/vocabulary sizes, (2) incremental scoring of derivations during sampling allows efficient inference even when we"
P11-1002,P08-1088,0,0.46365,"tion/transposition ciphers (Bauer, 2006; Knight et al., 2006). We must keep in mind, however, that foreign language is a much more demanding code, involving highly nondeterministic mappings and very large substitution tables. The contributions of this paper are therefore: • We give first results for training a full translation model from non-parallel text, and we apply the model to translate previously-unseen text. This work is thus distinguished from prior work on extracting or augmenting partial lexicons using non-parallel corpora (Rapp, 1995; Fung and McKeown, 1997; Koehn and Knight, 2000; Haghighi et al., 2008). It also contrasts with self-training (McClosky et al., 2006), which requires a parallel seed and often does not engage in iterative maximization. • We develop novel methods to deal with largescale vocabularies inherent in MT problems. 2 Word Substitution Decipherment Before we tackle machine translation without parallel data, we first solve a simpler problem—word substitution decipherment. Here, we do not have to worry about hidden alignments since there is only one alignment. In a word substitution cipher, every word in the natural language (plaintext) sequence is substituted by a cipher to"
P11-1002,knight-al-onaizan-1998-translation,1,0.158118,"1128 unique), 62k word tokens, 411 word types. TEST (Spanish): 13181 sentences (1127 unique), 39k word tokens, 562 word types. Both Spanish/English sides of TRAIN are used for parallel MT training, whereas decipherment uses only monolingual English data for training LMs. MT Systems: We build and compare different MT systems under two training scenarios: 1. Parallel training using: (a) MOSES, a phrase translation system (Koehn et al., 2007) widely used in MT literature, and (b) a simpler version of IBM Model 3 (without distortion parameters) which can be trained tractably using the strategy of Knight and Al-Onaizan (1998). 2. Decipherment without parallel data using: (a) EM method (from Section 3.1), and (b) Bayesian method (from Section 3.2). Evaluation: All the MT systems are run on the Spanish test data and the quality of the resulting English translations are evaluated using two different measures—(1) Normalized edit distance score (Navarro, 2001),6 and (2) BLEU (Papineni et 6 When computing edit distance, we account for substitutions, insertions, deletions as well as local-swap edit operations required to convert a given English string into the (gold) reference translation. 19 al., 2002), a standard MT ev"
P11-1002,P06-2065,1,0.770651,"ages 12–21, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics same full translation model. We note that for each f , not only is the alignment a still hidden, but now the English translation e is hidden as well. A language model P (e) is typically used in SMT decoding (Koehn, 2009), but here P (e) actually plays a central role in training translation model parameters. To distinguish the two, we refer to (5) as decipherment, rather than decoding. We can now draw on previous decipherment work for solving simpler substitution/transposition ciphers (Bauer, 2006; Knight et al., 2006). We must keep in mind, however, that foreign language is a much more demanding code, involving highly nondeterministic mappings and very large substitution tables. The contributions of this paper are therefore: • We give first results for training a full translation model from non-parallel text, and we apply the model to translate previously-unseen text. This work is thus distinguished from prior work on extracting or augmenting partial lexicons using non-parallel corpora (Rapp, 1995; Fung and McKeown, 1997; Koehn and Knight, 2000; Haghighi et al., 2008). It also contrasts with self-training"
P11-1002,P09-5002,1,0.730568,"nts, we get: arg max θ YX f e P (e) · X Pθ (f, a|e) (5) a Note that this formula has the same free Pθ (f, a|e) parameters as expression (2). We seek to manipulate these parameters in order to learn the 12 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 12–21, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics same full translation model. We note that for each f , not only is the alignment a still hidden, but now the English translation e is hidden as well. A language model P (e) is typically used in SMT decoding (Koehn, 2009), but here P (e) actually plays a central role in training translation model parameters. To distinguish the two, we refer to (5) as decipherment, rather than decoding. We can now draw on previous decipherment work for solving simpler substitution/transposition ciphers (Bauer, 2006; Knight et al., 2006). We must keep in mind, however, that foreign language is a much more demanding code, involving highly nondeterministic mappings and very large substitution tables. The contributions of this paper are therefore: • We give first results for training a full translation model from non-parallel text,"
P11-1002,N06-1020,0,0.0293911,"We must keep in mind, however, that foreign language is a much more demanding code, involving highly nondeterministic mappings and very large substitution tables. The contributions of this paper are therefore: • We give first results for training a full translation model from non-parallel text, and we apply the model to translate previously-unseen text. This work is thus distinguished from prior work on extracting or augmenting partial lexicons using non-parallel corpora (Rapp, 1995; Fung and McKeown, 1997; Koehn and Knight, 2000; Haghighi et al., 2008). It also contrasts with self-training (McClosky et al., 2006), which requires a parallel seed and often does not engage in iterative maximization. • We develop novel methods to deal with largescale vocabularies inherent in MT problems. 2 Word Substitution Decipherment Before we tackle machine translation without parallel data, we first solve a simpler problem—word substitution decipherment. Here, we do not have to worry about hidden alignments since there is only one alignment. In a word substitution cipher, every word in the natural language (plaintext) sequence is substituted by a cipher token, according to a substitution key. The key is deterministic"
P11-1002,P02-1040,0,0.0843365,"Missing"
P11-1002,P95-1050,0,0.62072,"w on previous decipherment work for solving simpler substitution/transposition ciphers (Bauer, 2006; Knight et al., 2006). We must keep in mind, however, that foreign language is a much more demanding code, involving highly nondeterministic mappings and very large substitution tables. The contributions of this paper are therefore: • We give first results for training a full translation model from non-parallel text, and we apply the model to translate previously-unseen text. This work is thus distinguished from prior work on extracting or augmenting partial lexicons using non-parallel corpora (Rapp, 1995; Fung and McKeown, 1997; Koehn and Knight, 2000; Haghighi et al., 2008). It also contrasts with self-training (McClosky et al., 2006), which requires a parallel seed and often does not engage in iterative maximization. • We develop novel methods to deal with largescale vocabularies inherent in MT problems. 2 Word Substitution Decipherment Before we tackle machine translation without parallel data, we first solve a simpler problem—word substitution decipherment. Here, we do not have to worry about hidden alignments since there is only one alignment. In a word substitution cipher, every word in"
P11-1002,P10-1107,1,0.522377,"lds a much smaller channel with size &lt; (2K + 1)2 . Retrain the new channel using EM algorithm. 4. Goto Step 2 and repeat the procedure, extending the channel size iteratively in each stage. Finally, we decode the given ciphertext c by using the Viterbi algorithm to choose the plaintext decoding e that maximizes P (e) · Pθtrained (c|e)3 , stretching the channel probabilities (Knight et al., 2006). 2.2 Bayesian Decipherment Bayesian inference methods have become popular in natural language processing (Goldwater and Griffiths, 2007; Finkel et al., 2005; Blunsom et al., 2009; Chiang et al., 2010; Snyder et al., 2010). These methods are attractive for their ability to manage uncertainty about model parameters and allow one to incorporate prior knowledge during inference. Here, we propose a novel decipherment approach using Bayesian learning. Our method holds several other advantages over the EM approach—(1) inference using smart sampling strategies permits efficient training, allowing us to scale to large data/vocabulary sizes, (2) incremental scoring of derivations during sampling allows efficient inference even when we use higher-order n-gram LMs, (3) there are no memory bottlenecks since the full channe"
P11-1002,1983.tc-1.13,0,0.830965,"Missing"
P11-1002,P07-2045,0,\N,Missing
P11-1025,P09-1088,0,0.0324409,"each plaintext letter pi with a ciphertext token ci , with probability P (ci |pi ) in order to generate the ciphertext sequence c = c1 ...cn . We build a statistical English language model (LM) for the plaintext source model P (p), which assigns a probability to any English letter sequence. Our goal is to estimate the channel model parameters θ in order to maximize the probability of the observed ciphertext c: X arg max P (c) = arg max Pθ (p, c) (1) θ θ = arg max Bayesian inference methods have become popular in natural language processing (Goldwater and Griffiths, 2007; Finkel et al., 2005; Blunsom et al., 2009; Chiang et al., 2010). Snyder et al. (2010) proposed a Bayesian approach in an archaeological decipherment scenario. These methods are attractive for their ability to manage uncertainty about model parameters and allow one to incorporate prior knowledge during inference. A common phenomenon observed while modeling natural language problems is sparsity. For simple letter substitution ciphers, the original substitution key exhibits a 1-to-1 correspondence between the plaintext letters and cipher types. It is not easy to model such information using conventional methods like EM. But we can easil"
P11-1025,N10-1068,1,0.811065,"pi with a ciphertext token ci , with probability P (ci |pi ) in order to generate the ciphertext sequence c = c1 ...cn . We build a statistical English language model (LM) for the plaintext source model P (p), which assigns a probability to any English letter sequence. Our goal is to estimate the channel model parameters θ in order to maximize the probability of the observed ciphertext c: X arg max P (c) = arg max Pθ (p, c) (1) θ θ = arg max Bayesian inference methods have become popular in natural language processing (Goldwater and Griffiths, 2007; Finkel et al., 2005; Blunsom et al., 2009; Chiang et al., 2010). Snyder et al. (2010) proposed a Bayesian approach in an archaeological decipherment scenario. These methods are attractive for their ability to manage uncertainty about model parameters and allow one to incorporate prior knowledge during inference. A common phenomenon observed while modeling natural language problems is sparsity. For simple letter substitution ciphers, the original substitution key exhibits a 1-to-1 correspondence between the plaintext letters and cipher types. It is not easy to model such information using conventional methods like EM. But we can easily specify priors that"
P11-1025,P10-1106,0,0.543437,"Missing"
P11-1025,P05-1045,0,0.0135964,"P (p). 2. Substitute each plaintext letter pi with a ciphertext token ci , with probability P (ci |pi ) in order to generate the ciphertext sequence c = c1 ...cn . We build a statistical English language model (LM) for the plaintext source model P (p), which assigns a probability to any English letter sequence. Our goal is to estimate the channel model parameters θ in order to maximize the probability of the observed ciphertext c: X arg max P (c) = arg max Pθ (p, c) (1) θ θ = arg max Bayesian inference methods have become popular in natural language processing (Goldwater and Griffiths, 2007; Finkel et al., 2005; Blunsom et al., 2009; Chiang et al., 2010). Snyder et al. (2010) proposed a Bayesian approach in an archaeological decipherment scenario. These methods are attractive for their ability to manage uncertainty about model parameters and allow one to incorporate prior knowledge during inference. A common phenomenon observed while modeling natural language problems is sparsity. For simple letter substitution ciphers, the original substitution key exhibits a 1-to-1 correspondence between the plaintext letters and cipher types. It is not easy to model such information using conventional methods lik"
P11-1025,P07-1094,0,0.0864767,"p = p1 ...pn , with probability P (p). 2. Substitute each plaintext letter pi with a ciphertext token ci , with probability P (ci |pi ) in order to generate the ciphertext sequence c = c1 ...cn . We build a statistical English language model (LM) for the plaintext source model P (p), which assigns a probability to any English letter sequence. Our goal is to estimate the channel model parameters θ in order to maximize the probability of the observed ciphertext c: X arg max P (c) = arg max Pθ (p, c) (1) θ θ = arg max Bayesian inference methods have become popular in natural language processing (Goldwater and Griffiths, 2007; Finkel et al., 2005; Blunsom et al., 2009; Chiang et al., 2010). Snyder et al. (2010) proposed a Bayesian approach in an archaeological decipherment scenario. These methods are attractive for their ability to manage uncertainty about model parameters and allow one to incorporate prior knowledge during inference. A common phenomenon observed while modeling natural language problems is sparsity. For simple letter substitution ciphers, the original substitution key exhibits a 1-to-1 correspondence between the plaintext letters and cipher types. It is not easy to model such information using con"
P11-1025,P06-2065,1,0.826387,"Missing"
P11-1025,D08-1085,1,0.676072,"Missing"
P11-1025,P10-1107,1,0.73913,"token ci , with probability P (ci |pi ) in order to generate the ciphertext sequence c = c1 ...cn . We build a statistical English language model (LM) for the plaintext source model P (p), which assigns a probability to any English letter sequence. Our goal is to estimate the channel model parameters θ in order to maximize the probability of the observed ciphertext c: X arg max P (c) = arg max Pθ (p, c) (1) θ θ = arg max Bayesian inference methods have become popular in natural language processing (Goldwater and Griffiths, 2007; Finkel et al., 2005; Blunsom et al., 2009; Chiang et al., 2010). Snyder et al. (2010) proposed a Bayesian approach in an archaeological decipherment scenario. These methods are attractive for their ability to manage uncertainty about model parameters and allow one to incorporate prior knowledge during inference. A common phenomenon observed while modeling natural language problems is sparsity. For simple letter substitution ciphers, the original substitution key exhibits a 1-to-1 correspondence between the plaintext letters and cipher types. It is not easy to model such information using conventional methods like EM. But we can easily specify priors that favor sparse distribut"
P11-1025,N10-1082,0,\N,Missing
P11-2014,P85-1034,0,0.867204,"assignments in lines of poetry under metrical constraints. Genzel et al. (2010) incorporate constraints on meter and rhyme (where the stress and rhyming information is derived from a pronunciation dictionary) into a machine translation system. Jiang and Zhou (2008) develop a system to generate the second line of a Chinese couplet given the first. A few researchers have also explored the problem of poetry generation under some constraints (Manurung et al., 2000; Netzer et al., 2009; Ramakrishnan et al., 2009). There has also been some work on computational approaches to characterizing rhymes (Byrd and Chodorow, 1985) and global properties of the rhyme network (Sonderegger, 2011) in English. To the best of our knowledge, there has been no language-independent computational work on finding rhyme schemes. 3 Finding Stanza Rhyme Schemes A collection of rhyming poetry inevitably contains repetition of rhyming pairs. For example, the word trees will often rhyme with breeze across different stanzas, even those with different rhyme schemes and written by different authors. This is partly due to sparsity of rhymes – many words that have no rhymes at all, and many others have only a handful, forcing poets to reuse"
P11-2014,D10-1016,0,0.244921,"and spelling conventions change over time. Words that rhymed historically may not anymore, like prove and love – or proued and beloued. 2 Related Work There have been a number of recent papers on the automated annotation, analysis, or translation of po77 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 77–82, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics etry. Greene et al. (2010) use a finite state transducer to infer the syllable-stress assignments in lines of poetry under metrical constraints. Genzel et al. (2010) incorporate constraints on meter and rhyme (where the stress and rhyming information is derived from a pronunciation dictionary) into a machine translation system. Jiang and Zhou (2008) develop a system to generate the second line of a Chinese couplet given the first. A few researchers have also explored the problem of poetry generation under some constraints (Manurung et al., 2000; Netzer et al., 2009; Ramakrishnan et al., 2009). There has also been some work on computational approaches to characterizing rhymes (Byrd and Chodorow, 1985) and global properties of the rhyme network (Sonderegger"
P11-2014,D10-1051,1,0.794167,"ltural and literary trends (partially spurred by projects like the Google Books Ngrams1 ). 1 http://ngrams.googlelabs.com/ • Pronunciations and spelling conventions change over time. Words that rhymed historically may not anymore, like prove and love – or proued and beloued. 2 Related Work There have been a number of recent papers on the automated annotation, analysis, or translation of po77 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 77–82, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics etry. Greene et al. (2010) use a finite state transducer to infer the syllable-stress assignments in lines of poetry under metrical constraints. Genzel et al. (2010) incorporate constraints on meter and rhyme (where the stress and rhyming information is derived from a pronunciation dictionary) into a machine translation system. Jiang and Zhou (2008) develop a system to generate the second line of a Chinese couplet given the first. A few researchers have also explored the problem of poetry generation under some constraints (Manurung et al., 2000; Netzer et al., 2009; Ramakrishnan et al., 2009). There has also been some"
P11-2014,C08-1048,0,0.0513272,"t papers on the automated annotation, analysis, or translation of po77 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 77–82, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics etry. Greene et al. (2010) use a finite state transducer to infer the syllable-stress assignments in lines of poetry under metrical constraints. Genzel et al. (2010) incorporate constraints on meter and rhyme (where the stress and rhyming information is derived from a pronunciation dictionary) into a machine translation system. Jiang and Zhou (2008) develop a system to generate the second line of a Chinese couplet given the first. A few researchers have also explored the problem of poetry generation under some constraints (Manurung et al., 2000; Netzer et al., 2009; Ramakrishnan et al., 2009). There has also been some work on computational approaches to characterizing rhymes (Byrd and Chodorow, 1985) and global properties of the rhyme network (Sonderegger, 2011) in English. To the best of our knowledge, there has been no language-independent computational work on finding rhyme schemes. 3 Finding Stanza Rhyme Schemes A collection of rhymi"
P11-2014,W09-2005,0,0.0660008,"2011 Association for Computational Linguistics etry. Greene et al. (2010) use a finite state transducer to infer the syllable-stress assignments in lines of poetry under metrical constraints. Genzel et al. (2010) incorporate constraints on meter and rhyme (where the stress and rhyming information is derived from a pronunciation dictionary) into a machine translation system. Jiang and Zhou (2008) develop a system to generate the second line of a Chinese couplet given the first. A few researchers have also explored the problem of poetry generation under some constraints (Manurung et al., 2000; Netzer et al., 2009; Ramakrishnan et al., 2009). There has also been some work on computational approaches to characterizing rhymes (Byrd and Chodorow, 1985) and global properties of the rhyme network (Sonderegger, 2011) in English. To the best of our knowledge, there has been no language-independent computational work on finding rhyme schemes. 3 Finding Stanza Rhyme Schemes A collection of rhyming poetry inevitably contains repetition of rhyming pairs. For example, the word trees will often rhyme with breeze across different stanzas, even those with different rhyme schemes and written by different authors. This"
P11-2014,W09-2006,0,0.105217,"Computational Linguistics etry. Greene et al. (2010) use a finite state transducer to infer the syllable-stress assignments in lines of poetry under metrical constraints. Genzel et al. (2010) incorporate constraints on meter and rhyme (where the stress and rhyming information is derived from a pronunciation dictionary) into a machine translation system. Jiang and Zhou (2008) develop a system to generate the second line of a Chinese couplet given the first. A few researchers have also explored the problem of poetry generation under some constraints (Manurung et al., 2000; Netzer et al., 2009; Ramakrishnan et al., 2009). There has also been some work on computational approaches to characterizing rhymes (Byrd and Chodorow, 1985) and global properties of the rhyme network (Sonderegger, 2011) in English. To the best of our knowledge, there has been no language-independent computational work on finding rhyme schemes. 3 Finding Stanza Rhyme Schemes A collection of rhyming poetry inevitably contains repetition of rhyming pairs. For example, the word trees will often rhyme with breeze across different stanzas, even those with different rhyme schemes and written by different authors. This is partly due to sparsity o"
P12-2016,P10-1106,0,0.0127236,"a substitution table as the secret key. The ciphertext is generated by substituting each letter of the plaintext according to the substitution table. The table may be homophonic; that is, a single plaintext letter could map to more than one possible ciphertext letter. Just as in running key ciphers, spaces in the plaintext are usually removed before encoding. Proposed decipherment solutions for letter substitution ciphers include techniques that use expectation maximization (Ravi and Knight, 2008), genetic algorithms (Oranchak, 2008), integer programming (Ravi and Knight, 2009), A* decoding (Corlett and Penn, 2010), and Bayesian learning with Dirichlet processes (Ravi and Knight, 2011). The main advantage of a sampling-based approach over Viterbi decoding is that it allows us to seamlessly use word-based language models. Lower order letter n-grams may fail to decipher most ciphertexts even with perfect search, since an incorrect plaintext and key could have higher likelihood under a weak language model than the actual message. 2.3 Vigen`ere Ciphers A scheme similar to the running key cipher is the Vigen`ere cipher, also known as the periodic key cipher. Instead of a single long string spanning the lengt"
P12-2016,D08-1085,1,0.651407,"ers 3.1 Previous work in decipherment of classical ciphers has mainly focused on letter substitution. These ciphers use a substitution table as the secret key. The ciphertext is generated by substituting each letter of the plaintext according to the substitution table. The table may be homophonic; that is, a single plaintext letter could map to more than one possible ciphertext letter. Just as in running key ciphers, spaces in the plaintext are usually removed before encoding. Proposed decipherment solutions for letter substitution ciphers include techniques that use expectation maximization (Ravi and Knight, 2008), genetic algorithms (Oranchak, 2008), integer programming (Ravi and Knight, 2009), A* decoding (Corlett and Penn, 2010), and Bayesian learning with Dirichlet processes (Ravi and Knight, 2011). The main advantage of a sampling-based approach over Viterbi decoding is that it allows us to seamlessly use word-based language models. Lower order letter n-grams may fail to decipher most ciphertexts even with perfect search, since an incorrect plaintext and key could have higher likelihood under a weak language model than the actual message. 2.3 Vigen`ere Ciphers A scheme similar to the running key c"
P12-2016,P11-1025,1,0.850768,"ubstituting each letter of the plaintext according to the substitution table. The table may be homophonic; that is, a single plaintext letter could map to more than one possible ciphertext letter. Just as in running key ciphers, spaces in the plaintext are usually removed before encoding. Proposed decipherment solutions for letter substitution ciphers include techniques that use expectation maximization (Ravi and Knight, 2008), genetic algorithms (Oranchak, 2008), integer programming (Ravi and Knight, 2009), A* decoding (Corlett and Penn, 2010), and Bayesian learning with Dirichlet processes (Ravi and Knight, 2011). The main advantage of a sampling-based approach over Viterbi decoding is that it allows us to seamlessly use word-based language models. Lower order letter n-grams may fail to decipher most ciphertexts even with perfect search, since an incorrect plaintext and key could have higher likelihood under a weak language model than the actual message. 2.3 Vigen`ere Ciphers A scheme similar to the running key cipher is the Vigen`ere cipher, also known as the periodic key cipher. Instead of a single long string spanning the length of the plaintext, the key is a short string – usually but not always a"
P13-1091,C12-1083,1,0.377003,"on of contextfree grammars, and some important implementation details, resulting in an algorithm that is practical for natural-language applications. The algorithm is part of Bolinas, a new software toolkit for HRG processing. 1 Introduction Hyperedge replacement grammar (HRG) is a context-free rewriting formalism for generating graphs (Drewes et al., 1997), and its synchronous counterpart can be used for transforming graphs to/from other graphs or trees. As such, it has great potential for applications in natural language understanding and generation, and semantics-based machine translation (Jones et al., 2012). Figure 1 shows some examples of graphs for naturallanguage semantics. A polynomial-time recognition algorithm for HRGs was described by Lautemann (1990), building on the work of Rozenberg and Welzl (1986) on boundary node label controlled grammars, and others have presented polynomial-time algorithms as well (Mazanek and Minas, 2008; Moot, 2008). Although Lautemann’s algorithm is correct and tractable, its presentation is prefaced with the remark: “As we are only interested in distinguishing polynomial time from non-polynomial time, the analysis will be rather crude, and implementation detai"
P13-1091,W08-2309,0,0.0254182,"7), and its synchronous counterpart can be used for transforming graphs to/from other graphs or trees. As such, it has great potential for applications in natural language understanding and generation, and semantics-based machine translation (Jones et al., 2012). Figure 1 shows some examples of graphs for naturallanguage semantics. A polynomial-time recognition algorithm for HRGs was described by Lautemann (1990), building on the work of Rozenberg and Welzl (1986) on boundary node label controlled grammars, and others have presented polynomial-time algorithms as well (Mazanek and Minas, 2008; Moot, 2008). Although Lautemann’s algorithm is correct and tractable, its presentation is prefaced with the remark: “As we are only interested in distinguishing polynomial time from non-polynomial time, the analysis will be rather crude, and implementation details will be explicated as little as possible.” Indeed, the key step of the algorithm, which matches a rule against the input graph, is described at a very high level, so that it is not obvious (for a non-expert in graph algorithms) how to implement it. More importantly, this step as described leads to a time complexity that is polynomial, but poten"
P13-1091,J11-1008,0,\N,Missing
P13-2131,W08-2227,0,0.0122743,"enerated AMRs. (Jones et al., 2012) use it to evaluate automatic semantic parsing in a narrow domain, while Ulf Hermjakob4 has developed a heuristic algorithm that exploits and supplements Ontonotes annotations (Pradhan et al., 2007) in order to automatically create AMRs for Ontonotes sentences, with a smatch score of 0.74 against human consensus AMRs. 5 1 Annotator A Annotator B Annotator C Annotator D 0.95 0.9 0.85 0.8 0.75 0.7 1 2 3 4 5 Time Figure 1: Smatch scores of annotators (A-D) against the consensus annotation (E) over time. ment between an input sentence and its semantic analysis. (Allen et al., 2008) propose a metric which computes the maximum score by any alignment between LF graphs, but they do not address how to determine the alignments. Related Work 6 Related work on directly measuring the semantic representation includes the method in (Dridan and Oepen, 2011), which evaluates semantic parser output directly by comparing semantic substructures, though they require an alignment between sentence spans and semantic sub-structures. In contrast, our metric does not require the align4 Smatch scores of each annotator against consensus Table 1: Inter-annotator smatch agreement for 5 groups of"
P13-2131,W11-2927,0,0.0517999,"Missing"
P13-2131,C12-1083,1,0.871298,"Missing"
P13-2131,kingsbury-palmer-2002-treebank,0,0.255543,"which range from 0 to 1 (“partial credit”) to measure whole-sentence semantic structures. By using such methods, we are able to differentiate between two similar wholesentence semantic structures regardless of specific 2 Semantic Overlap We work on a semantic feature structure representation in a standard neo-Davidsonian (Davidson, 1969; Parsons, 1990) framework. For example, semantics of the sentence “the boy wants to go” is represented by the following directed graph: In this graph, there are three concepts: want01, boy, and go-01. Both want-01 and go-01 are frames from PropBank framesets (Kingsbury and Palmer, 2002). The frame want-01 has two arguments connected with ARG0 and ARG1, and go01 has an argument (which is also the same boy instance) connected with ARG0. 748 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 748–752, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics of the second AMR against the first, R is its recall, and F is its f-score. The smatch score is the maximum of the f-scores. However, for AMRs that contain large number of variables, it is not efficient to get the f-score by simply using the method above. E"
P13-2131,P98-1116,1,0.53689,"Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics of the second AMR against the first, R is its recall, and F is its f-score. The smatch score is the maximum of the f-scores. However, for AMRs that contain large number of variables, it is not efficient to get the f-score by simply using the method above. Exhaustively enumerating all variable mappings requires computing the f-score for n!/(n − m)! variable mappings (assuming one AMR has n variables and the other has m variables, and m ≤ n). This algorithm is too slow for all but the shortest AMR pairs. Following (Langkilde and Knight, 1998) and (Langkilde-Geary, 2002), we refer to this semantic representation as AMR (Abstract Meaning Representation). Semantic relationships encoded in the AMR graph can also be viewed as a conjunction of logical propositions, or triples: instance(a, want-01) instance(b, boy) instance(c, go-01) ARG0(a, b) ARG1(a, c) ARG0(c, b) ∧ ∧ ∧ ∧ 3 ∧ This section describes how to compute the smatch score. As input, we are given AMR1 (with m variables) and AMR2 (with n variables). Without loss of generality, m ≤ n. Baseline. Our baseline first matches variables that share concepts. For example, it would match a"
P13-2131,W02-2103,0,0.0600238,"2013 Association for Computational Linguistics of the second AMR against the first, R is its recall, and F is its f-score. The smatch score is the maximum of the f-scores. However, for AMRs that contain large number of variables, it is not efficient to get the f-score by simply using the method above. Exhaustively enumerating all variable mappings requires computing the f-score for n!/(n − m)! variable mappings (assuming one AMR has n variables and the other has m variables, and m ≤ n). This algorithm is too slow for all but the shortest AMR pairs. Following (Langkilde and Knight, 1998) and (Langkilde-Geary, 2002), we refer to this semantic representation as AMR (Abstract Meaning Representation). Semantic relationships encoded in the AMR graph can also be viewed as a conjunction of logical propositions, or triples: instance(a, want-01) instance(b, boy) instance(c, go-01) ARG0(a, b) ARG1(a, c) ARG0(c, b) ∧ ∧ ∧ ∧ 3 ∧ This section describes how to compute the smatch score. As input, we are given AMR1 (with m variables) and AMR2 (with n variables). Without loss of generality, m ≤ n. Baseline. Our baseline first matches variables that share concepts. For example, it would match a in the first AMR example wi"
P13-2131,P02-1040,0,0.0870848,"Missing"
P13-2131,2006.amta-papers.25,0,0.0441851,"ves the number of triple matches. We experiment with two modifications to the greedy search: (1) executing multiple random restarts to avoid local optima, and (2) using our Baseline concept matching (“smart initialization”) instead of random initialization. NP-completeness. There is unlikely to be an exact polynomial-time algorithm for computing smatch. We can reduce the 0-1 Maximum Quadratic Assignment Problem (0-1-Max-QAP) (Nagarajan and Sviridenko, 2009) and the subgraph isomorphism problem directly to the full smatch problem on graphs.2 We note that other widely-used metrics, such as TER (Snover et al., 2006), are also NP-complete. Fortunately, the next section shows that the smatch methods above are efficient and effective. Table 1 shows smatch scores provided by the methods. Columns labeled 1-5 indicate sentence groups. Each individual smatch score is a document-level score of 4 AMR pairs.3 ILP scores are optimal, so lower scores (in bold) indicate search errors. Table 2 summarizes search accuracy as a percentage of smatch scores that equal that of ILP. Results show that the restarts are essential for hillclimbing, and that 9 restarts are sufficient to obtain good quality. The table also shows t"
P13-2131,C98-1112,1,\N,Missing
P14-2046,I08-8003,0,0.0195626,"and Viterbi alignments, such as: g ge 5.2 r r ae n uan uan d e m a d e r r ae n uan d de m m ah a dh d er e Second, we extract phoneme phrase pairs consistent with these alignments. We use no phrasesize limit, but we do not cross word boundaries. From the example above, we pull out phrase pairs like: g→g e g r→g e r ... r→r r ae n → r uan ... FSTs A, C, and D are unweighted, and remain so throughout this paper. 5.1 r Here, “ae n” should be decoded as “uan” when preceded by “r”. Following phrase-based methods in statistical machine translation (Koehn et al., 2003) and machine transliteration (Finch and Sumita, 2008), we model substitution of longer sequences. First, we obtain Viterbi alignments using the phoneme-based model, e.g.: to one or two Pinyin-split tokens, and it also allows two English sounds to map to one Pinyin-split token. Finally, FST C converts Pinyin-split into Pinyin, and FST D chooses Chinglish characters. We also experiment with an additional wFST E that translates English words directly into Chinglish. 5 e We add these phrase pairs to FST B, and call this the phoneme-phrase-based model. 5.3 Word-based model We now turn to WFST E, which short-cuts directly from English words to Pinyin."
P14-2046,N13-1072,0,0.0310329,"he system’s output should be both unambiguously pronounceable by the speaker and readily understood by the listener. Our goal is to build an application that covers many language pairs and directions. The current paper describes a single system that lets a Chinese person speak English. We take a statistical modeling approach to this problem, as is done in two lines of research that are most related. The first is machine transliteration (Knight and Graehl, 1998), in which names and technical terms are translated across languages with different sound systems. The other is respelling generation (Hauer and Kondrak, 2013), where an English speaker is given a phonetic hint about how to pronounce a rare or foreign word to another English speaker. By contrast, we aim English: Leave me alone. French: Laissez-moi tranquille. Franglish: Less-ay mwah trahn-KEEL. The user ignores the French and goes straight to the Franglish. If the Franglish is well designed, an English speaker can pronounce it and be understood by a French listener. Figure 1 shows a sample entry from another book—an English phrasebook for Chinese speakers. If a Chinese speaker wants to say “非 常 感 谢 你 这 顿 美 餐”, she need only read off the Chinglish “三"
P14-2046,N03-1017,0,0.0104914,"EM algorithm to learn FST B parameters (Table 3) and Viterbi alignments, such as: g ge 5.2 r r ae n uan uan d e m a d e r r ae n uan d de m m ah a dh d er e Second, we extract phoneme phrase pairs consistent with these alignments. We use no phrasesize limit, but we do not cross word boundaries. From the example above, we pull out phrase pairs like: g→g e g r→g e r ... r→r r ae n → r uan ... FSTs A, C, and D are unweighted, and remain so throughout this paper. 5.1 r Here, “ae n” should be decoded as “uan” when preceded by “r”. Following phrase-based methods in statistical machine translation (Koehn et al., 2003) and machine transliteration (Finch and Sumita, 2008), we model substitution of longer sequences. First, we obtain Viterbi alignments using the phoneme-based model, e.g.: to one or two Pinyin-split tokens, and it also allows two English sounds to map to one Pinyin-split token. Finally, FST C converts Pinyin-split into Pinyin, and FST D chooses Chinglish characters. We also experiment with an additional wFST E that translates English words directly into Chinglish. 5 e We add these phrase pairs to FST B, and call this the phoneme-phrase-based model. 5.3 Word-based model We now turn to WFST E, wh"
P14-2046,mcenery-xiao-2004-lancaster,0,0.0431329,"pular, because while consonant clusters like English “st” are impossible to reproduce exactly, the particular vowels in “si” and “te” are fortunately very weak. Evaluation Our system’s input is Chinese. The output is a string of Chinese characters that approximate English sounds, which we call Chinglish. We build several candidate Chinese-to-Chinglish systems and evaluate them as follows: Frequency Rank 1 2 3 4 5 • We compute the normalized edit distance between the system’s output and a humangenerated Chinglish reference. Chinglish si te de yi fu Table 2: Top 5 frequent syllables in Chinese (McEnery and Xiao, 2004) and Chinglish • A Chinese speaker pronounces the system’s output out loud, and an English listener takes dictation. We measure the normalized edit distance against an English reference. We find that multiple occurrences of an English word type are generally associated with the same Chinglish sequence. Also, Chinglish characters do not generally span multiple English words. It is reasonable for “can I” to be rendered as “kan nai”, with “nai” spanning both English words, but this is rare. • We automate the previous evaluation by replace the two humans with: (1) a Chinese speech synthesizer, and"
P14-2046,J98-4003,1,\N,Missing
P14-2115,P13-2041,0,0.0216675,"sformable, we substitute the part of e with t to form a new morph. For example, we can substitute the characters of “比尔 盖茨 (Bill Gates) [Bi Er Gai Ci]” with “鼻耳 (Nose and ear) [Bi Er]” and “盖子 (Lid) [Gai Zi]” to form new morph “鼻耳 盖子 (Nose and ear Lid) [Bi Er Gai Zi]”. We rank the candidates based on the following two criteria: (1) If the morph includes more negative words (based on a gazetteer including 11,729 negative words derived from HowNet (Dong and Dong, 1999), it’s more humorous (Valitutti et al., 2013). (2) If the morph includes rarer terms with low frequency, it is more interesting (Petrovic and Matthews, 2013). 2.3 2.5 M4: Translation and Transliteration Given an entity e, we search its English translation EN(e) based on 94,015 name translation pairs (Ji et al., 2009). Then, if any name component in EN(e) is a common English word, we search for its Chinese translation based on a 94,966 word translation pairs (Zens and Ney, 2004), and use the Chinese translation to replace the corresponding characters in e. For example, we create a morph “拉里 鸟儿 (Larry bird)” for “拉里 伯德 (Larry Bird)” by replacing the last name “伯德 (Bird)” with its Chinese translation “鸟儿 (bird)”. 2.6 M5: Semantic Interpretation For e"
P14-2115,P13-2044,0,0.0208414,") are mutually transformable. If a part of pinyin(e) and pinyin(t) are identical or their initials are transformable, we substitute the part of e with t to form a new morph. For example, we can substitute the characters of “比尔 盖茨 (Bill Gates) [Bi Er Gai Ci]” with “鼻耳 (Nose and ear) [Bi Er]” and “盖子 (Lid) [Gai Zi]” to form new morph “鼻耳 盖子 (Nose and ear Lid) [Bi Er Gai Zi]”. We rank the candidates based on the following two criteria: (1) If the morph includes more negative words (based on a gazetteer including 11,729 negative words derived from HowNet (Dong and Dong, 1999), it’s more humorous (Valitutti et al., 2013). (2) If the morph includes rarer terms with low frequency, it is more interesting (Petrovic and Matthews, 2013). 2.3 2.5 M4: Translation and Transliteration Given an entity e, we search its English translation EN(e) based on 94,015 name translation pairs (Ji et al., 2009). Then, if any name component in EN(e) is a common English word, we search for its Chinese translation based on a 94,966 word translation pairs (Zens and Ney, 2004), and use the Chinese translation to replace the corresponding characters in e. For example, we create a morph “拉里 鸟儿 (Larry bird)” for “拉里 伯德 (Larry Bird)” by rep"
P14-2115,P13-1107,1,0.575788,"ng morphs. One main purpose of encoding morphs is to disseminate them widely so they can become part of the new Internet language. Therefore morphs should be interesting, fun, intuitive and easy to remember. (5) Morphs rapidly evolve over time, as some morphs are discovered and blocked by censorship and newly created morphs emerge. We propose a brand new and challenging research problem - can we automatically encode morphs for any given entity to help users communicate in an appropriate and fun way? Introduction One of the most innovative linguistic forms in social media is Information Morph (Huang et al., 2013). Morph is a special case of alias to hide the original objects (e.g., sensitive entities and events) for different purposes, including avoiding censorship (Bamman et al., 2012; Chen et al., 2013), expressing strong sentiment, emotion or sarcasm, and making descriptions more vivid. Morphs are widely used in Chinese social media. Here is an example morphs: “由于瓜爹的事情，方便面与 天线摊牌. (Because of Gua Dad’s issue, Instant Noodles faces down with Antenna.)”, where • “瓜爹 (Gua Dad)” refers to “薄熙来 (Bo Xilai)” because it shares one character “瓜 (Gua)” with “薄瓜瓜 (Bo Guagua)” who is the son of “薄熙 来 (Bo Xilai)"
P14-2115,P13-1072,0,0.0337907,"Missing"
P14-2115,P11-1115,1,0.782334,"taring at sea and listening to surf)” as a present when he visited China. In the films Ma Jingtao starred, he always used exaggerated roaring to express various emotions. The morph derives from Ma Yingjiu’s political position on cross-strait relations. Table 2: Morph Examples Categorized based on Human Generation Methods based on their semantic contexts. For example, this approach generates a morph “太祖 (the First Emperor)” for “毛泽东 (Mao Zedong)” who is the first chairman of P. R. China and “高祖 (the Second Emperor )” for “邓小平 (Deng Xiaoping )” who succeeded Mao. 2.8 al., 2010; Ji et al., 2011; Ji and Grishman, 2011) which include 3 million news and web documents, and DARPA BOLT program’s discussion forum corpora with 300k threads. Given an entity e, we compute the semantic relationship between e and each word from these corpora. We then rank the words by: (1) cosine similarity, (2) the same criteria as in section 2.6. Finally we append the top ranking word to the entity’s last name to obtain a new morph. Using this method, we are able to generate many vivid morphs such as “姚 奇才 (Yao Wizard)” for “姚 明 (Yao Ming)”. M7: Characteristics Modeling Finally, we propose a novel approach to automatically generate"
P14-2115,I13-1015,0,0.0216269,"Missing"
P14-2115,W06-2808,0,0.0198146,"Missing"
P14-2115,I05-3013,0,0.0556604,"Missing"
P14-2115,P06-1125,0,0.0446686,"Missing"
P14-2115,N04-1033,0,0.0106403,"If the morph includes more negative words (based on a gazetteer including 11,729 negative words derived from HowNet (Dong and Dong, 1999), it’s more humorous (Valitutti et al., 2013). (2) If the morph includes rarer terms with low frequency, it is more interesting (Petrovic and Matthews, 2013). 2.3 2.5 M4: Translation and Transliteration Given an entity e, we search its English translation EN(e) based on 94,015 name translation pairs (Ji et al., 2009). Then, if any name component in EN(e) is a common English word, we search for its Chinese translation based on a 94,966 word translation pairs (Zens and Ney, 2004), and use the Chinese translation to replace the corresponding characters in e. For example, we create a morph “拉里 鸟儿 (Larry bird)” for “拉里 伯德 (Larry Bird)” by replacing the last name “伯德 (Bird)” with its Chinese translation “鸟儿 (bird)”. 2.6 M5: Semantic Interpretation For each character ck in the first name of a given entity name e, we search its semantic interpretation sentence from the Xinhua Chinese character dictionary including 20,894 entries 3 . If a word in the sentence contains ck , we append the word with the last name of e to form a new morph. Similarly to M1, we prefer positive, ne"
P14-2115,D08-1108,0,0.0445781,"Missing"
P14-2115,J98-4003,1,\N,Missing
P15-1057,W08-0336,0,0.0275193,"riginal meanings. 2 Problem Formulation Following the recent work on morphs (Huang et al., 2013; Zhang et al., 2014), we use Chinese Weibo tweets for experiments. Our goal is to develop an end-to-end system that automatically extract morph mentions and resolve them to their target entities. Given a corpus of tweets D = {d1 , d2 , ..., d|D |}, we define a candidate morph mi as a unique term tj in T , where T = {t1 , t2 , ..., t|T |} is the set of unique terms in D. To extract T , we first apply several well-developed Natural Language Processing tools, including Stanford Chinese word segmenter (Chang et al., 2008), Stanford part-ofspeech tagger (Toutanova et al., 2003) and Chinese lexical analyzer ICTCLAS (Zhang et al., 2003), to process the tweets and identify noun phrases. Then we define a morph mention mpi of mi as the p-th occurrence of mi in a specific document dj . Note that a mention with the same surface form as mi but referring to its original entity is not considered as a morph mention. For instance, the “平西 王 (Conquer West King)” in d1 and d3 in Figure 1 are morph mentions since they refer to the modern politician “薄熙来 (Bo Xilai)”, while the one in d4 is not a morph mention since it refers t"
P15-1057,P06-1017,0,0.020688,"Missing"
P15-1057,P13-2006,0,0.0430093,"Missing"
P15-1057,W13-0908,0,0.0277964,"d malicious software. In contrast our task tackles anomaly texts in semantic context. Deep learning-based approaches have been demonstrated to be effective in disambiguation related tasks such as WSD (Bordes et al., 2012), entity linking (He et al., 2013) and question linking (Yih et al., 2014; Bordes et al., 2014; Yang et al., 2014). In this paper we proved that it’s cruFigure 4: Resolution Acc@K for Perfect Morph Mentions NLP tasks: entity mention extraction (e.g., (Zitouni and Florian, 2008; Ohta et al., 2012; Li and Ji, 2014)), metaphor detection (e.g., (Wang et al., 2006; Tsvetkov, 2013; Heintz et al., 2013)), word sense disambiguation (WSD) (e.g., (Yarowsky, 1995; Mihalcea, 2007; Navigli, 2009)), and entity linking (EL) (e.g., (Mihalcea and Csomai, 2007; Ji et al., 2010; Ji et al., 2011; Ji et al., 2014). However, none of these previous techniques can be applied directly to tackle this problem. As mentioned in section 3.1, entity morphs are fundamentally different from regular entity mentions. Our task is also different from metaphor detection because morphs cover a much wider range of semantic categories and can include either abstractive or concrete information. Some common features for detect"
P15-1057,P13-1107,1,0.928563,"ecoding humangenerated morphs in text is critical for downstream deep language understanding tasks such as entity linking and event argument extraction. However, even for human, it is difficult to decode many morphs without certain historical, cultural, or political background knowledge (Zhang et al., 2014). For example, “The Hutt” can be used to refer to a fictional alien entity in the Star Wars universe (“The Hutt stayed and established himself as ruler of Nam Chorios”), or the governor of New Jersey, Chris Christie (“The Hutt announced a bid for a seat in the New Jersey General Assembly”). Huang et al. (2013) did a pioneering pilot study on morph resolution, but their approach assumed the entity morphs were already extracted and used a large amount of labeled data. In fact, they resolved morphs on corpus-level instead of mention-level and thus their approach was context-independent. A practical morph decoder, as depicted in Figure 1, consists of two problems: (1) Morph Extraction: given a corpus, extract morph mentions; and (2). Morph Resolution: For each morph mention, figure out the entity that it refers to. In this paper, we aim to solve the fundamental research problem of end-to-end morph deco"
P15-1057,P14-1036,1,0.900239,"Missing"
P15-1057,D14-1067,0,0.0149744,"(e.g., named entities), while morphs tend to be informal and convey implicit information. Morph mention detection is also related to malware detection (e.g., (Firdausi et al., 2010; Chandola et al., 2009; Firdausi et al., 2010; Christodorescu and Jha, 2003)) which discovers abnormal behavior in code and malicious software. In contrast our task tackles anomaly texts in semantic context. Deep learning-based approaches have been demonstrated to be effective in disambiguation related tasks such as WSD (Bordes et al., 2012), entity linking (He et al., 2013) and question linking (Yih et al., 2014; Bordes et al., 2014; Yang et al., 2014). In this paper we proved that it’s cruFigure 4: Resolution Acc@K for Perfect Morph Mentions NLP tasks: entity mention extraction (e.g., (Zitouni and Florian, 2008; Ohta et al., 2012; Li and Ji, 2014)), metaphor detection (e.g., (Wang et al., 2006; Tsvetkov, 2013; Heintz et al., 2013)), word sense disambiguation (WSD) (e.g., (Yarowsky, 1995; Mihalcea, 2007; Navigli, 2009)), and entity linking (EL) (e.g., (Mihalcea and Csomai, 2007; Ji et al., 2010; Ji et al., 2011; Ji et al., 2014). However, none of these previous techniques can be applied directly to tackle this problem. A"
P15-1057,W13-0906,0,0.0253293,"avior in code and malicious software. In contrast our task tackles anomaly texts in semantic context. Deep learning-based approaches have been demonstrated to be effective in disambiguation related tasks such as WSD (Bordes et al., 2012), entity linking (He et al., 2013) and question linking (Yih et al., 2014; Bordes et al., 2014; Yang et al., 2014). In this paper we proved that it’s cruFigure 4: Resolution Acc@K for Perfect Morph Mentions NLP tasks: entity mention extraction (e.g., (Zitouni and Florian, 2008; Ohta et al., 2012; Li and Ji, 2014)), metaphor detection (e.g., (Wang et al., 2006; Tsvetkov, 2013; Heintz et al., 2013)), word sense disambiguation (WSD) (e.g., (Yarowsky, 1995; Mihalcea, 2007; Navigli, 2009)), and entity linking (EL) (e.g., (Mihalcea and Csomai, 2007; Ji et al., 2010; Ji et al., 2011; Ji et al., 2014). However, none of these previous techniques can be applied directly to tackle this problem. As mentioned in section 3.1, entity morphs are fundamentally different from regular entity mentions. Our task is also different from metaphor detection because morphs cover a much wider range of semantic categories and can include either abstractive or concrete information. Some comm"
P15-1057,P14-1038,1,0.800559,"2010; Christodorescu and Jha, 2003)) which discovers abnormal behavior in code and malicious software. In contrast our task tackles anomaly texts in semantic context. Deep learning-based approaches have been demonstrated to be effective in disambiguation related tasks such as WSD (Bordes et al., 2012), entity linking (He et al., 2013) and question linking (Yih et al., 2014; Bordes et al., 2014; Yang et al., 2014). In this paper we proved that it’s cruFigure 4: Resolution Acc@K for Perfect Morph Mentions NLP tasks: entity mention extraction (e.g., (Zitouni and Florian, 2008; Ohta et al., 2012; Li and Ji, 2014)), metaphor detection (e.g., (Wang et al., 2006; Tsvetkov, 2013; Heintz et al., 2013)), word sense disambiguation (WSD) (e.g., (Yarowsky, 1995; Mihalcea, 2007; Navigli, 2009)), and entity linking (EL) (e.g., (Mihalcea and Csomai, 2007; Ji et al., 2010; Ji et al., 2011; Ji et al., 2014). However, none of these previous techniques can be applied directly to tackle this problem. As mentioned in section 3.1, entity morphs are fundamentally different from regular entity mentions. Our task is also different from metaphor detection because morphs cover a much wider range of semantic categories and ca"
P15-1057,D14-1071,0,0.0150722,"s), while morphs tend to be informal and convey implicit information. Morph mention detection is also related to malware detection (e.g., (Firdausi et al., 2010; Chandola et al., 2009; Firdausi et al., 2010; Christodorescu and Jha, 2003)) which discovers abnormal behavior in code and malicious software. In contrast our task tackles anomaly texts in semantic context. Deep learning-based approaches have been demonstrated to be effective in disambiguation related tasks such as WSD (Bordes et al., 2012), entity linking (He et al., 2013) and question linking (Yih et al., 2014; Bordes et al., 2014; Yang et al., 2014). In this paper we proved that it’s cruFigure 4: Resolution Acc@K for Perfect Morph Mentions NLP tasks: entity mention extraction (e.g., (Zitouni and Florian, 2008; Ohta et al., 2012; Li and Ji, 2014)), metaphor detection (e.g., (Wang et al., 2006; Tsvetkov, 2013; Heintz et al., 2013)), word sense disambiguation (WSD) (e.g., (Yarowsky, 1995; Mihalcea, 2007; Navigli, 2009)), and entity linking (EL) (e.g., (Mihalcea and Csomai, 2007; Ji et al., 2010; Ji et al., 2011; Ji et al., 2014). However, none of these previous techniques can be applied directly to tackle this problem. As mentioned in secti"
P15-1057,P95-1026,0,0.257659,"s in semantic context. Deep learning-based approaches have been demonstrated to be effective in disambiguation related tasks such as WSD (Bordes et al., 2012), entity linking (He et al., 2013) and question linking (Yih et al., 2014; Bordes et al., 2014; Yang et al., 2014). In this paper we proved that it’s cruFigure 4: Resolution Acc@K for Perfect Morph Mentions NLP tasks: entity mention extraction (e.g., (Zitouni and Florian, 2008; Ohta et al., 2012; Li and Ji, 2014)), metaphor detection (e.g., (Wang et al., 2006; Tsvetkov, 2013; Heintz et al., 2013)), word sense disambiguation (WSD) (e.g., (Yarowsky, 1995; Mihalcea, 2007; Navigli, 2009)), and entity linking (EL) (e.g., (Mihalcea and Csomai, 2007; Ji et al., 2010; Ji et al., 2011; Ji et al., 2014). However, none of these previous techniques can be applied directly to tackle this problem. As mentioned in section 3.1, entity morphs are fundamentally different from regular entity mentions. Our task is also different from metaphor detection because morphs cover a much wider range of semantic categories and can include either abstractive or concrete information. Some common features for detecting metaphors (e.g. (Tsvetkov, 593 cial to keep the genre"
P15-1057,P14-2105,0,0.0142968,"nd formal entities (e.g., named entities), while morphs tend to be informal and convey implicit information. Morph mention detection is also related to malware detection (e.g., (Firdausi et al., 2010; Chandola et al., 2009; Firdausi et al., 2010; Christodorescu and Jha, 2003)) which discovers abnormal behavior in code and malicious software. In contrast our task tackles anomaly texts in semantic context. Deep learning-based approaches have been demonstrated to be effective in disambiguation related tasks such as WSD (Bordes et al., 2012), entity linking (He et al., 2013) and question linking (Yih et al., 2014; Bordes et al., 2014; Yang et al., 2014). In this paper we proved that it’s cruFigure 4: Resolution Acc@K for Perfect Morph Mentions NLP tasks: entity mention extraction (e.g., (Zitouni and Florian, 2008; Ohta et al., 2012; Li and Ji, 2014)), metaphor detection (e.g., (Wang et al., 2006; Tsvetkov, 2013; Heintz et al., 2013)), word sense disambiguation (WSD) (e.g., (Yarowsky, 1995; Mihalcea, 2007; Navigli, 2009)), and entity linking (EL) (e.g., (Mihalcea and Csomai, 2007; Ji et al., 2010; Ji et al., 2011; Ji et al., 2014). However, none of these previous techniques can be applied directly to t"
P15-1057,N07-1025,0,0.0387675,"ntext. Deep learning-based approaches have been demonstrated to be effective in disambiguation related tasks such as WSD (Bordes et al., 2012), entity linking (He et al., 2013) and question linking (Yih et al., 2014; Bordes et al., 2014; Yang et al., 2014). In this paper we proved that it’s cruFigure 4: Resolution Acc@K for Perfect Morph Mentions NLP tasks: entity mention extraction (e.g., (Zitouni and Florian, 2008; Ohta et al., 2012; Li and Ji, 2014)), metaphor detection (e.g., (Wang et al., 2006; Tsvetkov, 2013; Heintz et al., 2013)), word sense disambiguation (WSD) (e.g., (Yarowsky, 1995; Mihalcea, 2007; Navigli, 2009)), and entity linking (EL) (e.g., (Mihalcea and Csomai, 2007; Ji et al., 2010; Ji et al., 2011; Ji et al., 2014). However, none of these previous techniques can be applied directly to tackle this problem. As mentioned in section 3.1, entity morphs are fundamentally different from regular entity mentions. Our task is also different from metaphor detection because morphs cover a much wider range of semantic categories and can include either abstractive or concrete information. Some common features for detecting metaphors (e.g. (Tsvetkov, 593 cial to keep the genres consistent bet"
P15-1057,W03-1730,0,0.00977908,"4), we use Chinese Weibo tweets for experiments. Our goal is to develop an end-to-end system that automatically extract morph mentions and resolve them to their target entities. Given a corpus of tweets D = {d1 , d2 , ..., d|D |}, we define a candidate morph mi as a unique term tj in T , where T = {t1 , t2 , ..., t|T |} is the set of unique terms in D. To extract T , we first apply several well-developed Natural Language Processing tools, including Stanford Chinese word segmenter (Chang et al., 2008), Stanford part-ofspeech tagger (Toutanova et al., 2003) and Chinese lexical analyzer ICTCLAS (Zhang et al., 2003), to process the tweets and identify noun phrases. Then we define a morph mention mpi of mi as the p-th occurrence of mi in a specific document dj . Note that a mention with the same surface form as mi but referring to its original entity is not considered as a morph mention. For instance, the “平西 王 (Conquer West King)” in d1 and d3 in Figure 1 are morph mentions since they refer to the modern politician “薄熙来 (Bo Xilai)”, while the one in d4 is not a morph mention since it refers to the original entity, who was king “吴三桂 (Wu Sangui)”. For each morph mention, we discover a list of target candid"
P15-1057,P14-2115,1,0.48139,"{zhangb8,huangh9,panx2,jih,yener}@rpi.edu, 2 lisujian@pku.edu.cn, 3 cyl@microsoft.com 4 hanj@illinois.edu, 5 zhenwen@us.ibm.com, 6 yzsun@ccs.neu.edu, 7 hanj@illinois.edu Abstract a morph “Su-tooth” was created to refer to the Uruguay striker “Luis Suarez” for his habit of biting other players. Automatically decoding humangenerated morphs in text is critical for downstream deep language understanding tasks such as entity linking and event argument extraction. However, even for human, it is difficult to decode many morphs without certain historical, cultural, or political background knowledge (Zhang et al., 2014). For example, “The Hutt” can be used to refer to a fictional alien entity in the Star Wars universe (“The Hutt stayed and established himself as ruler of Nam Chorios”), or the governor of New Jersey, Chris Christie (“The Hutt announced a bid for a seat in the New Jersey General Assembly”). Huang et al. (2013) did a pioneering pilot study on morph resolution, but their approach assumed the entity morphs were already extracted and used a large amount of labeled data. In fact, they resolved morphs on corpus-level instead of mention-level and thus their approach was context-independent. A practic"
P15-1057,P05-1049,0,0.0177967,"Missing"
P15-1057,D08-1063,0,0.0286454,"2010; Chandola et al., 2009; Firdausi et al., 2010; Christodorescu and Jha, 2003)) which discovers abnormal behavior in code and malicious software. In contrast our task tackles anomaly texts in semantic context. Deep learning-based approaches have been demonstrated to be effective in disambiguation related tasks such as WSD (Bordes et al., 2012), entity linking (He et al., 2013) and question linking (Yih et al., 2014; Bordes et al., 2014; Yang et al., 2014). In this paper we proved that it’s cruFigure 4: Resolution Acc@K for Perfect Morph Mentions NLP tasks: entity mention extraction (e.g., (Zitouni and Florian, 2008; Ohta et al., 2012; Li and Ji, 2014)), metaphor detection (e.g., (Wang et al., 2006; Tsvetkov, 2013; Heintz et al., 2013)), word sense disambiguation (WSD) (e.g., (Yarowsky, 1995; Mihalcea, 2007; Navigli, 2009)), and entity linking (EL) (e.g., (Mihalcea and Csomai, 2007; Ji et al., 2010; Ji et al., 2011; Ji et al., 2014). However, none of these previous techniques can be applied directly to tackle this problem. As mentioned in section 3.1, entity morphs are fundamentally different from regular entity mentions. Our task is also different from metaphor detection because morphs cover a much wide"
P15-1057,W12-4304,0,0.0148793,"; Firdausi et al., 2010; Christodorescu and Jha, 2003)) which discovers abnormal behavior in code and malicious software. In contrast our task tackles anomaly texts in semantic context. Deep learning-based approaches have been demonstrated to be effective in disambiguation related tasks such as WSD (Bordes et al., 2012), entity linking (He et al., 2013) and question linking (Yih et al., 2014; Bordes et al., 2014; Yang et al., 2014). In this paper we proved that it’s cruFigure 4: Resolution Acc@K for Perfect Morph Mentions NLP tasks: entity mention extraction (e.g., (Zitouni and Florian, 2008; Ohta et al., 2012; Li and Ji, 2014)), metaphor detection (e.g., (Wang et al., 2006; Tsvetkov, 2013; Heintz et al., 2013)), word sense disambiguation (WSD) (e.g., (Yarowsky, 1995; Mihalcea, 2007; Navigli, 2009)), and entity linking (EL) (e.g., (Mihalcea and Csomai, 2007; Ji et al., 2010; Ji et al., 2011; Ji et al., 2014). However, none of these previous techniques can be applied directly to tackle this problem. As mentioned in section 3.1, entity morphs are fundamentally different from regular entity mentions. Our task is also different from metaphor detection because morphs cover a much wider range of semantic"
P15-1057,N03-1033,0,0.00839243,"e recent work on morphs (Huang et al., 2013; Zhang et al., 2014), we use Chinese Weibo tweets for experiments. Our goal is to develop an end-to-end system that automatically extract morph mentions and resolve them to their target entities. Given a corpus of tweets D = {d1 , d2 , ..., d|D |}, we define a candidate morph mi as a unique term tj in T , where T = {t1 , t2 , ..., t|T |} is the set of unique terms in D. To extract T , we first apply several well-developed Natural Language Processing tools, including Stanford Chinese word segmenter (Chang et al., 2008), Stanford part-ofspeech tagger (Toutanova et al., 2003) and Chinese lexical analyzer ICTCLAS (Zhang et al., 2003), to process the tweets and identify noun phrases. Then we define a morph mention mpi of mi as the p-th occurrence of mi in a specific document dj . Note that a mention with the same surface form as mi but referring to its original entity is not considered as a morph mention. For instance, the “平西 王 (Conquer West King)” in d1 and d3 in Figure 1 are morph mentions since they refer to the modern politician “薄熙来 (Bo Xilai)”, while the one in d4 is not a morph mention since it refers to the original entity, who was king “吴三桂 (Wu Sangui)”. F"
P15-1081,N13-1056,0,0.024112,"Missing"
P15-1081,D13-1109,0,0.0234412,"Missing"
P15-1081,C10-1011,0,0.0174295,"9 Training Evaluation Spanish 992 million (Gigaword) 1.1 million (Europarl) English 940 million (Gigaword) 1.0 million (Europarl) Table 1: Size of data in tokens used in Spanish/English decipherment experiment cipher dependency bigrams and build a plaintext language model. We also use GIZA (Och and Ney, 2003) to align Europarl parallel data to build a dictionary for evaluating our decipherment. 4.2 Systems We implement a baseline system based on the work described in Dou and Knight (2013). The baseline system carries out decipherment on dependency bigrams. Therefore, we use the Bohnet parser (Bohnet, 2010) to parse the AFP section of both Spanish and English versions of the Gigaword corpus. Since not all dependency relations are shared across the two languages, we do not extract all dependency bigrams. Instead, we only use bigrams with dependency relations from the following list: • Verb / Subject • Verb / Object • Preposition / Object • Noun / Noun-Modifier We denote the system that uses our new method as DMRE (Dirichlet Multinomial Regression with Embedings). The system is the same as the baseline except that it uses a base distribution derived from word embeddings similarities. Word embeddin"
P15-1081,C12-1089,0,0.12694,"Missing"
P15-1081,P11-2071,0,0.0501802,"Missing"
P15-1081,P06-2065,1,0.644618,"eir context vectors are similar. Initially, the vectors contained only context ∗ Equal contribution 836 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 836–845, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics e, e0 range over the plaintext vocabulary. Given a plaintext bigram language model, the training objective is to learn P (f |e) that maximize P ({f n }N n=1 ). When formulated like this, one can directly apply EM to solve the problem (Knight et al., 2006). However, EM has time complexity O(N ·Ve2 ) and space complexity O(Vf ·Ve ), where Vf , Ve are the sizes of ciphertext and plaintext vocabularies respectively, and N is the number of cipher bigrams. This makes the EM approach unable to handle long ciphertexts with large vocabulary size. An alternative approach is Bayesian decipherment (Ravi and Knight, 2011). We assume that P (f |e) and P (e0 |e) are drawn from a Dirichet distribution with hyper-parameters αf,e and αe,e0 , that is: • We develop a new base-distribution technique that improves state-of-the art decipherment accuracy by a factor"
P15-1081,D12-1025,1,0.79622,"abstract representation such as word embeddings (Klementiev et al., 2012). Another promising approach to solve this problem is decipherment. It has drawn significant amounts of interest in the past few years (Ravi and Knight, 2011; Nuhn et al., 2012; Dou and Knight, 2013; Ravi, 2013) and has been shown to improve end-to-end translation. Decipherment views a foreign language as a cipher for English and finds a translation table that converts foreign texts into sensible English. Both approaches have been shown to improve quality of MT systems for domain adaptation (Daum´e and Jagarlamudi, 2011; Dou and Knight, 2012; Irvine et al., 2013) and low density languages (Irvine and Callison-Burch, 2013a; Dou et al., 2014). Meanwhile, they have their own advantages and disadvantages. While context vectors can take larger context into account, it requires high quality seed lexicons to learn a mapping between two vector spaces. In contrast, decipherment does not depend on any seed lexicon, but only looks at a limited n-gram context. In this work, we take advantage of both approaches and combine them in a joint inference process. More specifically, we extend previous work in large scale Bayesian decipherment by int"
P15-1081,P13-2109,0,0.0542467,"Missing"
P15-1081,D13-1173,1,0.864652,"swani,knight}@isi.edu Chris Dyer School of Computer Science Carnegie Mellon University cdyer@cs.cmu.edu Abstract words. Later extensions introduced more features (Haghighi et al., 2008; Garera et al., 2009; Bergsma and Van Durme, 2011; Daum´e and Jagarlamudi, 2011; Irvine and Callison-Burch, 2013b; Irvine and Callison-Burch, 2013a), and used more abstract representation such as word embeddings (Klementiev et al., 2012). Another promising approach to solve this problem is decipherment. It has drawn significant amounts of interest in the past few years (Ravi and Knight, 2011; Nuhn et al., 2012; Dou and Knight, 2013; Ravi, 2013) and has been shown to improve end-to-end translation. Decipherment views a foreign language as a cipher for English and finds a translation table that converts foreign texts into sensible English. Both approaches have been shown to improve quality of MT systems for domain adaptation (Daum´e and Jagarlamudi, 2011; Dou and Knight, 2012; Irvine et al., 2013) and low density languages (Irvine and Callison-Burch, 2013a; Dou et al., 2014). Meanwhile, they have their own advantages and disadvantages. While context vectors can take larger context into account, it requires high quality se"
P15-1081,D14-1061,1,0.762408,"Missing"
P15-1081,W09-1117,0,0.0547052,"Missing"
P15-1081,P08-1088,0,0.0847689,"Missing"
P15-1081,P12-1017,0,0.412426,"alifornia {qdou,avaswani,knight}@isi.edu Chris Dyer School of Computer Science Carnegie Mellon University cdyer@cs.cmu.edu Abstract words. Later extensions introduced more features (Haghighi et al., 2008; Garera et al., 2009; Bergsma and Van Durme, 2011; Daum´e and Jagarlamudi, 2011; Irvine and Callison-Burch, 2013b; Irvine and Callison-Burch, 2013a), and used more abstract representation such as word embeddings (Klementiev et al., 2012). Another promising approach to solve this problem is decipherment. It has drawn significant amounts of interest in the past few years (Ravi and Knight, 2011; Nuhn et al., 2012; Dou and Knight, 2013; Ravi, 2013) and has been shown to improve end-to-end translation. Decipherment views a foreign language as a cipher for English and finds a translation table that converts foreign texts into sensible English. Both approaches have been shown to improve quality of MT systems for domain adaptation (Daum´e and Jagarlamudi, 2011; Dou and Knight, 2012; Irvine et al., 2013) and low density languages (Irvine and Callison-Burch, 2013a; Dou et al., 2014). Meanwhile, they have their own advantages and disadvantages. While context vectors can take larger context into account, it re"
P15-1081,W13-2233,0,0.0621332,"Missing"
P15-1081,P95-1050,0,0.108409,"matically from parallel data. However, reliance on parallel data also limits the development and application of high-quality MT systems, as the amount of parallel data is far from adequate in low-density languages and domains. In general, it is easier to obtain non-parallel monolingual data. The ability to learn translations from monolingual data can alleviate obstacles caused by insufficient parallel data. Motivated by this idea, researchers have proposed different approaches to tackle this problem. They can be largely divided into two groups. The first group is based on the idea proposed by Rapp (1995), in which words are represented as context vectors, and two words are likely to be translations if their context vectors are similar. Initially, the vectors contained only context ∗ Equal contribution 836 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 836–845, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics e, e0 range over the plaintext vocabulary. Given a plaintext bigram language model, the training objective is to learn P (f |e) that"
P15-1081,P11-1002,1,0.954247,"niversity of Southern California {qdou,avaswani,knight}@isi.edu Chris Dyer School of Computer Science Carnegie Mellon University cdyer@cs.cmu.edu Abstract words. Later extensions introduced more features (Haghighi et al., 2008; Garera et al., 2009; Bergsma and Van Durme, 2011; Daum´e and Jagarlamudi, 2011; Irvine and Callison-Burch, 2013b; Irvine and Callison-Burch, 2013a), and used more abstract representation such as word embeddings (Klementiev et al., 2012). Another promising approach to solve this problem is decipherment. It has drawn significant amounts of interest in the past few years (Ravi and Knight, 2011; Nuhn et al., 2012; Dou and Knight, 2013; Ravi, 2013) and has been shown to improve end-to-end translation. Decipherment views a foreign language as a cipher for English and finds a translation table that converts foreign texts into sensible English. Both approaches have been shown to improve quality of MT systems for domain adaptation (Daum´e and Jagarlamudi, 2011; Dou and Knight, 2012; Irvine et al., 2013) and low density languages (Irvine and Callison-Burch, 2013a; Dou et al., 2014). Meanwhile, they have their own advantages and disadvantages. While context vectors can take larger context"
P15-1081,P13-1036,0,0.295978,"Missing"
P15-1081,1983.tc-1.13,0,0.0603901,"y relations between words. Throughout this paper, we use f to denote target language or ciphertext tokens, and e to denote source language or plaintext tokens. Given ciphertext f : f1 ...fn , the task of decipherment is to find a set of parameters P (fi |ei ) that convert f to sensible plaintext. The ciphertext f can either be full sentences (Ravi and Knight, 2011; Nuhn et al., 2012) or simply bigrams (Dou and Knight, 2013). Since using bigrams and their counts speeds up decipherment, in this work, we treat f as bigrams, n n N where f = {f n }N n=1 = {f1 , f2 }n=1 . Motivated by the idea from Weaver (1955), we model an observed cipher bigram f n with the following generative story: • First, a language model P (e) generates a sequence of two plaintext tokens en1 , en2 with probability P (en1 , en2 ). • Then, substitute en1 with f1n and en2 with f2n with probability P (f1n |en1 ) · P (f2n |en2 ). Based on the above generative story, the probability of any cipher bigram f n is: n P (f ) = X P (e1 e2 ) e1 e2 2 Y P (fin P (f |e) ∼ Dirichlet(αf,e ) P (e |e0 ) ∼ Dirichlet(αe,e0 ). The remainder of the generative story is the same as the noisy channel model for decipherment. In the next section, we des"
P15-1081,J03-1002,0,\N,Missing
P16-1006,W13-2322,1,0.579501,"ppear in the document. Figure 2: Examples of Cartoons in Chinese (left) and English (right). cept whose images appear frequently (e.g., “宝宝 (baby)” and “螃蟹 (crab)” in “Dora Exploration”, “海盗 (pirate)” in “the Garden Guardians”, and “亨利 (Henry)” in “Thomas Train”), as illustrated in Figure 2. Representation and Structured Knowledge Transfer for Entity Linking: Besides data sparsity, another challenge for low-resource language IE lies in the lack of knowledge resources. For example, there are advanced knowledge representation parsing tools available (e.g., Abstract Meaning Representation (AMR) (Banarescu et al., 2013)) and large-scale knowledge bases for English Entity Linking, but not for other languages, including some medium-resource ones such as Chinese. For example, the following documents are both about the event of Pistorius killing his girl friend Reeva: Next we will project these names extracted from HL documents directly to LL documents to identify and verify names. In addition to textual evidence, we check visual similarity to match an HL name with its equivalent in LL. And we apply face recognition techniques to verify person names by image search. This idea matches human knowledge acquisition"
P16-1006,W14-3902,0,0.0383599,"Missing"
P16-1006,W13-2233,0,0.0496064,"Missing"
P16-1006,W09-3107,1,0.886171,"Missing"
P16-1006,C10-1064,0,0.015831,"orms our baseline and the best reported results on the same test set (Ji et al., 2015). Our approach is particularly effective for rare nicknames (e.g., “C罗” (C Luo) is used to refer to Cristiano Ronaldo) or ambiguous abbreviations (e.g., “邦联” (federal) can refer to Confederate States of America, 邦联制 (Confederation) and many other entities) for which the contexts in LLs are not sufficient for making correct linking decisions due to the lack of rich knowledge rep8 Related Work Some previous cross-lingual projection methods focused on transferring data/annotation (e.g., (Pad´o and Lapata, 2009; Kim et al., 2010; Faruqui and Kumar, 2015)), shared feature representation/model (e.g., (McDonald et al., 2011; Kozhevnikov and Titov, 2013; Kozhevnikov and Titov, 2014)), or expectation (e.g., (Wang and Manning, 2014)). Most of them relied on a large 61 Approach Baseline State-of-the-art Our Approach Our Approach w/o KB Walker PER 49.12 49.85 52.36 50.44 ORG 60.18 64.30 67.05 67.05 Overall GPE 80.97 75.38 93.33 84.41 LOC 80.68 96.59 93.18 90.91 ALL 66.57 65.87 74.92 70.32 PER 67.27 68.28 71.72 69.09 Linkable Entities ORG GPE LOC 67.61 81.05 80.68 72.24 75.46 96.59 75.32 93.43 93.18 75.32 84.50 90.91 ALL 74.7"
P16-1006,W10-2407,0,0.030237,"Missing"
P16-1006,P06-1103,0,0.10768,"Missing"
P16-1006,N15-1151,0,0.0290662,"and the best reported results on the same test set (Ji et al., 2015). Our approach is particularly effective for rare nicknames (e.g., “C罗” (C Luo) is used to refer to Cristiano Ronaldo) or ambiguous abbreviations (e.g., “邦联” (federal) can refer to Confederate States of America, 邦联制 (Confederation) and many other entities) for which the contexts in LLs are not sufficient for making correct linking decisions due to the lack of rich knowledge rep8 Related Work Some previous cross-lingual projection methods focused on transferring data/annotation (e.g., (Pad´o and Lapata, 2009; Kim et al., 2010; Faruqui and Kumar, 2015)), shared feature representation/model (e.g., (McDonald et al., 2011; Kozhevnikov and Titov, 2013; Kozhevnikov and Titov, 2014)), or expectation (e.g., (Wang and Manning, 2014)). Most of them relied on a large 61 Approach Baseline State-of-the-art Our Approach Our Approach w/o KB Walker PER 49.12 49.85 52.36 50.44 ORG 60.18 64.30 67.05 67.05 Overall GPE 80.97 75.38 93.33 84.41 LOC 80.68 96.59 93.18 90.91 ALL 66.57 65.87 74.92 70.32 PER 67.27 68.28 71.72 69.09 Linkable Entities ORG GPE LOC 67.61 81.05 80.68 72.24 75.46 96.59 75.32 93.43 93.18 75.32 84.50 90.91 ALL 74.70 73.91 84.06 78.91 Table"
P16-1006,P13-1117,0,0.0415773,"Missing"
P16-1006,P14-2095,0,0.0558813,"Missing"
P16-1006,D15-1070,0,0.026279,". feedback to improve name classification (Sil and Yates, 2013; Heinzerling et al., 2015; Besancon et al., 2015; Sil et al., 2015). amount of parallel data to derive word alignment and translations, which are inadequate for many LLs. In contrast, we do not require any parallel data or bi-lingual lexicon. We introduce new cross-media techniques for projecting HLs to LLs, by inferring projections using domain-rich, nonparallel data automatically discovered by image search and processing. Similar image-mediated approaches have been applied to other tasks such as cross-lingual document retrieval (Funaki and Nakayama, 2015) and bilingual lexicon induction (Bergsma and Van Durme, 2011). Besides visual similarity, their method also relied on distributional similarity computed from a large amount of unlabeled data, which might not be available for some LLs. 9 Conclusions and Future Work We describe a novel multi-media approach to effectively transfer entity knowledge from highresource languages to low-resource languages. In the future we will apply visual pattern recognition and concept detection techniques to perform deep content analysis of the retrieved images, so we can do matching and inference on concept/enti"
P16-1006,D14-1198,1,0.827757,"for the walk-through example), we start by extracting its key phrases using the following three languageindependent methods: (1) TextRank (Mihalcea and Tarau, 2004), which is a graph-based ranking model to determine key phrases. (2) Topic modeling based on Latent Dirichlet allocation (LDA) model (Blei et al., 2003), which can generate a small number of key phrases representing the main topics of each document. (3) The title of the document if it’s available. 3.2 4.1 Name Tagging After we acquire HL (English in this paper) comparable documents, we apply a state-of-the-art English name tagger (Li et al., 2014) based on structured perceptron to extract names. From the output we filter out uninformative names such as news agencies. If the same name receives multiple types across documents, we use the majority one. 4.2 Entity Linking We apply a state-of-the-art Abstract Meaning Representation (AMR) parser (Wang et al., 2015a) to generate rich semantic representations. Then we apply an AMR based entity linker (Pan et al., 2015) to link all English entity mentions to the corresponding entities in the English KB. Given a name nh , this entity linker first constructs a Knowledge Graph g(nh ) with nh at th"
P16-1006,W11-2206,1,0.898665,"Missing"
P16-1006,P99-1067,0,0.0275473,"Missing"
P16-1006,P14-5010,0,0.00878046,"Missing"
P16-1006,C04-1089,0,0.0460909,"Missing"
P16-1006,D11-1006,0,0.0346974,"Missing"
P16-1006,J05-4003,0,0.0840005,"Missing"
P16-1006,P06-1010,0,0.0572594,"Missing"
P16-1006,W10-2412,0,0.0245475,"Missing"
P16-1006,W10-2408,0,0.0741018,"Missing"
P16-1006,E09-1091,0,0.0702251,"Missing"
P16-1006,N15-1119,1,0.842821,"e title of the document if it’s available. 3.2 4.1 Name Tagging After we acquire HL (English in this paper) comparable documents, we apply a state-of-the-art English name tagger (Li et al., 2014) based on structured perceptron to extract names. From the output we filter out uninformative names such as news agencies. If the same name receives multiple types across documents, we use the majority one. 4.2 Entity Linking We apply a state-of-the-art Abstract Meaning Representation (AMR) parser (Wang et al., 2015a) to generate rich semantic representations. Then we apply an AMR based entity linker (Pan et al., 2015) to link all English entity mentions to the corresponding entities in the English KB. Given a name nh , this entity linker first constructs a Knowledge Graph g(nh ) with nh at the hub and leaf nodes obtained from names reachable by AMR graph traversal from nh . A subset of the leaf nodes are selected as collaborators of nh . Names connected by AMR conjunction relations are grouped into sets of coherent names. For each name nh , an initial ranked list of entity candidates E = {e1 , ..., eM } is generated based on a salience measure (Medelyan and Legg, 2008). Then a Knowledge Graph g(em ) is gen"
P16-1006,voss-etal-2014-finding,0,0.0602614,"Missing"
P16-1006,Q14-1005,0,0.0529122,"Missing"
P16-1006,P15-2141,0,0.0178262,"), which can generate a small number of key phrases representing the main topics of each document. (3) The title of the document if it’s available. 3.2 4.1 Name Tagging After we acquire HL (English in this paper) comparable documents, we apply a state-of-the-art English name tagger (Li et al., 2014) based on structured perceptron to extract names. From the output we filter out uninformative names such as news agencies. If the same name receives multiple types across documents, we use the majority one. 4.2 Entity Linking We apply a state-of-the-art Abstract Meaning Representation (AMR) parser (Wang et al., 2015a) to generate rich semantic representations. Then we apply an AMR based entity linker (Pan et al., 2015) to link all English entity mentions to the corresponding entities in the English KB. Given a name nh , this entity linker first constructs a Knowledge Graph g(nh ) with nh at the hub and leaf nodes obtained from names reachable by AMR graph traversal from nh . A subset of the leaf nodes are selected as collaborators of nh . Names connected by AMR conjunction relations are grouped into sets of coherent names. For each name nh , an initial ranked list of entity candidates E = {e1 , ..., eM }"
P16-1006,D15-1081,1,0.841841,"), which can generate a small number of key phrases representing the main topics of each document. (3) The title of the document if it’s available. 3.2 4.1 Name Tagging After we acquire HL (English in this paper) comparable documents, we apply a state-of-the-art English name tagger (Li et al., 2014) based on structured perceptron to extract names. From the output we filter out uninformative names such as news agencies. If the same name receives multiple types across documents, we use the majority one. 4.2 Entity Linking We apply a state-of-the-art Abstract Meaning Representation (AMR) parser (Wang et al., 2015a) to generate rich semantic representations. Then we apply an AMR based entity linker (Pan et al., 2015) to link all English entity mentions to the corresponding entities in the English KB. Given a name nh , this entity linker first constructs a Knowledge Graph g(nh ) with nh at the hub and leaf nodes obtained from names reachable by AMR graph traversal from nh . A subset of the leaf nodes are selected as collaborators of nh . Names connected by AMR conjunction relations are grouped into sets of coherent names. For each name nh , an initial ranked list of entity candidates E = {e1 , ..., eM }"
P16-1006,W04-3252,0,\N,Missing
P16-1038,2010.amta-papers.12,0,0.011456,"such as whether the phoneme is nasal, consonantal, sonorant, or a tone. Each phoneme thus maps to a unique feature vector, with features expressed as +, -, or 0. 4.2 Named Entity Resources For our language-to-language distance metric, it is useful to have written text in many languages. The most easily accessible source of this data is multilingual named entity (NE) resources. We synthesize 7 different NE corpora: ChineseEnglish names (Ji et al., 2009), Geonames (Vatant and Wick, 2006), JRC names (Steinberger et al., 2011), corpora from LDC2 , NEWS 2015 (Banchs et al., 2015), Wikipedia names (Irvine et al., 2010), and Wikipedia titles (Lin et al., 2011); to this, we also add multilingual Wikipedia titles for place names from an online English-language gazetteer (Everett-Heath, 2014). This yields a list of 9.9m named entities (8.9 not including English data) across 384 languages, which include the En4.4 Wiktionary pronunciation dictionaries Ironically, to train data-driven g2p models for high-resource languages, and to evaluate our low-resource g2p models, we require pronunciation dictionaries for many languages. A common and successful technique for obtaining this data (Schlippe et al., 2010; Schlippe"
P16-1038,2005.mtsummit-papers.11,0,0.00794409,"res, it only takes into account broad phonetic features, such as whether each language has voiced plosives. This can result in some non-intuitive results: based on this metric, there are almost 100 languages phonetically equivalent to the South Asian language Gujarati, among them Arawak and Chechen. To provide a more fine-grained phonetic distance metric, we create a phoneme inventory distance metric using phon2phon. For each pair of Each entry in our lang2lang distance table also includes the following features for the second language: the number of named entities, whether it is in Europarl (Koehn, 2005), whether it has its own Wikipedia, whether it is primarily written in the same script as the first language, whether it has an IPA Help page, whether it is in our Wiktionary test set, and whether it is in our Wiktionary training set. Table 5 shows examples of the closest languages to English, Hindi, and Vietnamese, according to different lang2lang metrics. 403 60 This method prefers rules with longer grapheme segments; for example, for the word tin, the output ""ʃ n"" is preferred over the correct ""tʰ ɪ n"" because of the rule ti→ʃ. We build 97 IPA Help models, but have test data for only 91—som"
P16-1038,W11-2206,0,0.0121358,"antal, sonorant, or a tone. Each phoneme thus maps to a unique feature vector, with features expressed as +, -, or 0. 4.2 Named Entity Resources For our language-to-language distance metric, it is useful to have written text in many languages. The most easily accessible source of this data is multilingual named entity (NE) resources. We synthesize 7 different NE corpora: ChineseEnglish names (Ji et al., 2009), Geonames (Vatant and Wick, 2006), JRC names (Steinberger et al., 2011), corpora from LDC2 , NEWS 2015 (Banchs et al., 2015), Wikipedia names (Irvine et al., 2010), and Wikipedia titles (Lin et al., 2011); to this, we also add multilingual Wikipedia titles for place names from an online English-language gazetteer (Everett-Heath, 2014). This yields a list of 9.9m named entities (8.9 not including English data) across 384 languages, which include the En4.4 Wiktionary pronunciation dictionaries Ironically, to train data-driven g2p models for high-resource languages, and to evaluate our low-resource g2p models, we require pronunciation dictionaries for many languages. A common and successful technique for obtaining this data (Schlippe et al., 2010; Schlippe et al., 2012a; Yao and Kondrak, 2015) is"
P16-1038,W12-6208,0,0.0258895,"Missing"
P16-1038,P12-1074,0,0.031689,"→ pp to M ; end Algorithm 1: A condensed version of our procedure for mapping scraped phoneme sets from Wikipedia and Wiktionary to Phoible language inventories. The full algorithm handles segmentation of the scraped pronunciation and heuristically promotes coverage of the Phoible inventory. Phonetic Distance Metric Automatically comparing pronunciations across languages is especially difficult in text form. Although two versions of the “sh” sound, “ʃ” and “ɕ,” sound very similar to most people and very different from “m,” to a machine all three characters seem equidistant. Previous research (Özbal and Strapparava, 2012; Vu and Schultz, 2013; Vu et al., 2014) has addressed this issue by matching exact phonemes by character or manually selecting comparison features; however, we are interested in an automatic metric covering all possible IPA phoneme pairs. We handle this problem by using Phoible’s phoneme feature vectors to create phon2phon, a distance metric between IPA phonemes. In this section we also describe how we use this metric to clean open-source data and build phonememapping models between languages. 5.1 word jód څلور the features + and - are more similar than 0. We then compute the normalized Ham"
P16-1038,R11-1015,0,0.0277378,"Missing"
P16-1038,N15-1095,0,0.0234166,"titles (Lin et al., 2011); to this, we also add multilingual Wikipedia titles for place names from an online English-language gazetteer (Everett-Heath, 2014). This yields a list of 9.9m named entities (8.9 not including English data) across 384 languages, which include the En4.4 Wiktionary pronunciation dictionaries Ironically, to train data-driven g2p models for high-resource languages, and to evaluate our low-resource g2p models, we require pronunciation dictionaries for many languages. A common and successful technique for obtaining this data (Schlippe et al., 2010; Schlippe et al., 2012a; Yao and Kondrak, 2015) is scraping Wiktionary, an open-source multilingual dictionary maintained by Wikimedia. We extract unique word-pronunciation pairs from the English, German, Greek, Japanese, Korean, and Russian sites of Wiktionary. (Each Wiktionary site, while written in its respective language, contains word entries in multiple languages.) 2 LDC2015E13, LDC2015E70, LDC2015E82, LDC2015E90, LDC2015E84, LDC2014E115, and LDC2015E91 3 https://en.wikipedia.org/wiki/Category: International_Phonetic_Alphabet_help 401 lang ces pus kan hye ukr Since Wiktionary data is very noisy, we apply length filtering as discussed"
P16-1038,W15-3902,0,\N,Missing
P17-1178,N15-1107,0,0.0299644,"s through cross-lingual topic transfer. For the first time, we propose to customize name annotations for specific downstream applications. Again, we use a cross-lingual knowledge transfer strategy to leverage the widely available English corpora to choose entities with specific Wikipedia topic categories (Section 2.5). Derive morphology analysis from Wikipedia markups. Another unique challenge for morphologically rich languages is to segment each token into its stemming form and affixes. Previous methods relied on either high-cost supervised learning (Roth et al., 2008; Mahmoudi et al., 2013; Ahlberg et al., 2015), or low-quality unsupervised learning (Gr¨onroos et al., 2014; Ruokolainen et al., 2016). We exploit Wikipedia markups to automatically learn affixes as language-specific features (Section 2.3). Mine word translations from cross-lingual links. Name translation is a crucial step to generate candidate entities in cross-lingual entity linking. Only a small percentage of names can be directly translated by matching against cross-lingual Wikipedia title pairs. Based on the observation that Wikipedia titles within any language tend to follow a consistent style and format, we propose an effective me"
P17-1178,C12-2005,0,0.0398197,"Missing"
P17-1178,R13-1005,0,0.0213967,"hods required labeled data and name transliteration. We share the same goal as (Sil and Florian, 2016) to extend cross-lingual entity linking to all languages in Wikipedia. They exploited Wikipedia links to train a supervised linker. We mine reliable word translations from cross-lingual Wikipedia titles, which enables us to adopt unsupervised English entity linking techniques such as (Pan et al., 2015) to directly link translated English name mentions to English KB. Efforts to save annotation cost for name tagging: Some previous work including (Ji and Grishman, 2006; Richman and Schone, 2008; Althobaiti et al., 2013) exploited semi-supervised methods to save annotation cost. We observed that self-training can provide further gains when the training data contains certain amount of noise. 6 Conclusions and Future Work We developed a simple yet effective framework that can extract names from 282 languages and link them to an English KB. This framework follows a fully automatic training and testing pipeline, without the needs of any manual annotations or knowledge from native speakers. We evaluated our framework on both Wikipedia articles and external formal and informal texts and obtained promising results."
P17-1178,E14-3012,0,0.0322345,"Missing"
P17-1178,P03-2031,0,0.129469,"t handle unknown names. We developed a new method to derive generalizable affixes for morphologically rich language based on Wikipedia markups. Wikipedia as background features for IE: Wikipedia pages have been used as additional features to improve various Information Extraction (IE) tasks, including name tagging (Kazama and Torisawa, 2007), coreference resolution (Paolo Ponzetto and Strube, 2006), relation extraction (Chan and Roth, 2010) and event extraction (Hogue et al., 2014). Other automatic name annotation generation methods have been proposed, including KB driven distant supervision (An et al., 2003; Mintz et al., 2009; Ren et al., 2015) and cross-lingual projection (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013; Wang and Manning, 2014; Zhang et al., 2016b). Multi-lingual name tagging: Some recent research (Zhang et al., 2016a; Littell et al., 2016; Tsai et al., 2016) under the DARPA LORELEI program focused on developing name tagging techniques for low-resource languages. These approaches require English annotations for projection (Tsai et al., 2016), some input from a native speaker, either through manual annotations (Littell et al., 2016), or a linguistic surve"
P17-1178,W13-2322,1,0.762364,"omputational Linguistics, pages 1946–1958 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1178 tr/Çin_K en/Comm 282 languages, and link them to an English KB (Wikipedia in this work). The major challenges and our new solutions are summarized as follows. Creating “Silver-standard” through crosslingual entity transfer. The first step is to classify English Wikipedia entries into certain entity types and then propagate these labels to other languages. We exploit the English Abstract Meaning Representation (AMR) corpus (Banarescu et al., 2013) which includes both name tagging and linking annotations for fine-grained entity types to train an automatic classifier. Furthermore, we exploit each entry’s properties in DBpedia as features and thus eliminate the need of language-specific features and resources such as part-of-speech tagging as in previous work (Section 2.2). Refine annotations through self-training. The initial annotations obtained from above are too incomplete and inconsistent. Previous work used name string match to propagate labels. In contrast, we apply self-training to label other mentions without links in Wikipedia a"
P17-1178,C10-1018,0,0.0173,"advantage of richer structures in the KBs. Previous work on Arabic name tagging (Althobaiti et al., 2014) extracted entity titles as a gazetteer for stemming, and thus it cannot handle unknown names. We developed a new method to derive generalizable affixes for morphologically rich language based on Wikipedia markups. Wikipedia as background features for IE: Wikipedia pages have been used as additional features to improve various Information Extraction (IE) tasks, including name tagging (Kazama and Torisawa, 2007), coreference resolution (Paolo Ponzetto and Strube, 2006), relation extraction (Chan and Roth, 2010) and event extraction (Hogue et al., 2014). Other automatic name annotation generation methods have been proposed, including KB driven distant supervision (An et al., 2003; Mintz et al., 2009; Ren et al., 2015) and cross-lingual projection (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013; Wang and Manning, 2014; Zhang et al., 2016b). Multi-lingual name tagging: Some recent research (Zhang et al., 2016a; Littell et al., 2016; Tsai et al., 2016) under the DARPA LORELEI program focused on developing name tagging techniques for low-resource languages. These approaches requir"
P17-1178,N13-1006,0,0.0254816,"d on Wikipedia markups. Wikipedia as background features for IE: Wikipedia pages have been used as additional features to improve various Information Extraction (IE) tasks, including name tagging (Kazama and Torisawa, 2007), coreference resolution (Paolo Ponzetto and Strube, 2006), relation extraction (Chan and Roth, 2010) and event extraction (Hogue et al., 2014). Other automatic name annotation generation methods have been proposed, including KB driven distant supervision (An et al., 2003; Mintz et al., 2009; Ren et al., 2015) and cross-lingual projection (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013; Wang and Manning, 2014; Zhang et al., 2016b). Multi-lingual name tagging: Some recent research (Zhang et al., 2016a; Littell et al., 2016; Tsai et al., 2016) under the DARPA LORELEI program focused on developing name tagging techniques for low-resource languages. These approaches require English annotations for projection (Tsai et al., 2016), some input from a native speaker, either through manual annotations (Littell et al., 2016), or a linguistic survey (Zhang et al., 2016a). Without using any manual annotations, our name taggers outperform previous methods on the same d"
P17-1178,I08-1071,0,0.089043,"Missing"
P17-1178,D15-1103,0,0.0422904,"Missing"
P17-1178,W10-2415,0,0.0132221,"ired from previous work that leveraged Wikipedia markups to train name taggers (Nothman et al., 2008; Dakka and Cucerzan, 2008; Mika et al., 2008; Ringland et al., 2009; Alotaibi and Lee, 2012; Nothman et al., 2013; Althobaiti et al., 2014). Most of these previous methods manually classified many English Wikipedia entries into pre-defined entity types. In contrast, our approach doesn’t need any manual annotations or language-specific features, while generates both coarse-grained and fine-grained types. Many fine-grained entity typing approaches (Fleischman and Hovy, 2002; Giuliano, 1953 2009; Ekbal et al., 2010; Ling and Weld, 2012; Yosef et al., 2012; Nakashole et al., 2013; Gillick et al., 2014; Yogatama et al., 2015; Del Corro et al., 2015) also created annotations based on Wikipedia anchor links. Our framework performs both name identification and typing and takes advantage of richer structures in the KBs. Previous work on Arabic name tagging (Althobaiti et al., 2014) extracted entity titles as a gazetteer for stemming, and thus it cannot handle unknown names. We developed a new method to derive generalizable affixes for morphologically rich language based on Wikipedia markups. Wikipedia as back"
P17-1178,C02-1130,0,0.0349275,"er standard generation: Our work was mainly inspired from previous work that leveraged Wikipedia markups to train name taggers (Nothman et al., 2008; Dakka and Cucerzan, 2008; Mika et al., 2008; Ringland et al., 2009; Alotaibi and Lee, 2012; Nothman et al., 2013; Althobaiti et al., 2014). Most of these previous methods manually classified many English Wikipedia entries into pre-defined entity types. In contrast, our approach doesn’t need any manual annotations or language-specific features, while generates both coarse-grained and fine-grained types. Many fine-grained entity typing approaches (Fleischman and Hovy, 2002; Giuliano, 1953 2009; Ekbal et al., 2010; Ling and Weld, 2012; Yosef et al., 2012; Nakashole et al., 2013; Gillick et al., 2014; Yogatama et al., 2015; Del Corro et al., 2015) also created annotations based on Wikipedia anchor links. Our framework performs both name identification and typing and takes advantage of richer structures in the KBs. Previous work on Arabic name tagging (Althobaiti et al., 2014) extracted entity titles as a gazetteer for stemming, and thus it cannot handle unknown names. We developed a new method to derive generalizable affixes for morphologically rich language base"
P17-1178,W09-1125,0,0.0774484,"Missing"
P17-1178,C14-1111,0,0.0173537,"Missing"
P17-1178,D14-1012,0,0.0209447,"Missing"
P17-1178,U14-1006,1,0.83456,"Previous work on Arabic name tagging (Althobaiti et al., 2014) extracted entity titles as a gazetteer for stemming, and thus it cannot handle unknown names. We developed a new method to derive generalizable affixes for morphologically rich language based on Wikipedia markups. Wikipedia as background features for IE: Wikipedia pages have been used as additional features to improve various Information Extraction (IE) tasks, including name tagging (Kazama and Torisawa, 2007), coreference resolution (Paolo Ponzetto and Strube, 2006), relation extraction (Chan and Roth, 2010) and event extraction (Hogue et al., 2014). Other automatic name annotation generation methods have been proposed, including KB driven distant supervision (An et al., 2003; Mintz et al., 2009; Ren et al., 2015) and cross-lingual projection (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013; Wang and Manning, 2014; Zhang et al., 2016b). Multi-lingual name tagging: Some recent research (Zhang et al., 2016a; Littell et al., 2016; Tsai et al., 2016) under the DARPA LORELEI program focused on developing name tagging techniques for low-resource languages. These approaches require English annotations for projection (Tsai"
P17-1178,P06-2055,1,0.710096,"2011) extended it to 21 languages. But their methods required labeled data and name transliteration. We share the same goal as (Sil and Florian, 2016) to extend cross-lingual entity linking to all languages in Wikipedia. They exploited Wikipedia links to train a supervised linker. We mine reliable word translations from cross-lingual Wikipedia titles, which enables us to adopt unsupervised English entity linking techniques such as (Pan et al., 2015) to directly link translated English name mentions to English KB. Efforts to save annotation cost for name tagging: Some previous work including (Ji and Grishman, 2006; Richman and Schone, 2008; Althobaiti et al., 2013) exploited semi-supervised methods to save annotation cost. We observed that self-training can provide further gains when the training data contains certain amount of noise. 6 Conclusions and Future Work We developed a simple yet effective framework that can extract names from 282 languages and link them to an English KB. This framework follows a fully automatic training and testing pipeline, without the needs of any manual annotations or knowledge from native speakers. We evaluated our framework on both Wikipedia articles and external formal"
P17-1178,J03-1002,0,0.0210511,"Missing"
P17-1178,D07-1073,0,0.0495625,"ions based on Wikipedia anchor links. Our framework performs both name identification and typing and takes advantage of richer structures in the KBs. Previous work on Arabic name tagging (Althobaiti et al., 2014) extracted entity titles as a gazetteer for stemming, and thus it cannot handle unknown names. We developed a new method to derive generalizable affixes for morphologically rich language based on Wikipedia markups. Wikipedia as background features for IE: Wikipedia pages have been used as additional features to improve various Information Extraction (IE) tasks, including name tagging (Kazama and Torisawa, 2007), coreference resolution (Paolo Ponzetto and Strube, 2006), relation extraction (Chan and Roth, 2010) and event extraction (Hogue et al., 2014). Other automatic name annotation generation methods have been proposed, including KB driven distant supervision (An et al., 2003; Mintz et al., 2009; Ren et al., 2015) and cross-lingual projection (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013; Wang and Manning, 2014; Zhang et al., 2016b). Multi-lingual name tagging: Some recent research (Zhang et al., 2016a; Littell et al., 2016; Tsai et al., 2016) under the DARPA LORELEI prog"
P17-1178,P12-1073,0,0.103702,"rich language based on Wikipedia markups. Wikipedia as background features for IE: Wikipedia pages have been used as additional features to improve various Information Extraction (IE) tasks, including name tagging (Kazama and Torisawa, 2007), coreference resolution (Paolo Ponzetto and Strube, 2006), relation extraction (Chan and Roth, 2010) and event extraction (Hogue et al., 2014). Other automatic name annotation generation methods have been proposed, including KB driven distant supervision (An et al., 2003; Mintz et al., 2009; Ren et al., 2015) and cross-lingual projection (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013; Wang and Manning, 2014; Zhang et al., 2016b). Multi-lingual name tagging: Some recent research (Zhang et al., 2016a; Littell et al., 2016; Tsai et al., 2016) under the DARPA LORELEI program focused on developing name tagging techniques for low-resource languages. These approaches require English annotations for projection (Tsai et al., 2016), some input from a native speaker, either through manual annotations (Littell et al., 2016), or a linguistic survey (Zhang et al., 2016a). Without using any manual annotations, our name taggers outperform previous met"
P17-1178,N16-1030,0,0.0776175,"Missing"
P17-1178,C16-1095,0,0.0821433,"Missing"
P17-1178,R13-1053,0,0.0239555,"). Customize annotations through cross-lingual topic transfer. For the first time, we propose to customize name annotations for specific downstream applications. Again, we use a cross-lingual knowledge transfer strategy to leverage the widely available English corpora to choose entities with specific Wikipedia topic categories (Section 2.5). Derive morphology analysis from Wikipedia markups. Another unique challenge for morphologically rich languages is to segment each token into its stemming form and affixes. Previous methods relied on either high-cost supervised learning (Roth et al., 2008; Mahmoudi et al., 2013; Ahlberg et al., 2015), or low-quality unsupervised learning (Gr¨onroos et al., 2014; Ruokolainen et al., 2016). We exploit Wikipedia markups to automatically learn affixes as language-specific features (Section 2.3). Mine word translations from cross-lingual links. Name translation is a crucial step to generate candidate entities in cross-lingual entity linking. Only a small percentage of names can be directly translated by matching against cross-lingual Wikipedia title pairs. Based on the observation that Wikipedia titles within any language tend to follow a consistent style and format, we"
P17-1178,P14-5010,0,0.0041107,"Missing"
P17-1178,I11-1029,0,0.137923,"6) under the DARPA LORELEI program focused on developing name tagging techniques for low-resource languages. These approaches require English annotations for projection (Tsai et al., 2016), some input from a native speaker, either through manual annotations (Littell et al., 2016), or a linguistic survey (Zhang et al., 2016a). Without using any manual annotations, our name taggers outperform previous methods on the same data sets for many languages. Multi-lingual entity linking: NIST TAC-KBP Tri-lingual entity linking (Ji et al., 2016) focused on three languages: English, Chinese and Spanish. (McNamee et al., 2011) extended it to 21 languages. But their methods required labeled data and name transliteration. We share the same goal as (Sil and Florian, 2016) to extend cross-lingual entity linking to all languages in Wikipedia. They exploited Wikipedia links to train a supervised linker. We mine reliable word translations from cross-lingual Wikipedia titles, which enables us to adopt unsupervised English entity linking techniques such as (Pan et al., 2015) to directly link translated English name mentions to English KB. Efforts to save annotation cost for name tagging: Some previous work including (Ji and"
P17-1178,P09-1113,0,0.0368323,"names. We developed a new method to derive generalizable affixes for morphologically rich language based on Wikipedia markups. Wikipedia as background features for IE: Wikipedia pages have been used as additional features to improve various Information Extraction (IE) tasks, including name tagging (Kazama and Torisawa, 2007), coreference resolution (Paolo Ponzetto and Strube, 2006), relation extraction (Chan and Roth, 2010) and event extraction (Hogue et al., 2014). Other automatic name annotation generation methods have been proposed, including KB driven distant supervision (An et al., 2003; Mintz et al., 2009; Ren et al., 2015) and cross-lingual projection (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013; Wang and Manning, 2014; Zhang et al., 2016b). Multi-lingual name tagging: Some recent research (Zhang et al., 2016a; Littell et al., 2016; Tsai et al., 2016) under the DARPA LORELEI program focused on developing name tagging techniques for low-resource languages. These approaches require English annotations for projection (Tsai et al., 2016), some input from a native speaker, either through manual annotations (Littell et al., 2016), or a linguistic survey (Zhang et al., 201"
P17-1178,P13-1146,0,0.00785015,"rain name taggers (Nothman et al., 2008; Dakka and Cucerzan, 2008; Mika et al., 2008; Ringland et al., 2009; Alotaibi and Lee, 2012; Nothman et al., 2013; Althobaiti et al., 2014). Most of these previous methods manually classified many English Wikipedia entries into pre-defined entity types. In contrast, our approach doesn’t need any manual annotations or language-specific features, while generates both coarse-grained and fine-grained types. Many fine-grained entity typing approaches (Fleischman and Hovy, 2002; Giuliano, 1953 2009; Ekbal et al., 2010; Ling and Weld, 2012; Yosef et al., 2012; Nakashole et al., 2013; Gillick et al., 2014; Yogatama et al., 2015; Del Corro et al., 2015) also created annotations based on Wikipedia anchor links. Our framework performs both name identification and typing and takes advantage of richer structures in the KBs. Previous work on Arabic name tagging (Althobaiti et al., 2014) extracted entity titles as a gazetteer for stemming, and thus it cannot handle unknown names. We developed a new method to derive generalizable affixes for morphologically rich language based on Wikipedia markups. Wikipedia as background features for IE: Wikipedia pages have been used as additio"
P17-1178,U08-1016,1,0.909684,"Missing"
P17-1178,N15-1119,1,0.926167,"in English KB Salience, Similarity and Coherence Comparison Linking Translated and Linked Mentions m1 t1 e1 m2 t2 e1 m3 t3 e2 m4 t4 e3 m5 t5 NIL m6 t6 NIL Figure 3: Cross-lingual Entity Linking Overview 3.2 Name Translation The cross-lingual Wikipedia title pairs, generated through crowd-sourcing, generally follow a consistent style and format in each language. From Table 2 we can see that the order of modifier and head word keeps consistent in Turkish and English titles. 8 http://www.darpa.mil/program/low-resource-languagesfor-emergent-incidents 1949 to the KB, similar to our previous work (Pan et al., 2015). The only difference is that we construct knowledge networks (KNs) g(ti ) for T based on their co-occurrence within a context window 9 instead of their AMR relations, because AMR parsing is not available for foreign languages. For each translated name mention ti , an initial list of candidate entities E(ti ) = {e1 , e2 , ..., ek } is generated based on a surface form dictionary mined from KB properties (e.g., redirects, names, aliases). If no surface form can be matched then we determine the mention as unlinkable. Then we construct KNs g(ej ) for each entity candidate ej in ti ’s entity candi"
P17-1178,N06-1025,0,0.0111175,"ms both name identification and typing and takes advantage of richer structures in the KBs. Previous work on Arabic name tagging (Althobaiti et al., 2014) extracted entity titles as a gazetteer for stemming, and thus it cannot handle unknown names. We developed a new method to derive generalizable affixes for morphologically rich language based on Wikipedia markups. Wikipedia as background features for IE: Wikipedia pages have been used as additional features to improve various Information Extraction (IE) tasks, including name tagging (Kazama and Torisawa, 2007), coreference resolution (Paolo Ponzetto and Strube, 2006), relation extraction (Chan and Roth, 2010) and event extraction (Hogue et al., 2014). Other automatic name annotation generation methods have been proposed, including KB driven distant supervision (An et al., 2003; Mintz et al., 2009; Ren et al., 2015) and cross-lingual projection (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013; Wang and Manning, 2014; Zhang et al., 2016b). Multi-lingual name tagging: Some recent research (Zhang et al., 2016a; Littell et al., 2016; Tsai et al., 2016) under the DARPA LORELEI program focused on developing name tagging techniques for low-"
P17-1178,P08-1001,0,0.0334855,"1 languages. But their methods required labeled data and name transliteration. We share the same goal as (Sil and Florian, 2016) to extend cross-lingual entity linking to all languages in Wikipedia. They exploited Wikipedia links to train a supervised linker. We mine reliable word translations from cross-lingual Wikipedia titles, which enables us to adopt unsupervised English entity linking techniques such as (Pan et al., 2015) to directly link translated English name mentions to English KB. Efforts to save annotation cost for name tagging: Some previous work including (Ji and Grishman, 2006; Richman and Schone, 2008; Althobaiti et al., 2013) exploited semi-supervised methods to save annotation cost. We observed that self-training can provide further gains when the training data contains certain amount of noise. 6 Conclusions and Future Work We developed a simple yet effective framework that can extract names from 282 languages and link them to an English KB. This framework follows a fully automatic training and testing pipeline, without the needs of any manual annotations or knowledge from native speakers. We evaluated our framework on both Wikipedia articles and external formal and informal texts and ob"
P17-1178,U09-1004,1,0.913527,"Missing"
P17-1178,P08-2030,0,0.0100737,"ntions (Section 2.4). Customize annotations through cross-lingual topic transfer. For the first time, we propose to customize name annotations for specific downstream applications. Again, we use a cross-lingual knowledge transfer strategy to leverage the widely available English corpora to choose entities with specific Wikipedia topic categories (Section 2.5). Derive morphology analysis from Wikipedia markups. Another unique challenge for morphologically rich languages is to segment each token into its stemming form and affixes. Previous methods relied on either high-cost supervised learning (Roth et al., 2008; Mahmoudi et al., 2013; Ahlberg et al., 2015), or low-quality unsupervised learning (Gr¨onroos et al., 2014; Ruokolainen et al., 2016). We exploit Wikipedia markups to automatically learn affixes as language-specific features (Section 2.3). Mine word translations from cross-lingual links. Name translation is a crucial step to generate candidate entities in cross-lingual entity linking. Only a small percentage of names can be directly translated by matching against cross-lingual Wikipedia title pairs. Based on the observation that Wikipedia titles within any language tend to follow a consisten"
P17-1178,P16-1213,0,0.0247317,"notations for projection (Tsai et al., 2016), some input from a native speaker, either through manual annotations (Littell et al., 2016), or a linguistic survey (Zhang et al., 2016a). Without using any manual annotations, our name taggers outperform previous methods on the same data sets for many languages. Multi-lingual entity linking: NIST TAC-KBP Tri-lingual entity linking (Ji et al., 2016) focused on three languages: English, Chinese and Spanish. (McNamee et al., 2011) extended it to 21 languages. But their methods required labeled data and name transliteration. We share the same goal as (Sil and Florian, 2016) to extend cross-lingual entity linking to all languages in Wikipedia. They exploited Wikipedia links to train a supervised linker. We mine reliable word translations from cross-lingual Wikipedia titles, which enables us to adopt unsupervised English entity linking techniques such as (Pan et al., 2015) to directly link translated English name mentions to English KB. Efforts to save annotation cost for name tagging: Some previous work including (Ji and Grishman, 2006; Richman and Schone, 2008; Althobaiti et al., 2013) exploited semi-supervised methods to save annotation cost. We observed that s"
P17-1178,K16-1022,0,0.117852,"luding name tagging (Kazama and Torisawa, 2007), coreference resolution (Paolo Ponzetto and Strube, 2006), relation extraction (Chan and Roth, 2010) and event extraction (Hogue et al., 2014). Other automatic name annotation generation methods have been proposed, including KB driven distant supervision (An et al., 2003; Mintz et al., 2009; Ren et al., 2015) and cross-lingual projection (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013; Wang and Manning, 2014; Zhang et al., 2016b). Multi-lingual name tagging: Some recent research (Zhang et al., 2016a; Littell et al., 2016; Tsai et al., 2016) under the DARPA LORELEI program focused on developing name tagging techniques for low-resource languages. These approaches require English annotations for projection (Tsai et al., 2016), some input from a native speaker, either through manual annotations (Littell et al., 2016), or a linguistic survey (Zhang et al., 2016a). Without using any manual annotations, our name taggers outperform previous methods on the same data sets for many languages. Multi-lingual entity linking: NIST TAC-KBP Tri-lingual entity linking (Ji et al., 2016) focused on three languages: English, Chinese and Spanish. (Mc"
P17-1178,P13-1106,0,0.0824147,"kups. Wikipedia as background features for IE: Wikipedia pages have been used as additional features to improve various Information Extraction (IE) tasks, including name tagging (Kazama and Torisawa, 2007), coreference resolution (Paolo Ponzetto and Strube, 2006), relation extraction (Chan and Roth, 2010) and event extraction (Hogue et al., 2014). Other automatic name annotation generation methods have been proposed, including KB driven distant supervision (An et al., 2003; Mintz et al., 2009; Ren et al., 2015) and cross-lingual projection (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013; Wang and Manning, 2014; Zhang et al., 2016b). Multi-lingual name tagging: Some recent research (Zhang et al., 2016a; Littell et al., 2016; Tsai et al., 2016) under the DARPA LORELEI program focused on developing name tagging techniques for low-resource languages. These approaches require English annotations for projection (Tsai et al., 2016), some input from a native speaker, either through manual annotations (Littell et al., 2016), or a linguistic survey (Zhang et al., 2016a). Without using any manual annotations, our name taggers outperform previous methods on the same data sets for many l"
P17-1178,Q14-1005,0,0.016415,"background features for IE: Wikipedia pages have been used as additional features to improve various Information Extraction (IE) tasks, including name tagging (Kazama and Torisawa, 2007), coreference resolution (Paolo Ponzetto and Strube, 2006), relation extraction (Chan and Roth, 2010) and event extraction (Hogue et al., 2014). Other automatic name annotation generation methods have been proposed, including KB driven distant supervision (An et al., 2003; Mintz et al., 2009; Ren et al., 2015) and cross-lingual projection (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013; Wang and Manning, 2014; Zhang et al., 2016b). Multi-lingual name tagging: Some recent research (Zhang et al., 2016a; Littell et al., 2016; Tsai et al., 2016) under the DARPA LORELEI program focused on developing name tagging techniques for low-resource languages. These approaches require English annotations for projection (Tsai et al., 2016), some input from a native speaker, either through manual annotations (Littell et al., 2016), or a linguistic survey (Zhang et al., 2016a). Without using any manual annotations, our name taggers outperform previous methods on the same data sets for many languages. Multi-lingual"
P17-1178,J90-1003,0,0.414992,"Missing"
P17-1178,P15-2048,0,0.0158903,"and Cucerzan, 2008; Mika et al., 2008; Ringland et al., 2009; Alotaibi and Lee, 2012; Nothman et al., 2013; Althobaiti et al., 2014). Most of these previous methods manually classified many English Wikipedia entries into pre-defined entity types. In contrast, our approach doesn’t need any manual annotations or language-specific features, while generates both coarse-grained and fine-grained types. Many fine-grained entity typing approaches (Fleischman and Hovy, 2002; Giuliano, 1953 2009; Ekbal et al., 2010; Ling and Weld, 2012; Yosef et al., 2012; Nakashole et al., 2013; Gillick et al., 2014; Yogatama et al., 2015; Del Corro et al., 2015) also created annotations based on Wikipedia anchor links. Our framework performs both name identification and typing and takes advantage of richer structures in the KBs. Previous work on Arabic name tagging (Althobaiti et al., 2014) extracted entity titles as a gazetteer for stemming, and thus it cannot handle unknown names. We developed a new method to derive generalizable affixes for morphologically rich language based on Wikipedia markups. Wikipedia as background features for IE: Wikipedia pages have been used as additional features to improve various Information E"
P17-1178,C12-2133,0,0.0297288,"Missing"
P17-1178,N16-1029,1,0.814483,"IE: Wikipedia pages have been used as additional features to improve various Information Extraction (IE) tasks, including name tagging (Kazama and Torisawa, 2007), coreference resolution (Paolo Ponzetto and Strube, 2006), relation extraction (Chan and Roth, 2010) and event extraction (Hogue et al., 2014). Other automatic name annotation generation methods have been proposed, including KB driven distant supervision (An et al., 2003; Mintz et al., 2009; Ren et al., 2015) and cross-lingual projection (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013; Wang and Manning, 2014; Zhang et al., 2016b). Multi-lingual name tagging: Some recent research (Zhang et al., 2016a; Littell et al., 2016; Tsai et al., 2016) under the DARPA LORELEI program focused on developing name tagging techniques for low-resource languages. These approaches require English annotations for projection (Tsai et al., 2016), some input from a native speaker, either through manual annotations (Littell et al., 2016), or a linguistic survey (Zhang et al., 2016a). Without using any manual annotations, our name taggers outperform previous methods on the same data sets for many languages. Multi-lingual entity linking: NIST"
P17-1178,C16-1045,1,0.911502,"Missing"
P17-2091,D16-1139,0,0.0282704,"ly time-consuming for millions of sentences. Slow decoding speed is partly due to the large target vocabulary size V, which is usually in the tens of thousands. The first two columns of Table 1 show the breakdown of the runtimes required by sub-modules to decode 1812 Japanese sentences to English using a sequence-to-sequence model with local attention (Luong et al., 2015). 1. Using special hardware, such as GPU and Tensor Processing Unit (TPU), and lowprecision calculation (Wu et al., 2016). 2. Compressing deep neural models through knowledge distillation and weight pruning (See et al., 2016; Kim and Rush, 2016). 574 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 574–579 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2091 find current LSH algorithms have a poor performance/speed trade-off on GPU, due to the large overhead introduced by many hash table lookups and list-merging involved in LSH. 3. Several variants of Softmax have been proposed to solve its poor scaling properties on large vocabularies. Morin and Bengio (2005) propose hierarchical softmax, where a"
P17-2091,D15-1166,0,0.179286,"Missing"
P17-2091,P16-2021,0,0.0822251,"15; Zoph et al., 2016). Although these two approaches provide good speedups for training, they still suffer at test time. Chen et al. (2016) introduces differentiated softmax, where frequent words have more parameters in the embedding and rare words have less, offering speedups on both training and testing. 2. For our word alignment method, we find that only the candidate list derived from lexical translation table of IBM model 4 is adequate to achieve good BLEU/speedup trade-off for decoding on GPU. There is no need to combine the top frequent words or words from phrase table, as proposed in Mi et al. (2016). 3. We conduct our experiments on GPU and provide a detailed analysis of BLEU/speedup trade-off on both resource-rich/poor language pairs and both attention/non-attention NMT models. We achieve more than 2x speedup on 4 language pairs with only a tiny BLEU drop, demonstrating the robustness and efficiency of our methods. In this work, we aim to speed up decoding by shrinking the run-time target vocabulary size, and this approach is orthogonal to the methods above. It is important to note that approaches 1 and 2 will maintain or even increase the ratio of target word embedding parameters to th"
P17-2091,K16-1029,0,0.020564,", which is extremely time-consuming for millions of sentences. Slow decoding speed is partly due to the large target vocabulary size V, which is usually in the tens of thousands. The first two columns of Table 1 show the breakdown of the runtimes required by sub-modules to decode 1812 Japanese sentences to English using a sequence-to-sequence model with local attention (Luong et al., 2015). 1. Using special hardware, such as GPU and Tensor Processing Unit (TPU), and lowprecision calculation (Wu et al., 2016). 2. Compressing deep neural models through knowledge distillation and weight pruning (See et al., 2016; Kim and Rush, 2016). 574 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 574–579 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2091 find current LSH algorithms have a poor performance/speed trade-off on GPU, due to the large overhead introduced by many hash table lookups and list-merging involved in LSH. 3. Several variants of Softmax have been proposed to solve its poor scaling properties on large vocabularies. Morin and Bengio (2005) propose hierarch"
P17-2091,P16-1186,0,0.0197948,"rin and Bengio (2005) propose hierarchical softmax, where at each step log2 V binary classifications are performed instead of a single classification on a large number of classes. Gutmann and Hyv¨arinen (2010) propose noise-contrastive estimation which discriminate between positive labels and k (k << V ) negative labels sampled from a distribution, and is applied successfully on natural language processing tasks (Mnih and Teh, 2012; Vaswani et al., 2013; Williams et al., 2015; Zoph et al., 2016). Although these two approaches provide good speedups for training, they still suffer at test time. Chen et al. (2016) introduces differentiated softmax, where frequent words have more parameters in the embedding and rare words have less, offering speedups on both training and testing. 2. For our word alignment method, we find that only the candidate list derived from lexical translation table of IBM model 4 is adequate to achieve good BLEU/speedup trade-off for decoding on GPU. There is no need to combine the top frequent words or words from phrase table, as proposed in Mi et al. (2016). 3. We conduct our experiments on GPU and provide a detailed analysis of BLEU/speedup trade-off on both resource-rich/poor"
P17-2091,D13-1140,0,0.0309861,"sh table lookups and list-merging involved in LSH. 3. Several variants of Softmax have been proposed to solve its poor scaling properties on large vocabularies. Morin and Bengio (2005) propose hierarchical softmax, where at each step log2 V binary classifications are performed instead of a single classification on a large number of classes. Gutmann and Hyv¨arinen (2010) propose noise-contrastive estimation which discriminate between positive labels and k (k << V ) negative labels sampled from a distribution, and is applied successfully on natural language processing tasks (Mnih and Teh, 2012; Vaswani et al., 2013; Williams et al., 2015; Zoph et al., 2016). Although these two approaches provide good speedups for training, they still suffer at test time. Chen et al. (2016) introduces differentiated softmax, where frequent words have more parameters in the embedding and rare words have less, offering speedups on both training and testing. 2. For our word alignment method, we find that only the candidate list derived from lexical translation table of IBM model 4 is adequate to achieve good BLEU/speedup trade-off for decoding on GPU. There is no need to combine the top frequent words or words from phrase t"
P17-2091,W15-4110,0,0.067949,"Missing"
P17-2091,D16-1137,0,0.0404157,"Missing"
P17-2091,P15-1001,0,0.0440386,"greater portion of the decoding time. A small run-time vocabulary will dramatically reduce the time spent on these two portions and gain a further speedup even after applying other speedup methods. To shrink the run-time target vocabulary, our first method uses Locality Sensitive Hashing. Vijayanarasimhan et al. (2015) successfully applies it on CPUs and gains speedup on single step prediction tasks such as image classification and video identification. Our second method is to use word alignments to select a very small number of candidate target words given the source sentence. Recent works (Jean et al., 2015; Mi et al., 2016; L’Hostis et al., 2016) apply a similar strategy and report speedups for decoding on CPUs on richsource language pairs. Our major contributions are: 2 Methods At each step during decoding, the softmax function is calculated as: T ehi wj +bj P (y = j|hi ) = PV hT i wk +bk k=1 e (1) where P (y = j|hi ) is the probability of word j = 1...V given the hidden vector hi ∈ Rd , i = 1...B. B represents the beam size. wj ∈ Rd is output word embedding and bj ∈ R is the corresponding bias. The complexity is O(dBV ). To speed up softmax, we use word frequency, locality sensitive hashing,"
P17-2091,1983.tc-1.13,0,0.689291,"Missing"
P17-2091,N16-1145,1,0.851887,"LSH. 3. Several variants of Softmax have been proposed to solve its poor scaling properties on large vocabularies. Morin and Bengio (2005) propose hierarchical softmax, where at each step log2 V binary classifications are performed instead of a single classification on a large number of classes. Gutmann and Hyv¨arinen (2010) propose noise-contrastive estimation which discriminate between positive labels and k (k << V ) negative labels sampled from a distribution, and is applied successfully on natural language processing tasks (Mnih and Teh, 2012; Vaswani et al., 2013; Williams et al., 2015; Zoph et al., 2016). Although these two approaches provide good speedups for training, they still suffer at test time. Chen et al. (2016) introduces differentiated softmax, where frequent words have more parameters in the embedding and rare words have less, offering speedups on both training and testing. 2. For our word alignment method, we find that only the candidate list derived from lexical translation table of IBM model 4 is adequate to achieve good BLEU/speedup trade-off for decoding on GPU. There is no need to combine the top frequent words or words from phrase table, as proposed in Mi et al. (2016). 3. W"
P17-2091,P16-1159,0,\N,Missing
P17-4008,D16-1126,1,0.598922,"nerated poem. When poets compose a poem, they usually need to revise and polish the draft from different aspects (e.g., word choice, sentiment, alliteration, etc.) for several iterations until satisfaction. This is a crucial step for poetry creation. However, given a user-supplied topic or phrase, most existing automated systems can only generate different poems by using different random seeds, providing no other support for the user to polish the generated poem in a desired direction. 3. Slow generation speed. Generating a poem may require a heavy search procedure. For example, the system of Ghazvininejad et al. (2016) needs 20 seconds for a four-line poem. Such slow speed is a serious bottleneck for a smooth user experience, and prevents the large-scale collection of feedback for system tuning. Introduction Automated poetry generation is attracting increasing research effort. Researchers approach the problem by using grammatical and semantic templates (Oliveira, 2009, 2012) or treating the generation task as a translation/summarization task (Zhou et al., 2009; He et al., 2012; Yan et al., 2013; Zhang and Lapata, 2014; Yi et al., 2016; Wang et al., 2016; Ghazvininejad et al., 2016). However, such poetry gen"
P17-4008,D14-1074,0,0.199057,"d. Generating a poem may require a heavy search procedure. For example, the system of Ghazvininejad et al. (2016) needs 20 seconds for a four-line poem. Such slow speed is a serious bottleneck for a smooth user experience, and prevents the large-scale collection of feedback for system tuning. Introduction Automated poetry generation is attracting increasing research effort. Researchers approach the problem by using grammatical and semantic templates (Oliveira, 2009, 2012) or treating the generation task as a translation/summarization task (Zhou et al., 2009; He et al., 2012; Yan et al., 2013; Zhang and Lapata, 2014; Yi et al., 2016; Wang et al., 2016; Ghazvininejad et al., 2016). However, such poetry generation systems face these challenges: This work is based on our previous poetry generation system called Hafez (Ghazvininejad et al., 2016), which generates poems in three steps: (1) search for related rhyme words given usersupplied topic, (2) create a finite-state acceptor (FSA) that incorporates the rhyme words and controls meter, and (3) use a recurrent neural network (RNN) to generate the poem string, guided by the FSA. We address the above-mentioned challenges with the following approaches: 1. Diff"
P17-4008,Y09-1006,0,0.0330897,"d poem in a desired direction. 3. Slow generation speed. Generating a poem may require a heavy search procedure. For example, the system of Ghazvininejad et al. (2016) needs 20 seconds for a four-line poem. Such slow speed is a serious bottleneck for a smooth user experience, and prevents the large-scale collection of feedback for system tuning. Introduction Automated poetry generation is attracting increasing research effort. Researchers approach the problem by using grammatical and semantic templates (Oliveira, 2009, 2012) or treating the generation task as a translation/summarization task (Zhou et al., 2009; He et al., 2012; Yan et al., 2013; Zhang and Lapata, 2014; Yi et al., 2016; Wang et al., 2016; Ghazvininejad et al., 2016). However, such poetry generation systems face these challenges: This work is based on our previous poetry generation system called Hafez (Ghazvininejad et al., 2016), which generates poems in three steps: (1) search for related rhyme words given usersupplied topic, (2) create a finite-state acceptor (FSA) that incorporates the rhyme words and controls meter, and (3) use a recurrent neural network (RNN) to generate the poem string, guided by the FSA. We address the above-"
P17-4008,baccianella-etal-2010-sentiwordnet,0,\N,Missing
P18-1213,N15-1146,0,0.0222852,"s: I, me: 2-5 Cousin: 2, 5 Emotional Reaction: … Line 3, I, me: sad/disgusted/angry Figure 3: The annotation pipeline for the fine-grained annotations with an example story. In addition, they depict multiple interactions between story characters, presenting rich opportunities to reason about character motivations and reactions. Furthermore, there are more than 98k such stories currently available covering a wide range of everyday scenarios. Unique Challenges While there have been a variety of annotated resources developed on the related topics of sentiment analysis (Mohammad and Turney, 2013; Deng and Wiebe, 2015), entity tracking (Hoffart et al., 2011; Weston et al., 2015), and story understanding (Goyal et al., 2010; Ouyang and McKeown, 2015; Lukin et al., 2016), our study is the first to annotate the full chains of mental state effects for story characters. This poses several unique challenges as annotations require (1) interpreting discourse (2) understanding implicit causal effects, and (3) understanding formal psychology theory categories. In prior literature, annotations of this complexity have typically been performed by experts (Deng and Wiebe, 2015; Ouyang and McKeown, 2015). While reliable,"
P18-1213,E17-1059,0,0.0127701,"rm the strong baseline on both metrics, indicating that the generated short explanations are closer semantically to the reference annotation. 8 Related work Mental State Annotations Incorporating emotion theories into NLP tasks has been explored in previous projects. Ghosh et al. (2017) modulate language model distributions by increasing the probability of words that express certain affective LIWC (Tausczik and Pennebaker, 2016) categories. More generally, various projects tackle the problem of generating text from a set of attributes like sentiment or generic-ness (Ficler and Goldberg, 2017; Dong et al., 2017). Similarly, there is also a body of research in reasoning about commonsense stories and discourse (Li and Jurafsky, 2017; Mostafazadeh et al., 2016) or detecting emotional stimuli in stories (Gui et al., 2017). Previous work in plot units (Lehnert, 1981) developed formalisms for affect and mental state in story narratives that included motivations and reactions. In our work, we collect mental state annotations for stories to used as a new resource in this space. Modeling Entity State Recently, novel works in language modeling (Ji et al., 2017; Yang et al., 2016), question answering (Henaff et"
P18-1213,W17-4912,0,0.0165481,"owever, all models outperform the strong baseline on both metrics, indicating that the generated short explanations are closer semantically to the reference annotation. 8 Related work Mental State Annotations Incorporating emotion theories into NLP tasks has been explored in previous projects. Ghosh et al. (2017) modulate language model distributions by increasing the probability of words that express certain affective LIWC (Tausczik and Pennebaker, 2016) categories. More generally, various projects tackle the problem of generating text from a set of attributes like sentiment or generic-ness (Ficler and Goldberg, 2017; Dong et al., 2017). Similarly, there is also a body of research in reasoning about commonsense stories and discourse (Li and Jurafsky, 2017; Mostafazadeh et al., 2016) or detecting emotional stimuli in stories (Gui et al., 2017). Previous work in plot units (Lehnert, 1981) developed formalisms for affect and mental state in story narratives that included motivations and reactions. In our work, we collect mental state annotations for stories to used as a new resource in this space. Modeling Entity State Recently, novel works in language modeling (Ji et al., 2017; Yang et al., 2016), question"
P18-1213,P17-1059,0,0.0166532,"the task of generating explanations of motivations and emotions in Table 5. Because the explanations are closely tied to emotional and motivation states, the randomly selected explanation can often be close in embedding space to the reference explanations, making the random baseline fairly competitive. However, all models outperform the strong baseline on both metrics, indicating that the generated short explanations are closer semantically to the reference annotation. 8 Related work Mental State Annotations Incorporating emotion theories into NLP tasks has been explored in previous projects. Ghosh et al. (2017) modulate language model distributions by increasing the probability of words that express certain affective LIWC (Tausczik and Pennebaker, 2016) categories. More generally, various projects tackle the problem of generating text from a set of attributes like sentiment or generic-ness (Ficler and Goldberg, 2017; Dong et al., 2017). Similarly, there is also a body of research in reasoning about commonsense stories and discourse (Li and Jurafsky, 2017; Mostafazadeh et al., 2016) or detecting emotional stimuli in stories (Gui et al., 2017). Previous work in plot units (Lehnert, 1981) developed for"
P18-1213,D10-1008,0,0.0957523,"Missing"
P18-1213,D17-1167,0,0.0265127,"s into NLP tasks has been explored in previous projects. Ghosh et al. (2017) modulate language model distributions by increasing the probability of words that express certain affective LIWC (Tausczik and Pennebaker, 2016) categories. More generally, various projects tackle the problem of generating text from a set of attributes like sentiment or generic-ness (Ficler and Goldberg, 2017; Dong et al., 2017). Similarly, there is also a body of research in reasoning about commonsense stories and discourse (Li and Jurafsky, 2017; Mostafazadeh et al., 2016) or detecting emotional stimuli in stories (Gui et al., 2017). Previous work in plot units (Lehnert, 1981) developed formalisms for affect and mental state in story narratives that included motivations and reactions. In our work, we collect mental state annotations for stories to used as a new resource in this space. Modeling Entity State Recently, novel works in language modeling (Ji et al., 2017; Yang et al., 2016), question answering (Henaff et al., 2017), and text generation (Kiddon et al., 2016; Bosselut et al., 2018) have shown that modeling entity state explicitly can boost performance while providing a preliminary interface for interpreting a mo"
P18-1213,P82-1020,0,0.840014,"Missing"
P18-1213,D11-1072,0,0.0926033,"Missing"
P18-1213,D17-1195,1,0.863429,"ent or generic-ness (Ficler and Goldberg, 2017; Dong et al., 2017). Similarly, there is also a body of research in reasoning about commonsense stories and discourse (Li and Jurafsky, 2017; Mostafazadeh et al., 2016) or detecting emotional stimuli in stories (Gui et al., 2017). Previous work in plot units (Lehnert, 1981) developed formalisms for affect and mental state in story narratives that included motivations and reactions. In our work, we collect mental state annotations for stories to used as a new resource in this space. Modeling Entity State Recently, novel works in language modeling (Ji et al., 2017; Yang et al., 2016), question answering (Henaff et al., 2017), and text generation (Kiddon et al., 2016; Bosselut et al., 2018) have shown that modeling entity state explicitly can boost performance while providing a preliminary interface for interpreting a model’s prediction. Entity modeling in these works, however, was limited to tracking entity reference (Kiddon et al., 2016; Yang et al., 2016; Ji et al., 2017), recognizing entity state similarity (Henaff et al., 2017) or predicting simple attributes from entity states (Bosselut et al., 2018). Our work provides a new dataset for tracking e"
P18-1213,D16-1032,1,0.827555,"of research in reasoning about commonsense stories and discourse (Li and Jurafsky, 2017; Mostafazadeh et al., 2016) or detecting emotional stimuli in stories (Gui et al., 2017). Previous work in plot units (Lehnert, 1981) developed formalisms for affect and mental state in story narratives that included motivations and reactions. In our work, we collect mental state annotations for stories to used as a new resource in this space. Modeling Entity State Recently, novel works in language modeling (Ji et al., 2017; Yang et al., 2016), question answering (Henaff et al., 2017), and text generation (Kiddon et al., 2016; Bosselut et al., 2018) have shown that modeling entity state explicitly can boost performance while providing a preliminary interface for interpreting a model’s prediction. Entity modeling in these works, however, was limited to tracking entity reference (Kiddon et al., 2016; Yang et al., 2016; Ji et al., 2017), recognizing entity state similarity (Henaff et al., 2017) or predicting simple attributes from entity states (Bosselut et al., 2018). Our work provides a new dataset for tracking emotional reactions and motivations of characters in stories. 9 Conclusion We present a large scale datas"
P18-1213,D14-1181,0,0.0199399,"is applied to these features to yield hs : hs = (Ws v s + bs ) (2) where Ws 2 Rd⇥H . The character vector hc is encoded in the same way on sentences in the context pertaining to the character. GloVe We extract pretrained Glove vectors (Pennington et al., 2014) for each word in V . The word embeddings are max-pooled, yielding embedding v s 2 RH , where H is the dimensionality of the Glove vectors. Using this max-pooled representation, hs and hc are extracted in the same manner as for TF-IDF features (Equation 2). CNN We implement a CNN text categorization model using the same configuration as Kim (2014) to encode the sentence words. A sentence is represented as a matrix, v s 2 RM ⇥d where each row is a word embedding xsn for a word wns 2 xs . 2294 v s = [xs0 , xs1 , . . . , xsN ] s s h = CNN(v ) (3) (4) where CNN represents the categorization model from (Kim, 2014). The character vector hc is encoded in the same way with a separate CNN. Implementation details are provided in the appendix. LSTM A two-layer bi-LSTM encodes the sentence words and concatenates the final time step hidden states from both directions to yield hs . The character vector hc is encoded the same way. REN We use the “tie"
P18-1213,D17-1019,0,0.0165332,"e reference annotation. 8 Related work Mental State Annotations Incorporating emotion theories into NLP tasks has been explored in previous projects. Ghosh et al. (2017) modulate language model distributions by increasing the probability of words that express certain affective LIWC (Tausczik and Pennebaker, 2016) categories. More generally, various projects tackle the problem of generating text from a set of attributes like sentiment or generic-ness (Ficler and Goldberg, 2017; Dong et al., 2017). Similarly, there is also a body of research in reasoning about commonsense stories and discourse (Li and Jurafsky, 2017; Mostafazadeh et al., 2016) or detecting emotional stimuli in stories (Gui et al., 2017). Previous work in plot units (Lehnert, 1981) developed formalisms for affect and mental state in story narratives that included motivations and reactions. In our work, we collect mental state annotations for stories to used as a new resource in this space. Modeling Entity State Recently, novel works in language modeling (Ji et al., 2017; Yang et al., 2016), question answering (Henaff et al., 2017), and text generation (Kiddon et al., 2016; Bosselut et al., 2018) have shown that modeling entity state expli"
P18-1213,D16-1230,0,0.0522412,"Missing"
P18-1213,L16-1163,0,0.0201785,"an example story. In addition, they depict multiple interactions between story characters, presenting rich opportunities to reason about character motivations and reactions. Furthermore, there are more than 98k such stories currently available covering a wide range of everyday scenarios. Unique Challenges While there have been a variety of annotated resources developed on the related topics of sentiment analysis (Mohammad and Turney, 2013; Deng and Wiebe, 2015), entity tracking (Hoffart et al., 2011; Weston et al., 2015), and story understanding (Goyal et al., 2010; Ouyang and McKeown, 2015; Lukin et al., 2016), our study is the first to annotate the full chains of mental state effects for story characters. This poses several unique challenges as annotations require (1) interpreting discourse (2) understanding implicit causal effects, and (3) understanding formal psychology theory categories. In prior literature, annotations of this complexity have typically been performed by experts (Deng and Wiebe, 2015; Ouyang and McKeown, 2015). While reliable, these annotations are prohibitively expensive to scale up. Therefore, we introduce a new annotation framework that pipelines a set of smaller isolated ta"
P18-1213,N16-1098,0,0.506936,"soning is remarkably hard for both statistical and neural machine readers – despite being trivial for humans. This stark performance gap between humans and machines is not surprising as most powerful language models have been designed to effectively learn local fluency patterns. Consequently, they generally lack the ability to abstract away from surface patterns in text to model more complex implied dynamics, such as intuiting characters’ mental states or predicting their plausible next actions. In this paper, we construct a new annotation formalism to densely label commonsense short stories (Mostafazadeh et al., 2016) in terms of the mental states of the characters. The resultangry [est eem ] They grew tired and started playing worse after a while. E M need rest E M The instructor was furious and threw his chair. afra id E M He cancelled practice and [dis gust, fe ar] expected us to perform tomorrow. E M Figure 1: A story example with partial annotations for motivations (dashed) and emotional reactions (solid). Open text explanations are in black (e.g., “frustrated”) and formal theory labels are in blue with brackets (e.g., “[esteem]”). ing dataset offers three unique properties. First, as highlighted in F"
P18-1213,D15-1257,0,0.0287071,"e-grained annotations with an example story. In addition, they depict multiple interactions between story characters, presenting rich opportunities to reason about character motivations and reactions. Furthermore, there are more than 98k such stories currently available covering a wide range of everyday scenarios. Unique Challenges While there have been a variety of annotated resources developed on the related topics of sentiment analysis (Mohammad and Turney, 2013; Deng and Wiebe, 2015), entity tracking (Hoffart et al., 2011; Weston et al., 2015), and story understanding (Goyal et al., 2010; Ouyang and McKeown, 2015; Lukin et al., 2016), our study is the first to annotate the full chains of mental state effects for story characters. This poses several unique challenges as annotations require (1) interpreting discourse (2) understanding implicit causal effects, and (3) understanding formal psychology theory categories. In prior literature, annotations of this complexity have typically been performed by experts (Deng and Wiebe, 2015; Ouyang and McKeown, 2015). While reliable, these annotations are prohibitively expensive to scale up. Therefore, we introduce a new annotation framework that pipelines a set o"
P18-1213,D14-1162,0,0.0941119,"ific context (C[ej ] ! hc ) and concatenating these encodings (he = [hc ; hs ]). We describe the encoders below: TF-IDF We learn a TD-IDF model on the full training corpus of Mostafazadeh et al. (2016) (excluding the stories in our dev/test sets). To encode the sentence, we extract TF-IDF features for its words, yielding v s 2 RV . A projection and nonlinearity is applied to these features to yield hs : hs = (Ws v s + bs ) (2) where Ws 2 Rd⇥H . The character vector hc is encoded in the same way on sentences in the context pertaining to the character. GloVe We extract pretrained Glove vectors (Pennington et al., 2014) for each word in V . The word embeddings are max-pooled, yielding embedding v s 2 RH , where H is the dimensionality of the Glove vectors. Using this max-pooled representation, hs and hc are extracted in the same manner as for TF-IDF features (Equation 2). CNN We implement a CNN text categorization model using the same configuration as Kim (2014) to encode the sentence words. A sentence is represented as a matrix, v s 2 RM ⇥d where each row is a word embedding xsn for a word wns 2 xs . 2294 v s = [xs0 , xs1 , . . . , xsN ] s s h = CNN(v ) (3) (4) where CNN represents the categorization model"
P18-1213,D16-1061,0,0.0273026,"o the physiological needs Maslow category, the food and rest motives from Reiss (2004) are very different. While the Reiss theory allows for finergrained annotations of motivation, the larger set of abstract concepts can be overwhelming for annotators. Motivated by Straker (2013), we design a hybrid approach, where Reiss labels are annotated as sub-categories of Maslow categories. 2.2 Emotion Theory Among several theories of emotion, we work with the “wheel of emotions” of Plutchik (1980), as it has been a common choice in prior literature on emotion categorization (Mohammad and Turney, 2013; Zhou et al., 2016). We use the eight basic emotional dimensions as illustrated in Figure 2. 2.3 Mental State Explanations In addition to the motivation and emotion categories derived from psychology theories, we also obtain open text descriptions of character mental states. These open text descriptions allow learning computational models that can explain the mental states of characters in natural language, which is likely to be more accessible and informative to end users than having theory categories alone. Collecting both theory categories and open text also allows us to learn the automatic mappings between t"
P18-1213,W17-0906,0,\N,Missing
P18-2042,D16-1126,1,0.85459,"erˆome Louradour, Ronan Collobert, and Jason Weston. 2009. Curriculum learning. In Proceedings of the 26th Annual International Conference on Machine Learning. Related work Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoder-decoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. Deep neural networks are widely applied to text generation tasks such as poetry creation (Greene et al., 2010; Ghazvininejad et al., 2016; Zhang et al., 2017), recipe generation (Kiddon et al., 2016), abstractive summarization (Gu et al., 2016; Wang and Ling, 2016; See et al., 2017), and biography generation (Lebret et al., 2016; Liu et al., 2018). We introduce a new task of generating paper abstracts from the given titles. We design a Writing-editing Network which shares ideas with Curriculum Learning (Bengio et al., 2009), where training on a data point from coarse to finegrained can lead to better convergence (Krueger and Dayan, 2009). Our model is different from previous theme-rewriting (Polozov et al., 2015; Koncel-Kedzior"
P18-2042,D10-1051,1,0.710458,"ns. Yoshua Bengio, J´erˆome Louradour, Ronan Collobert, and Jason Weston. 2009. Curriculum learning. In Proceedings of the 26th Annual International Conference on Machine Learning. Related work Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoder-decoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. Deep neural networks are widely applied to text generation tasks such as poetry creation (Greene et al., 2010; Ghazvininejad et al., 2016; Zhang et al., 2017), recipe generation (Kiddon et al., 2016), abstractive summarization (Gu et al., 2016; Wang and Ling, 2016; See et al., 2017), and biography generation (Lebret et al., 2016; Liu et al., 2018). We introduce a new task of generating paper abstracts from the given titles. We design a Writing-editing Network which shares ideas with Curriculum Learning (Bengio et al., 2009), where training on a data point from coarse to finegrained can lead to better convergence (Krueger and Dayan, 2009). Our model is different from previous theme-rewriting (Polozov"
P18-2042,P16-1154,0,0.0520132,"ernational Conference on Machine Learning. Related work Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoder-decoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. Deep neural networks are widely applied to text generation tasks such as poetry creation (Greene et al., 2010; Ghazvininejad et al., 2016; Zhang et al., 2017), recipe generation (Kiddon et al., 2016), abstractive summarization (Gu et al., 2016; Wang and Ling, 2016; See et al., 2017), and biography generation (Lebret et al., 2016; Liu et al., 2018). We introduce a new task of generating paper abstracts from the given titles. We design a Writing-editing Network which shares ideas with Curriculum Learning (Bengio et al., 2009), where training on a data point from coarse to finegrained can lead to better convergence (Krueger and Dayan, 2009). Our model is different from previous theme-rewriting (Polozov et al., 2015; Koncel-Kedziorski et al., 2016) approach which has been applied to math word problems but more similar to the Feedback N"
P18-2042,I08-1046,0,0.0121287,"ps://en.wikipedia.org/wiki/Eight-legged essay 3 https://www.goodreads.com/quotes/398754-it-isperfectly-okay-to-write-garbage–as-long-as-you 260 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 260–265 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics Figure 1: Writing-editing Network architecture overview. Title Human written abstract LSTM LM Seq2seq with attention (Initial Draft) Writingediting Networks (Final Draft) An effective method of using Web based information for Relation Extraction (Keong and Su, 2008) We propose a method that incorporates paraphrase information from the Web to boost the performance of a supervised relation extraction system. Contextual information is extracted from the Web using a semisupervised process, and summarized by skip-bigram overlap measures over the entire extract. This allows the capture of local contextual information as well as more distant associations. We observe a statistically significant boost in relation extraction performance. This paper proposes a method for automatic extraction of salient information from an original text. Our method shows promising r"
P18-2042,D16-1032,0,0.0578948,"m learning. In Proceedings of the 26th Annual International Conference on Machine Learning. Related work Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoder-decoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. Deep neural networks are widely applied to text generation tasks such as poetry creation (Greene et al., 2010; Ghazvininejad et al., 2016; Zhang et al., 2017), recipe generation (Kiddon et al., 2016), abstractive summarization (Gu et al., 2016; Wang and Ling, 2016; See et al., 2017), and biography generation (Lebret et al., 2016; Liu et al., 2018). We introduce a new task of generating paper abstracts from the given titles. We design a Writing-editing Network which shares ideas with Curriculum Learning (Bengio et al., 2009), where training on a data point from coarse to finegrained can lead to better convergence (Krueger and Dayan, 2009). Our model is different from previous theme-rewriting (Polozov et al., 2015; Koncel-Kedziorski et al., 2016) approach which has been applied to math word"
P18-2042,D16-1168,0,0.0219676,"d et al., 2016; Zhang et al., 2017), recipe generation (Kiddon et al., 2016), abstractive summarization (Gu et al., 2016; Wang and Ling, 2016; See et al., 2017), and biography generation (Lebret et al., 2016; Liu et al., 2018). We introduce a new task of generating paper abstracts from the given titles. We design a Writing-editing Network which shares ideas with Curriculum Learning (Bengio et al., 2009), where training on a data point from coarse to finegrained can lead to better convergence (Krueger and Dayan, 2009). Our model is different from previous theme-rewriting (Polozov et al., 2015; Koncel-Kedziorski et al., 2016) approach which has been applied to math word problems but more similar to the Feedback Network (Zamir et al., 2017) by using previous generated outputs as feedback to guide subsequent generation. Moreover, our Writing-editing Network treats previous drafts as independent observations and does not propagate errors to previous draft generation stages. This property is vital for training feedback architectures for discrete data. Another similar approach is the deliberation network used for MaMichael Denkowski and Alon Lavie. 2014. Meteor universal: Language specific translation evaluation for an"
P18-2042,P17-1125,0,0.0246041,"obert, and Jason Weston. 2009. Curriculum learning. In Proceedings of the 26th Annual International Conference on Machine Learning. Related work Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoder-decoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. Deep neural networks are widely applied to text generation tasks such as poetry creation (Greene et al., 2010; Ghazvininejad et al., 2016; Zhang et al., 2017), recipe generation (Kiddon et al., 2016), abstractive summarization (Gu et al., 2016; Wang and Ling, 2016; See et al., 2017), and biography generation (Lebret et al., 2016; Liu et al., 2018). We introduce a new task of generating paper abstracts from the given titles. We design a Writing-editing Network which shares ideas with Curriculum Learning (Bengio et al., 2009), where training on a data point from coarse to finegrained can lead to better convergence (Krueger and Dayan, 2009). Our model is different from previous theme-rewriting (Polozov et al., 2015; Koncel-Kedziorski et al., 2016) app"
P18-2042,D16-1128,0,0.179894,"Missing"
P18-2042,W04-1013,0,0.0451412,"Missing"
P18-2042,D15-1166,0,0.123507,"Missing"
P18-2042,P17-1099,0,0.0575064,"ing. Related work Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoder-decoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. Deep neural networks are widely applied to text generation tasks such as poetry creation (Greene et al., 2010; Ghazvininejad et al., 2016; Zhang et al., 2017), recipe generation (Kiddon et al., 2016), abstractive summarization (Gu et al., 2016; Wang and Ling, 2016; See et al., 2017), and biography generation (Lebret et al., 2016; Liu et al., 2018). We introduce a new task of generating paper abstracts from the given titles. We design a Writing-editing Network which shares ideas with Curriculum Learning (Bengio et al., 2009), where training on a data point from coarse to finegrained can lead to better convergence (Krueger and Dayan, 2009). Our model is different from previous theme-rewriting (Polozov et al., 2015; Koncel-Kedziorski et al., 2016) approach which has been applied to math word problems but more similar to the Feedback Network (Zamir et al., 2017) by using pre"
P18-2042,N16-1007,0,0.0188604,"ence on Machine Learning. Related work Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoder-decoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. Deep neural networks are widely applied to text generation tasks such as poetry creation (Greene et al., 2010; Ghazvininejad et al., 2016; Zhang et al., 2017), recipe generation (Kiddon et al., 2016), abstractive summarization (Gu et al., 2016; Wang and Ling, 2016; See et al., 2017), and biography generation (Lebret et al., 2016; Liu et al., 2018). We introduce a new task of generating paper abstracts from the given titles. We design a Writing-editing Network which shares ideas with Curriculum Learning (Bengio et al., 2009), where training on a data point from coarse to finegrained can lead to better convergence (Krueger and Dayan, 2009). Our model is different from previous theme-rewriting (Polozov et al., 2015; Koncel-Kedziorski et al., 2016) approach which has been applied to math word problems but more similar to the Feedback Network (Zamir et al.,"
P18-2042,W14-3348,0,\N,Missing
P18-4003,P07-1015,0,0.0477465,"in 21 sample languages are available on the demo start page and more are accessible as ranTable 6: Romanization with alternatives Table 7 includes examples of the Romanization rules in uroman, including n-to-m mappings. 2.6 Caching uroman caches token romanizations for speed. 16 useful in cross-lingual information retrieval. There is a body of work mapping text to phonetic representations. Deri and Knight (2016) use Wiktionary and Wikipedia resources to learn text-to-phoneme mappings. Phonetic representations are used in a number of end-to-end transliteration systems (Knight and Graehl, 1998; Yoon et al., 2007). Qian et al. (2010) describe the toolkit ScriptTranscriber, which extracts crosslingual transliteration pairs from comparable corpora. A core component of ScriptTranscriber maps text to an ASCII variant of the International Phonetic Alphabet (IPA). Andy Hu’s transliterator8 is a fairly universal romanizer in JavaScript, limited to romanizing one Unicode character at a time, without context. dom texts. After picking the first random text, additional random texts will be available from three corpora to choose from (small, large, and Wikipedia articles about the US). Users can then restrict the"
P18-4003,P16-1038,1,0.837726,"and script of their choice, optionally specify a language code, and then have uroman romanize the text. Additionally, the demo page includes sample texts in 290 languages in a wide variety of scripts. Texts in 21 sample languages are available on the demo start page and more are accessible as ranTable 6: Romanization with alternatives Table 7 includes examples of the Romanization rules in uroman, including n-to-m mappings. 2.6 Caching uroman caches token romanizations for speed. 16 useful in cross-lingual information retrieval. There is a body of work mapping text to phonetic representations. Deri and Knight (2016) use Wiktionary and Wikipedia resources to learn text-to-phoneme mappings. Phonetic representations are used in a number of end-to-end transliteration systems (Knight and Graehl, 1998; Yoon et al., 2007). Qian et al. (2010) describe the toolkit ScriptTranscriber, which extracts crosslingual transliteration pairs from comparable corpora. A core component of ScriptTranscriber maps text to an ASCII variant of the International Phonetic Alphabet (IPA). Andy Hu’s transliterator8 is a fairly universal romanizer in JavaScript, limited to romanizing one Unicode character at a time, without context. do"
P18-4003,P18-4011,1,0.852386,"ation of string-similarity metrics to texts from different scripts without the need and complexity of an intermediate phonetic representation. The tool is freely and publicly available as a Perl script suitable for inclusion in data processing pipelines and as an interactive demo web page. 1 Table 1: Example of Hindi and Urdu romanization Foreign scripts also present a massive cognitive barrier to humans who are not familiar with them. We devised a utility that allows people to translate text from languages they don’t know, using the same information available to a machine translation system (Hermjakob et al., 2018). We found that when we asked native English speakers to use this utility to translate text from languages such as Uyghur or Bengali to English, they strongly preferred working on the romanized version of the source language compared to its original form and indeed found using the native, unfamiliar script to be a nearly impossible task. Introduction String similarity is a useful feature in many natural language processing tasks. In machine translation, it can be used to improve the alignment of bitexts, and for low-resource languages with a related language of larger resources, it can help to"
P18-4003,W17-1414,0,0.01822,"shown when a user hovers over it). The romanization of the output at the demo site is mouse sensitive. Hovering over characters of either the original or romanized text, the page will highlight corresponding characters. See Figure 1 for an example. Hovering over the original text will also display additional information such as the Unicode name and any numeric value. To support this interactive demo site, the uroman package also includes fonts for Burmese, Tifinagh, Klingon, and Egyptian hieroglyphs, as they are sometimes missing from standard browser font packages. 4 5.2 Ji et al. (2017) and Mayfield et al. (2017) use uroman for named entity recognition. Mayhew et al. (2016) use uroman for (end-to-end) transliteration. Cheung et al. (2017) use uroman for machine translation of low-resource languages. uroman has also been used in our aforementioned translation utility (Hermjakob et al., 2018), where humans translate text to another language, with computer support, with high fluency in the target language (English), but no prior knowledge of the source language. uroman has been partially ported by third parties to Python and Java.9 Limitations and Future Work The current version of uroman has a few limit"
P18-4003,qian-etal-2010-python,0,0.0247378,"es are available on the demo start page and more are accessible as ranTable 6: Romanization with alternatives Table 7 includes examples of the Romanization rules in uroman, including n-to-m mappings. 2.6 Caching uroman caches token romanizations for speed. 16 useful in cross-lingual information retrieval. There is a body of work mapping text to phonetic representations. Deri and Knight (2016) use Wiktionary and Wikipedia resources to learn text-to-phoneme mappings. Phonetic representations are used in a number of end-to-end transliteration systems (Knight and Graehl, 1998; Yoon et al., 2007). Qian et al. (2010) describe the toolkit ScriptTranscriber, which extracts crosslingual transliteration pairs from comparable corpora. A core component of ScriptTranscriber maps text to an ASCII variant of the International Phonetic Alphabet (IPA). Andy Hu’s transliterator8 is a fairly universal romanizer in JavaScript, limited to romanizing one Unicode character at a time, without context. dom texts. After picking the first random text, additional random texts will be available from three corpora to choose from (small, large, and Wikipedia articles about the US). Users can then restrict the randomness in a spec"
P18-4011,P18-4003,1,0.74235,"examples for five Hungarian affixes and two Tagalog function words. Table 2: Texts in Uyghur, Amharic and Tibetan. Table 3: Grammar entries for Hungarian, Tagalog. We found that when we asked native English speakers to use the Chinese Room to translate text from languages such as Uyghur or Bengali to English, they strongly preferred working on a romanized version of the source language compared to its original form and indeed found using the native, unfamiliar script to be a nearly impossible task. By default, we therefore romanize non-Latinscript text, using the universal romanizer uroman2 (Hermjakob et al., 2018). The Chinese Room Editor includes the option to display the original text or both the original and romanized source text. The Uyghur text in Table 2 is romanized as yaponie fukushima 1-yadro elektir istansisining toet genratorlar guruppisi which facilitates the recognition of cognates. The grammar files have been built manually, typically drawing on external resources such as Wiktionary.3 The size is language specific, ranging from a few dozen entries to several hundred entries for extremely suffix-rich Hungarian. 2 2.7 Process Figure 1 provides an overview of the Chinese Room process. Given"
P18-4011,N10-1078,0,0.02333,"English describing earthquakes and disaster relief efforts. However, we had no parallel data dealing with this topic, and our use of an unrelated test set (see Figure 3) to estimate overall task performance was not reliable. We thus wanted to construct an in-domain Uyghur-English parallel corpus. In the scenario we were given a small number of one-hour sessions with a native informant (NI), a Uyghur native who spoke English and was not a linguistics or computer science expert. We initially asked the NI use the time to translate docu5 Related Work Callison-Burch (2005); Albrecht et al. (2009); Koehn (2010) and Trados5 have built computeraided translation systems for high-resource languages, with an emphasis on post-editing. Hu et al. (2011) describe a monolingual translation protocol that combines MT with not only monolingual target language speakers, but, unlike the Chinese Room, also monolingual source language speakers. 5 66 https://www.sdltrados.com Figure 3: MT performance on an out-of-domain corpus (‘test’) does not predict performance on the evaluation (’eval’) set but performance on our ‘domain’ data set which comprises NI translations and Chinese Room post-edits, is predictive. 6 Futur"
P18-4011,E09-1008,0,0.031659,"ocuments from Uyghur to English describing earthquakes and disaster relief efforts. However, we had no parallel data dealing with this topic, and our use of an unrelated test set (see Figure 3) to estimate overall task performance was not reliable. We thus wanted to construct an in-domain Uyghur-English parallel corpus. In the scenario we were given a small number of one-hour sessions with a native informant (NI), a Uyghur native who spoke English and was not a linguistics or computer science expert. We initially asked the NI use the time to translate docu5 Related Work Callison-Burch (2005); Albrecht et al. (2009); Koehn (2010) and Trados5 have built computeraided translation systems for high-resource languages, with an emphasis on post-editing. Hu et al. (2011) describe a monolingual translation protocol that combines MT with not only monolingual target language speakers, but, unlike the Chinese Room, also monolingual source language speakers. 5 66 https://www.sdltrados.com Figure 3: MT performance on an out-of-domain corpus (‘test’) does not predict performance on the evaluation (’eval’) set but performance on our ‘domain’ data set which comprises NI translations and Chinese Room post-edits, is predi"
P19-1191,E17-1060,0,0.0353562,"he contextual sentences that include the target entity and its neighbors from the graph structure. This is the first work to incorporate new idea creation via link prediction into automatic paper writing. Knowledge-driven Generation. Deep Neural Networks have been applied to generate natural language to describe structured knowledge bases (Duma and Klein, 2013; Konstas and Lapata, 2013; Flanigan et al., 2016; Hardy and Vlachos, 2018; Pourdamghani et al., 2016; Trisedya et al., 2018; Xu et al., 2018; Madotto et al., 2018; Nie et al., 2018), biographies based on attributes (Lebret et al., 2016; Chisholm et al., 2017; Liu et al., 2018; Sha et al., 2018; Kaffee et al., 2018; Wang et al., 2018a; Wiseman et al., 2018), and image/video captions based on background entities and events (Krishnamoorthy et al., 2013; Wu et al., 2018; Whitehead et al., 2018; Lu et al., 2018). To handle unknown words, we design an architecture similar to pointer-generator networks (See et al., 2017) and copy mechanism (Gu et al., 2016). Some interesting applications include generating abstracts based on titles for the natural language processing domain (Wang et al., 2018b), generating a poster (Qiang et al., 2016) or a science news"
P19-1191,W14-3348,0,0.040687,"ov/pub/pmc/ oa_package/ cites a paper B, we assume the title of A is generated from B’s conclusion and future work session. We construct background knowledge graphs from 1,687,060 papers which include 30,483 entities and 875,698 relations. Tables 2 shows the detailed data statistics. The hyperparameters of our model are presented in the Appendix. 3.2 Automatic Evaluation Previous work (Liu et al., 2016; Li et al., 2016; Lowe et al., 2015) has proven it to be a major challenge to automatically evaluate long text generation. Following the story generation work (Fan et al., 2018), we use METEOR (Denkowski and Lavie, 2014) to measure the topic relevance towards given titles and use perplexity to further evaluate the quality of the language model. The perplexity scores of our model are based on the language model6 learned on other PubMed papers (500,000 titles, 50,000 abstracts, 50,000 conclusions and future work) which are not used for training or testing in our experiment.7 The results are shown in Table 3. We can see that our framework outperforms all previous approaches. 3.3 Turing Test Similar to (Wang et al., 2018b), we conduct Turing tests by a biomedical expert (non-native speaker) and a non-expert (nati"
P19-1191,W13-0108,0,0.0277008,"d graph attention (Sukhbaatar et al., 2015; Madotto et al., 2018; Veliˇckovi´c et al., 2018) to encourage the model to capture multi-aspect relevance among nodes. Similar to (Wang and Li, 2016; Xu et al., 2017), we enrich entity representation by combining the contextual sentences that include the target entity and its neighbors from the graph structure. This is the first work to incorporate new idea creation via link prediction into automatic paper writing. Knowledge-driven Generation. Deep Neural Networks have been applied to generate natural language to describe structured knowledge bases (Duma and Klein, 2013; Konstas and Lapata, 2013; Flanigan et al., 2016; Hardy and Vlachos, 2018; Pourdamghani et al., 2016; Trisedya et al., 2018; Xu et al., 2018; Madotto et al., 2018; Nie et al., 2018), biographies based on attributes (Lebret et al., 2016; Chisholm et al., 2017; Liu et al., 2018; Sha et al., 2018; Kaffee et al., 2018; Wang et al., 2018a; Wiseman et al., 2018), and image/video captions based on background entities and events (Krishnamoorthy et al., 2013; Wu et al., 2018; Whitehead et al., 2018; Lu et al., 2018). To handle unknown words, we design an architecture similar to pointer-generator netwo"
P19-1191,P18-1082,0,0.033721,"paper A 5 ftp://ftp.ncbi.nlm.nih.gov/pub/pmc/ oa_package/ cites a paper B, we assume the title of A is generated from B’s conclusion and future work session. We construct background knowledge graphs from 1,687,060 papers which include 30,483 entities and 875,698 relations. Tables 2 shows the detailed data statistics. The hyperparameters of our model are presented in the Appendix. 3.2 Automatic Evaluation Previous work (Liu et al., 2016; Li et al., 2016; Lowe et al., 2015) has proven it to be a major challenge to automatically evaluate long text generation. Following the story generation work (Fan et al., 2018), we use METEOR (Denkowski and Lavie, 2014) to measure the topic relevance towards given titles and use perplexity to further evaluate the quality of the language model. The perplexity scores of our model are based on the language model6 learned on other PubMed papers (500,000 titles, 50,000 abstracts, 50,000 conclusions and future work) which are not used for training or testing in our experiment.7 The results are shown in Table 3. We can see that our framework outperforms all previous approaches. 3.3 Turing Test Similar to (Wang et al., 2018b), we conduct Turing tests by a biomedical expert"
P19-1191,N16-1087,0,0.0455489,"tto et al., 2018; Veliˇckovi´c et al., 2018) to encourage the model to capture multi-aspect relevance among nodes. Similar to (Wang and Li, 2016; Xu et al., 2017), we enrich entity representation by combining the contextual sentences that include the target entity and its neighbors from the graph structure. This is the first work to incorporate new idea creation via link prediction into automatic paper writing. Knowledge-driven Generation. Deep Neural Networks have been applied to generate natural language to describe structured knowledge bases (Duma and Klein, 2013; Konstas and Lapata, 2013; Flanigan et al., 2016; Hardy and Vlachos, 2018; Pourdamghani et al., 2016; Trisedya et al., 2018; Xu et al., 2018; Madotto et al., 2018; Nie et al., 2018), biographies based on attributes (Lebret et al., 2016; Chisholm et al., 2017; Liu et al., 2018; Sha et al., 2018; Kaffee et al., 2018; Wang et al., 2018a; Wiseman et al., 2018), and image/video captions based on background entities and events (Krishnamoorthy et al., 2013; Wu et al., 2018; Whitehead et al., 2018; Lu et al., 2018). To handle unknown words, we design an architecture similar to pointer-generator networks (See et al., 2017) and copy mechanism (Gu et"
P19-1191,W07-2305,0,0.0841706,"Missing"
P19-1191,P16-1154,0,0.301994,"rk-to-title) follow the same architecture. Given a reference title τ = [w1 , ..., wl ], we apply the knowledge extractor (Section 2.2) to extract entities from τ . For each entity, we retrieve a set of related entities from the enriched knowledge e after link prediction. We rank all the regraph K lated entities by confidence scores and select up to 10 most related entities Eτ = [eτ1 , ..., eτv ]. Then we feed τ and Eτ together into the paper generation framework as shown in Figure 2. The framework is based on a hybrid approach of a Mem2seq model (Madotto et al., 2018) and a pointer generator (Gu et al., 2016; See et al., 2017). It allows us to balance three types of sources for each time step during decoding: the probability of generating a token from the entire word vocabulary based on language model, the probability of copying a word from the reference title, such as regulates in Table 1, and the probability of incorporating a related entity, such as Snail in Table 1. The output is a paragraph Y = [y1 , ..., yo ].3 Reference Encoder For each word in the refer3 During training, we truncate both of the input and the output to around 120 tokens to expedite training. We label the words with frequen"
P19-1191,D18-1086,0,0.0190419,"ckovi´c et al., 2018) to encourage the model to capture multi-aspect relevance among nodes. Similar to (Wang and Li, 2016; Xu et al., 2017), we enrich entity representation by combining the contextual sentences that include the target entity and its neighbors from the graph structure. This is the first work to incorporate new idea creation via link prediction into automatic paper writing. Knowledge-driven Generation. Deep Neural Networks have been applied to generate natural language to describe structured knowledge bases (Duma and Klein, 2013; Konstas and Lapata, 2013; Flanigan et al., 2016; Hardy and Vlachos, 2018; Pourdamghani et al., 2016; Trisedya et al., 2018; Xu et al., 2018; Madotto et al., 2018; Nie et al., 2018), biographies based on attributes (Lebret et al., 2016; Chisholm et al., 2017; Liu et al., 2018; Sha et al., 2018; Kaffee et al., 2018; Wang et al., 2018a; Wiseman et al., 2018), and image/video captions based on background entities and events (Krishnamoorthy et al., 2013; Wu et al., 2018; Whitehead et al., 2018; Lu et al., 2018). To handle unknown words, we design an architecture similar to pointer-generator networks (See et al., 2017) and copy mechanism (Gu et al., 2016). Some interest"
P19-1191,P15-1067,0,0.0119856,"om biomedical text”, PaperRobot mistakenly extracts “prolog” as a related entity and generates an abstract “In this paper, we present a novel approach to the problem of extracting relationships among the prolog program. We present a system that uses a macromolecular binding relationships to extract the relationships between the abstracts of the entry. The results show that the system is able to extract the most important concepts in the prolog program.”. 4 Related Work Link Prediction. Translation-based approaches (Nickel et al., 2011; Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015; Ji et al., 2015a) have been widely exploited for link prediction. Compared with these studies, we are the first to incorporate multi-head graph attention (Sukhbaatar et al., 2015; Madotto et al., 2018; Veliˇckovi´c et al., 2018) to encourage the model to capture multi-aspect relevance among nodes. Similar to (Wang and Li, 2016; Xu et al., 2017), we enrich entity representation by combining the contextual sentences that include the target entity and its neighbors from the graph structure. This is the first work to incorporate new idea creation via link prediction into automatic paper writing. Knowledge-driven"
P19-1191,N18-2101,0,0.303572,"its neighbors from the graph structure. This is the first work to incorporate new idea creation via link prediction into automatic paper writing. Knowledge-driven Generation. Deep Neural Networks have been applied to generate natural language to describe structured knowledge bases (Duma and Klein, 2013; Konstas and Lapata, 2013; Flanigan et al., 2016; Hardy and Vlachos, 2018; Pourdamghani et al., 2016; Trisedya et al., 2018; Xu et al., 2018; Madotto et al., 2018; Nie et al., 2018), biographies based on attributes (Lebret et al., 2016; Chisholm et al., 2017; Liu et al., 2018; Sha et al., 2018; Kaffee et al., 2018; Wang et al., 2018a; Wiseman et al., 2018), and image/video captions based on background entities and events (Krishnamoorthy et al., 2013; Wu et al., 2018; Whitehead et al., 2018; Lu et al., 2018). To handle unknown words, we design an architecture similar to pointer-generator networks (See et al., 2017) and copy mechanism (Gu et al., 2016). Some interesting applications include generating abstracts based on titles for the natural language processing domain (Wang et al., 2018b), generating a poster (Qiang et al., 2016) or a science news blog title (Vadapalli et al., 2018) about a published pa"
P19-1191,W13-1302,0,0.0196907,"paper writing. Knowledge-driven Generation. Deep Neural Networks have been applied to generate natural language to describe structured knowledge bases (Duma and Klein, 2013; Konstas and Lapata, 2013; Flanigan et al., 2016; Hardy and Vlachos, 2018; Pourdamghani et al., 2016; Trisedya et al., 2018; Xu et al., 2018; Madotto et al., 2018; Nie et al., 2018), biographies based on attributes (Lebret et al., 2016; Chisholm et al., 2017; Liu et al., 2018; Sha et al., 2018; Kaffee et al., 2018; Wang et al., 2018a; Wiseman et al., 2018), and image/video captions based on background entities and events (Krishnamoorthy et al., 2013; Wu et al., 2018; Whitehead et al., 2018; Lu et al., 2018). To handle unknown words, we design an architecture similar to pointer-generator networks (See et al., 2017) and copy mechanism (Gu et al., 2016). Some interesting applications include generating abstracts based on titles for the natural language processing domain (Wang et al., 2018b), generating a poster (Qiang et al., 2016) or a science news blog title (Vadapalli et al., 2018) about a published paper. This is the first work on automatic writing of key paper elements for the biomedical domain, especially conclusion and future work, a"
P19-1191,D16-1128,0,0.0872029,"Missing"
P19-1191,P16-1094,0,0.0146349,"t. 3 3.1 Experiment Data We collect biomedical papers from the PMC Open Access Subset.5 To construct ground truth for new title prediction, if a human written paper A 5 ftp://ftp.ncbi.nlm.nih.gov/pub/pmc/ oa_package/ cites a paper B, we assume the title of A is generated from B’s conclusion and future work session. We construct background knowledge graphs from 1,687,060 papers which include 30,483 entities and 875,698 relations. Tables 2 shows the detailed data statistics. The hyperparameters of our model are presented in the Appendix. 3.2 Automatic Evaluation Previous work (Liu et al., 2016; Li et al., 2016; Lowe et al., 2015) has proven it to be a major challenge to automatically evaluate long text generation. Following the story generation work (Fan et al., 2018), we use METEOR (Denkowski and Lavie, 2014) to measure the topic relevance towards given titles and use perplexity to further evaluate the quality of the language model. The perplexity scores of our model are based on the language model6 learned on other PubMed papers (500,000 titles, 50,000 abstracts, 50,000 conclusions and future work) which are not used for training or testing in our experiment.7 The results are shown in Table 3. We"
P19-1191,W04-1013,0,0.0826554,"Missing"
P19-1191,D16-1230,0,0.03642,"Missing"
P19-1191,W15-4640,0,0.0129704,"nt Data We collect biomedical papers from the PMC Open Access Subset.5 To construct ground truth for new title prediction, if a human written paper A 5 ftp://ftp.ncbi.nlm.nih.gov/pub/pmc/ oa_package/ cites a paper B, we assume the title of A is generated from B’s conclusion and future work session. We construct background knowledge graphs from 1,687,060 papers which include 30,483 entities and 875,698 relations. Tables 2 shows the detailed data statistics. The hyperparameters of our model are presented in the Appendix. 3.2 Automatic Evaluation Previous work (Liu et al., 2016; Li et al., 2016; Lowe et al., 2015) has proven it to be a major challenge to automatically evaluate long text generation. Following the story generation work (Fan et al., 2018), we use METEOR (Denkowski and Lavie, 2014) to measure the topic relevance towards given titles and use perplexity to further evaluate the quality of the language model. The perplexity scores of our model are based on the language model6 learned on other PubMed papers (500,000 titles, 50,000 abstracts, 50,000 conclusions and future work) which are not used for training or testing in our experiment.7 The results are shown in Table 3. We can see that our fr"
P19-1191,D18-1435,1,0.843574,"Missing"
P19-1191,D18-1360,1,0.841868,"t data. All of the system generated titles are declarative sentences while human generated titles are often more engaging (e.g., “Does HPV play any role in the initiation or prognosis of endometrial Requirements to Make PaperRobot Work: Case Study on NLP Domain When a cool Natural Language Processing (NLP) system like PaperRobot is built, it’s natural to ask whether she can benefit the NLP community itself. We re-build the system based on 23,594 NLP papers from the new ACL Anthology Network (Radev et al., 2013). For knowledge extraction we apply our previous system trained for the NLP domain (Luan et al., 2018). But the results are much less satisfactory compared to the 1987 biomedical domain. Due to the small size of data, the language model is not able to effectively copy out-of-vocabulary words and thus the output is often too generic. For example, given a title “Statistics based hybrid approach to Chinese base phrase identification”, PaperRobot generates a fluent but uninformative abstract “This paper describes a novel approach to the task of Chinese-base-phrase identification. We first utilize the solid foundation for the Chinese parser, and we show that our tool can be easily extended to meet"
P19-1191,P18-1136,0,0.174988,"n and future work, and conclusion and future work-to-title) follow the same architecture. Given a reference title τ = [w1 , ..., wl ], we apply the knowledge extractor (Section 2.2) to extract entities from τ . For each entity, we retrieve a set of related entities from the enriched knowledge e after link prediction. We rank all the regraph K lated entities by confidence scores and select up to 10 most related entities Eτ = [eτ1 , ..., eτv ]. Then we feed τ and Eτ together into the paper generation framework as shown in Figure 2. The framework is based on a hybrid approach of a Mem2seq model (Madotto et al., 2018) and a pointer generator (Gu et al., 2016; See et al., 2017). It allows us to balance three types of sources for each time step during decoding: the probability of generating a token from the entire word vocabulary based on language model, the probability of copying a word from the reference title, such as regulates in Table 1, and the probability of incorporating a related entity, such as Snail in Table 1. The output is a paragraph Y = [y1 , ..., yo ].3 Reference Encoder For each word in the refer3 During training, we truncate both of the input and the output to around 120 tokens to expedite"
P19-1191,P02-1040,0,0.104014,"Missing"
P19-1191,W16-6603,1,0.855329,"encourage the model to capture multi-aspect relevance among nodes. Similar to (Wang and Li, 2016; Xu et al., 2017), we enrich entity representation by combining the contextual sentences that include the target entity and its neighbors from the graph structure. This is the first work to incorporate new idea creation via link prediction into automatic paper writing. Knowledge-driven Generation. Deep Neural Networks have been applied to generate natural language to describe structured knowledge bases (Duma and Klein, 2013; Konstas and Lapata, 2013; Flanigan et al., 2016; Hardy and Vlachos, 2018; Pourdamghani et al., 2016; Trisedya et al., 2018; Xu et al., 2018; Madotto et al., 2018; Nie et al., 2018), biographies based on attributes (Lebret et al., 2016; Chisholm et al., 2017; Liu et al., 2018; Sha et al., 2018; Kaffee et al., 2018; Wang et al., 2018a; Wiseman et al., 2018), and image/video captions based on background entities and events (Krishnamoorthy et al., 2013; Wu et al., 2018; Whitehead et al., 2018; Lu et al., 2018). To handle unknown words, we design an architecture similar to pointer-generator networks (See et al., 2017) and copy mechanism (Gu et al., 2016). Some interesting applications include ge"
P19-1191,P17-1099,0,0.622639,"ow the same architecture. Given a reference title τ = [w1 , ..., wl ], we apply the knowledge extractor (Section 2.2) to extract entities from τ . For each entity, we retrieve a set of related entities from the enriched knowledge e after link prediction. We rank all the regraph K lated entities by confidence scores and select up to 10 most related entities Eτ = [eτ1 , ..., eτv ]. Then we feed τ and Eτ together into the paper generation framework as shown in Figure 2. The framework is based on a hybrid approach of a Mem2seq model (Madotto et al., 2018) and a pointer generator (Gu et al., 2016; See et al., 2017). It allows us to balance three types of sources for each time step during decoding: the probability of generating a token from the entire word vocabulary based on language model, the probability of copying a word from the reference title, such as regulates in Table 1, and the probability of incorporating a related entity, such as Snail in Table 1. The output is a paragraph Y = [y1 , ..., yo ].3 Reference Encoder For each word in the refer3 During training, we truncate both of the input and the output to around 120 tokens to expedite training. We label the words with frequency < 5 as Out-of-vo"
P19-1191,2006.amta-papers.25,0,0.221983,"Missing"
P19-1191,E17-2047,0,0.056946,"Missing"
P19-1191,P18-1151,0,0.0126333,"ure multi-aspect relevance among nodes. Similar to (Wang and Li, 2016; Xu et al., 2017), we enrich entity representation by combining the contextual sentences that include the target entity and its neighbors from the graph structure. This is the first work to incorporate new idea creation via link prediction into automatic paper writing. Knowledge-driven Generation. Deep Neural Networks have been applied to generate natural language to describe structured knowledge bases (Duma and Klein, 2013; Konstas and Lapata, 2013; Flanigan et al., 2016; Hardy and Vlachos, 2018; Pourdamghani et al., 2016; Trisedya et al., 2018; Xu et al., 2018; Madotto et al., 2018; Nie et al., 2018), biographies based on attributes (Lebret et al., 2016; Chisholm et al., 2017; Liu et al., 2018; Sha et al., 2018; Kaffee et al., 2018; Wang et al., 2018a; Wiseman et al., 2018), and image/video captions based on background entities and events (Krishnamoorthy et al., 2013; Wu et al., 2018; Whitehead et al., 2018; Lu et al., 2018). To handle unknown words, we design an architecture similar to pointer-generator networks (See et al., 2017) and copy mechanism (Gu et al., 2016). Some interesting applications include generating abstracts base"
P19-1191,D18-1422,0,0.0136042,"i, 2016; Xu et al., 2017), we enrich entity representation by combining the contextual sentences that include the target entity and its neighbors from the graph structure. This is the first work to incorporate new idea creation via link prediction into automatic paper writing. Knowledge-driven Generation. Deep Neural Networks have been applied to generate natural language to describe structured knowledge bases (Duma and Klein, 2013; Konstas and Lapata, 2013; Flanigan et al., 2016; Hardy and Vlachos, 2018; Pourdamghani et al., 2016; Trisedya et al., 2018; Xu et al., 2018; Madotto et al., 2018; Nie et al., 2018), biographies based on attributes (Lebret et al., 2016; Chisholm et al., 2017; Liu et al., 2018; Sha et al., 2018; Kaffee et al., 2018; Wang et al., 2018a; Wiseman et al., 2018), and image/video captions based on background entities and events (Krishnamoorthy et al., 2013; Wu et al., 2018; Whitehead et al., 2018; Lu et al., 2018). To handle unknown words, we design an architecture similar to pointer-generator networks (See et al., 2017) and copy mechanism (Gu et al., 2016). Some interesting applications include generating abstracts based on titles for the natural language processing domain (Wa"
P19-1191,P16-1008,0,0.060528,"Missing"
P19-1191,D18-2028,0,0.0454461,"Missing"
P19-1191,D18-1112,0,0.0200447,"nce among nodes. Similar to (Wang and Li, 2016; Xu et al., 2017), we enrich entity representation by combining the contextual sentences that include the target entity and its neighbors from the graph structure. This is the first work to incorporate new idea creation via link prediction into automatic paper writing. Knowledge-driven Generation. Deep Neural Networks have been applied to generate natural language to describe structured knowledge bases (Duma and Klein, 2013; Konstas and Lapata, 2013; Flanigan et al., 2016; Hardy and Vlachos, 2018; Pourdamghani et al., 2016; Trisedya et al., 2018; Xu et al., 2018; Madotto et al., 2018; Nie et al., 2018), biographies based on attributes (Lebret et al., 2016; Chisholm et al., 2017; Liu et al., 2018; Sha et al., 2018; Kaffee et al., 2018; Wang et al., 2018a; Wiseman et al., 2018), and image/video captions based on background entities and events (Krishnamoorthy et al., 2013; Wu et al., 2018; Whitehead et al., 2018; Lu et al., 2018). To handle unknown words, we design an architecture similar to pointer-generator networks (See et al., 2017) and copy mechanism (Gu et al., 2016). Some interesting applications include generating abstracts based on titles for t"
P19-1191,W18-6502,1,0.880435,"t generation. Following the story generation work (Fan et al., 2018), we use METEOR (Denkowski and Lavie, 2014) to measure the topic relevance towards given titles and use perplexity to further evaluate the quality of the language model. The perplexity scores of our model are based on the language model6 learned on other PubMed papers (500,000 titles, 50,000 abstracts, 50,000 conclusions and future work) which are not used for training or testing in our experiment.7 The results are shown in Table 3. We can see that our framework outperforms all previous approaches. 3.3 Turing Test Similar to (Wang et al., 2018b), we conduct Turing tests by a biomedical expert (non-native speaker) and a non-expert (native speaker). Each human judge is asked to compare a system output and a human-authored string, and select the better one. 6 https://github.com/pytorch/examples/ tree/master/word_language_model 7 The perplexity scores of the language model are in the Appendix. 1985 Task Input Human Title End-to-End System Abstract System Conclusion and Future work System Title Human Abstract Diagnostic Human Conclusion and Future work Output Different Same Different Same Different Same Different Different Same Differen"
P19-1191,P18-2042,1,0.914756,"t generation. Following the story generation work (Fan et al., 2018), we use METEOR (Denkowski and Lavie, 2014) to measure the topic relevance towards given titles and use perplexity to further evaluate the quality of the language model. The perplexity scores of our model are based on the language model6 learned on other PubMed papers (500,000 titles, 50,000 abstracts, 50,000 conclusions and future work) which are not used for training or testing in our experiment.7 The results are shown in Table 3. We can see that our framework outperforms all previous approaches. 3.3 Turing Test Similar to (Wang et al., 2018b), we conduct Turing tests by a biomedical expert (non-native speaker) and a non-expert (native speaker). Each human judge is asked to compare a system output and a human-authored string, and select the better one. 6 https://github.com/pytorch/examples/ tree/master/word_language_model 7 The perplexity scores of the language model are in the Appendix. 1985 Task Input Human Title End-to-End System Abstract System Conclusion and Future work System Title Human Abstract Diagnostic Human Conclusion and Future work Output Different Same Different Same Different Same Different Different Same Differen"
P19-1191,D18-1433,1,0.748118,"eep Neural Networks have been applied to generate natural language to describe structured knowledge bases (Duma and Klein, 2013; Konstas and Lapata, 2013; Flanigan et al., 2016; Hardy and Vlachos, 2018; Pourdamghani et al., 2016; Trisedya et al., 2018; Xu et al., 2018; Madotto et al., 2018; Nie et al., 2018), biographies based on attributes (Lebret et al., 2016; Chisholm et al., 2017; Liu et al., 2018; Sha et al., 2018; Kaffee et al., 2018; Wang et al., 2018a; Wiseman et al., 2018), and image/video captions based on background entities and events (Krishnamoorthy et al., 2013; Wu et al., 2018; Whitehead et al., 2018; Lu et al., 2018). To handle unknown words, we design an architecture similar to pointer-generator networks (See et al., 2017) and copy mechanism (Gu et al., 2016). Some interesting applications include generating abstracts based on titles for the natural language processing domain (Wang et al., 2018b), generating a poster (Qiang et al., 2016) or a science news blog title (Vadapalli et al., 2018) about a published paper. This is the first work on automatic writing of key paper elements for the biomedical domain, especially conclusion and future work, and follow-on paper titles. 5 Conclusions"
P19-1191,D18-1356,0,0.0165433,"his is the first work to incorporate new idea creation via link prediction into automatic paper writing. Knowledge-driven Generation. Deep Neural Networks have been applied to generate natural language to describe structured knowledge bases (Duma and Klein, 2013; Konstas and Lapata, 2013; Flanigan et al., 2016; Hardy and Vlachos, 2018; Pourdamghani et al., 2016; Trisedya et al., 2018; Xu et al., 2018; Madotto et al., 2018; Nie et al., 2018), biographies based on attributes (Lebret et al., 2016; Chisholm et al., 2017; Liu et al., 2018; Sha et al., 2018; Kaffee et al., 2018; Wang et al., 2018a; Wiseman et al., 2018), and image/video captions based on background entities and events (Krishnamoorthy et al., 2013; Wu et al., 2018; Whitehead et al., 2018; Lu et al., 2018). To handle unknown words, we design an architecture similar to pointer-generator networks (See et al., 2017) and copy mechanism (Gu et al., 2016). Some interesting applications include generating abstracts based on titles for the natural language processing domain (Wang et al., 2018b), generating a poster (Qiang et al., 2016) or a science news blog title (Vadapalli et al., 2018) about a published paper. This is the first work on automatic wr"
P19-1293,D18-1549,0,0.0357829,", 2017), which suffer from a huge hypothesis space. Recent approaches to zero-shot machine translation include pivot-based methods (Chen et al., 2017; Zheng et al., 2017; Cheng et al., 2016) and multi-lingual NMT methods (Firat et al., 2016a,b; Johnson et al., 2017; Ha et al., 2016, 2017). These systems are zero-shot for a specific source/target language pair, but need parallel data from source to a pivot or multiple other languages. More recently, totally unsupervised NMT methods are introduced that use only monolingual data for training a machine translation system. Lample et al. (2018a,c), Artetxe et al. (2018), and 3057 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3057–3062 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Yang et al. (2018) use iterative back-translation to train MT models in both directions simultaneously. Their training takes place on massive monolingual data and requires a long time to train as well as careful tuning of hyperparameters. The closest unsupervised NMT work to ours is by Kim et al. (2018). Similar to us, they break translation into glossing and correction steps. However, the"
P19-1293,Q17-1010,0,0.333604,"s a hard task and needs to be tuned for every language. Previous zero-shot NMT work compensates for a lack of source/target parallel data by either using source/pivot parallel data, extremely large monolingual data, or artificially generated data. These requirements and techniques limit the methods’ applicability to real-world low-resource languages. Instead, in this paper we propose using parallel data from high-resource languages to learn ‘how to translate’ and apply the trained system to low resource settings. We use off-theshelf technologies to build word embeddings from monolingual data (Bojanowski et al., 2017) and learn a source-to-target bilingual dictionary using source and target embeddings (Lample et al., 2018b). Given a target language, we train sourceto-target dictionaries for a diverse set of highresource source languages, and use them to convert the source side of the parallel data to Translationese. We combine this parallel data and train a Translationese-to-target translator on it. Later, we can build source-to-target dictionaries on-demand, generate Translationese from source texts, and use the pre-trained system to rapidly produce machine translation for many languages without requiring"
P19-1293,P18-4011,1,0.899332,"lel data, with the source side converted into Translationese using Step 1. Introduction Quality of machine translation, especially neural MT, highly depends on the amount of available parallel data. For a handful of languages, where parallel data is abundant, MT quality has reached quite good performance (Wu et al., 2016; Hassan et al., 2018). However, the quality of translation rapidly deteriorates as the amount of parallel data decreases (Koehn and Knowles, 2017). Unfortunately, many languages have close to zero parallel texts. Translating texts from these languages requires new techniques. Hermjakob et al. (2018) presented a hybrid human/machine translation tool that uses lexical translation tables to gloss a translation and relies on human language and world models to propagate glosses into fluent translations. Inspired by that work, this work investigates the following The notion of separating adequacy from fluency components into a pipeline of operations dates back to the early days of MT and NLP research, where the inadequacy of word-by-word MT was first observed (Yngve, 1955; Oswald, 1952). A subfield of MT research that seeks to improve fluency given disfluent but adequate first-pass translation"
P19-1293,D18-1101,0,0.617894,"onolingual data for training a machine translation system. Lample et al. (2018a,c), Artetxe et al. (2018), and 3057 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3057–3062 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Yang et al. (2018) use iterative back-translation to train MT models in both directions simultaneously. Their training takes place on massive monolingual data and requires a long time to train as well as careful tuning of hyperparameters. The closest unsupervised NMT work to ours is by Kim et al. (2018). Similar to us, they break translation into glossing and correction steps. However, their correction step is trained on artificially generated noisy data aimed at simulating glossed source texts. Although this correction method helps, simulating noise caused by natural language phenomena is a hard task and needs to be tuned for every language. Previous zero-shot NMT work compensates for a lack of source/target parallel data by either using source/pivot parallel data, extremely large monolingual data, or artificially generated data. These requirements and techniques limit the methods’ applicab"
P19-1293,2005.mtsummit-papers.11,0,0.109419,"et al., 2017). These embeddings are used to train bilingual dictionaries. We select English as the target language. In order to avoid biasing the trained system toward a language or a specific type of parallel data, we use diverse parallel data on a diverse set of languages to train the Translationese to English system. We use Arabic, Czech, Dutch, Finnish, French, German, Italian, Russian, and Spanish as the set of out training languages. We use roughly 2 million sentence pairs per language and limit the length of the sentences to 100 tokens. For Dutch, Finnish, and Italian we use Europarl (Koehn, 2005) for parallel data. For Arabic we use MultiUN (Tiedemann, 2012). For French we use CommonCrawl. For German we use a mix of CommonCrawl (1.7M), and NewsCommentary (300K). The numbers in parentheses show the number of sentences for each dataset. For Spanish we use CommonCrawl (1.8M), and Europarl (200K). For Russian we use Yandex (1M), CommonCrawl (800K), and NewsCommentary (200K), and finally for Czech we use a mix of ParaCrawl (1M), Europarl (640K), NewsCommentary (200K), and CommonCrawl (160K). We train one model on these nine languages and apply it to test languages not in this set. Also, to"
P19-1293,P17-1176,0,0.0163683,"(Yngve, 1955; Oswald, 1952). A subfield of MT research that seeks to improve fluency given disfluent but adequate first-pass translation is automatic post-editing (APE) pioneered by Knight and Chander (1994). Much of the current APE work targets correction of black-box MT systems, which are presumed to be supervised. Early approaches to unsupervised machine translation include decipherment methods (Nuhn et al., 2013; Ravi and Knight, 2011; Pourdamghani and Knight, 2017), which suffer from a huge hypothesis space. Recent approaches to zero-shot machine translation include pivot-based methods (Chen et al., 2017; Zheng et al., 2017; Cheng et al., 2016) and multi-lingual NMT methods (Firat et al., 2016a,b; Johnson et al., 2017; Ha et al., 2016, 2017). These systems are zero-shot for a specific source/target language pair, but need parallel data from source to a pivot or multiple other languages. More recently, totally unsupervised NMT methods are introduced that use only monolingual data for training a machine translation system. Lample et al. (2018a,c), Artetxe et al. (2018), and 3057 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3057–3062 c Florence,"
P19-1293,W02-0902,1,0.5872,"our proposed pipeline includes a word-by-word translation of the source texts. This requires a source/target dictionary. Manually constructed dictionaries exist for many language pairs, however cleaning these dictionaries to get a word to word lexicon is not trivial, and these dictionaries often cover a small portion of the source vocabulary, focusing on stems and specifically excluding inflected variants. In order to have a comprehensive, word to word, inflected bi-lingual dictionary we look for automatically built ones. Automatic lexical induction is an active field of research (Fung, 1995; Koehn and Knight, 2002; Haghighi et al., 2008; Lample et al., 2018b). A popular method for automatic extraction of bilingual dictionaries is through building cross-lingual word embeddings. Finding a shared word representation space between two languages enables us to calculate the distance between word embeddings of source and target, which helps us to find translation candidates for each word. We follow this approach for building the bilingual dictionaries. For a given source and target language, we start by separately training source and target word embeddings S and T , and use the method introduced by Lample et"
P19-1293,W17-3204,0,0.0315501,"nput into a pseudo-translation or ‘Translationese’. 2. Translate the Translationese into target language, using a model built in advance from various parallel data, with the source side converted into Translationese using Step 1. Introduction Quality of machine translation, especially neural MT, highly depends on the amount of available parallel data. For a handful of languages, where parallel data is abundant, MT quality has reached quite good performance (Wu et al., 2016; Hassan et al., 2018). However, the quality of translation rapidly deteriorates as the amount of parallel data decreases (Koehn and Knowles, 2017). Unfortunately, many languages have close to zero parallel texts. Translating texts from these languages requires new techniques. Hermjakob et al. (2018) presented a hybrid human/machine translation tool that uses lexical translation tables to gloss a translation and relies on human language and world models to propagate glosses into fluent translations. Inspired by that work, this work investigates the following The notion of separating adequacy from fluency components into a pipeline of operations dates back to the early days of MT and NLP research, where the inadequacy of word-by-word MT w"
P19-1293,N16-1101,0,0.0299477,"n disfluent but adequate first-pass translation is automatic post-editing (APE) pioneered by Knight and Chander (1994). Much of the current APE work targets correction of black-box MT systems, which are presumed to be supervised. Early approaches to unsupervised machine translation include decipherment methods (Nuhn et al., 2013; Ravi and Knight, 2011; Pourdamghani and Knight, 2017), which suffer from a huge hypothesis space. Recent approaches to zero-shot machine translation include pivot-based methods (Chen et al., 2017; Zheng et al., 2017; Cheng et al., 2016) and multi-lingual NMT methods (Firat et al., 2016a,b; Johnson et al., 2017; Ha et al., 2016, 2017). These systems are zero-shot for a specific source/target language pair, but need parallel data from source to a pivot or multiple other languages. More recently, totally unsupervised NMT methods are introduced that use only monolingual data for training a machine translation system. Lample et al. (2018a,c), Artetxe et al. (2018), and 3057 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3057–3062 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Yang et al."
P19-1293,D16-1026,0,0.018138,"n disfluent but adequate first-pass translation is automatic post-editing (APE) pioneered by Knight and Chander (1994). Much of the current APE work targets correction of black-box MT systems, which are presumed to be supervised. Early approaches to unsupervised machine translation include decipherment methods (Nuhn et al., 2013; Ravi and Knight, 2011; Pourdamghani and Knight, 2017), which suffer from a huge hypothesis space. Recent approaches to zero-shot machine translation include pivot-based methods (Chen et al., 2017; Zheng et al., 2017; Cheng et al., 2016) and multi-lingual NMT methods (Firat et al., 2016a,b; Johnson et al., 2017; Ha et al., 2016, 2017). These systems are zero-shot for a specific source/target language pair, but need parallel data from source to a pivot or multiple other languages. More recently, totally unsupervised NMT methods are introduced that use only monolingual data for training a machine translation system. Lample et al. (2018a,c), Artetxe et al. (2018), and 3057 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3057–3062 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Yang et al."
P19-1293,W95-0114,0,0.23432,"rst step of our proposed pipeline includes a word-by-word translation of the source texts. This requires a source/target dictionary. Manually constructed dictionaries exist for many language pairs, however cleaning these dictionaries to get a word to word lexicon is not trivial, and these dictionaries often cover a small portion of the source vocabulary, focusing on stems and specifically excluding inflected variants. In order to have a comprehensive, word to word, inflected bi-lingual dictionary we look for automatically built ones. Automatic lexical induction is an active field of research (Fung, 1995; Koehn and Knight, 2002; Haghighi et al., 2008; Lample et al., 2018b). A popular method for automatic extraction of bilingual dictionaries is through building cross-lingual word embeddings. Finding a shared word representation space between two languages enables us to calculate the distance between word embeddings of source and target, which helps us to find translation candidates for each word. We follow this approach for building the bilingual dictionaries. For a given source and target language, we start by separately training source and target word embeddings S and T , and use the method"
P19-1293,P13-1154,0,0.0171821,"e notion of separating adequacy from fluency components into a pipeline of operations dates back to the early days of MT and NLP research, where the inadequacy of word-by-word MT was first observed (Yngve, 1955; Oswald, 1952). A subfield of MT research that seeks to improve fluency given disfluent but adequate first-pass translation is automatic post-editing (APE) pioneered by Knight and Chander (1994). Much of the current APE work targets correction of black-box MT systems, which are presumed to be supervised. Early approaches to unsupervised machine translation include decipherment methods (Nuhn et al., 2013; Ravi and Knight, 2011; Pourdamghani and Knight, 2017), which suffer from a huge hypothesis space. Recent approaches to zero-shot machine translation include pivot-based methods (Chen et al., 2017; Zheng et al., 2017; Cheng et al., 2016) and multi-lingual NMT methods (Firat et al., 2016a,b; Johnson et al., 2017; Ha et al., 2016, 2017). These systems are zero-shot for a specific source/target language pair, but need parallel data from source to a pivot or multiple other languages. More recently, totally unsupervised NMT methods are introduced that use only monolingual data for training a machi"
P19-1293,1983.tc-1.13,0,0.670265,"have close to zero parallel texts. Translating texts from these languages requires new techniques. Hermjakob et al. (2018) presented a hybrid human/machine translation tool that uses lexical translation tables to gloss a translation and relies on human language and world models to propagate glosses into fluent translations. Inspired by that work, this work investigates the following The notion of separating adequacy from fluency components into a pipeline of operations dates back to the early days of MT and NLP research, where the inadequacy of word-by-word MT was first observed (Yngve, 1955; Oswald, 1952). A subfield of MT research that seeks to improve fluency given disfluent but adequate first-pass translation is automatic post-editing (APE) pioneered by Knight and Chander (1994). Much of the current APE work targets correction of black-box MT systems, which are presumed to be supervised. Early approaches to unsupervised machine translation include decipherment methods (Nuhn et al., 2013; Ravi and Knight, 2011; Pourdamghani and Knight, 2017), which suffer from a huge hypothesis space. Recent approaches to zero-shot machine translation include pivot-based methods (Chen et al., 2017; Zheng et"
P19-1293,D17-1266,1,0.859975,"cy components into a pipeline of operations dates back to the early days of MT and NLP research, where the inadequacy of word-by-word MT was first observed (Yngve, 1955; Oswald, 1952). A subfield of MT research that seeks to improve fluency given disfluent but adequate first-pass translation is automatic post-editing (APE) pioneered by Knight and Chander (1994). Much of the current APE work targets correction of black-box MT systems, which are presumed to be supervised. Early approaches to unsupervised machine translation include decipherment methods (Nuhn et al., 2013; Ravi and Knight, 2011; Pourdamghani and Knight, 2017), which suffer from a huge hypothesis space. Recent approaches to zero-shot machine translation include pivot-based methods (Chen et al., 2017; Zheng et al., 2017; Cheng et al., 2016) and multi-lingual NMT methods (Firat et al., 2016a,b; Johnson et al., 2017; Ha et al., 2016, 2017). These systems are zero-shot for a specific source/target language pair, but need parallel data from source to a pivot or multiple other languages. More recently, totally unsupervised NMT methods are introduced that use only monolingual data for training a machine translation system. Lample et al. (2018a,c), Artetxe"
P19-1293,P11-1002,1,0.864862,"ing adequacy from fluency components into a pipeline of operations dates back to the early days of MT and NLP research, where the inadequacy of word-by-word MT was first observed (Yngve, 1955; Oswald, 1952). A subfield of MT research that seeks to improve fluency given disfluent but adequate first-pass translation is automatic post-editing (APE) pioneered by Knight and Chander (1994). Much of the current APE work targets correction of black-box MT systems, which are presumed to be supervised. Early approaches to unsupervised machine translation include decipherment methods (Nuhn et al., 2013; Ravi and Knight, 2011; Pourdamghani and Knight, 2017), which suffer from a huge hypothesis space. Recent approaches to zero-shot machine translation include pivot-based methods (Chen et al., 2017; Zheng et al., 2017; Cheng et al., 2016) and multi-lingual NMT methods (Firat et al., 2016a,b; Johnson et al., 2017; Ha et al., 2016, 2017). These systems are zero-shot for a specific source/target language pair, but need parallel data from source to a pivot or multiple other languages. More recently, totally unsupervised NMT methods are introduced that use only monolingual data for training a machine translation system."
P19-1293,P16-1162,0,0.0432095,"ple et al. (2018b) to find the cross-lingual embedding vectors. In order to create the dictionary we limit the size of the source and target (English) vocabulary 1 https://github.com/tensorflow/ tensor2tensor 2 https://github.com/facebookresearch/ fastText/blob/master/pretrained-vectors. md 3059 to 100K tokens. For each source token we find 20 nearest neighbors in the target language. We use a 5-gram language model trained on 4 billion tokens of Gigaword to select between the translation options for each token. We use Moses scripts for tokenizing and lowercasing the data. We do not apply BPE (Sennrich et al., 2016) on the data. In order to be comparable to Kim et al. (2018) we split German compound words only for the newstest2016 test data. We use the CharSplit3 python package for this purpose. We use tensor2tensor’s transformer base hyperparameters to train the transformer model on a single gpu for each language. 4 Experiments We report translation results on newstest2013 for Spanish, newstest2014 for French, and newstest2016 for Czech, German, Finnish, Romanian, and Russian. We also report results on the first 3000 sentences of GlobalVoices20154 for Dutch, Bulgarian, Danish, Indonesian, Polish, Portug"
P19-1293,tiedemann-2012-parallel,0,0.0115774,"l dictionaries. We select English as the target language. In order to avoid biasing the trained system toward a language or a specific type of parallel data, we use diverse parallel data on a diverse set of languages to train the Translationese to English system. We use Arabic, Czech, Dutch, Finnish, French, German, Italian, Russian, and Spanish as the set of out training languages. We use roughly 2 million sentence pairs per language and limit the length of the sentences to 100 tokens. For Dutch, Finnish, and Italian we use Europarl (Koehn, 2005) for parallel data. For Arabic we use MultiUN (Tiedemann, 2012). For French we use CommonCrawl. For German we use a mix of CommonCrawl (1.7M), and NewsCommentary (300K). The numbers in parentheses show the number of sentences for each dataset. For Spanish we use CommonCrawl (1.8M), and Europarl (200K). For Russian we use Yandex (1M), CommonCrawl (800K), and NewsCommentary (200K), and finally for Czech we use a mix of ParaCrawl (1M), Europarl (640K), NewsCommentary (200K), and CommonCrawl (160K). We train one model on these nine languages and apply it to test languages not in this set. Also, to test on each of the training languages, we train a model where"
P19-1293,P18-1005,0,0.0332431,"t al., 2016a,b; Johnson et al., 2017; Ha et al., 2016, 2017). These systems are zero-shot for a specific source/target language pair, but need parallel data from source to a pivot or multiple other languages. More recently, totally unsupervised NMT methods are introduced that use only monolingual data for training a machine translation system. Lample et al. (2018a,c), Artetxe et al. (2018), and 3057 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3057–3062 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Yang et al. (2018) use iterative back-translation to train MT models in both directions simultaneously. Their training takes place on massive monolingual data and requires a long time to train as well as careful tuning of hyperparameters. The closest unsupervised NMT work to ours is by Kim et al. (2018). Similar to us, they break translation into glossing and correction steps. However, their correction step is trained on artificially generated noisy data aimed at simulating glossed source texts. Although this correction method helps, simulating noise caused by natural language phenomena is a hard task and needs"
P95-1034,J93-2003,0,0.0168377,"bility. By retraining the statistical component on a different domain, we can automatically pick up the peculiarities of the sublanguage such as preferences for particular words and collocations. At the same time, we take advantage of the strength of the knowledge-based approach which guarantees grammatical inputs to the statistical component, and reduces the amount of language structure that is to be retrieved from statistics. This approach addresses the problematic aspects of both pure knowledge-based generation (where incomplete knowledge is inevitable) and pure statistical bag generation (Brown et al., 1993) (where the statistical system has no linguistic guidance). Of course, the results are not perfect. We can improve on them by enhancing the statistical model, or by incorporating more knowledge and constraints in the lattices, possibly using automatic knowledge acquisition methods. One direction we intend to pursue is the rescoring of the top N generated sentences by more expensive (and extensive) methods, incorporating for example stylistic features or explicit knowledge of flexible collocations. Acknowledgments We would like to thank Yolanda Gil, Eduard Hovy, Kathleen McKeown, Jacques Robin,"
P95-1034,H89-2027,0,0.00685432,"oint in the search and causes the creation of a state in the lattice; all continuations of alternative lexicalizations for this island become paths that leave this state. Choices between alternative lexical islands for the same concept also become states in the lattice, with arcs leading to the sub-lattices corresponding to each island. Once the semantic input to the generator has been transformed to a word lattice, a search component identifies the N highest scoring paths from the start to the final state, according to our statistical language model. We use a version of the N-best algorithm (Chow and Schwartz, 1989), a Viterbistyle beam search algorithm that allows extraction of more than just the best scoring path. (Hatzivassiloglou and Knight, 1995) has more details on our search algorithm and the method we applied to estimate the parameters of the statistical model. Our approach differs from traditional top-down generation in the same way that top-down and bottom-up parsing differ. In top-down parsing, backtracking is employed to exhaustively examine the space of possible alternatives. Similarly, traditional control mechanisms in generation operate top-down, either deterministically (Meteer et al., 19"
P95-1034,J94-4003,0,0.0116872,"Missing"
P95-1034,C94-1058,0,0.0211499,"e at all, then we choose A. But ifA and B are long sentences, then probably we have seen neither. In that case, further approximations are required. For example, does A contain frequent three-word sequences? Does B? Following this reasoning, we are led into statistical language modeling. We built a language model As for new company, there is plan to establish in February. Here are three randomly selected translations; note that the object of the ""establishing"" action is unspecified in the Japanese input, but PENMAN supplies a placeholder it when necessary, to ensure grammaticality: 4See also (Harbusch et al., 1994) for a thorough discussion of defaulting in NLG systems. SAvailable from the ACL Data Collection Initiative, as CD ROM 1. 254 A new company will have in mind that it is establishing it on February. The new company plans the launching on February. New companies will have as a goal the launching at February. We then ranked the 3,456 sentences using the bigram version of our statistical language model, with the hope that good renditions would come out on top. Here is an abridged list of outputs, loglikelihood scores heuristically corrected for length, and rankings: 1 The new company plans to laun"
P95-1034,P81-1033,0,0.0151423,"Missing"
P95-1034,P94-1045,0,0.0164557,",000 conceptual terms and 100,000 words or phrases. Turning conceptual expressions into English requires the integration of large knowledge bases (KBs), including grammar, ontology, lexicon, collocations, and mappings between them. The quality of an NLG system depends on the quality of its inputs and knowledge bases. Given that perfect KBs do not yet exist, an important question arises: can we build high-quality NLG systems that are robust against incomplete KBs and inputs? Although robustness has been heavily studied in natural language understanding (Weischedel and Black, 1980; Hayes, 1981; Lavie, 1994), it has received much less attention in NLG (Robin, 1995). We describe a hybrid model for natural language generation which offers improved performance in the presence of knowledge gaps in the generator (the grammar and the lexicon), and of errors in the semantic input. The model comes out of our practical experience in building a large Japanese-English newspaper machine translation system, JAPANGLOSS (Knight et al., 1994; Knight et al., 1995). This system translates Japanese into representations whose terms are drawn from the SENSUS ontolVasileios Hatzivassiloglou D e p a r t m e n t of C o"
P95-1034,1994.amta-1.18,1,\N,Missing
P95-1034,J80-2003,0,\N,Missing
P95-1034,E87-1001,0,\N,Missing
P97-1017,H94-1050,0,0.0159127,"trings which are actually grist for Japanese transliteratots. For example, people rarely transliterate auxiliary verbs, but surnames are often transliterated. We have approximated such a model by removing high-frequency words like has, an, are, am, were, their, and does, plus unlikely words corresponding to Japanese sound bites, like coup and oh. We also built a separate word sequence model containing only English first and last names. If we know (from context) that the transliterated phrase is a personal name, this model is more precise. P(w) • P(e[w). P(jle)"" P ( k J j ) . P(olk) Following (Pereira et al., 1994; Pereira and Riley, I996), we implement P(w) in a weighted finite-state aceeptor (WFSA) and we implement the other distributions in weighted finite-state transducers (WFSTs). A WFSA is an state/transition diagram with weights and symbols on the transitions, making some output sequences more likely than others. A W F S T is a WFSA with a pair of symbols on each transition, one input, and one output. Inputs and outputs may include the empty symbol e. Also following (Pereira and Riley, 1996), we have implemented a general composition algorithm for constructing an integrated model P(zlz) from mod"
P97-1017,H94-1029,0,0.0124493,"information-losing operation: a i s u k u r i i m u loses the distinction between ice cream and I scream. Transliteration is not trivial to automate, but we will be concerned with an even more challenging problem--going from katakana back to English, i.e., back-transliteration. Automating backtransliteration has great practical importance in Japanese/English machine translation. Katakana phrases are the largest source of text phrases that do not appear in bilingual dictionaries or training corpora (a.k.a. ""not-found words""). However, very little computational work has been done in this area; (Yamron et al., 1994) briefly mentions a patternmatching approach, while (Arbabi et al., 1994) discuss a hybrid neural-net/expert-system approach to (forward) transliteration. The information-losing aspect of transliteration makes it hard to invert. Here are some problem instances, taken from actual newspaper articles: 1 ITexts used in ARPA Machine Translation evaluations, November 1994. 128 ? T--x~-(aasudee) '9 (robaato shyoon renaado) 2 ? ""~':~ ~--:~"" l . - - ) - ~ y I(masu~aazu~ oonamen~ o) English translations appear later in this paper. Here are a few observations about backtransliteration: • • solution. Choo"
P97-1017,H94-1096,0,\N,Missing
P98-1116,P96-1027,0,0.12938,"he grammar is not organized around English syntax. Nitrogen&apos;s algorithm operates bottom-up, efficiently encoding multiple analyses in a lattice data structure to allow structure sharing, analogous to the way a chart is used in bottom-up parsing. In contrast, traditional generation control mechanisms work top-down, either deterministically (Meteer et al., 1987; Penman, 1989) or by backtracking to previous choice points (Elhadad, 1993b). This unnecessarily duplicates work at run time, unless sophisticated control directives are included in the search engine (Elhadad and Robin, 1992). Recently, (Kay, 1996) has explored a bottom-up approach to generation as well, using a chart rather than a word lattice. Nitrogen&apos;s generation is robust and scalable. It can generate output even for unexpected or incomplete input, and is designed for broad coverage. It does not require the detailed, difficult-to-obtain knowledge bases that other NLG systems require, since it relies instead on corpus-based statistics to make a wide variety of linguistic decisions. Currently the quality of the output is limited by the use of only word bigram statistical information, which cannot handle long-distance agreement, or di"
P98-1116,P95-1034,1,0.709009,". Thus, shifting such linguistic decisions to the generator is significantly helpful for client applications. However, at the same time, it imposes enormous needs for knowledge on the generator program. Traditional large-scale NLG already requires immense amounts of knowledge, as does any large-scale AI enterprise. NLG operating on a scale of 200,000 entities (concepts, relations, and words) requires large and sophisticated lexicons, grammars, ontologies, collocation lists, and morphological tables. Acquiring and applying accurate, detailed knowledge of this breadth poses difficult problems. (Knight and Hatzivassiloglou, 1995) suggested tistical knowledge. Nitrogen performs sentence realization and some components of sentence planning-namely, mapping domain concepts to content words, and to some extent, mapping semantic relations to grammatical ones. It contributes: meaning symbolic generator ] ~- lexicon +- grammar • A flexible input representation based on conceptual meanings and the relations between them. word lattice of possible renderings • A new grammar formalism for defining the mapping of meanings onto word lattices. I statistical extractor ] &lt;--- corpus • A new efficient algorithm to do this mapping. Engl"
P98-1116,H94-1020,0,0.0475277,"ected or incomplete input, and is designed for broad coverage. It does not require the detailed, difficult-to-obtain knowledge bases that other NLG systems require, since it relies instead on corpus-based statistics to make a wide variety of linguistic decisions. Currently the quality of the output is limited by the use of only word bigram statistical information, which cannot handle long-distance agreement, or distinguish likely collocations from unlikely grammatical structure. However, we plan to remedy these problems by using statistical information extracted from the Penn Treebank corpus (Marcus et al., 1994) to rank tagged lattices and parse forests. Nitrogen&apos;s rule matching is much less expensive than graph unification, and lattices generated for sub-AMIRs are cached and reused in subsequent references. The semantic roles used in the grammar formalism cover most common syntactic phenomena, though our grammar does not yet generate questions, or infer pronouns from explicit coreference. Nitrogen has been used extensively as part of a semantics-based Japanese-English MT system (Knight et al., 1995). Japanese analysis provides AMR&apos;s, which Nitrogen transforms into word lattices on the order of hundr"
P98-1116,P89-1002,0,0.0561705,"Missing"
soricut-etal-2002-using,1999.tc-1.8,0,\N,Missing
soricut-etal-2002-using,J93-2003,0,\N,Missing
soricut-etal-2002-using,knight-al-onaizan-1998-translation,1,\N,Missing
soricut-etal-2002-using,W98-0301,1,\N,Missing
soricut-etal-2002-using,P01-1030,1,\N,Missing
W02-0505,W98-1005,1,\N,Missing
W02-0902,J90-2002,0,0.0632522,"Missing"
W02-0902,P98-1069,0,0.730799,"Missing"
W02-0902,W01-0504,1,0.33419,"Missing"
W02-0902,N01-1020,0,0.0613017,"nt and Pr¨asident. Still, these words – often called cognates – maintain a very similar spelling. This can be defined as differing in very few letters. This measurement can be formalized as the number of letters common in sequence between the two words, divided by the length of the longer word. The example word pair friend and freund shares 5 letters (fr-e-nd), and both words have length 6, hence there spelling similarity is 5/6, or 0.83. This measurement is called longest common subsequence ratio [Melamed, 1995]. In related work, string edit distance (or, Levenshtein distance) has been used [Mann and Yarowski, 2001]. With this computational means at hand, we can now measure the spelling similarity between every German and English word, and sort possible word pairs accordingly. By going through this list starting at the top we can collect new word pairs. We do this is in a greedy fashion – once a word is assigned to a word pair, we do not look for another match. Table 2 gives the top 24 generated word pairs by this algorithm. English organization president industries parliament interests institute satellite dividend machine magazine february program premium branch volume january warning parties debate ex"
W02-0902,W95-0115,0,0.00946485,"or words that can be traced back to common language roots, such as friend and Freund, or president and Pr¨asident. Still, these words – often called cognates – maintain a very similar spelling. This can be defined as differing in very few letters. This measurement can be formalized as the number of letters common in sequence between the two words, divided by the length of the longer word. The example word pair friend and freund shares 5 letters (fr-e-nd), and both words have length 6, hence there spelling similarity is 5/6, or 0.83. This measurement is called longest common subsequence ratio [Melamed, 1995]. In related work, string edit distance (or, Levenshtein distance) has been used [Mann and Yarowski, 2001]. With this computational means at hand, we can now measure the spelling similarity between every German and English word, and sort possible word pairs accordingly. By going through this list starting at the top we can collect new word pairs. We do this is in a greedy fashion – once a word is assigned to a word pair, we do not look for another match. Table 2 gives the top 24 generated word pairs by this algorithm. English organization president industries parliament interests institute sa"
W02-0902,P95-1050,0,0.66337,"propose a similar approach: They count how often another word occurs in the same sentence as the target word. The counts are then normalized by a using the tf/idf method which is often used in information retrieval [Jones, 1979]. The need for translating the context poses a chicken-and-egg problem: If we already have a translation lexicon we can translate the context vectors. But we can only construct a translation lexicon with this approach if we are already able to translate the context vectors. Theoretically, it is possible to use these methods to build a translation lexicon from scratch [Rapp, 1995]. The number of possible mappings has complexity O(n!), and the computing cost of each mapping has quadratic complexity O(n2 ). For a large number of words n – at least more than 10,000, maybe more than 100,000 – the combined complexity becomes prohibitively expensive. Because of this, both Rapp and Fung focus on expanding an existing large lexicon to add a few novel terms. Clearly, a seed lexicon to bootstrap these methods is needed. Fortunately, we have outlined in Section 2.1 how such a seed lexicon can be obtained: by finding words spelled identically in both languages. We can then constr"
W02-0902,P99-1067,0,0.925608,"Missing"
W02-0902,W99-0626,0,0.0158485,"Missing"
W02-0902,C98-1066,0,\N,Missing
W02-2102,W02-2103,1,\N,Missing
W02-2102,A00-2018,0,\N,Missing
W02-2102,A00-2023,0,\N,Missing
W02-2102,C00-1007,0,\N,Missing
W02-2102,P98-1116,1,\N,Missing
W02-2102,C98-1112,1,\N,Missing
W02-2102,P98-1035,0,\N,Missing
W02-2102,C98-1035,0,\N,Missing
W02-2102,P02-1039,1,\N,Missing
W02-2102,P01-1067,1,\N,Missing
W02-2102,P99-1042,0,\N,Missing
W02-2102,J01-2004,0,\N,Missing
W02-2102,P02-1057,1,\N,Missing
W02-2102,P01-1017,0,\N,Missing
W06-1606,J93-2003,0,0.0178886,"Missing"
W06-1606,P05-1033,0,0.882263,"(Marcu and Wong, 2002; Koehn et al., 2003; Och and Ney, 2004) have led to significant increases in machine translation accuracy. Although phrase-based models yield high-quality translations for language pairs that exhibit similar word order, they fail to produce grammatical outputs for language pairs that are syntactically divergent. Recent models that exploit syntactic information of the source language (Quirk et al., 2005) have been shown to produce better outputs than phrase-based systems when evaluated on relatively small scale, domain specific corpora. And syntax-inspired formal models (Chiang, 2005), in spite of being trained on significantly less data, have shown promising results when compared on the same test sets with mature phrase-based systems. To our knowledge though, no previous research has demonstrated that a syntax-based statistical translation system could produce better results than a phrase-based system on a large-scale, well-established, open domain translation task. In this paper we present such a system. Our translation models rely upon and naturally exploit submodels (feature functions) that have 2.1 An intuitive introduction to SPMT After being exposed to 100M+ words o"
W06-1606,J04-4002,0,0.831104,"de with a brief discussion. We introduce SPMT, a new class of statistical Translation Models that use Syntactified target language Phrases. The SPMT models outperform a state of the art phrase-based baseline model by 2.64 Bleu points on the NIST 2003 Chinese-English test corpus and 0.28 points on a humanbased quality metric that ranks translations on a scale from 1 to 5. 1 Introduction 2 SPMT: statistical Machine Translation with Syntactified Phrases During the last four years, various implementations and extentions to phrase-based statistical models (Marcu and Wong, 2002; Koehn et al., 2003; Och and Ney, 2004) have led to significant increases in machine translation accuracy. Although phrase-based models yield high-quality translations for language pairs that exhibit similar word order, they fail to produce grammatical outputs for language pairs that are syntactically divergent. Recent models that exploit syntactic information of the source language (Quirk et al., 2005) have been shown to produce better outputs than phrase-based systems when evaluated on relatively small scale, domain specific corpora. And syntax-inspired formal models (Chiang, 2005), in spite of being trained on significantly less"
W06-1606,J03-4003,0,0.00529773,"by the following derivation: r4 (r9 (r7 ), r3 (r6 (r12 (r8 )))). Figure 2: English parse tree derivation of the Chinese string COMINGFROM FRANCE AND RUSSIA pDE ASTRO- -NAUTS. ple, c(Θ) = (π, F, A). The probability of each derivation θi is given by the product of the probabilities of all the rules p(rj ) in the derivation (see equation 4). P r(π, F, A) = X Y p(rj ) (4) θi ∈Θ,c(Θ)=(π,F,A) rj ∈θi In order to acquire the rules specific to our model and to induce their probabilities, we parse the English side of our corpus with an in-house implementation (Soricut, 2005) of Collins parsing models (Collins, 2003) and we word-align the parallel corpus with the Giza++2 implementation of the IBM models (Brown et al., 1993). We use the automatically derived hEnglish-parse-tree, English-sentence, Foreign-sentence, Word-levelalignmenti tuples in order to induce xRS rules for several models. 2.2.2 SPMT Model 1 In our simplest model, we assume that each tuple (π, F, A) in our automatically annotated corpus could be produced by applying a combination of minimally syntactified, lexicalized, phrase-based compatible xRS rules, and minimal/necessary, non-lexicalized xRS rules. We call a rule non-lexicalized whenev"
W06-1606,N04-1035,1,0.399768,"minimal rules (lexicalized and non-lexicalized) by applying the algorithm proposed by Galley et al. (2006) and then remove the lexicalized rules. We remove the Galley et al.’s lexicalized rules because they are either already accounted for by the minimally syntactified, lexicalized, phrasebased-compatible xRS rules or they subsume noncontinuous source-target phrase pairs. It is worth mentioning that, in our framework, a rule is defined to be “minimal” with respect to a foreign/source language phrase, i.e., it is the minimal xRS rule that yields that source phrase. In contrast, in the work of Galley et al. (2004; 2006), a rule is defined to be minimal when it is necessary in order to explain a (π, F, A) tuple. Under SPMT model 1, the tree in Figure 2 can be produced, for example, by the following derivation: r4 (r9 (r7 ), r3 (r6 (r12 (r8 )))). Figure 2: English parse tree derivation of the Chinese string COMINGFROM FRANCE AND RUSSIA pDE ASTRO- -NAUTS. ple, c(Θ) = (π, F, A). The probability of each derivation θi is given by the product of the probabilities of all the rules p(rj ) in the derivation (see equation 4). P r(π, F, A) = X Y p(rj ) (4) θi ∈Θ,c(Θ)=(π,F,A) rj ∈θi In order to acquire the rules s"
W06-1606,P03-1021,0,0.664719,"iments described in this paper, we use the following submodels (feature functions): Syntax-based-like submodels: • m1inv(ri ) is the IBM model 1 inverse probability computed over the bags of words that occur on the source and target sides of a rule. • lm(e) is the language model probability of the target translation under an ngram language model. • wp(e) is a word penalty model designed to favor longer translations. All these models are combined log-linearly during decoding. The weights of the models are computed automatically using a variant of the Maximum Bleu training procedure proposed by Och (2003). The phrase-based-like submodels have been proved useful in phrase-based approaches to SMT (Och and Ney, 2004). The first two syntaxbased submodels implement a “fused” translation and lexical grounded distortion model (p root ) and a syntax-based distortion model (p cfg ). The indicator submodels are used to determine the extent to which our system prefers lexicalized vs. nonlexicalized rules; simple vs. composed rules; and high vs. low count rules. • proot (ri ) is the root normalized conditional probability of all the rules in a model. • pcfg (ri ) is the CFG-like probability of the non-lex"
W06-1606,P05-1034,0,0.755662,"Missing"
W06-1606,N04-4026,0,0.108335,"Missing"
W06-1606,N03-1017,1,0.0780121,"Section 5, we conclude with a brief discussion. We introduce SPMT, a new class of statistical Translation Models that use Syntactified target language Phrases. The SPMT models outperform a state of the art phrase-based baseline model by 2.64 Bleu points on the NIST 2003 Chinese-English test corpus and 0.28 points on a humanbased quality metric that ranks translations on a scale from 1 to 5. 1 Introduction 2 SPMT: statistical Machine Translation with Syntactified Phrases During the last four years, various implementations and extentions to phrase-based statistical models (Marcu and Wong, 2002; Koehn et al., 2003; Och and Ney, 2004) have led to significant increases in machine translation accuracy. Although phrase-based models yield high-quality translations for language pairs that exhibit similar word order, they fail to produce grammatical outputs for language pairs that are syntactically divergent. Recent models that exploit syntactic information of the source language (Quirk et al., 2005) have been shown to produce better outputs than phrase-based systems when evaluated on relatively small scale, domain specific corpora. And syntax-inspired formal models (Chiang, 2005), in spite of being trained o"
W06-1606,N06-1033,1,0.131843,"by definition pcfg = 1. 48 3 Decoding 4 Experiments 4.1 3.1 Decoding with one SPMT model We evaluate our models on a Chinese to English machine translation task. We use the same training corpus, 138.7M words of parallel Chinese-English data released by LDC, in order to train several statistical-based MT systems: We decode with each of our SPMT models using a straightforward, bottom-up, CKY-style decoder that builds English syntactic constituents on the top of Chinese sentences. The decoder uses a binarized representation of the rules, which is obtained via a syncronous binarization procedure (Zhang et al., 2006). The CKY-style decoder computes the probability of English syntactic constituents in a bottom up fashion, by log-linearly interpolating all the submodel scores described in Section 2.3. • PBMT, a strong state of the art phrase-based system that implements the alignment template model (Och and Ney, 2004); this is the system ISI has used in the 2004 and 2005 NIST evaluations. • four SPMT systems (M1, M1C, M2, M2C) that implement each of the models discussed in this paper; The decoder is capable of producing nbest derivations and nbest lists (Knight and Graehl, 2005), which are used for Maximum"
W06-1606,W02-1018,1,0.779424,"odels empirically. In Section 5, we conclude with a brief discussion. We introduce SPMT, a new class of statistical Translation Models that use Syntactified target language Phrases. The SPMT models outperform a state of the art phrase-based baseline model by 2.64 Bleu points on the NIST 2003 Chinese-English test corpus and 0.28 points on a humanbased quality metric that ranks translations on a scale from 1 to 5. 1 Introduction 2 SPMT: statistical Machine Translation with Syntactified Phrases During the last four years, various implementations and extentions to phrase-based statistical models (Marcu and Wong, 2002; Koehn et al., 2003; Och and Ney, 2004) have led to significant increases in machine translation accuracy. Although phrase-based models yield high-quality translations for language pairs that exhibit similar word order, they fail to produce grammatical outputs for language pairs that are syntactically divergent. Recent models that exploit syntactic information of the source language (Quirk et al., 2005) have been shown to produce better outputs than phrase-based systems when evaluated on relatively small scale, domain specific corpora. And syntax-inspired formal models (Chiang, 2005), in spit"
W06-1606,E06-1005,0,0.0241966,"Missing"
W06-1606,P06-1121,1,\N,Missing
W06-3601,N03-1017,0,0.00595915,"Missing"
W06-3601,J00-1004,0,0.0735197,"Missing"
W06-3601,koen-2004-pharaoh,0,0.0407508,"Missing"
W06-3601,J93-2003,0,0.0121903,"Missing"
W06-3601,A00-2018,0,0.0653872,"Missing"
W06-3601,P05-1033,0,0.0367601,"Missing"
W06-3601,N03-1019,0,0.0196045,"Missing"
W06-3601,C04-1090,0,0.0934174,"Missing"
W06-3601,N06-1045,1,0.849681,"Missing"
W06-3601,P05-1067,0,0.185749,"Missing"
W06-3601,P02-1038,0,0.0734509,"Missing"
W06-3601,P03-2041,0,0.0809973,"Missing"
W06-3601,J04-4002,0,0.0561881,"Missing"
W06-3601,W02-1039,0,0.136926,"Missing"
W06-3601,N04-1035,1,0.402231,"Missing"
W06-3601,P03-1021,0,0.00859701,"Missing"
W06-3601,P05-1034,0,0.293239,"Missing"
W06-3601,C90-3045,0,0.289084,"Missing"
W06-3601,N04-1014,1,0.880617,"Missing"
W06-3601,W05-1506,1,0.399291,"Missing"
W06-3601,J97-3002,0,0.0826936,"Missing"
W06-3601,P01-1067,1,0.837173,"Missing"
W06-3601,J03-4003,0,\N,Missing
W06-3601,J08-3004,1,\N,Missing
W06-3601,W90-0102,0,\N,Missing
W08-0306,P05-1033,0,0.0675646,"Missing"
W08-0306,P07-1003,0,0.426996,"Missing"
W08-0306,D07-1006,0,0.286333,"e left hand sides are rooted at the dotted boxed nodes in the parse tree (R5, R6, R7, R8). 45 deletion improves alignment quality and translation quality in Chinese-English and Arabic-English MT, relative to a strong baseline. Our link deletion algorithm is easy to implement, runs quickly, and has been used by a top-scoring MT system in the Chinese newswire track of the 2008 NIST evaluation. 1.2 Related Work Recently, discriminative methods for alignment have rivaled the quality of IBM Model 4 alignments (Liu et al., 2005; Ittycheriah and Roukos, 2005; Taskar et al., 2005; Moore et al., 2006; Fraser and Marcu, 2007b). However, except for (Fraser and Marcu, 2007b), none of these advances in alignment quality has improved translation quality of a state-of-the-art system. We use a discriminatively trained model to identify and delete incorrect links, and demonstrate that these gains in alignment quality lead to gains in translation quality in a stateof-the-art syntax-based MT system. In contrast to the semi-supervised LEAF alignment algorithm of (Fraser and Marcu, 2007b), which requires 1,5002,000 CPU days per iteration to align 8.4M ChineseEnglish sentences (anonymous, p.c.), link deletion requires only 4"
W08-0306,N04-1035,1,0.657648,"A++ union alignments to improve precision. The low precision of GIZA++ union alignments poses a particular problem for syntax-based rule extraction algorithms such as (Quirk et al., 2005; Galley et al., 2006; Huang et al., 2006; Liu et al., 2006): if the incorrect links violate syntactic correspondences, they force the rule extraction algorithm to extract rules that are large in size, few in number, and poor in generalization ability. Figure 1 illustrates this problem: the dotted line represents an incorrect link in the GIZA++ union alignment. Using the rule extraction algorithm described in (Galley et al., 2004), we extract the rules shown in the leftmost column (R1–R4). Rule R1 is large and unlikely to generalize well. If we delete the incorrect link in Figure 1, we can extract the rules shown in the rightmost column (R2–R9): Rule R1, the largest rule from the initial set, disappears, and several smaller, more modular rules (R5–R9) replace it. In this work, we present a supervised algorithm that uses these two features of the extracted rules (size of largest rule and total number of rules), as well as a handful of structural and lexical features, to automatically identify and delete incorrect links"
W08-0306,P06-1121,1,0.655457,", using features of the extracted rules. We obtain gains in both alignment quality and translation quality in Chinese-English and Arabic-English translation experiments relative to a GIZA++ union baseline. 1 Introduction 1.1 Motivation Word alignment typically constitutes the first stage of the statistical machine translation pipeline. GIZA++ (Och and Ney, 2003), an implementation of the IBM (Brown et al., 1993) and HMM (?) alignment models, is the most widely-used alignment system. GIZA++ union alignments have been used in the state-of-the-art syntax-based statistical MT system described in (Galley et al., 2006) and in the hierarchical phrase-based system Hiero (Chiang, 2007). GIZA++ refined alignments have been used in state-of-the-art phrase-based statistical MT systems such as (Och, 2004); variations on the refined heuristic have been used by (Koehn et al., 2003) (diag and diag-and) and by the phrase-based system Moses (grow-diag-final) (Koehn et al., 2007). GIZA++ union alignments have high recall but low precision, while intersection or refined alignSteven Abney Dept. of Linguistics University of Michigan Ann Arbor, MI 48104 abney@umich.edu ments have high precision but low recall.1 There are tw"
W08-0306,2006.amta-papers.8,1,0.376957,"w precision, while intersection or refined alignSteven Abney Dept. of Linguistics University of Michigan Ann Arbor, MI 48104 abney@umich.edu ments have high precision but low recall.1 There are two natural approaches to improving upon GIZA++ alignments, then: deleting links from union alignments, or adding links to intersection or refined alignments. In this work, we delete links from GIZA++ union alignments to improve precision. The low precision of GIZA++ union alignments poses a particular problem for syntax-based rule extraction algorithms such as (Quirk et al., 2005; Galley et al., 2006; Huang et al., 2006; Liu et al., 2006): if the incorrect links violate syntactic correspondences, they force the rule extraction algorithm to extract rules that are large in size, few in number, and poor in generalization ability. Figure 1 illustrates this problem: the dotted line represents an incorrect link in the GIZA++ union alignment. Using the rule extraction algorithm described in (Galley et al., 2004), we extract the rules shown in the leftmost column (R1–R4). Rule R1 is large and unlikely to generalize well. If we delete the incorrect link in Figure 1, we can extract the rules shown in the rightmost col"
W08-0306,H05-1012,0,0.100207,"f R2, R3, and R4 as before, and the extraction of additional rules whose left hand sides are rooted at the dotted boxed nodes in the parse tree (R5, R6, R7, R8). 45 deletion improves alignment quality and translation quality in Chinese-English and Arabic-English MT, relative to a strong baseline. Our link deletion algorithm is easy to implement, runs quickly, and has been used by a top-scoring MT system in the Chinese newswire track of the 2008 NIST evaluation. 1.2 Related Work Recently, discriminative methods for alignment have rivaled the quality of IBM Model 4 alignments (Liu et al., 2005; Ittycheriah and Roukos, 2005; Taskar et al., 2005; Moore et al., 2006; Fraser and Marcu, 2007b). However, except for (Fraser and Marcu, 2007b), none of these advances in alignment quality has improved translation quality of a state-of-the-art system. We use a discriminatively trained model to identify and delete incorrect links, and demonstrate that these gains in alignment quality lead to gains in translation quality in a stateof-the-art syntax-based MT system. In contrast to the semi-supervised LEAF alignment algorithm of (Fraser and Marcu, 2007b), which requires 1,5002,000 CPU days per iteration to align 8.4M ChineseE"
W08-0306,N03-1017,0,0.00995128,"Missing"
W08-0306,P07-2045,0,0.0178015,"y, 2003), an implementation of the IBM (Brown et al., 1993) and HMM (?) alignment models, is the most widely-used alignment system. GIZA++ union alignments have been used in the state-of-the-art syntax-based statistical MT system described in (Galley et al., 2006) and in the hierarchical phrase-based system Hiero (Chiang, 2007). GIZA++ refined alignments have been used in state-of-the-art phrase-based statistical MT systems such as (Och, 2004); variations on the refined heuristic have been used by (Koehn et al., 2003) (diag and diag-and) and by the phrase-based system Moses (grow-diag-final) (Koehn et al., 2007). GIZA++ union alignments have high recall but low precision, while intersection or refined alignSteven Abney Dept. of Linguistics University of Michigan Ann Arbor, MI 48104 abney@umich.edu ments have high precision but low recall.1 There are two natural approaches to improving upon GIZA++ alignments, then: deleting links from union alignments, or adding links to intersection or refined alignments. In this work, we delete links from GIZA++ union alignments to improve precision. The low precision of GIZA++ union alignments poses a particular problem for syntax-based rule extraction algorithms s"
W08-0306,P06-1096,0,0.0775552,"3) is due to the link deletion algorithm itself. 48 We construct a set of candidate alignments Acandidates for use in reranking as follows. Starting with A = Ainitial , we iteratively explore all alignments A′ in the neighborhood of A, adding each neighbor to Acandidates , then selecting the neighbor that maximizes Score(A′ ). When it is no longer possible to increase Score(A) by deleting any links, link deletion concludes and returns the highest-scoring alignment, A1-best . In general, Agold ∈ / Acandidates ; following (Collins, 2000) and (Charniak and Johnson, 2005) for parse reranking and (Liang et al., 2006) for translation reranking, we define Aoracle as alignment in Acandidates that is most similar to Agold .8 We update each feature weight λi as follows: λi = λi + oracle − hiA1-best .9 hA i Following (Moore, 2005), after each training pass, we average all the feature weight vectors seen during the pass, and decode the discriminative training set using the vector of averaged feature weights. When alignment quality stops increasing on the discriminative training set, perceptron training ends. 10 The weight vector returned by perceptron training is the average over the training set of all weight v"
W08-0306,P05-1057,0,0.0529548,", the extraction of R2, R3, and R4 as before, and the extraction of additional rules whose left hand sides are rooted at the dotted boxed nodes in the parse tree (R5, R6, R7, R8). 45 deletion improves alignment quality and translation quality in Chinese-English and Arabic-English MT, relative to a strong baseline. Our link deletion algorithm is easy to implement, runs quickly, and has been used by a top-scoring MT system in the Chinese newswire track of the 2008 NIST evaluation. 1.2 Related Work Recently, discriminative methods for alignment have rivaled the quality of IBM Model 4 alignments (Liu et al., 2005; Ittycheriah and Roukos, 2005; Taskar et al., 2005; Moore et al., 2006; Fraser and Marcu, 2007b). However, except for (Fraser and Marcu, 2007b), none of these advances in alignment quality has improved translation quality of a state-of-the-art system. We use a discriminatively trained model to identify and delete incorrect links, and demonstrate that these gains in alignment quality lead to gains in translation quality in a stateof-the-art syntax-based MT system. In contrast to the semi-supervised LEAF alignment algorithm of (Fraser and Marcu, 2007b), which requires 1,5002,000 CPU days per it"
W08-0306,P06-1077,0,0.035095,"ntersection or refined alignSteven Abney Dept. of Linguistics University of Michigan Ann Arbor, MI 48104 abney@umich.edu ments have high precision but low recall.1 There are two natural approaches to improving upon GIZA++ alignments, then: deleting links from union alignments, or adding links to intersection or refined alignments. In this work, we delete links from GIZA++ union alignments to improve precision. The low precision of GIZA++ union alignments poses a particular problem for syntax-based rule extraction algorithms such as (Quirk et al., 2005; Galley et al., 2006; Huang et al., 2006; Liu et al., 2006): if the incorrect links violate syntactic correspondences, they force the rule extraction algorithm to extract rules that are large in size, few in number, and poor in generalization ability. Figure 1 illustrates this problem: the dotted line represents an incorrect link in the GIZA++ union alignment. Using the rule extraction algorithm described in (Galley et al., 2004), we extract the rules shown in the leftmost column (R1–R4). Rule R1 is large and unlikely to generalize well. If we delete the incorrect link in Figure 1, we can extract the rules shown in the rightmost column (R2–R9): Rule R"
W08-0306,D07-1038,1,0.803104,"atively trained model to identify and delete incorrect links, and demonstrate that these gains in alignment quality lead to gains in translation quality in a stateof-the-art syntax-based MT system. In contrast to the semi-supervised LEAF alignment algorithm of (Fraser and Marcu, 2007b), which requires 1,5002,000 CPU days per iteration to align 8.4M ChineseEnglish sentences (anonymous, p.c.), link deletion requires only 450 CPU hours to re-align such a corpus (after initial alignment by GIZA++, which requires 20-24 CPU days). Several recent works incorporate syntactic features into alignment. (May and Knight, 2007) use syntactic constraints to re-align a parallel corpus that has been aligned by GIZA++ as follows: they extract string-to-tree transducer rules from the corpus, the target parse trees, and the alignment; discard the initial alignment; use the extracted rules to construct a forest of possible string-to-tree derivations for each string/tree pair in the corpus; use EM to select the Viterbi derivation tree for each pair; and finally, induce a new alignment from the Viterbi derivations, using the re-aligned corpus to train a syntax-based MT system. (May and Knight, 2007) differs from our approach"
W08-0306,H05-1011,0,0.163314,"s (excluding punctuation marks) include {a,in,to,of,and,the}, while the 10 most common Chinese words include { , , , , }. Of these, {a,the} and { , } have no explicit translational equivalent in the other language. These words are aligned with each other frequently (and erroneously) by GIZA++ union, but rarely in the gold standard. We delete all links in the set {a, an, the} × { , } from Ainitial as a preprocessing step.7 ê4óZ{ ê{ {ê 2.4 Perceptron Training We set the feature weights λ using a modified version of averaged perceptron learning with structured outputs (Collins, 2002). Following (Moore, 2005), we initialize the value of our expected most informative feature (ruleCount) to 1.0, and initialize all other feature weights to 0. During each pass over the discriminative training set, we “decode” each sentence pair by greedily deleting links from Ainitial in order to maximize the score of the resulting alignment using the current settings of λ (for details, refer to section 2.1). 5 On a 400-sentence-pair Chinese-English data set, GIZA++ union alignments have a precision of 77.32 while GIZA++ refined alignments have a precision of 85.26. 6 To see how GIZA++ refined alignments compare to GI"
W08-0306,P06-1065,0,0.0369121,"dditional rules whose left hand sides are rooted at the dotted boxed nodes in the parse tree (R5, R6, R7, R8). 45 deletion improves alignment quality and translation quality in Chinese-English and Arabic-English MT, relative to a strong baseline. Our link deletion algorithm is easy to implement, runs quickly, and has been used by a top-scoring MT system in the Chinese newswire track of the 2008 NIST evaluation. 1.2 Related Work Recently, discriminative methods for alignment have rivaled the quality of IBM Model 4 alignments (Liu et al., 2005; Ittycheriah and Roukos, 2005; Taskar et al., 2005; Moore et al., 2006; Fraser and Marcu, 2007b). However, except for (Fraser and Marcu, 2007b), none of these advances in alignment quality has improved translation quality of a state-of-the-art system. We use a discriminatively trained model to identify and delete incorrect links, and demonstrate that these gains in alignment quality lead to gains in translation quality in a stateof-the-art syntax-based MT system. In contrast to the semi-supervised LEAF alignment algorithm of (Fraser and Marcu, 2007b), which requires 1,5002,000 CPU days per iteration to align 8.4M ChineseEnglish sentences (anonymous, p.c.), link"
W08-0306,P03-1021,0,0.0179242,"Missing"
W08-0306,P02-1040,0,0.10346,"Missing"
W08-0306,P05-1034,0,0.0229964,"union alignments have high recall but low precision, while intersection or refined alignSteven Abney Dept. of Linguistics University of Michigan Ann Arbor, MI 48104 abney@umich.edu ments have high precision but low recall.1 There are two natural approaches to improving upon GIZA++ alignments, then: deleting links from union alignments, or adding links to intersection or refined alignments. In this work, we delete links from GIZA++ union alignments to improve precision. The low precision of GIZA++ union alignments poses a particular problem for syntax-based rule extraction algorithms such as (Quirk et al., 2005; Galley et al., 2006; Huang et al., 2006; Liu et al., 2006): if the incorrect links violate syntactic correspondences, they force the rule extraction algorithm to extract rules that are large in size, few in number, and poor in generalization ability. Figure 1 illustrates this problem: the dotted line represents an incorrect link in the GIZA++ union alignment. Using the rule extraction algorithm described in (Galley et al., 2004), we extract the rules shown in the leftmost column (R1–R4). Rule R1 is large and unlikely to generalize well. If we delete the incorrect link in Figure 1, we can ext"
W08-0306,H05-1010,0,0.131499,"d the extraction of additional rules whose left hand sides are rooted at the dotted boxed nodes in the parse tree (R5, R6, R7, R8). 45 deletion improves alignment quality and translation quality in Chinese-English and Arabic-English MT, relative to a strong baseline. Our link deletion algorithm is easy to implement, runs quickly, and has been used by a top-scoring MT system in the Chinese newswire track of the 2008 NIST evaluation. 1.2 Related Work Recently, discriminative methods for alignment have rivaled the quality of IBM Model 4 alignments (Liu et al., 2005; Ittycheriah and Roukos, 2005; Taskar et al., 2005; Moore et al., 2006; Fraser and Marcu, 2007b). However, except for (Fraser and Marcu, 2007b), none of these advances in alignment quality has improved translation quality of a state-of-the-art system. We use a discriminatively trained model to identify and delete incorrect links, and demonstrate that these gains in alignment quality lead to gains in translation quality in a stateof-the-art syntax-based MT system. In contrast to the semi-supervised LEAF alignment algorithm of (Fraser and Marcu, 2007b), which requires 1,5002,000 CPU days per iteration to align 8.4M ChineseEnglish sentences (ano"
W08-0306,C96-2141,0,0.234582,"Missing"
W08-0306,J93-2003,0,\N,Missing
W08-0306,W02-1001,0,\N,Missing
W08-0306,J07-3002,0,\N,Missing
W08-0306,H05-1022,0,\N,Missing
W08-0306,J04-4002,0,\N,Missing
W08-0306,P05-1022,0,\N,Missing
W08-0306,W05-0812,0,\N,Missing
W08-0306,P06-2014,0,\N,Missing
W08-0306,J03-1002,0,\N,Missing
W08-0306,J07-2003,0,\N,Missing
W09-1804,J93-2003,0,0.194849,"or example, that sprok corresponds to dat in the first sentence pair. Word alignment has several downstream consumers. One is machine translation, where programs extract translation rules from word-aligned corpora (Och and Ney, 2004; Galley et al., 2004; Chiang, 2007; Quirk et al., 2005). Other downstream processes exploit dictionaries derived by alignment, in order to translate queries in crosslingual IR (Sch¨onhofen et al., 2008) or re-score candidate translation outputs (Och et al., 2004). Many methods of automatic alignment have been proposed. Probabilistic generative models like IBM 1-5 (Brown et al., 1993), HMM (Vogel et al., 1996), ITG (Wu, 1997), and LEAF (Fraser and Marcu, 2007) define formulas for P(f |e) or P(e, f), with lalok sprok izok jok stok wat dat krat quat cat lalok farok ororok lalok sprok izok enemok wat jjat bichat wat dat vat eneat lalok brok anok plok nok iat lat pippat rrat nnat wiwok nok izok kantok ok-yurp totat nnat quat oloat at-yurp lalok mok nok yorok ghirok clok wat nnat gat mat bat hilat lalok nok crrrok hihok yorok zanzanok wat nnat arrat mat zanzanat lalok rarok nok izok hihok mok wat nnat forat arrat vat gat Figure 1: Word alignment exercise (Knight, 1997). 28 Proc"
W09-1804,J07-2003,0,0.0397522,"rat dat ok-voon anok drok brok jok at-voon krat pippat sat lat wiwok farok izok stok totat jjat quat cat 1 Introduction Word alignment is the problem of annotating a bilingual text with links connecting words that have the same meanings. Figure 1 shows sample input for a word aligner (Knight, 1997). After analyzing the text, we may conclude, for example, that sprok corresponds to dat in the first sentence pair. Word alignment has several downstream consumers. One is machine translation, where programs extract translation rules from word-aligned corpora (Och and Ney, 2004; Galley et al., 2004; Chiang, 2007; Quirk et al., 2005). Other downstream processes exploit dictionaries derived by alignment, in order to translate queries in crosslingual IR (Sch¨onhofen et al., 2008) or re-score candidate translation outputs (Och et al., 2004). Many methods of automatic alignment have been proposed. Probabilistic generative models like IBM 1-5 (Brown et al., 1993), HMM (Vogel et al., 1996), ITG (Wu, 1997), and LEAF (Fraser and Marcu, 2007) define formulas for P(f |e) or P(e, f), with lalok sprok izok jok stok wat dat krat quat cat lalok farok ororok lalok sprok izok enemok wat jjat bichat wat dat vat eneat"
W09-1804,P08-2007,0,0.0555579,"2006) can be mapped to a quadratic assignment problem and solved with linear programming tools. In that work, linear programming is not only used for alignment, but also for training weights for the discriminative model. These weights are trained on a manually-aligned subset of the parallel data. One important “mega” feature for the discriminative model is the score assigned by an IBM model, which must be separately trained on the full parallel data. Our work differs in two ways: (1) our training is unsupervised, requiring no manually aligned data, and (2) we do not bootstrap off IBM models. (DeNero and Klein, 2008) gives an integer linear programming formulation of another alignment model based on phrases. There, integer programming is used only for alignment, not for learning parameter values. 5 Conclusions and Future Work Our search for an optimal IP solution is not fast. It takes 1-5 hours to perform sub-word alignment on the Turkish-English corpus. Of course, if we wanted to obtain optimal alignments under IBM Model 4, that would also be expensive, in fact NP-complete (Raghavendra and Maji, 2006). Practical Model 4 34 We have presented a novel objective function for alignment, and we have applied it"
W09-1804,P06-1097,0,0.0170631,"re 1: Word alignment exercise (Knight, 1997). 28 Proceedings of the NAACL HLT Workshop on Integer Linear Programming for Natural Language Processing, pages 28–35, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics hidden alignment variables. EM algorithms estimate dictionary and other probabilities in order to maximize those quantities. One can then ask for Viterbi alignments that maximize P(alignment |e, f). Discriminative models, e.g. (Taskar et al., 2005), instead set parameters to maximize alignment accuracy against a hand-aligned development set. EMD training (Fraser and Marcu, 2006) combines generative and discriminative elements. Low accuracy is a weakness for all systems. Most practitioners still use 1990s algorithms to align their data. It stands to reason that we have not yet seen the last word in alignment models. In this paper, we develop a new objective function for alignment, inspired by watching people manually solve the alignment exercise of Figure 1. When people attack this problem, we find that once they create a bilingual dictionary entry, they like to reuse that entry as much as possible. Previous machine aligners emulate this to some degree, but they are n"
W09-1804,D07-1006,0,0.0284779,"d alignment has several downstream consumers. One is machine translation, where programs extract translation rules from word-aligned corpora (Och and Ney, 2004; Galley et al., 2004; Chiang, 2007; Quirk et al., 2005). Other downstream processes exploit dictionaries derived by alignment, in order to translate queries in crosslingual IR (Sch¨onhofen et al., 2008) or re-score candidate translation outputs (Och et al., 2004). Many methods of automatic alignment have been proposed. Probabilistic generative models like IBM 1-5 (Brown et al., 1993), HMM (Vogel et al., 1996), ITG (Wu, 1997), and LEAF (Fraser and Marcu, 2007) define formulas for P(f |e) or P(e, f), with lalok sprok izok jok stok wat dat krat quat cat lalok farok ororok lalok sprok izok enemok wat jjat bichat wat dat vat eneat lalok brok anok plok nok iat lat pippat rrat nnat wiwok nok izok kantok ok-yurp totat nnat quat oloat at-yurp lalok mok nok yorok ghirok clok wat nnat gat mat bat hilat lalok nok crrrok hihok yorok zanzanok wat nnat arrat mat zanzanat lalok rarok nok izok hihok mok wat nnat forat arrat vat gat Figure 1: Word alignment exercise (Knight, 1997). 28 Proceedings of the NAACL HLT Workshop on Integer Linear Programming for Natural L"
W09-1804,N04-1035,1,0.765488,"ubel at-voon pippat rrat dat ok-voon anok drok brok jok at-voon krat pippat sat lat wiwok farok izok stok totat jjat quat cat 1 Introduction Word alignment is the problem of annotating a bilingual text with links connecting words that have the same meanings. Figure 1 shows sample input for a word aligner (Knight, 1997). After analyzing the text, we may conclude, for example, that sprok corresponds to dat in the first sentence pair. Word alignment has several downstream consumers. One is machine translation, where programs extract translation rules from word-aligned corpora (Och and Ney, 2004; Galley et al., 2004; Chiang, 2007; Quirk et al., 2005). Other downstream processes exploit dictionaries derived by alignment, in order to translate queries in crosslingual IR (Sch¨onhofen et al., 2008) or re-score candidate translation outputs (Och et al., 2004). Many methods of automatic alignment have been proposed. Probabilistic generative models like IBM 1-5 (Brown et al., 1993), HMM (Vogel et al., 1996), ITG (Wu, 1997), and LEAF (Fraser and Marcu, 2007) define formulas for P(f |e) or P(e, f), with lalok sprok izok jok stok wat dat krat quat cat lalok farok ororok lalok sprok izok enemok wat jjat bichat wat"
W09-1804,J01-2001,0,0.0231395,"s the multi-morpheme structure inside Turkish words. Consider the tiny Turkish-English corpus in Figure 5. Even a non-Turkish speaker might plausibly align yurur to walk, um to I, and ler to they. However, none of the popular machine aligners is able to do this, since they align at the wholeword level. Designers of translation systems sometimes employ language-specific word breakers before alignment, though these are hard to build and maintain, and they are usually not only languagespecific, but also language-pair-specific. Good unsupervised monolingual morpheme segmenters are also available (Goldsmith, 2001; Creutz and Lagus, 2005), though again, these do not do joint inference of alignment and word segmentation. We extend our objective function straightforwardly to sub-word alignment. To test our extension, we construct a Turkish-English corpus of 1616 sentence pairs. We first manually construct a regular tree grammar (RTG) (Gecseg and Steinby, 1984) for a fragment of English. This grammar produces English trees; it has 86 rules, 26 states, and 53 terminals (English words). We then construct a tree-tostring transducer (Rounds, 1970) that converts English trees into Turkish character strings, in"
W09-1804,N06-1015,0,0.0680526,"tantial search approximations (Brown et al., 1993). 4 Related Work (Zhang et al., 2003) and (Wu, 1997) tackle the problem of segmenting Chinese while aligning it to English. (Snyder and Barzilay, 2008) use multilingual data to compute segmentations of Arabic, Hebrew, Aramaic, and English. Their method uses IBM models to bootstrap alignments, and they measure the resulting segmentation accuracy. (Taskar et al., 2005) cast their alignment model as a minimum cost quadratic flow problem, for which optimal alignments can be computed with off-theshelf optimizers. Alignment in the modified model of (Lacoste-Julien et al., 2006) can be mapped to a quadratic assignment problem and solved with linear programming tools. In that work, linear programming is not only used for alignment, but also for training weights for the discriminative model. These weights are trained on a manually-aligned subset of the parallel data. One important “mega” feature for the discriminative model is the score assigned by an IBM model, which must be separately trained on the full parallel data. Our work differs in two ways: (1) our training is unsupervised, requiring no manually aligned data, and (2) we do not bootstrap off IBM models. (DeNer"
W09-1804,P97-1063,0,0.0295588,"ies, including garcia/garcia, are/son, are/estan, not/no, has/tiene, etc. shows the gold alignment for the corpus in Figure 1 (displayed here as English-Spanish), which results in 28 distinct bilingual dictionary entries. By contrast, a monotone alignment induces 39 distinct entries, due to less re-use. Next we look at how to automatically rifle through all legal alignments to find the one with the best score. What is a legal alignment? For now, we consider it to be one where: • Every foreign word is aligned exactly once (Brown et al., 1993). • Every English word has either 0 or 1 alignments (Melamed, 1997). We formulate our integer program (IP) as follows. We set up two types of binary variables: • Alignment link variables. If link-i-j-k = 1, that means in sentence pair i, the foreign word at position j aligns to the English words at position k. • Bilingual dictionary variables. If dict-f-e = 1, that means word pair (f, e) is “in” the dictionary. We constrain the values of link variables to satisfy the two alignment conditions listed earlier. We also require that if link-i-j-k = 1 (i.e., we’ve decided on an alignment link), then dict-fij -eik should also equal 1 (the linked words are recorded a"
W09-1804,J04-4002,0,0.0728219,"ok plok sprok at-drubel at-voon pippat rrat dat ok-voon anok drok brok jok at-voon krat pippat sat lat wiwok farok izok stok totat jjat quat cat 1 Introduction Word alignment is the problem of annotating a bilingual text with links connecting words that have the same meanings. Figure 1 shows sample input for a word aligner (Knight, 1997). After analyzing the text, we may conclude, for example, that sprok corresponds to dat in the first sentence pair. Word alignment has several downstream consumers. One is machine translation, where programs extract translation rules from word-aligned corpora (Och and Ney, 2004; Galley et al., 2004; Chiang, 2007; Quirk et al., 2005). Other downstream processes exploit dictionaries derived by alignment, in order to translate queries in crosslingual IR (Sch¨onhofen et al., 2008) or re-score candidate translation outputs (Och et al., 2004). Many methods of automatic alignment have been proposed. Probabilistic generative models like IBM 1-5 (Brown et al., 1993), HMM (Vogel et al., 1996), ITG (Wu, 1997), and LEAF (Fraser and Marcu, 2007) define formulas for P(f |e) or P(e, f), with lalok sprok izok jok stok wat dat krat quat cat lalok farok ororok lalok sprok izok enemok"
W09-1804,N04-1021,0,0.0207175,"ame meanings. Figure 1 shows sample input for a word aligner (Knight, 1997). After analyzing the text, we may conclude, for example, that sprok corresponds to dat in the first sentence pair. Word alignment has several downstream consumers. One is machine translation, where programs extract translation rules from word-aligned corpora (Och and Ney, 2004; Galley et al., 2004; Chiang, 2007; Quirk et al., 2005). Other downstream processes exploit dictionaries derived by alignment, in order to translate queries in crosslingual IR (Sch¨onhofen et al., 2008) or re-score candidate translation outputs (Och et al., 2004). Many methods of automatic alignment have been proposed. Probabilistic generative models like IBM 1-5 (Brown et al., 1993), HMM (Vogel et al., 1996), ITG (Wu, 1997), and LEAF (Fraser and Marcu, 2007) define formulas for P(f |e) or P(e, f), with lalok sprok izok jok stok wat dat krat quat cat lalok farok ororok lalok sprok izok enemok wat jjat bichat wat dat vat eneat lalok brok anok plok nok iat lat pippat rrat nnat wiwok nok izok kantok ok-yurp totat nnat quat oloat at-yurp lalok mok nok yorok ghirok clok wat nnat gat mat bat hilat lalok nok crrrok hihok yorok zanzanok wat nnat arrat mat zan"
W09-1804,P05-1034,0,0.0277886,"n anok drok brok jok at-voon krat pippat sat lat wiwok farok izok stok totat jjat quat cat 1 Introduction Word alignment is the problem of annotating a bilingual text with links connecting words that have the same meanings. Figure 1 shows sample input for a word aligner (Knight, 1997). After analyzing the text, we may conclude, for example, that sprok corresponds to dat in the first sentence pair. Word alignment has several downstream consumers. One is machine translation, where programs extract translation rules from word-aligned corpora (Och and Ney, 2004; Galley et al., 2004; Chiang, 2007; Quirk et al., 2005). Other downstream processes exploit dictionaries derived by alignment, in order to translate queries in crosslingual IR (Sch¨onhofen et al., 2008) or re-score candidate translation outputs (Och et al., 2004). Many methods of automatic alignment have been proposed. Probabilistic generative models like IBM 1-5 (Brown et al., 1993), HMM (Vogel et al., 1996), ITG (Wu, 1997), and LEAF (Fraser and Marcu, 2007) define formulas for P(f |e) or P(e, f), with lalok sprok izok jok stok wat dat krat quat cat lalok farok ororok lalok sprok izok enemok wat jjat bichat wat dat vat eneat lalok brok anok plok"
W09-1804,E06-1004,0,0.0748656,"r training is unsupervised, requiring no manually aligned data, and (2) we do not bootstrap off IBM models. (DeNero and Klein, 2008) gives an integer linear programming formulation of another alignment model based on phrases. There, integer programming is used only for alignment, not for learning parameter values. 5 Conclusions and Future Work Our search for an optimal IP solution is not fast. It takes 1-5 hours to perform sub-word alignment on the Turkish-English corpus. Of course, if we wanted to obtain optimal alignments under IBM Model 4, that would also be expensive, in fact NP-complete (Raghavendra and Maji, 2006). Practical Model 4 34 We have presented a novel objective function for alignment, and we have applied it to whole-word and sub-word alignment problems. Preliminary results look good, especially given that new objective function is simpler than those previously proposed. The integer programming framework makes the model easy to implement, and its optimal behavior frees us from worrying about search errors. We believe there are good future possibilities for this work: • Extend legal alignments to cover n-to-m and discontinuous cases. While morphemeto-morpheme alignment is more frequently a • •"
W09-1804,C69-0101,0,0.681853,"pervised monolingual morpheme segmenters are also available (Goldsmith, 2001; Creutz and Lagus, 2005), though again, these do not do joint inference of alignment and word segmentation. We extend our objective function straightforwardly to sub-word alignment. To test our extension, we construct a Turkish-English corpus of 1616 sentence pairs. We first manually construct a regular tree grammar (RTG) (Gecseg and Steinby, 1984) for a fragment of English. This grammar produces English trees; it has 86 rules, 26 states, and 53 terminals (English words). We then construct a tree-tostring transducer (Rounds, 1970) that converts English trees into Turkish character strings, including space. Because it does not explicitly enumerate the Turkish vocabulary, this transducer can output a very large number of distinct Turkish words (i.e., character sequences preceded and followed by space). This transducer has 177 rules, 18 states, and 23 terminals (Turkish characters). RTG generation produces English trees that the transducer converts to Turkish, both via the tree automata toolkit Tiburon (May and Knight, 2006). From this, we obtain a parallel Turkish-English corpus. A fragment of the corpus is shown in Figu"
W09-1804,P08-1084,0,0.101809,"stands to reason that we have not yet seen the last word in alignment models. In this paper, we develop a new objective function for alignment, inspired by watching people manually solve the alignment exercise of Figure 1. When people attack this problem, we find that once they create a bilingual dictionary entry, they like to reuse that entry as much as possible. Previous machine aligners emulate this to some degree, but they are not explicitly programmed to do so. We also address another weakness of current aligners: they only align full words. With few exceptions, e.g. (Zhang et al., 2003; Snyder and Barzilay, 2008), aligners do not operate at the sub-word level, making them much less useful for agglutinative languages such as Turkish. Our present contributions are as follows: • We offer a simple new objective function that scores a corpus alignment based on how many distinct bilingual word pairs it contains. • We use an integer programming solver to carry out optimization and corpus alignment. • We extend the system to perform subword alignment, which we demonstrate on a Turkish-English corpus. The results in this paper constitute a proof of concept of these ideas, executed on small corpora. We conclude"
W09-1804,H05-1010,0,0.497013,"lalok nok crrrok hihok yorok zanzanok wat nnat arrat mat zanzanat lalok rarok nok izok hihok mok wat nnat forat arrat vat gat Figure 1: Word alignment exercise (Knight, 1997). 28 Proceedings of the NAACL HLT Workshop on Integer Linear Programming for Natural Language Processing, pages 28–35, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics hidden alignment variables. EM algorithms estimate dictionary and other probabilities in order to maximize those quantities. One can then ask for Viterbi alignments that maximize P(alignment |e, f). Discriminative models, e.g. (Taskar et al., 2005), instead set parameters to maximize alignment accuracy against a hand-aligned development set. EMD training (Fraser and Marcu, 2006) combines generative and discriminative elements. Low accuracy is a weakness for all systems. Most practitioners still use 1990s algorithms to align their data. It stands to reason that we have not yet seen the last word in alignment models. In this paper, we develop a new objective function for alignment, inspired by watching people manually solve the alignment exercise of Figure 1. When people attack this problem, we find that once they create a bilingual dicti"
W09-1804,C96-2141,0,0.570617,"responds to dat in the first sentence pair. Word alignment has several downstream consumers. One is machine translation, where programs extract translation rules from word-aligned corpora (Och and Ney, 2004; Galley et al., 2004; Chiang, 2007; Quirk et al., 2005). Other downstream processes exploit dictionaries derived by alignment, in order to translate queries in crosslingual IR (Sch¨onhofen et al., 2008) or re-score candidate translation outputs (Och et al., 2004). Many methods of automatic alignment have been proposed. Probabilistic generative models like IBM 1-5 (Brown et al., 1993), HMM (Vogel et al., 1996), ITG (Wu, 1997), and LEAF (Fraser and Marcu, 2007) define formulas for P(f |e) or P(e, f), with lalok sprok izok jok stok wat dat krat quat cat lalok farok ororok lalok sprok izok enemok wat jjat bichat wat dat vat eneat lalok brok anok plok nok iat lat pippat rrat nnat wiwok nok izok kantok ok-yurp totat nnat quat oloat at-yurp lalok mok nok yorok ghirok clok wat nnat gat mat bat hilat lalok nok crrrok hihok yorok zanzanok wat nnat arrat mat zanzanat lalok rarok nok izok hihok mok wat nnat forat arrat vat gat Figure 1: Word alignment exercise (Knight, 1997). 28 Proceedings of the NAACL HLT W"
W09-1804,J97-3002,0,0.615325,"st sentence pair. Word alignment has several downstream consumers. One is machine translation, where programs extract translation rules from word-aligned corpora (Och and Ney, 2004; Galley et al., 2004; Chiang, 2007; Quirk et al., 2005). Other downstream processes exploit dictionaries derived by alignment, in order to translate queries in crosslingual IR (Sch¨onhofen et al., 2008) or re-score candidate translation outputs (Och et al., 2004). Many methods of automatic alignment have been proposed. Probabilistic generative models like IBM 1-5 (Brown et al., 1993), HMM (Vogel et al., 1996), ITG (Wu, 1997), and LEAF (Fraser and Marcu, 2007) define formulas for P(f |e) or P(e, f), with lalok sprok izok jok stok wat dat krat quat cat lalok farok ororok lalok sprok izok enemok wat jjat bichat wat dat vat eneat lalok brok anok plok nok iat lat pippat rrat nnat wiwok nok izok kantok ok-yurp totat nnat quat oloat at-yurp lalok mok nok yorok ghirok clok wat nnat gat mat bat hilat lalok nok crrrok hihok yorok zanzanok wat nnat arrat mat zanzanat lalok rarok nok izok hihok mok wat nnat forat arrat vat gat Figure 1: Word alignment exercise (Knight, 1997). 28 Proceedings of the NAACL HLT Workshop on Integ"
W10-2502,N10-1130,0,0.0458281,"Missing"
W10-2502,D09-1076,1,0.830381,"of MT of natural languages (Yamada and Knight, 2001; Knight and Graehl, 2005; Graehl et al., 2008; Knight and May 2009). Martin and Vere (1970) and Schreiber (1975) established the first connections between the two traditions; also Shieber (2004, 2006) and Maletti (2008, 2010) investigated their relationship. The problem addressed in this paper is the decoding of source-language strings into targetlanguage trees where the transformation is described by a pSTIG. Currently, this decoding requires two steps: first, every source string is translated into a derivation tree of the underlying pSTIG (DeNeefe, 2009; DeNeefe and Knight 2009), and second, the derivation tree is transformed into the target tree using an embedded tree transducer (Shieber, 2006). We propose a transducer model, called a bottom-up tree adjoining transducer, which performs this decoding in a single step and, simultaneously, computes the probabilities of its derivations. As a basis of our approach, we present a formal definition of pSTIG. Synchronous tree insertion grammars (STIG) are formal models for syntaxbased machine translation. We formalize a decoder for probabilistic STIG; the decoder transforms every source-language str"
W10-2502,2006.amta-papers.15,0,0.404095,"Missing"
W10-2502,W04-3312,0,0.311568,"transducers (G´ecseg and Steinby, 1984, 1997). Roughly speaking, a tree transducer is a finite term rewriting system. If each rewrite rule carries a probablity or, in general, a weight from some semiring, then they are weighted tree transducers (Maletti, 2006, 2006a; F¨ul¨op and Vogler, 2009). Such weighted tree transducers have also been used for the specification of MT of natural languages (Yamada and Knight, 2001; Knight and Graehl, 2005; Graehl et al., 2008; Knight and May 2009). Martin and Vere (1970) and Schreiber (1975) established the first connections between the two traditions; also Shieber (2004, 2006) and Maletti (2008, 2010) investigated their relationship. The problem addressed in this paper is the decoding of source-language strings into targetlanguage trees where the transformation is described by a pSTIG. Currently, this decoding requires two steps: first, every source string is translated into a derivation tree of the underlying pSTIG (DeNeefe, 2009; DeNeefe and Knight 2009), and second, the derivation tree is transformed into the target tree using an embedded tree transducer (Shieber, 2006). We propose a transducer model, called a bottom-up tree adjoining transducer, which pe"
W10-2502,E06-1048,0,0.689816,"and Schreiber (1975) established the first connections between the two traditions; also Shieber (2004, 2006) and Maletti (2008, 2010) investigated their relationship. The problem addressed in this paper is the decoding of source-language strings into targetlanguage trees where the transformation is described by a pSTIG. Currently, this decoding requires two steps: first, every source string is translated into a derivation tree of the underlying pSTIG (DeNeefe, 2009; DeNeefe and Knight 2009), and second, the derivation tree is transformed into the target tree using an embedded tree transducer (Shieber, 2006). We propose a transducer model, called a bottom-up tree adjoining transducer, which performs this decoding in a single step and, simultaneously, computes the probabilities of its derivations. As a basis of our approach, we present a formal definition of pSTIG. Synchronous tree insertion grammars (STIG) are formal models for syntaxbased machine translation. We formalize a decoder for probabilistic STIG; the decoder transforms every source-language string into a target-language tree and calculates the probability of this transformation. 1 Introduction Tree adjoining grammars (TAG) were invented"
W10-2502,C90-3045,0,0.799853,"Missing"
W10-2502,P01-1067,1,0.517547,"y, CA 90292, USA {sdeneefe,knight}@isi.edu Heiko Vogler † Department of Computer Science Technische Universit¨at Dresden D-01062 Dresden Heiko.Vogler@tu-dresden.de Abstract led to the rich theory of tree transducers (G´ecseg and Steinby, 1984, 1997). Roughly speaking, a tree transducer is a finite term rewriting system. If each rewrite rule carries a probablity or, in general, a weight from some semiring, then they are weighted tree transducers (Maletti, 2006, 2006a; F¨ul¨op and Vogler, 2009). Such weighted tree transducers have also been used for the specification of MT of natural languages (Yamada and Knight, 2001; Knight and Graehl, 2005; Graehl et al., 2008; Knight and May 2009). Martin and Vere (1970) and Schreiber (1975) established the first connections between the two traditions; also Shieber (2004, 2006) and Maletti (2008, 2010) investigated their relationship. The problem addressed in this paper is the decoding of source-language strings into targetlanguage trees where the transformation is described by a pSTIG. Currently, this decoding requires two steps: first, every source string is translated into a derivation tree of the underlying pSTIG (DeNeefe, 2009; DeNeefe and Knight 2009), and second"
W10-2502,J95-4002,0,\N,Missing
W10-2502,C90-3001,0,\N,Missing
W10-2502,W90-0102,0,\N,Missing
W10-2502,J08-3004,1,\N,Missing
W11-1202,P06-2065,1,0.745878,"Missing"
W11-1511,J01-2001,0,0.0188226,"est that words are made up of morpheme-like chunks. Several hypotheses about VMS word structure have been proposed. Tiltman (1967) proposed a template consisting of roots and suffixes. Stolfi (2005) breaks down the morphology into ‘prefix-midfix-suffix’, where the letters in the midfixes are more or less disjoint from the letters in the suffixes and prefixes. Stolfi later modified this to a ‘core-mantel-crust’ model, where words are composed of three nested layers. To determine whether VMS words have affixal morphology, we run an unsupervised morphological segmentation algorithm, Linguistica (Goldsmith, 2001), on the VMS text. The MDL-based algorithm segments words into prefix+stem+suffix, and extracts ‘signatures’, sets of affixes that attach to the same set of stems. Table 3 lists a few sample signatures, showing that stems in the same signature tend to have some structural similarities. Table 3: Some morphological signatures. Affixes OE+, OP+, null+ OE+ +89, +9, + C89 5 5.1 Stems A3 AD AE AE9 AEOR AJ AM AN AR AT E O O2 OE OJ OM ON OR SAJ SAR SCC9 SCCO SCO2 SO BSC28 BSC9 CCC8 COC8CR FAEOE FAK FAU FC8 FC8AM FCC FCC2 FCC9R FCCAE FCCC2 FCCCAR9 FCO9 FCS9 FCZAR FCZC9 OEAR9 OESC9 OF9 OR8 SC29 SC89O SC"
W11-1511,P06-2065,1,0.580189,"), and 18791 words from the Chinese Sinica Treebank. 3 3.1 The Letter Are vowels and consonants represented? If a script is alphabetic, i.e., it uses approximately one character per phoneme, vowel and consonant characters can be separated in a fully unsupervised way. Guy (1991) applies the vowel-consonant separation algorithm of (Sukhotin, 1962) on two pages of the Biological section, and finds that four characters (O, A, C, G) separate out as vowels. However, the separation is not very strong, and several words do not contain these characters. Another method is to use a two-state bigram HMM (Knight et al., 2006; Goldsmith and Xanthos, 2009) over letters, and induce two clusters of letters with EM. In alphabetic languages like English, the clusters correspond almost perfectly to vowels and consonants. We find that a curious phenomenon occurs with the VMS – the last character of every word is generated by one of the HMM states, and all other characters by another; i.e., the word grammar is a∗ b. There are a few possible interpretations of this. It is possible that the vowels from every word are removed and placed at the end of the word, but this means that even long words have only one vowel, which is"
W12-4209,P03-2041,0,0.247061,"Missing"
W12-4209,J99-4004,0,0.140126,"tem has only ancestors from Q, apply a matching rule from R to obtain a new item This algorithm is correct and complete and can be implemented in time O(2|G |) since there are exponentially many configurations. Moreover, the set of derivation dags is the result of this parser, and a finite dag acceptor representing the derivation dags can be constructed on the fly. It can be easily extended to check membership of (dag, tree) pairs in a dag-to-tree transducer and to generate all the trees that are obtained from a given dag (“forward application”). In order to compute weights, the techniques by Goodman (1999) can be used. 5.2 1-best and k-best generation The k-best algorithm finds the highest-weighted k derivations (not dags) in a given (weighted) dag acceptor. If no weights are available, other measures can be used (e.g. the number of derivation steps or symbol frequencies). We can implement the k-best algorithm (of which 1-best is a special case) by generating graphs and putting incomplete graphs on a priority queue sorted by weight. If rule weights are probabilities between 0 and 1, monotonicity ensures that the k-best graphs are found, as the weights of incomplete hypotheses never increase. WA"
W12-4209,J08-3004,1,0.879418,"Missing"
W12-4209,W05-1506,0,0.114688,"Missing"
W12-4209,N06-1045,1,0.954074,"grammar. 77 string automata . . . paths through a WFSA (Viterbi, 1967; Eppstein, 1998) k-best EM training Determinization Forward-backward EM (Baum et al., 1970; Eisner, 2003) . . . of weighted string acceptors (Mohri, 1997) Transducer composition WFST composition (Pereira and Riley, 1997) General tools AT&T FSM (Mohri et al., 2000), Carmel (Graehl, 1997), OpenFST (Riley et al., 2009) tree automata . . . trees in a weighted forest (Jim´enez and Marzal, 2000; Huang and Chiang, 2005) Tree transducer EM training (Graehl et al., 2008) . . . of weighted tree acceptors (Borchardt and Vogler, 2003; May and Knight, 2006a) Many transducers not closed under composition (Maletti et al., 2009) Tiburon (May and Knight, 2006b) graph automata ? ? ? ? ? Table 1: General-purpose algorithms for strings, trees and feature structures. them. Such automata would be of great use. For example, a weighted graph acceptor could form the basis of a semantic language model, and a weighted graph-to-tree transducer could form the basis of a natural language understanding (NLU) or generation (NLG) system, depending on which direction it is employed. Putting NLU and NLG together, we can also envision semantics-based MT systems (Figu"
W12-4209,J97-2003,0,0.316718,"Missing"
W12-4209,P89-1005,0,0.632998,"is some distance to be traveled. Table 1 gives a snapshot of some efficient, generic algorithms for string automata (mainly developed in the last century), plus algorithms for tree automata (mainly developed in the last ten years). These algorithms have been packaged in general-purpose software toolkits like AT&T FSM (Mohri et al., 2000), OpenFST (Riley et al., 2009), and Tiburon (May and Knight, 2006b). A research program for graphs should hold similar value. Formal graph manipulation has, fortunately, received prior attention. A unification grammar can specify semantic mappings for strings (Moore, 1989), effectively capturing an infinite set of string/graph pairs. But unification grammars seem too powerful to admit the efficient algorithms we 78 desire in Table 1, and weighted versions are not popular. Hyperedge replacement grammars (Drewes et al., 1997; Courcelle and Engelfriet, 1995) are another natural candidate for graph acceptors, and a synchronous hyperedge replacement grammar might serve as a graph transducer. Finally, Kamimura and Slutzki (1981, 1982) propose graph acceptor and graph-to-tree transducer formalisms for rooted directed acyclic graphs. Their model has been extended to mu"
W12-4209,N09-4005,0,0.321432,"nerate → etree → rank → estring Figure 2: Pipelines for syntax-based and for semantics-based MT. Devices: FSA = finite string automaton; ln-XTOPs = linear non-deleting extended top-down tree-to-string transducer; RTG = regular tree grammar. 77 string automata . . . paths through a WFSA (Viterbi, 1967; Eppstein, 1998) k-best EM training Determinization Forward-backward EM (Baum et al., 1970; Eisner, 2003) . . . of weighted string acceptors (Mohri, 1997) Transducer composition WFST composition (Pereira and Riley, 1997) General tools AT&T FSM (Mohri et al., 2000), Carmel (Graehl, 1997), OpenFST (Riley et al., 2009) tree automata . . . trees in a weighted forest (Jim´enez and Marzal, 2000; Huang and Chiang, 2005) Tree transducer EM training (Graehl et al., 2008) . . . of weighted tree acceptors (Borchardt and Vogler, 2003; May and Knight, 2006a) Many transducers not closed under composition (Maletti et al., 2009) Tiburon (May and Knight, 2006b) graph automata ? ? ? ? ? Table 1: General-purpose algorithms for strings, trees and feature structures. them. Such automata would be of great use. For example, a weighted graph acceptor could form the basis of a semantic language model, and a weighted graph-to-tre"
W12-6207,W90-0108,0,0.0585207,"o build dag acceptors 40 and dag-to-tree transducers similar to their model. Compared to those devices, in order to use them for actual NLP tasks, our machines differ in certain aspects: • We do not require our dags to be planar, and we do not only consider derivation dags. • We add weights from any commutative semiring, e.g. real numbers. The toolkit is available under an open source licence.1 2 Dags and dag acceptors DAGGER comes with a variety of example dags and automata. Let us briefly illustrate some of them. The dag of Fig. 1(a) can be defined in a human-readable format called P ENMAN (Bateman, 1990): (1 / WANT :agent (2 / BOY) :patient (3 / BELIEVE :agent 2 :patient (4 / GIRL))) 1 http://www.ims.uni-stuttgart.de/ daniel/dagger/ ˜ Proceedings of the 10th International Workshop on Finite State Methods and Natural Language Processing, pages 40–44, c Donostia–San Sebasti´an, July 23–25, 2012. 2012 Association for Computational Linguistics s s s i s i i s s i i i i s s s s s i s i s i s i i s e e -> (WANT :agent i :patient s) -> (BELIEVE :agent i :patient s) -> (0) -> (0) -> (GIRL) -> (BOY) -> (GIRL) -> (BOY) i -> (GIRL) i -> (BOY) s -> (GIRL) s -> (BOY) s -> (GIRL) s -> (BOY) -> (WANT :agent"
W12-6207,bohnet-wanner-2010-open,0,0.0215006,"tree automata have proved to be useful tools in various areas of natural language processing (Knight and May, 2009). However, some applications, especially in semantics, require graph structures, in particular directed acyclic graphs (dags), to model reentrancies. For instance, the dags in Fig. 1 represents the semantics of the sentences “The boy wants to believe the girl” and “The boy wants the girl to believe him.” The double role of “the boy” is made clear by the two parent edges of the BOY node, making this structure non-tree-like. Powerful graph rewriting systems have been used for NLP (Bohnet and Wanner, 2010), yet we consider a rather simple model: finite dag automata that have been introduced by (Kamimura and Slutzki, 1981; Kamimura and Slutzki, 1982) as a straightforward extension of tree automata. We present the toolkit DAGGER (written in P YTHON) that can be used to visualize dags and to build dag acceptors 40 and dag-to-tree transducers similar to their model. Compared to those devices, in order to use them for actual NLP tasks, our machines differ in certain aspects: • We do not require our dags to be planar, and we do not only consider derivation dags. • We add weights from any commutative"
W12-6207,P03-2041,0,0.0977658,"Missing"
W12-6207,J08-3004,1,0.893967,"Missing"
W12-6207,W05-1506,0,0.112281,"Missing"
W12-6207,N06-1045,1,0.898389,"Missing"
W12-6207,J97-2003,0,0.127956,"Missing"
W12-6207,W12-4209,1,0.795801,"ation and checking”, i.e. a process that generates dags and trees at the same time. Whenever a partial tree does not match the input tree, it is discarded, until we find a derivation and a dag for the input tree. If we also restrict the dag part, we have force decoding. 43 Future work This work describes the basics of a dag automata toolkit. To the authors’ knowledge, no such implementation already exists. Of course, many algorithms are missing, and there is a lot of room for improvement, both from the theoretical and the practical viewpoint. This is a brief list of items for future research (Quernheim and Knight, 2012): • Complexity analysis of the algorithms. • Closure properties of dag acceptors and dagto-tree transducers as well as composition with tree transducers. • Extended left-hand sides to condition on a larger semantic context, just like extended topdown tree transducers (Maletti et al., 2009). • Handling flat, unordered, sparse sets of relations that are typical of feature structures. Currently, rules are specific to the rank of the nodes. A first step in this direction could be gone by getting rid of the explicit n=m symbols. • Hand-annotated resources such as (dag, tree) pairs, similar to treeb"
W12-6207,N09-4005,0,0.0696411,"Missing"
W13-2322,P13-1091,1,0.350566,"uses VerbNet roles (Schuler, 2005), and AMR uses frame-specific PropBank relations. UNL has a dedicated set of over 30 frequently used relations. Formalism. GMB meanings are written in DRT (Kamp et al., 2011), exploiting full first4 8 Future Work Sembanking. Our main goal is to continue sembanking. We would like to employ a large sembank to create shared tasks for natural language understanding and generation. These tasks may additionally drive interest in theoretical frameworks for probabilistically mapping between graphs and strings (Quernheim and Knight, 2012b; Quernheim and Knight, 2012a; Chiang et al., 2013). Applications. Just as syntactic parsing has found many unanticipated applications, we expect sembanks and statistical semantic processors to be used for many purposes. To get started, we are exploring the use of statistical NLU and NLG in 5 amr.isi.edu/download.html 184 UNL guidelines: www.undl.org/unlsys/unl/unl2005 a semantics-based machine translation (MT) system. In this system, we annotate bilingual Chinese/English data with AMR, then train components to map Chinese to AMR, and AMR to English. A prototype is described by Jones et al. (2012). Disjunctive AMR. AMR aims to canonicalize mul"
W13-2322,N12-1017,0,0.0148132,"Missing"
W13-2322,kingsbury-palmer-2002-treebank,1,0.410787,"ive. We draw on this work to design an Abstract Meaning Representation (AMR) appropriate for sembanking. Our basic principles are: • AMRs are rooted, labeled graphs that are easy for people to read, and easy for programs to traverse. • AMR aims to abstract away from syntactic idiosyncrasies. We attempt to assign the same AMR to sentences that have the same basic meaning. For example, the sentences “he described her as a genius”, “his description of her: genius”, and “she was a genius, according to his description” are all assigned the same AMR. • AMR makes extensive use of PropBank framesets (Kingsbury and Palmer, 2002; Palmer et al., 2005). For example, we represent a phrase like “bond investor” using the frame “invest-01”, even though no verbs appear in the phrase. • AMR is agnostic about how we might want to derive meanings from strings, or viceversa. In translating sentences to AMR, we do not dictate a particular sequence of rule applications or provide alignments that reflect such rule sequences. This makes sembanking very fast, and it allows researchers to explore their own ideas about how strings We describe Abstract Meaning Representation (AMR), a semantic representation language in which we are wri"
W13-2322,martins-2012-le,0,0.0102149,"re able to translate a full sentence into AMR in 7-10 minutes and postedit an AMR in 1-3 minutes. 7 Related Work Researchers working on whole-sentence semantic parsing today typically use small, domain-specific sembanks like GeoQuery (Wong and Mooney, 2006). The need for larger, broad-coverage sembanks has sparked several projects, including the Groningen Meaning Bank (GMB) (Basile et al., 2012a), UCCA (Abend and Rappoport, 2013), the Semantic Treebank (ST) (Butler and Yoshimoto, 2012), the Prague Dependency Treebank (B¨ohmov´a et al., 2003), and UNL (Uchida et al., 1999; Uchida et al., 1996; Martins, 2012). Concepts. Most systems use English words as concepts. AMR uses PropBank frames (e.g., “describe-01”), and UNL uses English WordNet synsets (e.g., “200752493”). Relations. GMB uses VerbNet roles (Schuler, 2005), and AMR uses frame-specific PropBank relations. UNL has a dedicated set of over 30 frequently used relations. Formalism. GMB meanings are written in DRT (Kamp et al., 2011), exploiting full first4 8 Future Work Sembanking. Our main goal is to continue sembanking. We would like to employ a large sembank to create shared tasks for natural language understanding and generation. These tas"
W13-2322,W04-2705,0,0.0181197,"Missing"
W13-2322,W13-0101,0,0.0287813,"nces from CCTV broadcast conversation (*) Collections marked with a star (*) are also in the OntoNotes corpus (Pradhan et al., 2007; Weischedel et al., 2011). Using the AMR Editor, annotators are able to translate a full sentence into AMR in 7-10 minutes and postedit an AMR in 1-3 minutes. 7 Related Work Researchers working on whole-sentence semantic parsing today typically use small, domain-specific sembanks like GeoQuery (Wong and Mooney, 2006). The need for larger, broad-coverage sembanks has sparked several projects, including the Groningen Meaning Bank (GMB) (Basile et al., 2012a), UCCA (Abend and Rappoport, 2013), the Semantic Treebank (ST) (Butler and Yoshimoto, 2012), the Prague Dependency Treebank (B¨ohmov´a et al., 2003), and UNL (Uchida et al., 1999; Uchida et al., 1996; Martins, 2012). Concepts. Most systems use English words as concepts. AMR uses PropBank frames (e.g., “describe-01”), and UNL uses English WordNet synsets (e.g., “200752493”). Relations. GMB uses VerbNet roles (Schuler, 2005), and AMR uses frame-specific PropBank relations. UNL has a dedicated set of over 30 frequently used relations. Formalism. GMB meanings are written in DRT (Kamp et al., 2011), exploiting full first4 8 Future"
W13-2322,P98-1013,0,0.597537,"Missing"
W13-2322,basile-etal-2012-developing,0,0.035603,". AMR and UNL remain agnostic about the relation between strings and their meanings, considering this a topic of open research. ST and GMB annotate words and phrases directly, recording derivations as (for example) Montaguestyle compositional semantic rules operating on CCG parses. Top-down verus bottom-up. AMR annotators find it fast to construct meanings from the top down, starting with the main idea of the sentence (though the AMR Editor allows bottom-up construction). GMB and UCCA annotators work bottom-up. Editors, guidelines, genres. These projects have graphical sembanking tools (e.g., Basile et al. (2012b)), annotation guidelines,5 and sembanks that cover a wide range of genres, from news to fiction. UNL and AMR have both annotated many of the same sentences, providing the potential for direct comparison. We currently have a manually-constructed AMR bank of several thousand sentences, a subset of which can be freely downloaded,4 the rest being distributed via the LDC catalog. In initially developing AMR, the authors built consensus AMRs for: • 225 short sentences for tutorial purposes • 142 sentences of newswire (*) • 100 sentences of web data (*) Trained annotators at LDC then produced AMRs"
W13-2322,J05-1004,1,0.185231,"design an Abstract Meaning Representation (AMR) appropriate for sembanking. Our basic principles are: • AMRs are rooted, labeled graphs that are easy for people to read, and easy for programs to traverse. • AMR aims to abstract away from syntactic idiosyncrasies. We attempt to assign the same AMR to sentences that have the same basic meaning. For example, the sentences “he described her as a genius”, “his description of her: genius”, and “she was a genius, according to his description” are all assigned the same AMR. • AMR makes extensive use of PropBank framesets (Kingsbury and Palmer, 2002; Palmer et al., 2005). For example, we represent a phrase like “bond investor” using the frame “invest-01”, even though no verbs appear in the phrase. • AMR is agnostic about how we might want to derive meanings from strings, or viceversa. In translating sentences to AMR, we do not dictate a particular sequence of rule applications or provide alignments that reflect such rule sequences. This makes sembanking very fast, and it allows researchers to explore their own ideas about how strings We describe Abstract Meaning Representation (AMR), a semantic representation language in which we are writing down the meanings"
W13-2322,P02-1040,0,0.108678,"ted script.3 Smatch reports the semantic overlap between two AMRs by viewing each AMR as a conjunction of logical triples (see Figure 1). Smatch computes precision, recall, and F-score of one AMR’s triples against the other’s. To match up variables from two input AMRs, smatch needs to execute a brief search, looking for the variable mapping that yields the highest F-score. Smatch makes no reference to English strings or word indices, as we do not enforce any particular string-to-meaning derivation. Instead, we compare semantic representations directly, in the same way that the MT metric Bleu (Papineni et al., 2002) compares target strings without making reference to the source. For an initial IAA study, and prior to adjusting the AMR Editor to encourage consistency, 4 expert AMR annotators annotated 100 newswire sentences and 80 web text sentences. They then created consensus AMRs through discussion. The average annotator vs. consensus IAA (smatch) was 0.83 for newswire and 0.79 for web text. When newly trained annotators doubly annotated 382 web text sentences, their annotator vs. annotator IAA was 0.71. (m / marble :location (j / jar)) the marble in the jar ... (b / be-located-at-91 :arg1 (m / marble)"
W13-2322,W12-6207,1,0.412113,"glish WordNet synsets (e.g., “200752493”). Relations. GMB uses VerbNet roles (Schuler, 2005), and AMR uses frame-specific PropBank relations. UNL has a dedicated set of over 30 frequently used relations. Formalism. GMB meanings are written in DRT (Kamp et al., 2011), exploiting full first4 8 Future Work Sembanking. Our main goal is to continue sembanking. We would like to employ a large sembank to create shared tasks for natural language understanding and generation. These tasks may additionally drive interest in theoretical frameworks for probabilistically mapping between graphs and strings (Quernheim and Knight, 2012b; Quernheim and Knight, 2012a; Chiang et al., 2013). Applications. Just as syntactic parsing has found many unanticipated applications, we expect sembanks and statistical semantic processors to be used for many purposes. To get started, we are exploring the use of statistical NLU and NLG in 5 amr.isi.edu/download.html 184 UNL guidelines: www.undl.org/unlsys/unl/unl2005 a semantics-based machine translation (MT) system. In this system, we annotate bilingual Chinese/English data with AMR, then train components to map Chinese to AMR, and AMR to English. A prototype is described by Jones et al. ("
W13-2322,W12-4209,1,0.36463,"glish WordNet synsets (e.g., “200752493”). Relations. GMB uses VerbNet roles (Schuler, 2005), and AMR uses frame-specific PropBank relations. UNL has a dedicated set of over 30 frequently used relations. Formalism. GMB meanings are written in DRT (Kamp et al., 2011), exploiting full first4 8 Future Work Sembanking. Our main goal is to continue sembanking. We would like to employ a large sembank to create shared tasks for natural language understanding and generation. These tasks may additionally drive interest in theoretical frameworks for probabilistically mapping between graphs and strings (Quernheim and Knight, 2012b; Quernheim and Knight, 2012a; Chiang et al., 2013). Applications. Just as syntactic parsing has found many unanticipated applications, we expect sembanks and statistical semantic processors to be used for many purposes. To get started, we are exploring the use of statistical NLU and NLG in 5 amr.isi.edu/download.html 184 UNL guidelines: www.undl.org/unlsys/unl/unl2005 a semantics-based machine translation (MT) system. In this system, we annotate bilingual Chinese/English data with AMR, then train components to map Chinese to AMR, and AMR to English. A prototype is described by Jones et al. ("
W13-2322,W13-0215,0,0.00777424,"ry))”, because “profess-01” 2 3 183 AMR Editor: amr.isi.edu/editor.html Smatch: amr.isi.edu/evaluation.html 6 Current AMR Bank order logic. GMB and ST both include universal quantification. Granularity. GMB and UCCA annotate short texts, so that the same entity can participate in events described in different sentences; other systems annotate individual sentences. Entities. AMR uses 80 entity types, while GMB uses 7. Manual versus automatic. AMR, UNL, and UCCA annotation is fully manual. GMB and ST produce meaning representations automatically, and these can be corrected by experts or crowds (Venhuizen et al., 2013). Derivations. AMR and UNL remain agnostic about the relation between strings and their meanings, considering this a topic of open research. ST and GMB annotate words and phrases directly, recording derivations as (for example) Montaguestyle compositional semantic rules operating on CCG parses. Top-down verus bottom-up. AMR annotators find it fast to construct meanings from the top down, starting with the main idea of the sentence (though the AMR Editor allows bottom-up construction). GMB and UCCA annotators work bottom-up. Editors, guidelines, genres. These projects have graphical sembanking"
W13-2322,N06-1056,0,0.0133733,"AMRs for: • 1546 sentences from the novel “The Little Prince” • 1328 sentences of web data • 1110 sentences of web data (*) • 926 sentences from Xinhua news (*) • 214 sentences from CCTV broadcast conversation (*) Collections marked with a star (*) are also in the OntoNotes corpus (Pradhan et al., 2007; Weischedel et al., 2011). Using the AMR Editor, annotators are able to translate a full sentence into AMR in 7-10 minutes and postedit an AMR in 1-3 minutes. 7 Related Work Researchers working on whole-sentence semantic parsing today typically use small, domain-specific sembanks like GeoQuery (Wong and Mooney, 2006). The need for larger, broad-coverage sembanks has sparked several projects, including the Groningen Meaning Bank (GMB) (Basile et al., 2012a), UCCA (Abend and Rappoport, 2013), the Semantic Treebank (ST) (Butler and Yoshimoto, 2012), the Prague Dependency Treebank (B¨ohmov´a et al., 2003), and UNL (Uchida et al., 1999; Uchida et al., 1996; Martins, 2012). Concepts. Most systems use English words as concepts. AMR uses PropBank frames (e.g., “describe-01”), and UNL uses English WordNet synsets (e.g., “200752493”). Relations. GMB uses VerbNet roles (Schuler, 2005), and AMR uses frame-specific Pr"
W13-2322,E12-2019,0,\N,Missing
W13-2322,C12-1083,1,\N,Missing
W13-2322,C98-1013,0,\N,Missing
W16-2701,P02-1051,1,0.755998,"tity Linking and projecting resources from related languages obtained comparable performance as the method using the same amount of training pairs in the original languages without Entity Linking.1 1 Introduction In Machine Translation and Cross-lingual Information Extraction tasks, an important problem is translating out-of-vocabulary words, mostly names. For some names, we can perform transliteration (Knight and Graehl, 1997; Knight and Graehl, 1998), namely converting them to their approximate phonetic equivalents. Previous methods have generally followed the two-step approach proposed by (Al-Onaizan and Knight, 2002): 1 The transliteration systems are publicly available for research purpose at http://nlp.cs.rpi.edu/transliteration/ Generating transliteration hypotheses based on phoneme, grapheme or correspondence, and validating or re-ranking hypotheses using language modeling (Oh and Isahara, 2007) or Information Extraction from the target language (Ji et al., 2009). In this paper, we focus on back-transliteration from languages lacking in Natural Language Processing (NLP) resources to English for two reasons: (1) In NLP tasks such as name tagging, we can take advantage of rich English resources by trans"
W16-2701,C04-1086,0,0.0292958,", some of their properties, such as gender, need to be inferred from the text. 8 Related Work In terms of transliteration unit, existing machine transliteration models can be classified into three categories, phoneme-based (Knight and Graehl, 1997; Lee and Choi, 1998; Wan and Verspoor, 1998; Jung et al., 2000; Meng et al., 2001; Oh and Choi, 2002; Virga and Khudanpur, 2003; Gao et al., 2005), grapheme-based (Li et al., 2004; Zhang et al., 2004; Ekbal et al., 2006; Ganesh et al., 2008; Das et al., 2009; Chinnakotla et al., 2010; Finch and Sumita, 2010), and hybrid (Al-Onaizan and Knight, 2002; Bilac and Tanaka, 2004; Oh and Choi, 2005; Oh et al., 2006; Kim et al., 1999). Since names are inherently associated with entities, it is natural to leverage entity linking to improve name transliteration. To the best of our knowledge, this is the first study using entity linking results to revise transliteration hypotheses. We also take the specific context of a name into consideration to improve the quality of entity linking and reduce ambiguity. Additionally, to tackle the data sparsity challenge in low-resource languages, we propose a simple but effective cross-lingual projection approach to take advantage of r"
W16-2701,W09-2010,0,0.0125686,"l., 2015). Scores of various data representation methods, namely P (character), M (character+boundary marker), T (bigram), and M+T (bigram+boundary marker), are reported in (Kunchukuttan and Bhattacharyya, 2015) . In our experiments, we estimate the conditional probability P (⟨s, t⟩k |⟨s, t⟩k−1 k−n+1 ) from the alignment result generated by the many-to-many alignment model (m2m-aligner). Originally designed for letter-to-phoneme conversion, the m2m-aligner has also been used in previous transliteration-related tasks (Jiampojamarn and Kondrak, 2009; Jiampojamarn et al., 2009; Dou et al., 2009; Cook and Stevenson, 2009; Jiampojamarn et al., 2008). We apply the m2malingner to the training data to obtain segmentations and alignments. For languages with a large Figure 2: Collective Inference based Entity Linking. If the context is available, the linker adopts an unsupervised collective inference approach which links multiple entity mentions simultaneously and selects corresponding entity candidates which are most strongly connected in the KB as the final linking results. Figure 2 shows the workflow of Collective Inference based Entity Linking. It first constructs a Mention Context Graph Gm for all entity menti"
W16-2701,P16-1038,1,0.915003,"nt vowel, namely schwa, of its preceding consonant in many Indic scripts. In the light of this fact, it is possible to transfer words or transliteration rules across related languages and thereby avoid collecting extra training data for each language. Therefore, we propose a Unicode namebased projection scheme that transfers IL words to their character equivalents in a high-resource related language so that we can apply the transliteration model trained for the high-resource language. This method is very similar to our recent work on building grapheme-to-phoneme models across related scripts (Deri and Knight, 2016). In Unicode character code charts5 , most vowels, consonants and signs are assigned a name with the following format: 5 http://unicode.org/charts SCRIPT TYPE NAME For example, Bengali independent vowel “অ”, dependent vowel sign “ ু” and consonant “ক” are named BENGALI LETTER A, BENGALI VOWEL SIGN U and BENGALI LETTER KA, respectively. Utilizing these Unicode character names as a bridge, our approach consists of the following steps: (1) For a low resource language L, select its related language L′ whose transliteration pairs can be extracted from existing resources with minimal effort; (2) Con"
W16-2701,P09-1014,0,0.019406,"d in (Nicolai et al., 2015). Scores of various data representation methods, namely P (character), M (character+boundary marker), T (bigram), and M+T (bigram+boundary marker), are reported in (Kunchukuttan and Bhattacharyya, 2015) . In our experiments, we estimate the conditional probability P (⟨s, t⟩k |⟨s, t⟩k−1 k−n+1 ) from the alignment result generated by the many-to-many alignment model (m2m-aligner). Originally designed for letter-to-phoneme conversion, the m2m-aligner has also been used in previous transliteration-related tasks (Jiampojamarn and Kondrak, 2009; Jiampojamarn et al., 2009; Dou et al., 2009; Cook and Stevenson, 2009; Jiampojamarn et al., 2008). We apply the m2malingner to the training data to obtain segmentations and alignments. For languages with a large Figure 2: Collective Inference based Entity Linking. If the context is available, the linker adopts an unsupervised collective inference approach which links multiple entity mentions simultaneously and selects corresponding entity candidates which are most strongly connected in the KB as the final linking results. Figure 2 shows the workflow of Collective Inference based Entity Linking. It first constructs a Mention Context Gra"
W16-2701,P06-2025,0,0.0245592,"ve the ambiguity and hence generate correct transliterations, while it does not work for out-of-KB ones. In order to transliterate such out-of-KB names, some of their properties, such as gender, need to be inferred from the text. 8 Related Work In terms of transliteration unit, existing machine transliteration models can be classified into three categories, phoneme-based (Knight and Graehl, 1997; Lee and Choi, 1998; Wan and Verspoor, 1998; Jung et al., 2000; Meng et al., 2001; Oh and Choi, 2002; Virga and Khudanpur, 2003; Gao et al., 2005), grapheme-based (Li et al., 2004; Zhang et al., 2004; Ekbal et al., 2006; Ganesh et al., 2008; Das et al., 2009; Chinnakotla et al., 2010; Finch and Sumita, 2010), and hybrid (Al-Onaizan and Knight, 2002; Bilac and Tanaka, 2004; Oh and Choi, 2005; Oh et al., 2006; Kim et al., 1999). Since names are inherently associated with entities, it is natural to leverage entity linking to improve name transliteration. To the best of our knowledge, this is the first study using entity linking results to revise transliteration hypotheses. We also take the specific context of a name into consideration to improve the quality of entity linking and reduce ambiguity. Additionally,"
W16-2701,N15-1151,0,0.031529,"t of a name into consideration to improve the quality of entity linking and reduce ambiguity. Additionally, to tackle the data sparsity challenge in low-resource languages, we propose a simple but effective cross-lingual projection approach to take advantage of resources in related languages. Similar cross-lingual projection methods based on data/annotation transfer have also been exploited for other Natural Language Processing tasks, including relation extraction, data annotation, entity recognition, and grapheme-tophoneme models (Xia and Lewis, 2007; Padó and Lapata, 2009; Kim et al., 2010; Faruqui and Kumar, 2015; Deri and Knight, 2016). 9 Conclusions and Future Work For many names we need to know the real-world entities they refer to before generating their correct transliterations. In this paper we developed a novel context-aware name transliteration approach by leveraging Entity Linking and related language projection. Experiments have demonstrated that our approach can significantly enhance the transliteration performance. In the future we will explore more knowledge from the KB such as types and properties of entities to improve disambiguation and transliteration. We will also aim to incorporate"
W16-2701,2010.iwslt-papers.7,0,0.0300468,"for out-of-KB ones. In order to transliterate such out-of-KB names, some of their properties, such as gender, need to be inferred from the text. 8 Related Work In terms of transliteration unit, existing machine transliteration models can be classified into three categories, phoneme-based (Knight and Graehl, 1997; Lee and Choi, 1998; Wan and Verspoor, 1998; Jung et al., 2000; Meng et al., 2001; Oh and Choi, 2002; Virga and Khudanpur, 2003; Gao et al., 2005), grapheme-based (Li et al., 2004; Zhang et al., 2004; Ekbal et al., 2006; Ganesh et al., 2008; Das et al., 2009; Chinnakotla et al., 2010; Finch and Sumita, 2010), and hybrid (Al-Onaizan and Knight, 2002; Bilac and Tanaka, 2004; Oh and Choi, 2005; Oh et al., 2006; Kim et al., 1999). Since names are inherently associated with entities, it is natural to leverage entity linking to improve name transliteration. To the best of our knowledge, this is the first study using entity linking results to revise transliteration hypotheses. We also take the specific context of a name into consideration to improve the quality of entity linking and reduce ambiguity. Additionally, to tackle the data sparsity challenge in low-resource languages, we propose a simple but e"
W16-2701,I08-6006,0,0.0271832,"hence generate correct transliterations, while it does not work for out-of-KB ones. In order to transliterate such out-of-KB names, some of their properties, such as gender, need to be inferred from the text. 8 Related Work In terms of transliteration unit, existing machine transliteration models can be classified into three categories, phoneme-based (Knight and Graehl, 1997; Lee and Choi, 1998; Wan and Verspoor, 1998; Jung et al., 2000; Meng et al., 2001; Oh and Choi, 2002; Virga and Khudanpur, 2003; Gao et al., 2005), grapheme-based (Li et al., 2004; Zhang et al., 2004; Ekbal et al., 2006; Ganesh et al., 2008; Das et al., 2009; Chinnakotla et al., 2010; Finch and Sumita, 2010), and hybrid (Al-Onaizan and Knight, 2002; Bilac and Tanaka, 2004; Oh and Choi, 2005; Oh et al., 2006; Kim et al., 1999). Since names are inherently associated with entities, it is natural to leverage entity linking to improve name transliteration. To the best of our knowledge, this is the first study using entity linking results to revise transliteration hypotheses. We also take the specific context of a name into consideration to improve the quality of entity linking and reduce ambiguity. Additionally, to tackle the data sp"
W16-2701,N07-1047,0,0.193094,"rs in the infobox of “Usher (Singer)”, we take “Usher” and “Lackey” as transliterations for “亚瑟” and “雷基” respectively. Challenge 4: Lack of Training Pairs. Statistical transliteration models usually rely on thousands of name pairs for training. However, it might be costly to collect required training data for lowresource languages. To address this issue, we propose a simple but effective method which transliterates names in a low-resource language using a model trained on one of its similar languages by means of a character mapping table derived from Unicode charts. ment model (m2m-aligner) (Jiampojamarn et al., 2007) to segment and align each transliteration pair in the training data. 2. Transliteration. For each name in the test set, we apply a joint source-channel model (JSCM) (Li et al., 2004) to generate a list of transliteration hypotheses, where the probabilities of n-grams of transliteration unit pairs are estimated from the alignment result. 3. Linking. We link each transliteration hypothesis to an English KB using a languageindependent entity linker (Wang et al., 2015). If context exists, we apply collective inference to link multiple related names simultaneously. 4. Hypotheses Correction. Finall"
W16-2701,P08-1103,0,0.0221752,"us data representation methods, namely P (character), M (character+boundary marker), T (bigram), and M+T (bigram+boundary marker), are reported in (Kunchukuttan and Bhattacharyya, 2015) . In our experiments, we estimate the conditional probability P (⟨s, t⟩k |⟨s, t⟩k−1 k−n+1 ) from the alignment result generated by the many-to-many alignment model (m2m-aligner). Originally designed for letter-to-phoneme conversion, the m2m-aligner has also been used in previous transliteration-related tasks (Jiampojamarn and Kondrak, 2009; Jiampojamarn et al., 2009; Dou et al., 2009; Cook and Stevenson, 2009; Jiampojamarn et al., 2008). We apply the m2malingner to the training data to obtain segmentations and alignments. For languages with a large Figure 2: Collective Inference based Entity Linking. If the context is available, the linker adopts an unsupervised collective inference approach which links multiple entity mentions simultaneously and selects corresponding entity candidates which are most strongly connected in the KB as the final linking results. Figure 2 shows the workflow of Collective Inference based Entity Linking. It first constructs a Mention Context Graph Gm for all entity mentions M = {m1 , m2 , ..., mn }"
W16-2701,W09-3504,0,0.0953896,"ne translation) are reported in (Nicolai et al., 2015). Scores of various data representation methods, namely P (character), M (character+boundary marker), T (bigram), and M+T (bigram+boundary marker), are reported in (Kunchukuttan and Bhattacharyya, 2015) . In our experiments, we estimate the conditional probability P (⟨s, t⟩k |⟨s, t⟩k−1 k−n+1 ) from the alignment result generated by the many-to-many alignment model (m2m-aligner). Originally designed for letter-to-phoneme conversion, the m2m-aligner has also been used in previous transliteration-related tasks (Jiampojamarn and Kondrak, 2009; Jiampojamarn et al., 2009; Dou et al., 2009; Cook and Stevenson, 2009; Jiampojamarn et al., 2008). We apply the m2malingner to the training data to obtain segmentations and alignments. For languages with a large Figure 2: Collective Inference based Entity Linking. If the context is available, the linker adopts an unsupervised collective inference approach which links multiple entity mentions simultaneously and selects corresponding entity candidates which are most strongly connected in the KB as the final linking results. Figure 2 shows the workflow of Collective Inference based Entity Linking. It first constructs a M"
W16-2701,C00-1056,0,0.0550885,"o “door”, “net”, “dream” and “dew”, respectively. For celebrities with corresponding entities in the KB, the collective inference method we employ can resolve the ambiguity and hence generate correct transliterations, while it does not work for out-of-KB ones. In order to transliterate such out-of-KB names, some of their properties, such as gender, need to be inferred from the text. 8 Related Work In terms of transliteration unit, existing machine transliteration models can be classified into three categories, phoneme-based (Knight and Graehl, 1997; Lee and Choi, 1998; Wan and Verspoor, 1998; Jung et al., 2000; Meng et al., 2001; Oh and Choi, 2002; Virga and Khudanpur, 2003; Gao et al., 2005), grapheme-based (Li et al., 2004; Zhang et al., 2004; Ekbal et al., 2006; Ganesh et al., 2008; Das et al., 2009; Chinnakotla et al., 2010; Finch and Sumita, 2010), and hybrid (Al-Onaizan and Knight, 2002; Bilac and Tanaka, 2004; Oh and Choi, 2005; Oh et al., 2006; Kim et al., 1999). Since names are inherently associated with entities, it is natural to leverage entity linking to improve name transliteration. To the best of our knowledge, this is the first study using entity linking results to revise translitera"
W16-2701,C10-1064,0,0.0221027,"he specific context of a name into consideration to improve the quality of entity linking and reduce ambiguity. Additionally, to tackle the data sparsity challenge in low-resource languages, we propose a simple but effective cross-lingual projection approach to take advantage of resources in related languages. Similar cross-lingual projection methods based on data/annotation transfer have also been exploited for other Natural Language Processing tasks, including relation extraction, data annotation, entity recognition, and grapheme-tophoneme models (Xia and Lewis, 2007; Padó and Lapata, 2009; Kim et al., 2010; Faruqui and Kumar, 2015; Deri and Knight, 2016). 9 Conclusions and Future Work For many names we need to know the real-world entities they refer to before generating their correct transliterations. In this paper we developed a novel context-aware name transliteration approach by leveraging Entity Linking and related language projection. Experiments have demonstrated that our approach can significantly enhance the transliteration performance. In the future we will explore more knowledge from the KB such as types and properties of entities to improve disambiguation and transliteration. We will"
W16-2701,P97-1017,1,0.568462,"nroe”, a famous American actress, where “门”, “罗”, “梦” and “露” refer to “door”, “net”, “dream” and “dew”, respectively. For celebrities with corresponding entities in the KB, the collective inference method we employ can resolve the ambiguity and hence generate correct transliterations, while it does not work for out-of-KB ones. In order to transliterate such out-of-KB names, some of their properties, such as gender, need to be inferred from the text. 8 Related Work In terms of transliteration unit, existing machine transliteration models can be classified into three categories, phoneme-based (Knight and Graehl, 1997; Lee and Choi, 1998; Wan and Verspoor, 1998; Jung et al., 2000; Meng et al., 2001; Oh and Choi, 2002; Virga and Khudanpur, 2003; Gao et al., 2005), grapheme-based (Li et al., 2004; Zhang et al., 2004; Ekbal et al., 2006; Ganesh et al., 2008; Das et al., 2009; Chinnakotla et al., 2010; Finch and Sumita, 2010), and hybrid (Al-Onaizan and Knight, 2002; Bilac and Tanaka, 2004; Oh and Choi, 2005; Oh et al., 2006; Kim et al., 1999). Since names are inherently associated with entities, it is natural to leverage entity linking to improve name transliteration. To the best of our knowledge, this is the"
W16-2701,W15-3912,0,0.0263527,"ration unit pairs ⟨s, t⟩k = ⟨xi xi+1 ...xi+p , yj yj+1 ...yj+q ⟩, where each s or t corresponds to one or more source or target characters, respectively. The JSCM is an ngram model defined as P (S, T ) = P (⟨s, t⟩1 , ⟨s, t⟩2 , ..., ⟨s, t⟩K ) = P (α, β, γ) = K ∏ P (⟨s, t⟩k |⟨s, t⟩k−1 k−n+1 ) 4 Entity Linking k=1 We can formulate forward-transliteration and back-transliteration as β¯ = arg max P (α, β, γ) β,γ α ¯ = arg max P (α, β, γ) α,γ Table 2 shows that the performance of JSCM on forward-transliteration (English to foreign language) is comparable with state-of-the-art (Nicolai et al., 2015; Kunchukuttan and Bhattacharyya, 2015) on the NEWS2015 development sets, thereby showing it is a simple but effective model. Target Hindi Kannada Bengali Tamil Hebrew Thai DTL 43.5 32.7 37.1 38.5 61.3 36.2 SEQ 40.4 35.7 37.8 34.4 56.6 35.8 SMT 36.8 28.1 34.9 29.3 53.1 30.6 P 38.8 27.6 35.4 28.6 54.6 - M 41.0 32.7 38.2 32.4 56.4 - T 37.0 28.9 34.5 31.4 54.4 - M+T 40.5 30.4 36.4 33.4 54.5 - number of characters, training pairs may not cover all characters. As a fallback option, we extend the m2m-aligner’s output with pronunciations or romanizations of characters out of the training data. For example, if the Chinese character 孔 (kǒng"
W16-2701,P04-1021,0,0.552024,"sually rely on thousands of name pairs for training. However, it might be costly to collect required training data for lowresource languages. To address this issue, we propose a simple but effective method which transliterates names in a low-resource language using a model trained on one of its similar languages by means of a character mapping table derived from Unicode charts. ment model (m2m-aligner) (Jiampojamarn et al., 2007) to segment and align each transliteration pair in the training data. 2. Transliteration. For each name in the test set, we apply a joint source-channel model (JSCM) (Li et al., 2004) to generate a list of transliteration hypotheses, where the probabilities of n-grams of transliteration unit pairs are estimated from the alignment result. 3. Linking. We link each transliteration hypothesis to an English KB using a languageindependent entity linker (Wang et al., 2015). If context exists, we apply collective inference to link multiple related names simultaneously. 4. Hypotheses Correction. Finally, we revise each hypothesis using the surface forms of the linked entities, and merge and rank the revised hypotheses. The detailed techniques for each step will be presented in the"
W16-2701,W15-3911,0,0.0747924,"ent γ with K transliteration unit pairs ⟨s, t⟩k = ⟨xi xi+1 ...xi+p , yj yj+1 ...yj+q ⟩, where each s or t corresponds to one or more source or target characters, respectively. The JSCM is an ngram model defined as P (S, T ) = P (⟨s, t⟩1 , ⟨s, t⟩2 , ..., ⟨s, t⟩K ) = P (α, β, γ) = K ∏ P (⟨s, t⟩k |⟨s, t⟩k−1 k−n+1 ) 4 Entity Linking k=1 We can formulate forward-transliteration and back-transliteration as β¯ = arg max P (α, β, γ) β,γ α ¯ = arg max P (α, β, γ) α,γ Table 2 shows that the performance of JSCM on forward-transliteration (English to foreign language) is comparable with state-of-the-art (Nicolai et al., 2015; Kunchukuttan and Bhattacharyya, 2015) on the NEWS2015 development sets, thereby showing it is a simple but effective model. Target Hindi Kannada Bengali Tamil Hebrew Thai DTL 43.5 32.7 37.1 38.5 61.3 36.2 SEQ 40.4 35.7 37.8 34.4 56.6 35.8 SMT 36.8 28.1 34.9 29.3 53.1 30.6 P 38.8 27.6 35.4 28.6 54.6 - M 41.0 32.7 38.2 32.4 56.4 - T 37.0 28.9 34.5 31.4 54.4 - M+T 40.5 30.4 36.4 33.4 54.5 - number of characters, training pairs may not cover all characters. As a fallback option, we extend the m2m-aligner’s output with pronunciations or romanizations of characters out of the training data. For ex"
W16-2701,C02-1099,0,0.0621322,"spectively. For celebrities with corresponding entities in the KB, the collective inference method we employ can resolve the ambiguity and hence generate correct transliterations, while it does not work for out-of-KB ones. In order to transliterate such out-of-KB names, some of their properties, such as gender, need to be inferred from the text. 8 Related Work In terms of transliteration unit, existing machine transliteration models can be classified into three categories, phoneme-based (Knight and Graehl, 1997; Lee and Choi, 1998; Wan and Verspoor, 1998; Jung et al., 2000; Meng et al., 2001; Oh and Choi, 2002; Virga and Khudanpur, 2003; Gao et al., 2005), grapheme-based (Li et al., 2004; Zhang et al., 2004; Ekbal et al., 2006; Ganesh et al., 2008; Das et al., 2009; Chinnakotla et al., 2010; Finch and Sumita, 2010), and hybrid (Al-Onaizan and Knight, 2002; Bilac and Tanaka, 2004; Oh and Choi, 2005; Oh et al., 2006; Kim et al., 1999). Since names are inherently associated with entities, it is natural to leverage entity linking to improve name transliteration. To the best of our knowledge, this is the first study using entity linking results to revise transliteration hypotheses. We also take the spec"
W16-2701,I05-1040,0,0.0367448,"es, such as gender, need to be inferred from the text. 8 Related Work In terms of transliteration unit, existing machine transliteration models can be classified into three categories, phoneme-based (Knight and Graehl, 1997; Lee and Choi, 1998; Wan and Verspoor, 1998; Jung et al., 2000; Meng et al., 2001; Oh and Choi, 2002; Virga and Khudanpur, 2003; Gao et al., 2005), grapheme-based (Li et al., 2004; Zhang et al., 2004; Ekbal et al., 2006; Ganesh et al., 2008; Das et al., 2009; Chinnakotla et al., 2010; Finch and Sumita, 2010), and hybrid (Al-Onaizan and Knight, 2002; Bilac and Tanaka, 2004; Oh and Choi, 2005; Oh et al., 2006; Kim et al., 1999). Since names are inherently associated with entities, it is natural to leverage entity linking to improve name transliteration. To the best of our knowledge, this is the first study using entity linking results to revise transliteration hypotheses. We also take the specific context of a name into consideration to improve the quality of entity linking and reduce ambiguity. Additionally, to tackle the data sparsity challenge in low-resource languages, we propose a simple but effective cross-lingual projection approach to take advantage of resources in related"
W16-2701,2007.mtsummit-papers.47,0,0.0465972,"t problem is translating out-of-vocabulary words, mostly names. For some names, we can perform transliteration (Knight and Graehl, 1997; Knight and Graehl, 1998), namely converting them to their approximate phonetic equivalents. Previous methods have generally followed the two-step approach proposed by (Al-Onaizan and Knight, 2002): 1 The transliteration systems are publicly available for research purpose at http://nlp.cs.rpi.edu/transliteration/ Generating transliteration hypotheses based on phoneme, grapheme or correspondence, and validating or re-ranking hypotheses using language modeling (Oh and Isahara, 2007) or Information Extraction from the target language (Ji et al., 2009). In this paper, we focus on back-transliteration from languages lacking in Natural Language Processing (NLP) resources to English for two reasons: (1) In NLP tasks such as name tagging, we can take advantage of rich English resources by transliterating a name to English. Our analysis of 986 transliteration pairs from the Named Entities Workshop 2015 (NEWS2015) 2 Bengali development set shows that 574 English names can be found in the DBpedia 3 , while only 47 Bengali names exist in the same knowledge base (KB). (2) Back-tran"
W16-2701,W03-1508,0,0.100687,"ebrities with corresponding entities in the KB, the collective inference method we employ can resolve the ambiguity and hence generate correct transliterations, while it does not work for out-of-KB ones. In order to transliterate such out-of-KB names, some of their properties, such as gender, need to be inferred from the text. 8 Related Work In terms of transliteration unit, existing machine transliteration models can be classified into three categories, phoneme-based (Knight and Graehl, 1997; Lee and Choi, 1998; Wan and Verspoor, 1998; Jung et al., 2000; Meng et al., 2001; Oh and Choi, 2002; Virga and Khudanpur, 2003; Gao et al., 2005), grapheme-based (Li et al., 2004; Zhang et al., 2004; Ekbal et al., 2006; Ganesh et al., 2008; Das et al., 2009; Chinnakotla et al., 2010; Finch and Sumita, 2010), and hybrid (Al-Onaizan and Knight, 2002; Bilac and Tanaka, 2004; Oh and Choi, 2005; Oh et al., 2006; Kim et al., 1999). Since names are inherently associated with entities, it is natural to leverage entity linking to improve name transliteration. To the best of our knowledge, this is the first study using entity linking results to revise transliteration hypotheses. We also take the specific context of a name into"
W16-2701,P98-2220,0,0.220149,"“罗”, “梦” and “露” refer to “door”, “net”, “dream” and “dew”, respectively. For celebrities with corresponding entities in the KB, the collective inference method we employ can resolve the ambiguity and hence generate correct transliterations, while it does not work for out-of-KB ones. In order to transliterate such out-of-KB names, some of their properties, such as gender, need to be inferred from the text. 8 Related Work In terms of transliteration unit, existing machine transliteration models can be classified into three categories, phoneme-based (Knight and Graehl, 1997; Lee and Choi, 1998; Wan and Verspoor, 1998; Jung et al., 2000; Meng et al., 2001; Oh and Choi, 2002; Virga and Khudanpur, 2003; Gao et al., 2005), grapheme-based (Li et al., 2004; Zhang et al., 2004; Ekbal et al., 2006; Ganesh et al., 2008; Das et al., 2009; Chinnakotla et al., 2010; Finch and Sumita, 2010), and hybrid (Al-Onaizan and Knight, 2002; Bilac and Tanaka, 2004; Oh and Choi, 2005; Oh et al., 2006; Kim et al., 1999). Since names are inherently associated with entities, it is natural to leverage entity linking to improve name transliteration. To the best of our knowledge, this is the first study using entity linking results to"
W16-2701,D15-1081,1,0.885331,"one of its similar languages by means of a character mapping table derived from Unicode charts. ment model (m2m-aligner) (Jiampojamarn et al., 2007) to segment and align each transliteration pair in the training data. 2. Transliteration. For each name in the test set, we apply a joint source-channel model (JSCM) (Li et al., 2004) to generate a list of transliteration hypotheses, where the probabilities of n-grams of transliteration unit pairs are estimated from the alignment result. 3. Linking. We link each transliteration hypothesis to an English KB using a languageindependent entity linker (Wang et al., 2015). If context exists, we apply collective inference to link multiple related names simultaneously. 4. Hypotheses Correction. Finally, we revise each hypothesis using the surface forms of the linked entities, and merge and rank the revised hypotheses. The detailed techniques for each step will be presented in the following sections. 2 Approach Overview Figure 1 illustrates the overall framework of our approach, which consists of four steps as follows. 1. Training. We employ a many-to-many align3 Transliteration Hypotheses Generation We use the joint source-channel model (JSCM) proposed in (Li et"
W16-2701,N07-1057,0,0.0174264,"e transliteration hypotheses. We also take the specific context of a name into consideration to improve the quality of entity linking and reduce ambiguity. Additionally, to tackle the data sparsity challenge in low-resource languages, we propose a simple but effective cross-lingual projection approach to take advantage of resources in related languages. Similar cross-lingual projection methods based on data/annotation transfer have also been exploited for other Natural Language Processing tasks, including relation extraction, data annotation, entity recognition, and grapheme-tophoneme models (Xia and Lewis, 2007; Padó and Lapata, 2009; Kim et al., 2010; Faruqui and Kumar, 2015; Deri and Knight, 2016). 9 Conclusions and Future Work For many names we need to know the real-world entities they refer to before generating their correct transliterations. In this paper we developed a novel context-aware name transliteration approach by leveraging Entity Linking and related language projection. Experiments have demonstrated that our approach can significantly enhance the transliteration performance. In the future we will explore more knowledge from the KB such as types and properties of entities to improve di"
W16-2701,C04-1103,0,0.0395382,"we employ can resolve the ambiguity and hence generate correct transliterations, while it does not work for out-of-KB ones. In order to transliterate such out-of-KB names, some of their properties, such as gender, need to be inferred from the text. 8 Related Work In terms of transliteration unit, existing machine transliteration models can be classified into three categories, phoneme-based (Knight and Graehl, 1997; Lee and Choi, 1998; Wan and Verspoor, 1998; Jung et al., 2000; Meng et al., 2001; Oh and Choi, 2002; Virga and Khudanpur, 2003; Gao et al., 2005), grapheme-based (Li et al., 2004; Zhang et al., 2004; Ekbal et al., 2006; Ganesh et al., 2008; Das et al., 2009; Chinnakotla et al., 2010; Finch and Sumita, 2010), and hybrid (Al-Onaizan and Knight, 2002; Bilac and Tanaka, 2004; Oh and Choi, 2005; Oh et al., 2006; Kim et al., 1999). Since names are inherently associated with entities, it is natural to leverage entity linking to improve name transliteration. To the best of our knowledge, this is the first study using entity linking results to revise transliteration hypotheses. We also take the specific context of a name into consideration to improve the quality of entity linking and reduce ambig"
W16-5603,D11-1120,0,0.0151853,"by-product of personalization is inadvertent discrimination: a study (Datta et al., 2015) finds that Google serves fewer ads for highpaying jobs to users profiled as female, and Sweeney (2013) shows that ads for public data about people who are profiled as black are more likely to suggest an arrest record regardless of whether the person had one. Introduction Recent work has demonstrated success in accurately detecting gender or other author attributes such as age, location, and political preferences from textual input, particularly on social media channels like Twitter (Bamman et al., 2014; Burger et al., 2011; Eisenstein et al., 2010; Li et al., 2014; Liu and Ruths, 2013; Pennacchiotti and Popescu, 2013; Rao et al., 2010; Volkova et al., 2015), weblogs (Mukherjee and Liu, 2010; Schler et al., 2006; Yan and Yan, 2006) and user-review sites (Johannsen et al., 2015). Outside of academic research, detection of author attributes is a major component of “behavioral targeting” which has been instrumental in online advertising and marketing from the early days of the Web. 2. Users living under authoritarian governments have the incentive to conceal their identity for personal safety (Jardine, 2016). Even"
W16-5603,P12-1094,0,0.0520261,"Missing"
W16-5603,D10-1124,0,0.10432,"Missing"
W16-5603,N13-1092,0,0.0197326,"Missing"
W16-5603,P11-2008,0,0.0234649,"Missing"
W16-5603,P16-2096,0,0.0398235,"Missing"
W16-5603,K15-1011,0,0.0228091,"as black are more likely to suggest an arrest record regardless of whether the person had one. Introduction Recent work has demonstrated success in accurately detecting gender or other author attributes such as age, location, and political preferences from textual input, particularly on social media channels like Twitter (Bamman et al., 2014; Burger et al., 2011; Eisenstein et al., 2010; Li et al., 2014; Liu and Ruths, 2013; Pennacchiotti and Popescu, 2013; Rao et al., 2010; Volkova et al., 2015), weblogs (Mukherjee and Liu, 2010; Schler et al., 2006; Yan and Yan, 2006) and user-review sites (Johannsen et al., 2015). Outside of academic research, detection of author attributes is a major component of “behavioral targeting” which has been instrumental in online advertising and marketing from the early days of the Web. 2. Users living under authoritarian governments have the incentive to conceal their identity for personal safety (Jardine, 2016). Even outside of repressive regimes, studies have shown that users value anonymity and are more likely to share controversial content when anonymous (Zhang and Kizilcec, 2014). This is evidenced by the popularity of anonymousposting networks like Yik Yak and Whispe"
W16-5603,D14-1108,0,0.0179017,"Missing"
W16-5603,P14-2050,0,0.0415875,"Missing"
W16-5603,P14-1016,0,0.0275993,"discrimination: a study (Datta et al., 2015) finds that Google serves fewer ads for highpaying jobs to users profiled as female, and Sweeney (2013) shows that ads for public data about people who are profiled as black are more likely to suggest an arrest record regardless of whether the person had one. Introduction Recent work has demonstrated success in accurately detecting gender or other author attributes such as age, location, and political preferences from textual input, particularly on social media channels like Twitter (Bamman et al., 2014; Burger et al., 2011; Eisenstein et al., 2010; Li et al., 2014; Liu and Ruths, 2013; Pennacchiotti and Popescu, 2013; Rao et al., 2010; Volkova et al., 2015), weblogs (Mukherjee and Liu, 2010; Schler et al., 2006; Yan and Yan, 2006) and user-review sites (Johannsen et al., 2015). Outside of academic research, detection of author attributes is a major component of “behavioral targeting” which has been instrumental in online advertising and marketing from the early days of the Web. 2. Users living under authoritarian governments have the incentive to conceal their identity for personal safety (Jardine, 2016). Even outside of repressive regimes, studies hav"
W16-5603,I11-1062,0,0.0226677,"Missing"
W16-5603,J10-3003,0,0.0194889,"sary who has a budget for black box access to the classifier rather than the entire classifier. Dalvi et al. (2004) sketch out an adversary’s strategy for evading a Na¨ıve Bayes classifier, and show how to detect if a test sample has been modified according to that strategy. Within the theoretical machine learning community, there is a great deal of interest on learning classifiers that do not adversely affect or discriminate against individuals, by constraining them to satisfy some formal definition of fairness (Zemel et al., 2013). Our problem can be considered one of paraphrase generation (Madnani and Dorr, 2010) with the objective of defeating a text classifier. 3 Problem Description The general problem of modifying text to fool a classifier is open-ended; the specific question depends on our goals and assumptions. We consider this (simplified) scenario: 19 1. We do not have access to the actual classifier or even knowledge of the type of classifier or its training algorithm. 2. However, we do have a corpus of labeled data for the class labels which approximate the actual training data of the classifier, and knowledge about the type of features that it uses, as in Biggio et al. (2013). In this paper,"
W16-5603,P14-5010,0,0.0036954,"Missing"
W16-5603,W15-1501,0,0.0159713,"rds that have the highest associations with each gender are listed in Table 1. While these top items tend to be content/topical words that cannot be easily substituted, adjectives and punctuations that are gender-specific also rank high. 5.2 Syntactic+Semantic Similarity (SynSem) We considered building the lexical similarity model from databases like PPDB (Ganitkevitch et al., 2013), as in Preotiuc-Pietro et al. (2016), but found 21 Substitutability (Subst) This determines which of the lexically similar candidates are most appropriate in a given context. We use the measure below, adapted from Melamud et al. (2015), giving the substitutability of a for b in the context of a list of tokens C by averaging over b and the context: P SynSem(a, b) + c∈C Sem(a, c) Subst(a, b, C) = |C |+ 1 Unlike Melamud et al. (2015) who rely on the dependency-parse-based system throughout, we take Sem(a, c) to be the cosine similarity between the regular window 5 skip-gram vectors Mikolov et al. (2013), and use the two adjacent words on either side of b as the context C. We found this works Table 1: Words having the highest associations with each gender Male Female Male Female Twitter bro, bruh, game, man, team, steady, drink"
W16-5603,D10-1021,0,0.00811256,"female, and Sweeney (2013) shows that ads for public data about people who are profiled as black are more likely to suggest an arrest record regardless of whether the person had one. Introduction Recent work has demonstrated success in accurately detecting gender or other author attributes such as age, location, and political preferences from textual input, particularly on social media channels like Twitter (Bamman et al., 2014; Burger et al., 2011; Eisenstein et al., 2010; Li et al., 2014; Liu and Ruths, 2013; Pennacchiotti and Popescu, 2013; Rao et al., 2010; Volkova et al., 2015), weblogs (Mukherjee and Liu, 2010; Schler et al., 2006; Yan and Yan, 2006) and user-review sites (Johannsen et al., 2015). Outside of academic research, detection of author attributes is a major component of “behavioral targeting” which has been instrumental in online advertising and marketing from the early days of the Web. 2. Users living under authoritarian governments have the incentive to conceal their identity for personal safety (Jardine, 2016). Even outside of repressive regimes, studies have shown that users value anonymity and are more likely to share controversial content when anonymous (Zhang and Kizilcec, 2014)."
W16-5603,C14-1184,0,0.0477209,"Missing"
W16-5603,N16-3020,0,0.00719696,"Missing"
W16-5603,S15-2001,0,0.0233902,"Missing"
W16-5907,P10-1131,0,0.0467125,"Missing"
W16-5907,N10-1083,0,0.157395,"ampling methods like MCMC has enabled the development of unsupervised systems for tag and grammar induction, alignment, topic models and more. These latent variable models discover hidden structure in text which aligns to known linguistic phenomena and whose clusters are easily identifiable. Recently, much of supervised NLP has found great success by augmenting or replacing context, features, and word representations with embeddings derived from Deep Neural Networks. These models allow for learning highly expressive non-convex functions by simply backpropagating prediction errors. Inspired by Berg-Kirkpatrick et al. (2010), who bridged the gap between supervised and unsupervised training with features, we bring neural networks to unsupervised learning by providing evidence that even in ∗ This research was carried out while all authors were at the Information Sciences Institute. unsupervised settings, simple neural network models trained to maximize the marginal likelihood can outperform more complicated models that use expensive inference. In this work, we show how a single latent variable sequence model, Hidden Markov Models (HMMs), can be implemented with neural networks by simply optimizing the incomplete da"
W16-5907,P15-2143,1,0.836803,"damental tool in downstream NLP applications. In English, the Penn Treebank (Marcus et al., 1994) distinguishes 36 categories and punctuation. Tag induction is the task of taking raw text and both discovering these latent clusters and assigning them to words in situ. Classes can be very specific (e.g. six types of verbs in English) to their syntactic role. Example tags are shown in Table 1. In this example, board is labeled as a singular noun while Pierre Vinken is a singular proper noun. Two natural applications of induced tags are as the basis for grammar induction (Spitkovsky et al., 2011; Bisk et al., 2015) or to provide a syntactically informed, though unsupervised, source of word embeddings. z1 zt 1 zt zt+1 zT x1 xt 1 xt xt+1 xT Figure 1: Pictorial representation of a Hidden Markov Model. Latent variable (zt ) transitions depend on the previous value (zt−1 ), and emit an observed word (xt ) at each time step. 3.1 The Hidden Markov Model A common model for this task, and our primary workhorse, is the Hidden Markov Model trained with the unsupervised message passing algorithm, BaumWelch (Welch, 2003). Model HMMs model a sentence by assuming that (a) every word token is generated by a latent clas"
W16-5907,P11-1087,0,0.098067,"multiplying transitions and emissions across all time steps (Eq. 6). Finding the optimal sequence of latent classes corresponds to computing an argmax over the values of z. p(x, z) = n+1 Y t=1 p(zt |zt−1 ) n Y p(xt |zt ) (6) t=1 Because our task is unsupervised we do not have a priori access to these distributions, but they can be estimated via Baum-Welch. The algorithm’s outline is provided in Algorithm 1. Training an HMM with EM is highly non-convex and likely to get stuck in local optima (Johnson, 2007). Despite this, sophisticated Bayesian smoothing leads to state-of-the-art performance (Blunsom and Cohn, 2011). Blunsom and Cohn (2011) further extend the HMM by augmenting its emission distributions with character models to capture morphological information and a tri-gram transition matrix which conditions on the previous two states. Recently, Lin et al. (2015) extended several models 65 Algorithm 1 Baum-Welch Algorithm Randomly Initialize distributions (θ) repeat Compute forward messages: ∀i,t αi (t) Compute backward messages: ∀i,t βi (t) Compute posteriors: p(zt = i |x, θ) ∝ αi (t)βi (t) p(zt = i, zt+1 = j |x, θ) ∝ αi (t)p(zt+1 = j|zt = i) ×βj (t + 1)p(xt+1 |zt+1 = j) Update θ until Converged inclu"
W16-5907,J92-4003,0,0.464732,"the HMM to include pre-trained word embeddings learned by different skip-gram models. Our work will fully neuralize the HMM and learn embeddings during the training of our generative model. There has also been recent work on by Rastogi et al. (2016) on neuralizing Finite-State Transducers. 3.2 Additional Comparisons While the main focus of our paper is the seamless extension of an unsupervised generative latent variable model with neural networks, for completeness we will also include comparisons to other techniques which do not adhere to the generative assumption. We include Brown clusters (Brown et al., 1992) as a baseline and two clustering techniques as stateof-the-art comparisons: Christodoulopoulos et al. (2011) and Yatbaz et al. (2012). Of particular interest to us is the work of Brown et al. (1992). Brown clusters group word types through a greedy agglomerative clustering according to their mutual information across the corpus based on bigram probabilities. Brown clusters do not account for a word’s membership in multiple syntactic classes, but are a very strong baseline for tag induction. It is possible our approach could be improved by augmenting our objective function to include mutual in"
W16-5907,D10-1056,0,0.224537,"nce is extracted for every sentence and evaluated on three metrics. Many-to-One (M-1) Many-to-one computes the most common true part-of-speech tag for each cluster. It then computes tagging accuracy as if the cluster were replaced with that tag. This metric is easily gamed by introducing a large number of clusters. One-to-One (1-1) One-to-One performs the same computation as Many-to-One but only one cluster is allowed to be assigned to a given tag. This prevents the gaming of M-1. V-Measure (VM) V-Measure is an F-measure which trades off conditional entropy between the clusters and gold tags. Christodoulopoulos et al. (2010) found VM is to be the most informative and consistent metric, in part because it is agnostic to the number of induced tags. 8 Data and Parameters To evaluate our approaches, we follow the existing literature and train and test on the full WSJ corpus. 1 This interpretation does not complicate the computation of forward-backward messages when running Baum-Welch, though it does, by design, break Markovian assumption about knowledge of the past. Initialization In addition to architectural choices we have to initialize all of our parameters. Word embeddings (and character embeddings in the CNN) ar"
W16-5907,D11-1059,0,0.0188252,"will fully neuralize the HMM and learn embeddings during the training of our generative model. There has also been recent work on by Rastogi et al. (2016) on neuralizing Finite-State Transducers. 3.2 Additional Comparisons While the main focus of our paper is the seamless extension of an unsupervised generative latent variable model with neural networks, for completeness we will also include comparisons to other techniques which do not adhere to the generative assumption. We include Brown clusters (Brown et al., 1992) as a baseline and two clustering techniques as stateof-the-art comparisons: Christodoulopoulos et al. (2011) and Yatbaz et al. (2012). Of particular interest to us is the work of Brown et al. (1992). Brown clusters group word types through a greedy agglomerative clustering according to their mutual information across the corpus based on bigram probabilities. Brown clusters do not account for a word’s membership in multiple syntactic classes, but are a very strong baseline for tag induction. It is possible our approach could be improved by augmenting our objective function to include mutual information computations or a bias towards a harder clustering. 4 Neural HMM The aforementioned training of an"
W16-5907,D11-1005,0,0.0365435,"elihood can outperform more complicated models that use expensive inference. In this work, we show how a single latent variable sequence model, Hidden Markov Models (HMMs), can be implemented with neural networks by simply optimizing the incomplete data likelihood. The key insight is to perform standard forward-backward inference to compute posteriors of latent variables and then backpropagate the posteriors through the networks to maximize the likelihood of the data. Using features in unsupervised learning has been a fruitful enterprise (Das and Petrov, 2011; BergKirkpatrick and Klein, 2010; Cohen et al., 2011) and attempts to combine HMMs and Neural Networks date back to 1991 (Bengio et al., 1991). Additionally, similarity metrics derived from word embeddings have also been shown to improve unsupervised word alignment (Songyot and Chiang, 2014). Interest in the interface of graphical models and neural networks has grown recently as new inference procedures have been proposed (Kingma and Welling, 2014; Johnson et al., 2016). Common to this work and ours is the use of neural networks to produce potentials. The approach presented here is easily applied to other latent variable models where inference i"
W16-5907,P11-1061,0,0.0262916,"ral network models trained to maximize the marginal likelihood can outperform more complicated models that use expensive inference. In this work, we show how a single latent variable sequence model, Hidden Markov Models (HMMs), can be implemented with neural networks by simply optimizing the incomplete data likelihood. The key insight is to perform standard forward-backward inference to compute posteriors of latent variables and then backpropagate the posteriors through the networks to maximize the likelihood of the data. Using features in unsupervised learning has been a fruitful enterprise (Das and Petrov, 2011; BergKirkpatrick and Klein, 2010; Cohen et al., 2011) and attempts to combine HMMs and Neural Networks date back to 1991 (Bengio et al., 1991). Additionally, similarity metrics derived from word embeddings have also been shown to improve unsupervised word alignment (Songyot and Chiang, 2014). Interest in the interface of graphical models and neural networks has grown recently as new inference procedures have been proposed (Kingma and Welling, 2014; Johnson et al., 2016). Common to this work and ours is the use of neural networks to produce potentials. The approach presented here is easily app"
W16-5907,D07-1031,0,0.0531705,"s are latent. The probability of a given sequence of observations x and latent variables z is given by multiplying transitions and emissions across all time steps (Eq. 6). Finding the optimal sequence of latent classes corresponds to computing an argmax over the values of z. p(x, z) = n+1 Y t=1 p(zt |zt−1 ) n Y p(xt |zt ) (6) t=1 Because our task is unsupervised we do not have a priori access to these distributions, but they can be estimated via Baum-Welch. The algorithm’s outline is provided in Algorithm 1. Training an HMM with EM is highly non-convex and likely to get stuck in local optima (Johnson, 2007). Despite this, sophisticated Bayesian smoothing leads to state-of-the-art performance (Blunsom and Cohn, 2011). Blunsom and Cohn (2011) further extend the HMM by augmenting its emission distributions with character models to capture morphological information and a tri-gram transition matrix which conditions on the previous two states. Recently, Lin et al. (2015) extended several models 65 Algorithm 1 Baum-Welch Algorithm Randomly Initialize distributions (θ) repeat Compute forward messages: ∀i,t αi (t) Compute backward messages: ∀i,t βi (t) Compute posteriors: p(zt = i |x, θ) ∝ αi (t)βi (t) p"
W16-5907,N15-1144,0,0.249275,"ed we do not have a priori access to these distributions, but they can be estimated via Baum-Welch. The algorithm’s outline is provided in Algorithm 1. Training an HMM with EM is highly non-convex and likely to get stuck in local optima (Johnson, 2007). Despite this, sophisticated Bayesian smoothing leads to state-of-the-art performance (Blunsom and Cohn, 2011). Blunsom and Cohn (2011) further extend the HMM by augmenting its emission distributions with character models to capture morphological information and a tri-gram transition matrix which conditions on the previous two states. Recently, Lin et al. (2015) extended several models 65 Algorithm 1 Baum-Welch Algorithm Randomly Initialize distributions (θ) repeat Compute forward messages: ∀i,t αi (t) Compute backward messages: ∀i,t βi (t) Compute posteriors: p(zt = i |x, θ) ∝ αi (t)βi (t) p(zt = i, zt+1 = j |x, θ) ∝ αi (t)p(zt+1 = j|zt = i) ×βj (t + 1)p(xt+1 |zt+1 = j) Update θ until Converged including the HMM to include pre-trained word embeddings learned by different skip-gram models. Our work will fully neuralize the HMM and learn embeddings during the training of our generative model. There has also been recent work on by Rastogi et al. (2016)"
W16-5907,D15-1166,0,0.0607821,"of simply maximizing performance on tag induction, a more subtle, but powerful contribution of this work may be its demonstration of the easy and effective nature of using neural networks with Bayesian models traditionally trained by EM. We hope this approach scales well to many other domains and tasks. Weight Initialization If we run our best model (NHMM+Conv+LSTM) with all the weights initialized from a uniform distribution U(−10−4 , 10−4 )3 we find a dramatic drop in V-Measure performance (61.7 vs 71.7 in Table 3). This is consistent with the common wisdom that unlike supervised learning (Luong et al., 2015), weight initialization is important to achieve good performance on unsupervised tasks. It is possible that performance could be further enhance via the popular technique of ensembling, would would allow for combining models which converged to different local optima. This work was supported by Contracts W911NF-151-0543 and HR0011-15-C-0115 with the US Defense Advanced Research Projects Agency (DARPA) and the Army Research Office (ARO). Additional thanks to Christos Christodoulopoulos. LSTM Layers And Dropout We find that dropout is important in training an unsupervised NHMM. References 3 We ch"
W16-5907,H94-1020,0,0.462941,"θ ln p(x, z |θ) is easy to evaluate, we can perform direct marginal likelihood optimization (Salakhutdinov et al., 2003). We do not address here the question of semi-supervised training, but believe the framework we present lends itself naturally to the incorporation of constraints or labeled data. Next, we demonstrate the application of this framework to HMMs in the service of part-of-speech tag induction. 3 Part-of-Speech Induction Part-of-speech tags encode morphosyntactic information about a language and are a fundamental tool in downstream NLP applications. In English, the Penn Treebank (Marcus et al., 1994) distinguishes 36 categories and punctuation. Tag induction is the task of taking raw text and both discovering these latent clusters and assigning them to words in situ. Classes can be very specific (e.g. six types of verbs in English) to their syntactic role. Example tags are shown in Table 1. In this example, board is labeled as a singular noun while Pierre Vinken is a singular proper noun. Two natural applications of induced tags are as the basis for grammar induction (Spitkovsky et al., 2011; Bisk et al., 2015) or to provide a syntactically informed, though unsupervised, source of word em"
W16-5907,N16-1076,0,0.0212823,"ly, Lin et al. (2015) extended several models 65 Algorithm 1 Baum-Welch Algorithm Randomly Initialize distributions (θ) repeat Compute forward messages: ∀i,t αi (t) Compute backward messages: ∀i,t βi (t) Compute posteriors: p(zt = i |x, θ) ∝ αi (t)βi (t) p(zt = i, zt+1 = j |x, θ) ∝ αi (t)p(zt+1 = j|zt = i) ×βj (t + 1)p(xt+1 |zt+1 = j) Update θ until Converged including the HMM to include pre-trained word embeddings learned by different skip-gram models. Our work will fully neuralize the HMM and learn embeddings during the training of our generative model. There has also been recent work on by Rastogi et al. (2016) on neuralizing Finite-State Transducers. 3.2 Additional Comparisons While the main focus of our paper is the seamless extension of an unsupervised generative latent variable model with neural networks, for completeness we will also include comparisons to other techniques which do not adhere to the generative assumption. We include Brown clusters (Brown et al., 1992) as a baseline and two clustering techniques as stateof-the-art comparisons: Christodoulopoulos et al. (2011) and Yatbaz et al. (2012). Of particular interest to us is the work of Brown et al. (1992). Brown clusters group word type"
W16-5907,D14-1197,0,0.021002,"the incomplete data likelihood. The key insight is to perform standard forward-backward inference to compute posteriors of latent variables and then backpropagate the posteriors through the networks to maximize the likelihood of the data. Using features in unsupervised learning has been a fruitful enterprise (Das and Petrov, 2011; BergKirkpatrick and Klein, 2010; Cohen et al., 2011) and attempts to combine HMMs and Neural Networks date back to 1991 (Bengio et al., 1991). Additionally, similarity metrics derived from word embeddings have also been shown to improve unsupervised word alignment (Songyot and Chiang, 2014). Interest in the interface of graphical models and neural networks has grown recently as new inference procedures have been proposed (Kingma and Welling, 2014; Johnson et al., 2016). Common to this work and ours is the use of neural networks to produce potentials. The approach presented here is easily applied to other latent variable models where inference is tractable and are typically trained with EM. We believe there are three important strengths: 1. Using a neural network to produce model probabilities allows for seamless integration of additional context not easily represented by conditi"
W16-5907,D11-1118,0,0.0732365,"a language and are a fundamental tool in downstream NLP applications. In English, the Penn Treebank (Marcus et al., 1994) distinguishes 36 categories and punctuation. Tag induction is the task of taking raw text and both discovering these latent clusters and assigning them to words in situ. Classes can be very specific (e.g. six types of verbs in English) to their syntactic role. Example tags are shown in Table 1. In this example, board is labeled as a singular noun while Pierre Vinken is a singular proper noun. Two natural applications of induced tags are as the basis for grammar induction (Spitkovsky et al., 2011; Bisk et al., 2015) or to provide a syntactically informed, though unsupervised, source of word embeddings. z1 zt 1 zt zt+1 zT x1 xt 1 xt xt+1 xT Figure 1: Pictorial representation of a Hidden Markov Model. Latent variable (zt ) transitions depend on the previous value (zt−1 ), and emit an observed word (xt ) at each time step. 3.1 The Hidden Markov Model A common model for this task, and our primary workhorse, is the Hidden Markov Model trained with the unsupervised message passing algorithm, BaumWelch (Welch, 2003). Model HMMs model a sentence by assuming that (a) every word token is genera"
W16-5907,N16-1036,1,0.81703,"ut also reduces the computational cost of comput2 This is the default parameter initialization in Torch. 68 M-1 1-1 VM Base HMM Brown 62.5 41.4 53.3 68.2 49.9 63.0 SOTA Architecture The first parameter is the number of hidden units. We chose 512 because it was the largest power of two we could fit in memory. When we extended our model to include the convolutional emission network, we only used 128 units, due to the intensive computation of Char-CNN over the whole vocabulary per minibatch. The second design choice was the number of LSTM layers. We used a three layer LSTM as it worked well for (Tran et al., 2016), and we applied dropout (Srivastava et al., 2014) over the vertical connections of the LSTMs (Pham et al., 2014) with a rate of 0.5. Finally, the maximum number of inner loop updates applied per batch is set to six. We train all the models for five epochs and perform gradient clipping whenever the gradient norm is greater than five. To determine when to stop applying the gradient during training we simply check when the log probability < 10−4 ) or if the maximum has converged ( new−old old number of inner loops has been reached. All optimization was done using Adam (Kingma and Ba, 2015) with"
W16-5907,D13-1140,1,0.635974,"ore data, would yield even greater gains. 11 Future Work In addition to parameter tuning and multilingual evaluation, the biggest open questions for our approach are the effects of additional data and augmenting the loss function. Neural networks are notoriously data hungry, indicating that while we achieve competitive results, it is possible our model will scale well when run with large corpora. This would likely require the use of techniques like NCE (Gutmann and Hyvärinen, 2010) which have been shown to be highly effective in related tasks like neural language modeling (Mnih and Teh, 2012; Vaswani et al., 2013). Secondly, despite focusing on ways to augment an HMM, Brown clustering and systems inspired by it perform very well. They aim to maximize mutual information rather than likelihood. It is possible that augmenting or constraining our loss will yield additional performance gains. Outside of simply maximizing performance on tag induction, a more subtle, but powerful contribution of this work may be its demonstration of the easy and effective nature of using neural networks with Bayesian models traditionally trained by EM. We hope this approach scales well to many other domains and tasks. Weight"
W16-5907,D12-1086,0,0.0771546,"Missing"
W16-6603,W13-2322,1,0.867187,"er Science University of Southern California {damghani,knight,ulf}@isi.edu Abstract We present a method for generating English sentences from Abstract Meaning Representation (AMR) graphs, exploiting a parallel corpus of AMRs and English sentences. We treat AMR-to-English generation as phrase-based machine translation (PBMT). We introduce a method that learns to linearize tokens of AMR graphs into an English-like order. Our linearization reduces the amount of distortion in PBMT and increases generation quality. We report a Bleu score of 26.8 on the standard AMR/English test set. 1 Introduction Banarescu et al. (2013) introduce Abstract Meaning Representation (AMR) graphs to represent sentence level semantics. Human annotators have created a dataset of more than 10, 000 AMR/English string pairs. AMRs are directed acyclic graphs, where leaves are labeled with concepts, internal nodes are labeled with variables representing instances of those concepts, and edges are labeled with roles that relate pairs of concepts. For instance, the sentence The boy wants to go is represented as: (w :instance-of want-01 :arg0 (b :instance-of boy) :arg1 (g :instance-of go-01 :arg0 b)) Colons discriminate roles from concepts."
W16-6603,J90-2002,0,0.403589,"re labeled with variables representing instances of those concepts, and edges are labeled with roles that relate pairs of concepts. For instance, the sentence The boy wants to go is represented as: (w :instance-of want-01 :arg0 (b :instance-of boy) :arg1 (g :instance-of go-01 :arg0 b)) Colons discriminate roles from concepts. In this paper, :instance-of is our way of writing the slash (/) found in the AMR corpus. 21 Because AMR and English are highly cognate, the AMR-to-English generation problem might seem similar to previous natural language generation (NLG) problems such as bag generation (Brown et al., 1990), restoring order to unordered dependency trees (Guo et al., 2011) or generation from logical form (Corston-Oliver et al., 2002). However, AMR’s deeper logic provides a serious challenge for English realization. AMR also abstracts away details of time, number, and voice, which must be inserted. Langkilde and Knight (1998) introduced Nitrogen, which used a precursor of AMR for generating English. Recently, Flanigan et al. (2016) presented the first trained AMR-to-English generator. They generate spanning trees from AMR graphs and apply tree-to-string transducers to the trees to generate English"
W16-6603,W02-2105,0,0.0751539,"of concepts. For instance, the sentence The boy wants to go is represented as: (w :instance-of want-01 :arg0 (b :instance-of boy) :arg1 (g :instance-of go-01 :arg0 b)) Colons discriminate roles from concepts. In this paper, :instance-of is our way of writing the slash (/) found in the AMR corpus. 21 Because AMR and English are highly cognate, the AMR-to-English generation problem might seem similar to previous natural language generation (NLG) problems such as bag generation (Brown et al., 1990), restoring order to unordered dependency trees (Guo et al., 2011) or generation from logical form (Corston-Oliver et al., 2002). However, AMR’s deeper logic provides a serious challenge for English realization. AMR also abstracts away details of time, number, and voice, which must be inserted. Langkilde and Knight (1998) introduced Nitrogen, which used a precursor of AMR for generating English. Recently, Flanigan et al. (2016) presented the first trained AMR-to-English generator. They generate spanning trees from AMR graphs and apply tree-to-string transducers to the trees to generate English. We attack AMR-to-English generation using the tools of phrase-based machine translation (PBMT). PBMT has already been applied"
W16-6603,N16-1087,0,0.660766,"AMR and English are highly cognate, the AMR-to-English generation problem might seem similar to previous natural language generation (NLG) problems such as bag generation (Brown et al., 1990), restoring order to unordered dependency trees (Guo et al., 2011) or generation from logical form (Corston-Oliver et al., 2002). However, AMR’s deeper logic provides a serious challenge for English realization. AMR also abstracts away details of time, number, and voice, which must be inserted. Langkilde and Knight (1998) introduced Nitrogen, which used a precursor of AMR for generating English. Recently, Flanigan et al. (2016) presented the first trained AMR-to-English generator. They generate spanning trees from AMR graphs and apply tree-to-string transducers to the trees to generate English. We attack AMR-to-English generation using the tools of phrase-based machine translation (PBMT). PBMT has already been applied to natural language generation from simple semantic structures (Mairesse et al., 2010), but deep semantic representations such as AMR are more challenging to deal with. PBMT expects strings for its source and target languages, so we cannot work with AMR graphs as input. Therefore, we develop a method t"
W16-6603,P07-2045,0,0.0303854,"t. Extend training data: We use special realization components for names, dates, and numbers found in the dev/test sets, adding their results to the training corpus. Linearize AMR graphs: We learn to convert AMR graphs into AMR strings in a way that linearized AMR tokens have an English-like order (Section 3). Clean AMR strings: We remove variables, quote marks, and sense tags from linearized AMRs. We also remove *-quantity and *-entity concepts, plus these roles: :op*, :snt*, :arg0, :arg1, :arg2, :name, :quant, :unit, :value, :year, :domain-of. Phrase-Based Machine Translation: We use Moses (Koehn et al., 2007) to train and tune a PBMT system on string/string training data. We then use this system to produce English realizations from linearized development and test AMRs. 22 (w :instance-of want-01 :arg0 (b :instance-of boy) :arg1 (g :instance-of go-01 :arg0 b)) yields “w :instance-of want-01 :arg0 b :instance-of boy :arg1 g :instance-of go-01 :arg0 b”. Of course, we are free to visit AMR sister nodes in any order. For instance, if we visit sisters in order (:arg0, :instance-of, :arg1), we get this string instead: “w :arg0 b :instance-of boy :instance-of want-01 :arg1 g :instance-of go-01 :arg0 b” ,"
W16-6603,P98-1116,1,0.138016,"om concepts. In this paper, :instance-of is our way of writing the slash (/) found in the AMR corpus. 21 Because AMR and English are highly cognate, the AMR-to-English generation problem might seem similar to previous natural language generation (NLG) problems such as bag generation (Brown et al., 1990), restoring order to unordered dependency trees (Guo et al., 2011) or generation from logical form (Corston-Oliver et al., 2002). However, AMR’s deeper logic provides a serious challenge for English realization. AMR also abstracts away details of time, number, and voice, which must be inserted. Langkilde and Knight (1998) introduced Nitrogen, which used a precursor of AMR for generating English. Recently, Flanigan et al. (2016) presented the first trained AMR-to-English generator. They generate spanning trees from AMR graphs and apply tree-to-string transducers to the trees to generate English. We attack AMR-to-English generation using the tools of phrase-based machine translation (PBMT). PBMT has already been applied to natural language generation from simple semantic structures (Mairesse et al., 2010), but deep semantic representations such as AMR are more challenging to deal with. PBMT expects strings for i"
W16-6603,D13-1049,0,0.0670241,":instance-of edge. If we drop this edge, we consider the rest of the edges as one group; otherwise, we divide them into two groups each appearing on one side of the :instance-of edge, using the second classifier. Next, we order the edges within each group. Let P(ri < rj ) be the probability—according to the third classifier—that ri precedes rj . For each edge ri , we assign it a “left-leaning” score, which is the product of all P(ri < rj ), for all j 6= i. We remove the edge with the highest left-leaning score. We then recursively process the remaining edges in the group. We were inspired by Lerner and Petrov (2013) to break the problem down this way. Because their dependencies are ordered, while our AMRs edges are not, we defined a different set of features and classifiers. 4 Experiments We use AMR/English data from the AMR 1.0 corpus,1 along with the provided train/development/test split (Table 1). We implement the method of Pourdamghani et al. (2014) to construct alignments for the training set. We train the linearization function introduced in Section 3 on the aligned training set and use it to re-linearize that training set, maintaining the alignment links. This gives us aligned string-to-string tra"
W16-6603,P10-1157,0,0.0554246,"Missing"
W16-6603,D14-1048,1,0.918662,"ical MT, which ignored much of the structure of Proceedings of The 9th International Natural Language Generation conference, pages 21–25, c Edinburgh, UK, September 5-8 2016. 2016 Association for Computational Linguistics Figure 1: AMR-to-English generation pipeline. language but nonetheless provided a strong baseline. 3 Linearization Figure 1 shows our pipeline for generating English from AMR. Our contributions are: When we linearize AMR, we would like—at a minimum—for semantically-related tokens to stay close together. A straightforward, pre-order depth first search (DFS) accomplishes this (Pourdamghani et al., 2014). For instance, linearizing 1. We present a strong baseline method for AMRto-English generation. 2. We introduce a method that learns to linearize AMR tokens into an order resembling English. 3. We obtain a Bleu score of 26.8 on the standard AMR/English test set, which is 4.9 points higher than previous work. 2 Method Given a set of AMR/English pairs, divided into train, development, and test sets, we follow these steps: Construct token-level alignments: We use the method proposed in (Pourdamghani et al., 2014) to construct alignments between AMR and English tokens in the training set. Extend"
W16-6603,C08-1038,0,\N,Missing
W16-6603,C98-1112,1,\N,Missing
W17-2315,W13-2004,0,0.0600022,"Missing"
W17-2315,W09-1404,0,0.0199083,"rly in biomedical texts. Our work departs from theirs in that we introduce a novel AMR path based heuristic to selectively sample the sentences obtained from distant supervision. (Liu et al., 2013; MacKinlay et al., 2013) learn subgraph patterns from the event annotations in the training data and cast the event detection as subgraph matching problem. Non-feature based approaches like graph kernels compare syntactic structures directly (Airola et al., 2008; Bunescu et al., 2005). Rule based methods that either use manually crafted rules or generate rules from training data (Cohen et al., 2009; Kaljurand et al., 2009; Kilicoglu and Bergler, 2011; Bui et al., 2013) have obtained high precision on these tasks. In our work, we take inspiration from the Turk Event Extraction System (TEES) (Bj¨orne and Salakoski, 2013) (the event extraction system for EVEX) that has consistently been the top performer in these series of tasks. They represent events using a graph format and break the event extraction task into separate multi-class classification tasks using SVM as their classifier. In our work we take a step further by making use of a deeper semantic representation as a starting point and identifying subgraphs"
W17-2315,W13-2003,0,0.0848906,"Missing"
W17-2315,W11-1827,0,0.0615738,"Our work departs from theirs in that we introduce a novel AMR path based heuristic to selectively sample the sentences obtained from distant supervision. (Liu et al., 2013; MacKinlay et al., 2013) learn subgraph patterns from the event annotations in the training data and cast the event detection as subgraph matching problem. Non-feature based approaches like graph kernels compare syntactic structures directly (Airola et al., 2008; Bunescu et al., 2005). Rule based methods that either use manually crafted rules or generate rules from training data (Cohen et al., 2009; Kaljurand et al., 2009; Kilicoglu and Bergler, 2011; Bui et al., 2013) have obtained high precision on these tasks. In our work, we take inspiration from the Turk Event Extraction System (TEES) (Bj¨orne and Salakoski, 2013) (the event extraction system for EVEX) that has consistently been the top performer in these series of tasks. They represent events using a graph format and break the event extraction task into separate multi-class classification tasks using SVM as their classifier. In our work we take a step further by making use of a deeper semantic representation as a starting point and identifying subgraphs in the AMR graph. AMR has bee"
W17-2315,W13-2014,0,0.103813,"Missing"
W17-2315,W09-1401,0,0.669066,"ntifies the event E1 and the subgraph rooted at induce-01 identifies the event E2 where Introduction For several years now, the biomedical community has been working towards the goal of creating a curated knowledge base of biomolecule entity interactions. The scientific literature in the biomedical domain runs to millions of articles and is an excellent source of such information. However, automatically extracting information from text is a challenge because natural language allows us to express the same information in several different ways. The series of Genia Event Extraction shared tasks (Kim et al., 2009, 2011, 2013, 2016) has resulted in various significant approaches to biomolecule event extraction spanning methods that use learnt patterns from annotated text (Bui et al., 2013) to machine learning methods (Bj¨orne and Salakoski, 2013) that use syntactic parses as features. In this work, we find that a semantic analysis of text that relies on Abstract Meaning Representations (Banarescu et al., 2013) is highly useful because it normalizes many lexical and syntactic variations in text. 1 E1 = phosphorylation of radixin; E2 = LPA induces E1. We hypothesize that an event structure is a subThis d"
W17-2315,W16-3003,0,0.0251471,"Missing"
W17-2315,W09-1407,0,0.0421696,"relations particularly in biomedical texts. Our work departs from theirs in that we introduce a novel AMR path based heuristic to selectively sample the sentences obtained from distant supervision. (Liu et al., 2013; MacKinlay et al., 2013) learn subgraph patterns from the event annotations in the training data and cast the event detection as subgraph matching problem. Non-feature based approaches like graph kernels compare syntactic structures directly (Airola et al., 2008; Bunescu et al., 2005). Rule based methods that either use manually crafted rules or generate rules from training data (Cohen et al., 2009; Kaljurand et al., 2009; Kilicoglu and Bergler, 2011; Bui et al., 2013) have obtained high precision on these tasks. In our work, we take inspiration from the Turk Event Extraction System (TEES) (Bj¨orne and Salakoski, 2013) (the event extraction system for EVEX) that has consistently been the top performer in these series of tasks. They represent events using a graph format and break the event extraction task into separate multi-class classification tasks using SVM as their classifier. In our work we take a step further by making use of a deeper semantic representation as a starting point an"
W17-2315,W11-1802,0,0.209454,"fying the relation between them. For e.g. in figure 1 the path {‘induce-01’, ‘arg0’, ‘LPA’} suggests that LPA is the cause of induce. We encode this path using word embeddings pre-trained on millions of biomedical text and develop two pipelined neural network models: (a) to identify the theme of an interaction; and (b) to identify the cause of the interaction, if there exists one. 2 2.1 AMR based event extraction model Task description The biomedical event extraction task in this work is adopted from the Genia Event Extraction subtask of the well-known BioNLP shared task ((Kim et al., 2009), (Kim et al., 2011), (Kim et al., 2013)). Table 2 shows a sample event annotation for the sentence in Figure 1. The protein annotations T1- T4 are given as starting points. The task is to identify the events E1-E4 with their interaction type and arguments. Table 1 describes the various event types and the arguments they accept. The first four event types require only unary theme argument. The binding event can take a variable number of theme arguments. The last four events take a theme argument and, when expressed, also a cause argument. Their theme or cause may in turn be another event, creating a nested event"
W17-2315,P14-1134,0,0.022913,"1 Figure 1: AMR with sample event annotations for sentence “This LPA-induced rapid phosphorylation of radixin was significantly suppressed in the presence of C3 toxin, a potent inhibitor of Rho” AMR is a rooted, directed acyclic graph (DAG) that captures the notion of who did what to whom in text, in a way that sentences that have the same basic meaning often have the same AMR. The nodes in the graph (also called concepts) map to words in the sentence and the edges map to relations between the words. In the recent past, there have been several efforts towards parsing a sentence into its AMR (Flanigan et al., 2014; Wang et al., 2015; Pust et al., 2015; May, 2016). AMR naturally captures hierarchical relations between entities in text making it favorable for complex event detection. For example, consider the following sentence from the biomedical literature: “This LPA-induced rapid phosphorylation of radixin was significantly suppressed in the presence of C3 toxin, a potent inhibitor of Rho”. Figure 1 shows its Abstract Meaning Representation (AMR). The subgraph rooted at phosphorylate-01 identifies the event E1 and the subgraph rooted at induce-01 identifies the event E2 where Introduction For several"
W17-2315,W13-2010,0,0.0148371,"column corresponds to the results of EVEX (Hakala et al., 2013) model on the 2013 test set. Certain notable numbers are emphasized and discussed under results 5.4. limited annotated data. Distant supervision techniques have been successfully used before for relation extraction (Mintz et al., 2009) in general domain. Recent work by (Liu et al., 2014) uses minimal supervision strategy for extracting relations particularly in biomedical texts. Our work departs from theirs in that we introduce a novel AMR path based heuristic to selectively sample the sentences obtained from distant supervision. (Liu et al., 2013; MacKinlay et al., 2013) learn subgraph patterns from the event annotations in the training data and cast the event detection as subgraph matching problem. Non-feature based approaches like graph kernels compare syntactic structures directly (Airola et al., 2008; Bunescu et al., 2005). Rule based methods that either use manually crafted rules or generate rules from training data (Cohen et al., 2009; Kaljurand et al., 2009; Kilicoglu and Bergler, 2011; Bui et al., 2013) have obtained high precision on these tasks. In our work, we take inspiration from the Turk Event Extraction System (TEES) (B"
W17-2315,W13-2005,0,0.0151187,"to the results of EVEX (Hakala et al., 2013) model on the 2013 test set. Certain notable numbers are emphasized and discussed under results 5.4. limited annotated data. Distant supervision techniques have been successfully used before for relation extraction (Mintz et al., 2009) in general domain. Recent work by (Liu et al., 2014) uses minimal supervision strategy for extracting relations particularly in biomedical texts. Our work departs from theirs in that we introduce a novel AMR path based heuristic to selectively sample the sentences obtained from distant supervision. (Liu et al., 2013; MacKinlay et al., 2013) learn subgraph patterns from the event annotations in the training data and cast the event detection as subgraph matching problem. Non-feature based approaches like graph kernels compare syntactic structures directly (Airola et al., 2008; Bunescu et al., 2005). Rule based methods that either use manually crafted rules or generate rules from training data (Cohen et al., 2009; Kaljurand et al., 2009; Kilicoglu and Bergler, 2011; Bui et al., 2013) have obtained high precision on these tasks. In our work, we take inspiration from the Turk Event Extraction System (TEES) (Bj¨orne and Salakoski, 201"
W17-2315,S16-1166,0,0.0218552,"This LPA-induced rapid phosphorylation of radixin was significantly suppressed in the presence of C3 toxin, a potent inhibitor of Rho” AMR is a rooted, directed acyclic graph (DAG) that captures the notion of who did what to whom in text, in a way that sentences that have the same basic meaning often have the same AMR. The nodes in the graph (also called concepts) map to words in the sentence and the edges map to relations between the words. In the recent past, there have been several efforts towards parsing a sentence into its AMR (Flanigan et al., 2014; Wang et al., 2015; Pust et al., 2015; May, 2016). AMR naturally captures hierarchical relations between entities in text making it favorable for complex event detection. For example, consider the following sentence from the biomedical literature: “This LPA-induced rapid phosphorylation of radixin was significantly suppressed in the presence of C3 toxin, a potent inhibitor of Rho”. Figure 1 shows its Abstract Meaning Representation (AMR). The subgraph rooted at phosphorylate-01 identifies the event E1 and the subgraph rooted at induce-01 identifies the event E2 where Introduction For several years now, the biomedical community has been worki"
W17-2315,P12-1076,0,0.0200641,"e may in turn be another event, creating a nested event (For e.g. event E2 in Table 2). Experimental results show that our model, although achieves a reasonable precision, suffers from low recall. Our third contribution is a distant supervision (Mintz et al., 2009) based approach to collect additional annotated training data. Distant supervision works on the assumption that given a known relation between two entities, a sentence containing the two entities is likely to express this relation and hence can serve as training data for that relation. Data gathered using such a method can be noisy (Takamatsu et al., 2012). Roth et al. (2013) have discussed several prior work that address this issue. In our work, we introduce a method based on AMR path heuristic 2.2 Model description We cast this event extraction problem as a subgraph identification problem. Given a sentence we 127 Event Type Gene expression Transcription Localization Protein catabolism ==[SVT-TOTAL]== Binding Phosphorylation Regulation Positive regulation Negative regulation ==[REG-TOTAL]== ==[ALL-TOTAL]== first obtain its AMR graph automatically using an AMR parser (Pust et al., 2015). Next, we identify protein nodes and interaction nodes in"
W17-2315,N15-1040,0,0.0124831,"ample event annotations for sentence “This LPA-induced rapid phosphorylation of radixin was significantly suppressed in the presence of C3 toxin, a potent inhibitor of Rho” AMR is a rooted, directed acyclic graph (DAG) that captures the notion of who did what to whom in text, in a way that sentences that have the same basic meaning often have the same AMR. The nodes in the graph (also called concepts) map to words in the sentence and the edges map to relations between the words. In the recent past, there have been several efforts towards parsing a sentence into its AMR (Flanigan et al., 2014; Wang et al., 2015; Pust et al., 2015; May, 2016). AMR naturally captures hierarchical relations between entities in text making it favorable for complex event detection. For example, consider the following sentence from the biomedical literature: “This LPA-induced rapid phosphorylation of radixin was significantly suppressed in the presence of C3 toxin, a potent inhibitor of Rho”. Figure 1 shows its Abstract Meaning Representation (AMR). The subgraph rooted at phosphorylate-01 identifies the event E1 and the subgraph rooted at induce-01 identifies the event E2 where Introduction For several years now, the biom"
W17-2315,P09-1113,0,0.228838,"1-E4 with their interaction type and arguments. Table 1 describes the various event types and the arguments they accept. The first four event types require only unary theme argument. The binding event can take a variable number of theme arguments. The last four events take a theme argument and, when expressed, also a cause argument. Their theme or cause may in turn be another event, creating a nested event (For e.g. event E2 in Table 2). Experimental results show that our model, although achieves a reasonable precision, suffers from low recall. Our third contribution is a distant supervision (Mintz et al., 2009) based approach to collect additional annotated training data. Distant supervision works on the assumption that given a known relation between two entities, a sentence containing the two entities is likely to express this relation and hence can serve as training data for that relation. Data gathered using such a method can be noisy (Takamatsu et al., 2012). Roth et al. (2013) have discussed several prior work that address this issue. In our work, we introduce a method based on AMR path heuristic 2.2 Model description We cast this event extraction problem as a subgraph identification problem. G"
W17-2315,N15-1119,1,0.71907,"our work, we take inspiration from the Turk Event Extraction System (TEES) (Bj¨orne and Salakoski, 2013) (the event extraction system for EVEX) that has consistently been the top performer in these series of tasks. They represent events using a graph format and break the event extraction task into separate multi-class classification tasks using SVM as their classifier. In our work we take a step further by making use of a deeper semantic representation as a starting point and identifying subgraphs in the AMR graph. AMR has been successfully used for deeper semantic tasks like entity linking (Pan et al., 2015) and abstractive summarization (Mihalcea et al., 2015). Work by Garg et al. (2015) is the first one to make use of AMR representation for extracting interactions from biomedical text. They use graph kernel methods to answer the binary question of whether a given AMR subgraph expresses an interaction or not. Our work departs from theirs in that they concentrate only on binary interactions whereas we use AMR to identify complex nested events. Also, our approach additionally makes use of distant supervision to cope with the problem of 7 Conclusion In this work, we show the effectiveness of using"
W17-2315,D15-1136,1,0.798412,"ions for sentence “This LPA-induced rapid phosphorylation of radixin was significantly suppressed in the presence of C3 toxin, a potent inhibitor of Rho” AMR is a rooted, directed acyclic graph (DAG) that captures the notion of who did what to whom in text, in a way that sentences that have the same basic meaning often have the same AMR. The nodes in the graph (also called concepts) map to words in the sentence and the edges map to relations between the words. In the recent past, there have been several efforts towards parsing a sentence into its AMR (Flanigan et al., 2014; Wang et al., 2015; Pust et al., 2015; May, 2016). AMR naturally captures hierarchical relations between entities in text making it favorable for complex event detection. For example, consider the following sentence from the biomedical literature: “This LPA-induced rapid phosphorylation of radixin was significantly suppressed in the presence of C3 toxin, a potent inhibitor of Rho”. Figure 1 shows its Abstract Meaning Representation (AMR). The subgraph rooted at phosphorylate-01 identifies the event E1 and the subgraph rooted at induce-01 identifies the event E2 where Introduction For several years now, the biomedical community ha"
W17-2315,W11-1807,0,0.0121183,"th between pk and pi via tj ; and the label is 1 if pk is the cause of the event em , 0 otherwise. 5.3 Results and Discussion 6 Related work The biomedical event extraction task described in this work was first introduced in the BioNLP Shared Task in 2009 (Kim et al., 2009). This task helped shift the focus of relation extraction efforts from identifying simple binary interactions to identifying complex nested events that better represent the biological interactions stated frequently in text. Existing approaches to this task include SVM (Bj¨orne and Salakoski, 2013) other ML based approaches (Riedel and McCallum, 2011; Miwa et al., 2010, 2012). Methods like LSTM model setup We implement our LSTM model using the lasagne library. For the first LSTM model, we use softmax as our non-linear function and optimize the cat132 Event Type Gene expression Transcription Localization Protein catabolism ==[SVT-TOTAL]== Binding Phosphorylation Regulation Positive regulation Negative regulation ==[REG-TOTAL]== ==[ALL-TOTAL]== Recall 66.33 55.10 36.55 73.33 57.82 27.61 49.21 16.30 25.98 23.17 21.81 44.42 LSTM Precision 66.55 28.57 63.72 84.62 60.86 25.94 53.75 29.18 35.16 30.50 31.61 51.01 F1 66.44 37.63 46.45 78.57 57.27"
W17-2315,W13-2002,0,\N,Missing
W17-2315,W13-2322,1,\N,Missing
W18-1505,P17-4008,1,0.80681,": An overview (upper) and an example (lower) of the proposed analyze-to-generate story framework. erate stories. Martin et al. (2017) train a recurrent encoder-decoder neural network (Sutskever et al., 2014) to predict the next event in the story. Despite significant progress in automatic story generation, there has been less emphasis on controllability: having a system takes human inputs and composes stories accordingly. With the recent successes on controllable generation of images (Chen et al., 2016; Siddharth et al., 2017; Lample et al., 2017), dialog responses (Wang et al., 2017), poems (Ghazvininejad et al., 2017), and different styles of text (Hu et al., 2017; Ficler and Goldberg, 2017; Shen et al., 2017; Fu et al., 2017). people would want to control a story generation system to produce interesting and personalized stories. This paper emphasizes the controllability aspect. We propose a completely data-driven approach towards controllable story generation by analyzing the existing story corpora. First, an analyzer extracts control factors from existing stories, and then a generator learns to generate stories according to the control factors. This creates an excellent interface for humans to interact:"
W18-1505,P17-4012,0,0.0146337,"ence of words ki = {ki,1 , ki,2 , . . . , ki,r } from each story xi . The ki s are ordered according to their order in the story. We adapt the RAKE algorithm (Rose et al., 2010) for keyword extraction, which builds document graphs and weights the importance of each word combining several word-level and graphlevel criteria. We extract the most important word from each sentence as the storyline. Generator. The generator for storylinecontrolled generation is also a conditional language model. Specifically, we employ the seq2seq model with attention (Bahdanau et al., 2014) implemented in OpenNMT (Klein et al., 2017). Specifically, the storyline words are encoded into vectors by a BiLSTM: hk = BiLST M (k) = → − ← − [ h k ; h k ], and the decoder generate each word according to the probability: att st = F (wt−1 , st−1 , ct ) r X ct = αtj hkj g() again denotes the softmax function, and V l denotes parameters that perform a linear transformation. F att () in Equation 5b denotes the computations of an LSTM-cell with attention mechanism, where the context vector ct is computed by an weighted summation of the storyline words vectors as in Equation 5c, and the weights are computed from some alignment function a("
W18-1505,D15-1167,0,0.0271169,"ending valence labels to facilitate the computation. Formally, we learn an embedding matrix E l to map each label lk into a vector: li = f v (xi ), where i indexes instances. Since there is no prior work on analyzing story ending valence, we build our own analyzer by collecting some annotations for story ending valence from Amazon Mechanical Turk (AMT) and building a supervised classifier. We employ an LSTM-based logistic regression classifier as it learns feature representations that capture long-term dependencies between the words, and has been shown efficient in text classification tasks (Tang et al., 2015). ekl = E l [lk ], 44 Agreement experiment Researcher vs. Researcher Turkers vs. Researcher Classifier vs. Turkers Always happyEnding where E l is a m × p matrix that maps each label (p of them) into a m-dimensional vector. The ending valence embeddings dimension are made the same as the word embedding dimension for simplicity. We add the ending valence as follows: p(wt |w1t−1 , l; θ) =  g(V F(el , F(wt−1 , ht−1 ))), t = s g(V F(wt−1 , ht−1 )), t = others valence. Labels are happyEnding, sadEnding, or cannotTell. The automatic classifier trained on 3980 turker annotated stories achieved much"
W18-1505,D17-1228,0,0.0622079,"Missing"
W18-1505,N16-1098,0,0.0786322,"→ − ← − [ h k ; h k ], and the decoder generate each word according to the probability: att st = F (wt−1 , st−1 , ct ) r X ct = αtj hkj g() again denotes the softmax function, and V l denotes parameters that perform a linear transformation. F att () in Equation 5b denotes the computations of an LSTM-cell with attention mechanism, where the context vector ct is computed by an weighted summation of the storyline words vectors as in Equation 5c, and the weights are computed from some alignment function a() as in Equation 5d. 3 Experimental Setup We conduct experiments on the ROCstories dataset (Mostafazadeh et al., 2016), which consists of 98,162 five-line stories for training, and 1871 stories each for the development and test sets. We treat the first four sentences of each story as the body and the last sentence as the ending. We build analyzers to annotate the ending valence and the storyline for every story, and train the two controlled generators with 98,162 annotated stories. 3.1 Ending Valence Annotation We conduct a three-stage data collection procedure to gather ending valence annotations and train a classifier to analyze the whole corpora. We classify all the stories into happyEnding, sadEnding, or"
W18-1505,W17-0911,0,0.0151733,"types of texts, such as novels, movies, and news articles. Automatic story generation efforts started as early as the 1970s with the TALE-SPIN system (Meehan, 1977). Early attempts in this field relied on symbolic planning (Meehan, 1977; Lebowitz, 1987; Turner, 1993; Bringsjord and Ferrucci, 1999; Perez and Sharples, 2001; Riedl and Young, 2010), casebased reasoning (Gervas et al., 2005), or generalizing knowledge from existing stories to assemble new ones (Swanson and Gordon, 2012; Li et al., 2013). In recent years, deep learning models are used to capture higher level structure in stories. Roemmele et al. (2017) use skip-thought vectors (Kiros et al., 2015) to encode sentences, and a Long Short-Term Memory (LSTM) network (Hochreiter and Schmidhuber, 1997) to gen1 Structured Control Factors Happy or sad endings. 43 Proceedings of the First Workshop on Storytelling, pages 43–49 c New Orleans, Louisiana, June 5, 2018. 2018 Association for Computational Linguistics Specifically, we use a bidirectional-LSTM to encode an input story into a sequence of vector representations hi = {hi,1 , hi,2 , · · · , hi,T }, where → − ← − hi = BiLST M (xi ) = [ h i ; h i ], T denotes the story length, [, :, ] denotes elem"
W18-4203,J14-2006,0,0.245424,"s 1967 book, “The Death Of A President”. the first character of 胡 锦 涛 can be decomposed into two characters 古月; and 古月 is a famous actor playing Mao Zedong. Both Mao and Hu acted as the former chairman of China. Angel historical book Air Force One terrorism 古月 (Gu Yue) drama 胡锦涛 (Hu Jintao) politics Table 4: Domain Mapping Examples for Other Purposes 3 System Encoding and Decoding 3.1 Cipher for Encoding Traditional text encryption techniques focus on alphabetic substitution or transposition based on lexical level (Franceschini and Mukherjee, 1996; Venkateswaran and Sundaram, 2010), synonyms (Chang and Clark, 2014) or image-adaptive public watermarking (Sun et al., 2008). Cipher systems can also be potentially developed and mutated to encode messages, including compare sophisticated ciphers such as historical ones (Knight et al., 2011) and simple mutations such as Leet10 and Martian script11 . Further strategies need to be developed to make them easy and fun for target human comprehension and widespread adoption, and difficult for automatic decoding. 3.2 Natural Language Generation for Encoding Knight and Hatzivassiloglou (1995) and Langkilde and Knight (1998) lay the foundation for statistical natural"
W18-4203,N15-1021,1,0.826946,"rical ones (Knight et al., 2011) and simple mutations such as Leet10 and Martian script11 . Further strategies need to be developed to make them easy and fun for target human comprehension and widespread adoption, and difficult for automatic decoding. 3.2 Natural Language Generation for Encoding Knight and Hatzivassiloglou (1995) and Langkilde and Knight (1998) lay the foundation for statistical natural language generation, and they recently apply these techniques to the generation of creative language: (1) Portmanteau neologism creation. They fuse existing English words to create novel ones (Deri and Knight, 2015). The aim is to create an amusing new form that is understandable by a reader, e.g., frenemy for an entity that is both friend and enemy. Doing this well requires fusion at the phonetic level followed by an appropriate choice of spelling. Machines cannot yet process such created neologisms. This portmanteau generation approach (Deri and Knight, 2015) can be used to encode other types of neologisms 10 11 https://en.wikipedia.org/wiki/Leet https://en.wikipedia.org/wiki/Martian_language 28 in both English and Chinese. We observe the words embedded are usually semantically and phonetically compati"
W18-4203,N15-1180,1,0.862423,"hography, so the coded messages are more easily remembered and adopted. An encoded message is as follows. • Chinese: 明天下午三点到鼓楼大街集合。 • English translation: Let’s gather at the Bell Tower Street at 3pm tomorrow. • Pronounce English in Chinese phonetic system (pinyin): Laici galete ate de beier taoer sijute aite teli piaimu temoluo. • Code by spelling out pinyin: 来此盖乐特爱特得贝尔套儿思聚特爱特特例皮埃姆特摩罗。 (3) Poetry passwords. (Greene et al., 2010) build the first statistical machine translation system to translate poetry. They subsequently apply poetry generation techniques to the problem of password security (Ghazvininejad and Knight, 2015). In this work, the machine first assigns a random 60-bit password to a user. Because the user cannot remember a random sequence of 0’s and 1’s, the machine converts the bit sequence into a more memorable iambic tetrameter couplet (e.g., The legendary Japanese // subsidiaries overseas). The mapping between bit sequences and poems is reversible, so the security of the randomly-assigned password is maintained. Brennan et al. (2012) propose three methods to create adversarial passages: obfuscation, imitation, and translation. They find manual circumvention methods work very well, while automated"
W18-4203,D10-1051,1,0.77301,"ion (MT) to make the results more challenging to decode because it is difficult to recover from MT errors, and (2) polish the output by using common words or phrases in the user’s own orthography, so the coded messages are more easily remembered and adopted. An encoded message is as follows. • Chinese: 明天下午三点到鼓楼大街集合。 • English translation: Let’s gather at the Bell Tower Street at 3pm tomorrow. • Pronounce English in Chinese phonetic system (pinyin): Laici galete ate de beier taoer sijute aite teli piaimu temoluo. • Code by spelling out pinyin: 来此盖乐特爱特得贝尔套儿思聚特爱特特例皮埃姆特摩罗。 (3) Poetry passwords. (Greene et al., 2010) build the first statistical machine translation system to translate poetry. They subsequently apply poetry generation techniques to the problem of password security (Ghazvininejad and Knight, 2015). In this work, the machine first assigns a random 60-bit password to a user. Because the user cannot remember a random sequence of 0’s and 1’s, the machine converts the bit sequence into a more memorable iambic tetrameter couplet (e.g., The legendary Japanese // subsidiaries overseas). The mapping between bit sequences and poems is reversible, so the security of the randomly-assigned password is ma"
W18-4203,W13-0908,0,0.0374128,"Missing"
W18-4203,P13-1107,1,0.836828,"between encoding and decoding objectives. This opens an unexplored area of coded language processing. 1 2 https://freedomhouse.org/report/freedom-net/freedom-net-2016 https://en.wikipedia.org/wiki/Internet_censorship_in_China 23 Proceedings of the First Workshop on Natural Language Processing for Internet Freedom, pages 23–33 Santa Fe, New Mexico, USA, August 20, 2018. 2 Human Encoding 2.1 Categories of Coded Language Netizens create and use obfuscated language for a variety of purposes. Discussing sensitive information and evading censorship. Code words widely exist in Chinese social media (Huang et al., 2013; Zhang et al., 2014; Zhang et al., 2015). Bamman et al. (2012) automatically discover politically sensitive terms from Chinese tweets based on message deletion analysis. When Chinese netizens talk about the former politician 周永康 (Zhou Yongkang), they use a coded word 康师傅 (Master Kang), a brand of instant noodles whose Chinese spellings share one character 康 (kang). The Enron emails3 also include many code words, such as dinosaur referring to an illegal stock. Masking illegal activity. In the dark web for human trafficking, arms dealing, and drug dealing, research chemical or RC is euphemistic"
W18-4203,jabbari-etal-2008-using,0,0.0134371,"The mapping between bit sequences and poems is reversible, so the security of the randomly-assigned password is maintained. Brennan et al. (2012) propose three methods to create adversarial passages: obfuscation, imitation, and translation. They find manual circumvention methods work very well, while automated translation methods are not effective. Potash et al. (2015) develop a GhostWriter system that can take a given artist’s rap lyrics and generate similar yet unique lyrics. Other recent work detects word obfuscation in adversarial communication (Roussinov et al., 2007; Fong et al., 2008; Jabbari et al., 2008; Deshmukh et al., 2014; Agarwal and Sureka, 2015) using existing commonsense KBs such as ConceptNet (Agarwal and Sureka, 2015). 3.3 Entity Encoding and Decoding Huang et al. (2013), Zhang et al. (2014) and Zhang et al. (2015) study a problem of encoding and decoding entity morph, which is a special case of coded name alias to hide the original entities for expressing strong sentiment or evading censorship in Chinese social media. They propose a variety of novel approaches to automatically encode proper and interesting morphs (Zhang et al., 2014), including Phonetic Substitution, Spelling Deco"
W18-4203,P95-1034,1,0.591728,"l level (Franceschini and Mukherjee, 1996; Venkateswaran and Sundaram, 2010), synonyms (Chang and Clark, 2014) or image-adaptive public watermarking (Sun et al., 2008). Cipher systems can also be potentially developed and mutated to encode messages, including compare sophisticated ciphers such as historical ones (Knight et al., 2011) and simple mutations such as Leet10 and Martian script11 . Further strategies need to be developed to make them easy and fun for target human comprehension and widespread adoption, and difficult for automatic decoding. 3.2 Natural Language Generation for Encoding Knight and Hatzivassiloglou (1995) and Langkilde and Knight (1998) lay the foundation for statistical natural language generation, and they recently apply these techniques to the generation of creative language: (1) Portmanteau neologism creation. They fuse existing English words to create novel ones (Deri and Knight, 2015). The aim is to create an amusing new form that is understandable by a reader, e.g., frenemy for an entity that is both friend and enemy. Doing this well requires fusion at the phonetic level followed by an appropriate choice of spelling. Machines cannot yet process such created neologisms. This portmanteau"
W18-4203,W11-1202,1,0.80527,"l book Air Force One terrorism 古月 (Gu Yue) drama 胡锦涛 (Hu Jintao) politics Table 4: Domain Mapping Examples for Other Purposes 3 System Encoding and Decoding 3.1 Cipher for Encoding Traditional text encryption techniques focus on alphabetic substitution or transposition based on lexical level (Franceschini and Mukherjee, 1996; Venkateswaran and Sundaram, 2010), synonyms (Chang and Clark, 2014) or image-adaptive public watermarking (Sun et al., 2008). Cipher systems can also be potentially developed and mutated to encode messages, including compare sophisticated ciphers such as historical ones (Knight et al., 2011) and simple mutations such as Leet10 and Martian script11 . Further strategies need to be developed to make them easy and fun for target human comprehension and widespread adoption, and difficult for automatic decoding. 3.2 Natural Language Generation for Encoding Knight and Hatzivassiloglou (1995) and Langkilde and Knight (1998) lay the foundation for statistical natural language generation, and they recently apply these techniques to the generation of creative language: (1) Portmanteau neologism creation. They fuse existing English words to create novel ones (Deri and Knight, 2015). The aim"
W18-4203,P98-1116,1,0.170803,"96; Venkateswaran and Sundaram, 2010), synonyms (Chang and Clark, 2014) or image-adaptive public watermarking (Sun et al., 2008). Cipher systems can also be potentially developed and mutated to encode messages, including compare sophisticated ciphers such as historical ones (Knight et al., 2011) and simple mutations such as Leet10 and Martian script11 . Further strategies need to be developed to make them easy and fun for target human comprehension and widespread adoption, and difficult for automatic decoding. 3.2 Natural Language Generation for Encoding Knight and Hatzivassiloglou (1995) and Langkilde and Knight (1998) lay the foundation for statistical natural language generation, and they recently apply these techniques to the generation of creative language: (1) Portmanteau neologism creation. They fuse existing English words to create novel ones (Deri and Knight, 2015). The aim is to create an amusing new form that is understandable by a reader, e.g., frenemy for an entity that is both friend and enemy. Doing this well requires fusion at the phonetic level followed by an appropriate choice of spelling. Machines cannot yet process such created neologisms. This portmanteau generation approach (Deri and Kn"
W18-4203,N07-1025,0,0.117845,"Missing"
W18-4203,N15-1119,1,0.878102,"Missing"
W18-4203,D15-1221,0,0.0296214,"ns a random 60-bit password to a user. Because the user cannot remember a random sequence of 0’s and 1’s, the machine converts the bit sequence into a more memorable iambic tetrameter couplet (e.g., The legendary Japanese // subsidiaries overseas). The mapping between bit sequences and poems is reversible, so the security of the randomly-assigned password is maintained. Brennan et al. (2012) propose three methods to create adversarial passages: obfuscation, imitation, and translation. They find manual circumvention methods work very well, while automated translation methods are not effective. Potash et al. (2015) develop a GhostWriter system that can take a given artist’s rap lyrics and generate similar yet unique lyrics. Other recent work detects word obfuscation in adversarial communication (Roussinov et al., 2007; Fong et al., 2008; Jabbari et al., 2008; Deshmukh et al., 2014; Agarwal and Sureka, 2015) using existing commonsense KBs such as ConceptNet (Agarwal and Sureka, 2015). 3.3 Entity Encoding and Decoding Huang et al. (2013), Zhang et al. (2014) and Zhang et al. (2015) study a problem of encoding and decoding entity morph, which is a special case of coded name alias to hide the original entit"
W18-4203,P14-2046,1,0.839502,"11 https://en.wikipedia.org/wiki/Leet https://en.wikipedia.org/wiki/Martian_language 28 in both English and Chinese. We observe the words embedded are usually semantically and phonetically compatible, such as “frenemy (friend + enemy)”. They are also terse, representative, expressive, interesting and easy to remember. For example, the following short phrases are created to refer to good men and bad men respectively: “暖男 (warm + man)” and “渣男 (dirt + man)”. (2) Dynamic phrasebooks. Tourists frequently carry phonetic phrasebooks that allow them to say things in a language they do not know. In (Shi et al., 2014), we developed a system that accepts text entered by a user (e.g., Chinese), translates the text (e.g., into English), then converts the translation into a phonetic spelling in the user’s own orthography (e.g., Chinese). This system let users say anything they want, even if it is not in any phrasebook, using their own voice. For example, if a Chinese visitor to the United States wants to say 早上好 (meaning Good morning), they enter this phrase, and the system tells them to instead say 古德莫宁 (pronounced gu-de-mo-ning). This user is not required to know any English, but can still be understood by m"
W18-4203,W13-0906,0,0.0390039,"Missing"
W18-4203,P95-1026,0,0.202486,"Missing"
W18-4203,P14-2115,1,0.664896,"d decoding objectives. This opens an unexplored area of coded language processing. 1 2 https://freedomhouse.org/report/freedom-net/freedom-net-2016 https://en.wikipedia.org/wiki/Internet_censorship_in_China 23 Proceedings of the First Workshop on Natural Language Processing for Internet Freedom, pages 23–33 Santa Fe, New Mexico, USA, August 20, 2018. 2 Human Encoding 2.1 Categories of Coded Language Netizens create and use obfuscated language for a variety of purposes. Discussing sensitive information and evading censorship. Code words widely exist in Chinese social media (Huang et al., 2013; Zhang et al., 2014; Zhang et al., 2015). Bamman et al. (2012) automatically discover politically sensitive terms from Chinese tweets based on message deletion analysis. When Chinese netizens talk about the former politician 周永康 (Zhou Yongkang), they use a coded word 康师傅 (Master Kang), a brand of instant noodles whose Chinese spellings share one character 康 (kang). The Enron emails3 also include many code words, such as dinosaur referring to an illegal stock. Masking illegal activity. In the dark web for human trafficking, arms dealing, and drug dealing, research chemical or RC is euphemistically used to discuss"
W18-4203,P15-1057,1,0.737244,"s. This opens an unexplored area of coded language processing. 1 2 https://freedomhouse.org/report/freedom-net/freedom-net-2016 https://en.wikipedia.org/wiki/Internet_censorship_in_China 23 Proceedings of the First Workshop on Natural Language Processing for Internet Freedom, pages 23–33 Santa Fe, New Mexico, USA, August 20, 2018. 2 Human Encoding 2.1 Categories of Coded Language Netizens create and use obfuscated language for a variety of purposes. Discussing sensitive information and evading censorship. Code words widely exist in Chinese social media (Huang et al., 2013; Zhang et al., 2014; Zhang et al., 2015). Bamman et al. (2012) automatically discover politically sensitive terms from Chinese tweets based on message deletion analysis. When Chinese netizens talk about the former politician 周永康 (Zhou Yongkang), they use a coded word 康师傅 (Master Kang), a brand of instant noodles whose Chinese spellings share one character 康 (kang). The Enron emails3 also include many code words, such as dinosaur referring to an illegal stock. Masking illegal activity. In the dark web for human trafficking, arms dealing, and drug dealing, research chemical or RC is euphemistically used to discuss psychoactive chemica"
W18-6502,D10-1049,0,0.138916,"Missing"
W18-6502,P15-1034,0,0.0613092,"Missing"
W18-6502,W13-2322,1,0.842586,"Missing"
W18-6502,N16-1087,0,0.0704839,"Missing"
W18-6502,D16-1006,0,0.0363935,"Missing"
W18-6502,P17-1017,0,0.205356,"te content into slots (Kukich, 1983; Cawsey et al., 1997; Angeli et al., 2010; Duma and Klein, 2013; Konstas and Lapata, 2013a; Flanigan et al., 2016a). These methods can generate high-quality descriptions but heavily rely on information redundancy to create templates. The second category is to directly generate a sequence of words using language model (Belz, 2008; Chen and Mooney, 2008; Liang et al., 2009; Angeli et al., 2010; Konstas and Lapata, 2012a,b, 2013a,b; Mahapatra et al., 2016) or deep neural networks (Sutskever et al., 2011; Wen et al., 2015; Kiddon et al., 2016; Mei et al., 2016; Gardent et al., 2017b; Wiseman et al., 2017; Wang et al., 2018; Song et al., 2018). Several studies (Lebret et al., 2016; Chisholm et al., 2017; Kaffee et al., 2018a,b; Liu et al., 2018; Sha et al., 2018) generate a person’s biography from an input structure, which are closely related to our task. However, instead of modeling the input structure as a sequence of facts and generating one sentence only, we introduce a table position self-attention, inspired from structure attention (Lin et al., 2017; Kim et al., 2017; Vaswani et al., 2017; Shen et al., 2018a,b), to capture the dependencies among facts and generate"
W18-6502,E17-1060,0,0.0844058,"Missing"
W18-6502,P16-1154,0,0.0527383,"sition-aware representation of each slot type and value, and update their context vectors L∗t and L∗v in Equation 1 as: L∗s = L∗v = 2.3 Xn i=1 Xn i=1 where P (y t ) is the prediction probability of the ground truth token y. λ is a hyperparameter. 3 αit s∗i 3.1 αit v∗i Traditional sequence-to-sequence models predict a target sequence by only selecting words from a vocabulary with a fixed size. However, in our task, we regard the slot value as a single information unit. Therefore, there is a certain amount of outof-vocabulary (OOV) words during the test phase. Inspired by the pointer-generator (Gu et al., 2016; See et al., 2017), which is designed to automatically locate particular source words and directly copy them into the target sequence, we design a structure-aware generator as follows. We first obtain a source attention distribution of all unique input slot values. Since one particular slot value may occur in the structure input for many times, we aggregate the attention weights for each unique slot value vj from αt and obtain its agj by gregated source attention distribution Psource X Data Using person and animal entities as case studies, we create a new dataset based on Wikipedia dump (2018"
W18-6502,P16-1014,0,0.0533797,"Missing"
W18-6502,W14-3348,0,0.158359,"Missing"
W18-6502,N18-2101,0,0.0901125,"Missing"
W18-6502,W13-0108,0,0.0911938,"Missing"
W18-6502,D11-1142,0,0.0918571,"Missing"
W18-6502,D16-1032,0,0.0374598,"Missing"
W18-6502,P15-1002,0,0.0745402,"Missing"
W18-6502,P12-1039,0,0.124605,"Missing"
W18-6502,P17-2100,0,0.0574529,"Missing"
W18-6502,N12-1093,0,0.0734882,"Missing"
W18-6502,W16-6624,0,0.0433841,"Missing"
W18-6502,D13-1157,0,0.0549744,"Missing"
W18-6502,N16-1086,0,0.0808646,"Missing"
W18-6502,P83-1022,0,0.671208,"Missing"
W18-6502,D16-1128,0,0.20676,"Missing"
W18-6502,D12-1094,0,0.0232392,"Missing"
W18-6502,P09-1011,0,0.313582,"Missing"
W18-6502,P02-1040,0,0.101095,"Missing"
W18-6502,W16-6603,1,0.886983,"Missing"
W18-6502,W04-1013,0,0.109953,"Missing"
W18-6502,P17-1099,0,0.67687,"equences and applies a sequence to sequence (seq2seq) framework (Cho et al., 2014) for generation. However, the task of describing structured knowledge is fundamentally different from creative writing, because we need to cover the knowledge elements contained in the input KB, and the goal of generation is mainly to clearly describe the semantic connections among these knowledge elements in an accurate and coherent way. The seq2seq model fails to capture such connections and tends to generate wrong information (e.g., Thailand in Table 2). To address this challenge, we choose a pointer network (See et al., 2017) to copy slot values directly from the 11 resentations of these fields as li = [si , vi , r1 , rˆ1 ], and obtain L = [l1 , l2 , ..., ln ]. We attempted to apply the average of L as the representation for the input KB. However, such flat representation vectors fail to capture the structured contextual information in the entire KB. Therefore, we apply a bi-directional Gated Recurrent Unit (GRU) encoder (Cho et al., 2014) on L to produce the encoder hidden states H = [h1 , h2 , ..., hn ], where hi is a hidden state for li . Decoder with Slot-aware Attention The decoder is a forward GRU network wi"
W18-6502,Q18-1005,0,0.0276597,"7) (3,6) (4,5) (5,4) (5,4) (5,4) Slot Value Table Position Attention pgen Table Position Vocabulary Distribution Final Distribution Bidirectional GRU &lt;SOS&gt; Zsolt Laczkó ( born Figure 1: KB-to-Language Generation Model Overview 2.2 a sequence of row index embeddings R = 0 0 0 [r1 , r2 , ..., rn ] with random initialization, where 0 ri = [ri ; ˆri ]. We model the inter-dependencies among slots as a latent structure, where for each position i we assume it has a latent in-link and an out-link to denote where it is linked to or from. This assumption is similar to the structure attention applied in Liu and Lapata (2018), which assumes each word within a sentence can be a parent node or a child node in a latent tree structure. For each pair of slots i and j, we compute the attention score fij as follows: Table Position Self-attention Although the sequence-to-sequence attention model takes into account the information of input triples, it still encodes the structured knowledge as sequential facts while ignoring the correlations between facts. In our task, multiple interdependent slots should be described within one sentence. For example, in Table 1, the sport team Israel women’s national football team should b"
W18-6502,D18-1435,1,0.872665,"Missing"
W18-6502,I17-1086,1,0.896453,"Missing"
W18-6502,P18-1150,0,0.0509342,"Missing"
W18-6502,P18-2042,1,0.903298,"Missing"
W18-6502,D15-1199,0,0.069351,"Missing"
W18-6502,D18-1433,1,0.89042,"Missing"
W18-6502,P10-1013,0,0.052207,"Missing"
W18-6502,N13-1107,0,0.0347677,"Missing"
W98-1005,P97-1017,1,0.665899,"effects. W e present results and examples of use in an Arabic-to-English machine translator. 1 Introduction Translators must deal with many problems, and one of the most frequent is translating proper names and technical terms. For language pairs like Spanish/English, this presents no great challenge: a phrase like Antonio Gil usually gets translated as Antonio Gil. However, the situation is more complicated for language pairs that employ very different alphabets and sound systems, such as Japanese/English and Arabic/English. Phonetic translation across these pairs is called transliteration. (Knight and Graehl, 1997) present a computational treatment of Japanese/English transliteration, which we adapt here to the case in Arabic. Arabic text, like Japanese, frequently contains foreign names and technical terms that are translated phonetically. Here are some examples from newspaper text: a Jim Leighton oA (j ym 1 !ytwn) Wall Street (wwl stryt) It is not trivial to write an algorithm for turning English letter sequences into Arabic letter sequences, and indeed, two human translators will often produce different Arabic versions of the same English phrase. There are many complexity-inducing factors. Some Engli"
W98-1426,P95-1034,1,0.459019,"ent path through the lattice. 4 The word lattice encodes alternative English expressions for the input when the symbolic knowledge is unavailable English string (whether from the input, or from the knowledge bases) for making realization decisions. The Nitrogen statistical exFigure h Combining Symbolic and Statistitractor ranks these alternative s using bigram (adjacent word cal Knowledge in a Natural Language Generpairs) and unigram (single word) statistics Collected from two ator (Knight and Hatzivassil0glou, 1995). years of the Wall Street Journal. The extraction algorithm is presented in (Knight and Hatzivassiloglou, 1995). In essence, Nitrogen uses ngram statistics to robustly make a wide variety of decisions, from tense to word choice• to syntactic subcategorization, that traditionally are handled either with defaults (e.g., assume present tense, use the alphabetically-first synonyms, use nominal arguments), explicit input specification, or by using deep, detailed knowledge bases. However, in scaling up a generator system, these methods become unsatisfactory. Defaults are too rigid and limit quality; detailed input specs are difficult or complex to construct, or m ~ ' be unavailable; and :: 248 ! !i i I I 1 I"
W98-1426,H94-1020,0,\N,Missing
W98-1426,P98-1116,1,\N,Missing
W98-1426,C98-1112,1,\N,Missing
W99-0906,P13-5003,1,0.892774,"liar script encodes a known language. The decipherment of a brief document or inscription is driven by data about the spoken language. We consider which scripts are easy or hard to decipher, how much data is required, and whether the techniques are robust against language change over time. 1 This situation has arisen in many famous cases of decipherment--for example, in the Linear B documents from Crete (which turned out to be a ""non-Greek"" script for writing ancient Greek) and in the Mayan documents from Mesoamerica. Both of these cases lay unsolved until the latter half of the 20th century (Chadwick, 1958; Coe, 1993). In computational linguistic terms, this decipherment task is not really translation, but rather text-to-speech conversion. The goal of the decipherment is to ""make the text speak,"" after which it can be interpreted, translated, etc. Of course, even after an ancient document is phonetically rendered, it will still contain many unknown words and strange constructions. Making the text speak is therefore only the beginning of the story, but it is a crucial step. Unfortunately, current text-to-speech systems cannot be applied directly, because they require up front a clearly specified"
W99-0906,J98-4003,1,\N,Missing
