2021.socialnlp-1.3,A Case Study of In-House Competition for Ranking Constructive Comments in a News Service,2021,-1,-1,10,0.789854,1077,hayato kobayashi,Proceedings of the Ninth International Workshop on Natural Language Processing for Social Media,0,"Ranking the user comments posted on a news article is important for online news services because comment visibility directly affects the user experience. Research on ranking comments with different metrics to measure the comment quality has shown {``}constructiveness{''} used in argument analysis is promising from a practical standpoint. In this paper, we report a case study in which this constructiveness is examined in the real world. Specifically, we examine an in-house competition to improve the performance of ranking constructive comments and demonstrate the effectiveness of the best obtained model for a commercial service."
2021.naacl-main.127,Improving Neural {RST} Parsing Model with Silver Agreement Subtrees,2021,-1,-1,4,1,3631,naoki kobayashi,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Most of the previous Rhetorical Structure Theory (RST) parsing methods are based on supervised learning such as neural networks, that require an annotated corpus of sufficient size and quality. However, the RST Discourse Treebank (RST-DT), the benchmark corpus for RST parsing in English, is small due to the costly annotation of RST trees. The lack of large annotated training data causes poor performance especially in relation labeling. Therefore, we propose a method for improving neural RST parsing models by exploiting silver data, i.e., automatically annotated data. We create large-scale silver data from an unlabeled corpus by using a state-of-the-art RST parser. To obtain high-quality silver data, we extract agreement subtrees from RST trees for documents built using the RST parsers. We then pre-train a neural RST parser with the obtained silver data and fine-tune it on the RST-DT. Experimental results show that our method achieved the best micro-F1 scores for Nuclearity and Relation at 75.0 and 63.2, respectively. Furthermore, we obtained a remarkable gain in the Relation score, 3.0 points, against the previous state-of-the-art parser."
2021.naacl-industry.32,An Empirical Study of Generating Texts for Search Engine Advertising,2021,-1,-1,4,1,3633,hidetaka kamigaito,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers,0,"Although there are many studies on neural language generation (NLG), few trials are put into the real world, especially in the advertising domain. Generating ads with NLG models can help copywriters in their creation. However, few studies have adequately evaluated the effect of generated ads with actual serving included because it requires a large amount of training data and a particular environment. In this paper, we demonstrate a practical use case of generating ad-text with an NLG model. Specially, we show how to improve the ads{'} impact, deploy models to a product, and evaluate the generated ads."
2021.findings-acl.152,Fusing Label Embedding into {BERT}: An Efficient Improvement for Text Classification,2021,-1,-1,5,0,7869,yijin xiong,Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,0,None
2021.emnlp-main.188,A Language Model-based Generative Classifier for Sentence-level Discourse Parsing,2021,-1,-1,3,0,7842,ying zhang,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"Discourse segmentation and sentence-level discourse parsing play important roles for various NLP tasks to consider textual coherence. Despite recent achievements in both tasks, there is still room for improvement due to the scarcity of labeled data. To solve the problem, we propose a language model-based generative classifier (LMGC) for using more information from labels by treating the labels as an input while enhancing label representations by embedding descriptions for each label. Moreover, since this enables LMGC to make ready the representations for labels, unseen in the pre-training step, we can effectively use a pre-trained language model in LMGC. Experimental results on the RST-DT dataset show that our LMGC achieved the state-of-the-art F1 score of 96.72 in discourse segmentation. It further achieved the state-of-the-art relation F1 scores of 84.69 with gold EDU boundaries and 81.18 with automatically segmented boundaries, respectively, in sentence-level discourse parsing."
2021.emnlp-main.330,Considering Nested Tree Structure in Sentence Extractive Summarization with Pre-trained Transformer,2021,-1,-1,4,1,9388,jingun kwon,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"Sentence extractive summarization shortens a document by selecting sentences for a summary while preserving its important contents. However, constructing a coherent and informative summary is difficult using a pre-trained BERT-based encoder since it is not explicitly trained for representing the information of sentences in a document. We propose a nested tree-based extractive summarization model on RoBERTa (NeRoBERTa), where nested tree structures consist of syntactic and discourse trees in a given document. Experimental results on the CNN/DailyMail dataset showed that NeRoBERTa outperforms baseline models in ROUGE. Human evaluation results also showed that NeRoBERTa achieves significantly better scores than the baselines in terms of coherence and yields comparable scores to the state-of-the-art models."
2021.eacl-main.125,Generating Weather Comments from Meteorological Simulations,2021,-1,-1,7,0,10697,soichiro murakami,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,0,"The task of generating weather-forecast comments from meteorological simulations has the following requirements: (i) the changes in numerical values for various physical quantities need to be considered, (ii) the weather comments should be dependent on delivery time and area information, and (iii) the comments should provide useful information for users. To meet these requirements, we propose a data-to-text model that incorporates three types of encoders for numerical forecast maps, observation data, and meta-data. We also introduce weather labels representing weather information, such as sunny and rain, for our model to explicitly describe useful information. We conducted automatic and human evaluations. The results indicate that our model performed best against baselines in terms of informativeness. We make our code and data publicly available."
2021.eacl-main.267,Metric-Type Identification for Multi-Level Header Numerical Tables in Scientific Papers,2021,-1,-1,3,0,10902,lya suadaa,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,0,"Numerical tables are widely used to present experimental results in scientific papers. For table understanding, a metric-type is essential to discriminate numbers in the tables. We introduce a new information extraction task, metric-type identification from multi-level header numerical tables, and provide a dataset extracted from scientific papers consisting of header tables, captions, and metric-types. We then propose two joint-learning neural classification and generation schemes featuring pointer-generator-based and BERT-based models. Our results show that the joint models can handle both in-header and out-of-header metric-type identification problems."
2021.eacl-main.296,One-class Text Classification with Multi-modal Deep Support Vector Data Description,2021,-1,-1,5,0,10949,chenlong hu,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,0,"This work presents multi-modal deep SVDD (mSVDD) for one-class text classification. By extending the uni-modal SVDD to a multiple modal one, we build mSVDD with multiple hyperspheres, that enable us to build a much better description for target one-class data. Additionally, the end-to-end architecture of mSVDD can jointly handle neural feature learning and one-class text learning. We also introduce a mechanism for incorporating negative supervision in the absence of real negative data, which can be beneficial to the mSVDD model. We conduct experiments on Reuters and 20 Newsgroup datasets, and the experimental results demonstrate that mSVDD outperforms uni-modal SVDD and mSVDD can get further improvements when negative supervision is incorporated."
2021.eacl-demos.27,A New Surprise Measure for Extracting Interesting Relationships between Persons,2021,-1,-1,4,1,3633,hidetaka kamigaito,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations,0,"One way to enhance user engagement in search engines is to suggest interesting facts to the user. Although relationships between persons are important as a target for text mining, there are few effective approaches for extracting the interesting relationships between persons. We therefore propose a method for extracting interesting relationships between persons from natural language texts by focusing on their surprisingness. Our method first extracts all personal relationships from dependency trees for the texts and then calculates surprise scores for distributed representations of the extracted relationships in an unsupervised manner. The unique point of our method is that it does not require any labeled dataset with annotation for the surprising personal relationships. The results of the human evaluation show that the proposed method could extract more interesting relationships between persons from Japanese Wikipedia articles than a popularity-based baseline method. We demonstrate our proposed method as a chrome plugin on google search."
2021.acl-long.115,Towards Table-to-Text Generation with Numerical Reasoning,2021,-1,-1,4,0,10902,lya suadaa,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"Recent neural text generation models have shown significant improvement in generating descriptive text from structured data such as table formats. One of the remaining important challenges is generating more analytical descriptions that can be inferred from facts in a data source. The use of a template-based generator and a pointer-generator is among the potential alternatives for table-to-text generators. In this paper, we propose a framework consisting of a pre-trained model and a copy mechanism. The pre-trained models are fine-tuned to produce fluent text that is enriched with numerical reasoning. However, it still lacks fidelity to the table contents. The copy mechanism is incorporated in the fine-tuning step by using general placeholders to avoid producing hallucinated phrases that are not supported by a table while preserving high fluency. In summary, our contributions are (1) a new dataset for numerical table-to-text generation using pairs of a table and a paragraph of a table description with richer inference from scientific papers, and (2) a table-to-text generation framework enriched with numerical reasoning."
2020.coling-main.28,Pointing to Subwords for Generating Function Names in Source Code,2020,-1,-1,4,0,21071,shogo fujita,Proceedings of the 28th International Conference on Computational Linguistics,0,"We tackle the task of automatically generating a function name from source code. Existing generators face difficulties in generating low-frequency or out-of-vocabulary subwords. In this paper, we propose two strategies for copying low-frequency or out-of-vocabulary subwords in inputs. Our best performing model showed an improvement over the conventional method in terms of our modified F1 and accuracy on the Java-small and Java-large datasets."
2020.coling-main.192,Neural text normalization leveraging similarities of strings and sounds,2020,-1,-1,5,0,21285,riku kawamura,Proceedings of the 28th International Conference on Computational Linguistics,0,"We propose neural models that can normalize text by considering the similarities of word strings and sounds. We experimentally compared a model that considers the similarities of both word strings and sounds, a model that considers only the similarity of word strings or of sounds, and a model without the similarities as a baseline. Results showed that leveraging the word string similarity succeeded in dealing with misspellings and abbreviations, and taking into account the sound similarity succeeded in dealing with phonetic substitutions and emphasized characters. So that the proposed models achieved higher F1 scores than the baseline."
2020.coling-main.424,Hierarchical Trivia Fact Extraction from {W}ikipedia Articles,2020,-1,-1,4,1,9388,jingun kwon,Proceedings of the 28th International Conference on Computational Linguistics,0,"Recently, automatic trivia fact extraction has attracted much research interest. Modern search engines have begun to provide trivia facts as the information for entities because they can motivate more user engagement. In this paper, we propose a new unsupervised algorithm that automatically mines trivia facts for a given entity. Unlike previous studies, the proposed algorithm targets at a single Wikipedia article and leverages its hierarchical structure via top-down processing. Thus, the proposed algorithm offers two distinctive advantages: it does not incur high computation time, and it provides a domain-independent approach for extracting trivia facts. Experimental results demonstrate that the proposed algorithm is over 100 times faster than the existing method which considers Wikipedia categories. Human evaluation demonstrates that the proposed algorithm can mine better trivia facts regardless of the target entity domain and outperforms the existing methods."
2020.coling-main.464,Diverse and Non-redundant Answer Set Extraction on Community {QA} based on {DPP}s,2020,-1,-1,3,0,21071,shogo fujita,Proceedings of the 28th International Conference on Computational Linguistics,0,"In community-based question answering (CQA) platforms, it takes time for a user to get useful information from among many answers. Although one solution is an answer ranking method, the user still needs to read through the top-ranked answers carefully. This paper proposes a new task of selecting a diverse and non-redundant answer set rather than ranking the answers. Our method is based on determinantal point processes (DPPs), and it calculates the answer importance and similarity between answers by using BERT. We built a dataset focusing on a Japanese CQA site, and the experiments on this dataset demonstrated that the proposed method outperformed several baseline methods."
2020.aacl-main.10,A Simple and Effective Usage of Word Clusters for {CBOW} Model,2020,-1,-1,5,1,7870,yukun feng,Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing,0,"We propose a simple and effective method for incorporating word clusters into the Continuous Bag-of-Words (CBOW) model. Specifically, we propose to replace infrequent input and output words in CBOW model with their clusters. The resulting cluster-incorporated CBOW model produces embeddings of frequent words and a small amount of cluster embeddings, which will be fine-tuned in downstream tasks. We empirically show our replacing method works well on several downstream tasks. Through our analysis, we show that our method might be also useful for other similar models which produce word embeddings."
W19-8641,A Large-Scale Multi-Length Headline Corpus for Analyzing Length-Constrained Headline Generation Model Evaluation,2019,0,0,8,0,9399,yuta hitomi,Proceedings of the 12th International Conference on Natural Language Generation,0,"Browsing news articles on multiple devices is now possible. The lengths of news article headlines have precise upper bounds, dictated by the size of the display of the relevant device or interface. Therefore, controlling the length of headlines is essential when applying the task of headline generation to news production. However, because there is no corpus of headlines of multiple lengths for a given article, previous research on controlling output length in headline generation has not discussed whether the system outputs could be adequately evaluated without multiple references of different lengths. In this paper, we introduce two corpora, which are Japanese News Corpus (JNC) and JApanese MUlti-Length Headline Corpus (JAMUL), to confirm the validity of previous evaluation settings. The JNC provides common supervision data for headline generation. The JAMUL is a large-scale evaluation dataset for headlines of three different lengths composed by professional editors. We report new findings on these corpora; for example, although the longest length reference summary can appropriately evaluate the existing methods controlling output length, this evaluation setting has several problems."
R19-1059,Discourse-Aware Hierarchical Attention Network for Extractive Single-Document Summarization,2019,0,0,4,1,5924,tatsuya ishigaki,Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019),0,"Discourse relations between sentences are often represented as a tree, and the tree structure provides important information for summarizers to create a short and coherent summary. However, current neural network-based summarizers treat the source document as just a sequence of sentences and ignore the tree-like discourse structure inherent in the document. To incorporate the information of a discourse tree structure into the neural network-based summarizers, we propose a discourse-aware neural extractive summarizer which can explicitly take into account the discourse dependency tree structure of the source document. Our discourse-aware summarizer can jointly learn the discourse structure and the salience score of a sentence by using novel hierarchical attention modules, which can be trained on automatically parsed discourse dependency trees. Experimental results showed that our model achieved competitive or better performances against state-of-the-art models in terms of ROUGE scores on the DailyMail dataset. We further conducted manual evaluations. The results showed that our approach also gained the coherence of the output summaries."
P19-1099,Global Optimization under Length Constraint for Neural Text Summarization,2019,0,5,4,0,25608,takuya makino,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"We propose a global optimization method under length constraint (GOLC) for neural text summarization models. GOLC increases the probabilities of generating summaries that have high evaluation scores, ROUGE in this paper, within a desired length. We compared GOLC with two optimization methods, a maximum log-likelihood and a minimum risk training, on CNN/Daily Mail and a Japanese single document summarization data set of The Mainichi Shimbun Newspapers. The experimental results show that a state-of-the-art neural summarization model optimized with GOLC generates fewer overlength summaries while maintaining the fastest processing speed; only 6.70{\%} overlength summaries on CNN/Daily and 7.8{\%} on long summary of Mainichi, compared to the approximately 20{\%} to 50{\%} on CNN/Daily Mail and 10{\%} to 30{\%} on Mainichi with the other optimization methods. We also demonstrate the importance of the generation of in-length summaries for post-editing with the dataset Mainich that is created with strict length constraints. The ex- perimental results show approximately 30{\%} to 40{\%} improved post-editing time by use of in-length summaries."
P19-1250,Dataset Creation for Ranking Constructive News Comments,2019,0,1,3,0,1082,soichiro fujita,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"Ranking comments on an online news service is a practically important task for the service provider, and thus there have been many studies on this task. However, most of them considered users{'} positive feedback, such as {``}Like{''}-button clicks, as a quality measure. In this paper, we address directly evaluating the quality of comments on the basis of {``}constructiveness,{''} separately from user feedback. To this end, we create a new dataset including 100K+ Japanese comments with constructiveness scores (C-scores). Our experiments clarify that C-scores are not always related to users{'} positive feedback, and the performance of pairwise ranking models tends to be enhanced by the variation of comments rather than articles."
K19-1086,A Simple and Effective Method for Injecting Word-Level Information into Character-Aware Neural Language Models,2019,0,0,4,1,7870,yukun feng,Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL),0,"We propose a simple and effective method to inject word-level information into character-aware neural language models. Unlike previous approaches which usually inject word-level information at the input of a long short-term memory (LSTM) network, we inject it into the softmax function. The resultant model can be seen as a combination of character-aware language model and simple word-level language model. Our injection method can also be used together with previous methods. Through the experiments on 14 typologically diverse languages, we empirically show that our injection method, when used together with the previous methods, works better than the previous methods, including a gating mechanism, averaging, and concatenation of word vectors. We also provide a comprehensive comparison of these injection methods."
D19-6505,Context-aware Neural Machine Translation with Coreference Information,2019,0,0,4,0,26410,takumi ohtani,Proceedings of the Fourth Workshop on Discourse in Machine Translation (DiscoMT 2019),0,"We present neural machine translation models for translating a sentence in a text by using a graph-based encoder which can consider coreference relations provided within the text explicitly. The graph-based encoder can dynamically encode the source text without attending to all tokens in the text. In experiments, our proposed models provide statistically significant improvement to the previous approach of at most 0.9 points in the BLEU score on the OpenSubtitle2018 English-to-Japanese data set. Experimental results also show that the graph-based encoder can handle a longer text well, compared with the previous approach."
D19-1587,Split or Merge: Which is Better for Unsupervised {RST} Parsing?,2019,0,0,5,1,3631,naoki kobayashi,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"Rhetorical Structure Theory (RST) parsing is crucial for many downstream NLP tasks that require a discourse structure for a text. Most of the previous RST parsers have been based on supervised learning approaches. That is, they require an annotated corpus of sufficient size and quality, and heavily rely on the language and domain dependent corpus. In this paper, we present two language-independent unsupervised RST parsing methods based on dynamic programming. The first one builds the optimal tree in terms of a dissimilarity score function that is defined for splitting a text span into smaller ones. The second builds the optimal tree in terms of a similarity score function that is defined for merging two adjacent spans into a large one. Experimental results on English and German RST treebanks showed that our parser based on span merging achieved the best score, around 0.8 F$_1$ score, which is close to the scores of the previous supervised parsers."
W18-6510,Stylistically User-Specific Generation,2018,0,0,3,0,27655,abdurrisyad fikri,Proceedings of the 11th International Conference on Natural Language Generation,0,"Recent neural models for response generation show good results in terms of general responses. In real conversations, however, depending on the speaker/responder, similar utterances should require different responses. In this study, we attempt to consider individual user{'}s information in adjusting the notable sequence-to-sequence (seq2seq) model for more diverse, user-specific responses. We assume that we need user-specific features to adjust the response and we argue that some selected representative words from the users are suitable for this task. Furthermore, we prove that even for unseen or unknown users, our model can provide more diverse and interesting responses, while maintaining correlation with input utterances. Experimental results with human evaluation show that our model can generate more interesting responses than the popular seq2seqmodel and achieve higher relevance with input utterances than our baseline."
C18-1274,Neural Machine Translation Incorporating Named Entity,2018,0,5,5,0,30905,arata ugawa,Proceedings of the 27th International Conference on Computational Linguistics,0,"This study proposes a new neural machine translation (NMT) model based on the encoder-decoder model that incorporates named entity (NE) tags of source-language sentences. Conventional NMT models have two problems enumerated as follows: (i) they tend to have difficulty in translating words with multiple meanings because of the high ambiguity, and (ii) these models{'}abilitytotranslatecompoundwordsseemschallengingbecausetheencoderreceivesaword, a part of the compound word, at each time step. To alleviate these problems, the encoder of the proposed model encodes the input word on the basis of its NE tag at each time step, which could reduce the ambiguity of the input word. Furthermore,the encoder introduces a chunk-level LSTM layer over a word-level LSTM layer and hierarchically encodes a source-language sentence to capture a compound NE as a chunk on the basis of the NE tags. We evaluate the proposed model on an English-to-Japanese translation task with the ASPEC, and English-to-Bulgarian and English-to-Romanian translation tasks with the Europarl corpus. The evaluation results show that the proposed model achieves up to 3.11 point improvement in BLEU."
P17-2044,{J}apanese Sentence Compression with a Large Training Dataset,2017,8,1,4,0,32567,shun hasegawa,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"In English, high-quality sentence compression models by deleting words have been trained on automatically created large training datasets. We work on Japanese sentence compression by a similar approach. To create a large Japanese training dataset, a method of creating English training dataset is modified based on the characteristics of the Japanese language. The created dataset is used to train Japanese sentence compression models based on the recurrent neural network."
I17-2002,Supervised Attention for Sequence-to-Sequence Constituency Parsing,2017,14,3,5,1,3633,hidetaka kamigaito,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"The sequence-to-sequence (Seq2Seq) model has been successfully applied to machine translation (MT). Recently, MT performances were improved by incorporating supervised attention into the model. In this paper, we introduce supervised attention to constituency parsing that can be regarded as another translation task. Evaluation results on the PTB corpus showed that the bracketing F-measure was improved by supervised attention."
I17-1080,Summarizing Lengthy Questions,2017,19,2,3,1,5924,tatsuya ishigaki,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers),0,"In this research, we propose the task of question summarization. We first analyzed question-summary pairs extracted from a Community Question Answering (CQA) site, and found that a proportion of questions cannot be summarized by extractive approaches but requires abstractive approaches. We created a dataset by regarding the question-title pairs posted on the CQA site as question-summary pairs. By using the data, we trained extractive and abstractive summarization models, and compared them based on ROUGE scores and manual evaluations. Our experimental results show an abstractive method using an encoder-decoder model with a copying mechanism achieves better scores for both ROUGE-2 F-measure and the evaluations by human judges."
D17-1246,Distinguishing {J}apanese Non-standard Usages from Standard Ones,2017,11,0,4,1,21286,tatsuya aoki,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,0,"We focus on non-standard usages of common words on social media. In the context of social media, words sometimes have other usages that are totally different from their original. In this study, we attempt to distinguish non-standard usages on social media from standard ones in an unsupervised manner. Our basic idea is that non-standardness can be measured by the inconsistency between the expected meaning of the target word and the given context. For this purpose, we use context embeddings derived from word embeddings. Our experimental results show that the model leveraging the context embedding outperforms other methods and provide us with findings, for example, on how to construct context embeddings and which corpus to use."
P16-1211,A Corpus-Based Analysis of Canonical Word Order of {J}apanese Double Object Constructions,2016,13,2,2,1,5794,ryohei sasano,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"The canonical word order of Japanese double object constructions has attracted considerable attention among linguists and has been a topic of many studies. However, most of these studies require either manual analyses or measurements of human characteristics such as brain activities or reading times for each example. Thus, while these analyses are reliable for the examples they focus on, they cannot be generalized to other examples. On the other hand, the trend of actual usage can be collected automatically from a large corpus. Thus, in this paper, we assume that there is a relationship between the canonical word order and the proportion of each word order in a large corpus and present a corpusbased analysis of canonical word order of Japanese double object constructions."
D16-1140,Controlling Output Length in Neural Encoder-Decoders,2016,38,26,5,1,27810,yuta kikuchi,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,"Neural encoder-decoder models have shown great success in many sequence generation tasks. However, previous work has not investigated situations in which we would like to control the length of encoder-decoder outputs. This capability is crucial for applications such as text summarization, in which we have to generate concise summaries with a desired length. In this paper, we propose methods for controlling the output sequence length for neural encoder-decoder models: two decoding-based methods and two learning-based methods. Results show that our learning-based methods have the capability to control length without degrading summary quality in a summarization task."
D16-1210,Unsupervised Word Alignment by Agreement Under {ITG} Constraint,2016,25,3,4,1,3633,hidetaka kamigaito,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,None
N15-1150,Context-Dependent Automatic Response Generation Using Statistical Machine Translation Techniques,2015,18,3,4,0,8089,andrew shin,Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Developing a system that can automatically respond to a userxe2x80x99s utterance has recently become a topic of research in natural language processing. However, most works on the topic take into account only a single preceding utterance to generate a response. Recent works demonstrate that the application of statistical machine translation (SMT) techniques towards monolingual dialogue setting, in which a response is treated as a translation of a stimulus, has a great potential, and we exploit the approach to tackle the context-dependent response generation task. We attempt to extract relevant and significant information from the wider contextual scope of the conversation, and incorporate it into the SMT techniques. We also discuss the advantages and limitations of this approach through our experimental results."
D15-1143,Hierarchical Back-off Modeling of {H}iero Grammar based on Non-parametric {B}ayesian Model,2015,32,1,4,1,3633,hidetaka kamigaito,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"In hierarchical phrase-based machine translation, a rule table is automatically learned by heuristically extracting synchronous rules from a parallel corpus. As a result, spuriously many rules are extracted which may be composed of various incorrect rules. The larger rule table incurs more run time for decoding and may result in lower translation quality. To resolve the problems, we propose a hierarchical back-off model for Hiero grammar, an instance of a synchronous context free grammar (SCFG), on the basis of the hierarchical Pitman-Yor process. The model can extract a compact rule and phrase table without resorting to any heuristics by hierarchically backing off to smaller phrases under SCFG. Inference is efficiently carried out using two-step synchronous parsing of Xiao et al., (2012) combined with slice sampling. In our experiments, the proposed model achieved higher or at least comparable translation quality against a previous Bayesian model on various language pairs; German/French/Spanish/JapaneseEnglish. When compared against heuristic models, our model achieved comparable translation quality on a full size GermanEnglish language pair in Europarl v7 corpus with significantly smaller grammar size; less than 10% of that for heuristic model."
P14-2052,Single Document Summarization based on Nested Tree Structure,2014,18,26,4,1,27810,yuta kikuchi,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Many methods of text summarization combining sentence selection and sentence compression have recently been proposed. Although the dependency between words has been used in most of these methods, the dependency between sentences, i.e., rhetorical structures, has not been exploited in such joint methods. We used both dependency between words and dependency between sentences by constructing a nested tree, in which nodes in the document tree representing dependency between sentences were replaced by a sentence tree representing dependency between words. We formulated a summarization task as a combinatorial optimization problem, in which the nested tree was trimmed without losing important content in the source document. The results from an empirical evaluation revealed that our method based on the trimming of the nested tree significantly improved the summarization of texts."
D14-1017,Unsupervised Word Alignment Using Frequency Constraint in Posterior Regularized {EM},2014,17,2,4,1,3633,hidetaka kamigaito,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,"Generative word alignment models, such as IBM Models, are restricted to oneto-many alignment, and cannot explicitly represent many-to-many relationships in a bilingual text. The problem is partially solved either by introducing heuristics or by agreement constraints such that two directional word alignments agree with each other. In this paper, we focus on the posterior regularization framework (Ganchev et al., 2010) that can force two directional word alignment models to agree with each other during training, and propose new constraints that can take into account the difference between function words and content words. Experimental results on French-to-English and Japanese-to-English alignment tasks show statistically significant gains over the previous posterior regularization baseline. We also observed gains in Japanese-toEnglish translation tasks, which prove the effectiveness of our methods under grammatically different language pairs."
P13-1083,Part-of-Speech Induction in Dependency Trees for Statistical Machine Translation,2013,38,7,5,1,1486,akihiro tamura,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"This paper proposes a nonparametric Bayesian method for inducing Part-ofSpeech (POS) tags in dependency trees to improve the performance of statistical machine translation (SMT). In particular, we extend the monolingual infinite tree model (Finkel et al., 2007) to a bilingual scenario: each hidden state (POS tag) of a source-side dependency tree emits a source word together with its aligned target word, either jointly (joint model), or independently (independent model). Evaluations of Japanese-to-English translation on the NTCIR-9 data show that our induced Japanese POS tags for dependency trees improve the performance of a forestto-string SMT system. Our independent model gains over 1 point in BLEU by resolving the sparseness problem introduced in the joint model."
P13-1101,Subtree Extractive Summarization via Submodular Maximization,2013,16,20,4,1,25335,hajime morita,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,None
I13-1019,A Simple Approach to Unknown Word Processing in {J}apanese Morphological Analysis,2013,14,8,3,1,5794,ryohei sasano,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,"This paper presents a simple but effective approach to unknown word processing in Japanese morphological analysis, which handles 1) unknown words that are derived from words in a pre-defined lexicon and 2) unknown onomatopoeias. Our approach leverages derivation rules and onomatopoeia patterns, and correctly recognizes certain types of unknown words. Experiments revealed that our approach recognized about 4,500 unknown words in 100,000 Web sentences with only 80 harmful side effects and a 6% loss in speed."
I13-1078,Construction of Emotional Lexicon Using Potts Model,2013,20,10,4,0,31213,braja patra,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,"Emotion is an instinctive state of mind aroused by some specific objects or situation. Exchange of textual information is an important medium for communication and contains a rich set of emotional expressions. The computational approaches to emotion analysis in textual data require annotated lexicons with polarity tags. In this paper we propose a novel method for constructing emotion lexicon annotated with Ekmanxe2x80x9fs six basic emotion classes (anger, disgust, fear, happy, sad and surprise). We adopt the Potts model for the probability modeling of the lexical network. The lexical network has been constructed by connecting each pair of words in which one of the two words appears in the gloss of the other. Starting with a small number of emotional seed words, the emotional categories of other words have been determined. With manual checking of top 200 words from each class an average precision of 85.41% has been achieved."
D13-1121,Automatic Knowledge Acquisition for Case Alternation between the Passive and Active Voices in {J}apanese,2013,25,5,4,1,5794,ryohei sasano,Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,0,"We present a method for automatically acquiring knowledge for case alternation between the passive and active voices in Japanese. By leveraging several linguistic constraints on alternation patterns and lexical case frames obtained from a large Web corpus, our method aligns a case frame in the passive voice to a corresponding case frame in the active voice and finds an alignment between their cases. We then apply the acquired knowledge to a case alternation task and prove its usefulness."
Y12-1008,Automatic Domain Adaptation for Word Sense Disambiguation Based on Comparison of Multiple Classifiers,2012,19,10,2,1,15816,kanako komiya,"Proceedings of the 26th Pacific Asia Conference on Language, Information, and Computation",0,"Domain adaptation (DA), which involves adapting a classifier developed from source to target data, has been studied intensively in recent years. However, when DA for word sense disambiguation (WSD) was carried out, the optimal DA method varied according to the properties of the source and target data. This paper proposes automatic DA based on comparing the degrees of confidence of multiple classifiers for each instance. We compared three classifiers for three DA methods, where 1) a classifier was trained with a small amount of target data that was randomly selected and manually labeled but without source data, 2) a classifier was trained with source data and a small amount of target data that was randomly selected and manually labeled, and 3) a classifier was trained with selected source data that were sufficiently similar to the target data and a small amount of target data that was randomly selected and manually labeled. We used the method whose degree of confidence was the highest for each instance when Japanese WSD was carried out. The average accuracy of WSD when the DA methods that were determined automatically were used was significantly higher than when the original methods were used collectively."
P12-2068,Sentence Compression with Semantic Role Constraints,2012,12,17,4,0,32762,katsumasa yoshikawa,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"For sentence compression, we propose new semantic constraints to directly capture the relations between a predicate and its arguments, whereas the existing approaches have focused on relatively shallow linguistic properties, such as lexical and syntactic information. These constraints are based on semantic roles and superior to the constraints of syntactic dependencies. Our empirical evaluation on the Written News Compression Corpus (Clarke and Lapata, 2008) demonstrates that our system achieves results comparable to other state-of-the-art techniques."
C12-1086,Generating {``}{A} for {A}lpha{''} When There Are Thousands of Characters,2012,7,0,4,0,43766,hiroaki kawasaki,Proceedings of {COLING} 2012,0,"The phonetic alphabet enables people to dictate letters of the alphabet accurately by using representative words, i.e., A for Alpha. Japanese kanji (idiographic Chinese characters) vastly outnumber the letters of the Roman alphabet, and thus Japanese requires an explanatory reading like a phonetic alphabet. We call the explanatory reading of a kanji a xe2x80x9cdistinctive explanation.xe2x80x9d Most kanji characters have their homophones, and the role of the distinctive explanations is to enable users to identify a specific kanji character only by listening to the explanation. In this paper, we propose a corpus-based method for automatically generating distinctive explanations for a kanji, in which information about familiarity and homophones of kanji are taken into consideration. Through the kanji-identification experiments, we show that the quality of the explanations generated by the proposed method is higher than that of the manually crafted distinctive explanations."
W11-1710,Developing {J}apanese {W}ord{N}et Affect for Analyzing Emotions,2011,18,12,4,0,44304,yoshimitsu torii,Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis ({WASSA} 2.011),0,"This paper reports the development of Japanese WordNet Affect from the English WordNet Affect lists with the help of English SentiWordNet and Japanese WordNet. Expanding the available synsets of the English WordNet Affect using SentiWordNet, we have performed the translation of the expanded lists into Japanese based on the synsetIDs in the Japanese WordNet. A baseline system for emotion analysis of Japanese sentences has been developed based on the Japanese WordNet Affect. The incorporation of morphology improves the performance of the system. Overall, the system achieves average precision, recall and F-scores of 32.76%, 53% and 40.49% respectively on 89 sentences of the Japanese judgment corpus and 83.52%, 49.58% and 62.22% on 1000 translated Japanese sentences of the SemEval 2007 affect sensing test corpus. Different experimental outcomes and morphological analysis suggest that irrespective of the google translation error, the performance of the system could be improved by enhancing the Japanese WordNet Affect in terms of coverage."
P11-2039,Query Snowball: A Co-occurrence-based Approach to Multi-document Summarization for Question Answering,2011,27,7,3,1,25335,hajime morita,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,0,"We propose a new method for query-oriented extractive multi-document summarization. To enrich the information need representation of a given query, we build a co-occurrence graph to obtain words that augment the original query terms. We then formulate the summarization problem as a Maximum Coverage Problem with Knapsack Constraints based on word pairs rather than single words. Our experiments with the NTCIR ACLIA question answering test collections show that our method achieves a pyramid F3-score of up to 0.313, a 36% improvement over a baseline using Maximal Marginal Relevance."
I11-1093,A Named Entity Recognition Method based on Decomposition and Concatenation of Word Chunks,2011,0,1,3,0.215517,10521,tomoya iwakura,Proceedings of 5th International Joint Conference on Natural Language Processing,0,None
I11-1103,Identification of relations between answers with global constraints for Community-based Question Answering services,2011,18,1,4,0,18349,hikaru yokono,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"Community-based Question Answering services contain many threads consisting of a question and its answers. When there are many answers for a question, it is hard for a user to understand them all. To address this problem, we focus on logi- cal relations between answers in a thread and present a model for identifying the relations between the answers. We con- sider that there are constraints among the relations, such as a transitive law, and that it might be useful to take these con- straints into account. To consider these constraints, we propose the model based on a Markov logic network. We also in- troduce super-relations to give additional information for logical relation identifica- tion into our model. Through the experi- ment, we show that global constraints and super-relations make it easier to identify the relations."
I11-1124,Automatic Determination of a Domain Adaptation Method for Word Sense Disambiguation Using Decision Tree Learning,2011,16,12,2,1,15816,kanako komiya,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"Domain adaptation (DA), which involves adapting a classifier developed from source to target data, has been studied intensively in recent years. However, when DA for word sense disambiguation (WSD) was carried out, the optimal DA method varied according to the properties of the source and target data. This paper describes how the optimal method for DA was determined depending on these properties using decision tree learning, given a triple of the target word type of WSD, the source data, and the target data, and discusses what properties affected the determination of the best method when Japanese WSD was performed."
I11-1158,Potts Model on the Case Fillers for Word Sense Disambiguation,2011,14,0,2,1,4800,hiroya takamura,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"We propose a new method for word sense disambiguation for verbs. In our method, sense-dependent selectional preference of verbs is obtained through the probabilistic model on the lexical network. The meanfield approximation is employed to compute the state of the lexical network. The outcome of the computation is used as features for discriminative classifiers. The method is evaluated on the dataset of the Japanese word sense disambiguation."
Y10-1049,An Approach toward Register Classification of Book Samples in the {B}alanced {C}orpus of {C}ontemporary {W}ritten {J}apanese,2010,12,0,2,0,29685,wakako kashino,"Proceedings of the 24th Pacific Asia Conference on Language, Information and Computation",0,"Japanese books are usually classified into ten genres by Nippon Decimal Classification (NDC) based on their subject. However, this classification is sometimes insufficient for corpus studies which describe characteristics of the texts in the book. Here, we propose a method of classifying text samples taken from Japanese books into some registers and text types. Firstly, we discuss useful criteria to describe various characteristics of the texts and propose a two-step approach for stable annotation. We then apply our method to 161 book samples from the prerelease version of the Balanced Corpus of Contemporary Written Japanese (BCCWJ), a balanced Japanese corpus comprising 100 million words developed by National Institute for Japanese Language and Linguistics. Finally, we evaluate our method in terms of stability of annotation using kappa coefficients and correlation coefficients."
S10-1012,{S}em{E}val-2010 Task: {J}apanese {WSD},2010,7,21,1,1,1086,manabu okumura,Proceedings of the 5th International Workshop on Semantic Evaluation,0,"An overview of the SemEval-2 Japanese WSD task is presented. It is a lexical sample task, and word senses are defined according to a Japanese dictionary, the Iwanami Kokugo Jiten. This dictionary and a training corpus were distributed to participants. The number of target words was 50, with 22 nouns, 23 verbs, and 5 adjectives. Fifty instances of each target word were provided, consisting of a total of 2,500 instances for the evaluation. Nine systems from four organizations participated in the task."
D10-1081,An Efficient Algorithm for Unsupervised Word Segmentation with Branching Entropy and {MDL},2010,27,16,3,0,41342,valentin zhikov,Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,0,"This paper proposes a fast and simple unsupervised word segmentation algorithm that utilizes the local predictability of adjacent character sequences, while searching for a least-effort representation of the data. The model uses branching entropy as a means of constraining the hypothesis space, in order to efficiently obtain a solution that minimizes the length of a two-part MDL code. An evaluation with corpora in Japanese, Thai, English, and the CHILDES corpus for research in language development reveals that the algorithm achieves an accuracy, comparable to that of the state-of-the-art methods in unsupervised word segmentation, in a significantly reduced computational time."
R09-1052,Structured Output Learning with Polynomial Kernel,2009,11,2,3,1,25335,hajime morita,Proceedings of the International Conference {RANLP}-2009,0,"We propose a new method which enables the training of a kernelized structured output model. The structured output learning can flexibly represent a problem, and thus is gaining popularity in natural language processing. Meanwhile the polynomial kernel method is effective in many natural language processing tasks, since it takes into account the combination of features. However, it is computationally difficult to simultaneously use both the structured output learning and the kernel method. Our method avoids this difficulty by transforming the kernel function, and enables the kernelized structured output learning. We theoretically discuss the computational complexity of the proposed method and also empirically show its high efficiency and effectiveness through experiments in the task of identifying agreement and disagreement relations between utterances in meetings. Identifying agreement and disagreement relations consists of two mutuallycorrelated problems: identification of the utterance which each utterance is intended for, and classification of each utterance into approval, disapproval or others. We simultaneously use both of the structured output learning and the kernel method in order to take into account this correlation of the two problems."
E09-1089,Text Summarization Model Based on Maximum Coverage Problem and its Variant,2009,19,126,2,1,4800,hiroya takamura,Proceedings of the 12th Conference of the {E}uropean Chapter of the {ACL} ({EACL} 2009),0,"We discuss text summarization in terms of maximum coverage problem and its variant. We explore some decoding algorithms including the ones never used in this summarization formulation, such as a greedy algorithm with performance guarantee, a randomized algorithm, and a branch-and-bound method. On the basis of the results of comparative experiments, we also augment the summarization model so that it takes into account the relevance to the document cluster. Through experiments, we showed that the augmented model is superior to the best-performing method of DUC'04 on ROUGE-1 without stopwords."
I08-1019,Identifying Cross-Document Relations between Sentences,2008,14,12,3,0,48665,yasunari miyabe,Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-{I},0,"A pair of sentences in different newspaper articles on an event can have one of several relations. Of these, we have focused on two, i.e., equivalence and transition. Equivalence is the relation between two sentences that have the same information on an event. Transition is the relation between two sentences that have the same information except for values of numeric attributes. We propose methods of identifying these relations. We first split a dataset consisting of pairs of sentences into clusters according to their similarities, and then construct a classifier for each cluster to identify equivalence relations. We also adopt a xe2x80x9ccoarse-to-finexe2x80x9d approach. We further propose using the identified equivalence relations to address the task of identifying transition relations."
I08-1039,Learning to Shift the Polarity of Words for Sentiment Classification,2008,12,50,4,0,13640,daisuke ikeda,Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-{I},0,"We propose a machine learning based method of sentiment classification of sentences using word-level polarity. The polarities of words in a sentence are not always the same as that of the sentence, because there can be polarity-shifters such as negation expressions. The proposed method models the polarity-shifters. Our model can be trained in two different ways: word-wise and sentence-wise learning. In sentence-wise learning, the model can be trained so that the prediction of sentence polarities should be accurate. The model can also be combined with features used in previous work such as bag-of-words and n-grams. We empirically show that our method almost always improves the performance of sentiment classification of sentences especially when we have only small amount of training data."
S07-1069,{TITPI}: Web People Search Task Using Semi-Supervised Clustering Approach,2007,10,4,2,0,12730,kazunari sugiyama,Proceedings of the Fourth International Workshop on Semantic Evaluations ({S}em{E}val-2007),0,"Most of the previous works that disambiguate personal names in Web search results employ agglomerative clustering approaches. However, these approaches tend to generate clusters that contain a single element depending on a certain criterion of merging similar clusters. In contrast to such previous works, we have adopted a semi-supervised clustering approach to integrate similar documents into a labeled document. Moreover, our proposed approach is characterized by controlling the fluctuation of the centroid of a cluster in order to generate more accurate clusters."
N07-1037,Extracting Semantic Orientations of Phrases from Dictionary,2007,22,62,3,1,4800,hiroya takamura,Human Language Technologies 2007: The Conference of the North {A}merican Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference,0,"We propose a method for extracting semantic orientations of phrases (pairs of an adjective and a noun): positive, negative, or neutral. Given an adjective, the semantic orientation classification of phrases can be reduced to the classification of words. We construct a lexical network by connecting similar/related words. In the network, each node has one of the three orientation values and the neighboring nodes tend to have the same value. We adopt the Potts model for the probability model of the lexical network. For each adjective, we estimate the states of the nodes, which indicate the semantic orientations of the adjective-noun pairs. Unlike existing methods for phrase classification, the proposed method can classify phrases consisting of unseen words. We also propose to use unlabeled data for a seed set of probability computation. Empirical evaluation shows the effectiveness of the proposed method."
D07-1063,{J}apanese Dependency Analysis Using the Ancestor-Descendant Relation,2007,9,3,3,1,1486,akihiro tamura,Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({EMNLP}-{C}o{NLL}),0,"We propose a novel method for Japanese dependency analysis, which is usually reduced to the construction of a dependency tree. In deterministic approaches to this task, dependency trees are constructed by series of actions of attaching a bunsetsu chunk to one of the nodes in the tree being constructed. Conventional techniques select the node based on whether the new bunsetsu chunk and each node in the trees are in a parent-child relation or not. However, tree structures include relations between two nodes other than the parent-child relation. Therefore, we use ancestor-descendant relations in addition to parent-child relations, so that the added redundancy helps errors be corrected. Experimental results show that the proposed method achieves higher accuracy."
W06-0507,Towards Large-scale Non-taxonomic Relation Extraction: Estimating the Precision of Rote Extractors,2006,23,9,3,0,37681,enrique alfonseca,Proceedings of the 2nd Workshop on Ontology Learning and Population: Bridging the Gap between Text and Knowledge,0,"In this paper, we describe a rote extractor that learns patterns for finding semantic relations in unrestricted text, with new procedures for pattern generalisation and scoring. An improved method for estimating the precision of the extracted patterns is presented. We show that our method approximates the precision values as evaluated by hand much better than the procedure traditionally used in rote extractors."
P06-2002,A Rote Extractor with Edit Distance-Based Generalisation and Multi-Corpora Precision Calculation,2006,18,9,3,0,37681,enrique alfonseca,Proceedings of the {COLING}/{ACL} 2006 Main Conference Poster Sessions,0,"In this paper, we describe a rote extractor that learns patterns for finding semantic relationships in unrestricted text, with new procedures for pattern generalization and scoring. These include the use of part-of-speech tags to guide the generalization, Named Entity categories inside the patterns, an edit-distance-based pattern generalization algorithm, and a pattern accuracy calculation procedure based on evaluating the patterns on several test corpora. In an evaluation with 14 entities, the system attains a precision higher than 50% for half of the relationships considered."
P06-2078,An Automatic Method for Summary Evaluation Using Multiple Evaluation Results by a Manual Method,2006,11,6,2,1,43352,hidetsugu nanba,Proceedings of the {COLING}/{ACL} 2006 Main Conference Poster Sessions,0,"To solve a problem of how to evaluate computer-produced summaries, a number of automatic and manual methods have been proposed. Manual methods evaluate summaries correctly, because humans evaluate them, but are costly. On the other hand, automatic methods, which use evaluation tools or programs, are low cost, although these methods cannot evaluate summaries as accurately as manual methods. In this paper, we investigate an automatic evaluation method that can reduce the errors of traditional automatic methods by using several evaluation results obtained manually. We conducted some experiments using the data of the Text Summarization Challenge 2 (TSC-2). A comparison with conventional automatic methods shows that our method outperforms other methods usually used."
P06-1105,{J}apanese Dependency Parsing Using Co-Occurrence Information and a Combination of Case Elements,2006,14,10,2,1,35650,takeshi abekawa,Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,1,"In this paper, we present a method that improves Japanese dependency parsing by using large-scale statistical information. It takes into account two kinds of information not considered in previous statistical (machine learning based) parsing methods: information about dependency relations among the case elements of a verb, and information about co-occurrence relations between a verb and its case element. This information can be collected from the results of automatic dependency parsing of large-scale corpora. The results of an experiment in which our method was used to rerank the results obtained using an existing machine learning based parsing method showed that our method can improve the accuracy of the results obtained using the existing method."
P06-1145,Time Period Identification of Events in Text,2006,12,8,4,0,50001,taichi noro,Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,1,"This study aims at identifying when an event written in text occurs. In particular, we classify a sentence for an event into four time-slots; morning, daytime, evening, and night. To realize our goal, we focus on expressions associated with time-slot (time-associated words). However, listing up all the time-associated words is impractical, because there are numerous time-associated expressions. We therefore use a semi-supervised learning method, the Naive Bayes classifier backed up with the Expectation Maximization algorithm, in order to iteratively extract time-associated words while improving the classifier. We also propose to use Support Vector Machines to filter out noisy instances that indicates no specific time period. As a result of experiments, the proposed method achieved 0.864 of accuracy and outperformed other methods."
senda-etal-2006-automatic,Automatic Terminology Intelligibility Estimation for Readership-oriented Technical Writing,2006,4,0,3,1,50341,yasuko senda,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"This paper describes automatic terminology intelligibility estimation for readership-oriented technical writing. We assume that the term frequency weighted by the types of documents can be an indicator of the term intelligibility for a certain readership. From this standpoint, we analyzed the relationship between the following: average intelligibility levels of 46 technical terms that were rated by about 120 laymen; numbers of documents that an Internet search"
E06-1026,Latent Variable Models for Semantic Orientations of Phrases,2006,17,56,3,1,4800,hiroya takamura,11th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"We propose models for semantic orientations of phrases as well as classification methods based on the models. Although each phrase consists of multiple words, the semantic orientation of the phrase is not a mere sum of the orientations of the component words. Some words can invert the orientation. In order to capture the property of such phrases, we introduce latent variables into the models. Through experiments, we show that the proposed latent variable models work well in the classification of semantic orientations of phrases and achieved nearly 82% classification accuracy."
W05-0306,Investigating the Characteristics of Causal Relations in {J}apanese Text,2005,8,13,2,1,14555,takashi inui,Proceedings of the Workshop on Frontiers in Corpus Annotations {II}: Pie in the Sky,0,"We investigated of the characteristics of in-text causal relations. We designed causal relation tags. With our designed tag set, three annotators annotated 750 Japanese newspaper articles. Then, using the annotated corpus, we investigated the causal relation instances from some viewpoints. Our quantitative study shows that what amount of causal relation instances are present, where these relation instances are present, and which types of linguistic expressions are used for expressing these relation instances in text."
P05-1017,Extracting Semantic Orientations of Words using Spin Model,2005,19,297,3,1,4800,hiroya takamura,Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics ({ACL}{'}05),1,"We propose a method for extracting semantic orientations of words: desirable or undesirable. Regarding semantic orientations as spins of electrons, we use the mean field approximation to compute the approximate probability function of the system instead of the intractable actual probability function. We also propose a criterion for parameter selection on the basis of magnetization. Given only a small number of seed words, the proposed method extracts semantic orientations with high accuracy in the experiments on English lexicon. The result is comparable to the best value ever reported."
I05-1005,Corpus-Based Analysis of {J}apanese Relative Clause Constructions,2005,8,5,2,1,35650,takeshi abekawa,Second International Joint Conference on Natural Language Processing: Full Papers,0,"Japanese relative clause constructions (RCC's) are defined as being the NP's of structure xe2x80x98S NP', noting the lack of a relative pronoun or any other explicit form of noun-clause demarcation. Japanese relative clause modification should be classified into at least two major semantic types: case-slot gapping and head restrictive. However, these types for relative clause modification cannot apparently be distinguished. In this paper we propose a method of identifying a RCC's type with a machine learning technique. The features used in our approach are not only representing RCC's characteristics, but also automatically obtained from large corpora. The results we obtained from evaluation revealed that our method outperformed the traditional case frame-based method, and the features that we presented were effective in identifying RCC's types."
I05-1038,Classification of Multiple-Sentence Questions,2005,9,25,3,1,1486,akihiro tamura,Second International Joint Conference on Natural Language Processing: Full Papers,0,"Conventional QA systems cannot answer to the questions composed of two or more sentences. Therefore, we aim to construct a QA system that can answer such multiple-sentence questions. As the first stage, we propose a method for classifying multiple-sentence questions into question types. Specifically, we first extract the core sentence from a given question text. We use the core sentence and its question focus in question classification. The result of experiments shows that the proposed method improves F-measure by 8.8% and accuracy by 4.4%."
H05-1019,Kernel-based Approach for Automatic Evaluation of Natural Language Generation Technologies: Application to Automatic Summarization,2005,11,5,2,0.869565,3632,tsutomu hirao,Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,0,"In order to promote the study of automatic summarization and translation, we need an accurate automatic evaluation method that is close to human evaluation. In this paper, we present an evaluation method that is based on convolution kernels that measure the similarities between texts considering their substructures. We conducted an experiment using automatic summarization evaluation data developed for Text Summarization Challenge 3 (TSC-3). A comparison with conventional techniques shows that our method correlates more closely with human evaluations and is more robust."
nanba-okumura-2004-comparison,Comparison of Some Automatic and Manual Methods for Summary Evaluation Based on the Text Summarization Challenge 2,2004,6,2,2,1,43352,hidetsugu nanba,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"In this paper, we compare some automatic and manual methods for summary evaluation. One of the essential points for evaluating a summary is how well the evaluation measure recognizes slight differences in the quality of the computer-produced summaries. In terms of this point, we examined xe2x80x98evaluation by revisionxe2x80x99 using the data of the Text Summarization Challenge 2 (TSC2). Evaluation by revision is a manual method that was first used in TSC2, whose effectiveness has not been tested. First, we compared evaluation by revision with a ranking evaluation, which is a manual method used both in TSC1 and in TSC2, by checking the gaps of the edit distance from 0 to 1 at 0.1 intervals. To investigate the effectiveness of evaluation by revision, we also tested other automatic methods: content-based evaluation, BLEU and RED, and compare their results with that of evaluation by revision for reference. As a result, we found that evaluation by revision is effective for recognizing slight differences between computer-produced summaries. Second, we evaluated content-based evaluation, BLEU and RED by evaluation by revision, and compared the effectiveness of the three automatic methods. We found that RED is superior to the others in some examinations."
C04-1023,A Support System for Revising Titles to Stimulate the Lay Reader{'}s Interest in Technical Achievements,2004,9,2,3,1,50341,yasuko senda,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"When we write a report or an explanation on a newly-developed technology for readers including laypersons, it is very important to compose a title that can stimulate their interest in the technology. However, it is difficult for inexperienced authors to come up with an appealing title.In this research, we developed a support system for revising titles. We call it title revision wizard. The wizard provides a guidance on revising draft title to compose a title meeting three key points, and support tools for coming up with and elaborating on comprehensible or appealing phrases.In order to test the effect of our title revision wizard, we conducted a questionnaire survey on the effect of the titles with or without using the wizard on the interest of lay readers. The survey showed that the wizard is effective and helpful for the authors who cannot compose appealing titles for lay readers by themselves."
C04-1077,Corpus and Evaluation Measures for Multiple Document Summarization with Multiple Sources,2004,14,17,3,0.869565,3632,tsutomu hirao,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"In this paper, we introduce a large-scale test collection for multiple document summarization, the Text Summarization Challenge 3 (TSC3) corpus. We detail the corpus construction and evaluation measures. The significant feature of the corpus is that it annotates not only the important sentences in a document set, but also those among them that have the same content. Moreover, we define new evaluation metrics taking redundancy into account and discuss the effectiveness of redundancy minimization."
W03-2007,Patent Claim Processing for Readability - Structure Analysis and Term Explanation,2003,5,71,2,0,52656,akihiro shinmori,Proceedings of the {ACL}-2003 Workshop on Patent Corpus Processing,0,"Patent corpus processing should be centered around patent claim processing because claims are the most important part in patent specifications. It is common that claims written in Japanese are described in one sentence with peculiar style and wording and are difficult to understand for ordinary people. The peculiarity is caused by structural complexity of the sentences and many difficult terms used in the description. We have already proposed a framework to represent the structure of patent claims and a method to automatically analyze it. We are currently investigating a method to clarify terms in patent claims and to find the explanatory portions from the detailed description part of the patent specifications. Through both approaches, we believe we can improve readability of patent claims."
W03-0507,Text Summarization Challenge 2 - Text summarization evaluation at {NTCIR} Workshop 3,2003,1,21,1,1,1086,manabu okumura,Proceedings of the {HLT}-{NAACL} 03 Text Summarization Workshop,0,"We describe the outline of Text Summarization Challenge 2 (TSC2 hereafter), a sequel text summarization evaluation conducted as one of the tasks at the NTCIR Workshop 3. First, we describe briefly the previous evaluation, Text Summarization Challenge (TSC1) as introduction to TSC2. Then we explain TSC2 including the participants, the two tasks in TSC2, data used, evaluation methods for each task, and brief report on the results."
E03-1061,Automatic Acquisition of Script Knowledge from a Text Collection,2003,2,16,3,0,52968,toshiaki fujiki,10th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"In this paper, we describe a method for automatic acquisition of script knowledge from a Japanese text collection. Script knowledge represents a typical sequence of actions that occur in a particular situation. We extracted sequences (pairs) of actions occurring in time order from a Japanese text collection and then chose those that were typical of certain situations by ranking these sequences (pairs) in terms of the frequency of their occurrence. To extract sequences of actions occurring in time order, we constructed a text collection in which texts describing facts relating to a similar situation were clustered together and arranged in time order.We also describe a preliminary experiment with our acquisition system and discuss the results."
tokunaga-etal-2002-constructing,Constructing a lexicon of action,2002,3,2,2,0,301,takenobu tokunaga,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"This paper describes a Japanese speech dialogue system that enables a user to interact with agents in a virtual world and proposes a design framework for building a lexicon of action. This lexicon is used to realize the behavior of the agents in response to the userxe2x80x99s commands. The lexicon has two levels xe2x80x93 a macro and micro level. The system uses the macro-level lexicon, which is similar to a conventional plan library, to translate the userxe2x80x99s goal to a sequence of basic movements. This process is the same as conventional planning with symbol manipulation. The micro-level lexicon is used to translate the basic movements into animation, which is represented by a sequence of avatar postures. We discuss how to define a set of basic movements and how to make these basic movements reusable."
nanba-okumura-2002-examinations,Some Examinations of Intrinsic Methods for Summary Evaluation Based on the Text Summarization Challenge ({TSC}),2002,9,2,2,1,43352,hidetsugu nanba,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"Computer-produced summaries have traditionally been evaluated by comparing them with human-produced summaries using the Fmeasure. However, the F-measure is not appropriate when alternative sentences are possible in a human-produced extract. In this paper, we examine some evaluation methods devised to overcome the problem, including utility-based evaluation. By giving scores for moderately important sentences that does not appear in the human-produced extract, utility-based evaluation can resolve the problem. However, the method requires much effort from humans to provide data for evaluation. In this paper, we first propose a pseudo-utilitybased evaluation that uses human-produced extracts at different compression ratios. To evaluate the effectiveness of pseudo-utility-based evaluation, we compare our method and the F-measure using the data of the Text Summarization Challenge (TSC), and show that pseudoutility-based evaluation can resolve this problem. Next, we focus on content-based evaluation. Instead of measuring the ratio of sentences that match exactly in the extract, the method evaluates extracts by comparing their content words to those of human-produced extracts. Although the method has been reported to be effective in resolving the problem, it has not been examined in the context of comparing two extracts produced from different systems. We evaluated computer-produced summaries by content-based evaluation, and compared the results with a subjective evaluation. We found that the evaluation by content-based measure matched those by subjective evaluation in 93% of the cases, if the gap in content-based scores between two summaries is more than 0.2."
mochizuki-okumura-2000-comparison,A Comparison of Summarization Methods Based on Task-based Evaluation,2000,12,9,2,1,48361,hajime mochizuki,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,None
C00-2160,Producing More Readable Extracts by Revising Them,2000,6,19,1,1,1086,manabu okumura,{COLING} 2000 Volume 2: The 18th International Conference on Computational Linguistics,0,"In this paper, we first experimentally investigated the factors that make extracts hard to read. We did this by having human subjects try to revise extracts to produce more readable ones. We then classified the factors into five, most of which are related to cohesion, after which we devised revision rules for each factor, and partially implemented a system that revises extracts."
P98-2145,Text Segmentation with Multiple Surface Linguistic Cues,1998,20,25,3,1,48361,hajime mochizuki,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 2",0,"In general, a certain range of sentences in a text, is widely assumed to form a coherent unit which is called a discourse segment. Identifying the segment boundaries is a first step to recognize the structure of a text. In this paper, we describe a method for identifying segment boundaries of a Japanese text with the aid of multiple surface linguistic cues, though our experiments might be small-scale. We also present a method of training the weights for multiple linguistic cues automatically without the overfitting problem."
C98-2140,Text Segmentation with Multiple Surface Linguistic Cues,1998,20,25,3,1,48361,hajime mochizuki,{COLING} 1998 Volume 2: The 17th International Conference on Computational Linguistics,0,"In general, a certain range of sentences in a text, is widely assumed to form a coherent unit which is called a discourse segment. Identifying the segment boundaries is a first step to recognize the structure of a text. In this paper, we describe a method for identifying segment boundaries of a Japanese text with the aid of multiple surface linguistic cues, though our experiments might be small-scale. We also present a method of training the weights for multiple linguistic cues automatically without the overfitting problem."
W97-1511,Exploiting Contextual Information in Hypothesis Selection for Grammar Refinement,1997,9,1,3,0,44100,thanaruk theeramunkong,Computational Environments for Grammar Development and Linguistic Engineering,0,None
W97-0106,Grammar Acquisition Based on Clustering Analysis and Its Application to Statistical Parsing,1997,10,2,2,0,44100,thanaruk theeramunkong,Fifth Workshop on Very Large Corpora,0,None
C96-2147,Zero Pronoun Resolution in {J}apanese Discourse Based on Centering Theory,1996,14,32,1,1,1086,manabu okumura,{COLING} 1996 Volume 2: The 16th International Conference on Computational Linguistics,0,"Recently there have been a number of works that model the zero pronoun resolution with the concept called 'center.' However, the usefulness of the previous centering frameworks has not fully evaluated with naturally occurring discourses. Furthermore, the previous centering theory has handled only the phenomena in successive simple sentences and has not adequately addressed the way to handle complex sentences that are prevalent in naturally occurring discourses. In this paper, we present a method to handle complex sentences with the centering theory and describe our framework that identifies the autecedents of zero pronouns in naturally occurring Japanese discourses. We also present the evaluation of our framework with real discourses."
C94-2121,Word Sense Disambiguation and Text Segmentation Based on Lexical Cohesion,1994,13,81,1,1,1086,manabu okumura,{COLING} 1994 Volume 2: The 15th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"In this paper, we describe how word sense ambiguity can be resolved with the aid of lexical cohesion. By checking lexical cohesion between the current word and lexical chains in the order of the salience, in tandem with generation of lexical chains, we realize incremental word sense disambiguation based on contextual information that lexical chains, reveal. Next, we describe how segment boundaries of a text can be determined with the aid of lexical cohesion. We can measure the plausibility of each point in the text as a segment boundary by computing a degree of agreement of the start and end points of lexical chains."
C92-1062,A Chart-based Method of {ID}/{LP} Parsing with Generalized Discrimination Networks,1992,12,0,2,0,55268,surapant meknavin,{COLING} 1992 Volume 1: The 14th {I}nternational {C}onference on {C}omputational {L}inguistics,0,None
