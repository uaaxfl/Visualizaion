2000.iwpt-1.18,A97-1001,0,0.0681108,"Missing"
2000.iwpt-1.18,E93-1036,0,0.547537,"Missing"
2000.iwpt-1.18,P94-1017,0,0.0445722,"Missing"
2000.iwpt-1.18,P80-1024,0,0.537066,"Missing"
2000.iwpt-1.18,P91-1014,0,0.0873299,"Missing"
2000.iwpt-1.18,E87-1037,0,0.0418013,"Missing"
2007.mtsummit-papers.43,J93-2003,0,0.0108973,"obabilities. Our choice is completely equivalent, since the product of a set of probabilities is monotonically related to the corresponding sum of log probabilities. • a distortion penalty reflecting the degree of divergence of the order of the target phrases from the order of the source phrases. The probabilities of source phrases given target phrases and target phrases given source phrases are estimated from a word-aligned bilingual corpus. The lexical scores are computed as the log of the unnormalized probability of the Viterbi alignment for a phrase pair under IBM wordtranslation Model 1 (Brown et al., 1993). For each phrase pair extracted from the word-aligned corpus, the values of these four features are stored in a “phrase table”. The target language model is a trigram model smoothed with bigram and unigram language models, estimated from the target language half of the bilingual training corpus. The distortion penalty is computed as required by the Pharaoh decoder, which we explain in Section 4. We train the feature weights for the overall translation model to maximize the B LEU metric using Och’s (2003) minimum-errorrate training procedure. 3. Description of Pharaoh The Pharaoh decoder uses"
2007.mtsummit-papers.43,N03-1017,0,0.0062092,"measured by the B LEU metric (Papineni et al., 2002). The first modification improves the estimated cost function used by Pharaoh to rank partial hypotheses, by incorporating an estimate of the distortion penalty to be incurred in translating the rest of the sentence. The second modification uses early pruning of possible next-phrase translations to cut down the overall size of the search space. These modifications enable decoding speed-ups of an order of magnitude or more, with no reduction in the B LEU score of the resulting translations. 2. A Phrasal SMT Model Phrasal SMT, as described by Koehn et al. (2003) translates a source sentence into a target sentence by decomposing the source sentence into a sequence of source phrases, which can be any contiguous sequences of words (or tokens treated as words) in the source sentence. For each source phrase, a target phrase translation is selected, and the target phrases are arranged in some order to produce the complete translation. A set of possible translation candidates created in this way is scored according to a weighted linear combination of feature values, and the highest scoring translation candidate is selected as the translation of the source s"
2007.mtsummit-papers.43,P06-1065,1,0.854122,"Missing"
2007.mtsummit-papers.43,P03-1021,0,0.00717514,"Missing"
2007.mtsummit-papers.43,P02-1040,0,0.110455,"ems have been much slower than the best RBMT systems. For example, Language Weaver, currently the only commercial provider of SMT systems, claims to translate 5,000 words per minute per CPU,1 , while SYSTRAN, the market leader in commercial RBMT, claims to translate up to 450 words per second (27,000 words per minute) per CPU.2 In this paper, we present two modifications to the algorithm implemented in the widely-used Pharaoh phrasal SMT decoder (Koehn, 2003; Koehn 2004a; Koehn 2004b) that together permit much faster decoding without losing translation quality as measured by the B LEU metric (Papineni et al., 2002). The first modification improves the estimated cost function used by Pharaoh to rank partial hypotheses, by incorporating an estimate of the distortion penalty to be incurred in translating the rest of the sentence. The second modification uses early pruning of possible next-phrase translations to cut down the overall size of the search space. These modifications enable decoding speed-ups of an order of magnitude or more, with no reduction in the B LEU score of the resulting translations. 2. A Phrasal SMT Model Phrasal SMT, as described by Koehn et al. (2003) translates a source sentence into"
2007.mtsummit-papers.43,koen-2004-pharaoh,0,\N,Missing
2007.mtsummit-papers.43,P06-1066,0,\N,Missing
2007.mtsummit-papers.43,J04-4002,0,\N,Missing
2007.mtsummit-papers.43,W06-1905,0,\N,Missing
2007.mtsummit-papers.43,W03-0301,0,\N,Missing
2007.mtsummit-papers.43,J03-1005,0,\N,Missing
2020.findings-emnlp.358,W17-5526,0,0.109715,"Missing"
2020.findings-emnlp.358,P02-1040,0,0.107529,"Missing"
2020.findings-emnlp.358,N19-1126,0,0.0440252,"Missing"
2020.findings-emnlp.358,P19-1004,0,0.0202239,"-art dialog systems, and • Publicly release improved testbeds for two datasets used extensively in goal-oriented dialog research: bAbI dialog task and SMD. Recently, few approaches have been explored to study the behavior of neural dialog systems in the presence of synthetically introduced perturbations to the dialog history. Eshghi et al. (2017) created the bAbI+ dataset, an extension of bAbI dialog task-1, by introducing variations like hesitations, restarts and corrections. Zhao and Eskenazi (2018) created SimDial, which simulates spoken language phenomena, e.g. self-repair and hesitation. Sankar et al. (2019) introduce utterance-level and wordlevel perturbations on various benchmarks. However, such variations have been largely artificial and do not reflect the ”natural variation” commonly found in naturally occuring conversational data. Geva et al. (2019) show that often models do not generalize well to examples from new annotators at test time who did not contribute to training data, which reinforces our choice of introducing natural variation in the test set for evaluation. 2 Datasets We study and observe issues in multiple goaloriented dialog benchmarks. In this work, we focus on two multi-turn"
2020.findings-emnlp.358,D18-1419,0,0.0462087,"Missing"
2020.findings-emnlp.358,E17-1042,0,0.0339745,"Missing"
2020.findings-emnlp.358,P18-1205,0,0.0306637,"full range of naturalistic variation (Schegloff et al., 1977; Moore and Arar, 2019). This naturalistic variation has been thoroughly documented in the Conversation Analysis literature (Sacks et al., 1974; Schegloff, 2007), and further adapted for designing automated conversational agents (Moore and Arar, 2019). The core reason for this omission is that natu4013 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4013–4020 c November 16 - 20, 2020. 2020 Association for Computational Linguistics ralistic variation is often confused with ”chit chat” (Dunbar et al., 1997; Zhang et al., 2018). Moore and Arar (2019, p. 121) writes, In common usage, “chit chat” means inconsequential talk. But much talk that may appear on the surface to be inconsequential in fact serves a variety of functions in managing the conversation itself. In this work, we focus on the full range of activities observed in naturally occurring conversations, referred to as ”natural variation”. Our goal in this work is three-fold: • Highlight the problem of unnatural data generated through crowdsourcing • Showcase the impact of natural variation in the performance of state-of-the-art dialog systems, and • Publicly"
A00-2033,H94-1010,0,0.0282265,"Missing"
A00-2033,P98-1101,0,0.0770733,", followed by PA. This produces another modest decrease in the size of the non-left-recursive form of the CT g r a m m a r and reduces the size of the nonleft-recursive form of the ATIS g r a m m a r by a factor of 27.8, compared to L F ÷ P A . The non-left-recursive form of the P T g r a m m a r remains larger than the cut-off size of 5,000,000 symbols, however. 5 Left-Recursion Elimination Based on the Left-Corner Transform An alternate approach to eliminating left-recursion is based on the left-corner (LC) g r a m m a r transform of Rosenkrantz and Lewis (1970) as presented and modified by Johnson (1998). Johnson's second form of the LC transform can be expressed as follows, with expressions of the form A-a, A - X , and A - B being new nonterminals in the transformed grammar: 1. If a terminal symbol a is a proper left corner of A in the original grammar, add A -4 aA-a to the transformed grammar. 2. If B is a proper left corner of A and B --+ X ~ is a production of the original grammar, add A - X -+ ~ A - B to the transformed grammar. 3. If X is a proper left corner of A and A --+ X ~ is a production of the original grammar, add A - X -+ ~ to the transformed grammar. In Rosenkrantz and Lewis's"
A00-2033,J93-2004,0,0.021539,"Missing"
A00-2033,A97-1001,0,0.0577639,"1: Grammars used for evaluation. 4 Paull's A l g o r i t h m Panll's algorithm for eliminating leftrecursion from CFGs attacks the problem by an iterativeprocedure for transforming indirect left recursion into direct left recursion, with a subprocedure for eliminating direct leftrecursion, This algorithm is perhaps more familiar to some as the firstphase of the textbook algorithm for transfomrming CFGs to Greibach norreal form (Greibach, 1965).3 The subprocedure to eliminate direct leftrecursion performs the following transformation (Hopcroft and UUman, 1979, p. 96): written for CommandTalk (Moore et al., 1997), a spoken-language interface to a military simulation system. The ATIS grammar was extracted from an internally generated treebank of the DARPA ATIS3 training sentences (Dahl et al., 1994). The P T grammar 2 was extracted from the Penn Treebank (Marcus et al., 1993). To these grammars we add a small ""toy"" grammar, simply because some of the algorithms cannot be run to completion on any of the ""real"" grammars within reasonable time and space bounds. Some statistics on the test grammars are contained in Table 1. The criterion we use to judge effectiveness of the algorithms under test is the siz"
A00-2033,C98-1098,0,\N,Missing
C02-1036,W02-2105,1,0.652431,"Missing"
C04-1080,H92-1022,0,0.0412024,"Missing"
C04-1080,W95-0101,0,0.96383,"ng, we believe that the way forward from the relatively small number of languages for which we can currently identify parts of speech in context with reasonable accuracy will make use of unsupervised methods that require only an untagged corpus and a lexicon of words and their possible parts of speech. We believe this based on the fact that such lexicons exist for many more languages (in the form of conventional dictionaries) than extensive human-tagged training corpora exist for. Unsupervised part-of-speech tagging, as defined above, has been attempted using a variety of learning algorithms (Brill 1995, Church, 1988, Cutting et. al. 1992, Elworthy, 1994 Kupiec 1992, Merialdo 1991). While this makes unsupervised part-of-speech tagging a relatively well-studied problem, published results to date have not been comparable with respect to the training and test data used, or the lexicons which have been made available to the learners. In this paper, we provide the first comprehensive comparison of methods for unsupervised part-ofspeech tagging. In addition, we explore two new ideas for improving tagging accuracy. First, we explore an HMM approach to tagging that uses context on both sides of the"
C04-1080,W02-1001,0,0.00896752,"r which we use a standard absolute discounting scheme (Ney, Essen and Knesser, 1994). 4 4.1 Unsupervised Tagging: A Comparison Corpora and Lexicon Construction For our comparison of unsupervised tagging methods, we implemented the HMM taggers described in Merialdo (1991) and Kupiec (1992), as well as the UTBL tagger described in Brill (1995). We also implemented a version of the contextualized HMM using the type of word classes utilized in the Kupiec model. The algorithms were trained and tested using version 3 of the Penn Treebank, using the training, development, and test split described in Collins (2002) and also employed by Toutanova et al. (2003) in testing their supervised tagging algorithm. Specifically, we allocated sections 0018 for training, 19-21 for development, and 22-24 for testing. To avoid the problem of unknown words, each learner was provided with a lexicon constructed from tagged versions of the full Treebank. We did not begin with any estimates of the likelihoods of tags for words, but only the knowledge of what tags are possible for each word in the lexicon, i.e., something we could obtain from a manually-constructed dictionary. 4.2 The Effect of Lexicon Construction on Tagg"
C04-1080,A92-1018,0,0.387854,"Missing"
C04-1080,A94-1009,0,0.17557,"atively small number of languages for which we can currently identify parts of speech in context with reasonable accuracy will make use of unsupervised methods that require only an untagged corpus and a lexicon of words and their possible parts of speech. We believe this based on the fact that such lexicons exist for many more languages (in the form of conventional dictionaries) than extensive human-tagged training corpora exist for. Unsupervised part-of-speech tagging, as defined above, has been attempted using a variety of learning algorithms (Brill 1995, Church, 1988, Cutting et. al. 1992, Elworthy, 1994 Kupiec 1992, Merialdo 1991). While this makes unsupervised part-of-speech tagging a relatively well-studied problem, published results to date have not been comparable with respect to the training and test data used, or the lexicons which have been made available to the learners. In this paper, we provide the first comprehensive comparison of methods for unsupervised part-ofspeech tagging. In addition, we explore two new ideas for improving tagging accuracy. First, we explore an HMM approach to tagging that uses context on both sides of the word to be tagged, inspired by previous work on buil"
C04-1080,P98-2177,0,0.0134582,"Missing"
C04-1080,N03-1033,0,0.0817572,"iscounting scheme (Ney, Essen and Knesser, 1994). 4 4.1 Unsupervised Tagging: A Comparison Corpora and Lexicon Construction For our comparison of unsupervised tagging methods, we implemented the HMM taggers described in Merialdo (1991) and Kupiec (1992), as well as the UTBL tagger described in Brill (1995). We also implemented a version of the contextualized HMM using the type of word classes utilized in the Kupiec model. The algorithms were trained and tested using version 3 of the Penn Treebank, using the training, development, and test split described in Collins (2002) and also employed by Toutanova et al. (2003) in testing their supervised tagging algorithm. Specifically, we allocated sections 0018 for training, 19-21 for development, and 22-24 for testing. To avoid the problem of unknown words, each learner was provided with a lexicon constructed from tagged versions of the full Treebank. We did not begin with any estimates of the likelihoods of tags for words, but only the knowledge of what tags are possible for each word in the lexicon, i.e., something we could obtain from a manually-constructed dictionary. 4.2 The Effect of Lexicon Construction on Tagging Accuracy To our surprise, we found initia"
C04-1080,A88-1019,0,\N,Missing
C04-1080,J94-2001,0,\N,Missing
C04-1080,C98-2172,0,\N,Missing
C04-1097,W01-0808,0,0.0216418,"German and find that a particular conditional model outperforms all others. We employ a version of that model in an evaluation on unordered trees from the Penn TreeBank. We offer this result on standard data as a reference-point for evaluations of ordering in sentence realization. 1 Introduction Word and constituent order play a crucial role in establishing the fluency and intelligibility of a sentence. In some systems, establishing order during the sentence realization stage of natural language generation has been accomplished by hand-crafted generation grammars in the past (see for example, Aikawa et al. (2001) and Reiter and Dale (2000)). In contrast, the Nitrogen (Langkilde and Knight, 1998a, 1998b) system employs a word n-gram language model to choose among a large set of word sequence candidates which vary in constituent order, word order, lexical choice, and morphological inflection. Nitrogen’s model does not take into consideration any non-surface linguistic features available during realization. The Fergus system (Bangalore and Rambow, 2000) employs a statistical tree model to select probable trees and a word n-gram model to rank the string candidates generated from the best trees. Like Nitro"
C04-1097,C00-1007,0,0.177575,"Missing"
C04-1097,A00-2018,0,0.0602743,"stems operating on similar input, and (2) to measure Amalgam’s capabilities on less domain-specific data than technical software manuals. We derive from the bracketed tree structures in the PTB using a deterministic procedure an abstract representation we refer to as a Dependency Structure Input Format (DSIF), which is only loosely related to NLPWin’s abstract predicateargument structures. The PTB to DSIF transformation pipeline includes the following stages, inspired by Langkilde-Geary’s (2002b) description: A. Deserialize the tree B. Label heads, according to Charniak’s head labeling rules (Charniak, 2000) C. Remove empty nodes and flatten any remaining empty non-terminals D. Relabel heads to conform more closely to the head conventions of NLPWin E. Label with logical roles, inferred from PTB functional roles F. Flatten to maximal projections of heads (MPH), except in the case of conjunctions G. Flatten non-branching non-terminals H. Perform dictionary look-up and morphological analysis I. Introduce structure for material between paired delimiters and for any coordination not already represented in the PTB J. Remove punctuation K. Remove function words L. Map the head of each maximal projection"
C04-1097,W02-2105,1,0.738121,"n system (Langkilde, 2000; Langkilde-Geary, 2002a, 2002b) uses word n-grams, but it extracts the best-scoring surface realizations efficiently from a packed forest by constraining the search first within the scope of each constituent. drojas@indiana.edu Our research is carried out within the Amalgam broad coverage sentence realization system. Amalgam generates sentence strings from abstract predicate-argument structures (Figure 1), using a pipeline of stages, many of which employ machine-learned models to predict where to perform specific linguistic operations based on the linguistic context (Corston-Oliver et al., 2002; Gamon et al., 2002a, 2002b; Smets et al., 2003). Amalgam has an explicit ordering stage that determines the order of constituents and their daughters. The input for this stage is an unordered tree of constituents; the output is an ordered tree of constituents or a ranked list of such trees. For ordering, Amalgam leverages tree constituent structure and, importantly, features of those constituents and the surrounding context. By separately establishing order within constituents, Amalgam heavily constrains the possible alternatives in later stages of the realization process. The design allows"
C04-1097,W02-2102,0,0.0284067,"Missing"
C04-1097,P02-1004,1,0.834978,"ngkilde-Geary, 2002a, 2002b) uses word n-grams, but it extracts the best-scoring surface realizations efficiently from a packed forest by constraining the search first within the scope of each constituent. drojas@indiana.edu Our research is carried out within the Amalgam broad coverage sentence realization system. Amalgam generates sentence strings from abstract predicate-argument structures (Figure 1), using a pipeline of stages, many of which employ machine-learned models to predict where to perform specific linguistic operations based on the linguistic context (Corston-Oliver et al., 2002; Gamon et al., 2002a, 2002b; Smets et al., 2003). Amalgam has an explicit ordering stage that determines the order of constituents and their daughters. The input for this stage is an unordered tree of constituents; the output is an ordered tree of constituents or a ranked list of such trees. For ordering, Amalgam leverages tree constituent structure and, importantly, features of those constituents and the surrounding context. By separately establishing order within constituents, Amalgam heavily constrains the possible alternatives in later stages of the realization process. The design allows for interaction betw"
C04-1097,P03-1054,0,0.0343259,"Missing"
C04-1097,A00-2023,0,0.0212868,"0)). In contrast, the Nitrogen (Langkilde and Knight, 1998a, 1998b) system employs a word n-gram language model to choose among a large set of word sequence candidates which vary in constituent order, word order, lexical choice, and morphological inflection. Nitrogen’s model does not take into consideration any non-surface linguistic features available during realization. The Fergus system (Bangalore and Rambow, 2000) employs a statistical tree model to select probable trees and a word n-gram model to rank the string candidates generated from the best trees. Like Nitrogen, the HALogen system (Langkilde, 2000; Langkilde-Geary, 2002a, 2002b) uses word n-grams, but it extracts the best-scoring surface realizations efficiently from a packed forest by constraining the search first within the scope of each constituent. drojas@indiana.edu Our research is carried out within the Amalgam broad coverage sentence realization system. Amalgam generates sentence strings from abstract predicate-argument structures (Figure 1), using a pipeline of stages, many of which employ machine-learned models to predict where to perform specific linguistic operations based on the linguistic context (Corston-Oliver et al., 20"
C04-1097,W02-2103,0,0.294206,"the Nitrogen (Langkilde and Knight, 1998a, 1998b) system employs a word n-gram language model to choose among a large set of word sequence candidates which vary in constituent order, word order, lexical choice, and morphological inflection. Nitrogen’s model does not take into consideration any non-surface linguistic features available during realization. The Fergus system (Bangalore and Rambow, 2000) employs a statistical tree model to select probable trees and a word n-gram model to rank the string candidates generated from the best trees. Like Nitrogen, the HALogen system (Langkilde, 2000; Langkilde-Geary, 2002a, 2002b) uses word n-grams, but it extracts the best-scoring surface realizations efficiently from a packed forest by constraining the search first within the scope of each constituent. drojas@indiana.edu Our research is carried out within the Amalgam broad coverage sentence realization system. Amalgam generates sentence strings from abstract predicate-argument structures (Figure 1), using a pipeline of stages, many of which employ machine-learned models to predict where to perform specific linguistic operations based on the linguistic context (Corston-Oliver et al., 2002; Gamon et al., 2002a"
C04-1097,W98-1426,0,0.0250888,"e employ a version of that model in an evaluation on unordered trees from the Penn TreeBank. We offer this result on standard data as a reference-point for evaluations of ordering in sentence realization. 1 Introduction Word and constituent order play a crucial role in establishing the fluency and intelligibility of a sentence. In some systems, establishing order during the sentence realization stage of natural language generation has been accomplished by hand-crafted generation grammars in the past (see for example, Aikawa et al. (2001) and Reiter and Dale (2000)). In contrast, the Nitrogen (Langkilde and Knight, 1998a, 1998b) system employs a word n-gram language model to choose among a large set of word sequence candidates which vary in constituent order, word order, lexical choice, and morphological inflection. Nitrogen’s model does not take into consideration any non-surface linguistic features available during realization. The Fergus system (Bangalore and Rambow, 2000) employs a statistical tree model to select probable trees and a word n-gram model to rank the string candidates generated from the best trees. Like Nitrogen, the HALogen system (Langkilde, 2000; Langkilde-Geary, 2002a, 2002b) uses word"
C04-1097,P98-1116,0,0.095937,"e employ a version of that model in an evaluation on unordered trees from the Penn TreeBank. We offer this result on standard data as a reference-point for evaluations of ordering in sentence realization. 1 Introduction Word and constituent order play a crucial role in establishing the fluency and intelligibility of a sentence. In some systems, establishing order during the sentence realization stage of natural language generation has been accomplished by hand-crafted generation grammars in the past (see for example, Aikawa et al. (2001) and Reiter and Dale (2000)). In contrast, the Nitrogen (Langkilde and Knight, 1998a, 1998b) system employs a word n-gram language model to choose among a large set of word sequence candidates which vary in constituent order, word order, lexical choice, and morphological inflection. Nitrogen’s model does not take into consideration any non-surface linguistic features available during realization. The Fergus system (Bangalore and Rambow, 2000) employs a statistical tree model to select probable trees and a word n-gram model to rank the string candidates generated from the best trees. Like Nitrogen, the HALogen system (Langkilde, 2000; Langkilde-Geary, 2002a, 2002b) uses word"
C04-1097,2001.mtsummit-papers.68,0,0.0144316,"ultiple random scramblings and average the results. We use the evaluation metrics employed in published evaluations of HALogen, FUF/SURGE, and FERGUS (e.g., Calloway, 2003), although our results are for ordering only. Coverage, or the percentage of inputs for which a system can produce a corresponding output, is uninformative for the Amalgam system, since in all cases, it can generate an output for any given DSIF. In addition to processing time per input, we apply four other metrics: exact match, NIST simple string accuracy (the complement of the familiar word error rate), the IBM Bleu score (Papineni et al., 2001), and the intra-constituent edit distance metric introduced earlier. We evaluate against ideal trees, directly computed from PTB bracketed tree structures. The results in Table 2 show the effects of varying the IOCC parameter. For both trials involving a greedy search, the results were averaged across 25 iterations. As should be expected, turning on the input-output faithfulness option (IOCC) improves the performance of the greedy search. Keeping coordinated material in the same relative order would only be called for in applications that plan discourse structure before or during generation. 7"
C04-1097,2003.jeptalnrecital-long.23,1,0.608048,"uses word n-grams, but it extracts the best-scoring surface realizations efficiently from a packed forest by constraining the search first within the scope of each constituent. drojas@indiana.edu Our research is carried out within the Amalgam broad coverage sentence realization system. Amalgam generates sentence strings from abstract predicate-argument structures (Figure 1), using a pipeline of stages, many of which employ machine-learned models to predict where to perform specific linguistic operations based on the linguistic context (Corston-Oliver et al., 2002; Gamon et al., 2002a, 2002b; Smets et al., 2003). Amalgam has an explicit ordering stage that determines the order of constituents and their daughters. The input for this stage is an unordered tree of constituents; the output is an ordered tree of constituents or a ranked list of such trees. For ordering, Amalgam leverages tree constituent structure and, importantly, features of those constituents and the surrounding context. By separately establishing order within constituents, Amalgam heavily constrains the possible alternatives in later stages of the realization process. The design allows for interaction between ordering choices and othe"
C04-1097,P02-1040,0,\N,Missing
C04-1097,C98-1112,0,\N,Missing
C08-1074,W06-3114,0,0.0262178,"h feature weight according to a uniform distribution over a fixed interval, say −1.0 to +1.0. The best point reached, starting from either the previous optimum or one of the random restart points, is selected as the optimum for the current set of hypotheses. This widely-used procedure is described by Koehn et al. (2007, p. 50). 3.1 Preliminary evaluation In our first experiments, we compared a variant of Och’s MERT procedure with and without random restarts as described above. For our training and test data we used the English-French subset of the Europarl corpus provided for the shared task (Koehn and Monz, 2006) at the Statistical Machine Translation workshop held in conjunction with the 2006 HLT-NAACL conference. We built a standard baseline phrasal SMT system, as described by Koehn et al. (2003), for translating from English to French (E-to-F), using the word alignments and French target language model provided by the workshop organizers. We trained a model with the standard eight features: E-to-F and F-to-E phrase translation log 1 Since additional hypotheses have been added, initiating an optimization search from this point on the new set of hypotheses will often lead to a higher local optimum. 5"
C08-1074,N03-1017,0,0.0051363,"ints, is selected as the optimum for the current set of hypotheses. This widely-used procedure is described by Koehn et al. (2007, p. 50). 3.1 Preliminary evaluation In our first experiments, we compared a variant of Och’s MERT procedure with and without random restarts as described above. For our training and test data we used the English-French subset of the Europarl corpus provided for the shared task (Koehn and Monz, 2006) at the Statistical Machine Translation workshop held in conjunction with the 2006 HLT-NAACL conference. We built a standard baseline phrasal SMT system, as described by Koehn et al. (2003), for translating from English to French (E-to-F), using the word alignments and French target language model provided by the workshop organizers. We trained a model with the standard eight features: E-to-F and F-to-E phrase translation log 1 Since additional hypotheses have been added, initiating an optimization search from this point on the new set of hypotheses will often lead to a higher local optimum. 586 probabilities, E-to-F and F-to-E phrase translation lexical scores, French language model log probabilities, phrase pair count, French word count, and distortion score. Feature weight op"
C08-1074,W04-3250,0,0.16091,"Missing"
C08-1074,P03-1021,0,0.165598,"dure is the most commonly used method for training feature weights in statistical machine translation (SMT) models. The use of multiple randomized starting points in MERT is a well-established practice, although there seems to be no published systematic study of its benefits. We compare several ways of performing random restarts with MERT. We find that all of our random restart methods outperform MERT without random restarts, and we develop some refinements of random restarts that are superior to the most common approach with regard to resulting model quality and training time. 1 Introduction Och (2003) introduced minimum error rate training (MERT) for optimizing feature weights in statistical machine translation (SMT) models, and demonstrated that it produced higher translation quality scores than maximizing the conditional likelihood of a maximum entropy model using the same features. Och’s method performs a series of one-dimensional optimizations of the feature weight vector, using an innovative line search that takes advantage of special properties of the mapping from sets of feature weights to the resulting translation quality measurement. Och’s line search is guaranteed to find a globa"
C14-1110,C04-1080,1,0.844768,"ith Intel Xeon X5550 2.67 GHz processors. In this implementation, feature weight combination increases tagging throughput by a factor of 5.82. 3.2 Pruning Possible Tags It has long been standard practice to prune the set of possible tags considered for each word, in order to speed up tagging. Ratnaparkhi (1996) may have been the first to use the common heuristic of defining a tag dictionary allowing any tag for unknown words, but restricting each known word to the tags it was observed with in the training data. In addition, the tag dictionary for known words is sometimes further pruned (e.g., Banko and Moore, 2004; Gim´enez and M`arquez, 2004, 2012) according to the relative frequency of tags for each word. Tags observed in the training data with less than some fixed proportion of the occurrences of a particular word are not considered as possible tags for that word in test data. In our experiments, we find these heuristics produce fast tagging, but lead to a noticable loss of accuracy, because known words are never allowed to be labeled with tags they were not observed with in the training data. This is similar to the problem of unseen n-grams in statistical language modeling, so we apply methods deve"
C14-1110,N06-1022,0,0.0192013,"considered not to be a possible POS tag for the word w. Our preferred threshold (p(t|w) &gt; 0.0005) is set to prune as agressively as possible while maintaining tagging accuracy on the WSJ development set. This threshold is applied to both known and unknown words, which produces 24 possible tags for unknown words by applying the threshold to the lower-order probability estimate p(t). Note that the probabilities we use for pruning can be viewed as posteriors of a very simple POS tagging model, which makes inferring a tag dictionary an instance of coarse-to-fine inference with posterior pruning (Charniak et al., 2006; Weiss and Taskar, 2010). The standard tag dictionary pruning heuristics can be viewed as a application of the same approach, but with the p(t|w) probabilities being unsmoothed relative-frequency estimates for known words and a uniform distribution for unknown words. The original Ratnaparkhi heuristic amounts to thresholding these probabilities at 0, with a higher threshold being applied when using additional pruning. 1172 Pruning Method None Ratnaparkhi Ratnaparkhi+ Kneser-Ney Kneser-Ney+ Tags per Token 45.0 3.7 2.9 3.5 1.8 Weights per Token 194.0 19.0 14.3 16.1 6.1 Tokens per Second 6400 47"
C14-1110,W02-1001,0,0.836759,"s seems to be the results presented by Liang et al. (2008). Their best model uses the following set of base features for each word: Whether the first character of the word is a capital letter Prefixes of the word up to three characters Suffixes of the word up to three characters Two “shape” features described below The full word For each base feature, Liang et al. define three expanded features: whether the token being tagged has the base feature, whether the preceding token has the base feature, and whether the following token has the base feature. The shape features were first introduced by Collins (2002b) for named-entity recognition. What we will call the “Shape 1” feature is a generalization of the spelling of the word with all capital 1166 letters treated as equivalent, all lower-case letters treated as equivalent, and all digits treated as equivalent. All other characters are treated as distinct. In the “Shape 2” feature, all sequences of capital letters, all sequences of lower case letters, and all sequences of digits are treated as equivalent, regardless of the length of the sequence or the identity of the upper case letters, lower case letters, or digits. With this feature set as our"
C14-1110,W03-0425,0,0.14639,"Missing"
C14-1110,gimenez-marquez-2004-svmtool,0,0.0979087,"Missing"
C14-1110,N12-1015,0,0.0692281,"Missing"
C14-1110,J93-2004,0,0.0502132,"t al., 2003). State-of-the-art-taggers typically employ discriminatively-trained models with hidden tag-sequence features. These models include features of the observable input sequence, plus hidden features consisting of tag sequences up to some fixed length. With a tag-sequence model, the highest scoring tagging for an input sentence can be found by the Viterbi algorithm, but exact search can be slow with a large tag set. If tri-tag features are used, the full search space is O(|T |3 n), where |T |is the size of the tag set and n is the length of the sentence. For the English Penn Treebank (Marcus et al., 1993) , |T |= 45, hence |T |3 = 91125. For efficiency, some form of approximate search is normally used. For example, both Shen et al. (2007) and Huang et al. (2012) use approximate search in both training and tagging. Shen et al. use a specialized bi-directional beam search in which the search order is learned at training time and applied at tagging time, along with the model. Huang et al. use a more conventional left-to-right beam search, but they explore various special variants of the perceptron algorithm to cope with search errors during model training. These two taggers represent the current"
C14-1110,P05-1012,0,0.175307,"Missing"
C14-1110,W96-0213,0,0.93809,"” feature is a generalization of the spelling of the word with all capital 1166 letters treated as equivalent, all lower-case letters treated as equivalent, and all digits treated as equivalent. All other characters are treated as distinct. In the “Shape 2” feature, all sequences of capital letters, all sequences of lower case letters, and all sequences of digits are treated as equivalent, regardless of the length of the sequence or the identity of the upper case letters, lower case letters, or digits. With this feature set as our starting point, and partially drawing from the feature sets of Ratnaparkhi (1996) and Collins (2002a), we settled on the following set of base features through experimentation on the WSJ development set: Whether the word contains a capital letter Whether the word contains a digit Whether the word contains a hyphen Lower-cased prefixes of the word up to four characters Lower-cased suffixes of the word up to four characters The Shape 1 feature for the word The Shape 2 feature for the word The full lower-cased word The full word A distributional-similarity-based class for the full word In all these features we ignore distinctions among digits (rather than just in the shape fe"
C14-1110,P07-1096,0,0.163921,"Missing"
C14-1110,E09-1087,0,0.036586,"Missing"
C14-1110,P11-2009,0,0.0156104,"had direct access to on the Brown Corpus subset (3279 sentences, 83769 tokens) from the Penn Treebank. As might be expected, tagging was in general both slower and less accurate than on in-domain data. Our tagger maintained its relative position with respect to both speed and accuracy compared to all the other taggers. The only qualitative change in position of any tagger is that on the Brown Corpus data, the accurate Stanford tagger is slower than COMPOST, which actually runs faster than it does on the WSJ test set. 3 A fourth tagger, the semi-supervised condensed nearest neighbor tagger of Søgaard (2011), has some released source code, but not a complete tagger nor detailed instructions on how to build the tagger Søgaard evaluates. 1174 5 Conclusions We have shown that a feature-rich model for POS tagging by independent classifiers can reach tagging accuracies comparable to several state-of-the art taggers, and we have introduced implementation strategies that result in much faster tagging than any other high-accuracy tagger we are aware of, despite these other taggers being implemented in faster programming languages. A number of the techniques introduced here may have applications to other"
C14-1110,N03-1033,0,0.430999,"Missing"
C14-1110,P02-1062,0,\N,Missing
C82-1015,P81-1028,1,0.764824,"Missing"
C82-1015,P82-1001,1,0.832758,"Missing"
C94-1099,J88-1004,0,\N,Missing
C94-1099,J86-3002,0,\N,Missing
C94-1099,A88-1007,0,\N,Missing
C94-1099,H92-1003,0,\N,Missing
C94-1099,P93-1008,1,\N,Missing
C94-1099,H93-1008,1,\N,Missing
D08-1011,J07-2003,0,0.127138,"ion effort, the results on the dev set are reported in case insensitive BLEU (ciBLEU) score instead. 5.3 Experimental results In our main experiments, outputs from a total of eight single MT systems were combined. As listed in Table 1, Sys-1 is a tree-to-string system proposed by Quirk et al., (2005); Sys-2 is a phrasebased system with fast pruning proposed by Moore and Quirk (2008); Sys-3 is a phrase-based system with syntactic source reordering proposed by Wang et al. (2007a); Sys-4 is a syntax-based preordering system proposed by Li et. al. (2007); Sys5 is a hierarchical system proposed by Chiang (2007); Sys-6 is a lexicalized re-ordering system proposed by Xiong et al. (2006); Sys-7 is a twopass phrase-based system with adapted LM proposed by Foster and Kuhn (2007); and Sys-8 is a hierarchical system with two-pass rescoring using a parser-based LM proposed by Wang et al., (2007b). All systems were trained within the confines of the constrained training condition of NIST MT08 evaluation. These single systems are optimized with maximum-BLEU training on different subsets of the previous NIST MT test data. The bilingual translation models used to compute the semantic similarity are from the wor"
D08-1011,W07-0717,0,0.033654,"s from a total of eight single MT systems were combined. As listed in Table 1, Sys-1 is a tree-to-string system proposed by Quirk et al., (2005); Sys-2 is a phrasebased system with fast pruning proposed by Moore and Quirk (2008); Sys-3 is a phrase-based system with syntactic source reordering proposed by Wang et al. (2007a); Sys-4 is a syntax-based preordering system proposed by Li et. al. (2007); Sys5 is a hierarchical system proposed by Chiang (2007); Sys-6 is a lexicalized re-ordering system proposed by Xiong et al. (2006); Sys-7 is a twopass phrase-based system with adapted LM proposed by Foster and Kuhn (2007); and Sys-8 is a hierarchical system with two-pass rescoring using a parser-based LM proposed by Wang et al., (2007b). All systems were trained within the confines of the constrained training condition of NIST MT08 evaluation. These single systems are optimized with maximum-BLEU training on different subsets of the previous NIST MT test data. The bilingual translation models used to compute the semantic similarity are from the worddependent HMMs proposed by He (2007), which are trained on two million parallel sentence-pairs selected from the training corpus allowed by the constrained training"
D08-1011,P06-1121,0,0.056997,"Missing"
D08-1011,W07-0711,1,0.77443,"ordering system proposed by Xiong et al. (2006); Sys-7 is a twopass phrase-based system with adapted LM proposed by Foster and Kuhn (2007); and Sys-8 is a hierarchical system with two-pass rescoring using a parser-based LM proposed by Wang et al., (2007b). All systems were trained within the confines of the constrained training condition of NIST MT08 evaluation. These single systems are optimized with maximum-BLEU training on different subsets of the previous NIST MT test data. The bilingual translation models used to compute the semantic similarity are from the worddependent HMMs proposed by He (2007), which are trained on two million parallel sentence-pairs selected from the training corpus allowed by the constrained training condition of MT08. 5.3.1 Comparison with TER alignment In the IHMM-based method, the smoothing factor for surface similarity model is set to ρ = 3, the interpolation factor of the overall similarity model is set to α = 0.3, and the controlling factor of the distance-based distortion parameters is set to K=2. These settings are optimized on the dev set. Individual system results and system combination results using both IHMM and TER alignment, on both the dev and test"
D08-1011,P05-3026,0,0.323475,"onnormalized version of our IHMM, in which the similarity model assigns no penalty to an exact surface match and a fixed penalty to all substitutions, insertions, and deletions, and the distortion model simply assigns no penalty to a monotonic jump, and a fixed penalty to all other jumps, equal to the non-exact-match penalty in the similarity model. There have been other hypothesis alignment methods. Karakos, et al. (2008) proposed an ITGbased method for hypothesis alignment, Rosti et al. (2008) proposed an incremental alignment method, and a heuristic-based matching algorithm was proposed by Jayaraman and Lavie (2005). 5 Evaluation In this section, we evaluate our IHMM-based hypothesis alignment method on the Chinese-toEnglish (C2E) test in the constrained training track of the 2008 NIST Open MT Evaluation (NIST, 2008). We compare to the TER-based method used by Rosti et al. (2007). In the following experiments, the NIST BLEU score is used as the evaluation metric (Papineni et al., 2002), which is reported as a percentage in the following sections. 5.1 Implementation details In our implementation, the backbone is selected with MBR. Only the top hypothesis from each single system is considered as a backbone"
D08-1011,P08-2021,0,0.424303,"while it is only used implicitly via parameter initialization for IBM Model-1 training by Matusov et al. (2006). On the other hand, the TER-based alignment model is similar to a coarse-grained, nonnormalized version of our IHMM, in which the similarity model assigns no penalty to an exact surface match and a fixed penalty to all substitutions, insertions, and deletions, and the distortion model simply assigns no penalty to a monotonic jump, and a fixed penalty to all other jumps, equal to the non-exact-match penalty in the similarity model. There have been other hypothesis alignment methods. Karakos, et al. (2008) proposed an ITGbased method for hypothesis alignment, Rosti et al. (2008) proposed an incremental alignment method, and a heuristic-based matching algorithm was proposed by Jayaraman and Lavie (2005). 5 Evaluation In this section, we evaluate our IHMM-based hypothesis alignment method on the Chinese-toEnglish (C2E) test in the constrained training track of the 2008 NIST Open MT Evaluation (NIST, 2008). We compare to the TER-based method used by Rosti et al. (2007). In the following experiments, the NIST BLEU score is used as the evaluation metric (Papineni et al., 2002), which is reported as"
D08-1011,P07-1091,0,0.038442,"Missing"
D08-1011,N06-1014,0,0.101297,"ld be based on exact match: the surface similarity model, psur (ej |ei ) , would take the value 1.0 if e’= e, and ( f k |ei ) where pt 2s ( fk |ei ) is the translation model from the target-to-source word alignment model. In our method, pt 2 s (null |ei ) for all target words is 100 The other direction, ps 2t (ei |null ) , is available from the source-to-target translation model. 2 Usually a small back-off value is assigned instead of 0. 1 p(a j  i |a j1  i, I ) depend only on the jump distance (i - i') (Vogel et al., 1996): p(i |i, I )  c(i  i) I c(l  i) (5) l 1 As suggested by Liang et al. (2006), we can group the distortion parameters {c(d)}, d= i - i', into a few buckets. In our implementation, 11 buckets are used for c(≤-4), c(-3), ... c(0), ..., c(5), c(≥6). The probability mass for transitions with jump distance larger than 6 and less than -4 is uniformly divided. By doing this, only a handful of c(d) parameters need to be estimated. Although it is possible to estimate them using the EM algorithm on a small development set, we found that a particularly simple model, described below, works surprisingly well in our experiments. Since both the backbone and the hypothesis are in the"
D08-1011,E06-1005,0,0.643756,"significantly outperforms the state-of-the-art TER-based alignment model in our experiments on NIST benchmark datasets. Our combined SMT system using the proposed method achieved the best Chinese-to-English translation result in the constrained training track of the 2008 NIST Open MT Evaluation. 1 Introduction* System combination has been applied successfully to various machine translation tasks. Recently, confusion-network-based system combination algorithms have been developed to combine outputs of multiple machine translation (MT) systems to form a consensus output (Bangalore, et al. 2001, Matusov et al., 2006, Rosti et al., 2007, Sim et al., 2007). A confusion network comprises a sequence of sets of alternative words, possibly including null’s, with associated scores. The consensus output is then derived by selecting one word from each set of alternatives, to produce the sequence with the best overall score, which could be assigned in various ways such as by voting, by * Mei Yang performed this work when she was an intern with Microsoft Research. using posterior probability estimates, or by using a combination of these measures and other features. Constructing a confusion network requires choosing"
D08-1011,2007.mtsummit-papers.43,1,0.70273,"Missing"
D08-1011,P02-1040,0,0.108033,"sis alignment methods. Karakos, et al. (2008) proposed an ITGbased method for hypothesis alignment, Rosti et al. (2008) proposed an incremental alignment method, and a heuristic-based matching algorithm was proposed by Jayaraman and Lavie (2005). 5 Evaluation In this section, we evaluate our IHMM-based hypothesis alignment method on the Chinese-toEnglish (C2E) test in the constrained training track of the 2008 NIST Open MT Evaluation (NIST, 2008). We compare to the TER-based method used by Rosti et al. (2007). In the following experiments, the NIST BLEU score is used as the evaluation metric (Papineni et al., 2002), which is reported as a percentage in the following sections. 5.1 Implementation details In our implementation, the backbone is selected with MBR. Only the top hypothesis from each single system is considered as a backbone. A uniform posteriori probability is assigned to all hypotheses. TER is used as loss function in the MBR computation. Similar to (Rosti et al., 2007), each word in the confusion network is associated with a word posterior probability. Given a system S, each of its hypotheses is assigned with a rank-based score of 1/(1+r)η, where r is the rank of the hypothesis, and η is a r"
D08-1011,N03-1017,0,0.0156845,"Missing"
D08-1011,P05-1034,0,0.0651166,"to the system combination, 10-best hypotheses for each source sentence in the dev and test sets are collected from each of the eight single systems. All outputs on the MT08 test set were true-cased before scoring using a log-linear conditional Markov model proposed by Toutanova et al. (2008). However, to save computation effort, the results on the dev set are reported in case insensitive BLEU (ciBLEU) score instead. 5.3 Experimental results In our main experiments, outputs from a total of eight single MT systems were combined. As listed in Table 1, Sys-1 is a tree-to-string system proposed by Quirk et al., (2005); Sys-2 is a phrasebased system with fast pruning proposed by Moore and Quirk (2008); Sys-3 is a phrase-based system with syntactic source reordering proposed by Wang et al. (2007a); Sys-4 is a syntax-based preordering system proposed by Li et. al. (2007); Sys5 is a hierarchical system proposed by Chiang (2007); Sys-6 is a lexicalized re-ordering system proposed by Xiong et al. (2006); Sys-7 is a twopass phrase-based system with adapted LM proposed by Foster and Kuhn (2007); and Sys-8 is a hierarchical system with two-pass rescoring using a parser-based LM proposed by Wang et al., (2007b). All"
D08-1011,N07-1029,0,0.604428,"emented in GIZA++, and heuristically combines results from aligning in both directions. System combination based on this approach gives an improvement over the best single system. However, the number of hypothesis pairs for training is limited by the size of the test corpus. Also, MT hypotheses from the same source sentence are correlated with each other and these hypothesis pairs are not i.i.d. data samples. Therefore, GIZA++ training on such a data set may be unreliable. Bangalore et al. (2001) used a multiple stringmatching algorithm based on Levenshtein edit distance, and later Sim et al. (2007) and Rosti et al. (2007) extended it to a TER-based method for hypothesis alignment. TER (Snover et al., 2006) 3 This only happens if no hypothesis word is aligned to a backbone word but some hypothesis words are aligned to the null associated with that backbone word. 102 measures the minimum number of edits, including substitution, insertion, deletion, and shift of blocks of words, that are needed to modify a hypothesis so that it exactly matches the other hypothesis. The best alignment is the one that gives the minimum number of translation edits. TER-based confusion network construction and"
D08-1011,P07-1040,0,0.668355,"rms the state-of-the-art TER-based alignment model in our experiments on NIST benchmark datasets. Our combined SMT system using the proposed method achieved the best Chinese-to-English translation result in the constrained training track of the 2008 NIST Open MT Evaluation. 1 Introduction* System combination has been applied successfully to various machine translation tasks. Recently, confusion-network-based system combination algorithms have been developed to combine outputs of multiple machine translation (MT) systems to form a consensus output (Bangalore, et al. 2001, Matusov et al., 2006, Rosti et al., 2007, Sim et al., 2007). A confusion network comprises a sequence of sets of alternative words, possibly including null’s, with associated scores. The consensus output is then derived by selecting one word from each set of alternatives, to produce the sequence with the best overall score, which could be assigned in various ways such as by voting, by * Mei Yang performed this work when she was an intern with Microsoft Research. using posterior probability estimates, or by using a combination of these measures and other features. Constructing a confusion network requires choosing one of the hypothes"
D08-1011,W08-0329,0,0.532485,"1 training by Matusov et al. (2006). On the other hand, the TER-based alignment model is similar to a coarse-grained, nonnormalized version of our IHMM, in which the similarity model assigns no penalty to an exact surface match and a fixed penalty to all substitutions, insertions, and deletions, and the distortion model simply assigns no penalty to a monotonic jump, and a fixed penalty to all other jumps, equal to the non-exact-match penalty in the similarity model. There have been other hypothesis alignment methods. Karakos, et al. (2008) proposed an ITGbased method for hypothesis alignment, Rosti et al. (2008) proposed an incremental alignment method, and a heuristic-based matching algorithm was proposed by Jayaraman and Lavie (2005). 5 Evaluation In this section, we evaluate our IHMM-based hypothesis alignment method on the Chinese-toEnglish (C2E) test in the constrained training track of the 2008 NIST Open MT Evaluation (NIST, 2008). We compare to the TER-based method used by Rosti et al. (2007). In the following experiments, the NIST BLEU score is used as the evaluation metric (Papineni et al., 2002), which is reported as a percentage in the following sections. 5.1 Implementation details In our"
D08-1011,2006.amta-papers.25,0,0.112438,"ombination based on this approach gives an improvement over the best single system. However, the number of hypothesis pairs for training is limited by the size of the test corpus. Also, MT hypotheses from the same source sentence are correlated with each other and these hypothesis pairs are not i.i.d. data samples. Therefore, GIZA++ training on such a data set may be unreliable. Bangalore et al. (2001) used a multiple stringmatching algorithm based on Levenshtein edit distance, and later Sim et al. (2007) and Rosti et al. (2007) extended it to a TER-based method for hypothesis alignment. TER (Snover et al., 2006) 3 This only happens if no hypothesis word is aligned to a backbone word but some hypothesis words are aligned to the null associated with that backbone word. 102 measures the minimum number of edits, including substitution, insertion, deletion, and shift of blocks of words, that are needed to modify a hypothesis so that it exactly matches the other hypothesis. The best alignment is the one that gives the minimum number of translation edits. TER-based confusion network construction and system combination has demonstrated superior performance on various large-scale MT tasks (Rosti. et al, 2007)"
D08-1011,C96-2141,0,0.703549,"nice car a sedan he has hypothesis set he have ε good car E4 a ε sedan he has (c) hypothesis alignment EB  argmin TER( E, E) EE EE e.g., EB = E1 (b) backbone selection he have ε good car he has ε nice sedan it ε a nice car he has a ε sedan (d) confusion network Figure 1: Confusion-network-based combination. MT system 3 Indirect-HMM-based Hypothesis Alignment In confusion-network-based system combination for SMT, a major difficulty is aligning hypotheses to the backbone. One possible statistical model for word alignment is the HMM, which has been widely used for bilingual word alignment (Vogel et al., 1996, Och and Ney, 2003). In this paper, we propose an indirect-HMM method for monolingual hypothesis alignment. 99 3.1 IHMM for hypothesis alignment e1I  (e1,..., eI ) denote the backbone, e1J  (e1,..., eJ ) a hypothesis to be aligned to e1I , and a1J  (a1,..., aJ ) the alignment that specifies Let the position of the backbone word aligned to each hypothesis word. We treat each word in the backbone as an HMM state and the words in the hypothesis as the observation sequence. We use a first-order HMM, assuming that the emission probability p(ej |ea j ) depends only on the backbone word, and"
D08-1011,D07-1077,0,0.0163745,"were true-cased before scoring using a log-linear conditional Markov model proposed by Toutanova et al. (2008). However, to save computation effort, the results on the dev set are reported in case insensitive BLEU (ciBLEU) score instead. 5.3 Experimental results In our main experiments, outputs from a total of eight single MT systems were combined. As listed in Table 1, Sys-1 is a tree-to-string system proposed by Quirk et al., (2005); Sys-2 is a phrasebased system with fast pruning proposed by Moore and Quirk (2008); Sys-3 is a phrase-based system with syntactic source reordering proposed by Wang et al. (2007a); Sys-4 is a syntax-based preordering system proposed by Li et. al. (2007); Sys5 is a hierarchical system proposed by Chiang (2007); Sys-6 is a lexicalized re-ordering system proposed by Xiong et al. (2006); Sys-7 is a twopass phrase-based system with adapted LM proposed by Foster and Kuhn (2007); and Sys-8 is a hierarchical system with two-pass rescoring using a parser-based LM proposed by Wang et al., (2007b). All systems were trained within the confines of the constrained training condition of NIST MT08 evaluation. These single systems are optimized with maximum-BLEU training on different"
D08-1011,P06-1066,0,0.209309,"ve BLEU (ciBLEU) score instead. 5.3 Experimental results In our main experiments, outputs from a total of eight single MT systems were combined. As listed in Table 1, Sys-1 is a tree-to-string system proposed by Quirk et al., (2005); Sys-2 is a phrasebased system with fast pruning proposed by Moore and Quirk (2008); Sys-3 is a phrase-based system with syntactic source reordering proposed by Wang et al. (2007a); Sys-4 is a syntax-based preordering system proposed by Li et. al. (2007); Sys5 is a hierarchical system proposed by Chiang (2007); Sys-6 is a lexicalized re-ordering system proposed by Xiong et al. (2006); Sys-7 is a twopass phrase-based system with adapted LM proposed by Foster and Kuhn (2007); and Sys-8 is a hierarchical system with two-pass rescoring using a parser-based LM proposed by Wang et al., (2007b). All systems were trained within the confines of the constrained training condition of NIST MT08 evaluation. These single systems are optimized with maximum-BLEU training on different subsets of the previous NIST MT test data. The bilingual translation models used to compute the semantic similarity are from the worddependent HMMs proposed by He (2007), which are trained on two million par"
D08-1011,P08-1059,0,\N,Missing
D08-1011,J03-1002,0,\N,Missing
D08-1011,2005.eamt-1.20,0,\N,Missing
D08-1011,P08-1066,0,\N,Missing
D09-1078,D07-1090,0,0.153673,"Missing"
D09-1078,P96-1041,0,0.150307,"Missing"
D09-1078,W06-3114,0,0.0121708,"simply by dividing Seymore and Rosenfeld’s discounted N-gram count K by the total number of highest-order Ngrams in the training corpus. This is equivalent to smoothing only the highest-order conditional Ngram model in estimating p(w1 ...wn ), estimating all the lower-order probabilities in the chain by the corresponding MLE model. We refer to this joint probability estimate as “partially-smoothed”, and the one suggested by Stolcke as “fully-smoothed”. 5 5.1 Data and Base Smoothing Methods For training, parameter optimzation, and test data we used English text from the WMT-06 Europarl corpus (Koehn and Monz, 2006). We trained on the designated 1,003,349 sentences (27,493,499 words) of English language model training data, and used 2000 sentences each for testing and parameter optimization, from the English half of the English-French dev and devtest data sets. We conducted our experiments on seven language model smoothing methods. Five of these are well-known: (1) interpolated absolute discounting with one discount per N-gram length, estimated according to the formula derived by Ney et al. (1994); (2) Katz backoff with Good-Turing discounts for N-grams occurring 5 times or less; (3) backoff absolute dis"
D09-1078,P09-2088,1,0.839443,"derived by Ney et al. (1994); (2) Katz backoff with Good-Turing discounts for N-grams occurring 5 times or less; (3) backoff absolute discounting with Ney et al. formula discounts; (4) backoff absolute discounting with one discount used for all N-gram lengths, optimized on held-out data; (5) modified interpolated Kneser-Ney smoothing with three discounts per N-gram length, estimated according to the formulas suggested by Chen and Goodman (1998). We also experimented with two variants of a new smoothing method that we have recently developed. Full details of the new method are given elsewhere (Moore and Quirk, 2009), but since it is not well-known, we summarize the method here. Smoothed N-gram probabilities are defined by the formulas shown in Figure 1, for all n such that N ≥ n ≥ 2,3 where N is the greatest N-gram length used in the model. The novelty of this model is that, while it is an interpolated model, the interpolation weights β for the lower-order model Evaluation We carried out three sets of evaluations to test the new techniques described above. First we compared the perplexity of full models and models reduced by significance-based N-gram selection for seven language model smoothing methods."
D09-1078,H92-1021,0,\N,Missing
D15-1151,A92-1018,0,0.733383,"thod of constructing tag dictionaries for part-of-speech (POS) tagging. A tag dictionary is simply a list of words1 along with a set of possible tags for each word listed, plus one additional set of possible tags for all words not listed. Tag dictionaries are commonly used to speed up POS-tag inference by restricting the tags considered for a particular word to those specified by the dictionary. Early work on POS tagging generally relied heavily on manually constructed tag dictionaries, sometimes agumented with tag statistics derived from an annotated corpus (Leech et al., 1983; Church, 1988; Cutting et al., 1992). Merialdo (1994) relied only on a tag dictionary extracted from annotated data, but he used the annotated 1 According to the conventions of the field, POS tags are assigned to all tokens in a tokenized text, including punctuation marks and other non-word tokens. In this paper, all of these will be covered by the term word. 1.1 Tag Dictionaries and Tagging Speed A typical modern POS tagger applies a statistical model to compute a score for a sequence of tags t1 , . . . , tn given a sequence of words w1 , . . . , wn . The tag sequence assigned the highest score by the model for a given word seq"
D15-1151,P14-5010,0,0.0126008,"Missing"
D15-1151,A88-1019,0,0.741631,"esent a new method of constructing tag dictionaries for part-of-speech (POS) tagging. A tag dictionary is simply a list of words1 along with a set of possible tags for each word listed, plus one additional set of possible tags for all words not listed. Tag dictionaries are commonly used to speed up POS-tag inference by restricting the tags considered for a particular word to those specified by the dictionary. Early work on POS tagging generally relied heavily on manually constructed tag dictionaries, sometimes agumented with tag statistics derived from an annotated corpus (Leech et al., 1983; Church, 1988; Cutting et al., 1992). Merialdo (1994) relied only on a tag dictionary extracted from annotated data, but he used the annotated 1 According to the conventions of the field, POS tags are assigned to all tokens in a tokenized text, including punctuation marks and other non-word tokens. In this paper, all of these will be covered by the term word. 1.1 Tag Dictionaries and Tagging Speed A typical modern POS tagger applies a statistical model to compute a score for a sequence of tags t1 , . . . , tn given a sequence of words w1 , . . . , wn . The tag sequence assigned the highest score by the mod"
D15-1151,N06-1020,0,0.0507468,"h We now present a new method that reduces the average number of tags per token to about 1.5, with no loss of tagging accuracy. We apply a simple variant of Ratnaparkhi’s method, with a training set more than 4,000 times larger than the Penn Treebank WSJ training set. Since no such handannotated corpus exists, we create the training set automatically by running a version of our tagger on the LDC English Gigaword corpus. We thus describe our approach as a semi-supervised variant of Ratnaparkhi’s method. Our method can be viewed as an instance of the well-known technique of self-training (e.g., McClosky et al., 2006), but ours is the first use of self-training we know of for learning inference-time search-space pruning. We introduce two additional modifications of Ratnaparki’s approach. First, with such a large training corpus, we find it unnecessary to keep in the dictionary every tag observed with every word in the automatically-annotated data. So, we estimate a probability distribution over tags for each word in the dictionary according to unsmoothed relative tag frequencies, and include for each word in the dictionary only tags whose probability given the word is greater than a fixed threshold. Second"
D15-1151,J94-2001,0,0.569747,"g dictionaries for part-of-speech (POS) tagging. A tag dictionary is simply a list of words1 along with a set of possible tags for each word listed, plus one additional set of possible tags for all words not listed. Tag dictionaries are commonly used to speed up POS-tag inference by restricting the tags considered for a particular word to those specified by the dictionary. Early work on POS tagging generally relied heavily on manually constructed tag dictionaries, sometimes agumented with tag statistics derived from an annotated corpus (Leech et al., 1983; Church, 1988; Cutting et al., 1992). Merialdo (1994) relied only on a tag dictionary extracted from annotated data, but he used the annotated 1 According to the conventions of the field, POS tags are assigned to all tokens in a tokenized text, including punctuation marks and other non-word tokens. In this paper, all of these will be covered by the term word. 1.1 Tag Dictionaries and Tagging Speed A typical modern POS tagger applies a statistical model to compute a score for a sequence of tags t1 , . . . , tn given a sequence of words w1 , . . . , wn . The tag sequence assigned the highest score by the model for a given word sequence is selected"
D15-1151,C14-1110,1,0.918829,"for Faster Part-of-Speech Tagging Robert C. Moore Google Inc. bobmoore@google.com Abstract tags from his test data as well as his training data to construct his tag dictionary, so his evaluation was not really fair.2 Ratnaparkhi (1996) seems to have been the first to use a tag dictionary automatically extracted only from training data. Ratnaparkhi’s method of constructing a tag dictionary substantially speeds up tagging compared to considering every possible tag for every word, but it noticeably degrades accuracy when used with a current state-of-the-art tagging model. We recently presented (Moore, 2014) a new method of constructing a tag dictionary that produces a tagging speed-up comparable to Ratnaparkhi’s, but with no decrease in tagging accuracy. In this paper, we show that a very simple semi-supervised variant of Ratnaparkhi’s method results in a much tighter tag dictionary than either Ratnaparkhi’s or our previous method, with accuracy as high as we previously obtained, while allowing much faster tagging—more than 100,000 tokens per second even in a Perl implementation. Ratnaparkhi (1996) introduced a method of inferring a tag dictionary from annotated data to speed up part-of-speech t"
D15-1151,W96-0213,0,0.545108,"ceably degrades accuracy when used with a current state-of-the-art tagging model. We recently presented (Moore, 2014) a new method of constructing a tag dictionary that produces a tagging speed-up comparable to Ratnaparkhi’s, but with no decrease in tagging accuracy. In this paper, we show that a very simple semi-supervised variant of Ratnaparkhi’s method results in a much tighter tag dictionary than either Ratnaparkhi’s or our previous method, with accuracy as high as we previously obtained, while allowing much faster tagging—more than 100,000 tokens per second even in a Perl implementation. Ratnaparkhi (1996) introduced a method of inferring a tag dictionary from annotated data to speed up part-of-speech tagging by limiting the set of possible tags for each word. While Ratnaparkhi’s tag dictionary makes tagging faster but less accurate, an alternative tag dictionary that we recently proposed (Moore, 2014) makes tagging as fast as with Ratnaparkhi’s tag dictionary, but with no decrease in accuracy. In this paper, we show that a very simple semi-supervised variant of Ratnaparkhi’s method results in a much tighter tag dictionary than either Ratnaparkhi’s or our previous method, with accuracy as high"
D15-1151,N03-1033,0,0.648077,"Missing"
D15-1151,J93-2004,0,\N,Missing
E03-1035,P02-1051,0,0.134477,"Missing"
E03-1035,J93-1003,0,0.0606646,"Missing"
E03-1035,P93-1003,0,0.0847831,"Missing"
E03-1035,W97-0311,0,0.048397,"Missing"
E03-1035,J96-1001,0,0.111692,"Missing"
E03-1035,1995.tmi-1.28,0,0.0693412,"Missing"
E03-1035,W01-1412,0,0.0520447,"Missing"
E03-1035,J00-2004,0,\N,Missing
H05-1011,J93-2003,0,0.130009,"rmally used. These models have the the advantages that they are easy to add features to and they allow fast optimization of model parameters using small amounts of annotated data. 1 Motivation Bilingual word alignment is the first step of most current approaches to statistical machine translation. Although the best performing systems are “phrasebased” (e.g, Och and Ney, 2004), possible phrase translations are normally first extracted from wordaligned bilingual text segments. The standard approach to word alignment makes use of various combinations of five generative models developed at IBM by Brown et al. (1993), sometimes augmented by an HMM-based model or Och and Ney’s “Model 6” (Och and Ney, 2003). The best combinations of these models can produce high accuracy alignments, at least when trained on a large corpus of fairly direct translations in related languages. These standard models are less than ideal, however, in a number of ways, two of which we address in this paper. First, although the standard models can theoretically be trained without supervision, in practice various parameters are introduced that should be optimized using annotated data. For, example, Och and Ney (2003) suggest supervis"
H05-1011,W02-1001,0,0.445226,"ch possible word alignment considered, we simply multiply the values of each of these features by a corresponding weight to give a score for that feature, and sum the features scores to give an overall score for the alignment. The possible alignment having the best overall score is selected as the word alignment for that sentence pair. Thus, for a sentence pair (e, f ) we seek the alignment a ˆ such that a ˆ = argmaxa n  λi fi (a, e, f ) i=1 where the fi are features and the λi are weights. We optimize the model weights using a modified version of averaged perceptron learning as described by Collins (2002). This is fast to train, because selecting the feature weights is the last step in building the model and the “online” nature of perceptron learning allows the parameter optimization to converge quickly. Furthermore, no generative story has to be invented to explain how the features generate the data, so new features can be easily added without having to change the overall structure of the model. In theory, a disadvantage of a discrimintative approach compared to a generative approach is that it requires annotated data for training. In practice, however, effective discriminative models for wor"
H05-1011,W05-0814,0,0.0217525,"for CLP1 , but total training time for each model was around an hour. 7 Related Work When the first version of this paper was submitted for review, we could honestly state, “We are not aware of any previous work on discriminative word alignment models.” Callison-Burch et al. (2004) had investigated the use of small amounts of annotated data to help train the IBM and HMM models, but the models were still generative and were trained using maximum-likelihood methods. Recently, however, three efforts nearly simultaneous with ours have made use of discriminative methods to train alignment models. Fraser and Marcu (2005) modify Model 4 to be a log-linear combination of 11 submodels (5 based on standard Model 4 parameters, and 6 based on additional features) and discriminatively optimize the submodel weights on each iteration of a Viterbi approximation to EM. Liu et al. (2005) also develop a log-linear model, based on IBM Model 3. They train Model 3 using Giza++, and then use the Model 3 score of a possible alignment as a feature value in a discriminatively trained log-linear model, along with features incorporating part-of-speech information, and whether the aligned words are given as translations in a biling"
H05-1011,P05-1057,0,0.554536,"rch et al. (2004) had investigated the use of small amounts of annotated data to help train the IBM and HMM models, but the models were still generative and were trained using maximum-likelihood methods. Recently, however, three efforts nearly simultaneous with ours have made use of discriminative methods to train alignment models. Fraser and Marcu (2005) modify Model 4 to be a log-linear combination of 11 submodels (5 based on standard Model 4 parameters, and 6 based on additional features) and discriminatively optimize the submodel weights on each iteration of a Viterbi approximation to EM. Liu et al. (2005) also develop a log-linear model, based on IBM Model 3. They train Model 3 using Giza++, and then use the Model 3 score of a possible alignment as a feature value in a discriminatively trained log-linear model, along with features incorporating part-of-speech information, and whether the aligned words are given as translations in a bilingual dictionary. The log-linear model is trained by standard maximum-entropy methods. Klein and Taskar (2005), in a tutorial on maximum margin methods for natural-language processing, described a weighted linear model incorporating association, position, and or"
H05-1011,W03-0301,0,0.105777,"l simplex method (Press et al., 2002, Section 10.4) and Powell’s method (Press et al., 2002, Section 10.5), but we obtained slightly better results with a more heuristic method designed to look past minor local minima. We found that using this approach on top of perceptron learning led to slightly lower error rates on the development set with the CLP-based model, but not with the LLR-base model, so we used it only with the former in our final evaluations. 5 Data and Methodology for Evaluation We evaluated our models using data from the bilingual word alignment workshop held at HLT-NAACL 2003 (Mihalcea and Pedersen, 2003). We used a subset of the Canadian Hansards bilingual corpus supplied for the workshop, comprising 500,000 English-French sentences pairs, including 447 manually word-aligned sentence pairs designated as test data. The test data annotates particular pairs of words either as “sure” or “possible” links. Automatic sentence alignment of the training data was provided by Ulrich Germann, and the hand alignments of the words in the test data were created by Franz Och and Hermann Ney (Och and Ney, 2003). Since our discriminative training approach requires a small amount of annotated data for parameter"
H05-1011,W04-3243,1,0.651887,"ties; the reordering scores correspond to distortion probabilities; the scores for words left unlinked corresponds to probabilities of words being linked to the null word; and the scores for one-to-many links correspond to fertility probabilities. 2.1 The Log-Likelihood-Based Model In our first model, we use a log-likelihood-ratio (LLR) statistic as our measure of word association. We chose this statistic because it has previously been found to be effective for automatically constructing translation lexicons (e.g., Melamed, 2000). We compute LLR scores using the following formula presented by Moore (2004): LLR(f, e) =   f ?∈{f,¬f } e?∈{e,¬e} C(f ?, e?) log p(f ?|e?) p(f ?) In this formula f and e mean that the words whose degree of association is being measured occur in the respective target and source sentences of an aligned sentence pair, ¬f and ¬e mean that the corresponding words do not occur in the respective sentences, f ? and e? are variables ranging over these values, and C(f ?, e?) is the observed joint count for the values of f ? and e?. All the probabilities in the formula refer to maximum likelihood estimates. The LLR score for a pair of words is high if the words have either a s"
H05-1011,W05-0801,1,0.745324,"tures as the LLR-based model, except that it omits the one-to-many feature, since we assume that the one-to-one vs. one-to-many trade-off is already modeled in the conditional link probabilities for particular one-to-one and one-to-many clusters. We have developed two versions of the CLPbased model, using two different estimates for the conditional link probabilities. One estimate of the conditional link probabilities comes from the LLRbased model described above, optimized on an annotated development set. The other estimate comes from a heuristic alignment model that we previously developed (Moore, 2005).2 Space does not permit a full description of this heuristic model here, but in brief, it utilizes a series of greedy searches inspired by Melamed’s competitive linking algorithm (2000), in which constraints limiting alignments to being one-to-one and monotonic are applied at different thresholds of the LLR score, with a final cutoff of the LLR score below which no alignments are made. While the discriminative models presented above are very simple to describe, finding the optimal alignment according to these models is non-trivial. Adding a link for a new pair of words can affect the nonmonot"
H05-1011,J03-1002,0,0.452942,"they allow fast optimization of model parameters using small amounts of annotated data. 1 Motivation Bilingual word alignment is the first step of most current approaches to statistical machine translation. Although the best performing systems are “phrasebased” (e.g, Och and Ney, 2004), possible phrase translations are normally first extracted from wordaligned bilingual text segments. The standard approach to word alignment makes use of various combinations of five generative models developed at IBM by Brown et al. (1993), sometimes augmented by an HMM-based model or Och and Ney’s “Model 6” (Och and Ney, 2003). The best combinations of these models can produce high accuracy alignments, at least when trained on a large corpus of fairly direct translations in related languages. These standard models are less than ideal, however, in a number of ways, two of which we address in this paper. First, although the standard models can theoretically be trained without supervision, in practice various parameters are introduced that should be optimized using annotated data. For, example, Och and Ney (2003) suggest supervised optimization of a number of parameters, including the probablity of jumping to the empt"
H05-1011,J04-4002,0,0.0775583,"t word alignment methods are predominantly based on generative models. In this paper, we demonstrate a discriminative approach to training simple word alignment models that are comparable in accuracy to the more complex generative models normally used. These models have the the advantages that they are easy to add features to and they allow fast optimization of model parameters using small amounts of annotated data. 1 Motivation Bilingual word alignment is the first step of most current approaches to statistical machine translation. Although the best performing systems are “phrasebased” (e.g, Och and Ney, 2004), possible phrase translations are normally first extracted from wordaligned bilingual text segments. The standard approach to word alignment makes use of various combinations of five generative models developed at IBM by Brown et al. (1993), sometimes augmented by an HMM-based model or Och and Ney’s “Model 6” (Och and Ney, 2003). The best combinations of these models can produce high accuracy alignments, at least when trained on a large corpus of fairly direct translations in related languages. These standard models are less than ideal, however, in a number of ways, two of which we address in"
H05-1011,P04-1023,0,\N,Missing
H05-1011,J00-2004,0,\N,Missing
H91-1034,H91-1036,1,0.862094,"ed in a grammar are viewed. They must be treated as soft, rather than hard, constraints. This has significant implications for the rest of a spoken language system. If we want the parser to find grammatical fragments of the input that may be of use to the Template Matcher, then the parsing algorithm we previously used, which imposed strong leftcontext constraints, is no longer appropriate. We want something closer to pure bottom-up parsing to find all the phrases that the Template Matcher might use. We have developed such a parser, whose details are outlined in another paper for this workshop [1]. Perhaps the most significant consequence of using robust interpretation methods in a spoken language system, however, is that the failure to find a complete parse can no longer be used as a hard constraint to reduce perplexity for the speech recognizer. An analytical grammar still contains valuable information that should be used by the recognizer, however. We feel that one promising approach to making use of this information is to extend the idea of a word-based statistical language model, such as a bi-gram model, to a phrase-based statistical language model, e.g., a &quot;bi-phrase&quot; model. The"
H91-1034,H89-1018,0,0.0356023,"rted. Since we have no better than a fifty-fifty chance of guessing which is the correct filler, we are better off not attempting any answer. Second, if a template has no slots filled, it will receive a score of zero. This restriction is relaxed when the Template Matcher is operating in &quot;context-dependent&quot; mode, where follow-up questions are expected. A query like &quot;show me the fares,&quot; which would not fill any slots, Comparison with Other Systems Systems using the basic idea behind the Template Matcher go back as least as far as the SAM system at Yale [2], and include the Phoenix system at CMU [3, 4] and the SCISOR system at General Electric [5] as recent examples. There is also a degree of similarity to &quot;case-frame&quot;-based parsing methods [6, 7]. The main distinction is that the slots in our templates are domainspecific concepts rather than general linguistic or conceptual cases. Of these precursors, the Phoenix system seems most similar to the Template Matcher. Like the Template Matcher, the Phoenix system has templates (which they call &quot;frames&quot;) with slots that get filled with information from the sentence. The scoring mechanisms of the two systems are similar, but not identical. For bo"
H91-1034,H90-1027,0,0.0357686,"rted. Since we have no better than a fifty-fifty chance of guessing which is the correct filler, we are better off not attempting any answer. Second, if a template has no slots filled, it will receive a score of zero. This restriction is relaxed when the Template Matcher is operating in &quot;context-dependent&quot; mode, where follow-up questions are expected. A query like &quot;show me the fares,&quot; which would not fill any slots, Comparison with Other Systems Systems using the basic idea behind the Template Matcher go back as least as far as the SAM system at Yale [2], and include the Phoenix system at CMU [3, 4] and the SCISOR system at General Electric [5] as recent examples. There is also a degree of similarity to &quot;case-frame&quot;-based parsing methods [6, 7]. The main distinction is that the slots in our templates are domainspecific concepts rather than general linguistic or conceptual cases. Of these precursors, the Phoenix system seems most similar to the Template Matcher. Like the Template Matcher, the Phoenix system has templates (which they call &quot;frames&quot;) with slots that get filled with information from the sentence. The scoring mechanisms of the two systems are similar, but not identical. For bo"
H91-1034,A88-1018,0,0.0104334,"chance of guessing which is the correct filler, we are better off not attempting any answer. Second, if a template has no slots filled, it will receive a score of zero. This restriction is relaxed when the Template Matcher is operating in &quot;context-dependent&quot; mode, where follow-up questions are expected. A query like &quot;show me the fares,&quot; which would not fill any slots, Comparison with Other Systems Systems using the basic idea behind the Template Matcher go back as least as far as the SAM system at Yale [2], and include the Phoenix system at CMU [3, 4] and the SCISOR system at General Electric [5] as recent examples. There is also a degree of similarity to &quot;case-frame&quot;-based parsing methods [6, 7]. The main distinction is that the slots in our templates are domainspecific concepts rather than general linguistic or conceptual cases. Of these precursors, the Phoenix system seems most similar to the Template Matcher. Like the Template Matcher, the Phoenix system has templates (which they call &quot;frames&quot;) with slots that get filled with information from the sentence. The scoring mechanisms of the two systems are similar, but not identical. For both, the basic score of an interpretation is th"
H91-1034,J83-3001,0,\N,Missing
H91-1036,C86-1016,0,0.0703406,"American fly to from Boston does American fly to from Boston all as being legitimate phrases that contain a noun phrase gap. Without that preceding context, we would not want to consider any of these word strings as legitimate phrases. To implement this approach we partitioned the set of grammatical categories into context-independent and context-dependent subsets, with the context-dependent categories being those that implicitly contain gaps. Defining which categories those are is relatively easy in our grammar, because we have a uniform treatment of ""wh"" gaps, usually called ""gap-threading"" [5], so that every category that implicitly or explicitly contains a gap has a feature g a p s i n whose value is something other than n u l l . We have a similar treatment of the fronting of auxiliary verbs in yes/no questions, controlled by the feature v s t o r e . Finally, an additional quirk of our g r a m m a r required us to treat all relative clauses as context dependent categories. So we defined the context-independent categories to be those that • Have n u l l as the value of g a p s i n or lack the feature g a p s i n , and • Have n u l l as the value of v s t o r e or lack the feature"
H91-1036,H90-1031,1,\N,Missing
H91-1036,H91-1034,1,\N,Missing
H93-1008,H92-1003,0,0.0140764,"language recognition to limit overgeneration, but to extend the language analysis to recognize certain characteristic patterns of spoken utterances (but not generally thought of as part of grammar) and to recognize specific types of performance errors by the speaker. Since this paper describes a component by component view of Gemini, we will provide detailed statistics on the size, speed, coverage, and accuracy of the various components. These numbers detail our performance on the subdomain of air-travel planning that is currently being used by the DARPA spoken language understanding community[13]. Gemini was trained on a 5875 utterance dataset from this domain, with another 688 utterances used as a blind test (not explicitly trained on, but run multiple times) to monitor our performance on a dataset that we didn't train on. We will also report here our results on another 756 utterance fair test set, that we ran only once. Table 1 contains a summary of the coverage of the various components on the both the training and fair test sets. More detailed explanations of these numbers are given in the relevant sections. Processing starts in Gemini when syntactic, semantic, and lexical rules a"
H93-1008,P88-1005,0,0.037225,"Missing"
H93-1008,J88-1004,0,0.0209041,"Missing"
H93-1008,H91-1036,1,0.857105,"Missing"
H93-1008,H92-1060,0,0.0206522,"cs I statistics for the 5875 utterance training set. 2.7. Utterance Parser Grammar and Utterance The constituent parser uses the constituent grammar to build all possible categories bottom-up, independent of location within the string. Thus, the constituent parser does not force any constituent to occur either at the beginning of the utterance, or at the end. The utterance parser is a top-down back-tracking parser that uses a different grammar called the utterance grammar to glue the constituents found during constituent parsing together to span the entire utterance. M a n y systems [4], [9], [20], [22] have added robustness 1 Gemini is implemented primarily in Quintus Prolog version 3.1.1. All timing numbers given in this paper were run on a lightly loaded S u n Spaxcstation 2 with at least 4 8 M B of memory. Under normal conditions, Gemini runs in under 1 2 M B of memory. 46 (1) a. How many American airline flights leave Denver on June June tenth. b. Can you give me information on all the flights from San Francisco no from Pittsburgh to San Francisco on Monday. The mechanism used in Gemini to detect and correct repairs is currently applied as a fall-back mechanism if no semantically"
H93-1008,H92-1061,0,0.0271222,"tatistics for the 5875 utterance training set. 2.7. Utterance Parser Grammar and Utterance The constituent parser uses the constituent grammar to build all possible categories bottom-up, independent of location within the string. Thus, the constituent parser does not force any constituent to occur either at the beginning of the utterance, or at the end. The utterance parser is a top-down back-tracking parser that uses a different grammar called the utterance grammar to glue the constituents found during constituent parsing together to span the entire utterance. M a n y systems [4], [9], [20], [22] have added robustness 1 Gemini is implemented primarily in Quintus Prolog version 3.1.1. All timing numbers given in this paper were run on a lightly loaded S u n Spaxcstation 2 with at least 4 8 M B of memory. Under normal conditions, Gemini runs in under 1 2 M B of memory. 46 (1) a. How many American airline flights leave Denver on June June tenth. b. Can you give me information on all the flights from San Francisco no from Pittsburgh to San Francisco on Monday. The mechanism used in Gemini to detect and correct repairs is currently applied as a fall-back mechanism if no semantically accept"
H93-1008,C86-1045,0,0.00773892,"rs_num=(3rdAsg)] This category can be instantiated by any noun phrase with the value ynq for its wh feature (which means it must be a wh-bearing noun phrase like which book, who, or whose mother), either acc (accusative) or nora (nominative) for its case feature, and the conjunctive value 3rdAsg (third and singular) for its person-number feature. This formalism is related directly to the Core Language Engine, but more conceptually it is closely related to that of other unification-based grammar formalisms with a context-free skeleton, such as PATR-II [21], Categorial Unification G r a m m a r [23], Generalized PhraseStructure G r a m m a r [6] and Lexical Functional Grammar [3]. We list some ways in which Gemini differs from other unification formalisms. Since many of the most interesting issues regarding the formalism concern typing, we defer discussing motivation until section 2.5. . Gemini uses typed-unification. Each category has a set of features declared for it. Each feature has a declared value-space of possible values (value spaces may be shared by different features). Feature structures in Gemini can be recursive, but only by having categories in their value-space, so typing i"
H93-1008,P92-1008,1,0.524353,"Missing"
H93-1008,J83-3001,0,0.0207799,"g statistics I statistics for the 5875 utterance training set. 2.7. Utterance Parser Grammar and Utterance The constituent parser uses the constituent grammar to build all possible categories bottom-up, independent of location within the string. Thus, the constituent parser does not force any constituent to occur either at the beginning of the utterance, or at the end. The utterance parser is a top-down back-tracking parser that uses a different grammar called the utterance grammar to glue the constituents found during constituent parsing together to span the entire utterance. M a n y systems [4], [9], [20], [22] have added robustness 1 Gemini is implemented primarily in Quintus Prolog version 3.1.1. All timing numbers given in this paper were run on a lightly loaded S u n Spaxcstation 2 with at least 4 8 M B of memory. Under normal conditions, Gemini runs in under 1 2 M B of memory. 46 (1) a. How many American airline flights leave Denver on June June tenth. b. Can you give me information on all the flights from San Francisco no from Pittsburgh to San Francisco on Monday. The mechanism used in Gemini to detect and correct repairs is currently applied as a fall-back mechanism if no se"
H93-1008,C90-3029,1,0.826668,"Missing"
H93-1008,A92-1026,1,0.784485,"tistics I statistics for the 5875 utterance training set. 2.7. Utterance Parser Grammar and Utterance The constituent parser uses the constituent grammar to build all possible categories bottom-up, independent of location within the string. Thus, the constituent parser does not force any constituent to occur either at the beginning of the utterance, or at the end. The utterance parser is a top-down back-tracking parser that uses a different grammar called the utterance grammar to glue the constituents found during constituent parsing together to span the entire utterance. M a n y systems [4], [9], [20], [22] have added robustness 1 Gemini is implemented primarily in Quintus Prolog version 3.1.1. All timing numbers given in this paper were run on a lightly loaded S u n Spaxcstation 2 with at least 4 8 M B of memory. Under normal conditions, Gemini runs in under 1 2 M B of memory. 46 (1) a. How many American airline flights leave Denver on June June tenth. b. Can you give me information on all the flights from San Francisco no from Pittsburgh to San Francisco on Monday. The mechanism used in Gemini to detect and correct repairs is currently applied as a fall-back mechanism if no semanti"
H93-1008,P83-1017,0,\N,Missing
H94-1022,J93-2004,0,0.0320477,"or more participants&apos; systems to produce a first-pass annotation, which the annotators would then correct. There is some concern that this would produce annotations that are biased in favor of the system used to produce the initial structures. This might be partly alleviated by using multiple systems to produce a first pass, perhaps presenting to the annotators only the parts of the annotation that multiple systems agree on. The annotators will also need specialized editing tools tailored to creating and correcting SemEval structures. Such tools have been created by the Penn Treebank project [4] for producing syntactic bracketings of utterances, and it may be possible to a d a p t these for SemEval. 2. Annotation checker - - The annotations themselves will have a quite complex syntax and semantics. Software to check the resulting annotations will no doubt catch many annotation errors. It might be possible to build this functionality directly into the editing tools. 3. Annotation translators - - We have defined several levels of notation for SemEval. The highest, most syntactically sugared level seems likely to be used by the annotators, and the lowest level seems likely to be t h a t"
J90-1004,E89-1032,0,0.0953576,"d&apos;s BUG (Bottom-Up Generator) system, part of MiMo2, an experimental machine translation system for translating international news items of Teletext, which uses a Prolog version of Computational Linguistics Volume 16, Number 1, March 1990 Shieber et al. Semantic Head-Driven Grammar PATR-II similar to that of Hirsh (1987). According to Martin Kay (personal communication), the STREP machine translation project at the Center for the Study of Language and Information uses a version of our algorithm to generate with respect to grammars based on head-driven phrase structure grammar (HPSG). Finally, Calder et al. (1989) report on a generation algorithm for unification categorial grammar that appears to be a special case of ours. 1.2 PRELIMINARIES Despite the general applicability of the algorithm, we will, for the sake of concreteness, describe it and other generation algorithms in terms of their implementation for definiteclause grammars (DCG). For ease of exposition, the encoding will be a bit more cumbersome than is typically found in Prolog DCG interpreters. The standard DCG encoding in Prolog uses the notation (cat o) --&gt; (cat I ) . . . . . (cat,). where the (cat i) are terms representing the grammatica"
J90-1004,J87-1005,1,0.800251,"they might be because during the distribution of store elements among the subject and complements of a verb no check is performed as to whether the variable bound by a store element actually appears in the semantics of the phrase to which it is being assigned, leading to many dead ends in the generation process. Also, the rules are sound for generation but not for analysis, because they do not enforce the constraint that every occurrence of a variable in logical form be outscoped by the variable&apos;s binder. Adding appropriate side conditions to the rules, following the constraints discussed by Hobbs and Shieber (1987) would not be difficult. 38 4 EXTENSIONS Tile basic semantic-head-driven generation algorithm can be augmented in various ways so as to encompass some important analyses and constraints. In particular, we discuss the incorporation of • completeness and coherence constraints, • the postponing of lexical choice, and • the ability to handle certain problematic empty-headed phrases 4.1 COMPLETENESS AND COHERENCE Wedckind (1988) defines completeness and coherence of a generation algorithm as follows. Suppose a generator derives a string w from a logical form s, and the grammar assigns to w the logi"
J90-1004,P83-1021,1,0.813733,"INTRODUCTION The problem of generating a well-formed natural language expression from an encoding of its meaning possesses properties that distinguish it from the converse problem of recovering a meaning encoding from a given natural language expression. This much is axiomatic. In previous work (Shieber 1988), however, one of us attempted to characterize these differing properties in such a way that a single uniform architecture, appropriately parameterized, might be used for both natural language processes. In particular, we developed an architecture inspired by the Earley deduction work of Pereira and Warren (1983), but which generalized that work allowing for its use in both a parsing and generation mode merely by setting the values of a small number of parameters. As a method for generating natural language expressions, the Earley deduction method is reasonably successful along certain dimensions. It is quite simple, general in its applicability to a range of unification-based and logic grammar formalisms, and uniform, in that it places only one restriction (discussed below) on the form of the linguistic analyses allowed by the grammars used in generation. In particular, generation from grammars with"
J90-1004,J81-4003,1,0.825833,"Missing"
J90-1004,C88-2128,1,0.954033,"evious bottom-up generator, it allows use of semlantically nonmonotonic grammars, yet unlike top-down methods, it also permits left-recursion. The enabling design feature of the algorithm is its implicit traversal of the analysis tree for the string being generated in a semantic-head-driven fashion. 1 INTRODUCTION The problem of generating a well-formed natural language expression from an encoding of its meaning possesses properties that distinguish it from the converse problem of recovering a meaning encoding from a given natural language expression. This much is axiomatic. In previous work (Shieber 1988), however, one of us attempted to characterize these differing properties in such a way that a single uniform architecture, appropriately parameterized, might be used for both natural language processes. In particular, we developed an architecture inspired by the Earley deduction work of Pereira and Warren (1983), but which generalized that work allowing for its use in both a parsing and generation mode merely by setting the values of a small number of parameters. As a method for generating natural language expressions, the Earley deduction method is reasonably successful along certain dimensi"
J90-1004,C88-2150,0,0.136143,"a small number of parameters. As a method for generating natural language expressions, the Earley deduction method is reasonably successful along certain dimensions. It is quite simple, general in its applicability to a range of unification-based and logic grammar formalisms, and uniform, in that it places only one restriction (discussed below) on the form of the linguistic analyses allowed by the grammars used in generation. In particular, generation from grammars with recursions whose well-foundedness relies on lexical information will terminate; top-down generation regimes such as those of Wedekind (1988) or Dymetman and Isabelle (1988) lack this property; further discussion can be found in Section 2.1. Unfortunately, the bottom-up, left-to-right processing regime of Earley generation--as it might be called---has its 30 own inherent frailties. Efficiency considerations require that only grammars possessing a property of semantic monotonicity can be effectively used, and even for those grammars, processing can become overly nondeterministic. Tile algorithm described in this paper is an attempt to resolve these problems in a satisfactory manner. Although we believe that this algorithm could be s"
moore-2002-fast,J93-1004,0,\N,Missing
moore-2002-fast,W96-0201,0,\N,Missing
moore-2002-fast,J93-2003,0,\N,Missing
moore-2002-fast,P91-1022,0,\N,Missing
moore-2002-fast,J93-1006,0,\N,Missing
moore-2002-fast,P97-1039,0,\N,Missing
moore-2002-fast,P94-1012,0,\N,Missing
moore-2002-fast,P93-1002,0,\N,Missing
N07-2053,W06-3123,0,0.0276401,"Missing"
N07-2053,W06-3105,0,0.0673368,"Missing"
N07-2053,W06-1607,0,0.0279155,"e 1. A full list using phrases of up to three words would include 28 pairs. For each extracted phrase pair (s, t), feature values φ(s, t) = hlog p(s|t), log p(t|s), log l(s, t)i are computed. The first two features, the log translation and inverse translation probabilities, are estimated by counting phrase cooccurrences, following Koehn et al. (2003). The third feature is the logarithm of a lexical score l(s, t) that provides a simple form of smoothing by weighting a phrase pair based on how likely individual words within the phrases are to be translations of each other. We use a version from Foster et al. (2006), modified from (Koehn et al., 2003), which is an average of pairwise word translation probabilities. In phrase-based SMT, the decoder produces translations by dividing the source sentence into a sequence of phrases, choosing a target language phrase 210 Source Lang. Phrase Monsieur Monsieur le Monsieur le Orateur le Orateur Orateur ... le R`eglement le R`eglement le R`eglement R`eglement R`eglement R`eglement Target Lang. Phrase Mr. Mr. Mr. Speaker Speaker Speaker ... point of order of order order point of order of order order Figure 2: Phrase pairs consistent with the word alignment in Figur"
N07-2053,N03-1017,0,0.0254231,"according to a translation model. This method not only reduces the size of the phrase translation table, but also improves translation quality as measured by the BLEU metric. 1 Introduction Phrase translation tables are the heart of phrasebased statistical machine translation (SMT) systems. They provide pairs of phrases that are used to construct a large set of potential translations for each input sentence, along with feature values associated with each phrase pair that are used to select the best translation from this set.1 The most widely used method for building phrase translation tables (Koehn et al., 2003) selects, from a word alignment of a parallel bilingual training corpus, all pairs of phrases (up to a given length) that are consistent with the alignment. This procedure 1 A “phrase” in this sense can be any contiguous sequence of words, and need not be a complete linguistic constituent. typically generates many phrase pairs that are not remotely reasonable translation candidates.2 To avoid creating translations that use these pairs, a set of features is computed for each pair. These features are used to train a translation model, and phrase pairs that produce low scoring translations are av"
N07-2053,koen-2004-pharaoh,0,0.040776,"order of order order Figure 2: Phrase pairs consistent with the word alignment in Figure 1. as a translation for each source language phrase, and ordering the target language phrases to build the final translated sentence. Each potential translation is scored according to a weighted linear model. We use the three features from the phrase translation table, summing their values for each phrase pair used in the translation. We also use four additional features: a target language model, a distortion penalty, the target sentence word count, and the phrase pair count, all computed as described in (Koehn, 2004). For all of the experiments in this paper, we used the Pharaoh beam-search decoder (Koehn, 2004) with the features described above. Finally, to estimate the parameters λi of the weighted linear model, we adopt the popular minimum error rate training procedure (Och, 2003) which directly optimizes translation quality as measured by the BLEU metric. 3 Selective Phrase Pair Extraction In order to improve performance, it is important to select high quality phrase pairs for the phrase translation table. We use two key ideas to guide selection: • Preferential Scoring: Phrase pairs are selected using"
N07-2053,W02-1018,0,0.138599,"Missing"
N07-2053,J00-2004,0,0.0564255,"rase pair (s, t) in the phrase translation table, and λ is a vector of the three parameter values that were learned for these features by the full translation model. The rest of the features are ignored because they are either constant or depend on the target language sentence which is fixed during phrase extraction. In essence, we are using the subpart of a full translation model that looks at phrase pair identity and scoring the pair based on how the full model would like it. This scoring metric is used in a phrase pair selection algorithm inspired by competitive linking for word alignment (Melamed, 2000). Local competitive linking extracts high scoring phrase pairs while enforcing a redundancy constraint that minimizes the number of phrase pairs that share a common phrase. For each sentence pair in the training set, this algorithm marks the highest scoring phrase pair, according to q(s, t), containing each source language phrase and the highest scoring phrase pair containing each target language phrase. Each of these marked phrase pairs is selected and the phrase translation table is rebuilt. This is a soft redundancy constraint because a phrase pair will only be excluded if there is a higher"
N07-2053,W03-0301,0,0.0267616,"ter estimate is computed. Then, based on that estimate, a subset of the phrases is selected which, in turn, supplies a new estimate for the parameters. One question is how many times to run this reestimation procedure. We found, on the development set, that it never helped to run more than one iteration. Perhaps because of the hard nature of the algorithm, repeated iterations caused slight decreases in phrase translation table size and overall performance. 4 Experiments In this section, we report experiments conducted with Canadian Hansards data from the 2003 HLTNAACL word-alignment workshop (Mihalcea and Pedersen, 2003). Phrase pairs are extracted from 500,000 word-aligned French-English sentence pairs. Translation quality is evaluated according to the BLEU metric (with one reference translation). Three additional disjoint data sets (from the same source) were used, one with 500 sentence pairs for minimum error rate training, another with 1000 pairs for development testing, and a final set of 2000 sentence pairs for the final test. For each experiment, we trained the full translation model as described in Section 2. Each trial varied only in the phrase translation table that was used.3 One important question"
N07-2053,P06-1065,1,0.828852,"teur , je invoque le R`egement , &quot;&quot; , Mr. Speaker , # 1 2 3 4 5 ... 23 24 25 26 27 28 &quot;&quot; I rise on a point of order Figure 1: A word aligned sentence pair. t is constructed from s. The weights λi associated with each feature fi are tuned to maximize the quality of the translations. The training procedure starts by computing a word alignment for each sentence pair in the training corpus. A word alignment is a relation between the words in two sentences where, intuitively, words are aligned to their translation in the other language. In this work, we use a discriminatively trained word aligner (Moore et al., 2006) that has state of the art performance. Figure 1 presents a high quality alignment produced by this aligner. Given a word aligned corpus, the second step is to extract a phrase translation table. Each entry in this table contains a source language phrase s, a target language phrase t, and a list of feature values φ(s, t). It is usual to extract every phrase pair, up to a certain phrase length, that is consistent with the word alignment that is annotated in the corpus. Each consistent pair must have at least one word alignment between words within the phrases and no words in either phrase can b"
N07-2053,P03-1021,0,0.0165118,"weighted linear model. We use the three features from the phrase translation table, summing their values for each phrase pair used in the translation. We also use four additional features: a target language model, a distortion penalty, the target sentence word count, and the phrase pair count, all computed as described in (Koehn, 2004). For all of the experiments in this paper, we used the Pharaoh beam-search decoder (Koehn, 2004) with the features described above. Finally, to estimate the parameters λi of the weighted linear model, we adopt the popular minimum error rate training procedure (Och, 2003) which directly optimizes translation quality as measured by the BLEU metric. 3 Selective Phrase Pair Extraction In order to improve performance, it is important to select high quality phrase pairs for the phrase translation table. We use two key ideas to guide selection: • Preferential Scoring: Phrase pairs are selected using a function q(s, t) that returns a high score for source, target phrase pairs (s, t) that lead to high quality translations. • Redundancy Constraints: Our intuition is that each occurrence of a source or target language phrase really has at most one translation for that s"
N07-2053,2006.amta-papers.2,0,\N,Missing
N07-2053,D08-1076,0,\N,Missing
P00-1037,J00-4006,0,\N,Missing
P00-1037,A94-1037,0,\N,Missing
P02-1004,C00-1007,0,0.305172,"g can be performed purely by rules, by application of statistical models, or by a combination of both techniques. Among the systems that use statistical or machine learned techniques in sentence realization, there are various degrees of intermediate syntactic structure. Nitrogen (Langkilde and Knight, 1998a, 1998b) produces a large set of alternative surface realizations of an input structure (which can vary in abstractness). This set of candidate surface strings, represented as a word lattice, is then rescored by a wordbigram language model, to produce the bestranked output sentence. FERGUS (Bangalore and Rambow, 2000), on the other hand, employs a model of syntactic structure during sentence realization. In simple terms, it adds a tree-based stochastic model to the approach taken by the Nitrogen system. This tree-based model chooses a best-ranked XTAG representation for a given dependency structure. Possible linearizations of the XTAG representation are generated and then evaluated by a language model to pick the best possible linearization, as in Nitrogen. In contrast, the sentence realization system code-named Amalgam (A Machine Learned Generation Module) (Corston-Oliver et al., 2002; Gamon et al., 2002b"
P02-1004,W02-2105,1,0.905939,"ut sentence. FERGUS (Bangalore and Rambow, 2000), on the other hand, employs a model of syntactic structure during sentence realization. In simple terms, it adds a tree-based stochastic model to the approach taken by the Nitrogen system. This tree-based model chooses a best-ranked XTAG representation for a given dependency structure. Possible linearizations of the XTAG representation are generated and then evaluated by a language model to pick the best possible linearization, as in Nitrogen. In contrast, the sentence realization system code-named Amalgam (A Machine Learned Generation Module) (Corston-Oliver et al., 2002; Gamon et al., 2002b) employs a series of linguistic operations which map a semantic representation to a surface syntactic tree via intermediate syntactic representations. The contexts for most of these operations in Amalgam are machine learned. The resulting syntactic tree contains all the necessary information on its leaf nodes from which a surface string can be read. The goal of this paper is to show that it is possible to learn accurately the contexts for linguistically complex operations in sentence realization. We propose that learning the contexts for the application of these linguisti"
P02-1004,C02-1036,1,0.847241,"Missing"
P02-1004,W98-1426,0,0.660353,"tion, sentence realization, creates the surface string from an abstract (typically semantic) representation. This mapping from abstract representation to surface string can be direct, or it can employ intermediate syntactic representations which significantly constrain the output. Furthermore, the mapping can be performed purely by rules, by application of statistical models, or by a combination of both techniques. Among the systems that use statistical or machine learned techniques in sentence realization, there are various degrees of intermediate syntactic structure. Nitrogen (Langkilde and Knight, 1998a, 1998b) produces a large set of alternative surface realizations of an input structure (which can vary in abstractness). This set of candidate surface strings, represented as a word lattice, is then rescored by a wordbigram language model, to produce the bestranked output sentence. FERGUS (Bangalore and Rambow, 2000), on the other hand, employs a model of syntactic structure during sentence realization. In simple terms, it adds a tree-based stochastic model to the approach taken by the Nitrogen system. This tree-based model chooses a best-ranked XTAG representation for a given dependency str"
P02-1004,P98-1116,0,0.0316932,"anguage generation, sentence realization, creates the surface string from an abstract (typically semantic) representation. This mapping from abstract representation to surface string can be direct, or it can employ intermediate syntactic representations which significantly constrain the output. Furthermore, the mapping can be performed purely by rules, by application of statistical models, or by a combination of both techniques. Among the systems that use statistical or machine learned techniques in sentence realization, there are various degrees of intermediate syntactic structure. Nitrogen (Langkilde and Knight, 1998a, 1998b) produces a large set of alternative surface realizations of an input structure (which can vary in abstractness). This set of candidate surface strings, represented as a word lattice, is then rescored by a wordbigram language model, to produce the bestranked output sentence. FERGUS (Bangalore and Rambow, 2000), on the other hand, employs a model of syntactic structure during sentence realization. In simple terms, it adds a tree-based stochastic model to the approach taken by the Nitrogen system. This tree-based model chooses a best-ranked XTAG representation for a given dependency str"
P02-1004,P98-2199,0,0.0246939,"s around the baseline for these two types of extraposed clauses. Table 3. Accuracy of the extraposition model. Extraposable clause RELCL INFCL COMPCL Overall 7 Accuracy 0.8387 0.9202 0.9857 0.8612 Baseline 0.6093 0.9370 0.9429 0.6758 Syntactic aggregation Any sentence realization component that generates from an abstract semantic representation and strives to produce fluent output beyond simple templates will have to deal with coordination and the problem of duplicated material in coordination. This is generally viewed as a subarea of aggregation in the generation literature (Wilkinson, 1995; Shaw, 1998; Reape and Mellish, 1999; Dalianis and Hovy, 1993). In Amalgam, the approach we take is strictly intrasentential, along the lines of what has been called conjunction reduction in the linguistic literature (McCawley, 1988). While this may seem a fairly straightforward task compared to inter-sentential, semantic and lexical aggregation, it should be noted that the cross-linguistic complexity of the phenomenon makes it much less trivial than a first glance at English would suggest. In German, for example, position of the verb in the coordinated VPs plays an important role in determining which du"
P02-1004,C98-1112,0,\N,Missing
P02-1004,C98-2194,0,\N,Missing
P02-1019,P00-1037,1,0.649272,"ing corrections for words that are not found in a dictionary. Notice, however, that the noisy channel model offers the possibility of correcting misspellings without a dictionary, as long as sufficient data is available to estimate the source model facOsama bin Laden and tors. For example, if r w Ossama bin Laden, the model will predict that the correct spelling r is more likely than the incorrect spelling w, provided that () ( ) () = ( ) = P (w) P (wjr) &lt; P (r) P (wjw) ( ) ( ) where P wjr =P wjw would be approximately the odds of doubling the s in Osama. We do not pursue this, here, however. Brill and Moore (2000) present an improved error model for noisy channel spelling correction that goes beyond single insertions, deletions, substitutions, and transpositions. The model has a set of parameters P ! for letter sequences of lengths up to . An extension they presented has refined parameters P ! jP SN which also depend on the position of the substitution in the source word. According to this model, the misspelling is generated by the correct word as follows: First, a person picks a partition of the correct word and then types each partition independently, possibly making some errors. The probability for"
P02-1019,C90-2036,0,0.944079,"Missing"
P04-1066,H93-1039,0,0.363472,"Missing"
P04-1066,P96-1041,0,0.00651076,"r of distinct words observed in the target language training, but we know that the target language will have many words that we have never observed. We arbitrarily chose |V |to be 100,000, which is somewhat more than the total number of distinct words in our target language training data. The value of n is empirically optimized on annotated development test data. This sort of “add-n” smoothing has a poor reputation in statistical NLP, because it has repeatedly been shown to perform badly compared to other methods of smoothing higher-order n-gram models for statistical language modeling (e.g., Chen and Goodman, 1996). In those studies, however, add-n smoothing was used to smooth bigram or trigram models. Add-n smoothing is a way of smoothing with a uniform distribution, so it is not surprising that it performs poorly in language modeling when it is compared to smoothing with higher order models; e.g, smoothing trigrams with bigrams or smoothing bigrams with unigrams. In situations where smoothing with a uniform distribution is appropriate, it is not clear that add-n is a bad way to do it. Furthermore, we would argue that the word translation probabilities of Model 1 are a case where there is no clearly be"
P04-1066,2003.mtsummit-papers.13,0,0.0176856,"ction IBM Model 1 (Brown et al., 1993a) is a wordalignment model that is widely used in working with parallel bilingual corpora. It was originally developed to provide reasonable initial parameter estimates for more complex word-alignment models, but it has subsequently found a host of additional uses. Among the applications of Model 1 are segmenting long sentences into subsentental units for improved word alignment (Nevado et al., 2003), extracting parallel sentences from comparable corpora (Munteanu et al., 2004), bilingual sentence alignment (Moore, 2002), aligning syntactictree fragments (Ding et al., 2003), and estimating phrase translation probabilities (Venugopal et al., 2003). Furthermore, at the 2003 Johns Hopkins summer workshop on statistical machine translation, a large number of features were tested to discover which ones could improve a state-of-the-art translation system, and the only feature that produced a “truly significant improvement” was the Model 1 score (Och et al., 2004). Despite the fact that IBM Model 1 is so widely used, essentially no attention seems to have been paid to whether it is possible to improve on the standard Expectation-Maximization (EM) procedure for estimati"
P04-1066,J93-1003,0,0.0510867,"t to obtain the parameter values at convergence, and we have strong reasons to believe that these values do not produce the most accurate sentence alignments. Even though EM will head towards those values from any initial position in the parameter space, there may be some starting points we can systematically find that will take us closer to the optimal parameter values for alignment accuracy along the way. To test whether a better set of initial parameter estimates can improve Model 1 alignment accuracy, we use a heuristic model based on the loglikelihood-ratio (LLR) statistic recommended by Dunning (1993). We chose this statistic because it has previously been found to be effective for automatically constructing translation lexicons (e.g., Melamed, 2000; Moore, 2001). In our application, the statistic can be defined by the following formula:   t?∈{t,¬t} s?∈{s,¬s} C(t?, s?) log p(t?|s?) p(t?) (4) In this formula t and s mean that the corresponding words occur in the respective target and source sentences of an aligned sentence pair, ¬t and ¬s mean that the corresponding words do not occur in the respective sentences, t? and s? are variables ranging over these values, and C(t?, s?) is the obse"
P04-1066,N03-1017,0,0.0122968,"Missing"
P04-1066,W03-0301,0,0.0124793,"lize the distribution for the null word to be the unigram distribution of target words, so that frequent function words will receive a higher probability of aligning to the null word than rare words, which tend to be content words that do have a translation. Finally, we also effectively add extra null words to every sentence in this heuristic model, by multiplying the null word probabilities by a constant, as described in Section 5. 7 Training and Evaluation We trained and evaluated our various modifications to Model 1 on data from the bilingual word alignment workshop held at HLT-NAACL 2003 (Mihalcea and Pedersen, 2003). We used a subset of the Canadian Hansards bilingual corpus supplied for the workshop, comprising 500,000 English-French sentences pairs, including 37 sentence pairs designated as “trial” data, and 447 sentence pairs designated as test data. The trial and test data had been manually aligned at the word level, noting particular pairs of words either as “sure” or “possible” alignments, as described by Och and Ney (2003). To limit the number of translation probabilities that we had to store, we first computed LLR association scores for all bilingual word pairs with a positive association (p(t, s"
P04-1066,W01-1411,1,0.836902,"ugh EM will head towards those values from any initial position in the parameter space, there may be some starting points we can systematically find that will take us closer to the optimal parameter values for alignment accuracy along the way. To test whether a better set of initial parameter estimates can improve Model 1 alignment accuracy, we use a heuristic model based on the loglikelihood-ratio (LLR) statistic recommended by Dunning (1993). We chose this statistic because it has previously been found to be effective for automatically constructing translation lexicons (e.g., Melamed, 2000; Moore, 2001). In our application, the statistic can be defined by the following formula:   t?∈{t,¬t} s?∈{s,¬s} C(t?, s?) log p(t?|s?) p(t?) (4) In this formula t and s mean that the corresponding words occur in the respective target and source sentences of an aligned sentence pair, ¬t and ¬s mean that the corresponding words do not occur in the respective sentences, t? and s? are variables ranging over these values, and C(t?, s?) is the observed joint count for the values of t? and s?. All the probabilities in the formula refer to maximum likelihood estimates.1 These LLR scores can range in value from 0"
P04-1066,moore-2002-fast,1,0.144004,"lace, EM training of model parameters. 1 Introduction IBM Model 1 (Brown et al., 1993a) is a wordalignment model that is widely used in working with parallel bilingual corpora. It was originally developed to provide reasonable initial parameter estimates for more complex word-alignment models, but it has subsequently found a host of additional uses. Among the applications of Model 1 are segmenting long sentences into subsentental units for improved word alignment (Nevado et al., 2003), extracting parallel sentences from comparable corpora (Munteanu et al., 2004), bilingual sentence alignment (Moore, 2002), aligning syntactictree fragments (Ding et al., 2003), and estimating phrase translation probabilities (Venugopal et al., 2003). Furthermore, at the 2003 Johns Hopkins summer workshop on statistical machine translation, a large number of features were tested to discover which ones could improve a state-of-the-art translation system, and the only feature that produced a “truly significant improvement” was the Model 1 score (Och et al., 2004). Despite the fact that IBM Model 1 is so widely used, essentially no attention seems to have been paid to whether it is possible to improve on the standar"
P04-1066,W04-3243,1,0.564012,"this, however, would discard one of the major advantages of using LLR scores as a measure of word association. All the LLR scores for rare words tend to be small; thus we do not put too much confidence in any of the hypothesized word associations for such words. This is exactly the property needed to prevent rare source words from becoming garbage collectors. To maintain this property, for each source word we compute the sum of the 1 This is not the form in which the LLR statistic is usually presented, but it can easily be shown by basic algebra to be equivalent to −λ in Dunning’s paper. See Moore (2004) for details. LLR scores over all target words, but we then divide every LLR score by the single largest of these sums. Thus the source word with the highest LLR score sum receives a conditional probability distribution over target words summing to 1, but the corresponding distribution for every other source word sums to less than 1, reserving some probability mass for target words not seen with that word, with more probability mass being reserved the rarer the word. There is no guarantee, of course, that this is the optimal way of discounting the probabilities assigned to less frequent words."
P04-1066,N04-1034,0,0.0309941,"mple heuristic estimation method to initialize, or replace, EM training of model parameters. 1 Introduction IBM Model 1 (Brown et al., 1993a) is a wordalignment model that is widely used in working with parallel bilingual corpora. It was originally developed to provide reasonable initial parameter estimates for more complex word-alignment models, but it has subsequently found a host of additional uses. Among the applications of Model 1 are segmenting long sentences into subsentental units for improved word alignment (Nevado et al., 2003), extracting parallel sentences from comparable corpora (Munteanu et al., 2004), bilingual sentence alignment (Moore, 2002), aligning syntactictree fragments (Ding et al., 2003), and estimating phrase translation probabilities (Venugopal et al., 2003). Furthermore, at the 2003 Johns Hopkins summer workshop on statistical machine translation, a large number of features were tested to discover which ones could improve a state-of-the-art translation system, and the only feature that produced a “truly significant improvement” was the Model 1 score (Och et al., 2004). Despite the fact that IBM Model 1 is so widely used, essentially no attention seems to have been paid to whet"
P04-1066,W03-2205,0,0.0258723,"word, (2) smoothing probability estimates for rare words, and (3) using a simple heuristic estimation method to initialize, or replace, EM training of model parameters. 1 Introduction IBM Model 1 (Brown et al., 1993a) is a wordalignment model that is widely used in working with parallel bilingual corpora. It was originally developed to provide reasonable initial parameter estimates for more complex word-alignment models, but it has subsequently found a host of additional uses. Among the applications of Model 1 are segmenting long sentences into subsentental units for improved word alignment (Nevado et al., 2003), extracting parallel sentences from comparable corpora (Munteanu et al., 2004), bilingual sentence alignment (Moore, 2002), aligning syntactictree fragments (Ding et al., 2003), and estimating phrase translation probabilities (Venugopal et al., 2003). Furthermore, at the 2003 Johns Hopkins summer workshop on statistical machine translation, a large number of features were tested to discover which ones could improve a state-of-the-art translation system, and the only feature that produced a “truly significant improvement” was the Model 1 score (Och et al., 2004). Despite the fact that IBM Mode"
P04-1066,J03-1002,0,0.0886082,"global maximum for the likelihood of the training data, then this result would seem to show no improvement is possible. However, in virtually every application of statistical techniques in natural-language processing, maximizing the likelihood of the training data causes overfitting, resulting in lower task performance than some other estimates for the model parameters. This is implicitly recognized in the widespread adoption of early stopping in estimating the parameters of Model 1. Brown et al. (1993a) stopped after only one iteration of EM in using Model 1 to initialize their Model 2, and Och and Ney (2003) stop after five iterations in using Model 1 to initialize the HMM word-alignment model. Both of these are far short of convergence to the maximum likelihood estimates for the model parameters. We have identified at least two ways in which the standard EM training method for Model 1 leads to suboptimal performance in terms of wordalignment accuracy. In this paper we show that by addressing these issues, substantial improvements in word-alignment accuracy can be achieved. 2 Definition of Model 1 Model 1 is a probabilistic generative model within a framework that assumes a source sentence S of l"
P04-1066,N04-1021,0,0.0494444,"Missing"
P04-1066,P03-1041,0,0.0148702,"is widely used in working with parallel bilingual corpora. It was originally developed to provide reasonable initial parameter estimates for more complex word-alignment models, but it has subsequently found a host of additional uses. Among the applications of Model 1 are segmenting long sentences into subsentental units for improved word alignment (Nevado et al., 2003), extracting parallel sentences from comparable corpora (Munteanu et al., 2004), bilingual sentence alignment (Moore, 2002), aligning syntactictree fragments (Ding et al., 2003), and estimating phrase translation probabilities (Venugopal et al., 2003). Furthermore, at the 2003 Johns Hopkins summer workshop on statistical machine translation, a large number of features were tested to discover which ones could improve a state-of-the-art translation system, and the only feature that produced a “truly significant improvement” was the Model 1 score (Och et al., 2004). Despite the fact that IBM Model 1 is so widely used, essentially no attention seems to have been paid to whether it is possible to improve on the standard Expectation-Maximization (EM) procedure for estimating its parameters. This may be due in part to the fact that Brown et al. ("
P04-1066,J93-2003,0,\N,Missing
P04-1066,J00-2004,0,\N,Missing
P06-1065,N06-1015,0,0.255337,"Missing"
P06-1065,N06-1014,0,0.18572,"Missing"
P06-1065,H05-1009,0,0.0719034,"tences and a proposed word alignment of them. The possible alignment having the highest overall score is selected for each sentence pair. Thus, for a sentence pair (e, f ) we seek the alignment a ˆ such that 1 Introduction Until recently, almost all work in statistical machine translation was based on word alignments obtained from combinations of generative probabalistic models developed at IBM by Brown et al. (1993), sometimes augmented by an HMMbased model or Och and Ney’s “Model 6” (Och and Ney, 2003). In 2005, however, several independent efforts (Liu et al., 2005; Fraser and Marcu, 2005; Ayan et al., 2005; Taskar et al., 2005; Moore, 2005; Ittycheriah and Roukos, 2005) demonstrated that discriminatively trained models can equal or surpass the alignment accuracy of the standard models, if the usual unlabeled bilingual training corpus is supplemented with human-annotated word alignments for only a small subset of the training data. The work cited above makes use of various training procedures and a wide variety of features. Indeed, whereas it can be difficult to design a factorization of a generative model that incorporates a ˆ = argmaxa n X λi fi (a, e, f ) i=1 where the fi are features and the"
P06-1065,P05-1057,0,0.417305,"eature values extracted from a pair of sentences and a proposed word alignment of them. The possible alignment having the highest overall score is selected for each sentence pair. Thus, for a sentence pair (e, f ) we seek the alignment a ˆ such that 1 Introduction Until recently, almost all work in statistical machine translation was based on word alignments obtained from combinations of generative probabalistic models developed at IBM by Brown et al. (1993), sometimes augmented by an HMMbased model or Och and Ney’s “Model 6” (Och and Ney, 2003). In 2005, however, several independent efforts (Liu et al., 2005; Fraser and Marcu, 2005; Ayan et al., 2005; Taskar et al., 2005; Moore, 2005; Ittycheriah and Roukos, 2005) demonstrated that discriminatively trained models can equal or surpass the alignment accuracy of the standard models, if the usual unlabeled bilingual training corpus is supplemented with human-annotated word alignments for only a small subset of the training data. The work cited above makes use of various training procedures and a wide variety of features. Indeed, whereas it can be difficult to design a factorization of a generative model that incorporates a ˆ = argmaxa n X λi fi (a, e"
P06-1065,J93-2003,0,0.032918,"Missing"
P06-1065,W03-0301,0,0.08921,"Missing"
P06-1065,P03-1012,0,0.250546,"Missing"
P06-1065,H05-1011,1,0.716698,"or many years, statistical machine translation relied on generative models to provide bilingual word alignments. In 2005, several independent efforts showed that discriminative models could be used to enhance or replace the standard generative approach. Building on this work, we demonstrate substantial improvement in word-alignment accuracy, partly though improved training methods, but predominantly through selection of more and better features. Our best model produces the lowest alignment error rate yet reported on Canadian Hansards bilingual data. 2 Overall Approach As in our previous work (Moore, 2005), we train two models we call stage 1 and stage 2, both in the form of a weighted linear combination of feature values extracted from a pair of sentences and a proposed word alignment of them. The possible alignment having the highest overall score is selected for each sentence pair. Thus, for a sentence pair (e, f ) we seek the alignment a ˆ such that 1 Introduction Until recently, almost all work in statistical machine translation was based on word alignments obtained from combinations of generative probabalistic models developed at IBM by Brown et al. (1993), sometimes augmented by an HMMba"
P06-1065,W02-1001,0,0.301818,"e the possiblity that more than one alignment may have the same score, which we previously did not take into account. First, we modified the beam search so that the beam size dynamically expands if needed to accomodate all the possible alignments that have the same score. Second we implemented a structural tie breaker, so that the same alignment will always be chosen as the one-best from a set of alignments having the same score. Neither of these changes significantly affected the alignment results. The principal training method is an adaptation of averaged perceptron learning as described by Collins (2002). The differences between our current and earlier training methods mainly address the observation that perceptron training is very sensitive to the order in which data is presented to the learner. We also investigated the large-margin training technique described by Tsochantaridis et al. (2004). The training procedures are described in Sections 5 and 6. two hard constraints. One constraint was that the only alignment patterns allowed were 1–1, 1–2, 1– 3, 2–1, and 3–1. Thus, many-to-many link patterns were disallowed, and a single word could be linked to at most three other words. The second co"
P06-1065,J03-1002,0,0.113683,"and stage 2, both in the form of a weighted linear combination of feature values extracted from a pair of sentences and a proposed word alignment of them. The possible alignment having the highest overall score is selected for each sentence pair. Thus, for a sentence pair (e, f ) we seek the alignment a ˆ such that 1 Introduction Until recently, almost all work in statistical machine translation was based on word alignments obtained from combinations of generative probabalistic models developed at IBM by Brown et al. (1993), sometimes augmented by an HMMbased model or Och and Ney’s “Model 6” (Och and Ney, 2003). In 2005, however, several independent efforts (Liu et al., 2005; Fraser and Marcu, 2005; Ayan et al., 2005; Taskar et al., 2005; Moore, 2005; Ittycheriah and Roukos, 2005) demonstrated that discriminatively trained models can equal or surpass the alignment accuracy of the standard models, if the usual unlabeled bilingual training corpus is supplemented with human-annotated word alignments for only a small subset of the training data. The work cited above makes use of various training procedures and a wide variety of features. Indeed, whereas it can be difficult to design a factorization of a"
P06-1065,W05-0814,0,0.0230991,"acted from a pair of sentences and a proposed word alignment of them. The possible alignment having the highest overall score is selected for each sentence pair. Thus, for a sentence pair (e, f ) we seek the alignment a ˆ such that 1 Introduction Until recently, almost all work in statistical machine translation was based on word alignments obtained from combinations of generative probabalistic models developed at IBM by Brown et al. (1993), sometimes augmented by an HMMbased model or Och and Ney’s “Model 6” (Och and Ney, 2003). In 2005, however, several independent efforts (Liu et al., 2005; Fraser and Marcu, 2005; Ayan et al., 2005; Taskar et al., 2005; Moore, 2005; Ittycheriah and Roukos, 2005) demonstrated that discriminatively trained models can equal or surpass the alignment accuracy of the standard models, if the usual unlabeled bilingual training corpus is supplemented with human-annotated word alignments for only a small subset of the training data. The work cited above makes use of various training procedures and a wide variety of features. Indeed, whereas it can be difficult to design a factorization of a generative model that incorporates a ˆ = argmaxa n X λi fi (a, e, f ) i=1 where the fi a"
P06-1065,H05-1010,0,0.813027,"ed word alignment of them. The possible alignment having the highest overall score is selected for each sentence pair. Thus, for a sentence pair (e, f ) we seek the alignment a ˆ such that 1 Introduction Until recently, almost all work in statistical machine translation was based on word alignments obtained from combinations of generative probabalistic models developed at IBM by Brown et al. (1993), sometimes augmented by an HMMbased model or Och and Ney’s “Model 6” (Och and Ney, 2003). In 2005, however, several independent efforts (Liu et al., 2005; Fraser and Marcu, 2005; Ayan et al., 2005; Taskar et al., 2005; Moore, 2005; Ittycheriah and Roukos, 2005) demonstrated that discriminatively trained models can equal or surpass the alignment accuracy of the standard models, if the usual unlabeled bilingual training corpus is supplemented with human-annotated word alignments for only a small subset of the training data. The work cited above makes use of various training procedures and a wide variety of features. Indeed, whereas it can be difficult to design a factorization of a generative model that incorporates a ˆ = argmaxa n X λi fi (a, e, f ) i=1 where the fi are features and the λi are weights. The"
P06-1065,H05-1012,0,0.164313,"sible alignment having the highest overall score is selected for each sentence pair. Thus, for a sentence pair (e, f ) we seek the alignment a ˆ such that 1 Introduction Until recently, almost all work in statistical machine translation was based on word alignments obtained from combinations of generative probabalistic models developed at IBM by Brown et al. (1993), sometimes augmented by an HMMbased model or Och and Ney’s “Model 6” (Och and Ney, 2003). In 2005, however, several independent efforts (Liu et al., 2005; Fraser and Marcu, 2005; Ayan et al., 2005; Taskar et al., 2005; Moore, 2005; Ittycheriah and Roukos, 2005) demonstrated that discriminatively trained models can equal or surpass the alignment accuracy of the standard models, if the usual unlabeled bilingual training corpus is supplemented with human-annotated word alignments for only a small subset of the training data. The work cited above makes use of various training procedures and a wide variety of features. Indeed, whereas it can be difficult to design a factorization of a generative model that incorporates a ˆ = argmaxa n X λi fi (a, e, f ) i=1 where the fi are features and the λi are weights. The models are trained on a large number of bili"
P08-1012,J93-2003,0,0.0547987,"models toward generalizable, parsimonious parameter sets, leading to significant improvements in word alignment. This preference for sparse solutions together with effective pruning methods forms a phrase alignment regimen that produces better end-to-end translations than standard word alignment approaches. 1 Introduction Most state-of-the-art statistical machine translation systems are based on large phrase tables extracted from parallel text using word-level alignments. These word-level alignments are most often obtained using Expectation Maximization on the conditional generative models of Brown et al. (1993) and Vogel et al. (1996). As these word-level alignment models restrict the word alignment complexity by requiring each target word to align to zero or one source words, results are improved by aligning both source-to-target as well as target-to-source, then heuristically combining these alignments. Finally, the set of phrases consistent with the word alignments are extracted from every sentence pair; these form the basis of the decoding process. While this approach has been very successful, poor wordlevel alignments are nonetheless a common source of error in machine translation systems. A na"
P08-1012,W07-0403,0,0.541593,"ly speaking, the goal of this section is the same as the previous section, namely, to limit the set of phrase pairs that needs to be considered in the training process. The tic-tac-toe pruning relies on IBM model 1 for scoring a given aligned area. In this part, we use word-based ITG alignments as anchor points in the alignment space to pin down the potential phrases. The scope of iterative phrasal ITG training, therefore, is limited to determining the boundaries of the phrases anchored on the given one-toone word alignments. The heuristic method is based on the NonCompositional Constraint of Cherry and Lin (2007). Cherry and Lin (2007) use GIZA++ intersections which have high precision as anchor points in the 102 bitext space to constraint ITG phrases. We use ITG Viterbi alignments instead. The benefit is two-fold. First of all, we do not have to run a GIZA++ aligner. Second, we do not need to worry about non-ITG word alignments, such as the (2, 4, 1, 3) permutation patterns. GIZA++ does not limit the set of permutations allowed during translation, so it can produce permutations that are not reachable using an ITG. Formally, given a word-based ITG alignment, the bootstrapping algorithm finds all the p"
P08-1012,P05-1033,0,0.0486337,"recision as anchor points in the 102 bitext space to constraint ITG phrases. We use ITG Viterbi alignments instead. The benefit is two-fold. First of all, we do not have to run a GIZA++ aligner. Second, we do not need to worry about non-ITG word alignments, such as the (2, 4, 1, 3) permutation patterns. GIZA++ does not limit the set of permutations allowed during translation, so it can produce permutations that are not reachable using an ITG. Formally, given a word-based ITG alignment, the bootstrapping algorithm finds all the phrase pairs according to the definition of Och and Ney (2004) and Chiang (2005) with the additional constraint that each phrase pair contains at most one word link. Mathematically, let e(i, j) count the number of word links that are emitted from the substring ei...j , and f (l, m) count the number of word links emitted from the substring fl...m . The non-compositional phrase pairs satisfy e(i, j) = f (l, m) ≤ 1. Figure 3 (a) shows all possible non-compositional phrases given the Viterbi word alignment of the example sentence pair. 6 Summary of the Pipeline We summarize the pipeline of our system, demonstrating the interactions between the three main contributions of this"
P08-1012,P07-1094,0,0.0064648,"r non-zero values. Our second approach was to constrain the search space using simpler alignment models, which has the further benefit of significantly speeding up training. First we train a lower level word alignment model, then we place hard constraints on the phrasal alignment space using confident word links from this simpler model. Combining the two approaches, we have a staged training procedure going from the simplest unconstrained word based model to a constrained Bayesian word-level ITG model, and finally proceeding to a constrained Bayesian phrasal model. 3 Variational Bayes for ITG Goldwater and Griffiths (2007) and Johnson (2007) show that modifying an HMM to include a sparse prior over its parameters and using Bayesian estimation leads to improved accuracy for unsupervised part-of-speech tagging. In this section, we describe a Bayesian estimator for ITG: we select parameters that optimize the probability of the data given a prior. The traditional estimation method for word alignment models is the EM algorithm (Brown et al., 1993) which iteratively updates parameters to maximize the likelihood of the data. The drawback of maximum likelihood is obvious for phrase-based models. If we do not put any co"
P08-1012,D07-1031,0,0.0815598,"ach was to constrain the search space using simpler alignment models, which has the further benefit of significantly speeding up training. First we train a lower level word alignment model, then we place hard constraints on the phrasal alignment space using confident word links from this simpler model. Combining the two approaches, we have a staged training procedure going from the simplest unconstrained word based model to a constrained Bayesian word-level ITG model, and finally proceeding to a constrained Bayesian phrasal model. 3 Variational Bayes for ITG Goldwater and Griffiths (2007) and Johnson (2007) show that modifying an HMM to include a sparse prior over its parameters and using Bayesian estimation leads to improved accuracy for unsupervised part-of-speech tagging. In this section, we describe a Bayesian estimator for ITG: we select parameters that optimize the probability of the data given a prior. The traditional estimation method for word alignment models is the EM algorithm (Brown et al., 1993) which iteratively updates parameters to maximize the likelihood of the data. The drawback of maximum likelihood is obvious for phrase-based models. If we do not put any constraint on the dis"
P08-1012,koen-2004-pharaoh,0,0.0360953,"translation tables using five iterations of Model 1. These values were used to perform tic-tac-toe pruning with τb = 1 × 10−3 and τs = 1 × 10−6 . Over the pruned charts, we ran 10 iterations of word-based ITG using EM or VB. The charts were then pruned further by applying the non-compositional constraint from the Viterbi alignment links of that model. Finally we ran 10 iterations of phrase-based ITG over the residual charts, using EM or VB, and extracted the Viterbi alignments. For translation, we used the standard phrasal decoding approach, based on a re-implementation of the Pharaoh system (Koehn, 2004). The output of the word alignment systems (GIZA++ or ITG) were fed to a standard phrase extraction procedure that extracted all phrases of length up to 7 and estimated the conditional probabilities of source given target and target given source using relative frequencies. Thus our phrasal ITG learns only the minimal non-compositional phrases; the standard phrase-extraction algorithm learns larger combinations of these minimal units. In addition the phrases were annotated with lexical weights using the IBM Model 1 tables. The decoder also used a trigram language model trained on the target sid"
P08-1012,W02-1018,0,0.18838,"ues is unite the word-level and phrase-level models into one learning procedure. Ideally, such a procedure would remedy the deficiencies of word-level alignment models, including the strong restrictions on the form of the alignment, and the strong independence assumption between words. Furthermore it would obviate the need for heuristic combination of word alignments. A unified procedure may also improve the identification of non-compositional phrasal translations, and the attachment decisions for unaligned words. In this direction, Expectation Maximization at the phrase level was proposed by Marcu and Wong (2002), who, however, experienced two major difficulties: computational complexity and controlling overfitting. Computational complexity arises from the exponentially large number of decompositions of a sentence pair into phrase pairs; overfitting is a problem because as EM attempts to maximize the likelihood of its training data, it prefers to directly explain a sentence pair with a single phrase pair. In this paper, we attempt to address these two issues in order to apply EM above the word level. 97 Proceedings of ACL-08: HLT, pages 97–105, c Columbus, Ohio, USA, June 2008. 2008 Association for Co"
P08-1012,E03-1035,1,0.44657,"ee (either as terminals or non-terminals) serves to not only speed up ITG parsing, but also to provide a kind of initialization hint to the training procedures, encouraging it to focus on promising regions of the alignment space. Given a bitext cell defined by the four boundary indices (i, j, l, m) as shown in Figure 1a, we prune based on a figure of merit V (i, j, l, m) approximating the utility of that cell in a full ITG parse. The figure of merit considers the Model 1 scores of not only the words inside a given cell, but also all the words not included in the source and target spans, as in Moore (2003) and Vogel (2005). Like Zhang and Gildea (2005), it is used to prune bitext cells rather than score phrases. The total score is the product of the Model 1 probabilities for each column; “inside” columns in the range [l, m] are scored according to the sum (or maximum) of Model 1 probabilities for [i, j], and “outside” columns use the sum (or maximum) of all probabilities not in the range [i, j]. Our pruning differs from Zhang and Gildea (2005) in two major ways. First, we perform pruning using both directions of the IBM Model 1 scores; instead of a single figure of merit V , we have two: VF and"
P08-1012,J03-1002,0,0.0141074,"7.2 End-to-end Evaluation Given an unlimited amount of time, we would tune the prior to maximize end-to-end performance, using an objective function such as BLEU. Unfortunately these experiments are very slow. Since we observed monotonic increases in alignment performance with smaller values of αC , we simply fixed the prior at a very small value (10−100 ) for all translation experiments. We do compare VB against EM in terms of final BLEU scores in the translation experiments to ensure that this sparse prior has a significant impact on the output. We also trained a baseline model with GIZA++ (Och and Ney, 2003) following a regimen of 5 iterations of Model 1, 5 iterations of HMM, and 5 iterations of Model 4. We computed Chinese-toEnglish and English-to-Chinese word translation tables using five iterations of Model 1. These values were used to perform tic-tac-toe pruning with τb = 1 × 10−3 and τs = 1 × 10−6 . Over the pruned charts, we ran 10 iterations of word-based ITG using EM or VB. The charts were then pruned further by applying the non-compositional constraint from the Viterbi alignment links of that model. Finally we ran 10 iterations of phrase-based ITG over the residual charts, using EM or VB"
P08-1012,J04-4002,0,0.0390477,"tions which have high precision as anchor points in the 102 bitext space to constraint ITG phrases. We use ITG Viterbi alignments instead. The benefit is two-fold. First of all, we do not have to run a GIZA++ aligner. Second, we do not need to worry about non-ITG word alignments, such as the (2, 4, 1, 3) permutation patterns. GIZA++ does not limit the set of permutations allowed during translation, so it can produce permutations that are not reachable using an ITG. Formally, given a word-based ITG alignment, the bootstrapping algorithm finds all the phrase pairs according to the definition of Och and Ney (2004) and Chiang (2005) with the additional constraint that each phrase pair contains at most one word link. Mathematically, let e(i, j) count the number of word links that are emitted from the substring ei...j , and f (l, m) count the number of word links emitted from the substring fl...m . The non-compositional phrase pairs satisfy e(i, j) = f (l, m) ≤ 1. Figure 3 (a) shows all possible non-compositional phrases given the Viterbi word alignment of the example sentence pair. 6 Summary of the Pipeline We summarize the pipeline of our system, demonstrating the interactions between the three main con"
P08-1012,P03-1021,0,0.03586,"d all phrases of length up to 7 and estimated the conditional probabilities of source given target and target given source using relative frequencies. Thus our phrasal ITG learns only the minimal non-compositional phrases; the standard phrase-extraction algorithm learns larger combinations of these minimal units. In addition the phrases were annotated with lexical weights using the IBM Model 1 tables. The decoder also used a trigram language model trained on the target side of the training data, as well as word count, phrase count, and distortion penalty features. Minimum Error Rate training (Och, 2003) over BLEU was used to optimize the weights for each of these models over the development test data. We used the NIST 2002 evaluation datasets for tuning and evaluation; the 10-reference development set was used for minimum error rate training, and the 4-reference test set was used for evaluation. We trained several phrasal translation systems, varying only the word alignment (or phrasal alignment) method. Table 1 compares the four systems: the GIZA++ baseline, the ITG word-based model, the ITG multiword model using EM training, and the ITG multiword model using VB training. ITG-mwm-VB is our"
P08-1012,C96-2141,0,0.614103,"ble, parsimonious parameter sets, leading to significant improvements in word alignment. This preference for sparse solutions together with effective pruning methods forms a phrase alignment regimen that produces better end-to-end translations than standard word alignment approaches. 1 Introduction Most state-of-the-art statistical machine translation systems are based on large phrase tables extracted from parallel text using word-level alignments. These word-level alignments are most often obtained using Expectation Maximization on the conditional generative models of Brown et al. (1993) and Vogel et al. (1996). As these word-level alignment models restrict the word alignment complexity by requiring each target word to align to zero or one source words, results are improved by aligning both source-to-target as well as target-to-source, then heuristically combining these alignments. Finally, the set of phrases consistent with the word alignments are extracted from every sentence pair; these form the basis of the decoding process. While this approach has been very successful, poor wordlevel alignments are nonetheless a common source of error in machine translation systems. A natural solution to severa"
P08-1012,2005.mtsummit-papers.33,0,0.067956,"minals or non-terminals) serves to not only speed up ITG parsing, but also to provide a kind of initialization hint to the training procedures, encouraging it to focus on promising regions of the alignment space. Given a bitext cell defined by the four boundary indices (i, j, l, m) as shown in Figure 1a, we prune based on a figure of merit V (i, j, l, m) approximating the utility of that cell in a full ITG parse. The figure of merit considers the Model 1 scores of not only the words inside a given cell, but also all the words not included in the source and target spans, as in Moore (2003) and Vogel (2005). Like Zhang and Gildea (2005), it is used to prune bitext cells rather than score phrases. The total score is the product of the Model 1 probabilities for each column; “inside” columns in the range [l, m] are scored according to the sum (or maximum) of Model 1 probabilities for [i, j], and “outside” columns use the sum (or maximum) of all probabilities not in the range [i, j]. Our pruning differs from Zhang and Gildea (2005) in two major ways. First, we perform pruning using both directions of the IBM Model 1 scores; instead of a single figure of merit V , we have two: VF and VB . Only those"
P08-1012,J97-3002,0,0.894319,"arning small noncompositional phrases. We address the tendency of EM to overfit by using Bayesian methods, where sparse priors assign greater mass to parameter vectors with fewer non-zero values therefore favoring shorter, more frequent phrases. We test our model by extracting longer phrases from our model’s alignments using traditional phrase extraction, and find that a phrase table based on our system improves MT results over a phrase table extracted from traditional word-level alignments. 2 Phrasal Inversion Transduction Grammar We use a phrasal extension of Inversion Transduction Grammar (Wu, 1997) as the generative framework. Our ITG has two nonterminals: X and C, where X represents compositional phrase pairs that can have recursive structures and C is the preterminal over terminal phrase pairs. There are three rules with X on the left-hand side: X → [X X], X → hX Xi, X → C. The first two rules are the straight rule and inverted rule respectively. They split the left-hand side constituent which represents a phrase pair into two smaller phrase pairs on the right-hand side and order them according to one of the two possible permutations. The rewriting process continues until the third ru"
P08-1012,P05-1059,1,0.873,"nals) serves to not only speed up ITG parsing, but also to provide a kind of initialization hint to the training procedures, encouraging it to focus on promising regions of the alignment space. Given a bitext cell defined by the four boundary indices (i, j, l, m) as shown in Figure 1a, we prune based on a figure of merit V (i, j, l, m) approximating the utility of that cell in a full ITG parse. The figure of merit considers the Model 1 scores of not only the words inside a given cell, but also all the words not included in the source and target spans, as in Moore (2003) and Vogel (2005). Like Zhang and Gildea (2005), it is used to prune bitext cells rather than score phrases. The total score is the product of the Model 1 probabilities for each column; “inside” columns in the range [l, m] are scored according to the sum (or maximum) of Model 1 probabilities for [i, j], and “outside” columns use the sum (or maximum) of all probabilities not in the range [i, j]. Our pruning differs from Zhang and Gildea (2005) in two major ways. First, we perform pruning using both directions of the IBM Model 1 scores; instead of a single figure of merit V , we have two: VF and VB . Only those spans that pass the pruning th"
P09-2088,W06-3114,0,0.0777906,"Missing"
P09-2088,H93-1017,0,0.128565,"the same context: p(wn |w1 . . . wn−1 ) = P w0 C 0 (w1 . . . wn ) = {w0 |C(w0 w1 . . . wn ) > 0} In other words, the count used for a lower-order N-gram is the number of distinct word types that precede it in the training corpus. The fact that the lower-order models are estimated differently from the highest-order model makes the use of Kneser-Ney (KN) smoothing awkward in some situations. For example, coarse-to-fine search using a sequence of lowerorder to higher-order language models has been shown to be an efficient way of constraining highdimensional search spaces for speech recognition (Murveit et al., 1993) and machine translation (Petrov et al., 2008). The lower-order models used in KN smoothing, however, are very poor estimates of the probabilities for N-grams that have been observed in the training corpus, so they are C(w1 . . . wn ) C(w1 . . . wn−1 w0 ) One obvious problem with this method is that it assigns a probability of zero to any N-gram that is 349 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 349–352, c Suntec, Singapore, 4 August 2009. 2009 ACL and AFNLP p(wn |w1 . . . wn−1 ) =            αw1 ...wn−1 Cn (w1 ...wn )−Dn,Cn (w1 ...wn ) P C (w1 ...wn−1 w 0"
P09-2088,D08-1012,0,0.0303439,"C 0 (w1 . . . wn ) = {w0 |C(w0 w1 . . . wn ) > 0} In other words, the count used for a lower-order N-gram is the number of distinct word types that precede it in the training corpus. The fact that the lower-order models are estimated differently from the highest-order model makes the use of Kneser-Ney (KN) smoothing awkward in some situations. For example, coarse-to-fine search using a sequence of lowerorder to higher-order language models has been shown to be an efficient way of constraining highdimensional search spaces for speech recognition (Murveit et al., 1993) and machine translation (Petrov et al., 2008). The lower-order models used in KN smoothing, however, are very poor estimates of the probabilities for N-grams that have been observed in the training corpus, so they are C(w1 . . . wn ) C(w1 . . . wn−1 w0 ) One obvious problem with this method is that it assigns a probability of zero to any N-gram that is 349 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 349–352, c Suntec, Singapore, 4 August 2009. 2009 ACL and AFNLP p(wn |w1 . . . wn−1 ) =            αw1 ...wn−1 Cn (w1 ...wn )−Dn,Cn (w1 ...wn ) P C (w1 ...wn−1 w 0 ) w0 n + βw1 ...wn−1 p(wn |w2 . . . wn−1 ) γw"
P10-2041,2005.mtsummit-papers.11,0,0.018717,"extract from N . By a simple variant of Bayes rule, the probability P (NI |s, N ) of a text segment s, drawn randomly from N , being in NI is given by 221 Corpus Gigaword Europarl train Europarl test Sentence count 133,310,562 1,651,392 2,000 Token count 3,445,946,266 48,230,859 55,566 Table 1: Corpus size statistics 3 Experiments We have empirically evaluated our proposed method for selecting data from a non-domainspecific source to model text in a specific domain. For the in-domain corpus, we chose the English side of the English-French parallel text from release v5 of the Europarl corpus (Koehn, 2005). This consists of proceedings of the European Parliament from 1999 through 2009. We used the text from 1999 through 2008 as in-domain training data, and we used the first 2000 sentences from January 2009 as test data. For the nondomain-specific corpus, we used the LDC English Gigaword Third Edition (LDC Catalog No.: LDC2007T07). We used a simple tokenization scheme on all data, splitting on white space and on boundaries between alphanumeric and nonalphanumeric (e.g., punctuation) characters. With this tokenization, the sizes of our data sets in terms of sentences and tokens are shown in Table"
P10-2041,D07-1090,0,0.0282354,"ining data is reasonably well-matched to the desired output. This presents a problem, because in virtually any particular application the amount of in-domain data is limited. Thus it has become standard practice to combine in-domain data with other data, either by combining N-gram counts from in-domain and other data (usually weighting the counts in some way), or building separate language models from different data sources, interpolating the language model probabilities either linearly or log-linearly. Log-linear interpolation is particularly popular in statistical machine translation (e.g., Brants et al., 2007), because the interpolation weights can easily be discriminatively trained to optimize an end-to-end translation objective function (such as B LEU) by making the log probability according to each language model a separate feature function in the overall translation model. 2 Approaches to the Problem Our approach to the problem assumes that we have enough in-domain data to train a reasonable indomain language model, which we then use to help score text segments from other data sources, and we select segments based on a score cutoff optimized on held-out in-domain data. We are aware of two compa"
P11-1131,2009.eamt-1.23,0,0.575459,"Missing"
P11-1131,W04-3216,0,0.0424911,"Missing"
P11-1131,W06-3105,0,0.110753,"-word links. ments, such English not to French ne ? pas. Pruning the set of allowed phrases preserves the time complexity of the word-to-word HMM alignment model. 1.1 Related Work Our first major influence is that of conditional phrase-based models. An early approach by Deng and Byrne (2005) changed the parameterization of the traditional word-based HMM model, modeling subsequent words from the same state using a bigram model. However, this model changes only the parameterization and not the set of possible alignments. More closely related are the approaches of Daum´e III and Marcu (2004) and DeNero et al. (2006), which allow phrase-to-phrase alignments between the source and target domain. As DeNero warns, though, an unconstrained model may overfit using unusual segmentations. Interestingly, the phrase-based hidden semi-Markov model of Andr´es-Ferrer and Juan (2009) does not seem to encounter these problems. We suspect two main causes: first, the model interpolates with Model 1 (Brown et al., 1994), which may help prevent overfitting, and second, the model is monotonic, which screens out many possible alignments. Monotonicity is generally undesirable, though: almost all parallel sentences exhibit som"
P11-1131,H05-1022,0,0.192963,"Figure 2: The model of E given F can represent the phrasal alignment {e1 , e2 } ∼ {f1 }. However, the model of F given E cannot: the probability mass is distributed between {e1 } ∼ {f1 } and {e2 } ∼ {f1 }. Agreement of the forward and backward HMM alignments tends to place less mass on phrasal links and greater mass on word-to-word links. ments, such English not to French ne ? pas. Pruning the set of allowed phrases preserves the time complexity of the word-to-word HMM alignment model. 1.1 Related Work Our first major influence is that of conditional phrase-based models. An early approach by Deng and Byrne (2005) changed the parameterization of the traditional word-based HMM model, modeling subsequent words from the same state using a bigram model. However, this model changes only the parameterization and not the set of possible alignments. More closely related are the approaches of Daum´e III and Marcu (2004) and DeNero et al. (2006), which allow phrase-to-phrase alignments between the source and target domain. As DeNero warns, though, an unconstrained model may overfit using unusual segmentations. Interestingly, the phrase-based hidden semi-Markov model of Andr´es-Ferrer and Juan (2009) does not see"
P11-1131,P10-1016,0,0.0111129,"h as entailment, paraphrasing, question answering, summarization and spelling correction. A limitation to word-based alignment is undesirable. As seen in the French-English example in Figure 1, many sentence pairs are naturally aligned with multi-word units in both languages (chemin de fer; would ? like, where ? indicates a gap). Much work has addressed this problem: generative models for direct phrasal alignment (Marcu and Wong, 2002), heuristic word-alignment combinations (Koehn et al., 2003; Och and Ney, 2003), models with pseudoword collocations (Lambert and Banchs, 2006; Ma et al., 2007; Duan et al., 2010), synchronous grammar based approaches (Wu, 1997), etc. Most have a large state-space, using constraints and approximations for efficient inference. We present a new phrasal alignment model based on the hidden Markov framework (Vogel et al., 1996). Our approach is semi-Markov: each state can generate multiple observations, representing wordto-phrase alignments. We also augment the state space to include contiguous sequences. This corresponds to phrase-to-word and phrase-to-phrase alignments. We generalize alignment by agreement (Liang et al., 2006) to this space, and find that agreement discou"
P11-1131,J07-3002,0,0.0790977,"Missing"
P11-1131,N10-1140,0,0.0267577,"ligned sentence pairs. models prevents EM from overfitting, even in the absence of harsh penalties. We also allow gappy (noncontiguous) phrases on the state side, which makes agreement more successful but agreement needs approximation of posterior marginals. Using pruned lists of good phrases, we maintain complexity equal to the baseline word-to-word model. There are several steps forward from this point. Limiting the gap length also prevents combinatorial explosion; we hope to explore this in future work. Clearly a translation system that uses discontinuous mappings at runtime (Chiang, 2007; Galley and Manning, 2010) may make better use of discontinuous alignments. This model can also be applied at the morpheme or character level, allowing joint inference of segmentation and alignment. Furthermore the state space could be expanded and enhanced to include more possibilities: states with multiple gaps might be useful for alignment in languages with template morphology, such as Arabic or Hebrew. More exploration in the model space could be useful – a better distortion model might place a stronger distribution on the likely starting and ending points of phrases. Acknowledgments We would like to thank the anon"
P11-1131,N04-1035,0,0.0383112,"arkov Models, while maintaining asymptotically equivalent runtime. 1 English would not like traveling by railroad Figure 1: French-English pair with complex word alignment. Introduction Word alignment is an important part of statistical machine translation (MT) pipelines. Phrase tables containing pairs of source and target language phrases are extracted from word alignments, forming the core of phrase-based statistical machine translation systems (Koehn et al., 2003). Most syntactic machine translation systems extract synchronous context-free grammars (SCFGs) from aligned syntactic fragments (Galley et al., 2004; Zollmann et al., 2006), which in turn are derived from bilingual word alignments and syntactic ∗ Author was a summer intern at Microsoft Research during this project. parses. Alignment is also used in various other NLP problems such as entailment, paraphrasing, question answering, summarization and spelling correction. A limitation to word-based alignment is undesirable. As seen in the French-English example in Figure 1, many sentence pairs are naturally aligned with multi-word units in both languages (chemin de fer; would ? like, where ? indicates a gap). Much work has addressed this proble"
P11-1131,N03-1017,0,0.0294345,"ontinuous alignments. The resulting system shows substantial improvements in both alignment quality and translation quality over word-based Hidden Markov Models, while maintaining asymptotically equivalent runtime. 1 English would not like traveling by railroad Figure 1: French-English pair with complex word alignment. Introduction Word alignment is an important part of statistical machine translation (MT) pipelines. Phrase tables containing pairs of source and target language phrases are extracted from word alignments, forming the core of phrase-based statistical machine translation systems (Koehn et al., 2003). Most syntactic machine translation systems extract synchronous context-free grammars (SCFGs) from aligned syntactic fragments (Galley et al., 2004; Zollmann et al., 2006), which in turn are derived from bilingual word alignments and syntactic ∗ Author was a summer intern at Microsoft Research during this project. parses. Alignment is also used in various other NLP problems such as entailment, paraphrasing, question answering, summarization and spelling correction. A limitation to word-based alignment is undesirable. As seen in the French-English example in Figure 1, many sentence pairs are n"
P11-1131,W06-2402,0,0.0162878,"also used in various other NLP problems such as entailment, paraphrasing, question answering, summarization and spelling correction. A limitation to word-based alignment is undesirable. As seen in the French-English example in Figure 1, many sentence pairs are naturally aligned with multi-word units in both languages (chemin de fer; would ? like, where ? indicates a gap). Much work has addressed this problem: generative models for direct phrasal alignment (Marcu and Wong, 2002), heuristic word-alignment combinations (Koehn et al., 2003; Och and Ney, 2003), models with pseudoword collocations (Lambert and Banchs, 2006; Ma et al., 2007; Duan et al., 2010), synchronous grammar based approaches (Wu, 1997), etc. Most have a large state-space, using constraints and approximations for efficient inference. We present a new phrasal alignment model based on the hidden Markov framework (Vogel et al., 1996). Our approach is semi-Markov: each state can generate multiple observations, representing wordto-phrase alignments. We also augment the state space to include contiguous sequences. This corresponds to phrase-to-word and phrase-to-phrase alignments. We generalize alignment by agreement (Liang et al., 2006) to this"
P11-1131,N06-1014,0,0.704607,"ns (Lambert and Banchs, 2006; Ma et al., 2007; Duan et al., 2010), synchronous grammar based approaches (Wu, 1997), etc. Most have a large state-space, using constraints and approximations for efficient inference. We present a new phrasal alignment model based on the hidden Markov framework (Vogel et al., 1996). Our approach is semi-Markov: each state can generate multiple observations, representing wordto-phrase alignments. We also augment the state space to include contiguous sequences. This corresponds to phrase-to-word and phrase-to-phrase alignments. We generalize alignment by agreement (Liang et al., 2006) to this space, and find that agreement discourages EM from overfitting. Finally, we make the alignment space more symmetric by including gappy (or non-contiguous) phrases. This allows agreement to reinforce non-contiguous align1308 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1308–1317, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics Observations→ e1 e2 e3 f1 f2 f3 States→ f1 e1 ? f2 e2 ? f3 e3 HMM(E|F) HMM(F|E) Figure 2: The model of E given F can represent the phrasal alignment {e1 , e2 } ∼ {f1 }. However"
P11-1131,P07-1039,0,0.0160259,"NLP problems such as entailment, paraphrasing, question answering, summarization and spelling correction. A limitation to word-based alignment is undesirable. As seen in the French-English example in Figure 1, many sentence pairs are naturally aligned with multi-word units in both languages (chemin de fer; would ? like, where ? indicates a gap). Much work has addressed this problem: generative models for direct phrasal alignment (Marcu and Wong, 2002), heuristic word-alignment combinations (Koehn et al., 2003; Och and Ney, 2003), models with pseudoword collocations (Lambert and Banchs, 2006; Ma et al., 2007; Duan et al., 2010), synchronous grammar based approaches (Wu, 1997), etc. Most have a large state-space, using constraints and approximations for efficient inference. We present a new phrasal alignment model based on the hidden Markov framework (Vogel et al., 1996). Our approach is semi-Markov: each state can generate multiple observations, representing wordto-phrase alignments. We also augment the state space to include contiguous sequences. This corresponds to phrase-to-word and phrase-to-phrase alignments. We generalize alignment by agreement (Liang et al., 2006) to this space, and find t"
P11-1131,W02-1018,0,0.0409262,"from bilingual word alignments and syntactic ∗ Author was a summer intern at Microsoft Research during this project. parses. Alignment is also used in various other NLP problems such as entailment, paraphrasing, question answering, summarization and spelling correction. A limitation to word-based alignment is undesirable. As seen in the French-English example in Figure 1, many sentence pairs are naturally aligned with multi-word units in both languages (chemin de fer; would ? like, where ? indicates a gap). Much work has addressed this problem: generative models for direct phrasal alignment (Marcu and Wong, 2002), heuristic word-alignment combinations (Koehn et al., 2003; Och and Ney, 2003), models with pseudoword collocations (Lambert and Banchs, 2006; Ma et al., 2007; Duan et al., 2010), synchronous grammar based approaches (Wu, 1997), etc. Most have a large state-space, using constraints and approximations for efficient inference. We present a new phrasal alignment model based on the hidden Markov framework (Vogel et al., 1996). Our approach is semi-Markov: each state can generate multiple observations, representing wordto-phrase alignments. We also augment the state space to include contiguous seq"
P11-1131,J03-1002,0,0.202395,"osoft Research during this project. parses. Alignment is also used in various other NLP problems such as entailment, paraphrasing, question answering, summarization and spelling correction. A limitation to word-based alignment is undesirable. As seen in the French-English example in Figure 1, many sentence pairs are naturally aligned with multi-word units in both languages (chemin de fer; would ? like, where ? indicates a gap). Much work has addressed this problem: generative models for direct phrasal alignment (Marcu and Wong, 2002), heuristic word-alignment combinations (Koehn et al., 2003; Och and Ney, 2003), models with pseudoword collocations (Lambert and Banchs, 2006; Ma et al., 2007; Duan et al., 2010), synchronous grammar based approaches (Wu, 1997), etc. Most have a large state-space, using constraints and approximations for efficient inference. We present a new phrasal alignment model based on the hidden Markov framework (Vogel et al., 1996). Our approach is semi-Markov: each state can generate multiple observations, representing wordto-phrase alignments. We also augment the state space to include contiguous sequences. This corresponds to phrase-to-word and phrase-to-phrase alignments. We"
P11-1131,C96-2141,0,0.958377,"-word units in both languages (chemin de fer; would ? like, where ? indicates a gap). Much work has addressed this problem: generative models for direct phrasal alignment (Marcu and Wong, 2002), heuristic word-alignment combinations (Koehn et al., 2003; Och and Ney, 2003), models with pseudoword collocations (Lambert and Banchs, 2006; Ma et al., 2007; Duan et al., 2010), synchronous grammar based approaches (Wu, 1997), etc. Most have a large state-space, using constraints and approximations for efficient inference. We present a new phrasal alignment model based on the hidden Markov framework (Vogel et al., 1996). Our approach is semi-Markov: each state can generate multiple observations, representing wordto-phrase alignments. We also augment the state space to include contiguous sequences. This corresponds to phrase-to-word and phrase-to-phrase alignments. We generalize alignment by agreement (Liang et al., 2006) to this space, and find that agreement discourages EM from overfitting. Finally, we make the alignment space more symmetric by including gappy (or non-contiguous) phrases. This allows agreement to reinforce non-contiguous align1308 Proceedings of the 49th Annual Meeting of the Association fo"
P11-1131,J97-3002,0,0.620845,"zation and spelling correction. A limitation to word-based alignment is undesirable. As seen in the French-English example in Figure 1, many sentence pairs are naturally aligned with multi-word units in both languages (chemin de fer; would ? like, where ? indicates a gap). Much work has addressed this problem: generative models for direct phrasal alignment (Marcu and Wong, 2002), heuristic word-alignment combinations (Koehn et al., 2003; Och and Ney, 2003), models with pseudoword collocations (Lambert and Banchs, 2006; Ma et al., 2007; Duan et al., 2010), synchronous grammar based approaches (Wu, 1997), etc. Most have a large state-space, using constraints and approximations for efficient inference. We present a new phrasal alignment model based on the hidden Markov framework (Vogel et al., 1996). Our approach is semi-Markov: each state can generate multiple observations, representing wordto-phrase alignments. We also augment the state space to include contiguous sequences. This corresponds to phrase-to-word and phrase-to-phrase alignments. We generalize alignment by agreement (Liang et al., 2006) to this space, and find that agreement discourages EM from overfitting. Finally, we make the a"
P11-1131,W06-3119,0,0.016468,"aintaining asymptotically equivalent runtime. 1 English would not like traveling by railroad Figure 1: French-English pair with complex word alignment. Introduction Word alignment is an important part of statistical machine translation (MT) pipelines. Phrase tables containing pairs of source and target language phrases are extracted from word alignments, forming the core of phrase-based statistical machine translation systems (Koehn et al., 2003). Most syntactic machine translation systems extract synchronous context-free grammars (SCFGs) from aligned syntactic fragments (Galley et al., 2004; Zollmann et al., 2006), which in turn are derived from bilingual word alignments and syntactic ∗ Author was a summer intern at Microsoft Research during this project. parses. Alignment is also used in various other NLP problems such as entailment, paraphrasing, question answering, summarization and spelling correction. A limitation to word-based alignment is undesirable. As seen in the French-English example in Figure 1, many sentence pairs are naturally aligned with multi-word units in both languages (chemin de fer; would ? like, where ? indicates a gap). Much work has addressed this problem: generative models for"
P11-1131,J93-2003,0,\N,Missing
P11-1131,J07-2003,0,\N,Missing
P81-1028,T75-2036,0,\N,Missing
P81-1028,P80-1001,0,\N,Missing
P82-1007,P81-1009,0,\N,Missing
P89-1002,E89-1032,0,0.151303,"ieve the underlying method to be more broadly applicable. A variant of our method is used in Van Noord&apos;s BUG (Bottom-Up Generator) system, part of MiMo2, an experimental machine translation system for translating international news items of Teletext, which uses a Prolog version of PATI~-II similar to that of Hirsh (1987). According to Martin Kay (personal communication), the STREP machine translation project at the Center for the Study of Language and Information uses a version of our algorithm to generate with respect to grammars based on head-driven phrase-structure grammar (HPSG). Finally, Calder et al. (1989) report on a generation algorithm for unification categorial grammar that appears to be a special case of ours. 3 Problems with Generators Existing Existing generation algorithms have efficiency or termination problems with respect to certain classes of grammars. We review the problems of both top-down and bottom-up regimes in this section. 3.1 Problems with Top-Down Generators Consider a naive top-down generation mechanism that takes as input the semantics to generate from and a corresponding syntactic category and builds a complete tree, top-down, left-to-right by applying rules of the gramm"
P89-1002,1988.tmi-1.12,0,0.737425,"ction method is reasonably successful along certain dimensions. It is quite simple, general in its applicability to a range of unification-based and logic grammar formalisms, and uniform, in that it places only one restriction (discussed below) on the form of the linguistic analyses allowed by the grammars used in generation. In particular, generation from grammars with recursions whose welbfoundedness relies t D e p a r t m e n t of Linguistics Rijksuniversiteit Utrecht Utrecht, Netherlands on lexical information will terminate; top-down generation regimes such as those of Wedekind (1988) or Dymetman and Isabelle (1988) lack this property, discussed further in Section 3.1. Unfortunately, the bottom-up, left-to-right processing regime of Earley generation--as it might be called--has its own inherent frailties. Efficiency considerations require that only grammars possessing a property of semantic monotonicity can be effectively used, and even for those grammars, processing can become overly nondeterministic. The algorithm described in this paper is an attempt to resolve these problems in a satisfactory manner. Although we believe that this algorithm could be seen as an instance of a uniform architecture for pa"
P89-1002,J87-1005,1,0.63248,"than necessary because the distribution of store elements among the subject and complements of a verb does not check whether the variable bound by a store element actually appears in the semantics of the phrase to which it is being assigned, leading to many dead ends in the generation process. Also, the rules are sound for generation but not for analysis, because they do not enforce the constraint that every occurrence of a variable in logical form be outscoped by the variable&apos;s binder. Adding appropriate side conditions to the rules, following the constraints discussed by Hobbs and Shieber (Hobbs and Shieber, 1987) would not be difficult. 6.3 Postponing Lexical Choice As it stands, the generation algorithm chooses particular lexical forms on-line. This approach can lead to a certain amount of unnecessary nondeterminism. For instance, the choice of verb form might depend on syntactic features of the verb&apos;s subject available only after the subject has been generated. This nondeterminism can be eliminated by deferring lexical choice to a postprocess. The generator will yield a list of lexical items instead of a list of words. To this list a small phonological front end is applied. BUG uses such a mechanism"
P89-1002,P83-1021,1,0.865954,"iven fashion. 1 Introduction The problem of generating a well-formed naturallanguage expression from an encoding of its meaning possesses certain properties which distinguish it from the converse problem of recovering a meaning encoding from a given natural-language expression. In previous work (Shieber, 1988), however, one of us attempted to characterize these differing properties in such a way that a single uniform architecture, appropriately parameterized, might be used for both natural-language processes. In particular, we developed an architecture inspired by the Earley deduction work of Pereira and Warren (1983) but which generalized that work allowing for its use in both a parsing and generation mode merely by setting the values of a small number of parameters. As a method for generating natural-language expressions, the Earley deduction method is reasonably successful along certain dimensions. It is quite simple, general in its applicability to a range of unification-based and logic grammar formalisms, and uniform, in that it places only one restriction (discussed below) on the form of the linguistic analyses allowed by the grammars used in generation. In particular, generation from grammars with r"
P89-1002,P85-1018,1,0.928086,"ortunately, the bottom-up, left-to-right processing regime of Earley generation--as it might be called--has its own inherent frailties. Efficiency considerations require that only grammars possessing a property of semantic monotonicity can be effectively used, and even for those grammars, processing can become overly nondeterministic. The algorithm described in this paper is an attempt to resolve these problems in a satisfactory manner. Although we believe that this algorithm could be seen as an instance of a uniform architecture for parsing and generation--just as the extended Earley parser (Shieber, 1985b) and the bottom-up generator were instances of the generalized Earley deduction architecture= our efforts to date have been aimed foremost toward the development of the algorithm for generation alone. We will have little to say about its relation to parsing, leaving such questions for later research.1 2 Applicability of the Algorithm As does the Earley-based generator, the new algorithm assumes that the grammar is a unificationbased or logic grammar with a phrase-structure backbone and complex nonterminMs. Furthermore, and again consistent with previous work, we assume that the nonterminals"
P89-1002,C88-2128,1,0.294441,"n Earley deduction generator (Shieber, 1988), it allows use of semantically nonmonotonic grammars, yet unlike topdown methods, it also permits left-recursion. The enabling design feature of the algorithm is its implicit traversal of the analysis tree for the string being generated in a semantic-head-driven fashion. 1 Introduction The problem of generating a well-formed naturallanguage expression from an encoding of its meaning possesses certain properties which distinguish it from the converse problem of recovering a meaning encoding from a given natural-language expression. In previous work (Shieber, 1988), however, one of us attempted to characterize these differing properties in such a way that a single uniform architecture, appropriately parameterized, might be used for both natural-language processes. In particular, we developed an architecture inspired by the Earley deduction work of Pereira and Warren (1983) but which generalized that work allowing for its use in both a parsing and generation mode merely by setting the values of a small number of parameters. As a method for generating natural-language expressions, the Earley deduction method is reasonably successful along certain dimensio"
P89-1002,C88-2150,0,0.450177,"ns, the Earley deduction method is reasonably successful along certain dimensions. It is quite simple, general in its applicability to a range of unification-based and logic grammar formalisms, and uniform, in that it places only one restriction (discussed below) on the form of the linguistic analyses allowed by the grammars used in generation. In particular, generation from grammars with recursions whose welbfoundedness relies t D e p a r t m e n t of Linguistics Rijksuniversiteit Utrecht Utrecht, Netherlands on lexical information will terminate; top-down generation regimes such as those of Wedekind (1988) or Dymetman and Isabelle (1988) lack this property, discussed further in Section 3.1. Unfortunately, the bottom-up, left-to-right processing regime of Earley generation--as it might be called--has its own inherent frailties. Efficiency considerations require that only grammars possessing a property of semantic monotonicity can be effectively used, and even for those grammars, processing can become overly nondeterministic. The algorithm described in this paper is an attempt to resolve these problems in a satisfactory manner. Although we believe that this algorithm could be seen as an instance"
P89-1002,J81-4003,1,\N,Missing
P89-1005,C86-1016,0,0.0467717,"Missing"
P93-1008,P92-1008,1,0.537922,"Missing"
P93-1008,J88-1004,0,0.0191928,"Missing"
P93-1008,J83-3001,0,0.0242272,"Missing"
P93-1008,H91-1036,1,0.855442,"Missing"
P93-1008,P93-1007,0,0.0665809,"Missing"
P93-1008,C90-3029,1,0.810386,"Missing"
P93-1008,H92-1060,0,0.060108,"Missing"
P93-1008,P83-1017,0,0.0291575,"sprefer parse trees containing specific &quot;marked&quot; syntax rules. As an example of a dispreferred rule, consider: Book those three flights to Boston. This sentence has a parse on which those three is a noun phrase with a missing head (consider a continuation of the discourse Three of our clients have sufficient credit). After penalizing such dispreferred parses, the preference 2For these results, we ignored repairs consisting of only an isolate fragment word, or sentence-initial filler words like &quot;yes&quot; and &quot;okay&quot;. 58 mechanism applies attachment heuristics based on the work by Pereira (1985) and Shieber (1983) Pereira&apos;s paper shows how the heuristics of Minimal Attachment and Right Association (Kimball, 1973) can both be implemented using a bottom-up shift-reduce parser. constituent a song for Mary from two constituents, while the corresponding reduce in (a) builds sang a song for Mary from three constituents. Parse (b) thus loses by Minimal Attachment. Questions about the exact nature of parse preferences (and thus about the empirical adequacy of Pereira&apos;s proposal) still remain open, but the mechanism sketched does provide plausible results for a number of examples. (2)(a) John sang a song for Ma"
P93-1008,A92-1026,1,0.852889,"Missing"
P93-1008,H92-1061,0,0.0639319,"Missing"
P93-1008,C86-1045,0,0.0101966,"Missing"
P93-1008,H92-1003,0,0.0149615,"r scoping rules are applied to this best interpretation to produce the final logical form, which is then used as input to a query-answering system. The following sections describe each of these components in detail, with the exception of the query-answering subsystem, which is not described in this paper. In our component-by-component view of Gemini, we provide detailed statistics on each component&apos;s size, speed, coverage, and accuracy. These numbers detail our performance on the subdomain of air-travel planning that is currently being used by the ARPA spoken language understanding community (MADCOW, 1992). Gemini was trained on a 5875-utterance dataset from this domain, with another 688 utterances used as a blind test (not explicitly trained on, but run nmltiple times) to monitor our performance on a dataset on which we did not train. We also report here our results on another 756-utterance fair test set that we ran only once. Table 1 contains a summary of the coverage of the various components on both the training and fair test sets. More detailed Gemini is a natural language (NL) understanding system developed for spoken language applications. This paper describes the details of the system,"
P93-1008,P88-1005,1,0.220194,"The final logical form produced by Gemini is the result of applying a set of quantifier scoping rules to the best interpretation chosen by the parse preference mechanism. The semantic rules build quasi-logical forms, which contain complete semantic predicate-argument structure, but do not specify quantifier scoping. The scoping algorithm that we use combines syntactic and semantic information with a set of quantifier scoping preference rules to rank the possible scoped logical forms consistent with the quasi-logical form selected by parse preferences. This algorithm is described in detail in (Moran, 1988). 3. C O N C L U S I O N In our approach to resolving the tension between overgeneration and robustness in a spoken language understanding system, some aspects of Gemini are specifically oriented towards limiting overgeneration, such as the on-line property for the parser, and fully interleaved syntactic and semantic processing. Other components, such as the fragment and run-on processing provided by the utterance grammar, and the correction of recognizable grammatical repairs, increase the robustness of Gemini. We believe a robust system can still recognize and disprefer utterances containing"
P94-1016,H92-1003,0,0.0212075,"n syntax-only parsing, which can introduce additional ambiguity, multiplying the number of distinct phrases found and increasing parse time. We describe two special techniques for speeding up bottom-up parsing by reducing local ambiguity without sacrificing completeness. One technique, &quot;limited left-context checking,&quot; reduces local syntactic ambiguity; the other, &quot;deferred sortal-constraint application,&quot; reduces local semantic ambiguity. Both techniques are applied to unification-based grammars. We analyze the performance of these techniques on a 194-utterance subset of the AP~PA ATIS corpus (MADCOW, 1992), using a broad-coverage grammar of English. Finally, we present results using the output of the parser to improve the accuracy of a speech recognizer in a way that takes advantage of our ability to find all syntactically well-formed semantically meaningful phrases. We describe an efficient bottom-up parser that interleaves syntactic and semantic structure building. Two techniques are presented for reducing search by reducing local ambiguity: Limited leftcontext constraints are used to reduce local syntactic ambiguity, and deferred sortal-constraint application is used to reduce local semantic"
P94-1016,H91-1036,1,0.802234,"er processing. Analyses are collapsed if they have the same parent nonterminal, incorporating both syntactic and semantic features, and the same semantic sortal properties. method, no predictions need to be generated for the context-independent categories; from another point of view, context-independent categories are predicted statically, at compile time, for all points in the input, rather than dynamically at run time. Time is saved both because the predictions do not have to be generated at run time, and because the process of checking these static predictions is simpler. In previous work (Moore and Dowding, 1991), we compared limited left-context checking to some other methods for dealing with empty categories in a bottom-up parser. Standard grammar transformation techniques (Hopcroft and Ullman, 1980) can be used to eliminate empty nonterminals. This approach is useful to eliminate some edges, but still allows edges that dominate empty categories to be created. We found that using this technique was faster than pure bottom-up parsing, but still significantly slower than limited leftcontext checking. A further refinement is to transform the grammar to eliminate both empty and nonbranching rules. I.n t"
P94-1016,H94-1011,0,0.0367957,"Missing"
P94-1016,P85-1018,0,0.110288,"ed at each point in the input for the context-dependent phrases that are licensed at that point. Some details of the parser have been omitted, particularly those related to parsing unification-based grammars that do not arise when parsing context-free grammars. In addition, the parser maintains a skeletal copy of the chart in which edges are labeled only by the nonterminal symbols contained in their context-free backbone, which gives us more efficient indexing of the full grammar rules. Other optimizations include using one-word look-ahead before adding new predictions, and using restrictors (Shieber, 1985) to increase the generality of the predictions. standing system (Dowding et al., 1993), which features a broad-coverage unification-based grammar of English, with independent syntactic, semantic and lexical components, in the style of the SRI Core Language Engine (Alshawi, 1992). Although we describe the syntactic parsing algorithm as though it were parsing purely contextfree grammars, the ideas extend in a natural way to unification-based grammar parsing. While the chart for a context-free grammar contains edges labeled by atomic nonterminal symbols, the chart for a unification-based grammar"
P94-1016,P93-1008,1,0.915593,"Missing"
P94-1016,P81-1022,0,0.112114,"Missing"
P94-1016,H93-1003,0,\N,Missing
P94-1016,H93-1008,1,\N,Missing
P99-1024,P97-1034,0,0.0161568,"e (OAA) (Martin et al., 1998). This architecture allows components to be developed independently, and then flexibly and dynamically combined to support distributed computation. Most of the agents that compose CommandTalk have been described elsewhere !for more detail, see (Moore et al., 1997)). This paper describes extensions to CommandTalk to support spoken dialogue. While we make no theoretical claims about the nature and structure of dialogue, we are influenced by the theoretical work of (Grosz and Sidner, 1986) and will use terminology from that tradition when appropriate. We also follow (Chu-Carroll and Brown, 1997) in distinguishing task initiative and dialogue initiative. Section 2 demonstrates the dialogue capabilities of CommandTalk by way of an extended example. Section 3 describes how language in CommandTalk is modeled for understanding and generation. Section 4 describes the architecture of the dialogue manager in detail. Section 5 compares CommandTalk with other spo* This research was supported by the Defense Advanced Research Projects Agency under Contract N66001-94-C6046 with the Space and Naval Warfare Systems Center. The views and conclusions contained in this document are those of the author"
P99-1024,P93-1008,1,0.266453,"ctive Bravo. The system then tests the guard, which succeeds because Objective Bravo now exists. The system therefore takes dialogue initiative by asking the operator in utterance 31 if that operator would like to carry out the original command. Although, in this case, the simulated world changed in direct response to a linguistic act, in general the world can change for a variety of reasons, including the operator's activities on the GUI or the activities of other operators. Language Interpretation and Generation The language used in CommandTalk is derived from a single grammar using Gemini (Dowding et al., 1993), a unification-based grammar formalism. This grammar is used to provide all the language modeling capabilities of the system, including the language model used in the speech recognizer, the syntactic and semantic interpretation of user utterances (Dowding et al., 1994), and the generation of system responses (Shieber et al., 1990). For speech recognition, Gemini uses the Nuance speech recognizer. Nuance accepts language models written in a Grammar Specification Language (GSL) format that allows context-free, as well as the more commonly used finite-state, models. 3 Using a technique described"
P99-1024,P94-1016,1,0.73156,"the simulated world changed in direct response to a linguistic act, in general the world can change for a variety of reasons, including the operator's activities on the GUI or the activities of other operators. Language Interpretation and Generation The language used in CommandTalk is derived from a single grammar using Gemini (Dowding et al., 1993), a unification-based grammar formalism. This grammar is used to provide all the language modeling capabilities of the system, including the language model used in the speech recognizer, the syntactic and semantic interpretation of user utterances (Dowding et al., 1994), and the generation of system responses (Shieber et al., 1990). For speech recognition, Gemini uses the Nuance speech recognizer. Nuance accepts language models written in a Grammar Specification Language (GSL) format that allows context-free, as well as the more commonly used finite-state, models. 3 Using a technique described in (Moore, 1999), we compile a contextfree covering grammar into GSL format from the main Gemini grammar. This approach of using a single grammar source for both sides of the dialogue has several advantages. First, although there are differences between the language us"
P99-1024,P98-2129,0,0.0224411,"xed-initiative dialogues, and dialogues involving multi-modality. 5 Related Work C o m m a n d T a l k differs from other recent spoken language systems in that it is a c o m m a n d and control application. It provides a particularly 188 interesting environment in which to design spoken dialogue systems in that it supports distributed stochastic simulations, in which one operator controls a certain collection of forces while other operators simultaneously control other allied a n d / o r opposing forces, and unexpected events can occur that require responses in real time. Other applications (Litman et al., 1998; Walker et al., 1998) have been in domains that were sufficiently limited (e.g., queries about train schedules, or reading email) that the system could presume much about the user's goals, and make significant contributions to task initiative. However, the high number of possible commands available in CommandTalk, and the more abstract nature of the user's high-level goals (to carry out a simulation of a complex military engagement) preclude the system from taking significant task initiative in most cases. The system most closely related to CommandTalk in terms of dialogue use is TRIPS (Fergu"
P99-1024,H92-1003,0,0.019807,"s that it has been impossible to perform a formal evaluation of the system. This is due to the difficulty of collecting data in this domain, which requires speakers who are both knowledgeable about the domain and familiar with ModSAF. CommandTalk has been used in simulations of real military exercises, but those exercises have always taken place in classified environments where data collection is not permitted. To facilitate such an evaluation, we are currently porting the CommandTalk dialogue manager to the domain of air travel planning. There is a large body of existing data in that domain (MADCOW, 1992), and speakers familiar with the domain are easily available. The internal representation of actions in CommandTalk is derived from ModSAF. We would like to port that to a domain-independent representation such as frames or explicit representations of plans. Finally, there are interesting options regarding the finite state model. We are investigating other representations for the semantic contents of a discourse segment, such as frames or active templates. 7 Acknowledgments We would like to thank Andrew Kehler, David Israel, Jerry Hobbs, and Sharon Goldwater for comments on an earlier version"
P99-1024,P98-2136,0,0.0164003,"task initiative in most cases. The system most closely related to CommandTalk in terms of dialogue use is TRIPS (Ferguson and Allen, 1998), although there are several important differences. In contrast to TRIPS, in CommandTalk gestures are fully incorporated into the dialogue state. Also, CommandTalk provides the same language capabilities for user and system utterances. Unlike other simulation systems, such as QuickSet (Cohen et al., 1997), CommandTalk has extensive dialogue capabilities. In QuickSet, the user is required to confirm each spoken utterance before it is processed by the system (McGee et al., 1998). Our earlier work on spoken dialogue in the air travel planning domain (Bratt et al., 1995) (and related systems) interpreted speaker utterances in context, but did not support structured dialogues. The technique of using dialogue context to control the speech recognition state is similar to one used in (Andry, 1992). 6 Future Work We have discussed some aspects of CommandTalk that make it especially suited to handle different kinds of interactions. We have looked at the use of a dialogue stack, salience information, and focus spaces to assist interpretation and generation. We have seen that"
P99-1024,A97-1001,0,0.0757605,"dSAF battlefield simulator that allows simulation operators to generate and execute military exercises by creating forces and control measures, assigning missions to forces, and controlling the display (Ceranowicz, 1994). CommandTalk consists of independent, cooperating agents interacting through SRI's Open Agent Architecture (OAA) (Martin et al., 1998). This architecture allows components to be developed independently, and then flexibly and dynamically combined to support distributed computation. Most of the agents that compose CommandTalk have been described elsewhere !for more detail, see (Moore et al., 1997)). This paper describes extensions to CommandTalk to support spoken dialogue. While we make no theoretical claims about the nature and structure of dialogue, we are influenced by the theoretical work of (Grosz and Sidner, 1986) and will use terminology from that tradition when appropriate. We also follow (Chu-Carroll and Brown, 1997) in distinguishing task initiative and dialogue initiative. Section 2 demonstrates the dialogue capabilities of CommandTalk by way of an extended example. Section 3 describes how language in CommandTalk is modeled for understanding and generation. Section 4 describ"
P99-1024,P98-2219,0,0.00572617,"ues, and dialogues involving multi-modality. 5 Related Work C o m m a n d T a l k differs from other recent spoken language systems in that it is a c o m m a n d and control application. It provides a particularly 188 interesting environment in which to design spoken dialogue systems in that it supports distributed stochastic simulations, in which one operator controls a certain collection of forces while other operators simultaneously control other allied a n d / o r opposing forces, and unexpected events can occur that require responses in real time. Other applications (Litman et al., 1998; Walker et al., 1998) have been in domains that were sufficiently limited (e.g., queries about train schedules, or reading email) that the system could presume much about the user's goals, and make significant contributions to task initiative. However, the high number of possible commands available in CommandTalk, and the more abstract nature of the user's high-level goals (to carry out a simulation of a complex military engagement) preclude the system from taking significant task initiative in most cases. The system most closely related to CommandTalk in terms of dialogue use is TRIPS (Ferguson and Allen, 1998),"
P99-1024,H93-1008,1,\N,Missing
P99-1024,C98-2214,0,\N,Missing
P99-1024,J86-3001,0,\N,Missing
P99-1024,C98-2124,0,\N,Missing
P99-1024,P89-1002,1,\N,Missing
P99-1024,C98-2131,0,\N,Missing
ringger-etal-2004-using,A00-2018,1,\N,Missing
ringger-etal-2004-using,J93-2004,0,\N,Missing
ringger-etal-2004-using,J03-4003,0,\N,Missing
ringger-etal-2004-using,P02-1034,0,\N,Missing
ringger-etal-2004-using,P02-1035,0,\N,Missing
ringger-etal-2004-using,P95-1037,0,\N,Missing
W00-1603,J98-2004,0,\N,Missing
W00-1603,A00-2018,0,\N,Missing
W00-1603,W00-1604,0,\N,Missing
W00-1603,J96-1002,0,\N,Missing
W00-1603,A97-1001,0,\N,Missing
W00-1603,P99-1066,0,\N,Missing
W01-1411,J93-2003,0,0.0145169,"Missing"
W01-1411,J93-1003,0,0.053256,"eparate tokens in our French logical forms. 3.2 Computing association scores For Step 2, we compute the degree of association between a lemma wL1 and a lemma wL2 in terms of the frequencies with which wL1 occurs in sentences of the L1 part of the training corpus and wL2 occurs in sentences of the L2 part of the training corpus, compared to the frequency with which wL1 and wL2 co-occur in aligned sentences of the training corpus. For this purpose, we ignore multiple occurrences of a lemma in a single sentence. As a measure of association, we use the loglikelihood-ratio statistic recommended by Dunning (1993), which is the same statistic used by Melamed to initialize his models. This statistic gives a measure of the likelihood that two samples are not generated by the same probability distribution. We use it to compare the overall frequency of wL1 in our training data to the frequency of wL1 given wL2 (i.e., the frequency with which wL1 occurs in sentences of L1 that are aligned with sentences of L2 in which wL2 occurs). Since p(wL1 ) = p(wL1 jwL2 ) only if occurrences of wL1 and wL2 are independent, a measure of the likelihood that these distributions are different is, therefore, a measure of the"
W01-1411,P95-1032,0,0.0357394,"ansformed into target language strings using the target-language lexicon, and a generation grammar written by a linguist. The principal roles played by the translation relationships derived by the methods discussed in this paper are to provide correspondences between content word lemmas in logical forms to assist in the alignment process, and to augment the lexicons used in parsing and generation, for a special case described in Section 4. 2 Previous Work The most common approach to deriving translation lexicons from empirical data (Catizone, Russell, and Warwick, 1989; Gale and Church, 1991; Fung, 1995; Kumano and Hirakawa, 1994; Wu and Xia, 1994; Melamed, 1995) is to use some variant of the following procedure:1  Pick a good measure of the degree of association between words in language L1 and words in language L2 in aligned sentences of a parallel bilingual corpus.  Rank order pairs consisting of a word from L1 and a word from L2 according to the measure of association. 1 The important work of Brown et al. (1993) is not directly comparable, since their globally-optimized generative probabilistic model of translation never has to make a firm commitment as to what can or cannot be a trans"
W01-1411,H91-1026,0,0.0599102,"se logical forms are transformed into target language strings using the target-language lexicon, and a generation grammar written by a linguist. The principal roles played by the translation relationships derived by the methods discussed in this paper are to provide correspondences between content word lemmas in logical forms to assist in the alignment process, and to augment the lexicons used in parsing and generation, for a special case described in Section 4. 2 Previous Work The most common approach to deriving translation lexicons from empirical data (Catizone, Russell, and Warwick, 1989; Gale and Church, 1991; Fung, 1995; Kumano and Hirakawa, 1994; Wu and Xia, 1994; Melamed, 1995) is to use some variant of the following procedure:1  Pick a good measure of the degree of association between words in language L1 and words in language L2 in aligned sentences of a parallel bilingual corpus.  Rank order pairs consisting of a word from L1 and a word from L2 according to the measure of association. 1 The important work of Brown et al. (1993) is not directly comparable, since their globally-optimized generative probabilistic model of translation never has to make a firm commitment as to what can or canno"
W01-1411,C94-1009,0,0.0168849,"to target language strings using the target-language lexicon, and a generation grammar written by a linguist. The principal roles played by the translation relationships derived by the methods discussed in this paper are to provide correspondences between content word lemmas in logical forms to assist in the alignment process, and to augment the lexicons used in parsing and generation, for a special case described in Section 4. 2 Previous Work The most common approach to deriving translation lexicons from empirical data (Catizone, Russell, and Warwick, 1989; Gale and Church, 1991; Fung, 1995; Kumano and Hirakawa, 1994; Wu and Xia, 1994; Melamed, 1995) is to use some variant of the following procedure:1  Pick a good measure of the degree of association between words in language L1 and words in language L2 in aligned sentences of a parallel bilingual corpus.  Rank order pairs consisting of a word from L1 and a word from L2 according to the measure of association. 1 The important work of Brown et al. (1993) is not directly comparable, since their globally-optimized generative probabilistic model of translation never has to make a firm commitment as to what can or cannot be a translation pair. They assign so"
W01-1411,W95-0115,0,0.102304,"anguage lexicon, and a generation grammar written by a linguist. The principal roles played by the translation relationships derived by the methods discussed in this paper are to provide correspondences between content word lemmas in logical forms to assist in the alignment process, and to augment the lexicons used in parsing and generation, for a special case described in Section 4. 2 Previous Work The most common approach to deriving translation lexicons from empirical data (Catizone, Russell, and Warwick, 1989; Gale and Church, 1991; Fung, 1995; Kumano and Hirakawa, 1994; Wu and Xia, 1994; Melamed, 1995) is to use some variant of the following procedure:1  Pick a good measure of the degree of association between words in language L1 and words in language L2 in aligned sentences of a parallel bilingual corpus.  Rank order pairs consisting of a word from L1 and a word from L2 according to the measure of association. 1 The important work of Brown et al. (1993) is not directly comparable, since their globally-optimized generative probabilistic model of translation never has to make a firm commitment as to what can or cannot be a translation pair. They assign some nonzero probability to every po"
W01-1411,1996.amta-1.13,0,0.294979,"Missing"
W01-1411,W97-0311,0,0.0355259,"st previous work that lexical translation relationships involve 2 Melamed does not report computation time for the version of his approach without generation of compounds, but our approach omits a number of computationally very expensive steps performed in his approach. only single words. This is manifestly not the case, as is shown by the following list of translation pairs selected from our corpus: base de donn´ees/database mot de passe/password sauvegarder/back up annuler/roll back ouvrir session/log on Some of the most sophisticated work on this aspect of problem again seems to be that of Melamed (1997). Our approach in this case is quite different from Melamed’s. It is more general in that it can propose compounds that are discontiguous in the training text, as roll back would be in a phrase such as roll the failed transaction back. Melamed does allow skipping over one or two function words, but our basic method is not limited at all by word adjacency. Also, our approach is again much simpler computationally than Melamed’s and apparently runs orders of magnitude faster.3 3 Our Basic Method Our basic method for deriving translation pairs consists of the following steps: 1. Extract word lemma"
W01-1411,W01-1414,0,0.188729,"Missing"
W01-1411,W01-1402,0,0.128172,"es, and they perform particularly well on a class of multi-word compounds of special interest to our translation effort. 1 Introduction This paper is a report on work in progress aimed at learning word translation relationships automatically from parallel bilingual corpora. Our effort is distinguished by the use of simple statistical models that are easier to implement and faster to run than previous high-accuracy approaches to this problem. Our overall approach to machine translation is a deep-transfer approach in which the transfer relationships are learned from a parallel bilingual corpus (Richardson et al., 2001). More specifically, the transfer component is trained by parsing both sides of the corpus to produce parallel logical forms, using lexicons and analysis grammars constructed by linguists. The parallel logical forms are then aligned at the level of content word stems (lemmas), and logical-form transfer patterns are learned from the aligned logicalform corpus. At run time, the source language text is parsed into logical forms employing the source language grammar and lexicon used in constructing the logical-form training corpus, and the logical-form transfer patterns are used to construct targe"
W01-1411,1994.amta-1.26,0,0.0591996,"using the target-language lexicon, and a generation grammar written by a linguist. The principal roles played by the translation relationships derived by the methods discussed in this paper are to provide correspondences between content word lemmas in logical forms to assist in the alignment process, and to augment the lexicons used in parsing and generation, for a special case described in Section 4. 2 Previous Work The most common approach to deriving translation lexicons from empirical data (Catizone, Russell, and Warwick, 1989; Gale and Church, 1991; Fung, 1995; Kumano and Hirakawa, 1994; Wu and Xia, 1994; Melamed, 1995) is to use some variant of the following procedure:1  Pick a good measure of the degree of association between words in language L1 and words in language L2 in aligned sentences of a parallel bilingual corpus.  Rank order pairs consisting of a word from L1 and a word from L2 according to the measure of association. 1 The important work of Brown et al. (1993) is not directly comparable, since their globally-optimized generative probabilistic model of translation never has to make a firm commitment as to what can or cannot be a translation pair. They assign some nonzero probabi"
W01-1411,J00-2004,0,\N,Missing
W02-2105,W01-0808,0,0.160919,"l., 1999) and the grammatical relations of noun phrases (Corston-Oliver, 2000) as well as performing lexical selection (Bangalore and Rambow, 2000b). Traditional knowledge engineering approaches to sentence realization founder on our inadequate understanding of the mapping from propositional content to surface form, a mapping that encompasses such problems as equi-NP deletion, movement operations such as extraposition and left-dislocation, ordering of modifiers within constituents, and voice alternations. Research in knowledge engineered solutions to these problems continues (see for example, Aikawa et al., 2001). However, the task of broad-coverage sentence realization would appear to be of comparable complexity to sentence analysis, and equally difficult to adapt to new domains. Sentence realization therefore appears to be an ideal candidate for statistical and machinelearned approaches. Some recently described systems have attempted to side-step the encoding decisions involved in sentence realization by proposing alternative realizations of an input semantic form and leaving it to a word-level language model to select the most likely candidate sentence for output. The Nitrogen system (Langkilde and"
W02-2105,C00-1007,0,0.535591,"planning phase in that it only executes decisions made previously.” Although the kinds and number of decisions to be made during sentence realization will depend on the nature of the prior sentence planning phase and on the linguistic complexity of the domain, myriad encoding decisions must still be made. Machine learning approaches have been successfully applied to such aspects of sentence realization as determining the appropriate form of referring expressions (Poesio et al., 1999) and the grammatical relations of noun phrases (Corston-Oliver, 2000) as well as performing lexical selection (Bangalore and Rambow, 2000b). Traditional knowledge engineering approaches to sentence realization founder on our inadequate understanding of the mapping from propositional content to surface form, a mapping that encompasses such problems as equi-NP deletion, movement operations such as extraposition and left-dislocation, ordering of modifiers within constituents, and voice alternations. Research in knowledge engineered solutions to these problems continues (see for example, Aikawa et al., 2001). However, the task of broad-coverage sentence realization would appear to be of comparable complexity to sentence analysis, a"
W02-2105,P00-1059,0,0.0872084,"planning phase in that it only executes decisions made previously.” Although the kinds and number of decisions to be made during sentence realization will depend on the nature of the prior sentence planning phase and on the linguistic complexity of the domain, myriad encoding decisions must still be made. Machine learning approaches have been successfully applied to such aspects of sentence realization as determining the appropriate form of referring expressions (Poesio et al., 1999) and the grammatical relations of noun phrases (Corston-Oliver, 2000) as well as performing lexical selection (Bangalore and Rambow, 2000b). Traditional knowledge engineering approaches to sentence realization founder on our inadequate understanding of the mapping from propositional content to surface form, a mapping that encompasses such problems as equi-NP deletion, movement operations such as extraposition and left-dislocation, ordering of modifiers within constituents, and voice alternations. Research in knowledge engineered solutions to these problems continues (see for example, Aikawa et al., 2001). However, the task of broad-coverage sentence realization would appear to be of comparable complexity to sentence analysis, a"
W02-2105,W00-1401,0,0.0219099,"Missing"
W02-2105,W00-1008,1,0.79937,"no doubt a widely-held belief, namely that “This phase is not a planning phase in that it only executes decisions made previously.” Although the kinds and number of decisions to be made during sentence realization will depend on the nature of the prior sentence planning phase and on the linguistic complexity of the domain, myriad encoding decisions must still be made. Machine learning approaches have been successfully applied to such aspects of sentence realization as determining the appropriate form of referring expressions (Poesio et al., 1999) and the grammatical relations of noun phrases (Corston-Oliver, 2000) as well as performing lexical selection (Bangalore and Rambow, 2000b). Traditional knowledge engineering approaches to sentence realization founder on our inadequate understanding of the mapping from propositional content to surface form, a mapping that encompasses such problems as equi-NP deletion, movement operations such as extraposition and left-dislocation, ordering of modifiers within constituents, and voice alternations. Research in knowledge engineered solutions to these problems continues (see for example, Aikawa et al., 2001). However, the task of broad-coverage sentence realization"
W02-2105,P01-1023,0,0.0112687,"al language sentences from logical form inputs. We describe the decomposition of the task of sentence realization into a linguistically informed series of steps, with particular attention to the linguistic issues that arise in German. We report on the evaluation of component steps and of the overall system. 1 Introduction Since the mid 1990s, there has been increasing interest in the application of statistical and machine learning techniques to various aspects of natural language generation, ranging from learning plans for high-level organization of texts and dialogues (Zukerman et al., 1998; Duboue and McKeown, 2001) or ensuring that the macro properties of generated texts such as the distribution of sentence lengths and lexical variety mirror the properties of naturally occurring texts (Oberlander and Brew, 2000) to sentence planning (Walker et al., 2001.). As generation proceeds through successively less abstract stages, nearing the final output string, it would appear that current generation systems are still likely to employ knowledge-engineered approaches. Indeed Walker et al. (2001), commenting on sentence realization, rather succinctly summarize what is no doubt a widely-held belief, namely that “T"
W02-2105,C02-1036,1,0.638852,"current paper we describe an on-going research project code-named Amalgam. Amalgam is a (predominantly) machine-learned generation module that performs sentence realization and a small degree of lexical selection. Amalgam takes as input a logical form graph. Proceeding through a series of machine-learned and knowledgeengineered steps, it transforms that graph into a fully articulated tree structure from which an output sentence can be read. Amalgam has been successfully applied to the realization of non-trivial German sentences in diverse technical domains. An extended description is given in Gamon et al. (2002a). 2 Linguistic issues in German generation Although English and German are closely related languages, they now differ typologically in dramatic ways. German makes a three-way distinction in lexical gender (masculine, feminine, neuter). Nominal elements are morphologically marked for one of four grammatical cases (nominative, accusative, dative and genitive), with adjectives and determiners agreeing in gender, number and case. Verbal position is fixed, and is sensitive to clause type; the order of other constituents is relatively free. So-called “separable prefixes” are elements that may occu"
W02-2105,W98-1426,0,0.676318,"et al., 2001). However, the task of broad-coverage sentence realization would appear to be of comparable complexity to sentence analysis, and equally difficult to adapt to new domains. Sentence realization therefore appears to be an ideal candidate for statistical and machinelearned approaches. Some recently described systems have attempted to side-step the encoding decisions involved in sentence realization by proposing alternative realizations of an input semantic form and leaving it to a word-level language model to select the most likely candidate sentence for output. The Nitrogen system (Langkilde and Knight, 1998a, 1998b), for example, uses a rather permissive knowledge-engineered component to propose candidate output sentences that are then scored using word bigrams. Statistics garnered from actual texts are thus used as a substitute for deeper knowledge. The addition of syntactic information, either to constrain the range of candidate sentences or to augment the n-gram model, has produced favorable improvements over n-gram models used alone (Bangalore and Rambow, 2000a; Ratnaparkhi, 2000). In the current paper we describe an on-going research project code-named Amalgam. Amalgam is a (predominantly)"
W02-2105,P98-1116,0,0.508107,"et al., 2001). However, the task of broad-coverage sentence realization would appear to be of comparable complexity to sentence analysis, and equally difficult to adapt to new domains. Sentence realization therefore appears to be an ideal candidate for statistical and machinelearned approaches. Some recently described systems have attempted to side-step the encoding decisions involved in sentence realization by proposing alternative realizations of an input semantic form and leaving it to a word-level language model to select the most likely candidate sentence for output. The Nitrogen system (Langkilde and Knight, 1998a, 1998b), for example, uses a rather permissive knowledge-engineered component to propose candidate output sentences that are then scored using word bigrams. Statistics garnered from actual texts are thus used as a substitute for deeper knowledge. The addition of syntactic information, either to constrain the range of candidate sentences or to augment the n-gram model, has produced favorable improvements over n-gram models used alone (Bangalore and Rambow, 2000a; Ratnaparkhi, 2000). In the current paper we describe an on-going research project code-named Amalgam. Amalgam is a (predominantly)"
W02-2105,A00-2026,0,0.0142837,"word-level language model to select the most likely candidate sentence for output. The Nitrogen system (Langkilde and Knight, 1998a, 1998b), for example, uses a rather permissive knowledge-engineered component to propose candidate output sentences that are then scored using word bigrams. Statistics garnered from actual texts are thus used as a substitute for deeper knowledge. The addition of syntactic information, either to constrain the range of candidate sentences or to augment the n-gram model, has produced favorable improvements over n-gram models used alone (Bangalore and Rambow, 2000a; Ratnaparkhi, 2000). In the current paper we describe an on-going research project code-named Amalgam. Amalgam is a (predominantly) machine-learned generation module that performs sentence realization and a small degree of lexical selection. Amalgam takes as input a logical form graph. Proceeding through a series of machine-learned and knowledgeengineered steps, it transforms that graph into a fully articulated tree structure from which an output sentence can be read. Amalgam has been successfully applied to the realization of non-trivial German sentences in diverse technical domains. An extended description is"
W02-2105,N01-1003,0,0.0117568,"he evaluation of component steps and of the overall system. 1 Introduction Since the mid 1990s, there has been increasing interest in the application of statistical and machine learning techniques to various aspects of natural language generation, ranging from learning plans for high-level organization of texts and dialogues (Zukerman et al., 1998; Duboue and McKeown, 2001) or ensuring that the macro properties of generated texts such as the distribution of sentence lengths and lexical variety mirror the properties of naturally occurring texts (Oberlander and Brew, 2000) to sentence planning (Walker et al., 2001.). As generation proceeds through successively less abstract stages, nearing the final output string, it would appear that current generation systems are still likely to employ knowledge-engineered approaches. Indeed Walker et al. (2001), commenting on sentence realization, rather succinctly summarize what is no doubt a widely-held belief, namely that “This phase is not a planning phase in that it only executes decisions made previously.” Although the kinds and number of decisions to be made during sentence realization will depend on the nature of the prior sentence planning phase and on the"
W02-2105,C98-1112,0,\N,Missing
W02-2106,P93-1008,1,0.758749,"unction. This is arguably the most important subcase of what has come to be called “the logical-form equivalence problem” (Appelt, 1987; Shieber, 1988, 1993). Allowing permutation of conjunction makes the generation problem inherently worst-case exponential, but our algorithm seems to be about as efficient as one could expect under the circumstances. 2 Grammatical Framework We assume a grammar formalism having the expressive power of definte clause grammar (Pereira and Shieber, 1987) , or equivalently, the syntactically sugared forms used in the Core Language Engine (Alshawi, 1992) or Gemini (Dowding et al., 1993). The following is an example of the sort of grammar rule we assume: s:[stype=decl] --&gt; np:[prsn=P,num=N] vp:[vtype=tensed,prsn=P,num=N] The notation is that of augmented phrase structure rules, where nonterminals are complex category expressions having the form of a major category symbol followed by a list of feature constraints. Atomic values beginning with uppercase letters are variables; those beginning with lower case letters are constants. Unification constraints are indicated by shared variables. For instance, the sample rule above would be interpreted to mean that a declarative sentenc"
W02-2106,P81-1022,0,0.310457,"Missing"
W02-2106,C88-2128,0,0.727766,"semantic monotonicity property. Under fairly modest constraints on the grammar, the algorithm is shown to have polynomial time complexity for strict generation; i.e., not addressing LF equivalence. The exact polynomial depends on details of the grammar, and we discuss the likely order of the polynomial for English. We extend the algorithm to be complete for the generation of sentences whose LF is equivalent to a goal LF under permutation of n-ary logical conjunction. This is arguably the most important subcase of what has come to be called “the logical-form equivalence problem” (Appelt, 1987; Shieber, 1988, 1993). Allowing permutation of conjunction makes the generation problem inherently worst-case exponential, but our algorithm seems to be about as efficient as one could expect under the circumstances. 2 Grammatical Framework We assume a grammar formalism having the expressive power of definte clause grammar (Pereira and Shieber, 1987) , or equivalently, the syntactically sugared forms used in the Core Language Engine (Alshawi, 1992) or Gemini (Dowding et al., 1993). The following is an example of the sort of grammar rule we assume: s:[stype=decl] --&gt; np:[prsn=P,num=N] vp:[vtype=tensed,prsn=P"
W02-2106,J90-1004,1,0.877149,"Missing"
W02-2106,T87-1042,0,0.349642,"eber’s (1988) semantic monotonicity property. Under fairly modest constraints on the grammar, the algorithm is shown to have polynomial time complexity for strict generation; i.e., not addressing LF equivalence. The exact polynomial depends on details of the grammar, and we discuss the likely order of the polynomial for English. We extend the algorithm to be complete for the generation of sentences whose LF is equivalent to a goal LF under permutation of n-ary logical conjunction. This is arguably the most important subcase of what has come to be called “the logical-form equivalence problem” (Appelt, 1987; Shieber, 1988, 1993). Allowing permutation of conjunction makes the generation problem inherently worst-case exponential, but our algorithm seems to be about as efficient as one could expect under the circumstances. 2 Grammatical Framework We assume a grammar formalism having the expressive power of definte clause grammar (Pereira and Shieber, 1987) , or equivalently, the syntactically sugared forms used in the Core Language Engine (Alshawi, 1992) or Gemini (Dowding et al., 1993). The following is an example of the sort of grammar rule we assume: s:[stype=decl] --&gt; np:[prsn=P,num=N] vp:[vtyp"
W02-2106,C92-2092,0,\N,Missing
W02-2106,H93-1008,1,\N,Missing
W02-2106,J93-1008,0,\N,Missing
W02-2106,P96-1027,0,\N,Missing
W04-3243,J93-1003,0,0.44325,"singleton pairs have the lowest possible expected joint count, this is probably the effect of known problems with estimating p-values from likelihood ratios when expected counts are very small. 7 Conclusions When we use Fisher’s exact test to estimate pvalues, our new method for estimating noise for collections of rare events seems to give results that are quite consistent with our previous annecdotal experience in using LLR scores as a measure of word association. Using likelihood ratios to estimate p-values introduces a substantial amount of error, but not the orders-of-magnitude error that Dunning (1993) demonstrated for estimates that rely on the assumption of a normal distribution. However, since we have also shown that Fisher’s exact test can be applied to this type of problem without a major computational penalty, there seems to be no reason to compromise in this regard. 8 Acknowledgements Thanks to Ken Church, Joshua Goodman, David Heckerman, Mark Johnson, Chris Meek, Ted Pedersen, and Chris Quirk for many valuable discussions of the issues raised in this paper. Thanks especially to Joshua Goodman for pointing out the existence of fast numerical approximations for the factorial function,"
W04-3243,W02-0909,0,0.0615283,"Missing"
W04-3243,W03-0301,0,0.024294,"Missing"
W05-0801,J93-2003,0,0.0350178,"of the standard models using algorithms that are much faster, and in some ways simpler, based on basic word-association statistics. 2 Data and Methodology for these Experiments 1 Motivation Bilingual word alignment is the first step of most current approaches to statistical machine translation. Although the best performing systems are “phrasebased” (see, for instance, Och and Ney (2004) or Koehn et al. (2003)), possible phrase translations must first be extracted from word-aligned bilingual text segments. The standard approach to word alignment makes use of five translation models defined by Brown et al. (1993), sometimes augmented by an HMM-based model or Och and Ney’s “Model 6” (Och and Ney, 2003). The best of these models can produce high accuracy alignments, at least when trained on a large parallel corpus of fairly direct translations in closely related languages. There are a number of ways in which these standard models are less than ideal, however. The higher-accuracy models are mathematically complex, and also difficult to train, as they do not factor The experiments reported here were carried out using data from the workshop on building and using parallel texts held at HLT-NAACL 2003 (Mihal"
W05-0801,P03-1012,0,0.0261545,"Missing"
W05-0801,J93-1003,0,0.103621,"ous ways of choosing a word token alignment for a given word type alignment, and all our final evaluations are conducted on the basis of the alignment of individual word tokens. and the hand alignments of the words in the trial and test data were created by Franz Och and Hermann Ney (Och and Ney, 2003). The manual word alignments for the English-Romanian test data were created by Rada Mihalcea and Ted Pedersen. 2 3 The Log-Likelihood-Ratio Association Measure We base all our association-based word-alignment methods on the log-likelihood-ratio (LLR) statistic introduced to the NLP community by Dunning (1993). We chose this statistic because it has previously been found to be effective for automatically constructing translation lexicons (e.g., Melamed, 2000). We compute LLR scores using the following formula presented by Moore (2004): LLR(f, e) =   f ?∈{f,¬f } e?∈{e,¬e} C(f ?, e?) log p(f ?|e?) p(f ?) In this formula f and e mean that the words whose degree of association is being measured occur in the respective target and source sentences of an aligned sentence pair, ¬f and ¬e mean that the corresponding words do not occur in the respective sentences, f ? and e? are variables ranging over thes"
W05-0801,H91-1026,0,0.788029,"Missing"
W05-0801,N03-1017,0,0.0150073,"tandard wordalignment methods involve the use of probabilistic generative models that are complex to implement and slow to train. In this paper we show that it is possible to approach the alignment accuracy of the standard models using algorithms that are much faster, and in some ways simpler, based on basic word-association statistics. 2 Data and Methodology for these Experiments 1 Motivation Bilingual word alignment is the first step of most current approaches to statistical machine translation. Although the best performing systems are “phrasebased” (see, for instance, Och and Ney (2004) or Koehn et al. (2003)), possible phrase translations must first be extracted from word-aligned bilingual text segments. The standard approach to word alignment makes use of five translation models defined by Brown et al. (1993), sometimes augmented by an HMM-based model or Och and Ney’s “Model 6” (Och and Ney, 2003). The best of these models can produce high accuracy alignments, at least when trained on a large parallel corpus of fairly direct translations in closely related languages. There are a number of ways in which these standard models are less than ideal, however. The higher-accuracy models are mathematica"
W05-0801,W03-0301,0,0.0222348,"1993), sometimes augmented by an HMM-based model or Och and Ney’s “Model 6” (Och and Ney, 2003). The best of these models can produce high accuracy alignments, at least when trained on a large parallel corpus of fairly direct translations in closely related languages. There are a number of ways in which these standard models are less than ideal, however. The higher-accuracy models are mathematically complex, and also difficult to train, as they do not factor The experiments reported here were carried out using data from the workshop on building and using parallel texts held at HLT-NAACL 2003 (Mihalcea and Pedersen, 2003). For the majority of our experiments, we used a subset of the Canadian Hansards bilingual corpus supplied for the workshop, comprising 500,000 English-French sentences pairs, including 37 sentence pairs designated as “trial” data, and 447 sentence pairs designated as test data. The trial and test data have been manually aligned at the word level, noting particular pairs of words either as “sure” or “possible” alignments. As an additional test, we evaluated our best alignment method using the workshop corpus of approximately 49,000 English-Romanian sentences pairs from diverse sources, includi"
W05-0801,W04-3243,1,0.770618,"test data were created by Franz Och and Hermann Ney (Och and Ney, 2003). The manual word alignments for the English-Romanian test data were created by Rada Mihalcea and Ted Pedersen. 2 3 The Log-Likelihood-Ratio Association Measure We base all our association-based word-alignment methods on the log-likelihood-ratio (LLR) statistic introduced to the NLP community by Dunning (1993). We chose this statistic because it has previously been found to be effective for automatically constructing translation lexicons (e.g., Melamed, 2000). We compute LLR scores using the following formula presented by Moore (2004): LLR(f, e) =   f ?∈{f,¬f } e?∈{e,¬e} C(f ?, e?) log p(f ?|e?) p(f ?) In this formula f and e mean that the words whose degree of association is being measured occur in the respective target and source sentences of an aligned sentence pair, ¬f and ¬e mean that the corresponding words do not occur in the respective sentences, f ? and e? are variables ranging over these values, and C(f ?, e?) is the observed joint count for the values of f ? and e?. The probabilities in the formula refer to maximum likelihood estimates. Since the LLR score for a pair of words is high if the words have either a"
W05-0801,J03-1002,0,0.24623,"sed on basic word-association statistics. 2 Data and Methodology for these Experiments 1 Motivation Bilingual word alignment is the first step of most current approaches to statistical machine translation. Although the best performing systems are “phrasebased” (see, for instance, Och and Ney (2004) or Koehn et al. (2003)), possible phrase translations must first be extracted from word-aligned bilingual text segments. The standard approach to word alignment makes use of five translation models defined by Brown et al. (1993), sometimes augmented by an HMM-based model or Och and Ney’s “Model 6” (Och and Ney, 2003). The best of these models can produce high accuracy alignments, at least when trained on a large parallel corpus of fairly direct translations in closely related languages. There are a number of ways in which these standard models are less than ideal, however. The higher-accuracy models are mathematically complex, and also difficult to train, as they do not factor The experiments reported here were carried out using data from the workshop on building and using parallel texts held at HLT-NAACL 2003 (Mihalcea and Pedersen, 2003). For the majority of our experiments, we used a subset of the Cana"
W05-0801,W02-1001,0,\N,Missing
W05-0801,J00-2004,0,\N,Missing
W05-0801,P06-1097,0,\N,Missing
W07-0715,W06-3123,0,0.225738,"airs are extracted from word-aligned bilingual sentence pairs, and their translation probabilities estimated heuristically from surface statistics of the extracted phrase pairs. We will refer to this approach as “the standard model”. There have been several attempts to estimate phrase translation probabilities directly, using generative models trained iteratively on a parallel corpus using the Expectation Maximization (EM) algorithm. The first of these models, that of Marcu and Wong (2002), was found by Koehn, et al. (2003), to produce translations not quite as good as their method. Recently, Birch et al. (2006) tried the Marcu and Wong model constrained by a word alignment and also found that Koehn, et al.’s model worked better, with the advantage of the standard model increasing as more features were added to the overall translation model. DeNero et al. (2006) tried a different generative phrase translation model analogous to IBM word-translation Model 3 (Brown et al., 1993), and again found that the standard model outperformed their generative model. DeNero et al. (2006) attribute the inferiority of their model and the Marcu and Wong model to a hidden segmentation variable, which enables the EM al"
W07-0715,J93-2003,0,0.0128557,"Missing"
W07-0715,P03-1012,0,0.0266405,"s not assume a segmentation of the training data into non-overlapping phrase pairs. We refer to our model as “iteratively-trained” rather than “generative” because we have not proved any of the mathematical properties usually associated with generative models; e.g., that the training procedure maximizes the likelihood of the training data. We will motivate the model, however, with a generative story as to how phrase alignments are produced, given a pair of source and target sentences. Our model extends to phrase alignment the concept of a sentence pair generating a word alignment developed by Cherry and Lin (2003). Our model is defined in terms of two stochastic processes, selection and alignment, as follows: 1. For each word-aligned sentence pair, we identify all the possible phrase pair instances according to the criteria used by Koehn et al. 2. Each source phrase instance that is included in any of the possible phrase pair instances independently selects one of the target phrase instances that it forms a possible phrase pair instance with. We have seen how to derive phrase translation probabilities from the selection probabilities, but where do the latter come from? We answer this question by adding"
W07-0715,W06-3105,0,0.16634,"ral attempts to estimate phrase translation probabilities directly, using generative models trained iteratively on a parallel corpus using the Expectation Maximization (EM) algorithm. The first of these models, that of Marcu and Wong (2002), was found by Koehn, et al. (2003), to produce translations not quite as good as their method. Recently, Birch et al. (2006) tried the Marcu and Wong model constrained by a word alignment and also found that Koehn, et al.’s model worked better, with the advantage of the standard model increasing as more features were added to the overall translation model. DeNero et al. (2006) tried a different generative phrase translation model analogous to IBM word-translation Model 3 (Brown et al., 1993), and again found that the standard model outperformed their generative model. DeNero et al. (2006) attribute the inferiority of their model and the Marcu and Wong model to a hidden segmentation variable, which enables the EM algorithm to maximize the probability of the training data without really improving the quality of the model. We propose an iteratively-trained phrase translation model that does not require different segmentations to compete against one another, and we sho"
W07-0715,N03-1017,0,0.0586483,"alignments as described by Koehn, et al. (2003). We propose a new iteratively-trained phrase translation model that produces translations of quality equal to or better than those produced by Koehn, et al.’s model. Moreover, with the new model, translation quality degrades much more slowly as pruning is tightend to reduce translation time. 1 Introduction Estimates of conditional phrase translation probabilities provide a major source of translation knowledge in phrase-based statistical machine translation (SMT) systems. The most widely used method for estimating these probabilities is that of Koehn, et al. (2003), in which phrase pairs are extracted from word-aligned bilingual sentence pairs, and their translation probabilities estimated heuristically from surface statistics of the extracted phrase pairs. We will refer to this approach as “the standard model”. There have been several attempts to estimate phrase translation probabilities directly, using generative models trained iteratively on a parallel corpus using the Expectation Maximization (EM) algorithm. The first of these models, that of Marcu and Wong (2002), was found by Koehn, et al. (2003), to produce translations not quite as good as their"
W07-0715,N06-1014,0,0.0354758,"is training procedure as iteratively trying to find a set of phrase translation probabilities that satisfies all the constraints of the model, although we have not proved that this training procedure always converges. We also have not proved that the procedure maximizes the likelihood of anything, although we find empirically that each iteration decreases the conditional entropy of the phrase translation model. In any case, the training procedure seems to work well in practice. It is also very similar to the joint training procedure for HMM wordalignment models in both directions described by Liang et al. (2006), which was the original inspiration for our training procedure. 4 Experimental Set-Up and Data We evaluated our phrase translation model compared to the standard model of Koehn et al. in the context of a fairly typical end-to-end phrase-based SMT system. The overall translation model score consists of a weighted sum of the following eight aggregated feature values for each translation hypothesis: • the sum of the log probabilities of each source phrase in the hypothesis given the corresponding target phrase, computed either by our model or the standard model, • the sum of the log probabilitie"
W07-0715,W02-1018,0,0.0977314,"SMT) systems. The most widely used method for estimating these probabilities is that of Koehn, et al. (2003), in which phrase pairs are extracted from word-aligned bilingual sentence pairs, and their translation probabilities estimated heuristically from surface statistics of the extracted phrase pairs. We will refer to this approach as “the standard model”. There have been several attempts to estimate phrase translation probabilities directly, using generative models trained iteratively on a parallel corpus using the Expectation Maximization (EM) algorithm. The first of these models, that of Marcu and Wong (2002), was found by Koehn, et al. (2003), to produce translations not quite as good as their method. Recently, Birch et al. (2006) tried the Marcu and Wong model constrained by a word alignment and also found that Koehn, et al.’s model worked better, with the advantage of the standard model increasing as more features were added to the overall translation model. DeNero et al. (2006) tried a different generative phrase translation model analogous to IBM word-translation Model 3 (Brown et al., 1993), and again found that the standard model outperformed their generative model. DeNero et al. (2006) att"
W07-0715,W03-0301,0,0.016243,"rce/target phrase pairs composing the hypothesis, (Brown et al., 1993). The feature weights for the overall translation models were trained using Och’s (2003) minimum-error-rate training procedure. The weights were optimized separately for our model and for the standard phrase translation model. Our decoder is a reimplementation in Perl of the algorithm used by the Pharaoh decoder as described by Koehn (2003).2 The data we used comes from an English-French bilingual corpus of Canadian Hansards parliamentary proceedings supplied for the bilingual word alignment workshop held at HLT-NAACL 2003 (Mihalcea and Pedersen, 2003). Automatic sentence alignment of this data was provided by Ulrich Germann. We used 500,000 sentences pairs from this corpus for training both the phrase translation models and IBM Model 1 lexical scores. These 500,000 sentence pairs were word-aligned using a state-ofthe-art word-alignment method (Moore et al., 2006). A separate set of 500 sentence pairs was used to train the translation model weights, and two additional held-out sets of 2000 sentence pairs each were used as test data. The two phrase translation models were trained using the same set of possible phrase pairs extracted from the"
W07-0715,P06-1065,1,0.830362,"ion in Perl of the algorithm used by the Pharaoh decoder as described by Koehn (2003).2 The data we used comes from an English-French bilingual corpus of Canadian Hansards parliamentary proceedings supplied for the bilingual word alignment workshop held at HLT-NAACL 2003 (Mihalcea and Pedersen, 2003). Automatic sentence alignment of this data was provided by Ulrich Germann. We used 500,000 sentences pairs from this corpus for training both the phrase translation models and IBM Model 1 lexical scores. These 500,000 sentence pairs were word-aligned using a state-ofthe-art word-alignment method (Moore et al., 2006). A separate set of 500 sentence pairs was used to train the translation model weights, and two additional held-out sets of 2000 sentence pairs each were used as test data. The two phrase translation models were trained using the same set of possible phrase pairs extracted from the word-aligned 500,000 sentence pair corpus, finding all possible phrase pairs permitted by the criteria followed by Koehn et al., up to a phrase length of seven words. This produced approximately 69 million distinct phrase pair types. No pruning of the set of possible phrase pairs was done during or before training t"
W07-0715,W99-0604,0,0.0675212,"0 x0 C(x , y) p(x|y) = P This method is used to estimate the conditional probabilities of both target phrases give source phrases and source phrases given target phrases. In contrast to the standard model, DeNero, et al. (2006) estimate phrase translation probabilities according to the following generative model: 1. Begin with a source sentence a. 2. Stochastically segment a into some number of phrases. 3. For each selected phrase in a, stochastically choose a phrase position in the target sentence b that is being generated. 1 This method of phrase pair extraction was originally described by Och et al. (1999). 113 4. For each selected phrase in a and the corresponding phrase position in b, stochastically choose a target phrase. 5. Read off the target sentence b from the sequence of target phrases. DeNero et al.’s analysis of why their model performs relatively poorly hinges on the fact that the segmentation probabilities used in step 2 are, in fact, not trained, but simply assumed to be uniform. Given complete freedom to select whatever segmentation maximizes the likelihood of any given sentence pair, EM tends to favor segmentations that yield source phrases with as few occurrences as possible, si"
W07-0715,P03-1021,0,0.100988,"Missing"
W07-0715,P02-1040,0,0.0741709,"he standard model measured 4.30 bits per phrase. DeNero et al. obtained corresponding measurements of 1.55 bits per phrase and 3.76 bits per phrase, for their model and the standard model, using a different data set and a slightly different estimation method. 6 Translation Experiments We wanted to look at the trade-off between decoding time and translation quality for our new phrase translation model compared to the standard model. Since this trade-off is also affected by the settings of various pruning parameters, we compared decoding time and translation quality, as measured by B LEU score (Papineni et al, 2002), for the two models on our first test set over a broad range of settings for the decoder pruning parameters. The Pharaoh decoding algorithm, has five pruning parameters that affect decoding time: • Distortion limit • Translation table limit The distortion limit is the maximum distance allowed between two source phrases that produce adjacent target phrases in the decoder output. The distortion limit can be viewed as a model parameter, as well as a pruning paramter, because setting it to an optimum value usually improves translation quality over leaving it unrestricted. We carried out experimen"
W07-0715,2006.amta-papers.2,0,\N,Missing
W12-3125,N12-1047,1,0.88858,"Missing"
W12-3125,D08-1024,0,0.0773411,"Missing"
W12-3125,C10-2033,0,0.317379,"lin Cherry National Research Council colin.cherry@nrc-cnrc.gc.ca Robert C. Moore Google bobmoore@google.com Abstract decoding can be constrained by distortion limits or by mimicking the restrictions of inversion transduction grammars (Wu, 1997; Zens et al., 2004). The addition of a deterministic permutation parser can provide valuable hierarchical information to a phrase-based statistical machine translation (PBSMT) system. Permutation parsers have been used to implement hierarchical re-ordering models (Galley and Manning, 2008) and to enforce inversion transduction grammar (ITG) constraints (Feng et al., 2010). We present a number of theoretical results regarding the use of permutation parsers in PBSMT. In particular, we show that an existing ITG constraint (Zens et al., 2004) does not prevent all non-ITG permutations, and we demonstrate that the hierarchical reordering model can produce analyses during decoding that are inconsistent with analyses made during training. Experimentally, we verify the utility of hierarchical re-ordering, and compare several theoretically-motivated variants in terms of both translation quality and the syntactic complexity of their output. 1 Chris Quirk Microsoft Resear"
W12-3125,D08-1089,0,0.866851,"of hierarchical re-ordering, and compare several theoretically-motivated variants in terms of both translation quality and the syntactic complexity of their output. 1 Chris Quirk Microsoft Research chrisq@microsoft.com Recently, we have begun to see deterministic permutation parsers incorporated into phrase-based decoders. These efficient parsers analyze the sequence of phrases used to produce the target, and assemble them into a hierarchical translation history that can be used to inform re-ordering decisions. Thus far, they have been used to enable a hierarchical re-ordering model, or HRM (Galley and Manning, 2008), as well as an ITG constraint (Feng et al., 2010). We discuss each of these techniques in turn, and then explore the implications of ITG violations on hierarchical re-ordering. Introduction Despite the emergence of a number of syntax-based techniques, phrase-based statistical machine translation remains a competitive and very efficient translation paradigm (Galley and Manning, 2010). However, it lacks the syntactically-informed movement models and constraints that are provided implicitly by working with synchronous grammars. Therefore, re-ordering must be modeled and constrained explicitly. M"
W12-3125,N10-1140,0,0.04572,"to produce the target, and assemble them into a hierarchical translation history that can be used to inform re-ordering decisions. Thus far, they have been used to enable a hierarchical re-ordering model, or HRM (Galley and Manning, 2008), as well as an ITG constraint (Feng et al., 2010). We discuss each of these techniques in turn, and then explore the implications of ITG violations on hierarchical re-ordering. Introduction Despite the emergence of a number of syntax-based techniques, phrase-based statistical machine translation remains a competitive and very efficient translation paradigm (Galley and Manning, 2010). However, it lacks the syntactically-informed movement models and constraints that are provided implicitly by working with synchronous grammars. Therefore, re-ordering must be modeled and constrained explicitly. Movement can be modeled with a distortion penalty or lexicalized re-ordering probabilities (Koehn et al., 2003; Koehn et al., 2007), while We present one experimental and four theoretical contributions. Examining the HRM alone, we present an improved algorithm for extracting HRM statistics, reducing the complexity of Galley and Manning’s solution from O(n4 ) to O(n2 ). Examining ITG c"
W12-3125,J99-4005,0,0.485608,"Missing"
W12-3125,N03-1017,0,0.0695883,"turn, and then explore the implications of ITG violations on hierarchical re-ordering. Introduction Despite the emergence of a number of syntax-based techniques, phrase-based statistical machine translation remains a competitive and very efficient translation paradigm (Galley and Manning, 2010). However, it lacks the syntactically-informed movement models and constraints that are provided implicitly by working with synchronous grammars. Therefore, re-ordering must be modeled and constrained explicitly. Movement can be modeled with a distortion penalty or lexicalized re-ordering probabilities (Koehn et al., 2003; Koehn et al., 2007), while We present one experimental and four theoretical contributions. Examining the HRM alone, we present an improved algorithm for extracting HRM statistics, reducing the complexity of Galley and Manning’s solution from O(n4 ) to O(n2 ). Examining ITG constraints alone, we demonstrate that the three-stack constraint of Feng et al. can be reduced to one augmented stack, and we show that another phrase-based ITG constraint (Zens et al., 2004) actually allows some ITG violations to pass. Finally, we show that in the presence of ITG violations, the original HRM can fail to"
W12-3125,P07-2045,0,0.0669013,"re the implications of ITG violations on hierarchical re-ordering. Introduction Despite the emergence of a number of syntax-based techniques, phrase-based statistical machine translation remains a competitive and very efficient translation paradigm (Galley and Manning, 2010). However, it lacks the syntactically-informed movement models and constraints that are provided implicitly by working with synchronous grammars. Therefore, re-ordering must be modeled and constrained explicitly. Movement can be modeled with a distortion penalty or lexicalized re-ordering probabilities (Koehn et al., 2003; Koehn et al., 2007), while We present one experimental and four theoretical contributions. Examining the HRM alone, we present an improved algorithm for extracting HRM statistics, reducing the complexity of Galley and Manning’s solution from O(n4 ) to O(n2 ). Examining ITG constraints alone, we demonstrate that the three-stack constraint of Feng et al. can be reduced to one augmented stack, and we show that another phrase-based ITG constraint (Zens et al., 2004) actually allows some ITG violations to pass. Finally, we show that in the presence of ITG violations, the original HRM can fail to produce orientations"
W12-3125,2007.mtsummit-papers.43,1,0.945694,"Missing"
W12-3125,2009.mtsummit-papers.10,0,0.327246,"Missing"
W12-3125,J04-4002,0,0.132136,"training data. Galley and Manning (2008) propose an algorithm that begins by run1 This would require a second, right-to-left decoding pass. Galley and Manning (2008) present an under-specified approximation that is consistent with what we present here. 2 202 2 Prev Cov / Approx Top 4 5 6 7 Figure 2: Illustration of the coverage-vector stack approximation, as applied to right-to-left HRM orientation. èS Source Op S S R S S R R S R çM Phrase èM çS Target Figure 3: Relevant corners in HRM extraction. → indicates left-to-right orientation, and ← right-to-left. ning standard phrase extraction (Och and Ney, 2004) without a phrase-length limit, noting the corners of each phrase found. Next, the left-to-right and rightto-left orientation for each phrase of interest (those within the phrase-length limit) can be determined by checking to see if any corners noted in the previous step are adjacent, as shown in Figure 3. 2.1 Efficient Extraction of HRM statistics The time complexity of phrase extraction is bounded by the number of phrases to be extracted, which is determined by the sparsity of the input word alignment. Without a limit on phrase length, a sentence pair with n words in each language can have a"
W12-3125,P02-1040,0,0.0838935,"Missing"
W12-3125,N04-4026,0,0.4744,"ation, including an approximate HRM that requires no permutation parser, and compare them experimentally. The variants perform similarly to the original in terms of BLEU score, but differently in terms of how they permute the source sentence. 200 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 200–209, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics 2 Hierarchical Re-ordering Hierarchical re-ordering models (HRMs) for phrasebased SMT are an extension of lexicalized reordering models (LRMs), so we begin by briefly reviewing the LRM (Tillmann, 2004; Koehn et al., 2007). The goal of an LRM is to characterize how a phrase-pair tends to be placed with respect to the block that immediately precedes it. Both the LRM and the HRM track orientations traveling through the target from left-to-right as well as right-to-left. For the sake of brevity and clarity, we discuss only the left-to-right direction except when stated otherwise. Re-ordering is typically categorized into three orientations, which are determined by examining two sequential blocks [si−1 , ti−1 ] and [si , ti ]: • Monotone Adjacent (M): ti−1 = si • Swap Adjacent (S): ti = si−1 •"
W12-3125,J97-3002,0,0.703132,"Missing"
W12-3125,P03-1019,0,0.285671,"Missing"
W12-3125,C04-1030,0,0.649146,"y mimicking the restrictions of inversion transduction grammars (Wu, 1997; Zens et al., 2004). The addition of a deterministic permutation parser can provide valuable hierarchical information to a phrase-based statistical machine translation (PBSMT) system. Permutation parsers have been used to implement hierarchical re-ordering models (Galley and Manning, 2008) and to enforce inversion transduction grammar (ITG) constraints (Feng et al., 2010). We present a number of theoretical results regarding the use of permutation parsers in PBSMT. In particular, we show that an existing ITG constraint (Zens et al., 2004) does not prevent all non-ITG permutations, and we demonstrate that the hierarchical reordering model can produce analyses during decoding that are inconsistent with analyses made during training. Experimentally, we verify the utility of hierarchical re-ordering, and compare several theoretically-motivated variants in terms of both translation quality and the syntactic complexity of their output. 1 Chris Quirk Microsoft Research chrisq@microsoft.com Recently, we have begun to see deterministic permutation parsers incorporated into phrase-based decoders. These efficient parsers analyze the sequ"
W12-3125,W07-0404,0,0.154947,"Missing"
W12-3125,N06-1033,0,0.0624761,"Missing"
W12-3125,D11-1003,0,\N,Missing
