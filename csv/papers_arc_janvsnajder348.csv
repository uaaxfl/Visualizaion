W19-5118,Evaluating Automatic Term Extraction Methods on Individual Documents,2019,0,0,3,0,23914,antonio vsajatovic,Proceedings of the Joint Workshop on Multiword Expressions and WordNet (MWE-WN 2019),0,"Automatic Term Extraction (ATE) extracts terminology from domain-specific corpora. ATE is used in many NLP tasks, including Computer Assisted Translation, where it is typically applied to individual documents rather than the entire corpus. While corpus-level ATE has been extensively evaluated, it is not obvious how the results transfer to document-level ATE. To fill this gap, we evaluate 16 state-of-the-art ATE methods on full-length documents from three different domains, on both corpus and document levels. Unlike existing studies, our evaluation is more realistic as we take into account all gold terms. We show that no single method is best in corpus-level ATE, but C-Value and KeyConceptRelatendess surpass others in document-level ATE."
W19-4405,Analysing Rhetorical Structure as a Key Feature of Summary Coherence,2019,0,0,1,1,23915,jan vsnajder,Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications,0,"We present a model for automatic scoring of coherence based on comparing the rhetorical structure (RS) of college student summaries in L2 (English) against expert summaries. Coherence is conceptualised as a construct consisting of the rhetorical relation and its arguments. Comparison with expert-assigned scores shows that RS scores correlate with both cohesion and coherence. Furthermore, RS scores improve the accuracy of a regression model for cohesion score prediction."
W19-3514,Preemptive Toxic Language Detection in {W}ikipedia Comments Using Thread-Level Context,2019,0,0,2,1,1116,mladen karan,Proceedings of the Third Workshop on Abusive Language Online,0,"We address the task of automatically detecting toxic content in user generated texts. We fo cus on exploring the potential for preemptive moderation, i.e., predicting whether a particular conversation thread will, in the future, incite a toxic comment. Moreover, we perform preliminary investigation of whether a model that jointly considers all comments in a conversation thread outperforms a model that considers only individual comments. Using an existing dataset of conversations among Wikipedia contributors as a starting point, we compile a new large-scale dataset for this task consisting of labeled comments and comments from their conversation threads."
S19-2172,{T}ake{L}ab at {S}em{E}val-2019 Task 4: Hyperpartisan News Detection,2019,0,0,6,0,25156,niko palic,Proceedings of the 13th International Workshop on Semantic Evaluation,0,"In this paper, we demonstrate the system built to solve the SemEval-2019 task 4: Hyperpartisan News Detection (Kiesel et al., 2019), the task of automatically determining whether an article is heavily biased towards one side of the political spectrum. Our system receives an article in its raw, textual form, analyzes it, and predicts with moderate accuracy whether the article is hyperpartisan. The learning model used was primarily trained on a manually prelabeled dataset containing news articles. The system relies on the previously constructed SVM model, available in the Python Scikit-Learn library. We ranked 6th in the competition of 42 teams with an accuracy of 79.1{\%} (the winning team had 82.2{\%})."
W18-6211,Not Just Depressed: Bipolar Disorder Prediction on {R}eddit,2018,0,5,3,1,22657,ivan sekulic,"Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis",0,"Bipolar disorder, an illness characterized by manic and depressive episodes, affects more than 60 million people worldwide. We present a preliminary study on bipolar disorder prediction from user-generated text on Reddit, which relies on users{'} self-reported labels. Our benchmark classifiers for bipolar disorder prediction outperform the baselines and reach accuracy and F1-scores of above 86{\%}. Feature analysis shows interesting differences in language use between users with bipolar disorders and the control group, including differences in the use of emotion-expressive words."
W18-5427,Iterative Recursive Attention Model for Interpretable Sequence Classification,2018,35,0,2,1,15648,martin tutek,Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP},0,"Natural language processing has greatly benefited from the introduction of the attention mechanism. However, standard attention models are of limited interpretability for tasks that involve a series of inference steps. We describe an iterative recursive attention model, which constructs incremental representations of input data through reusing results of previously computed queries. We train our model on sentiment classification datasets and demonstrate its capacity to identify and combine different aspects of the input in an easily interpretable manner, while obtaining performance close to the state of the art."
W18-5117,Cross-Domain Detection of Abusive Language Online,2018,0,3,2,1,1116,mladen karan,Proceedings of the 2nd Workshop on Abusive Language Online ({ALW}2),0,"We investigate to what extent the models trained to detect general abusive language generalize between different datasets labeled with different abusive language types. To this end, we compare the cross-domain performance of simple classification models on nine different datasets, finding that the models fail to generalize to out-domain datasets and that having at least some in-domain data is important. We also show that using the frustratingly simple domain adaptation (Daume III, 2007) in most cases improves the results over in-domain training, especially when used to augment a smaller dataset with a larger one."
W18-4422,Combining Shallow and Deep Learning for Aggressive Text Detection,2018,0,5,3,0,28179,viktor golem,"Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying ({TRAC}-2018)",0,"We describe the participation of team TakeLab in the aggression detection shared task at the TRAC1 workshop for English. Aggression manifests in a variety of ways. Unlike some forms of aggression that are impossible to prevent in day-to-day life, aggressive speech abounding on social networks could in principle be prevented or at least reduced by simply disabling users that post aggressively worded messages. The first step in achieving this is to detect such messages. The task, however, is far from being trivial, as what is considered as aggressive speech can be quite subjective, and the task is further complicated by the noisy nature of user-generated text on social networks. Our system learns to distinguish between open aggression, covert aggression, and non-aggression in social media texts. We tried different machine learning approaches, including traditional (shallow) machine learning models, deep learning models, and a combination of both. We achieved respectable results, ranking 4th and 8th out of 31 submissions on the Facebook and Twitter test sets, respectively."
W18-1112,{R}eddit: A Gold Mine for Personality Prediction,2018,-1,-1,2,0,1115,matej gjurkovic,"Proceedings of the Second Workshop on Computational Modeling of People{'}s Opinions, Personality, and Emotions in Social Media",0,"Automated personality prediction from social media is gaining increasing attention in natural language processing and social sciences communities. However, due to high labeling costs and privacy issues, the few publicly available datasets are of limited size and low topic diversity. We address this problem by introducing a large-scale dataset derived from Reddit, a source so far overlooked for personality prediction. The dataset is labeled with Myers-Briggs Type Indicators (MBTI) and comes with a rich set of features for more than 9k users. We carry out a preliminary feature analysis, revealing marked differences between the MBTI dimensions and poles. Furthermore, we use the dataset to train and evaluate benchmark personality prediction models, achieving macro F1-scores between 67{\%} and 82{\%} on the individual dimensions and 82{\%} accuracy for exact or one-off accurate type prediction. These results are encouraging and comparable with the reliability of standardized tests."
S18-1135,{T}ake{L}ab at {S}em{E}val-2018 Task 7: Combining Sparse and Dense Features for Relation Classification in Scientific Texts,2018,0,0,4,0,28871,martin gluhak,Proceedings of The 12th International Workshop on Semantic Evaluation,0,"We describe two systems for semantic relation classification with which we participated in the SemEval 2018 Task 7, subtask 1 on semantic relation classification: an SVM model and a CNN model. Both models combine dense pretrained word2vec features and hancrafted sparse features. For training the models, we combine the two datasets provided for the subtasks in order to balance the under-represented classes. The SVM model performed better than CNN, achieving a F1-macro score of 69.98{\%} on subtask 1.1 and 75.69{\%} on subtask 1.2. The system ranked 7th on among 28 submissions on subtask 1.1 and 7th among 20 submissions on subtask 1.2."
S18-1192,{T}ake{L}ab at {S}em{E}val-2018 Task12: Argument Reasoning Comprehension with Skip-Thought Vectors,2018,0,1,4,0,4153,ana brassard,Proceedings of The 12th International Workshop on Semantic Evaluation,0,"This paper describes our system for the SemEval-2018 Task 12: Argument Reasoning Comprehension Task. We utilize skip-thought vectors, sentence-level distributional vectors inspired by the popular word embeddings and the skip-gram model. We encode preprocessed sentences from the dataset into vectors, then perform a binary supervised classification of the warrant that justifies the use of the reason as support for the claim. We explore a few variations of the model, reaching 54.1{\%} accuracy on the test set, which placed us 16th out of 22 teams participating in the task."
N18-2033,Lexical Substitution for Evaluating Compositional Distributional Models,2018,0,0,3,1,17079,maja buljan,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",0,"Compositional Distributional Semantic Models (CDSMs) model the meaning of phrases and sentences in vector space. They have been predominantly evaluated on limited, artificial tasks such as semantic sentence similarity on hand-constructed datasets. This paper argues for lexical substitution (LexSub) as a means to evaluate CDSMs. LexSub is a more natural task, enables us to evaluate meaning composition at the level of individual words, and provides a common ground to compare CDSMs with dedicated LexSub models. We create a LexSub dataset for CDSM evaluation from a corpus with manual {``}all-words{''} LexSub annotation. Our experiments indicate that the Practical Lexical Function CDSM outperforms simple component-wise CDSMs and performs on par with the context2vec LexSub model using the same context."
W17-5210,Toward Stance Classification Based on Claim Microstructures,2017,0,2,2,1,28929,filip boltuvzic,"Proceedings of the 8th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis",0,"Claims are the building blocks of arguments and the reasons underpinning opinions, thus analyzing claims is important for both argumentation mining and opinion mining. We propose a framework for representing claims as microstructures, which express the beliefs, judgments, and policies about the relations between domain-specific concepts. In a proof-of-concept study, we manually build microstructures for over 800 claims extracted from an online debate. We test the so-obtained microstructures on the task of claim stance classification, achieving considerable improvements over text-based baselines."
W17-4201,Predicting News Values from Headline Text and Emotions,2017,14,0,2,1,17273,maria buono,Proceedings of the 2017 {EMNLP} Workshop: Natural Language Processing meets Journalism,0,"We present a preliminary study on predicting news values from headline text and emotions. We perform a multivariate analysis on a dataset manually annotated with news values and emotions, discovering interesting correlations among them. We then train two competitive machine learning models {--} an SVM and a CNN {--} to predict news values from headline text and emotions as features. We find that, while both models yield a satisfactory performance, some news values are more difficult to detect than others, while some profit more from including emotion information."
W17-1727,Combining Linguistic Features for the Detection of {C}roatian Multiword Expressions,2017,10,0,2,1,17079,maja buljan,Proceedings of the 13th Workshop on Multiword Expressions ({MWE} 2017),0,"As multiword expressions (MWEs) exhibit a range of idiosyncrasies, their automatic detection warrants the use of many different features. Tsvetkov and Wintner (2014) proposed a Bayesian network model that combines linguistically motivated features and also models their interactions. In this paper, we extend their model with new features and apply it to Croatian, a morphologically complex and a relatively free word order language, achieving a satisfactory performance of 0.823 F1-score. Furthermore, by comparing against (semi)naive Bayes models, we demonstrate that manually modeling feature interactions is indeed important. We make our annotated dataset of Croatian MWEs freely available."
W17-1403,A Preliminary Study of {C}roatian Lexical Substitution,2017,16,0,2,1,32046,domagoj alagic,Proceedings of the 6th Workshop on {B}alto-{S}lavic Natural Language Processing,0,"Lexical substitution is a task of determining a meaning-preserving replacement for a word in context. We report on a preliminary study of this task for the Croatian language on a small-scale lexical sample dataset, manually annotated using three different annotation schemes. We compare the annotations, analyze the inter-annotator agreement, and observe a number of interesting language specific details in the obtained lexical substitutes. Furthermore, we apply a recently-proposed, dependency-based lexical substitution model to our dataset. The model achieves a P@3 score of 0.35, which indicates the difficulty of the task."
W17-1409,Debunking Sentiment Lexicons: A Case of Domain-Specific Sentiment Classification for {C}roatian,2017,12,0,4,0,32049,paula gombar,Proceedings of the 6th Workshop on {B}alto-{S}lavic Natural Language Processing,0,"Sentiment lexicons are widely used as an intuitive and inexpensive way of tackling sentiment classification, often within a simple lexicon word-counting approach or as part of a supervised model. However, it is an open question whether these approaches can compete with supervised models that use only word-representation features. We address this question in the context of domain-specific sentiment classification for Croatian. We experiment with the graph-based acquisition of sentiment lexicons, analyze their quality, and investigate how effectively they can be used in sentiment classification. Our results indicate that, even with as few as 500 labeled instances, a supervised model substantially outperforms a word-counting model. We also observe that adding lexicon-based features does not significantly improve supervised sentiment classification."
W17-1411,Comparison of Short-Text Sentiment Analysis Methods for {C}roatian,2017,25,0,2,0,32050,leon rotim,Proceedings of the 6th Workshop on {B}alto-{S}lavic Natural Language Processing,0,"We focus on the task of supervised sentiment classification of short and informal texts in Croatian, using two simple yet effective methods: word embeddings and string kernels. We investigate whether word embeddings offer any advantage over corpus- and preprocessing-free string kernels, and how these compare to bag-of-words baselines. We conduct a comparison on three different datasets, using different preprocessing methods and kernel functions. Results show that, on two out of three datasets, word embeddings outperform string kernels, which in turn outperform word and n-gram bag-of-words baselines."
W17-1412,"The First Cross-Lingual Challenge on Recognition, Normalization, and Matching of Named Entities in {S}lavic Languages",2017,24,4,3,0,6080,jakub piskorski,Proceedings of the 6th Workshop on {B}alto-{S}lavic Natural Language Processing,0,"This paper describes the outcomes of the first challenge on multilingual named entity recognition that aimed at recognizing mentions of named entities in web documents in Slavic languages, their normalization/lemmatization, and cross-language matching. It was organised in the context of the 6th Balto-Slavic Natural Language Processing Workshop, co-located with the EACL 2017 conference. Although eleven teams signed up for the evaluation, due to the complexity of the task(s) and short time available for elaborating a solution, only two teams submitted results on time. The reported evaluation figures reflect the relatively higher level of complexity of named entity-related tasks in the context of processing texts in Slavic languages. Since the duration of the challenge goes beyond the date of the publication of this paper and updated picture of the participating systems and their corresponding performance can be found on the web page of the challenge."
W17-0810,Two Layers of Annotation for Representing Event Mentions in News Stories,2017,3,0,3,1,17273,maria buono,Proceedings of the 11th Linguistic Annotation Workshop,0,"In this paper, we describe our preliminary study on annotating event mention as a part of our research on high-precision news event extraction models. To this end, we propose a two-layer annotation scheme, designed to separately capture the functional and conceptual aspects of event mentions. We hypothesize that the precision of models can be improved by modeling and extracting separately the different aspects of news events, and then combining the extracted information by leveraging the complementarities of the models. In addition, we carry out a preliminary annotation using the proposed scheme and analyze the annotation quality in terms of inter-annotator agreement."
S17-2055,{T}ake{L}ab-{QA} at {S}em{E}val-2017 Task 3: Classification Experiments for Answer Retrieval in Community {QA},2017,0,1,5,0,32282,filip vsaina,Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val-2017),0,"In this paper we present the TakeLab-QA entry to SemEval 2017 task 3, which is a question-comment re-ranking problem. We present a classification based approach, including two supervised learning models {--} Support Vector Machines (SVM) and Convolutional Neural Networks (CNN). We use features based on different semantic similarity models (e.g., Latent Dirichlet Allocation), as well as features based on several types of pre-trained word embeddings. Moreover, we also use some hand-crafted task-specific features. For training, our system uses no external labeled data apart from that provided by the organizers. Our primary submission achieves a MAP-score of 81.14 and F1-score of 66.99 {--} ranking us 10th on the SemEval 2017 task 3, subtask A."
S17-2066,{T}ake{L}ab at {S}em{E}val-2017 Task 6: {\\#}{R}anking{H}umor{I}n4{P}ages,2017,0,0,6,0,32296,marin kukovavcec,Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val-2017),0,"This paper describes our system for humor ranking in tweets within the SemEval 2017 Task 6: {\#}HashtagWars (6A and 6B). For both subtasks, we use an off-the-shelf gradient boosting model built on a rich set of features, handcrafted to provide the model with the external knowledge needed to better predict the humor in the text. The features capture various cultural references and specific humor patterns. Our system ranked 2nd (officially 7th) among 10 submissions on the Subtask A and 2nd among 9 submissions on the Subtask B."
S17-2132,{T}ake{L}ab at {S}em{E}val-2017 Task 4: Recent Deaths and the Power of Nostalgia in Sentiment Analysis in {T}witter,2017,0,2,5,0,32355,david lozic,Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val-2017),0,"This paper describes the system we submitted to SemEval-2017 Task 4 (Sentiment Analysis in Twitter), specifically subtasks A, B, and D. Our main focus was topic-based message polarity classification on a two-point scale (subtask B). The system we submitted uses a Support Vector Machine classifier with rich set of features, ranging from standard to more creative, task-specific features, including a series of rating-based features as well as features that account for sentimental reminiscence of past topics and deceased famous people. Our system ranked 14th out of 39 submissions in subtask A, 5th out of 24 submissions in subtask B, and 3rd out of 16 submissions in subtask D."
S17-2148,{T}ake{L}ab at {S}em{E}val-2017 Task 5: Linear aggregation of word embeddings for fine-grained sentiment analysis of financial news,2017,0,2,3,0,32050,leon rotim,Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val-2017),0,This paper describes our system for fine-grained sentiment scoring of news headlines submitted to SemEval 2017 task 5{--}subtask 2. Our system uses a feature-light method that consists of a Support Vector Regression (SVR) with various kernels and word vectors as features. Our best-performing submission scored 3rd on the task out of 29 teams and 4th out of 45 submissions with a cosine score of 0.733.
S17-1014,Does Free Word Order Hurt? Assessing the Practical Lexical Function Model for {C}roatian,2017,0,0,2,0,15426,zoran medic,Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*{SEM} 2017),0,"The Practical Lexical Function (PLF) model is a model of computational distributional semantics that attempts to strike a balance between expressivity and learnability in predicting phrase meaning and shows competitive results. We investigate how well the PLF carries over to free word order languages, given that it builds on observations of predicate-argument combinations that are harder to recover in free word order languages. We evaluate variants of the PLF for Croatian, using a new lexical substitution dataset. We find that the PLF works about as well for Croatian as for English, but demonstrate that its strength lies in modeling verbs, and that the free word order affects the less robust PLF variant."
Q17-1032,Unsupervised Acquisition of Comprehensive Multiword Lexicons using Competition in an n-gram Lattice,2017,11,0,2,0,28552,julian brooke,Transactions of the Association for Computational Linguistics,0,"We present a new model for acquiring comprehensive multiword lexicons from large corpora based on competition among n-gram candidates. In contrast to the standard approach of simple ranking by association measure, in our model n-grams are arranged in a lattice structure based on subsumption and overlap relationships, with nodes inhibiting other nodes in their vicinity when they are selected as a lexical item. We show how the configuration of such a lattice can be optimized tractably, and demonstrate using annotations of sampled n-grams that our method consistently outperforms alternatives by at least 0.05 F-score across several corpora and languages."
I17-2031,Using Analytic Scoring Rubrics in the Automatic Assessment of College-Level Summary Writing Tasks in {L}2,2017,17,1,2,0,24153,tamara sladoljevagejev,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"Assessing summaries is a demanding, yet useful task which provides valuable information on language competence, especially for second language learners. We consider automated scoring of college-level summary writing task in English as a second language (EL2). We adopt the Reading-for-Understanding (RU) cognitive framework, extended with the Reading-to-Write (RW) element, and use analytic scoring with six rubrics covering content and writing quality. We show that regression models with reference-based and linguistic features considerably outperform the baselines across all the rubrics. Moreover, we find interesting correlations between summary features and analytic rubrics, revealing the links between the RU and RW constructs."
W16-2815,Fill the Gap! Analyzing Implicit Premises between Claims from Online Debates,2016,22,5,2,1,28929,filip boltuvzic,Proceedings of the Third Workshop on Argument Mining ({A}rg{M}ining2016),0,None
W16-2102,Analysis of Policy Agendas: Lessons Learned from Automatic Topic Classification of {C}roatian Political Texts,2016,11,2,2,1,1116,mladen karan,"Proceedings of the 10th {SIGHUM} Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities",0,"Policy agenda research is concerned with measuring the policymaker activities. Topic classification has proven a valuable tool for policy agenda research. However, manual topic coding is extremely costly and time-consuming. Supervised topic classification offers a cost- effective and reliable alternative, yet it introduces new challenges, the most significant of which are the training set coding, classifier design, and accuracy-efficiency trade-off. In this work, we address these challenges in the context of the recently launched Croatian Policy Agendas project. We describe a new policy agenda dataset, explore the many system design choices, and report on the in- sights gained. Our best-performing model reaches 77% and 68% of F1- score for ma- jor topics and subtopics, respectively."
S16-1075,{T}ake{L}ab at {S}em{E}val-2016 Task 6: Stance Classification in Tweets Using a Genetic Algorithm Based Ensemble,2016,9,14,9,1,15648,martin tutek,Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016),0,None
L16-1267,{C}ro36{WSD}: A Lexical Sample for {C}roatian Word Sense Disambiguation,2016,17,2,2,1,32046,domagoj alagic,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"We introduce Cro36WSD, a freely-available medium-sized lexical sample for Croatian word sense disambiguation (WSD).Cro36WSD comprises 36 words: 12 adjectives, 12 nouns, and 12 verbs, balanced across both frequency bands and polysemy levels. We adopt the multi-label annotation scheme in the hope of lessening the drawbacks of discrete sense inventories and obtaining more realistic annotations from human experts. Sense-annotated data is collected through multiple annotation rounds to ensure high-quality annotations: with a 115 person-hours effort we reached an inter-annotator agreement score of 0.877. We analyze the obtained data and perform a correlation analysis between several relevant variables, including word frequency, number of senses, sense distribution skewness, average annotation time, and the observed inter-annotator agreement (IAA). Using the obtained data, we compile multi- and single-labeled dataset variants using different label aggregation schemes. Finally, we evaluate three different baseline WSD models on both dataset variants and report on the insights gained. We make both dataset variants freely available."
L16-1425,{V}erb{CRO}cean: A Repository of Fine-Grained Semantic Verb Relations for {C}roatian,2016,0,0,2,1,22657,ivan sekulic,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"In this paper we describe VerbCROcean, a broad-coverage repository of fine-grained semantic relations between Croatian verbs. Adopting the methodology of Chklovski and Pantel (2004) used for acquiring the English VerbOcean, we first acquire semantically related verb pairs from a web corpus hrWaC by relying on distributional similarity of subject-verb-object paths in the dependency trees. We then classify the semantic relations between each pair of verbs as similarity, intensity, antonymy, or happens-before, using a number of manually-constructed lexico-syntatic patterns. We evaluate the quality of the resulting resource on a manually annotated sample of 1000 semantic verb relations. The evaluation revealed that the predictions are most accurate for the similarity relation, and least accurate for the intensity relation. We make available two variants of VerbCROcean: a coverage-oriented version, containing about 36k verb pairs at a precision of 41{\%}, and a precision-oriented version containing about 5k verb pairs, at a precision of 56{\%}."
L16-1481,Graph-Based Induction of Word Senses in {C}roatian,2016,0,0,2,0,35203,marko bekavac,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Word sense induction (WSI) seeks to induce senses of words from unannotated corpora. In this paper, we address the WSI task for the Croatian language. We adopt the word clustering approach based on co-occurrence graphs, in which senses are taken to correspond to strongly inter-connected components of co-occurring words. We experiment with a number of graph construction techniques and clustering algorithms, and evaluate the sense inventories both as a clustering problem and extrinsically on a word sense disambiguation (WSD) task. In the cluster-based evaluation, Chinese Whispers algorithm outperformed Markov Clustering, yielding a normalized mutual information score of 64.3. In contrast, in WSD evaluation Markov Clustering performed better, yielding an accuracy of about 75{\%}. We are making available two induced sense inventories of 10,000 most frequent Croatian words: one coarse-grained and one fine-grained inventory, both obtained using the Markov Clustering algorithm."
C16-1122,Predictability of Distributional Semantics in Derivational Word Formation,2016,30,4,4,0,411,sebastian pado,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"Compositional distributional semantic models (CDSMs) have successfully been applied to the task of predicting the meaning of a range of linguistic constructions. Their performance on semi-compositional word formation process of (morphological) derivation, however, has been extremely variable, with no large-scale empirical investigation to date. This paper fills that gap, performing an analysis of CDSM predictions on a large dataset (over 30,000 German derivationally related word pairs). We use linear regression models to analyze CDSM performance and obtain insights into the linguistic factors that influence how predictable the distributional context of a derived word is going to be. We identify various such factors, notably part of speech, argument structure, and semantic regularity."
W15-5303,Resolving Entity Coreference in {C}roatian with a Constrained Mention-Pair Model,2015,21,0,2,1,7439,goran glavavs,The 5th Workshop on {B}alto-{S}lavic Natural Language Processing,0,None
W15-5309,Experiments on Active Learning for {C}roatian Word Sense Disambiguation,2015,40,0,2,1,32046,domagoj alagic,The 5th Workshop on {B}alto-{S}lavic Natural Language Processing,0,"Supervised word sense disambiguation (WSD) has been shown to achieve state-ofthe-art results but at high annotation costs. Active learning can ameliorate that problem by allowing the model to dynamically choose the most informative word contexts for manual labeling. In this paper we investigate the use of active learning for Croatian WSD. We adopt a lexical sample approach and compile a corresponding senseannotated dataset on which we evaluate our models. We carry out a detailed investigation of the different active learning setups, and show that labeling as few as 100 instances suffices to reach near-optimal performance."
W15-0514,Identifying Prominent Arguments in Online Debates Using Semantic Textual Similarity,2015,18,18,2,1,28929,filip boltuvzic,Proceedings of the 2nd Workshop on Argumentation Mining,0,"Online debates sparkle argumentative discussions from which generally accepted arguments often emerge. We consider the task of unsupervised identification of prominent argument in online debates. As a first step, in this paper we perform a cluster analysis using semantic textual similarity to detect similar arguments. We perform a preliminary cluster evaluation and error analysis based on cluster-class matching against a manually labeled dataset."
W15-0108,Obtaining a Better Understanding of Distributional Models of {G}erman Derivational Morphology,2015,19,7,4,0.740741,31328,max kisselew,Proceedings of the 11th International Conference on Computational Semantics,0,"Predicting the (distributional) meaning of derivationally related words (read / reader) from one another has recently been recognized as an instance of distributional compositional meaning construction. However, the properties of this task are not yet well understood. In this paper, we present an analysis of two such composition models on a set of German derivation patterns (e.g., -in, durch-). We begin by introducing a rank-based evaluation metric, which reveals the task to be challenging due to specific properties of German (compounding, capitalization). We also find that performance varies greatly between patterns and even among base-derived term pairs of the same pattern. A regression analysis shows that semantic coherence of the base and derived terms within a pattern, as well as coherence of the semantic shifts from base to derived terms, all significantly impact prediction quality."
S15-2012,{TKLBLIIR}: Detecting {T}witter Paraphrases with {T}weeting{J}ay,2015,19,5,3,1,1116,mladen karan,Proceedings of the 9th International Workshop on Semantic Evaluation ({S}em{E}val 2015),0,"When tweeting on a topic, Twitter users often post messages that convey the same or similar meaning. We describe TweetingJay, a system for detecting paraphrases and semantic similarity of tweets, with which we participated in Task 1 of SemEval 2015. TweetingJay uses a supervised model that combines semantic overlap and word alignment features, previously shown to be effective for detecting semantic textual similarity. TweetingJay reaches 65.9% F1-score and ranked fourth among the 18 participating systems. We additionally provide an analysis of the dataset and point to some peculiarities of the evaluation setup."
W14-3705,Constructing Coherent Event Hierarchies from News Stories,2014,22,0,2,1,7439,goran glavavs,Proceedings of {T}ext{G}raphs-9: the workshop on Graph-based Methods for Natural Language Processing,0,"News describe real-world events of varying granularity, and recognition of internal structure of events is important for automated reasoning over events. We propose an approach for constructing coherent event hierarchies from news by enforcing document-level coherence over pairwise decisions of spatiotemporal containment. Evaluation on a news corpus annotated with event hierarchies shows that enforcing global spatiotemporal coreference of events leads to significant improvements (7.6% F1-score) in the accuracy of pairwise decisions."
W14-2107,Back up your Stance: Recognizing Arguments in Online Discussions,2014,30,65,2,1,28929,filip boltuvzic,Proceedings of the First Workshop on Argumentation Mining,0,"In online discussions, users often back up their stance with arguments. Their arguments are often vague, implicit, and poorly worded, yet they provide valuable insights into reasons underpinning usersxe2x80x99 opinions. In this paper, we make a first step towards argument-based opinion mining from online discussions and introduce a new task of argument recognition. We match usercreated comments to a set of predefined topic-based arguments, which can be either attacked or supported in the comment. We present a manually-annotated corpus for argument recognition in online discussions. We describe a supervised model based on comment-argument similarity and entailment features. Depending on problem formulation, model performance ranges from 70.5% to 81.8% F1-score, and decreases only marginally when applied to an unseen topic."
glavas-etal-2014-hieve,{H}i{E}ve: A Corpus for Extracting Event Hierarchies from News Stories,2014,21,3,2,1,7439,goran glavavs,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"In news stories, event mentions denote real-world events of different spatial and temporal granularity. Narratives in news stories typically describe some real-world event of coarse spatial and temporal granularity along with its subevents. In this work, we present HiEve, a corpus for recognizing relations of spatiotemporal containment between events. In HiEve, the narratives are represented as hierarchies of events based on relations of spatiotemporal containment (i.e., supereventâsubevent relations). We describe the process of manual annotation of HiEve. Furthermore, we build a supervised classifier for recognizing spatiotemporal containment between events to serve as a baseline for future research. Preliminary experimental results are encouraging, with classifier performance reaching 58{\%} F1-score, only 11{\%} less than the inter annotator agreement."
snajder-2014-derivbase,{D}eriv{B}ase.hr: A High-Coverage Derivational Morphology Resource for {C}roatian,2014,26,5,1,1,23915,jan vsnajder,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"Knowledge about derivational morphology has been proven useful for a number of natural language processing (NLP) tasks. We describe the construction and evaluation of DerivBase.hr, a large-coverage morphological resource for Croatian. DerivBase.hr groups 100k lemmas from web corpus hrWaC into 56k clusters of derivationally related lemmas, so-called derivational families. We focus on suffixal derivation between and within nouns, verbs, and adjectives. We propose two approaches: an unsupervised approach and a knowledge-based approach based on a hand-crafted morphology model but without using any additional lexico-semantic resources The resource acquisition procedure consists of three steps: corpus preprocessing, acquisition of an inflectional lexicon, and the induction of derivational families. We describe an evaluation methodology based on manually constructed derivational families from which we sample and annotate pairs of lemmas. We evaluate DerivBase.hr on the so-obtained sample, and show that the knowledge-based version attains good clustering quality of 81.2{\%} precision, 76.5{\%} recall, and 78.8{\%} F1 -score. As with similar resources for other languages, we expect DerivBase.hr to be useful for a number of NLP tasks."
C14-1163,Towards Semantic Validation of a Derivational Lexicon,2014,28,4,3,1,40299,britta zeller,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"Derivationally related lemmas like friendN xe2x80x90 friendlyA xe2x80x90 friendshipN are derived from a common stem. Frequently, their meanings are also systematically related. However, there are also many examples of derivationally related lemma pairs whose meanings differ substantially, e.g., objectN xe2x80x90 objective N . Most broad-coverage derivational lexicons do not reflect this distinction, mixing up semantically related and unrelated word pairs. In this paper, we investigate strategies to recover the above distinction by recognizing semantically related lemma pairs, a process we call semantic validation. We make two main contributions: First, we perform a detailed data analysis on the basis of a large German derivational lexicon. It reveals two promising sources of information (distributional semantics and structural information about derivational rules), but also systematic problems with these sources. Second, we develop a classification model for the task that reflects the noisy nature of the data. It achieves an improvement of 13.6% in precision and 5.8% in F1-score over a strong majority class baseline. Our experiments confirm that both information sources contribute to semantic validation, and that they are complementary enough that the best results are obtained from a combined model."
W13-5001,Event-Centered Information Retrieval Using Kernels on Event Graphs,2013,-1,-1,2,1,7439,goran glavavs,Proceedings of {T}ext{G}raphs-8 Graph-based Methods for Natural Language Processing,0,None
W13-2404,Aspect-Oriented Opinion Mining from User Reviews in {C}roatian,2013,24,3,3,1,7439,goran glavavs,Proceedings of the 4th Biennial International Workshop on {B}alto-{S}lavic Natural Language Processing,0,"Aspect-oriented opinion mining aims to identify product aspects (features of products) about which opinion has been expressed in the text. We present an approach for aspect-oriented opinion mining from user reviews in Croatian. We propose methods for acquiring a domain-specific opinion lexicon, linking opinion clues to product aspects, and predicting polarity and rating of reviews. We show that a supervised approach to linking opinion clues to aspects is feasible, and that the extracted clues and aspects improve polarity and rating predictions."
W13-2405,Frequently Asked Questions Retrieval for {C}roatian Based on Semantic Textual Similarity,2013,30,1,3,1,1116,mladen karan,Proceedings of the 4th Biennial International Workshop on {B}alto-{S}lavic Natural Language Processing,0,"Frequently asked questions (FAQ) are an efficient way of communicating domainspecific information to the users. Unlike general purpose retrieval engines, FAQ retrieval engines have to address the lexical gap between the query and the usually short answer. In this paper we describe the design and evaluation of a FAQ retrieval engine for Croatian. We frame the task as a binary classification problem, and train a model to classify each FAQ as either relevant or not relevant for a given query. We use a variety of semantic textual similarity features, including term overlap and vector space features. We train and evaluate on a FAQ test collection built specifically for this purpose. Our best-performing model reaches 0.47 of mean reciprocal rank, i.e., on average ranks the relevant answer among the top two returned answers."
W13-2407,{GPKEX}: Genetically Programmed Keyphrase Extraction from {C}roatian Texts,2013,19,4,2,0,35203,marko bekavac,Proceedings of the 4th Biennial International Workshop on {B}alto-{S}lavic Natural Language Processing,0,"We describe GPKEX, a keyphrase extraction method based on genetic programming. We represent keyphrase scoring measures as syntax trees and evolve them to produce rankings for keyphrase candidates extracted from text. We apply and evaluate GPKEX on Croatian newspaper articles. We show that GPKEX can evolve simple and interpretable keyphrase scoring measures that perform comparably to more complex machine learning methods previously developed for Croatian."
P13-2128,Derivational Smoothing for Syntactic Distributional Semantics,2013,29,7,2,0,411,sebastian pado,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Syntax-based vector spaces are used widely in lexical semantics and are more versatile than word-based spaces (Baroni and Lenci, 2010). However, they are also sparse, with resulting reliability and coverage problems. We address this problem by derivational smoothing, which uses knowledge about derivationally related words (oldish! old) to improve semantic similarity estimates. We develop a set of derivational smoothing methods and evaluate them on two lexical semantics tasks in German. Even for models built from very large corpora, simple derivational smoothing can improve coverage considerably."
P13-2137,Building and Evaluating a Distributional Memory for {C}roatian,2013,23,7,1,1,23915,jan vsnajder,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"We report on the first structured distributional semantic model for Croatian, DM.HR. It is constructed after the model of the English Distributional Memory (Baroni and Lenci, 2010), from a dependencyparsed Croatian web corpus, and covers about 2M lemmas. We give details on the linguistic processing and the design principles. An evaluation shows state-of-theart performance on a semantic similarity task with particularly good performance on nouns. The resource is freely available."
P13-2139,Recognizing Identical Events with Graph Kernels,2013,23,0,2,1,7439,goran glavavs,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Identifying news stories that discuss the same real-world events is important for news tracking and retrieval. Most existing approaches rely on the traditional vector space model. We propose an approach for recognizing identical real-world events based on a structured, event-oriented document representation. We structure documents as graphs of event mentions and use graph kernels to measure the similarity between document pairs. Our experiments indicate that the proposed graph-based approach can outperform the traditional vector space model, and is especially suitable for distinguishing between topically similar, yet non-identical events."
P13-1118,{DE}riv{B}ase: Inducing and Evaluating a Derivational Morphology Resource for {G}erman,2013,38,20,2,1,40299,britta zeller,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Derivational models are still an underresearched area in computational morphology. Even for German, a rather resourcerich language, there is a lack of largecoverage derivational knowledge. This paper describes a rule-based framework for inducing derivational families (i.e., clusters of lemmas in derivational relationships) and its application to create a highcoverage German resource, DERIVBASE, mapping over 280k lemmas into more than 17k non-singleton clusters. We focus on the rule component and a qualitative and quantitative evaluation. Our approach achieves up to 93% precision and 71% recall. We attribute the high precision to the fact that our rules are based on information from grammar books."
W12-0501,Experiments on Hybrid Corpus-Based Sentiment Lexicon Acquisition,2012,20,0,2,1,7439,goran glavavs,Proceedings of the Workshop on Innovative Hybrid Approaches to the Processing of Textual Data,0,"Numerous sentiment analysis applications make usage of a sentiment lexicon. In this paper we present experiments on hybrid sentiment lexicon acquisition. The approach is corpus-based and thus suitable for languages lacking general dictionary-based resources. The approach is a hybrid two-step process that combines semi-supervised graph-based algorithms and supervised models. We evaluate the performance on three tasks that capture different aspects of a sentiment lexicon: polarity ranking task, polarity regression task, and sentiment classification task. Extensive evaluation shows that the results are comparable to those of a well-known sentiment lexicon SentiWordNet on the polarity ranking task. On the sentiment classification task, the results are also comparable to SentiWordNet when restricted to monosentimous (all senses carry the same sentiment) words. This is satisfactory, given the absence of explicit semantic relations between words in the corpus."
S12-1060,{T}ake{L}ab: Systems for Measuring Semantic Text Similarity,2012,25,133,4,0,42604,frane vsaric,"*{SEM} 2012: The First Joint Conference on Lexical and Computational Semantics {--} Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation ({S}em{E}val 2012)",0,"This paper describes the two systems for determining the semantic similarity of short texts submitted to the SemEval 2012 Task 6. Most of the research on semantic similarity of textual content focuses on large documents. However, a fair amount of information is condensed into short text snippets such as social media posts, image captions, and scientific abstracts. We predict the human ratings of sentence similarity using a support vector regression model with multiple features measuring word-overlap similarity and syntax similarity. Out of 89 systems submitted, our two systems ranked in the top 5, for the three overall evaluation metrics used (overall Pearson -- 2nd and 3rd, normalized Pearson -- 1st and 3rd, weighted mean -- 2nd and 5th)."
karan-etal-2012-evaluation,Evaluation of Classification Algorithms and Features for Collocation Extraction in {C}roatian,2012,14,3,2,1,1116,mladen karan,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"Collocations can be defined as words that occur together significantly more often than it would be expected by chance. Many natural language processing applications such as natural language generation, word sense disambiguation and machine translation can benefit from having access to information about collocated words. We approach collocation extraction as a classification problem where the task is to classify a given n-gram as either a collocation (positive) or a non-collocation (negative). Among the features used are word frequencies, classical association measures (Dice, PMI, chi2), and POS tags. In addition, semantic word relatedness modeled by latent semantic analysis is also included. We apply wrapper feature subset selection to determine the best set of features. Performance of various classification algorithms is tested. Experiments are conducted on a manually annotated set of bigrams and trigrams sampled from a Croatian newspaper corpus. Best results obtained are 79.8 F1 measure for bigrams and 67.5 F1 measure for trigrams. The best classifier for bigrams was SVM, while for trigrams the decision tree gave the best performance. Features which contributed the most to overall performance were PMI, semantic relatedness, and POS information."
seljan-etal-2010-corpus,Corpus Aligner ({C}or{A}l) Evaluation on {E}nglish-{C}roatian Parallel Corpora,2010,8,2,4,0,42908,sanja seljan,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"An increasing demand for new language resources of recent EU members and accessing countries has in turn initiated the development of different language tools and resources, such as alignment tools and corresponding translation memories for new languages pairs. The primary goal of this paper is to provide a description of a free sentence alignment tool CorAl (Corpus Aligner), developed at the Faculty of Electrical Engineering and Computing, University of Zagreb. The tool performs paragraph alignment at the first step of the alignment process, which is followed by sentence alignment. Description of the tool is followed by its evaluation. The paper describes an experiment with applying the CorAl aligner to a English-Croatian parallel corpus of legislative domain using metrics of precision, recall and F1-measure. Results are discussed and the concluding sections discuss future directions of CorAl development."
R09-1074,String Distance-Based Stemming of the Highly Inflected {C}roatian Language,2009,15,6,1,1,23915,jan vsnajder,Proceedings of the International Conference {RANLP}-2009,0,"Stemming refers to the grouping of morphologically related words into so-called stem classes for the purpose of improving information retrieval performance. Traditional approaches to stemming are language-specific and require a substantial amount of linguistic knowledge. A viable alternative is string distance-based stemming, in which stem classes are obtained by clustering word-forms from a corpus. In this paper, we apply string distance-based stemming to the highly inflected Croatian language using a number of string distance measures proposed in the literature. We focus on evaluating the stemming performance at both inflectional and derivational level, and investigate how this performance relates to the choice of the distance threshold value. Although our focus is on the Croatian language, we believe our results transfer well to languages of similar morphological complexity."
P08-2046,Evolving New Lexical Association Measures Using Genetic Programming,2008,8,5,1,1,23915,jan vsnajder,"Proceedings of ACL-08: HLT, Short Papers",0,"Automatic extraction of collocations from large corpora has been the focus of many research efforts. Most approaches concentrate on improving and combining known lexical association measures. In this paper, we describe a genetic programming approach for evolving new association measures, which is not limited to any specific language, corpus, or type of collocation. Our preliminary experimental results show that the evolved measures outperform three known association measures."
