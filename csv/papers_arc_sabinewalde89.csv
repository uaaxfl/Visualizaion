2021.vardial-1.3,Regression Analysis of Lexical and Morpho-Syntactic Properties of Kiezdeutsch,2021,-1,-1,5,1,627,diego frassinelli,"Proceedings of the Eighth Workshop on NLP for Similar Languages, Varieties and Dialects",0,"Kiezdeutsch is a variety of German predominantly spoken by teenagers from multi-ethnic urban neighborhoods in casual conversations with their peers. In recent years, the popularity of Kiezdeutsch has increased among young people, independently of their socio-economic origin, and has spread in social media, too. While previous studies have extensively investigated this language variety from a linguistic and qualitative perspective, not much has been done from a quantitative point of view. We perform the first large-scale data-driven analysis of the lexical and morpho-syntactic properties of Kiezdeutsch in comparison with standard German. At the level of results, we confirm predictions of previous qualitative analyses and integrate them with further observations on specific linguistic phenomena such as slang and self-centered speaker attitude. At the methodological level, we provide logistic regression as a framework to perform bottom-up feature selection in order to quantify differences across language varieties."
2021.starsem-1.23,Modeling Sense Structure in Word Usage Graphs with the Weighted Stochastic Block Model,2021,-1,-1,4,1,630,dominik schlechtweg,Proceedings of *SEM 2021: The Tenth Joint Conference on Lexical and Computational Semantics,0,"We suggest to model human-annotated Word Usage Graphs capturing fine-grained semantic proximity distinctions between word uses with a Bayesian formulation of the Weighted Stochastic Block Model, a generative model for random graphs popular in biology, physics and social sciences. By providing a probabilistic model of graded word meaning we aim to approach the slippery and yet widely used notion of word sense in a novel way. The proposed framework enables us to rigorously compare models of word senses with respect to their fit to the data. We perform extensive experiments and select the empirically most adequate model."
2021.starsem-1.24,Compound or Term Features? Analyzing Salience in Predicting the Difficulty of {G}erman Noun Compounds across Domains,2021,-1,-1,5,1,1000,anna hatty,Proceedings of *SEM 2021: The Tenth Joint Conference on Lexical and Computational Semantics,0,"Predicting the difficulty of domain-specific vocabulary is an important task towards a better understanding of a domain, and to enhance the communication between lay people and experts. We investigate German closed noun compounds and focus on the interaction of compound-based lexical features (such as frequency and productivity) and terminology-based features (contrasting domain-specific and general language) across word representations and classifiers. Our prediction experiments complement insights from classification using (a) manually designed features to characterise termhood and compound formation and (b) compound and constituent word embeddings. We find that for a broad binary distinction into {`}easy{'} vs. {`}difficult{'} general-language compound frequency is sufficient, but for a more fine-grained four-class distinction it is crucial to include contrastive termhood features and compound and constituent features."
2021.konvens-1.24,"{W}ord{G}uess: Using Associations for Guessing, Learning and Exploring Related Words",2021,-1,-1,4,0.555556,5582,cennet oguz,Proceedings of the 17th Conference on Natural Language Processing (KONVENS 2021),0,None
2021.findings-acl.16,More than just Frequency? Demasking Unsupervised Hypernymy Prediction Methods,2021,-1,-1,3,0,7527,thomas bott,Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,0,None
2021.eacl-srw.25,Explaining and Improving {BERT} Performance on Lexical Semantic Change Detection,2021,-1,-1,5,0,10513,severin laicher,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Student Research Workshop,0,"Type- and token-based embedding architectures are still competing in lexical semantic change detection. The recent success of type-based models in SemEval-2020 Task 1 has raised the question why the success of token-based models on a variety of other NLP tasks does not translate to our field. We investigate the influence of a range of variables on clusterings of BERT vectors and show that its low performance is largely due to orthographic information on the target word, which is encoded even in the higher layers of BERT representations. By reducing the influence of orthography we considerably improve BERT{'}s performance."
2021.acl-long.543,Lexical Semantic Change Discovery,2021,-1,-1,5,0,10514,sinan kurtyigit,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"While there is a large amount of research in the field of Lexical Semantic Change Detection, only few approaches go beyond a standard benchmark evaluation of existing models. In this paper, we propose a shift of focus from change detection to change discovery, i.e., discovering novel word senses over time from the full corpus vocabulary. By heavily fine-tuning a type-based and a token-based approach on recently published German data, we demonstrate that both models can successfully be applied to discover new words undergoing meaning change. Furthermore, we provide an almost fully automated framework for both evaluation and discovery."
2020.winlp-1.13,Variants of Vector Space Reductions for Predicting the Compositionality of {E}nglish Noun Compounds,2020,-1,-1,2,0,7692,pegah alipoormolabashi,Proceedings of the The Fourth Widening Natural Language Processing Workshop,0,"Predicting the degree of compositionality of noun compounds is a crucial ingredient for lexicography and NLP applications, to know whether the compound should be treated as a whole, or through its constituents. Computational approaches for an automatic prediction typically represent compounds and their constituents within a vector space to have a numeric relatedness measure for the words. This paper provides a systematic evaluation of using different vector-space reduction variants for the prediction. We demonstrate that Word2vec and nouns-only dimensionality reductions are the most successful and stable vector space reduction variants for our task."
2020.semeval-1.8,{IMS} at {S}em{E}val-2020 Task 1: How Low Can You Go? Dimensionality in Lexical Semantic Change Detection,2020,-1,-1,4,0,10530,jens kaiser,Proceedings of the Fourteenth Workshop on Semantic Evaluation,0,"We present the results of our system for SemEval-2020 Task 1 that exploits a commonly used lexical semantic change detection model based on Skip-Gram with Negative Sampling. Our system focuses on Vector Initialization (VI) alignment, compares VI to the currently top-ranking models for Subtask 2 and demonstrates that these can be outperformed if we optimize VI dimensionality. We demonstrate that differences in performance can largely be attributed to model-specific sources of noise, and we reveal a strong relationship between dimensionality and frequency-induced noise in VI alignment. Our results suggest that lexical semantic change models integrating vector space alignment should pay more attention to the role of the dimensionality parameter."
2020.lrec-1.537,"A Domain-Specific Dataset of Difficulty Ratings for {G}erman Noun Compounds in the Domains {DIY}, Cooking and Automotive",2020,-1,-1,4,0,1001,julia bettinger,Proceedings of the 12th Language Resources and Evaluation Conference,0,"We present a dataset with difficulty ratings for 1,030 German closed noun compounds extracted from domain-specific texts for do-it-ourself (DIY), cooking and automotive. The dataset includes two-part compounds for cooking and DIY, and two- to four-part compounds for automotive. The compounds were identified in text using the Simple Compound Splitter (Weller-Di Marco, 2017); a subset was filtered and balanced for frequency and productivity criteria as basis for manual annotation and fine-grained interpretation. This study presents the creation, the final dataset with ratings from 20 annotators and statistics over the dataset, to provide insight into the perception of domain-specific term difficulty. It is particularly striking that annotators agree on a coarse, binary distinction between easy vs. difficult domain-specific compounds but that a more fine grained distinction of difficulty is not meaningful. We finally discuss the challenges of an annotation for difficulty, which includes both the task description as well as the selection of the data basis."
2020.lrec-1.539,Variants of Vector Space Reductions for Predicting the Compositionality of {E}nglish Noun Compounds,2020,-1,-1,2,0,17754,pegah alipoor,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Predicting the degree of compositionality of noun compounds such as {``}snowball{''} and {``}butterfly{''} is a crucial ingredient for lexicography and Natural Language Processing applications, to know whether the compound should be treated as a whole, or through its constituents, and what it means. Computational approaches for an automatic prediction typically represent and compare compounds and their constituents within a vector space and use distributional similarity as a proxy to predict the semantic relatedness between the compounds and their constituents as the compound{'}s degree of compositionality. This paper provides a systematic evaluation of vector-space reduction variants across kinds, exploring reductions based on part-of-speech next to and also in combination with Principal Components Analysis using Singular Value and word2vec embeddings. We show that word2vec and nouns only dimensionality reductions are the most successful and stable vector space variants for our task."
2020.lrec-1.540,Varying Vector Representations and Integrating Meaning Shifts into a {P}age{R}ank Model for Automatic Term Extraction,2020,-1,-1,3,0,17755,anurag nigam,Proceedings of the 12th Language Resources and Evaluation Conference,0,"We perform a comparative study for automatic term extraction from domain-specific language using a PageRank model with different edge-weighting methods. We vary vector space representations within the PageRank graph algorithm, and we go beyond standard co-occurrence and investigate the influence of measures of association strength and first- vs. second-order co-occurrence. In addition, we incorporate meaning shifts from general to domain-specific language as personalized vectors, in order to distinguish between termhood strengths of ambiguous words across word senses. Our study is performed for two domain-specific English corpora: ACL and do-it-yourself (DIY); and a domain-specific German corpus: cooking. The models are assessed by applying average precision and the roc score as evaluation metrices."
2020.lrec-1.859,{CCOHA}: Clean Corpus of Historical {A}merican {E}nglish,2020,-1,-1,4,0,629,reem alatrash,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Modelling language change is an increasingly important area of interest within the fields of sociolinguistics and historical linguistics. In recent years, there has been a growing number of publications whose main concern is studying changes that have occurred within the past centuries. The Corpus of Historical American English (COHA) is one of the most commonly used large corpora in diachronic studies in English. This paper describes methods applied to the downloadable version of the COHA corpus in order to overcome its main limitations, such as inconsistent lemmas and malformed tokens, without compromising its qualitative and distributional properties. The resulting corpus CCOHA contains a larger number of cleaned word tokens which can offer better insights into language change and allow for a larger variety of tasks to be performed."
2020.acl-main.258,Predicting Degrees of Technicality in Automatic Terminology Extraction,2020,-1,-1,4,1,1000,anna hatty,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"While automatic term extraction is a well-researched area, computational approaches to distinguish between degrees of technicality are still understudied. We semi-automatically create a German gold standard of technicality across four domains, and illustrate the impact of a web-crawled general-language corpus on technicality prediction. When defining a classification approach that combines general-language and domain-specific word embeddings, we go beyond previous work and align vector spaces to gain comparative embeddings. We suggest two novel models to exploit general- vs. domain-specific comparisons: a simple neural network model with pre-computed comparative-embedding information as input, and a multi-channel model computing the comparison internally. Both models outperform previous approaches, with the multi-channel model performing best."
W19-4803,Second-order Co-occurrence Sensitivity of Skip-Gram with Negative Sampling,2019,0,0,3,1,630,dominik schlechtweg,Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP,0,"We simulate first- and second-order context overlap and show that Skip-Gram with Negative Sampling is similar to Singular Value Decomposition in capturing second-order co-occurrence information, while Pointwise Mutual Information is agnostic to it. We support the results with an empirical study finding that the models react differently when provided with additional second-order information. Our findings reveal a basic property of Skip-Gram with Negative Sampling and point towards an explanation of its success on a variety of tasks."
W19-0506,Distributional Interaction of Concreteness and Abstractness in Verb{--}Noun Subcategorisation,2019,-1,-1,2,1,627,diego frassinelli,Proceedings of the 13th International Conference on Computational Semantics - Short Papers,0,"In recent years, both cognitive and computational research has provided empirical analyses of contextual co-occurrence of concrete and abstract words, partially resulting in inconsistent pictures. In this work we provide a more fine-grained description of the distributional nature in the corpus-based interaction of verbs and nouns within subcategorisation, by investigating the concreteness of verbs and nouns that are in a specific syntactic relationship with each other, i.e., subject, direct object, and prepositional object. Overall, our experiments show consistent patterns in the distributional representation of subcategorising and subcategorised concrete and abstract words. At the same time, the studies reveal empirical evidence why contextual abstractness represents a valuable indicator for automatic non-literal language identification."
S19-1001,{SUR}el: A Gold Standard for Incorporating Meaning Shifts into Term Extraction,2019,0,3,3,1,1000,anna hatty,Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*{SEM} 2019),0,"We introduce SURel, a novel dataset with human-annotated meaning shifts between general-language and domain-specific contexts. We show that meaning shifts of term candidates cause errors in term extraction, and demonstrate that the SURel annotation reflects these errors. Furthermore, we illustrate that SURel enables us to assess optimisations of term extraction techniques when incorporating meaning shifts."
P19-1072,A Wind of Change: Detecting and Evaluating Lexical Semantic Change across Times and Domains,2019,0,4,4,1,630,dominik schlechtweg,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"We perform an interdisciplinary large-scale evaluation for detecting lexical semantic divergences in a diachronic and in a synchronic task: semantic sense changes across time, and semantic sense changes across domains. Our work addresses the superficialness and lack of comparison in assessing models of diachronic lexical change, by bringing together and extending benchmark models on a common state-of-the-art evaluation task. In addition, we demonstrate that the same evaluation task and modelling approaches can successfully be utilised for the synchronic detection of domain-specific sense divergences in the field of term extraction."
D19-1477,You Shall Know a User by the Company It Keeps: Dynamic Representations for Social Media Users in {NLP},2019,0,1,3,0,21574,marco tredici,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"Information about individuals can help to better understand what they say, particularly in social media where texts are short. Current approaches to modelling social media users pay attention to their social connections, but exploit this information in a static way, treating all connections uniformly. This ignores the fact, well known in sociolinguistics, that an individual may be part of several communities which are not equally relevant in all communicative situations. We present a model based on Graph Attention Networks that captures this observation. It dynamically explores the social graph of a user, computes a user representation given the most relevant connections for a target task, and combines it with linguistic information to make a prediction. We apply our model to three different tasks, evaluate it against alternative models, and analyse the results extensively, showing that it significantly outperforms other current methods."
W18-4909,Fine-Grained Termhood Prediction for {G}erman Compound Terms Using Neural Networks,2018,0,1,2,1,1000,anna hatty,"Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions ({LAW}-{MWE}-{C}x{G}-2018)",0,"Automatic term identification and investigating the understandability of terms in a specialized domain are often treated as two separate lines of research. We propose a combined approach for this matter, by defining fine-grained classes of termhood and framing a classification task. The classes reflect tiers of a term{'}s association to a domain. The new setup is applied to German closed compounds as term candidates in the domain of cooking. For the prediction of the classes, we compare several neural network architectures and also take salient information about the compounds{'} components into account. We show that applying a similar class distinction to the compounds{'} components and propagating this information within the network improves the compound class prediction results."
W18-0705,Integrating Predictions from Neural-Network Relation Classifiers into Coreference and Bridging Resolution,2018,0,0,4,0,28149,ina roesiger,"Proceedings of the First Workshop on Computational Models of Reference, Anaphora and Coreference",0,Cases of coreference and bridging resolution often require knowledge about semantic relations between anaphors and antecedents. We suggest state-of-the-art neural-network classifiers trained on relation benchmarks to predict and integrate likelihoods for relations. Two experiments with representations differing in noise and complexity improve our bridging but not our coreference resolver.
S18-2003,Assessing Meaning Components in {G}erman Complex Verbs: A Collection of Source-Target Domains and Directionality,2018,0,1,1,1,631,sabine walde,Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics,0,"This paper presents a collection to assess meaning components in German complex verbs, which frequently undergo meaning shifts. We use a novel strategy to obtain source and target domain characterisations via sentence generation rather than sentence annotation. A selection of arrows adds spatial directional information to the generated contexts. We provide a broad qualitative description of the dataset, and a series of standard classification experiments verifies the quantitative reliability of the presented resource. The setup for collecting the meaning components is applicable also to other languages, regarding complex verbs as well as other language-specific targets that involve meaning shifts."
S18-2008,Quantitative Semantic Variation in the Contexts of Concrete and Abstract Words,2018,0,1,3,0,28717,daniela naumann,Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics,0,"Across disciplines, researchers are eager to gain insight into empirical features of abstract vs. concrete concepts. In this work, we provide a detailed characterisation of the distributional nature of abstract and concrete words across 16,620 English nouns, verbs and adjectives. Specifically, we investigate the following questions: (1) What is the distribution of concreteness in the contexts of concrete and abstract target words? (2) What are the differences between concrete and abstract words in terms of contextual semantic diversity? (3) How does the entropy of concrete and abstract word contexts differ? Overall, our studies show consistent differences in the distributional representation of concrete and abstract words, thus challenging existing theories of cognition and providing a more fine-grained description of their nature."
P18-1231,Bilingual Sentiment Embeddings: Joint Projection of Sentiment Across Languages,2018,24,2,3,1,2620,jeremy barnes,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Sentiment analysis in low-resource languages suffers from a lack of annotated corpora to estimate high-performing models. Machine translation and bilingual word embeddings provide some relief through cross-lingual sentiment approaches. However, they either require large amounts of parallel data or do not sufficiently capture sentiment information. We introduce Bilingual Sentiment Embeddings (BLSE), which jointly represent sentiment information in a source and target language. This model only requires a small bilingual lexicon, a source-language corpus annotated for sentiment, and monolingual word embeddings for each language. We perform experiments on three language combinations (Spanish, Catalan, Basque) for sentence-level cross-lingual sentiment classification and find that our model significantly outperforms state-of-the-art methods on four out of six experimental setups, as well as capturing complementary information to machine translation. Our analysis of the resulting embedding space provides evidence that it represents sentiment information in the resource-poor target language without any annotated data in that language."
N18-4002,Combining Abstractness and Language-specific Theoretical Indicators for Detecting Non-Literal Usage of {E}stonian Particle Verbs,2018,0,0,3,0,15439,eleri aedmaa,Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Student Research Workshop,0,"This paper presents two novel datasets and a random-forest classifier to automatically predict literal vs. non-literal language usage for a highly frequent type of multi-word expression in a low-resource language, i.e., Estonian. We demonstrate the value of language-specific indicators induced from theoretical linguistic research, which outperform a high majority baseline when combined with language-independent features of non-literal language (such as abstractness)."
N18-2024,Analogies in Complex Verb Meaning Shifts: the Effect of Affect in Semantic Similarity Models,2018,0,0,2,1,28599,maximilian koper,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",0,"We present a computational model to detect and distinguish analogies in meaning shifts between German base and complex verbs. In contrast to corpus-based studies, a novel dataset demonstrates that {``}regular{''} shifts represent the smallest class. Classification experiments relying on a standard similarity model successfully distinguish between four types of shifts, with verb classes boosting the performance, and affective features for abstractness, emotion and sentiment representing the most salient indicators."
N18-2027,Diachronic Usage Relatedness ({DUR}el): A Framework for the Annotation of Lexical Semantic Change,2018,12,4,2,1,630,dominik schlechtweg,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",0,"We propose a framework that extends synchronic polysemy annotation to diachronic changes in lexical meaning, to counteract the lack of resources for evaluating computational models of lexical semantic change. Our framework exploits an intuitive notion of semantic relatedness, and distinguishes between innovative and reductive meaning changes with high inter-annotator agreement. The resulting test set for German comprises ratings from five annotators for the relatedness of 1,320 use pairs across 22 target words."
N18-2032,Introducing Two {V}ietnamese Datasets for Evaluating Semantic Models of (Dis-)Similarity and Relatedness,2018,26,0,2,1,28018,kim nguyen,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",0,"We present two novel datasets for the low-resource language Vietnamese to assess models of semantic similarity: ViCon comprises pairs of synonyms and antonyms across word classes, thus offering data to distinguish between similarity and dissimilarity. ViSim-400 provides degrees of similarity across five semantic relations, as rated by human judges. The two datasets are verified through standard co-occurrence and neural network models, showing results comparable to the respective English datasets."
N18-2052,A Laypeople Study on Terminology Identification across Domains and Task Definitions,2018,0,2,2,1,1000,anna hatty,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",0,"This paper introduces a new dataset of term annotation. Given that even experts vary significantly in their understanding of termhood, and that term identification is mostly performed as a binary task, we offer a novel perspective to explore the common, natural understanding of what constitutes a term: Laypeople annotate single-word and multi-word terms, across four domains and across four task definitions. Analyses based on inter-annotator agreement offer insights into differences in term specificity, term granularity and subtermhood."
C18-1070,Projecting Embeddings for Domain Adaption: Joint Modeling of Sentiment Analysis in Diverse Domains,2018,23,4,3,1,2620,jeremy barnes,Proceedings of the 27th International Conference on Computational Linguistics,0,"Domain adaptation for sentiment analysis is challenging due to the fact that supervised classifiers are very sensitive to changes in domain. The two most prominent approaches to this problem are structural correspondence learning and autoencoders. However, they either require long training times or suffer greatly on highly divergent domains. Inspired by recent advances in cross-lingual sentiment analysis, we provide a novel perspective and cast the domain adaptation problem as an embedding projection task. Our model takes as input two mono-domain embedding spaces and learns to project them to a bi-domain space, which is jointly optimized to (1) project across domains and to (2) predict sentiment. We perform domain adaptation experiments on 20 source-target domain pairs for sentiment classification and report novel state-of-the-art results on 11 domain pairs, including the Amazon domain adaptation datasets and SemEval 2013 and 2016 datasets. Our analysis shows that our model performs comparably to state-of-the-art approaches on domains that are similar, while performing significantly better on highly divergent domains. Our code is available at https://github.com/jbarnesspain/domain{\_}blse"
W17-7101,Exploring Multi-Modal {T}ext+{I}mage Models to Distinguish between Abstract and Concrete Nouns,2017,-1,-1,3,0,23987,sai bhaskar,Proceedings of the {IWCS} workshop on Foundations of Situated and Multimodal Communication,0,None
W17-6910,Contextual Characteristics of Concrete and Abstract Words,2017,16,1,4,1,627,diego frassinelli,{IWCS} 2017 {---} 12th International Conference on Computational Semantics {---} Short papers,0,None
W17-6942,Exploring Soft-Clustering for {G}erman (Particle) Verbs across Frequency Ranges,2017,26,0,3,1,31332,moritz wittmann,{IWCS} 2017 {---} 12th International Conference on Computational Semantics {---} Short papers,0,None
W17-5202,Assessing State-of-the-Art Sentiment Models on State-of-the-Art Sentiment Datasets,2017,2,7,3,1,2620,jeremy barnes,"Proceedings of the 8th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis",0,"There has been a good amount of progress in sentiment analysis over the past 10 years, including the proposal of new methods and the creation of benchmark datasets. In some papers, however, there is a tendency to compare models only on one or two datasets, either because of time restraints or because the model is tailored to a specific task. Accordingly, it is hard to understand how well a certain model generalizes across different tasks and datasets. In this paper, we contribute to this situation by comparing several models on six different benchmarks, which belong to different domains and additionally have different levels of granularity (binary, 3-class, 4-class and 5-class). We show that Bi-LSTMs perform well across datasets and that both LSTMs and Bi-LSTMs are particularly good at fine-grained sentiment tasks (\textit{i.e.}, with more than two classes). Incorporating sentiment information into word embeddings during training gives good results for datasets that are lexically similar to the training data. With our experiments, we contribute to a better understanding of the performance of different model architectures on different data sets. Consequently, we detect novel state-of-the-art results on the \textit{SenTube} datasets."
W17-1903,"Improving Verb Metaphor Detection by Propagating Abstractness to Words, Phrases and Individual Senses",2017,16,3,2,1,28599,maximilian koper,"Proceedings of the 1st Workshop on Sense, Concept and Entity Representations and their Applications",0,"Abstract words refer to things that can not be seen, heard, felt, smelled, or tasted as opposed to concrete words. Among other applications, the degree of abstractness has been shown to be a useful information for metaphor detection. Our contribution to this topic are as follows: i) we compare supervised techniques to learn and extend abstractness ratings for huge vocabularies ii) we learn and investigate norms for larger units by propagating abstractness to verb-noun pairs which lead to better metaphor detection iii) we overcome the limitation of learning a single rating per word and show that multi-sense abstractness ratings are potentially useful for metaphor detection. Finally, with this paper we publish automatically created abstractness norms for 3million English words and multi-words as well as automatically created sense specific abstractness ratings"
W17-1708,Factoring Ambiguity out of the Prediction of Compositionality for {G}erman Multi-Word Expressions,2017,27,0,2,1,32029,stefan bott,Proceedings of the 13th Workshop on Multiword Expressions ({MWE} 2017),0,"Ambiguity represents an obstacle for distributional semantic models(DSMs), which typically subsume the contexts of all word senses within one vector. While individual vector space approaches have been concerned with sense discrimination (e.g., Sch{\""u}tze 1998, Erk 2009, Erk and Pado 2010), such discrimination has rarely been integrated into DSMs across semantic tasks. This paper presents a soft-clustering approach to sense discrimination that filters sense-irrelevant features when predicting the degrees of compositionality for German noun-noun compounds and German particle verbs."
W17-1728,Complex Verbs are Different: Exploring the Visual Modality in Multi-Modal Models to Predict Compositionality,2017,23,0,2,1,28599,maximilian koper,Proceedings of the 13th Workshop on Multiword Expressions ({MWE} 2017),0,"This paper compares a neural network DSM relying on textual co-occurrences with a multi-modal model integrating visual information. We focus on nominal vs. verbal compounds, and zoom into lexical, empirical and perceptual target properties to explore the contribution of the visual modality. Our experiments show that (i) visual features contribute differently for verbs than for nouns, and (ii) images complement textual information, if (a) the textual modality by itself is poor and appropriate image subsets are used, or (b) the textual modality by itself is rich and large (potentially noisy) images are added."
K17-1036,{G}erman in Flux: Detecting Metaphoric Change via Word Entropy,2017,31,2,4,1,630,dominik schlechtweg,Proceedings of the 21st Conference on Computational Natural Language Learning ({C}o{NLL} 2017),0,"This paper explores the information-theoretic measure entropy to detect metaphoric change, transferring ideas from hypernym detection to research on language change. We build the first diachronic test set for German as a standard for metaphoric change annotation. Our model is unsupervised, language-independent and generalizable to other processes of semantic change."
E17-4012,Evaluating the Reliability and Interaction of Recursively Used Feature Classes for Terminology Extraction,2017,15,1,3,1,1000,anna hatty,Proceedings of the Student Research Workshop at the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"Feature design and selection is a crucial aspect when treating terminology extraction as a machine learning classification problem. We designed feature classes which characterize different properties of terms based on distributions, and propose a new feature class for components of term candidates. By using random forests, we infer optimal features which are later used to build decision tree classifiers. We evaluate our method using the ACL RD-TEC dataset. We demonstrate the importance of the novel feature class for downgrading termhood which exploits properties of term components. Furthermore, our classification suggests that the identification of reliable term candidates should be performed successively, rather than just once."
E17-2086,Applying Multi-Sense Embeddings for {G}erman Verbs to Determine Semantic Relatedness and to Detect Non-Literal Language,2017,0,1,2,1,28599,maximilian koper,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers",0,"Up to date, the majority of computational models still determines the semantic relatedness between words (or larger linguistic units) on the type level. In this paper, we compare and extend multi-sense embeddings, in order to model and utilise word senses on the token level. We focus on the challenging class of complex verbs, and evaluate the model variants on various semantic tasks: semantic classification; predicting compositionality; and detecting non-literal language usage. While there is no overall best model, all models significantly outperform a word2vec single-sense skip baseline, thus demonstrating the need to distinguish between word senses in a distributional semantic model."
E17-2099,"Addressing Problems across Linguistic Levels in {SMT}: Combining Approaches to Model Morphology, Syntax and Lexical Choice",2017,9,0,3,1,22882,marion marco,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers",0,"Many errors in phrase-based SMT can be attributed to problems on three linguistic levels: morphological complexity in the target language, structural differences and lexical choice. We explore combinations of linguistically motivated approaches to address these problems in English-to-German SMT and show that they are complementary to one another, but also that the popular verbal pre-ordering can cause problems on the morphological and lexical level. A discriminative classifier can overcome these problems, in particular when enriching standard lexical features with features geared towards verbal inflection."
E17-1008,Distinguishing Antonyms and Synonyms in a Pattern-based Neural Network,2017,17,8,2,1,28018,kim nguyen,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers",0,"Distinguishing between antonyms and synonyms is a key task to achieve high performance in NLP systems. While they are notoriously difficult to distinguish by distributional co-occurrence models, pattern-based methods have proven effective to differentiate between the relations. In this paper, we present a novel neural network model AntSynNET that exploits lexico-syntactic patterns from syntactic parse trees. In addition to the lexical and syntactic information, we successfully integrate the distance between the related words along the syntactic path as a new pattern feature. The results from classification experiments show that AntSynNET improves the performance over prior pattern-based methods."
D17-1022,Hierarchical Embeddings for Hypernymy Detection and Directionality,2017,40,4,3,1,28018,kim nguyen,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,0,"We present a novel neural model HyperVec to learn hierarchical embeddings for hypernymy detection and directionality. While previous embeddings have shown limitations on prototypical hypernyms, HyperVec represents an unsupervised measure where embeddings are learned in a specific order and capture the hypernym{--}hyponym distributional hierarchy. Moreover, our model is able to generalize over unseen hypernymy pairs, when using only small sets of training data, and by mapping to other languages. Results on benchmark datasets show that HyperVec outperforms both state-of-the-art unsupervised measures and embedding models on hypernymy detection and directionality, and on predicting graded lexical entailment."
W16-5318,{G}ho{S}t-{PV}: A Representative Gold Standard of {G}erman Particle Verbs,2016,13,2,4,1,32029,stefan bott,Proceedings of the 5th Workshop on Cognitive Aspects of the Lexicon ({C}og{AL}ex - V),0,"German particle verbs represent a frequent type of multi-word-expression that forms a highly productive paradigm in the lexicon. Similarly to other multi-word expressions, particle verbs exhibit various levels of compositionality. One of the major obstacles for the study of compositionality is the lack of representative gold standards of human ratings. In order to address this bottleneck, this paper presents such a gold standard data set containing 400 randomly selected German particle verbs. It is balanced across several particle types and three frequency bands, and accomplished by human ratings on the degree of semantic compositionality."
W16-2205,Modeling Complement Types in Phrase-Based {SMT},2016,18,0,3,1,22882,marion marco,"Proceedings of the First Conference on Machine Translation: Volume 1, Research Papers",0,"We explore two approaches to model complement types (NPs and PPs) in an Englishto-German SMT system: A simple abstract representation inserts pseudo-prepositions that mark the beginning of noun phrases, to improve the symmetry of source and target complement types, and to provide a flat structural information on phrase boundaries. An extension of this representation generates context-aware synthetic phrasetable entries conditioned on the source side, to model complement types in terms of grammatical case and preposition choice. Both the simple preposition-informed system and the context-aware system significantly improve over the baseline; and the context-aware system is slightly better than the system without context information."
W16-1805,Graph-based Clustering of Synonym Senses for {G}erman Particle Verbs,2016,19,2,3,1,31332,moritz wittmann,Proceedings of the 12th Workshop on Multiword Expressions,0,"In this paper, we address the automatic induction of synonym paraphrases for the empirically challenging class of German particle verbs. Similarly to Cocos and Callison-Burch (2016), we incorporate a graph-based clustering approach for word sense discrimination into an existing paraphrase extraction system, (i) to improve the precision of synonym identification and ranking, and (ii) to enlarge the diversity of synonym senses. Our approach significantly improves over the standard system, but does not outperform an extended baseline integrating a simple distributional similarity measure."
S16-2010,Improving Zero-Shot-Learning for {G}erman Particle Verbs by using Training-Space Restrictions and Local Scaling,2016,9,1,2,1,28599,maximilian koper,Proceedings of the Fifth Joint Conference on Lexical and Computational Semantics,0,None
S16-2020,The Role of Modifier and Head Properties in Predicting the Compositionality of {E}nglish and {G}erman Noun-Noun Compounds: A Vector-Space Perspective,2016,24,0,1,1,631,sabine walde,Proceedings of the Fifth Joint Conference on Lexical and Computational Semantics,0,None
P16-2042,Automatic Semantic Classification of {G}erman Preposition Types: Comparing Hard and Soft Clustering Approaches across Features,2016,39,0,2,1,28599,maximilian koper,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"This paper addresses an automatic classification of preposition types in German, comparing hard and soft clustering approaches and various window- and syntax-based co-occurrence features. We show that (i) the semantically most salient preposition features (i.e., subcategorised nouns) are the most successful, and that (ii) soft clustering approaches are required for the task but reveal quite different attitudes towards predicting ambiguity."
P16-2074,Integrating Distributional Lexical Contrast into Word Embeddings for Antonym-Synonym Distinction,2016,20,5,2,1,28018,kim nguyen,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"We propose a novel vector representation that integrates lexical contrast into distributional vectors and strengthens the most salient features for determining degrees of word similarity. The improved vectors significantly outperform standard models and distinguish antonyms from synonyms with an average precision of 0.66-0.76 across word classes (adjectives, nouns, verbs). Moreover, we integrate the lexical contrast vectors into the objective function of a skip-gram model. The novel embedding outperforms state-of-the-art models on predicting word similarities in SimLex-999, and on distinguishing antonyms from synonyms."
N16-1039,Distinguishing Literal and Non-Literal Usage of {G}erman Particle Verbs,2016,14,5,2,1,28599,maximilian koper,Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,None
L16-1191,Visualisation and Exploration of High-Dimensional Distributional Features in Lexical Semantic Classification,2016,0,1,5,1,28599,maximilian koper,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Vector space models and distributional information are widely used in NLP. The models typically rely on complex, high-dimensional objects. We present an interactive visualisation tool to explore salient lexical-semantic features of high-dimensional word objects and word similarities. Most visualisation tools provide only one low-dimensional map of the underlying data, so they are not capable of retaining the local and the global structure. We overcome this limitation by providing an additional trust-view to obtain a more realistic picture of the actual object distances. Additional tool options include the reference to a gold standard classification, the reference to a cluster analysis as well as listing the most salient (common) features for a selected subset of the words."
L16-1362,{G}ho{S}t-{NN}: A Representative Gold Standard of {G}erman Noun-Noun Compounds,2016,0,9,1,1,631,sabine walde,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"This paper presents a novel gold standard of German noun-noun compounds (Ghost-NN) including 868 compounds annotated with corpus frequencies of the compounds and their constituents, productivity and ambiguity of the constituents, semantic relations between the constituents, and compositionality ratings of compound-constituent pairs. Moreover, a subset of the compounds containing 180 compounds is balanced for the productivity of the modifiers (distinguishing low/mid/high productivity) and the ambiguity of the heads (distinguishing between heads with 1, 2 and {\textgreater}2 senses"
L16-1413,"Automatically Generated Affective Norms of Abstractness, Arousal, Imageability and Valence for 350 000 {G}erman Lemmas",2016,0,10,2,1,28599,maximilian koper,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"This paper presents a collection of 350,000 German lemmatised words, rated on four psycholinguistic affective attributes. All ratings were obtained via a supervised learning algorithm that can automatically calculate a numerical rating of a word. We applied this algorithm to abstractness, arousal, imageability and valence. Comparison with human ratings reveals high correlation across all rating types. The full resource is publically available at: http://www.ims.uni-stuttgart.de/data/affective{\_}norms/"
C16-1254,Neural-based Noise Filtering from Word Embeddings,2016,19,1,2,1,28018,kim nguyen,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"Word embeddings have been demonstrated to benefit NLP tasks impressively. Yet, there is room for improvements in the vector representations, because current word embeddings typically contain unnecessary information, i.e., noise. We propose two novel models to improve word embeddings by unsupervised learning, in order to yield word denoising embeddings. The word denoising embeddings are obtained by strengthening salient information and weakening noise in the original word embeddings, based on a deep feed-forward neural network filter. Results from benchmark tasks show that the filtered word denoising embeddings outperform the original word embeddings."
W15-4923,Target-Side Generation of Prepositions for {SMT},2015,16,3,3,0.985062,36553,marion weller,Proceedings of the 18th Annual Conference of the {E}uropean Association for Machine Translation,0,"We present a translation system that models the selection of prepositions in a targetside generation component. This novel approach allows the modeling of all subcategorized elements of a verb as either NPs or PPs according to target-side requirements relying on source and target side features. The BLEU scores are encouraging, but fail to surpass the baseline. We additionally evaluate the preposition accuracy for a carefully selected subset and discuss how typical problems of translating prepositions can be modeled with our method."
W15-1008,Predicting Prepositions for {SMT},2015,4,0,3,0.985062,36553,marion weller,"Proceedings of the Ninth Workshop on Syntax, Semantics and Structure in Statistical Translation",0,"Representation and Prediction Features Initial experiments showed that replacing prepositions by simple place-holders decreases the translation quality. As an extension to the basic approach with plain place-holders, we thus experiment with enriching the place-holders such that they contain more relevant information and represent the content of a preposition while still being in an abstract form. For example, the representation can be enriched by annotating the place-holder with the grammatical case of the preposition it represents: for overt prepositions, case is often an indicator of the content (e.g. direction/location), whereas for NPs, case indicates"
W15-0903,How to Account for Idiomatic {G}erman Support Verb Constructions in Statistical Machine Translation,2015,29,8,4,0.416667,30805,fabienne cap,Proceedings of the 11th Workshop on Multiword Expressions,0,"Support-verb constructions (i.e., multiword expressions combining a semantically light verb with a predicative noun) are problematic for standard statistical machine translation systems, because SMT systems cannot distinguish between literal and idiomatic uses of the verb. We work on the German to English translation direction, for which the identification of support-verb constructions is challenging due to the relatively free word order of German. We show that we achieve improved translation quality for verb-object supportverb constructions by marking the verbs when occuring in such constructions. Additional evaluations revealed that our systems produce more correct verb translations than a contrastive baseline system without verb markup."
W15-0104,Exploiting Fine-grained Syntactic Transfer Features to Predict the Compositionality of {G}erman Particle Verbs,2015,16,5,2,1,32029,stefan bott,Proceedings of the 11th International Conference on Computational Semantics,0,"This article presents a distributional approach to predict the compositionality of German particle verbs by modelling changes in syntactic argument structure. We justify the experiments on theoretical grounds and employ GermaNet, Topic Models and Singular Value Decomposition for generalization, to compensate for data sparseness. Evaluating against three human-rated gold standards, our finegrained syntactic approach is able to predict the level of compositionality of the particle verbs but is nevertheless inferior to a coarse-grained bag-of-words approach."
W15-0105,Multilingual Reliability and {``}Semantic{''} Structure of Continuous Word Spaces,2015,20,20,3,1,28599,maximilian koper,Proceedings of the 11th International Conference on Computational Semantics,0,"While continuous word vector representations enjoy increasing popularity, it is still poorly understood (i) how reliable they are for other languages than English, and (ii) to what extent they encode deep semantic relatedness such as paradigmatic relations. This study presents experiments with continuous word vectors for English and German, a morphologically rich language. For evaluation, we use both published and newly created datasets of morpho-syntactic and semantic relations. Our results show that (i) morphological complexity causes a drop in accuracy, and (ii) continuous representations lack the ability to solve analogies of paradigmatic relations."
W14-5814,"A Database of Paradigmatic Semantic Relation Pairs for {G}erman Nouns, Verbs, and Adjectives",2014,17,7,2,1,38217,silke scheible,Proceedings of Workshop on Lexical and Grammatical Resources for Language Processing,0,"A new collection of semantically related word pairs in German is presented, which was compiled via human judgement experiments and comprises (i) a representative selection of target lexical units balanced for semantic category, polysemy, and corpus frequency, (ii) a set of humangenerated semantically related word pairs based on the target units, and (iii) a subset of the generated word pairs rated for their relation strength, including positive and negative relation evidence. We address the three paradigmatic relations antonymy, hypernymy and synonymy, and systematically work across the three word classes of adjectives, nouns, and verbs. A series of quantitative and qualitative analyses demonstrates that (i) antonyms are more canonical than hypernyms and synonyms, (ii) relations are more or less natural with regard to the specific word classes, (iii) antonymy is clearly distinguishable from hypernymy and synonymy, but hypernymy and synonymy are often confused. We anticipate that our new collection of semantic relation pairs will not only be of considerable use in computational areas in which semantic relations play a role, but also in studies in theoretical linguistics and psycholinguistics."
W14-5701,Modelling Regular Subcategorization Changes in {G}erman Particle Verbs,2014,17,0,2,1,32029,stefan bott,Proceedings of the First Workshop on Computational Approaches to Compound Analysis ({C}om{AC}om{A} 2014),0,"German particle verbs are a type of multi word expression which is often compositional with respect to a base verb. If they are compositional they tend to express the same types of semantic arguments, but they do not necessarily express them in the same syntactic subcategorization frame: some arguments may be expressed by differing syntactic subcategorization slots and other arguments may be only implicit in either the base or the particle verb. In this paper we present a method which predicts syntactic slot correspondences between syntactic slots of base and particle verb pairs. We can show that this method can predict subcategorization slot correspondences with a fair degree of success."
W14-5709,Distinguishing Degrees of Compositionality in Compound Splitting for Statistical Machine Translation,2014,28,12,4,0.985062,36553,marion weller,Proceedings of the First Workshop on Computational Approaches to Compound Analysis ({C}om{AC}om{A} 2014),0,"The paper presents an approach to morphological compound splitting that takes the degree of compositionality into account. We apply our approach to German noun compounds and particle verbs within a Germanxe2x80x90English SMT system, and study the effect of only splitting compositional compounds as opposed to an aggressive splitting. A qualitative study explores the translational behaviour of non-compositional compounds."
W14-0818,Feature Norms of {G}erman Noun Compounds,2014,9,3,2,1,3621,stephen roller,Proceedings of the 10th Workshop on Multiword Expressions ({MWE}),0,"This paper presents a new data collection of feature norms for 572 German nounnoun compounds. The feature norms complement existing data sets for the same targets, including compositionality ratings, association norms, and images. We demonstrate that the feature norms are potentially useful for research on the nounnoun compounds and their semantic transparency: The feature overlap of the compounds and their constituents correlates with human ratings on the compoundxe2x80x90 constituent degrees of compositionality, = 0.46."
S14-1020,Contrasting Syntagmatic and Paradigmatic Relations: Insights from Distributional Semantic Models,2014,30,15,3,0.25,628,gabriella lapesa,Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*{SEM} 2014),0,"This paper presents a large-scale evaluation of bag-of-words distributional models on two datasets from priming experiments involving syntagmatic and paradigmatic relations. We interpret the variation in performance achieved by different settings of the model parameters as an indication of which aspects of distributional patterns characterize these types of relations. Contrary to what has been argued in the literature (Rapp, 2002; Sahlgren, 2006) xe2x80x90 that bag-of-words models based on secondorder statistics mainly capture paradigmatic relations and that syntagmatic relations need to be gathered from first-order models xe2x80x90 we show that second-order models perform well on both paradigmatic and syntagmatic relations if their parameters are properly tuned. In particular, our results show that size of the context window and dimensionality reduction play a key role in differentiating DSM performance on paradigmatic vs. syntagmatic relations."
S14-1022,Syntactic Transfer Patterns of {G}erman Particle Verbs and their Impact on Lexical Semantics,2014,25,3,2,1,32029,stefan bott,Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*{SEM} 2014),0,"German particle verbs, like anblicken (to gaze at) combine a base verb (blicken) with a particle (an) to form a special kind of Multi Word Expression. Particle verbs may share the semantics of the base verb and the particle to a variable degree. However, while syntactic subcategorization frames tend to be good predictor for the semantics of verbs in general (verbs that are similar in meaning also tend to have similar subcategorization frames and selectional preferences), there are regular changes in subcategorization frames by particle verbs with regard to the corresponding base verbs. This paper demonstrates that the syntactic behavior of particle verbs and base verbs together (modeling regular changes in subcategorization frames by particle verbs and corresponding base verbs) and applying clustering techniques allows us to distinguish particle verb meaning and shows the tight connection between transfer patterns and the semantic classes of particle verbs."
P14-2086,Combining Word Patterns and Discourse Markers for Paradigmatic Relation Classification,2014,38,9,2,0.333333,660,michael roth,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Distinguishing between paradigmatic relations such as synonymy, antonymy and hypernymy is an important prerequisite in a range of NLP applications. In this paper, we explore discourse relations as an alternative set of features to lexico-syntactic patterns. We demonstrate that statistics over discourse relations, collected via explicit discourse markers as proxies, can be utilized as salient indicators for paradigmatic relations in multiple languages, outperforming patterns in terms of recall and F1-score. In addition, we observe that markers and patterns provide complementary information, leading to significant classification improvements when applied in combination."
wittmann-etal-2014-automatic,Automatic Extraction of Synonyms for {G}erman Particle Verbs from Parallel Data with Distributional Similarity as a Re-Ranking Feature,2014,20,1,3,1,31332,moritz wittmann,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"We present a method for the extraction of synonyms for German particle verbs based on a word-aligned German-English parallel corpus: by translating the particle verb to a pivot, which is then translated back, a set of synonym candidates can be extracted and ranked according to the respective translation probabilities. In order to deal with separated particle verbs, we apply re-ordering rules to the German part of the data. In our evaluation against a gold standard, we compare different pre-processing strategies (lemmatized vs. inflected forms) and introduce language model scores of synonym candidates in the context of the input particle verb as well as distributional similarity as additional re-ranking criteria. Our evaluation shows that distributional similarity as a re-ranking feature is more robust than language model scores and leads to an improved ranking of the synonym candidates. In addition to evaluating against a gold standard, we also present a small-scale manual evaluation."
koper-schulte-im-walde-2014-rank,A Rank-based Distance Measure to Detect Polysemy and to Determine Salient Vector-Space Features for {G}erman Prepositions,2014,26,4,2,1,28599,maximilian koper,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"This paper addresses vector space models of prepositions, a notoriously ambiguous word class. We propose a rank-based distance measure to explore the vector-spatial properties of the ambiguous objects, focusing on two research tasks: (i) to distinguish polysemous from monosemous prepositions in vector space; and (ii) to determine salient vector-space features for a classification of preposition senses. The rank-based measure predicts the polysemy vs. monosemy of prepositions with a precision of up to 88{\%}, and suggests preposition-subcategorised nouns as more salient preposition features than preposition-subcategorising verbs."
utt-etal-2014-fuzzy,Fuzzy {V}-Measure - An Evaluation Method for Cluster Analyses of Ambiguous Data,2014,7,4,4,0.743863,31309,jason utt,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"This paper discusses an extension of the V-measure (Rosenberg and Hirschberg, 2007), an entropy-based cluster evaluation metric. While the original work focused on evaluating hard clusterings, we introduce the Fuzzy V-measure which can be used on data that is inherently ambiguous. We perform multiple analyses varying the sizes and ambiguity rates and show that while entropy-based measures in general tend to suffer when ambiguity increases, a measure with desirable properties can be derived from these in a straightforward manner."
bott-schulte-im-walde-2014-optimizing,Optimizing a Distributional Semantic Model for the Prediction of {G}erman Particle Verb Compositionality,2014,14,8,2,1,32029,stefan bott,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"In the work presented here we assess the degree of compositionality of German Particle Verbs with a Distributional Semantics Model which only relies on word window information and has no access to syntactic information as such. Our method only takes the lexical distributional distance between the Particle Verb to its Base Verb as a predictor for compositionality. We show that the ranking of distributional similarity correlates significantly with the ranking of human judgements on semantic compositionality for a series of Particle Verbs and the Base Verbs they are derived from. We also investigate the influence of further linguistic factors, such as the ambiguity and the overall frequency of the verbs and a syntactically separate occurrences of verbs and particles that causes difficulties for the correct lemmatization of Particle Verbs. We analyse in how far these factors may influence the success with which the compositionality of the Particle Verbs may be predicted."
E14-4008,Chasing Hypernyms in Vector Spaces with Entropy,2014,20,58,4,0.193237,181,enrico santus,"Proceedings of the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics, volume 2: Short Papers",0,"In this paper, we introduce SLQS , a new entropy-based measure for the unsupervised identification of hypernymy and its directionality in Distributional Semantic Models (DSMs). SLQS is assessed through two tasks: (i.) identifying the hypernym in hyponym-hypernym pairs, and (ii.) discriminating hypernymy among various semantic relations. In both tasks, SLQS outperforms other state-of-the-art measures."
2014.amta-researchers.21,Using noun class information to model selectional preferences for translating prepositions in {SMT},2014,-1,-1,2,0.985062,36553,marion weller,Proceedings of the 11th Conference of the Association for Machine Translation in the Americas: MT Researchers Track,0,"Translating prepositions is a difficult and under-studied problem in SMT. We present a novel method to improve the translation of prepositions by using noun classes to model their selectional preferences. We compare three variants of noun class information: (i) classes induced from the lexical resource GermaNet or obtained from clusterings based on either (ii) window information or (iii) syntactic features. Furthermore, we experiment with PP rule generalization. While we do not significantly improve over the baseline, our results demonstrate that (i) integrating selectional preferences as rigid class annotation in the parse tree is sub-optimal, and that (ii) clusterings based on window co-occurrence are more robust than syntax-based clusters or GermaNet classes for the task of modeling selectional preferences."
W13-3813,Potential and limits of distributional approaches for semantic relatedness,2013,0,0,1,1,631,sabine walde,Proceedings of the Joint Symposium on Semantic Processing. Textual Inference and Structures in Corpora,0,None
W13-1005,The (Un)expected Effects of Applying Standard Cleansing Models to Human Ratings on Compositionality,2013,19,8,2,1,3621,stephen roller,Proceedings of the 9th Workshop on Multiword Expressions,0,"Human ratings are an important source for evaluating computational models that predict compositionality, but like many data sets of human semantic judgements, are often fraught with uncertainty and noise. However, despite their importance, to our knowledge there has been no extensive look at the effects of cleansing methods on human rating data. This paper assesses two standard cleansing approaches on two sets of compositionality ratings for German noun-noun compounds, in their ability to produce compositionality ratings of higher consistency, while reducing data quantity. We find (i) that our ratings are highly robust against aggressive filtering; (ii) Z-score filtering fails to detect unreliable item ratings; and (iii) Minimum Subject Agreement is highly effective at detecting unreliable subjects."
W13-0120,Regular Meaning Shifts in {G}erman Particle Verbs: A Case Study,2013,20,0,3,1,28715,sylvia springorum,Proceedings of the 10th International Conference on Computational Semantics ({IWCS} 2013) {--} Long Papers,0,"This paper provides a corpus-based study on German particle verbs. We hypothesize that there are regular mechanisms in meaning shifts of a base verb in combination with a particle that do not only apply to the individual verb, but across a semantically coherent set of verbs. For example, the syntactically similar base verbs brummen xe2x80x98humxe2x80x99 and donnern xe2x80x98rumblexe2x80x99 both describe an irritating, displeasing loud sound. Combined with the particle auf, they result in near-synonyms roughly meaning xe2x80x98forcefully assigning a taskxe2x80x99 (in one of their senses). Covering 6 base verb groups and 3 particles with 4 particle meanings, we demonstrate that corpus-based information on the verbsxe2x80x99 subcategorization frames plus conceptual properties of the nominal complements is a sufficient basis for defining such meaning shifts. While the paper is considerably more extensive than earlier related work, we view it as a case study toward a more automatic approach to identify and formalize meaning shifts in German particle verbs."
S13-1038,Exploring Vector Space Models to Predict the Compositionality of {G}erman Noun-Noun Compounds,2013,30,26,1,1,631,sabine walde,"Second Joint Conference on Lexical and Computational Semantics (*{SEM}), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity",0,"This paper explores two hypotheses regarding vector space models that predict the compositionality of German noun-noun compounds: (1) Against our intuition, we demonstrate that window-based rather than syntax-based distributional features perform better predictions, and that not adjectives or verbs but nouns represent the most salient part-of-speech. Our overall best result is state-of-the-art, reaching Spearmanxe2x80x99s = 0.65 with a wordspace model of nominal features from a 20word window of a 1.5 billion word web corpus. (2) While there are no significant differences in predicting compoundxe2x80x90modifier vs. compoundxe2x80x90head ratings on compositionality, we show that the modifier (rather than the head) properties predominantly influence the degree of compositionality of the compound."
P13-1058,Using subcategorization knowledge to improve case prediction for translation to {G}erman,2013,34,14,3,0.906768,36553,marion weller,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"This paper demonstrates the need and impact of subcategorization information for SMT. We combine (i) features on sourceside syntactic subcategorization and (ii) an external knowledge base with quantitative, dependency-based information about target-side subcategorization frames. A manual evaluation of an English-toGerman translation task shows that the subcategorization information has a positive impact on translation quality through better prediction of case."
I13-1056,Uncovering Distributional Differences between Synonyms and Antonyms in a Word Space Model,2013,29,14,2,1,38217,silke scheible,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,"For many NLP applications such as Information Extraction and Sentiment Detection, it is of vital importance to distinguish between synonyms and antonyms. While the general assumption is that distributional models are not suitable for this task, we demonstrate that using suitable features, differences in the contexts of synonymous and antonymous German adjective pairs can be identified with a simple word space model. Experimenting with two context settings (a simple windowbased model and a xe2x80x98co-disambiguation modelxe2x80x99 to approximate adjective sense disambiguation), our best model significantly outperforms the 50% baseline and achieves 70.6% accuracy in a synonym/antonym classification task."
I13-1072,Detecting Polysemy in Hard and Soft Cluster Analyses of {G}erman Preposition Vector Spaces,2013,21,6,2,1,28715,sylvia springorum,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,"This paper presents a methodology to identify polysemous German prepositions by exploring their vector spatial properties. We apply two cluster evaluation metrics (the Silhouette Value (Kaufman and Rousseeuw, 1990) and a fuzzy version of the V-Measure (Rosenberg and Hirschberg, 2007)) as well as various correlations, to exploit hard vs. soft cluster analyses based on Self-Organising Maps. Our main hypothesis is that polysemous prepositions are outliers, and thus represent either (i) singletons or (ii) marginals of the clusters within a cluster analysis. Our analyses demonstrate that (a) in a subset of the clusterings, singletons have a tendency to contain polysemous prepositions; and (b) misclassification and cluster membership rate exhibit a moderate correlation with ambiguity rate."
D13-1115,"A Multimodal {LDA} Model integrating Textual, Cognitive and Visual Modalities",2013,49,54,2,1,3621,stephen roller,Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,0,"Recent investigations into grounded models of language have shown that holistic views of language and perception can provide higher performance than independent views. In this work, we improve a two-dimensional multimodal version of Latent Dirichlet Allocation (Andrews et al., 2009) in various ways. (1) We outperform text-only models in two different evaluations, and demonstrate that low-level visual features are directly compatible with the existing model. (2) We present a novel way to integrate visual features into the LDA model using unsupervised clusters of images. The clusters are directly interpretable and improve on our evaluation tasks. (3) We provide two novel ways to extend the bimodal models to support three or more modalities. We find that the three-, four-, and five-dimensional models significantly outperform models using only one or two modalities, and that nontextual modalities each provide separate, disjoint knowledge that cannot be forced into a shared, latent structure."
springorum-etal-2012-automatic,Automatic classification of {G}erman \\textit{an} particle verbs,2012,25,6,2,1,28715,sylvia springorum,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"The current study works at the interface of theoretical and computational linguistics to explore the semantic properties of an particle verbs, i.e., German particle verbs with the particle an. Based on a thorough analysis of the particle verbs from a theoretical point of view, we identified empirical features and performed an automatic semantic classification. A focus of the study was on the mutual profit of theoretical and empirical perspectives with respect to salient semantic properties of the an particle verbs: (a) how can we transform the theoretical insights into empirical, corpus-based features, (b) to what extent can we replicate the theoretical classification by a machine learning approach, and (c) can the computational analysis in turn deepen our insights to the semantic properties of the particle verbs? The best classification result of 70{\%} correct class assignments was reached through a GermaNet-based generalization of direct object nouns plus a prepositional phrase feature. These particle verb features in combination with a detailed analysis of the results at the same time confirmed and enlarged our knowledge about salient properties."
schulte-im-walde-etal-2012-association,Association Norms of {G}erman Noun Compounds,2012,33,7,1,1,631,sabine walde,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"This paper introduces association norms of German noun compounds as a lexical semantic resource for cognitive and computational linguistics research on compositionality. Based on an existing database of German noun compounds, we collected human associations to the compounds and their constituents within a web experiment. The current study describes the collection process and a part-of-speech analysis of the association resource. In addition, we demonstrate that the associations provide insight into the semantic properties of the compounds, and perform a case study that predicts the degree of compositionality of the experiment compound nouns, as relying on the norms. Applying a comparatively simple measure of association overlap, we reach a Spearman rank correlation coefficient of rs=0.5228; p{\textless}000001, when comparing our predictions with human judgements."
J12-3005,Modeling Regular Polysemy: A Study on the Semantic Classification of {C}atalan Adjectives,2012,81,13,2,0.423729,11383,gemma boleda,Computational Linguistics,0,"We present a study on the automatic acquisition of semantic classes for Catalan adjectives from distributional and morphological information, with particular emphasis on polysemous adjectives. The aim is to distinguish and characterize broad classes, such as qualitative (gran xe2x80x98bigxe2x80x99) and relational (pulmonar xe2x80x98pulmonaryxe2x80x99) adjectives, as well as to identify polysemous adjectives such as economic (xe2x80x98economic xe2x88xa3 cheapxe2x80x99). We specifically aim at modeling regular polysemy, that is, types of sense alternations that are shared across lemmata. To date, both semantic classes for adjectives and regular polysemy have only been sparsely addressed in empirical computational linguistics.Two main specific questions are tackled in this article. First, what is an adequate broad semantic classification for adjectives? We provide empirical support for the qualitative and relational classes as defined in theoretical work, and uncover one type of adjective that has not received enough attention, namely, the event-related class. Se..."
poesio-etal-2010-babyexp,{B}aby{E}xp: Constructing a Huge Multimodal Resource to Acquire Commonsense Knowledge Like Children Do,2010,15,1,7,0,1743,massimo poesio,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"There is by now widespread agreement that the most realistic way to construct the large-scale commonsense knowledge repositories required by natural language and artificial intelligence applications is by letting machines learn such knowledge from large quantities of data, like humans do. A lot of attention has consequently been paid to the development of increasingly sophisticated machine learning algorithms for knowledge extraction. However, the nature of the input that humans are exposed to while learning commonsense knowledge has received much less attention. The BabyExp project is collecting very dense audio and video recordings of the first 3 years of life of a baby. The corpus constructed in this way will be transcribed with automated techniques and made available to the research community. Moreover, techniques to extract commonsense conceptual knowledge incrementally from these multimodal data are also being explored within the project. The current paper describes BabyExp in general, and presents pilot studies on the feasibility of the automated audio and video transcriptions."
schulte-im-walde-2010-comparing,Comparing Computational Models of Selectional Preferences - Second-order Co-Occurrence vs. Latent Semantic Clusters,2010,22,6,1,1,631,sabine walde,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"This paper presents a comparison of three computational approaches to selectional preferences: (i) an intuitive distributional approach that uses second-order co-occurrence of predicates and complement properties; (ii) an EM-based clustering approach that models the strengths of predicate--noun relationships by latent semantic clusters (Rooth et al., 1999); and (iii) an extension of the latent semantic clusters by incorporating the MDL principle into the EM training, thus explicitly modelling the predicate--noun selectional preferences by WordNet classes (Schulte im Walde et al., 2008). Concerning the distributional approach, we were interested not only in how well the model describes selectional preferences, but moreover which second-order properties are most salient. For example, a typical direct object of the verb 'drink' is usually fluid, might be hot or cold, can be bought, might be bottled, etc. The general question we ask is: what characterises the predicate's restrictions to the semantic realisation of its complements? Our second interest lies in the actual comparison of the models: How does a very simple distributional model compare to much more complex approaches, and which representation of selectional preferences is more appropriate, using (i) second-order properties, (ii) an implicit generalisation of nouns (by clusters), or (iii) an explicit generalisation of nouns by WordNet classes within clusters? We describe various experiments on German data and two evaluations, and demonstrate that the simple distributional model outperforms the more complex cluster-based models in most cases, but does itself not always beat the powerful frequency baseline."
P08-1057,Combining {EM} Training and the {MDL} Principle for an Automatic Verb Classification Incorporating Selectional Preferences,2008,23,32,1,1,631,sabine walde,Proceedings of ACL-08: HLT,1,"This paper presents an innovative, complex approach to semantic verb classification that relies on selectional preferences as verb properties. The probabilistic verb class model underlying the semantic classes is trained by a combination of the EM algorithm and the MDL principle, providing soft clusters with two dimensions (verb senses and subcategorisation frames with selectional preferences) as a result. A language-model-based evaluation shows that after 10 training iterations the verb class model results are above the baseline results."
ivanova-etal-2008-evaluating,Evaluating a {G}erman Sketch Grammar: A Case Study on Noun Phrase Case,2008,11,7,3,0,47984,kremena ivanova,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"Word sketches are part of the Sketch Engine corpus query system. They represent automatic, corpus-derived summaries of the words grammatical and collocational behaviour. Besides the corpus itself, word sketches require a sketch grammar, a regular expression-based shallow grammar over the part-of-speech tags, to extract evidence for the properties of the targeted words from the corpus. The paper presents a sketch grammar for German, a language which is not strictly configurational and which shows a considerable amount of case syncretism, and evaluates its accuracy, which has not been done for other sketch grammars. The evaluation focuses on NP case as a crucial part of the German grammar. We present various versions of NP definitions, so demonstrating the influence of grammar detail on precision and recall."
roth-schulte-im-walde-2008-corpus,"Corpus Co-Occurrence, Dictionary and {W}ikipedia Entries as Resources for Semantic Relatedness Information",2008,21,5,2,0.333333,660,michael roth,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"Distributional, corpus-based descriptions have frequently been applied to model aspects of word meaning. However, distributional models that use corpus data as their basis have one well-known disadvantage: even though the distributional features based on corpus co-occurrence were often successful in capturing meaning aspects of the words to be described, they generally fail to capture those meaning aspects that refer to world knowledge, because coherent texts tend not to provide redundant information that is presumably available knowledge. The question we ask in this paper is whether dictionary and encyclopaedic resources might complement the distributional information in corpus data, and provide world knowledge that is missing in corpora. As test case for meaning aspects, we rely on a collection of semantic associates to German verbs and nouns. Our results indicate that a combination of the knowledge resources should be helpful in work on distributional descriptions."
D07-1018,Modelling Polysemy in Adjective Classes by Multi-Label Classification,2007,33,8,2,0.952381,11383,gemma boleda,Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({EMNLP}-{C}o{NLL}),0,"This paper assesses the role of multi-label classification in modelling polysemy for language acquisition tasks. We focus on the acquisition of semantic classes for Catalan adjectives, and show that polysemy acquisition naturally suits architectures used for multilabel classification. Furthermore, we explore the performance of information drawn from different levels of linguistic description, using feature sets based on morphology, syntax, semantics, and n-gram distribution. Finally, we demonstrate that ensemble classifiers are a powerful and adequate way to combine different types of linguistic evidence: a simple, majority voting ensemble classifier improves the accuracy from 62.5% (best single classifier) to 84%."
W06-2910,Can Human Verb Associations Help Identify Salient Features for Semantic Verb Classification?,2006,17,9,1,1,631,sabine walde,Proceedings of the Tenth Conference on Computational Natural Language Learning ({C}o{NLL}-X),0,"This paper investigates whether human associations to verbs as collected in a web experiment can help us to identify salient verb features for semantic verb classes. Assuming that the associations model aspects of verb meaning, we apply a clustering to the verbs, as based on the associations, and validate the resulting verb classes against standard approaches to semantic verb classes, i.e. GermaNet and FrameNet. Then, various clusterings of the same verbs are performed on the basis of standard corpus-based types, and evaluated against the association-based clustering as well as GermaNet and FrameNet classes. We hypothesise that the corpus-based clusterings are better if the instantiations of the feature types show more overlap with the verb associations, and that the associations therefore help to identify salient feature types."
W06-2506,Characterizing Response Types and Revealing Noun Ambiguity in {G}erman Association Norms,2006,8,7,2,0,49704,alissa melinger,Proceedings of the Workshop on Making Sense of Sense: Bringing Psycholinguistics and Computational Linguistics Together,0,"This paper presents an analysis of semantic association norms for German nouns. In contrast to prior studies, we not only collected associations elicited by written representations of target objects but also by their pictorial representations. In a first analysis, we identified systematic differences in the type and distribution of associate responses for the two presentation forms. In a second analysis, we applied a soft cluster analysis to the collected targetresponse pairs. We subsequently used the clustering to predict noun ambiguity and to discriminate senses in our target nouns."
schulte-im-walde-2006-human,Human Verb Associations as the Basis for Gold Standard Verb Classes: Validation against {G}erma{N}et and {F}rame{N}et,2006,9,5,1,1,631,sabine walde,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"We describe a gold standard for semantic verb classes which is based on human associations to verbs. The associations were collected in a web experiment and then applied as verb features in a hierarchical cluster analysis. We claim that the resulting classes represent a theory-independent gold standard classification which covers a variety of semantic verb relations, and whose features can be used to guide the feature selection in automatic processes. To evaluate our claims, the association-based classification is validated against two standard approaches to semantic verb classes, GermaNet and FrameNet."
J06-2001,Experiments on the Automatic Induction of {G}erman Semantic Verb Classes,2006,108,157,1,1,631,sabine walde,Computational Linguistics,0,"This article presents clustering experiments on German verbs: A statistical grammar model for German serves as the source for a distributional verb description at the lexical syntax-semantics interface, and the unsupervised clustering algorithm k-means uses the empirical verb properties to perform an automatic induction of verb classes. Various evaluation measures are applied to compare the clustering results to gold standard German semantic verb classes under different criteria. The primary goals of the experiments are (1) to empirically utilize and investigate the well-established relationship between verb meaning and verb behavior within a cluster analysis and (2) to investigate the required technical parameters of a cluster analysis with respect to this specific linguistic task. The clustering methodology is developed on a small-scale verb set and then applied to a larger-scale verb set including 883 German verbs."
W05-1009,Morphology vs. Syntax in Adjective Class Acquisition,2005,12,6,3,0.952381,11383,gemma boleda,Proceedings of the {ACL}-{SIGLEX} Workshop on Deep Lexical Acquisition,0,"This paper discusses the role of morphological and syntactic information in the automatic acquisition of semantic classes for Catalan adjectives, using decision trees as a tool for exploratory data analysis. We show that a simple mapping from the derivational type to the semantic class achieves 70.1% accuracy; syntactic function reaches a slightly higher accuracy of 73.5%. Although the accuracy scores are quite similar with the two resulting classifications, the kinds of mistakes are qualitatively very different. Morphology can be used as a baseline classification, and syntax can be used as a clue when there are mismatches between morphology and semantics."
H05-1077,Identifying Semantic Relations and Functional Properties of Human Verb Associations,2005,14,15,1,1,631,sabine walde,Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,0,"This paper uses human verb associations as the basis for an investigation of verb properties, focusing on semantic verb relations and prominent nominal features. First, the lexical semantic taxonymy GermaNet is checked on the types of classic semantic relations in our data; verb-verb pairs not covered by GermaNet can help to detect missing links in the taxonomy, and provide a useful basis for defining non-classical relations. Second, a statistical grammar is used for determining the conceptual roles of the noun responses. We present prominent syntax-semantic roles and evidence for the usefulness of co-occurrence information in distributional verb descriptions."
W04-2118,"Identification, Quantitative Description, and Preliminary Distributional Analysis of {G}erman Particle Verbs",2004,8,10,1,1,631,sabine walde,Proceedings of the Workshop on Enhancing and Using Electronic Dictionaries,0,A statistical grammar model is used to identify German particle verbs and induce quantitative lexical information on their subcategorisation frames and selectional preferences. A simple approach to address the semantic class of the particle verb is introduced.
E03-1037,Experiments on the Choice of Features for Learning Verb Classes,2003,12,15,1,1,631,sabine walde,10th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"The choice of verb features is crucial for the learning of verb classes. This paper presents clustering experiments on 168 German verbs, which explore the relevance of features on three levels of verb description, purely syntactic frame types, prepositional phrase information and selectional preferences. In contrast to previous approaches concentrating on the sparse data problem, we present evidence for a linguistically defined limit on the usefulness of features which is driven by the idiosyncratic properties of the verbs and the specific attributes of the desired verb classification."
W02-1016,Spectral Clustering for {G}erman Verbs,2002,20,61,2,0,22390,chris brew,Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing ({EMNLP} 2002),0,"We describe and evaluate the application of a spectral clustering technique (Ng et al., 2002) to the unsupervised clustering of German verbs. Our previous work has shown that standard clustering techniques succeed in inducing Levin-style semantic classes from verb subcategorisation information. But clustering in the very high dimensional spaces that we use is fraught with technical and conceptual difficulties. Spectral clustering performs a dimensionality reduction on the verb frame patterns, and provides a robustness and efficiency that standard clustering methods do not display in direct use. The clustering results are evaluated according to the alignment (Christianini et al., 2002) between the Gram matrix defined by the cluster output and the corresponding matrix defined by a gold standard."
P02-1029,Inducing {G}erman Semantic Verb Classes from Purely Syntactic Subcategorisation Information,2002,15,72,1,1,631,sabine walde,Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,1,"The paper describes the application of k-Means, a standard clustering technique, to the task of inducing semantic classes for German verbs. Using probability distributions over verb subcategorisation frames, we obtained an intuitively plausible clustering of 57 verbs into 14 classes. The automatic clustering was evaluated against independently motivated, hand-constructed semantic verb classes. A series of post-hoc cluster analyses explored the influence of specific frames and frame groups on the coherence of the verb classes, and supported the tight connection between the syntactic behaviour of the verbs and their lexical meaning components."
schulte-im-walde-2002-subcategorisation,A Subcategorisation Lexicon for {G}erman Verbs induced from a Lexicalised {PCFG},2002,10,36,1,1,631,sabine walde,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"The paper presents a large-scale computational subcategorisation lexicon for several thousand German verbs. The lexical entries were obtained by unsupervised learning in a statistical grammar framework: a German context-free grammar containing frame-predicting grammar rules and information about lexical heads was trained on 18.7 million words of a large German newspaper corpus. We developed a simple methodology to utilise frequency distributions in the lexicalised version of the probabilistic grammar for inducing syntactic verb frame descriptions. The frame definition is variable with respect to the inclusion of prepositional phrase refinement. An evaluation against a manual dictionary justifies the utilisation of the machine-readable lexicon as a valuable component for supporting NLP-tasks. As to our knowledge, no former computational approach has obtained a subcategorisation lexicon for German comparable in size (the number of verbs in the lexicon), restriction (no limit concerning the frequencies of the verbs), or verified reliability (successful extensive evaluation against dictionary)."
poesio-etal-2002-acquiring,Acquiring Lexical Knowledge for Anaphora Resolution,2002,26,79,3,0,1743,massimo poesio,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"The lack of adequate bases of commonsense or even lexical knowledge is perhaps the main obstacle to the development of highperformance, robust tools for semantic interpretation. It is also generally accepted that, notwithstanding the increasing availability in recent years of substantial hand-coded lexical resources such as WordNet and EuroWordNet, addressing the commonsense knowledge bottleneck will eventually require the development of effective techniques for acquiring such information automatically, e.g., from corpora. We discuss research aimed at improving the performance of anaphora resolution systems by acquiring the commonsense knowledge require to resolve the more complex cases of anaphora, such as bridging references. We focus in particular on the problem of acquiring information about part-of relations."
C00-2105,Robust {G}erman Noun Chunking With a Probabilistic Context-Free Grammar,2000,17,58,2,0,13978,helmut schmid,{COLING} 2000 Volume 2: The 18th International Conference on Computational Linguistics,0,"We present a noun chunker for German which is based on a head-lexicalised probabilistic context-free grammar. A manually developed grammar was semi-automatically extended with robustness rules in order to allow parsing of unrestricted text. The model parameters were learned from unlabelled training data by a probabilistic context-free parser. For extracting noun chunks, the parser generates all possible noun chunk analyses, scores them with a novel algorithm which maximizes the best chunk sequence criterion, and chooses the most probable chunk sequence. An evaluation of the chunker on 2,140 hand-annotated noun chunks yielded 92% recall and 93% precision."
C00-2108,Clustering Verbs Semantically According to their Alternation Behaviour,2000,13,87,1,1,631,sabine walde,{COLING} 2000 Volume 2: The 18th International Conference on Computational Linguistics,0,"Verbs were clustered semantically on the basis of their alternation behaviour, as characterised by their syntactic subcategorisation frames extracted from maximum probability parses of a robust statistical parser, and completed by assigning WordNet classes as selectional preferences to the frame arguments. The clustering was achieved (a) iteratively by measuring the relative entropy between the verbs' probability distributions over the frame types, and (b) by utilising a latent class analysis based on the joint frequencies of verbs and frame types."
