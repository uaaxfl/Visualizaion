2020.acl-main.578,D17-1209,0,0.0551015,"Missing"
2020.acl-main.578,P19-1140,0,0.216946,"common neighbors between different KGs. By aligning entities from different knowledge graphs (KGs) to the same real-world identity, entity alignment is a powerful technique for knowledge integration. Unfortunately, entity alignment is nontrivial because real-life KGs are often incomplete and different KGs typically have heterogeneous schemas. Consequently, equivalent entities from two KGs could have distinct surface forms or dissimilar neighborhood structures. In recent years, embedding-based methods have become the dominated approach for entity alignment (Zhu et al., 2017; Pei et al., 2019a; Cao et al., 2019; Xu et al., 2019; Li et al., 2019a; Sun et al., Corresponding author. 纽约 子行政区 Introduction ∗ subdivision 最大城市 子行政区 布鲁克林区 KG2: Brooklyn # neighbors: 21 # common neighbors: 2 2020). Such approaches have the advantage of not relying on manually constructed features or rules (Mahdisoltani et al., 2015). Using a set of seed alignments, an embedding-based method models the KG structures to automatically learn how to map the equivalent entities among different KGs into a unified vector space where entity alignment can be performed by measuring the distance between the embeddings of two entities. The"
2020.acl-main.578,D19-1274,0,0.313861,"KGs. By aligning entities from different knowledge graphs (KGs) to the same real-world identity, entity alignment is a powerful technique for knowledge integration. Unfortunately, entity alignment is nontrivial because real-life KGs are often incomplete and different KGs typically have heterogeneous schemas. Consequently, equivalent entities from two KGs could have distinct surface forms or dissimilar neighborhood structures. In recent years, embedding-based methods have become the dominated approach for entity alignment (Zhu et al., 2017; Pei et al., 2019a; Cao et al., 2019; Xu et al., 2019; Li et al., 2019a; Sun et al., Corresponding author. 纽约 子行政区 Introduction ∗ subdivision 最大城市 子行政区 布鲁克林区 KG2: Brooklyn # neighbors: 21 # common neighbors: 2 2020). Such approaches have the advantage of not relying on manually constructed features or rules (Mahdisoltani et al., 2015). Using a set of seed alignments, an embedding-based method models the KG structures to automatically learn how to map the equivalent entities among different KGs into a unified vector space where entity alignment can be performed by measuring the distance between the embeddings of two entities. The vast majority of prior works in t"
2020.acl-main.578,D17-1159,0,0.0254039,"ead for training data labeling. AliNet considers all one-hop neighbors of an entity to be equally important when aggregating information. However, not all one-hop neighbors contribute positively to characterizing the target entity. Thus, considering all of them without careful selection can introduce noise and degrade the performance. NMN avoids these pitfalls. With only a small set of pre-aligned entities as training data, NMN chooses the most informative neighbors for entity alignment. Graph neural networks. GNNs have recently been employed for various NLP tasks like semantic role labeling (Marcheggiani and Titov, 2017) and machine translation (Bastings et al., 2017). GNNs learn node representations by recursively aggregating the representations of neighboring nodes. There are a range of GNN variants, including the Graph Convolutional Network (GCN) (Kipf and Welling, 2017), the Relational Graph Convolutional Network (Schlichtkrull et al., 2018), the Graph Attention Network (Veliˇckovi´c et al., 2018). Giving the powerful capability for modeling graph structures, we also leverage GNNs to encode the structural information of KGs (Sec. 3.2). and Lau, 2013). In our entity alignment framework, we propose a vertex"
2020.acl-main.578,P18-1187,0,0.029954,"regation d(e1, e2) G2 e2 e2 e2 e2 Neighborhood Aggregation Figure 2: Overall architecture and processing pipeline of Neighborhood Matching Network (NMN). (l) (l) (l) (l) (l) where {h1 , h2 , ..., hn |hi ∈ Rd } is the output node (entity) features of l-th GCN layer, i is the normalization constant, Ni is the set of neigh(l) (l−1) bor indices of entity i, and W(l) ∈ Rd ×d is a layer-specific trainable weight matrix. To control the accumulated noise, we also introduce highway networks (Srivastava et al., 2015) to GCN layers, which can effectively control the noise propagation across GCN layers (Rahimi et al., 2018; Wu et al., 2019b). 3.3 Neighborhood Sampling The one-hop neighbors of an entity are key to determine whether the entity should be aligned with other entities. However, as we have discussed in Sec. 1, not all one-hop neighbors contribute positively for entity alignment. To choose the right neighbors, we apply a down-sampling process to select the most informative entities towards the central target entity from its one-hop neighbors. Recall that we use pre-trained word embeddings of entity names to initialize the input node features of GCNs. As a result, the entity embeddings learned by GCNs c"
2020.acl-main.578,D18-1032,0,0.0842916,"ng mechanism to jointly compare discriminative subgraphs of two entities for robust entity alignment (Sec. 3.4). 2 Related Work Embedding-based entity alignment. In recent years, embedding-based methods have emerged as viable means for entity alignment. Early works in the area utilize TransE (Bordes et al., 2013) to embed KG structures, including MTransE (Chen et al., 2017), JAPE (Sun et al., 2017), IPTransE (Zhu et al., 2017), BootEA (Sun et al., 2018), NAEA (Zhu et al., 2019) and OTEA (Pei et al., 2019b). Some more recent studies use GNNs to model the structures of KGs, including GCN-Align (Wang et al., 2018), GMNN (Xu et al., 2019), RDGCN (Wu et al., 2019a), AVR-GCN (Ye et al., 2019), and HGCN-JE (Wu et al., 2019b). Besides the structural information, some recent methods like KDCoE (Chen et al., 2018), AttrE (Trisedya et al., 2019), MultiKE (Zhang et al., 2019) and HMAN 6478 (Yang et al., 2019) also utilize additional information like Wikipedia entity descriptions and attributes to improve entity representations. However, all the aforementioned methods ignore the neighborhood heterogeneity of KGs. MuGNN (Cao et al., 2019) and AliNet (Sun et al., 2020) are two most recent efforts for addressing th"
2020.acl-main.578,P19-1304,1,0.694509,"etween different KGs. By aligning entities from different knowledge graphs (KGs) to the same real-world identity, entity alignment is a powerful technique for knowledge integration. Unfortunately, entity alignment is nontrivial because real-life KGs are often incomplete and different KGs typically have heterogeneous schemas. Consequently, equivalent entities from two KGs could have distinct surface forms or dissimilar neighborhood structures. In recent years, embedding-based methods have become the dominated approach for entity alignment (Zhu et al., 2017; Pei et al., 2019a; Cao et al., 2019; Xu et al., 2019; Li et al., 2019a; Sun et al., Corresponding author. 纽约 子行政区 Introduction ∗ subdivision 最大城市 子行政区 布鲁克林区 KG2: Brooklyn # neighbors: 21 # common neighbors: 2 2020). Such approaches have the advantage of not relying on manually constructed features or rules (Mahdisoltani et al., 2015). Using a set of seed alignments, an embedding-based method models the KG structures to automatically learn how to map the equivalent entities among different KGs into a unified vector space where entity alignment can be performed by measuring the distance between the embeddings of two entities. The vast majority of"
2020.acl-main.578,D19-1451,0,0.0505817,"(Bordes et al., 2013) to embed KG structures, including MTransE (Chen et al., 2017), JAPE (Sun et al., 2017), IPTransE (Zhu et al., 2017), BootEA (Sun et al., 2018), NAEA (Zhu et al., 2019) and OTEA (Pei et al., 2019b). Some more recent studies use GNNs to model the structures of KGs, including GCN-Align (Wang et al., 2018), GMNN (Xu et al., 2019), RDGCN (Wu et al., 2019a), AVR-GCN (Ye et al., 2019), and HGCN-JE (Wu et al., 2019b). Besides the structural information, some recent methods like KDCoE (Chen et al., 2018), AttrE (Trisedya et al., 2019), MultiKE (Zhang et al., 2019) and HMAN 6478 (Yang et al., 2019) also utilize additional information like Wikipedia entity descriptions and attributes to improve entity representations. However, all the aforementioned methods ignore the neighborhood heterogeneity of KGs. MuGNN (Cao et al., 2019) and AliNet (Sun et al., 2020) are two most recent efforts for addressing this issue. While promising, both models still have drawbacks. MuGNN requires both pre-aligned entities and relations as training data, which can have expensive overhead for training data labeling. AliNet considers all one-hop neighbors of an entity to be equally important when aggregating inf"
2020.acl-main.578,D19-1023,1,0.824616,"raphs of two entities for robust entity alignment (Sec. 3.4). 2 Related Work Embedding-based entity alignment. In recent years, embedding-based methods have emerged as viable means for entity alignment. Early works in the area utilize TransE (Bordes et al., 2013) to embed KG structures, including MTransE (Chen et al., 2017), JAPE (Sun et al., 2017), IPTransE (Zhu et al., 2017), BootEA (Sun et al., 2018), NAEA (Zhu et al., 2019) and OTEA (Pei et al., 2019b). Some more recent studies use GNNs to model the structures of KGs, including GCN-Align (Wang et al., 2018), GMNN (Xu et al., 2019), RDGCN (Wu et al., 2019a), AVR-GCN (Ye et al., 2019), and HGCN-JE (Wu et al., 2019b). Besides the structural information, some recent methods like KDCoE (Chen et al., 2018), AttrE (Trisedya et al., 2019), MultiKE (Zhang et al., 2019) and HMAN 6478 (Yang et al., 2019) also utilize additional information like Wikipedia entity descriptions and attributes to improve entity representations. However, all the aforementioned methods ignore the neighborhood heterogeneity of KGs. MuGNN (Cao et al., 2019) and AliNet (Sun et al., 2020) are two most recent efforts for addressing this issue. While promising, both models still hav"
2020.ccl-1.80,D18-1255,0,0.02896,"ght play a negative interference role for the model to find the answer, while answering the current question qk . Therefore, this paper focuses on how to select historical context and integrate its information into current question and passage. ConvQA is very similar to the Background Based Conversations (BBCs) which recently proposed in the field of conversational systems. The latter is proposed to generate a more informative response based on unstructured background knowledge. But most of the research is aimed at topic-specific field (Meng et al., 2019), such as the conversation for movies (Moghe et al., 2018; Zhou et al., 2018) and diverse set of topics of Wikipedia (Dinan et al., 2018). Therefore, question answering based on reading comprehension and BBCs, these two tasks have in common that when responsing to each current sentence, not only the passage or background, but also the historical conversational context must be considered. The difference is that the former pays more attention to the ability of the model to understand the passage. When answering questions, the passage is mainly learned, and the historical conversation is supplemented to make the answer more accurate. The latter pays mo"
2020.ccl-1.80,N18-1202,0,0.08258,"Missing"
2020.ccl-1.80,2020.acl-main.432,0,0.0362756,"ed to enhance the useful history turns of conversation and weaken the unnecessary information. Finally, we demonstrate the effectiveness of the proposed method on the QuAC dataset, analyze the impact of different feature selection methods, and verify the validity and reliability of the proposed features through visualization and human evaluation. CC Conversational Question Answering (ConvQA) is a new question answering task that requires a comprehension of the context, which has recently received more and more attention (Zhu et al., 2018; Qu et al., 2019a; Qu et al., 2019b; Meng et al., 2019; Pruthi et al., 2020). Since conversation is one of the most natural ways for humans to seek information, it carries over context through the dialogue flow. Specifically, we ask other people a question, dependending on their answer, we follow up with a new question, and second answer with additional information will be given based on what has been discussed (Reddy et al., 2019). Therefore, multi-turn conversational question answering is formed in this way. It can be used in many fields as a personal assistant systems, such as, customer service, medical, finance, education, etc. Moreover, with the rapid development"
2020.ccl-1.80,D16-1264,0,0.333651,"kipedia passage by asking a sequence ©2020 China National Conference on Computational Linguistics Published under Creative Commons Attribution 4.0 International License Proceedings of the 19th China National Conference on Computational Linguistics, pages 863-873, Hainan, China, October 30 - Novermber 1, 2020. (c) Technical Committee on Computational Linguistics, Chinese Information Processing Society of China Computational Linguistics CC L2 02 0 of freeform questions. The teacher answers the question by providing a span of text in the passage, as in existing reading comprehension tasks SQuAD (Rajpurkar et al., 2016), and gives the dialog acts which indicate the student whether the conversation should follow up. The CoQA has abstractive answers involving adding a pronoun (Coref) or inserting prepositions and changing word forms (Fluency) to existing extractive answers (Yatskar, 2018). Both datasets contain yes/no questions and extractive answers. Compared with the CoQA1 , the QuAC2 setting is similar to a user query on search engines. The latter is designed to model and understand information-seeking conversation, which is closer to the people’s daily question-answering style conversation than other datas"
2020.ccl-1.80,P18-2124,0,0.022389,"omputational Linguistics, pages 863-873, Hainan, China, October 30 - Novermber 1, 2020. (c) Technical Committee on Computational Linguistics, Chinese Information Processing Society of China Computational Linguistics CC L2 02 0 born?”. Here, the pronoun “she” of Q2 associates Q2 with Grandma Li of A1 , which indicates that the A2 depend on A1 . If the QA model does not use Q1 and A1 , then it does not know who she refers to in Q2 , making it difficult for the model to accurately answer Q2 . However, the questions of traditional MRC datasets (such as SQuAD (Rajpurkar et al., 2016) and SQuAD2.0 (Rajpurkar et al., 2018)) are independent of each other and have no relevance. Compared with the traditional MRC task, multi-turn ConvQA based on MRC adds multiple turns of conversation history to the original MRC task, making the ConvQA task more suitable for human daily conversation habits. The exsiting methods for ConvQA in (Qu et al., 2019a) and (Qu et al., 2019b) determine whether the token in the question and the passage appear in each round of the historical conversation, and take the distance from the history turn of answers to the current question as the relative position, finally use the embedding of the re"
2020.ccl-1.80,Q19-1016,0,0.359998,"tional Question Answering (ConvQA) is a new question answering task that requires a comprehension of the context, which has recently received more and more attention (Zhu et al., 2018; Qu et al., 2019a; Qu et al., 2019b; Meng et al., 2019; Pruthi et al., 2020). Since conversation is one of the most natural ways for humans to seek information, it carries over context through the dialogue flow. Specifically, we ask other people a question, dependending on their answer, we follow up with a new question, and second answer with additional information will be given based on what has been discussed (Reddy et al., 2019). Therefore, multi-turn conversational question answering is formed in this way. It can be used in many fields as a personal assistant systems, such as, customer service, medical, finance, education, etc. Moreover, with the rapid development of artificial intelligence technology in theory and practical applications, many personal assistant products have been launched in the market, such as Alibaba AliMe, Apple Siri, Amazon Alexa, etc. Although these assistants are capable to cover some simple tasks, they cannot handle complicated information-seeking conversations that require multiple turns of"
2020.ccl-1.80,D18-1076,0,0.0288913,"interference role for the model to find the answer, while answering the current question qk . Therefore, this paper focuses on how to select historical context and integrate its information into current question and passage. ConvQA is very similar to the Background Based Conversations (BBCs) which recently proposed in the field of conversational systems. The latter is proposed to generate a more informative response based on unstructured background knowledge. But most of the research is aimed at topic-specific field (Meng et al., 2019), such as the conversation for movies (Moghe et al., 2018; Zhou et al., 2018) and diverse set of topics of Wikipedia (Dinan et al., 2018). Therefore, question answering based on reading comprehension and BBCs, these two tasks have in common that when responsing to each current sentence, not only the passage or background, but also the historical conversational context must be considered. The difference is that the former pays more attention to the ability of the model to understand the passage. When answering questions, the passage is mainly learned, and the historical conversation is supplemented to make the answer more accurate. The latter pays more attention to the"
2020.ccl-1.83,K16-1002,0,0.519192,"attempt to learn mid-level representations, such as events (Martin et al., 2018), prompts (Fan et al., 2018), keywords (Yao et al., 2019) or actions (Fan et al., 2019), to guide the sentences generation. Although these approaches have shown their encouraging effectiveness in improving the thematic consistency, most of them have no guarantee for the wording diversity. The main reason is that most of these methods are based on recurrent neural networks (RNNs), which tend to be entrapped within local word co-occurrences and cannot explicitly model holistic properties of sentences such as topic (Bowman et al., 2016; Li et al., 2018; Li et al., 2019). As a result, RNN tends to generate common words that appear frequently (Zhao et al., 2017) and this will lead to both high inter- and intra-story content repetition rates. On the other hand, a well-composed story also needs to contain vivid and diversified words. To address the issue of wording diversity, some studies have employed models based on variational autoencoder (VAE) (Kingma and Welling, 2013) or conditional variational autoencoder (CVAE) as a possible solution. It has been proved that, through learning distributed latent representation of the ent"
2020.ccl-1.83,P18-1082,0,0.113237,"ory generation is a challenging task since it requires generating texts which satisfy not only thematic consistency but also wording diversity. Despite that considerable efforts have been made in the past decades, the requirement of thematic consistency and wording diversity is still one of the main problems in the task of story generation. On the one hand, a well-composed story is supposed to contain sentences that are tightly connected with a given theme. To address this problem, most previous methods attempt to learn mid-level representations, such as events (Martin et al., 2018), prompts (Fan et al., 2018), keywords (Yao et al., 2019) or actions (Fan et al., 2019), to guide the sentences generation. Although these approaches have shown their encouraging effectiveness in improving the thematic consistency, most of them have no guarantee for the wording diversity. The main reason is that most of these methods are based on recurrent neural networks (RNNs), which tend to be entrapped within local word co-occurrences and cannot explicitly model holistic properties of sentences such as topic (Bowman et al., 2016; Li et al., 2018; Li et al., 2019). As a result, RNN tends to generate common words that"
2020.ccl-1.83,P19-1254,0,0.0867221,"rating texts which satisfy not only thematic consistency but also wording diversity. Despite that considerable efforts have been made in the past decades, the requirement of thematic consistency and wording diversity is still one of the main problems in the task of story generation. On the one hand, a well-composed story is supposed to contain sentences that are tightly connected with a given theme. To address this problem, most previous methods attempt to learn mid-level representations, such as events (Martin et al., 2018), prompts (Fan et al., 2018), keywords (Yao et al., 2019) or actions (Fan et al., 2019), to guide the sentences generation. Although these approaches have shown their encouraging effectiveness in improving the thematic consistency, most of them have no guarantee for the wording diversity. The main reason is that most of these methods are based on recurrent neural networks (RNNs), which tend to be entrapped within local word co-occurrences and cannot explicitly model holistic properties of sentences such as topic (Bowman et al., 2016; Li et al., 2018; Li et al., 2019). As a result, RNN tends to generate common words that appear frequently (Zhao et al., 2017) and this will lead to"
2020.ccl-1.83,N16-1014,0,0.0341933,"All initial weights are sampled from a uniform distribution [−0.08, 0.08]. The batch size is 80. Evaluation We utilize both automatic and human metrics to evaluate the performance of our method. BLUE Score. This metric is designed for calculating the word-overlap score between the golden texts and the generated ones (Papineni et al., 2002), and has been used in many previous story generation works (Yao et al., 2019; Li et al., 2019). Distinct Score. To measure the diversity of the generated stories, we employ this metric to compute the proportion of distinct n-grams in the generated outputs (Li et al., 2016). Note that the final distinct scores are scaled to [0, 100]. Inter- and intra-story repetition. These two metrics are proposed in (Yao et al., 2019) and used for calculating the inter- and intra-story tri-grams 1 repetition rates by sentences and for the whole stories. The final results are also scaled to [0, 100]. Human Evaluation. We also employ three metrics for human evaluation, i.e., Readability, Consistency, and Creativity. Their descriptions are shown in Table 1. We randomly sample 100 generated stories from each baseline model and our method and then perform pairwise comparisons betwe"
2020.ccl-1.83,D18-1423,1,0.913573,"-level representations, such as events (Martin et al., 2018), prompts (Fan et al., 2018), keywords (Yao et al., 2019) or actions (Fan et al., 2019), to guide the sentences generation. Although these approaches have shown their encouraging effectiveness in improving the thematic consistency, most of them have no guarantee for the wording diversity. The main reason is that most of these methods are based on recurrent neural networks (RNNs), which tend to be entrapped within local word co-occurrences and cannot explicitly model holistic properties of sentences such as topic (Bowman et al., 2016; Li et al., 2018; Li et al., 2019). As a result, RNN tends to generate common words that appear frequently (Zhao et al., 2017) and this will lead to both high inter- and intra-story content repetition rates. On the other hand, a well-composed story also needs to contain vivid and diversified words. To address the issue of wording diversity, some studies have employed models based on variational autoencoder (VAE) (Kingma and Welling, 2013) or conditional variational autoencoder (CVAE) as a possible solution. It has been proved that, through learning distributed latent representation of the entire sentences, VA"
2020.ccl-1.83,N16-1098,0,0.305974,"ely Plan-CVAE, which first plans a keyword sequence and then generates a story based on the keyword sequence. In our method, the keywords planning strategy is used to improve thematic consistency while the CVAE module allows enhancing wording diversity. Experimental results on a benchmark dataset confirm that our proposed method can generate stories with both thematic consistency and wording novelty, and outperforms state-of-the-art methods on both automatic metrics and human evaluations. CC A narrative story is a sequence of sentences or words which describe a logically linked set of events (Mostafazadeh et al., 2016). Automatic story generation is a challenging task since it requires generating texts which satisfy not only thematic consistency but also wording diversity. Despite that considerable efforts have been made in the past decades, the requirement of thematic consistency and wording diversity is still one of the main problems in the task of story generation. On the one hand, a well-composed story is supposed to contain sentences that are tightly connected with a given theme. To address this problem, most previous methods attempt to learn mid-level representations, such as events (Martin et al., 20"
2020.ccl-1.83,P02-1040,0,0.108711,"e size of encoder, decoder, and prior network are 500, 500, 600 respectively. And the size of the latent variable z is set to 300. To train our model, we adopt the Adam (Kingma and Ba, 2015) optimization algorithm with an initial learning rate of 0.001 and gradient clipping of 5. All initial weights are sampled from a uniform distribution [−0.08, 0.08]. The batch size is 80. Evaluation We utilize both automatic and human metrics to evaluate the performance of our method. BLUE Score. This metric is designed for calculating the word-overlap score between the golden texts and the generated ones (Papineni et al., 2002), and has been used in many previous story generation works (Yao et al., 2019; Li et al., 2019). Distinct Score. To measure the diversity of the generated stories, we employ this metric to compute the proportion of distinct n-grams in the generated outputs (Li et al., 2016). Note that the final distinct scores are scaled to [0, 100]. Inter- and intra-story repetition. These two metrics are proposed in (Yao et al., 2019) and used for calculating the inter- and intra-story tri-grams 1 repetition rates by sentences and for the whole stories. The final results are also scaled to [0, 100]. Human Ev"
2020.ccl-1.83,W17-0911,0,0.0188795,"g diversity of the story. To evaluate our proposed method, we conduct experiments on a benchmark dataset, i.e., the Rocstories corpus (Mostafazadeh et al., 2016). Experimental results demonstrate that our introduced method can generate stories that are more preferable for human annotators in terms of thematic consistency and wording diversity, and meanwhile outperforms state-of-the-art methods on automatic metrics. CC L2 In recent years, neural network models have been demonstrated effective in natural language processing tasks (Mikolov et al., 2010; Sutskever et al., 2014; Rush et al., 2015; Roemmele et al., 2017; Liu et al., 2020; Yu et al., 2020). In story generation, previous studies have employed neural networks for enhancing the quality of generated content. Jain et al. (2017) explored generating coherent stories from independent short descriptions by using a sequence to sequence (S2S) architecture with a bidirectional RNN encoder and an RNN decoder. Since this model is insufficient for generating stories with consistent themes, to improve the thematic consistency of the generated stories, many other methods have been explored. Martin et al. (2018) argued that using events representations as the"
2020.ccl-1.83,D15-1044,0,0.036916,"to keep the wording diversity of the story. To evaluate our proposed method, we conduct experiments on a benchmark dataset, i.e., the Rocstories corpus (Mostafazadeh et al., 2016). Experimental results demonstrate that our introduced method can generate stories that are more preferable for human annotators in terms of thematic consistency and wording diversity, and meanwhile outperforms state-of-the-art methods on automatic metrics. CC L2 In recent years, neural network models have been demonstrated effective in natural language processing tasks (Mikolov et al., 2010; Sutskever et al., 2014; Rush et al., 2015; Roemmele et al., 2017; Liu et al., 2020; Yu et al., 2020). In story generation, previous studies have employed neural networks for enhancing the quality of generated content. Jain et al. (2017) explored generating coherent stories from independent short descriptions by using a sequence to sequence (S2S) architecture with a bidirectional RNN encoder and an RNN decoder. Since this model is insufficient for generating stories with consistent themes, to improve the thematic consistency of the generated stories, many other methods have been explored. Martin et al. (2018) argued that using events"
2020.ccl-1.83,D18-1462,0,0.0219582,"r and an RNN decoder. Since this model is insufficient for generating stories with consistent themes, to improve the thematic consistency of the generated stories, many other methods have been explored. Martin et al. (2018) argued that using events representations as the guidance for story generation is able to improve the thematic consistency of generated content. Fan et al. (2018) presented a hierarchical method that first generates a prompt from the title, and then a story is generated conditioned on the previously generated prompt. Following the idea of learning mid-level representations, Xu et al. (2018) proposed a skeleton-based model that first extracts skeleton from previous sentences, and then generates new sentences under the guidance of the skeleton. Similarly, Yao et al. (2019) explored using a storyline planning strategy for guiding the story generation process to ensure the output story can describe a consistent topic. Fan et al. (2019) further adopted a structure-based strategy that first generates sequences of predicates and arguments, and then outputs a story by filling placeholder entities. Although these methods have achieved promising results, most of them are implemented with"
2020.ccl-1.83,D16-1050,0,0.0173614,"r improving the wording diversity in story generation (Li et al., 2019). 2.2 Conditional Variational Autoencoder The Variational Auto-Encoder (VAE) model is proposed in (Kingma and Welling, 2013). Through forcing the latent variables to follow a prior distribution, VAE is able to generate diverse text successfully by randomly sampling from the latent space (Bowman et al., 2016). Conditional Variational AutoEncoder (CVAE), as a variant of VAE, can generate specific outputs conditioned on a given input. CVAE has been used in many other related text generation tasks, such as machine translation (Zhang et al., 2016), dialogue generation (Serban et al., 2017; Zhao et al., 2017; Shen et al., 2017), and poem composing (Yang et al., 2018; Li et al., 2018). Subsequently, in recent years, CVAE has begun to be applied in Proceedings of the 19th China National Conference on Computational Linguistics, pages 892-902, Hainan, China, October 30 - Novermber 1, 2020. (c) Technical Committee on Computational Linguistics, Chinese Information Processing Society of China Computational Linguistics story generation task to tackle the common wording problem. Li et al. (2019) explored adopting CVAE to generate stories with no"
2020.ccl-1.83,P17-1061,0,0.283728,"al., 2019) or actions (Fan et al., 2019), to guide the sentences generation. Although these approaches have shown their encouraging effectiveness in improving the thematic consistency, most of them have no guarantee for the wording diversity. The main reason is that most of these methods are based on recurrent neural networks (RNNs), which tend to be entrapped within local word co-occurrences and cannot explicitly model holistic properties of sentences such as topic (Bowman et al., 2016; Li et al., 2018; Li et al., 2019). As a result, RNN tends to generate common words that appear frequently (Zhao et al., 2017) and this will lead to both high inter- and intra-story content repetition rates. On the other hand, a well-composed story also needs to contain vivid and diversified words. To address the issue of wording diversity, some studies have employed models based on variational autoencoder (VAE) (Kingma and Welling, 2013) or conditional variational autoencoder (CVAE) as a possible solution. It has been proved that, through learning distributed latent representation of the entire sentences, VAE can capture global features such as topics and high-level syntactic properties, and thus can generate novel"
2020.ccl-1.88,P15-1017,0,0.306368,"stages (Nguyen et al., 2016). The advantages of such a joint system are twofold: (1) mitigating the error propagation from the upstream component (trigger extraction) to the downstream classifier (argument extraction), and (2) benefiting from the inter-dependencies among event triggers and argument roles (Nguyen and Nguyen, 2019). Traditional methods that rely heavily on hand-craft features are hard to transfer among languages and annotation standards (Chen and Ng, 2012; Liao and Grishman , 2010; Li et al., 2013). The neural network based methods that are able to learn features automatically (Chen et al., 2015; Feng et al., 2016; Nguyen et al., 2016; Nguyen and Grishman, 2016; Zeng et al., 2016) have achieved significant progress. Most of them have followed the pipelined approach. Some improvements have been made by jointly predicting triggers and arguments (Liu et al., 2018; Nguyen et al., 2016; Nguyen and Nguyen, 2019) and introducing more complicated architectures to capture larger scale of contexts. These methods have achieved promising results in EE. Unfortunately, roles overlap problem has been put forward (He and Duan, 2019; Yang et al., 2019), but there are only few works in the literature"
2020.ccl-1.88,N09-2053,0,0.455257,"ger classification and argument 1 Introduction L2 classification. Event extraction (EE) is of utility and challenge task in natural language processing (NLP). CC It aims to identify event triggers of specified types and their arguments in text. As defined in Automatic Content Extraction (ACE) program, the event extraction task is divided into two subtasks, i.e., trigger extraction (identifying and classifying event triggers) and argument extraction (identifying arguments and labeling their roles). Chinese event extraction is a more difficult task because of language specific issue in Chinese (Chen and Ji, 2009). Since Chinese does not have delimiters between words, segmentation is usually a necessary step for further processing, leading to word-trigger mismatch problem (Lin et al., 2018). The approaches based on word-wise classification paradigm commonly suffer from this. For instance, two characters in one word “打死” (hit and die) trigger two different events: an “Attack”event triggered by “打” (hit) and a “Die” event triggered by “死” (die). It is hard to extract accurately when a trigger is part of a word or cross multiple words. To avoid this issue, we formulate Chinese event extraction as a charac"
2020.ccl-1.88,N19-1423,0,0.0263852,"Missing"
2020.ccl-1.88,P16-2011,0,0.0143561,"l., 2016). The advantages of such a joint system are twofold: (1) mitigating the error propagation from the upstream component (trigger extraction) to the downstream classifier (argument extraction), and (2) benefiting from the inter-dependencies among event triggers and argument roles (Nguyen and Nguyen, 2019). Traditional methods that rely heavily on hand-craft features are hard to transfer among languages and annotation standards (Chen and Ng, 2012; Liao and Grishman , 2010; Li et al., 2013). The neural network based methods that are able to learn features automatically (Chen et al., 2015; Feng et al., 2016; Nguyen et al., 2016; Nguyen and Grishman, 2016; Zeng et al., 2016) have achieved significant progress. Most of them have followed the pipelined approach. Some improvements have been made by jointly predicting triggers and arguments (Liu et al., 2018; Nguyen et al., 2016; Nguyen and Nguyen, 2019) and introducing more complicated architectures to capture larger scale of contexts. These methods have achieved promising results in EE. Unfortunately, roles overlap problem has been put forward (He and Duan, 2019; Yang et al., 2019), but there are only few works in the literature to study this. He a"
2020.ccl-1.88,P13-1008,0,0.0493842,"the pipelined approach that first identifies trigger and then identifies arguments in separate CC stages (Nguyen et al., 2016). The advantages of such a joint system are twofold: (1) mitigating the error propagation from the upstream component (trigger extraction) to the downstream classifier (argument extraction), and (2) benefiting from the inter-dependencies among event triggers and argument roles (Nguyen and Nguyen, 2019). Traditional methods that rely heavily on hand-craft features are hard to transfer among languages and annotation standards (Chen and Ng, 2012; Liao and Grishman , 2010; Li et al., 2013). The neural network based methods that are able to learn features automatically (Chen et al., 2015; Feng et al., 2016; Nguyen et al., 2016; Nguyen and Grishman, 2016; Zeng et al., 2016) have achieved significant progress. Most of them have followed the pipelined approach. Some improvements have been made by jointly predicting triggers and arguments (Liu et al., 2018; Nguyen et al., 2016; Nguyen and Nguyen, 2019) and introducing more complicated architectures to capture larger scale of contexts. These methods have achieved promising results in EE. Unfortunately, roles overlap problem has been"
2020.ccl-1.88,P10-1081,0,0.111721,"Missing"
2020.ccl-1.88,D18-1156,0,0.605302,"edicts any one argument or role, which leads to omission and incompleteness of information for knowledge graph construction and is obviously far from real-world applications. Therefore, the roles overlap problem is of great importance and needs to be seriously addressed. It is thus appealing to design a single architecture to solve the problem. Although there exist prior studies that mention the roles overlap problem on ACE 2005 dataset, they share the limitations that include either depending on elaborate engineering features (i.e, hand-crafted features (He and Duan, 2019), dependency paths (Liu et al., 2018), etc.) or following the pipelined approach (Yang et al., 2019). To overcome the issues of such prior works, in this paper, we propose a single framework to jointly extract triggers and arguments. Inspired by the effectiveness of pre-trained language models, we adopt bidirectional encoder representation from transformer (BERT) as the encoder to obtain the shared feature representations. Specifically, the relations among triggers (t), Proceedings of the 19th China National Conference on Computational Linguistics, pages 950-961, Hainan, China, October 30 - Novermber 1, 2020. (c) Technical Commit"
2020.ccl-1.88,N16-1034,0,0.0550685,"ch to tackle problems of roles overlap. Extensive experiments are conducted to evaluate the effectiveness of the proposed model on widely-used dataset ACE 2005 in Section 4. Besides, more rigorous evaluation criteria are adopted in experiments. Conclusions and future work are given in Section 5. 2 Related Work L2 EE is an important task which has attracted many attentions. There are two main paradigms for EE: a) the joint approach that predicts event triggers and arguments jointly, and b) the pipelined approach that first identifies trigger and then identifies arguments in separate CC stages (Nguyen et al., 2016). The advantages of such a joint system are twofold: (1) mitigating the error propagation from the upstream component (trigger extraction) to the downstream classifier (argument extraction), and (2) benefiting from the inter-dependencies among event triggers and argument roles (Nguyen and Nguyen, 2019). Traditional methods that rely heavily on hand-craft features are hard to transfer among languages and annotation standards (Chen and Ng, 2012; Liao and Grishman , 2010; Li et al., 2013). The neural network based methods that are able to learn features automatically (Chen et al., 2015; Feng et a"
2020.ccl-1.88,D16-1085,0,0.0188059,"t system are twofold: (1) mitigating the error propagation from the upstream component (trigger extraction) to the downstream classifier (argument extraction), and (2) benefiting from the inter-dependencies among event triggers and argument roles (Nguyen and Nguyen, 2019). Traditional methods that rely heavily on hand-craft features are hard to transfer among languages and annotation standards (Chen and Ng, 2012; Liao and Grishman , 2010; Li et al., 2013). The neural network based methods that are able to learn features automatically (Chen et al., 2015; Feng et al., 2016; Nguyen et al., 2016; Nguyen and Grishman, 2016; Zeng et al., 2016) have achieved significant progress. Most of them have followed the pipelined approach. Some improvements have been made by jointly predicting triggers and arguments (Liu et al., 2018; Nguyen et al., 2016; Nguyen and Nguyen, 2019) and introducing more complicated architectures to capture larger scale of contexts. These methods have achieved promising results in EE. Unfortunately, roles overlap problem has been put forward (He and Duan, 2019; Yang et al., 2019), but there are only few works in the literature to study this. He and Duan (2019) construct a multi-task learning w"
2020.ccl-1.88,N18-1202,0,0.0167007,"and arguments separately which suffers from error propagation. It does not employ shared feature representations as we do in this work. In recent years, pre-trained language models are successful in capturing words semantic information dynamically by considering their context. McCann et al.(2017) pre-train a deep LSTM encoder from an attentional sequence-to-sequence model for machine translation (MT) to contextualize word vectors. ELMo (Embeddings from Language Models) improve 6 challenging NLP problems by learning the internal states of the stacked bidirectional LSTM (Long ShortTerm Memory) (Peters et al., 2018). Open AI GPT (Generative Pre-Training) improves the state-of-the-art in 9 of 12 tasks (Radford et al., 2018). BERT obtains new state-of-the-art results on 11 NLP tasks (Devlin et al., 2018). 3 Extraction Model This section describes our approach that is designed to extract events occurring in plain 02 0 text. We now define the scope of our work. The task of argument extraction is defined as automatically extracting event relation triples defined. In our model, instead of treating entity mentions as being provided by human annotators, only event label types and argument role types are utilized"
2020.ccl-1.88,P19-1522,0,0.571965,"ping on some words. There are ©2020 China National Conference on Computational Linguistics Published under Creative Commons Attribution 4.0 International License Proceedings of the 19th China National Conference on Computational Linguistics, pages 950-961, Hainan, China, October 30 - Novermber 1, 2020. (c) Technical Committee on Computational Linguistics, Chinese Information Processing Society of China Computational Linguistics Figure 1: Examples of roles overlap problem multiple events existing in the one sentence, which commonly causes the roles overlap problem 02 0 and is easy to overlook (Yang et al., 2019). Fig. 1(a) shows example of roles sharing the same argument in ACE 2005 dataset. “控” (accuse) triggers a Charge-Indict event and “杀害” (kill) triggers an Attack event, while argument “他们” (them) plays the role “Defendant” as well as the role “Attacker” at the same time. Fig. 1(b) shows example of arguments overlapping on some words in ACE 2005 dataset. “来往” (traveled between) triggers a Transport event, while argument “中国” (China) plays not only the role “Origin” but “Destination” and argument L2 “来往于中国和澳大利亚之间的乘客” (passengers who traveled between China and Australia) plays the role “Artifact”."
2020.coling-main.290,D15-1075,0,0.0166434,"71.04 61.07 70.38 65.51 71.16 68.32 76.38 69.17 77.15 SST1 36.80 34.80 36.44 36.96 29.97 35.79 33.97 38.69 35.60 38.24 35.42 41.78 SST2 80.60 78.30 77.02 75.11 68.04 75.11 65.25 76.03 71.33 78.33 71.33 80.70 TREC 90.20 81.56 82.31 87.60 63.40 80.00 73.40 87.06 79.60 86.20 81.60 87.80 Experiments We evaluate the proposed method using one natural language inference dataset and four text classification datasets. The tasks act as good quality checks for the learned representations. The five datasets are SNLI, MR, SST1, SST2 and TREC, detailed training/dev/test splits are shown on Table 2: • SNLI (Bowman et al., 2015): a collection of human-written English sentence pairs manually labeled for balanced classificaTable 2: Summary statistics for the tion with labels: entailment, contradiction, and neutral. This is datasets after tokenization. c dethe natural language inference dataset, which is also solved via notes the number of target classes. classification. Data c Train Dev Test • MR v1.08 : Movie reviews with one sentence per review laSNLI 3 549367 9842 9842 MR 2 8529 1067 1066 beled positive or negative for sentiment classification. SST1 5 8544 1101 2210 • SST19 : an extension of MR but with fine-grained"
2020.coling-main.290,D17-1219,0,0.011919,"14; Chang et al., 2018). Several tasks have benefited from sparse representations, e.g., part-of-speech tagging (Ganchev et al., 2009), dependency parsing (Martins et al., 2011), and supervised classification (Yogatama and Smith, 2014). However, much of the research advances so far for NLP tasks are based on dense representations, e.g., text classification (Kim, 2014; Tang et al., 2015; Wu et al., 2017; Wang et al., 2018), natural language inference (Liu et al., 2019; Kim et al., 2019), machine translation (Cheng, 2019; He et al., 2016) and generation (Serban et al., 2017; Zhang et al., 2019; Du and Cardie, 2017). The study of sparse representations is still limited. There are two key limitations in the study of sparse representations. First, little work has been done to well connect dense and sparse spaces. The two types of representation are rather independent and cannot help each other to achieve synergy. Second, limited work has been done to generate representations of sentences or phrases in the sparse space using sparse word embeddings. Inspired by Fourier Transformation, this paper proposes a novel method called Semantic Transformation (ST) to address the problems. With the help of ST, dense an"
2020.coling-main.290,P15-2076,0,0.0422137,"Missing"
2020.coling-main.290,P15-1144,0,0.0197996,"th positive and negative classes. This demonstrates that the neutral class is more difficult to identify. 4 Related Work Sparse embeddings have been used in image (Ji et al., 2019; Zhou et al., 2016; Zhang and Patel, 2016), signal (Caiafa and Cichocki, 2013; Huang and Aviyente, 2007), and NLP (Subramanian et al., 2018; Kober et al., 2016) applications. Several sparse models have been proposed to produce sparse embeddings. For example, some previous works trained word embeddings with sparse or non-negative constraints (Murphy et al., 2012; Luo et al., 2015). Linguistically inspired dimensions (Faruqui et al., 2015) is another way to increase sparsity and interpretability. SPINE (SParse Interpretable Neural Embeddings) (Subramanian et al., 2018), a variant of denoising k-sparse autoencoder, can generate efficient and interpretable distributed word representations. Our method is different from these approaches. We not only construct sparse representations but also transform between dense and sparse spaces. We also combine word sparse representations to produce sentence representations. Some recent studies tried to achieve sparsity in novel ways (Park et al., 2017). We also proposed a novel method in this"
2020.coling-main.290,P14-1046,0,0.025271,"everaged. Inspired by Fourier Transformation, in this paper, we propose a novel Semantic Transformation method to bridge the dense and sparse spaces, which can facilitate the NLP research to shift from dense spaces to sparse spaces or to jointly use both spaces. Experiments using classification tasks and natural language inference task show that the proposed Semantic Transformation is effective. 1 Introduction A sparse vector is a vector that has a large number of zeros or near zeros. Many studies have shown that sparsity is a desirable property of representations, especially for explanation (Fyshe et al., 2014; Faruqui and Dyer, 2015). In this sense, sparse representation may hold the key to solving the explainability problem of deep neural networks. Apart from the interpretability property, sparse representations can also improve the usability of word vectors as features. The embeddings with good sparsity, interpretability or special meanings can also benefit downstream tasks (Guo et al., 2014; Chang et al., 2018). Several tasks have benefited from sparse representations, e.g., part-of-speech tagging (Ganchev et al., 2009), dependency parsing (Martins et al., 2011), and supervised classification ("
2020.coling-main.290,D14-1012,0,0.0327083,"ive. 1 Introduction A sparse vector is a vector that has a large number of zeros or near zeros. Many studies have shown that sparsity is a desirable property of representations, especially for explanation (Fyshe et al., 2014; Faruqui and Dyer, 2015). In this sense, sparse representation may hold the key to solving the explainability problem of deep neural networks. Apart from the interpretability property, sparse representations can also improve the usability of word vectors as features. The embeddings with good sparsity, interpretability or special meanings can also benefit downstream tasks (Guo et al., 2014; Chang et al., 2018). Several tasks have benefited from sparse representations, e.g., part-of-speech tagging (Ganchev et al., 2009), dependency parsing (Martins et al., 2011), and supervised classification (Yogatama and Smith, 2014). However, much of the research advances so far for NLP tasks are based on dense representations, e.g., text classification (Kim, 2014; Tang et al., 2015; Wu et al., 2017; Wang et al., 2018), natural language inference (Liu et al., 2019; Kim et al., 2019), machine translation (Cheng, 2019; He et al., 2016) and generation (Serban et al., 2017; Zhang et al., 2019; Du"
2020.coling-main.290,D14-1181,0,0.112956,"from the interpretability property, sparse representations can also improve the usability of word vectors as features. The embeddings with good sparsity, interpretability or special meanings can also benefit downstream tasks (Guo et al., 2014; Chang et al., 2018). Several tasks have benefited from sparse representations, e.g., part-of-speech tagging (Ganchev et al., 2009), dependency parsing (Martins et al., 2011), and supervised classification (Yogatama and Smith, 2014). However, much of the research advances so far for NLP tasks are based on dense representations, e.g., text classification (Kim, 2014; Tang et al., 2015; Wu et al., 2017; Wang et al., 2018), natural language inference (Liu et al., 2019; Kim et al., 2019), machine translation (Cheng, 2019; He et al., 2016) and generation (Serban et al., 2017; Zhang et al., 2019; Du and Cardie, 2017). The study of sparse representations is still limited. There are two key limitations in the study of sparse representations. First, little work has been done to well connect dense and sparse spaces. The two types of representation are rather independent and cannot help each other to achieve synergy. Second, limited work has been done to generate"
2020.coling-main.290,D16-1175,0,0.0272618,"t negative responses to negative classes are the positive sentiment bases, which directly indicate the sentiment polarities. Comparing with positive and negative classes, neutral class shows relative mixed responses. That means neutral class has similar semantemes to those of both positive and negative classes. This demonstrates that the neutral class is more difficult to identify. 4 Related Work Sparse embeddings have been used in image (Ji et al., 2019; Zhou et al., 2016; Zhang and Patel, 2016), signal (Caiafa and Cichocki, 2013; Huang and Aviyente, 2007), and NLP (Subramanian et al., 2018; Kober et al., 2016) applications. Several sparse models have been proposed to produce sparse embeddings. For example, some previous works trained word embeddings with sparse or non-negative constraints (Murphy et al., 2012; Luo et al., 2015). Linguistically inspired dimensions (Faruqui et al., 2015) is another way to increase sparsity and interpretability. SPINE (SParse Interpretable Neural Embeddings) (Subramanian et al., 2018), a variant of denoising k-sparse autoencoder, can generate efficient and interpretable distributed word representations. Our method is different from these approaches. We not only constr"
2020.coling-main.290,P19-1441,0,0.0223038,"rd vectors as features. The embeddings with good sparsity, interpretability or special meanings can also benefit downstream tasks (Guo et al., 2014; Chang et al., 2018). Several tasks have benefited from sparse representations, e.g., part-of-speech tagging (Ganchev et al., 2009), dependency parsing (Martins et al., 2011), and supervised classification (Yogatama and Smith, 2014). However, much of the research advances so far for NLP tasks are based on dense representations, e.g., text classification (Kim, 2014; Tang et al., 2015; Wu et al., 2017; Wang et al., 2018), natural language inference (Liu et al., 2019; Kim et al., 2019), machine translation (Cheng, 2019; He et al., 2016) and generation (Serban et al., 2017; Zhang et al., 2019; Du and Cardie, 2017). The study of sparse representations is still limited. There are two key limitations in the study of sparse representations. First, little work has been done to well connect dense and sparse spaces. The two types of representation are rather independent and cannot help each other to achieve synergy. Second, limited work has been done to generate representations of sentences or phrases in the sparse space using sparse word embeddings. Inspired by"
2020.coling-main.290,D15-1196,0,0.022459,"ans neutral class has similar semantemes to those of both positive and negative classes. This demonstrates that the neutral class is more difficult to identify. 4 Related Work Sparse embeddings have been used in image (Ji et al., 2019; Zhou et al., 2016; Zhang and Patel, 2016), signal (Caiafa and Cichocki, 2013; Huang and Aviyente, 2007), and NLP (Subramanian et al., 2018; Kober et al., 2016) applications. Several sparse models have been proposed to produce sparse embeddings. For example, some previous works trained word embeddings with sparse or non-negative constraints (Murphy et al., 2012; Luo et al., 2015). Linguistically inspired dimensions (Faruqui et al., 2015) is another way to increase sparsity and interpretability. SPINE (SParse Interpretable Neural Embeddings) (Subramanian et al., 2018), a variant of denoising k-sparse autoencoder, can generate efficient and interpretable distributed word representations. Our method is different from these approaches. We not only construct sparse representations but also transform between dense and sparse spaces. We also combine word sparse representations to produce sentence representations. Some recent studies tried to achieve sparsity in novel ways (P"
2020.coling-main.290,D11-1139,0,0.0390268,"ations, especially for explanation (Fyshe et al., 2014; Faruqui and Dyer, 2015). In this sense, sparse representation may hold the key to solving the explainability problem of deep neural networks. Apart from the interpretability property, sparse representations can also improve the usability of word vectors as features. The embeddings with good sparsity, interpretability or special meanings can also benefit downstream tasks (Guo et al., 2014; Chang et al., 2018). Several tasks have benefited from sparse representations, e.g., part-of-speech tagging (Ganchev et al., 2009), dependency parsing (Martins et al., 2011), and supervised classification (Yogatama and Smith, 2014). However, much of the research advances so far for NLP tasks are based on dense representations, e.g., text classification (Kim, 2014; Tang et al., 2015; Wu et al., 2017; Wang et al., 2018), natural language inference (Liu et al., 2019; Kim et al., 2019), machine translation (Cheng, 2019; He et al., 2016) and generation (Serban et al., 2017; Zhang et al., 2019; Du and Cardie, 2017). The study of sparse representations is still limited. There are two key limitations in the study of sparse representations. First, little work has been don"
2020.coling-main.290,C12-1118,0,0.034484,"ed responses. That means neutral class has similar semantemes to those of both positive and negative classes. This demonstrates that the neutral class is more difficult to identify. 4 Related Work Sparse embeddings have been used in image (Ji et al., 2019; Zhou et al., 2016; Zhang and Patel, 2016), signal (Caiafa and Cichocki, 2013; Huang and Aviyente, 2007), and NLP (Subramanian et al., 2018; Kober et al., 2016) applications. Several sparse models have been proposed to produce sparse embeddings. For example, some previous works trained word embeddings with sparse or non-negative constraints (Murphy et al., 2012; Luo et al., 2015). Linguistically inspired dimensions (Faruqui et al., 2015) is another way to increase sparsity and interpretability. SPINE (SParse Interpretable Neural Embeddings) (Subramanian et al., 2018), a variant of denoising k-sparse autoencoder, can generate efficient and interpretable distributed word representations. Our method is different from these approaches. We not only construct sparse representations but also transform between dense and sparse spaces. We also combine word sparse representations to produce sentence representations. Some recent studies tried to achieve sparsi"
2020.coling-main.290,D17-1041,0,0.0200617,"). Linguistically inspired dimensions (Faruqui et al., 2015) is another way to increase sparsity and interpretability. SPINE (SParse Interpretable Neural Embeddings) (Subramanian et al., 2018), a variant of denoising k-sparse autoencoder, can generate efficient and interpretable distributed word representations. Our method is different from these approaches. We not only construct sparse representations but also transform between dense and sparse spaces. We also combine word sparse representations to produce sentence representations. Some recent studies tried to achieve sparsity in novel ways (Park et al., 2017). We also proposed a novel method in this paper and experimentally verified its effectiveness. 5 Conclusion and Future Works This paper proposed a novel method to transform representations between dense and sparse spaces, and a technique to combine semantics in the sparse space. It also proposed and experimentally verified a new activation function that can be used to achieve sparseness. Natural language inference and text classification tasks were used to evaluate the proposed transformations with promising results. Based on this study, many other interesting directions can be pursued in the"
2020.coling-main.290,P14-1074,0,0.0235891,"; Faruqui and Dyer, 2015). In this sense, sparse representation may hold the key to solving the explainability problem of deep neural networks. Apart from the interpretability property, sparse representations can also improve the usability of word vectors as features. The embeddings with good sparsity, interpretability or special meanings can also benefit downstream tasks (Guo et al., 2014; Chang et al., 2018). Several tasks have benefited from sparse representations, e.g., part-of-speech tagging (Ganchev et al., 2009), dependency parsing (Martins et al., 2011), and supervised classification (Yogatama and Smith, 2014). However, much of the research advances so far for NLP tasks are based on dense representations, e.g., text classification (Kim, 2014; Tang et al., 2015; Wu et al., 2017; Wang et al., 2018), natural language inference (Liu et al., 2019; Kim et al., 2019), machine translation (Cheng, 2019; He et al., 2016) and generation (Serban et al., 2017; Zhang et al., 2019; Du and Cardie, 2017). The study of sparse representations is still limited. There are two key limitations in the study of sparse representations. First, little work has been done to well connect dense and sparse spaces. The two types o"
2020.coling-main.290,D18-1350,0,0.130254,"maintaining the semantic information consistency. The last term helps learn similar representations with LSTM. Table 1: Average accuracy over all tasks. Y and X’ are representations for making predictions (X’ is the back transformation of Y; Y is the sparse representation). Traditional penalty means the partial sparsity loss in (Subramanian et al., 2018). Helper loss refers to ML or BL. Note that only the experiments using X’ as the representations for prediction has RLo . RLo is not used when using Y as the prediction feature. Model CNN (Kim, 2014) Transformer (Vaswani et al., 2017) Capsule (Zhao et al., 2018) LSTM (Hochreiter and Schmidhuber, 1997) ST¶ [X’] (without sparse activation or helper loss) ST‡ [X’] (with sparse activation, without helper loss) † ST [X’] (using the traditional penalty, without sparse activation or helper loss) ST[X’] (full model) ST¶ [Y] (without sparse activation or helper loss) ST‡ [Y] (with sparse activation, without helper loss) † ST [Y] (using the traditional penalty, without sparse activation or helper loss) ST[Y] (full model) 3 SNLI 59.71 55.32 54.53 66.66 32.90 63.34 59.89 66.58 62.46 63.53 62.62 66.85 MR 76.10 75.23 72.57 71.04 61.07 70.38 65.51 71.16 68.32 76.38"
2020.emnlp-main.272,2021.ccl-1.108,0,0.133441,"Missing"
2020.emnlp-main.272,P15-1152,0,0.0416797,"aper are three-fold: (1) proposal of a knowledge selection module for applying pre-trained language models to the task of knowledge-grounded dialogue generation; (2) proposal of an unsupervised approach in which learning of knowledge selection and fine-tuning of the pre-trained model are conducted in a joint manner; and (3) empirical verification of the effectiveness of the proposed method on benchmarks of knowledge-grounded dialogue generation. 3378 2 Related Work Early work on end-to-end open domain dialogue generation is inspired by the research of machine translation (Ritter et al., 2011; Shang et al., 2015; Vinyals and Le, 2015). Later, the vanilla encoderdecoder architecture is widely extended to improve diversity of responses (Li et al., 2015; Xing et al., 2017a; Zhao et al., 2017; Tao et al., 2018); to model the structure of conversation contexts (Serban et al., 2016, 2017; Xing et al., 2017b; Zhang et al., 2019a); to control attributes of responses (Xu et al., 2019; Zhou et al., 2017; Zhang et al., 2018a; Wang et al., 2018; See et al., 2019); and to bias responses to some specific personas (Li et al., 2016; Zhang et al., 2018b). Recently, grounding dialogue generation by extra knowledge is"
2020.emnlp-main.272,P19-1081,0,0.0228249,"t al., 2017a; Zhao et al., 2017; Tao et al., 2018); to model the structure of conversation contexts (Serban et al., 2016, 2017; Xing et al., 2017b; Zhang et al., 2019a); to control attributes of responses (Xu et al., 2019; Zhou et al., 2017; Zhang et al., 2018a; Wang et al., 2018; See et al., 2019); and to bias responses to some specific personas (Li et al., 2016; Zhang et al., 2018b). Recently, grounding dialogue generation by extra knowledge is emerging as an important step towards human-like conversational AI, where the knowledge could be obtained from knowledge graphs (Zhou et al., 2018a; Moon et al., 2019; Tuan et al., 2019), retrieved from unstructured documents (Dinan et al., 2019; Lian et al., 2019; Zhao et al., 2020; Kim et al., 2020), or extracted from visual background (Mostafazadeh et al., 2017; Shuster et al., 2018; Huber et al., 2018). In this work, we study document-grounded dialogue generation. Rather than learning from scratch like most existing work, we take advantage of the pre-trained language models and achieve new stateof-the-art on the benchmarks of the task. Big, deep neural language models pre-trained on huge unlabeled text corpus have led to strong improvements on numerous"
2020.emnlp-main.272,I17-1047,0,0.0592179,"of responses (Xu et al., 2019; Zhou et al., 2017; Zhang et al., 2018a; Wang et al., 2018; See et al., 2019); and to bias responses to some specific personas (Li et al., 2016; Zhang et al., 2018b). Recently, grounding dialogue generation by extra knowledge is emerging as an important step towards human-like conversational AI, where the knowledge could be obtained from knowledge graphs (Zhou et al., 2018a; Moon et al., 2019; Tuan et al., 2019), retrieved from unstructured documents (Dinan et al., 2019; Lian et al., 2019; Zhao et al., 2020; Kim et al., 2020), or extracted from visual background (Mostafazadeh et al., 2017; Shuster et al., 2018; Huber et al., 2018). In this work, we study document-grounded dialogue generation. Rather than learning from scratch like most existing work, we take advantage of the pre-trained language models and achieve new stateof-the-art on the benchmarks of the task. Big, deep neural language models pre-trained on huge unlabeled text corpus have led to strong improvements on numerous natural language understanding and natural language generation benchmarks (Devlin et al., 2018; Yang et al., 2019; Liu et al., 2019; Radford et al., 2019; Song et al., 2019; Dong et al., 2019; Lewis"
2020.emnlp-main.272,N19-1035,0,0.0282509,"an learning from scratch like most existing work, we take advantage of the pre-trained language models and achieve new stateof-the-art on the benchmarks of the task. Big, deep neural language models pre-trained on huge unlabeled text corpus have led to strong improvements on numerous natural language understanding and natural language generation benchmarks (Devlin et al., 2018; Yang et al., 2019; Liu et al., 2019; Radford et al., 2019; Song et al., 2019; Dong et al., 2019; Lewis et al., 2019), and therefore are revolutionizing almost the full spectrum of NLP applications (Raffel et al., 2019; Sun et al., 2019b; Qiao et al., 2019; Zhang et al., 2019b; Lample and Conneau, 2019) and some interdisciplinary applications in NLP and computer vision (Lu et al., 2019; Su et al., 2019; Sun et al., 2019a). In the context of dialogue generation, by fine-tuning GPT-2 (Radford et al., 2019) in different sizes on social media data, recent work has (Zhang et al., 2019c; Wolf et al., 2019) shown promising progress on conversation engagement and commonsense questionanswering. In this work, we further explore the application of pre-training to the task of open domain dialogue generation by equipping the pre-trained"
2020.emnlp-main.272,D19-1194,0,0.0334788,"et al., 2017; Tao et al., 2018); to model the structure of conversation contexts (Serban et al., 2016, 2017; Xing et al., 2017b; Zhang et al., 2019a); to control attributes of responses (Xu et al., 2019; Zhou et al., 2017; Zhang et al., 2018a; Wang et al., 2018; See et al., 2019); and to bias responses to some specific personas (Li et al., 2016; Zhang et al., 2018b). Recently, grounding dialogue generation by extra knowledge is emerging as an important step towards human-like conversational AI, where the knowledge could be obtained from knowledge graphs (Zhou et al., 2018a; Moon et al., 2019; Tuan et al., 2019), retrieved from unstructured documents (Dinan et al., 2019; Lian et al., 2019; Zhao et al., 2020; Kim et al., 2020), or extracted from visual background (Mostafazadeh et al., 2017; Shuster et al., 2018; Huber et al., 2018). In this work, we study document-grounded dialogue generation. Rather than learning from scratch like most existing work, we take advantage of the pre-trained language models and achieve new stateof-the-art on the benchmarks of the task. Big, deep neural language models pre-trained on huge unlabeled text corpus have led to strong improvements on numerous natural language un"
2020.emnlp-main.272,P18-1204,0,0.0159904,"ialogue generation. 3378 2 Related Work Early work on end-to-end open domain dialogue generation is inspired by the research of machine translation (Ritter et al., 2011; Shang et al., 2015; Vinyals and Le, 2015). Later, the vanilla encoderdecoder architecture is widely extended to improve diversity of responses (Li et al., 2015; Xing et al., 2017a; Zhao et al., 2017; Tao et al., 2018); to model the structure of conversation contexts (Serban et al., 2016, 2017; Xing et al., 2017b; Zhang et al., 2019a); to control attributes of responses (Xu et al., 2019; Zhou et al., 2017; Zhang et al., 2018a; Wang et al., 2018; See et al., 2019); and to bias responses to some specific personas (Li et al., 2016; Zhang et al., 2018b). Recently, grounding dialogue generation by extra knowledge is emerging as an important step towards human-like conversational AI, where the knowledge could be obtained from knowledge graphs (Zhou et al., 2018a; Moon et al., 2019; Tuan et al., 2019), retrieved from unstructured documents (Dinan et al., 2019; Lian et al., 2019; Zhao et al., 2020; Kim et al., 2020), or extracted from visual background (Mostafazadeh et al., 2017; Shuster et al., 2018; Huber et al., 2018). In this work, we s"
2020.emnlp-main.272,P19-1538,1,0.866264,"the proposed method on benchmarks of knowledge-grounded dialogue generation. 3378 2 Related Work Early work on end-to-end open domain dialogue generation is inspired by the research of machine translation (Ritter et al., 2011; Shang et al., 2015; Vinyals and Le, 2015). Later, the vanilla encoderdecoder architecture is widely extended to improve diversity of responses (Li et al., 2015; Xing et al., 2017a; Zhao et al., 2017; Tao et al., 2018); to model the structure of conversation contexts (Serban et al., 2016, 2017; Xing et al., 2017b; Zhang et al., 2019a); to control attributes of responses (Xu et al., 2019; Zhou et al., 2017; Zhang et al., 2018a; Wang et al., 2018; See et al., 2019); and to bias responses to some specific personas (Li et al., 2016; Zhang et al., 2018b). Recently, grounding dialogue generation by extra knowledge is emerging as an important step towards human-like conversational AI, where the knowledge could be obtained from knowledge graphs (Zhou et al., 2018a; Moon et al., 2019; Tuan et al., 2019), retrieved from unstructured documents (Dinan et al., 2019; Lian et al., 2019; Zhao et al., 2020; Kim et al., 2020), or extracted from visual background (Mostafazadeh et al., 2017; Sh"
2020.emnlp-main.272,P18-1205,0,0.0519876,"knowledge-grounded dialogue generation. 3378 2 Related Work Early work on end-to-end open domain dialogue generation is inspired by the research of machine translation (Ritter et al., 2011; Shang et al., 2015; Vinyals and Le, 2015). Later, the vanilla encoderdecoder architecture is widely extended to improve diversity of responses (Li et al., 2015; Xing et al., 2017a; Zhao et al., 2017; Tao et al., 2018); to model the structure of conversation contexts (Serban et al., 2016, 2017; Xing et al., 2017b; Zhang et al., 2019a); to control attributes of responses (Xu et al., 2019; Zhou et al., 2017; Zhang et al., 2018a; Wang et al., 2018; See et al., 2019); and to bias responses to some specific personas (Li et al., 2016; Zhang et al., 2018b). Recently, grounding dialogue generation by extra knowledge is emerging as an important step towards human-like conversational AI, where the knowledge could be obtained from knowledge graphs (Zhou et al., 2018a; Moon et al., 2019; Tuan et al., 2019), retrieved from unstructured documents (Dinan et al., 2019; Lian et al., 2019; Zhao et al., 2020; Kim et al., 2020), or extracted from visual background (Mostafazadeh et al., 2017; Shuster et al., 2018; Huber et al., 2018)"
2020.emnlp-main.272,P19-1499,0,0.294003,"pen domain dialogue generation. Prototypes ∗ Corresponding author: Rui Yan (ruiyan@pku.edu.cn). Context I just discovered star trek and I really like watching star trek . Gene Roddenberry created it based upon science fiction and it is American media. ... If I remember Captain Kirk was not the original captain . The Star Trek Canon of the series an animated had 5 spin offs. I watched a little of the next generation but could not get into it like i did with the original show . Response These adventures went on but were short lived and six feature films. I think it’s worth it. such as DialoGPT (Zhang et al., 2019c) have exhibited compelling performance on generating responses that make sense under conversation contexts and at the same time carry specific content for keeping the conversation going. While the giant language models can memorize enough patterns in language during pre-training, they only capture “average” semantics of the data (Zhang et al., 2019c). As a result, responses could still be bland or inappropriate when specific knowledge is required, as illustrated by the example in Table 1. The other line is to ground dialogue generation by extra knowledge such as unstructured documents (Zhao"
2020.emnlp-main.272,P17-1061,0,0.0304824,"unsupervised approach in which learning of knowledge selection and fine-tuning of the pre-trained model are conducted in a joint manner; and (3) empirical verification of the effectiveness of the proposed method on benchmarks of knowledge-grounded dialogue generation. 3378 2 Related Work Early work on end-to-end open domain dialogue generation is inspired by the research of machine translation (Ritter et al., 2011; Shang et al., 2015; Vinyals and Le, 2015). Later, the vanilla encoderdecoder architecture is widely extended to improve diversity of responses (Li et al., 2015; Xing et al., 2017a; Zhao et al., 2017; Tao et al., 2018); to model the structure of conversation contexts (Serban et al., 2016, 2017; Xing et al., 2017b; Zhang et al., 2019a); to control attributes of responses (Xu et al., 2019; Zhou et al., 2017; Zhang et al., 2018a; Wang et al., 2018; See et al., 2019); and to bias responses to some specific personas (Li et al., 2016; Zhang et al., 2018b). Recently, grounding dialogue generation by extra knowledge is emerging as an important step towards human-like conversational AI, where the knowledge could be obtained from knowledge graphs (Zhou et al., 2018a; Moon et al., 2019; Tuan et al.,"
2020.emnlp-main.272,D18-1076,0,0.166245,"d still be bland or inappropriate when specific knowledge is required, as illustrated by the example in Table 1. The other line is to ground dialogue generation by extra knowledge such as unstructured documents (Zhao et al., 2020). By the means, the documents (e.g., wiki articles) serve as content sources, and make a dialogue system knowledgeable regarding to a variety of concepts in discussion. However, collecting enough dialogues that are naturally grounded on documents for model training is not trivial. Although some benchmarks built upon crowd-sourcing have been released by recent papers (Zhou et al., 2018b; Dinan et al., 2019; Gopalakrishnan et al., 2019), the small training size 3377 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 3377–3390, c November 16–20, 2020. 2020 Association for Computational Linguistics makes the generation models generalize badly on unseen topics (Dinan et al., 2019) and the cost of building such data also prevents from transferring the techniques proved on the benchmarks to new domains and new languages. Encouraged by the results on pre-training for dialogue generation and knowledge-grounded dialogue generation, and moti"
2020.emnlp-main.272,P19-1362,0,0.409394,"pen domain dialogue generation. Prototypes ∗ Corresponding author: Rui Yan (ruiyan@pku.edu.cn). Context I just discovered star trek and I really like watching star trek . Gene Roddenberry created it based upon science fiction and it is American media. ... If I remember Captain Kirk was not the original captain . The Star Trek Canon of the series an animated had 5 spin offs. I watched a little of the next generation but could not get into it like i did with the original show . Response These adventures went on but were short lived and six feature films. I think it’s worth it. such as DialoGPT (Zhang et al., 2019c) have exhibited compelling performance on generating responses that make sense under conversation contexts and at the same time carry specific content for keeping the conversation going. While the giant language models can memorize enough patterns in language during pre-training, they only capture “average” semantics of the data (Zhang et al., 2019c). As a result, responses could still be bland or inappropriate when specific knowledge is required, as illustrated by the example in Table 1. The other line is to ground dialogue generation by extra knowledge such as unstructured documents (Zhao"
2020.emnlp-main.272,P18-1102,0,0.0223918,"knowledge-grounded dialogue generation. 3378 2 Related Work Early work on end-to-end open domain dialogue generation is inspired by the research of machine translation (Ritter et al., 2011; Shang et al., 2015; Vinyals and Le, 2015). Later, the vanilla encoderdecoder architecture is widely extended to improve diversity of responses (Li et al., 2015; Xing et al., 2017a; Zhao et al., 2017; Tao et al., 2018); to model the structure of conversation contexts (Serban et al., 2016, 2017; Xing et al., 2017b; Zhang et al., 2019a); to control attributes of responses (Xu et al., 2019; Zhou et al., 2017; Zhang et al., 2018a; Wang et al., 2018; See et al., 2019); and to bias responses to some specific personas (Li et al., 2016; Zhang et al., 2018b). Recently, grounding dialogue generation by extra knowledge is emerging as an important step towards human-like conversational AI, where the knowledge could be obtained from knowledge graphs (Zhou et al., 2018a; Moon et al., 2019; Tuan et al., 2019), retrieved from unstructured documents (Dinan et al., 2019; Lian et al., 2019; Zhao et al., 2020; Kim et al., 2020), or extracted from visual background (Mostafazadeh et al., 2017; Shuster et al., 2018; Huber et al., 2018)"
2020.emnlp-main.313,K16-1002,0,0.0224319,"ucts. (2) We build the MGenNet to generate the copywriting of each product based on the multi-agent communication framework. (3) We combine the above networks as an end-to-end SMG Net which can generate attractive AD posts. 2 Related Work Text generation. Text generation has become one of the hottest subfields of natural language processing. Previous researches mainly focus on several popular text generation tasks, such as dialogue generation (Serban et al., 2016; Tao et al., 2018a; Hu et al., 2019; Chan et al., 2019b) and story generation (Xu et al., 2018; Li et al., 2019; Yao et al., 2019). Bowman et al. (2016) are proposed to improve wording novelty. Serban et al. (2017) target to the intra-sentence consistency and thematic consistency is improved by Fan et al. (2018); Litschko et al. (2018). Besides, text generation from different data formats have also been widely studied in recent years, e.g., table-to-text generation (Liu et al., 2018), which can fit various data formats in 3819 real-world scenarios. Multi-Agent Communication. CommNet proposed by Sukhbaatar et al. (2016) is the first deep learning framework for multi-agent communication. There are several previous works built on the CommNet fra"
2020.emnlp-main.313,N18-1150,0,0.097371,", 2018), which can fit various data formats in 3819 real-world scenarios. Multi-Agent Communication. CommNet proposed by Sukhbaatar et al. (2016) is the first deep learning framework for multi-agent communication. There are several previous works built on the CommNet framework, for example, researchers Add & Norm use the multi-agent communication method to play FFNal., 2017). Morthe starcraft games (Vinyals et datch and Abbeel (2018) deal with natural language Add & Norm processing tasks such as machine translation and W-Sum sentiment analysis with the multi-agent communiV-Attn cation method. Celikyilmaz et al. (2018) present K V the first study using theQ multi-agent framework for summarization. Product descriptions generation. Product description copywriting is critical for the e-commerce platform, and automatically generating the product description copywriting has attracted considerable interest from both academia and industry because of its importance. Wang et al. (2017) first focus on the product description generation task and incorporates the preset template to generate product descriptions automatically. With the development of deep learning, Zhang et al. (2019) proposed a pointer-based generation"
2020.emnlp-main.313,D19-1501,1,0.679067,"opriate combination of the products based on the post topic and the relationship among the products. (2) We build the MGenNet to generate the copywriting of each product based on the multi-agent communication framework. (3) We combine the above networks as an end-to-end SMG Net which can generate attractive AD posts. 2 Related Work Text generation. Text generation has become one of the hottest subfields of natural language processing. Previous researches mainly focus on several popular text generation tasks, such as dialogue generation (Serban et al., 2016; Tao et al., 2018a; Hu et al., 2019; Chan et al., 2019b) and story generation (Xu et al., 2018; Li et al., 2019; Yao et al., 2019). Bowman et al. (2016) are proposed to improve wording novelty. Serban et al. (2017) target to the intra-sentence consistency and thematic consistency is improved by Fan et al. (2018); Litschko et al. (2018). Besides, text generation from different data formats have also been widely studied in recent years, e.g., table-to-text generation (Liu et al., 2018), which can fit various data formats in 3819 real-world scenarios. Multi-Agent Communication. CommNet proposed by Sukhbaatar et al. (2016) is the first deep learning"
2020.emnlp-main.313,D19-1201,1,0.628667,"opriate combination of the products based on the post topic and the relationship among the products. (2) We build the MGenNet to generate the copywriting of each product based on the multi-agent communication framework. (3) We combine the above networks as an end-to-end SMG Net which can generate attractive AD posts. 2 Related Work Text generation. Text generation has become one of the hottest subfields of natural language processing. Previous researches mainly focus on several popular text generation tasks, such as dialogue generation (Serban et al., 2016; Tao et al., 2018a; Hu et al., 2019; Chan et al., 2019b) and story generation (Xu et al., 2018; Li et al., 2019; Yao et al., 2019). Bowman et al. (2016) are proposed to improve wording novelty. Serban et al. (2017) target to the intra-sentence consistency and thematic consistency is improved by Fan et al. (2018); Litschko et al. (2018). Besides, text generation from different data formats have also been widely studied in recent years, e.g., table-to-text generation (Liu et al., 2018), which can fit various data formats in 3819 real-world scenarios. Multi-Agent Communication. CommNet proposed by Sukhbaatar et al. (2016) is the first deep learning"
2020.emnlp-main.313,D15-1166,0,0.00704384,"in the j-th step as: sum{Iji } . Iˆji = M −1 (11) The Iˆji contains all the information of other agents, and the i-th agent can get more information as the prior for generating. After obtaining the information Iˆji , we attach it as an extra input to the corresponding agent. We use the below equation to express this process yt0 = ([yt ; Iˆji ; at·t ] · Wy ) + by , a·t+1 = Agent(a·t , yt0 ), (12) where Wy , by are trainable parameters. yt is the input of agent at t-th time step and the at·t is the attention vector which is calculated from the corresponding product RNN encoder status as same as Luong et al. (2015). Then, we use a linear layer to obtain the generated word. Finally, we can use the beam search algorithm to get all copywriting Cˆ = {ˆ ud1 , u ˆd2 , · · · , u ˆdM } which d ,w d ,··· ,w d u ˆdi = {w ˆi,1 ˆi,2 ˆi,L n }. 3822 c,i 4.3 Training Objection Table 1: Criteria of human evaluation. To start with, we combine SelectNet and MGenNet as an end-to-end framework. We launch the following objective to minimize the MLE loss between the ground truth and generated copywriting. Meanwhile, we minimize the loss between selected product ground truth in real multi-product AD post, and the objective fu"
2020.emnlp-main.313,P18-1082,0,0.156792,"ral encoder-decoder framework shows remarkable effects on various text generation tasks, e.g., sum∗ This work was done while Z. Chan was an intern at Alibaba Group. Y. Zhang works at Ant Group now. † Corresponding Author: Rui Yan (ruiyan@pku.edu.cn). 1 https://www.amazon.com/ https://www.taobao.com/ 3 https://www.jd.com/ 2 3818 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 3818–3829, c November 16–20, 2020. 2020 Association for Computational Linguistics marization (See et al., 2017; Chen et al., 2019b; Gao et al., 2019a, 2020), story generation (Fan et al., 2018; Li et al., 2019; Yao et al., 2019) and so on. The researchers from academia and industry begin to explore how to generate product advertising copywriting through deep learning methods. Zhang et al. (2019) propose a pointer-generator model to generate the product advertising copywriting whose patterns are controlled. Chen et al. (2019a) explore a new way to generate personalized product copywriting by enhancing the transformer with an extra knowledge base. However, all the previous studies focus on copywriting generation for a single product. In such case, consumers need to compare among prod"
2020.emnlp-main.313,P08-1028,0,0.0799334,"Evaluation Metrics As mentioned in Section 3, we ignore the order of products in the post and regard each product as equal. Hence, traditional metrics such as BLEU and ROUGE, are unsuitable in this scenario. To evaluate the results of the generated AD post, we adopt the following widely used metrics. Embedding Metrics. To obtain semantic matches between the generated copywriting and ground-truth, we perform evaluation using the embedding metrics. Following Gao et al. (2019b), we calculate three measures: 1) Average, cosine similarity between the averaged word embeddings in the two utterances (Mitchell and Lapata, 2008); 2) Greedy, i.e., greedily matching words in two utterances based on the cosine similarities, and the total scores are then averaged across all words (Rus and Lintean, 2012); 3) Extrema, cosine similarity between the largest values among the word embeddings in the two utterances (Forgues et al., 2014). The used word2vec embedding is trained by ourselves because there is no open-access e-commerce embedding. Distinct Metrics. We use distinct scores to reflect the diversity of the copywriting. Dist-n is defined as the ratio of unique n-grams (n = 1,2,3,4) overall n-grams in the generated copywri"
2020.emnlp-main.313,W12-2018,0,0.0897866,"unsuitable in this scenario. To evaluate the results of the generated AD post, we adopt the following widely used metrics. Embedding Metrics. To obtain semantic matches between the generated copywriting and ground-truth, we perform evaluation using the embedding metrics. Following Gao et al. (2019b), we calculate three measures: 1) Average, cosine similarity between the averaged word embeddings in the two utterances (Mitchell and Lapata, 2008); 2) Greedy, i.e., greedily matching words in two utterances based on the cosine similarities, and the total scores are then averaged across all words (Rus and Lintean, 2012); 3) Extrema, cosine similarity between the largest values among the word embeddings in the two utterances (Forgues et al., 2014). The used word2vec embedding is trained by ourselves because there is no open-access e-commerce embedding. Distinct Metrics. We use distinct scores to reflect the diversity of the copywriting. Dist-n is defined as the ratio of unique n-grams (n = 1,2,3,4) overall n-grams in the generated copywriting. Following Gu et al. (2018), we define intra-dist as the average of distinct values within each copywriting and inter-dist as the distinct value among all copywriting. H"
2020.emnlp-main.313,P17-1099,0,0.0251345,"ed on the predefined template. With the surge of deep learning techniques, the neural encoder-decoder framework shows remarkable effects on various text generation tasks, e.g., sum∗ This work was done while Z. Chan was an intern at Alibaba Group. Y. Zhang works at Ant Group now. † Corresponding Author: Rui Yan (ruiyan@pku.edu.cn). 1 https://www.amazon.com/ https://www.taobao.com/ 3 https://www.jd.com/ 2 3818 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 3818–3829, c November 16–20, 2020. 2020 Association for Computational Linguistics marization (See et al., 2017; Chen et al., 2019b; Gao et al., 2019a, 2020), story generation (Fan et al., 2018; Li et al., 2019; Yao et al., 2019) and so on. The researchers from academia and industry begin to explore how to generate product advertising copywriting through deep learning methods. Zhang et al. (2019) propose a pointer-generator model to generate the product advertising copywriting whose patterns are controlled. Chen et al. (2019a) explore a new way to generate personalized product copywriting by enhancing the transformer with an extra knowledge base. However, all the previous studies focus on copywriting g"
2020.emnlp-main.313,J19-1005,0,0.0172083,"odel based on Transformer. We adapt the model for our scenario by removing the personal module. We cannot generate copywriting for each product individually when we don’t know the corresponding relation. To apply these baseline models to our scenario, we separate the description text of each product in the post with a unique character “<SOP>” and then concatenate the whole text as the final target for training. To analyze our SelectNet, we select several models for text matching as baselines. 1). RNN (Liu et al., 2016). A model uses RNN to model input as the hidden state to classify. 2). SCN (Wu et al., 2019). A classic retrieval model for response selection. The model lets the post title interact with product information and transforms interaction matrices into a matching vector with CNN. 3). MRFN (Tao et al., 2019). A strong retrieval3823 Table 2: The automatic metric result of our model and baselines. Since the number of sentences generated by ConvSeq is extremely small, so evaluating the intra-dist metric is meaningless, and we omit the intra-dist score. The results of our methodologies are significant with p-value <0.05 measured by t-test over the best baseline. Models Embedding Metrics Inter"
2020.emnlp-main.313,D18-1462,0,0.0286482,"on the post topic and the relationship among the products. (2) We build the MGenNet to generate the copywriting of each product based on the multi-agent communication framework. (3) We combine the above networks as an end-to-end SMG Net which can generate attractive AD posts. 2 Related Work Text generation. Text generation has become one of the hottest subfields of natural language processing. Previous researches mainly focus on several popular text generation tasks, such as dialogue generation (Serban et al., 2016; Tao et al., 2018a; Hu et al., 2019; Chan et al., 2019b) and story generation (Xu et al., 2018; Li et al., 2019; Yao et al., 2019). Bowman et al. (2016) are proposed to improve wording novelty. Serban et al. (2017) target to the intra-sentence consistency and thematic consistency is improved by Fan et al. (2018); Litschko et al. (2018). Besides, text generation from different data formats have also been widely studied in recent years, e.g., table-to-text generation (Liu et al., 2018), which can fit various data formats in 3819 real-world scenarios. Multi-Agent Communication. CommNet proposed by Sukhbaatar et al. (2016) is the first deep learning framework for multi-agent communication."
2020.emnlp-main.313,P19-1465,0,0.013975,"951 13.59 20.77 25.23 28.38 33.83 31.07 39.79 33.41 45.48 PCPG KOBE 0.8830 0.8783 540.41 539.23 0.3713 0.4023 1.409 1.523 3.943 5.334 7.423 11.34 10.43 18.32 22.43 26.46 29.98 37.43 36.80 43.23 40.31 53.84 S-MGC S-SG S-MG 0.9438 0.8774 0.9428 560.45 566.86 558.62 0.4481 0.4280 0.4440 1.763 1.294 1.713 8.051 4.059 7.502 18.37 8.479 17.21 28.30 12.71 26.60 44.66 24.76 44.49 66.22 33.06 65.97 73.57 38.72 73.26 78.96 44.09 78.64 based model in response selection. Tao et al. (2019) encode the interaction between two texts from multiple kinds of representations and study how to fuse them. 4). SETM (Yang et al., 2019). It lets the post title interact with product text and uses the representation to obtain a weight matrix for each word. 5). Self-Attn (Vaswani et al., 2017). We use the self-attention mechanism to capture the relationship among the product candidates for product selection. We also conduct the exploration of the MGenNet, and the setting is shown as follows 1). S-MGC . Our proposed multi-agent generation framework with communication strategy. 2). S-MG. The original multi-agent generation framework without communication. 3). S-SG. We replace the MGenNet with a pretrained Transformer decoder and"
2020.emnlp-main.313,D19-1011,0,0.0278928,"y these baseline models to our scenario, we separate the description text of each product in the post with a unique character “<SOP>” and then concatenate the whole text as the final target for training. To analyze our SelectNet, we select several models for text matching as baselines. 1). RNN (Liu et al., 2016). A model uses RNN to model input as the hidden state to classify. 2). SCN (Wu et al., 2019). A classic retrieval model for response selection. The model lets the post title interact with product information and transforms interaction matrices into a matching vector with CNN. 3). MRFN (Tao et al., 2019). A strong retrieval3823 Table 2: The automatic metric result of our model and baselines. Since the number of sentences generated by ConvSeq is extremely small, so evaluating the intra-dist metric is meaningless, and we omit the intra-dist score. The results of our methodologies are significant with p-value <0.05 measured by t-test over the best baseline. Models Embedding Metrics Inter-Distinct Intra-Distinct Average Greedy Extrema Dist-1 Dist-2 Dist-3 Dist-4 Dist-1 Dist-2 Dist-3 Dist-4 Seq2seq ConvSeq Transformer 0.9197 0.6049 0.8662 548.69 326.99 537.69 0.4293 0.1123 0.3941 0.937 1.308 1.473"
2020.emnlp-main.313,I17-2032,0,0.137962,"ge factor in e-commerce, and well-written advertising copywriting can encourage consumers to understand further and purchase products. However, there is an important restriction factor for traditional advertising copywriting, i.e., the writing efficiency of human copywriters cannot match the growth rate of new products. Many e-commerce websites, such as Amazon1 , Taobao2 and JD3 , have billions of products, so it is impossible to write all copywriting manually. To address this issue, researchers pay more and more attention to the automatic advertising copywriting generation. The initial work (Wang et al., 2017) on automatic advertising copywriting generation is based on the predefined template. With the surge of deep learning techniques, the neural encoder-decoder framework shows remarkable effects on various text generation tasks, e.g., sum∗ This work was done while Z. Chan was an intern at Alibaba Group. Y. Zhang works at Ant Group now. † Corresponding Author: Rui Yan (ruiyan@pku.edu.cn). 1 https://www.amazon.com/ https://www.taobao.com/ 3 https://www.jd.com/ 2 3818 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 3818–3829, c November 16–20, 2020. 2020"
2020.emnlp-main.591,Q17-1010,0,0.0118385,"ities involved. Figure 1 shows the architecture of our model. IEN consists of three levels. The bottom is for word-level language comprehension, which encodes words to distributed vectors. The middle is for sentence-level process understanding, which conducts entity state tracking. At the top, we use different classifiers to predict entities’ actions and locations, respectively. 1 http://www.nltk.org/ Word-Level Encoding Given a procedural text, our model first encodes each word wi in the paragraph to a vector wi = [emb(wi ); vi ]. Here, emb(wi ) is an embedding function, and we use fastText (Bojanowski et al., 2017) and ELMo (Peters et al., 2018) for experiments. vi is a scalar binary indicator for identifying whether wi is a verb. Then we use a BiLSTM (Hochreiter and Schmidhuber, 1997) over the whole paragraph for contextual encoding. We denote ui = BiLSTM([wi ]) as the output of BiLSTM with respect to word wi . Sentence-Level Encoding and IEN cell To track the state changes, we extract sentence features from word-level encodings by running another RNN at the sentence level. In order to take entity interactions into consideration, we propose a novel IEN cell that leverages attention mechanisms to help e"
2020.emnlp-main.591,N18-1144,0,0.262776,"ly find that carbon dioxide is 7281 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 7281–7290, c November 16–20, 2020. 2020 Association for Computational Linguistics just taken away from the blood thus impossible to be in the blood. Furthermore if we look back to the first sentence, we can finally get the right answer, lung. This tells us that an entity’s location is closely related or even determined by the most recent event it involves in. However, current systems either model a general state of an entity regardless of specific tracking targets (Dalvi et al., 2018), or model different tracking targets (e.g., action, location) completely separately (Gupta and Durrett, 2019), without considering the relationship between different tracking targets. In this work, we focus on the scientific process understanding task (Dalvi et al., 2018), in which the tracking targets are action and location of entities. We propose a novel Interactive Entity Network, IEN, that explicitly models the interactions among multiple entities and explores the relationship between an entity’s action and its location. IEN is a two-layer RNN model, the bottom RNN encodes word-level inf"
2020.emnlp-main.591,N19-1423,0,0.0522405,"Missing"
2020.emnlp-main.591,N19-1244,0,0.461156,"tate changes described in each individual sentence. ProGlobal (Dalvi et al., 2018) considers the entire paragraph while predicting states for an entity. As the whole context is incorporated, more state changes are captured, and result in a higher recall. However, this may lead to over-prediction. To address these problems, several models are proposed to incorporate different constraints. ProStruct (Tandon et al., 2018) reformulate the procedural text comprehension task as a structured prediction task, and incorporates a set of commonsense constraints for globally consistent predictions. LACE (Du et al., 2019) leverages label consistency among different paragraphs on the same topic during training. NCET (Gupta and Durrett, 2019) uses a neural CRF to explicitly capture the constrains. Other interesting attempts include KG-MRC (Das et al., 2018), which constructs dynamic bipartite graphs from the procedural text, and updates the graphs 7282 Figure 1: The left is an example of the procedural text comprehension task and ProPara dataset. The paragraph describes the process of photosynthesis in five sentences, and the system needs to track the actions and locations of the three entities listed below, wat"
2020.emnlp-main.591,W19-1502,0,0.513264,"nguage Processing, pages 7281–7290, c November 16–20, 2020. 2020 Association for Computational Linguistics just taken away from the blood thus impossible to be in the blood. Furthermore if we look back to the first sentence, we can finally get the right answer, lung. This tells us that an entity’s location is closely related or even determined by the most recent event it involves in. However, current systems either model a general state of an entity regardless of specific tracking targets (Dalvi et al., 2018), or model different tracking targets (e.g., action, location) completely separately (Gupta and Durrett, 2019), without considering the relationship between different tracking targets. In this work, we focus on the scientific process understanding task (Dalvi et al., 2018), in which the tracking targets are action and location of entities. We propose a novel Interactive Entity Network, IEN, that explicitly models the interactions among multiple entities and explores the relationship between an entity’s action and its location. IEN is a two-layer RNN model, the bottom RNN encodes word-level information, and the upper RNN encodes the sentence-level information while keeping tracking entities’ states. Sp"
2020.emnlp-main.591,D15-1114,0,0.0244217,"nd their subsequent state changes. 2) We conduct intensive experiments to show how our IEN learns to encourage the synergy among different entities involved in one event, and explain how multiple tracking targets can be properly leveraged to improve context reasoning. 2 Related Work Recently, many procedural text comprehension datasets are constructed and relesed to prompt the research in this direction. bAbI (Weston et al., 2015) is a QA dataset that the questions are about movements of entities, however it is synthetically generated and the language expression is relatively simple. RECIPES (Kiddon et al., 2015) dataset introduces the task of predicting the locations of cooking ingredients. ProPara (Dalvi et al., 2018) includes scientific procedural paragraphs, and the task is to predict the entities’ actions and locations. In this paper, we continue this line of exploration using ProPara. The solutions are mainly RNN based or memory network based. Most early models are designed for QA task, e.g., bAbI, and thus researchers pay more attention to question processing. EntNet (Henaff et al., 2016) uses dynamic memories to maintain entity states, with a gated update at each step. These states are decoded"
2020.emnlp-main.591,D18-1006,0,0.408509,"erves each context sentence through time. More recently, ProPara becomes the popular testbed and methods on ProPara focus more on state tracking. ProLocal (Dalvi et al., 2018) locally predicts the state changes described in each individual sentence. ProGlobal (Dalvi et al., 2018) considers the entire paragraph while predicting states for an entity. As the whole context is incorporated, more state changes are captured, and result in a higher recall. However, this may lead to over-prediction. To address these problems, several models are proposed to incorporate different constraints. ProStruct (Tandon et al., 2018) reformulate the procedural text comprehension task as a structured prediction task, and incorporates a set of commonsense constraints for globally consistent predictions. LACE (Du et al., 2019) leverages label consistency among different paragraphs on the same topic during training. NCET (Gupta and Durrett, 2019) uses a neural CRF to explicitly capture the constrains. Other interesting attempts include KG-MRC (Das et al., 2018), which constructs dynamic bipartite graphs from the procedural text, and updates the graphs 7282 Figure 1: The left is an example of the procedural text comprehension"
2020.emnlp-main.591,P14-5010,0,0.00602419,"Missing"
2020.emnlp-main.591,D16-1147,0,0.0251317,"lj in sentence st . Then, ( e [ut i ; uvt ], if ei ∈ st ei xt = (1) 0, otherwise l xtj = ( l [utj ; uvt ], if 0, lj ∈ st otherwise (2) where uet , ult and uvt denote the contextual encodings of the entity, location candidate and the 7284 predicate verb, respectively. If the entity or location candidate consists of multiple words, mean pooling over the word representations is used. We stack all the entity representations together to get xet ∈ Rn×d , and similarly get xlt ∈ Rm×d . xet and xlt are the inputs to the tth IEN cell. Inspired by GRU (Chung et al., 2014) and keyvalue memory networks (Miller et al., 2016), we place memory slots inside IEN cells and let them recurrently update as GRU. In each IEN cell, there are n entity slots and m location slots, corresponding to the given entity set E and the extracted location candidate set L, respectively. Each memory slot represents the state of a specific entity or a location candidate. We use het ∈ Rn×d to represent all the entity memory slots in the tth IEN cell, and use hlt ∈ Rm×d to represent all the location memory slots in the tth IEN cell. The detailed structure of an IEN cell is shown in Figure 2. First, we update the entity memory slots as follo"
2020.emnlp-main.591,N18-1202,0,0.0258934,"architecture of our model. IEN consists of three levels. The bottom is for word-level language comprehension, which encodes words to distributed vectors. The middle is for sentence-level process understanding, which conducts entity state tracking. At the top, we use different classifiers to predict entities’ actions and locations, respectively. 1 http://www.nltk.org/ Word-Level Encoding Given a procedural text, our model first encodes each word wi in the paragraph to a vector wi = [emb(wi ); vi ]. Here, emb(wi ) is an embedding function, and we use fastText (Bojanowski et al., 2017) and ELMo (Peters et al., 2018) for experiments. vi is a scalar binary indicator for identifying whether wi is a verb. Then we use a BiLSTM (Hochreiter and Schmidhuber, 1997) over the whole paragraph for contextual encoding. We denote ui = BiLSTM([wi ]) as the output of BiLSTM with respect to word wi . Sentence-Level Encoding and IEN cell To track the state changes, we extract sentence features from word-level encodings by running another RNN at the sentence level. In order to take entity interactions into consideration, we propose a novel IEN cell that leverages attention mechanisms to help entities or locations get inform"
2020.emnlp-main.591,D16-1264,0,0.0682442,"Missing"
2020.emnlp-main.752,D19-1501,1,0.845708,"ractive methods (Gao et al., 2020b). Extractive models (Zhang et al., 2018; Narayan et al., 2018; Chen et al., 2018; Luo et al., 2019; Xiao and Carenini, 2019) directly pick sentences from article and regard the aggregate of them as the summary. In contrast, abstractive models (Sutskever et al., 2014; See et al., 2017; Wenbo et al., 2019; Gui et al., 2019; Gao et al., 2019a; Chen et al., 2019a; Gao et al., 2019b) generate a summary from scratch and the abstractive summaries are typically less redundant. Multimodal Summarization. A series of works (Li et al., 2017, 2018; Palaskar et al., 2019; Chan et al., 2019; Chen et al., 2019b; Gao et al., 2020a) focused on generating better textual summaries with the help of multimodal input. Multimodal summarization with multimodal output is relatively less explored. Zhu et al. (2018) proposed to jointly generate textual summary and select the most relevant image from 6 candidates. Following their work, Zhu et al. (2020) added a multimodal objective function to use the loss from the textual summary generation and the image selection. However, in the real-world application, we usually need to choose the cover figure for a continuous video consisting of hundreds"
2020.emnlp-main.752,D18-1442,1,0.931904,"textual summary with video cover simultaneously. • We construct a large-scale dataset for VMSMO, and experimental results demonstrate that our model outperforms other baselines in terms of both automatic and human evaluations. 2 Related Work Our research builds on previous works in three fields: text summarization, multimodal summarization, and visual question answering. Text Summarization. Our proposed task bases on text summarization, the methods of which can be divided into extractive and abstractive methods (Gao et al., 2020b). Extractive models (Zhang et al., 2018; Narayan et al., 2018; Chen et al., 2018; Luo et al., 2019; Xiao and Carenini, 2019) directly pick sentences from article and regard the aggregate of them as the summary. In contrast, abstractive models (Sutskever et al., 2014; See et al., 2017; Wenbo et al., 2019; Gui et al., 2019; Gao et al., 2019a; Chen et al., 2019a; Gao et al., 2019b) generate a summary from scratch and the abstractive summaries are typically less redundant. Multimodal Summarization. A series of works (Li et al., 2017, 2018; Palaskar et al., 2019; Chan et al., 2019; Chen et al., 2019b; Gao et al., 2020a) focused on generating better textual summaries with the h"
2020.emnlp-main.752,D19-1388,1,0.809996,"s on previous works in three fields: text summarization, multimodal summarization, and visual question answering. Text Summarization. Our proposed task bases on text summarization, the methods of which can be divided into extractive and abstractive methods (Gao et al., 2020b). Extractive models (Zhang et al., 2018; Narayan et al., 2018; Chen et al., 2018; Luo et al., 2019; Xiao and Carenini, 2019) directly pick sentences from article and regard the aggregate of them as the summary. In contrast, abstractive models (Sutskever et al., 2014; See et al., 2017; Wenbo et al., 2019; Gui et al., 2019; Gao et al., 2019a; Chen et al., 2019a; Gao et al., 2019b) generate a summary from scratch and the abstractive summaries are typically less redundant. Multimodal Summarization. A series of works (Li et al., 2017, 2018; Palaskar et al., 2019; Chan et al., 2019; Chen et al., 2019b; Gao et al., 2020a) focused on generating better textual summaries with the help of multimodal input. Multimodal summarization with multimodal output is relatively less explored. Zhu et al. (2018) proposed to jointly generate textual summary and select the most relevant image from 6 candidates. Following their work, Zhu et al. (2020) a"
2020.emnlp-main.752,D19-1117,0,0.0128247,"Our research builds on previous works in three fields: text summarization, multimodal summarization, and visual question answering. Text Summarization. Our proposed task bases on text summarization, the methods of which can be divided into extractive and abstractive methods (Gao et al., 2020b). Extractive models (Zhang et al., 2018; Narayan et al., 2018; Chen et al., 2018; Luo et al., 2019; Xiao and Carenini, 2019) directly pick sentences from article and regard the aggregate of them as the summary. In contrast, abstractive models (Sutskever et al., 2014; See et al., 2017; Wenbo et al., 2019; Gui et al., 2019; Gao et al., 2019a; Chen et al., 2019a; Gao et al., 2019b) generate a summary from scratch and the abstractive summaries are typically less redundant. Multimodal Summarization. A series of works (Li et al., 2017, 2018; Palaskar et al., 2019; Chan et al., 2019; Chen et al., 2019b; Gao et al., 2020a) focused on generating better textual summaries with the help of multimodal input. Multimodal summarization with multimodal output is relatively less explored. Zhu et al. (2018) proposed to jointly generate textual summary and select the most relevant image from 6 candidates. Following their work, Z"
2020.emnlp-main.752,P18-1013,0,0.0227106,"f our data contains an article with a textual summary and a video with a cover picture. The average video duration is one minute and the frame rate of video is 25 fps. For the text part, the average length of article is 96.84 words and the average length of textual summary is 11.19 words. Overall, there are 184,920 samples in the dataset, which is split into a training set of 180,000 samples, a validation set of 2,460 samples, and a test set of 2,460 samples. 5.2 R-2 R-L extractive summarization Lead TextRank 16.2 13.7 5.3 4.0 13.9 12.5 abstractive summarization PG (See et al., 2017) Unified (Hsu et al., 2018) GPG (Shen et al., 2019) 19.4 23.0 20.1 6.8 6.0 4.5 17.4 20.9 17.3 our models DIMS 25.1 9.6 23.2 Table 1: Rouge scores comparison with traditional textual summarization baselines. Comparisons We compare our proposed method against summarization baselines and VQA baselines. Traditional Textual Summarization baselines: Lead: selects the first sentence of article as the textual summary (Nallapati et al., 2017). TexkRank: a graph-based extractive summarizer which adds sentences as nodes and uses edges to weight similarity (Mihalcea and Tarau, 2004). PG: a sequence-to-sequence framework combined wi"
2020.emnlp-main.752,D17-1114,0,0.149343,"module and multimodal generator. In the dual interaction module, we propose a conditional self-attention mechanism that captures local semantic information within video and a global-attention mechanism that handles the semantic relationship between news text and video from a high level. Extensive experiments conducted on a large-scale real-world VMSMO dataset1 show that DIMS achieves the state-of-the-art performance in terms of both automatic metrics and human evaluations. 1 Figure 1: An example of video-based multimodal summarization with multimodal output. Introduction Existing experiments (Li et al., 2017) have proven that multimodal news can significantly improve users’ sense of satisfaction for informativeness. As one of these multimedia data forms, introducing news events with video and textual descriptions is ∗ Equal contribution. Ordering is decided by a coin flip. Corresponding author. 1 https://github.com/yingtaomj/VMSMO † becoming increasingly popular, and has been employed as the main form of news reporting by news media including BBC, Weibo, CNN, and Daily Mail. An illustration is shown in Figure 1, where the news contains a video with a cover picture and a full news article with a sh"
2020.emnlp-main.752,P19-1500,0,0.0406666,"Missing"
2020.emnlp-main.752,D19-1300,0,0.0114807,"th video cover simultaneously. • We construct a large-scale dataset for VMSMO, and experimental results demonstrate that our model outperforms other baselines in terms of both automatic and human evaluations. 2 Related Work Our research builds on previous works in three fields: text summarization, multimodal summarization, and visual question answering. Text Summarization. Our proposed task bases on text summarization, the methods of which can be divided into extractive and abstractive methods (Gao et al., 2020b). Extractive models (Zhang et al., 2018; Narayan et al., 2018; Chen et al., 2018; Luo et al., 2019; Xiao and Carenini, 2019) directly pick sentences from article and regard the aggregate of them as the summary. In contrast, abstractive models (Sutskever et al., 2014; See et al., 2017; Wenbo et al., 2019; Gui et al., 2019; Gao et al., 2019a; Chen et al., 2019a; Gao et al., 2019b) generate a summary from scratch and the abstractive summaries are typically less redundant. Multimodal Summarization. A series of works (Li et al., 2017, 2018; Palaskar et al., 2019; Chan et al., 2019; Chen et al., 2019b; Gao et al., 2020a) focused on generating better textual summaries with the help of multimodal"
2020.emnlp-main.752,W04-3252,0,0.0712491,"abstractive summarization PG (See et al., 2017) Unified (Hsu et al., 2018) GPG (Shen et al., 2019) 19.4 23.0 20.1 6.8 6.0 4.5 17.4 20.9 17.3 our models DIMS 25.1 9.6 23.2 Table 1: Rouge scores comparison with traditional textual summarization baselines. Comparisons We compare our proposed method against summarization baselines and VQA baselines. Traditional Textual Summarization baselines: Lead: selects the first sentence of article as the textual summary (Nallapati et al., 2017). TexkRank: a graph-based extractive summarizer which adds sentences as nodes and uses edges to weight similarity (Mihalcea and Tarau, 2004). PG: a sequence-to-sequence framework combined with attention mechanism and pointer network (See et al., 2017). Unified: a model which combines the strength of extractive and abstractive summarization (Hsu et al., 2018). GPG: Shen et al. (2019) proposed to generate textual summary by “editing” pointed tokens instead of hard copying. Multimodal baselines: How2: a model proposed to generate textual summary with video information (Palaskar et al., 2019). Synergistic: a image-question-answer synergistic network to value the role of the answer for precise visual dialog(Guo et al., 2019). PSAC: a m"
2020.emnlp-main.752,N18-1158,0,0.018938,"article, and generates textual summary with video cover simultaneously. • We construct a large-scale dataset for VMSMO, and experimental results demonstrate that our model outperforms other baselines in terms of both automatic and human evaluations. 2 Related Work Our research builds on previous works in three fields: text summarization, multimodal summarization, and visual question answering. Text Summarization. Our proposed task bases on text summarization, the methods of which can be divided into extractive and abstractive methods (Gao et al., 2020b). Extractive models (Zhang et al., 2018; Narayan et al., 2018; Chen et al., 2018; Luo et al., 2019; Xiao and Carenini, 2019) directly pick sentences from article and regard the aggregate of them as the summary. In contrast, abstractive models (Sutskever et al., 2014; See et al., 2017; Wenbo et al., 2019; Gui et al., 2019; Gao et al., 2019a; Chen et al., 2019a; Gao et al., 2019b) generate a summary from scratch and the abstractive summaries are typically less redundant. Multimodal Summarization. A series of works (Li et al., 2017, 2018; Palaskar et al., 2019; Chan et al., 2019; Chen et al., 2019b; Gao et al., 2020a) focused on generating better textual s"
2020.emnlp-main.752,P19-1659,0,0.0412125,"Missing"
2020.emnlp-main.752,P17-1099,0,0.479182,"and human evaluations. 2 Related Work Our research builds on previous works in three fields: text summarization, multimodal summarization, and visual question answering. Text Summarization. Our proposed task bases on text summarization, the methods of which can be divided into extractive and abstractive methods (Gao et al., 2020b). Extractive models (Zhang et al., 2018; Narayan et al., 2018; Chen et al., 2018; Luo et al., 2019; Xiao and Carenini, 2019) directly pick sentences from article and regard the aggregate of them as the summary. In contrast, abstractive models (Sutskever et al., 2014; See et al., 2017; Wenbo et al., 2019; Gui et al., 2019; Gao et al., 2019a; Chen et al., 2019a; Gao et al., 2019b) generate a summary from scratch and the abstractive summaries are typically less redundant. Multimodal Summarization. A series of works (Li et al., 2017, 2018; Palaskar et al., 2019; Chan et al., 2019; Chen et al., 2019b; Gao et al., 2020a) focused on generating better textual summaries with the help of multimodal input. Multimodal summarization with multimodal output is relatively less explored. Zhu et al. (2018) proposed to jointly generate textual summary and select the most relevant image from"
2020.emnlp-main.752,D19-1390,0,0.0303845,"Missing"
2020.emnlp-main.752,D19-1011,0,0.0278561,"extual summary and used coverage to help select picture (Zhu et al., 2018). MOF: the model based on MSMO which added consideration of image accuracy as another loss (Zhu et al., 2020). 5.3 R-1 Evaluation Metrics The quality of generated textual summary is evaluated by standard full-length Rouge F1 (Lin, 2004) following previous works (See et al., 2017; Chen et al., 2018). R-1, R-2, and R-L refer to unigram, bigrams, and the longest common subsequence respectively. The quality of chosen cover frame is evaluated by mean average precision (MAP) (Zhou et al., 2018) and recall at position (Rn @k) (Tao et al., 2019). Rn @k measures if the positive sample is ranked in the top k positions of n candidates. 5.4 Implementation Details We implement our experiments in Tensorflow (Abadi et al., 2016) on an NVIDIA GTX 1080 Ti GPU. The code for our model is available online2 . For all models, we set the word embedding dimension and the hidden dimension to 128. The encoding step is set to 100, while the minimum decoding step is 10 and the maximum step is 30. For video preprocessing, we extract one of every 120 frames to obtain 10 frames as cover candidates. All candidates are resized to 128x64. We regard the frame"
2020.emnlp-main.752,D19-1304,0,0.017723,"ons. 2 Related Work Our research builds on previous works in three fields: text summarization, multimodal summarization, and visual question answering. Text Summarization. Our proposed task bases on text summarization, the methods of which can be divided into extractive and abstractive methods (Gao et al., 2020b). Extractive models (Zhang et al., 2018; Narayan et al., 2018; Chen et al., 2018; Luo et al., 2019; Xiao and Carenini, 2019) directly pick sentences from article and regard the aggregate of them as the summary. In contrast, abstractive models (Sutskever et al., 2014; See et al., 2017; Wenbo et al., 2019; Gui et al., 2019; Gao et al., 2019a; Chen et al., 2019a; Gao et al., 2019b) generate a summary from scratch and the abstractive summaries are typically less redundant. Multimodal Summarization. A series of works (Li et al., 2017, 2018; Palaskar et al., 2019; Chan et al., 2019; Chen et al., 2019b; Gao et al., 2020a) focused on generating better textual summaries with the help of multimodal input. Multimodal summarization with multimodal output is relatively less explored. Zhu et al. (2018) proposed to jointly generate textual summary and select the most relevant image from 6 candidates. Follo"
2020.emnlp-main.752,D19-1298,0,0.0778183,"ultaneously. • We construct a large-scale dataset for VMSMO, and experimental results demonstrate that our model outperforms other baselines in terms of both automatic and human evaluations. 2 Related Work Our research builds on previous works in three fields: text summarization, multimodal summarization, and visual question answering. Text Summarization. Our proposed task bases on text summarization, the methods of which can be divided into extractive and abstractive methods (Gao et al., 2020b). Extractive models (Zhang et al., 2018; Narayan et al., 2018; Chen et al., 2018; Luo et al., 2019; Xiao and Carenini, 2019) directly pick sentences from article and regard the aggregate of them as the summary. In contrast, abstractive models (Sutskever et al., 2014; See et al., 2017; Wenbo et al., 2019; Gui et al., 2019; Gao et al., 2019a; Chen et al., 2019a; Gao et al., 2019b) generate a summary from scratch and the abstractive summaries are typically less redundant. Multimodal Summarization. A series of works (Li et al., 2017, 2018; Palaskar et al., 2019; Chan et al., 2019; Chen et al., 2019b; Gao et al., 2020a) focused on generating better textual summaries with the help of multimodal input. Multimodal summariz"
2020.emnlp-main.752,D18-1088,0,0.0159518,"semantic meaning of article, and generates textual summary with video cover simultaneously. • We construct a large-scale dataset for VMSMO, and experimental results demonstrate that our model outperforms other baselines in terms of both automatic and human evaluations. 2 Related Work Our research builds on previous works in three fields: text summarization, multimodal summarization, and visual question answering. Text Summarization. Our proposed task bases on text summarization, the methods of which can be divided into extractive and abstractive methods (Gao et al., 2020b). Extractive models (Zhang et al., 2018; Narayan et al., 2018; Chen et al., 2018; Luo et al., 2019; Xiao and Carenini, 2019) directly pick sentences from article and regard the aggregate of them as the summary. In contrast, abstractive models (Sutskever et al., 2014; See et al., 2017; Wenbo et al., 2019; Gui et al., 2019; Gao et al., 2019a; Chen et al., 2019a; Gao et al., 2019b) generate a summary from scratch and the abstractive summaries are typically less redundant. Multimodal Summarization. A series of works (Li et al., 2017, 2018; Palaskar et al., 2019; Chan et al., 2019; Chen et al., 2019b; Gao et al., 2020a) focused on gener"
2020.emnlp-main.752,P18-1103,0,0.0195082,"id attention to text and images during generating textual summary and used coverage to help select picture (Zhu et al., 2018). MOF: the model based on MSMO which added consideration of image accuracy as another loss (Zhu et al., 2020). 5.3 R-1 Evaluation Metrics The quality of generated textual summary is evaluated by standard full-length Rouge F1 (Lin, 2004) following previous works (See et al., 2017; Chen et al., 2018). R-1, R-2, and R-L refer to unigram, bigrams, and the longest common subsequence respectively. The quality of chosen cover frame is evaluated by mean average precision (MAP) (Zhou et al., 2018) and recall at position (Rn @k) (Tao et al., 2019). Rn @k measures if the positive sample is ranked in the top k positions of n candidates. 5.4 Implementation Details We implement our experiments in Tensorflow (Abadi et al., 2016) on an NVIDIA GTX 1080 Ti GPU. The code for our model is available online2 . For all models, we set the word embedding dimension and the hidden dimension to 128. The encoding step is set to 100, while the minimum decoding step is 10 and the maximum step is 30. For video preprocessing, we extract one of every 120 frames to obtain 10 frames as cover candidates. All cand"
2020.emnlp-main.752,D18-1448,0,0.336754,"and has been employed as the main form of news reporting by news media including BBC, Weibo, CNN, and Daily Mail. An illustration is shown in Figure 1, where the news contains a video with a cover picture and a full news article with a short textual summary. In such a case, automatically generating multimodal summaries, i.e., choosing a proper cover frame of the video and generating an appropriate textual summary of the article can help editors save time and readers make decisions more effectively. There are several works focusing on multimodal summarization. The most related work to ours is (Zhu et al., 2018), where they propose the task of generating textual summary and picking the most representative picture from 6 input candidates. However, in real-world applications, the input is usually a video consisting of hundreds of frames. Consequently, the temporal dependency in a video cannot be simply modeled by static encoding methods. Hence, in this work, we propose a novel task, Video-based Multimodal Summarization with Multimodal Output (VMSMO), which selects cover frame from news video and generates textual summary of the news article in the meantime. 9360 Proceedings of the 2020 Conference on Em"
2020.findings-emnlp.350,D14-1179,0,0.00581195,"Missing"
2020.findings-emnlp.350,P16-1195,0,0.515381,"ur layers since we find using more layers does not improve the results. Each of the GAT layers has a residual connection to avoid gradient vanishment except for the last layer. We set the dropout (Srivastava et al., 2014) rate to 0.1, the weight decay rate to 0.0001 and the batch size to 20. We use Adam optimizer (Kingma and Ba, 2015) with a learning rate of 0.001. To reduce randomness, we run each model setting five times and take the average as the report performance. 3.5 Baseline Models We compare our model with three types of models, described as follows. The first kind of models, CodeNN (Iyer et al., 2016) and Seq2Seq (Luong et al., 2015), treat source code as a sequential sequence of words. CodeNN is a modified language model with attention mechanism, and it is among the earliest NN models for code comment generation. Seq2Seq is the most widely used model for many generation tasks. For these models, we use a bidirectional GRU as the encoder and a GRU with attention as the decoder. The second kind of models incorporate information from the AST into the model and exploit the structural information of the AST to assist comment generation. We choose two state-ofthe-art AST-based generation models:"
2020.findings-emnlp.350,W04-1013,0,0.063184,"Missing"
2020.findings-emnlp.350,D15-1166,0,0.16432,"nd guard against mismatch and obsolete comments is to automatically generate them. Classical approaches for auto-comment generation use hand-crafted templates to produce code descriptions (Sridhara et al., 2010; Cortes-Coy et al., 2014; Dawood et al., 2017), but suffer from poor scalability and high maintenance cost due to the expensive overhead of writing comment templates. More recent work takes a learning-based approach by employing neural network (NN) models developed for natural language processing tasks like machine translation to automatically generate comments (Sutskever et al., 2014; Luong et al., 2015). Compared to hand-written templates, a learningbased approach based on empirical data is more scalable and sustainable. The key to generating high-quality comments is to utilize as much relevant information as possible from the source code to infer the high-level algorithmic intents. Prior work achieves this by converting a representation of the program, e.g., an Abstract Syntax Tree (AST), into a sequential sequence where a sequential model like LSTM can be applied to translate the token sequence into natural language descriptions (Hu et al., 2018a; Alon et al., 2019; LeClair et al., 2019)."
2020.findings-emnlp.350,P02-1040,0,0.108303,"Missing"
2020.findings-emnlp.350,P17-1099,0,0.0347233,"s for an activation function, usually LeakyReLU and [·||·] means the concatenation of two matrices. We repeat this neighborhood aggregation process L times, and get the final state for each vertex of the C-Graph, represented as {giL |vi ∈ Vf }. We use the final state of the target function gtL as the global representation. 2.3 where qj and qk are the contextual embeddings of words xi and xj in the input sequence of the target function and Wla is a trainable parameter. Pointer. As source codes may contain information which can be directly used in comment, we propose to add a pointer mechanism (See et al., 2017) which can copy useful words from source codes. Pointer mechanism merges a copy distribution with a normal output prediction distribution. In the ith decoding step, Pvocab = sof tmax(Wv [hi ||ci ||cgi ] + bv ) (9) Decoder As our encoders embed both local and global information, the decoder needs to integrate information extracted at different levels and takes the integrated information into consideration during the generation of comment tokens. Once again, we adopt a GRU as decoder and use the concatenation of local representation qn and global representation gtL as its initial state. hi is th"
2021.acl-long.433,D18-1216,0,0.0180143,"regular expressions for text classification tasks. Most of these works provide effective ways to utilize word-level knowledge, but none of them formally considers the quality issues with the distantly-labeled rationales. Additionally, Poulis and Dasgupta (2017) discuss the insufficiency issue in the feature feedback framework, and try to incorporate vague feature feedback into a linear classifier. As a widely-used explanation method, the attention mechanism is often applied with constraints to guide model focus towards the significant part of inputs (Liu et al., 2017; Nguyen and Nguyen, 2018; Bao et al., 2018). Our proposed methods are currently based on gradient-based salience calculation, which is easier to obtain and model-agnostic, thus can be applied to a wider range with ease. But our methods do not depend on specific calculation methods for word salience, and can be easily transplanted to attention-based constraints, which we will leave for future work. Recent studies have provided various techniques to constrain gradient-based word salience. Ross et al. (2017) forces the gradient of features, which are annotated non-helpful, to be zero, to alter the decision boundary of the model. Liu and A"
2021.acl-long.433,N19-1404,0,0.206367,"rent rationale words may exhibit varied importance. If those distantly-labeled rationales are applied in a 0-1 form to all instances and treated equally important, the tremendous diversity of actual importance in individual cases is just ignored. We refer to the distantly-labeled rationale words that are not helpful in a specific instance as Non-Important Rationales, or NoIRs for short. Although many existing works have attempted to incorporate automatically-obtained rationales in different ways and achieved promising results in various applications (Liu et al., 2017; Nguyen and Nguyen, 2018; Ghaeini et al., 2019; Liu and Avci, 2019), they do not explicitly examine the quality issues of distantly-labeled rationales, nor formally consider them during modeling, except (Poulis and Dasgupta, 2017), which incorporate vague feature feedback into a linear classifier. On the one hand, most existing methods try to apply strict constraints to require model focus to conform to rationales, often encouraging those words to share all the model focus (Nguyen and Nguyen, 2018; Liu and Avci, 2019). However, as distantlylabeled rationales are often insufficient to draw correct conclusions, the rigid requirements may tu"
2021.acl-long.433,P08-1030,0,0.170866,"Missing"
2021.acl-long.433,2020.emnlp-main.258,0,0.140089,"ling to take ::::: Labeled a chance will be rewarded with two of the year’s most accomplished and riveting film per::::: formance. Table 1: An example of rationale annotation for sentiment analysis. Words in underline are rationales annotated by human experts, and words in wavy underline ::::: are annotated via sentiment lexicon matching. Numbers in [] are salience scores labeled by experts. Introduction Recent studies have shown an increasing interest in incorporating human knowledge into neural network models (Xu et al., 2018; Vashishth et al., 2018; Luo et al., 2018; Li and Srikumar, 2019; Jiang et al., 2020). For many natural language processing (NLP) tasks, such domain knowledge often refers to salient words annotated by human experts, which are also called rationales. Table 1 (top) shows an example of expert-annotated rationales for sentiment analysis, which highlight noteworthy tokens and score the contributions of these tokens. The detailed annotations reflect the importance of these words from the expert annotator’s viewpoint and are expected to help training better sentiment classification models. ∗ Corresponding author. Nonetheless, careful, case-by-case rationale annotations inevitably in"
2021.acl-long.433,D16-1011,0,0.0545267,"Missing"
2021.acl-long.433,N16-1082,0,0.028393,"indicates how much xi contributes to the model f . Prior works have explored different methods to determine word salience (Ribeiro et al., 2016; Jin et al., 2020). Among them, we choose gradientbased methods since they are model-agnostic and easy to obtain. Moreover, since gradient-based 5572 word salience is differentiable with respect to model parameters, taking it as part of the objective makes it more convenient to optimize the loss. For a function f , the magnitude (absolute value) of its gradients with respect to input x indicates how sensitive the final decision is to the change of x (Li et al., 2016). In most NLP settings, the gradient of a word is the sum of gradients for each dimension of word embeddings. Formally, the gradient of an input word xi to a function f can be calculated as: ∂f (1) gi = ∂xi 1 where k·k1 is the L1 norm that sums up the absolute value of gradients over the embedding dimensions. For gradient-based methods, we use the normalized gradients to calculate word salience, which represents the proportion of a word’s contribution in a sentence: gi si = Pn (2) j=1 gj There exist more complicated gradient-based methods for calculating word salience (Sundararajan et al., 201"
2021.acl-long.433,P19-1028,0,0.0231312,"watch, but viewers willing to take ::::: Labeled a chance will be rewarded with two of the year’s most accomplished and riveting film per::::: formance. Table 1: An example of rationale annotation for sentiment analysis. Words in underline are rationales annotated by human experts, and words in wavy underline ::::: are annotated via sentiment lexicon matching. Numbers in [] are salience scores labeled by experts. Introduction Recent studies have shown an increasing interest in incorporating human knowledge into neural network models (Xu et al., 2018; Vashishth et al., 2018; Luo et al., 2018; Li and Srikumar, 2019; Jiang et al., 2020). For many natural language processing (NLP) tasks, such domain knowledge often refers to salient words annotated by human experts, which are also called rationales. Table 1 (top) shows an example of expert-annotated rationales for sentiment analysis, which highlight noteworthy tokens and score the contributions of these tokens. The detailed annotations reflect the importance of these words from the expert annotator’s viewpoint and are expected to help training better sentiment classification models. ∗ Corresponding author. Nonetheless, careful, case-by-case rationale anno"
2021.acl-long.433,P19-1631,0,0.313689,"ay exhibit varied importance. If those distantly-labeled rationales are applied in a 0-1 form to all instances and treated equally important, the tremendous diversity of actual importance in individual cases is just ignored. We refer to the distantly-labeled rationale words that are not helpful in a specific instance as Non-Important Rationales, or NoIRs for short. Although many existing works have attempted to incorporate automatically-obtained rationales in different ways and achieved promising results in various applications (Liu et al., 2017; Nguyen and Nguyen, 2018; Ghaeini et al., 2019; Liu and Avci, 2019), they do not explicitly examine the quality issues of distantly-labeled rationales, nor formally consider them during modeling, except (Poulis and Dasgupta, 2017), which incorporate vague feature feedback into a linear classifier. On the one hand, most existing methods try to apply strict constraints to require model focus to conform to rationales, often encouraging those words to share all the model focus (Nguyen and Nguyen, 2018; Liu and Avci, 2019). However, as distantlylabeled rationales are often insufficient to draw correct conclusions, the rigid requirements may turn out to incorrectly"
2021.acl-long.433,P17-1164,0,0.339404,"ly helpful, given a specific context, different rationale words may exhibit varied importance. If those distantly-labeled rationales are applied in a 0-1 form to all instances and treated equally important, the tremendous diversity of actual importance in individual cases is just ignored. We refer to the distantly-labeled rationale words that are not helpful in a specific instance as Non-Important Rationales, or NoIRs for short. Although many existing works have attempted to incorporate automatically-obtained rationales in different ways and achieved promising results in various applications (Liu et al., 2017; Nguyen and Nguyen, 2018; Ghaeini et al., 2019; Liu and Avci, 2019), they do not explicitly examine the quality issues of distantly-labeled rationales, nor formally consider them during modeling, except (Poulis and Dasgupta, 2017), which incorporate vague feature feedback into a linear classifier. On the one hand, most existing methods try to apply strict constraints to require model focus to conform to rationales, often encouraging those words to share all the model focus (Nguyen and Nguyen, 2018; Liu and Avci, 2019). However, as distantlylabeled rationales are often insufficient to draw cor"
2021.acl-long.433,P18-1194,1,0.943051,"stantly Painful to watch, but viewers willing to take ::::: Labeled a chance will be rewarded with two of the year’s most accomplished and riveting film per::::: formance. Table 1: An example of rationale annotation for sentiment analysis. Words in underline are rationales annotated by human experts, and words in wavy underline ::::: are annotated via sentiment lexicon matching. Numbers in [] are salience scores labeled by experts. Introduction Recent studies have shown an increasing interest in incorporating human knowledge into neural network models (Xu et al., 2018; Vashishth et al., 2018; Luo et al., 2018; Li and Srikumar, 2019; Jiang et al., 2020). For many natural language processing (NLP) tasks, such domain knowledge often refers to salient words annotated by human experts, which are also called rationales. Table 1 (top) shows an example of expert-annotated rationales for sentiment analysis, which highlight noteworthy tokens and score the contributions of these tokens. The detailed annotations reflect the importance of these words from the expert annotator’s viewpoint and are expected to help training better sentiment classification models. ∗ Corresponding author. Nonetheless, careful, case"
2021.acl-long.433,2020.acl-main.419,0,0.0704263,"Missing"
2021.acl-long.433,C18-1193,0,0.310181,"a specific context, different rationale words may exhibit varied importance. If those distantly-labeled rationales are applied in a 0-1 form to all instances and treated equally important, the tremendous diversity of actual importance in individual cases is just ignored. We refer to the distantly-labeled rationale words that are not helpful in a specific instance as Non-Important Rationales, or NoIRs for short. Although many existing works have attempted to incorporate automatically-obtained rationales in different ways and achieved promising results in various applications (Liu et al., 2017; Nguyen and Nguyen, 2018; Ghaeini et al., 2019; Liu and Avci, 2019), they do not explicitly examine the quality issues of distantly-labeled rationales, nor formally consider them during modeling, except (Poulis and Dasgupta, 2017), which incorporate vague feature feedback into a linear classifier. On the one hand, most existing methods try to apply strict constraints to require model focus to conform to rationales, often encouraging those words to share all the model focus (Nguyen and Nguyen, 2018; Liu and Avci, 2019). However, as distantlylabeled rationales are often insufficient to draw correct conclusions, the rig"
2021.acl-long.433,D13-1170,0,0.00833621,"Missing"
2021.acl-long.433,P15-2060,0,0.0677755,"Missing"
2021.acl-long.433,D14-1162,0,0.0923824,"Missing"
2021.acl-long.433,N16-3020,0,0.0736573,"ated rationales, even in a lower quality. 2 Word Salience Before elaborating on our proposed methods, we first introduce the definition of word salience, a measure of the importance of words, which is widely applied in previous works (Luo et al., 2018; Nguyen and Nguyen, 2018; Jin et al., 2020) Given a model f and an input word sequence x = (x1 , x2 , ..., xn ), the word salience is a vector s = (s1 , s2 , ..., sn ) that denotes the importance of every word in x, where si indicates how much xi contributes to the model f . Prior works have explored different methods to determine word salience (Ribeiro et al., 2016; Jin et al., 2020). Among them, we choose gradientbased methods since they are model-agnostic and easy to obtain. Moreover, since gradient-based 5572 word salience is differentiable with respect to model parameters, taking it as part of the objective makes it more convenient to optimize the loss. For a function f , the magnitude (absolute value) of its gradients with respect to input x indicates how sensitive the final decision is to the change of x (Li et al., 2016). In most NLP settings, the gradient of a word is the sum of gradients for each dimension of word embeddings. Formally, the grad"
2021.acl-long.433,D18-1157,0,0.056442,"Missing"
2021.acl-long.433,D19-1420,0,0.033442,"Missing"
2021.acl-long.433,N07-1033,0,0.183416,"Missing"
2021.acl-long.473,D18-1442,1,0.901625,"Missing"
2021.acl-long.473,N13-1136,0,0.0281328,"ers that cite the reference papers of the paper being written to cover the Steiner tree. However, abstractive approaches on related work generation have met with limited success. Apart from the lack of sufficient training data, neural models also face the challenge of identifying the logic relationship between multiple input documents. Multi-document Summarization. The multidocument summarization task aims to cover the key shared relevant information among all the documents while avoiding redundancy (Goldstein et al., 2000). Existing multi-document summarization methods are mostly extractive (Christensen et al., 2013; Parveen and Strube, 2014; Ma et al., 2016; Chu and Liu, 2018). For example, Wang et al. (2020) present a heterogeneous graph-based neural network which contains semantic nodes of different granularity levels apart from sentences. Recently, a vast majority of the literature is dedicated to abstractive multi-document summarization. Lu et al. (2020) propose a large-scale multi-document summarization dataset created from scientific articles. Jin et al. (2020) propose a multi-granularity interaction network for extractive and abstractive approaches. Li et al. (2020a) develop a neural abstractive"
2021.acl-long.473,P19-1610,0,0.0252513,"Missing"
2021.acl-long.473,D19-1388,1,0.847232,"g the weight of each edge, i.e., l−1,r βi,j , we also incorporate relation edge information, since close relationships such as succession or transition can have a great impact on edge weight. Concretely, Equation 4 is changed to:   T Q l−1 K + hl hl−1 W h W r r r d d i,j i j l−1,r √ . βi,j = d (10) We summarize the whole relationship modeling process as: L 0 0 hL d , hr = RM(hd , hr ). (11) For brevity, we omit the subscript L in the following section. 5.4 Related Work Generator To generate a consistent and informative summary, we propose an RNN-based decoder following (Chen et al., 2019; Gao et al., 2019) that incorporates the outputs of the hierarchical encoder and the relationship graph as illustrated in Figure 1. Our decoder is a single-layer unidirectional LSTM. At each step t, the decoder updates the hidden state from st−1 to st :  h i d st = LSTM st−1 , cw , c , e(y ) . t−1 t−1 t−1 6072 (12) Following previous works (Bahdanau et al., 2015), we employ an attention mechanism to compute the attention distribution over the source words in the sequence-to-sequence structure:   w0 ,i αt,j = Wag tanh Wbg st + Wcg hwi , (13) j  0  P  0  w,i w ,i w ,i i / N , (14) αt,j = exp αt,j l=1 exp"
2021.acl-long.473,W00-0405,0,0.289426,"r tree of the keywords. Then the summary is generated by extracting the sentences from the papers that cite the reference papers of the paper being written to cover the Steiner tree. However, abstractive approaches on related work generation have met with limited success. Apart from the lack of sufficient training data, neural models also face the challenge of identifying the logic relationship between multiple input documents. Multi-document Summarization. The multidocument summarization task aims to cover the key shared relevant information among all the documents while avoiding redundancy (Goldstein et al., 2000). Existing multi-document summarization methods are mostly extractive (Christensen et al., 2013; Parveen and Strube, 2014; Ma et al., 2016; Chu and Liu, 2018). For example, Wang et al. (2020) present a heterogeneous graph-based neural network which contains semantic nodes of different granularity levels apart from sentences. Recently, a vast majority of the literature is dedicated to abstractive multi-document summarization. Lu et al. (2020) propose a large-scale multi-document summarization dataset created from scientific articles. Jin et al. (2020) propose a multi-granularity interaction net"
2021.acl-long.473,P16-1154,0,0.0360376,"is being introduced. Hence, we employ the document-level attention weights in Equation 17 to read the relationship graph:   hrim = meanpool hri,1 , · · · , hri,N , (19) P d m crt = N i=1 αt,i hri . Finally, an output projection layer is applied to get the final generating distribution Ptv over vocabulary, as shown in Equation 20: d r Ptv = softmax(MLPc [st ; cw t ; ct ; ct ]). (20) Our objective function is the negative log likelihood of the target word yt : P L = − Tt=1 log Ptv (yt ). (21) In order to handle the out-of-vocabulary (OOV) problem, we equip our decoder with a pointer network (Gu et al., 2016; See et al., 2017). This process is the same as the model described in (See et al., 2017), thus, is omit here due to limited space. 6 6.1 Experimental Setup Baselines To evaluate the performance of our proposed model, we compare it with the following baselines: Extractive Methods: (1) LEAD: selects the first sentence of each document as the summary as a baseline. (2) TextRank (Mihalcea and Tarau, 2004): is a multi-document graph-based ranking model. (3) BertSumEXT (Liu and Lapata, 2019b): is an extractive summarization model with BERT. (4) MGSum-ext (Jin et al., 2020): is a multi-granularity"
2021.acl-long.473,C10-2049,0,0.0360115,"gle miRNA on various gene targets are the limitations to the use of this modern technology specifically in brain disorders like prion diseases [3]. Table 1: Comparison of a related work paragraph generated by an extractive method (human-annotated) and an abstractive man-made related work paragraph with the same multiple original papers. Introduction The related work section generation task aims to automatically generate a summary of the most relevant works in a specific research area, which can help researchers to familiarize themselves with the state of the art in the field. Several methods (Hoang and Kan, 2010; Hu and Wan, 2014; Chen and Zhuge, 2019) have been proposed to study how to obtain the related work section automatically by ∗ Corresponding author. https://github.com/iriscxy/ relatedworkgeneration 1 extracting important sentences from multiple original papers. However, extractive approaches lack the sophisticated abilities that are crucial to highquality summarization such as paraphrasing and generalization, and often lead to a related work section with poor coherence and readability (See et al., 2017; Hsu et al., 2018). For example, as shown in Table 1, the extracted sentences share the pa"
2021.acl-long.473,P18-1013,0,0.0189753,"e themselves with the state of the art in the field. Several methods (Hoang and Kan, 2010; Hu and Wan, 2014; Chen and Zhuge, 2019) have been proposed to study how to obtain the related work section automatically by ∗ Corresponding author. https://github.com/iriscxy/ relatedworkgeneration 1 extracting important sentences from multiple original papers. However, extractive approaches lack the sophisticated abilities that are crucial to highquality summarization such as paraphrasing and generalization, and often lead to a related work section with poor coherence and readability (See et al., 2017; Hsu et al., 2018). For example, as shown in Table 1, the extracted sentences share the pattern “We find...” as the subject of sentences, which, as a matter of fact, refer to different authors. On the contrary, the abstractive related work in Table 1 reveals that the works are conducted by different scholars. It also has conjunction words such as “Furthermore” and “However”, which can explain the logical relationship between the cited works, and thus form an elegant narration. Hence, in this paper, we target on the abstractive related work generation task, which generates a related work including novel words an"
2021.acl-long.473,D14-1170,0,0.0290996,"gene targets are the limitations to the use of this modern technology specifically in brain disorders like prion diseases [3]. Table 1: Comparison of a related work paragraph generated by an extractive method (human-annotated) and an abstractive man-made related work paragraph with the same multiple original papers. Introduction The related work section generation task aims to automatically generate a summary of the most relevant works in a specific research area, which can help researchers to familiarize themselves with the state of the art in the field. Several methods (Hoang and Kan, 2010; Hu and Wan, 2014; Chen and Zhuge, 2019) have been proposed to study how to obtain the related work section automatically by ∗ Corresponding author. https://github.com/iriscxy/ relatedworkgeneration 1 extracting important sentences from multiple original papers. However, extractive approaches lack the sophisticated abilities that are crucial to highquality summarization such as paraphrasing and generalization, and often lead to a related work section with poor coherence and readability (See et al., 2017; Hsu et al., 2018). For example, as shown in Table 1, the extracted sentences share the pattern “We find...”"
2021.acl-long.473,2020.acl-main.556,0,0.327486,"documents while avoiding redundancy (Goldstein et al., 2000). Existing multi-document summarization methods are mostly extractive (Christensen et al., 2013; Parveen and Strube, 2014; Ma et al., 2016; Chu and Liu, 2018). For example, Wang et al. (2020) present a heterogeneous graph-based neural network which contains semantic nodes of different granularity levels apart from sentences. Recently, a vast majority of the literature is dedicated to abstractive multi-document summarization. Lu et al. (2020) propose a large-scale multi-document summarization dataset created from scientific articles. Jin et al. (2020) propose a multi-granularity interaction network for extractive and abstractive approaches. Li et al. (2020a) develop a neural abstractive multi-document summarization model which leverages explicit graph representations of documents to guide the summary generation process. While the multi-document summarization task aims to extract information shared by multiple documents, related work generation aims to compare and introduce the cited works in logic order. 3 We discuss the related work on related work generation and multi-document summarization. Related Work Generation. Most of the previous"
2021.acl-long.473,2020.acl-main.555,0,0.0722303,"are mostly extractive (Christensen et al., 2013; Parveen and Strube, 2014; Ma et al., 2016; Chu and Liu, 2018). For example, Wang et al. (2020) present a heterogeneous graph-based neural network which contains semantic nodes of different granularity levels apart from sentences. Recently, a vast majority of the literature is dedicated to abstractive multi-document summarization. Lu et al. (2020) propose a large-scale multi-document summarization dataset created from scientific articles. Jin et al. (2020) propose a multi-granularity interaction network for extractive and abstractive approaches. Li et al. (2020a) develop a neural abstractive multi-document summarization model which leverages explicit graph representations of documents to guide the summary generation process. While the multi-document summarization task aims to extract information shared by multiple documents, related work generation aims to compare and introduce the cited works in logic order. 3 We discuss the related work on related work generation and multi-document summarization. Related Work Generation. Most of the previous related work section generation methods are extractive. For example, Hoang and Kan (2010) take in a set of"
2021.acl-long.473,P19-1500,0,0.23334,"(yt ). (21) In order to handle the out-of-vocabulary (OOV) problem, we equip our decoder with a pointer network (Gu et al., 2016; See et al., 2017). This process is the same as the model described in (See et al., 2017), thus, is omit here due to limited space. 6 6.1 Experimental Setup Baselines To evaluate the performance of our proposed model, we compare it with the following baselines: Extractive Methods: (1) LEAD: selects the first sentence of each document as the summary as a baseline. (2) TextRank (Mihalcea and Tarau, 2004): is a multi-document graph-based ranking model. (3) BertSumEXT (Liu and Lapata, 2019b): is an extractive summarization model with BERT. (4) MGSum-ext (Jin et al., 2020): is a multi-granularity interaction network for extractive multi-document summarization. Abstractive Methods: (1) PTGen+Cov: combines the sequence-tosequence framework with copy and coverage mechanism in summarization task (See et al., 2017). (2) TransformerABS: is an abstractive summarization model based on the Transformer (Vaswani et al., 2017). (3) BertSumABS (Liu and Lapata, 2019b): is an abstractive summarization network built on BERT. (4) MGSum-abs (Jin et al., 2020): is a multi-granularity interaction n"
2021.acl-long.473,D19-1387,0,0.302697,"(yt ). (21) In order to handle the out-of-vocabulary (OOV) problem, we equip our decoder with a pointer network (Gu et al., 2016; See et al., 2017). This process is the same as the model described in (See et al., 2017), thus, is omit here due to limited space. 6 6.1 Experimental Setup Baselines To evaluate the performance of our proposed model, we compare it with the following baselines: Extractive Methods: (1) LEAD: selects the first sentence of each document as the summary as a baseline. (2) TextRank (Mihalcea and Tarau, 2004): is a multi-document graph-based ranking model. (3) BertSumEXT (Liu and Lapata, 2019b): is an extractive summarization model with BERT. (4) MGSum-ext (Jin et al., 2020): is a multi-granularity interaction network for extractive multi-document summarization. Abstractive Methods: (1) PTGen+Cov: combines the sequence-tosequence framework with copy and coverage mechanism in summarization task (See et al., 2017). (2) TransformerABS: is an abstractive summarization model based on the Transformer (Vaswani et al., 2017). (3) BertSumABS (Liu and Lapata, 2019b): is an abstractive summarization network built on BERT. (4) MGSum-abs (Jin et al., 2020): is a multi-granularity interaction n"
2021.acl-long.473,2020.acl-main.447,0,0.0402838,"rk generation aims to compare and introduce the cited works in logic order. 3 We discuss the related work on related work generation and multi-document summarization. Related Work Generation. Most of the previous related work section generation methods are extractive. For example, Hoang and Kan (2010) take in a set of keywords arranged in a hierarchical Related Work Generation Dataset Since there are no public large-scale related work generation datasets, we collect two survey datasets composed of related work sections and their corresponding papers. The first dataset is collected from S2ORC (Lo et al., 2020), which consists of papers in multiple domains (physics, math, computer sci6069 Dataset S2ORC Delve Multi-News RWS DUC03+04 TAC 2011 # Pairs (train/valid/test) 126,655/5,000/5,000 72,927/3,000/3,000 44,972/5,622/5,622 25 320 176 # source (articles) 5.02 3.69 2.78 9.47 10 10 # words (doc) 1,079 626 2,103 5,496 4,636 4,695 # sents (docs) 45 26 82 237 173 188 # words (summary) 148 181 263 367 109 99 # sents (summary) 6.69 7.88 9.97 18.28 2.88 1.00 vocab size 377,431 190,381 666,515 15,019 19,734 24,672 Table 2: Comparison of our S2ORC and Delve dataset to other related work and multi-document dat"
2021.acl-long.473,2020.emnlp-main.648,0,0.0262993,"rization. The multidocument summarization task aims to cover the key shared relevant information among all the documents while avoiding redundancy (Goldstein et al., 2000). Existing multi-document summarization methods are mostly extractive (Christensen et al., 2013; Parveen and Strube, 2014; Ma et al., 2016; Chu and Liu, 2018). For example, Wang et al. (2020) present a heterogeneous graph-based neural network which contains semantic nodes of different granularity levels apart from sentences. Recently, a vast majority of the literature is dedicated to abstractive multi-document summarization. Lu et al. (2020) propose a large-scale multi-document summarization dataset created from scientific articles. Jin et al. (2020) propose a multi-granularity interaction network for extractive and abstractive approaches. Li et al. (2020a) develop a neural abstractive multi-document summarization model which leverages explicit graph representations of documents to guide the summary generation process. While the multi-document summarization task aims to extract information shared by multiple documents, related work generation aims to compare and introduce the cited works in logic order. 3 We discuss the related w"
2021.acl-long.473,C16-1143,0,0.0697368,"Missing"
2021.acl-long.473,W04-3252,0,0.674528,"ctive function is the negative log likelihood of the target word yt : P L = − Tt=1 log Ptv (yt ). (21) In order to handle the out-of-vocabulary (OOV) problem, we equip our decoder with a pointer network (Gu et al., 2016; See et al., 2017). This process is the same as the model described in (See et al., 2017), thus, is omit here due to limited space. 6 6.1 Experimental Setup Baselines To evaluate the performance of our proposed model, we compare it with the following baselines: Extractive Methods: (1) LEAD: selects the first sentence of each document as the summary as a baseline. (2) TextRank (Mihalcea and Tarau, 2004): is a multi-document graph-based ranking model. (3) BertSumEXT (Liu and Lapata, 2019b): is an extractive summarization model with BERT. (4) MGSum-ext (Jin et al., 2020): is a multi-granularity interaction network for extractive multi-document summarization. Abstractive Methods: (1) PTGen+Cov: combines the sequence-tosequence framework with copy and coverage mechanism in summarization task (See et al., 2017). (2) TransformerABS: is an abstractive summarization model based on the Transformer (Vaswani et al., 2017). (3) BertSumABS (Liu and Lapata, 2019b): is an abstractive summarization network"
2021.acl-long.473,W14-3703,0,0.029514,"e papers of the paper being written to cover the Steiner tree. However, abstractive approaches on related work generation have met with limited success. Apart from the lack of sufficient training data, neural models also face the challenge of identifying the logic relationship between multiple input documents. Multi-document Summarization. The multidocument summarization task aims to cover the key shared relevant information among all the documents while avoiding redundancy (Goldstein et al., 2000). Existing multi-document summarization methods are mostly extractive (Christensen et al., 2013; Parveen and Strube, 2014; Ma et al., 2016; Chu and Liu, 2018). For example, Wang et al. (2020) present a heterogeneous graph-based neural network which contains semantic nodes of different granularity levels apart from sentences. Recently, a vast majority of the literature is dedicated to abstractive multi-document summarization. Lu et al. (2020) propose a large-scale multi-document summarization dataset created from scientific articles. Jin et al. (2020) propose a multi-granularity interaction network for extractive and abstractive approaches. Li et al. (2020a) develop a neural abstractive multi-document summarizati"
2021.acl-long.473,P17-1099,0,0.337005,"hers to familiarize themselves with the state of the art in the field. Several methods (Hoang and Kan, 2010; Hu and Wan, 2014; Chen and Zhuge, 2019) have been proposed to study how to obtain the related work section automatically by ∗ Corresponding author. https://github.com/iriscxy/ relatedworkgeneration 1 extracting important sentences from multiple original papers. However, extractive approaches lack the sophisticated abilities that are crucial to highquality summarization such as paraphrasing and generalization, and often lead to a related work section with poor coherence and readability (See et al., 2017; Hsu et al., 2018). For example, as shown in Table 1, the extracted sentences share the pattern “We find...” as the subject of sentences, which, as a matter of fact, refer to different authors. On the contrary, the abstractive related work in Table 1 reveals that the works are conducted by different scholars. It also has conjunction words such as “Furthermore” and “However”, which can explain the logical relationship between the cited works, and thus form an elegant narration. Hence, in this paper, we target on the abstractive related work generation task, which generates a related work inclu"
2021.acl-long.473,2020.acl-main.553,0,0.0135877,"ractive approaches on related work generation have met with limited success. Apart from the lack of sufficient training data, neural models also face the challenge of identifying the logic relationship between multiple input documents. Multi-document Summarization. The multidocument summarization task aims to cover the key shared relevant information among all the documents while avoiding redundancy (Goldstein et al., 2000). Existing multi-document summarization methods are mostly extractive (Christensen et al., 2013; Parveen and Strube, 2014; Ma et al., 2016; Chu and Liu, 2018). For example, Wang et al. (2020) present a heterogeneous graph-based neural network which contains semantic nodes of different granularity levels apart from sentences. Recently, a vast majority of the literature is dedicated to abstractive multi-document summarization. Lu et al. (2020) propose a large-scale multi-document summarization dataset created from scientific articles. Jin et al. (2020) propose a multi-granularity interaction network for extractive and abstractive approaches. Li et al. (2020a) develop a neural abstractive multi-document summarization model which leverages explicit graph representations of documents t"
2021.acl-short.126,D19-1498,0,0.0288646,"Missing"
2021.acl-short.126,P16-1200,0,0.101815,"Missing"
2021.acl-short.126,2020.acl-main.141,0,0.0305485,"Missing"
2021.acl-short.126,Q17-1008,0,0.0560226,"Missing"
2021.acl-short.126,D14-1162,0,0.0859413,"Missing"
2021.acl-short.126,E17-1110,0,0.0293799,"d and tail entities respectively, and S∗ represents a sentence. both the head and tail entities, the sentence itself can be seen as a path (the intra-sentence case). For more complex situations where the head and tail entities do not co-occur in one sentence, we define the following 3 types of paths which indicate how the head and tail entities can be possibly related in the context. Figure 2 provides a visualization of the three types of paths. Consecutive Paths Previous studies have shown that the majority of inter-sentence relations are often in nearby text (Swampillai and Stevenson, 2010; Quirk and Poon, 2017). We thus select the consecutive sentences to form a path when the head and tail entities are in nearby sentences. Formally, if one mention of the head entity appears in sentence Si and one mention of the tail entity is in sentence Sj , these two sentences along with the sentence in between, i.e., sentence Si+1 , . . . , Sj−1 (or Sj+1 , . . . , Si−1 when i ≥ j) forms a possible path that connects the two entities. Given that no more than 3 sentences would suffice for inference, we limit the length of these Consecutive Paths to be at most 3, which means |j − i |≤ 2. Note that this definition ca"
2021.acl-short.126,swampillai-stevenson-2010-inter,0,0.0436497,"d et stands for a mention of head and tail entities respectively, and S∗ represents a sentence. both the head and tail entities, the sentence itself can be seen as a path (the intra-sentence case). For more complex situations where the head and tail entities do not co-occur in one sentence, we define the following 3 types of paths which indicate how the head and tail entities can be possibly related in the context. Figure 2 provides a visualization of the three types of paths. Consecutive Paths Previous studies have shown that the majority of inter-sentence relations are often in nearby text (Swampillai and Stevenson, 2010; Quirk and Poon, 2017). We thus select the consecutive sentences to form a path when the head and tail entities are in nearby sentences. Formally, if one mention of the head entity appears in sentence Si and one mention of the tail entity is in sentence Sj , these two sentences along with the sentence in between, i.e., sentence Si+1 , . . . , Sj−1 (or Sj+1 , . . . , Si−1 when i ≥ j) forms a possible path that connects the two entities. Given that no more than 3 sentences would suffice for inference, we limit the length of these Consecutive Paths to be at most 3, which means |j − i |≤ 2. Note"
2021.acl-short.126,P19-1074,0,0.0159357,"−1 when i ≥ j) forms a possible path that connects the two entities. Given that no more than 3 sentences would suffice for inference, we limit the length of these Consecutive Paths to be at most 3, which means |j − i |≤ 2. Note that this definition can be naturally extended to the intra-sentence case where j = i. We thus consider the intra-sentence case as a type of the Consecutive Path. A pair of entities can correspond to multiple consecutive paths since they can be mentioned more than once. Multi-Hop Paths Another typical case for intersentence relation instances is the multi-hop relation (Yao et al., 2019; Zeng et al., 2020a). In such cases, the head and tail entities are far from each other in the document but can be connected through bridge entities, just like the entity The Espoo Cathedral in Figure 1 bridges the EC Parish and Finland in sentence 1 and 6. For these cases, we start from the head entity, go through all the bridge entities, arrive at the tail entity, and select all the corresponding sentences in this route as a path. Formally, for the head entity eh and the tail entity et , the multi-hop relation indicates that there exist a list of bridge entities eb1 , . . . , ebk such that"
2021.acl-short.126,2020.emnlp-main.127,0,0.287353,"have released our code at https://github.com/AndrewZhe/ThreeSentences-Are-All-You-Need. 1 Figure 1: A case extracted from the DocRED dataset. While the document has 6 sentences, only 1 or 2 sentences form the evidence for each relation instance. Introduction The task of relation extraction (RE) focuses on extracting relations between entity pairs in texts, and has played an important role in information extraction. While earlier works focus on extracting relations within a sentence (Lin et al., 2016; Zhang et al., 2018), recent studies begin to explore RE at document level (Peng et al., 2017; Zeng et al., 2020a; Nan et al., 2020a), which is more challenging as it often requires reasoning across multiple sentences. Compared with sentence level extraction, documents are significantly longer with useful information scattered in a larger scale. However, given a pair of entities, one may only need a few sentences, not the entire document, to infer their relationship; reading the whole document may not be necessary, since it may introduce unrelated information inevitably. As we can see in Figure 1, S[1] is sufficient to recognize Finland as the country of Espoo, and recognizing the rest two instances req"
2021.acl-short.126,D18-1244,0,0.0505205,"Missing"
2021.findings-acl.419,D19-1501,1,0.854038,"Missing"
2021.findings-acl.419,D19-1201,1,0.891582,"Missing"
2021.findings-acl.419,2020.emnlp-main.313,1,0.855696,"Missing"
2021.findings-acl.419,D18-1442,1,0.913849,"es in the source text across different datasets. • To the best of our knowledge, we are the first to use factual tables to guide the summarization procedure so as to generate better summaries. • We release a large-scale abstractive biography summarization dataset with tables. Experiments conducted on this dataset demonstrate the effectiveness of incorporating table information in generating summaries. 2 2.1 Related Work Text Summarization Text summarization is an important task which can be classified into extractive and abstractive approaches. Extractive summarization (Narayan et al., 2018b; Chen et al., 2018; Jadhav and Rajan, 2018) tends to generate a summary by integrating the most salient sentences in the document. Cheng and Lapata (2016) first propose using recurrent neural network (RNN) to extract salient sentences. After that researchers explore many the neural based method (Nallapati et al., 2017; Liu and Lapata, 2019; Chen et al., 2018; Zhang et al., 2018; Zhou et al., 2018; Liu et al., 2019), and achieve the state-of-the-art performance (Liu and Lapata, 2019) on the benchmark dataset CNN/DailyMail. In the mean time, the Nallapati et al. (2016) firstly apply this text generation method to"
2021.findings-acl.419,P16-1046,0,0.0197469,"he summarization procedure so as to generate better summaries. • We release a large-scale abstractive biography summarization dataset with tables. Experiments conducted on this dataset demonstrate the effectiveness of incorporating table information in generating summaries. 2 2.1 Related Work Text Summarization Text summarization is an important task which can be classified into extractive and abstractive approaches. Extractive summarization (Narayan et al., 2018b; Chen et al., 2018; Jadhav and Rajan, 2018) tends to generate a summary by integrating the most salient sentences in the document. Cheng and Lapata (2016) first propose using recurrent neural network (RNN) to extract salient sentences. After that researchers explore many the neural based method (Nallapati et al., 2017; Liu and Lapata, 2019; Chen et al., 2018; Zhang et al., 2018; Zhou et al., 2018; Liu et al., 2019), and achieve the state-of-the-art performance (Liu and Lapata, 2019) on the benchmark dataset CNN/DailyMail. In the mean time, the Nallapati et al. (2016) firstly apply this text generation method to the abstractive summarization task and Gehrmann et al. (2018) achieve the state-of-the-art performance by using a data-efficient conten"
2021.findings-acl.419,D19-1388,1,0.871882,"he other is abstractive summarization (See et al., 2017; Hsu et al., 2018a), which aims to concisely paraphrase the input article. In both methods, the summary should always focus on important information, though a document may include trivial facts. ∗ Equal contribution. Ordering is decided by a coin flip. Corresponding Author: Dongyan Zhao 1 https://github.com/gsh199449/ table-summ † To focus on the main information when generating summaries, some researchers propose to incorporate manifold information to improve the performance. Narayan et al. (2017) proposed to incorporate the figures and Gao et al. (2019b) investigated the using of reader comments for more effective summarization. As another type of side information, factual tables provide a natural summary of the biography document. On Wikipedia, in each wiki page about people, there is a factual table (infobox) on the right side of the page summarizing the main properties. Clearly, infobox is helpful for capturing the salient information during summarizing the biography. However, no existing work takes advantage of tables, though are widely available in the biography on Wikipedia. In this paper, we propose Table-Guided Summarization (TaGS)"
2021.findings-acl.419,D18-1443,0,0.0119376,"ate a summary by integrating the most salient sentences in the document. Cheng and Lapata (2016) first propose using recurrent neural network (RNN) to extract salient sentences. After that researchers explore many the neural based method (Nallapati et al., 2017; Liu and Lapata, 2019; Chen et al., 2018; Zhang et al., 2018; Zhou et al., 2018; Liu et al., 2019), and achieve the state-of-the-art performance (Liu and Lapata, 2019) on the benchmark dataset CNN/DailyMail. In the mean time, the Nallapati et al. (2016) firstly apply this text generation method to the abstractive summarization task and Gehrmann et al. (2018) achieve the state-of-the-art performance by using a data-efficient content selector. 2.2 Summarization with Side Information Traditional text summarization methods only use the document as input. However, the gist of the document may lie in side information, such as the title, image captions, or comments which are often available for news-wire articles. As such, various studies (Gao et al., 2020; Hu et al., 2008) have tried to use such information for more efficient and accurate summarization. However, to the best of our knowledge, no existing works consider the use of tables to guide biograp"
2021.findings-acl.419,N18-1065,0,0.0438769,"Missing"
2021.findings-acl.419,P18-1013,0,0.283719,"ataset to validate the quality of the dataset. We also benchmark several commonly used summarization methods on TaGS and hope this will inspire more exciting methods. 1 Introduction Text summarization generates a short text version of a long passage which retains the most important information. Recently, two kinds of approaches have been proposed for automatic text summarization. One is extractive summarization (Nallapati et al., 2017; Liu and Lapata, 2019), which directly selects salient sentences from the passage to create a summary. The other is abstractive summarization (See et al., 2017; Hsu et al., 2018a), which aims to concisely paraphrase the input article. In both methods, the summary should always focus on important information, though a document may include trivial facts. ∗ Equal contribution. Ordering is decided by a coin flip. Corresponding Author: Dongyan Zhao 1 https://github.com/gsh199449/ table-summ † To focus on the main information when generating summaries, some researchers propose to incorporate manifold information to improve the performance. Narayan et al. (2017) proposed to incorporate the figures and Gao et al. (2019b) investigated the using of reader comments for more eff"
2021.findings-acl.419,P18-1014,0,0.0210932,"xt across different datasets. • To the best of our knowledge, we are the first to use factual tables to guide the summarization procedure so as to generate better summaries. • We release a large-scale abstractive biography summarization dataset with tables. Experiments conducted on this dataset demonstrate the effectiveness of incorporating table information in generating summaries. 2 2.1 Related Work Text Summarization Text summarization is an important task which can be classified into extractive and abstractive approaches. Extractive summarization (Narayan et al., 2018b; Chen et al., 2018; Jadhav and Rajan, 2018) tends to generate a summary by integrating the most salient sentences in the document. Cheng and Lapata (2016) first propose using recurrent neural network (RNN) to extract salient sentences. After that researchers explore many the neural based method (Nallapati et al., 2017; Liu and Lapata, 2019; Chen et al., 2018; Zhang et al., 2018; Zhou et al., 2018; Liu et al., 2019), and achieve the state-of-the-art performance (Liu and Lapata, 2019) on the benchmark dataset CNN/DailyMail. In the mean time, the Nallapati et al. (2016) firstly apply this text generation method to the abstractive summariz"
2021.findings-acl.419,N19-1260,0,0.0316544,"Missing"
2021.findings-acl.419,D19-1387,0,0.0496443,"the TaGS (Table-Guided Summarization) dataset1 , the first large-scale biography summarization dataset with tables. Next, we report some statistics about this dataset to validate the quality of the dataset. We also benchmark several commonly used summarization methods on TaGS and hope this will inspire more exciting methods. 1 Introduction Text summarization generates a short text version of a long passage which retains the most important information. Recently, two kinds of approaches have been proposed for automatic text summarization. One is extractive summarization (Nallapati et al., 2017; Liu and Lapata, 2019), which directly selects salient sentences from the passage to create a summary. The other is abstractive summarization (See et al., 2017; Hsu et al., 2018a), which aims to concisely paraphrase the input article. In both methods, the summary should always focus on important information, though a document may include trivial facts. ∗ Equal contribution. Ordering is decided by a coin flip. Corresponding Author: Dongyan Zhao 1 https://github.com/gsh199449/ table-summ † To focus on the main information when generating summaries, some researchers propose to incorporate manifold information to impro"
2021.findings-acl.419,N19-1173,0,0.011415,"ies. 2 2.1 Related Work Text Summarization Text summarization is an important task which can be classified into extractive and abstractive approaches. Extractive summarization (Narayan et al., 2018b; Chen et al., 2018; Jadhav and Rajan, 2018) tends to generate a summary by integrating the most salient sentences in the document. Cheng and Lapata (2016) first propose using recurrent neural network (RNN) to extract salient sentences. After that researchers explore many the neural based method (Nallapati et al., 2017; Liu and Lapata, 2019; Chen et al., 2018; Zhang et al., 2018; Zhou et al., 2018; Liu et al., 2019), and achieve the state-of-the-art performance (Liu and Lapata, 2019) on the benchmark dataset CNN/DailyMail. In the mean time, the Nallapati et al. (2016) firstly apply this text generation method to the abstractive summarization task and Gehrmann et al. (2018) achieve the state-of-the-art performance by using a data-efficient content selector. 2.2 Summarization with Side Information Traditional text summarization methods only use the document as input. However, the gist of the document may lie in side information, such as the title, image captions, or comments which are often available for n"
2021.findings-acl.419,D18-1206,0,0.0138241,"of ground truth summaries in the source text across different datasets. • To the best of our knowledge, we are the first to use factual tables to guide the summarization procedure so as to generate better summaries. • We release a large-scale abstractive biography summarization dataset with tables. Experiments conducted on this dataset demonstrate the effectiveness of incorporating table information in generating summaries. 2 2.1 Related Work Text Summarization Text summarization is an important task which can be classified into extractive and abstractive approaches. Extractive summarization (Narayan et al., 2018b; Chen et al., 2018; Jadhav and Rajan, 2018) tends to generate a summary by integrating the most salient sentences in the document. Cheng and Lapata (2016) first propose using recurrent neural network (RNN) to extract salient sentences. After that researchers explore many the neural based method (Nallapati et al., 2017; Liu and Lapata, 2019; Chen et al., 2018; Zhang et al., 2018; Zhou et al., 2018; Liu et al., 2019), and achieve the state-of-the-art performance (Liu and Lapata, 2019) on the benchmark dataset CNN/DailyMail. In the mean time, the Nallapati et al. (2016) firstly apply this text"
2021.findings-acl.419,N18-1158,0,0.02071,"of ground truth summaries in the source text across different datasets. • To the best of our knowledge, we are the first to use factual tables to guide the summarization procedure so as to generate better summaries. • We release a large-scale abstractive biography summarization dataset with tables. Experiments conducted on this dataset demonstrate the effectiveness of incorporating table information in generating summaries. 2 2.1 Related Work Text Summarization Text summarization is an important task which can be classified into extractive and abstractive approaches. Extractive summarization (Narayan et al., 2018b; Chen et al., 2018; Jadhav and Rajan, 2018) tends to generate a summary by integrating the most salient sentences in the document. Cheng and Lapata (2016) first propose using recurrent neural network (RNN) to extract salient sentences. After that researchers explore many the neural based method (Nallapati et al., 2017; Liu and Lapata, 2019; Chen et al., 2018; Zhang et al., 2018; Zhou et al., 2018; Liu et al., 2019), and achieve the state-of-the-art performance (Liu and Lapata, 2019) on the benchmark dataset CNN/DailyMail. In the mean time, the Nallapati et al. (2016) firstly apply this text"
2021.findings-acl.419,P17-1099,0,0.0471813,"stics about this dataset to validate the quality of the dataset. We also benchmark several commonly used summarization methods on TaGS and hope this will inspire more exciting methods. 1 Introduction Text summarization generates a short text version of a long passage which retains the most important information. Recently, two kinds of approaches have been proposed for automatic text summarization. One is extractive summarization (Nallapati et al., 2017; Liu and Lapata, 2019), which directly selects salient sentences from the passage to create a summary. The other is abstractive summarization (See et al., 2017; Hsu et al., 2018a), which aims to concisely paraphrase the input article. In both methods, the summary should always focus on important information, though a document may include trivial facts. ∗ Equal contribution. Ordering is decided by a coin flip. Corresponding Author: Dongyan Zhao 1 https://github.com/gsh199449/ table-summ † To focus on the main information when generating summaries, some researchers propose to incorporate manifold information to improve the performance. Narayan et al. (2017) proposed to incorporate the figures and Gao et al. (2019b) investigated the using of reader com"
2021.findings-acl.419,D18-1088,0,0.0136327,"table information in generating summaries. 2 2.1 Related Work Text Summarization Text summarization is an important task which can be classified into extractive and abstractive approaches. Extractive summarization (Narayan et al., 2018b; Chen et al., 2018; Jadhav and Rajan, 2018) tends to generate a summary by integrating the most salient sentences in the document. Cheng and Lapata (2016) first propose using recurrent neural network (RNN) to extract salient sentences. After that researchers explore many the neural based method (Nallapati et al., 2017; Liu and Lapata, 2019; Chen et al., 2018; Zhang et al., 2018; Zhou et al., 2018; Liu et al., 2019), and achieve the state-of-the-art performance (Liu and Lapata, 2019) on the benchmark dataset CNN/DailyMail. In the mean time, the Nallapati et al. (2016) firstly apply this text generation method to the abstractive summarization task and Gehrmann et al. (2018) achieve the state-of-the-art performance by using a data-efficient content selector. 2.2 Summarization with Side Information Traditional text summarization methods only use the document as input. However, the gist of the document may lie in side information, such as the title, image captions, or co"
2021.findings-acl.419,P18-1061,0,0.0165519,"n generating summaries. 2 2.1 Related Work Text Summarization Text summarization is an important task which can be classified into extractive and abstractive approaches. Extractive summarization (Narayan et al., 2018b; Chen et al., 2018; Jadhav and Rajan, 2018) tends to generate a summary by integrating the most salient sentences in the document. Cheng and Lapata (2016) first propose using recurrent neural network (RNN) to extract salient sentences. After that researchers explore many the neural based method (Nallapati et al., 2017; Liu and Lapata, 2019; Chen et al., 2018; Zhang et al., 2018; Zhou et al., 2018; Liu et al., 2019), and achieve the state-of-the-art performance (Liu and Lapata, 2019) on the benchmark dataset CNN/DailyMail. In the mean time, the Nallapati et al. (2016) firstly apply this text generation method to the abstractive summarization task and Gehrmann et al. (2018) achieve the state-of-the-art performance by using a data-efficient content selector. 2.2 Summarization with Side Information Traditional text summarization methods only use the document as input. However, the gist of the document may lie in side information, such as the title, image captions, or comments which are of"
2021.findings-acl.432,D19-1501,1,0.854308,"ogue evaluation Metric in latent Space. Experimental results on two real-world dialogue datasets confirm the superiority of our method for open-domain dialogue evaluation, where both Pearson and Spearman correlations with human judgments outperform all baselines. 1 Introduction With the surge of deep learning techniques, generation-based open-domain dialogue systems have witnessed significant improvement in recent years. Plenty of novel and effective models (Sutskever et al., 2014; Serban et al., 2016; Li et al., 2015; Serban et al., 2016; Zhao et al., 2017; Gu et al., 2018; Qiu et al., 2019; Chan et al., 2019b; Serban et al., 2017; Wolf et al., 2019; Hu et al., 2019; Chen et al., 2020) are proposed and have greatly promoted the development of the opendomain dialogue generation. Unlike the endless emergence of novel methods, however, there is still no meaningful and widely accepted automatic evaluation metric for dialogue generation yet. As we ∗ This work was done while Z. Chan was an intern at Tencent AI Lab. Corresponding Author: Rui Yan. know, automatic evaluation allows quick and effective comparison between different systems and is crucial for the development of natural language generation (NL"
2021.findings-acl.432,D19-1201,1,0.869864,"ogue evaluation Metric in latent Space. Experimental results on two real-world dialogue datasets confirm the superiority of our method for open-domain dialogue evaluation, where both Pearson and Spearman correlations with human judgments outperform all baselines. 1 Introduction With the surge of deep learning techniques, generation-based open-domain dialogue systems have witnessed significant improvement in recent years. Plenty of novel and effective models (Sutskever et al., 2014; Serban et al., 2016; Li et al., 2015; Serban et al., 2016; Zhao et al., 2017; Gu et al., 2018; Qiu et al., 2019; Chan et al., 2019b; Serban et al., 2017; Wolf et al., 2019; Hu et al., 2019; Chen et al., 2020) are proposed and have greatly promoted the development of the opendomain dialogue generation. Unlike the endless emergence of novel methods, however, there is still no meaningful and widely accepted automatic evaluation metric for dialogue generation yet. As we ∗ This work was done while Z. Chan was an intern at Tencent AI Lab. Corresponding Author: Rui Yan. know, automatic evaluation allows quick and effective comparison between different systems and is crucial for the development of natural language generation (NL"
2021.findings-acl.432,2020.emnlp-main.313,1,0.729602,"roved BLEU, which compares the generated response with multiply diverse references. Embedding-based Metrics. Unlike word overlap-based metrics comparing two raw sentences, Embedding Metrics (Mitchell and Lapata, 2008; Forgues et al., 2014; Rus and Lintean, 2012) map sentences to a high dimensional space, and calculate similarity based on the high-dimensional representations. Embedding Metrics are recently popular for evaluating the generation tasks, such as text summarization (Gao et al., 2020; Chen et al., 2021), question answer (Gao et al., 2019) and text generation (Hashimoto et al., 2019; Chan et al., 2020). Meanwhile, several works (Qiu et al., 2019; Chen et al., 2020; Gao et al., 2021) have shown their effectiveness in the open-domain dialogue systems. With the development of the large4890 Learning-based Metrics. Recent studies (Tao et al., 2018; Sinha et al., 2020) attempt to mitigate the one-to-many issue by considering the similarity of the generated response with the conversational contexts. The similarity is calculated by a designed discriminative model which learns to evaluate whether a response matches the conversational context well. The discriminative model is learned from tuples of d"
2021.findings-acl.432,2021.acl-long.34,0,0.469022,"ious one-to-many nature of the open-domain dialogues. Therefore, Yuma et al. (2020) proposed the improved BLEU, which compares the generated response with multiply diverse references. Embedding-based Metrics. Unlike word overlap-based metrics comparing two raw sentences, Embedding Metrics (Mitchell and Lapata, 2008; Forgues et al., 2014; Rus and Lintean, 2012) map sentences to a high dimensional space, and calculate similarity based on the high-dimensional representations. Embedding Metrics are recently popular for evaluating the generation tasks, such as text summarization (Gao et al., 2020; Chen et al., 2021), question answer (Gao et al., 2019) and text generation (Hashimoto et al., 2019; Chan et al., 2020). Meanwhile, several works (Qiu et al., 2019; Chen et al., 2020; Gao et al., 2021) have shown their effectiveness in the open-domain dialogue systems. With the development of the large4890 Learning-based Metrics. Recent studies (Tao et al., 2018; Sinha et al., 2020) attempt to mitigate the one-to-many issue by considering the similarity of the generated response with the conversational contexts. The similarity is calculated by a designed discriminative model which learns to evaluate whether a re"
2021.findings-acl.432,N19-1021,0,0.0144273,"dding. posterior qθ (z|hq ), i.e. a conditional Gaussian distribution. Training procedures. Previous works (Bowman et al., 2015; Zhao et al., 2017) mentioned that VAE and CVAE training is challenging due to the KL vanishing issue, where the decoder ignores the conditional information and all the resulting posteriors almost collapse to a same Gaussian prior. To mitigate this issue, first, we initialize our model with Optimus (Li et al., 2020), a large-scale VAE-based PLM model, while optimizing Eq. 2. To mitigate the same issue while optimizing Eq. 3, we use the cyclical KL annealing schedule (Fu et al., 2019). Specifically, we add a hyperparameter α to control the weight of the KLdivergence in Eq. 3. We set α close to zero in the first half of cyclic schedule, linearly anneal α to 1 in the next one-fourth of cyclic schedule and kept α = 1 in the remaining cyclic schedule. Moreover, the Free Bits (Bowman et al., 2015) is also crucial for the training. It replaces the KLdivergence in Eq. 3 by a hinge loss max(γ, KL(qφ (z|hq )||pθ (z|hp ))) (4) where γ is a hyperparameter which controls the information space for the each dimension of the latent variable. Finally, an extra bag-of-word loss (Zhao et al"
2021.findings-acl.432,2021.findings-acl.220,1,0.534829,"Embedding-based Metrics. Unlike word overlap-based metrics comparing two raw sentences, Embedding Metrics (Mitchell and Lapata, 2008; Forgues et al., 2014; Rus and Lintean, 2012) map sentences to a high dimensional space, and calculate similarity based on the high-dimensional representations. Embedding Metrics are recently popular for evaluating the generation tasks, such as text summarization (Gao et al., 2020; Chen et al., 2021), question answer (Gao et al., 2019) and text generation (Hashimoto et al., 2019; Chan et al., 2020). Meanwhile, several works (Qiu et al., 2019; Chen et al., 2020; Gao et al., 2021) have shown their effectiveness in the open-domain dialogue systems. With the development of the large4890 Learning-based Metrics. Recent studies (Tao et al., 2018; Sinha et al., 2020) attempt to mitigate the one-to-many issue by considering the similarity of the generated response with the conversational contexts. The similarity is calculated by a designed discriminative model which learns to evaluate whether a response matches the conversational context well. The discriminative model is learned from tuples of data, {conversational context, response reference, negative sample}, in an unsuperv"
2021.findings-acl.432,2020.acl-main.124,0,0.101111,"ignoring the notorious one-to-many nature of the open-domain dialogues. Therefore, Yuma et al. (2020) proposed the improved BLEU, which compares the generated response with multiply diverse references. Embedding-based Metrics. Unlike word overlap-based metrics comparing two raw sentences, Embedding Metrics (Mitchell and Lapata, 2008; Forgues et al., 2014; Rus and Lintean, 2012) map sentences to a high dimensional space, and calculate similarity based on the high-dimensional representations. Embedding Metrics are recently popular for evaluating the generation tasks, such as text summarization (Gao et al., 2020; Chen et al., 2021), question answer (Gao et al., 2019) and text generation (Hashimoto et al., 2019; Chan et al., 2020). Meanwhile, several works (Qiu et al., 2019; Chen et al., 2020; Gao et al., 2021) have shown their effectiveness in the open-domain dialogue systems. With the development of the large4890 Learning-based Metrics. Recent studies (Tao et al., 2018; Sinha et al., 2020) attempt to mitigate the one-to-many issue by considering the similarity of the generated response with the conversational contexts. The similarity is calculated by a designed discriminative model which learns to e"
2021.findings-acl.432,W19-2310,0,0.198948,"Missing"
2021.findings-acl.432,P19-1590,0,0.0127027,"rior distribution P (z|c) to capture the feasible latent reference information in the latent space. Specifically, when training CVAEs, P (z|c) is forced to be close to the posterior distribution Q(z|c, ri ) for any reference response ri as illustrated in Fig. 1. In this sense, if z is sampled from P (z|c), z may contain some information of any ri in some extent, and z can be used as a surrogate of {rk }K k=1 . Therefore, we can expect 4891 1 There is a brief proof in Apendix A. I(l; c, {rk }N k=1 ) ≥ I(l; c, ri , z) ≥ I(l; c, ri ). 3.2 3.3 Overall Architecture Previous works (Li et al., 2019; Gururangan et al., 2019; Li et al., 2020) concluded that VAEs can learn a smooth latent space through the regularization from the gaussian prior. Inspired by Li et al. (2020), we propose a novel architecture which can be regarded as a large-scale pretrained language model (PLM) based on VAEs/CVAEs. Encoder. Li et al. (2019) argue that the VAEs might benefit from initialization with a noncollapsed encoder, because the encoder provides useful information from the beginning of training. We use the Masked PLMs (Devlin et al., 2018; Liu et al., 2019) as the text encoder because of their impressive effectiveness in natura"
2021.findings-acl.432,N19-1169,0,0.0184016,"(2020) proposed the improved BLEU, which compares the generated response with multiply diverse references. Embedding-based Metrics. Unlike word overlap-based metrics comparing two raw sentences, Embedding Metrics (Mitchell and Lapata, 2008; Forgues et al., 2014; Rus and Lintean, 2012) map sentences to a high dimensional space, and calculate similarity based on the high-dimensional representations. Embedding Metrics are recently popular for evaluating the generation tasks, such as text summarization (Gao et al., 2020; Chen et al., 2021), question answer (Gao et al., 2019) and text generation (Hashimoto et al., 2019; Chan et al., 2020). Meanwhile, several works (Qiu et al., 2019; Chen et al., 2020; Gao et al., 2021) have shown their effectiveness in the open-domain dialogue systems. With the development of the large4890 Learning-based Metrics. Recent studies (Tao et al., 2018; Sinha et al., 2020) attempt to mitigate the one-to-many issue by considering the similarity of the generated response with the conversational contexts. The similarity is calculated by a designed discriminative model which learns to evaluate whether a response matches the conversational context well. The discriminative model is lear"
2021.findings-acl.432,D19-1370,0,0.23084,"ntation learning and dialogue modeling, we propose to learn the dialogue representations via VAEs/CVAEs for better evaluation. Equip with such dialogue representations, we obtain an Enhanced dialogue evaluation Metric in latent Space (EMS). EMS is a self-supervised evaluation metric with a two-stage training procedure. It represents dialogue sentences in a smooth latent space to both capture discourse-level context information and model more feasible latent references. Specifically, in the first stage, we build a VAE based model to map the dialogue sentences into a latent (or semantic) space. Li et al. (2019) showed that VAEs can be viewed as a regularized version of the auto-encoder and learn a smooth latent space through the regularization from the Gaussian prior. Then, we train our model by optimizing CVAEs’ objective which forces the prior distribution to capture the feasible latent references information (details in Section 3.3). In the second stage, we combine the dialogue representations and the captured feasible latent reference information to train a discriminative model. Meanwhile, we give a potential explanation of our motivation about why using feasible latent reference information can"
2021.findings-acl.432,2020.emnlp-main.378,0,0.250324,"to capture the feasible latent reference information in the latent space. Specifically, when training CVAEs, P (z|c) is forced to be close to the posterior distribution Q(z|c, ri ) for any reference response ri as illustrated in Fig. 1. In this sense, if z is sampled from P (z|c), z may contain some information of any ri in some extent, and z can be used as a surrogate of {rk }K k=1 . Therefore, we can expect 4891 1 There is a brief proof in Apendix A. I(l; c, {rk }N k=1 ) ≥ I(l; c, ri , z) ≥ I(l; c, ri ). 3.2 3.3 Overall Architecture Previous works (Li et al., 2019; Gururangan et al., 2019; Li et al., 2020) concluded that VAEs can learn a smooth latent space through the regularization from the gaussian prior. Inspired by Li et al. (2020), we propose a novel architecture which can be regarded as a large-scale pretrained language model (PLM) based on VAEs/CVAEs. Encoder. Li et al. (2019) argue that the VAEs might benefit from initialization with a noncollapsed encoder, because the encoder provides useful information from the beginning of training. We use the Masked PLMs (Devlin et al., 2018; Liu et al., 2019) as the text encoder because of their impressive effectiveness in natural language underst"
2021.findings-acl.432,I17-1099,0,0.0443873,"Missing"
2021.findings-acl.432,D16-1230,0,0.030509,"ning variational model to capture the feasible latent references; • Experiments performed on two large datasets demonstrate the effectiveness of our proposed model and outperform all baseline methods. 2 Related Work Word overlap-based Metrics. Several word overlap-based automatic evaluation metrics, such as BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and ROUGE (Lin, 2004), have been widely used to evaluate the quality of generated responses. These word overlap-based metrics measure how many words overlap in a given generated response when compared to a reference response. Liu et al. (2016); Lowe et al. (2017); Tao et al. (2018) argued that these word overlap-based metric scores are weakly correlated to human judgment due to ignoring the notorious one-to-many nature of the open-domain dialogues. Therefore, Yuma et al. (2020) proposed the improved BLEU, which compares the generated response with multiply diverse references. Embedding-based Metrics. Unlike word overlap-based metrics comparing two raw sentences, Embedding Metrics (Mitchell and Lapata, 2008; Forgues et al., 2014; Rus and Lintean, 2012) map sentences to a high dimensional space, and calculate similarity based on the"
2021.findings-acl.432,2021.ccl-1.108,0,0.0249961,"Missing"
2021.findings-acl.432,P17-1103,0,0.0589553,"del to capture the feasible latent references; • Experiments performed on two large datasets demonstrate the effectiveness of our proposed model and outperform all baseline methods. 2 Related Work Word overlap-based Metrics. Several word overlap-based automatic evaluation metrics, such as BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and ROUGE (Lin, 2004), have been widely used to evaluate the quality of generated responses. These word overlap-based metrics measure how many words overlap in a given generated response when compared to a reference response. Liu et al. (2016); Lowe et al. (2017); Tao et al. (2018) argued that these word overlap-based metric scores are weakly correlated to human judgment due to ignoring the notorious one-to-many nature of the open-domain dialogues. Therefore, Yuma et al. (2020) proposed the improved BLEU, which compares the generated response with multiply diverse references. Embedding-based Metrics. Unlike word overlap-based metrics comparing two raw sentences, Embedding Metrics (Mitchell and Lapata, 2008; Forgues et al., 2014; Rus and Lintean, 2012) map sentences to a high dimensional space, and calculate similarity based on the high-dimensional rep"
2021.findings-acl.432,P08-1028,0,0.819519,"ural language generation (NLG) tasks (Dathathri et al., 2019; Gu et al., 2019; Gao et al., 2019; Chan et al., 2019a, 2020). The lack of meaningful automatic evaluation metrics has become a significant impediment for open-domain dialog generation research. Over the past decade, many automatic evaluation metrics are proposed to evaluate the opendomain dialogue systems. Among them, the word overlap-based automatic evaluation metrics from NLG tasks, such as BLEU (Papineni et al., 2002) in machine translation and ROUGE (Lin, 2004) in text summarization, are popular. In addition, Embedding Metrics (Mitchell and Lapata, 2008; Forgues et al., 2014; Rus and Lintean, 2012) have been utilized to evaluate the open-domain dialogue systems (Gu et al., 2018; Chan et al., 2019b; Shen et al., 2018). Recently, with the fantastic development of the large-scale pre-training model (Devlin et al., 2018; Liu et al., 2019; Radford et al., 2019), researchers proposed to enhance the embedding metrics by converting the dialogue sentences to hidden space via pre-training model (Zhang et al., 2019; Sellam et al., 2020; Zhao et al., 2019; Xiang et al., 2021). The common idea behind these metrics is that they measure the semantic simila"
2021.findings-acl.432,P02-1040,0,0.116934,"an. know, automatic evaluation allows quick and effective comparison between different systems and is crucial for the development of natural language generation (NLG) tasks (Dathathri et al., 2019; Gu et al., 2019; Gao et al., 2019; Chan et al., 2019a, 2020). The lack of meaningful automatic evaluation metrics has become a significant impediment for open-domain dialog generation research. Over the past decade, many automatic evaluation metrics are proposed to evaluate the opendomain dialogue systems. Among them, the word overlap-based automatic evaluation metrics from NLG tasks, such as BLEU (Papineni et al., 2002) in machine translation and ROUGE (Lin, 2004) in text summarization, are popular. In addition, Embedding Metrics (Mitchell and Lapata, 2008; Forgues et al., 2014; Rus and Lintean, 2012) have been utilized to evaluate the open-domain dialogue systems (Gu et al., 2018; Chan et al., 2019b; Shen et al., 2018). Recently, with the fantastic development of the large-scale pre-training model (Devlin et al., 2018; Liu et al., 2019; Radford et al., 2019), researchers proposed to enhance the embedding metrics by converting the dialogue sentences to hidden space via pre-training model (Zhang et al., 2019;"
2021.findings-acl.432,P19-1372,1,0.904042,", an Enhanced dialogue evaluation Metric in latent Space. Experimental results on two real-world dialogue datasets confirm the superiority of our method for open-domain dialogue evaluation, where both Pearson and Spearman correlations with human judgments outperform all baselines. 1 Introduction With the surge of deep learning techniques, generation-based open-domain dialogue systems have witnessed significant improvement in recent years. Plenty of novel and effective models (Sutskever et al., 2014; Serban et al., 2016; Li et al., 2015; Serban et al., 2016; Zhao et al., 2017; Gu et al., 2018; Qiu et al., 2019; Chan et al., 2019b; Serban et al., 2017; Wolf et al., 2019; Hu et al., 2019; Chen et al., 2020) are proposed and have greatly promoted the development of the opendomain dialogue generation. Unlike the endless emergence of novel methods, however, there is still no meaningful and widely accepted automatic evaluation metric for dialogue generation yet. As we ∗ This work was done while Z. Chan was an intern at Tencent AI Lab. Corresponding Author: Rui Yan. know, automatic evaluation allows quick and effective comparison between different systems and is crucial for the development of natural lang"
2021.findings-acl.432,W12-2018,0,0.667018,"t al., 2019; Gu et al., 2019; Gao et al., 2019; Chan et al., 2019a, 2020). The lack of meaningful automatic evaluation metrics has become a significant impediment for open-domain dialog generation research. Over the past decade, many automatic evaluation metrics are proposed to evaluate the opendomain dialogue systems. Among them, the word overlap-based automatic evaluation metrics from NLG tasks, such as BLEU (Papineni et al., 2002) in machine translation and ROUGE (Lin, 2004) in text summarization, are popular. In addition, Embedding Metrics (Mitchell and Lapata, 2008; Forgues et al., 2014; Rus and Lintean, 2012) have been utilized to evaluate the open-domain dialogue systems (Gu et al., 2018; Chan et al., 2019b; Shen et al., 2018). Recently, with the fantastic development of the large-scale pre-training model (Devlin et al., 2018; Liu et al., 2019; Radford et al., 2019), researchers proposed to enhance the embedding metrics by converting the dialogue sentences to hidden space via pre-training model (Zhang et al., 2019; Sellam et al., 2020; Zhao et al., 2019; Xiang et al., 2021). The common idea behind these metrics is that they measure the semantic similarity between a reference response and a genera"
2021.findings-acl.432,2020.tacl-1.52,0,0.402268,"for Computational Linguistics responses with the conversational context. Specifically, these works design discriminative models which can judge whether the generated responses match the conversational context well, which learn from {conversational context, response reference, negative sample} pairs in unsupervised learning manner. Zhao et al. (2020) further proposed to enhance such discriminative evaluation metrics by finetuning on a few human-annotated data to improve the robustness. These discriminative metrics trained using a single relevant response and multiple negative samples. However, Sai et al. (2020) argued that such discriminative metrics should be trained on multiple relevant responses (i.e., positive samples) and multiple negative samples, to favor the one-to-many nature in open-domain dialogues. Therefore, they collected a new dataset which contains multiple relevant and irrelevant responses for any given conversational context to train their discriminative evaluation model and the model trained by multiple relevant responses shows impressive performance. However, there are no organized relevant multiple responses in most existing datasets. Collecting a new dataset is expensive and ti"
2021.findings-acl.432,2020.acl-main.704,0,0.0671024,"in machine translation and ROUGE (Lin, 2004) in text summarization, are popular. In addition, Embedding Metrics (Mitchell and Lapata, 2008; Forgues et al., 2014; Rus and Lintean, 2012) have been utilized to evaluate the open-domain dialogue systems (Gu et al., 2018; Chan et al., 2019b; Shen et al., 2018). Recently, with the fantastic development of the large-scale pre-training model (Devlin et al., 2018; Liu et al., 2019; Radford et al., 2019), researchers proposed to enhance the embedding metrics by converting the dialogue sentences to hidden space via pre-training model (Zhang et al., 2019; Sellam et al., 2020; Zhao et al., 2019; Xiang et al., 2021). The common idea behind these metrics is that they measure the semantic similarity between a reference response and a generated response, independent on the conversational context. However, due to the notorious one-to-many nature (Li et al., 2015; Zhao et al., 2017; Qiu et al., 2019; Gu et al., 2018) of open-domain dialogue, a good response should be related well to its context yet may be largely different from a reference response in semantics. Some other works (Tao et al., 2018; Ghazarian et al., 2019; Sinha et al., 2020) thereby proposed to build aut"
2021.findings-acl.432,P18-1205,0,0.100736,"Missing"
2021.findings-acl.432,P17-1061,0,0.20045,"space. Specifically, we present EMS, an Enhanced dialogue evaluation Metric in latent Space. Experimental results on two real-world dialogue datasets confirm the superiority of our method for open-domain dialogue evaluation, where both Pearson and Spearman correlations with human judgments outperform all baselines. 1 Introduction With the surge of deep learning techniques, generation-based open-domain dialogue systems have witnessed significant improvement in recent years. Plenty of novel and effective models (Sutskever et al., 2014; Serban et al., 2016; Li et al., 2015; Serban et al., 2016; Zhao et al., 2017; Gu et al., 2018; Qiu et al., 2019; Chan et al., 2019b; Serban et al., 2017; Wolf et al., 2019; Hu et al., 2019; Chen et al., 2020) are proposed and have greatly promoted the development of the opendomain dialogue generation. Unlike the endless emergence of novel methods, however, there is still no meaningful and widely accepted automatic evaluation metric for dialogue generation yet. As we ∗ This work was done while Z. Chan was an intern at Tencent AI Lab. Corresponding Author: Rui Yan. know, automatic evaluation allows quick and effective comparison between different systems and is crucial"
2021.findings-acl.432,2020.acl-main.4,0,0.417553,"9; Sinha et al., 2020) thereby proposed to build automatic dialogue evaluation metrics by considering the similarity of the generated 4889 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4889–4900 August 1–6, 2021. ©2021 Association for Computational Linguistics responses with the conversational context. Specifically, these works design discriminative models which can judge whether the generated responses match the conversational context well, which learn from {conversational context, response reference, negative sample} pairs in unsupervised learning manner. Zhao et al. (2020) further proposed to enhance such discriminative evaluation metrics by finetuning on a few human-annotated data to improve the robustness. These discriminative metrics trained using a single relevant response and multiple negative samples. However, Sai et al. (2020) argued that such discriminative metrics should be trained on multiple relevant responses (i.e., positive samples) and multiple negative samples, to favor the one-to-many nature in open-domain dialogues. Therefore, they collected a new dataset which contains multiple relevant and irrelevant responses for any given conversational con"
2021.findings-acl.432,D19-1053,0,0.201466,"n and ROUGE (Lin, 2004) in text summarization, are popular. In addition, Embedding Metrics (Mitchell and Lapata, 2008; Forgues et al., 2014; Rus and Lintean, 2012) have been utilized to evaluate the open-domain dialogue systems (Gu et al., 2018; Chan et al., 2019b; Shen et al., 2018). Recently, with the fantastic development of the large-scale pre-training model (Devlin et al., 2018; Liu et al., 2019; Radford et al., 2019), researchers proposed to enhance the embedding metrics by converting the dialogue sentences to hidden space via pre-training model (Zhang et al., 2019; Sellam et al., 2020; Zhao et al., 2019; Xiang et al., 2021). The common idea behind these metrics is that they measure the semantic similarity between a reference response and a generated response, independent on the conversational context. However, due to the notorious one-to-many nature (Li et al., 2015; Zhao et al., 2017; Qiu et al., 2019; Gu et al., 2018) of open-domain dialogue, a good response should be related well to its context yet may be largely different from a reference response in semantics. Some other works (Tao et al., 2018; Ghazarian et al., 2019; Sinha et al., 2020) thereby proposed to build automatic dialogue eva"
2021.findings-acl.432,D18-1463,0,0.0149516,"s has become a significant impediment for open-domain dialog generation research. Over the past decade, many automatic evaluation metrics are proposed to evaluate the opendomain dialogue systems. Among them, the word overlap-based automatic evaluation metrics from NLG tasks, such as BLEU (Papineni et al., 2002) in machine translation and ROUGE (Lin, 2004) in text summarization, are popular. In addition, Embedding Metrics (Mitchell and Lapata, 2008; Forgues et al., 2014; Rus and Lintean, 2012) have been utilized to evaluate the open-domain dialogue systems (Gu et al., 2018; Chan et al., 2019b; Shen et al., 2018). Recently, with the fantastic development of the large-scale pre-training model (Devlin et al., 2018; Liu et al., 2019; Radford et al., 2019), researchers proposed to enhance the embedding metrics by converting the dialogue sentences to hidden space via pre-training model (Zhang et al., 2019; Sellam et al., 2020; Zhao et al., 2019; Xiang et al., 2021). The common idea behind these metrics is that they measure the semantic similarity between a reference response and a generated response, independent on the conversational context. However, due to the notorious one-to-many nature (Li et al., 201"
2021.findings-acl.432,2020.acl-main.220,0,0.122106,"ng model (Zhang et al., 2019; Sellam et al., 2020; Zhao et al., 2019; Xiang et al., 2021). The common idea behind these metrics is that they measure the semantic similarity between a reference response and a generated response, independent on the conversational context. However, due to the notorious one-to-many nature (Li et al., 2015; Zhao et al., 2017; Qiu et al., 2019; Gu et al., 2018) of open-domain dialogue, a good response should be related well to its context yet may be largely different from a reference response in semantics. Some other works (Tao et al., 2018; Ghazarian et al., 2019; Sinha et al., 2020) thereby proposed to build automatic dialogue evaluation metrics by considering the similarity of the generated 4889 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4889–4900 August 1–6, 2021. ©2021 Association for Computational Linguistics responses with the conversational context. Specifically, these works design discriminative models which can judge whether the generated responses match the conversational context well, which learn from {conversational context, response reference, negative sample} pairs in unsupervised learning manner. Zhao et al. (2020) fur"
2021.findings-acl.432,2021.findings-acl.193,1,0.84362,"004) in text summarization, are popular. In addition, Embedding Metrics (Mitchell and Lapata, 2008; Forgues et al., 2014; Rus and Lintean, 2012) have been utilized to evaluate the open-domain dialogue systems (Gu et al., 2018; Chan et al., 2019b; Shen et al., 2018). Recently, with the fantastic development of the large-scale pre-training model (Devlin et al., 2018; Liu et al., 2019; Radford et al., 2019), researchers proposed to enhance the embedding metrics by converting the dialogue sentences to hidden space via pre-training model (Zhang et al., 2019; Sellam et al., 2020; Zhao et al., 2019; Xiang et al., 2021). The common idea behind these metrics is that they measure the semantic similarity between a reference response and a generated response, independent on the conversational context. However, due to the notorious one-to-many nature (Li et al., 2015; Zhao et al., 2017; Qiu et al., 2019; Gu et al., 2018) of open-domain dialogue, a good response should be related well to its context yet may be largely different from a reference response in semantics. Some other works (Tao et al., 2018; Ghazarian et al., 2019; Sinha et al., 2020) thereby proposed to build automatic dialogue evaluation metrics by co"
2021.findings-acl.432,2020.acl-srw.27,0,0.0191815,"rics. Several word overlap-based automatic evaluation metrics, such as BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and ROUGE (Lin, 2004), have been widely used to evaluate the quality of generated responses. These word overlap-based metrics measure how many words overlap in a given generated response when compared to a reference response. Liu et al. (2016); Lowe et al. (2017); Tao et al. (2018) argued that these word overlap-based metric scores are weakly correlated to human judgment due to ignoring the notorious one-to-many nature of the open-domain dialogues. Therefore, Yuma et al. (2020) proposed the improved BLEU, which compares the generated response with multiply diverse references. Embedding-based Metrics. Unlike word overlap-based metrics comparing two raw sentences, Embedding Metrics (Mitchell and Lapata, 2008; Forgues et al., 2014; Rus and Lintean, 2012) map sentences to a high dimensional space, and calculate similarity based on the high-dimensional representations. Embedding Metrics are recently popular for evaluating the generation tasks, such as text summarization (Gao et al., 2020; Chen et al., 2021), question answer (Gao et al., 2019) and text generation (Hashimo"
2021.findings-acl.85,D14-1162,0,0.103024,"Missing"
2021.findings-acl.85,D16-1264,0,0.0515683,"The paraphrasing challenge requires MRC models to identify the same meaning represented in different words. Regarding the shortcut tricks, we study two typical kinds: question word matching (QWM) and simple matching (SpM) (Sugawara et al., 2018). For QWM, MRC models can simply obtain an answer phrase by recognizing the expected entity type confined by the wh-question words of question Q. For SpM, a model can find the answers by identifying the word overlap between answer sentences and the questions. QWM-Para Dataset: As elaborated in Algorithm 1, given an original instance (Q, P ) from SQuAD (Rajpurkar et al., 2016), we paraphrase the question Q in Qp to embed the paraphrasing challenge, and derive the corresponding shortcut version by dropping the sentences containing other entities with the matched type according to the question words from the given passage. An example is shown in the left of Figure 2. In the challenging version of Q.2, both Beyonce and Lisa are person names which match the question word who. Thus, one should at least recognize the paraphrasing relationship between named the most influential music girl and rated as the most powerful female musician to distinguish between the two names"
2021.findings-acl.85,Q19-1016,0,0.08589,"training sets hinder models from exploring the sophisticated reasoning skills in the later stage of training. 1 Paraphrasing P: ... Begun as a claims to be the oldest continuous collegiate publication in the United States … Comprehension Challenge Shortcut Figure 1: An illustration of shortcuts in Machine Reading Comprehension. P is an excerpt of the original passage. different benchmarks. These benchmarks are designed to address challenging features, such as evidence checking in multi-document inference (Yang et al., 2018), co-reference resolution (Dasigi et al., 2019), dialog understanding (Reddy et al., 2019), symbolic reasoning (Dua et al., 2019), and so on. The task of machine reading comprehension (MRC) aims at evaluating whether a model can understand natural language texts by answering a series of questions. Recently, MRC research has seen considerable progress in terms of model performance, and many models are reported to approach or even outperform human-level performance on Corresponding author. Answer the Scholastic magazine is issued twice monthly and Introduction ∗ one-page journal in September 1876 , Coreference However, recent analysis indicates that many MRC models unintentionally le"
2021.findings-acl.85,W17-2623,0,0.0190064,"n Shortcut Version Step3 Substitute & Sentence Shuﬄe Q.3: Who was rated as the most powerful female musician? P: She released a new album with Lisa… Forbes Magazine named Beyonce as the most influential music creator. Challenging Version Figure 2: An illustration of how the instances in the synthetic datasets are constructed from original SQuAD data. Each instance has a shortcut version paired with a challenging version where comprehension skills are necessary. In this work, we focus on paraphrasing (Para) as the complex reasoning challenge, since it widely exists in many recent MRC datasets (Trischler et al., 2017; Reddy et al., 2019; Clark et al., 2019). The paraphrasing challenge requires MRC models to identify the same meaning represented in different words. Regarding the shortcut tricks, we study two typical kinds: question word matching (QWM) and simple matching (SpM) (Sugawara et al., 2018). For QWM, MRC models can simply obtain an answer phrase by recognizing the expected entity type confined by the wh-question words of question Q. For SpM, a model can find the answers by identifying the word overlap between answer sentences and the questions. QWM-Para Dataset: As elaborated in Algorithm 1, give"
2021.findings-acl.85,D19-1221,0,0.0185872,"y find that manually designed features (Chen et al., 2016) or simple model architectures (Weissenborn et al., 2017) could obtain competitive performance, indicating that complicated inference procedure may be dispensable. Even without reading the entire questions or documents, models can still correctly answer a large portion of the questions (Sugawara et al., 2018; Kaushik and Lipton, 2018; Min et al., 2019). Therefore, current MRC datasets may lack the benchmarking capacity on requisite skills (Sugawara et al., 2020), and models may be vulnerable to adversarial attacks (Jia and Liang, 2017; Wallace et al., 2019; Si et al., 2019). However, they do not formally discuss or analyze why models could learn shortcuts from the perspectives of the learning procedure. On the way of designing better MRC datasets, Jiang and Bansal (2019) construct adversarial questions to guide model learning the multi-hop reasoning skills. Bartolo et al. (2020) propose a model-inloop paradigm to annotate challenging questions. More recent works (Jhamtani and Clark, 2020; Ho et al., 2020) propose new datasets with evidence based metrics to evaluate whether the questions are solved via shortcuts. Our work aims at providing empir"
2021.findings-acl.85,K17-1028,0,0.0158608,"conomic or financial factors]Ans ... P-shortcut: ... Most of these defections occur because of [economic or financial factors]Ans . Most of these errors are caused by [economic or financial factors]Ans . ... 6 Related Works Reading documents to answer natural language questions has drawn more and more attention in recent years (Xu et al., 2016; Minjoon et al., 2017; Lai et al., 2019; Glass et al., 2020). Most previous works focus on revealing the shortcut phenomenon in MRC from different perspectives. They find that manually designed features (Chen et al., 2016) or simple model architectures (Weissenborn et al., 2017) could obtain competitive performance, indicating that complicated inference procedure may be dispensable. Even without reading the entire questions or documents, models can still correctly answer a large portion of the questions (Sugawara et al., 2018; Kaushik and Lipton, 2018; Min et al., 2019). Therefore, current MRC datasets may lack the benchmarking capacity on requisite skills (Sugawara et al., 2020), and models may be vulnerable to adversarial attacks (Jia and Liang, 2017; Wallace et al., 2019; Si et al., 2019). However, they do not formally discuss or analyze why models could learn sho"
2021.findings-acl.85,C16-1226,1,0.827302,"ining with both type of questions, BERT can learn the simple matching trick earlier than identifying the required paraphrasing between why defections occur and errors caused by. Q.6: Why do these defections occur? P-challenging: ... Most of these errors are caused by [economic or financial factors]Ans ... P-shortcut: ... Most of these defections occur because of [economic or financial factors]Ans . Most of these errors are caused by [economic or financial factors]Ans . ... 6 Related Works Reading documents to answer natural language questions has drawn more and more attention in recent years (Xu et al., 2016; Minjoon et al., 2017; Lai et al., 2019; Glass et al., 2020). Most previous works focus on revealing the shortcut phenomenon in MRC from different perspectives. They find that manually designed features (Chen et al., 2016) or simple model architectures (Weissenborn et al., 2017) could obtain competitive performance, indicating that complicated inference procedure may be dispensable. Even without reading the entire questions or documents, models can still correctly answer a large portion of the questions (Sugawara et al., 2018; Kaushik and Lipton, 2018; Min et al., 2019). Therefore, current MR"
2021.findings-acl.85,D18-1259,0,0.0227192,"ns earlier than challenging questions, and the high proportions of shortcut questions in training sets hinder models from exploring the sophisticated reasoning skills in the later stage of training. 1 Paraphrasing P: ... Begun as a claims to be the oldest continuous collegiate publication in the United States … Comprehension Challenge Shortcut Figure 1: An illustration of shortcuts in Machine Reading Comprehension. P is an excerpt of the original passage. different benchmarks. These benchmarks are designed to address challenging features, such as evidence checking in multi-document inference (Yang et al., 2018), co-reference resolution (Dasigi et al., 2019), dialog understanding (Reddy et al., 2019), symbolic reasoning (Dua et al., 2019), and so on. The task of machine reading comprehension (MRC) aims at evaluating whether a model can understand natural language texts by answering a series of questions. Recently, MRC research has seen considerable progress in terms of model performance, and many models are reported to approach or even outperform human-level performance on Corresponding author. Answer the Scholastic magazine is issued twice monthly and Introduction ∗ one-page journal in September 187"
2021.findings-emnlp.111,D18-1431,0,0.0170367,"ate between the output provided by the student and the teacher are assumed to be easier ones. Starting from easier samples, the model progressively strengthens its relation between the teacher and student models. In the rest of this paper, we give detailed descriptions of the proposed approach. 4 4.1 Model Data-Level Curriculum Following existing studies (Platanios et al., 2019; Kocmi and Bojar, 2017) that the model should be trained from easy samples to hard ones, we schedule the curriculum based on three intuitive notions of difficulty: response length (Serban et al., 2017; S. et al., 2017; Baheti et al., 2018), word and utterance entropy (Serban et al., 2017), coherence (Xu et al., 2018). These features compensate each other by capturing the information in a sentence pairs from different aspects. All these features are from previous research and here we integrate them together: we first use the method from these papers to compute the scores for individual sentences; then normalize the scores; finally add all these scores together as a total score. We rank all sentence pairs according to their scores, and we break down the dataset Do into N subsets, in which those examples with similar complexity ar"
2021.findings-emnlp.111,D14-1179,0,0.0196611,"Missing"
2021.findings-emnlp.111,P19-1370,1,0.901031,"Missing"
2021.findings-emnlp.111,kocmi-bojar-2017-curriculum,0,0.0226888,"um schedules to gradually transfer knowledge from the the teacher to student, which controls the difficulty of soften labels that are distilled from teacher to students. The samples that discriminator cannot differentiate between the output provided by the student and the teacher are assumed to be easier ones. Starting from easier samples, the model progressively strengthens its relation between the teacher and student models. In the rest of this paper, we give detailed descriptions of the proposed approach. 4 4.1 Model Data-Level Curriculum Following existing studies (Platanios et al., 2019; Kocmi and Bojar, 2017) that the model should be trained from easy samples to hard ones, we schedule the curriculum based on three intuitive notions of difficulty: response length (Serban et al., 2017; S. et al., 2017; Baheti et al., 2018), word and utterance entropy (Serban et al., 2017), coherence (Xu et al., 2018). These features compensate each other by capturing the information in a sentence pairs from different aspects. All these features are from previous research and here we integrate them together: we first use the method from these papers to compute the scores for individual sentences; then normalize the s"
2021.findings-emnlp.111,I17-1099,0,0.0156343,"= 0, if Scorei ≥ λ, (7) (H (Sθ (xi ), yi ) + i=1 (8) vi H (Tθ (xi ), Sθ (xi ))). In conclusion, our hierarchical curriculum learning algorithm framework is described in Algorithm 1. 5 5.1 in Equation (3). The conventional self-paced learning selects the samples based on the loss value. While we replace it with our difficulty score described in the last section. Then we use self-paced learning to estimate V by the optimization as: n X Experiment Datasets We conduct experiments on two English conversation datasets, which have been widely used in open-domain dialogue generation. (1) DailyDialog (Li et al., 2017): it is a collection of real-world daily conversations for an English learner in daily life. It is a multi-turn dataset, and we treat each turn as a single-turn training pair in this work. (2) PersonaChat (Zhang et al., 2018): it is collected by two crowdsourced workers chit-chatting with each other, conditioned on the assigned personas. In our experiments, we only use the conversation text and process it as DailyDialog. 5.2 Evaluation Methods Automatic Evaluation Method It is challenging to assess the quality of the generated responses. In this paper, we adopt several evaluation methods to me"
2021.findings-emnlp.111,W17-5546,0,0.0519816,"Missing"
2021.findings-emnlp.111,D19-1187,0,0.0129279,"d more value and history of the discriminator into account, researchers have focused on generation-based conbased on the following two intuitions. (1) The versation system. A popular framework for diadiscriminator defines an objective of progressive logue generation is using extra information such as difficulty (Doan et al., 2019), if the discriminator conversation topics(Xing et al., 2017) , persona procan successfully distinguish the output, then it is a file (Song et al., 2019), user emotions (Zhou et al., hard case, and vice versa (Doan et al., 2019). (2) 2018), or out-sourcing knowledge (Liu et al., 2019) The model evolves during training and therefore is introduced to benefit the dialogue model with additional evaluation pass to measure the change more diverse response generation (Serban et al., in a performance is needed (Matiisen et al., 2020). 2017; Zhao et al., 2017; Gu et al., 2019). Latent In this paper we consider the change in the dis- variables also benefit the model with more diverse criminator. If the change is negative, this must response generations (Zhao et al., 2017). This pamean that the sample is difficult to train. Then per improve the dialogue model from a different based o"
2021.findings-emnlp.111,D16-1139,0,0.0197479,"combination of curriculum learning methods and knowledge distilOur study is also related to the knowledge distillation for efficient dialogue generation models. (2) lation method (Hinton et al., 2015), which employs 1285 a teacher model and tries to minimize the KL divergence between teacher distribution and student distribution. In (Romero et al., 2015), the student network is trained not only using the soft targets, but also using hints from the intermediate layers. Knowledge distillation was first introduced for classification tasks as a way to compress large networks into smaller models. Kim and Rush (2016) extend this to neural machine translation, and then (Zhang et al., 2020) has proposed further applications of dialogue generation task. However, these papers do not consider the order of the learning schedule. In a sense, our method is different from theirs because we borrow the idea of curriculum learning for knowledge distillation. 2.3 Curriculum Learning in NLP Inspired by the human learning process, curriculum learning (Bengio et al., 2009) is proposed as a machine learning strategy by feeding training instances to the model from easy to hard. It has been applied to many NLP tasks. To nam"
2021.findings-emnlp.111,C16-1316,0,0.0201707,"ethods Automatic Evaluation Method It is challenging to assess the quality of the generated responses. In this paper, we adopt several evaluation methods to measure different aspects of our results: BLEU (Papineni et al., 2002): it is used as a reward to evaluate dialog systems by measuring word overlap between the generated reply and the ground truth for the final evaluation. We compute BLEU scores for n &lt;= 4 using smoothing techniques 1 . Entropy-based metrics : it includes word and sentence entropy as (Serban et al., 2017), which suggests the diversity of responses. Length: as proposed by (Mou et al., 2016), the length of an utterance is an objective, surface metric that reflects the substance of a generated reply. where λ is used to control the learning pace of if self-paced learning. In our paper, suppose T is the total number of training steps and t is the current training step. During training, to select the training instances with desired difficulty, we resort to a pre-defined pacing function λ = f (t) to control how fast the output will be distilled from teacher to student. We define three different pacing functions named as 1 linear-scheduler, log-scheduler and exp-scheduler https://www.n"
2021.findings-emnlp.111,P02-1040,0,0.109319,"rsations for an English learner in daily life. It is a multi-turn dataset, and we treat each turn as a single-turn training pair in this work. (2) PersonaChat (Zhang et al., 2018): it is collected by two crowdsourced workers chit-chatting with each other, conditioned on the assigned personas. In our experiments, we only use the conversation text and process it as DailyDialog. 5.2 Evaluation Methods Automatic Evaluation Method It is challenging to assess the quality of the generated responses. In this paper, we adopt several evaluation methods to measure different aspects of our results: BLEU (Papineni et al., 2002): it is used as a reward to evaluate dialog systems by measuring word overlap between the generated reply and the ground truth for the final evaluation. We compute BLEU scores for n &lt;= 4 using smoothing techniques 1 . Entropy-based metrics : it includes word and sentence entropy as (Serban et al., 2017), which suggests the diversity of responses. Length: as proposed by (Mou et al., 2016), the length of an utterance is an objective, surface metric that reflects the substance of a generated reply. where λ is used to control the learning pace of if self-paced learning. In our paper, suppose T is"
2021.findings-emnlp.111,N19-1119,0,0.0237574,"evel, we design curriculum schedules to gradually transfer knowledge from the the teacher to student, which controls the difficulty of soften labels that are distilled from teacher to students. The samples that discriminator cannot differentiate between the output provided by the student and the teacher are assumed to be easier ones. Starting from easier samples, the model progressively strengthens its relation between the teacher and student models. In the rest of this paper, we give detailed descriptions of the proposed approach. 4 4.1 Model Data-Level Curriculum Following existing studies (Platanios et al., 2019; Kocmi and Bojar, 2017) that the model should be trained from easy samples to hard ones, we schedule the curriculum based on three intuitive notions of difficulty: response length (Serban et al., 2017; S. et al., 2017; Baheti et al., 2018), word and utterance entropy (Serban et al., 2017), coherence (Xu et al., 2018). These features compensate each other by capturing the information in a sentence pairs from different aspects. All these features are from previous research and here we integrate them together: we first use the method from these papers to compute the scores for individual sentenc"
2021.findings-emnlp.111,D11-1054,0,0.101257,"Missing"
2021.findings-emnlp.111,P16-1043,0,0.0234984,"is to neural machine translation, and then (Zhang et al., 2020) has proposed further applications of dialogue generation task. However, these papers do not consider the order of the learning schedule. In a sense, our method is different from theirs because we borrow the idea of curriculum learning for knowledge distillation. 2.3 Curriculum Learning in NLP Inspired by the human learning process, curriculum learning (Bengio et al., 2009) is proposed as a machine learning strategy by feeding training instances to the model from easy to hard. It has been applied to many NLP tasks. To name a few, (Sachan and Xing, 2016) propose and study other heuristics that define a measure of easiness and learn the curriculum by selecting samples using this measure. More recently, (Wang et al., 2019) learns a multi-Domain curriculum for neural machine translation. Xu et al. (2020) uses curriculum Learning to distinguish easy examples from difficult ones for natural language understanding by reviewing the trainset in a crossed way. Our paper is quite different from theirs because we arrange a hierarchical curriculum based on the above two aspects (data and model) for the distillation model. 3 Problem Formulation The overal"
2021.findings-emnlp.111,P15-1152,0,0.0484829,"Missing"
2021.findings-emnlp.111,2020.emnlp-main.80,0,0.0373735,"s can yield significant perforof difficulty including the specificity and repetimance boost for student models. Hence, in this tiveness of the response, the relevance between paper, we introduce a combination of curricuthe query and the response, etc. Also, Wan et al. lum learning and knowledge distillation for ef(2020) resolves this problem by introducing selfficient dialogue generation models, where curpaced learning (Kumar et al., 2010), which is a riculum learning can help knowledge distillaspecial kind of curriculum learning (Eppe et al., tion from data and model aspects. To start 2019). Wan et al. (2020) measures the level of with, from the data aspect, we cluster the training cases according to their complexity, which confidence on each training example, where an is calculated by various types of features such easy sample is actually one of high confidence by as sentence length and coherence between dithe current trained model. Both curriculum learnalog pairs. Furthermore, we employ an advering and self-paced learning suggest that samples sarial training strategy to identify the complexshould be selected in a meaningful order for trainity of cases from model level. The intuition ing. The dif"
2021.findings-emnlp.111,P19-1123,0,0.0293509,"Missing"
2021.findings-emnlp.111,2020.acl-main.542,0,0.426232,") proposes an adaptive multi-curricula learntate the dialogue generation task. Meanwhile, ing framework to train the dialogue model with knowledge distillation, a knowledge transforeasy-to-complex dataset based on various concepts mation methodology among teachers and students networks can yield significant perforof difficulty including the specificity and repetimance boost for student models. Hence, in this tiveness of the response, the relevance between paper, we introduce a combination of curricuthe query and the response, etc. Also, Wan et al. lum learning and knowledge distillation for ef(2020) resolves this problem by introducing selfficient dialogue generation models, where curpaced learning (Kumar et al., 2010), which is a riculum learning can help knowledge distillaspecial kind of curriculum learning (Eppe et al., tion from data and model aspects. To start 2019). Wan et al. (2020) measures the level of with, from the data aspect, we cluster the training cases according to their complexity, which confidence on each training example, where an is calculated by various types of features such easy sample is actually one of high confidence by as sentence length and coherence between d"
2021.findings-emnlp.111,D18-1432,0,0.0132236,"sier ones. Starting from easier samples, the model progressively strengthens its relation between the teacher and student models. In the rest of this paper, we give detailed descriptions of the proposed approach. 4 4.1 Model Data-Level Curriculum Following existing studies (Platanios et al., 2019; Kocmi and Bojar, 2017) that the model should be trained from easy samples to hard ones, we schedule the curriculum based on three intuitive notions of difficulty: response length (Serban et al., 2017; S. et al., 2017; Baheti et al., 2018), word and utterance entropy (Serban et al., 2017), coherence (Xu et al., 2018). These features compensate each other by capturing the information in a sentence pairs from different aspects. All these features are from previous research and here we integrate them together: we first use the method from these papers to compute the scores for individual sentences; then normalize the scores; finally add all these scores together as a total score. We rank all sentence pairs according to their scores, and we break down the dataset Do into N subsets, in which those examples with similar complexity are categorized into the same subset. 4.2 Output Knowledge Distillation Knowledge"
2021.findings-emnlp.111,D17-1065,0,0.0158443,"ong with the enormous prosperity of social can spread uncertainty over multiple outputs when media on the Internet, there is a resurgent inter- it is not confident of its prediction. As a conseest in developing open domain dialogue systems. quence, student models can yield significant perforHowever, the complexity of conversations crawled mance boost under the guidance of a teacher. Since from the Internet may vary significantly. Sachan the knowledge from the teacher to student also has and Xing (2016); Cai et al. (2020); Lison and different difficulty degrees, it is intuitive to apply Bibauw (2017). To adapt to this phenomenon, curriculum learning during this distillation process. some prior works (Cai et al., 2020; Sachan and To our best knowledge, very little is known about ∗ Corresponding authors: JunFei Liu and Dongyan Zhao. how curriculum learning and knowledge distilla1284 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 1284–1295 November 7–11, 2021. ©2021 Association for Computational Linguistics Figure 1: Architecture of our model. tion work together. Hence, in this work, we propose We arrange a hierarchical curriculum based on the a dialogue generat"
2021.findings-emnlp.111,2020.emnlp-main.277,0,0.0344695,"is also related to the knowledge distillation for efficient dialogue generation models. (2) lation method (Hinton et al., 2015), which employs 1285 a teacher model and tries to minimize the KL divergence between teacher distribution and student distribution. In (Romero et al., 2015), the student network is trained not only using the soft targets, but also using hints from the intermediate layers. Knowledge distillation was first introduced for classification tasks as a way to compress large networks into smaller models. Kim and Rush (2016) extend this to neural machine translation, and then (Zhang et al., 2020) has proposed further applications of dialogue generation task. However, these papers do not consider the order of the learning schedule. In a sense, our method is different from theirs because we borrow the idea of curriculum learning for knowledge distillation. 2.3 Curriculum Learning in NLP Inspired by the human learning process, curriculum learning (Bengio et al., 2009) is proposed as a machine learning strategy by feeding training instances to the model from easy to hard. It has been applied to many NLP tasks. To name a few, (Sachan and Xing, 2016) propose and study other heuristics that"
2021.findings-emnlp.111,P18-1205,0,0.0262101,"self-paced learning selects the samples based on the loss value. While we replace it with our difficulty score described in the last section. Then we use self-paced learning to estimate V by the optimization as: n X Experiment Datasets We conduct experiments on two English conversation datasets, which have been widely used in open-domain dialogue generation. (1) DailyDialog (Li et al., 2017): it is a collection of real-world daily conversations for an English learner in daily life. It is a multi-turn dataset, and we treat each turn as a single-turn training pair in this work. (2) PersonaChat (Zhang et al., 2018): it is collected by two crowdsourced workers chit-chatting with each other, conditioned on the assigned personas. In our experiments, we only use the conversation text and process it as DailyDialog. 5.2 Evaluation Methods Automatic Evaluation Method It is challenging to assess the quality of the generated responses. In this paper, we adopt several evaluation methods to measure different aspects of our results: BLEU (Papineni et al., 2002): it is used as a reward to evaluate dialog systems by measuring word overlap between the generated reply and the ground truth for the final evaluation. We c"
2021.findings-emnlp.111,P17-1061,0,0.0246484,"sing extra information such as difficulty (Doan et al., 2019), if the discriminator conversation topics(Xing et al., 2017) , persona procan successfully distinguish the output, then it is a file (Song et al., 2019), user emotions (Zhou et al., hard case, and vice versa (Doan et al., 2019). (2) 2018), or out-sourcing knowledge (Liu et al., 2019) The model evolves during training and therefore is introduced to benefit the dialogue model with additional evaluation pass to measure the change more diverse response generation (Serban et al., in a performance is needed (Matiisen et al., 2020). 2017; Zhao et al., 2017; Gu et al., 2019). Latent In this paper we consider the change in the dis- variables also benefit the model with more diverse criminator. If the change is negative, this must response generations (Zhao et al., 2017). This pamean that the sample is difficult to train. Then per improve the dialogue model from a different based on these model-level difficulty scores, we angle that we make an empirical study on the comfurther transfer the knowledge from teacher to stu- bination of curriculum learning methods and knowldent network gradually by incorporating self-paced edge distillation. learning m"
2021.findings-emnlp.255,N19-1423,0,0.0152983,", and the average dependency tree depth of VGaokao is 1.2 times larger than that of C3 . Longer sentences and more complicated syntactic structures usually exhibit rich linguistic phenomena, thus requiring models to learn more sophisticated language understanding skills. Lastly, the passages in VGaokao, which contain 1,159 Chinese characters on average, are approximately 3 times longer than that of RACE (Lai et al., 2017) and 10 times longer than that of C3 (Sun et al., 2020). The length of most passages even exceeds the maximum input length of general pre-trained language models such as BERT(Devlin et al., 2019). To exploit the long passages, models may need to take discourse structures into consideration so as to better integrate multiple evidence sentences. As shown in the sample articles and questions from VGaokao and C3 in Table 1, the example article from VGaokao involves domain-specific terminologies such as testosterone and estrogen. The sentences in the VGaokao example are longer and involves more complicated sentence structures such as compound sentences. Besides, the options in VGaokao seem to be more confusing: Option C and D both discuss the subtle relationship between the brains and hand"
2021.findings-emnlp.255,E17-1011,0,0.0553602,"Missing"
2021.findings-emnlp.255,2020.findings-emnlp.314,0,0.0359953,"uire two evidence sentences, we set the maximum number of iterations to 2. Considering that maintaining all evidence chains during iterative extraction has exponential complexity to the steps, we use beam search, where only top 2 evidence sentences remain in each step. We use Sentence-BERT (Reimers and Gurevych, 2019, 2020) to measure the relevance between the query and the evidence chains in the adaptive integrator. For the pairwise option competition, we use Chinese RoBERTa-wwm-ext-Large (Cui et al., 2019) with Transformers toolkit (Wolf et al., 2020). We first fine-tune our model on OCNLI (Hu et al., 2020), a Chinese natural language inference dataset before fine-tuning on VGaokao, which has 8 epochs, with maximum input length 256, batch size 64, and learning rate 2e-5. 4 Experiments We conduct experiments on our proposed VGaokao dataset and compare our ExtractIntegrate-Compete approach with several baselines. RoBERTa-Large-Chunk (Liu et al., 2019) is an end-to-end method without explicit evidence retrieval. This model splits the long passages into fix-length chunks of 200 tokens. Candidate answers are obtained from each chunk using existing MRC models, which are further aggregated over all chu"
2021.findings-emnlp.255,D19-1597,0,0.0213101,"Missing"
2021.findings-emnlp.255,Q19-1026,0,0.011953,"a novel query updating mechanism. Our hinge loss based competition component can push the model to capture fine-grained differences among different choices. 3) Experiments show that our approach outperforms a variety of baselines in both evidence retrieval F1 and QA accuracy on VGaokao while showing the merits of efficiency and explainability. 2 VGaokao: Verification Style Reading Comprehension Dataset Standardized language tests have been considered as a test-bed to harvest machine reading comprehension datasets. While most existing efforts focus on SQuAD-like QA datasets (Yang et al., 2018; Kwiatkowski et al., 2019), or cloze style questions (Zhang et al., 2018; Zheng et al., 2019), few efforts are made to verification style questions. In the Chinese Language test of Gaokao, approximately half of the reading comprehension questions instruct students to select a statement (i.e., an option from four choices) that is the most consistent or contradicting with the given passage. These questions are designed to evaluate students’ ability in extracting and integrating information from long passages, and analyzing certain linguistic phenomena or semantic relations among several similar sentences3 . According to"
2021.findings-emnlp.255,D17-1082,0,0.368168,", or verify a given statement according to the passage. For example, in the Chinese Language tests of Gaokao (also known as China National College Entrance Examination), approximately half of the reading comprehension questions are in a verification style. As shown in Table 1 (bottom), students are expected to read a passage, and then select from four choices (A~D) the best statement that is the most consistent with the passage, or sometimes, contracts the most to the passage. While the question answering styled tasks have been intensively studied in the NLP community (Rajpurkar et al., 2016; Lai et al., 2017; Yang et al., 2018; Sun et al., 2020), the verification styled ∗ 1 Figure 1: An example of iterative evidence extraction. The darker tokens in bold are more important for the updated query. MRC task actually receives much less attention. Here, as indicated in the Gaokao Instructions2 , the abilities of gathering multiple evidence pieces from long articles, distilling supportive evidence, and making decisions accordingly by capturing the subtle difference among similar text pieces (i.e., choices), are necessary skills for Chinese Language learning. This type of questions actually provides an i"
2021.findings-emnlp.255,2021.ccl-1.108,0,0.0394672,"Missing"
2021.findings-emnlp.255,2020.emnlp-main.713,0,0.0294949,"Missing"
2021.findings-emnlp.255,D19-1261,0,0.0151235,"hod for solving the long article challenge in VGaokao. Iterative evidence extraction can be seen as a sort of question decomposition method, a technique widely used in QA tasks with complex questions (Talmor and Berant, 2018; Perez et al., 2020). However, in VGaokao, the queries may interweave by implicit semantic relationship, so that models could not explicitly separate the queries into independent sub-queries. We thus adopt an iteative extractor with an adaptive integrator to decompose queries in an implicit way. Another stream of works adopt an iterative framework by updating the queries. Qi et al. (2019) iteratively generate new queries by selecting a span from the question and retrieved evidence while Xu et al. (2019); Xiong et al. (2021); Zhao et al. (2021) directly append retrieved evidence to the query. Compared with these works, we introduce two novel query updating techniques, hard masking and soft masking, together with an evidence integration module to avoid too much overlap between evidence and to dynamically determine the number of required evidence sentences. 6 Conclusion In this paper, we present a novel verification style reading comprehension dataset named VGaokao from the Chine"
2021.findings-emnlp.255,D16-1264,0,0.0515677,"er a series of questions, or verify a given statement according to the passage. For example, in the Chinese Language tests of Gaokao (also known as China National College Entrance Examination), approximately half of the reading comprehension questions are in a verification style. As shown in Table 1 (bottom), students are expected to read a passage, and then select from four choices (A~D) the best statement that is the most consistent with the passage, or sometimes, contracts the most to the passage. While the question answering styled tasks have been intensively studied in the NLP community (Rajpurkar et al., 2016; Lai et al., 2017; Yang et al., 2018; Sun et al., 2020), the verification styled ∗ 1 Figure 1: An example of iterative evidence extraction. The darker tokens in bold are more important for the updated query. MRC task actually receives much less attention. Here, as indicated in the Gaokao Instructions2 , the abilities of gathering multiple evidence pieces from long articles, distilling supportive evidence, and making decisions accordingly by capturing the subtle difference among similar text pieces (i.e., choices), are necessary skills for Chinese Language learning. This type of questions actu"
2021.findings-emnlp.255,D19-1410,0,0.0332241,"Missing"
2021.findings-emnlp.255,2020.emnlp-main.365,0,0.0223656,"Missing"
2021.findings-emnlp.255,Q19-1014,0,0.0526079,"Missing"
2021.findings-emnlp.255,2020.tacl-1.10,0,0.113313,"ng to the passage. For example, in the Chinese Language tests of Gaokao (also known as China National College Entrance Examination), approximately half of the reading comprehension questions are in a verification style. As shown in Table 1 (bottom), students are expected to read a passage, and then select from four choices (A~D) the best statement that is the most consistent with the passage, or sometimes, contracts the most to the passage. While the question answering styled tasks have been intensively studied in the NLP community (Rajpurkar et al., 2016; Lai et al., 2017; Yang et al., 2018; Sun et al., 2020), the verification styled ∗ 1 Figure 1: An example of iterative evidence extraction. The darker tokens in bold are more important for the updated query. MRC task actually receives much less attention. Here, as indicated in the Gaokao Instructions2 , the abilities of gathering multiple evidence pieces from long articles, distilling supportive evidence, and making decisions accordingly by capturing the subtle difference among similar text pieces (i.e., choices), are necessary skills for Chinese Language learning. This type of questions actually provides an ideal test-bed for natural language und"
2021.findings-emnlp.255,N18-1059,0,0.0324607,"Missing"
2021.findings-emnlp.255,N18-1074,0,0.0583002,"Missing"
2021.findings-emnlp.255,2020.emnlp-demos.6,0,0.0402704,"Missing"
2021.findings-emnlp.255,N19-1301,1,0.833831,"n decomposition method, a technique widely used in QA tasks with complex questions (Talmor and Berant, 2018; Perez et al., 2020). However, in VGaokao, the queries may interweave by implicit semantic relationship, so that models could not explicitly separate the queries into independent sub-queries. We thus adopt an iteative extractor with an adaptive integrator to decompose queries in an implicit way. Another stream of works adopt an iterative framework by updating the queries. Qi et al. (2019) iteratively generate new queries by selecting a span from the question and retrieved evidence while Xu et al. (2019); Xiong et al. (2021); Zhao et al. (2021) directly append retrieved evidence to the query. Compared with these works, we introduce two novel query updating techniques, hard masking and soft masking, together with an evidence integration module to avoid too much overlap between evidence and to dynamically determine the number of required evidence sentences. 6 Conclusion In this paper, we present a novel verification style reading comprehension dataset named VGaokao from the Chinese Language tests of Gaokao for Chinese native speakers, which embed multiple advanced language understanding skills."
2021.findings-emnlp.255,D18-1259,0,0.104276,"n statement according to the passage. For example, in the Chinese Language tests of Gaokao (also known as China National College Entrance Examination), approximately half of the reading comprehension questions are in a verification style. As shown in Table 1 (bottom), students are expected to read a passage, and then select from four choices (A~D) the best statement that is the most consistent with the passage, or sometimes, contracts the most to the passage. While the question answering styled tasks have been intensively studied in the NLP community (Rajpurkar et al., 2016; Lai et al., 2017; Yang et al., 2018; Sun et al., 2020), the verification styled ∗ 1 Figure 1: An example of iterative evidence extraction. The darker tokens in bold are more important for the updated query. MRC task actually receives much less attention. Here, as indicated in the Gaokao Instructions2 , the abilities of gathering multiple evidence pieces from long articles, distilling supportive evidence, and making decisions accordingly by capturing the subtle difference among similar text pieces (i.e., choices), are necessary skills for Chinese Language learning. This type of questions actually provides an ideal test-bed for n"
2021.findings-emnlp.255,2021.naacl-main.368,0,0.0861752,"into fix-length chunks of 200 tokens. Candidate answers are obtained from each chunk using existing MRC models, which are further aggregated over all chunks. We use the pre-trained Chinese RoBERTawwm-ext-Large (Cui et al., 2019). BM25 (Robertson and Zaragoza, 2009) is a bagof-words retrieval method, which uses sparse features to retrieve evidence sentences. We use the version implemented by Pyserini6 . Sent-BERT (Reimers and Gurevych, 2019) uses BERT to obtain contextualized dense representations for the texts and retrieve evidence sentences via cosine similarity. BeamDR (Xiong et al., 2021; Zhao et al., 2021) is an iterative evidence selection technique with beam search and dense retrieval. It updates the query by appending the newly-extracted evidence in each iteration. The BM25, Sent-BERT, and BeamDR are evidence extraction methods, which are combined with our proposed pair-wise option competition method to obtain the final question-level results. For BM25 and Sent-BERT, which cannot address the problem of multiple evidence, we report their 6 https://github.com/fxsjy/jieba 2981 https://github.com/castorini/pyserini Evidence Metrics QA Metrics P R F1 Acc. — — — 41.9 BM25 Top 1 BM25 Top 2 Sent-BER"
2021.findings-emnlp.255,P19-1075,0,0.0126622,"ent can push the model to capture fine-grained differences among different choices. 3) Experiments show that our approach outperforms a variety of baselines in both evidence retrieval F1 and QA accuracy on VGaokao while showing the merits of efficiency and explainability. 2 VGaokao: Verification Style Reading Comprehension Dataset Standardized language tests have been considered as a test-bed to harvest machine reading comprehension datasets. While most existing efforts focus on SQuAD-like QA datasets (Yang et al., 2018; Kwiatkowski et al., 2019), or cloze style questions (Zhang et al., 2018; Zheng et al., 2019), few efforts are made to verification style questions. In the Chinese Language test of Gaokao, approximately half of the reading comprehension questions instruct students to select a statement (i.e., an option from four choices) that is the most consistent or contradicting with the given passage. These questions are designed to evaluate students’ ability in extracting and integrating information from long passages, and analyzing certain linguistic phenomena or semantic relations among several similar sentences3 . According to the target language skills, we call these questions verification st"
2021.naacl-main.134,E14-1028,0,0.0519677,"Missing"
2021.naacl-main.134,W17-3531,0,0.044374,"Missing"
2021.naacl-main.134,I11-1144,0,0.036573,"words. der” information takes effects in natural language learning for neural models. On the basis of the pooling-based and memory-based approaches, we introduce the self-attention to encode the semantic dependencies between input words without considering order information, so as to enrich individual words with contextual information from different semantic aspects. We systematically compare the ability of different neural models to organize sentences from a bag of words in terms of three typical scenarios shown in Table 1. The contributions of this paper are summarized as follows: sentence (He and Liang, 2011). In dialogue systems, we need systems that are enabled to converse smoothly with people that have troubles in ordering words, such as children, language learners, and • We present an empirical study to investigate speech impaired. In image caption, the caption can the ability of neural models to organize senbe organized with a bag of attribute words extracted tences from a bag of words. from the image (Fang et al., 2015). Moreover, such a model can help non-native speakers of English to • We introduce a bag-to-sentence transformawrite a sentence just from keywords. tion model based on self-at"
2021.naacl-main.134,W04-1013,0,0.0539533,"rds in this paper. So we compare all methods under standard beam search method (with a beam size of 5) in our experiment, to highlight the differences among different encoders. N-GRAM∗ RNNLM∗ Pooling LSTM Memory AttP AttM BLEU 0.2330 0.2450 0.3118 0.3140 0.3328 0.3469 0.3489 ROUGE-L 0.5916 0.5875 0.6053 0.6169 0.6194 WAcc 0.4105 0.3873 0.4089 0.4297 0.4304 PMR 0.0863 0.0850 0.0941 0.1013 0.1059 Table 3: Results of word ordering task on PTB datasets (beam size = 5), * denotes the results reported in (Hasler et al., 2017). cal units (e.g., unigram, bigram) with the reference sentences. ROUGE-L (Lin, 2004) measures the longest common subsequence (LCS) between the reference sentence and the generated sentence. WAcc (Word Accuracy) is the negative word error rate (WER) (Mangu et al., 2000). It measures the edit distance between the generated sentence and the reference sentence (higher is better). Besides, we also conduct human evaluations to further analyze our generated results and explore the detail sort of wrong cases. 5.4 Overall Results Table 2 illustrates the performance of all models for three scenarios on the Wikipedia dataset. Firstly, we can find that Pooling shows the worse performance"
2021.naacl-main.134,D15-1043,0,0.0161204,"o, and employed pooling to obtain aggregated features. On this basis of the transformer architecture, Lee et al. (2019) presented an Set Transformer designed to model interactions among elements in the input set. Without considering missing words or noisy words, our task devolves into word ordering problem, which is a fundamental task in natural language generation. Previous, researchers usually employed N-gram based language models (De Gispert et al., 2014; Schmaltz et al., 2016), syntacticbased language models (Zhang and Clark, 2011; Liu et al., 2015) or combined models (Zhang et al., 2012; Liu and Zhang, 2015) to solve this problem. More recently, Hasler et al. (2017) proposed a bag-to-sequence model, where the decoder RNN directly attended to the word embeddings. However, all these methods aim at finding the best permutation of a bag of words based on language models, and do not consider how to encode a bag of words. 3 Problem Formulation Given a bag of words X = {x1 , x2 , · · · , xm } which consists of m tokens, our model will generate a sentence Y = {y1 , y2 , · · · , yn }, where n is the length of target sentence. In the normal scenario, the words of X come from a disordered sentence and are t"
2021.naacl-main.134,N15-1012,0,0.0178143,"n to model interactions between 1683 the objects in a video, and employed pooling to obtain aggregated features. On this basis of the transformer architecture, Lee et al. (2019) presented an Set Transformer designed to model interactions among elements in the input set. Without considering missing words or noisy words, our task devolves into word ordering problem, which is a fundamental task in natural language generation. Previous, researchers usually employed N-gram based language models (De Gispert et al., 2014; Schmaltz et al., 2016), syntacticbased language models (Zhang and Clark, 2011; Liu et al., 2015) or combined models (Zhang et al., 2012; Liu and Zhang, 2015) to solve this problem. More recently, Hasler et al. (2017) proposed a bag-to-sequence model, where the decoder RNN directly attended to the word embeddings. However, all these methods aim at finding the best permutation of a bag of words based on language models, and do not consider how to encode a bag of words. 3 Problem Formulation Given a bag of words X = {x1 , x2 , · · · , xm } which consists of m tokens, our model will generate a sentence Y = {y1 , y2 , · · · , yn }, where n is the length of target sentence. In the normal scena"
2021.naacl-main.134,J93-2004,0,0.0812258,"0.5808 0.5563 0.4369 0.6063 0.5789 0.4520 0.6613 0.6367 0.4700 0.6697 0.6461 0.4702 Table 2: Results on the test sets of three scenarios for Wikipedia dataset. We randomly generate noisy words with the number between 1 and half length of the sentence from the vocabulary for each sentence as the input of the the noise scenario. For the missing scenario, random words with number between 1 and half length of the sentence are removed from each sentence. It is worth noting that we randomly shuttle input bags with three different seeds and report the mean score of each metrics for LSTM. data (PTB) (Marcus et al., 1993), which is a widelyused dataset for word ordering task (Schmaltz et al., 2016; Hasler et al., 2017). To facilitate fair comparisons, we use the data preprocessed by (Schmaltz et al., 2016), which consists of 39, 832 training sentences, 1, 700 validation sentences and 2, 416 test sentences. 5.2 Implementation Details For all models, we set the dimension of word embedding as 128. In the LSTM-based encoder, the dimension of hidden unit is 256. In the self-attentionbased encoder, we set the number of head in Equation (6) as 8 and the hidden size of feed-forward layer in Equation (7) as 256. All pa"
2021.naacl-main.134,P02-1040,0,0.109582,"Missing"
2021.naacl-main.134,D16-1255,0,0.0833071,"al. (2018) utiIn this paper, we aim to investigate how “or- lized self-attention to model interactions between 1683 the objects in a video, and employed pooling to obtain aggregated features. On this basis of the transformer architecture, Lee et al. (2019) presented an Set Transformer designed to model interactions among elements in the input set. Without considering missing words or noisy words, our task devolves into word ordering problem, which is a fundamental task in natural language generation. Previous, researchers usually employed N-gram based language models (De Gispert et al., 2014; Schmaltz et al., 2016), syntacticbased language models (Zhang and Clark, 2011; Liu et al., 2015) or combined models (Zhang et al., 2012; Liu and Zhang, 2015) to solve this problem. More recently, Hasler et al. (2017) proposed a bag-to-sequence model, where the decoder RNN directly attended to the word embeddings. However, all these methods aim at finding the best permutation of a bag of words based on language models, and do not consider how to encode a bag of words. 3 Problem Formulation Given a bag of words X = {x1 , x2 , · · · , xm } which consists of m tokens, our model will generate a sentence Y = {y1 , y2 , ·"
2021.naacl-main.134,P17-1099,0,0.0479644,"e scenario by randomly introducing some To highlight the differences among different en- noisy words to the source bag, and construct the coders, we utilize the same decoder for different training data for the missing scenario by randomly encoders. removing some words from the source bag. Since the target Y corresponds to a sequence, We also compare the normal scenario of and has significant vocabulary overlap with the our model on The English Penn Treebank input bags of words, we blend a pointer-based de3 The corpus removes all links and other irrelevant material coder (Vinyals et al., 2015; See et al., 2017), which 2 It is worth noting that the current memory is composed of the word representations output by self-attention layer (e.g., navigation text, etc), and contains about one billion words, over 2 million documents. 4 http://www.nltk.org/api/nltk.tokenize.html 1685 Pooling LSTM Memory AttP AttM Normal 0.4656 0.4736 0.5030 0.5740 0.5886 BLEU Noise 0.4382 0.4327 0.4537 0.5372 0.5433 Missing 0.2636 0.2538 0.2664 0.2882 0.2914 Normal 0.6917 0.7311 0.7485 0.7860 0.7925 ROUGE-L Noise Missing 0.6587 0.5470 0.6761 0.5453 0.6939 0.5607 0.7396 0.5722 0.7465 0.5738 Perfect Matching Rate (PMR) Normal No"
2021.naacl-main.134,P17-1018,0,0.0390888,"Missing"
2021.naacl-main.134,E12-1075,0,0.0284739,"he objects in a video, and employed pooling to obtain aggregated features. On this basis of the transformer architecture, Lee et al. (2019) presented an Set Transformer designed to model interactions among elements in the input set. Without considering missing words or noisy words, our task devolves into word ordering problem, which is a fundamental task in natural language generation. Previous, researchers usually employed N-gram based language models (De Gispert et al., 2014; Schmaltz et al., 2016), syntacticbased language models (Zhang and Clark, 2011; Liu et al., 2015) or combined models (Zhang et al., 2012; Liu and Zhang, 2015) to solve this problem. More recently, Hasler et al. (2017) proposed a bag-to-sequence model, where the decoder RNN directly attended to the word embeddings. However, all these methods aim at finding the best permutation of a bag of words based on language models, and do not consider how to encode a bag of words. 3 Problem Formulation Given a bag of words X = {x1 , x2 , · · · , xm } which consists of m tokens, our model will generate a sentence Y = {y1 , y2 , · · · , yn }, where n is the length of target sentence. In the normal scenario, the words of X come from a disorde"
2021.naacl-main.134,D11-1106,0,0.0375037,"or- lized self-attention to model interactions between 1683 the objects in a video, and employed pooling to obtain aggregated features. On this basis of the transformer architecture, Lee et al. (2019) presented an Set Transformer designed to model interactions among elements in the input set. Without considering missing words or noisy words, our task devolves into word ordering problem, which is a fundamental task in natural language generation. Previous, researchers usually employed N-gram based language models (De Gispert et al., 2014; Schmaltz et al., 2016), syntacticbased language models (Zhang and Clark, 2011; Liu et al., 2015) or combined models (Zhang et al., 2012; Liu and Zhang, 2015) to solve this problem. More recently, Hasler et al. (2017) proposed a bag-to-sequence model, where the decoder RNN directly attended to the word embeddings. However, all these methods aim at finding the best permutation of a bag of words based on language models, and do not consider how to encode a bag of words. 3 Problem Formulation Given a bag of words X = {x1 , x2 , · · · , xm } which consists of m tokens, our model will generate a sentence Y = {y1 , y2 , · · · , yn }, where n is the length of target sentence."
2021.naacl-main.137,2020.tacl-1.5,0,0.0466146,"Missing"
2021.naacl-main.137,W06-0115,0,0.112433,"Missing"
2021.naacl-main.137,2020.acl-main.611,0,0.275752,"de will be available at https://github. com/alibaba/pretrained-language-models/ LatticeBERT. However, the meanings of many Chinese words cannot be fully understood through direct compositions of their characters’ meanings. For example, 老板/boss does not mean 老/elder 板/board.2 The importance of word-level inputs in Chinese has been addressed in different tasks, including relation classification (Li et al., 2019), short text matching (Lai et al., 2019; Chen et al., 2020; Lyu et al., 2021), trigger detection (Lin et al., 2018), and named entity recognition (Zhang and Yang, 2018; Gui et al., 2019; Li et al., 2020a). The coarsegrained inputs benefit these tasks by introducing word-level semantics with multi-granularity representations, which is potentially complementary in character-level Chinese PLMs. In this work, we discuss how to pre-train a Chinese PLM over a word lattice structure to exploit multi-granularity inputs. We argue that by incorporating the coarse-grained units into PLM, models could learn to utilize the multi-granularity information for downstream tasks. Specifically, we organize characters and words in sentences as word lattices (see Figure 1), which enable the models to explore the"
2021.naacl-main.137,2020.acl-main.315,0,0.5274,"de will be available at https://github. com/alibaba/pretrained-language-models/ LatticeBERT. However, the meanings of many Chinese words cannot be fully understood through direct compositions of their characters’ meanings. For example, 老板/boss does not mean 老/elder 板/board.2 The importance of word-level inputs in Chinese has been addressed in different tasks, including relation classification (Li et al., 2019), short text matching (Lai et al., 2019; Chen et al., 2020; Lyu et al., 2021), trigger detection (Lin et al., 2018), and named entity recognition (Zhang and Yang, 2018; Gui et al., 2019; Li et al., 2020a). The coarsegrained inputs benefit these tasks by introducing word-level semantics with multi-granularity representations, which is potentially complementary in character-level Chinese PLMs. In this work, we discuss how to pre-train a Chinese PLM over a word lattice structure to exploit multi-granularity inputs. We argue that by incorporating the coarse-grained units into PLM, models could learn to utilize the multi-granularity information for downstream tasks. Specifically, we organize characters and words in sentences as word lattices (see Figure 1), which enable the models to explore the"
2021.naacl-main.137,P19-1430,0,0.0266985,"se characters — as the input, following the English PLMs’ practice (Devlin et al., 2019, BERT). ∗ investigate life Work done during an internship at Alibaba DAMO Academy. † Corresponding author. 1 Our code will be available at https://github. com/alibaba/pretrained-language-models/ LatticeBERT. However, the meanings of many Chinese words cannot be fully understood through direct compositions of their characters’ meanings. For example, 老板/boss does not mean 老/elder 板/board.2 The importance of word-level inputs in Chinese has been addressed in different tasks, including relation classification (Li et al., 2019), short text matching (Lai et al., 2019; Chen et al., 2020; Lyu et al., 2021), trigger detection (Lin et al., 2018), and named entity recognition (Zhang and Yang, 2018; Gui et al., 2019; Li et al., 2020a). The coarsegrained inputs benefit these tasks by introducing word-level semantics with multi-granularity representations, which is potentially complementary in character-level Chinese PLMs. In this work, we discuss how to pre-train a Chinese PLM over a word lattice structure to exploit multi-granularity inputs. We argue that by incorporating the coarse-grained units into PLM, models could lea"
2021.naacl-main.137,P18-1145,0,0.0605616,"Missing"
2021.naacl-main.155,C18-1041,0,0.0187589,"are parts of the whole causal inference process. Weber et al. (2020) show how to formalize causal relationships in script learning, but it is limited to pairwise learning of events and cannot be generalized to sequential and compositional events. Legal Judgement Prediction. Previous works in legal text analysis focus on the task of legal judgement prediction (LJP). Luo et al. (2017) and Zhong et al. (2018) exploit neural networks to solve LJP tasks. Zhong et al. (2020) provide interpretable judgements by iteratively questioning and answering. Another line pays attention to confusing charges: Hu et al. (2018) manually design discriminative attributes, and Xu et al. (2020) use attention mechanisms to highlight differences between similar charges. Using knowledge derived from causal graphs, GCI exhibits a different and interpretable discrimination process. 8 clustered into one factor, but the differences matter in legal judgement. For example, in Personal Injury’s graph, different ways of killing are summarized as the factor kill, therefore lose valuable information. Speciﬁcally, beaten to death might occur in cases of involuntary manslaughter, while shooting cases are more likely to be associated w"
2021.naacl-main.155,D17-1042,0,0.0291644,"n contrast, with Attention we set our sights on text containing rich causal inFollowing Lei et al. (2017), we conduct human formation in itself. Paul (2017) looks into text by evaluation on words accorded with high attention computing propensity score for each word, but only 1935 focuses on causal relationship between words and sentiment. We instead take a causal graph perspective, discover and utilize causal relationship inside text to perform reasoning. Neural Networks for Causal Discovery. Recently, researchers attempt to apply neural networks to causal discovery (Ponti and Korhonen, 2017; Alvarez-Melis and Jaakkola, 2017; Ning et al., 2018; Gao et al., 2019; Weber et al., 2020). However, Alvarez-Melis and Jaakkola (2017) model causal relationship by correlation, which may introduce bias into causal inference; Ning et al. (2018) and Gao et al. (2019) merely focus on capturing causality by explicit textual features or supervision from labeled causal pairs. There are also a line of works focusing on how to use neural networks to summarize confounders and estimate treatment effects (Louizos et al., 2017; Yao et al., 2018; Künzel et al., 2018), which are parts of the whole causal inference process. Weber et al. (2"
2021.naacl-main.155,N19-1179,0,0.0264114,"containing rich causal inFollowing Lei et al. (2017), we conduct human formation in itself. Paul (2017) looks into text by evaluation on words accorded with high attention computing propensity score for each word, but only 1935 focuses on causal relationship between words and sentiment. We instead take a causal graph perspective, discover and utilize causal relationship inside text to perform reasoning. Neural Networks for Causal Discovery. Recently, researchers attempt to apply neural networks to causal discovery (Ponti and Korhonen, 2017; Alvarez-Melis and Jaakkola, 2017; Ning et al., 2018; Gao et al., 2019; Weber et al., 2020). However, Alvarez-Melis and Jaakkola (2017) model causal relationship by correlation, which may introduce bias into causal inference; Ning et al. (2018) and Gao et al. (2019) merely focus on capturing causality by explicit textual features or supervision from labeled causal pairs. There are also a line of works focusing on how to use neural networks to summarize confounders and estimate treatment effects (Louizos et al., 2017; Yao et al., 2018; Künzel et al., 2018), which are parts of the whole causal inference process. Weber et al. (2020) show how to formalize causal rel"
2021.naacl-main.155,2020.acl-main.474,0,0.0531871,"Missing"
2021.naacl-main.155,D17-1289,1,0.786249,"pervision from labeled causal pairs. There are also a line of works focusing on how to use neural networks to summarize confounders and estimate treatment effects (Louizos et al., 2017; Yao et al., 2018; Künzel et al., 2018), which are parts of the whole causal inference process. Weber et al. (2020) show how to formalize causal relationships in script learning, but it is limited to pairwise learning of events and cannot be generalized to sequential and compositional events. Legal Judgement Prediction. Previous works in legal text analysis focus on the task of legal judgement prediction (LJP). Luo et al. (2017) and Zhong et al. (2018) exploit neural networks to solve LJP tasks. Zhong et al. (2020) provide interpretable judgements by iteratively questioning and answering. Another line pays attention to confusing charges: Hu et al. (2018) manually design discriminative attributes, and Xu et al. (2020) use attention mechanisms to highlight differences between similar charges. Using knowledge derived from causal graphs, GCI exhibits a different and interpretable discrimination process. 8 clustered into one factor, but the differences matter in legal judgement. For example, in Personal Injury’s graph, di"
2021.naacl-main.155,P18-1212,0,0.0661854,"Missing"
2021.naacl-main.155,D18-1302,0,0.0340661,"Missing"
2021.naacl-main.155,K17-1018,0,0.0258579,"easurement error for the causal graphs constructed by structured data. Egami et al. (2018) focus on mapping text to a low-dimensional representation of the treatment or outcome. Veitch et al. (2019) and Yao et al. (2019) treat text as confounder and covariate, which help to make causal estimation more accurate. These works all build causal graphs manually, and regard 6.3 Effect of Integrating Causal Strength text as a whole to be one of the factors. In contrast, with Attention we set our sights on text containing rich causal inFollowing Lei et al. (2017), we conduct human formation in itself. Paul (2017) looks into text by evaluation on words accorded with high attention computing propensity score for each word, but only 1935 focuses on causal relationship between words and sentiment. We instead take a causal graph perspective, discover and utilize causal relationship inside text to perform reasoning. Neural Networks for Causal Discovery. Recently, researchers attempt to apply neural networks to causal discovery (Ponti and Korhonen, 2017; Alvarez-Melis and Jaakkola, 2017; Ning et al., 2018; Gao et al., 2019; Weber et al., 2020). However, Alvarez-Melis and Jaakkola (2017) model causal relation"
2021.naacl-main.155,W17-0903,0,0.0223768,"o be one of the factors. In contrast, with Attention we set our sights on text containing rich causal inFollowing Lei et al. (2017), we conduct human formation in itself. Paul (2017) looks into text by evaluation on words accorded with high attention computing propensity score for each word, but only 1935 focuses on causal relationship between words and sentiment. We instead take a causal graph perspective, discover and utilize causal relationship inside text to perform reasoning. Neural Networks for Causal Discovery. Recently, researchers attempt to apply neural networks to causal discovery (Ponti and Korhonen, 2017; Alvarez-Melis and Jaakkola, 2017; Ning et al., 2018; Gao et al., 2019; Weber et al., 2020). However, Alvarez-Melis and Jaakkola (2017) model causal relationship by correlation, which may introduce bias into causal inference; Ning et al. (2018) and Gao et al. (2019) merely focus on capturing causality by explicit textual features or supervision from labeled causal pairs. There are also a line of works focusing on how to use neural networks to summarize confounders and estimate treatment effects (Louizos et al., 2017; Yao et al., 2018; Künzel et al., 2018), which are parts of the whole causal"
2021.naacl-main.155,N18-2028,0,0.0223367,"Missing"
2021.naacl-main.155,D18-1488,0,0.0237612,"Take Violent Acquisition as an example. Although the cases are predicted correctly by BiLSTM+Att, the model tends to attend to words bag, RMB and value, which frequently occur but cannot be treated as clues for judgement. Instead, BiLSTM+Att+Cons values factors such as grab, rob and hold, which are more helpful for judgement. 7 Related Works Causal Inference with Text. Recently, a few works try to take text into account when performing causal inference. Landeiro and Culotta (2016) introduce causal inference to text classiﬁcation, and manage to remove bias from certain out-of-text confounders. Wood-Doughty et al. (2018) use text as a supplement of missing data and measurement error for the causal graphs constructed by structured data. Egami et al. (2018) focus on mapping text to a low-dimensional representation of the treatment or outcome. Veitch et al. (2019) and Yao et al. (2019) treat text as confounder and covariate, which help to make causal estimation more accurate. These works all build causal graphs manually, and regard 6.3 Effect of Integrating Causal Strength text as a whole to be one of the factors. In contrast, with Attention we set our sights on text containing rich causal inFollowing Lei et al."
2021.naacl-main.155,2020.acl-main.280,0,0.0122528,"2020) show how to formalize causal relationships in script learning, but it is limited to pairwise learning of events and cannot be generalized to sequential and compositional events. Legal Judgement Prediction. Previous works in legal text analysis focus on the task of legal judgement prediction (LJP). Luo et al. (2017) and Zhong et al. (2018) exploit neural networks to solve LJP tasks. Zhong et al. (2020) provide interpretable judgements by iteratively questioning and answering. Another line pays attention to confusing charges: Hu et al. (2018) manually design discriminative attributes, and Xu et al. (2020) use attention mechanisms to highlight differences between similar charges. Using knowledge derived from causal graphs, GCI exhibits a different and interpretable discrimination process. 8 clustered into one factor, but the differences matter in legal judgement. For example, in Personal Injury’s graph, different ways of killing are summarized as the factor kill, therefore lose valuable information. Speciﬁcally, beaten to death might occur in cases of involuntary manslaughter, while shooting cases are more likely to be associated with murder. Also, factors with low frequency may be omitted in c"
2021.naacl-main.155,D18-1390,0,0.0186565,"causal pairs. There are also a line of works focusing on how to use neural networks to summarize confounders and estimate treatment effects (Louizos et al., 2017; Yao et al., 2018; Künzel et al., 2018), which are parts of the whole causal inference process. Weber et al. (2020) show how to formalize causal relationships in script learning, but it is limited to pairwise learning of events and cannot be generalized to sequential and compositional events. Legal Judgement Prediction. Previous works in legal text analysis focus on the task of legal judgement prediction (LJP). Luo et al. (2017) and Zhong et al. (2018) exploit neural networks to solve LJP tasks. Zhong et al. (2020) provide interpretable judgements by iteratively questioning and answering. Another line pays attention to confusing charges: Hu et al. (2018) manually design discriminative attributes, and Xu et al. (2020) use attention mechanisms to highlight differences between similar charges. Using knowledge derived from causal graphs, GCI exhibits a different and interpretable discrimination process. 8 clustered into one factor, but the differences matter in legal judgement. For example, in Personal Injury’s graph, different ways of killing"
2021.naacl-main.155,P16-2034,0,0.0203501,"rom the view of graph, the nodes pointing to Yi ). The calculated scores are fed into a random forest classiﬁer (Ho, 1995) to learn thresholds between the charges. More advanced classiﬁers can also be used. B. Leverage Causal Chains Labeled Cases Neural networks (NN) are considered to be good at exploring large volumes of textual data. This motivates us to integrate the causal framework with NN, to beneﬁt each other. Here we propose two integration methods as shown in Figure 3. First, we inject the estimated causal strength to constrain the attention weights of a Bi-LSTM with attention model (Zhou et al., 2016). A Bi-LSTM layer is ﬁrst applied to the fact descriptions to obtain contextual embeddings H = {h1 , h2 , . . . , hn }, hi ∈ Rb0 , where b0 is the dimension of embeddings. Then, an attention layer assigns different weights {a1 , a2 , . . . , an } to each word, and sums the words up according to the weights to build a text embedding v: n  exp(qT · hi ) ai × hi , (5) ai = n ,v = T k=1 exp(q · hk ) i=1 A B Y1 C D Y2 ... B Y1 LSTM Chain Embeddings Linear Predictions ... strength: Lcons = n  (ai − gi )2 , i=1 (6) L = Lcross + αLcons . Note that in the validation and testing stages, the inputs do"
2021.naacl-main.155,2020.emnlp-main.612,0,0.0468084,"Missing"
C16-1226,P15-1034,0,0.00769022,"), which involves 461 Freebase predicates. 3.3 Open Relation Extraction Despite huge amounts of precise knowledge facts, structured KBs still have natural limitation in the coverage of knowledge domains compared to the vast information on the web. For example, out of 500,000 relations extracted by the ReVerb Open IE system (Fader et al., 2011), only about 10,000 can be aligned to Freebase (Berant et al., 2013). To alleviate this problem, we propose a paraphrase based method that can map relational phrases to proper textual relations. Specifically, we first apply an open information extractor (Angeli et al., 2015) on the English Wikipedia to construct a repository of <argument 1 , relation, argument 2 &gt; triples, where the arguments are entity phrases found in the input sentence and the relation represents certain relationship between the arguments. By linking these arguments to KB entities, we can obtain a textual knowledge repository. Paraphrasing Once the candidate set of textual relations T R = {tr1 , tr3 , ..., tr|T R |} are constructed, given a relational phrase rp, our goal is to find the tr that has the same meaning as rp, which can be treated as a paraphrase task. Our framework accommodates any"
C16-1226,N07-4013,0,0.0109451,"structured data. TREC QA evaluations (Voorhees and Tice, 1999) have been explored as a platform for advancing the state of the art in unstructured QA (Wang et al., 2007; Heilman and Smith, 2010; Yao et al., 2013; Yih et al., 2013; Yu et al., 2014; Yang et al., 2015; Hermann et al., 2015). While initial progress on structured QA started with small toy domains like GeoQuery (Zelle and Mooney, 1996), recent trend in QA has shifted to large scale structured KBs like DBPedia, Freebase (Unger et al., 2012; Cai and Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013), and on text repository (Banko et al., 2007; Carlson et al., 2010; Krishnamurthy and Mitchell, 2012; Fader et al., 2013; Parikh et al., 2015). An exciting development in structured QA is to exploit multiple KBs (with different schemas) at the same time to answer questions jointly (Yahya et al., 2012; Fader et al., 2014; Zhang et al., 2016). Our model combines the best of both worlds by inferring over the structured KB and unstructured text. Our work is closely related to Joshi et al. (2014) who aim to answer noisy telegraphic queries using both structured and unstructured data. Their work is limited in answering single relation queries"
C16-1226,P14-1091,0,0.0241474,"Missing"
C16-1226,D13-1160,0,0.440177,"2) which consists of 127,811 pairs of relational phrases and DBpedia predicates involving 225 DBpedia predicates. For Freebase, we use 3,022 phrase-predicate pairs of W EB Q UESTIONS used in (Xu et al., 2016), which involves 461 Freebase predicates. 3.3 Open Relation Extraction Despite huge amounts of precise knowledge facts, structured KBs still have natural limitation in the coverage of knowledge domains compared to the vast information on the web. For example, out of 500,000 relations extracted by the ReVerb Open IE system (Fader et al., 2011), only about 10,000 can be aligned to Freebase (Berant et al., 2013). To alleviate this problem, we propose a paraphrase based method that can map relational phrases to proper textual relations. Specifically, we first apply an open information extractor (Angeli et al., 2015) on the English Wikipedia to construct a repository of <argument 1 , relation, argument 2 &gt; triples, where the arguments are entity phrases found in the input sentence and the relation represents certain relationship between the arguments. By linking these arguments to KB entities, we can obtain a textual knowledge repository. Paraphrasing Once the candidate set of textual relations T R = {"
C16-1226,D14-1067,0,0.148494,"Missing"
C16-1226,P13-1042,0,0.0317011,"Over time, the QA task has evolved into two main streams – QA on unstructured data, and QA on structured data. TREC QA evaluations (Voorhees and Tice, 1999) have been explored as a platform for advancing the state of the art in unstructured QA (Wang et al., 2007; Heilman and Smith, 2010; Yao et al., 2013; Yih et al., 2013; Yu et al., 2014; Yang et al., 2015; Hermann et al., 2015). While initial progress on structured QA started with small toy domains like GeoQuery (Zelle and Mooney, 1996), recent trend in QA has shifted to large scale structured KBs like DBPedia, Freebase (Unger et al., 2012; Cai and Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013), and on text repository (Banko et al., 2007; Carlson et al., 2010; Krishnamurthy and Mitchell, 2012; Fader et al., 2013; Parikh et al., 2015). An exciting development in structured QA is to exploit multiple KBs (with different schemas) at the same time to answer questions jointly (Yahya et al., 2012; Fader et al., 2014; Zhang et al., 2016). Our model combines the best of both worlds by inferring over the structured KB and unstructured text. Our work is closely related to Joshi et al. (2014) who aim to answer noisy telegraphic queries using both"
C16-1226,P14-1077,1,0.827079,"ntic parsing, i.e., mapping from text to logical forms containing the predicates from the given knowledge base. However, the closed predicate vocabulary assumed by the traditional KB-QA paradigm has inherent limitations. First, a closed predicate vocabulary has limited coverage, as such vocabularies are typically powered by community efforts. Second, a closed predicate vocabulary may abstract away potentially relevant semantic differences. Third, even a logical form was produced, the answers may be incomplete due to the imperfection of the KB, which has been addressed by (Riedel et al., 2013; Chen et al., 2014). For example, no logical form could be produced for the question who is the front man of the band that wrote Coffee & TV. Because the semantics of front man cannot be adequately encoded using Freebase or DBpedia predicates. On the other hand, knowledge bases like DBpedia capture real world facts, and web resources like Wikipedia may provide a large repository of sentences that complement those facts. For instance, we can find in Wikipedia a sentence In August 2009, Debelle performed at Africa Express in Paris, an event set up by Blur and Gorillaz front-man Damon Albarn, which indicates the fr"
C16-1226,J81-4005,0,0.761527,"Missing"
C16-1226,P15-1026,0,0.160102,"Missing"
C16-1226,D11-1142,0,0.0162979,"ively. For DBpedia, we use the PATTY dataset (Nakashole et al., 2012) which consists of 127,811 pairs of relational phrases and DBpedia predicates involving 225 DBpedia predicates. For Freebase, we use 3,022 phrase-predicate pairs of W EB Q UESTIONS used in (Xu et al., 2016), which involves 461 Freebase predicates. 3.3 Open Relation Extraction Despite huge amounts of precise knowledge facts, structured KBs still have natural limitation in the coverage of knowledge domains compared to the vast information on the web. For example, out of 500,000 relations extracted by the ReVerb Open IE system (Fader et al., 2011), only about 10,000 can be aligned to Freebase (Berant et al., 2013). To alleviate this problem, we propose a paraphrase based method that can map relational phrases to proper textual relations. Specifically, we first apply an open information extractor (Angeli et al., 2015) on the English Wikipedia to construct a repository of <argument 1 , relation, argument 2 &gt; triples, where the arguments are entity phrases found in the input sentence and the relation represents certain relationship between the arguments. By linking these arguments to KB entities, we can obtain a textual knowledge reposito"
C16-1226,P13-1158,0,0.0360702,"d to measure the word-wise and phrase-wise similarities between two sentences. Since sentences may be of arbitrary length, the resulting matrix of similarity measures is of variable size. Then a dynamic pooling layer is introduced to compute a fixedsized representation from the variable-sized matrices. Finally the pooled representation is used as input to a classifier Cp . Learning In our experiment, we directly used the pre-trained RAE which is trained on a subset of 150,000 sentences from the NYT and AP sections of the Gigaword corpus. To train the classifier Cp , we use the PARALEX corpus (Fader et al., 2013), which is a large monolingual parallel corpora, containing 18 million pairs of question paraphrases from wikianswers.com, which were tagged as having the same meaning by the users of the website. 4 Joint Inference The goal of the inference step is to find a global optimal configuration of entity phrases and relational phrases with semantic components. As the result of disambiguating one phrase can influence the mapping of other phrases, we consider all phrases jointly in one disambiguation task. Now, we will first describe three key criteria that are used to evaluate the configuration in deta"
C16-1226,D15-1205,0,0.0133779,"ebase, respectively. These entities are treated as candidate entities that will be eventually disambiguated in the joint inference step. 3.2 KB-based Relation Extraction The choice of KB-based relation extraction model is also broad. In this paper, we employ the MultiChannel Convolutional Neural Networks (MCCNNs) model presented in (Xu et al., 2016) to learn a compact and robust relation representation. This is crucial since there exist thousands of relations in a KB, using lexicalized features inevitably suffers from the sparsity problem and their poor generalization ability on unseen words (Gormley et al., 2015). The MCCNN model treats the conjunction of three parts in a ungrounded triple as a sentence (subject relational phrase object). The first channel takes the shortest path between the subject and object in the dependency tree4 as input, while the other channel takes the relational phrase itself as input. Each channel uses the network structure described in (Collobert et al., 2011), which uses a convolutional layer to project the word-trigram vectors of words within a context window of 3 words to a local contextual feature vector, followed by a max pooling layer that extracts the most salient lo"
C16-1226,N10-1145,0,0.0173798,"gest river in USA involving aggregation operations. Our current assumption that free text could provide useful textual relations may work only for frequently typed queries or for popular domains like movies, politics and geography. We note these limitations and hope our result will foster further research in this area. 6 Related Work Over time, the QA task has evolved into two main streams – QA on unstructured data, and QA on structured data. TREC QA evaluations (Voorhees and Tice, 1999) have been explored as a platform for advancing the state of the art in unstructured QA (Wang et al., 2007; Heilman and Smith, 2010; Yao et al., 2013; Yih et al., 2013; Yu et al., 2014; Yang et al., 2015; Hermann et al., 2015). While initial progress on structured QA started with small toy domains like GeoQuery (Zelle and Mooney, 1996), recent trend in QA has shifted to large scale structured KBs like DBPedia, Freebase (Unger et al., 2012; Cai and Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013), and on text repository (Banko et al., 2007; Carlson et al., 2010; Krishnamurthy and Mitchell, 2012; Fader et al., 2013; Parikh et al., 2015). An exciting development in structured QA is to exploit multiple KBs (with di"
C16-1226,D14-1117,0,0.0617263,"uctured KBs like DBPedia, Freebase (Unger et al., 2012; Cai and Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013), and on text repository (Banko et al., 2007; Carlson et al., 2010; Krishnamurthy and Mitchell, 2012; Fader et al., 2013; Parikh et al., 2015). An exciting development in structured QA is to exploit multiple KBs (with different schemas) at the same time to answer questions jointly (Yahya et al., 2012; Fader et al., 2014; Zhang et al., 2016). Our model combines the best of both worlds by inferring over the structured KB and unstructured text. Our work is closely related to Joshi et al. (2014) who aim to answer noisy telegraphic queries using both structured and unstructured data. Their work is limited in answering single relation queries. Our work also has similarities to Sun et al. (2015) who does question answering on unstructured data but enrich it with Freebase. Joint inference methods over multiple local models has been applied to KB-QA systems (Yahya et al., 2012). In contrast to this prior work concentrating on the structured KB, our constraints are more complex, as we address the joint mapping of relational phrases onto KB predicates and textual relations. 7 Conclusion and"
C16-1226,D12-1069,0,0.0135943,"rhees and Tice, 1999) have been explored as a platform for advancing the state of the art in unstructured QA (Wang et al., 2007; Heilman and Smith, 2010; Yao et al., 2013; Yih et al., 2013; Yu et al., 2014; Yang et al., 2015; Hermann et al., 2015). While initial progress on structured QA started with small toy domains like GeoQuery (Zelle and Mooney, 1996), recent trend in QA has shifted to large scale structured KBs like DBPedia, Freebase (Unger et al., 2012; Cai and Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013), and on text repository (Banko et al., 2007; Carlson et al., 2010; Krishnamurthy and Mitchell, 2012; Fader et al., 2013; Parikh et al., 2015). An exciting development in structured QA is to exploit multiple KBs (with different schemas) at the same time to answer questions jointly (Yahya et al., 2012; Fader et al., 2014; Zhang et al., 2016). Our model combines the best of both worlds by inferring over the structured KB and unstructured text. Our work is closely related to Joshi et al. (2014) who aim to answer noisy telegraphic queries using both structured and unstructured data. Their work is limited in answering single relation queries. Our work also has similarities to Sun et al. (2015) wh"
C16-1226,D13-1161,0,0.0280254,"o main streams – QA on unstructured data, and QA on structured data. TREC QA evaluations (Voorhees and Tice, 1999) have been explored as a platform for advancing the state of the art in unstructured QA (Wang et al., 2007; Heilman and Smith, 2010; Yao et al., 2013; Yih et al., 2013; Yu et al., 2014; Yang et al., 2015; Hermann et al., 2015). While initial progress on structured QA started with small toy domains like GeoQuery (Zelle and Mooney, 1996), recent trend in QA has shifted to large scale structured KBs like DBPedia, Freebase (Unger et al., 2012; Cai and Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013), and on text repository (Banko et al., 2007; Carlson et al., 2010; Krishnamurthy and Mitchell, 2012; Fader et al., 2013; Parikh et al., 2015). An exciting development in structured QA is to exploit multiple KBs (with different schemas) at the same time to answer questions jointly (Yahya et al., 2012; Fader et al., 2014; Zhang et al., 2016). Our model combines the best of both worlds by inferring over the structured KB and unstructured text. Our work is closely related to Joshi et al. (2014) who aim to answer noisy telegraphic queries using both structured and unstructured data. Their work is"
C16-1226,D12-1104,0,0.0317568,"g The model is learned using pairs of relational phrase and its corresponding KB predicate. Given an input phrase, the network outputs a distribution vector over the predicates o. We denote t as the target distribution vector, in which the value for gold relation is set 1, others are set 0. We compute the cross entropy error between t and o as the loss function. The model parameters can be efficiently computed via back-propagation through network structures. In experiment, we train two distinct relation extractors over DBpedia and Freebase, respectively. For DBpedia, we use the PATTY dataset (Nakashole et al., 2012) which consists of 127,811 pairs of relational phrases and DBpedia predicates involving 225 DBpedia predicates. For Freebase, we use 3,022 phrase-predicate pairs of W EB Q UESTIONS used in (Xu et al., 2016), which involves 461 Freebase predicates. 3.3 Open Relation Extraction Despite huge amounts of precise knowledge facts, structured KBs still have natural limitation in the coverage of knowledge domains compared to the vast information on the web. For example, out of 500,000 relations extracted by the ReVerb Open IE system (Fader et al., 2011), only about 10,000 can be aligned to Freebase (Be"
C16-1226,N15-1077,0,0.0471632,"for advancing the state of the art in unstructured QA (Wang et al., 2007; Heilman and Smith, 2010; Yao et al., 2013; Yih et al., 2013; Yu et al., 2014; Yang et al., 2015; Hermann et al., 2015). While initial progress on structured QA started with small toy domains like GeoQuery (Zelle and Mooney, 1996), recent trend in QA has shifted to large scale structured KBs like DBPedia, Freebase (Unger et al., 2012; Cai and Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013), and on text repository (Banko et al., 2007; Carlson et al., 2010; Krishnamurthy and Mitchell, 2012; Fader et al., 2013; Parikh et al., 2015). An exciting development in structured QA is to exploit multiple KBs (with different schemas) at the same time to answer questions jointly (Yahya et al., 2012; Fader et al., 2014; Zhang et al., 2016). Our model combines the best of both worlds by inferring over the structured KB and unstructured text. Our work is closely related to Joshi et al. (2014) who aim to answer noisy telegraphic queries using both structured and unstructured data. Their work is limited in answering single relation queries. Our work also has similarities to Sun et al. (2015) who does question answering on unstructured"
C16-1226,Q16-1010,0,0.0662561,"Missing"
C16-1226,N13-1008,0,0.0356622,"oblem reduces to semantic parsing, i.e., mapping from text to logical forms containing the predicates from the given knowledge base. However, the closed predicate vocabulary assumed by the traditional KB-QA paradigm has inherent limitations. First, a closed predicate vocabulary has limited coverage, as such vocabularies are typically powered by community efforts. Second, a closed predicate vocabulary may abstract away potentially relevant semantic differences. Third, even a logical form was produced, the answers may be incomplete due to the imperfection of the KB, which has been addressed by (Riedel et al., 2013; Chen et al., 2014). For example, no logical form could be produced for the question who is the front man of the band that wrote Coffee & TV. Because the semantics of front man cannot be adequately encoded using Freebase or DBpedia predicates. On the other hand, knowledge bases like DBpedia capture real world facts, and web resources like Wikipedia may provide a large repository of sentences that complement those facts. For instance, we can find in Wikipedia a sentence In August 2009, Debelle performed at Africa Express in Paris, an event set up by Blur and Gorillaz front-man Damon Albarn, wh"
C16-1226,P10-1040,0,0.031333,"Missing"
C16-1226,D07-1003,0,0.0202971,"t is the second longest river in USA involving aggregation operations. Our current assumption that free text could provide useful textual relations may work only for frequently typed queries or for popular domains like movies, politics and geography. We note these limitations and hope our result will foster further research in this area. 6 Related Work Over time, the QA task has evolved into two main streams – QA on unstructured data, and QA on structured data. TREC QA evaluations (Voorhees and Tice, 1999) have been explored as a platform for advancing the state of the art in unstructured QA (Wang et al., 2007; Heilman and Smith, 2010; Yao et al., 2013; Yih et al., 2013; Yu et al., 2014; Yang et al., 2015; Hermann et al., 2015). While initial progress on structured QA started with small toy domains like GeoQuery (Zelle and Mooney, 1996), recent trend in QA has shifted to large scale structured KBs like DBPedia, Freebase (Unger et al., 2012; Cai and Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013), and on text repository (Banko et al., 2007; Carlson et al., 2010; Krishnamurthy and Mitchell, 2012; Fader et al., 2013; Parikh et al., 2015). An exciting development in structured QA is to expl"
C16-1226,P16-1220,1,0.573515,"cannot be adequately encoded using Freebase or DBpedia predicates. On the other hand, knowledge bases like DBpedia capture real world facts, and web resources like Wikipedia may provide a large repository of sentences that complement those facts. For instance, we can find in Wikipedia a sentence In August 2009, Debelle performed at Africa Express in Paris, an event set up by Blur and Gorillaz front-man Damon Albarn, which indicates the front man of the band in the example question is Damon Albarn1 . Moreover, text corpora is also shown effective in refining the answers retrieved from the KBs (Xu et al., 2016). Motivated by these observations, we tackle the This work is licenced under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/ 1 The Blur band wrote the Coffee & TV song. Licence details: http:// 2397 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 2397–2407, Osaka, Japan, December 11-17 2016. question answering task by integrating these two types of heterogeneous data, i.e., structured knowledge bases and free text, while is rarely investigated before. This task involves three main"
C16-1226,D12-1035,0,0.0169377,"al., 2015; Hermann et al., 2015). While initial progress on structured QA started with small toy domains like GeoQuery (Zelle and Mooney, 1996), recent trend in QA has shifted to large scale structured KBs like DBPedia, Freebase (Unger et al., 2012; Cai and Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013), and on text repository (Banko et al., 2007; Carlson et al., 2010; Krishnamurthy and Mitchell, 2012; Fader et al., 2013; Parikh et al., 2015). An exciting development in structured QA is to exploit multiple KBs (with different schemas) at the same time to answer questions jointly (Yahya et al., 2012; Fader et al., 2014; Zhang et al., 2016). Our model combines the best of both worlds by inferring over the structured KB and unstructured text. Our work is closely related to Joshi et al. (2014) who aim to answer noisy telegraphic queries using both structured and unstructured data. Their work is limited in answering single relation queries. Our work also has similarities to Sun et al. (2015) who does question answering on unstructured data but enrich it with Freebase. Joint inference methods over multiple local models has been applied to KB-QA systems (Yahya et al., 2012). In contrast to thi"
C16-1226,P15-1049,0,0.00860844,"g of a question using clues from two types of heterogeneous resources, we tackle the QA problem in an IE-based fashion involving entity linking and relation extraction. In particular, we simultaneously map relational phrases to KB predicates and textual relations. 3.1 Entity Linking The preliminary entity linking model can be any approach which outputs a score for each entity candidate. Note that a recall-oriented model will be more than welcome, since we expect to introduce more potentially correct local predictions into the inference step. In this paper, we adopt DBpedia Lookup3 and S-MART (Yang and Chang, 2015) to retrieve top 10 entities from DBpedia and Freebase, respectively. These entities are treated as candidate entities that will be eventually disambiguated in the joint inference step. 3.2 KB-based Relation Extraction The choice of KB-based relation extraction model is also broad. In this paper, we employ the MultiChannel Convolutional Neural Networks (MCCNNs) model presented in (Xu et al., 2016) to learn a compact and robust relation representation. This is crucial since there exist thousands of relations in a KB, using lexicalized features inevitably suffers from the sparsity problem and th"
C16-1226,D15-1237,0,0.0102964,"t free text could provide useful textual relations may work only for frequently typed queries or for popular domains like movies, politics and geography. We note these limitations and hope our result will foster further research in this area. 6 Related Work Over time, the QA task has evolved into two main streams – QA on unstructured data, and QA on structured data. TREC QA evaluations (Voorhees and Tice, 1999) have been explored as a platform for advancing the state of the art in unstructured QA (Wang et al., 2007; Heilman and Smith, 2010; Yao et al., 2013; Yih et al., 2013; Yu et al., 2014; Yang et al., 2015; Hermann et al., 2015). While initial progress on structured QA started with small toy domains like GeoQuery (Zelle and Mooney, 1996), recent trend in QA has shifted to large scale structured KBs like DBPedia, Freebase (Unger et al., 2012; Cai and Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013), and on text repository (Banko et al., 2007; Carlson et al., 2010; Krishnamurthy and Mitchell, 2012; Fader et al., 2013; Parikh et al., 2015). An exciting development in structured QA is to exploit multiple KBs (with different schemas) at the same time to answer questions jointly (Yahya et"
C16-1226,N13-1106,0,0.0372165,"Missing"
C16-1226,N15-3014,0,0.0666292,"Missing"
C16-1226,P13-1171,0,0.0130659,"rations. Our current assumption that free text could provide useful textual relations may work only for frequently typed queries or for popular domains like movies, politics and geography. We note these limitations and hope our result will foster further research in this area. 6 Related Work Over time, the QA task has evolved into two main streams – QA on unstructured data, and QA on structured data. TREC QA evaluations (Voorhees and Tice, 1999) have been explored as a platform for advancing the state of the art in unstructured QA (Wang et al., 2007; Heilman and Smith, 2010; Yao et al., 2013; Yih et al., 2013; Yu et al., 2014; Yang et al., 2015; Hermann et al., 2015). While initial progress on structured QA started with small toy domains like GeoQuery (Zelle and Mooney, 1996), recent trend in QA has shifted to large scale structured KBs like DBPedia, Freebase (Unger et al., 2012; Cai and Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013), and on text repository (Banko et al., 2007; Carlson et al., 2010; Krishnamurthy and Mitchell, 2012; Fader et al., 2013; Parikh et al., 2015). An exciting development in structured QA is to exploit multiple KBs (with different schemas) at the same time to"
C16-1226,P15-1128,0,0.254026,"Missing"
D12-1076,S07-1012,0,0.278431,"Missing"
D12-1076,P98-1012,0,0.0306486,", including two steps: feature extraction and person clustering. Most research efforts so far have been made to the former, exploring various features according to specific applications, while the second step is currently dominated by hierarchical agglomerative clustering (HAC). According to the reliance of extra knowledge resources, existing works can be categorized into non-resource methods and resource-based methods. Non-resource methods extract various local features from the context of ambiguous names, and compute the similarity between feature vectors. These features include plain words(Bagga and Baldwin, 1998), biographical information(Mann and Yarowsky, 2003; Niu et al., 2004), named entities, compound key phrases, hyperlinks(Ikeda et al., 2009), etc. The similarity between namesakes are usually measured by the cosine similarity(Bagga and Baldwin, 1998), or other graph based metrics(Iria et al., 2007; Kalashnikov et al., 2008a; Jiang et al., 2009). Those methods pay more attention to extracting informative features and their co-occurrences, but they usually treat the features locally, and ignore the semantic relatedness of features beyond the current document. Resource-based approaches, on the oth"
D12-1076,E06-1002,0,0.251655,"Missing"
D12-1076,D07-1074,0,0.214831,"Missing"
D12-1076,P05-1045,0,0.00760577,"ateness(David and Ian, 2008) with other concepts in the current page. Named Entity and Biographical Information Extraction Although Wikipedia concepts can provide rich background knowledge, they suffer from the limited coverage. It is common that some discriminative features are not likely to be found in Wikipedia, such as names of infamous people or organizations, email addresses, phone numbers, etc. We therefore extract two extra kinds of features, named entities that do not appear in the Wikipedia anchor text dictionary, and biographical information. We use Stanford Named Entity Recognizer(Finkel et al., 2005) to collect named entities which are not in the Wikipedia list. We use regular expressions to extract email address, phone numbers and birth years. For convenience, we will also call concept features for Wikipedia concept features and non-concept features for the other two in the rest of this paper. 3.2 Stolen Base 0.4145 0.4228 Home Run 0.3467 Major League Baseball Graph Construction In our model, we capture the topic structure through a semantic graph. Specifically, for each name observation set, we connect all Wikipedia concepts appearing in the current observation set by their pairwise sem"
D12-1076,P10-1006,0,0.356658,"und knowledge about the semantic relatedness between entities can be leveraged to improve the disambiguation performance, and relieve the coverage problem, to some extent. Bunescu and Pasca and Cucerzan utilize Wikipedia’s category hierarchy to disambiguate entities, while Pilz uses Wikipedia’s link information. Han and Zhao adopt Wikipedia semantic relatedness to compute the similarity between name observations. They also combine multiple knowledge sources and capture explicit semantic relatedness between concepts and implicit semantic relationship embedded in a semantic graph simultaneously(Han and Zhao, 2010). Most approaches discussed above explore various features in the current page or rely on external knowledge resources to bridge the vocabulary gap, but pay less attention to the lack of clues since they ignore the person specific evidence in the current corpus level. Our model focuses on solving the data sparsity problem by utilizing other web pages in the same name observation set to provide a robust but person specific weighting for discriminative features beyond the current document alone. In terms of extra resources, the Wikipedia based model (WS) by Han and Zhao (2009) is close to our mo"
D12-1076,S07-1107,0,0.0145354,"e of extra knowledge resources, existing works can be categorized into non-resource methods and resource-based methods. Non-resource methods extract various local features from the context of ambiguous names, and compute the similarity between feature vectors. These features include plain words(Bagga and Baldwin, 1998), biographical information(Mann and Yarowsky, 2003; Niu et al., 2004), named entities, compound key phrases, hyperlinks(Ikeda et al., 2009), etc. The similarity between namesakes are usually measured by the cosine similarity(Bagga and Baldwin, 1998), or other graph based metrics(Iria et al., 2007; Kalashnikov et al., 2008a; Jiang et al., 2009). Those methods pay more attention to extracting informative features and their co-occurrences, but they usually treat the features locally, and ignore the semantic relatedness of features beyond the current document. Resource-based approaches, on the other hand, can leverage external resources to benefit from rich background knowledge, which is crucial to remedy the data sparsity problem. The employed resources include raw texts available on the web and online encyclopedias. Kalashnikov et al. and Yiming et al. use extra web corpora to obtain co"
D12-1076,W03-0405,0,0.745249,"s first extracting various features from the web pages, and then grouping these pages into several clusters each of which is assumed to represent one specific person. Despite of the inevitably noisy nature of web data, a key challenge is how to handle the data sparsity problem which we mean as: mismatch of vocabulary and lack of clues. The former refers to the case that two web pages may describe the same person but use different words thus the word overlap between them are small. Various features, including entities, biographical information, URL, etc., have been introduced to bridge the gap(Mann and Yarowsky, 2003; Kalashnikov et al., 2008a; Ikeda et al., 2009; Jiang et al., 2009), 832 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 832–842, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics and external knowledge resources are also employed to capture the semantic relationship between entities(Han and Zhao, 2009, 2010). However, a more challenging scenario is that there are few clues available in the web pages. For example, there is a page mentioning a nutritionist Emily B"
D12-1076,P04-1076,0,0.614882,"ch efforts so far have been made to the former, exploring various features according to specific applications, while the second step is currently dominated by hierarchical agglomerative clustering (HAC). According to the reliance of extra knowledge resources, existing works can be categorized into non-resource methods and resource-based methods. Non-resource methods extract various local features from the context of ambiguous names, and compute the similarity between feature vectors. These features include plain words(Bagga and Baldwin, 1998), biographical information(Mann and Yarowsky, 2003; Niu et al., 2004), named entities, compound key phrases, hyperlinks(Ikeda et al., 2009), etc. The similarity between namesakes are usually measured by the cosine similarity(Bagga and Baldwin, 1998), or other graph based metrics(Iria et al., 2007; Kalashnikov et al., 2008a; Jiang et al., 2009). Those methods pay more attention to extracting informative features and their co-occurrences, but they usually treat the features locally, and ignore the semantic relatedness of features beyond the current document. Resource-based approaches, on the other hand, can leverage external resources to benefit from rich backgro"
D12-1076,S07-1042,0,0.035341,"Missing"
D12-1076,C98-1012,0,\N,Missing
D14-1205,D11-1071,0,0.0223021,"er, 2012). Entity linking is a crucial part in many KB re1913 lated tasks. Many EL models explore local contexts of entity mentions to measure the similarity between mentions and candidate entities (Han et al., 2011; Han and Sun, 2011; Ratinov et al., 2011; Cheng and Roth, 2013). Some methods further exploit global coherence among candidate entities in the same document by assuming that these entities should be closely related (Han et al., 2011; Ratinov et al., 2011; Sen, 2012; Cheng and Roth, 2013). There are also some approaches regarding entity linking as a ranking task (Zhou et al., 2010; Chen and Ji, 2011). Lin et al. (2012) propose an approach to detect and type entities that are currently not in the KB. Note that the EL task in KBP is different from the name entity mention extraction task, mainly in the ACE task style, which mainly identifies the boundaries and types of entity mentions and does not explicitly link entity mentions into a KB (ACE, 2004; Florian et al., 2006; Florian et al., 2010; Li and Ji, 2014), thus are different from our work. isting KB and identify the relations between pairs of entities based on that text corpus. Choi et al. (2006) jointly extracts the expressions and sou"
D14-1205,P14-1077,1,0.879017,"entity pair level can be utilized here (again, a recall-oriented version will be welcome), such as Mintz++ mentioned in (Surdeanu et al., 2012), which we adapt into a Maximum Entropy version. We also include a special label, NA, to represent the case where there is no predefined relationship between an entity pair. For each sentence, we retain the relations with top q scores for the inference step, and we also call that this sentence supports those candidate relations. As for the features of RE models, we use the same features (lexical features and syntactic features) with the previous works (Chen et al., 2014; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011). 4.2 Relations’ Expectations for Argument Types In most KBs’ schemas, canonicalized relations are designed to expect specific types of entities to be their arguments. For example, in Figure 2, it is more likely that an entity Kobe Bryant takes the subject position of a relation fb:pro athlete.teams, but it is unlikely for this entity to take the subject position of a relation fb:org.headquarters. Making use of these type requirements can encourage the framework to select relation and entity candidates which are coherent with eac"
D14-1205,D13-1184,0,0.0249217,"he types of entities. In contrast, our RE models only require minimal supervision and do not need well-annotated training data. Our framework is therefore easy to adapt to new scenarios and suits real-world applications. The cold-start task aims at constructing a KB from scratch in a slot-filling style (Sun et al., 2012; Monahan and Carpenter, 2012). Entity linking is a crucial part in many KB re1913 lated tasks. Many EL models explore local contexts of entity mentions to measure the similarity between mentions and candidate entities (Han et al., 2011; Han and Sun, 2011; Ratinov et al., 2011; Cheng and Roth, 2013). Some methods further exploit global coherence among candidate entities in the same document by assuming that these entities should be closely related (Han et al., 2011; Ratinov et al., 2011; Sen, 2012; Cheng and Roth, 2013). There are also some approaches regarding entity linking as a ranking task (Zhou et al., 2010; Chen and Ji, 2011). Lin et al. (2012) propose an approach to detect and type entities that are currently not in the KB. Note that the EL task in KBP is different from the name entity mention extraction task, mainly in the ACE task style, which mainly identifies the boundaries an"
D14-1205,W06-1651,0,0.12043,"ng as a ranking task (Zhou et al., 2010; Chen and Ji, 2011). Lin et al. (2012) propose an approach to detect and type entities that are currently not in the KB. Note that the EL task in KBP is different from the name entity mention extraction task, mainly in the ACE task style, which mainly identifies the boundaries and types of entity mentions and does not explicitly link entity mentions into a KB (ACE, 2004; Florian et al., 2006; Florian et al., 2010; Li and Ji, 2014), thus are different from our work. isting KB and identify the relations between pairs of entities based on that text corpus. Choi et al. (2006) jointly extracts the expressions and sources of opinion as well as the linking relations (i.e., a source entity expresses an opinion expression) between them, while we focus on jointly modeling EL and RE in open domain, which is a different and challenging task. Since the automatically extracted knowledge facts inevitably contain errors, many approaches manage to assign confidences for those extracted facts (Fader et al., 2011; Wick et al., 2013). Wick et al. (2013) also point out that confidence estimation should be a crucial part in the automated KB constructions and will play a key role fo"
D14-1205,C10-1032,0,0.0357622,"efine the knowledge base population task that we will address in this paper. Next we detail the proposed framework and present our experiments and results. Finally, we conclude this paper with future directions. 2 Related Work Knowledge base population (KBP), the task of extending existing KBs with entities and relations, has been studied in the TAC-KBP evaluations (Ji et al., 2011), containing three tasks. The entity linking task links entity mentions to existing KB nodes and creates new nodes for the entities absent in the current KBs, which can be considered as a kind of entity population (Dredze et al., 2010; Tamang et al., 2012; Cassidy et al., 2011). The slot-filling task populates new relations to the KB (Tamang et al., 2012; Roth et al., 2012; Liu and Zhao, 2012), but the relations are limited to a predefined sets of attributes according to the types of entities. In contrast, our RE models only require minimal supervision and do not need well-annotated training data. Our framework is therefore easy to adapt to new scenarios and suits real-world applications. The cold-start task aims at constructing a KB from scratch in a slot-filling style (Sun et al., 2012; Monahan and Carpenter, 2012). Enti"
D14-1205,D11-1142,0,0.0904765,"006; Florian et al., 2010; Li and Ji, 2014), thus are different from our work. isting KB and identify the relations between pairs of entities based on that text corpus. Choi et al. (2006) jointly extracts the expressions and sources of opinion as well as the linking relations (i.e., a source entity expresses an opinion expression) between them, while we focus on jointly modeling EL and RE in open domain, which is a different and challenging task. Since the automatically extracted knowledge facts inevitably contain errors, many approaches manage to assign confidences for those extracted facts (Fader et al., 2011; Wick et al., 2013). Wick et al. (2013) also point out that confidence estimation should be a crucial part in the automated KB constructions and will play a key role for the wide applications of automatically built KBs. We thus propose to model the reliability of the complete extraction process and take the argument type expectations of the relation, coherence with other predictions and the triples in the existing KB into account for each populated triple. Meanwhile, relation extraction has also been studied extensively in recent years, ranging from supervised learning methods (ACE, 2004; Zha"
D14-1205,D12-1082,0,0.0303539,"Missing"
D14-1205,P06-1060,0,0.024091,"by assuming that these entities should be closely related (Han et al., 2011; Ratinov et al., 2011; Sen, 2012; Cheng and Roth, 2013). There are also some approaches regarding entity linking as a ranking task (Zhou et al., 2010; Chen and Ji, 2011). Lin et al. (2012) propose an approach to detect and type entities that are currently not in the KB. Note that the EL task in KBP is different from the name entity mention extraction task, mainly in the ACE task style, which mainly identifies the boundaries and types of entity mentions and does not explicitly link entity mentions into a KB (ACE, 2004; Florian et al., 2006; Florian et al., 2010; Li and Ji, 2014), thus are different from our work. isting KB and identify the relations between pairs of entities based on that text corpus. Choi et al. (2006) jointly extracts the expressions and sources of opinion as well as the linking relations (i.e., a source entity expresses an opinion expression) between them, while we focus on jointly modeling EL and RE in open domain, which is a different and challenging task. Since the automatically extracted knowledge facts inevitably contain errors, many approaches manage to assign confidences for those extracted facts (Fad"
D14-1205,D10-1033,0,0.0133747,"entities should be closely related (Han et al., 2011; Ratinov et al., 2011; Sen, 2012; Cheng and Roth, 2013). There are also some approaches regarding entity linking as a ranking task (Zhou et al., 2010; Chen and Ji, 2011). Lin et al. (2012) propose an approach to detect and type entities that are currently not in the KB. Note that the EL task in KBP is different from the name entity mention extraction task, mainly in the ACE task style, which mainly identifies the boundaries and types of entity mentions and does not explicitly link entity mentions into a KB (ACE, 2004; Florian et al., 2006; Florian et al., 2010; Li and Ji, 2014), thus are different from our work. isting KB and identify the relations between pairs of entities based on that text corpus. Choi et al. (2006) jointly extracts the expressions and sources of opinion as well as the linking relations (i.e., a source entity expresses an opinion expression) between them, while we focus on jointly modeling EL and RE in open domain, which is a different and challenging task. Since the automatically extracted knowledge facts inevitably contain errors, many approaches manage to assign confidences for those extracted facts (Fader et al., 2011; Wick"
D14-1205,P11-1095,0,0.0335071,"Missing"
D14-1205,P11-1055,0,0.743046,"ument type expectations of the relation, coherence with other predictions and the triples in the existing KB into account for each populated triple. Meanwhile, relation extraction has also been studied extensively in recent years, ranging from supervised learning methods (ACE, 2004; Zhao and Grishman, 2005; Li and Ji, 2014) to unsupervised open extractions (Fader et al., 2011; Carlson et al., 2010). There are also models, with distant supervision (DS), utilizing reliable texts resources and existing KBs to predict relations for a large amount of texts (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). These distantly supervised models can extract relations from texts in open domain, and do not need much human involvement. Hence, DS is more suitable for our task compared to other traditional RE approaches. 3 Joint inference over multiple local models has been applied to many NLP tasks. Our task is different from the traditional joint IE works based in the ACE framework (Singh et al., 2013; Li and Ji, 2014; Kate and Mooney, 2010), which jointly extract and/or classify named entity mentions to several predefined types in a sentence and identify in a sentence level whi"
D14-1205,W10-2924,0,0.251682,"ilizing reliable texts resources and existing KBs to predict relations for a large amount of texts (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). These distantly supervised models can extract relations from texts in open domain, and do not need much human involvement. Hence, DS is more suitable for our task compared to other traditional RE approaches. 3 Joint inference over multiple local models has been applied to many NLP tasks. Our task is different from the traditional joint IE works based in the ACE framework (Singh et al., 2013; Li and Ji, 2014; Kate and Mooney, 2010), which jointly extract and/or classify named entity mentions to several predefined types in a sentence and identify in a sentence level which relation this specific sentence describes (between a pair of entity mentions in this sentence). Li and Ji (2014) follow the ACE task definitions and present a neat incremental joint framework to simultaneously extract entity mentions and relations by structure perceptron. In contrast, we link entity mentions from a text corpus to their corresponding entities in an ex4 Task definition We formalize our task as follows. Given a set of entities sampled from"
D14-1205,P14-1038,0,0.402784,"osely related (Han et al., 2011; Ratinov et al., 2011; Sen, 2012; Cheng and Roth, 2013). There are also some approaches regarding entity linking as a ranking task (Zhou et al., 2010; Chen and Ji, 2011). Lin et al. (2012) propose an approach to detect and type entities that are currently not in the KB. Note that the EL task in KBP is different from the name entity mention extraction task, mainly in the ACE task style, which mainly identifies the boundaries and types of entity mentions and does not explicitly link entity mentions into a KB (ACE, 2004; Florian et al., 2006; Florian et al., 2010; Li and Ji, 2014), thus are different from our work. isting KB and identify the relations between pairs of entities based on that text corpus. Choi et al. (2006) jointly extracts the expressions and sources of opinion as well as the linking relations (i.e., a source entity expresses an opinion expression) between them, while we focus on jointly modeling EL and RE in open domain, which is a different and challenging task. Since the automatically extracted knowledge facts inevitably contain errors, many approaches manage to assign confidences for those extracted facts (Fader et al., 2011; Wick et al., 2013). Wic"
D14-1205,P11-1138,0,0.0322282,"ributes according to the types of entities. In contrast, our RE models only require minimal supervision and do not need well-annotated training data. Our framework is therefore easy to adapt to new scenarios and suits real-world applications. The cold-start task aims at constructing a KB from scratch in a slot-filling style (Sun et al., 2012; Monahan and Carpenter, 2012). Entity linking is a crucial part in many KB re1913 lated tasks. Many EL models explore local contexts of entity mentions to measure the similarity between mentions and candidate entities (Han et al., 2011; Han and Sun, 2011; Ratinov et al., 2011; Cheng and Roth, 2013). Some methods further exploit global coherence among candidate entities in the same document by assuming that these entities should be closely related (Han et al., 2011; Ratinov et al., 2011; Sen, 2012; Cheng and Roth, 2013). There are also some approaches regarding entity linking as a ranking task (Zhou et al., 2010; Chen and Ji, 2011). Lin et al. (2012) propose an approach to detect and type entities that are currently not in the KB. Note that the EL task in KBP is different from the name entity mention extraction task, mainly in the ACE task style, which mainly ident"
D14-1205,N13-1008,0,0.0421681,"obtain the subject’s type basketball player, and then we go through the initial matrix and find another entity Kobe Bryant with the same type taking the subject position of fb:pro athlete.teams, indicating that Jay Fletcher Vincent may take the relation fb:pro athlete.teams. The matrix Sobj is processed in the same way. Implicit Type Expectations In practice, few KBs have well-defined schemas. In order to make our framework more flexible, we need to come up with an approach to implicitly capture the relations’ type expectations, which will also be represented as preference scores. Inspired by Riedel et al. (2013) who use a matrix factorization approach to capture the association between textual patterns, relations and entities based on large text corpora, we adopt a collaborative filtering (CF) method to compute the preference scores between entities and relations based on the statistics obtained from an existing KB. In CF, the preferences between customers and items are calculated via matrix factorization over the initial customer-item matrix. In our framework, we compute the preference scores between entities and relations via the same approach over the two initialized matrices Ssubj and Sobj , resu"
D14-1205,D12-1042,0,0.733785,"of the relation, coherence with other predictions and the triples in the existing KB into account for each populated triple. Meanwhile, relation extraction has also been studied extensively in recent years, ranging from supervised learning methods (ACE, 2004; Zhao and Grishman, 2005; Li and Ji, 2014) to unsupervised open extractions (Fader et al., 2011; Carlson et al., 2010). There are also models, with distant supervision (DS), utilizing reliable texts resources and existing KBs to predict relations for a large amount of texts (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). These distantly supervised models can extract relations from texts in open domain, and do not need much human involvement. Hence, DS is more suitable for our task compared to other traditional RE approaches. 3 Joint inference over multiple local models has been applied to many NLP tasks. Our task is different from the traditional joint IE works based in the ACE framework (Singh et al., 2013; Li and Ji, 2014; Kate and Mooney, 2010), which jointly extract and/or classify named entity mentions to several predefined types in a sentence and identify in a sentence level which relation this specifi"
D14-1205,D10-1099,0,0.24755,"Missing"
D14-1205,P05-1052,0,0.0506913,"011; Wick et al., 2013). Wick et al. (2013) also point out that confidence estimation should be a crucial part in the automated KB constructions and will play a key role for the wide applications of automatically built KBs. We thus propose to model the reliability of the complete extraction process and take the argument type expectations of the relation, coherence with other predictions and the triples in the existing KB into account for each populated triple. Meanwhile, relation extraction has also been studied extensively in recent years, ranging from supervised learning methods (ACE, 2004; Zhao and Grishman, 2005; Li and Ji, 2014) to unsupervised open extractions (Fader et al., 2011; Carlson et al., 2010). There are also models, with distant supervision (DS), utilizing reliable texts resources and existing KBs to predict relations for a large amount of texts (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). These distantly supervised models can extract relations from texts in open domain, and do not need much human involvement. Hence, DS is more suitable for our task compared to other traditional RE approaches. 3 Joint inference over multiple local models has bee"
D14-1205,C10-1150,0,0.0170709,"Monahan and Carpenter, 2012). Entity linking is a crucial part in many KB re1913 lated tasks. Many EL models explore local contexts of entity mentions to measure the similarity between mentions and candidate entities (Han et al., 2011; Han and Sun, 2011; Ratinov et al., 2011; Cheng and Roth, 2013). Some methods further exploit global coherence among candidate entities in the same document by assuming that these entities should be closely related (Han et al., 2011; Ratinov et al., 2011; Sen, 2012; Cheng and Roth, 2013). There are also some approaches regarding entity linking as a ranking task (Zhou et al., 2010; Chen and Ji, 2011). Lin et al. (2012) propose an approach to detect and type entities that are currently not in the KB. Note that the EL task in KBP is different from the name entity mention extraction task, mainly in the ACE task style, which mainly identifies the boundaries and types of entity mentions and does not explicitly link entity mentions into a KB (ACE, 2004; Florian et al., 2006; Florian et al., 2010; Li and Ji, 2014), thus are different from our work. isting KB and identify the relations between pairs of entities based on that text corpus. Choi et al. (2006) jointly extracts the"
D14-1205,P09-1113,0,\N,Missing
D15-1062,C08-1088,0,0.0499677,"ned as follows: given a sentence S with a pair of nominals e1 and e2 , we aim to identify the relationship between e1 and e2 . RE is typically investigated in a classification style, where many features have been proposed, e.g., Hendrickx et al. (2010) designed 16 types of features including POS, WordNet, FrameNet, dependency parse features, etc. Among them, syntactic features are considered to bring significant improvements in extraction accuracy (Bunescu and Mooney, 2005a). Earlier attempts to encode syntactic information are mainly kernel-based methods, such as the convolution tree kernel (Qian et al., 2008), subsequence kernel (Bunescu and Mooney, 2005b), and dependency tree kernel (Bunescu and Mooney, 2005a). With the recent success of neural networks in natural language processing, different neural network models are proposed to learn syntactic features from raw sequences of words or constituent 536 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 536–540, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. and our model outperforms the-state-of-the-art methods on the SemEval-2010 Task 8 dataset. 2 table layer,"
D15-1062,D12-1110,0,0.370317,"parisons of our models with other methods on the SemEval 2010 task 8. Negative sampling schemes No negative examples Randomly sampled negative examples from NYT Dependency paths from the object to subject F1 81.3 83.5 85.4 Table 3: Comparisons of different negtive sampling methods on the development set. for We , W1 , W2 and W3 . The best setting was obtained with the values: 3, 200, 100, 10−4 , 10−3 , 10−4 and 2 × 10−3 , respectively. Results and Discussion Table 2 summarizes the performances of our model, depLCNN+NS(+), and state-of-the-art models, SVM (Hendrickx et al., 2010), RNN, MV-RNN (Socher et al., 2012), CNN (Zeng et al., 2014) and DepNN (Liu et al., 2015). For fair comparisons, we also add two types of lexical features, WordNet hypernyms and words around nominals, as part of input vector to the final softmax layer. We can see that our vanilla depLCNN+NS, without extra lexical features, still outperforms, by a large margin, previously reported best systems, MVRNN+ and CNN+, both of which have taken extra lexical features into account, showing that our treatment to dependency path can learn a robust and effective relation representation. When augmented with similar lexical features, our depLC"
D15-1062,H05-1091,0,0.835081,"Missing"
D15-1062,P10-1040,0,0.0177957,"y paths corresponding to two opposite subject/object directions, and then make predictions for the two paths, respectively. We choose the relation other if and only if both predictions are other. And for the rest cases, we choose the non-other relation with highest confidence as the output, since ideally, for a non-other instance, our model will output the correct label for the right subject/object direction and an other label for the wrong direction. We evaluate our models by macro-averaged F1 using the official evaluation script. We initialized We with 50-dimensional word vectors trained by Turian et al. (2010). We tuned the hyper parameters using the development set for each experimental setting. The hyper parameters include w, n1 , n2 , and regularization parameters Negative Sampling We start by presenting three pilot experiments on the development set. In the first one, we assume that the assignment of the subject and object for a relation is not given (blind), we simply extract features from e1 to e2 , and test it in a blind setting as well. In the second one, we assume that the assignment is given (sighted) during training, but still blind in the test phase. The last one is assumed to give the"
D15-1062,C14-1220,0,0.323001,"ill blind in the test phase. The last one is assumed to give the assignment during both training and test steps. The results are listed in Table 1. The third experiment can be seen as an upper bound, where we do not need to worry about the assignments of subjects and objects. By comparing the first and the second one, we can see that when adding assignment information during training, our model can be significantly improved, 1 Note that, there may be more than one relation existing between two nominals. A dependency path thus may correspond to multiple relations. 538 Method SVM RNN MVRNN CNN (Zeng et al., 2014) DepNN depCNN depLCNN depLCNN depLCNN+NS Feature Sets 16 types of features +POS, NER, WordNet +POS, NER, WordNet +WordNet,words around nominals +NER +WordNet,words around nominals +WordNet,words around nominals F1 82.2 74.8 77.6 79.1 82.4 78.9 82.7 83.6 81.3 81.9 83.7 84.0 85.6 Table 2: Comparisons of our models with other methods on the SemEval 2010 task 8. Negative sampling schemes No negative examples Randomly sampled negative examples from NYT Dependency paths from the object to subject F1 81.3 83.5 85.4 Table 3: Comparisons of different negtive sampling methods on the development set. for"
D15-1062,P14-1077,1,0.862752,"Missing"
D15-1062,W09-2415,0,0.357197,"Missing"
D15-1062,S10-1006,0,\N,Missing
D17-1233,W14-4012,0,0.00569695,"Missing"
D17-1233,J90-1003,0,0.369463,"In this work, we implement f using GRU. The decoder RNN generates each reply word conditioned on the context vector C. The probability distribution pt of candidate words at every time step t is calculated as: Technical Background Seq2Seq Model and Attention Mechanism (1) (4) where η is usually implemented as a multi-layer perceptron (MLP) with tanh as an activation function. 2191 User’s query ?? … ?? … ?? ?? Online process Cue word ?? (?? ) ?? … ?? ?? … ?? ℎ? Pre-process ?? … ?? ?? … ?? Trained model Input: ??−1 ?? … ?? ?? … ?? Pointwise Mutual Information Pointwise mutual information (PMI) (Church and Hanks, 1990) is a measure of association ratio based on the information theoretic concept of mutual information. Given a pair of outcomes x and y belonging to discrete random variables X and Y, the PMI quantifies the discrepancy between the probability of their coincidence based on their joint distribution and their individual distributions. Mathematically: PMI(x, y) = log p(x, y) p(x|y) = log p(x)p(y) p(x) (5) This quantity is zero if x and y are independent, positive if they are positively correlated, and negative if they are negatively correlated. 3 ?1 ?2 ?3 ?? = ℎ1 ℎ2 ℎ3 … ℎ? ?0 ?1 ?2 ??−1 Cue word pr"
D17-1233,N16-1014,0,0.449992,"ery as a vector and to decode the vector into a reply. Inspired by (Mou et al., 2016), we mainly focus on the generative short-text conversation without context information. Despite this, the performance of Seq2Seq generation-based conversation systems is far from satisfactory because its generation process is not controllable; it responses to a query according to the pattern learned from the training corpus. As a result, the system is likely to generate an unexpected reply even with little semantics, e.g, “I don’t know” and “Okay” due to the high frequency of these patterns in training data (Li et al., 2016a; Mou et al., 2016). To address this issue, Li et al. (2016a) proposed to increase diversity in the Seq2Seq model so that more informative utterances have a chance to stand out. Mou et al. (2016) provided a content-introducing approach that generates a reply based on a predicted word. The word is usually enlightening and drives the generated response to be more meaningful. However, this method is to some extent rigid; it requires the predicted word to explicitly occur in the generated utterance. As shown in Table 1, sometimes, it is better to generate a semantic related sentence based on the"
D17-1233,C16-1316,1,0.897694,"n open domains, researchers mainly focus on data-driven approaches, since the diversity and uncertainty make it impossible to prepare the interaction logic and domain knowledge. Basically, there are two mainstream ways to build an opendomain conversation system: 1) to search preestablished database for candidate responses by ∗ Corresponding author: ruiyan@pku.edu.cn query retrieval (Isbell et al., 2000; Wang et al., 2013; Yan et al., 2016; Song et al., 2016), and 2) to generate a new, tailored utterance given the userissued query (Shang et al., 2015; Vinyals and Le, 2015; Serban et al., 2016; Mou et al., 2016; Song et al., 2016). In these studies, generation-based conversation systems have shown impressive potential. Especially, the Sequence-to-Sequence (Seq2Seq) model (Sutskever et al., 2014) based on neural networks has been extensively used in practice; the idea is to encode a query as a vector and to decode the vector into a reply. Inspired by (Mou et al., 2016), we mainly focus on the generative short-text conversation without context information. Despite this, the performance of Seq2Seq generation-based conversation systems is far from satisfactory because its generation process is not contr"
D17-1233,P02-1040,0,0.107195,"tic Language Generation in Dialogue (SLGD) method (Wen et al., 2015a), which added additional features in each gate of the neural cell. FGRU: To explore more fusion strategies, intuitively, we fused the cue word and hidden states by vector concatenation during the decoding process. Note that rGRU and SCGRU incorporate additional information by gating mechanisms, while SLGD and FGRU fuse the information into each gate of the neural cell directly. 4.3 Experiment Evaluation Objective metrics. To evaluate the performance of different methods for the conversation generation task, we leverage BLEU (Papineni et al., 2002) as the automatic evaluation metric, which is originally designed for machine translation and evaluates the output by using n-gram matching between the output and the reference. Here, we use BLEU-1, BLEU-2 and BLEU-3 in our experiments. Subjective metrics. Since automatic metrics may not consistently agree with human perception (Stent et al., 2005), human testing is essential to assess subjective quality. Hence, we randomly sampled 150 queries in the test set, then we invited five annotators to offer a judgment. For fairness, all of our human evaluation was conducted in a random, blind fashion"
D17-1233,D11-1054,0,0.017995,"hange the “hard” content-introducing method into a new “soft” schema. The rest of paper is organized as follows. We start by introducing the technical background. In Section 3, we describe our proposed method. In Section 4, we illustrate the experimental setup and evaluations against a variety of baselines. Section 5 briefly reviews related work. Finally, we conclude our paper in Section 6. 2 2.1 sentence as a vector by a recurrent neural network (RNN) and to decode the vector to a target sentence by another RNN. Now, the conversational generation is treated as a monolingual translation task (Ritter et al., 2011; Shang et al., 2015). Given a query Q = (x1 , ..., xn ), the encoder represents it as a context vector C and then the decoder generates a response R = (y1 , ..., ym ) word by word by maximizing the generation probability of R conditioned on Q. The objective function of Seq2Seq can be written as: p(y1 , ..., ym |x1 , ..., xn ) =p(y1 |C) T Y p(yt |C, y1 , ..., yt−1 ) t=2 To be specific, the encoder RNN calculates the context vector by: ht = f (xt , ht−1 ); C = hT st = f (yt−1 , st−1 , C); pt = softmax(st , yt−1 ) (3) where st is the hidden state of decoder RNN at time t and yt−1 is the generate"
D17-1233,P15-1152,0,0.195597,". For now, buildinga conversation systemmainly falls into two categories: retrievalbased and generation-based. As information retrieval techniques are developing fast, Leuski et al. (2009) build systems to select the most suitable response from the query-reply pairs using a statistical language model in cross-lingual information retrieval. Yan et al. (2016) propose a retrieval-based conversation system with the deep learning-to-respond schema through a deep neural network framework driven by web data. Recently, generation-based conversation systems have shownimpressive potential. Shang et al. (2015) generate replies for short-text conversation by Seq2Seq-basedneural networks with local and global attentions. 5.2 Content Introducing In vertical domains, Wen et al. (2015b) apply an additional control cell to gate the dialogue act (DA) features during the generation process to ensure the generated repliesexpressthe intended meaning. Also, the Stochastic Language Generation in Dialogue method (Wen et al., 2015a) adds additional features in each gate of the neural cell. Xu et al. (2016) introduce a new trainable gate to recall the global domain memory to enhance the ability of modeling the se"
D17-1233,P01-1066,0,0.104456,"ken as a desktop for a long while. (Screenshot) As the lockscreen? Make acquaintance and seek chances for further relations! (Freshman) I am also the new! Nice to meet you. Table 5: The implicit introducing-content cases of our HGFU model. The cue word in bold is not contained in the reply, while the response is still related to the cue word. T aemin† is a Korean singer. 5 5.1 Related work Conversation Systems Automatic human-computer conversation has attractedincreasing attention over the past few years. At the very beginning, people start the research using hand-crafted rules and templates (Walker et al., 2001; Misu and Kawahara, 2007; Williams et al., 2013). These approaches require no data or little data for trainingbuthuge manual effort to build the model, which is very timeconsuming. For now, buildinga conversation systemmainly falls into two categories: retrievalbased and generation-based. As information retrieval techniques are developing fast, Leuski et al. (2009) build systems to select the most suitable response from the query-reply pairs using a statistical language model in cross-lingual information retrieval. Yan et al. (2016) propose a retrieval-based conversation system with the deep"
D17-1233,D13-1096,0,0.0172018,"ers and practitioners. In particular, automatic conversation systems in open domains are attracting increasing attention due to its wide applications, such as virtual assistants and chatbots. In open domains, researchers mainly focus on data-driven approaches, since the diversity and uncertainty make it impossible to prepare the interaction logic and domain knowledge. Basically, there are two mainstream ways to build an opendomain conversation system: 1) to search preestablished database for candidate responses by ∗ Corresponding author: ruiyan@pku.edu.cn query retrieval (Isbell et al., 2000; Wang et al., 2013; Yan et al., 2016; Song et al., 2016), and 2) to generate a new, tailored utterance given the userissued query (Shang et al., 2015; Vinyals and Le, 2015; Serban et al., 2016; Mou et al., 2016; Song et al., 2016). In these studies, generation-based conversation systems have shown impressive potential. Especially, the Sequence-to-Sequence (Seq2Seq) model (Sutskever et al., 2014) based on neural networks has been extensively used in practice; the idea is to encode a query as a vector and to decode the vector into a reply. Inspired by (Mou et al., 2016), we mainly focus on the generative short-te"
D17-1233,W15-4639,0,0.345872,"edings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2190–2199 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics Query Cue Word Reply Query Cue Word Reply 你不觉得好丑吗(Don’t you think it is ugly?) 审美(Aesthetics) 好恶心啊! (It’s disgusting!) 先放个大招(Let me use my ultimate power.) 技能(Skill) 新技能？(New skill?) Table 1: The content-introducing conversation examples. eration. 1) How to add the additional cue words during the generation process? One of the prevailing methods is modifying the neural cell with various gating mechanisms (Wen et al., 2015a,b; Xu et al., 2016). However, we need careful operation to ensure the neuron works as expected. 2) How to display the cue words in replies? As mentioned above, the explicit content-introducing approach in (Mou et al., 2016) does not fit well with all situations. In this paper, we present an implicit contentintroducing method for generative conversation systems, which incorporates cue words using our proposed hierarchical gated fusion unit (HGFU) in a flexible way. Our main contributions are as follows: • We propose the cue word GRU, another neural cell, to deal with the auxiliary information"
D17-1233,D15-1199,0,0.0607099,"Missing"
D17-1233,W13-4065,0,0.00407011,") As the lockscreen? Make acquaintance and seek chances for further relations! (Freshman) I am also the new! Nice to meet you. Table 5: The implicit introducing-content cases of our HGFU model. The cue word in bold is not contained in the reply, while the response is still related to the cue word. T aemin† is a Korean singer. 5 5.1 Related work Conversation Systems Automatic human-computer conversation has attractedincreasing attention over the past few years. At the very beginning, people start the research using hand-crafted rules and templates (Walker et al., 2001; Misu and Kawahara, 2007; Williams et al., 2013). These approaches require no data or little data for trainingbuthuge manual effort to build the model, which is very timeconsuming. For now, buildinga conversation systemmainly falls into two categories: retrievalbased and generation-based. As information retrieval techniques are developing fast, Leuski et al. (2009) build systems to select the most suitable response from the query-reply pairs using a statistical language model in cross-lingual information retrieval. Yan et al. (2016) propose a retrieval-based conversation system with the deep learning-to-respond schema through a deep neural"
D17-1233,W06-1303,0,\N,Missing
D17-1289,D14-1181,0,0.00894182,"ecision making in the common law system. Rather than finding relevant cases, our work focuses on predicting specific charges, and we also emphasize the importance of law articles in decision making, which is important in the civil law system where the decisions are made based solely on statutory laws. Our work is also related to the task of document classification, but mainly differs in that we also need to automatically identify applicable law articles to support and improve the charge prediction. Recently, various neural network (NN) architectures such as Convolutional Neural Network (CNN) (Kim, 2014) and Recurrent Neural Network (RNN) have been used for document embedding, which is further used for classification. (Tang et al., 2015) propose a two-layer scheme, RNN or CNN for sentence embedding, and another RNN for document embedding. (Yang et al., 2016) further use global context vectors to attentively distinguish informative words or sentences from non-informative ones during embedding, which we share similar spirit with. But, we take a more flexible and descriptive two-stack attention mechanism, one stack for fact embedding, and the other for article embedding which is dynamically gene"
D17-1289,N16-1063,0,0.00409045,"words or sentences from non-informative ones during embedding, which we share similar spirit with. But, we take a more flexible and descriptive two-stack attention mechanism, one stack for fact embedding, and the other for article embedding which is dynamically generated for each instance according to the fact-side clues as extra guidance. Another difference is the multi-label nature of our task, where, rather than optimizing as multiple binary classification tasks (Nam et al., 2014), we convert the multi-label target to label distribution during training with cross entropy as loss function (Kurata et al., 2016), and use a threshold tuned on validation set to produce the final prediction, which performs better in our pilot experiments. 3 Data Preparation Our data are collected from China Judgements Online1 , where the Chinese government has been publishing judgement documents since 2013. We randomly choose 50,000 documents for training, 5,000 for validation and 5,000 for testing. To ensure enough training data for each charge, we only classify the charges that appear more than 80 times in the training data, and treat documents with other charges as negative data. As for law articles, we consider thos"
D17-1289,O12-5004,0,0.230881,"rt) to support the decision. Even in countries using the common law system, e.g., the United States (except Louisiana), where the judgement is based mainly on decisions of previous cases, there are still some statutory laws that need to be followed when making decisions. Existing attempts formulate the task of automatic charge prediction as a single-label classification problem, by either adopting a k-Nearest Neighbor (KNN) (Liu et al., 2004; Liu and Hsieh, 2006) as the classifier with shallow textual features, or manually designing key factors for specific charges to help text understanding (Lin et al., 2012), which make those works hard to scale to more types of charges. There are also works addressing a related task, finding the law articles that are involved in a given case. A simple solution is to convert this multi-label problem into a multiclass classification task by only considering a fixed set of article combinations (Liu and Liao, 2005; Liu and Hsieh, 2006), which can only be applied to a small set of articles and does not fit to real applications. Recent improvement takes a twostep approach by performing a preliminary classification first and then re-ranking the results with 2727 Procee"
D17-1289,D15-1167,0,0.0516587,"d we also emphasize the importance of law articles in decision making, which is important in the civil law system where the decisions are made based solely on statutory laws. Our work is also related to the task of document classification, but mainly differs in that we also need to automatically identify applicable law articles to support and improve the charge prediction. Recently, various neural network (NN) architectures such as Convolutional Neural Network (CNN) (Kim, 2014) and Recurrent Neural Network (RNN) have been used for document embedding, which is further used for classification. (Tang et al., 2015) propose a two-layer scheme, RNN or CNN for sentence embedding, and another RNN for document embedding. (Yang et al., 2016) further use global context vectors to attentively distinguish informative words or sentences from non-informative ones during embedding, which we share similar spirit with. But, we take a more flexible and descriptive two-stack attention mechanism, one stack for fact embedding, and the other for article embedding which is dynamically generated for each instance according to the fact-side clues as extra guidance. Another difference is the multi-label nature of our task, wh"
D17-1289,P12-2018,0,0.00966167,"e extraction task as multiple binary classifications. Specifically, we build a binary classifier for each article, focusing on its relevance to the input case, which results in 321 binary classifiers corresponding to the 321 distinct law articles appearing in our dataset. When more articles are considered, we can simply add more binary classifiers accordingly, with the existing classifiers untouched. Similar to the preliminary classification phase of (Liu et al., 2015), we also use word-based SVM as our binary classifier, which is fast and performs well in text classification (Joachims, 2002; Wang and Manning, 2012). Specifically, we use bag-ofwords TF-IDF features, chi-square for feature selection and linear kernel for binary classification. (1) where hf t and hbt are the states of the forward and backward GRU at position t. The final sequence embedding is either the concatenation of hf T and hb1 or simply the average of ht . Attentive Sequence Encoder However, directly using [hf T , hb1 ] for sequence encoding often fails to capture all the information when the sequence is long, while using the average of ht also has the drawback of treating useless elements equally with informative ones. Inspired by ("
D17-1289,N16-1174,0,0.120055,"decisions are made based solely on statutory laws. Our work is also related to the task of document classification, but mainly differs in that we also need to automatically identify applicable law articles to support and improve the charge prediction. Recently, various neural network (NN) architectures such as Convolutional Neural Network (CNN) (Kim, 2014) and Recurrent Neural Network (RNN) have been used for document embedding, which is further used for classification. (Tang et al., 2015) propose a two-layer scheme, RNN or CNN for sentence embedding, and another RNN for document embedding. (Yang et al., 2016) further use global context vectors to attentively distinguish informative words or sentences from non-informative ones during embedding, which we share similar spirit with. But, we take a more flexible and descriptive two-stack attention mechanism, one stack for fact embedding, and the other for article embedding which is dynamically generated for each instance according to the fact-side clues as extra guidance. Another difference is the multi-label nature of our task, where, rather than optimizing as multiple binary classification tasks (Nam et al., 2014), we convert the multi-label target t"
D18-1380,D17-1047,0,0.654271,"ethods, such as support vector machine (SVM) (Wagner et al., 2014; Kiritchenko et al., 2014), are employed with welldesigned handcrafted features. In recent years, neural network models (Socher et al., 2011; Dong et al., 2014; Nguyen and Shirai, 2015) are studied to automatically learn low-dimensional representations for aspects and their context. Attention mechanism (Wang et al., 2016; Li et al., 2017; Ma et al., 2017) is also be studied to characterize the effect of aspect on enforcing the model to pay more attention on the important words of the context. Previous works (Tang et al., 2016b; Chen et al., 2017) mainly employed the simple averaged aspect vector to learn the attention weights on the context words. Ma et al. [2017] further proposed the bidirectional attention mechanism, which interactively learns the attention weights on context/aspect words, with respect to the averaged vector of aspect/context, respectively. These above attention methods are all at the coarse-grained level, which simply averages the aspect/context vector to guide learning the attention weights on the context/aspect words. The simple average pooling mechanism might cause information loss, especially for the aspect wit"
D18-1380,P14-2009,0,0.761413,"e.g. positive, neutral, negative) of sentence with respect to the aspects. For example, in sentence “I like coming back to Mac OS but this laptop is lacking in speaker quality compared to my $400 old HP laptop”, the polarity of the sentence towards the aspect “Mac OS” is positive while the polarity is negative in terms of aspect “speaker quality”. ∗ corresponding author. Many statistical methods, such as support vector machine (SVM) (Wagner et al., 2014; Kiritchenko et al., 2014), are employed with welldesigned handcrafted features. In recent years, neural network models (Socher et al., 2011; Dong et al., 2014; Nguyen and Shirai, 2015) are studied to automatically learn low-dimensional representations for aspects and their context. Attention mechanism (Wang et al., 2016; Li et al., 2017; Ma et al., 2017) is also be studied to characterize the effect of aspect on enforcing the model to pay more attention on the important words of the context. Previous works (Tang et al., 2016b; Chen et al., 2017) mainly employed the simple averaged aspect vector to learn the attention weights on the context words. Ma et al. [2017] further proposed the bidirectional attention mechanism, which interactively learns the"
D18-1380,P11-1016,0,0.376953,"e context and different sentiment polarities. As far as we know, we are the first to explore the interactions among the aspects with the same context. To evaluate the proposed approach, we conduct experiments on three datasets: laptop and restaurant are from the SemEval 2014 Task 4 and the third one is a tweet collection. Experimental results show that our method achieves the best performance on all three datasets. 2 Related Work Aspect-level sentiment analysis is a branch of sentiment classification, which requires considering both the sentence and aspect information. Traditional approaches (Jiang et al., 2011; Kiritchenko et al., 2014; Vo and Zhang, 2015) regard this task as the text classification problem and design effective features, which are utilized in statistical learning algorithms for training a classifier. Kiritchenko et al. [2014] proposed to use SVM based on n-gram features, parse features and lexicon features, which achieved the best performance in SemEval 2014. Vo and Zhang [2015] designed sentiment-specific word embedding and sentiment lexicons as rich features for prediction. These methods highly depend on the effectiveness of the laborious feature engineering work and easily reach"
D18-1380,S14-2076,0,0.639666,"iment classification is a fundamental task in sentiment analysis (Pang et al., 2008; Liu, 2012), which aims to infer the sentiment polarity (e.g. positive, neutral, negative) of sentence with respect to the aspects. For example, in sentence “I like coming back to Mac OS but this laptop is lacking in speaker quality compared to my $400 old HP laptop”, the polarity of the sentence towards the aspect “Mac OS” is positive while the polarity is negative in terms of aspect “speaker quality”. ∗ corresponding author. Many statistical methods, such as support vector machine (SVM) (Wagner et al., 2014; Kiritchenko et al., 2014), are employed with welldesigned handcrafted features. In recent years, neural network models (Socher et al., 2011; Dong et al., 2014; Nguyen and Shirai, 2015) are studied to automatically learn low-dimensional representations for aspects and their context. Attention mechanism (Wang et al., 2016; Li et al., 2017; Ma et al., 2017) is also be studied to characterize the effect of aspect on enforcing the model to pay more attention on the important words of the context. Previous works (Tang et al., 2016b; Chen et al., 2017) mainly employed the simple averaged aspect vector to learn the attention"
D18-1380,D16-1011,0,0.0406189,"Missing"
D18-1380,E17-2091,0,0.16824,"n states to predict the polarity. TD-LSTM (Tang et al., 2016a) employs two directional LSTM networks, which estimate the left context and right context of the target aspect, respectively. Finally it takes the last hidden states of LSTM networks for prediction. MemNet (Tang et al., 2016b) applys multi-hop attentions on the word embeddings, learns the attention weights on context word vectors with respect to the averaged query vector. IAN (Ma et al., 2017) interactively learns the coarse-grained attentions between the context and aspect, and concatenate the vectors for prediction. BILSTM-ATT-G (Liu and Zhang, 2017) models left and right context with two attention-based LSTMs and utilizes gates to control the importance of left context, right context and the entire sentence for prediction. RAM(Chen et al., 2017) learns multi-hop attentions on the hidden states of bidirectional LSTM networks for context words, and proposes to use GRU network to get the aggregated vector from the attentions. Similar with MemNet, the atten3438 tion weights on context words are steered by the simple averaged aspect vector. We also list the variants of MGAN model, which are used to analyze the effects of coarsegrained attenti"
D18-1380,D15-1298,0,0.29463,"Missing"
D18-1380,D14-1162,0,0.0806935,"and each aspect ai = {wi1 , · · · , wiM } is a subsequence of sentence s, which contains M ∈ [1, N ) words. Aspect-level sentiment classification evaluates sentiment polarity of the sentence s with respect to each aspect ai . We present the overall architecture of the proposed Multi-grained Attention Network (MGAN) model in Figure 1. It consists of the Input Embedding layer, the Contextual Layer, the Multigrained Attention Layer and the Output Layer. 3.2 Input Embedding Layer Input Embedding Layer maps each word to a high dimensional vector space. We employ the pretrained word vector, GloVe (Pennington et al., 2014), to obtain the fixed word embedding of each word. Specifically, we denote the embedding lookup matrix as L ∈ Rdv ×|V |, where dv is the word vector dimension and |V |is the vocabulary size. 3.3 Contextual Layer We employ a bidirectional Long Short-Term Memory Network (BiLSTM) on top of the embedding layer to capture the temporal interactions among words. Specifically, at time step t, given the input word embedding x, the update process of forward LSTM network can be formalized as follows: − → − − → → − it = σ(W i · [ h t−1 , → x t] + b i) − → − − → → − ft = σ(W f · [ h t−1 , → x t] + b f ) −"
D18-1380,S14-2004,0,0.366679,"and update the training parameters. In addition, we utilize dropout strategy to avoid overfitting. 4 Experiments In this section, we conduct experiments to evaluate our two hypotheses: (1) whether the wordlevel interaction between aspect and context can help relieve the information loss and improve the performance. (2) whether the relationship among the aspects, which have the same context and different sentiment polarities, can bring extra useful information. 4.1 Experiment Setting We conduct experiments on three datasets, as shown in Table 1. The first two are from the SemEval 2014 Task 41 (Pontiki et al., 2014), which contains the reviews in laptop and restaurants, respectively. The third one is a tweet collection, which are gathered by (Dong et al., 2014). Each aspect with the context is labeled by three sentiment polarities, namely Positive, Neutral and Negative. In addition, we adopt Accuracy and MacroF1 as the metrics to evaluate the performance of aspect-level sentiment classification, which is widely used in previous works (Tang et al., 2016b; Ma et al., 2017; Chen et al., 2017; Wang et al., 2016). In our experiments, word embeddings for both context and aspect words are initialized by Glove ("
D18-1380,D11-1014,0,0.230789,"sentiment polarity (e.g. positive, neutral, negative) of sentence with respect to the aspects. For example, in sentence “I like coming back to Mac OS but this laptop is lacking in speaker quality compared to my $400 old HP laptop”, the polarity of the sentence towards the aspect “Mac OS” is positive while the polarity is negative in terms of aspect “speaker quality”. ∗ corresponding author. Many statistical methods, such as support vector machine (SVM) (Wagner et al., 2014; Kiritchenko et al., 2014), are employed with welldesigned handcrafted features. In recent years, neural network models (Socher et al., 2011; Dong et al., 2014; Nguyen and Shirai, 2015) are studied to automatically learn low-dimensional representations for aspects and their context. Attention mechanism (Wang et al., 2016; Li et al., 2017; Ma et al., 2017) is also be studied to characterize the effect of aspect on enforcing the model to pay more attention on the important words of the context. Previous works (Tang et al., 2016b; Chen et al., 2017) mainly employed the simple averaged aspect vector to learn the attention weights on the context words. Ma et al. [2017] further proposed the bidirectional attention mechanism, which inter"
D18-1380,C16-1311,0,0.287843,". Many statistical methods, such as support vector machine (SVM) (Wagner et al., 2014; Kiritchenko et al., 2014), are employed with welldesigned handcrafted features. In recent years, neural network models (Socher et al., 2011; Dong et al., 2014; Nguyen and Shirai, 2015) are studied to automatically learn low-dimensional representations for aspects and their context. Attention mechanism (Wang et al., 2016; Li et al., 2017; Ma et al., 2017) is also be studied to characterize the effect of aspect on enforcing the model to pay more attention on the important words of the context. Previous works (Tang et al., 2016b; Chen et al., 2017) mainly employed the simple averaged aspect vector to learn the attention weights on the context words. Ma et al. [2017] further proposed the bidirectional attention mechanism, which interactively learns the attention weights on context/aspect words, with respect to the averaged vector of aspect/context, respectively. These above attention methods are all at the coarse-grained level, which simply averages the aspect/context vector to guide learning the attention weights on the context/aspect words. The simple average pooling mechanism might cause information loss, especial"
D18-1380,D16-1021,0,0.609338,". Many statistical methods, such as support vector machine (SVM) (Wagner et al., 2014; Kiritchenko et al., 2014), are employed with welldesigned handcrafted features. In recent years, neural network models (Socher et al., 2011; Dong et al., 2014; Nguyen and Shirai, 2015) are studied to automatically learn low-dimensional representations for aspects and their context. Attention mechanism (Wang et al., 2016; Li et al., 2017; Ma et al., 2017) is also be studied to characterize the effect of aspect on enforcing the model to pay more attention on the important words of the context. Previous works (Tang et al., 2016b; Chen et al., 2017) mainly employed the simple averaged aspect vector to learn the attention weights on the context words. Ma et al. [2017] further proposed the bidirectional attention mechanism, which interactively learns the attention weights on context/aspect words, with respect to the averaged vector of aspect/context, respectively. These above attention methods are all at the coarse-grained level, which simply averages the aspect/context vector to guide learning the attention weights on the context/aspect words. The simple average pooling mechanism might cause information loss, especial"
D18-1380,S14-2036,0,0.0226756,"ion Aspect level sentiment classification is a fundamental task in sentiment analysis (Pang et al., 2008; Liu, 2012), which aims to infer the sentiment polarity (e.g. positive, neutral, negative) of sentence with respect to the aspects. For example, in sentence “I like coming back to Mac OS but this laptop is lacking in speaker quality compared to my $400 old HP laptop”, the polarity of the sentence towards the aspect “Mac OS” is positive while the polarity is negative in terms of aspect “speaker quality”. ∗ corresponding author. Many statistical methods, such as support vector machine (SVM) (Wagner et al., 2014; Kiritchenko et al., 2014), are employed with welldesigned handcrafted features. In recent years, neural network models (Socher et al., 2011; Dong et al., 2014; Nguyen and Shirai, 2015) are studied to automatically learn low-dimensional representations for aspects and their context. Attention mechanism (Wang et al., 2016; Li et al., 2017; Ma et al., 2017) is also be studied to characterize the effect of aspect on enforcing the model to pay more attention on the important words of the context. Previous works (Tang et al., 2016b; Chen et al., 2017) mainly employed the simple averaged aspect vec"
D18-1380,D16-1058,0,0.342146,"Missing"
D18-1423,K16-1002,0,0.064903,"in a topic, especially when they are generated or extracted from an inventory (Wang et al., 2016c). On the other hand, Chinese poems are generally short in length, with every character carefully chosen to be concise and elegant. Yet, prior poem generation models with recurrent neural networks (RNN) are likely to generate highfrequency characters (Zhang et al., 2017a), and the resulted poems are trivial and boring. The reason is that RNN tends to be entrapped within local word co-occurrences, they normally fail to capture global characteristic such as topic or hierarchical semantic properties (Bowman et al., 2016). To address the aforementioned shortcomings, RNN is extended to autoencoder (Dai and Le, 2015) for improving sequence learning, which has *Corresponding author: Rui Yan (ruiyan@pku.edu.cn) †Work was partially done at Tencent AI Lab. 1 We use term and character interchangeably in this paper. 3890 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3890–3900 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics been proven to be appealing in explicitly modeling global properties such as syntactic, semantic, a"
D18-1423,D16-1126,0,0.031283,"s confirm the validity and effectiveness of our model, where its automatic and human evaluation scores outperform existing models. 1 Introduction In mastering concise, elegant wordings with aesthetic rhythms in fixed patterns, classical Chinese poem is a special cultural heritage to record personal emotions and political views, as well as document daily or historical events. Being a fascinating art, writing poems is an attractive task that researchers of artificial intelligence are interested in (Tosa et al., 2008; Wu et al., 2009; Netzer et al., 2009; Oliveira, 2012; Yan et al., 2013, 2016a; Ghazvininejad et al., 2016, 2017; Singh et al., 2017; Xu et al., 2018), partially for the reason that poem generation and its related research could benefit other constrained natural language generation tasks. Conventionally, rule-based models (Zhou et al., 2010) and statistical machine translation (SMT) models (He et al., 2012) are proposed for this task. Recently, deep neural models are employed to generate fluent and natural poems (Wang et al., 2016a; Yan, 2016; Zhang et al., 2017a). Although these models look promising, they are limited in many aspects, e.g., previous studies generally fail to keep thematic consist"
D18-1423,P17-4008,0,0.0532257,"Missing"
D18-1423,P17-1059,0,0.0269444,"e the reconstruction log-likelihood of the input x under the condition of c. Following the operation for VAE, we have the corresponding variational lower bound of pθ (x|c) formulated as L(θ, φ; x, c) = − KL(qφ (z|x, c) k pθ (z|c)) + Eqφ (z|x,c) [log pθ (x|z, c)] (2) which is similar to Eq.1 except that all items are introduced with c, such as qφ (z|x, c) and pθ (z|c), 3891 referring to the conditioned approximate posterior and the conditioned prior, respectively. 2.2 Problem Formulation Following the text-to-text generation paradigm (Ranzato et al., 2015; Kiddon et al., 2016; Hu et al., 2017; Ghosh et al., 2017), our task has a similar problem setting with conventional studies (Zhang and Lapata, 2014; Wang et al., 2016c), where a poem is generated in a line-by-line manner that each line serves as the input for the next one, as illustrated in Figure 1. To formulate this task, we separate its input and output with necessary notations as follows. The I NPUT of the entire model is a title, T =(e1 ,e2 ,. . . ,eN ), functionalized as the theme of the target poem2 , where ei refers to i-the character’s embedding and N is the length of the title. The first line L1 is generated only conditioned on the title T"
D18-1423,D10-1051,0,0.142401,"ion with (Hu et al., 2017; Yu et al., 2017; Lin et al., 2017; Zhang et al., 2017b; Guo et al., 2018). To the best of our knowledge, this work is the first one integrating CVAE and adversarial training with a discriminator for text generation, especially in a particular text genre, poetry. Automatic Poem Generation. According to methodology, previous approaches can be roughly classified into three categories: 1) rule and template based methods (Tosa et al., 2008; Wu et al., 2009; Netzer et al., 2009; Zhou et al., 2010; Oliveira, 2012; Yan et al., 2013); 2) SMT approaches (Jiang and Zhou, 2008; Greene et al., 2010; He et al., 2012); 3) deep neural models (Zhang and Lapata, 2014; Wang et al., 2016b; Yan, 2016). Compared to rule-based and SMT models, neural models are able to learn more complicated representations and generate smooth poems. Most recent studies followed this paradigm. For example, Wang et al. (2016c) proposed a modified encoder-decoder model with keyword planning; Zhang et al. (2017a) adopted memory-augmented RNNs to dynamically choose each term from RNN output or a reserved inventory. To improve thematic consistency, Yang et al. (2017) combined CVAE and keywords planning. Compared to the"
D18-1423,P17-1016,0,0.0172345,"(Wang et al., 2016c; Yang et al., 2017) and improve term1 novelty (Zhang et al., 2017a), which are important characteristics of poems. In classical Chinese poem composing, thematic consistency and term novelty are usually mutually exclusive conditions to each other, i.e., consistent lines may bring duplicated terms while intriguing choices of characters could result in thematic diversities. On one hand, thematic consistency is essential for poems; it is preferred that all lines concentrate on the same theme throughout a poem. Previous work mainly focused on using keywords (Wang et al., 2016c; Hopkins and Kiela, 2017) to plan a poem so as to generate each line with a specific keyword. Such strategy is risky for the reason that the keywords are not guaranteed consistent in a topic, especially when they are generated or extracted from an inventory (Wang et al., 2016c). On the other hand, Chinese poems are generally short in length, with every character carefully chosen to be concise and elegant. Yet, prior poem generation models with recurrent neural networks (RNN) are likely to generate highfrequency characters (Zhang et al., 2017a), and the resulted poems are trivial and boring. The reason is that RNN tend"
D18-1423,C08-1048,0,0.0401591,"2017) and text generation with (Hu et al., 2017; Yu et al., 2017; Lin et al., 2017; Zhang et al., 2017b; Guo et al., 2018). To the best of our knowledge, this work is the first one integrating CVAE and adversarial training with a discriminator for text generation, especially in a particular text genre, poetry. Automatic Poem Generation. According to methodology, previous approaches can be roughly classified into three categories: 1) rule and template based methods (Tosa et al., 2008; Wu et al., 2009; Netzer et al., 2009; Zhou et al., 2010; Oliveira, 2012; Yan et al., 2013); 2) SMT approaches (Jiang and Zhou, 2008; Greene et al., 2010; He et al., 2012); 3) deep neural models (Zhang and Lapata, 2014; Wang et al., 2016b; Yan, 2016). Compared to rule-based and SMT models, neural models are able to learn more complicated representations and generate smooth poems. Most recent studies followed this paradigm. For example, Wang et al. (2016c) proposed a modified encoder-decoder model with keyword planning; Zhang et al. (2017a) adopted memory-augmented RNNs to dynamically choose each term from RNN output or a reserved inventory. To improve thematic consistency, Yang et al. (2017) combined CVAE and keywords plan"
D18-1423,D16-1032,0,0.0321585,"e objective of CVAE is thus to maximize the reconstruction log-likelihood of the input x under the condition of c. Following the operation for VAE, we have the corresponding variational lower bound of pθ (x|c) formulated as L(θ, φ; x, c) = − KL(qφ (z|x, c) k pθ (z|c)) + Eqφ (z|x,c) [log pθ (x|z, c)] (2) which is similar to Eq.1 except that all items are introduced with c, such as qφ (z|x, c) and pθ (z|c), 3891 referring to the conditioned approximate posterior and the conditioned prior, respectively. 2.2 Problem Formulation Following the text-to-text generation paradigm (Ranzato et al., 2015; Kiddon et al., 2016; Hu et al., 2017; Ghosh et al., 2017), our task has a similar problem setting with conventional studies (Zhang and Lapata, 2014; Wang et al., 2016c), where a poem is generated in a line-by-line manner that each line serves as the input for the next one, as illustrated in Figure 1. To formulate this task, we separate its input and output with necessary notations as follows. The I NPUT of the entire model is a title, T =(e1 ,e2 ,. . . ,eN ), functionalized as the theme of the target poem2 , where ei refers to i-the character’s embedding and N is the length of the title. The first line L1 is gen"
D18-1423,N16-1014,0,0.0818547,"is paper. Similarity: For thematic consistency, it is challenging to automatically evaluate different models. We adopt the embedding average metric to score sentence-level similarity as that was applied in Wieting et al. (2015). In this paper, we accumulate the embeddings of all characters from the generated poems and that from the given title, and use cosine to compute the similarity between the two accumulated embeddings. Distinctness: As an important characteristic, poems use novel and unique characters to maintain their elegance and delicacy. Similar to that proposed for dialogue systems (Li et al., 2016), this evaluation is employed to measure character diversity by calculating the proportion of distinctive [1,4]-grams12 in the generated poems, where final distinctness values are normalized to [0,100]. Human Evaluation: Since writing poems is a complicated task, there always exist incoordinations between automatic metrics and human experiences. Hence, we conduct human evaluation to 11 We tried different values for λ, varying from 0.001 to 1, which result in similar performance of the CVAE-D. 12 Defined as the number of distinctive n-grams divided by the total number of n-grams, shown as Dist-"
D18-1423,P15-1107,0,0.0609514,"Missing"
D18-1423,D17-1230,0,0.0259758,"s may deliver different mood from others. Since our model does not explicitly control such attributes, thus one potential solution to address this issue is to introduce other features to model such information, which requires a special design to adjust the current model. We also notice there exists a few extraordinary bad cases where their basic characteristics, such as wording, fluency, etc., are unacceptable. This phenomenon is randomly observed with no patterns, which could be explained by the complexity of the model and the fragile natural of adversarial training (Goodfellow et al., 2014; Li et al., 2017). Careful parameter setting and considerate module assemble could mitigate this problem, thus lead to potential future work of designing more robust frameworks. 6 Related Work Deep Generative Models. This work can be seen as an extension of research on deep generative models (Salakhutdinov and Hinton, 2009; Bengio et al., 2014), where most of the previous work, including VAE and CVAE, focused on image generation (Sohn et al., 2015; Yan et al., 2016b). Since GAN (Goodfellow et al., 2014) is also a successful generative model, there are studies tried to integrate VAE and GAN (Larsen et al., 2016"
D18-1423,P02-1040,0,0.102408,"2.16 2.14 2.58 2.29 2.08 2.53 2.35 2.34 2.96 Ovr. 1.74 1.79 2.13 2.23 2.13 2.14 2.18 2.56 Table 3: Results of automatic and human evaluations. BLEU-1 and BLEU-2 are BLEU scores on unigrams and bigrams (p < 0.01); Sim refer to the similarity score; Dist-n corresponds to the distinctness of n-gram, with n = 1 to 4; Con., Flu., Mea., Poe., Ovr. represent consistency, fluency, meaning, poeticness, and overall, respectively. 2017) to set the balancing parameter λ to 0.1.11 4.4 Evaluation Metrics To comprehensively evaluate the generated poems, we employ the following metrics: BLEU: The BLEU score (Papineni et al., 2002) is an effective metric, widely used in machine translation, for measuring word overlapping between ground truth and generated sentences. In poem generation, BLEU is also utilized as a metric in previous studies (Zhang and Lapata, 2014; Wang et al., 2016a; Yan, 2016; Wang et al., 2016b). We follow their settings in this paper. Similarity: For thematic consistency, it is challenging to automatically evaluate different models. We adopt the embedding average metric to score sentence-level similarity as that was applied in Wieting et al. (2015). In this paper, we accumulate the embeddings of all c"
D18-1423,C16-1100,0,0.272591,"that researchers of artificial intelligence are interested in (Tosa et al., 2008; Wu et al., 2009; Netzer et al., 2009; Oliveira, 2012; Yan et al., 2013, 2016a; Ghazvininejad et al., 2016, 2017; Singh et al., 2017; Xu et al., 2018), partially for the reason that poem generation and its related research could benefit other constrained natural language generation tasks. Conventionally, rule-based models (Zhou et al., 2010) and statistical machine translation (SMT) models (He et al., 2012) are proposed for this task. Recently, deep neural models are employed to generate fluent and natural poems (Wang et al., 2016a; Yan, 2016; Zhang et al., 2017a). Although these models look promising, they are limited in many aspects, e.g., previous studies generally fail to keep thematic consistency (Wang et al., 2016c; Yang et al., 2017) and improve term1 novelty (Zhang et al., 2017a), which are important characteristics of poems. In classical Chinese poem composing, thematic consistency and term novelty are usually mutually exclusive conditions to each other, i.e., consistent lines may bring duplicated terms while intriguing choices of characters could result in thematic diversities. On one hand, thematic consisten"
D18-1423,P17-1046,0,0.0143534,"aintaining the advantages of VAE. It is verified in supervised dialogue generation (Serban et al., 2017; Shen et al., 2017; Zhao et al., 2017) that CVAE can generate better responses with given dialogue contexts. Given the above background and to align it with our expectations for poem generation, it is worth trying to apply CVAE to create poems. In the meantime, consider that modeling thematic consistency with adversarial training is proven to be promising in controlled text generation (Hu et al., 2017), models for semantic matching can be potentially improved with an explicit discriminator (Wu et al., 2017), so does poem generation. In this paper, we propose a novel poem generation model (CVAE-D) using CVAE to generate novel terms and a discriminator (D) to explicitly control thematic consistency with adversarial training. To the best of our knowledge, this is the first work of generating poems with the combination of CVAE and adversarial training. Experiments on a large classical Chinese poetry corpus confirm that, through encoding inputs with latent variables and explicit measurement of thematic information, the proposed model outperforms existing ones in various evaluations. Quantitative and"
D18-1423,P17-1125,0,0.200231,"intelligence are interested in (Tosa et al., 2008; Wu et al., 2009; Netzer et al., 2009; Oliveira, 2012; Yan et al., 2013, 2016a; Ghazvininejad et al., 2016, 2017; Singh et al., 2017; Xu et al., 2018), partially for the reason that poem generation and its related research could benefit other constrained natural language generation tasks. Conventionally, rule-based models (Zhou et al., 2010) and statistical machine translation (SMT) models (He et al., 2012) are proposed for this task. Recently, deep neural models are employed to generate fluent and natural poems (Wang et al., 2016a; Yan, 2016; Zhang et al., 2017a). Although these models look promising, they are limited in many aspects, e.g., previous studies generally fail to keep thematic consistency (Wang et al., 2016c; Yang et al., 2017) and improve term1 novelty (Zhang et al., 2017a), which are important characteristics of poems. In classical Chinese poem composing, thematic consistency and term novelty are usually mutually exclusive conditions to each other, i.e., consistent lines may bring duplicated terms while intriguing choices of characters could result in thematic diversities. On one hand, thematic consistency is essential for poems; it is"
D18-1423,D14-1074,0,0.518039,"the operation for VAE, we have the corresponding variational lower bound of pθ (x|c) formulated as L(θ, φ; x, c) = − KL(qφ (z|x, c) k pθ (z|c)) + Eqφ (z|x,c) [log pθ (x|z, c)] (2) which is similar to Eq.1 except that all items are introduced with c, such as qφ (z|x, c) and pθ (z|c), 3891 referring to the conditioned approximate posterior and the conditioned prior, respectively. 2.2 Problem Formulation Following the text-to-text generation paradigm (Ranzato et al., 2015; Kiddon et al., 2016; Hu et al., 2017; Ghosh et al., 2017), our task has a similar problem setting with conventional studies (Zhang and Lapata, 2014; Wang et al., 2016c), where a poem is generated in a line-by-line manner that each line serves as the input for the next one, as illustrated in Figure 1. To formulate this task, we separate its input and output with necessary notations as follows. The I NPUT of the entire model is a title, T =(e1 ,e2 ,. . . ,eN ), functionalized as the theme of the target poem2 , where ei refers to i-the character’s embedding and N is the length of the title. The first line L1 is generated only conditioned on the title T , once this step is done, the model takes the input of the previous generated line as wel"
D18-1423,Q17-1036,0,0.166232,"intelligence are interested in (Tosa et al., 2008; Wu et al., 2009; Netzer et al., 2009; Oliveira, 2012; Yan et al., 2013, 2016a; Ghazvininejad et al., 2016, 2017; Singh et al., 2017; Xu et al., 2018), partially for the reason that poem generation and its related research could benefit other constrained natural language generation tasks. Conventionally, rule-based models (Zhou et al., 2010) and statistical machine translation (SMT) models (He et al., 2012) are proposed for this task. Recently, deep neural models are employed to generate fluent and natural poems (Wang et al., 2016a; Yan, 2016; Zhang et al., 2017a). Although these models look promising, they are limited in many aspects, e.g., previous studies generally fail to keep thematic consistency (Wang et al., 2016c; Yang et al., 2017) and improve term1 novelty (Zhang et al., 2017a), which are important characteristics of poems. In classical Chinese poem composing, thematic consistency and term novelty are usually mutually exclusive conditions to each other, i.e., consistent lines may bring duplicated terms while intriguing choices of characters could result in thematic diversities. On one hand, thematic consistency is essential for poems; it is"
D18-1423,P17-1061,0,0.472874,"obal properties such as syntactic, semantic, and discourse coherence (Li et al., 2015). Moreover, boosting autoencoder with variational inference (Kingma and Welling, 2014), known as variational autoencoder (VAE), can generate not only consistent but also novel and fluent term sequences (Bowman et al., 2016). To generalize VAE for versatile scenarios, conditional variational autoencoders (CVAE) are proposed to supervise a generation process with certain attributes while maintaining the advantages of VAE. It is verified in supervised dialogue generation (Serban et al., 2017; Shen et al., 2017; Zhao et al., 2017) that CVAE can generate better responses with given dialogue contexts. Given the above background and to align it with our expectations for poem generation, it is worth trying to apply CVAE to create poems. In the meantime, consider that modeling thematic consistency with adversarial training is proven to be promising in controlled text generation (Hu et al., 2017), models for semantic matching can be potentially improved with an explicit discriminator (Wu et al., 2017), so does poem generation. In this paper, we propose a novel poem generation model (CVAE-D) using CVAE to generate novel terms"
D18-1423,P16-1222,1,0.843427,"with no patterns, which could be explained by the complexity of the model and the fragile natural of adversarial training (Goodfellow et al., 2014; Li et al., 2017). Careful parameter setting and considerate module assemble could mitigate this problem, thus lead to potential future work of designing more robust frameworks. 6 Related Work Deep Generative Models. This work can be seen as an extension of research on deep generative models (Salakhutdinov and Hinton, 2009; Bengio et al., 2014), where most of the previous work, including VAE and CVAE, focused on image generation (Sohn et al., 2015; Yan et al., 2016b). Since GAN (Goodfellow et al., 2014) is also a successful generative model, there are studies tried to integrate VAE and GAN (Larsen et al., 2016). In natural language processing, many recent deep generative models are applied to dialogue systems Serban et al. (2017); Shen et al. (2017); Zhao et al. (2017) and text generation with (Hu et al., 2017; Yu et al., 2017; Lin et al., 2017; Zhang et al., 2017b; Guo et al., 2018). To the best of our knowledge, this work is the first one integrating CVAE and adversarial training with a discriminator for text generation, especially in a particular tex"
D18-1442,N18-1158,0,0.411279,"the focus on abstractive summarization, extractive summarization remains an attractive method as it is capable of generating more grammatically and semantically correct summaries. This is the method we follow in this work. In extractive summarization, Cheng and Lapata (2016) propose a general framework for single-document text summarization using a hierarchical article encoder composed with an attention-based extractor. Following this, Nallapati et al. (2016a) propose a simple RNN-based sequence classifier which outperforms or matches the state-of-art models at the time. In another approach, Narayan et al. (2018) use a reinforcement learning method to optimize the Rouge evaluation metric for text summarization. The most recent work on this topic is (Wu and Hu, 2018), where the authors train a reinforced neural extractive summarization model called RNES that captures cross-sentence coherence patterns. Due to the fact that they use a different dataset and have not released their code, we are unable to compare our models with theirs. The idea of iteration has not been well explored for summarization. One related study is Xiong et al. (2016)’s work on dynamic memory networks, which designs neural networks"
D18-1442,D14-1162,0,0.0836015,"ed our model in Tensorflow (Abadi et al., 2016). The code for our models is available online1 . We mostly followed the settings in (Nallapati et al., 2016a) and trained the model using the Adam optimizer (Kingma and Ba, 2014) with initial learning rate 0.001 and anneals of 0.5 every 6 epochs until reaching 30 epochs. We selected three sentences with highest scores as summary. After preliminary exploration, we found that arranging them according to their scores consistently achieved the best performance. Experiments were performed with a batch size of 64 documents. We used 100-dimension GloVe (Pennington et al., 2014) embeddings trained on Wikipedia 2014 as our embedding initialization with a vocabulary size limited to 100k for speed purposes. We initialized out-of-vocabulary word embeddings over a uniform distribution within [0.2,0,2]. We also padded or cut sentences to contain exactly 70 words. Each GRU module had 1 layer with 200-dimensional hidden states and with either an initial state set up as described above or a random initial state. To prevent overfitting, we used dropout after each GRU network and embedding layer, and also applied L2 loss to all unbiased variables. The iteration number was set t"
D18-1442,radev-etal-2004-mead,0,0.215319,"icle, removing secondary or redundant concepts. Nowadays as there is a growing need for storing and digesting large amounts of textual data, automatic summarization systems have significant usage potential in society. Extractive summarization is a technique for generating summaries by directly choosing a subset of salient sentences from the original document to constitute the summary. Most efforts made towards extractive summarization either rely ∗ Corresponding author: Rui Yan (ruiyan@pku.edu.cn) on human-engineered features such as sentence length, word position, and frequency (Cohen, 2002; Radev et al., 2004; Woodsend and Lapata, 2010; Yan et al., 2011a,b, 2012) or use neural networks to automatically learn features for sentence selection (Cheng and Lapata, 2016; Nallapati et al., 2016a). Although existing extractive summarization methods have achieved great success, one limitation they share is that they generate the summary after only one pass through the document. However, in real-world human cognitive processes, people read a document multiple times in order to capture the main ideas. Browsing through the document only once often means the model cannot fully get at the document’s main ideas,"
D18-1442,W05-0620,0,0.154828,"Missing"
D18-1442,D15-1044,0,0.0610103,"an be classified into extractive summarization and abstractive summarization. Extractive summarization aims to generate a summary by integrating the most salient sentences in the document. Abstractive summarization aims to generate new content that concisely paraphrases the document from scratch. With the emergence of powerful neural network models for text processing, a vast majority of the literature on document summarization is dedicated to abstractive summarization. These models typically take the form of convolutional neural networks (CNN) or recurrent neural networks (RNN). For example, Rush et al. (2015) propose an encoder-decoder model which uses a local attention mechanism to generate summaries. Nallapati et al. (2016b) further develop this work by addressing problems that had not been adequately solved by the basic architecture, such as keyword modeling and capturing the hierarchy of sentenceto-word structures. In a follow-up work, Nallapati et al. (2017) propose a new summarization model which generates summaries by sampling a topic one sentence at a time, then producing words using an RNN decoder conditioned on the sentence topic. Another related work is by See et al. (2017), where the a"
D18-1442,P16-1046,0,0.554299,"mmarization systems have significant usage potential in society. Extractive summarization is a technique for generating summaries by directly choosing a subset of salient sentences from the original document to constitute the summary. Most efforts made towards extractive summarization either rely ∗ Corresponding author: Rui Yan (ruiyan@pku.edu.cn) on human-engineered features such as sentence length, word position, and frequency (Cohen, 2002; Radev et al., 2004; Woodsend and Lapata, 2010; Yan et al., 2011a,b, 2012) or use neural networks to automatically learn features for sentence selection (Cheng and Lapata, 2016; Nallapati et al., 2016a). Although existing extractive summarization methods have achieved great success, one limitation they share is that they generate the summary after only one pass through the document. However, in real-world human cognitive processes, people read a document multiple times in order to capture the main ideas. Browsing through the document only once often means the model cannot fully get at the document’s main ideas, leading to a subpar summarization. We share two examples of this. (1) Consider the situation where we almost finish reading a long article and forget some ma"
D18-1442,E17-2007,0,0.0838447,"Missing"
D18-1442,P17-1099,0,0.114979,"or example, Rush et al. (2015) propose an encoder-decoder model which uses a local attention mechanism to generate summaries. Nallapati et al. (2016b) further develop this work by addressing problems that had not been adequately solved by the basic architecture, such as keyword modeling and capturing the hierarchy of sentenceto-word structures. In a follow-up work, Nallapati et al. (2017) propose a new summarization model which generates summaries by sampling a topic one sentence at a time, then producing words using an RNN decoder conditioned on the sentence topic. Another related work is by See et al. (2017), where the authors use “pointing” and “coverage” techniques to generate more accurate summaries. Despite the focus on abstractive summarization, extractive summarization remains an attractive method as it is capable of generating more grammatically and semantically correct summaries. This is the method we follow in this work. In extractive summarization, Cheng and Lapata (2016) propose a general framework for single-document text summarization using a hierarchical article encoder composed with an attention-based extractor. Following this, Nallapati et al. (2016a) propose a simple RNN-based se"
D18-1442,P10-1058,0,0.110183,"dary or redundant concepts. Nowadays as there is a growing need for storing and digesting large amounts of textual data, automatic summarization systems have significant usage potential in society. Extractive summarization is a technique for generating summaries by directly choosing a subset of salient sentences from the original document to constitute the summary. Most efforts made towards extractive summarization either rely ∗ Corresponding author: Rui Yan (ruiyan@pku.edu.cn) on human-engineered features such as sentence length, word position, and frequency (Cohen, 2002; Radev et al., 2004; Woodsend and Lapata, 2010; Yan et al., 2011a,b, 2012) or use neural networks to automatically learn features for sentence selection (Cheng and Lapata, 2016; Nallapati et al., 2016a). Although existing extractive summarization methods have achieved great success, one limitation they share is that they generate the summary after only one pass through the document. However, in real-world human cognitive processes, people read a document multiple times in order to capture the main ideas. Browsing through the document only once often means the model cannot fully get at the document’s main ideas, leading to a subpar summari"
D18-1442,W04-3212,0,0.0184597,"tructed sentence representations. The iterative unit (also depicted above in Fig.1) is designed for this purpose. We use a GRUiter cell to generate the polished document representation, whose input is the final state of the selective reading network from the previous iteration, hns and whose initial state is set to the document representation of the previous iteration, Dk−1 . The updated document representation is computed by: Dk = GRUiter (hns , Dk−1 ) 4.3 (14) Decoder Next, we describe our decoders, which are depicted shaded in the right part of Fig.1. Following most sequence labeling task (Xue and Palmer, 2004; Carreras and M`arquez, 2005) where they learn a feature vector for each sentence, we use a bidirectional GRUdec network in each iteration to output features so as to calculate extracting probabilities. For k-th iteration, given the sentence rep→ resentation ← s as input and the document representation Dk as the initial state, our decoder encodes the features of all sentences in the hidden state hk = {hk0 , ..., hkns }: 4.4 → hki = GRUdec (← s , hki−1 ) (15) hk0 (16) = Dk Sentence Labeling Module Next, we use the feature of each sentence to generate corresponding extracting probability. Since"
D18-1442,P16-1222,1,0.822294,"ence patterns. Due to the fact that they use a different dataset and have not released their code, we are unable to compare our models with theirs. The idea of iteration has not been well explored for summarization. One related study is Xiong et al. (2016)’s work on dynamic memory networks, which designs neural networks with memory and attention mechanisms that exhibit certain reasoning capabilities required for question answering. Another related work is (Yan, 2016), where they generate poetry with iterative polishing sn chema. Similiar method can also be applied on couplet generation as in (Yan et al., 2016). We take some inspiration from their work but focus on document summarization. Another related work is (Singh et al., 2017), where the authors present a deep network called Hybrid MemNet for the single document summarization task, using a memory network as the document encoder. Compared to them, we do not borrow the memory network structure but propose a new iterative architecture. 4089 3 3.1 Methodology Problem Formulation In this work, we propose Iterative Text Summarization (ITS), an iteration-based supervised model for extractive text summarization. We treat the extractive summarization t"
D18-1442,D11-1124,1,0.758224,"Nowadays as there is a growing need for storing and digesting large amounts of textual data, automatic summarization systems have significant usage potential in society. Extractive summarization is a technique for generating summaries by directly choosing a subset of salient sentences from the original document to constitute the summary. Most efforts made towards extractive summarization either rely ∗ Corresponding author: Rui Yan (ruiyan@pku.edu.cn) on human-engineered features such as sentence length, word position, and frequency (Cohen, 2002; Radev et al., 2004; Woodsend and Lapata, 2010; Yan et al., 2011a,b, 2012) or use neural networks to automatically learn features for sentence selection (Cheng and Lapata, 2016; Nallapati et al., 2016a). Although existing extractive summarization methods have achieved great success, one limitation they share is that they generate the summary after only one pass through the document. However, in real-world human cognitive processes, people read a document multiple times in order to capture the main ideas. Browsing through the document only once often means the model cannot fully get at the document’s main ideas, leading to a subpar summarization. We share t"
D19-1023,D18-1307,0,0.0230206,"al Linguistics lation alignment as a by-product. We evaluate our approach by applying it to three real-world datasets. Experimental results show that our approach delivers better and more robust results when compared with state-of-the-art methods for entity and relation alignments. The key contribution of this paper is a novel joint learning model for entity and relation alignments. Our approach reduces the human involvement and the associated cost in constructing seed alignments, but yields better performance over prior works. improve tasks like information extraction (Miwa and Bansal, 2016; Bekoulis et al., 2018). We hypothesize that this will be the case for entity alignment too; that is, the rich relation information could be useful for improving entity alignment as entities and their relations are usually closely related. Our experiments show that this is even a conservative target: by jointly learning entity and relation representations, we can promote the results of both entity and relation alignment. In this work, we aim to build a learning framework that jointly learns entity and relation representations for entity alignment; and we want to achieve this with only a small set of pre-aligned enti"
D19-1023,D18-1248,0,0.347336,", the task of linking entities with the same real-world identity from different KGs. Classical methods for entity alignment typically involve a labor-intensive and time-consuming process of feature construction (Mahdisoltani et al., 2013) or rely on external information constructed by others (Suchanek et al., 2011). Recently, efforts have been devoted to the so-called embeddingbased approaches. Representative works of this direction include JE (Hao et al., 2016), MTransE (Chen et al., 2017), JAPE (Sun et al., 2017), IPTransE (Zhu et al., 2017), and BootEA (Sun et al., 2018). More recent work (Wang et al., 2018b) uses the Graph Convolutional Network (GCN) (Kipf and Welling, 2017) to jointly embed multiple KGs. Most of the recent works (e.g., JE, MTransE, JAPE, IPTransE and BootEA) rely on the translation-based models, such as TransE (Bordes et al., 2013), which enable these approaches to encode both entities and relations of KGs. These methods often put more emphasis on the entity embeddings, but do not explicitly utilize relation embeddings to help with entity alignment. Another drawback of such approaches is that they usually rely on pre-aligned relations (JAPE and IPTransE) or triples (MTransE)."
D19-1023,D17-1159,0,0.0221954,"ed on entity representations. By linking entity representations with relation representations, they promote each other in our framework and ultimately achieve better alignment results. Graph Convolutional Networks GCNs (Duvenaud et al., 2015; Kearnes et al., 2016; Kipf and Welling, 2017) are neural networks operating on unlabeled graphs and inducing features of nodes based on the structures of their neighborhoods. Recently, GCNs have demonstrated promising performance in tasks like node classification (Kipf and Welling, 2017), relation extraction (Zhang et al., 2018a), semantic role labeling (Marcheggiani and Titov, 2017), etc. As an extension of GCNs, the R-GCNs (Schlichtkrull et al., 2018) have recently been proposed to model relational data for link prediction and entity classification. However, R-GCNs usually require a large number of parameters that are often hard to train, when applied to multi-relational graphs. In this work, we choose to use GCNs to first encode KG entities and to approximate relation representations based on entity embeddings. Our work is the first to utilize GCNs for jointly aligning entities and relations for heterogeneous KGs. 3 Our Approach 4.1 Overall Architecture As illustrated"
D19-1023,P16-1105,0,0.0203275,"ciation for Computational Linguistics lation alignment as a by-product. We evaluate our approach by applying it to three real-world datasets. Experimental results show that our approach delivers better and more robust results when compared with state-of-the-art methods for entity and relation alignments. The key contribution of this paper is a novel joint learning model for entity and relation alignments. Our approach reduces the human involvement and the associated cost in constructing seed alignments, but yields better performance over prior works. improve tasks like information extraction (Miwa and Bansal, 2016; Bekoulis et al., 2018). We hypothesize that this will be the case for entity alignment too; that is, the rich relation information could be useful for improving entity alignment as entities and their relations are usually closely related. Our experiments show that this is even a conservative target: by jointly learning entity and relation representations, we can promote the results of both entity and relation alignment. In this work, we aim to build a learning framework that jointly learns entity and relation representations for entity alignment; and we want to achieve this with only a small"
D19-1023,D18-1032,0,0.252046,", the task of linking entities with the same real-world identity from different KGs. Classical methods for entity alignment typically involve a labor-intensive and time-consuming process of feature construction (Mahdisoltani et al., 2013) or rely on external information constructed by others (Suchanek et al., 2011). Recently, efforts have been devoted to the so-called embeddingbased approaches. Representative works of this direction include JE (Hao et al., 2016), MTransE (Chen et al., 2017), JAPE (Sun et al., 2017), IPTransE (Zhu et al., 2017), and BootEA (Sun et al., 2018). More recent work (Wang et al., 2018b) uses the Graph Convolutional Network (GCN) (Kipf and Welling, 2017) to jointly embed multiple KGs. Most of the recent works (e.g., JE, MTransE, JAPE, IPTransE and BootEA) rely on the translation-based models, such as TransE (Bordes et al., 2013), which enable these approaches to encode both entities and relations of KGs. These methods often put more emphasis on the entity embeddings, but do not explicitly utilize relation embeddings to help with entity alignment. Another drawback of such approaches is that they usually rely on pre-aligned relations (JAPE and IPTransE) or triples (MTransE)."
D19-1023,P17-1132,0,0.0272134,"e them using entity embeddings learned by the GCN. We then incorporate the relation approximation into entities to iteratively learn better representations for both. Experiments performed on three real-world cross-lingual datasets show that our approach substantially outperforms state-of-the-art entity alignment methods. 1 Introduction Knowledge graphs (KGs) transform unstructured knowledge into simple and clear triples of <head entity, relation, tail entity> for rapid response and reasoning of knowledge. They are an effective way for supporting various NLP-enabled tasks like machine reading (Yang and Mitchell, 2017), information extraction (Wang et al., 2018a), and question-answering (Zhang et al., 2018b). Even though many KGs originate from the same resource, e.g., Wikipedia, they are usually created independently. Therefore, different KGs often use ∗ Corresponding author. 240 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 240–249, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics lation alignment as a by-product. We evaluate our approach by applyi"
D19-1023,P18-1187,0,0.0413936,"ions, and the black solid lines denote the process of continuing using GCNs to iteratively learn better entity and relation representations. between aligned entity pairs to be as close as possible, and the distance between positive and negative alignment pairs to be as large as possible. The loss function is defined as: ing forward propagation as: 1 1 ˜ − 2 X(l) W(l) ), ˜ − 2 A˜D X(l+1) = ReLU(D (1) where A˜ = A + I is the adjacency matrix of Ga with self-connections, I is an identity matrix, ˜ jj = P A˜jk , and W(l) ∈ Rd(l) ×d(l+1) is a layerD k specific trainable weight matrix. Inspired by (Rahimi et al., 2018) that uses highway gates (Srivastava et al., 2015) to control the noise propagation in GCNs for geographic localization, we also employ layer-wise highway gates to build a Highway-GCN (HGCN) model. Our layer-wise gates work as follow: (l) (l) T (X(l) ) = σ(X(l) WT + bT ), L= X max{0, d(p, q)−d(p0 , q 0 )+γ}, (p,q)∈L (p0 ,q 0 )∈L0 (5) where γ > 0 is a margin hyper-parameter; L0 stands for the negative alignment set of L. Rather than simply random sampling for negative instances, we look for more challenging negative samples, e.g., those with subtle differences from the positive ones, to train o"
D19-1023,D18-1244,0,0.107257,"on into entities to iteratively learn better representations for both. Experiments performed on three real-world cross-lingual datasets show that our approach substantially outperforms state-of-the-art entity alignment methods. 1 Introduction Knowledge graphs (KGs) transform unstructured knowledge into simple and clear triples of <head entity, relation, tail entity> for rapid response and reasoning of knowledge. They are an effective way for supporting various NLP-enabled tasks like machine reading (Yang and Mitchell, 2017), information extraction (Wang et al., 2018a), and question-answering (Zhang et al., 2018b). Even though many KGs originate from the same resource, e.g., Wikipedia, they are usually created independently. Therefore, different KGs often use ∗ Corresponding author. 240 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 240–249, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics lation alignment as a by-product. We evaluate our approach by applying it to three real-world datasets. Experimental results show that our approach delivers"
D19-1128,W15-4640,0,0.41228,"dely used random sampling strategy, although the first two strategies lead to performance drop, the latter two ones can bring consistent improvement to the performance of all the models on both benchmarks. 1 Introduction In this work, we study the problem of response selection as an approach to implementing a retrievalbased dialogue system (Ji et al., 2014; Wang et al., 2013). A key step in response selection is measuring the matching degree between a conversation context and a response candidate. Existing studies focus on constructing a matching model with sophisticated neural architectures (Lowe et al., 2015; Zhou et al., 2016; Yan et al., 2016; Wu et al., 2017; Zhang et al., 2018; Zhou et al., 2018; Tao et al., 2019), but pay little attention to how to effectively learn such architectures from data. On the one hand, it is well known that learning of complicated neural architectures requires large-scale high quality training data; on the other hand, since human labeling is expensive and exhausting, most of the existing work just adopts a simple heuristic to automatically build a training set where human responses are treated as positive examples and negative response candidates are randomly sampl"
D19-1128,W17-7301,0,0.123569,"ative sampling strategies have been studied in many machine learning tasks. In the computer vision fields, Faghri et al. (2017) studies hard negatives and introduces a simple change to common loss function on image-caption retrieval tasks. Guo et al. (2018) proposes a fast negative sampler which chooses negative examples that are most likely to meet the requirements of violation according to the latent factors of image. In natural language processing fields, Kotnis and Nastase (2017) analyses the impact of negative sampling strategies on the performance of link prediction in knowledge graphs. Saeidi et al. (2017) studies the affect of a tailored sample strategy on the performance of document retrieval task. Rao et al. (2016) uses three negative strategies to select the most informative negative samples on the pairwise ranking model for answer selection. Xu et al. (2015) introduces a straightforward negative sampling strategy to improve the assignment of subjects and objects on a convolution neural network. To our best knowledge, this is the first work to empirical study of negative sampling strategies for learning of matching models in multi-turn retrieval-based dialogue systems, which may enlighten f"
D19-1128,P19-1001,1,0.819447,"nes can bring consistent improvement to the performance of all the models on both benchmarks. 1 Introduction In this work, we study the problem of response selection as an approach to implementing a retrievalbased dialogue system (Ji et al., 2014; Wang et al., 2013). A key step in response selection is measuring the matching degree between a conversation context and a response candidate. Existing studies focus on constructing a matching model with sophisticated neural architectures (Lowe et al., 2015; Zhou et al., 2016; Yan et al., 2016; Wu et al., 2017; Zhang et al., 2018; Zhou et al., 2018; Tao et al., 2019), but pay little attention to how to effectively learn such architectures from data. On the one hand, it is well known that learning of complicated neural architectures requires large-scale high quality training data; on the other hand, since human labeling is expensive and exhausting, most of the existing work just adopts a simple heuristic to automatically build a training set where human responses are treated as positive examples and negative response candidates are randomly sampled. ∗ Corresponding author: Rui Yan (ruiyan@pku.edu.cn). Such a training set might contain many false negatives"
D19-1128,D13-1096,0,0.350074,"ing models in learning, we consider four strategies including minimum sampling, maximum sampling, semi-hard sampling, and decay-hard sampling. Empirical studies on two benchmarks with three matching models indicate that compared with the widely used random sampling strategy, although the first two strategies lead to performance drop, the latter two ones can bring consistent improvement to the performance of all the models on both benchmarks. 1 Introduction In this work, we study the problem of response selection as an approach to implementing a retrievalbased dialogue system (Ji et al., 2014; Wang et al., 2013). A key step in response selection is measuring the matching degree between a conversation context and a response candidate. Existing studies focus on constructing a matching model with sophisticated neural architectures (Lowe et al., 2015; Zhou et al., 2016; Yan et al., 2016; Wu et al., 2017; Zhang et al., 2018; Zhou et al., 2018; Tao et al., 2019), but pay little attention to how to effectively learn such architectures from data. On the one hand, it is well known that learning of complicated neural architectures requires large-scale high quality training data; on the other hand, since human"
D19-1128,P18-2067,1,0.725816,"igh quality training data; on the other hand, since human labeling is expensive and exhausting, most of the existing work just adopts a simple heuristic to automatically build a training set where human responses are treated as positive examples and negative response candidates are randomly sampled. ∗ Corresponding author: Rui Yan (ruiyan@pku.edu.cn). Such a training set might contain many false negatives and trivial true negatives that are very easy to distinguish from those true positives. As a result, models with advanced architectures can only reach sub-optimal performance after learning (Wu et al., 2018). In this paper, instead of configuring new architectures, we investigate how to improve the performance of existing matching models with a better learning method. A learning method usually involves choice of loss functions and construction of training data, and we are particularly interested in automatic training data construction, as data are often more crucial to the performance of models. The key problem in training data construction lies in how to properly choose negative examples, and our idea is that negative examples should adapt to the matching models at different learning stages. Fol"
D19-1128,P17-1046,1,0.961365,"wo strategies lead to performance drop, the latter two ones can bring consistent improvement to the performance of all the models on both benchmarks. 1 Introduction In this work, we study the problem of response selection as an approach to implementing a retrievalbased dialogue system (Ji et al., 2014; Wang et al., 2013). A key step in response selection is measuring the matching degree between a conversation context and a response candidate. Existing studies focus on constructing a matching model with sophisticated neural architectures (Lowe et al., 2015; Zhou et al., 2016; Yan et al., 2016; Wu et al., 2017; Zhang et al., 2018; Zhou et al., 2018; Tao et al., 2019), but pay little attention to how to effectively learn such architectures from data. On the one hand, it is well known that learning of complicated neural architectures requires large-scale high quality training data; on the other hand, since human labeling is expensive and exhausting, most of the existing work just adopts a simple heuristic to automatically build a training set where human responses are treated as positive examples and negative response candidates are randomly sampled. ∗ Corresponding author: Rui Yan (ruiyan@pku.edu.cn"
D19-1128,D15-1062,1,0.889697,"Missing"
D19-1128,C18-1317,0,0.0686812,"d to performance drop, the latter two ones can bring consistent improvement to the performance of all the models on both benchmarks. 1 Introduction In this work, we study the problem of response selection as an approach to implementing a retrievalbased dialogue system (Ji et al., 2014; Wang et al., 2013). A key step in response selection is measuring the matching degree between a conversation context and a response candidate. Existing studies focus on constructing a matching model with sophisticated neural architectures (Lowe et al., 2015; Zhou et al., 2016; Yan et al., 2016; Wu et al., 2017; Zhang et al., 2018; Zhou et al., 2018; Tao et al., 2019), but pay little attention to how to effectively learn such architectures from data. On the one hand, it is well known that learning of complicated neural architectures requires large-scale high quality training data; on the other hand, since human labeling is expensive and exhausting, most of the existing work just adopts a simple heuristic to automatically build a training set where human responses are treated as positive examples and negative response candidates are randomly sampled. ∗ Corresponding author: Rui Yan (ruiyan@pku.edu.cn). Such a training s"
D19-1128,D16-1036,1,0.902016,"Missing"
D19-1128,P18-1103,0,0.783361,"p, the latter two ones can bring consistent improvement to the performance of all the models on both benchmarks. 1 Introduction In this work, we study the problem of response selection as an approach to implementing a retrievalbased dialogue system (Ji et al., 2014; Wang et al., 2013). A key step in response selection is measuring the matching degree between a conversation context and a response candidate. Existing studies focus on constructing a matching model with sophisticated neural architectures (Lowe et al., 2015; Zhou et al., 2016; Yan et al., 2016; Wu et al., 2017; Zhang et al., 2018; Zhou et al., 2018; Tao et al., 2019), but pay little attention to how to effectively learn such architectures from data. On the one hand, it is well known that learning of complicated neural architectures requires large-scale high quality training data; on the other hand, since human labeling is expensive and exhausting, most of the existing work just adopts a simple heuristic to automatically build a training set where human responses are treated as positive examples and negative response candidates are randomly sampled. ∗ Corresponding author: Rui Yan (ruiyan@pku.edu.cn). Such a training set might contain ma"
D19-1199,I17-2028,0,0.0211377,"ing for utterances, another key issue is to model multiple participants in the conversations. It is intuitive to introduce multiple user embeddings for multi-party conversations, either as persona-dependent embeddings (Li et al., 2016b), or as persona-independent embeddings (Ouchi and Tsuboi, 2016; Zhang et al., 2017; Meng et al., 2017). Recently, some researchers utilized users’ 1 To make the model practical in learning, we assume that one utterance is associated with only one addressee. information based on different roles in conversations, such as senders and recipients (Chen et al., 2017; Chi et al., 2017; Luan et al., 2016). In multi-party conversations, identifying the relationship among users is also an important task. It can be categorized into two topics, 1) predicting who will be the next speaker (Meng et al., 2017) and 2) who is the addressee (Ouchi and Tsuboi, 2016; Zhang et al., 2017). For the first topic, Meng et al. (2017) investigated a temporal-based and a content-based method to jointly model the users and context. For the second topic, which is closely related to ours, Ouchi and Tsuboi (2016) proposed to predict the addressee and utterance given a context with all available info"
D19-1199,D14-1179,0,0.00765946,"Missing"
D19-1199,C16-1073,1,0.885519,"Missing"
D19-1199,D17-1230,0,0.0155911,"oach to jointly learn the representations of users and utterances and enhance them mutually. • The proposed approach (W2W) considers both previous and subsequent information in the session while incorporating the correlation with users and utterances. For conversations with complex structures, W2W models them in a uniform way and could handle any kind of occasion even when all the addressee information is missing. 2 Related Work In this section, we briefly review recent works and progresses on multi-party conversations. Multi-party conversations, as a general case of multi-turn conversations (Li et al., 2017, 2016c; Yan et al., 2016; Serban et al., 2016) involve more than two participants. In addition to the representation of learning for utterances, another key issue is to model multiple participants in the conversations. It is intuitive to introduce multiple user embeddings for multi-party conversations, either as persona-dependent embeddings (Li et al., 2016b), or as persona-independent embeddings (Ouchi and Tsuboi, 2016; Zhang et al., 2017; Meng et al., 2017). Recently, some researchers utilized users’ 1 To make the model practical in learning, we assume that one utterance is associated with"
D19-1199,D16-1231,0,0.148673,"2 Related Work In this section, we briefly review recent works and progresses on multi-party conversations. Multi-party conversations, as a general case of multi-turn conversations (Li et al., 2017, 2016c; Yan et al., 2016; Serban et al., 2016) involve more than two participants. In addition to the representation of learning for utterances, another key issue is to model multiple participants in the conversations. It is intuitive to introduce multiple user embeddings for multi-party conversations, either as persona-dependent embeddings (Li et al., 2016b), or as persona-independent embeddings (Ouchi and Tsuboi, 2016; Zhang et al., 2017; Meng et al., 2017). Recently, some researchers utilized users’ 1 To make the model practical in learning, we assume that one utterance is associated with only one addressee. information based on different roles in conversations, such as senders and recipients (Chen et al., 2017; Chi et al., 2017; Luan et al., 2016). In multi-party conversations, identifying the relationship among users is also an important task. It can be categorized into two topics, 1) predicting who will be the next speaker (Meng et al., 2017) and 2) who is the addressee (Ouchi and Tsuboi, 2016; Zhang e"
D19-1199,D14-1162,0,0.0858018,"descending order according to the first time when they speak, and the i-th user is assigned with the i-th row of A(0) as ai(0) . The user matrix A(0) is trained as parameters along with other weight matrices in the neural network. Users of the same order in different sessions share the same initialization user embedding. Note that the user representations are independent of each personality (unique user). Such strategy guarantees the initialization user embeddings to carry position information as well as handle new users unseen in training data during addressee identification. 3 We use GloVe(Pennington et al., 2014), but it can be any word embeddings(Mikolov et al., 2013; Hu et al., 2016). 4 Such a hierarchical framework (Serban et al., 2016) takes into account the context of all previous and future sentences in the whole session, thus enables the model to learn a strong representation. 1911 ???? ????????????? Forward a?(?) a?(?) a?(?) (?1 , a???? ) 0 a?(?) a?(?) ? ????2 ) 0 a?(?) a?(?) ? ????3 ) 0 Utterance ? ? ????1 ) 2 a? a?(?) a? ????2 ) 2 (?32 , a a?(?) ????3 ) 1 (?23 , a Backward a? (?31 , a ? (?2 , a???? ) 1 a?(?) (?13 , a User ? ???? (?21 , a 1 1 ) a?(?) (?12 , a ?(0) a?(?) a? a? (?3 , a???? ) 2"
D19-1199,N16-1014,0,0.230717,"hich models users and utterances in the session jointly in an interactive way. We conduct experiments on the benchmark Ubuntu Multi-Party Conversation Corpus and the experimental results demonstrate that our model outperforms baselines with consistent improvements. 1 Speaker User 1 User 1 User 2 User 3 User 4 Introduction As an essential aspect of artificial intelligence, dialogue systems have attracted extensive attention in recent studies (Vinyals and Le, 2015; Serban et al., 2016). Researchers have paid great efforts to understand conversations between two participants, either single-turn (Li et al., 2016a; Shang et al., 2015; Vinyals and Le, 2015) or multi-turn (Zhou et al., 2016; Yan et al., 2016; Tao et al., 2019a,b), and achieved encouraging results. A more general and challenging scenario is that a conversation may involve more than two interlocutors conversing among each other (Uthus and Aha, 2013; Hu et al., 2019), which is known as multi-party conversation. Ubuntu Internet Relay Chat channel (IRC) is a multi-party conversation scenario as shown in Table 1. Generally, each utterance is associated with a speaker and one or more addressees in the conversation. Such a characteristic ∗ † Eq"
D19-1199,P15-1152,0,0.0970801,"Missing"
D19-1199,P16-1094,0,0.136522,"hich models users and utterances in the session jointly in an interactive way. We conduct experiments on the benchmark Ubuntu Multi-Party Conversation Corpus and the experimental results demonstrate that our model outperforms baselines with consistent improvements. 1 Speaker User 1 User 1 User 2 User 3 User 4 Introduction As an essential aspect of artificial intelligence, dialogue systems have attracted extensive attention in recent studies (Vinyals and Le, 2015; Serban et al., 2016). Researchers have paid great efforts to understand conversations between two participants, either single-turn (Li et al., 2016a; Shang et al., 2015; Vinyals and Le, 2015) or multi-turn (Zhou et al., 2016; Yan et al., 2016; Tao et al., 2019a,b), and achieved encouraging results. A more general and challenging scenario is that a conversation may involve more than two interlocutors conversing among each other (Uthus and Aha, 2013; Hu et al., 2019), which is known as multi-party conversation. Ubuntu Internet Relay Chat channel (IRC) is a multi-party conversation scenario as shown in Table 1. Generally, each utterance is associated with a speaker and one or more addressees in the conversation. Such a characteristic ∗ † Eq"
D19-1199,D19-1011,0,0.257771,"chmark Ubuntu Multi-Party Conversation Corpus and the experimental results demonstrate that our model outperforms baselines with consistent improvements. 1 Speaker User 1 User 1 User 2 User 3 User 4 Introduction As an essential aspect of artificial intelligence, dialogue systems have attracted extensive attention in recent studies (Vinyals and Le, 2015; Serban et al., 2016). Researchers have paid great efforts to understand conversations between two participants, either single-turn (Li et al., 2016a; Shang et al., 2015; Vinyals and Le, 2015) or multi-turn (Zhou et al., 2016; Yan et al., 2016; Tao et al., 2019a,b), and achieved encouraging results. A more general and challenging scenario is that a conversation may involve more than two interlocutors conversing among each other (Uthus and Aha, 2013; Hu et al., 2019), which is known as multi-party conversation. Ubuntu Internet Relay Chat channel (IRC) is a multi-party conversation scenario as shown in Table 1. Generally, each utterance is associated with a speaker and one or more addressees in the conversation. Such a characteristic ∗ † Equal contribution. Corresponding author. Utterance ”Good point, tmux is the thing I miss.” ”Cool thanks for ur hel"
D19-1199,D16-1127,0,0.179163,"hich models users and utterances in the session jointly in an interactive way. We conduct experiments on the benchmark Ubuntu Multi-Party Conversation Corpus and the experimental results demonstrate that our model outperforms baselines with consistent improvements. 1 Speaker User 1 User 1 User 2 User 3 User 4 Introduction As an essential aspect of artificial intelligence, dialogue systems have attracted extensive attention in recent studies (Vinyals and Le, 2015; Serban et al., 2016). Researchers have paid great efforts to understand conversations between two participants, either single-turn (Li et al., 2016a; Shang et al., 2015; Vinyals and Le, 2015) or multi-turn (Zhou et al., 2016; Yan et al., 2016; Tao et al., 2019a,b), and achieved encouraging results. A more general and challenging scenario is that a conversation may involve more than two interlocutors conversing among each other (Uthus and Aha, 2013; Hu et al., 2019), which is known as multi-party conversation. Ubuntu Internet Relay Chat channel (IRC) is a multi-party conversation scenario as shown in Table 1. Generally, each utterance is associated with a speaker and one or more addressees in the conversation. Such a characteristic ∗ † Eq"
D19-1199,D16-1036,1,0.896926,"ay. We conduct experiments on the benchmark Ubuntu Multi-Party Conversation Corpus and the experimental results demonstrate that our model outperforms baselines with consistent improvements. 1 Speaker User 1 User 1 User 2 User 3 User 4 Introduction As an essential aspect of artificial intelligence, dialogue systems have attracted extensive attention in recent studies (Vinyals and Le, 2015; Serban et al., 2016). Researchers have paid great efforts to understand conversations between two participants, either single-turn (Li et al., 2016a; Shang et al., 2015; Vinyals and Le, 2015) or multi-turn (Zhou et al., 2016; Yan et al., 2016; Tao et al., 2019a,b), and achieved encouraging results. A more general and challenging scenario is that a conversation may involve more than two interlocutors conversing among each other (Uthus and Aha, 2013; Hu et al., 2019), which is known as multi-party conversation. Ubuntu Internet Relay Chat channel (IRC) is a multi-party conversation scenario as shown in Table 1. Generally, each utterance is associated with a speaker and one or more addressees in the conversation. Such a characteristic ∗ † Equal contribution. Corresponding author. Utterance ”Good point, tmux is the th"
D19-1201,P16-1094,0,0.617678,"ion. We denote the trained user embeddings as U = {u1 , u2 , . . . , ui } where ui represents the vector representations of i-th user (User i). Based on the user embeddings as U, we utilize learned user personalizations in the latent space. Specifically, the conditional prior distribution of WAE part is a Gaussian mixture distribution (GMD) conditioned on the learned user embeddings, namely personalization GMD. We formulate the conditional prior as: p(zu |ui ) = K X vk N (zu ; µk , σk2 I) (4) k=1 2.3 Problem Formulation We follow the conventional personalized conversation generation research (Li et al., 2016) and formulate the response generation task with the following necessary notations. A dataset with user dialogue history content D = {(ci , ri , mi )}N i=1 is firstly given, where ci , ri , mi represent dialogue context, response candidate, and user specific dialogue utterance respectively. Note that we treat the user dialogue utterance for extracting personalization information in multi-turn response generation. Herein, the context is formulated by: ci = (s1 , s2 , · · · , sj , · · · , sni ) where sj represents an utterance in the j-th turn of dialogue context and there are ni utterances in t"
D19-1201,P08-1028,0,0.173515,"egrees between generated responses and ground-truth, we perform evaluations on embedding space. In consistent with previous study (Gu et al., 2019), we compute the similarity between the bag-of-words (BOW) embeddings representations of generated results and reference. In particular, we calculate three metrics:1) Greedy (BOW-Greedy), i.e., greedily matching words in two utterances based on the cosine similarities, and the total scores is then averaged across all words (Rus and Lintean, 2012); 2) Average (BOWAverage), cosine similarity between the averaged word embeddings in the two utterances (Mitchell and Lapata, 2008); 3) Extrema (BOW-Extrema), 4 http://www.nltk.org/_modules/nltk/ translate/bleu_score.html 1936 -这孩子，太稚嫩了，真想踹一脚 (This child is so immature that I really want to kick him.) -wow，你大粗腿，能抬多高啊 (Wow, how high can you get with those big legs.) -求种草水乳，性价比高点的。 (Please recommend cost-effective make-up water and lotion to me.) -我水乳用的老慢啦哈哈，感觉两年用一套 (I use make-up water and lotion so slow that one can be used for 2 years.) ground truth 你要不要试试？见证一下我能办得到！ (Try it ? Prove I can do it.) RL-Persona 哈哈，那就好了 (Haha, it’s all right.) User1 [UNK]，谢谢，我的小U ([UNK], thank you, my xiaoU (name)) 啊，完了，我就不敢说 (Game over, I ca"
D19-1201,P02-1040,0,0.104244,"t is a Gaussian distribution. Table 4: The results of Ablation Experiments. w/o denotes without. Fusion represents fusion of personalization in decoder. ing rates are set to 5e-5 and 1e-5, respectively. The gradient penalty is used for training discriminator (Gulrajani et al., 2017). The value of τ in Gumbel softmax is set to 0.1. cosine similarity between the largest extreme values among the word embeddings in the two utterances (Forgues et al., 2014). We report the maximum BOW embedding scores of the 10 sampled responses for each testing context. Overlap-based Metric. We utilize BLEU score (Papineni et al., 2002) to measure n-grams overlaps between ground-truth and generated response. Specifically, we follow the conventional setting in previous work (Gu et al., 2019) to compute BLEU scores using smoothing techniques (smoothing 7) 4 . For each testing context, we sample 10 responses from the models and compute their BLEU scores, i.e., n-gram precision (BLEUPrecision), n-gram recall (BLEU-Recall), and ngram F1 (BLEU-F1). Human Evaluation. We also employ human evaluation to assess the responses generated by our model and the baselines. Three well-educated annotators are hired to evaluate the quality of g"
D19-1201,D11-1054,0,0.0428144,"tractive and prevalent task within the community of artificial intelligence. Previous studies mainly focus on vertical domains by applying rule- and template-based models (Pieraccini et al., 2009). Later on, with the explosive growth of data, the application of open-domain conversation model is promising. Conventional methods in vertical domains have obstacles to scale to open domain. Given this, various data-driven approaches have been proposed for modeling open-domain conversation, including retrieval-based methods (Yan et al., 2016; Tao et al., 2019), statistical machine translation model (Ritter et al., 2011), and neural networks (Serban et al., 2015; Hu et al., 2019). Recently, building a personalized conversation system has been attached more attention, e.g., implicitly learning user personalizations from dialog history (Li et al., 2015), explicitly collecting and modeling user profiles as personalizations for generating personalized responses (Zhang et al., 2017, 2018). To improve wording diversity, CVAE models (Serban et al., 2017; Zhao et al., 2017; Shen et al., 2018) are well-investigated for opendomain response generation. As the extension of 1938 CVAE, Wasserstein autoencoder (Gu et al., 2"
D19-1201,P18-1205,0,0.0410254,"riable for modeling utterance-level information such as topic, and syntactic structure (Bowman et al., 2015). It is verified in various open-domain response generation situations that conditional variational autoencoders (CVAE) (Serban et al., 2017; Zhao et al., 2017) are effective for addressing the “universal response” issue. In user-level information modeling, existing models either implicitly learn user information from training data such as learning user embedding (Li et al., 2015) or explicitly collect user profiles as the accurate personalization (Zhang et al., 2017; Yang et al., 2018; Zhang et al., 2018). Although obtaining user profiles is more effective and accurate than user embeddings, it is time-consuming and economically costly, or even impossible under the condition of protecting user privacy. We propose the PersonaWAE model, a novel conversational system which simultaneously captures user-level personalization and utterancelevel information as extra hints for generating better responses. Our model is motivated by following two points: 1) existing embedding based per1931 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joi"
D19-1201,W12-2018,0,0.170328,"sponses, we adopt the following metrics widely used in existing research. Embedding Metrics. To capture the semantic matching degrees between generated responses and ground-truth, we perform evaluations on embedding space. In consistent with previous study (Gu et al., 2019), we compute the similarity between the bag-of-words (BOW) embeddings representations of generated results and reference. In particular, we calculate three metrics:1) Greedy (BOW-Greedy), i.e., greedily matching words in two utterances based on the cosine similarities, and the total scores is then averaged across all words (Rus and Lintean, 2012); 2) Average (BOWAverage), cosine similarity between the averaged word embeddings in the two utterances (Mitchell and Lapata, 2008); 3) Extrema (BOW-Extrema), 4 http://www.nltk.org/_modules/nltk/ translate/bleu_score.html 1936 -这孩子，太稚嫩了，真想踹一脚 (This child is so immature that I really want to kick him.) -wow，你大粗腿，能抬多高啊 (Wow, how high can you get with those big legs.) -求种草水乳，性价比高点的。 (Please recommend cost-effective make-up water and lotion to me.) -我水乳用的老慢啦哈哈，感觉两年用一套 (I use make-up water and lotion so slow that one can be used for 2 years.) ground truth 你要不要试试？见证一下我能办得到！ (Try it ? Prove I can do"
D19-1201,P17-1061,0,0.424849,"of our model for generating informative and personalized responses, where both automatic and human evaluations outperform state-of-the-art models. 1 Introduction Over the past decade, a myriad of conversational systems have been proposed in the field of artificial intelligence and achieved remarkable success in various industry scenarios, such as e-commerce assistant (Li et al., 2017) and chit-chat machine XiaoIce (Shum et al., 2018). Based on the domains involved in previous research, existing work can be categorized into two groups, i.e., verticaldomain (Glas et al., 2015) and open-domain (Zhao et al., 2017), where the former group pursues to complete a specific target with limited domain knowledge while the latter one involves massive topics in conversations. In this work, we focus on ∗ † Equal contribution. Ordering is decided by a coin flip. Corresponding author. the latter one and intend to generate a natural and meaningful response for a given conversation context. Most recent works build upon the sequence to sequence model (Bahdanau et al., 2014) and can generate a fluent response. But they suffer from the notorious “universal response” issue, i.e., generating safe and uninformative respons"
D19-1201,P19-1001,1,0.826691,"ork Constructing an automatic conversation system is an attractive and prevalent task within the community of artificial intelligence. Previous studies mainly focus on vertical domains by applying rule- and template-based models (Pieraccini et al., 2009). Later on, with the explosive growth of data, the application of open-domain conversation model is promising. Conventional methods in vertical domains have obstacles to scale to open domain. Given this, various data-driven approaches have been proposed for modeling open-domain conversation, including retrieval-based methods (Yan et al., 2016; Tao et al., 2019), statistical machine translation model (Ritter et al., 2011), and neural networks (Serban et al., 2015; Hu et al., 2019). Recently, building a personalized conversation system has been attached more attention, e.g., implicitly learning user personalizations from dialog history (Li et al., 2015), explicitly collecting and modeling user profiles as personalizations for generating personalized responses (Zhang et al., 2017, 2018). To improve wording diversity, CVAE models (Serban et al., 2017; Zhao et al., 2017; Shen et al., 2018) are well-investigated for opendomain response generation. As the"
D19-1201,Q18-1029,0,0.0252311,"bution of PersonaWAE (which is also a GMD).     wc c = sof tmax(Wf ( )) wu ui (9) z p = wc · z c + wu · z u where Wf is a trainable parameter. Decoder. The decoder is a one-layer GRU network to output the sentence in the generation, which is shown in the right hand of Figure 2. Taking the generation of response ri as an example, the initial state of the decoder is calculated as:   z si,0 = Wd ( p ) + bd (10) c where Wd is a trainable matrix for dimension transformation. To facilitate the combination of user personalization ui and decoder hidden states, 1934 we incorporate a gate module (Tu et al., 2018) in our model: g = f (Ust−1 + Vdt + Wui )   dt ot = GRU(st−1 , ) g · ui + W (qφ (z|x, c)||pθ (z|c, ui )) (12) Experiments Dataset To evaluate the effectiveness of our proposed personalized WAE model (PersonaWAE), we collect a dataset from an open online chatting forum, i.e., Weibo 2 , which contains massive multi-turn conversation sessions and user identification information. Overall, there are 31,128,520 utterances in the raw dataset with corresponded user identifications. To construct the personalized conversation systems, we retrieve users with more than 14 utterances from the raw Weibo c"
D19-1265,P15-1067,0,0.0417072,"a year. 5 Related Work KG Representation Learning aims to embed a KG into a continuous vector space, which preserves the KG structures. There are mainly two streams of researches. The first is additionbased models (also called translation-based models), which based on the principle that for every valid triple (h, r, t), their embeddings holds: h + r ≈ t. TransE(Bordes et al., 2013) is the first such model. TransH(Wang et al., 2014) projects entity embeddings into relation-specific hyperplanes. TransR(Lin et al., 2015) generalize TransH by extending projection to linear transformation. TransD (Ji et al., 2015) simplifies TransR by decomposing the transformation matrix into the product of two vectors. Another is multiplicationbased models, which comes from the idea of tensor decomposition. RESCAL(Nickel et al., 2011) is one of the earlist studies, which using a bilinear scoring function. DistMult(Yang et al., 2014) simplifies RESCAL by using a diagonal matrix. ComplEx(Trouillon et al., 2016) extend DistMult into the complex space to handle asymmetric relations. SimplE (Kazemi and Poole, 2018) learns two embeddings for each entity dependently. A direct application is KG completion. Graph Neural Netwo"
D19-1265,P17-1040,1,0.855489,"s of GUpdater. into GNN. To better capture KG information, RGCN(Schlichtkrull et al., 2018) was proposed to incorporates relation embeddings into GNN. Relation Extraction task aims to extract relations from texts. Current relation extraction models are mainly under distant supervision (Mintz et al., 2009), and are trained in bag level. PCNN (Zeng et al., 2015) divides the sentence into three pieces and applies max-pooling in a piecewise manner after the convolution layer. APCNN (Ji et al., 2017) uses sentence-level attention to select multiple valid sentences with different weights in a bag. (Luo et al., 2017) uses a transition matrix to model the noise and use curriculum for training. 6 Conclusion In this paper, we propose a new text-based KG updating task and construct a dataset, NBAtransactions, for evaluation. We design a novel GNNbased model, GUpdater, which uses text information to guide the message passing through the KG structure. Experiments show that our model can effectively handle both explicit and implicit information, and perform necessary link-adding and link-deleting operations accordingly. In the future, we will try to investigate how to update KGs when entities are involved in sev"
D19-1265,D15-1166,0,0.0257303,"ach layer. Basisdecomposition regularizes these matrices by defining each weight matrix as a linear combination of B(B &lt; R) basis matrices, which significantly decreases the parameter number. 3.2 The core idea of GNNs is to gather neighbors’ information. In our case, we propose a text-based attention mechanism to utilize the news snippet to guide the message passing along the KG structure within the R-GAT layer. We first use bi-GRU (Cho et al., 2014) to encode the given news text S into a sequence of representations {u1 , u2 , · · · , u|S |}, then we leverage the sequence attention mechanism (Luong et al., 2015) to compute the context vector: c = |S| X blr t ut (3) t=1 where blr t is the text attention weight, and is computed as follow: lr ) exp(uTt gtext blr = P|S| t T lr k=1 exp(uk gtext ) T lr attlr (hli , hlj ) = ggraph [hli ||hlj ] (4) lr is a trainable guidance vector to guide where gtext the extraction of relation-dependent context. (5) where ||denotes concatenation operation, and lr ggraph is a relation-specific trainable vector that serves as a graph guidance vector to decide which edge to pay attention to. Here, we generate the final guidance vector gflrin that combines textual information"
D19-1265,D15-1203,0,0.188549,"s the evaluation metrics. Further, to evaluate the ability of link-adding, link-deleting and link-preserving, respectively, we collect all added edges, deleted edges, and unchanged edges and compute the prediction accuracies separately, which we denote as Added Acc, Deleted Acc and Unchanged Acc, respectively. 4.4 Baseline Models Because most current text-based KG updating methods are in two steps: first extract information from texts, then add and remove links in KGs, we select non-rule-based models that perform well on these two steps and also their combination as our baseline models. PCNN (Zeng et al., 2015) is a strong baseline for relation extraction, which divides the sentence into three pieces and applies max-pooling in a piecewise manner after the convolution layer. IE-gold is a simulation of an ideal IE model that can perfectly extract explicit information from given texts. This is an upper bound for the information extraction step. DistMult (Yang et al., 2014) is a widely-used multiplication-based triple scoring function for KG completion, which computes the three-way inner-product of the triples. Its symmetric nature is suitable for NBAtransactions as the KGs are undirected. R-GCN (Schlic"
D19-1265,P09-1113,0,0.0296151,"le of one type of trade. Gtext and G 0 text represent the actual text-subgraphs before and after the trade, respectively. Selected Shortcuts represents the shortcuts selected by the attention mechanism, and the arrows indicate the directions of message passing. Results in Text-Subgraph lists the prediction errors of GUpdater. into GNN. To better capture KG information, RGCN(Schlichtkrull et al., 2018) was proposed to incorporates relation embeddings into GNN. Relation Extraction task aims to extract relations from texts. Current relation extraction models are mainly under distant supervision (Mintz et al., 2009), and are trained in bag level. PCNN (Zeng et al., 2015) divides the sentence into three pieces and applies max-pooling in a piecewise manner after the convolution layer. APCNN (Ji et al., 2017) uses sentence-level attention to select multiple valid sentences with different weights in a bag. (Luo et al., 2017) uses a transition matrix to model the noise and use curriculum for training. 6 Conclusion In this paper, we propose a new text-based KG updating task and construct a dataset, NBAtransactions, for evaluation. We design a novel GNNbased model, GUpdater, which uses text information to guide"
D19-1388,N19-1124,0,0.0302264,"Missing"
D19-1388,P18-1015,0,0.337799,"ic pattern, such as court judgments, diagnosis certificates, abstracts in academic papers, etc. Take the court judgments for example, there is always a statement of the crime committed by the accused, followed by the motives and the results of the judgment. An example case is shown in Table 1, where the summary shares the same writing style and has words in common with the prototype summary (retrieved from the training dataset). Introduction Abstractive summarization can be regarded as a sequence mapping task that maps the source text to the target summary (Rush et al., 2015; Li et al., 2017; Cao et al., 2018; Gao et al., 2019a). It has drawn significant attention since the introduction ∗ Equal contribution. Ordering is decided by a coin flip. Corresponding author. 1 https://github.com/gsh199449/proto-summ † The court held that the defendant Wang had stolen the property of others for the purpose of illegal possession. The amount was large, and his behavior constituted the crime of theft. The accusation of the public prosecution agency was established. The defendant Wang has a criminal record and will be considered when sentencing. Since the defendant Wang did not succeed because of reasons other t"
D19-1388,D18-1442,1,0.738797,"lp generate better summaries with patterns. • Specifically, we propose to generate the summary incorporating the prototype summary pattern and extracted facts from input document. • We provide mutual information signal for the generator to prevent copying irrelevant facts from the prototype. • We release a large-scale prototype based summarization dataset that is beneficial for the community. 2 Related Work We detail related work on text summarization and prototype editing. Text summarization can be classified into extractive and abstractive methods. Extractive methods (Narayan et al., 2018b; Chen et al., 2018) directly select salient sentences from an article to compose a summary. One shortcoming of these models is that they tend to suffer from redundancy. Recently, with the emergence of neural network models for text generation, a vast majority of the literature on summarization (Ma et al., 2018; Zhou et al., 2018; Gao et al., 2019a; Chen et al., 2019) is dedicated to abstractive summarization, which aims to generate new content that concisely paraphrases a document from scratch. Another line of research focuses on prototype editing. (Guu et al., 2018) proposed the first prototype editing model, w"
D19-1388,Q18-1031,0,0.0423837,". Extractive methods (Narayan et al., 2018b; Chen et al., 2018) directly select salient sentences from an article to compose a summary. One shortcoming of these models is that they tend to suffer from redundancy. Recently, with the emergence of neural network models for text generation, a vast majority of the literature on summarization (Ma et al., 2018; Zhou et al., 2018; Gao et al., 2019a; Chen et al., 2019) is dedicated to abstractive summarization, which aims to generate new content that concisely paraphrases a document from scratch. Another line of research focuses on prototype editing. (Guu et al., 2018) proposed the first prototype editing model, which samples a prototype sentence from training data and then edits it into a new sentence. Following this work, (Wu et al., 2018) proposed a new paradigm for response generation, which first retrieves a prototype response from a pre-defined index and then edits the prototype response. (Cao et al., 2018) applied this method on summarization, where they employed existing summaries as soft templates to generate new summary without modeling the dependency between the prototype document, summary and input document. Different from these soft attention m"
D19-1388,P18-1152,0,0.0463162,"dule. § 4.3, we obtain the fact representation of an input document ri and prototype facts rˆi . Combining these with the final hidden state dTn of the generator RNN (in Equation 13), yields the local features of input extracted facts and the prototype facts: C r = {dTn ⊕ r1 , . . . , dTn ⊕ rTm }, f In order to handle the out-of-vocabulary (OOV) problem, we equip our decoder with a pointer network (Gu et al., 2016; Vinyals et al., 2015; See et al., 2017). This process is the same as the model described in (See et al., 2017), thus, is omit here due to limited space. What’s more, previous work (Holtzman et al., 2018) has found that using a cross entropy loss alone is not enough for generating coherent text. Similarly, in our task, using Ls alone is not enough to distinguish a good summary with accurate facts from a bad summary with detailed facts from the prototype document (see § 6.2). Thus, we propose a fact checker to determine whether the generated summary is highly related to the input document. 4.5 Fact Checker To generate accurate summaries that are consistent with the detailed facts from the input document rather than facts from the prototype document, we add a fact checker to provide additional t"
D19-1388,P18-1013,0,0.114359,"b.com/gsh199449/proto-summ commonly used summarization baseline (Nallapati et al., 2017; See et al., 2017), which selects the first three sentences of document as the summary. (2) S2S is a sequence-to-sequence framework with a pointer network, proposed by (See et al., 2017). (3) Proto is a context-aware prototype editing dialog response generation model proposed by (Wu et al., 2018). (4) Re3 Sum, proposed by (Cao et al., 2018), uses an IR platform to retrieve proper summaries and extends the seq2seq framework to jointly conduct template-aware summary generation. (5) Uni-model was proposed by (Hsu et al., 2018), and is the current stateof-the-art abstractive summarization approach on the CNN/DailyMail dataset. (6) We also directly concatenate the prototype summary with the original document as input for S2S and Uni-model, named as Concat-S2S and Concat-Uni, respectively. Evaluation Metrics For the court judgment dataset, we evaluate standard ROUGE-1, ROUGE-2 and ROUGEL (Lin, 2004) on full-length F1 following previous works (Nallapati et al., 2017; See et al., 2017; Paulus et al., 2018), where ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L (RL) refer to the matches of unigram, bigrams, and the longest commo"
D19-1388,N19-1260,0,0.0967986,"Missing"
D19-1388,D17-1222,1,0.896698,"Missing"
D19-1388,P18-2115,0,0.0124479,"We release a large-scale prototype based summarization dataset that is beneficial for the community. 2 Related Work We detail related work on text summarization and prototype editing. Text summarization can be classified into extractive and abstractive methods. Extractive methods (Narayan et al., 2018b; Chen et al., 2018) directly select salient sentences from an article to compose a summary. One shortcoming of these models is that they tend to suffer from redundancy. Recently, with the emergence of neural network models for text generation, a vast majority of the literature on summarization (Ma et al., 2018; Zhou et al., 2018; Gao et al., 2019a; Chen et al., 2019) is dedicated to abstractive summarization, which aims to generate new content that concisely paraphrases a document from scratch. Another line of research focuses on prototype editing. (Guu et al., 2018) proposed the first prototype editing model, which samples a prototype sentence from training data and then edits it into a new sentence. Following this work, (Wu et al., 2018) proposed a new paradigm for response generation, which first retrieves a prototype response from a pre-defined index and then edits the prototype response. (Cao"
D19-1388,D18-1206,0,0.0343561,"otype information to help generate better summaries with patterns. • Specifically, we propose to generate the summary incorporating the prototype summary pattern and extracted facts from input document. • We provide mutual information signal for the generator to prevent copying irrelevant facts from the prototype. • We release a large-scale prototype based summarization dataset that is beneficial for the community. 2 Related Work We detail related work on text summarization and prototype editing. Text summarization can be classified into extractive and abstractive methods. Extractive methods (Narayan et al., 2018b; Chen et al., 2018) directly select salient sentences from an article to compose a summary. One shortcoming of these models is that they tend to suffer from redundancy. Recently, with the emergence of neural network models for text generation, a vast majority of the literature on summarization (Ma et al., 2018; Zhou et al., 2018; Gao et al., 2019a; Chen et al., 2019) is dedicated to abstractive summarization, which aims to generate new content that concisely paraphrases a document from scratch. Another line of research focuses on prototype editing. (Guu et al., 2018) proposed the first proto"
D19-1388,N18-1065,0,0.0458773,"Missing"
D19-1388,N18-1158,0,0.0405132,"otype information to help generate better summaries with patterns. • Specifically, we propose to generate the summary incorporating the prototype summary pattern and extracted facts from input document. • We provide mutual information signal for the generator to prevent copying irrelevant facts from the prototype. • We release a large-scale prototype based summarization dataset that is beneficial for the community. 2 Related Work We detail related work on text summarization and prototype editing. Text summarization can be classified into extractive and abstractive methods. Extractive methods (Narayan et al., 2018b; Chen et al., 2018) directly select salient sentences from an article to compose a summary. One shortcoming of these models is that they tend to suffer from redundancy. Recently, with the emergence of neural network models for text generation, a vast majority of the literature on summarization (Ma et al., 2018; Zhou et al., 2018; Gao et al., 2019a; Chen et al., 2019) is dedicated to abstractive summarization, which aims to generate new content that concisely paraphrases a document from scratch. Another line of research focuses on prototype editing. (Guu et al., 2018) proposed the first proto"
D19-1388,P16-1154,0,0.0260448,"ed Summary Finally, the context vector gth is concatenated with the decoder state dt and fed into a linear layer to obtain the generated word distribution Pv : Figure 3: Framework of fact checker module. § 4.3, we obtain the fact representation of an input document ri and prototype facts rˆi . Combining these with the final hidden state dTn of the generator RNN (in Equation 13), yields the local features of input extracted facts and the prototype facts: C r = {dTn ⊕ r1 , . . . , dTn ⊕ rTm }, f In order to handle the out-of-vocabulary (OOV) problem, we equip our decoder with a pointer network (Gu et al., 2016; Vinyals et al., 2015; See et al., 2017). This process is the same as the model described in (See et al., 2017), thus, is omit here due to limited space. What’s more, previous work (Holtzman et al., 2018) has found that using a cross entropy loss alone is not enough for generating coherent text. Similarly, in our task, using Ls alone is not enough to distinguish a good summary with accurate facts from a bad summary with detailed facts from the prototype document (see § 6.2). Thus, we propose a fact checker to determine whether the generated summary is highly related to the input document. 4.5"
D19-1388,W14-4407,0,0.437984,"sation of the public prosecution agency was established and supported. This crime was committed within two years after the release of the defendants Zhang and Fan. Thus they are recidivists and this situation will be considered when sentencing. The fact that defendants Zhang and Fan surrendered themselves and pleaded guilty in court gives a lighter punishment according to law. Existing prototype based generation models such as (Wu et al., 2018) are all applied on short text, thus, cannot handle long documents summarization task. Another series of works focus on template-based methods such as (Oya et al., 2014). However, template-based methods are too rigid for our patternized summary generation task. Hence, in this paper, we propose a summarization framework named Prototype Editing based Summary Generator (PESG) that incorporates prototype document-summary pairs to improve summa3741 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 3741–3751, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics rization performance when generating summaries with pat"
D19-1388,D15-1044,0,0.0622959,"are required to conform to a specific pattern, such as court judgments, diagnosis certificates, abstracts in academic papers, etc. Take the court judgments for example, there is always a statement of the crime committed by the accused, followed by the motives and the results of the judgment. An example case is shown in Table 1, where the summary shares the same writing style and has words in common with the prototype summary (retrieved from the training dataset). Introduction Abstractive summarization can be regarded as a sequence mapping task that maps the source text to the target summary (Rush et al., 2015; Li et al., 2017; Cao et al., 2018; Gao et al., 2019a). It has drawn significant attention since the introduction ∗ Equal contribution. Ordering is decided by a coin flip. Corresponding author. 1 https://github.com/gsh199449/proto-summ † The court held that the defendant Wang had stolen the property of others for the purpose of illegal possession. The amount was large, and his behavior constituted the crime of theft. The accusation of the public prosecution agency was established. The defendant Wang has a criminal record and will be considered when sentencing. Since the defendant Wang did not"
D19-1388,E17-2007,0,0.015777,"the-art abstractive summarization approach on the CNN/DailyMail dataset. (6) We also directly concatenate the prototype summary with the original document as input for S2S and Uni-model, named as Concat-S2S and Concat-Uni, respectively. Evaluation Metrics For the court judgment dataset, we evaluate standard ROUGE-1, ROUGE-2 and ROUGEL (Lin, 2004) on full-length F1 following previous works (Nallapati et al., 2017; See et al., 2017; Paulus et al., 2018), where ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L (RL) refer to the matches of unigram, bigrams, and the longest common subsequence respectively. (Schluter, 2017) notes that only using the ROUGE metric to evaluate summarization quality can be misleading. Therefore, we also evaluate our model by human evaluation. Three highly educated participants are asked to score 100 randomly sampled summaries generated by three models: Uni-model, Re3 Sum and PESG. The statistical significance of observed differences between the performance of two runs is tested using a twotailed paired t-test and is denoted using N (or H ) for strong (or weak) significance for α = 0.01. 5.4 Implementation Details We implement our experiments in TensorFlow (Abadi et al., 2016) on an"
D19-1388,P17-1099,0,0.105924,"gth is concatenated with the decoder state dt and fed into a linear layer to obtain the generated word distribution Pv : Figure 3: Framework of fact checker module. § 4.3, we obtain the fact representation of an input document ri and prototype facts rˆi . Combining these with the final hidden state dTn of the generator RNN (in Equation 13), yields the local features of input extracted facts and the prototype facts: C r = {dTn ⊕ r1 , . . . , dTn ⊕ rTm }, f In order to handle the out-of-vocabulary (OOV) problem, we equip our decoder with a pointer network (Gu et al., 2016; Vinyals et al., 2015; See et al., 2017). This process is the same as the model described in (See et al., 2017), thus, is omit here due to limited space. What’s more, previous work (Holtzman et al., 2018) has found that using a cross entropy loss alone is not enough for generating coherent text. Similarly, in our task, using Ls alone is not enough to distinguish a good summary with accurate facts from a bad summary with detailed facts from the prototype document (see § 6.2). Thus, we propose a fact checker to determine whether the generated summary is highly related to the input document. 4.5 Fact Checker To generate accurate summar"
D19-1499,N16-1012,0,0.0398371,"space of different styles and design two constraints to train it. We also introduce two other simple but effective semisupervised methods to compare with. To evaluate the performance of the proposed methods, we build and release a novel style transfer dataset that alters sentences between the style of ancient Chinese poem and the modern Chinese. 1 Introduction Recently, the natural language generation (NLG) tasks have been attracting the growing attention of researchers, including response generation (Vinyals and Le, 2015), machine translation (Bahdanau et al., 2014), automatic summarization (Chopra et al., 2016), question generation (Gao et al., 2019), etc. Among these generation tasks, one interesting but challenging problem is text style transfer (Shen et al., 2017; Fu et al., 2018; Logeswaran et al., 2018). Given a sentence from one style domain, a style transfer system is required to convert it to another style domain as well as keeping its content meaning unchanged. As a fundamental attribute of text, style can have a broad and ambiguous scope, such as ancient poetry style v.s. modern language style and positive sentiment v.s. negative sentiment. ∗ This work was done when Mingyue Shang was an in"
D19-1499,D19-1306,0,0.0616094,"Missing"
D19-1499,D14-1181,0,0.00593167,"Missing"
D19-1499,J82-2005,0,0.727621,"Missing"
D19-1499,D18-1420,1,0.850292,"gn a strategy to disentangle the variables for content and style. Shen et al. (2017) first map the text corpora belonging to different styles to their own space respectively, and leverages the alignment of latent representations from different styles to perform style transfer. Chen et al. (2019) propose to extract and control the style of the image caption through domain layer normalization. Prabhumoye et al. (2018) and Zhang et al. (2018b) employ the back-translation mechanism to ensure that the input from the source style can be reconstructed from the transferred result in the target style. Liao et al. (2018) associate the style latent variable with a numeric discrete or continues numeric value and can generate sentences controlled by this value. Among them, many use the adversarial training mechanism (Goodfellow et al., 2014) to improve the performance of the basic models (Shen et al., 2017; Zhao et al., 2018). To sum up, most of the existing unsupervised frameworks on the text style transfer focus on getting the disentangled representations of style and content. However, Lample et al. (2019) illustrated that the disentanglement not adequate to learn the style-independent representations, thus th"
D19-1499,W02-0109,0,0.189984,"by Rao and Tetreault (2018) which contains texts of formal and informal style. With the released data, we randomly sample 5,000 sentence pairs from it as the parallel corpus with limited data volume. We then use the Yahoo Answers L6 corpus7 as the source which is in the same content domain as the parallel data to construct the large-scale nonparallel data. To divide nonparallel dataset into two styles, we train a CNN-based classifier (Kim, 2014) on the parallel data with annotation of styles and use it to classify the nonparallel data. sentence as 30. For the formality datasets, we use NLTK (Loper and Bird, 2002) to tokenize the texts and set the minimum length as 5 and the maximum length as 30 for both formal and informal styles. We adopt GloVE (Pennington et al., 2014) to pretrain the embeddings, and the dimensions of the embeddings are set to 300 for all the datasets. The hidden states are set to 500 for both encoders and decoders. We adopt SGD optimizer with the learning rate as 1 for DAE models and 0.1 for S2S models. The dropout rate is 0.4. In the inference stage, the beam size is set to 5. 7.2 8.1 Experimental Settings We perform different data preprocessing on different datasets. The Chinese"
D19-1499,P15-2097,0,0.0419915,"e supervised baseline and the three semi-supervised models. Fluency 0.3575 0.4425 0.5800 0.6325 0.3050 0.5475 0.5725 0.6200 9 Table 4: The human annotation results of the S2S model and CPLS model from three aspects. as the automatic evaluation metrics to measure the content preservation degree and the style changing degree. BLEU calculates the N-gram overlap between the generated sentence and the references, thus can be used to measure the preservation of text content. Considering that text style transfer is a monolingual text generation task, we also use GLEU, a generalized BLEU proposed by (Napoles et al., 2015). To evaluate the extent to which the sentences are transferred to the target style, we follow Shen et al. (2017); Hu et al. (2017) that build a CNN-based style classifier and use it to measure the style accuracy. 8.3 Human Evaluation We also adopt human evaluations to judge the quality of the transferred sentences from three aspects, namely content, style and fluency. These aspects evaluate how well the transferred text preserve the content of the input, the style strength and the fluency of the transferred text. Take the content relevance for example, the criterion is as follows: +2: The tra"
D19-1499,D18-1138,0,0.0162566,"e the sentence with classifier favored style. Hu et al. (2017) employ variational autoencoders (Kingma and Welling, 2013) to conduct the style latent variable learning and design a strategy to disentangle the variables for content and style. Shen et al. (2017) first map the text corpora belonging to different styles to their own space respectively, and leverages the alignment of latent representations from different styles to perform style transfer. Chen et al. (2019) propose to extract and control the style of the image caption through domain layer normalization. Prabhumoye et al. (2018) and Zhang et al. (2018b) employ the back-translation mechanism to ensure that the input from the source style can be reconstructed from the transferred result in the target style. Liao et al. (2018) associate the style latent variable with a numeric discrete or continues numeric value and can generate sentences controlled by this value. Among them, many use the adversarial training mechanism (Goodfellow et al., 2014) to improve the performance of the basic models (Shen et al., 2017; Zhao et al., 2018). To sum up, most of the existing unsupervised frameworks on the text style transfer focus on getting the disentangl"
D19-1499,P02-1040,0,0.106223,"Missing"
D19-1499,D14-1162,0,0.0825285,"parallel corpus with limited data volume. We then use the Yahoo Answers L6 corpus7 as the source which is in the same content domain as the parallel data to construct the large-scale nonparallel data. To divide nonparallel dataset into two styles, we train a CNN-based classifier (Kim, 2014) on the parallel data with annotation of styles and use it to classify the nonparallel data. sentence as 30. For the formality datasets, we use NLTK (Loper and Bird, 2002) to tokenize the texts and set the minimum length as 5 and the maximum length as 30 for both formal and informal styles. We adopt GloVE (Pennington et al., 2014) to pretrain the embeddings, and the dimensions of the embeddings are set to 300 for all the datasets. The hidden states are set to 500 for both encoders and decoders. We adopt SGD optimizer with the learning rate as 1 for DAE models and 0.1 for S2S models. The dropout rate is 0.4. In the inference stage, the beam size is set to 5. 7.2 8.1 Experimental Settings We perform different data preprocessing on different datasets. The Chinese literary datasets are segmented by characters instead of word to alleviate the issue of unknown words. Our statistics show that the average length of ancient poe"
D19-1499,P18-1080,0,0.0387814,"ting the training mode between supervised and unsupervised. • We introduce another two semi-supervised methods that are simple but effective to leverage both the nonparallel and parallel data. • We build a small-scale parallel dataset that contains ancient Chinese poem style and modern Chinese style sentences. We also collect two large nonparallel datasets of these styles.1 2 Related Works Recently, text style transfer has stimulated great interests of researchers from the area of neural language processing and some encouraging results are obtained (Shen et al., 2017; Rao and Tetreault, 2018; Prabhumoye et al., 2018; Hu et al., 2017; Jin et al., 2019). In the primary stage, due to the lacking of parallel corpus, most of the methods employ unsupervised learning paradigm to conduct the semantic modeling and transfer. 1 Download link: https://tinyurl.com/yyc8zkqg Unsupervised Learning Methods. Mueller et al. (2017) modify the latent variables of sentences in a certain direction guided by a classifier to generate the sentence with classifier favored style. Hu et al. (2017) employ variational autoencoders (Kingma and Welling, 2013) to conduct the style latent variable learning and design a strategy to disenta"
D19-1499,N18-1012,0,0.358646,"el is flexible in alternating the training mode between supervised and unsupervised. • We introduce another two semi-supervised methods that are simple but effective to leverage both the nonparallel and parallel data. • We build a small-scale parallel dataset that contains ancient Chinese poem style and modern Chinese style sentences. We also collect two large nonparallel datasets of these styles.1 2 Related Works Recently, text style transfer has stimulated great interests of researchers from the area of neural language processing and some encouraging results are obtained (Shen et al., 2017; Rao and Tetreault, 2018; Prabhumoye et al., 2018; Hu et al., 2017; Jin et al., 2019). In the primary stage, due to the lacking of parallel corpus, most of the methods employ unsupervised learning paradigm to conduct the semantic modeling and transfer. 1 Download link: https://tinyurl.com/yyc8zkqg Unsupervised Learning Methods. Mueller et al. (2017) modify the latent variables of sentences in a certain direction guided by a classifier to generate the sentence with classifier favored style. Hu et al. (2017) employ variational autoencoders (Kingma and Welling, 2013) to conduct the style latent variable learning and des"
D19-1499,P16-1009,0,0.0730157,"Missing"
D19-1501,P18-1063,0,0.0230098,"ropose an ELSTM and a keyword memory to incorporate entity label information, so as to generate more accurate descriptions. 2 Related Work We detail related work on text generation, entityrelated generation, and product description. Text generation. Recently, sequence-tosequence (Seq2Seq) neural network models have been widely used in NLG approaches. Their effectiveness has been demonstrated in a variety of text generation tasks, such as neural machine translation (Luong et al., 2015; Bahdanau et al., 2014; Wu et al., 2016), abstractive text summarization (See et al., 2017a; Hsu et al., 2018; Chen and Bansal, 2018), dialogue generation (Tao et al., 2018a; Xing et al., 2017), etc. Along another line, there are also works based on an attention mechanism. Vaswani et al. (2017) proposed a Transformer architecture that utilizes the self-attention mechanism and has achieved state-of-the-art results in neural machine translation. Since then, the attention mechanism has been used in a variety of tasks (Devlin et al., 2018; Fan et al., 2018; Zhou et al., 2018). Entity-related generation. Named entity recognition (NER) is a fundamental component in language understanding and reasoning (Greenberg et al., 2018; Kat"
D19-1501,N18-1204,0,0.052123,"Missing"
D19-1501,P18-1082,0,0.0362371,"s, such as neural machine translation (Luong et al., 2015; Bahdanau et al., 2014; Wu et al., 2016), abstractive text summarization (See et al., 2017a; Hsu et al., 2018; Chen and Bansal, 2018), dialogue generation (Tao et al., 2018a; Xing et al., 2017), etc. Along another line, there are also works based on an attention mechanism. Vaswani et al. (2017) proposed a Transformer architecture that utilizes the self-attention mechanism and has achieved state-of-the-art results in neural machine translation. Since then, the attention mechanism has been used in a variety of tasks (Devlin et al., 2018; Fan et al., 2018; Zhou et al., 2018). Entity-related generation. Named entity recognition (NER) is a fundamental component in language understanding and reasoning (Greenberg et al., 2018; Katiyar and Cardie, 2018). In (Ji et al., 2017), they proved that adding entity related information can improve the performance of language modeling. Building upon this work, in (Clark et al., 2018), they combined entity context with previous-sentence context, and demonstrated the importance of the latter in coherence test. Another line of related work generates recipes using neural networks to track and update entity repres"
D19-1501,D18-1306,0,0.0294082,"018; Chen and Bansal, 2018), dialogue generation (Tao et al., 2018a; Xing et al., 2017), etc. Along another line, there are also works based on an attention mechanism. Vaswani et al. (2017) proposed a Transformer architecture that utilizes the self-attention mechanism and has achieved state-of-the-art results in neural machine translation. Since then, the attention mechanism has been used in a variety of tasks (Devlin et al., 2018; Fan et al., 2018; Zhou et al., 2018). Entity-related generation. Named entity recognition (NER) is a fundamental component in language understanding and reasoning (Greenberg et al., 2018; Katiyar and Cardie, 2018). In (Ji et al., 2017), they proved that adding entity related information can improve the performance of language modeling. Building upon this work, in (Clark et al., 2018), they combined entity context with previous-sentence context, and demonstrated the importance of the latter in coherence test. Another line of related work generates recipes using neural networks to track and update entity representations (Bosselut et al., 2018). Different from the above works, we utilize entity labels as supplementary information to assist decoding in the text generation task. P"
D19-1501,P18-1013,0,0.0536131,"this problem, we propose an ELSTM and a keyword memory to incorporate entity label information, so as to generate more accurate descriptions. 2 Related Work We detail related work on text generation, entityrelated generation, and product description. Text generation. Recently, sequence-tosequence (Seq2Seq) neural network models have been widely used in NLG approaches. Their effectiveness has been demonstrated in a variety of text generation tasks, such as neural machine translation (Luong et al., 2015; Bahdanau et al., 2014; Wu et al., 2016), abstractive text summarization (See et al., 2017a; Hsu et al., 2018; Chen and Bansal, 2018), dialogue generation (Tao et al., 2018a; Xing et al., 2017), etc. Along another line, there are also works based on an attention mechanism. Vaswani et al. (2017) proposed a Transformer architecture that utilizes the self-attention mechanism and has achieved state-of-the-art results in neural machine translation. Since then, the attention mechanism has been used in a variety of tasks (Devlin et al., 2018; Fan et al., 2018; Zhou et al., 2018). Entity-related generation. Named entity recognition (NER) is a fundamental component in language understanding and reasoning (Gre"
D19-1501,D17-1195,0,0.0238879,"t al., 2018a; Xing et al., 2017), etc. Along another line, there are also works based on an attention mechanism. Vaswani et al. (2017) proposed a Transformer architecture that utilizes the self-attention mechanism and has achieved state-of-the-art results in neural machine translation. Since then, the attention mechanism has been used in a variety of tasks (Devlin et al., 2018; Fan et al., 2018; Zhou et al., 2018). Entity-related generation. Named entity recognition (NER) is a fundamental component in language understanding and reasoning (Greenberg et al., 2018; Katiyar and Cardie, 2018). In (Ji et al., 2017), they proved that adding entity related information can improve the performance of language modeling. Building upon this work, in (Clark et al., 2018), they combined entity context with previous-sentence context, and demonstrated the importance of the latter in coherence test. Another line of related work generates recipes using neural networks to track and update entity representations (Bosselut et al., 2018). Different from the above works, we utilize entity labels as supplementary information to assist decoding in the text generation task. Product descriptions. Quality product descriptions"
D19-1501,N18-1079,0,0.0312599,"18), dialogue generation (Tao et al., 2018a; Xing et al., 2017), etc. Along another line, there are also works based on an attention mechanism. Vaswani et al. (2017) proposed a Transformer architecture that utilizes the self-attention mechanism and has achieved state-of-the-art results in neural machine translation. Since then, the attention mechanism has been used in a variety of tasks (Devlin et al., 2018; Fan et al., 2018; Zhou et al., 2018). Entity-related generation. Named entity recognition (NER) is a fundamental component in language understanding and reasoning (Greenberg et al., 2018; Katiyar and Cardie, 2018). In (Ji et al., 2017), they proved that adding entity related information can improve the performance of language modeling. Building upon this work, in (Clark et al., 2018), they combined entity context with previous-sentence context, and demonstrated the importance of the latter in coherence test. Another line of related work generates recipes using neural networks to track and update entity representations (Bosselut et al., 2018). Different from the above works, we utilize entity labels as supplementary information to assist decoding in the text generation task. Product descriptions. Qualit"
D19-1501,D18-1423,1,0.846892,"Missing"
D19-1501,P15-1002,0,0.025973,"owledge, we are the first to explore the fidelity problem of product description generation. Besides, to tackle this problem, we propose an ELSTM and a keyword memory to incorporate entity label information, so as to generate more accurate descriptions. 2 Related Work We detail related work on text generation, entityrelated generation, and product description. Text generation. Recently, sequence-tosequence (Seq2Seq) neural network models have been widely used in NLG approaches. Their effectiveness has been demonstrated in a variety of text generation tasks, such as neural machine translation (Luong et al., 2015; Bahdanau et al., 2014; Wu et al., 2016), abstractive text summarization (See et al., 2017a; Hsu et al., 2018; Chen and Bansal, 2018), dialogue generation (Tao et al., 2018a; Xing et al., 2017), etc. Along another line, there are also works based on an attention mechanism. Vaswani et al. (2017) proposed a Transformer architecture that utilizes the self-attention mechanism and has achieved state-of-the-art results in neural machine translation. Since then, the attention mechanism has been used in a variety of tasks (Devlin et al., 2018; Fan et al., 2018; Zhou et al., 2018). Entity-related gene"
D19-1501,P17-1099,0,0.711428,"Besides, to tackle this problem, we propose an ELSTM and a keyword memory to incorporate entity label information, so as to generate more accurate descriptions. 2 Related Work We detail related work on text generation, entityrelated generation, and product description. Text generation. Recently, sequence-tosequence (Seq2Seq) neural network models have been widely used in NLG approaches. Their effectiveness has been demonstrated in a variety of text generation tasks, such as neural machine translation (Luong et al., 2015; Bahdanau et al., 2014; Wu et al., 2016), abstractive text summarization (See et al., 2017a; Hsu et al., 2018; Chen and Bansal, 2018), dialogue generation (Tao et al., 2018a; Xing et al., 2017), etc. Along another line, there are also works based on an attention mechanism. Vaswani et al. (2017) proposed a Transformer architecture that utilizes the self-attention mechanism and has achieved state-of-the-art results in neural machine translation. Since then, the attention mechanism has been used in a variety of tasks (Devlin et al., 2018; Fan et al., 2018; Zhou et al., 2018). Entity-related generation. Named entity recognition (NER) is a fundamental component in language understanding"
D19-1501,P17-2060,0,0.0582283,"Missing"
D19-1501,P18-1103,0,0.0283355,"machine translation (Luong et al., 2015; Bahdanau et al., 2014; Wu et al., 2016), abstractive text summarization (See et al., 2017a; Hsu et al., 2018; Chen and Bansal, 2018), dialogue generation (Tao et al., 2018a; Xing et al., 2017), etc. Along another line, there are also works based on an attention mechanism. Vaswani et al. (2017) proposed a Transformer architecture that utilizes the self-attention mechanism and has achieved state-of-the-art results in neural machine translation. Since then, the attention mechanism has been used in a variety of tasks (Devlin et al., 2018; Fan et al., 2018; Zhou et al., 2018). Entity-related generation. Named entity recognition (NER) is a fundamental component in language understanding and reasoning (Greenberg et al., 2018; Katiyar and Cardie, 2018). In (Ji et al., 2017), they proved that adding entity related information can improve the performance of language modeling. Building upon this work, in (Clark et al., 2018), they combined entity context with previous-sentence context, and demonstrated the importance of the latter in coherence test. Another line of related work generates recipes using neural networks to track and update entity representations (Bosselut"
D19-1501,I17-2032,0,0.0756169,"nstrated the importance of the latter in coherence test. Another line of related work generates recipes using neural networks to track and update entity representations (Bosselut et al., 2018). Different from the above works, we utilize entity labels as supplementary information to assist decoding in the text generation task. Product descriptions. Quality product descriptions are critical for providing a competitive customer experience in an e-commerce platform. Due to its importance, automatically generating the product description has attracted considerable interests. Initial works include (Wang et al., 2017), which incorporates statistical methods with the template to generate product descriptions. With the development of neural networks, (Chen et al., 2019) explored a new way to generate personalized product descriptions by combining the power of neural networks and a knowledge base. (Zhang et al., 2019b) proposed a pointer-generator neural network to generate product description whose patterns are controlled. In real-world product description generation application, however, the most important prerequisite is the fidelity of generated text, and to the best of our knowledge, no research has been"
I17-2029,C16-1316,1,0.851439,"Missing"
I17-2029,P15-2130,0,0.0626508,"Missing"
I17-2029,P15-1152,0,0.0611304,"Missing"
P14-1077,P07-1073,0,0.0995413,"rest of the paper, we first review related work in Section 2, and in Section 3, we describe our framework in detail. Experimental setup and results are discussed in Section 4. We conclude this paper in Section 5. 2 Related Work Since traditional supervised relation extraction methods (Soderland et al., 1995; Zhao and Grishman, 2005) require manual annotations and are often domain-specific, nowadays many efforts focus on semi-supervised or unsupervised methods (Banko et al., 2007; Fader et al., 2011). Distant supervision (DS) is a semi-supervised RE framework and has attracted many attentions (Bunescu, 2007; Mintz et al., 2009; Yao et al., 2010; Surdeanu et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). DS approaches can predict canonicalized (predefined in KBs) relations for large amount of data and do not need much human involvement. Since the automatically generated training datasets in DS often contain noises, there are also research efforts focusing on reducing the noisy labels in the training data (Takamatsu et al., 2012). To bridge the gaps between the relations extracted from open information extraction and the canonicalized relations in KBs, Yao et al. (2012) and Riedel et al"
P14-1077,W13-3809,0,0.197069,"Missing"
P14-1077,D11-1142,0,0.176793,"he datasets. We also show that the automatically learnt clues perform comparably to those refined manually. In the rest of the paper, we first review related work in Section 2, and in Section 3, we describe our framework in detail. Experimental setup and results are discussed in Section 4. We conclude this paper in Section 5. 2 Related Work Since traditional supervised relation extraction methods (Soderland et al., 1995; Zhao and Grishman, 2005) require manual annotations and are often domain-specific, nowadays many efforts focus on semi-supervised or unsupervised methods (Banko et al., 2007; Fader et al., 2011). Distant supervision (DS) is a semi-supervised RE framework and has attracted many attentions (Bunescu, 2007; Mintz et al., 2009; Yao et al., 2010; Surdeanu et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). DS approaches can predict canonicalized (predefined in KBs) relations for large amount of data and do not need much human involvement. Since the automatically generated training datasets in DS often contain noises, there are also research efforts focusing on reducing the noisy labels in the training data (Takamatsu et al., 2012). To bridge the gaps between the relations extracte"
P14-1077,D12-1042,0,0.574967,"es, a city can be the capital of only one country. All these clues are no doubt helpful, for instance, Yao et al. (2010) explicitly modeled the expected types of a relation’s arguments with the help of Freebase’s type taxonomy and obtained promising results for RE. ∗ Yansong Feng is the corresponding author. 818 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 818–827, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics than one relations, but still predicts each entity tuple locally (Hoffmann et al., 2011). Surdeanu et al. (2012) propose a two-layer multi-instance multi-label (MIML) framework to capture the dependencies among relations. The first layer is a multi-class classifier making local predictions for single sentences, the output of which are aggregated by the second layer into the entity pair level. Their approach only captures relation dependencies, while we learn implicit relation backgrounds from knowledge bases, including argument type and cardinality requirements. Riedel et al. (2013) propose to use latent vectors to estimate the preferences between relations and entities. These can be considered as the l"
P14-1077,P12-1076,0,0.014792,"upervised or unsupervised methods (Banko et al., 2007; Fader et al., 2011). Distant supervision (DS) is a semi-supervised RE framework and has attracted many attentions (Bunescu, 2007; Mintz et al., 2009; Yao et al., 2010; Surdeanu et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). DS approaches can predict canonicalized (predefined in KBs) relations for large amount of data and do not need much human involvement. Since the automatically generated training datasets in DS often contain noises, there are also research efforts focusing on reducing the noisy labels in the training data (Takamatsu et al., 2012). To bridge the gaps between the relations extracted from open information extraction and the canonicalized relations in KBs, Yao et al. (2012) and Riedel et al. (2013) propose a universal schema which is a union of KB schemas and natural language patterns, making it possible to integrate the unlimited set of uncanonicalized relations in open settings with the relations in existing KBs. As far as we know, few works have managed to take the relation specific requirements for arguments into account, and most existing works make predictions locally and individually. The MultiR system allows entit"
P14-1077,P11-1055,0,0.714541,"object, and in most cases, a city can be the capital of only one country. All these clues are no doubt helpful, for instance, Yao et al. (2010) explicitly modeled the expected types of a relation’s arguments with the help of Freebase’s type taxonomy and obtained promising results for RE. ∗ Yansong Feng is the corresponding author. 818 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 818–827, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics than one relations, but still predicts each entity tuple locally (Hoffmann et al., 2011). Surdeanu et al. (2012) propose a two-layer multi-instance multi-label (MIML) framework to capture the dependencies among relations. The first layer is a multi-class classifier making local predictions for single sentences, the output of which are aggregated by the second layer into the entity pair level. Their approach only captures relation dependencies, while we learn implicit relation backgrounds from knowledge bases, including argument type and cardinality requirements. Riedel et al. (2013) propose to use latent vectors to estimate the preferences between relations and entities. These ca"
P14-1077,D10-1099,0,0.584396,"relationship between pairs of entities is crucial for many knowledge base related applications(Suchanek et al., 2013). In the literature, relation extraction (RE) is usually investigated in a classification style, where relations are simply treated as isolated class labels, while their definitions or background information are sometimes ignored. Take the relation Capital as an example, we can imagine that this relation will expect a country as its subject and a city as object, and in most cases, a city can be the capital of only one country. All these clues are no doubt helpful, for instance, Yao et al. (2010) explicitly modeled the expected types of a relation’s arguments with the help of Freebase’s type taxonomy and obtained promising results for RE. ∗ Yansong Feng is the corresponding author. 818 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 818–827, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics than one relations, but still predicts each entity tuple locally (Hoffmann et al., 2011). Surdeanu et al. (2012) propose a two-layer multi-instance multi-label (MIML) framework to capture the dependencies among"
P14-1077,W12-3022,0,0.0222652,"ted many attentions (Bunescu, 2007; Mintz et al., 2009; Yao et al., 2010; Surdeanu et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). DS approaches can predict canonicalized (predefined in KBs) relations for large amount of data and do not need much human involvement. Since the automatically generated training datasets in DS often contain noises, there are also research efforts focusing on reducing the noisy labels in the training data (Takamatsu et al., 2012). To bridge the gaps between the relations extracted from open information extraction and the canonicalized relations in KBs, Yao et al. (2012) and Riedel et al. (2013) propose a universal schema which is a union of KB schemas and natural language patterns, making it possible to integrate the unlimited set of uncanonicalized relations in open settings with the relations in existing KBs. As far as we know, few works have managed to take the relation specific requirements for arguments into account, and most existing works make predictions locally and individually. The MultiR system allows entity tuples to have more 3 The Framework Our framework takes a set of entity pairs and their supporting sentences as its input. We first train a p"
P14-1077,P13-1008,0,0.037412,"on dependencies, while we learn implicit relation backgrounds from knowledge bases, including argument type and cardinality requirements. Riedel et al. (2013) propose to use latent vectors to estimate the preferences between relations and entities. These can be considered as the latent type information of the relations’ arguments, which is learnt from various data sources. In contrast, our approach learn implicit clues from existing KBs, and jointly optimize local predictions among different entity tuples to capture both relation argument type clues and cardinality clues. Li et al. (2011) and Li et al. (2013) use co-occurring statistics among relations or events to jointly improve information extraction performances in ACE tasks, while we mine existing KBs to collect global clues to solve local conflicts and find the optimal aggregation assignments, regarding existing knowledge facts. de Lacalle and Lapata (2013) encode general domain knowledge as FOL rules in a topic model while our instantiated constraints are directly operated in an ILP model. Zhang et al. (2013) utilize relation cardinality to create negative samples for distant supervision while we use both implicit type clues and relation ca"
P14-1077,P09-1113,0,0.636972,"er, we first review related work in Section 2, and in Section 3, we describe our framework in detail. Experimental setup and results are discussed in Section 4. We conclude this paper in Section 5. 2 Related Work Since traditional supervised relation extraction methods (Soderland et al., 1995; Zhao and Grishman, 2005) require manual annotations and are often domain-specific, nowadays many efforts focus on semi-supervised or unsupervised methods (Banko et al., 2007; Fader et al., 2011). Distant supervision (DS) is a semi-supervised RE framework and has attracted many attentions (Bunescu, 2007; Mintz et al., 2009; Yao et al., 2010; Surdeanu et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). DS approaches can predict canonicalized (predefined in KBs) relations for large amount of data and do not need much human involvement. Since the automatically generated training datasets in DS often contain noises, there are also research efforts focusing on reducing the noisy labels in the training data (Takamatsu et al., 2012). To bridge the gaps between the relations extracted from open information extraction and the canonicalized relations in KBs, Yao et al. (2012) and Riedel et al. (2013) propose a u"
P14-1077,P13-2141,0,0.012565,"local predictions among different entity tuples to capture both relation argument type clues and cardinality clues. Li et al. (2011) and Li et al. (2013) use co-occurring statistics among relations or events to jointly improve information extraction performances in ACE tasks, while we mine existing KBs to collect global clues to solve local conflicts and find the optimal aggregation assignments, regarding existing knowledge facts. de Lacalle and Lapata (2013) encode general domain knowledge as FOL rules in a topic model while our instantiated constraints are directly operated in an ILP model. Zhang et al. (2013) utilize relation cardinality to create negative samples for distant supervision while we use both implicit type clues and relation cardinality expectations to discover possible inconsistencies among local predictions. the expected type and cardinality requirements for a relation’s arguments, and jointly resolve the disagreements among candidate predictions. We formalize this procedure as a constrained optimization problem, which can be solved by many optimization frameworks. We use integer linear programming (ILP) as the solver and evaluate our framework on English and Chinese datasets. The e"
P14-1077,P05-1052,0,0.141743,"work on English and Chinese datasets. The experimental results show that our framework performs better than the state-of-the-art approaches when such clues are applicable to the datasets. We also show that the automatically learnt clues perform comparably to those refined manually. In the rest of the paper, we first review related work in Section 2, and in Section 3, we describe our framework in detail. Experimental setup and results are discussed in Section 4. We conclude this paper in Section 5. 2 Related Work Since traditional supervised relation extraction methods (Soderland et al., 1995; Zhao and Grishman, 2005) require manual annotations and are often domain-specific, nowadays many efforts focus on semi-supervised or unsupervised methods (Banko et al., 2007; Fader et al., 2011). Distant supervision (DS) is a semi-supervised RE framework and has attracted many attentions (Bunescu, 2007; Mintz et al., 2009; Yao et al., 2010; Surdeanu et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). DS approaches can predict canonicalized (predefined in KBs) relations for large amount of data and do not need much human involvement. Since the automatically generated training datasets in DS often contain nois"
P14-1077,N13-1008,0,0.499272,"for Computational Linguistics than one relations, but still predicts each entity tuple locally (Hoffmann et al., 2011). Surdeanu et al. (2012) propose a two-layer multi-instance multi-label (MIML) framework to capture the dependencies among relations. The first layer is a multi-class classifier making local predictions for single sentences, the output of which are aggregated by the second layer into the entity pair level. Their approach only captures relation dependencies, while we learn implicit relation backgrounds from knowledge bases, including argument type and cardinality requirements. Riedel et al. (2013) propose to use latent vectors to estimate the preferences between relations and entities. These can be considered as the latent type information of the relations’ arguments, which is learnt from various data sources. In contrast, our approach learn implicit clues from existing KBs, and jointly optimize local predictions among different entity tuples to capture both relation argument type clues and cardinality clues. Li et al. (2011) and Li et al. (2013) use co-occurring statistics among relations or events to jointly improve information extraction performances in ACE tasks, while we mine exis"
P14-1077,D13-1040,0,\N,Missing
P15-2037,D13-1160,0,0.0476537,"ension of the comparison and its ranking order. Currently, our analysis suffers from the limited coverage of our WikiDiF. In the future, it would be interesting to improve our method to cover more KB predicates, and extend our NN model with more advanced structures to further improve the performances and also simultaneously characterize the target and comparison set involved. Question Answering over Freebase Acknowledgments We also investigate how our semantic analysis for superlatives can help improve question answering on two benchmark datasets, Free917(Cai and Yates, 2013) and WebQuestions(Berant et al., 2013), which contain 35 questions with attributive superlative expressions in total. We inject our formal analysis as superlative-triggered aggregation operations into an existing system, Xu14(Xu et al., 2014). Note that we leave the comparison set to be decided by Xu14’s parser. The 35 superlative-triggered complex questions can not be correctly answered by most stateof-the-art systems(Berant et al., 2013; Yao and We would like to thank Heng Ji, Benjamin Van Durme, Liwei Chen and Bingfeng Luo for their helpful discussions and three anonymous reviewers for their insightful comments that greatly imp"
P15-2037,W06-1602,0,0.458391,"lect, from Freebase predicates, the most appropriate comparison dimension for a given superlative expression, and further determine its ranking order heuristically. Experimental results show that it is possible to learn from coarsely obtained training data to semantically characterize the comparative constructions involved in attributive superlative expressions. 1 1. Target: one or more items that work as the protagonist of the utterance, and are being compared within the comparative construction, e.g., Nile; 2. Comparison set: the set of items that are being compared against in the utterance(Bos and Nissim, 2006), e.g., all rivers in the world; Introduction Superlatives are fairly common in natural languages and play an essential role in daily communications, when in conveying comparisons among a set of items or degrees of certain properties. Properly analyzing superlative expressions holds the promise for many applications such as question answering (QA), text entailment, sentiment analysis and so on. In literature, analysis of superlatives has drawn more interests from both formal linguistics and semantics(Szabolcsi, 1986; Gawron, 1995; Heim, 1999; Farkas and Kiss, 2000), but relatively less attenti"
P15-2037,S13-1045,0,0.0281309,"e construction in two aspects, the dimension of the comparison and its ranking order. Currently, our analysis suffers from the limited coverage of our WikiDiF. In the future, it would be interesting to improve our method to cover more KB predicates, and extend our NN model with more advanced structures to further improve the performances and also simultaneously characterize the target and comparison set involved. Question Answering over Freebase Acknowledgments We also investigate how our semantic analysis for superlatives can help improve question answering on two benchmark datasets, Free917(Cai and Yates, 2013) and WebQuestions(Berant et al., 2013), which contain 35 questions with attributive superlative expressions in total. We inject our formal analysis as superlative-triggered aggregation operations into an existing system, Xu14(Xu et al., 2014). Note that we leave the comparison set to be decided by Xu14’s parser. The 35 superlative-triggered complex questions can not be correctly answered by most stateof-the-art systems(Berant et al., 2013; Yao and We would like to thank Heng Ji, Benjamin Van Durme, Liwei Chen and Bingfeng Luo for their helpful discussions and three anonymous reviewers for thei"
P15-2037,P14-1090,0,0.0455063,"Missing"
P15-2037,P09-1113,0,0.0480433,"no available datasets that can be used directly for our task, especially no annotations against structured KBs. We therefore present a distantly supervised method to collect annotated training data from rich text resources of Wikipedia and the help of Freebase, without much human involvement. The key assumption behind our method is that if a superlative expression frequently appears in a context that may describe a KB predicate, then this predicate probably plays an important role in the comparative construction triggered by this superlative. Inspired by recent advances in relation extraction(Mintz et al., 2009), given a Freebase predicate, we are able to collect many sentences from Wikipedia pages, which more or less describe this predicate, without extra human annotation. These sentences in turn can be used to collect the cooccurrences between a superlative expression and this predicate. In more detail, we first find all Freebase predicates that may involve in comparative constructions, i.e., all gradable predicates, e.g., fb:geography.river.length, on which differIn this paper, we propose a novel task, semantically interpreting the comparative constructions inherent in attributive superlative expr"
P15-2037,P07-3012,0,0.0580022,"Missing"
P16-1220,N07-4013,0,0.14223,"Missing"
P16-1220,P14-1091,0,0.527771,"efine these candidate answers by applying an answer refinement model which takes the Wikipedia page of the topic entity into consideration to filter out the wrong answers and pick the correct ones. While the overview in Figure 1 works for questions containing single Freebase relation, it also works for questions involving multiple Freebase relations. Consider the question who plays anakin skywalker in star wars 1. The actors who are the answers to this question should satisfy the following constraints: (1) the actor played anakin skywalker; and (2) the actor played in star wars 1. Inspired by Bao et al. (2014), we design a dependency treebased method to handle such multi-relational questions. We first decompose the original question into a set of sub-questions using syntactic patterns which are listed in Appendix. The final answer set of the original question is obtained by intersecting the answer sets of all its sub-questions. For the 2327 example question, the sub-questions are who plays anakin skywalker and who plays in star wars 1. These sub-questions are answered separately over Freebase and Wikipedia, and the intersection of their answers to these sub-questions is treated as the final answer."
P16-1220,P14-1133,0,0.149467,"The state-of-the-art methods for this task can be roughly categorized into two streams. The first is based on semantic parsing (Berant et al., 2013; Kwiatkowski et al., 2013), which typically learns a grammar that can parse natural language to a sophisticated meaning representation language. But such sophistication requires a lot of annotated training examples that contains compositional structures, a practically impossible solution for large KBs such as Freebase. Furthermore, mismatches between grammar predicted structures and KB structure is also a common problem (Kwiatkowski et al., 2013; Berant and Liang, 2014; Reddy et al., 2014). On the other hand, instead of building a formal meaning representation, information extraction methods retrieve a set of candidate answers from KB using relation extraction (Yao and Van Durme, 2014; Yih et al., 2014; Yao, 2015; Bast and Haussmann, 2015) or distributed representations (Bordes et al., 2014; Dong et al., 2015). Designing large training datasets for these methods is relatively easy (Yao and Van Durme, 2014; Bordes et al., 2015; Serban et al., 2016). These methods are often good at producing an answer irrespective of their correctness. However, handling compo"
P16-1220,Q15-1039,0,0.599273,"Missing"
P16-1220,P11-1055,0,0.0472067,"Missing"
P16-1220,D13-1160,0,0.510828,"the MCCNN, and the target prediction during testing time is over these relations. 5.2 Experimental Settings We have 6 dependency tree patterns based on Bao et al. (2014) to decompose the question into subquestions (See Appendix). We initialize the word embeddings with Turian et al. (2010)’s word representations with dimensions set to 50. The hyper parameters in our model are tuned using the development set. The window size of MCCNN is set to 3. The sizes of the hidden layer 1 and the hidden layer 2 of the two MCCNN channels are set to 200 and 100, respectively. We use the Freebase version of Berant et al. (2013), containing 4M entities and 5,323 relations. 5.3 Results and Discussion We use the average question-wise F1 as our evaluation metric.4 To give an idea of the impact of different configurations of our method, we compare the following with existing methods. Structured. This method involves inference on Freebase only. First the entity linking (EL) system is run to predict the topic entity. Then we run the relation extraction (RE) system and select the best relation that can occur with the topic entity. We choose this entity-relation pair to predict the answer. 4 We use the evaluation script avai"
P16-1220,D14-1067,0,0.330735,"t of annotated training examples that contains compositional structures, a practically impossible solution for large KBs such as Freebase. Furthermore, mismatches between grammar predicted structures and KB structure is also a common problem (Kwiatkowski et al., 2013; Berant and Liang, 2014; Reddy et al., 2014). On the other hand, instead of building a formal meaning representation, information extraction methods retrieve a set of candidate answers from KB using relation extraction (Yao and Van Durme, 2014; Yih et al., 2014; Yao, 2015; Bast and Haussmann, 2015) or distributed representations (Bordes et al., 2014; Dong et al., 2015). Designing large training datasets for these methods is relatively easy (Yao and Van Durme, 2014; Bordes et al., 2015; Serban et al., 2016). These methods are often good at producing an answer irrespective of their correctness. However, handling compositional questions that involve multiple entities and relations, still remains a challenge. Consider the question what mountain is the highest in north america. Relation extraction methods typically answer with all the mountains in North America because of the lack of sophisticated representation for the mathematical function"
P16-1220,P13-1042,0,0.372956,"Missing"
P16-1220,D14-1117,0,0.0839941,"Missing"
P16-1220,D12-1069,0,0.143,"Missing"
P16-1220,D13-1161,0,0.250474,"mprovement over the state-of-the-art. 1 Introduction Since the advent of large structured knowledge bases (KBs) like Freebase (Bollacker et al., 2008), YAGO (Suchanek et al., 2007) and DBpedia (Auer et al., 2007), answering natural language questions using those structured KBs, also known as KBbased question answering (or KB-QA), is attracting increasing research efforts from both natural language processing and information retrieval communities. The state-of-the-art methods for this task can be roughly categorized into two streams. The first is based on semantic parsing (Berant et al., 2013; Kwiatkowski et al., 2013), which typically learns a grammar that can parse natural language to a sophisticated meaning representation language. But such sophistication requires a lot of annotated training examples that contains compositional structures, a practically impossible solution for large KBs such as Freebase. Furthermore, mismatches between grammar predicted structures and KB structure is also a common problem (Kwiatkowski et al., 2013; Berant and Liang, 2014; Reddy et al., 2014). On the other hand, instead of building a formal meaning representation, information extraction methods retrieve a set of candidate"
P16-1220,P15-2047,0,0.00839767,"ntually be disambiguated in the joint inference step. For a given mention span, S-MART first retrieves all possible entities of Freebase by surface matching, and then ranks them using a statistical model, which is trained on the frequency counts with which the surface form occurs with the entity. 3.2 Relation Extraction We now proceed to identify the relation between the answer and the entity in the question. Inspired by the recent success of neural network models in KB question-answering (Yih et al., 2015; Dong et al., 2015), and the success of syntactic dependencies for relation extraction (Liu et al., 2015; Xu et al., 2015), we propose a Multi-Channel Convolutional Neural Network (MCCNN) which could exploit both syntactic and sentential information for relation extraction. 1 who, when, what, where, how, which, why, whom, whose. S-MART demo can be accessed at http://msre2edemo.azurewebsites.net/ 2 dobj Feature Extraction Word Representation Given a sub-question, we assume the question word1 that represents the answer has a distinct KB relation r with an entity e found in the question, and predict a single KB triple (e, r, ?) for each subquestion (here ? stands for the answer entities). The QA pr"
P16-1220,P14-5010,0,0.00510178,"use the shortest path between an entity mention and the question word in the dependency tree3 as input to the first channel. Similar to Xu et al. (2015), we treat the path as a concatenation of vectors of words, dependency edge directions and dependency labels, and feed it to the convolution layer. Note that, the entity mention and the question word are excluded from the dependency path so as to learn a more general relation representation in syntactic level. As shown in Figure 2, the dependency path between who and shaq is ← dobj – play – nsubj →. 3 We use Stanford CoreNLP dependency parser (Manning et al., 2014). 2328 Sentential Features This channel takes the words in the sentence as input excluding the question word and the entity mention. As illustrated in Figure 2, the vectors for did, first, play and for are fed into this channel. 3.2.2 Objective Function and Learning The model is learned using pairs of question and its corresponding gold relation from the training data. Given an input question x with an annotated entity mention, the network outputs a vector o(x), where the entry ok (x) is the probability that there exists the k-th relation between the entity and the expected answer. We denote t"
P16-1220,P09-1113,0,0.125939,"Missing"
P16-1220,D13-1184,0,0.0223252,"s to Wikipedia, that person might first determine that the question is about Shaquille O’Neal, then go to O’Neal ’s Wikipedia page, and search for the sentences that contain the candidate answers as evidence. By analyzing these sentences, one can figure out whether a candidate answer is correct or not. 4.1 Finding Evidence from Wikipedia As mentioned above, we should first find the Wikipedia page corresponding to the topic entity in the given question. We use Freebase API to convert Freebase entity to Wikipedia page. We extract the content from the Wikipedia page and process it with Wikifier (Cheng and Roth, 2013) which recognizes Wikipedia entities, which can further be linked to Freebase entities using Freebase API. Additionally we use Stanford CoreNLP (Manning et al., 2014) for tokenization and entity co-reference resolution. We search for the sentences containing the candidate answer entities retrieved from Freebase. For example, the Wikipedia page of O’Neal contains a sentence “O’Neal was drafted by the Orlando Magic with the first overall pick in the 1992 NBA draft”, which is taken into account by the refinement model (our inference model on Wikipedia) to discriminate whether Orlando Magic is the"
P16-1220,N15-1077,0,0.0182235,"Missing"
P16-1220,P15-1026,0,0.785384,"ng examples that contains compositional structures, a practically impossible solution for large KBs such as Freebase. Furthermore, mismatches between grammar predicted structures and KB structure is also a common problem (Kwiatkowski et al., 2013; Berant and Liang, 2014; Reddy et al., 2014). On the other hand, instead of building a formal meaning representation, information extraction methods retrieve a set of candidate answers from KB using relation extraction (Yao and Van Durme, 2014; Yih et al., 2014; Yao, 2015; Bast and Haussmann, 2015) or distributed representations (Bordes et al., 2014; Dong et al., 2015). Designing large training datasets for these methods is relatively easy (Yao and Van Durme, 2014; Bordes et al., 2015; Serban et al., 2016). These methods are often good at producing an answer irrespective of their correctness. However, handling compositional questions that involve multiple entities and relations, still remains a challenge. Consider the question what mountain is the highest in north america. Relation extraction methods typically answer with all the mountains in North America because of the lack of sophisticated representation for the mathematical function highest. To select t"
P16-1220,Q14-1030,1,0.879493,"thods for this task can be roughly categorized into two streams. The first is based on semantic parsing (Berant et al., 2013; Kwiatkowski et al., 2013), which typically learns a grammar that can parse natural language to a sophisticated meaning representation language. But such sophistication requires a lot of annotated training examples that contains compositional structures, a practically impossible solution for large KBs such as Freebase. Furthermore, mismatches between grammar predicted structures and KB structure is also a common problem (Kwiatkowski et al., 2013; Berant and Liang, 2014; Reddy et al., 2014). On the other hand, instead of building a formal meaning representation, information extraction methods retrieve a set of candidate answers from KB using relation extraction (Yao and Van Durme, 2014; Yih et al., 2014; Yao, 2015; Bast and Haussmann, 2015) or distributed representations (Bordes et al., 2014; Dong et al., 2015). Designing large training datasets for these methods is relatively easy (Yao and Van Durme, 2014; Bordes et al., 2015; Serban et al., 2016). These methods are often good at producing an answer irrespective of their correctness. However, handling compositional questions th"
P16-1220,Q16-1010,1,0.244478,"Missing"
P16-1220,P13-1158,0,0.290087,"Missing"
P16-1220,N10-1145,0,0.0208895,"Missing"
P16-1220,N13-1008,0,0.0574113,"Missing"
P16-1220,P16-1056,0,0.00682235,"Missing"
P16-1220,P10-1040,0,0.0233341,"ct to a mediator node, and the second from the mediator to the object node. For each relation candidate r, we issue the query (e, r, ?) to the KB, and label the relation that produces the answer with minimal F1 -loss against the gold answer, as the surrogate gold relation. From the training set, we collect 461 relations to train the MCCNN, and the target prediction during testing time is over these relations. 5.2 Experimental Settings We have 6 dependency tree patterns based on Bao et al. (2014) to decompose the question into subquestions (See Appendix). We initialize the word embeddings with Turian et al. (2010)’s word representations with dimensions set to 50. The hyper parameters in our model are tuned using the development set. The window size of MCCNN is set to 3. The sizes of the hidden layer 1 and the hidden layer 2 of the two MCCNN channels are set to 200 and 100, respectively. We use the Freebase version of Berant et al. (2013), containing 4M entities and 5,323 relations. 5.3 Results and Discussion We use the average question-wise F1 as our evaluation metric.4 To give an idea of the impact of different configurations of our method, we compare the following with existing methods. Structured. T"
P16-1220,D07-1003,0,0.0587728,"Missing"
P16-1220,P15-1129,0,0.0159012,"gates representational issues in relation extraction, but also alleviates the data scarcity problem to some extent. Consider the question, who was queen isabella’s mother. Answering this question involves predicting two constraints hidden in the word mother. One constraint is that the answer should be the parent of Isabella, and the other is that the answer’s gender is female. Such words with multiple latent constraints have been a pain-in-the-neck for both semantic parsing and relation extraction, and requires larger training data (this phenomenon is coined as sub-lexical compositionality by Wang et al. (2015)). Most systems are good at triggering the parent constraint, but fail on the other, i.e., the answer entity should be female. Whereas the textual evidence from Wikipedia, . . . her mother was Isabella of Barcelos . . . , can act as a further constraint to answer the question correctly. We present a novel method for question answering which infers on both structured and unstructured resources. Our method consists of two main steps as outlined in §2. In the first step we extract answers for a given question using a structured KB (here Freebase) by jointly performing entity linking and relation"
P16-1220,P14-1090,0,0.525003,"Missing"
P16-1220,N13-1106,0,0.0263742,"Missing"
P16-1220,N15-3014,0,0.252342,"eaning representation language. But such sophistication requires a lot of annotated training examples that contains compositional structures, a practically impossible solution for large KBs such as Freebase. Furthermore, mismatches between grammar predicted structures and KB structure is also a common problem (Kwiatkowski et al., 2013; Berant and Liang, 2014; Reddy et al., 2014). On the other hand, instead of building a formal meaning representation, information extraction methods retrieve a set of candidate answers from KB using relation extraction (Yao and Van Durme, 2014; Yih et al., 2014; Yao, 2015; Bast and Haussmann, 2015) or distributed representations (Bordes et al., 2014; Dong et al., 2015). Designing large training datasets for these methods is relatively easy (Yao and Van Durme, 2014; Bordes et al., 2015; Serban et al., 2016). These methods are often good at producing an answer irrespective of their correctness. However, handling compositional questions that involve multiple entities and relations, still remains a challenge. Consider the question what mountain is the highest in north america. Relation extraction methods typically answer with all the mountains in North America bec"
P16-1220,P13-1171,0,0.0610357,"Missing"
P16-1220,P14-2105,0,0.102811,"a sophisticated meaning representation language. But such sophistication requires a lot of annotated training examples that contains compositional structures, a practically impossible solution for large KBs such as Freebase. Furthermore, mismatches between grammar predicted structures and KB structure is also a common problem (Kwiatkowski et al., 2013; Berant and Liang, 2014; Reddy et al., 2014). On the other hand, instead of building a formal meaning representation, information extraction methods retrieve a set of candidate answers from KB using relation extraction (Yao and Van Durme, 2014; Yih et al., 2014; Yao, 2015; Bast and Haussmann, 2015) or distributed representations (Bordes et al., 2014; Dong et al., 2015). Designing large training datasets for these methods is relatively easy (Yao and Van Durme, 2014; Bordes et al., 2015; Serban et al., 2016). These methods are often good at producing an answer irrespective of their correctness. However, handling compositional questions that involve multiple entities and relations, still remains a challenge. Consider the question what mountain is the highest in north america. Relation extraction methods typically answer with all the mountains in North"
P16-1220,P15-1128,0,0.571937,"retrieve the top 5 entities from Freebase. These entities are treated as candidate entities that will eventually be disambiguated in the joint inference step. For a given mention span, S-MART first retrieves all possible entities of Freebase by surface matching, and then ranks them using a statistical model, which is trained on the frequency counts with which the surface form occurs with the entity. 3.2 Relation Extraction We now proceed to identify the relation between the answer and the entity in the question. Inspired by the recent success of neural network models in KB question-answering (Yih et al., 2015; Dong et al., 2015), and the success of syntactic dependencies for relation extraction (Liu et al., 2015; Xu et al., 2015), we propose a Multi-Channel Convolutional Neural Network (MCCNN) which could exploit both syntactic and sentential information for relation extraction. 1 who, when, what, where, how, which, why, whom, whose. S-MART demo can be accessed at http://msre2edemo.azurewebsites.net/ 2 dobj Feature Extraction Word Representation Given a sub-question, we assume the question word1 that represents the answer has a distinct KB relation r with an entity e found in the question, and pre"
P16-1220,D15-1062,1,0.178255,"guated in the joint inference step. For a given mention span, S-MART first retrieves all possible entities of Freebase by surface matching, and then ranks them using a statistical model, which is trained on the frequency counts with which the surface form occurs with the entity. 3.2 Relation Extraction We now proceed to identify the relation between the answer and the entity in the question. Inspired by the recent success of neural network models in KB question-answering (Yih et al., 2015; Dong et al., 2015), and the success of syntactic dependencies for relation extraction (Liu et al., 2015; Xu et al., 2015), we propose a Multi-Channel Convolutional Neural Network (MCCNN) which could exploit both syntactic and sentential information for relation extraction. 1 who, when, what, where, how, which, why, whom, whose. S-MART demo can be accessed at http://msre2edemo.azurewebsites.net/ 2 dobj Feature Extraction Word Representation Given a sub-question, we assume the question word1 that represents the answer has a distinct KB relation r with an entity e found in the question, and predict a single KB triple (e, r, ?) for each subquestion (here ? stands for the answer entities). The QA problem is thus form"
P16-1220,D12-1035,0,0.0586932,"Missing"
P16-1220,P15-1049,0,0.0342182,"the 2327 example question, the sub-questions are who plays anakin skywalker and who plays in star wars 1. These sub-questions are answered separately over Freebase and Wikipedia, and the intersection of their answers to these sub-questions is treated as the final answer. 3 dobj [Who] did [shaq] first Inference on Freebase Convolution 3.1 Entity Linking For each question, we use hand-built sequences of part-of-speech categories to identify all possible named entity mention spans, e.g., the sequence NN (shaq) may indicate an entity. For each mention span, we use the entity linking tool S-MART2 (Yang and Chang, 2015) to retrieve the top 5 entities from Freebase. These entities are treated as candidate entities that will eventually be disambiguated in the joint inference step. For a given mention span, S-MART first retrieves all possible entities of Freebase by surface matching, and then ranks them using a statistical model, which is trained on the frequency counts with which the surface form occurs with the entity. 3.2 Relation Extraction We now proceed to identify the relation between the answer and the entity in the question. Inspired by the recent success of neural network models in KB question-answeri"
P16-1220,D15-1237,0,0.0132239,"Missing"
P17-1040,D11-1141,0,0.0360751,"he only work in neural-network-based noise modeling is to use one single global transition matrix to model the noise introduced by crosslingual projection of training data (Fang and Cohn, 2016). Our work advances them through generating a transition matrix dynamically for each instance, to avoid using one single component to characterize both reliable and unreliable data. In addition to relation extraction, distant supervision (DS) is shown to be effective in generating training data for various NLP tasks, e.g., tweet sentiment classification (Go et al., 2009), tweet named entity classifying (Ritter et al., 2011), etc. However, these early applications of DS do not well address the issue of data noise. In relation extraction (RE), recent works have been proposed to reduce the influence of wrongly labeled data. The work presented by (Takamatsu et al., 2012) removes potential noisy sentences by identifying bad syntactic patterns at the preprocessing stage. (Xu et al., 2013) use pseudorelevance feedback to find possible false negative data. (Riedel et al., 2010) make the at-leastone assumption and propose to alleviate the noise problem by considering RE as a multi-instance classification problem. Followi"
P17-1040,K16-1018,0,0.0182451,"vely trained using a novel curriculum learning based method without any direct supervision about the noise. We thoroughly evaluate our approach under a wide range of extraction scenarios. Experimental results show that our approach consistently improves the extraction results and outperforms the state-of-the-art in various evaluation scenarios. 1 Introduction Distant supervision (DS) is rapidly emerging as a viable means for supporting various classification tasks – from relation extraction (Mintz et al., 2009) and sentiment classification (Go et al., 2009) to cross-lingual semantic analysis (Fang and Cohn, 2016). By using knowledge learned from seed examples to label data, DS automatically prepares large scale training data for these tasks. While promising, DS does not guarantee perfect results and often introduces noise to the generated data. In the context of relation extraction, DS works by considering sentences containing both the subject and object of a &lt;subj, rel, obj&gt; triple 430 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 430–439 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18"
P17-1040,Q13-1030,0,0.120447,"hanxing Zhu3 , Songfang Huang4 , Rui Yan1 and Dongyan Zhao1 1 ICST, Peking University, China 2 School of Computing and Communications, Lancaster University, UK 3 Peking University, China 4 IBM China Research Lab, China {bf luo,fengyansong,zhanxing.zhu,ruiyan,zhaody}@pku.edu.cn z.wang@lancaster.ac.uk huangsf@cn.ibm.com Abstract as its supports. However, the generated data are not always perfect. For instance, DS could match the knowledge base (KB) triple, &lt;Donald Trump, born-in, New York&gt; in false positive contexts like Donald Trump worked in New York City. Prior works (Takamatsu et al., 2012; Ritter et al., 2013) show that DS often mistakenly labels real positive instances as negative (false negative) or versa vice (false positive), and there could be confusions among positive labels as well. These noises can severely affect training and lead to poorlyperforming models. Tackling the noisy data problem of DS is nontrivial, since there usually lacks of explicit supervision to capture the noise. Previous works have tried to remove sentences containing unreliable syntactic patterns (Takamatsu et al., 2012), design new models to capture certain types of noise or aggregate multiple predictions under the at-"
P17-1040,P11-1055,0,0.0938601,"n extraction (RE), recent works have been proposed to reduce the influence of wrongly labeled data. The work presented by (Takamatsu et al., 2012) removes potential noisy sentences by identifying bad syntactic patterns at the preprocessing stage. (Xu et al., 2013) use pseudorelevance feedback to find possible false negative data. (Riedel et al., 2010) make the at-leastone assumption and propose to alleviate the noise problem by considering RE as a multi-instance classification problem. Following this assumption, people further improves the original paradigm using probabilistic graphic models (Hoffmann et al., 2011; Surdeanu et al., 2012), and neural network methods (Zeng et al., 2015). Recently, (Lin et al., 2016) propose to use attention mechanism to reduce the noise within a sentence bag. Instead of characterizing the noise, these approaches only aim to alleviate the effect of noise. The at-least-one assumption is often too strong in practice, and there are still chances that the sentence bag may be false positive or false negative. Thus it is important to model the noise pattern to guide the learning procedure. (Ritter et al., 2013) and (Min et al., 2013) try to employ a set of latent variables to r"
P17-1040,D12-1042,0,0.523535,"positive), and there could be confusions among positive labels as well. These noises can severely affect training and lead to poorlyperforming models. Tackling the noisy data problem of DS is nontrivial, since there usually lacks of explicit supervision to capture the noise. Previous works have tried to remove sentences containing unreliable syntactic patterns (Takamatsu et al., 2012), design new models to capture certain types of noise or aggregate multiple predictions under the at-leastone assumption that at least one of the aligned sentences supports the triple in KB (Riedel et al., 2010; Surdeanu et al., 2012; Ritter et al., 2013; Min et al., 2013). These approaches represent a substantial leap forward towards making DS more practical. however, are either tightly couple to certain types of noise, or have to rely on manual rules to filter noise, thus unable to scale. Recent breakthrough in neural networks provides a new way to reduce the influence of incorrectly labeled data by aggregating multiple training instances attentively for relation classification, without explicitly characterizing the inherent noise (Lin et al., 2016; Zeng et al., 2015). Although promising, modeling noise within neural ne"
P17-1040,P12-1076,0,0.121946,"Feng∗1 , Zheng Wang2 , Zhanxing Zhu3 , Songfang Huang4 , Rui Yan1 and Dongyan Zhao1 1 ICST, Peking University, China 2 School of Computing and Communications, Lancaster University, UK 3 Peking University, China 4 IBM China Research Lab, China {bf luo,fengyansong,zhanxing.zhu,ruiyan,zhaody}@pku.edu.cn z.wang@lancaster.ac.uk huangsf@cn.ibm.com Abstract as its supports. However, the generated data are not always perfect. For instance, DS could match the knowledge base (KB) triple, &lt;Donald Trump, born-in, New York&gt; in false positive contexts like Donald Trump worked in New York City. Prior works (Takamatsu et al., 2012; Ritter et al., 2013) show that DS often mistakenly labels real positive instances as negative (false negative) or versa vice (false positive), and there could be confusions among positive labels as well. These noises can severely affect training and lead to poorlyperforming models. Tackling the noisy data problem of DS is nontrivial, since there usually lacks of explicit supervision to capture the noise. Previous works have tried to remove sentences containing unreliable syntactic patterns (Takamatsu et al., 2012), design new models to capture certain types of noise or aggregate multiple pre"
P17-1040,P16-1200,0,0.318747,"e aligned sentences supports the triple in KB (Riedel et al., 2010; Surdeanu et al., 2012; Ritter et al., 2013; Min et al., 2013). These approaches represent a substantial leap forward towards making DS more practical. however, are either tightly couple to certain types of noise, or have to rely on manual rules to filter noise, thus unable to scale. Recent breakthrough in neural networks provides a new way to reduce the influence of incorrectly labeled data by aggregating multiple training instances attentively for relation classification, without explicitly characterizing the inherent noise (Lin et al., 2016; Zeng et al., 2015). Although promising, modeling noise within neural network architectures is still in its early stage and much remains to be done. In this paper, we aim to enhance DS noise modeling by providing the capability to explicitly characterize the noise in the DS-style training data Distant supervision significantly reduces human efforts in building training data for many classification tasks. While promising, this technique often introduces noise to the generated training data, which can severely affect the model performance. In this paper, we take a deep look at the application o"
P17-1040,N13-1095,0,0.0949015,"ong positive labels as well. These noises can severely affect training and lead to poorlyperforming models. Tackling the noisy data problem of DS is nontrivial, since there usually lacks of explicit supervision to capture the noise. Previous works have tried to remove sentences containing unreliable syntactic patterns (Takamatsu et al., 2012), design new models to capture certain types of noise or aggregate multiple predictions under the at-leastone assumption that at least one of the aligned sentences supports the triple in KB (Riedel et al., 2010; Surdeanu et al., 2012; Ritter et al., 2013; Min et al., 2013). These approaches represent a substantial leap forward towards making DS more practical. however, are either tightly couple to certain types of noise, or have to rely on manual rules to filter noise, thus unable to scale. Recent breakthrough in neural networks provides a new way to reduce the influence of incorrectly labeled data by aggregating multiple training instances attentively for relation classification, without explicitly characterizing the inherent noise (Lin et al., 2016; Zeng et al., 2015). Although promising, modeling noise within neural network architectures is still in its earl"
P17-1040,P13-2117,0,0.069312,"Missing"
P17-1040,P09-1113,0,0.0499241,"erize the noise in the training data built by distant supervision. The transition matrix can be effectively trained using a novel curriculum learning based method without any direct supervision about the noise. We thoroughly evaluate our approach under a wide range of extraction scenarios. Experimental results show that our approach consistently improves the extraction results and outperforms the state-of-the-art in various evaluation scenarios. 1 Introduction Distant supervision (DS) is rapidly emerging as a viable means for supporting various classification tasks – from relation extraction (Mintz et al., 2009) and sentiment classification (Go et al., 2009) to cross-lingual semantic analysis (Fang and Cohn, 2016). By using knowledge learned from seed examples to label data, DS automatically prepares large scale training data for these tasks. While promising, DS does not guarantee perfect results and often introduces noise to the generated data. In the context of relation extraction, DS works by considering sentences containing both the subject and object of a &lt;subj, rel, obj&gt; triple 430 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 430–439 c Vancouver"
P17-1040,D15-1203,0,0.521273,"s supports the triple in KB (Riedel et al., 2010; Surdeanu et al., 2012; Ritter et al., 2013; Min et al., 2013). These approaches represent a substantial leap forward towards making DS more practical. however, are either tightly couple to certain types of noise, or have to rely on manual rules to filter noise, thus unable to scale. Recent breakthrough in neural networks provides a new way to reduce the influence of incorrectly labeled data by aggregating multiple training instances attentively for relation classification, without explicitly characterizing the inherent noise (Lin et al., 2016; Zeng et al., 2015). Although promising, modeling noise within neural network architectures is still in its early stage and much remains to be done. In this paper, we aim to enhance DS noise modeling by providing the capability to explicitly characterize the noise in the DS-style training data Distant supervision significantly reduces human efforts in building training data for many classification tasks. While promising, this technique often introduces noise to the generated training data, which can severely affect the model performance. In this paper, we take a deep look at the application of distant supervisio"
P17-1040,D14-1162,0,0.114232,"serve as gold standard, we only evaluate bag-level models on E NTITY RE, a standard practice in previous works (Surdeanu et al., 2012; Zeng et al., 2015; Lin et al., 2016). Datasets We evaluate our approach on two datasets. 434 5.2 1 .0 0 Experimental Setup Hyper-parameters We use 200 convolution kernels with widow size 3. During training, we use stochastic gradient descend (SGD) with batch size 20. The learning rates for sentence-level and bag-level models are 0.1 and 0.01, respectively. Sentence level experiments are performed on T IME RE, using 100-d word embeddings pretrained using GloVe (Pennington et al., 2014) on Wikipedia and Gigaword (Parker et al., 2011), and 20-d vectors for distance embeddings. Each of the three subsets of T IME RE is added after the previous phase has run for 15 epochs. The trace regularization weights are β1 = 0.01, β2 = −0.01 and β3 = −0.1, respectively, from the reliable to the most unreliable, with the ratio of β3 and β2 fixed to 10 or 5 when tuning. Bag level experiments are performed on both T IME RE and E NTITY RE. For T IME RE, we use the same parameters as above. For E NTITY RE, we use 50-d word embeddings pre-trained on the NYT corpus using word2vec (Mikolov et al.,"
P17-2036,W14-4012,0,0.128818,"Missing"
P17-2036,D16-1230,0,0.0722005,"Missing"
P17-2036,C16-1316,1,0.792893,"end to generate longer, more meaningful and diverse replies, which sheds light on neural sequence generation. Table 2: The length, entropy, and diversity of the replies on the context-insensitive and contextaware (WSeq,concat) methods. relevant context utterances as well as weakens irrelevant contexts. RQ2: What is the effect of context on neural dialog systems? We are now curious about how context information affects neural conversational systems. In Table 2, we present three auxiliary metrics, i.e., sentence length, entropy, and diversity. The former two are used in Serban et al. (2016) and Mou et al. (2016), whereas the latter one is used in Zhang and Hurley (2008). As shown, content-aware conversational models tend to generate longer, more meaningful and diverse replies compared with content-insensitive models, given that they also improve BLEU scores.2 This shows an interesting phenomenon of neural sequence generation: an encoder-decoder framework needs sufficient source information for meaningful generation of the target; it simply does not fall into meaningful content from less meaningful input. A similar phenomenon is also reported in our previous work (Mou et al., 2016); we show that, a sa"
P17-2036,D11-1054,0,0.240659,"Missing"
P17-2036,P15-1152,0,0.283936,"rison to analyze how to use context effectively. In this paper, we conduct an empirical study to compare various models and investigate the effect of context information in dialog systems. We also propose a variant that explicitly weights context vectors by context-query relevance, outperforming the other baselines. 1 Introduction Recently, human-computer conversation is attracting increasing attention due to its promising potentials and alluring commercial values. Researchers have proposed both retrieval methods (Ji et al., 2014; Yan et al., 2016) and generative methods (Ritter et al., 2011; Shang et al., 2015) for automatic conversational systems. With the success of deep learning techniques, neural networks have demonstrated powerful capability of learning human dialog patterns; given a user-issued utterance as an input query q, the network can generate a reply r, which is usually accomplished in a sequence-to-sequence (Seq2Seq) manner (Shang et al., 2015). In the literature, there are two typical research setups for dialog systems: single-turn and multiturn. Single-turn conversation is, perhaps, the simplest setting where the model only takes q into consideration when generating r (Shang et al.,"
P17-2036,N15-1020,0,0.0169469,"Missing"
P17-2036,D16-1172,0,0.0123649,"Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2036 text weighting approach, outperforming the other baselines. 2 2.1 Models Non-Hierarchical Model To model a few utterances before the current query, several studies directly concatenate these sentences together and use a single model to capture the meaning of context and the query (Yan et al., 2016; Sordoni et al., 2015). They are referred to as non-hierarchical models in our paper. Such method is also used in other NLP tasks, e.g., document-level sentiment analysis (Xu et al., 2016) and machine comprehension (Wang and Jiang, 2017). Following the classic encode-decoder framework, we use a Seq2Seq network, which transforms the query and context into a fixed-length vector venc by a recurrent neural network (RNN) during encoding; then, in the decoding phase, it generates a reply r with another RNN in a wordby-word fashion. (See Figure 1a.) In our study, we adopt RNNs with gated recurrent units (Cho et al., 2014, GRUs), which alleviates the long propagation problem of vanilla RNNs. When decoding, we apply beam search with a size of 5. 2.2 (a) Non-hierarchical model. (b) Hiera"
P18-1194,D16-1146,0,0.0368066,"Missing"
P18-1194,H90-1021,0,0.0607127,"hich matches the property of wk zk better than probability. Actually, when performing model ensemble, ensembling with logits is often empirically better than with the final probability3 . This is also the reason why we choose to operate on logits in Sec. 3.3. 4 Evaluation Methodology Our experiments aim to answer three questions: Q1: Does the use of REs enhance the learning quality when the number of annotated instances is small? Q2: Does the use of REs still help when using the full training data? Q3: How can we choose from different combination methods? 4.1 Datasets We use the ATIS dataset (Hemphill et al., 1990) to evaluate our approach. This dataset is widely used in SLU research. It includes queries of flights, meal, etc. We follow the setup of Liu and Lane (2016) by using 4,978 queries for training and 893 for testing, with 18 intent labels and 127 slot labels. We also split words like Miami’s into Miami ’s during the tokenization phase to reduce the number of words that do not have a pre-trained word embedding. This strategy is useful for fewshot learning. 3 An example can be found in the ensemble version that Juan et al. (2016) used in the Avazu Kaggle competition. To answer Q1 , we also exploit"
P18-1194,P16-1228,0,0.0358084,"Missing"
P18-1194,D16-1173,0,0.0517657,"Missing"
P18-1194,D17-1201,0,0.0204074,"s the effectiveness of our methods, and indicates that simple REs are quite costefficient since these simple REs only contain 1-2 RE groups and thus very easy to produce. We can also see that using complex REs generally leads to better results compared to using simple REs. This indicates that when considering using REs to improve a NN model, we can start with simple REs, and gradually increase the RE complexity to improve the performance over time7 . 6 Related Work Our work builds upon the following techniques, while qualitatively differing from each NN with Rules. On the initialization side, Li et al. (2017) uses important n-grams to initialize the convolution filters. On the input side, Wang et al. (2017a) uses knowledge base rules to find relevant concepts for short texts to augment input. On the output side, Hu et al. (2016a; 2016b) and Guo et al. (2017) use FOL rules to rectify the output probability of NN, and then let NN learn from the rectified distribution in a teacher-student framework. Xiao et al. (2017), on the other hand, modifies the decoding score of NN by multiplying a weight derived from rules. On the loss function side, people modify the loss function to model the relationship be"
P18-1194,D16-1197,0,0.0227221,"n initialization or in loss function often require special properties of the task, these approaches are not applicable to our problem. Our work thus offers new ways to exploit RE rules at different levels of a NN. NNs and REs. As for NNs and REs, previous work has tried to use RE to speed up the decoding phase of a NN (Strauß et al., 2016) and generating REs from natural language specifications of the 7 We do not include results of both for slot filling since its REs are different from feat and logit, and we have already shown that the attention loss method does not work for slot filling. RE (Locascio et al., 2016). By contrast, our work aims to use REs to improve the prediction ability of a NN. Few-Shot Learning. Prior work either considers few-shot learning in a metric learning framework (Koch et al., 2015; Vinyals et al., 2016), or stores instances in a memory (Santoro et al., 2016; Kaiser et al., 2017) to match similar instances in the future. Wang et al. (2017b) further uses the semantic meaning of the class name itself to provide extra information for few-shot learning. Unlike these previous studies, we seek to use the humangenerated REs to provide additional information. Natural Language Understa"
P18-1194,D14-1162,0,0.0835473,"ositive RE for intent (or slot) k can often be treated as negative REs for other intents (or slots). As such, we use the positive REs for intent (or slot) k as the negative REs for other intents (or slots) in our experiments. 4.3 Experimental Setup Hyper-parameters. Our hyper-parameters for the BLSTM are similar to the ones used by Liu and Lane (2016). Specifically, we use batch size 16, dropout probability 0.5, and BLSTM cell size 100. The attention loss weight is 16 (both positive and negative) for full few-shot learning settings and 1 for other settings. We use the 100d GloVe word vectors (Pennington et al., 2014) pre-trained on Wikipedia and Gigaword (Parker et al., 2011), and the Adam optimizer (Kingma and Ba, 2014) with learning rate 0.001. Evaluation Metrics. We report accuracy and macro-F1 for intent detection, and micro/macroF1 for slot filling. Micro/macro-F1 are the harmonic mean of micro/macro precision and recall. Macro-precision/recall are calculated by averaging precision/recall of each label, and microprecision/recall are averaged over each prediction. Competitors and Naming Conventions. Here, a bold Courier typeface like BLSTM denotes the notations of the models that we will compare in Se"
P18-2070,W01-1605,0,0.241076,"till fall behind this state-of-the-art method. The main reason might be that MST-full follows a global graph-based dependency parsing framework, where their high order methods (in cubic time complexity) can directly analyze the relationship between any EDUs pairs in the discourse, while, we choose the transition-based local method with linear time complexity, which can only investigate the top EDUs in S and B according to the selected actions, thus usually has a lower performance than the global graph-based methods, but with a Evaluation and Results Dataset: We use the RST Discourse Treebank (Carlson et al., 2001) with the same split as in (Li et al., 2014), i.e., 312 for training, 30 for development and 38 for testing. We experiment with two set of relations, the 111 types of fine-grained relations and the 19 types of coarse-grained relations, respectively. Evaluation Metrics: In the Rhetorical Structure Theory (RST) (Mann and Thompson, 1988), head is the core of a discourse, and a dependent gives supporting evidence to its head with certain relationship. We adopt unlabeled accuracy U AS (the ratio of EDUs that correctly identify their heads) and labeled accuracy LAS (the ratio of EDUs that have both"
P18-2070,W10-4327,0,0.0325967,"account. The automatically captured discourse cohesion benefits discourse parsing, especially for long span scenarios. Experiments on the RST discourse treebank show that our method outperforms traditional featured based methods, and the memory based discourse cohesion can improve the overall parsing performance significantly 1 . 1 Introduction Discourse parsing aims to identify the structure and relationship between different element discourse units (EDUs). As a fundamental topic in natural language processing, discourse parsing can assist many down-stream applications such as summarization (Louis et al., 2010), sentiment analysis (Polanyi and van den Berg, 2011) and question-answering (Ferrucci et al., 2010). However, the performance of discourse parsing is still far from perfect, especially for EDUs that are distant to each other in the discourse. In fact, as found in (Jia et al., 2018), the discourse parsing performance drops quickly as the dependency span increases. The reason may be twofold: 1 Code for replicating our experiments is available at https://github.com/PKUYeYuan/ACL2018 CFDP. 438 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers),"
P18-2070,P04-1015,0,0.130798,"rtain relationship. We adopt unlabeled accuracy U AS (the ratio of EDUs that correctly identify their heads) and labeled accuracy LAS (the ratio of EDUs that have both correct heads and relations) as our evaluation metrics. Baselines: We compare our method with the following baselines and models: (1) Perceptron: We re-implement the perceptron based arc-eager style dependency discourse parser as mentioned in (Jia et al., 2018) with coarse-grained relation. The Perceptron model chooses words, POS tags, positions and length features, totally 100 feature templates, with the early update strategy (Collins and Roark, 2004). (2) Jia18: Jia et al. (2018) implement a transition-based discourse parser with stacked LSTM, where they choose a two-layer LSTM to represent EDUs by encoding four kinds of features including words, POS tags, positions and length features. (3) Basic EDU representation (Basic): Our discourse parser with the basic EDU representation method mentioned in Section 3. (4) Memory refined representation (Refined): Our full parser equipped with the basic EDU representation method and the memory networks to capture the discourse cohesion mentioned in Section 3. (5) MST-full (Li et al., 2014): a graph-b"
P18-2070,P15-1033,0,0.0222777,"hub.com/PKUYeYuan/ACL2018 CFDP. 438 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 438–443 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics 2 (1) I feel hungry after wake up, (2) I rush into the kitchen and make my breakfast. (3) My breakfast is hamburger. (4) It is eight o'clock when I leave home. (5) So late! Model overview Memory network slot1 Our parser is an arc-eager style transition system (Nivre, 2003) with 2 stacks and a queue as shown in Figure 2, which is similar in spirit with (Dyer et al., 2015; Ballesteros et al., 2015). We follow the conventional data structures in transition-based dependency parsing, i.e., a queue (B) of EDUs to be processed, a stack (S) to store the partially constructed discourse trees, and a stack (A) to represent the history of transitions (actions combined with discourse relations). slot2 slot3 (9) It is nine o'clock. (10) Thank God, I am not late for work. slotn ... (6) I drive into the highway, (7) but meet a traffic jam. (8) Oh, I finally arrive at the company. (11) But the hamburger is cold, (12) order some take-away food is better, maybe. Figure 1: An i"
P18-2070,P14-5010,0,0.00567472,"Missing"
P18-2070,J91-1002,0,0.744108,"he syntactic or semantic relationship between words or phrases in a discourse, and, to some extent, can indicate the topic changing or threads in a discourse. Discourse cohesion includes five situations, including reference, substitution, ellipsis, conjunction and lexical cohesion (Halliday and Hasan, 1989). Here, lexical cohesion reflects the semantic relationship of words, and can be modeled as the recurrence of words, synonym and contextual words. However, previous works do not well model the discourse cohesion within the discourse parsing task, or do not even take this issue into account. Morris and Hirst (1991) proposes to utilize Roget thesauri to form lexical chains (sequences of semantically related words that can reflect the topic shifts within a discourse), which are used to extract features to characterize discourse structures. (Joty et al., 2013) uses lexical chain feature to model multi-sentential relation. Actually, these simplified cohesion features can already improve parsing performance, especially in long spans. Secondly, in modern neural network methods, modeling discourse cohesion as part of the networks is not a trivial task. One can still use off-the-shell tools to obtain lexical ch"
P18-2070,W03-3017,0,0.457685,"The reason may be twofold: 1 Code for replicating our experiments is available at https://github.com/PKUYeYuan/ACL2018 CFDP. 438 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 438–443 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics 2 (1) I feel hungry after wake up, (2) I rush into the kitchen and make my breakfast. (3) My breakfast is hamburger. (4) It is eight o'clock when I leave home. (5) So late! Model overview Memory network slot1 Our parser is an arc-eager style transition system (Nivre, 2003) with 2 stacks and a queue as shown in Figure 2, which is similar in spirit with (Dyer et al., 2015; Ballesteros et al., 2015). We follow the conventional data structures in transition-based dependency parsing, i.e., a queue (B) of EDUs to be processed, a stack (S) to store the partially constructed discourse trees, and a stack (A) to represent the history of transitions (actions combined with discourse relations). slot2 slot3 (9) It is nine o'clock. (10) Thank God, I am not late for work. slotn ... (6) I drive into the highway, (7) but meet a traffic jam. (8) Oh, I finally arrive at the compa"
P18-2070,P13-1048,0,0.0540609,"Missing"
P19-1001,E17-1104,0,0.0356527,"on between c and r, and how to learn such a deep model from D. form. As far as we know, this is the first architecture that realizes deep interaction for multi-turn response selection. Encouraged by the big success of deep neural architectures such as Resnet (He et al., 2016) and inception (Szegedy et al., 2015) in computer vision, researchers have studied if they can achieve similar results with deep neural networks on NLP tasks. Although deep models have not yet brought breakthroughs to NLP as they do to computer vision, they have proven effective in a few tasks such as text classification (Conneau et al., 2017), natural language inference (Kim et al., 2018; Tay et al., 2018), and question answering (Tay et al., 2018; Kim et al., 2018), etc. In this work, we attempt to improve the accuracy of multi-turn response selection in retrieval-based dialogue systems by increasing the depth of context-response interaction in matching. Through extensive studies on benchmarks, we show that depth can bring significant improvement to model performance on the task. 3 4 Interaction-over-Interaction Network We define g(·, ·) as an interaction-over-interaction network (IoI). Figure 1 illustrates the architecture of Io"
P19-1001,P16-1094,0,0.0600994,"Missing"
P19-1001,D16-1127,0,0.0465592,"Missing"
P19-1001,D17-1230,0,0.0810916,"generation model estimated from a largescale conversation corpus (Serban et al., 2016; Li et al., 2017b). In this work, we study the problem of multi-turn response selection for retrievalbased dialogue systems where the input is a conversation context consisting of a sequence of utterances. Compared with generation-based methods, retrieval-based methods are superior in terms of response fluency and diversity, and thus have been widely applied in commercial chatbots such as the social bot XiaoIce (Shum et al., 2018) from Microsoft, and the e-commerce assistant AliMe Assist from Alibaba Group (Li et al., 2017a). A key step in multi-turn response selection is to measure the matching degree between a conversation context and a response candidate. Stateof-the-art methods (Wu et al., 2017; Zhou et al., 2018b) perform matching within a representationinteraction-aggregation framework (Wu et al., 2018b) where matching signals in each utteranceresponse pair are distilled from their interaction based on their representations, and then are aggregated as a matching score. Although utteranceresponse interaction has proven to be crucial to the performance of the matching models (Wu et al., 2017), it is execute"
P19-1001,W15-4640,0,0.323161,"ses with specific personas or emotions (Li et al., 2016a; Zhang et al., 2018a; Zhou et al., 2018a); and to pursue better optimization strategies (Li et al., 2017b, 2016b). The second group learns a matching model of a human input and a response candidate for response selection. Along this line, the focus of research starts from single-turn response selection by setting the human input as a single message (Wang et al., 2013; Hu et al., 2014; Wang et al., 2015), and moves to context-response matching for multi-turn response selection recently. Representative methods include the dual LSTM model (Lowe et al., 2015), the deep learning to respond architecture (Yan et al., 2016), the multi-view matching model (Zhou et al., 2016), the sequential matching network (Wu et al., 2017, 2018b), and the deep attention matching network (Zhou et al., 2018b). Besides model design, some attention is also paid to the learning problem of matching models (Wu et al., 2018a). Our work belongs to the second group. The proposed interaction-over-interaction network is unique in that it performs matching by stacking multiple interaction blocks, and thus extends the shallow interaction in state-of-the-art methods to a deep We co"
P19-1001,D18-1479,0,0.0157468,"As far as we know, this is the first architecture that realizes deep interaction for multi-turn response selection. Encouraged by the big success of deep neural architectures such as Resnet (He et al., 2016) and inception (Szegedy et al., 2015) in computer vision, researchers have studied if they can achieve similar results with deep neural networks on NLP tasks. Although deep models have not yet brought breakthroughs to NLP as they do to computer vision, they have proven effective in a few tasks such as text classification (Conneau et al., 2017), natural language inference (Kim et al., 2018; Tay et al., 2018), and question answering (Tay et al., 2018; Kim et al., 2018), etc. In this work, we attempt to improve the accuracy of multi-turn response selection in retrieval-based dialogue systems by increasing the depth of context-response interaction in matching. Through extensive studies on benchmarks, we show that depth can bring significant improvement to model performance on the task. 3 4 Interaction-over-Interaction Network We define g(·, ·) as an interaction-over-interaction network (IoI). Figure 1 illustrates the architecture of IoI. The model pairs each utterance in a context with a response ca"
P19-1001,C16-1316,1,0.886108,"Missing"
P19-1001,D13-1096,0,0.300989,"Missing"
P19-1001,N18-1202,0,0.0568928,"Missing"
P19-1001,N16-1170,0,0.233766,"b), we directly copy the numbers from the paper. For the E-commerce data, Zhang et al. (2018b) report performance of all baselines except DAM. Thus, we copy all available numbers from the paper and implement DAM with the published code4 . In order to conduct statistical tests, we also run the code of DAM on the Ubuntu data and the Douban data. Baselines We compare IoI with the following models: Single-turn Matching Models: these models, including RNN (Lowe et al., 2015), CNN (Lowe et al., 2015), LSTM (Lowe et al., 2015), BiLSTM (Kadlec et al., 2015), MV-LSTM (Wan et al., 2016) and Match-LSTM (Wang and Jiang, 2016), perform context-response matching by concatenating all utterances in a context into a single long document and calculating a matching score between the document and a response candidate. Multi-View (Zhou et al., 2016): the model calculates matching degree between a context and a response candidate from both a word sequence view and an utterance sequence view. DL2R (Yan et al., 2016): the model first reformulates the last utterance with previous turns in a context with different approaches. A response candidate and the reformulated message are then represented by a composition of RNN and CNN."
P19-1001,P18-2067,1,0.930561,". Compared with generation-based methods, retrieval-based methods are superior in terms of response fluency and diversity, and thus have been widely applied in commercial chatbots such as the social bot XiaoIce (Shum et al., 2018) from Microsoft, and the e-commerce assistant AliMe Assist from Alibaba Group (Li et al., 2017a). A key step in multi-turn response selection is to measure the matching degree between a conversation context and a response candidate. Stateof-the-art methods (Wu et al., 2017; Zhou et al., 2018b) perform matching within a representationinteraction-aggregation framework (Wu et al., 2018b) where matching signals in each utteranceresponse pair are distilled from their interaction based on their representations, and then are aggregated as a matching score. Although utteranceresponse interaction has proven to be crucial to the performance of the matching models (Wu et al., 2017), it is executed in a rather shallow manner where matching between an utterance and a response candidate is determined only by one step of interaction on each type or each layer of representations. In this paper, we attempt to move from shallow interaction to deep interaction, and consider context-respons"
P19-1001,P15-1152,0,0.108809,"Missing"
P19-1001,P18-1103,0,0.332465,"alogue systems where the input is a conversation context consisting of a sequence of utterances. Compared with generation-based methods, retrieval-based methods are superior in terms of response fluency and diversity, and thus have been widely applied in commercial chatbots such as the social bot XiaoIce (Shum et al., 2018) from Microsoft, and the e-commerce assistant AliMe Assist from Alibaba Group (Li et al., 2017a). A key step in multi-turn response selection is to measure the matching degree between a conversation context and a response candidate. Stateof-the-art methods (Wu et al., 2017; Zhou et al., 2018b) perform matching within a representationinteraction-aggregation framework (Wu et al., 2018b) where matching signals in each utteranceresponse pair are distilled from their interaction based on their representations, and then are aggregated as a matching score. Although utteranceresponse interaction has proven to be crucial to the performance of the matching models (Wu et al., 2017), it is executed in a rather shallow manner where matching between an utterance and a response candidate is determined only by one step of interaction on each type or each layer of representations. In this paper,"
P19-1001,P17-1046,1,0.75996,"retrievalbased dialogue systems where the input is a conversation context consisting of a sequence of utterances. Compared with generation-based methods, retrieval-based methods are superior in terms of response fluency and diversity, and thus have been widely applied in commercial chatbots such as the social bot XiaoIce (Shum et al., 2018) from Microsoft, and the e-commerce assistant AliMe Assist from Alibaba Group (Li et al., 2017a). A key step in multi-turn response selection is to measure the matching degree between a conversation context and a response candidate. Stateof-the-art methods (Wu et al., 2017; Zhou et al., 2018b) perform matching within a representationinteraction-aggregation framework (Wu et al., 2018b) where matching signals in each utteranceresponse pair are distilled from their interaction based on their representations, and then are aggregated as a matching score. Although utteranceresponse interaction has proven to be crucial to the performance of the matching models (Wu et al., 2017), it is executed in a rather shallow manner where matching between an utterance and a response candidate is determined only by one step of interaction on each type or each layer of representatio"
P19-1001,P18-1205,0,0.0768423,"Missing"
P19-1001,C18-1317,0,0.174548,"and the response candidate. 2 Related Work Existing methods for building an open-domain dialogue system can be categorized into two groups. The first group learns response generation models under an encoder-decoder framework. On top of the basic sequence-to-sequence with attention architecture (Vinyals and Le, 2015; Shang et al., 2015; Tao et al., 2018), various extensions have been made to tackle the “safe response” problem (Li et al., 2015; Mou et al., 2016; Xing et al., 2017; Zhao et al., 2017; Song et al., 2018); to generate responses with specific personas or emotions (Li et al., 2016a; Zhang et al., 2018a; Zhou et al., 2018a); and to pursue better optimization strategies (Li et al., 2017b, 2016b). The second group learns a matching model of a human input and a response candidate for response selection. Along this line, the focus of research starts from single-turn response selection by setting the human input as a single message (Wang et al., 2013; Hu et al., 2014; Wang et al., 2015), and moves to context-response matching for multi-turn response selection recently. Representative methods include the dual LSTM model (Lowe et al., 2015), the deep learning to respond architecture (Yan et al., 2"
P19-1001,P17-1061,0,0.0351046,"Missing"
P19-1001,D16-1036,1,0.905357,"Missing"
P19-1370,P16-1094,0,0.0315846,"nd δ to co-teaching. Experiments are conducted with DAM on the two data sets. 5 Related Work So far, methods used to build an open domain dialogue system can be divided into two categories. The first category utilize an encoderdecoder framework to learn response generation models. Since the basic sequence-to-sequence models (Vinyals and Le, 2015; Shang et al., 2015; Tao et al., 2018) tend to generate generic responses, extensions have been made to incorporate external knowledge into generation (Mou et al., 2016; Xing et al., 2017), and to generate responses with specific personas or emotions (Li et al., 2016; Zhang et al., 2018a; Zhou et al., 2018a). The second category design a discriminative model to measure the matching degree between a human input and a response candidate for response selection. At the beginning, research along this line assumes that the human input is a single message (Lu and Li, 2013; Wang et al., 2013; Hu et al., 2014; Wang et al., 2015). Recently, researchers begin to make use of conversation history in matching. Representative methods include the dual LSTM model (Lowe et al., 2015), the deep learning to respond architecture (Yan et al., 2016), the multi-view matching mod"
P19-1370,W15-4640,0,0.563357,"val-based systems are often superior to their generation-based counterparts on response fluency and diversity, are easy to evaluate, and have powered some real products such as the social bot XiaoIce from Microsoft (Shum et al., 2018), and the E-commerce assistant AliMe Assist from Alibaba Group (Li et al., 2017). A key problem in response selection is how to measure the matching degree between a conversation context (a message with several turns of conversation history) and a response candidate. Existing studies have paid tremendous effort to build a matching model with neural architectures (Lowe et al., 2015; Zhou et al., 2016; Wu et al., 2017; Zhou et al., 2018b), and advanced models such as the deep attention matching network (DAM) (Zhou et al., 2018b) have achieved impressive performance on benchmarks. In contrary to the progress on model architectures, there is little exploration on learning approaches of the models. On the one hand, neural matching models are becoming more and more complicated; on the other hand, all models are simply learned by distinguishing human responses from some automatically constructed negative response candidates (e.g., by random sampling). Although this heuristic"
P19-1370,C16-1316,1,0.851966,"ns on Douban 0.60 (b) 0.1 0.3 0.5 0.7 0.9 0.95 1.0 δ Data curriculum on ECD Figure 3: Effects of λ and δ to co-teaching. Experiments are conducted with DAM on the two data sets. 5 Related Work So far, methods used to build an open domain dialogue system can be divided into two categories. The first category utilize an encoderdecoder framework to learn response generation models. Since the basic sequence-to-sequence models (Vinyals and Le, 2015; Shang et al., 2015; Tao et al., 2018) tend to generate generic responses, extensions have been made to incorporate external knowledge into generation (Mou et al., 2016; Xing et al., 2017), and to generate responses with specific personas or emotions (Li et al., 2016; Zhang et al., 2018a; Zhou et al., 2018a). The second category design a discriminative model to measure the matching degree between a human input and a response candidate for response selection. At the beginning, research along this line assumes that the human input is a single message (Lu and Li, 2013; Wang et al., 2013; Hu et al., 2014; Wang et al., 2015). Recently, researchers begin to make use of conversation history in matching. Representative methods include the dual LSTM model (Lowe et al"
P19-1370,P15-1152,0,0.15213,"Missing"
P19-1370,N10-1116,0,0.0290891,"both the Douban data and the E-commerce data with SMN and DAM which achieves state-of-theart performance on benchmarks. Moreover, improvement to SMN on the Douban data from coteaching is bigger than that from weak supervision, when the ratio of the positive and the negative is 1:1 in training7 . Our work, in a broad sense, belongs to the effort on learning with noisy data. Previous studies including curriculum learning (CL) (Bengio et al., 2009) and self-paced learning (SPL) (Jiang et al., 2014, 2015) tackle the problem with heuristics, such as ordering data from easy instances to hard ones (Spitkovsky et al., 2010; Tsvetkov et al., 2016) and retaining training instances whose losses are smaller than a threshold (Jiang et al., 2015). Recently, Fan et al. (2018) propose a deep reinforcement learning framework in which a simple deep neural network is used to adaptively select and filter important data instances from the training data. Jiang et al. (2017) propose a MentorNet which learns a data-driven curriculum with a Student-Net to mitigate overfitting on corrupted labels. In parallel to curriculum learning, several studies explore sample weighting schemes where training samples are re-weighted according"
P19-1370,D19-1011,0,0.279741,"selection. At the beginning, research along this line assumes that the human input is a single message (Lu and Li, 2013; Wang et al., 2013; Hu et al., 2014; Wang et al., 2015). Recently, researchers begin to make use of conversation history in matching. Representative methods include the dual LSTM model (Lowe et al., 2015), the deep learning to respond architecture (Yan et al., 2016), the multi-view matching model (Zhou et al., 2016), the sequential matching network (Wu et al., 2017, 2018c), the deep attention matching network (Zhou et al., 2018b), and the multi-representation fusion network (Tao et al., 2019). Our work belongs to the second group. Rather than crafting a new model, we are interested in how to learn the existing models with a better approach. Probably the most related work is the weakly supervised learning approach proposed in Wu et al. (2018b). However, there is stark difference between our approach and the weak supervision approach: (1) weak supervision employs a static generative model to teach a discriminative model, while co-teaching dynamically lets two discriminative models teach each other and evolve together; (2) weak supervision needs pretraining a generative model with ex"
P19-1370,P16-1013,0,0.0575427,"Missing"
P19-1370,D13-1096,0,0.199215,"dels (Vinyals and Le, 2015; Shang et al., 2015; Tao et al., 2018) tend to generate generic responses, extensions have been made to incorporate external knowledge into generation (Mou et al., 2016; Xing et al., 2017), and to generate responses with specific personas or emotions (Li et al., 2016; Zhang et al., 2018a; Zhou et al., 2018a). The second category design a discriminative model to measure the matching degree between a human input and a response candidate for response selection. At the beginning, research along this line assumes that the human input is a single message (Lu and Li, 2013; Wang et al., 2013; Hu et al., 2014; Wang et al., 2015). Recently, researchers begin to make use of conversation history in matching. Representative methods include the dual LSTM model (Lowe et al., 2015), the deep learning to respond architecture (Yan et al., 2016), the multi-view matching model (Zhou et al., 2016), the sequential matching network (Wu et al., 2017, 2018c), the deep attention matching network (Zhou et al., 2018b), and the multi-representation fusion network (Tao et al., 2019). Our work belongs to the second group. Rather than crafting a new model, we are interested in how to learn the existing"
P19-1370,P18-2067,1,0.90322,"epresentative methods include the dual LSTM model (Lowe et al., 2015), the deep learning to respond architecture (Yan et al., 2016), the multi-view matching model (Zhou et al., 2016), the sequential matching network (Wu et al., 2017, 2018c), the deep attention matching network (Zhou et al., 2018b), and the multi-representation fusion network (Tao et al., 2019). Our work belongs to the second group. Rather than crafting a new model, we are interested in how to learn the existing models with a better approach. Probably the most related work is the weakly supervised learning approach proposed in Wu et al. (2018b). However, there is stark difference between our approach and the weak supervision approach: (1) weak supervision employs a static generative model to teach a discriminative model, while co-teaching dynamically lets two discriminative models teach each other and evolve together; (2) weak supervision needs pretraining a generative model with extra resources and pre-building an index for training data construction, while co-teaching does not have such request; and (3) in terms of multi-turn response selection, weak supervision is only tested on the Douban data with SMN and the multi-view match"
P19-1370,P17-1046,1,0.308055,"o their generation-based counterparts on response fluency and diversity, are easy to evaluate, and have powered some real products such as the social bot XiaoIce from Microsoft (Shum et al., 2018), and the E-commerce assistant AliMe Assist from Alibaba Group (Li et al., 2017). A key problem in response selection is how to measure the matching degree between a conversation context (a message with several turns of conversation history) and a response candidate. Existing studies have paid tremendous effort to build a matching model with neural architectures (Lowe et al., 2015; Zhou et al., 2016; Wu et al., 2017; Zhou et al., 2018b), and advanced models such as the deep attention matching network (DAM) (Zhou et al., 2018b) have achieved impressive performance on benchmarks. In contrary to the progress on model architectures, there is little exploration on learning approaches of the models. On the one hand, neural matching models are becoming more and more complicated; on the other hand, all models are simply learned by distinguishing human responses from some automatically constructed negative response candidates (e.g., by random sampling). Although this heuristic approach can avoid expensive and exh"
P19-1370,P18-1205,0,0.0536553,"Missing"
P19-1370,C18-1317,0,0.375056,"d negative responses are randomly sampled. The ratio of the positive and the negative is 1:1 in training and validation. In the test set, each context has 10 response candidates retrieved from an index whose appropriateness regarding to the context is judged by human annotators. The average number of positive responses per context is 1.18. Following Wu et al. (2017), we employ R10 @1, R10 @2, R10 @5, mean average precision (MAP), mean reciprocal rank (MRR), and precision at position 1 (P@1) as evaluation metrics. In addition to the Douban data, we also choose E-commerce Dialogue Corpus (ECD) (Zhang et al., 2018b) as an experimental data set. The data consists of real-world conversations between customers and customer service staff in Taobao4 , which is the largest e-commerce platform in China. There are 1 million context-response pairs in the training set, and 10 thousand pairs in both the validation set and the test set. Each context in the training set and the validation set corresponds to one positive response candidate and one negative response candidate, while in the test set, the number of response candidates per context is 10 with only one of them positive. In the released data, human respons"
P19-1370,D16-1036,1,0.925812,"Missing"
P19-1370,P18-1103,0,0.245472,"roach can generally and significantly improve the performance of existing matching models. 1 Introduction Human-machine conversation is a long-standing goal of artificial intelligence. Recently, building a dialogue system for open domain human-machine conversation is attracting more and more attention due to both availability of large-scale human conversation data and powerful models learned with neural networks. Existing methods are either retrieval-based or generation-based. Retrievalbased methods reply to a human input by selecting a proper response from a pre-built index (Ji et al., 2014; Zhou et al., 2018b; Yan and Zhao, 2018), while generation-based methods synthesize a response with a natural language model (Shang et al., 2015; Serban et al., 2017). In this ∗ † Equal Contribution. Corresponding author: Rui Yan (ruiyan@pku.edu.cn). work, we study the problem of response selection for retrieval-based dialogue systems, since retrieval-based systems are often superior to their generation-based counterparts on response fluency and diversity, are easy to evaluate, and have powered some real products such as the social bot XiaoIce from Microsoft (Shum et al., 2018), and the E-commerce assistant Ali"
P19-1372,K16-1002,0,0.458299,"ate various responses. Xing et al. (2017) incorporate topic information to generate informative responses. However, these models suffer from the deterministic structure when generating multiple diverse responses. Besides, during the training of these models, response utterances are only used in the loss function and ignored when forward computing, which can confuse the model for pursuing multiple objectives simultaneously. A few works explore to change the deterministic structure of sequence-to-sequence models by introducing stochastic latent variables. VAE is one of the most popular methods (Bowman et al., 2016; Zhao et al., 2017; Serban et al., 2017; Cao and Clark, 2017), where the discourse-level diversity is modeled by a Gaussian distribution. However, it is observed that in the CVAE with a fixed Gaussian prior, the learned conditional posteriors tend to collapse to a single mode, resulting in a relatively simple scope (Wang et al., 2017). To tackle this, WAE (Gu et al., 2018) which adopts a Gaussian mixture prior network with Wasserstein distance and VAD (Du et al., 2018) which sequentially introduces a series of latent variables to condition each word in the response sequence are proposed. Alth"
P19-1372,E17-2029,0,0.0183995,"nformation to generate informative responses. However, these models suffer from the deterministic structure when generating multiple diverse responses. Besides, during the training of these models, response utterances are only used in the loss function and ignored when forward computing, which can confuse the model for pursuing multiple objectives simultaneously. A few works explore to change the deterministic structure of sequence-to-sequence models by introducing stochastic latent variables. VAE is one of the most popular methods (Bowman et al., 2016; Zhao et al., 2017; Serban et al., 2017; Cao and Clark, 2017), where the discourse-level diversity is modeled by a Gaussian distribution. However, it is observed that in the CVAE with a fixed Gaussian prior, the learned conditional posteriors tend to collapse to a single mode, resulting in a relatively simple scope (Wang et al., 2017). To tackle this, WAE (Gu et al., 2018) which adopts a Gaussian mixture prior network with Wasserstein distance and VAD (Du et al., 2018) which sequentially introduces a series of latent variables to condition each word in the response sequence are proposed. Although these models overcome the deterministic structure of sequ"
P19-1372,D18-1354,0,0.0257911,"ructure of sequence-to-sequence models by introducing stochastic latent variables. VAE is one of the most popular methods (Bowman et al., 2016; Zhao et al., 2017; Serban et al., 2017; Cao and Clark, 2017), where the discourse-level diversity is modeled by a Gaussian distribution. However, it is observed that in the CVAE with a fixed Gaussian prior, the learned conditional posteriors tend to collapse to a single mode, resulting in a relatively simple scope (Wang et al., 2017). To tackle this, WAE (Gu et al., 2018) which adopts a Gaussian mixture prior network with Wasserstein distance and VAD (Du et al., 2018) which sequentially introduces a series of latent variables to condition each word in the response sequence are proposed. Although these models overcome the deterministic structure of sequence-to-sequence model, they still ignore the correlation of multiple valid responses and each case is trained separately. To consider the multiple responses jointly, the maximum likelihood strategy is explored. Zhang et al. (2018a) propose the maximum generated likelihood criteria which model a query with its multiple responses as a bag of instances and proposes to optimize the model towards the most likely"
P19-1372,N16-1014,0,0.827037,"open-domain dialogue generation has become a research hotspot in Natural Language Processing due to its broad application prospect, including chatbots, virtual personal assistants, etc. Though plenty of systems have been proposed to improve the quality of generated responses from various aspects such as topic (Xing et al., 2017), persona modeling (Zhang et al., 2018b) and emotion controlling (Zhou et al., 2018b), most of these recent approaches are primarily built upon the sequence-to-sequence architecture (Cho et al., 2014; Shang et al., 2015) which suffers from the “safe” response problem (Li et al., 2016a; Sato et al., 2017). This can be ascribed to modeling the response generation process as 1to-1 mapping, which ignores the nature of 1-to∗ Corresponding author: Rui Yan (ruiyan@pku.edu.cn) n mapping of dialogue that multiple possible responses can correspond to the same query. To deal with the generic response problem, various methods have been proposed, including diversity-promoting objective function (Li et al., 2016a), enhanced beam search (Shao et al., 2016), latent dialogue mechanism (Zhou et al., 2017, 2018a), Variational Autoencoders (VAEs) based models (Zhao et al., 2017; Serban et al"
P19-1372,D14-1162,0,0.0822003,"the recognition networks if other complementary responses can be predicted from the distinctive variable z. Besides, since the probability of the complementary term may approach zero which makes it difficult to optimize, we actually adopt its lower bound in practice: log(1 − p(ybow |x, z)) = log(1 − |y| Y efyt P|V |f ) j t=1 j e |y| Y efyt ≥ log( (1 − P|V | )) fj e t=1 j (11) where |V |is vocabulary size. Totally, the whole loss for the step-two generation is then: Our whole model can be trained in an end-to-end fashion. To train the model, we first pre-train the word embedding using Glove ((Pennington et al., 2014))1 . Then modules of the model are jointly trained by optimizing the losses Lf irst and Lsecond of the two generation phases respectively. To overcome the vanishing latent variable problem (Wang et al., 2017) of CVAE, we adopt the KL annealing strategy (Bowman et al., 2016), where the weight of the KL term is gradually increased during training. The other technique employed is the MBOW loss which is able to sharpen the distribution of latent variable z for each specific response and alleviate the vanishing problem at the same time. During testing, diverse responses can be obtained by the two g"
P19-1372,D18-1418,0,0.156419,"e been proposed, including diversity-promoting objective function (Li et al., 2016a), enhanced beam search (Shao et al., 2016), latent dialogue mechanism (Zhou et al., 2017, 2018a), Variational Autoencoders (VAEs) based models (Zhao et al., 2017; Serban et al., 2017), etc. However, these methods still view multiple responses as independent ones and fail to model multiple responses jointly. Recently, Zhang et al. (2018a) introduce a maximum likelihood strategy that given an input query, the most likely response is approximated rather than all possible responses, which is further implemented by Rajendran et al. (2018) with reinforcement learning for task-oriented dialogue. Although capable of generating the most likely response, these methods fail to model other possible responses and ignore the correlation of different responses. In this paper, we propose a novel response generation model for open-domain conversation, which learns to generate multiple diverse responses with multiple references by considering 3826 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3826–3835 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistic"
P19-1372,P17-3020,0,0.0121152,"gue generation has become a research hotspot in Natural Language Processing due to its broad application prospect, including chatbots, virtual personal assistants, etc. Though plenty of systems have been proposed to improve the quality of generated responses from various aspects such as topic (Xing et al., 2017), persona modeling (Zhang et al., 2018b) and emotion controlling (Zhou et al., 2018b), most of these recent approaches are primarily built upon the sequence-to-sequence architecture (Cho et al., 2014; Shang et al., 2015) which suffers from the “safe” response problem (Li et al., 2016a; Sato et al., 2017). This can be ascribed to modeling the response generation process as 1to-1 mapping, which ignores the nature of 1-to∗ Corresponding author: Rui Yan (ruiyan@pku.edu.cn) n mapping of dialogue that multiple possible responses can correspond to the same query. To deal with the generic response problem, various methods have been proposed, including diversity-promoting objective function (Li et al., 2016a), enhanced beam search (Shao et al., 2016), latent dialogue mechanism (Zhou et al., 2017, 2018a), Variational Autoencoders (VAEs) based models (Zhao et al., 2017; Serban et al., 2017), etc. Howeve"
P19-1372,P15-1152,0,0.214146,"Missing"
P19-1372,N15-1020,0,0.101772,"Missing"
P19-1372,D17-1233,1,0.875255,", 2017) for diverse response generation. CVAE: the vanilla CVAE model (Zhao et al., 2017) with and without BOW (bag-of-word) loss (CVAE+BOW and CVAE). WAE: the conditional Wasserstein autoencoder model for dialogue generation (Gu et al., 2018) which models the distribution of data by training a GAN within the latent variable space. Ours: we explore our model Ours and conduct 4.3 Evaluation Metrics To comprehensively evaluate the quality of generated response utterances, we adopt both automatic and human evaluation metrics: BLEU: In dialogue generation, BLEU is widely used in previous studies (Yao et al., 2017; Shang et al., 2018). Since multiple valid responses exist in this paper, we adopt multi-reference BLEU where the evaluated utterance is compared to provided multiple references simultaneously. Distinctness: To distinguish safe and commonplace responses, the distinctness score (Li et al., 2016a) is designed to measure word-level diversity by counting the ratio of distinctive [1,2]-grams. In our experiments, we adopt both Intra-Dist: the distinctness scores of multiple responses for a given query and Inter-Dist: the distinctness scores of generated responses of the whole testing set. Embedding"
P19-1372,P18-1137,0,0.159121,"rent from the conventional methods (shown in green color) which model each response from scratch every time, our method first builds a common feature of multiple responses and models each response based on it afterward. Introduction In recent years, open-domain dialogue generation has become a research hotspot in Natural Language Processing due to its broad application prospect, including chatbots, virtual personal assistants, etc. Though plenty of systems have been proposed to improve the quality of generated responses from various aspects such as topic (Xing et al., 2017), persona modeling (Zhang et al., 2018b) and emotion controlling (Zhou et al., 2018b), most of these recent approaches are primarily built upon the sequence-to-sequence architecture (Cho et al., 2014; Shang et al., 2015) which suffers from the “safe” response problem (Li et al., 2016a; Sato et al., 2017). This can be ascribed to modeling the response generation process as 1to-1 mapping, which ignores the nature of 1-to∗ Corresponding author: Rui Yan (ruiyan@pku.edu.cn) n mapping of dialogue that multiple possible responses can correspond to the same query. To deal with the generic response problem, various methods have been propos"
P19-1372,P18-1205,0,0.105266,"Missing"
P19-1372,P17-1061,0,0.436588,"sponse problem (Li et al., 2016a; Sato et al., 2017). This can be ascribed to modeling the response generation process as 1to-1 mapping, which ignores the nature of 1-to∗ Corresponding author: Rui Yan (ruiyan@pku.edu.cn) n mapping of dialogue that multiple possible responses can correspond to the same query. To deal with the generic response problem, various methods have been proposed, including diversity-promoting objective function (Li et al., 2016a), enhanced beam search (Shao et al., 2016), latent dialogue mechanism (Zhou et al., 2017, 2018a), Variational Autoencoders (VAEs) based models (Zhao et al., 2017; Serban et al., 2017), etc. However, these methods still view multiple responses as independent ones and fail to model multiple responses jointly. Recently, Zhang et al. (2018a) introduce a maximum likelihood strategy that given an input query, the most likely response is approximated rather than all possible responses, which is further implemented by Rajendran et al. (2018) with reinforcement learning for task-oriented dialogue. Although capable of generating the most likely response, these methods fail to model other possible responses and ignore the correlation of different responses. In t"
W12-3312,D10-1099,0,0.114349,"ese. In this paper, we propose a new framework to build a KB in Chinese from online resources without much human involvement. Since the Chinese portion of Wikipedia is much smaller than its English part, we harvest knowledge facts from a Chinese online encyclopedia, HudongBaike2 . HudongBaike is the largest Chinese online encyclopedia and features similar managing rules and writing styles with Wikipedia. We first obtain knowledge facts by parsing the infoboxes of HudongBaike. Then we use these triples as seeds, and adopt the idea of distant supervision(Mintz et al., 2009; Riedel et al., 2010; Yao et al., 2010) to extract more facts from other HudongBaike articles and build a KB accordingly. Moreover, to make the knowledge base more up-todate, we also propose to propagate the KB with news events. The rest of this paper is organized as follows: we first introduce the related work, and briefly introduce two online encyclopedias. In Section 4 we describe our framework in detail. Our current work are discussed in Section 5. In Section 6 we conclude this paper. 2 Related Work KB construction is an important task and has attracted many research efforts from artificial intelligence, information retrieval,"
W12-3312,N10-1072,0,0.0747158,"Missing"
W12-3312,P09-1113,0,0.0209114,"used in English may not work well in Chinese. In this paper, we propose a new framework to build a KB in Chinese from online resources without much human involvement. Since the Chinese portion of Wikipedia is much smaller than its English part, we harvest knowledge facts from a Chinese online encyclopedia, HudongBaike2 . HudongBaike is the largest Chinese online encyclopedia and features similar managing rules and writing styles with Wikipedia. We first obtain knowledge facts by parsing the infoboxes of HudongBaike. Then we use these triples as seeds, and adopt the idea of distant supervision(Mintz et al., 2009; Riedel et al., 2010; Yao et al., 2010) to extract more facts from other HudongBaike articles and build a KB accordingly. Moreover, to make the knowledge base more up-todate, we also propose to propagate the KB with news events. The rest of this paper is organized as follows: we first introduce the related work, and briefly introduce two online encyclopedias. In Section 4 we describe our framework in detail. Our current work are discussed in Section 5. In Section 6 we conclude this paper. 2 Related Work KB construction is an important task and has attracted many research efforts from artifici"
