2009.mtsummit-papers.20,D08-1064,0,0.259405,"core? Callison-Burch et al. (2009) used the results of the manual evaluation to analyze the translation quality of the different systems that were submitted to the WMT09 and found that in general, system combinations performed as well as the best individual systems, but not statistically significantly better than them. Some practical cases in our evaluation suggest that the higher BLEU score doesn’t always mean higher translation adequacy no matter in single system MT task or in system combination task. For the evaluation measurement, we choose two novel metrics as our alternatives: BLEU-SBP (Chiang et al., 2008) and linguistic check-point method (Zhou et al., 2008). We encountered two actual cases which happened to be very similar with those in (Chiang et al., 2008). These cases can be traced to the fact that BLEU (Papineni et al., 2002) is not decomposable at the sentence level, which means if a system generates a long translation for one sentence, it can generate a short translation for another sentence without facing a penalty. Our experiments validated BLEU-SBP’s effectivity in resolving the nondecomposability problem of both NIST-BLEU and IBM-BLEU at sentence level. On the other hand, we choose"
2009.mtsummit-papers.20,P05-1066,0,0.0871819,"and LCM, the automatic evaluation metrics of CWMT2008 evaluation also include: BLEU, NIST, GTM, mWER, mPER and ICT (a metric developed by the Institute of Computing Technology, CAS). All these metrics are case-sensitive. The evaluation of Chinese translation is based on Chinese characters instead of words. 2.5 Evaluation Results Figures 1-4 show the evaluation results. 3 BLEU-SBP 3.1 BLEU’s Deficiency We encountered the following two practical cases in our evaluation. In this paper, if not specified particularly, all the BLEU means NIST-BLEU. 3.1.1 The Sign Test When we applied the sign test (Collins et al., 2005) for significance testing with BLEU, we encountered such problem (Table 4): when comparing system A and system B, if we select A as the baseline system, we found B is significantly better than A, but if we select B as the baseline system, we found A is significantly better than B. We tried two kinds of BLEU: NIST-BLUE and IBM-BLEU, the results are similar (Table 4). This is because the sign test requires a function (ai,bi) that indicates whether bi is better, worse or same quality translation relative to ai. Because BLEU is not defined on single sentences, Collins et al. (2005) use an approxim"
2009.mtsummit-papers.20,P02-1040,0,0.0823026,"s well as the best individual systems, but not statistically significantly better than them. Some practical cases in our evaluation suggest that the higher BLEU score doesn’t always mean higher translation adequacy no matter in single system MT task or in system combination task. For the evaluation measurement, we choose two novel metrics as our alternatives: BLEU-SBP (Chiang et al., 2008) and linguistic check-point method (Zhou et al., 2008). We encountered two actual cases which happened to be very similar with those in (Chiang et al., 2008). These cases can be traced to the fact that BLEU (Papineni et al., 2002) is not decomposable at the sentence level, which means if a system generates a long translation for one sentence, it can generate a short translation for another sentence without facing a penalty. Our experiments validated BLEU-SBP’s effectivity in resolving the nondecomposability problem of both NIST-BLEU and IBM-BLEU at sentence level. On the other hand, we choose linguistic checkpoint method (LCM) as another alternative metric with an attempt to detect and report richer linguistic information on the system. Now most MT evaluation methods only generate a general similarity score. At the pre"
2009.mtsummit-papers.20,H93-1040,0,0.212743,"Missing"
2009.mtsummit-papers.20,W09-0401,0,\N,Missing
2009.mtsummit-papers.20,W03-1709,1,\N,Missing
2009.mtsummit-papers.20,C08-1141,1,\N,Missing
2009.mtsummit-papers.20,J03-1002,0,\N,Missing
2011.mtsummit-papers.14,J96-1002,0,0.0526965,"Missing"
2011.mtsummit-papers.14,W09-0436,0,0.0226187,"Missing"
2011.mtsummit-papers.14,J07-2003,0,0.0233923,"reordering and partial hypotheses ranking. The experimental results show that our approach improves the SMT performance signiﬁcantly on ChineseEnglish translation task. 1 Introduction Function words belong to a relatively closed set with very high frequencies in a language compared to content words such as nouns, verbs, adjectives and most adverbs. They often play an important role in sentence structures and express grammatical relationships with other words within a sentence, but have few semantic meanings or have multiple meanings. In most SMT systems (Koehn et al., 2003; Och and Ney, 2004; Chiang, 2007), function words ∗ This work has been done while the ﬁrst author was visiting Microsoft Research Asia. 139 Consequently, many target function words may be either missing or inappropriately generated in the translations. This not only degrades the readability but also impairs the quality of content word translations. For example, in Figure 1, the translation of a Chinese phrase “内心 渴望”is missing because no appropriate target function words are generated. Both of the source words “内心”(heart) and “渴望”(eager) are translated into NULL according to our phrase translation table, where all the transla"
2011.mtsummit-papers.14,D09-1024,0,0.0211603,"tion of target function words can lead to better reordering and better partial hypotheses ranking in SMT decoding. The experimental results show that our method brings signiﬁcant BLEU improvements over the baseline system. 2 Related Work Some previous work has been done to improve SMT performance by leveraging function words. (Chang et al., 2009) studied the special source function words translation, such as the Chinese function word “ 的 ”. (Setiawan et al., 2007; Setiawan et al., 2009) use function words to get better reordering in both phrasal SMT and hierarchical SMT systems. In addition, (Hermjakob, 2009) proposed to improve word alignment by separately considering function words and content words. These previous work only used function words as lexical anchors, which is different from our work in this paper. Moreover, some other work focused on function words insertion and deletion. (Li et al., 2008) proposed three models to address spurious source words deletion during SMT decoding. The method brings signiﬁcant improvements on Chinese-English translation task. (Zhang et al., 2008) tried to generate Chinese measure words in the target-side of EnglishChinese translation task. They proposed a s"
2011.mtsummit-papers.14,W04-3250,0,0.0218324,"el data with the Xinhua portion of LDC English Gigaword, where no target function words are deleted. The LM is integrated into the SMT decoder rather than used in a post reranking step. In addition, a CRF-based POS tagger is trained over Penn Treebank to label the target portion of bilingual data. The development data is NIST 2003 data set and the test data comes from NIST 2005 and NIST 2006 evaluation data set. The caseinsensitive BLEU4 (Papineni et al., 2002) is used as the evaluation metric, where statistical signiﬁcance test is performed using the bootstrap re-sampling method proposed by (Koehn, 2004). 4.2 Words Accuracy of TFWIM Following (Setiawan et al., 2009), the selection of the target function words is based on their frequencies in the training corpus. Although our method can be applied to the generation of any number of distinct function words, in our experiments we mainly focus on ﬁve typical target function words contained in the set W ={”the”, ”of”, ”to”, ”in”, ”for”}. They are the most frequent target function words that are unaligned (i.e., aligned to NULL) in word alignment. Table 5 shows their statistical information. For convenience, each setting of TFWIM is de144 Table 5:"
2011.mtsummit-papers.14,N03-1017,0,0.017962,"odel for SMT, which can lead to better reordering and partial hypotheses ranking. The experimental results show that our approach improves the SMT performance signiﬁcantly on ChineseEnglish translation task. 1 Introduction Function words belong to a relatively closed set with very high frequencies in a language compared to content words such as nouns, verbs, adjectives and most adverbs. They often play an important role in sentence structures and express grammatical relationships with other words within a sentence, but have few semantic meanings or have multiple meanings. In most SMT systems (Koehn et al., 2003; Och and Ney, 2004; Chiang, 2007), function words ∗ This work has been done while the ﬁrst author was visiting Microsoft Research Asia. 139 Consequently, many target function words may be either missing or inappropriately generated in the translations. This not only degrades the readability but also impairs the quality of content word translations. For example, in Figure 1, the translation of a Chinese phrase “内心 渴望”is missing because no appropriate target function words are generated. Both of the source words “内心”(heart) and “渴望”(eager) are translated into NULL according to our phrase transl"
2011.mtsummit-papers.14,W08-0301,1,0.923541,"everaging function words. (Chang et al., 2009) studied the special source function words translation, such as the Chinese function word “ 的 ”. (Setiawan et al., 2007; Setiawan et al., 2009) use function words to get better reordering in both phrasal SMT and hierarchical SMT systems. In addition, (Hermjakob, 2009) proposed to improve word alignment by separately considering function words and content words. These previous work only used function words as lexical anchors, which is different from our work in this paper. Moreover, some other work focused on function words insertion and deletion. (Li et al., 2008) proposed three models to address spurious source words deletion during SMT decoding. The method brings signiﬁcant improvements on Chinese-English translation task. (Zhang et al., 2008) tried to generate Chinese measure words in the target-side of EnglishChinese translation task. They proposed a statistical model to calculate the probability of measure word generation by utilizing lexical and syntactic knowledge. The method works as a post-generation step over the decoder’s output. High precision and recall for measure word generation can be achieved in their experimental results. As function"
2011.mtsummit-papers.14,D08-1077,0,0.0845194,"t improvements on Chinese-English translation task. (Zhang et al., 2008) tried to generate Chinese measure words in the target-side of EnglishChinese translation task. They proposed a statistical model to calculate the probability of measure word generation by utilizing lexical and syntactic knowledge. The method works as a post-generation step over the decoder’s output. High precision and recall for measure word generation can be achieved in their experimental results. As function words are more ﬂexible than measure words, the generation of function words faces more challenges. In addition, (Menezes and Quirk, 2008) introduced an extension approach to the syntactic-based SMT system that allows structural word insertion and deletion. The effectiveness of these methods motivates us to address the generation of target function words in the phrasebased SMT which is a popular system in both academic and industrial areas. 141 3 Our Method Our method focuses on the processing of target function words, including the deletion and the insertion. The deletion takes place during the model training, where the target function words are removed from the training data before conducting translation modeling. The insertio"
2011.mtsummit-papers.14,P03-1021,0,0.0381847,"Missing"
2011.mtsummit-papers.14,J04-4002,0,0.0445212,"can lead to better reordering and partial hypotheses ranking. The experimental results show that our approach improves the SMT performance signiﬁcantly on ChineseEnglish translation task. 1 Introduction Function words belong to a relatively closed set with very high frequencies in a language compared to content words such as nouns, verbs, adjectives and most adverbs. They often play an important role in sentence structures and express grammatical relationships with other words within a sentence, but have few semantic meanings or have multiple meanings. In most SMT systems (Koehn et al., 2003; Och and Ney, 2004; Chiang, 2007), function words ∗ This work has been done while the ﬁrst author was visiting Microsoft Research Asia. 139 Consequently, many target function words may be either missing or inappropriately generated in the translations. This not only degrades the readability but also impairs the quality of content word translations. For example, in Figure 1, the translation of a Chinese phrase “内心 渴望”is missing because no appropriate target function words are generated. Both of the source words “内心”(heart) and “渴望”(eager) are translated into NULL according to our phrase translation table, where"
2011.mtsummit-papers.14,P02-1040,0,0.0820753,"alized reordering model comes from LDC2003E14, which contains 128K sentence pairs. A 5-gram language model (LM) is trained over the English portion of parallel data with the Xinhua portion of LDC English Gigaword, where no target function words are deleted. The LM is integrated into the SMT decoder rather than used in a post reranking step. In addition, a CRF-based POS tagger is trained over Penn Treebank to label the target portion of bilingual data. The development data is NIST 2003 data set and the test data comes from NIST 2005 and NIST 2006 evaluation data set. The caseinsensitive BLEU4 (Papineni et al., 2002) is used as the evaluation metric, where statistical signiﬁcance test is performed using the bootstrap re-sampling method proposed by (Koehn, 2004). 4.2 Words Accuracy of TFWIM Following (Setiawan et al., 2009), the selection of the target function words is based on their frequencies in the training corpus. Although our method can be applied to the generation of any number of distinct function words, in our experiments we mainly focus on ﬁve typical target function words contained in the set W ={”the”, ”of”, ”to”, ”in”, ”for”}. They are the most frequent target function words that are unaligne"
2011.mtsummit-papers.14,P07-1090,0,0.0177966,"anwhile, the second step is aim to recover those function words to make translation results more ﬂuent. Intuitively, it is expected that the correct insertion of target function words can lead to better reordering and better partial hypotheses ranking in SMT decoding. The experimental results show that our method brings signiﬁcant BLEU improvements over the baseline system. 2 Related Work Some previous work has been done to improve SMT performance by leveraging function words. (Chang et al., 2009) studied the special source function words translation, such as the Chinese function word “ 的 ”. (Setiawan et al., 2007; Setiawan et al., 2009) use function words to get better reordering in both phrasal SMT and hierarchical SMT systems. In addition, (Hermjakob, 2009) proposed to improve word alignment by separately considering function words and content words. These previous work only used function words as lexical anchors, which is different from our work in this paper. Moreover, some other work focused on function words insertion and deletion. (Li et al., 2008) proposed three models to address spurious source words deletion during SMT decoding. The method brings signiﬁcant improvements on Chinese-English tr"
2011.mtsummit-papers.14,P09-1037,0,0.0521647,"p is aim to recover those function words to make translation results more ﬂuent. Intuitively, it is expected that the correct insertion of target function words can lead to better reordering and better partial hypotheses ranking in SMT decoding. The experimental results show that our method brings signiﬁcant BLEU improvements over the baseline system. 2 Related Work Some previous work has been done to improve SMT performance by leveraging function words. (Chang et al., 2009) studied the special source function words translation, such as the Chinese function word “ 的 ”. (Setiawan et al., 2007; Setiawan et al., 2009) use function words to get better reordering in both phrasal SMT and hierarchical SMT systems. In addition, (Hermjakob, 2009) proposed to improve word alignment by separately considering function words and content words. These previous work only used function words as lexical anchors, which is different from our work in this paper. Moreover, some other work focused on function words insertion and deletion. (Li et al., 2008) proposed three models to address spurious source words deletion during SMT decoding. The method brings signiﬁcant improvements on Chinese-English translation task. (Zhang e"
2011.mtsummit-papers.14,J97-3002,0,0.242662,"Missing"
2011.mtsummit-papers.14,P06-1066,0,0.0601275,"Missing"
2011.mtsummit-papers.14,P08-1011,1,0.912537,", 2009) use function words to get better reordering in both phrasal SMT and hierarchical SMT systems. In addition, (Hermjakob, 2009) proposed to improve word alignment by separately considering function words and content words. These previous work only used function words as lexical anchors, which is different from our work in this paper. Moreover, some other work focused on function words insertion and deletion. (Li et al., 2008) proposed three models to address spurious source words deletion during SMT decoding. The method brings signiﬁcant improvements on Chinese-English translation task. (Zhang et al., 2008) tried to generate Chinese measure words in the target-side of EnglishChinese translation task. They proposed a statistical model to calculate the probability of measure word generation by utilizing lexical and syntactic knowledge. The method works as a post-generation step over the decoder’s output. High precision and recall for measure word generation can be achieved in their experimental results. As function words are more ﬂexible than measure words, the generation of function words faces more challenges. In addition, (Menezes and Quirk, 2008) introduced an extension approach to the syntact"
2011.mtsummit-papers.20,P08-1023,0,0.0201031,"MBR definition, where  ܯis a constant large enough. We define ܩሺߩǡ ߩᇱ ሻ as the similarity measure between two hypotheses ߩ and ߩᇱ . In this sense, ܵ ሺߩሻ can be viewed as the expected similarity between ߩ and all hypotheses in ሺሻ. W formulate ܩሺߩǡ ߩᇱ ሻ as a weighted combination of a set of similarity features: Step 1: Baseline Phrase Extraction ܩሺߩǡ ߩᇱ ሻ ൌ  ߣ ߠ ሺߩǡ ߩᇱ ሻ In this step, all potential phrase pairs that are consistent with word alignments are extracted from a given training corpus ࣝ using the standard phrase extraction method. Furthermore, inspired by several studies (Mi et al., 2008; Dyer et al., ሺͳሻ ఘᇲ אሺሻ ሺʹሻ  where ߠ is the  th feature with its weight ߣ . x 1 Given a source phrase  and a target phrase , the phrase pair ሺǡ ሻ is said to be consistent with word alignment if and only if: (1) at least one word in one phrase is aligned to one word in the other phrase; (2) no words in one phrase can be aligned to a word outside the other phrase. ܲሺߩȁሻ is the hypothesis distribution over all hypotheses contained in ሺሻ: ܲሺߩȁሻ ൌ 190 σሺாǡிሻ ࣝאσࣛ ߜሺࣛǡாǡிሻ ሺߩሻܲሺࣛȁܧǡ ܨሻ σఘᇲ אுሺሻ σሺாǡிሻ ࣝאσࣛ ߜሺࣛǡாǡிሻ ሺߩᇱ ሻܲሺࣛȁܧǡ ܨሻ where ߜሺࣛǡாǡிሻ ሺߩሻ equals to 1 when ߩ"
2011.mtsummit-papers.20,P08-1010,0,0.0501013,"Missing"
2011.mtsummit-papers.20,W04-3243,0,0.205854,"Missing"
2011.mtsummit-papers.20,P08-1115,0,0.0374457,"Missing"
2011.mtsummit-papers.20,E99-1010,0,0.0253761,"BR model, and use them as extra phrasal features. In our experimental part, we will show that besides alignment pruning, using similarity scores as additional features can provide further improvements as well. When using n-best alignment results instead of 1-best ones, translation probabilities and lexical weights are estimated based on fractional counts instead of absolute frequencies of phrases. 3 alignment-based features 3) ߠௐଶ ሺߩǡ ߩᇱ ሻ. A feature that counts how many (source word)-to-(target word’s class) link pairs in ߩ co-occur in ߩᇱ . Word clusters are obtained by using mkcls toolkit (Och, 1999) that trains word classes based on the maximum-likelihood criterion. The total numbers of word classes are set to be 80 for both Chinese and English. One question may be asked is the reason that we remove all alignment links from phrase pairs of relative low MBR scores. In fact, although all alignment links are not necessarily bad in those low-quality phrase pairs, we just remove all of them from training corpus for convenience. By varying different values of ݐ, we can empirically find an optimal setting, where alignment pruning can bring benefits for final translation quality. 2.4 x Similar"
2011.mtsummit-papers.20,P06-1097,0,0.0229039,"rs. Because of using MBR-inspired techniques, we also investigate the impacts of our method on MBR decoding over translation hypergraphs of the baseline system. We re-implement an MBR decoder (Kumar et al., 2009) that uses the linear BLEU score as its loss function. 4.3 Word Aligner Although discriminative methods have already shown comparable word alignment accuracy in benchmarks, generative methods are still widely used to produce word alignments for large scale corpus. As a result, we evaluate our approach based on two different word aligners. x Disc-Aligner. A discriminative word aligner (Fraser and Marcu, 2006) is re-implemented to predict alignments for the training corpus. A data set of 491 sentence pairs with human annotated word alignments is used to tune model parameters. Disc-Aligner can produce n-best alignment alternatives. x GIZA-Aligner: An unsupervised word aligner GIZA++ (Och and Ney, 2003) is used with the default parameters. In this paper, we only use its Viterbi (1-best) alignment outputs. 4.4 Baseline Phrase Extraction Method The standard phrase extraction method (Base-PE) proposed by Och and Ney (2004) is utilized to generate the baseline phrase table. The length limitations are set"
2011.mtsummit-papers.20,P03-1021,0,0.0600191,"re peak, while Ͳ  ߙ  ͳ makes the distribution more uniform. Due to the fact that varying ߙ to modify the entropy of the alignment distribution doesn’t have consistent impacts on translation quality (Venugopal et al., 2008), in this paper we just fix this value to be 1.0. ሼܵ ሺߩǡ ሺሻሽ maintained for each phase pair as additional phrasal features; then, we use this phrase table in our log-linear SMT system and optimize the weights of these similarity scores together with the weights of original SMT model features to maximize BLEU on development data set using the MERT algorithm proposed by Och (2003) 3 ; last, we collect the well-tuned feature weights ሼߣ ሽ and ሼߣ ሽ and compute ܵ ሺߩሻ and ܵ ሺߩሻ for each ߩ based on Equation (3) and (4). Algorithm 1: MBR Phrase Scoring 1: 2: 3: 4: 5: 6: 7: 8 We rewrite Equation (1) by replacing ܩሺߩǡ ߩᇱ ሻ using Equation (2) as: ܵ ሺߩሻ ൌ  ߣ ሼ  ߠ ሺߩǡ ߩᇱ ሻܲሺߩᇱ ȁሻሽ ఘᇲ אሺሻ  9: 10: 11: 12: 13: 14: 15: 16: 17: 18: ሺ͵ሻ ൌ  ߣ ܵ ሺߩǡ ሺሻሻ  ܵ ൫ߩǡ ሺሻ൯ ൌ σఘᇲ אሺሻ ߠ ሺߩǡ ߩᇱ ሻܲሺߩᇱ ȁሻ is defined as the expected value of the  th similarity feature ߠ for ߩ based on the entire ሺሻ. We then consider scoring phrase pairs based on their target phra"
2011.mtsummit-papers.20,J07-3002,0,0.0249927,"Missing"
2011.mtsummit-papers.20,J04-4002,0,0.157296,"in the parallel data; last, a new phrase table is learned from the link-pruned parallel data and used in SMT decoding. We evaluate our approach on the NIST Chinese-to-English MT tasks, and show significant improvements on parallel data sets of different scales. 1 ≒ᕩ ≒ᕩ hydrogen bomb bomb hydrogen bombs (a) (b) ≒ᕩ ≒ᕩ was (c) detonated against (d) Figure 1: Phrase pairs extracted from different bilingual sentence pairs with the same source phrases, in which dashed lines denote wrong alignment links. Introduction Bilingual phrases are the fundamental building blocks for phrase-based SMT systems (Och and Ney, 2004; Koehn et al., 2004a; Chiang, 2005), and their abilities to handle local reorderings and translation ambiguity as well as many-to-many word translations are key factors to the success of phrasal SMT models. The common practice of extracting bilingual phrases from the parallel data usually consists of three steps: first, words in bilingual sentence pairs are aligned using state-of-the-art automatic word alignment tools, such as GIZA++ (Och and Ney, 2003), in both directions; second, word alignment links are refined using heuristics, such as Grow-Diagonal-Final (GDF) method; third, bilingual ph"
2011.mtsummit-papers.20,C10-1056,0,0.218567,"Missing"
2011.mtsummit-papers.20,P02-1040,0,0.0824251,"investigate the impacts of different parameter settings on this corpus; the second data set includes the following data sets, LDC2003E07, LDC2003E14, LDC2005T06 and LDC2005T10 with 354K sentence pairs contained after pre-processing. We confirm the effectiveness of our method on it using the optimal parameter setting determined on the first data set. We train a 5-gram language model on the Xinhua portion of LDC English Gigaword Version 3.0. Translation quality is measured in terms of the case-insensitive IBM-BLEU scores that compute the brevity penalty using the closest reference translation (Papineni et al., 2002). Statistical significance is computed by using the bootstrap resampling method proposed by Koehn (2004b). 5 The stability of MERT should be concerned when using so many features. Here, we alleviate this issue by enlarging the beam size and increasing the rounds of MERT iterations. 193 SMT Decoder A re-implemented phrase-based SMT decoder (Xiong et al., 2006) with a lexicalized reordering component based on maximum entropy is used to generate translation outputs. Both the phrase table and the reordering model are trained on the same bilingual corpus used. The default beam size is set to be 100"
2011.mtsummit-papers.20,P09-1104,0,0.0455259,"Missing"
2011.mtsummit-papers.20,W96-0213,0,0.212647,"ܽǡ ǡ ሽ do find ܵመ ሺߩሻ ൌ ݔܽܯఘᇲאሺሻ ሼܵ ሺߩᇱ ሻሽ from ሺሻ find ܵመ ሺߩሻ ൌ ݔܽܯఘᇲאሺሻ ሼܵ ሺߩᇱ ሻሽ from ሺሻ ܵ௫ ሺߩሻ ൌ ܵመ ሺߩሻ ܵ כመ ሺߩሻ if ܵ ሺߩሻ ܵ כ ሺߩሻൟ ൏ ሼܵ௫ ሺߩሻ  ݐ כሽ then prune all alignment links contained in ߩ from the positions they were extracted in ࣝ end if end for return ࣝ with link-pruned word alignments 1) ߠௐଶௐ ሺߩǡ ߩᇱ ሻ . A feature that counts how many (source word)-to-(target word) link pairs in ߩ co-occur in ߩᇱ . 2) ߠௐଶ ሺߩǡ ߩᇱ ሻ. A feature that counts how many (source word)-to-(target word’s POS) link pairs in ߩ co-occur in ߩᇱ . Two MaxEnt-based POS taggers (Ratnaparkhi, 1996) are used to tag Chinese and English words contained in the bilingual corpus respectively. 4) ߠௐଶௌ ሺߩǡ ߩᇱ ሻ. A feature that counts how many (source word)-to-(target word’s stem) link pairs in ߩ co-occur in ߩᇱ . A stem dictionary that contains 22,660 entries is used to convert English words into their stem forms. We consider the stem for each Chinese word as the Chinese word itself. 5) ߠி௧ ሺߩǡ ߩᇱ ሻ . A feature that reflects the agreement on word fertilities for ߩ and ߩᇱ : Step 4: Phrase Re-Extraction Last, we re-extract bilingual phrases based on the link-pruned training corpus to learn a new"
2011.mtsummit-papers.20,P09-2031,0,0.0430818,"Missing"
2011.mtsummit-papers.20,D08-1065,0,0.0163685,"ሺߩǡ ߩᇱ ሻ. A feature that counts how many ngrams in ߩ௧ co-occur in ߩ௧ ᇱ : 192 4.2 ߠ ሺߩǡ ߩᇱ ሻ ൌ  ఠ ሺߩ௧ ሻߜఘᇲ  ሺሻ ఠאఘ ఠ ሺߩ௧ ሻ is the number of times that  occurs in ߩ௧ , ߜఘᇲ  ሺሻ equals to 1 when  occurs in ߩ௧ᇱ , and 0 otherwise. In this paper, the order of n-gram considered varies from 1 to 4. 8) ߠ ሺߩǡ ߩᇱ ሻ . A feature that reflects the agreement on word lengths for ߩ௧ and ߩ௧ᇱ : ߠ ሺߩǡ ߩᇱ ሻ ൌ ȁߩ௧ ȁߜȁఘ ȁ ሺȁߩ௧ᇱ ȁሻ ߜȁఘ ȁ ሺȁߩ௧ᇱ ȁሻ equals to 1 when ȁߩ௧ ȁ ൌ ȁߩ௧ᇱ ȁ, and 0 otherwise. These features are motivated by the success of consensus-based techniques (Kumar and Byrne, 2004; Tromble et al., 2008; Kumar et al., 2009). To summarize, 6 features are contained in the first category and 5 features are contained in the second category. Because that source and target phrases are exchangeable for each phrase pair, there will be (2*11=22) similarity features in total for each bilingual phrase5. 4 4.1 Experiments Data and Metric We evaluate on the NIST Chinese-to-English MT tasks. The NIST 2003 (MT03) data set is used as the development set to tune model parameters, and evaluation results are reported on the NIST 2005 (MT05) and 2008 (MT08) data sets. Two parallel data sets with different scale"
2011.mtsummit-papers.20,J07-1003,0,0.011579,"al times, all these bilingual phrases were maintained in the generated phrase table as valid entries. Incorrect phrase entries fed into SMT decoder are one of the major reasons for translation errors in phrase-based SMT systems. For example, even (d) doesn’t occupy a large portion of probabilities in all translation alternatives of the source phrase ≒ᕩ, it is still picked up by SMT decoder sometimes, because it is strongly preferred by the language model in certain circumstances. Motivated by the success of consensus-based methods in SMT research (Kumar and Byrne, 2002; Kumar and Byrne, 2004; Ueffing and Ney, 2007; Kumar et al., 2009), this paper proposes a novel approach that makes use of MBR principle to improve the accuracy of phrase extraction. Our approach operates as a four-stage pipeline: first, bilingual phrases are extracted from parallel corpus using a standard phrase induction method; then, phrases are separated into groups under specific constraints and scored using an MBR model; next, word alignment links contained in phrases with their MBR scores lower than a certain threshold are pruned in the parallel data; last, a new phrase table is learned from the linkpruned parallel data and used i"
2011.mtsummit-papers.20,W04-3250,0,0.0256834,"g data sets, LDC2003E07, LDC2003E14, LDC2005T06 and LDC2005T10 with 354K sentence pairs contained after pre-processing. We confirm the effectiveness of our method on it using the optimal parameter setting determined on the first data set. We train a 5-gram language model on the Xinhua portion of LDC English Gigaword Version 3.0. Translation quality is measured in terms of the case-insensitive IBM-BLEU scores that compute the brevity penalty using the closest reference translation (Papineni et al., 2002). Statistical significance is computed by using the bootstrap resampling method proposed by Koehn (2004b). 5 The stability of MERT should be concerned when using so many features. Here, we alleviate this issue by enlarging the beam size and increasing the rounds of MERT iterations. 193 SMT Decoder A re-implemented phrase-based SMT decoder (Xiong et al., 2006) with a lexicalized reordering component based on maximum entropy is used to generate translation outputs. Both the phrase table and the reordering model are trained on the same bilingual corpus used. The default beam size is set to be 100, and MERT algorithm (Och, 2003) is utilized to optimize model parameters. Because of using MBR-inspire"
2011.mtsummit-papers.20,W02-1019,0,0.0800667,"Because of appearing in training corpus several times, all these bilingual phrases were maintained in the generated phrase table as valid entries. Incorrect phrase entries fed into SMT decoder are one of the major reasons for translation errors in phrase-based SMT systems. For example, even (d) doesn’t occupy a large portion of probabilities in all translation alternatives of the source phrase ≒ᕩ, it is still picked up by SMT decoder sometimes, because it is strongly preferred by the language model in certain circumstances. Motivated by the success of consensus-based methods in SMT research (Kumar and Byrne, 2002; Kumar and Byrne, 2004; Ueffing and Ney, 2007; Kumar et al., 2009), this paper proposes a novel approach that makes use of MBR principle to improve the accuracy of phrase extraction. Our approach operates as a four-stage pipeline: first, bilingual phrases are extracted from parallel corpus using a standard phrase induction method; then, phrases are separated into groups under specific constraints and scored using an MBR model; next, word alignment links contained in phrases with their MBR scores lower than a certain threshold are pruned in the parallel data; last, a new phrase table is learne"
2011.mtsummit-papers.20,N04-1022,0,0.138498,"n training corpus several times, all these bilingual phrases were maintained in the generated phrase table as valid entries. Incorrect phrase entries fed into SMT decoder are one of the major reasons for translation errors in phrase-based SMT systems. For example, even (d) doesn’t occupy a large portion of probabilities in all translation alternatives of the source phrase ≒ᕩ, it is still picked up by SMT decoder sometimes, because it is strongly preferred by the language model in certain circumstances. Motivated by the success of consensus-based methods in SMT research (Kumar and Byrne, 2002; Kumar and Byrne, 2004; Ueffing and Ney, 2007; Kumar et al., 2009), this paper proposes a novel approach that makes use of MBR principle to improve the accuracy of phrase extraction. Our approach operates as a four-stage pipeline: first, bilingual phrases are extracted from parallel corpus using a standard phrase induction method; then, phrases are separated into groups under specific constraints and scored using an MBR model; next, word alignment links contained in phrases with their MBR scores lower than a certain threshold are pruned in the parallel data; last, a new phrase table is learned from the linkpruned p"
2011.mtsummit-papers.20,2008.amta-papers.18,0,0.0751871,"sing a standard phrase induction method; then, phrases are separated into groups under specific constraints and scored using an MBR model; next, word alignment links contained in phrases with their MBR scores lower than a certain threshold are pruned in the parallel data; last, a new phrase table is learned from the linkpruned parallel data and used in SMT decoding. We evaluate on a state-of-the-art phrase-based SMT decoder on the NIST Chinese-to-English MT tasks, and experiments show that our MBRbased approach outperforms the standard phrase extraction method by up to 1.45 BLEU points. 2008; Venugopal et al., 2008; Liu et al., 2009), in which n-best alternatives of annotations to SMT systems are leveraged to improve translation quality, we allow our proposed phrase extraction method to operate on n-best word alignments as well: given a sentence pair with n-best alignment candidates, we use alignments in the n-best list one at a time with the same sentence pair to form a new word-aligned sentence pair, and annotate it with the posterior probability of the alignment it used. These posterior probabilities will be used in the next step to compute MBR model scores. 2 The objective of this step is to score p"
2011.mtsummit-papers.20,P06-1066,0,0.0260477,"5-gram language model on the Xinhua portion of LDC English Gigaword Version 3.0. Translation quality is measured in terms of the case-insensitive IBM-BLEU scores that compute the brevity penalty using the closest reference translation (Papineni et al., 2002). Statistical significance is computed by using the bootstrap resampling method proposed by Koehn (2004b). 5 The stability of MERT should be concerned when using so many features. Here, we alleviate this issue by enlarging the beam size and increasing the rounds of MERT iterations. 193 SMT Decoder A re-implemented phrase-based SMT decoder (Xiong et al., 2006) with a lexicalized reordering component based on maximum entropy is used to generate translation outputs. Both the phrase table and the reordering model are trained on the same bilingual corpus used. The default beam size is set to be 100, and MERT algorithm (Och, 2003) is utilized to optimize model parameters. Because of using MBR-inspired techniques, we also investigate the impacts of our method on MBR decoding over translation hypergraphs of the baseline system. We re-implement an MBR decoder (Kumar et al., 2009) that uses the linear BLEU score as its loss function. 4.3 Word Aligner Althou"
2011.mtsummit-papers.20,P09-2060,0,0.218602,"Missing"
2011.mtsummit-papers.20,W04-3227,0,0.0699735,"Missing"
2011.mtsummit-papers.20,P09-1019,0,0.105678,"ingual phrases were maintained in the generated phrase table as valid entries. Incorrect phrase entries fed into SMT decoder are one of the major reasons for translation errors in phrase-based SMT systems. For example, even (d) doesn’t occupy a large portion of probabilities in all translation alternatives of the source phrase ≒ᕩ, it is still picked up by SMT decoder sometimes, because it is strongly preferred by the language model in certain circumstances. Motivated by the success of consensus-based methods in SMT research (Kumar and Byrne, 2002; Kumar and Byrne, 2004; Ueffing and Ney, 2007; Kumar et al., 2009), this paper proposes a novel approach that makes use of MBR principle to improve the accuracy of phrase extraction. Our approach operates as a four-stage pipeline: first, bilingual phrases are extracted from parallel corpus using a standard phrase induction method; then, phrases are separated into groups under specific constraints and scored using an MBR model; next, word alignment links contained in phrases with their MBR scores lower than a certain threshold are pruned in the parallel data; last, a new phrase table is learned from the linkpruned parallel data and used in SMT decoding. We ev"
2011.mtsummit-papers.20,2006.amta-papers.11,0,0.0384085,"Missing"
2020.coling-main.288,S17-2126,0,0.0277777,"d Emotion Network and BERT+EmNet for BERT-based Emotion Network. 3238 3 Experiments 3.1 Setup Dataset We use the English subset of Twitter dataset provided by SemEval 2018 (Mohammad et al., 2018). The dataset contains 11 emotions: anger, anticipation, disgust, fear, joy, love, optimism, pessimism, sadness, surprise and trust. The training data contains 6,838 tweets. The development and test sets have 886 and 3,259 tweets respectively. The data preprossing in the LSTM and BERT models are different. For LSTM models we preprocess the corpus following (Baziotis et al., 2018) where the ekphrasis2 (Baziotis et al., 2017) tool is used. The preprocessing steps included in ekphrasis are: Twitter-specific tokenization, spell correction, word normalization, word segmentation (for splitting hashtags) and word annotation.The BPE (Sennrich et al., 2015) is not applied and 800K unique words are collected. For BERT models, we just use the default preprocessing procedures in BERT including tokenization and BPE to preprocess the corpus. Evaluation Metrics We use the official competition metric provided by SemEval 2018 for comparison that is the multi-label accuracy (or Jaccard index) (Mohammad et al., 2018). Multi-label"
2020.coling-main.288,N19-1423,0,0.172993,"he sentence emotions based on both the encoded sentence representations and generated word emotions. With the newly introduced emotion generator, our EmNet can alleviate the domain mismatch and emotion ambiguity problems of using external lexicons. For example, the contextual words “how long”, “keeps diving and ducking” can help disambiguate the emotion of the word “joke”, thus improve the accuracy of emotion classification. We validate the proposed approach on the Twitter dataset of SemEval-2018 task (Mohammad et al., 2018) on top of both the LSTM (Hochreiter and Schmidhuber, 1997) and BERT (Devlin et al., 2019) architectures. Our approach consistently outperforms the baseline models across model architectures, demonstrating the effectiveness and universality of the proposed approach. In addition, our model also outperforms the SOTA method of leveraging external emotion lexicon. Further analyses reveal that the proposed EmNet can learn reasonable emotion lexicons as expected, which avoids the mismatch problem of using external resource. Contributions. The main contributions of this paper are listed as follows: • We propose a novel emotional network for multi-label emotion classification which jointly"
2020.coling-main.288,P15-1101,0,0.0141223,"lassification task (e.g., positive, negative), emotion classification or affect detection is a multi-label classification task which is to detect a discrete set of emotions present in a given sentence such as anger, joy, sadness etc (Dalgleish and Power, 2000; Plutchik, 2001). Traditional methods such as lexicon, n-gram and graph models have been used. Xu et al. (2012) proposed a coarse-to-fine strategy for multi-label emotion classification. They dealt with the data sparseness problem by incorporating the transfer probabilities from the neighboring sentences to refine the emotion categories. Li et al. (2015) recast multi-label emotion classification as a factor graph inferring problem in which the label and context dependence are modeled as various factor functions. Yan and Turtle (2016) 3242 long -0.793 0.596 0.987 0.092 1.161 -1.899 0.951 0.015 0.787 -1.26 -1.395 0.014 long will -0.853 0.72 1.025 0.191 1.051 -1.871 0.927 0.018 0.747 -1.324 -1.374 0.011 will he -0.71 0.705 1.072 0.167 1.117 -1.855 0.809 0.034 0.727 -1.303 -1.503 0.013 he keeps -0.715 0.675 1.106 0.119 1.032 -1.927 0.737 0.098 0.873 -1.307 -1.437 0.015 keeps words anger anticipation disgust fear joy love optimism pessimism sadnes"
2020.coling-main.288,S18-1043,0,0.100652,"in previous studies and achieves new state-of-the-art on the benchmark Twitter dataset. 1 Introduction Tweet Emotion The last several years have seen a land rush in research on identification of emotions in short This is a joke really how long will disgust text such as Twitter or product reviews due to he keep diving and ducking. its greatly commercial value. For example, the That’s the joke. I know it’s incense. joy emotions (e.g., anger or joy) expressed in prodTable 1: Example sentences and their emotions. uct reviews can be a major factor in deciding the marketing strategy for a company (Meisheri and Dey, 2018). The SOTA approaches to this task (Baziotis et al., 2018; Meisheri and Dey, 2018) generally employ pre-defined emotion lexicons, which have two major limitations: 1. Most established emotion lexicons were created for a general domain, and suffer from limited coverage and inaccuracies when applied to the highly informal short text. 2. The pre-defined lexicons suffer from the ambiguity problem: the emotion of a word is highly influenced by the context. Table 1 shows an example. The word “joke” carries different emotions according to different context. In this work, we tackle these challenges by"
2020.coling-main.288,S18-1001,0,0.372314,"motions, which dynamically adapt to the sentence context. 3. Emotion classifier classifies the sentence emotions based on both the encoded sentence representations and generated word emotions. With the newly introduced emotion generator, our EmNet can alleviate the domain mismatch and emotion ambiguity problems of using external lexicons. For example, the contextual words “how long”, “keeps diving and ducking” can help disambiguate the emotion of the word “joke”, thus improve the accuracy of emotion classification. We validate the proposed approach on the Twitter dataset of SemEval-2018 task (Mohammad et al., 2018) on top of both the LSTM (Hochreiter and Schmidhuber, 1997) and BERT (Devlin et al., 2019) architectures. Our approach consistently outperforms the baseline models across model architectures, demonstrating the effectiveness and universality of the proposed approach. In addition, our model also outperforms the SOTA method of leveraging external emotion lexicon. Further analyses reveal that the proposed EmNet can learn reasonable emotion lexicons as expected, which avoids the mismatch problem of using external resource. Contributions. The main contributions of this paper are listed as follows: •"
2020.coling-main.288,D19-5541,0,0.0152637,"odel is a two-layer LSTM network where external emotion lexicons are used to provide word level affective knowledge. • TCS Research The Rank 2 method of SemEval-2018 Task 1 proposed by Meisheri and Dey (2018). The model uses two BiLSTM networks to encode tweets from different aspects. Then they concatenated the hidden states for the final classifications. • DATN-2 A transfer learning method proposed by Yu et al. (2018) for emotion classification in tweets. They used a shared-private architecture with the dual attention mechanism to encode tweets into features. • BERTbase +DK and BERTlarge +DK Ying et al. (2019) proposed to integrate domain knowledge into BERT for emotion classification. We compare with both their BERTbase and BERTlarge models. 2 https://competitions.codalab.org/competitions/17751 3239 ID 1 2 3 4 5 6 7 8 9 Method NTUA-SLP (Baziotis et al., 2018) TCS Research (Meisheri and Dey, 2018) DATN-2 (Yu et al., 2018) BERTbase +DK (Ying et al., 2019) BERTlarge +DK (Ying et al., 2019) Bi-LSTM Baseline 6 + EmNet BERTbase Baseline 8 + EmNet Accuracy 58.8 58.2 58.3 59.1 59.5 56.6 59.0† 58.0 59.6† F1-micro 70.1 69.3 71.3 71.6 68.3 70.1† 70.1 71.6† F1-macro 52.8 53.0 54.4 54.9 56.3 49.2 55.5† 53.0 56"
2020.coling-main.288,D18-1137,0,0.0699925,"aseline, we remove the emotion generator in Figure 1 and use the [CLS] embedding for classification. • NTUA-SLP The Rank 1 method of SemEval-2018 Task 1 proposed by Baziotis et al. (2018). The model is a two-layer LSTM network where external emotion lexicons are used to provide word level affective knowledge. • TCS Research The Rank 2 method of SemEval-2018 Task 1 proposed by Meisheri and Dey (2018). The model uses two BiLSTM networks to encode tweets from different aspects. Then they concatenated the hidden states for the final classifications. • DATN-2 A transfer learning method proposed by Yu et al. (2018) for emotion classification in tweets. They used a shared-private architecture with the dual attention mechanism to encode tweets into features. • BERTbase +DK and BERTlarge +DK Ying et al. (2019) proposed to integrate domain knowledge into BERT for emotion classification. We compare with both their BERTbase and BERTlarge models. 2 https://competitions.codalab.org/competitions/17751 3239 ID 1 2 3 4 5 6 7 8 9 Method NTUA-SLP (Baziotis et al., 2018) TCS Research (Meisheri and Dey, 2018) DATN-2 (Yu et al., 2018) BERTbase +DK (Ying et al., 2019) BERTlarge +DK (Ying et al., 2019) Bi-LSTM Baseline 6"
2020.coling-main.288,D16-1061,1,0.821774,"sgust fear the joy joke love . i know it &apos; s incense optimism pessimism sadness surprise trust (b) Figure 3: Visualization of attention weights and word emotions for tweets in Table 1. (a) the case study of T1 in Table 1. (b) the case study of T2 in Table 1. The color in deep means more weights. We highlight the words with larger attention weights. built a separate binary classifier for each emotion category to detect if an emotion category were present or absent in a tweet with traditional unigram features. The neural network models have also been used in emotion classification. For example, Zhou et al. (2016) proposed an emotion distribution learning (EDL) method, which first used recursive autoencoders (RAEs) to extract features and then conducted multi-label emotion classification by incorporating the label relations into the cost function. He and Xia (2018) provided an end-to-end learning framework by integrating representation learning and multi-label classification in one neural network. Recently, external knowledge has been widely employed for this task. One representative research line is the transfer learning. Yu et al. (2018) proposed a new transfer learning architecture to divide the sen"
2020.wmt-1.34,D18-1457,1,0.850332,"Missing"
2020.wmt-1.34,D18-1045,0,0.0270558,"arge-scale Back-translation Back-translation is the most commonly used data augmentation technique to incorporate monolingual data into NMT (Sennrich et al., 2016a). The method first trains an intermediate target-to-source system, which is used to translate target monolingual corpus into source. Then the synthetic parallel corpus is used to train models together with the bilingual data. In this work we apply the noise back-translations method as introduced in (Lample et al., 2018). When translating monolingual data we use an ensemble of two models to get better source translations. We follow (Edunov et al., 2018) to add noise to the synthetic source data. Furthermore, we use a tag at the head of each synthetic source sentence as Caswell et al. (2019) does. To filter the pseudo corpus, we translate the synthetic source into target and calculate a Round-Trip BLEU score, the synthetic pairs are dropped if the BLEU score is lower than 30. Notably, we only apply back translation to the English → German task. We find that back translation decrease the translation quality to Chinese ↔ English tasks in our experiments. 2 https://github.com/mosessmt/mosesdecoder/tree/master/scripts/tokenizer/tokenizer.perl 3 3"
2020.wmt-1.34,D19-1135,1,0.88946,"Missing"
2020.wmt-1.34,N19-4009,0,0.0304603,", 2019) encoder. • B IG D EEP T RANSFORMER is the T RANSFORMER - BIG model with 20 encoder layers. • L ARGER T RANSFORMER is similar to B IG D EEP model except that it uses 8192 as the FFN inner width. The main differences between these models are presented in Table 1. To stabilize the training of deep model, we use the Pre-Norm strategy (Li et al., 2019). The layer normalization was applied to the input of every sub-layer which the computation sequence could be expressed as: normalize → Transform → dropout → residual-add. All models are implemented on top of the open-source toolkit Fairseq3 (Ott et al., 2019). 3.2 Data Augmentation Data augmentation is a commonly used technique to improve the translation quality. There are various of methods to conduct data augmentation such as back-translation (Sennrich et al., 2016a), joint training (Zhang et al., 2018) etc. In this section, we will introduce the methods we used in WMT2020. 3.2.1 Large-scale Back-translation Back-translation is the most commonly used data augmentation technique to incorporate monolingual data into NMT (Sennrich et al., 2016a). The method first trains an intermediate target-to-source system, which is used to translate target mono"
2020.wmt-1.34,P16-1009,0,0.261753,"tion task, our Tencent Translation team participated in three WMT2020 shared news translation tasks, including Chinese → English, English → Chinese and English → German. For the three tasks, we use similar model architectures and training strategies. Four structures are used and all of them are based on deep transformer which are proven more effective than the standard Transformer-big models (Li et al., 2019). In terms of data augmentation, we adopt R2L training (Zhang et al., 2019) to all the tasks. Monolingual data is only used in English → German task following the back-translation manner (Sennrich et al., 2016b). Different from the standard backtranslation, we add noise to the synthetic source ∗ Equal contribution. Correspondence to {frostwu, brightxwang, vinnylywang, fangxuliu}@tencent.com. sentence in order to take advantage of large-scale monolingual text. In addition, we add a special token to the synthetic source sentence to help the model better distinguish the bilingual data and synthetic data. The in-domain finetuning (Sun et al., 2019) is very effective in our three experiments and specially, we propose a boosted finetuning method for English ↔ Chinese tasks. We also take advantage of the"
2020.wmt-1.34,P16-1162,0,0.542977,"tion task, our Tencent Translation team participated in three WMT2020 shared news translation tasks, including Chinese → English, English → Chinese and English → German. For the three tasks, we use similar model architectures and training strategies. Four structures are used and all of them are based on deep transformer which are proven more effective than the standard Transformer-big models (Li et al., 2019). In terms of data augmentation, we adopt R2L training (Zhang et al., 2019) to all the tasks. Monolingual data is only used in English → German task following the back-translation manner (Sennrich et al., 2016b). Different from the standard backtranslation, we add noise to the synthetic source ∗ Equal contribution. Correspondence to {frostwu, brightxwang, vinnylywang, fangxuliu}@tencent.com. sentence in order to take advantage of large-scale monolingual text. In addition, we add a special token to the synthetic source sentence to help the model better distinguish the bilingual data and synthetic data. The in-domain finetuning (Sun et al., 2019) is very effective in our three experiments and specially, we propose a boosted finetuning method for English ↔ Chinese tasks. We also take advantage of the"
2020.wmt-1.34,W19-5341,0,0.0876143,"to improve single models. Ensemble is used to combine single models and we propose an iterative transductive ensemble method which can further improve the translation performance based on the ensemble results. We achieve a BLEU score of 36.8 and the highest chrF score of 0.648 on Chinese → English task. 1 Introduction Recently, Transformer (Vaswani et al., 2017), that depends on self-attention mechanism , has significantly improved the translation quality. It is widely used as basic Neural Machine Translation (NMT) models in previous WMT translation tasks (Wang et al., 2018b; Li et al., 2019; Sun et al., 2019). In this year’s translation task, our Tencent Translation team participated in three WMT2020 shared news translation tasks, including Chinese → English, English → Chinese and English → German. For the three tasks, we use similar model architectures and training strategies. Four structures are used and all of them are based on deep transformer which are proven more effective than the standard Transformer-big models (Li et al., 2019). In terms of data augmentation, we adopt R2L training (Zhang et al., 2019) to all the tasks. Monolingual data is only used in English → German task following the b"
2020.wmt-1.34,2020.wmt-1.60,1,0.813015,"and the “update-freq” parameter in Fairseq is set to 8. Specifically, for L ARGE settings, the batch size is 4096 and “update-freq” is 16. We set max learning rate to 0.0007 and warmup-steps to 4000. All the dropout probabilities are set to 0.1. We select the checkpoint with the lowest loss on development set as the final checkpoint in each training. We calculate sacreBLEU score 6 for all experiments which is officially recommended. The WMT2019 testset (test2019) is used as the development set for all the tasks. 4.2 3.7 Iterative Transductive Ensemble Transductive ensemble (TE) is proposed by Wang et al. (2020c). The key idea is that source input sentences from the validation and test sets are firstly translated to the target language space with multiple different well-trained NMT models, which results in a pretranslated synthetic dataset. Then individual models are finetuned on the generated synthetic dataset. We propose an variation of TE, the Iterative Transductive Ensemble (ITE) which is based on Ensemble, as following: Algorithm 1: Iterative Transductive Ensemble 6 Input: Single models M1m , In-domain corpus D, E1n is n different ensemble combinations Output: Single models M1m Translate D with"
2020.wmt-1.34,W18-6429,1,0.42399,"boosted in-domain finetuning method to improve single models. Ensemble is used to combine single models and we propose an iterative transductive ensemble method which can further improve the translation performance based on the ensemble results. We achieve a BLEU score of 36.8 and the highest chrF score of 0.648 on Chinese → English task. 1 Introduction Recently, Transformer (Vaswani et al., 2017), that depends on self-attention mechanism , has significantly improved the translation quality. It is widely used as basic Neural Machine Translation (NMT) models in previous WMT translation tasks (Wang et al., 2018b; Li et al., 2019; Sun et al., 2019). In this year’s translation task, our Tencent Translation team participated in three WMT2020 shared news translation tasks, including Chinese → English, English → Chinese and English → German. For the three tasks, we use similar model architectures and training strategies. Four structures are used and all of them are based on deep transformer which are proven more effective than the standard Transformer-big models (Li et al., 2019). In terms of data augmentation, we adopt R2L training (Zhang et al., 2019) to all the tasks. Monolingual data is only used in"
2020.wmt-1.34,W18-6430,0,0.0695928,"boosted in-domain finetuning method to improve single models. Ensemble is used to combine single models and we propose an iterative transductive ensemble method which can further improve the translation performance based on the ensemble results. We achieve a BLEU score of 36.8 and the highest chrF score of 0.648 on Chinese → English task. 1 Introduction Recently, Transformer (Vaswani et al., 2017), that depends on self-attention mechanism , has significantly improved the translation quality. It is widely used as basic Neural Machine Translation (NMT) models in previous WMT translation tasks (Wang et al., 2018b; Li et al., 2019; Sun et al., 2019). In this year’s translation task, our Tencent Translation team participated in three WMT2020 shared news translation tasks, including Chinese → English, English → Chinese and English → German. For the three tasks, we use similar model architectures and training strategies. Four structures are used and all of them are based on deep transformer which are proven more effective than the standard Transformer-big models (Li et al., 2019). In terms of data augmentation, we adopt R2L training (Zhang et al., 2019) to all the tasks. Monolingual data is only used in"
2020.wmt-1.34,P19-1176,0,0.0535969,"Missing"
2020.wmt-1.34,2020.wmt-1.97,1,0.841946,"and the “update-freq” parameter in Fairseq is set to 8. Specifically, for L ARGE settings, the batch size is 4096 and “update-freq” is 16. We set max learning rate to 0.0007 and warmup-steps to 4000. All the dropout probabilities are set to 0.1. We select the checkpoint with the lowest loss on development set as the final checkpoint in each training. We calculate sacreBLEU score 6 for all experiments which is officially recommended. The WMT2019 testset (test2019) is used as the development set for all the tasks. 4.2 3.7 Iterative Transductive Ensemble Transductive ensemble (TE) is proposed by Wang et al. (2020c). The key idea is that source input sentences from the validation and test sets are firstly translated to the target language space with multiple different well-trained NMT models, which results in a pretranslated synthetic dataset. Then individual models are finetuned on the generated synthetic dataset. We propose an variation of TE, the Iterative Transductive Ensemble (ITE) which is based on Ensemble, as following: Algorithm 1: Iterative Transductive Ensemble 6 Input: Single models M1m , In-domain corpus D, E1n is n different ensemble combinations Output: Single models M1m Translate D with"
2020.wmt-1.34,J82-2005,0,0.607939,"Missing"
2021.acl-long.103,D16-1162,0,0.0226188,"ethods, in various text classification tasks. We also present the correlation analysis in the lessdiscussed Transformer-based NMT and reach a similar conclusion. As opposed to the critiques of regarding attention weights as explanation, Wiegreffe and Pinter claim that the trained attention mechanisms do learn something meaningful about the relationship between inputs and outputs, such as syntactic information (Raganato and Tiedemann, 2018; Vig and Belinkov, 2019; Pham et al., 2019). Can Attention be improved? There is plenty of work on supervising attention weights with lexical probabilities (Arthur et al., 2016), word alignment (Chen et al., 2016; Liu et al., 2016; Mi et al., 2016; Cohn et al., 2016; Garg et al., 2019; Feng et al., 2020), human rationales (Strout et al., 2019) and sparsity regularization (Zhang et al., 2019). Unlike them, we never introduce any external knowledge but highlight the inputs whose removal would significantly decrease Transformer’s performance. Another work line aims to make attention better indicative of the inputs’ importance (Kitada and Iyatomi, 2020; Tutek and Snajder, 2020; Mohankumar et al., 2020) which is designed for analysis with no significant performance gain,"
2021.acl-long.103,Q19-1004,0,0.0275735,"yers except the 1-st layer on Zh⇒En task. These findings prove that each attention layer plays a different role in the decoding process. The low layers generally grasp information from various inputs, while the high layers look for some particular words tied to the model predictions. 6 Related Work The attention mechanism is first introduced to augment vanilla recurrent network (Bahdanau et al., 2015; Luong et al., 2015), which are then the backbone of state-of-the-art Transformer (Vaswani et al., 2017) for NMT. It yields better performance and provides a window into how a model is operating (Belinkov and Glass, 2019; Du et al., 2020). This section reviews the recent researches on analyzing and improving attention mechanisms. The Attention Debate Many recent studies have spawned interest in whether attention weights faithfully represent each input token’s responsibility for model prediction. Serrano and Smith flip the model’s decision by permuting some small attention weights, with high-weighted components not being the reason for the decision. Some work (Jain and Wallace, 2019; Vashishth et al., 2019) find a weak correlation between attention scores and other well-ground feature importance metrics, speci"
2021.acl-long.103,2016.amta-researchers.10,0,0.0184399,"tasks. We also present the correlation analysis in the lessdiscussed Transformer-based NMT and reach a similar conclusion. As opposed to the critiques of regarding attention weights as explanation, Wiegreffe and Pinter claim that the trained attention mechanisms do learn something meaningful about the relationship between inputs and outputs, such as syntactic information (Raganato and Tiedemann, 2018; Vig and Belinkov, 2019; Pham et al., 2019). Can Attention be improved? There is plenty of work on supervising attention weights with lexical probabilities (Arthur et al., 2016), word alignment (Chen et al., 2016; Liu et al., 2016; Mi et al., 2016; Cohn et al., 2016; Garg et al., 2019; Feng et al., 2020), human rationales (Strout et al., 2019) and sparsity regularization (Zhang et al., 2019). Unlike them, we never introduce any external knowledge but highlight the inputs whose removal would significantly decrease Transformer’s performance. Another work line aims to make attention better indicative of the inputs’ importance (Kitada and Iyatomi, 2020; Tutek and Snajder, 2020; Mohankumar et al., 2020) which is designed for analysis with no significant performance gain, while our methods incorporate the a"
2021.acl-long.103,N16-1000,0,0.226226,"Missing"
2021.acl-long.103,D19-1453,0,0.0238084,"nsformer-based NMT and reach a similar conclusion. As opposed to the critiques of regarding attention weights as explanation, Wiegreffe and Pinter claim that the trained attention mechanisms do learn something meaningful about the relationship between inputs and outputs, such as syntactic information (Raganato and Tiedemann, 2018; Vig and Belinkov, 2019; Pham et al., 2019). Can Attention be improved? There is plenty of work on supervising attention weights with lexical probabilities (Arthur et al., 2016), word alignment (Chen et al., 2016; Liu et al., 2016; Mi et al., 2016; Cohn et al., 2016; Garg et al., 2019; Feng et al., 2020), human rationales (Strout et al., 2019) and sparsity regularization (Zhang et al., 2019). Unlike them, we never introduce any external knowledge but highlight the inputs whose removal would significantly decrease Transformer’s performance. Another work line aims to make attention better indicative of the inputs’ importance (Kitada and Iyatomi, 2020; Tutek and Snajder, 2020; Mohankumar et al., 2020) which is designed for analysis with no significant performance gain, while our methods incorporate the analytical results to enhance the NMT performance. 7 Conclusion In this pa"
2021.acl-long.103,N19-1357,0,0.119117,"to the current translation are expected to receive more attention. However, many studies doubt whether highlyattended inputs have a large impact on the model outputs. On the one hand, erasing the representations accorded high attention weights do not necessarily lead to a performance decrease (Serrano and † 远 连 日 大 多 人 死 交 中 郊 雪 亡 通 断 deaths _ [before] Introduction ∗ 远 连 日 大 多 人 死 交 中 郊 雪 亡 通 断 <EOS> in _ [after] Smith, 2019), which can be attributed to that unimportant words (e.g., punctuations) are frequently assigned with high attention weights (Mohankumar et al., 2020). On the other hand, Jain and Wallace (2019) state that attention weights are inconsistent with other feature importance metrics in text classification tasks. It further proves that attention mechanisms are incapable of precisely identifying decisive inputs for each prediction, which would result in wrong-translation or over-translation in NMT (Tu et al., 2016). We take Figure 1 as an example. After producing the target-side word “deaths”, attention mechanisms wrongly attribute most attention to the “hEOSi”, making parts of the source sentence untranslated. In this paper, we propose to calibrate the vanilla attention mechanism by focusi"
2021.acl-long.103,P07-2045,0,0.0145057,"man (En⇒De), WMT16 English-Romanian (En⇔Ro), WMT17 English-Finnish (En⇔Fi) and English-Latvian (En⇔Lv). 1291 Source LDC1 WMT142 WMT173 WMT164 Lang. Zh⇒En En⇒De En⇒Lv Lv⇒En En⇒Fi Fi⇒En En⇒Ro Ro⇒En Train 2.09M 4.54M Dev. 878 3000 Test 4789 3003 Vocab. 32k 37k 4.46M 2003 2001 37k 2.63M 3000 3002 32k 0.61M 1999 1999 32k Model GNMT (Wu et al., 2016)‡ Conv (Gehring et al., 2017)‡ AttIsAll (Vaswani et al., 2017)‡ (Feng et al., 2020)‡ (Weng et al., 2020)‡ Our Implemented Baseline Fixed Ours Anneal Gate Table 1: Statistics of the datasets. 4.1 Dataset We tokenize the corpora using a script from Moses (Koehn et al., 2007). Byte pair encoding (BPE) (Sennrich et al., 2016) is applied to all language pairs to construct a join vocabulary except for Zh⇒En where the source and target languages are separately encoded. For Zh⇒En, we remove the sentences of more than 50 words. We use NIST 2002 as validation set, NIST 2003-2006 as the testbed. For En⇒De, newstest2013 and newstest2014 are set as validation and test sets. We use the standard 4-gram BLEU (Papineni et al., 2002) on the true-case output to score the performance. For En⇔Ro, we use newsdev2016 and newstest2016 as development and test sets. For En⇔Lv and En⇔Fi,"
2021.acl-long.103,N16-1082,0,0.0346824,"which simulates the process of perturbation. Formally, let mt be a mask at t-th step. The perturbed attention weight apt is calculated as: apt = mt at + (1 − mt ) µ0 often presented as communicating the relative importance of inputs. However, recent work has cautioned against whether the inputs accorded high attention weights decide the model outputs (Jain and Wallace, 2019). Our analysis examines the correlation with attention weights and feature importance metrics in NMT to test if the attention mechanisms focus on the decisive inputs. We apply gradient-based methods (Simonyan et al., 2014; Li et al., 2016) to measure the importance of each contextual representation hi for model output yt : τit = |∇hi p(yt |x1:n )| (2) We train a baseline Transformer model on NIST Zh⇒En dataset and extract the averaged attention weights over heads. Figure 2 reports the statistics of Kendall-τ correlation for each attention layer, where the observed correlation is all modest (0 indicates no correlation, while 1 implies perfect concordance). The inconsistency with feature importance metrics reveals that the high-attention inputs are not always responsible for the model prediction. It further motivates us whether w"
2021.acl-long.103,C16-1291,0,0.0216138,"sent the correlation analysis in the lessdiscussed Transformer-based NMT and reach a similar conclusion. As opposed to the critiques of regarding attention weights as explanation, Wiegreffe and Pinter claim that the trained attention mechanisms do learn something meaningful about the relationship between inputs and outputs, such as syntactic information (Raganato and Tiedemann, 2018; Vig and Belinkov, 2019; Pham et al., 2019). Can Attention be improved? There is plenty of work on supervising attention weights with lexical probabilities (Arthur et al., 2016), word alignment (Chen et al., 2016; Liu et al., 2016; Mi et al., 2016; Cohn et al., 2016; Garg et al., 2019; Feng et al., 2020), human rationales (Strout et al., 2019) and sparsity regularization (Zhang et al., 2019). Unlike them, we never introduce any external knowledge but highlight the inputs whose removal would significantly decrease Transformer’s performance. Another work line aims to make attention better indicative of the inputs’ importance (Kitada and Iyatomi, 2020; Tutek and Snajder, 2020; Mohankumar et al., 2020) which is designed for analysis with no significant performance gain, while our methods incorporate the analytical results"
2021.acl-long.103,D15-1166,0,0.0570323,"ry across layers. For En⇒De translation, the calibrated attention weights are more uniform at 1-3 layers and more focused at 4-6 layers, while the attention weights become more focused for all layers except the 1-st layer on Zh⇒En task. These findings prove that each attention layer plays a different role in the decoding process. The low layers generally grasp information from various inputs, while the high layers look for some particular words tied to the model predictions. 6 Related Work The attention mechanism is first introduced to augment vanilla recurrent network (Bahdanau et al., 2015; Luong et al., 2015), which are then the backbone of state-of-the-art Transformer (Vaswani et al., 2017) for NMT. It yields better performance and provides a window into how a model is operating (Belinkov and Glass, 2019; Du et al., 2020). This section reviews the recent researches on analyzing and improving attention mechanisms. The Attention Debate Many recent studies have spawned interest in whether attention weights faithfully represent each input token’s responsibility for model prediction. Serrano and Smith flip the model’s decision by permuting some small attention weights, with high-weighted components no"
2021.acl-long.103,D16-1249,0,0.0120831,"on analysis in the lessdiscussed Transformer-based NMT and reach a similar conclusion. As opposed to the critiques of regarding attention weights as explanation, Wiegreffe and Pinter claim that the trained attention mechanisms do learn something meaningful about the relationship between inputs and outputs, such as syntactic information (Raganato and Tiedemann, 2018; Vig and Belinkov, 2019; Pham et al., 2019). Can Attention be improved? There is plenty of work on supervising attention weights with lexical probabilities (Arthur et al., 2016), word alignment (Chen et al., 2016; Liu et al., 2016; Mi et al., 2016; Cohn et al., 2016; Garg et al., 2019; Feng et al., 2020), human rationales (Strout et al., 2019) and sparsity regularization (Zhang et al., 2019). Unlike them, we never introduce any external knowledge but highlight the inputs whose removal would significantly decrease Transformer’s performance. Another work line aims to make attention better indicative of the inputs’ importance (Kitada and Iyatomi, 2020; Tutek and Snajder, 2020; Mohankumar et al., 2020) which is designed for analysis with no significant performance gain, while our methods incorporate the analytical results to enhance the NM"
2021.acl-long.103,2020.acl-main.387,0,0.0690076,"nputs, where the ones that are most relevant to the current translation are expected to receive more attention. However, many studies doubt whether highlyattended inputs have a large impact on the model outputs. On the one hand, erasing the representations accorded high attention weights do not necessarily lead to a performance decrease (Serrano and † 远 连 日 大 多 人 死 交 中 郊 雪 亡 通 断 deaths _ [before] Introduction ∗ 远 连 日 大 多 人 死 交 中 郊 雪 亡 通 断 <EOS> in _ [after] Smith, 2019), which can be attributed to that unimportant words (e.g., punctuations) are frequently assigned with high attention weights (Mohankumar et al., 2020). On the other hand, Jain and Wallace (2019) state that attention weights are inconsistent with other feature importance metrics in text classification tasks. It further proves that attention mechanisms are incapable of precisely identifying decisive inputs for each prediction, which would result in wrong-translation or over-translation in NMT (Tu et al., 2016). We take Figure 1 as an example. After producing the target-side word “deaths”, attention mechanisms wrongly attribute most attention to the “hEOSi”, making parts of the source sentence untranslated. In this paper, we propose to calibra"
2021.acl-long.103,P02-1040,0,0.110384,")‡ Our Implemented Baseline Fixed Ours Anneal Gate Table 1: Statistics of the datasets. 4.1 Dataset We tokenize the corpora using a script from Moses (Koehn et al., 2007). Byte pair encoding (BPE) (Sennrich et al., 2016) is applied to all language pairs to construct a join vocabulary except for Zh⇒En where the source and target languages are separately encoded. For Zh⇒En, we remove the sentences of more than 50 words. We use NIST 2002 as validation set, NIST 2003-2006 as the testbed. For En⇒De, newstest2013 and newstest2014 are set as validation and test sets. We use the standard 4-gram BLEU (Papineni et al., 2002) on the true-case output to score the performance. For En⇔Ro, we use newsdev2016 and newstest2016 as development and test sets. For En⇔Lv and En⇔Fi, newsdev2017 and newstest2017 are validation set and test set. See Table 1 for statistics of the data. 4.2 TEST 24.61 25.16 27.3 27.55 27.7 27.37 27.38 28.1∗ 27.75 Settings We implement the described models with fairseq5 toolkit for training and evaluating. We experiment with Transformer Base (Vaswani et al., 2017): hidden size dmodel = 512, 6 encoder and decoder layers, 8 attention heads and 2048 feed-forward innerlayer dimension. The dropout rate"
2021.acl-long.103,W18-5431,0,0.0183701,"and Wallace, 2019; Vashishth et al., 2019) find a weak correlation between attention scores and other well-ground feature importance metrics, specially gradient-based and leave-one-out methods, in various text classification tasks. We also present the correlation analysis in the lessdiscussed Transformer-based NMT and reach a similar conclusion. As opposed to the critiques of regarding attention weights as explanation, Wiegreffe and Pinter claim that the trained attention mechanisms do learn something meaningful about the relationship between inputs and outputs, such as syntactic information (Raganato and Tiedemann, 2018; Vig and Belinkov, 2019; Pham et al., 2019). Can Attention be improved? There is plenty of work on supervising attention weights with lexical probabilities (Arthur et al., 2016), word alignment (Chen et al., 2016; Liu et al., 2016; Mi et al., 2016; Cohn et al., 2016; Garg et al., 2019; Feng et al., 2020), human rationales (Strout et al., 2019) and sparsity regularization (Zhang et al., 2019). Unlike them, we never introduce any external knowledge but highlight the inputs whose removal would significantly decrease Transformer’s performance. Another work line aims to make attention better indic"
2021.acl-long.103,P16-1162,0,0.0617801,"T17 English-Finnish (En⇔Fi) and English-Latvian (En⇔Lv). 1291 Source LDC1 WMT142 WMT173 WMT164 Lang. Zh⇒En En⇒De En⇒Lv Lv⇒En En⇒Fi Fi⇒En En⇒Ro Ro⇒En Train 2.09M 4.54M Dev. 878 3000 Test 4789 3003 Vocab. 32k 37k 4.46M 2003 2001 37k 2.63M 3000 3002 32k 0.61M 1999 1999 32k Model GNMT (Wu et al., 2016)‡ Conv (Gehring et al., 2017)‡ AttIsAll (Vaswani et al., 2017)‡ (Feng et al., 2020)‡ (Weng et al., 2020)‡ Our Implemented Baseline Fixed Ours Anneal Gate Table 1: Statistics of the datasets. 4.1 Dataset We tokenize the corpora using a script from Moses (Koehn et al., 2007). Byte pair encoding (BPE) (Sennrich et al., 2016) is applied to all language pairs to construct a join vocabulary except for Zh⇒En where the source and target languages are separately encoded. For Zh⇒En, we remove the sentences of more than 50 words. We use NIST 2002 as validation set, NIST 2003-2006 as the testbed. For En⇒De, newstest2013 and newstest2014 are set as validation and test sets. We use the standard 4-gram BLEU (Papineni et al., 2002) on the true-case output to score the performance. For En⇔Ro, we use newsdev2016 and newstest2016 as development and test sets. For En⇔Lv and En⇔Fi, newsdev2017 and newstest2017 are validation set a"
2021.acl-long.103,P19-1282,0,0.0266268,"Missing"
2021.acl-long.103,W19-4807,0,0.0443838,"Missing"
2021.acl-long.103,P16-1008,0,0.0193538,"郊 雪 亡 通 断 deaths _ [before] Introduction ∗ 远 连 日 大 多 人 死 交 中 郊 雪 亡 通 断 <EOS> in _ [after] Smith, 2019), which can be attributed to that unimportant words (e.g., punctuations) are frequently assigned with high attention weights (Mohankumar et al., 2020). On the other hand, Jain and Wallace (2019) state that attention weights are inconsistent with other feature importance metrics in text classification tasks. It further proves that attention mechanisms are incapable of precisely identifying decisive inputs for each prediction, which would result in wrong-translation or over-translation in NMT (Tu et al., 2016). We take Figure 1 as an example. After producing the target-side word “deaths”, attention mechanisms wrongly attribute most attention to the “hEOSi”, making parts of the source sentence untranslated. In this paper, we propose to calibrate the vanilla attention mechanism by focusing more on key in1288 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1288–1298 August 1–6, 2021. ©2021 Association for Computational Linguistics puts. To test what inputs affect the model predicti"
2021.acl-long.103,2020.repl4nlp-1.17,0,0.0172795,"be improved? There is plenty of work on supervising attention weights with lexical probabilities (Arthur et al., 2016), word alignment (Chen et al., 2016; Liu et al., 2016; Mi et al., 2016; Cohn et al., 2016; Garg et al., 2019; Feng et al., 2020), human rationales (Strout et al., 2019) and sparsity regularization (Zhang et al., 2019). Unlike them, we never introduce any external knowledge but highlight the inputs whose removal would significantly decrease Transformer’s performance. Another work line aims to make attention better indicative of the inputs’ importance (Kitada and Iyatomi, 2020; Tutek and Snajder, 2020; Mohankumar et al., 2020) which is designed for analysis with no significant performance gain, while our methods incorporate the analytical results to enhance the NMT performance. 7 Conclusion In this paper, we present a mask perturbation model to automatically discover the decisive inputs for the model prediction. We propose three methods to calibrate the attention mechanism by focusing on the discovered vital inputs. Extensive experimental results show that our approaches obtain significant improvements over the state-of-the-art system. Analytical results indicate that our proposed methods"
2021.acl-long.103,W19-4808,0,0.018581,"t al., 2019) find a weak correlation between attention scores and other well-ground feature importance metrics, specially gradient-based and leave-one-out methods, in various text classification tasks. We also present the correlation analysis in the lessdiscussed Transformer-based NMT and reach a similar conclusion. As opposed to the critiques of regarding attention weights as explanation, Wiegreffe and Pinter claim that the trained attention mechanisms do learn something meaningful about the relationship between inputs and outputs, such as syntactic information (Raganato and Tiedemann, 2018; Vig and Belinkov, 2019; Pham et al., 2019). Can Attention be improved? There is plenty of work on supervising attention weights with lexical probabilities (Arthur et al., 2016), word alignment (Chen et al., 2016; Liu et al., 2016; Mi et al., 2016; Cohn et al., 2016; Garg et al., 2019; Feng et al., 2020), human rationales (Strout et al., 2019) and sparsity regularization (Zhang et al., 2019). Unlike them, we never introduce any external knowledge but highlight the inputs whose removal would significantly decrease Transformer’s performance. Another work line aims to make attention better indicative of the inputs’ imp"
2021.acl-long.103,2021.acl-long.91,0,0.0670852,"Missing"
2021.acl-long.103,P19-1580,0,0.0210165,"Q , W K } represents the parameters of mask perturbation model. The first term indicates that the perturbation operation aims to harm the translation quality. The second one serves as a penalty term to encourage most of the mask to be turned off (perturb inputs as few as possible). Lc (θm ) = k1 − mt k2 (6) The perturbation extent is determined by the hyperparameter α. Notably, earlier studies employ masks and “deletion game” as the analytical tools to explore the importance of each attention head (Fong and Vedaldi, 2017) or the contributions of the pixels in the figure to the model outputs (Voita et al., 2019). However, we extend to probing the inputs’ contributions to the model prediction in NMT and further use the masks to calibrate the attention mechanisms based on the analytical results. 1290 K Q Attention Mechanisms NMT decoder Attention Calibration Network (ACN) Original （?） Attention Weights mask （?） Calibrated （?? ） Attention Weights Better performance after calibration Worse performance after perturbation Perturbed （?? ） Attention Weights Mask Perturbation Model Figure 3: The overview of the framework. The mask perturbation model is trained to perturb the attention weights of decisive inpu"
2021.acl-long.103,2020.emnlp-main.212,0,0.145882,"an accessible measurement of the inputs’ contributions to the model predictions. 4 Experiments We evaluate our method in LDC Chinese-English (Zh⇒En), WMT14 English-German (En⇒De), WMT16 English-Romanian (En⇔Ro), WMT17 English-Finnish (En⇔Fi) and English-Latvian (En⇔Lv). 1291 Source LDC1 WMT142 WMT173 WMT164 Lang. Zh⇒En En⇒De En⇒Lv Lv⇒En En⇒Fi Fi⇒En En⇒Ro Ro⇒En Train 2.09M 4.54M Dev. 878 3000 Test 4789 3003 Vocab. 32k 37k 4.46M 2003 2001 37k 2.63M 3000 3002 32k 0.61M 1999 1999 32k Model GNMT (Wu et al., 2016)‡ Conv (Gehring et al., 2017)‡ AttIsAll (Vaswani et al., 2017)‡ (Feng et al., 2020)‡ (Weng et al., 2020)‡ Our Implemented Baseline Fixed Ours Anneal Gate Table 1: Statistics of the datasets. 4.1 Dataset We tokenize the corpora using a script from Moses (Koehn et al., 2007). Byte pair encoding (BPE) (Sennrich et al., 2016) is applied to all language pairs to construct a join vocabulary except for Zh⇒En where the source and target languages are separately encoded. For Zh⇒En, we remove the sentences of more than 50 words. We use NIST 2002 as validation set, NIST 2003-2006 as the testbed. For En⇒De, newstest2013 and newstest2014 are set as validation and test sets. We use the standard 4-gram BLEU (P"
2021.acl-long.103,D19-1002,0,0.0400659,"Missing"
2021.emnlp-main.14,N18-2105,0,0.224697,"ing, China {xnliang,lizj}@buaa.edu.cn; {frostwu,ethanlli}@tencent.com; Abstract universal and adaptive via extracting phrases based on information from input document itself. In this Embedding based methods are widely used paper, we focus on the unsupervised keyphrase exfor unsupervised keyphrase extraction (UKE) traction (UKE) model. tasks. Generally, these methods simply calculate similarities between phrase embeddings UKE has been widely studied (Mihalcea, 2004; and document embedding, which is insuffiWan and Xiao, 2008a; Bougouin et al., 2013; cient to capture different context for a more Boudin, 2018; Bennani-Smires et al., 2018; Sun effective UKE model. In this paper, we proet al., 2020) in the keyphrase extraction field. Repose a novel method for UKE, where local cently, with the development of text representaand global contexts are jointly modeled. From tion, embedding-based models (Bennani-Smires a global view, we calculate the similarity beet al., 2018; Sun et al., 2020) have achieved promistween a certain phrase and the whole docuing results and become the new state-of-the-art ment in the vector space as transitional embedding based models do. In terms of the local models. Usually,"
2021.emnlp-main.14,I13-1062,0,0.377053,"ent, Beihang University, Beijing, China 2 Tencent Cloud Xiaowei, Beijing, China {xnliang,lizj}@buaa.edu.cn; {frostwu,ethanlli}@tencent.com; Abstract universal and adaptive via extracting phrases based on information from input document itself. In this Embedding based methods are widely used paper, we focus on the unsupervised keyphrase exfor unsupervised keyphrase extraction (UKE) traction (UKE) model. tasks. Generally, these methods simply calculate similarities between phrase embeddings UKE has been widely studied (Mihalcea, 2004; and document embedding, which is insuffiWan and Xiao, 2008a; Bougouin et al., 2013; cient to capture different context for a more Boudin, 2018; Bennani-Smires et al., 2018; Sun effective UKE model. In this paper, we proet al., 2020) in the keyphrase extraction field. Repose a novel method for UKE, where local cently, with the development of text representaand global contexts are jointly modeled. From tion, embedding-based models (Bennani-Smires a global view, we calculate the similarity beet al., 2018; Sun et al., 2020) have achieved promistween a certain phrase and the whole docuing results and become the new state-of-the-art ment in the vector space as transitional embedd"
2021.emnlp-main.14,N19-1423,0,0.078136,"and become the new state-of-the-art ment in the vector space as transitional embedding based models do. In terms of the local models. Usually, these methods compute phrase view, we first build a graph structure based on embeddings and document embedding with static the document where phrases are regarded as word2vec models (e.g. GloVe (Pennington et al., vertices and the edges are similarities between 2014; Le and Mikolov, 2014; Pagliardini et al., vertices. Then, we proposed a new centrality 2018)) or dynamic pre-trained language models computation method to capture local salient (e.g. BERT (Devlin et al., 2019)). Then, they rank information based on the graph structure. Ficandidate phrases by computing the similarity benally, we further combine the modeling of global and local context for ranking. We evaltween phrases and the whole document in the vecuate our models on three public benchmarks tor space. Though, these methods performed better (Inspec, DUC 2001, SemEval 2010) and comthan traditional methods (Mihalcea, 2004; Wan and pare with existing state-of-the-art models. The Xiao, 2008a; Bougouin et al., 2013), the simple results show that our model outperforms most similarity between phrase and d"
2021.emnlp-main.14,2021.eacl-main.93,0,0.0316195,"is paper, we proposed a novel method which jointly models the local and global context of the input document. Specifically, we calculate the similarity between candidate phrases and the whole document for modeling global context. For local context modeling, we first build a graph structure, which represents each phrase as nodes and the edges are similarity between nodes. Then, we proposed a new centrality computation method, which is based on the insight that the most important information typically occurs at the start or end of documents (document boundary) (Lin and Hovy, 1997; Teufel, 1997; Dong et al., 2021), to measure salience of local context based on the graph structure. Finally, we further combine the measure of global similarity and local salience for ranking. To evaluate the effectiveness of our method, we compare our method with recent state-of-the-art models on three public benchmarks (Inspec, DUC 2001, SemEval 2010). The results show that our model can outperform most models while generalizing better on input documents with different domains and length. It is deservedly mentioned that our models have a huge improvement on long scientific documents. 2 Methodology vised keyphrase extracti"
2021.emnlp-main.14,P17-1102,0,0.036178,"Missing"
2021.emnlp-main.14,P14-1119,0,0.0709339,"Missing"
2021.emnlp-main.14,W03-1028,0,0.289061,"ence of phrases which do not appear near the boundary to the centrality of node i. Besides, we employ a threshold θ = β(max(eij − min(eij )) to filter the noise from nodes, which is far different from node i. We remove the influence of them to centrality by setting all eij < θ to zero. β is a hyper-parameter that controls the filter boundary. With the introduction of the noise filter strategy, we rewrite the Equ. (6) as Equ. (7). C(HKPi ) = (8) Experiments Datasets and Evaluation Metrics We evaluate our model on three public datasets: Inspec, DUC2001 and SemEval2010. The Inspec (7) X dataset (Hulth, 2003) consists of 2,000 short doc+λ max(eij − θ, 0) uments from scientific journal abstracts. We foldb (i)≥db (j) low previous works (Bennani-Smires et al., 2018; Where C(HKPi ) represents the local salience of Sun et al., 2020) to use 500 test documents and candidate phrase i. the version of uncontrolled annotated keyphrases For most long documents or news articles, the as ground truth. The DUC2001 dataset (Wan and author tends to write the key information at the Xiao, 2008a) is a collection of 308 long length beginning of the document. Florescu and Caragea news articles with average 828.4 tokens."
2021.emnlp-main.14,E14-1053,0,0.0286422,"ombine global similarity and boundary-aware centrality for local salient information to rank and extract phrases. Unsupervised keyphrase extraction can be divided into four main types: statistics-based models, graph-based models, topic-based models, and embedding-based models. Statistics-based models 5 Conclusion and Future Work (Campos et al., 2018) mainly analyze an article’s probability features such as word frequency fea- In this paper, we point out that embedding-based ture, position feature, linguistic features, etc. Topic- models ignore the local information and propose a based models (Jardine and Teufel, 2014; Liu et al., novel model which jointly models global and local 2009) focus on how to mine keyphrases by making context. Our model revisited degree centrality and use of the probability distribution of articles. modified it with boundary function for modeling local context. We combine global similarity with Graph-based models are the most proposed and popular used in early works which convert the doc- our proposed boundary-aware centrality to extract ument into a graph. Inspired by (Page et al., 1999), keyphrases. Experiments on 3 public benchmarks demonstrate that our model can effectively ca"
2021.emnlp-main.14,E17-2068,0,0.0121188,"and then fine-tuned on downWe can see that all keyphrases occur at the start stream tasks. The pre-trained language model withof the document. Our model extracted many correct phrases which are the same as gold truth and ex- out fine-tuning also can provide high quality embedding of natural texts for unsupervised tasks. tracted the phrase “existing word record"" which is semantically same with “word record"" in gold truth. Different from static word embedding, such as Word2Vec (Mikolov et al., 2013), GloVe (PenningIt is worth mentioning that our model focuses on ton et al., 2014), and FastText (Joulin et al., 2017). the boundary of the document and most extracted phrases were located at the start of the document, Pre-trained language models can encode words or sentences with context dynamically and solve the which is controlled by our setting of α. This proves the effectiveness of our boundary-aware central- OOV problem. In addition, pre-trained language models can provide document-level or sentenceity. From the figure, we also can find that wrong phrases are highly relevant to topics of this docu- level embedding which contains more semantic information than Sen2Vec (Pagliardini et al., 2018) ment, whi"
2021.emnlp-main.14,S10-1004,0,0.047616,"oc+λ max(eij − θ, 0) uments from scientific journal abstracts. We foldb (i)≥db (j) low previous works (Bennani-Smires et al., 2018; Where C(HKPi ) represents the local salience of Sun et al., 2020) to use 500 test documents and candidate phrase i. the version of uncontrolled annotated keyphrases For most long documents or news articles, the as ground truth. The DUC2001 dataset (Wan and author tends to write the key information at the Xiao, 2008a) is a collection of 308 long length beginning of the document. Florescu and Caragea news articles with average 828.4 tokens. The SemEval2010 dataset (Kim et al., 2010) contains (2017a) point out that the position-biased weight can greatly improve the performance for keyphrase ACM full length papers. In our experiments, we extraction and they employ the sum of the posi- use the 100 test documents and the combined set tion’s inverse of words in the document as the of author- and reader- annotated keyphrases. weight. For example, the word appearing at We follow the common practice and evaluate the 2th, 5th and 10th, has a weight p(wi ) = 1/2 + performance of our models in terms of f-measure 1/5 + 1/10 = 0.8. Our boundary-aware centrality at the top N keyphrase"
2021.emnlp-main.14,2021.findings-acl.147,1,0.729494,"ng et al., 2020), etc. In this paper, we choose BERT, the most used, to obtain vector representation of documents and phrases by merging the embedding of tokens. 4.2 Unsupervised Keyphrase Extraction phrases by measuring the similarity between phrase embedding and document embedding. (Sun et al., 2020) proposed SIFRank, which improves the static embedding from EmbedRank with a pretrained language model. Embedding-based models just measured the similarity between document and candidate phrases and ignored the local information. To jointly model global and local context (Zheng and Lapata, 2019; Liang et al., 2021), in this paper, we revisit degree centrality, which can model local context, and convert it into boundary-aware centrality. Then, we combine global similarity and boundary-aware centrality for local salient information to rank and extract phrases. Unsupervised keyphrase extraction can be divided into four main types: statistics-based models, graph-based models, topic-based models, and embedding-based models. Statistics-based models 5 Conclusion and Future Work (Campos et al., 2018) mainly analyze an article’s probability features such as word frequency fea- In this paper, we point out that em"
2021.emnlp-main.14,A97-1042,0,0.596809,"ion from context adequately, in this paper, we proposed a novel method which jointly models the local and global context of the input document. Specifically, we calculate the similarity between candidate phrases and the whole document for modeling global context. For local context modeling, we first build a graph structure, which represents each phrase as nodes and the edges are similarity between nodes. Then, we proposed a new centrality computation method, which is based on the insight that the most important information typically occurs at the start or end of documents (document boundary) (Lin and Hovy, 1997; Teufel, 1997; Dong et al., 2021), to measure salience of local context based on the graph structure. Finally, we further combine the measure of global similarity and local salience for ranking. To evaluate the effectiveness of our method, we compare our method with recent state-of-the-art models on three public benchmarks (Inspec, DUC 2001, SemEval 2010). The results show that our model can outperform most models while generalizing better on input documents with different domains and length. It is deservedly mentioned that our models have a huge improvement on long scientific documents. 2 Me"
2021.emnlp-main.14,D09-1027,0,0.0943428,"Missing"
2021.emnlp-main.14,P04-3020,0,0.749142,"Wu2 , Mu Li2 and Zhoujun Li1† State Key Lab of Software Development Environment, Beihang University, Beijing, China 2 Tencent Cloud Xiaowei, Beijing, China {xnliang,lizj}@buaa.edu.cn; {frostwu,ethanlli}@tencent.com; Abstract universal and adaptive via extracting phrases based on information from input document itself. In this Embedding based methods are widely used paper, we focus on the unsupervised keyphrase exfor unsupervised keyphrase extraction (UKE) traction (UKE) model. tasks. Generally, these methods simply calculate similarities between phrase embeddings UKE has been widely studied (Mihalcea, 2004; and document embedding, which is insuffiWan and Xiao, 2008a; Bougouin et al., 2013; cient to capture different context for a more Boudin, 2018; Bennani-Smires et al., 2018; Sun effective UKE model. In this paper, we proet al., 2020) in the keyphrase extraction field. Repose a novel method for UKE, where local cently, with the development of text representaand global contexts are jointly modeled. From tion, embedding-based models (Bennani-Smires a global view, we calculate the similarity beet al., 2018; Sun et al., 2020) have achieved promistween a certain phrase and the whole docuing results"
2021.emnlp-main.14,W04-3252,0,0.363219,"Missing"
2021.emnlp-main.14,N18-1049,0,0.0219319,"and FastText (Joulin et al., 2017). the boundary of the document and most extracted phrases were located at the start of the document, Pre-trained language models can encode words or sentences with context dynamically and solve the which is controlled by our setting of α. This proves the effectiveness of our boundary-aware central- OOV problem. In addition, pre-trained language models can provide document-level or sentenceity. From the figure, we also can find that wrong phrases are highly relevant to topics of this docu- level embedding which contains more semantic information than Sen2Vec (Pagliardini et al., 2018) ment, which is influenced by our phrase-document or Doc2Vec (Le and Mikolov, 2014). relevance weighting. This example shows that the joint modeling of global and local context can imELMo (Peters et al., 2018) employs Bi-LSTM prove the performance of keyphrase extraction and structure and concatenate forward and backward our model really captures local and global informa- information to capture bidirectional information. 161 BERT (Devlin et al., 2019) is a bidirectional transformer structure pre-trained language model. Compared with the concatenation of bidirectional information, BERT can capt"
2021.emnlp-main.14,D14-1162,0,0.124094,"Missing"
2021.emnlp-main.14,C16-1089,0,0.0281886,"Missing"
2021.emnlp-main.14,P19-1628,0,0.0188321,"et al., 2019), XLNET (Yang et al., 2020), etc. In this paper, we choose BERT, the most used, to obtain vector representation of documents and phrases by merging the embedding of tokens. 4.2 Unsupervised Keyphrase Extraction phrases by measuring the similarity between phrase embedding and document embedding. (Sun et al., 2020) proposed SIFRank, which improves the static embedding from EmbedRank with a pretrained language model. Embedding-based models just measured the similarity between document and candidate phrases and ignored the local information. To jointly model global and local context (Zheng and Lapata, 2019; Liang et al., 2021), in this paper, we revisit degree centrality, which can model local context, and convert it into boundary-aware centrality. Then, we combine global similarity and boundary-aware centrality for local salient information to rank and extract phrases. Unsupervised keyphrase extraction can be divided into four main types: statistics-based models, graph-based models, topic-based models, and embedding-based models. Statistics-based models 5 Conclusion and Future Work (Campos et al., 2018) mainly analyze an article’s probability features such as word frequency fea- In this paper,"
2021.emnlp-main.14,N18-1202,0,0.0235346,"ally and solve the which is controlled by our setting of α. This proves the effectiveness of our boundary-aware central- OOV problem. In addition, pre-trained language models can provide document-level or sentenceity. From the figure, we also can find that wrong phrases are highly relevant to topics of this docu- level embedding which contains more semantic information than Sen2Vec (Pagliardini et al., 2018) ment, which is influenced by our phrase-document or Doc2Vec (Le and Mikolov, 2014). relevance weighting. This example shows that the joint modeling of global and local context can imELMo (Peters et al., 2018) employs Bi-LSTM prove the performance of keyphrase extraction and structure and concatenate forward and backward our model really captures local and global informa- information to capture bidirectional information. 161 BERT (Devlin et al., 2019) is a bidirectional transformer structure pre-trained language model. Compared with the concatenation of bidirectional information, BERT can capture better context information. There are also a lot of other pre-trained language models such as RoBERTa (Liu et al., 2019), XLNET (Yang et al., 2020), etc. In this paper, we choose BERT, the most used, to ob"
2021.emnlp-main.14,2020.coling-main.184,0,0.0352814,"Missing"
2021.emnlp-main.14,W97-0710,0,0.43423,"quately, in this paper, we proposed a novel method which jointly models the local and global context of the input document. Specifically, we calculate the similarity between candidate phrases and the whole document for modeling global context. For local context modeling, we first build a graph structure, which represents each phrase as nodes and the edges are similarity between nodes. Then, we proposed a new centrality computation method, which is based on the insight that the most important information typically occurs at the start or end of documents (document boundary) (Lin and Hovy, 1997; Teufel, 1997; Dong et al., 2021), to measure salience of local context based on the graph structure. Finally, we further combine the measure of global similarity and local salience for ranking. To evaluate the effectiveness of our method, we compare our method with recent state-of-the-art models on three public benchmarks (Inspec, DUC 2001, SemEval 2010). The results show that our model can outperform most models while generalizing better on input documents with different domains and length. It is deservedly mentioned that our models have a huge improvement on long scientific documents. 2 Methodology vise"
2021.emnlp-main.258,W14-4012,0,0.194244,"Missing"
2021.emnlp-main.258,D19-1223,0,0.0344893,"Missing"
2021.emnlp-main.258,D19-5622,0,0.0192277,"search/bert 10 https://github.com/Maluuba/nlg-eval et al., 2019; Voita et al., 2019; Michel et al., 2019). These studies have shown that Transformer models are over-parametrized and the self-attention models learn redundant information that can be pruned in various ways. The observations motivate lots of attempts in improvement of SAN, including 1) improving its computation efficiency and 2) completely replacing it with fixed or learnable global attention patterns. For the former thread, several studies bias attention distributions towards more local areas (Yang et al., 2018; Xu et al., 2019; Cui et al., 2019) or replace SAN with convolutional modules (Yang et al., 2019; Wu et al., 2019), which are more in line with the linguistic expectation. Xiao et al. (2019) share attention weights in adjacent layers and enable efficient re-use of hidden states in a vertical manner. On the other hand, given that most attention heads learn simple, and often positional patterns, many researchers turn to substitute instance-wise self-attention with global position-based attention patterns. Concretely, Zhang et al. (2018) use average attention models in the decoder of Transformer. You et al. (2020) model the attent"
2021.emnlp-main.258,2020.wmt-1.66,0,0.0212247,"ieved based on the index of the query without dot-product operation. However, these variants are not the ideal alternatives of selfattention due to the unsatisfactory performance. In this paper, we go further along this research line and show that self-attention is empirically re1 Introduction placeable. We propose a novel attention mechanism: Recurrent AtteNtion (RAN). Specifically, Transformer models have achieved remarkable RAN starts with an unnormalized Initial Attention success in Neural Machine Translation (NMT) Matrix for each head, which is randomly initialized (Vaswani et al., 2017; Freitag and Firat, 2020; Fan and trained together with other model parameters. et al., 2020). One of the most crucial component Then we introduce a Recurrent Transition Module, of Transformer is the dot-product multi-head selfwhich takes the Initial Attention Matrices as the attention, which is essential to learn relationships input and refines them by layer-wise interaction bebetween words as well as complex structural repretween adjacent layers. The motivation of Recurrent sentations. However, many studies have shown that Transition Module is based on the observation that the pairwise self-attention is over-parame"
2021.emnlp-main.258,I17-1004,0,0.0151809,"1.2x. Note that the previous studies of simplifying attention mechanisms (You et al., 2020; Wu et al., 2019; Michel et al., 2019) also report efficiency improvement of similar magnitudes. 4 Analysis In order to better understand RAN, we conduct comprehensive empirical studies on its behavior on the WMT14 En⇒De test set. 4.1 Distribution of RAN Weights In this experiment, we investigate the difference between the learned attention distribution of the different models. To this end, first, we follow Tang et al. (2019) to measure the concentration of attention distribution with attention entropy (Ghader and Monz, 2017): EA (xt ) = − |x| X A(xt , xi )logA(xt , xi ), (5) i=1 where xi denotes the i-th token and A(xt , xi ) represents the attention distribution at timestep t. Then, we average the attention entropy over all timesteps and then average the attention entropy over all heads in each layer. Figure 4 displays the entropy of attention distri3.3 Decoding Speedups bution. As for encoder, the attention distribution of We plot the decoding speed as functions of batch the TransF has the lowest entropy, which gets dissize and beam size in Figure 3. Each experiment is tributed first and then becomes concentrat"
2021.emnlp-main.258,D15-1166,0,0.0436978,"ail dataset and dialogue generation using the PersonaChat dataset.8 On summarization task, all of the models are trained for 300k steps on 2 GPUs with a batch size of 128 sentences. For dialog generation, we segment all dialog with BERT tokenizer9 and train a SMALL Transformer for 20K steps. We use NLG_Eval10 for evaluation and report the results in Table 4. RANs achieve competitive results compared to TransF, which demonstrates the generalization of RAN on other generation tasks. 6 Related Work The introduction of attention mechanisms into NMT can be traced back to Bahdanau et al. (2015) and Luong et al. (2015), which are used to learn soft word alignments between language pairs. Due to the significant improvements in translation quality, the attention models have become an critical component of NMT models. More recently, Vaswani et al. (2017) proposed Transformer that achieved the state-of-the-art and soon becomes the most popular NMT architecture. The Self-Attention Network (SAN), playing an important role in the Transformer, has been investigated and analyzed by a number of recent studies (Sanh et al., 2019; Correia 8 we directly use the dataset from https://github.com/PaddlePaddle/Research/tree/"
2021.emnlp-main.258,P02-1040,0,0.109334,"the setting of You et al. (2020) to replace the encoder selfattention with distributions centered around i − 1 and i + 1 and the decoder self-attention with distributions centered around i − 1 and i, and set the standard deviation as 1.0. All models are trained for 150k steps except WMT14 (250k steps) and 3 4 En-Tr (20k steps). Training is performed using 8 x V100 GPUs for all language pairs except En-Tr and Zh-En which use 2. When decoding, we use a beam width of 4 and a length penalty of 0.6 for the WMT tasks and a length penalty of 1.0 for the Zh-En task. We report the case-sensitive BLEU (Papineni et al., 2002) with Multi-bleu.perl 5 and detokenized BLEU score with SacreBLEU 6 (Post, 2018) of the best checkpoint in the validation set. 3.2 Main Result First, we leverage RAN to replace the self-attention of encoder or decoder, respectively. Table 1 shows the overall results on the 10 language pairs. Compared with TransF, our RAN models consistently yield competitive or even better results against TransF on all datasets. Concretely, 0.13/0.16, 0.48/0.44 and 0.16/0.22 more average BLEU/SacreBLEU are achieved by RAN-E, RAND and RAN-ALL, respectively. Although different languages have different linguistic"
2021.emnlp-main.258,W18-6319,0,0.0136339,"ntered around i − 1 and i + 1 and the decoder self-attention with distributions centered around i − 1 and i, and set the standard deviation as 1.0. All models are trained for 150k steps except WMT14 (250k steps) and 3 4 En-Tr (20k steps). Training is performed using 8 x V100 GPUs for all language pairs except En-Tr and Zh-En which use 2. When decoding, we use a beam width of 4 and a length penalty of 0.6 for the WMT tasks and a length penalty of 1.0 for the Zh-En task. We report the case-sensitive BLEU (Papineni et al., 2002) with Multi-bleu.perl 5 and detokenized BLEU score with SacreBLEU 6 (Post, 2018) of the best checkpoint in the validation set. 3.2 Main Result First, we leverage RAN to replace the self-attention of encoder or decoder, respectively. Table 1 shows the overall results on the 10 language pairs. Compared with TransF, our RAN models consistently yield competitive or even better results against TransF on all datasets. Concretely, 0.13/0.16, 0.48/0.44 and 0.16/0.22 more average BLEU/SacreBLEU are achieved by RAN-E, RAND and RAN-ALL, respectively. Although different languages have different linguistic and syntactic structures, RAN can learn reasonable global atten5 https://github"
2021.emnlp-main.258,2020.findings-emnlp.49,0,0.343314,"ish translation task. In addition, we conduct extensive analysis on the attention weights of RAN to confirm their reasonableness. Our RAN is a promising alternative to build more effective and efficient NMT models. et al., 2019; Michel et al., 2019) or replacing selfattention with more efficient one (Xu et al., 2019; Wu et al., 2019; Kitaev et al., 2020; Beltagy et al., 2020). More recently, several researches take this direction to an extreme by replacing dot-product selfattention with fixed or trainable global positionbased attentions (Zhang et al., 2018; Tay et al., 2020; You et al., 2020; Raganato et al., 2020). For example, You et al. (2020) roughly modeled attention weights as hard-coded Gaussian distributions, based on the observation that most heads only focus their attention on a local neighborhood. Another more representative method is Random Synthesizer proposed by Tay et al. (2020). Different from You et al. (2020), they simply treat attention weights of all heads in each layer as trainable parameters. At inference time, the attention weights are directly retrieved based on the index of the query without dot-product operation. However, these variants are not the ideal alternatives of selfatt"
2021.emnlp-main.258,R19-1136,0,0.0316434,"Missing"
2021.emnlp-main.258,P19-1580,0,0.0193726,"quality, the attention models have become an critical component of NMT models. More recently, Vaswani et al. (2017) proposed Transformer that achieved the state-of-the-art and soon becomes the most popular NMT architecture. The Self-Attention Network (SAN), playing an important role in the Transformer, has been investigated and analyzed by a number of recent studies (Sanh et al., 2019; Correia 8 we directly use the dataset from https://github.com/PaddlePaddle/Research/tree/master/NLP /Dialogue-PLATO 9 https://github.com/google-research/bert 10 https://github.com/Maluuba/nlg-eval et al., 2019; Voita et al., 2019; Michel et al., 2019). These studies have shown that Transformer models are over-parametrized and the self-attention models learn redundant information that can be pruned in various ways. The observations motivate lots of attempts in improvement of SAN, including 1) improving its computation efficiency and 2) completely replacing it with fixed or learnable global attention patterns. For the former thread, several studies bias attention distributions towards more local areas (Yang et al., 2018; Xu et al., 2019; Cui et al., 2019) or replace SAN with convolutional modules (Yang et al., 2019; Wu"
2021.emnlp-main.258,P19-1295,0,0.0593016,"dels are competitive and outperform their Transformer counterpart in certain scenarios, with fewer parameters and inference time. Particularly, when apply RAN to the decoder of Transformer, there brings consistent improvements by about +0.5 BLEU on 6 translation tasks and +1.0 BLEU on Turkish-English translation task. In addition, we conduct extensive analysis on the attention weights of RAN to confirm their reasonableness. Our RAN is a promising alternative to build more effective and efficient NMT models. et al., 2019; Michel et al., 2019) or replacing selfattention with more efficient one (Xu et al., 2019; Wu et al., 2019; Kitaev et al., 2020; Beltagy et al., 2020). More recently, several researches take this direction to an extreme by replacing dot-product selfattention with fixed or trainable global positionbased attentions (Zhang et al., 2018; Tay et al., 2020; You et al., 2020; Raganato et al., 2020). For example, You et al. (2020) roughly modeled attention weights as hard-coded Gaussian distributions, based on the observation that most heads only focus their attention on a local neighborhood. Another more representative method is Random Synthesizer proposed by Tay et al. (2020). Different"
2021.emnlp-main.258,D18-1475,0,0.0249738,"PLATO 9 https://github.com/google-research/bert 10 https://github.com/Maluuba/nlg-eval et al., 2019; Voita et al., 2019; Michel et al., 2019). These studies have shown that Transformer models are over-parametrized and the self-attention models learn redundant information that can be pruned in various ways. The observations motivate lots of attempts in improvement of SAN, including 1) improving its computation efficiency and 2) completely replacing it with fixed or learnable global attention patterns. For the former thread, several studies bias attention distributions towards more local areas (Yang et al., 2018; Xu et al., 2019; Cui et al., 2019) or replace SAN with convolutional modules (Yang et al., 2019; Wu et al., 2019), which are more in line with the linguistic expectation. Xiao et al. (2019) share attention weights in adjacent layers and enable efficient re-use of hidden states in a vertical manner. On the other hand, given that most attention heads learn simple, and often positional patterns, many researchers turn to substitute instance-wise self-attention with global position-based attention patterns. Concretely, Zhang et al. (2018) use average attention models in the decoder of Transformer"
2021.emnlp-main.258,N19-1407,0,0.0199503,"19; Voita et al., 2019; Michel et al., 2019). These studies have shown that Transformer models are over-parametrized and the self-attention models learn redundant information that can be pruned in various ways. The observations motivate lots of attempts in improvement of SAN, including 1) improving its computation efficiency and 2) completely replacing it with fixed or learnable global attention patterns. For the former thread, several studies bias attention distributions towards more local areas (Yang et al., 2018; Xu et al., 2019; Cui et al., 2019) or replace SAN with convolutional modules (Yang et al., 2019; Wu et al., 2019), which are more in line with the linguistic expectation. Xiao et al. (2019) share attention weights in adjacent layers and enable efficient re-use of hidden states in a vertical manner. On the other hand, given that most attention heads learn simple, and often positional patterns, many researchers turn to substitute instance-wise self-attention with global position-based attention patterns. Concretely, Zhang et al. (2018) use average attention models in the decoder of Transformer. You et al. (2020) model the attention distribution as hard-coded Gaussian ones, and Raganato et"
2021.emnlp-main.258,2020.acl-main.687,0,0.0467265,"Missing"
2021.emnlp-main.258,P18-1166,0,0.201207,"LEU on 6 translation tasks and +1.0 BLEU on Turkish-English translation task. In addition, we conduct extensive analysis on the attention weights of RAN to confirm their reasonableness. Our RAN is a promising alternative to build more effective and efficient NMT models. et al., 2019; Michel et al., 2019) or replacing selfattention with more efficient one (Xu et al., 2019; Wu et al., 2019; Kitaev et al., 2020; Beltagy et al., 2020). More recently, several researches take this direction to an extreme by replacing dot-product selfattention with fixed or trainable global positionbased attentions (Zhang et al., 2018; Tay et al., 2020; You et al., 2020; Raganato et al., 2020). For example, You et al. (2020) roughly modeled attention weights as hard-coded Gaussian distributions, based on the observation that most heads only focus their attention on a local neighborhood. Another more representative method is Random Synthesizer proposed by Tay et al. (2020). Different from You et al. (2020), they simply treat attention weights of all heads in each layer as trainable parameters. At inference time, the attention weights are directly retrieved based on the index of the query without dot-product operation. Howev"
2021.findings-acl.147,N19-1071,0,0.0163608,"lel datasets. Supervised summarization algorithms develop sharply (Chopra et al., 2016; Cao et al., 2018; Zhang et al., 2018; Zhong et al., 2019; Gehrmann et al., 2019; Cho et al., 2019; Jin et al., 2020b; Cao et al., 2020; Jin et al., 2020a; Zhong et al., 2020). However, high-quality parallel datasets are not always available. Researches on unsupervised summarization are necessary, which can be diveided into extractive and abstractive. Unsupervised abstractive summarization is more challenging than extractive. There are also many interesting works (Wang and Lee, 2018; F´evry and Phang, 2018; Baziotis et al., 2019; Jernite, 2019; Zhou and Rush, 2019; West et al., 2019; Chu and Liu, 2019; Yang et al., 2020) on unsupervised abstractive summarization. However, most unsupervised summarization models are extractive (Radev et al., 2000; Mihalcea and Tarau, 2004; Erkan and Radev, 2004; Carbonell and Goldstein, 1998; Wan, 2008; Wan and Yang, 2008; Schluter and Søgaard, 2015; Zhao et al., 2020) and focused on the measure of sentence salient. Graph-based models are effective and widely concerned in unsupervised extractive methods. Different from traditional undirected graph rank models (Radev et al., 2000; Mihal"
2021.findings-acl.147,2020.acl-main.554,0,0.0203501,"acSum all focus on the facet which describes terroristic attacks in Iraq. However, FAR can cover all 3 facets in gold reference. This shows that our FAR can effectively improve the performance by reducing the facet bias problem. 6 Related Work Summarization is a long-standing challenge for researchers to address. Thanks to the power of the neural network and availability of large-scale parallel datasets. Supervised summarization algorithms develop sharply (Chopra et al., 2016; Cao et al., 2018; Zhang et al., 2018; Zhong et al., 2019; Gehrmann et al., 2019; Cho et al., 2019; Jin et al., 2020b; Cao et al., 2020; Jin et al., 2020a; Zhong et al., 2020). However, high-quality parallel datasets are not always available. Researches on unsupervised summarization are necessary, which can be diveided into extractive and abstractive. Unsupervised abstractive summarization is more challenging than extractive. There are also many interesting works (Wang and Lee, 2018; F´evry and Phang, 2018; Baziotis et al., 2019; Jernite, 2019; Zhou and Rush, 2019; West et al., 2019; Chu and Liu, 2019; Yang et al., 2020) on unsupervised abstractive summarization. However, most unsupervised summarization models are extractive"
2021.findings-acl.147,P16-1046,0,0.0229968,". Extensive analyses confirm that the performance gains come from alleviating the facet bias problem. 1 Figure 1: Examples from New York Times. We selected part of key sentences from the source document to show in this table. “...” refers to the omissions of context sentences due to space limitation. Introduction Document summarization is the task of transforming a long document into a shorter version while retaining its most important content (Nenkova and McKeown, 2011).Existing extractive or abstractive methods are mostly in supervised fashion which rely on large amounts of labeled corpora (Cheng and Lapata, 2016; Nallapati et al., 2017; Gehrmann et al., 2018; Liu and Lapata, 2019a,b; Zhang et al., 2019; Wang et al., 2020). However, this is not available for different summarization styles, domains, and languages. Fortunately, recent work has shown successful practices on unsupervised * Contribution † during internship at Tencent Inc. Corresponding Author extractive summarization (Radev et al., 2000; Mihalcea and Tarau, 2004; Erkan and Radev, 2004; Schluter and Søgaard, 2015; Tixier et al., 2017; Zheng and Lapata, 2019; Xu et al., 2020; Dong et al., 2020). Compare with supervised ones, unsupervised met"
2021.findings-acl.147,P19-1098,0,0.0186627,"can see that sentences extracted by PacSum all focus on the facet which describes terroristic attacks in Iraq. However, FAR can cover all 3 facets in gold reference. This shows that our FAR can effectively improve the performance by reducing the facet bias problem. 6 Related Work Summarization is a long-standing challenge for researchers to address. Thanks to the power of the neural network and availability of large-scale parallel datasets. Supervised summarization algorithms develop sharply (Chopra et al., 2016; Cao et al., 2018; Zhang et al., 2018; Zhong et al., 2019; Gehrmann et al., 2019; Cho et al., 2019; Jin et al., 2020b; Cao et al., 2020; Jin et al., 2020a; Zhong et al., 2020). However, high-quality parallel datasets are not always available. Researches on unsupervised summarization are necessary, which can be diveided into extractive and abstractive. Unsupervised abstractive summarization is more challenging than extractive. There are also many interesting works (Wang and Lee, 2018; F´evry and Phang, 2018; Baziotis et al., 2019; Jernite, 2019; Zhou and Rush, 2019; West et al., 2019; Chu and Liu, 2019; Yang et al., 2020) on unsupervised abstractive summarization. However, most unsupervised"
2021.findings-acl.147,N16-1012,0,0.0407289,"from a news report and only used to analyze the effectiveness of our model.) As shown in Table 5, we can see that sentences extracted by PacSum all focus on the facet which describes terroristic attacks in Iraq. However, FAR can cover all 3 facets in gold reference. This shows that our FAR can effectively improve the performance by reducing the facet bias problem. 6 Related Work Summarization is a long-standing challenge for researchers to address. Thanks to the power of the neural network and availability of large-scale parallel datasets. Supervised summarization algorithms develop sharply (Chopra et al., 2016; Cao et al., 2018; Zhang et al., 2018; Zhong et al., 2019; Gehrmann et al., 2019; Cho et al., 2019; Jin et al., 2020b; Cao et al., 2020; Jin et al., 2020a; Zhong et al., 2020). However, high-quality parallel datasets are not always available. Researches on unsupervised summarization are necessary, which can be diveided into extractive and abstractive. Unsupervised abstractive summarization is more challenging than extractive. There are also many interesting works (Wang and Lee, 2018; F´evry and Phang, 2018; Baziotis et al., 2019; Jernite, 2019; Zhou and Rush, 2019; West et al., 2019; Chu and"
2021.findings-acl.147,N18-2097,0,0.11448,"are written by library scientists. Different from CNNDM, salient sentences distribute evenly in an article (Durrett et al., 2016). We filter out documents whose length of summaries are shorter than 50 tokens (Zheng and Lapata, 2019). MultiNews dataset consists of news articles and human-written summaries. The dataset is the first large-scale Multi-Documents Summarization (MDS) news dataset and comes from a diverse set of news sources (over 1500 sites) (Fabbri et al., 2019). arXiv&PubMed datasets are two long document datasets of scientific publications from arXiv.org (113k) and PubMed (215k) (Cohan et al., 2018). The task is to generate the abstract from the paper body. WikiSum dataset is a multi-documents summarization dataset from Wikipedia (Liu et al., 2018). We use the version provided by (Liu and Lapata, 2019a), which selects ranked top-40 paragraphs as input. For this dataset, we filter out documents whose summary length is less than 100 tokens. After the process, WikiSum test set contains 15,795 examples and the average length of summaries is 198. WikiHow dataset is a large-scale dataset of instructions from the online WikiHow.com website (Koupaee and Wang, 2018). The task is to generate the c"
2021.findings-acl.147,P16-1188,0,0.0183875,"Experiments Datasets We introduce the datasets used in our experiments in this section. CNN/DM dataset contains 93k articles from CNN, and 220k articles from Daily Mail newspapers (Hermann et al., 2015). We use the nonanonymous version. Following (Zheng and Lapata, 2019), documents whose length of summaries are shorter than 30 tokens are filtered out. NYT dataset contains articles published by the New York Times between January 1, 1987 and June 19, 2007 (Li et al., 2016). The summaries are written by library scientists. Different from CNNDM, salient sentences distribute evenly in an article (Durrett et al., 2016). We filter out documents whose length of summaries are shorter than 50 tokens (Zheng and Lapata, 2019). MultiNews dataset consists of news articles and human-written summaries. The dataset is the first large-scale Multi-Documents Summarization (MDS) news dataset and comes from a diverse set of news sources (over 1500 sites) (Fabbri et al., 2019). arXiv&PubMed datasets are two long document datasets of scientific publications from arXiv.org (113k) and PubMed (215k) (Cohan et al., 2018). The task is to generate the abstract from the paper body. WikiSum dataset is a multi-documents summarization"
2021.findings-acl.147,P19-1102,0,0.0121204,"out. NYT dataset contains articles published by the New York Times between January 1, 1987 and June 19, 2007 (Li et al., 2016). The summaries are written by library scientists. Different from CNNDM, salient sentences distribute evenly in an article (Durrett et al., 2016). We filter out documents whose length of summaries are shorter than 50 tokens (Zheng and Lapata, 2019). MultiNews dataset consists of news articles and human-written summaries. The dataset is the first large-scale Multi-Documents Summarization (MDS) news dataset and comes from a diverse set of news sources (over 1500 sites) (Fabbri et al., 2019). arXiv&PubMed datasets are two long document datasets of scientific publications from arXiv.org (113k) and PubMed (215k) (Cohan et al., 2018). The task is to generate the abstract from the paper body. WikiSum dataset is a multi-documents summarization dataset from Wikipedia (Liu et al., 2018). We use the version provided by (Liu and Lapata, 2019a), which selects ranked top-40 paragraphs as input. For this dataset, we filter out documents whose summary length is less than 100 tokens. After the process, WikiSum test set contains 15,795 examples and the average length of summaries is 198. WikiHo"
2021.findings-acl.147,K18-1040,0,0.0455017,"Missing"
2021.findings-acl.147,D18-1443,0,0.0120095,"e gains come from alleviating the facet bias problem. 1 Figure 1: Examples from New York Times. We selected part of key sentences from the source document to show in this table. “...” refers to the omissions of context sentences due to space limitation. Introduction Document summarization is the task of transforming a long document into a shorter version while retaining its most important content (Nenkova and McKeown, 2011).Existing extractive or abstractive methods are mostly in supervised fashion which rely on large amounts of labeled corpora (Cheng and Lapata, 2016; Nallapati et al., 2017; Gehrmann et al., 2018; Liu and Lapata, 2019a,b; Zhang et al., 2019; Wang et al., 2020). However, this is not available for different summarization styles, domains, and languages. Fortunately, recent work has shown successful practices on unsupervised * Contribution † during internship at Tencent Inc. Corresponding Author extractive summarization (Radev et al., 2000; Mihalcea and Tarau, 2004; Erkan and Radev, 2004; Schluter and Søgaard, 2015; Tixier et al., 2017; Zheng and Lapata, 2019; Xu et al., 2020; Dong et al., 2020). Compare with supervised ones, unsupervised methods 1). remove the dependency on large-scale a"
2021.findings-acl.147,W19-8665,0,0.0138412,"s shown in Table 5, we can see that sentences extracted by PacSum all focus on the facet which describes terroristic attacks in Iraq. However, FAR can cover all 3 facets in gold reference. This shows that our FAR can effectively improve the performance by reducing the facet bias problem. 6 Related Work Summarization is a long-standing challenge for researchers to address. Thanks to the power of the neural network and availability of large-scale parallel datasets. Supervised summarization algorithms develop sharply (Chopra et al., 2016; Cao et al., 2018; Zhang et al., 2018; Zhong et al., 2019; Gehrmann et al., 2019; Cho et al., 2019; Jin et al., 2020b; Cao et al., 2020; Jin et al., 2020a; Zhong et al., 2020). However, high-quality parallel datasets are not always available. Researches on unsupervised summarization are necessary, which can be diveided into extractive and abstractive. Unsupervised abstractive summarization is more challenging than extractive. There are also many interesting works (Wang and Lee, 2018; F´evry and Phang, 2018; Baziotis et al., 2019; Jernite, 2019; Zhou and Rush, 2019; West et al., 2019; Chu and Liu, 2019; Yang et al., 2020) on unsupervised abstractive summarization. However,"
2021.findings-acl.147,D19-5406,0,0.012944,"d Lapata, 2019a), which selects ranked top-40 paragraphs as input. For this dataset, we filter out documents whose summary length is less than 100 tokens. After the process, WikiSum test set contains 15,795 examples and the average length of summaries is 198. WikiHow dataset is a large-scale dataset of instructions from the online WikiHow.com website (Koupaee and Wang, 2018). The task is to generate the concatenated summary-sentences from the paragraphs. BillSum dataset contains US Congressional bills and human-written reference summaries from the 103rd-115th (1993-2018) sessions of Congress (Kornilova and Eidelman, 2019). These datasets differ in scale, domain and task type. We collect details of the 8 corpus in Table 1. 4.2 Implementation Details and Metrics FAR has 4 hyper-parameters and the best set of them are chosen from the following setting: α ∈ {1, 2}, β ∈ {0.0, 0.1, . . . , 0.9}, λ1 + λ2 = 1, λ1 ∈ {0.0, 0.1, . . . , 1.0}. In most case, FAR with the default setting (α = 1, β = 0.5, λ1 = 0.5, λ2 = 0.5) can achieve satisfied performance on all datasets. We select best hyper-parameters by sampling 1,000 1688 Datasets Sources Type CNN/DM NYT MultiNews arXiv PubMed WikiSum WikiHow BillSum News News News Sc"
2021.findings-acl.147,W16-3617,0,0.0128032,"vector of “[CLS]” is used as sentence representations and we set µ to 1 following (Reimers and Gurevych, 2019) in post-training phase. 4 4.1 Experiments Datasets We introduce the datasets used in our experiments in this section. CNN/DM dataset contains 93k articles from CNN, and 220k articles from Daily Mail newspapers (Hermann et al., 2015). We use the nonanonymous version. Following (Zheng and Lapata, 2019), documents whose length of summaries are shorter than 30 tokens are filtered out. NYT dataset contains articles published by the New York Times between January 1, 1987 and June 19, 2007 (Li et al., 2016). The summaries are written by library scientists. Different from CNNDM, salient sentences distribute evenly in an article (Durrett et al., 2016). We filter out documents whose length of summaries are shorter than 50 tokens (Zheng and Lapata, 2019). MultiNews dataset consists of news articles and human-written summaries. The dataset is the first large-scale Multi-Documents Summarization (MDS) news dataset and comes from a diverse set of news sources (over 1500 sites) (Fabbri et al., 2019). arXiv&PubMed datasets are two long document datasets of scientific publications from arXiv.org (113k) and"
2021.findings-acl.147,N03-1020,0,0.320371,"Missing"
2021.findings-acl.147,P19-1500,0,0.272245,"iating the facet bias problem. 1 Figure 1: Examples from New York Times. We selected part of key sentences from the source document to show in this table. “...” refers to the omissions of context sentences due to space limitation. Introduction Document summarization is the task of transforming a long document into a shorter version while retaining its most important content (Nenkova and McKeown, 2011).Existing extractive or abstractive methods are mostly in supervised fashion which rely on large amounts of labeled corpora (Cheng and Lapata, 2016; Nallapati et al., 2017; Gehrmann et al., 2018; Liu and Lapata, 2019a,b; Zhang et al., 2019; Wang et al., 2020). However, this is not available for different summarization styles, domains, and languages. Fortunately, recent work has shown successful practices on unsupervised * Contribution † during internship at Tencent Inc. Corresponding Author extractive summarization (Radev et al., 2000; Mihalcea and Tarau, 2004; Erkan and Radev, 2004; Schluter and Søgaard, 2015; Tixier et al., 2017; Zheng and Lapata, 2019; Xu et al., 2020; Dong et al., 2020). Compare with supervised ones, unsupervised methods 1). remove the dependency on large-scale annotated document-summ"
2021.findings-acl.147,D19-1387,0,0.213859,"iating the facet bias problem. 1 Figure 1: Examples from New York Times. We selected part of key sentences from the source document to show in this table. “...” refers to the omissions of context sentences due to space limitation. Introduction Document summarization is the task of transforming a long document into a shorter version while retaining its most important content (Nenkova and McKeown, 2011).Existing extractive or abstractive methods are mostly in supervised fashion which rely on large amounts of labeled corpora (Cheng and Lapata, 2016; Nallapati et al., 2017; Gehrmann et al., 2018; Liu and Lapata, 2019a,b; Zhang et al., 2019; Wang et al., 2020). However, this is not available for different summarization styles, domains, and languages. Fortunately, recent work has shown successful practices on unsupervised * Contribution † during internship at Tencent Inc. Corresponding Author extractive summarization (Radev et al., 2000; Mihalcea and Tarau, 2004; Erkan and Radev, 2004; Schluter and Søgaard, 2015; Tixier et al., 2017; Zheng and Lapata, 2019; Xu et al., 2020; Dong et al., 2020). Compare with supervised ones, unsupervised methods 1). remove the dependency on large-scale annotated document-summ"
2021.findings-acl.147,W04-3252,0,0.671202,"ing its most important content (Nenkova and McKeown, 2011).Existing extractive or abstractive methods are mostly in supervised fashion which rely on large amounts of labeled corpora (Cheng and Lapata, 2016; Nallapati et al., 2017; Gehrmann et al., 2018; Liu and Lapata, 2019a,b; Zhang et al., 2019; Wang et al., 2020). However, this is not available for different summarization styles, domains, and languages. Fortunately, recent work has shown successful practices on unsupervised * Contribution † during internship at Tencent Inc. Corresponding Author extractive summarization (Radev et al., 2000; Mihalcea and Tarau, 2004; Erkan and Radev, 2004; Schluter and Søgaard, 2015; Tixier et al., 2017; Zheng and Lapata, 2019; Xu et al., 2020; Dong et al., 2020). Compare with supervised ones, unsupervised methods 1). remove the dependency on large-scale annotated document-summary pairs; 2). are more general for various scenarios. Graph-based models are commonly used in unsupervised extractive methods (Radev et al., 2000; Mihalcea and Tarau, 2004; Erkan and Radev, 2004). For example, Zheng and Lapata (2019) proposed a directed centrality-based method named PacSum 1685 Findings of the Association for Computational Linguis"
2021.findings-acl.147,N18-1158,0,0.0118299,"Lead selects the first k tokens as a summary. We also report previous best centrality-based model PacSum (Zheng and Lapata, 2019) in the third block of each table. Overall, FAR outperforms above-mentioned unsupervised strong baselines on most datasets, especially on long-document and multi-documents datasets and is more generalized than them for differnt types, domains datasets. Results on SDS Table 2 reports the results on single document summarization (SDS) datasets CNN/DM, NYT and WikiHow. PTR-GEN (See et al., 2017) is a supervised abstractive model with classic seq2seq structure. REFRESH (Narayan et al., 2018) and BertExt (Liu and Lapata, 2019b) are supervised extractive models. STAS (Xu et al., 2020) is the best unsupervised model on CNN/DM 1689 Method Oracle PTR-GEN Discourse-aware SummaRuNNer GlobalLocalCont Lead TextRank LexRank MMR PacSum FAR R-1 53.88 32.06 35.80 42.81 43.62 33.66 24.38 33.85 29.75 39.33 40.92 arXiv R-2 23.05 9.04 11.05 16.52 17.36 8.94 10.57 10.73 6.14 12.19 13.75 R-L 34.9 25.16 31.8 28.23 29.14 22.19 22.18 28.99 26.41 34.18 35.56 R-1 55.05 35.86 38.93 43.89 44.85 35.63 38.66 39.19 37.65 39.79 41.98 PubMed R-2 27.48 10.22 15.37 18.78 19.70 12.28 15.87 13.89 10.61 14.00 15.66"
2021.findings-acl.147,P11-5003,0,0.0366053,"method consistently outperforms strong baselines especially in longand multi-document scenarios and even performs comparably to some supervised models. Extensive analyses confirm that the performance gains come from alleviating the facet bias problem. 1 Figure 1: Examples from New York Times. We selected part of key sentences from the source document to show in this table. “...” refers to the omissions of context sentences due to space limitation. Introduction Document summarization is the task of transforming a long document into a shorter version while retaining its most important content (Nenkova and McKeown, 2011).Existing extractive or abstractive methods are mostly in supervised fashion which rely on large amounts of labeled corpora (Cheng and Lapata, 2016; Nallapati et al., 2017; Gehrmann et al., 2018; Liu and Lapata, 2019a,b; Zhang et al., 2019; Wang et al., 2020). However, this is not available for different summarization styles, domains, and languages. Fortunately, recent work has shown successful practices on unsupervised * Contribution † during internship at Tencent Inc. Corresponding Author extractive summarization (Radev et al., 2000; Mihalcea and Tarau, 2004; Erkan and Radev, 2004; Schluter"
2021.findings-acl.147,W00-0403,0,0.418697,"version while retaining its most important content (Nenkova and McKeown, 2011).Existing extractive or abstractive methods are mostly in supervised fashion which rely on large amounts of labeled corpora (Cheng and Lapata, 2016; Nallapati et al., 2017; Gehrmann et al., 2018; Liu and Lapata, 2019a,b; Zhang et al., 2019; Wang et al., 2020). However, this is not available for different summarization styles, domains, and languages. Fortunately, recent work has shown successful practices on unsupervised * Contribution † during internship at Tencent Inc. Corresponding Author extractive summarization (Radev et al., 2000; Mihalcea and Tarau, 2004; Erkan and Radev, 2004; Schluter and Søgaard, 2015; Tixier et al., 2017; Zheng and Lapata, 2019; Xu et al., 2020; Dong et al., 2020). Compare with supervised ones, unsupervised methods 1). remove the dependency on large-scale annotated document-summary pairs; 2). are more general for various scenarios. Graph-based models are commonly used in unsupervised extractive methods (Radev et al., 2000; Mihalcea and Tarau, 2004; Erkan and Radev, 2004). For example, Zheng and Lapata (2019) proposed a directed centrality-based method named PacSum 1685 Findings of the Association"
2021.findings-acl.147,D19-1410,0,0.019753,"g model. The previous study shows that improving the quality of sentence representations helps improve the ranking performance (Zheng and Lapata, 2019; Dong et al., 2020). We post-train BERT on a sentence-level task constructed based on the corpus of a specific task. The idea is that its representation is affected not only by the words in it, but also the sentences around it. For a sentence in a document, we take its previous sentence and its following sentence to be positive examples and random sample sentences from documents as negative examples. The objective function follows that used in (Reimers and Gurevych, 2019). Specifically, for sentence si , a positive sentence sj , and a negative sentence sk , the BERT is trained to minimize the following equation: max(k vi − vj k − k vi − vk k +µ, 0) (7) where v is the sentence representation, and µ is margin which ensures that vj is at least µ closer to si than sk . The hidden state vector of “[CLS]” is used as sentence representations and we set µ to 1 following (Reimers and Gurevych, 2019) in post-training phase. 4 4.1 Experiments Datasets We introduce the datasets used in our experiments in this section. CNN/DM dataset contains 93k articles from CNN, and 220"
2021.findings-acl.147,P15-2138,0,0.144104,"wn, 2011).Existing extractive or abstractive methods are mostly in supervised fashion which rely on large amounts of labeled corpora (Cheng and Lapata, 2016; Nallapati et al., 2017; Gehrmann et al., 2018; Liu and Lapata, 2019a,b; Zhang et al., 2019; Wang et al., 2020). However, this is not available for different summarization styles, domains, and languages. Fortunately, recent work has shown successful practices on unsupervised * Contribution † during internship at Tencent Inc. Corresponding Author extractive summarization (Radev et al., 2000; Mihalcea and Tarau, 2004; Erkan and Radev, 2004; Schluter and Søgaard, 2015; Tixier et al., 2017; Zheng and Lapata, 2019; Xu et al., 2020; Dong et al., 2020). Compare with supervised ones, unsupervised methods 1). remove the dependency on large-scale annotated document-summary pairs; 2). are more general for various scenarios. Graph-based models are commonly used in unsupervised extractive methods (Radev et al., 2000; Mihalcea and Tarau, 2004; Erkan and Radev, 2004). For example, Zheng and Lapata (2019) proposed a directed centrality-based method named PacSum 1685 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1685–1697 August 1–6,"
2021.findings-acl.147,P17-1099,0,0.0363626,"kan and Radev, 2004), MMR (Carbonell and Goldstein, 1998) in the second block of each table. Lead selects the first k tokens as a summary. We also report previous best centrality-based model PacSum (Zheng and Lapata, 2019) in the third block of each table. Overall, FAR outperforms above-mentioned unsupervised strong baselines on most datasets, especially on long-document and multi-documents datasets and is more generalized than them for differnt types, domains datasets. Results on SDS Table 2 reports the results on single document summarization (SDS) datasets CNN/DM, NYT and WikiHow. PTR-GEN (See et al., 2017) is a supervised abstractive model with classic seq2seq structure. REFRESH (Narayan et al., 2018) and BertExt (Liu and Lapata, 2019b) are supervised extractive models. STAS (Xu et al., 2020) is the best unsupervised model on CNN/DM 1689 Method Oracle PTR-GEN Discourse-aware SummaRuNNer GlobalLocalCont Lead TextRank LexRank MMR PacSum FAR R-1 53.88 32.06 35.80 42.81 43.62 33.66 24.38 33.85 29.75 39.33 40.92 arXiv R-2 23.05 9.04 11.05 16.52 17.36 8.94 10.57 10.73 6.14 12.19 13.75 R-L 34.9 25.16 31.8 28.23 29.14 22.19 22.18 28.99 26.41 34.18 35.56 R-1 55.05 35.86 38.93 43.89 44.85 35.63 38.66 39."
2021.findings-acl.147,W17-4507,0,0.0145455,"e or abstractive methods are mostly in supervised fashion which rely on large amounts of labeled corpora (Cheng and Lapata, 2016; Nallapati et al., 2017; Gehrmann et al., 2018; Liu and Lapata, 2019a,b; Zhang et al., 2019; Wang et al., 2020). However, this is not available for different summarization styles, domains, and languages. Fortunately, recent work has shown successful practices on unsupervised * Contribution † during internship at Tencent Inc. Corresponding Author extractive summarization (Radev et al., 2000; Mihalcea and Tarau, 2004; Erkan and Radev, 2004; Schluter and Søgaard, 2015; Tixier et al., 2017; Zheng and Lapata, 2019; Xu et al., 2020; Dong et al., 2020). Compare with supervised ones, unsupervised methods 1). remove the dependency on large-scale annotated document-summary pairs; 2). are more general for various scenarios. Graph-based models are commonly used in unsupervised extractive methods (Radev et al., 2000; Mihalcea and Tarau, 2004; Erkan and Radev, 2004). For example, Zheng and Lapata (2019) proposed a directed centrality-based method named PacSum 1685 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1685–1697 August 1–6, 2021. ©2021 Associati"
2021.findings-acl.147,D08-1079,0,0.0509865,"lable. Researches on unsupervised summarization are necessary, which can be diveided into extractive and abstractive. Unsupervised abstractive summarization is more challenging than extractive. There are also many interesting works (Wang and Lee, 2018; F´evry and Phang, 2018; Baziotis et al., 2019; Jernite, 2019; Zhou and Rush, 2019; West et al., 2019; Chu and Liu, 2019; Yang et al., 2020) on unsupervised abstractive summarization. However, most unsupervised summarization models are extractive (Radev et al., 2000; Mihalcea and Tarau, 2004; Erkan and Radev, 2004; Carbonell and Goldstein, 1998; Wan, 2008; Wan and Yang, 2008; Schluter and Søgaard, 2015; Zhao et al., 2020) and focused on the measure of sentence salient. Graph-based models are effective and widely concerned in unsupervised extractive methods. Different from traditional undirected graph rank models (Radev et al., 2000; Mihalcea and Tarau, 2004; Erkan and Radev, 2004), (Zheng and Lapata, 2019) proposed directed centrality method, which is based on the Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) assumption. (Dong et al., 2020) point out that PACSUM has position bias, which makes PACSUM not suitable for long document"
2021.findings-acl.147,2020.acl-main.553,0,0.0224673,"amples from New York Times. We selected part of key sentences from the source document to show in this table. “...” refers to the omissions of context sentences due to space limitation. Introduction Document summarization is the task of transforming a long document into a shorter version while retaining its most important content (Nenkova and McKeown, 2011).Existing extractive or abstractive methods are mostly in supervised fashion which rely on large amounts of labeled corpora (Cheng and Lapata, 2016; Nallapati et al., 2017; Gehrmann et al., 2018; Liu and Lapata, 2019a,b; Zhang et al., 2019; Wang et al., 2020). However, this is not available for different summarization styles, domains, and languages. Fortunately, recent work has shown successful practices on unsupervised * Contribution † during internship at Tencent Inc. Corresponding Author extractive summarization (Radev et al., 2000; Mihalcea and Tarau, 2004; Erkan and Radev, 2004; Schluter and Søgaard, 2015; Tixier et al., 2017; Zheng and Lapata, 2019; Xu et al., 2020; Dong et al., 2020). Compare with supervised ones, unsupervised methods 1). remove the dependency on large-scale annotated document-summary pairs; 2). are more general for various"
2021.findings-acl.147,D18-1451,0,0.0187379,"etwork and availability of large-scale parallel datasets. Supervised summarization algorithms develop sharply (Chopra et al., 2016; Cao et al., 2018; Zhang et al., 2018; Zhong et al., 2019; Gehrmann et al., 2019; Cho et al., 2019; Jin et al., 2020b; Cao et al., 2020; Jin et al., 2020a; Zhong et al., 2020). However, high-quality parallel datasets are not always available. Researches on unsupervised summarization are necessary, which can be diveided into extractive and abstractive. Unsupervised abstractive summarization is more challenging than extractive. There are also many interesting works (Wang and Lee, 2018; F´evry and Phang, 2018; Baziotis et al., 2019; Jernite, 2019; Zhou and Rush, 2019; West et al., 2019; Chu and Liu, 2019; Yang et al., 2020) on unsupervised abstractive summarization. However, most unsupervised summarization models are extractive (Radev et al., 2000; Mihalcea and Tarau, 2004; Erkan and Radev, 2004; Carbonell and Goldstein, 1998; Wan, 2008; Wan and Yang, 2008; Schluter and Søgaard, 2015; Zhao et al., 2020) and focused on the measure of sentence salient. Graph-based models are effective and widely concerned in unsupervised extractive methods. Different from traditional undirect"
2021.findings-acl.147,D19-1389,0,0.012644,"harply (Chopra et al., 2016; Cao et al., 2018; Zhang et al., 2018; Zhong et al., 2019; Gehrmann et al., 2019; Cho et al., 2019; Jin et al., 2020b; Cao et al., 2020; Jin et al., 2020a; Zhong et al., 2020). However, high-quality parallel datasets are not always available. Researches on unsupervised summarization are necessary, which can be diveided into extractive and abstractive. Unsupervised abstractive summarization is more challenging than extractive. There are also many interesting works (Wang and Lee, 2018; F´evry and Phang, 2018; Baziotis et al., 2019; Jernite, 2019; Zhou and Rush, 2019; West et al., 2019; Chu and Liu, 2019; Yang et al., 2020) on unsupervised abstractive summarization. However, most unsupervised summarization models are extractive (Radev et al., 2000; Mihalcea and Tarau, 2004; Erkan and Radev, 2004; Carbonell and Goldstein, 1998; Wan, 2008; Wan and Yang, 2008; Schluter and Søgaard, 2015; Zhao et al., 2020) and focused on the measure of sentence salient. Graph-based models are effective and widely concerned in unsupervised extractive methods. Different from traditional undirected graph rank models (Radev et al., 2000; Mihalcea and Tarau, 2004; Erkan and Radev, 2004), (Zheng and"
2021.findings-acl.147,D19-1298,0,0.0169793,"ts. 2) Especially on NYT, our model outperforms the previous best unsupervised extractive system STAS and supervised method REFERSH. After we re-implement the trigram blocking trick (i.e., removing sentences with repeating trigrams to existing summary sentences) which STAS used (Xu et al., 2020), FAR can achieve a better ROUGE-1 score 40.93/17.80/37.00 than STAS on CNN/DM. Results on LDS Table 3 reports the results on long document summarization (LDS) datasets arXiv, PubMed and BillSum. For supervised extractive models, we compare with SummaRuNNer (Nallapati et al., 2017) and GlobalLocalCont (Xiao and Carenini, 2019). We also compare with supervised abstractive models Discourse-aware (Cohan et al., 2018) and PRT-GEN. As shown in Table 3, our model has obviously higher ROUGE-1/2/L score (+1.89 +1.56 +1.38) on arXiv and (+2.22 +1.55 +1.45) on PubMed than PacSum. Compare with supervised models, our unsupervised model outperforms supervised abstractive models PTR-GEN and Discourse-aware, but still have a gap with supervised extractive models. The reason for this gap is that supervised extractive models can extract sentences with dynamic length through training with labeled corpus, but unsupervised models need"
2021.findings-acl.147,2020.findings-emnlp.161,0,0.27012,"ised fashion which rely on large amounts of labeled corpora (Cheng and Lapata, 2016; Nallapati et al., 2017; Gehrmann et al., 2018; Liu and Lapata, 2019a,b; Zhang et al., 2019; Wang et al., 2020). However, this is not available for different summarization styles, domains, and languages. Fortunately, recent work has shown successful practices on unsupervised * Contribution † during internship at Tencent Inc. Corresponding Author extractive summarization (Radev et al., 2000; Mihalcea and Tarau, 2004; Erkan and Radev, 2004; Schluter and Søgaard, 2015; Tixier et al., 2017; Zheng and Lapata, 2019; Xu et al., 2020; Dong et al., 2020). Compare with supervised ones, unsupervised methods 1). remove the dependency on large-scale annotated document-summary pairs; 2). are more general for various scenarios. Graph-based models are commonly used in unsupervised extractive methods (Radev et al., 2000; Mihalcea and Tarau, 2004; Erkan and Radev, 2004). For example, Zheng and Lapata (2019) proposed a directed centrality-based method named PacSum 1685 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1685–1697 August 1–6, 2021. ©2021 Association for Computational Linguistics Figure 2"
2021.findings-acl.147,2020.findings-emnlp.168,0,0.0211582,"., 2018; Zhang et al., 2018; Zhong et al., 2019; Gehrmann et al., 2019; Cho et al., 2019; Jin et al., 2020b; Cao et al., 2020; Jin et al., 2020a; Zhong et al., 2020). However, high-quality parallel datasets are not always available. Researches on unsupervised summarization are necessary, which can be diveided into extractive and abstractive. Unsupervised abstractive summarization is more challenging than extractive. There are also many interesting works (Wang and Lee, 2018; F´evry and Phang, 2018; Baziotis et al., 2019; Jernite, 2019; Zhou and Rush, 2019; West et al., 2019; Chu and Liu, 2019; Yang et al., 2020) on unsupervised abstractive summarization. However, most unsupervised summarization models are extractive (Radev et al., 2000; Mihalcea and Tarau, 2004; Erkan and Radev, 2004; Carbonell and Goldstein, 1998; Wan, 2008; Wan and Yang, 2008; Schluter and Søgaard, 2015; Zhao et al., 2020) and focused on the measure of sentence salient. Graph-based models are effective and widely concerned in unsupervised extractive methods. Different from traditional undirected graph rank models (Radev et al., 2000; Mihalcea and Tarau, 2004; Erkan and Radev, 2004), (Zheng and Lapata, 2019) proposed directed centra"
2021.findings-acl.147,D18-1088,0,0.0177477,"alyze the effectiveness of our model.) As shown in Table 5, we can see that sentences extracted by PacSum all focus on the facet which describes terroristic attacks in Iraq. However, FAR can cover all 3 facets in gold reference. This shows that our FAR can effectively improve the performance by reducing the facet bias problem. 6 Related Work Summarization is a long-standing challenge for researchers to address. Thanks to the power of the neural network and availability of large-scale parallel datasets. Supervised summarization algorithms develop sharply (Chopra et al., 2016; Cao et al., 2018; Zhang et al., 2018; Zhong et al., 2019; Gehrmann et al., 2019; Cho et al., 2019; Jin et al., 2020b; Cao et al., 2020; Jin et al., 2020a; Zhong et al., 2020). However, high-quality parallel datasets are not always available. Researches on unsupervised summarization are necessary, which can be diveided into extractive and abstractive. Unsupervised abstractive summarization is more challenging than extractive. There are also many interesting works (Wang and Lee, 2018; F´evry and Phang, 2018; Baziotis et al., 2019; Jernite, 2019; Zhou and Rush, 2019; West et al., 2019; Chu and Liu, 2019; Yang et al., 2020) on unsup"
2021.findings-acl.147,P19-1499,0,0.0143095,"blem. 1 Figure 1: Examples from New York Times. We selected part of key sentences from the source document to show in this table. “...” refers to the omissions of context sentences due to space limitation. Introduction Document summarization is the task of transforming a long document into a shorter version while retaining its most important content (Nenkova and McKeown, 2011).Existing extractive or abstractive methods are mostly in supervised fashion which rely on large amounts of labeled corpora (Cheng and Lapata, 2016; Nallapati et al., 2017; Gehrmann et al., 2018; Liu and Lapata, 2019a,b; Zhang et al., 2019; Wang et al., 2020). However, this is not available for different summarization styles, domains, and languages. Fortunately, recent work has shown successful practices on unsupervised * Contribution † during internship at Tencent Inc. Corresponding Author extractive summarization (Radev et al., 2000; Mihalcea and Tarau, 2004; Erkan and Radev, 2004; Schluter and Søgaard, 2015; Tixier et al., 2017; Zheng and Lapata, 2019; Xu et al., 2020; Dong et al., 2020). Compare with supervised ones, unsupervised methods 1). remove the dependency on large-scale annotated document-summary pairs; 2). are more"
2021.findings-acl.147,P19-1628,0,0.280163,"ods are mostly in supervised fashion which rely on large amounts of labeled corpora (Cheng and Lapata, 2016; Nallapati et al., 2017; Gehrmann et al., 2018; Liu and Lapata, 2019a,b; Zhang et al., 2019; Wang et al., 2020). However, this is not available for different summarization styles, domains, and languages. Fortunately, recent work has shown successful practices on unsupervised * Contribution † during internship at Tencent Inc. Corresponding Author extractive summarization (Radev et al., 2000; Mihalcea and Tarau, 2004; Erkan and Radev, 2004; Schluter and Søgaard, 2015; Tixier et al., 2017; Zheng and Lapata, 2019; Xu et al., 2020; Dong et al., 2020). Compare with supervised ones, unsupervised methods 1). remove the dependency on large-scale annotated document-summary pairs; 2). are more general for various scenarios. Graph-based models are commonly used in unsupervised extractive methods (Radev et al., 2000; Mihalcea and Tarau, 2004; Erkan and Radev, 2004). For example, Zheng and Lapata (2019) proposed a directed centrality-based method named PacSum 1685 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1685–1697 August 1–6, 2021. ©2021 Association for Computational Lin"
2021.findings-acl.147,2020.acl-main.552,0,0.0105022,"scribes terroristic attacks in Iraq. However, FAR can cover all 3 facets in gold reference. This shows that our FAR can effectively improve the performance by reducing the facet bias problem. 6 Related Work Summarization is a long-standing challenge for researchers to address. Thanks to the power of the neural network and availability of large-scale parallel datasets. Supervised summarization algorithms develop sharply (Chopra et al., 2016; Cao et al., 2018; Zhang et al., 2018; Zhong et al., 2019; Gehrmann et al., 2019; Cho et al., 2019; Jin et al., 2020b; Cao et al., 2020; Jin et al., 2020a; Zhong et al., 2020). However, high-quality parallel datasets are not always available. Researches on unsupervised summarization are necessary, which can be diveided into extractive and abstractive. Unsupervised abstractive summarization is more challenging than extractive. There are also many interesting works (Wang and Lee, 2018; F´evry and Phang, 2018; Baziotis et al., 2019; Jernite, 2019; Zhou and Rush, 2019; West et al., 2019; Chu and Liu, 2019; Yang et al., 2020) on unsupervised abstractive summarization. However, most unsupervised summarization models are extractive (Radev et al., 2000; Mihalcea and Tarau,"
2021.findings-acl.147,P19-1100,0,0.0166466,"ess of our model.) As shown in Table 5, we can see that sentences extracted by PacSum all focus on the facet which describes terroristic attacks in Iraq. However, FAR can cover all 3 facets in gold reference. This shows that our FAR can effectively improve the performance by reducing the facet bias problem. 6 Related Work Summarization is a long-standing challenge for researchers to address. Thanks to the power of the neural network and availability of large-scale parallel datasets. Supervised summarization algorithms develop sharply (Chopra et al., 2016; Cao et al., 2018; Zhang et al., 2018; Zhong et al., 2019; Gehrmann et al., 2019; Cho et al., 2019; Jin et al., 2020b; Cao et al., 2020; Jin et al., 2020a; Zhong et al., 2020). However, high-quality parallel datasets are not always available. Researches on unsupervised summarization are necessary, which can be diveided into extractive and abstractive. Unsupervised abstractive summarization is more challenging than extractive. There are also many interesting works (Wang and Lee, 2018; F´evry and Phang, 2018; Baziotis et al., 2019; Jernite, 2019; Zhou and Rush, 2019; West et al., 2019; Chu and Liu, 2019; Yang et al., 2020) on unsupervised abstractive"
2021.findings-acl.147,P19-1503,0,0.0188958,"algorithms develop sharply (Chopra et al., 2016; Cao et al., 2018; Zhang et al., 2018; Zhong et al., 2019; Gehrmann et al., 2019; Cho et al., 2019; Jin et al., 2020b; Cao et al., 2020; Jin et al., 2020a; Zhong et al., 2020). However, high-quality parallel datasets are not always available. Researches on unsupervised summarization are necessary, which can be diveided into extractive and abstractive. Unsupervised abstractive summarization is more challenging than extractive. There are also many interesting works (Wang and Lee, 2018; F´evry and Phang, 2018; Baziotis et al., 2019; Jernite, 2019; Zhou and Rush, 2019; West et al., 2019; Chu and Liu, 2019; Yang et al., 2020) on unsupervised abstractive summarization. However, most unsupervised summarization models are extractive (Radev et al., 2000; Mihalcea and Tarau, 2004; Erkan and Radev, 2004; Carbonell and Goldstein, 1998; Wan, 2008; Wan and Yang, 2008; Schluter and Søgaard, 2015; Zhao et al., 2020) and focused on the measure of sentence salient. Graph-based models are effective and widely concerned in unsupervised extractive methods. Different from traditional undirected graph rank models (Radev et al., 2000; Mihalcea and Tarau, 2004; Erkan and Radev"
2021.sustainlp-1.13,D18-1045,0,0.0334402,"Missing"
2021.sustainlp-1.13,2021.ccl-1.108,0,0.0932195,"Missing"
C08-1141,E06-1032,0,0.273045,"phenomena to provide diagnostic evaluation. The effectiveness of our approach for diagnostic evaluation is verified through experiments on various types of MT systems. 1 Introduction Automatic MT evaluation is a crucial issue for MT system developers. The state-of-the-art methods for automatic MT evaluation are using an n-gram based metric represented by BLEU (Papineni et al., 2002) and its variants. Ever since its invention, the BLEU score has been a widely accepted benchmark for MT system evaluation. Nevertheless, the research community has been aware of the deficiencies of the BLEU metric (Callison-Burch et al., 2006). For instance, BLEU fails to sufficiently capture the vitality of natural languages: all grams of a sentence are © 2008. Licensed under the Creative Commons AttributionNoncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. treated equally ignoring their linguistic significance; only consecutive grams are considered ignoring the skipped grams of certain linguistic relations; candidate translation gets acknowledged only if it uses exactly the same lexicon as the reference ignoring the variation in lexical choice. Furthermore, BL"
C08-1141,A00-2019,0,0.0155641,"Missing"
C08-1141,W04-3250,0,0.149378,"Missing"
C08-1141,P04-1077,0,0.0226721,"uding 3,200 pairs of sentences containing 6 classes of check-points. Their check-points were manually constructed by human experts, therefore it will be costly to build new test corpus while the check-points in our approach are constructed automatically. Another limitation of their work is that only binary score is used for credits while we use n-gram matching rate which provides a broader coverage of different levels of matching. There are many recent work motivated by ngram based approach. (Callison-Burch et al., 2006) criticized the inadequate accuracy of evaluation at the sentence level. (Lin and Och, 2004) used longest common subsequence and skipbigram statistics. (Banerjee and Lavie, 2005) calculated the scores by matching the unigrams on the surface forms, stemmed forms and senses. (Liu et al., 2005) used syntactic features and unlabeled head-modifier dependencies to evaluate MT quality, outperforming BLEU on sentence level correlations with human judgment. (Gimenez and Marquez, 2007) showed that linguistic features at more abstract levels such as dependency relation may provide more reliable system rankings. (Yang et al., 2007) formulates MT evaluation as a ranking problems leading to greate"
C08-1141,W05-0904,0,0.0285694,"Missing"
C08-1141,J03-1002,0,0.00842467,"Missing"
C08-1141,P02-1040,0,0.0750984,"Missing"
C08-1141,W00-1212,1,0.735942,"Missing"
C08-1141,P03-1054,0,0.00524886,"Missing"
C08-1141,W07-0738,0,\N,Missing
C08-1141,W05-0909,0,\N,Missing
C10-1036,J07-2003,0,0.117997,"Missing"
C10-1036,P09-1107,0,0.113563,"Missing"
C10-1036,P08-1023,0,0.055569,"Missing"
C10-1036,W05-1506,0,0.103769,"Missing"
C10-1036,P08-1067,0,0.0669287,"Missing"
C10-1036,W04-3250,0,0.0749687,"Missing"
C10-1036,N04-1022,0,0.722962,"Missing"
C10-1036,P09-1019,0,0.462334,"Missing"
C10-1036,P03-1021,0,0.202425,"Missing"
C10-1036,J04-4002,0,0.151503,"Missing"
C10-1036,P07-1040,0,0.0636909,"Missing"
C10-1036,D08-1065,0,0.787903,"n model’s distribution. Kumar and Byrne (2004) first introduced MBR decoding to SMT field and developed it on the N-best list translations. Their work has shown that MBR decoding performs better than Maximum a Posteriori (MAP) decoding for different evaluation criteria. After that, many dedi1 This work has been done while the author was visiting Microsoft Research Asia. Mu Li, Dongdong Zhang, Ming Zhou Microsoft Research Asia muli@microsoft.com dozhang@microsoft.com mingzhou@microsoft.com cated efforts have been made to improve the performances of SMT systems by utilizing MBRinspired methods. Tromble et al. (2008) proposed a linear approximation to BLEU score (log-BLEU) as a new loss function in MBR decoding and extended it from N-best lists to lattices, and Kumar et al. (2009) presented more efficient algorithms for MBR decoding on both lattices and hypergraphs to alleviate the high computational cost problem in Tromble et al.’s work. DeNero et al. (2009) proposed a fast consensus decoding algorithm for MBR for both linear and non-linear similarity measures. All work mentioned above share a common setting: an MBR decoder is built based on one and only one MAP decoder. On the other hand, recent researc"
C10-1036,W02-1021,0,0.0878161,"Missing"
C10-1036,P09-1065,0,0.171644,"Missing"
C10-1036,D08-1022,0,\N,Missing
C10-1036,D07-1105,0,\N,Missing
C10-1036,P09-1066,1,\N,Missing
C10-1036,P06-1066,0,\N,Missing
C10-1036,P09-1064,0,\N,Missing
C10-1036,J97-3002,0,\N,Missing
C10-1036,2008.amta-srw.3,0,\N,Missing
C10-1075,P05-1033,0,0.132848,"llel corpora available to the constrained track of NIST 2008 Chinese-English MT evaluation task were used for translation model training, which consist of around 5.1M bilingual sentence pairs. GIZA++ was used for word alignment in both directions, which was further refined with the intersec-diaggrow heuristics. We used a 5-gram language model which was trained from the Xinhua portion of English Gigaword corpus version 3.0 from LDC and the English part of parallel corpora. 4.2 Machine Translation System We used an in-house implementation of the hierarchical phrase-based decoder as described in Chiang (2005). In addtion to the standard features used in Chiang (2005), we also used a lexicon feature indicating how many word paris in the translation found in a conventional Chinese-English lexicon. Phrasal rules were extracted from all the parallel data, but hierarchical rules were only extracted from the FBIS part of the parallel data which contains around 128,000 sentence pairs. For all the development data, feature weights of the decoder were tuned using the MERT algorithm (Och, 2003). 4.3 Results of Development Data Pre-construction In the following we first present some overall results using the"
C10-1075,C04-1059,0,0.0924552,"olingual and bilingual) to the existing training corpora has been shown to be very effective in practice. But model adaptation is required in more scenarios other than explicitly defined domains. As shown by the results in Table 2, even for the data from the same domain, distribution mismatch can also be a problem. There are also considerable efforts made to deal with the unknown distribution of text to be translated, and the research topics were still focused on translation and language model adaptation. Typical methods used in this direction include dynamic data selection (L¨u et al., 2007; Zhao et al., 2004; Hildebrand et al., 1995) and data weighting (Foster and Kuhn, 2007; Matsoukas et al., 2009). All the mentioned methods use information retrieval techniques to identify relevant training data from the entire training corpora. Our work presented here also makes no assumption about the distribution of test data, but it differs from the previous methods significantly from a log-linear model’s perspective. Adjusting translation and language models based on test data can be viewed as adaptation of feature values, while our method is essentially adaptation of feature weights. This difference makes"
C10-1075,W07-0717,0,0.0553139,"n shown to be very effective in practice. But model adaptation is required in more scenarios other than explicitly defined domains. As shown by the results in Table 2, even for the data from the same domain, distribution mismatch can also be a problem. There are also considerable efforts made to deal with the unknown distribution of text to be translated, and the research topics were still focused on translation and language model adaptation. Typical methods used in this direction include dynamic data selection (L¨u et al., 2007; Zhao et al., 2004; Hildebrand et al., 1995) and data weighting (Foster and Kuhn, 2007; Matsoukas et al., 2009). All the mentioned methods use information retrieval techniques to identify relevant training data from the entire training corpora. Our work presented here also makes no assumption about the distribution of test data, but it differs from the previous methods significantly from a log-linear model’s perspective. Adjusting translation and language models based on test data can be viewed as adaptation of feature values, while our method is essentially adaptation of feature weights. This difference makes these two kinds of methods complementary to each other — it is possi"
C10-1075,N04-1035,0,0.0759513,"Missing"
C10-1075,N03-1017,0,0.00907053,"Missing"
C10-1075,2007.mtsummit-tutorials.1,0,0.030423,"Missing"
C10-1075,D07-1036,0,0.303405,"Missing"
C10-1075,D09-1074,0,0.0531116,"ctive in practice. But model adaptation is required in more scenarios other than explicitly defined domains. As shown by the results in Table 2, even for the data from the same domain, distribution mismatch can also be a problem. There are also considerable efforts made to deal with the unknown distribution of text to be translated, and the research topics were still focused on translation and language model adaptation. Typical methods used in this direction include dynamic data selection (L¨u et al., 2007; Zhao et al., 2004; Hildebrand et al., 1995) and data weighting (Foster and Kuhn, 2007; Matsoukas et al., 2009). All the mentioned methods use information retrieval techniques to identify relevant training data from the entire training corpora. Our work presented here also makes no assumption about the distribution of test data, but it differs from the previous methods significantly from a log-linear model’s perspective. Adjusting translation and language models based on test data can be viewed as adaptation of feature values, while our method is essentially adaptation of feature weights. This difference makes these two kinds of methods complementary to each other — it is possible to make further impro"
C10-1075,P02-1038,0,0.0329115,"l methods that use a fixed model parameter setting across different data sets. 1 e∗ = argmax Pr(e|f ) e and the posterior probability distribution Pr(e|f ) is directly approximated by a log-linear formulation: Pr(e|f ) = pλ (e|f ) P exp( M m=1 λm hm (e, f )) =P PM 0 e0 exp( m=1 λm hm (e , f )) (1) in which hm ’s are feature functions and λ = (λ1 , . . . , λM ) are model parameters (feature weights). For a successful practical log-linear SMT model, it is usually a combined result of the several efforts: • Construction of well-motivated SMT models Introduction In recent years, log-linear model (Och and Ney, 2002) has been a mainstream method to formulate statistical models for machine translation. Using this formulation, various kinds of relevant properties and data statistics used in the translation process, either on the monolingual-side or on the bilingual-side, are encoded and used as realvalued feature functions, thus it provides an effective mathematical framework to accommodate a large variety of SMT formalisms with different computational linguistic motivations. ∗ This work was done while the author was visiting Microsoft Research Asia. • Accurate estimation of feature functions • Appropriate"
C10-1075,P03-1021,0,0.403672,"such λ∗ that is optimal for multiple data sets at the same time. Table 1 shows some empirical evidences when two data sets are mutually used as development and test data. In this setting, we used a hierarchical phrase based decoder and 2 years’ evaluation data of NIST Chineseto-English machine translation task (for the year 2008 only the newswire subset was used because we want to limit both data sets within the same domain to show that data mismatch also exists even if there is no domain difference), and report results using BLEU scores. Model parameters were tuned using the MERT algorithm (Och, 2003) optimized for BLEU metric. Dev data MT05 MT08-nw MT05 0.402 0.372 MT08-nw 0.306 0.343 Table 1: Translation performance of cross development/test on two NIST evaluation data sets. In our work, we present a solution to this problem by using test data dependent model parameters for test data translation. As discussed above, since model parameters are solely determined by development data D, selection of log-linear model parameters is basically equivalent to selecting a set of development data D. However, automatic development data selection in current SMT research remains a relatively open issue"
C10-1075,P07-1004,0,0.0590762,"e task. To our knowledge, there is no dedicated discussion on principled methods to perform development data selection in previous research. In L¨u et al. (2007), log-linear model parameters can also be adjusted at decoding time. But in their approach, the adjustment was based on heuristic rules and re-weighted training data distribution. In addition, compared with training data selection, the computational cost of development data selection is much smaller. From machine learning perspective, both proposed methods can be viewed as certain form of transductive learning applied to the SMT task (Ueffing et al., 2007). But our methods do not rely on surface similarities between training and training/development sentences, and development/test sentences are not used to re-train SMT sub-models. 6 Conclusions and Future Work In this paper, we addressed the data mismatch issue between training and decoding time of loglinear SMT models, and presented principled methods for dynamically inferring test data dependent model parameters with development set selection. We describe two algorithms for this task, development set pre-construction and dynamic construction, and evaluated our method on the NIST data sets for"
C10-1075,P06-1077,0,\N,Missing
C10-1075,2005.eamt-1.19,0,\N,Missing
C10-2025,C08-1005,0,0.0879333,"rne, 2004) decoding over n-best list finds a translation that has lowest expected loss with all the other hypotheses, and it shows that improvement over the Maximum a Posteriori (MAP) decoding. Several word-based methods (Rosti et al., 2007a; Sim et al., 2007) have also been proposed. Usually, these methods take n-best list from different SMT systems as inputs, and construct a confusion network for second-pass decoding. There are also a lot of research work to advance the confusion network construction by finding better alignment between the skeleton and the other hypotheses (He et al., 2008; Ayan et al., 2008). Typically, all the approaches above only use full hypotheses but have no access to the PHS information. In this paper, we present hybrid decoding — a novel statistical machine translation (SMT) decoding paradigm using multiple SMT systems. In our work, in addition to component SMT systems, system combination method is also employed in generating partial translation hypotheses throughout the decoding process, in which smaller hypotheses generated by each component decoder and hypotheses combination are used in the following decoding steps to generate larger hypotheses. Experimental results on"
C10-2025,P06-1121,0,0.0535726,"es word alignment information. However, in hybrid decoding, it is quite time-consuming and impractical to conduct word alignment like GIZA++ for each span. Fortunately, unit hypotheses word alignment can be obtained from the model training process, which is shown in Figure 2. We devise a heuristic approach for PHS alignment that leverages the translation derivations from the sub-phrases. The derivation information ultimately comes from the phrase table in phrase-based systems (Koehn et al., 2003; Xiong et al., 2006) or the rule table in syntactic-based systems (Chiang, 2007; Liu et al., 2007; Galley et al., 2006). The derivation is built in a phrase-based system as follows. For example, we have two phrase where string “m-n”means the mth word in the source phrase is aligned to the nth word in the target phrase. When combining the two phrases for generating “我们 的 经济 利益”, we obtain the translation hypothesis as “our economic interests”and also integrate the alignment fragment to get “0-0 1-0 2-1 3-2”. The case is similar in syntactic-based system for non-terminal substitution, which we will not discuss further here. Next, we introduce the skeleton-to-hypothesis word alignment algorithm in detail. With th"
C10-2025,N03-1017,0,0.0544342,"Missing"
C10-2025,P02-1040,0,0.0893694,"Suppose we have n individual decoders. The ranking function Fn of the nth decoder can be written as: Fn (e) = m X λn,i hn,i (f, e) (2) i=1 Figure 3: Algorithm for skeleton-to-hypothesis alignment Subroutines UNION(A,B) GETALIGN(S,align) tial in confusion network construction. The simplest way is to use the top-1 PHS from any individual decoder with the best performance under some criteria. However, this cannot always lead to better performance on some evaluation metrics (Rosti et al., 2007a). An alternative would be MBR method with some loss function such as TER (Snover et al., 2006) or BLEU (Papineni et al., 2002). We show the experimental results of two skeleton selection methods for PHS combination in Section 3. Description the union of set A and set B get the words aligned to S based on align similarity between w1 and w2 , we use edit distance here align w1 with w2 Table 1: Description for subroutines Due to the variety of the word order in nbest outputs, skeleton selection becomes essenwhere each hn,i (f, e) is a feature function of the nth decoder, and λn,i is the corresponding feature weight. m is the number of features in each decoder. The final result of hybrid decoder is the top1 translation f"
C10-2025,P00-1056,0,\N,Missing
C10-2025,E06-1005,0,\N,Missing
C10-2025,P07-1089,0,\N,Missing
C10-2025,P09-1066,1,\N,Missing
C10-2025,P06-1066,0,\N,Missing
C10-2025,P07-1040,0,\N,Missing
C10-2025,N04-1022,0,\N,Missing
C10-2025,D07-1056,1,\N,Missing
C10-2025,P09-1065,0,\N,Missing
C10-2025,P03-1021,0,\N,Missing
C10-2025,P05-1033,0,\N,Missing
C10-2025,N07-1029,0,\N,Missing
C10-2025,J97-3002,0,\N,Missing
C10-2025,D08-1011,0,\N,Missing
C10-2025,W04-3250,0,\N,Missing
C10-2025,J07-2003,0,\N,Missing
C14-1108,P06-1067,0,0.0317284,"problem of reordering source language into the word order of the target language remains a central research topic. Statistical phrase-based translation models (Och and Ney, 2004; Koehn et al., 2003) are good at local reordering, or the reordering of words within the phrase, since the order is specified by phrasal translations. However, phrase-based models remain weak at long-distance reordering, or the reordering of the phrases. To improve the reordering of the phrases, two types of models have been developed. The first one is lexicalized reordering models (Tillman, 2004; Huang et al., 2005; Al-Onaizan and Papineni, 2006; Nagata et al., 2006; Xiong et al., 2006; Zens and Ney, 2006; Koehn et al., 2007; Galley and Manning, 2008; Cherry et al., 2012) which predict reordering by taking advantage of lexical the next phrase are D and S respectively, as shown in Figure 1. Such a model is simple and effective, and has become a standard component of phrase-based systems such as MOSES. Figure 1. Phrase orientations for Chinese-English translation. The other is a hierarchical phrase-based (HPB) translation model (Chiang, 2007) based on synchronous grammar. In the HPB model, a synchronous grammar rule may contain both te"
C14-1108,J07-2003,0,0.649224,"first one is lexicalized reordering models (Tillman, 2004; Huang et al., 2005; Al-Onaizan and Papineni, 2006; Nagata et al., 2006; Xiong et al., 2006; Zens and Ney, 2006; Koehn et al., 2007; Galley and Manning, 2008; Cherry et al., 2012) which predict reordering by taking advantage of lexical the next phrase are D and S respectively, as shown in Figure 1. Such a model is simple and effective, and has become a standard component of phrase-based systems such as MOSES. Figure 1. Phrase orientations for Chinese-English translation. The other is a hierarchical phrase-based (HPB) translation model (Chiang, 2007) based on synchronous grammar. In the HPB model, a synchronous grammar rule may contain both terminals (words) and nonterminals (sub-phrases). The order of terminals and nonterminal are specified by the rule. For This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 1144 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1144–1153, Dublin, Ireland, August 23-29 2014. example"
C14-1108,D08-1089,0,0.57593,"ic. Statistical phrase-based translation models (Och and Ney, 2004; Koehn et al., 2003) are good at local reordering, or the reordering of words within the phrase, since the order is specified by phrasal translations. However, phrase-based models remain weak at long-distance reordering, or the reordering of the phrases. To improve the reordering of the phrases, two types of models have been developed. The first one is lexicalized reordering models (Tillman, 2004; Huang et al., 2005; Al-Onaizan and Papineni, 2006; Nagata et al., 2006; Xiong et al., 2006; Zens and Ney, 2006; Koehn et al., 2007; Galley and Manning, 2008; Cherry et al., 2012) which predict reordering by taking advantage of lexical the next phrase are D and S respectively, as shown in Figure 1. Such a model is simple and effective, and has become a standard component of phrase-based systems such as MOSES. Figure 1. Phrase orientations for Chinese-English translation. The other is a hierarchical phrase-based (HPB) translation model (Chiang, 2007) based on synchronous grammar. In the HPB model, a synchronous grammar rule may contain both terminals (words) and nonterminals (sub-phrases). The order of terminals and nonterminal are specified by the"
C14-1108,C10-1050,0,0.44024,"nsistency in Moses between training and decoding. Here we would like to note that phrase based orientation depends on phrase segmentation. For example, in Figure 1, the orientation of phrase “this is” with respect to next phrase could be either:  D, if we think the next phrase is “the lower reach of ” which is what Figure 1 shows.  or S, if the next phrase is “the lower reach of the yellow river” which can compose a legal phrase pair with “huanghe xiayou” according to the standard phrase pair extraction algorithm. 1150 The decision to adopt word-based orientation makes our work similar with Hayashi et al. (2010) who proposed a word-based reordering model for HPB system. The difference between our work and Hayashi et al. (2010) is: they adopt the reordering model proposed by Tromble and Eisner (2009) for the preprocessing approach, while we borrow the idea of lexicalized reordering models which are originally proposed for phrase-based machine translation. 5 5.1 Experiments Experimental settings Our baseline system is re-implementation of Hiero, a hierarchical phrase-based system (Chiang, 2007). Besides the standard features of a HPB model, there are six reordering features in our reordering model whic"
C14-1108,C08-1041,0,0.126231,"29 2014. example, the translation rule <X xiayou, the lower reach of X > specifies that the translation of sub phrase X before “xiayou” should be put after “the lower reach of”. One problem with the HPB model is that the application of a rule is independent of the actual sub phrase. For example, the rule <X xiayou, the lower reach of X > will always swap the translation of X and “xiayou”, no matter what is covered by X. This is an over-generalization problem. Much work has been done to solve this issue. For example, Zollmann and Venugopal (2006) annotate non-terminals by syntactic categories. He et al. (2008) proposes maximum entropy models which combine rich context information for selecting translation rules during decoding. Huang et al. (2010) automatically induce a set of latent syntactic categories to annotate nonterminals. These works alleviate the overgeneralization problem by considering the content of X. In this paper, we try to solve it from an alternative view by modeling whether the phrases covered by X prefer the order specified by the rule. This has led us to borrow the lexicalized reordering model from the phrase-based model for the HPB model. We propose a novel lexicalized reorderi"
C14-1108,W05-1507,0,0.0354703,"ine translation, the problem of reordering source language into the word order of the target language remains a central research topic. Statistical phrase-based translation models (Och and Ney, 2004; Koehn et al., 2003) are good at local reordering, or the reordering of words within the phrase, since the order is specified by phrasal translations. However, phrase-based models remain weak at long-distance reordering, or the reordering of the phrases. To improve the reordering of the phrases, two types of models have been developed. The first one is lexicalized reordering models (Tillman, 2004; Huang et al., 2005; Al-Onaizan and Papineni, 2006; Nagata et al., 2006; Xiong et al., 2006; Zens and Ney, 2006; Koehn et al., 2007; Galley and Manning, 2008; Cherry et al., 2012) which predict reordering by taking advantage of lexical the next phrase are D and S respectively, as shown in Figure 1. Such a model is simple and effective, and has become a standard component of phrase-based systems such as MOSES. Figure 1. Phrase orientations for Chinese-English translation. The other is a hierarchical phrase-based (HPB) translation model (Chiang, 2007) based on synchronous grammar. In the HPB model, a synchronous g"
C14-1108,D10-1014,0,0.0157829,"hould be put after “the lower reach of”. One problem with the HPB model is that the application of a rule is independent of the actual sub phrase. For example, the rule <X xiayou, the lower reach of X > will always swap the translation of X and “xiayou”, no matter what is covered by X. This is an over-generalization problem. Much work has been done to solve this issue. For example, Zollmann and Venugopal (2006) annotate non-terminals by syntactic categories. He et al. (2008) proposes maximum entropy models which combine rich context information for selecting translation rules during decoding. Huang et al. (2010) automatically induce a set of latent syntactic categories to annotate nonterminals. These works alleviate the overgeneralization problem by considering the content of X. In this paper, we try to solve it from an alternative view by modeling whether the phrases covered by X prefer the order specified by the rule. This has led us to borrow the lexicalized reordering model from the phrase-based model for the HPB model. We propose a novel lexicalized reordering model for hierarchical phrase-based translation and achieved a 0.6-1.2 BLEU point improvements for Chinese-English translation over a str"
C14-1108,W13-2258,0,0.237941,"r internal structure. For example, in Figure 1, suppose nonterminal X1 is not the root node and the orientation probability of X1 will condition on “zhe, xiayou, this, river”. In this paper, we will consider how the words covered by the nonterminal X1 are reordered. Rather than using “xiayou” as a feature to determine the orientation of X1 with respect to the next phrase, we think the immediately translated source word “huanghe” could be more informative through it is not on the boundary of X1 , since “huanghe” is the exact starting point from where we search for the next phrase to translate. Huck et al. (2013) proposed a very effective phrase orientation model for HPB translation. The model is also based on nonterminal. They extracted phrase orientation probabilities from word-aligned training data for use with hierarchical phrase inventories, and scored orientations in hierarchical decoding. 2.2 Path-based lexicalized reordering model The most recent related work is Nguyen and Vogel (2013). They map a HPB derivation into a discontinuous phrase-based translation path in the following two steps: 1) Represent each rule as a sequence of phrase pairs and non-terminals. 2) The rules’ sequences are used"
C14-1108,P07-2045,0,0.00826616,"central research topic. Statistical phrase-based translation models (Och and Ney, 2004; Koehn et al., 2003) are good at local reordering, or the reordering of words within the phrase, since the order is specified by phrasal translations. However, phrase-based models remain weak at long-distance reordering, or the reordering of the phrases. To improve the reordering of the phrases, two types of models have been developed. The first one is lexicalized reordering models (Tillman, 2004; Huang et al., 2005; Al-Onaizan and Papineni, 2006; Nagata et al., 2006; Xiong et al., 2006; Zens and Ney, 2006; Koehn et al., 2007; Galley and Manning, 2008; Cherry et al., 2012) which predict reordering by taking advantage of lexical the next phrase are D and S respectively, as shown in Figure 1. Such a model is simple and effective, and has become a standard component of phrase-based systems such as MOSES. Figure 1. Phrase orientations for Chinese-English translation. The other is a hierarchical phrase-based (HPB) translation model (Chiang, 2007) based on synchronous grammar. In the HPB model, a synchronous grammar rule may contain both terminals (words) and nonterminals (sub-phrases). The order of terminals and nonter"
C14-1108,P06-1090,0,0.0287036,"anguage into the word order of the target language remains a central research topic. Statistical phrase-based translation models (Och and Ney, 2004; Koehn et al., 2003) are good at local reordering, or the reordering of words within the phrase, since the order is specified by phrasal translations. However, phrase-based models remain weak at long-distance reordering, or the reordering of the phrases. To improve the reordering of the phrases, two types of models have been developed. The first one is lexicalized reordering models (Tillman, 2004; Huang et al., 2005; Al-Onaizan and Papineni, 2006; Nagata et al., 2006; Xiong et al., 2006; Zens and Ney, 2006; Koehn et al., 2007; Galley and Manning, 2008; Cherry et al., 2012) which predict reordering by taking advantage of lexical the next phrase are D and S respectively, as shown in Figure 1. Such a model is simple and effective, and has become a standard component of phrase-based systems such as MOSES. Figure 1. Phrase orientations for Chinese-English translation. The other is a hierarchical phrase-based (HPB) translation model (Chiang, 2007) based on synchronous grammar. In the HPB model, a synchronous grammar rule may contain both terminals (words) and n"
C14-1108,P13-1156,0,0.275902,"the immediately translated source word “huanghe” could be more informative through it is not on the boundary of X1 , since “huanghe” is the exact starting point from where we search for the next phrase to translate. Huck et al. (2013) proposed a very effective phrase orientation model for HPB translation. The model is also based on nonterminal. They extracted phrase orientation probabilities from word-aligned training data for use with hierarchical phrase inventories, and scored orientations in hierarchical decoding. 2.2 Path-based lexicalized reordering model The most recent related work is Nguyen and Vogel (2013). They map a HPB derivation into a discontinuous phrase-based translation path in the following two steps: 1) Represent each rule as a sequence of phrase pairs and non-terminals. 2) The rules’ sequences are used to find the corresponding phrase-based path of a HPB derivation and calculate the phrase-based reordering features. Figure 2. The phrase-based path of the derivation in Figure 1. 1145 A phrase-based path is the sequence of phrase pairs, whose source sides covers the source sentences and whose target sides generated the target sentences from left to right. For example, the phrase-based"
C14-1108,P00-1056,0,0.144249,"ectively. They are integrated into the log-linear model of the HPB system. The Minimum Error Rate Training (MERT) (Och, 2003) algorithm is adopted to tune feature weights for translation systems. We test our reordering model on a Chinese-English translation task. The NIST evaluation set MT06 was used as our development set to tune the feature weights, and the test data are MT04, MT 05 and MT08. We first conduct experiments by using the FBIS parallel training corpus, and then further test the effect of our method on a large scale parallel training corpus. Word alignment is performed by GIZA++ (Och and Ney, 2000) in both directions with the default setting. The language model is a 4-gram model trained with the Xinhua portion of LDC English Gigaword Version 3.0 and the English part of the bilingual training data. Translation performances are measured with case-insensitive BLEU4 score (Papineni et al., 2002). 5.2 Experimental results on FBIS corpus We first conduct experiments by using the FBIS parallel corpus to train the model of both the baseline and our lexicalized reordering model. After pre-processing, the statistics of FBIS corpus is shown in table 1. Chinese English #sentences 128832 128832 #wor"
C14-1108,J04-4002,0,0.219684,"rectly on synchronous rules. For each target phrase contained in a rule, we calculate its orientation probability conditioned on the rule. We test our model on both small and large scale data. On NIST machine translation test sets, our reordering model achieved a 0.6-1.2 BLEU point improvements for Chinese-English translation over a strong baseline hierarchical phrase-based system. 1 Introduction In statistical machine translation, the problem of reordering source language into the word order of the target language remains a central research topic. Statistical phrase-based translation models (Och and Ney, 2004; Koehn et al., 2003) are good at local reordering, or the reordering of words within the phrase, since the order is specified by phrasal translations. However, phrase-based models remain weak at long-distance reordering, or the reordering of the phrases. To improve the reordering of the phrases, two types of models have been developed. The first one is lexicalized reordering models (Tillman, 2004; Huang et al., 2005; Al-Onaizan and Papineni, 2006; Nagata et al., 2006; Xiong et al., 2006; Zens and Ney, 2006; Koehn et al., 2007; Galley and Manning, 2008; Cherry et al., 2012) which predict reord"
C14-1108,P03-1021,0,0.111575,"ner (2009) for the preprocessing approach, while we borrow the idea of lexicalized reordering models which are originally proposed for phrase-based machine translation. 5 5.1 Experiments Experimental settings Our baseline system is re-implementation of Hiero, a hierarchical phrase-based system (Chiang, 2007). Besides the standard features of a HPB model, there are six reordering features in our reordering model which are M, S and D with respect to the previous and next phrase respectively. They are integrated into the log-linear model of the HPB system. The Minimum Error Rate Training (MERT) (Och, 2003) algorithm is adopted to tune feature weights for translation systems. We test our reordering model on a Chinese-English translation task. The NIST evaluation set MT06 was used as our development set to tune the feature weights, and the test data are MT04, MT 05 and MT08. We first conduct experiments by using the FBIS parallel training corpus, and then further test the effect of our method on a large scale parallel training corpus. Word alignment is performed by GIZA++ (Och and Ney, 2000) in both directions with the default setting. The language model is a 4-gram model trained with the Xinhua"
C14-1108,P02-1040,0,0.0916478,"s used as our development set to tune the feature weights, and the test data are MT04, MT 05 and MT08. We first conduct experiments by using the FBIS parallel training corpus, and then further test the effect of our method on a large scale parallel training corpus. Word alignment is performed by GIZA++ (Och and Ney, 2000) in both directions with the default setting. The language model is a 4-gram model trained with the Xinhua portion of LDC English Gigaword Version 3.0 and the English part of the bilingual training data. Translation performances are measured with case-insensitive BLEU4 score (Papineni et al., 2002). 5.2 Experimental results on FBIS corpus We first conduct experiments by using the FBIS parallel corpus to train the model of both the baseline and our lexicalized reordering model. After pre-processing, the statistics of FBIS corpus is shown in table 1. Chinese English #sentences 128832 128832 #words 3016570 3922816 Table 1. The statistics of FBIS corpus Table 2 summarizes the translation performance. The first row shows the results of baseline HPB system, and the second row shows the results when we integrated our lexicalized reordering model (LRM). We get 1.2, 0.8 and 0.7 BLEU point improv"
C14-1108,N04-4026,0,0.184029,"Missing"
C14-1108,D09-1105,0,0.113438,"ase “this is” with respect to next phrase could be either:  D, if we think the next phrase is “the lower reach of ” which is what Figure 1 shows.  or S, if the next phrase is “the lower reach of the yellow river” which can compose a legal phrase pair with “huanghe xiayou” according to the standard phrase pair extraction algorithm. 1150 The decision to adopt word-based orientation makes our work similar with Hayashi et al. (2010) who proposed a word-based reordering model for HPB system. The difference between our work and Hayashi et al. (2010) is: they adopt the reordering model proposed by Tromble and Eisner (2009) for the preprocessing approach, while we borrow the idea of lexicalized reordering models which are originally proposed for phrase-based machine translation. 5 5.1 Experiments Experimental settings Our baseline system is re-implementation of Hiero, a hierarchical phrase-based system (Chiang, 2007). Besides the standard features of a HPB model, there are six reordering features in our reordering model which are M, S and D with respect to the previous and next phrase respectively. They are integrated into the log-linear model of the HPB system. The Minimum Error Rate Training (MERT) (Och, 2003)"
C14-1108,2011.mtsummit-papers.43,0,0.115615,"has led us to borrow the lexicalized reordering model from the phrase-based model for the HPB model. We propose a novel lexicalized reordering model for hierarchical phrase-based translation and achieved a 0.6-1.2 BLEU point improvements for Chinese-English translation over a strong HPB baseline system. 2 Related work In this section, we briefly review two types of related work which are a nonterminal-based lexicalized reordering models and a path-based lexicalized reordering model. Both of them calculate the orientation for HPB translation. 2.1 Nonterminal-based lexicalized reordering models Xiao et al. (2011) proposed an orientation model for HPB translation. The orientation probability of a derivation is calculated as the product of orientation probabilities of all nonterminals except the root. In order to define the relative orders of nonterminals and their adjacent phrase, they expand the alignment in a rule to include both terminals and nonterminals. There may be multiple ways to segment a rule into phrases; they use the maximum adjacent phrase similar to Galley and Manning (2008). They significantly outperformed the HPB system on both Chinese-English and German-English translation. Xiao et al"
C14-1108,P06-1066,0,0.0289271,"order of the target language remains a central research topic. Statistical phrase-based translation models (Och and Ney, 2004; Koehn et al., 2003) are good at local reordering, or the reordering of words within the phrase, since the order is specified by phrasal translations. However, phrase-based models remain weak at long-distance reordering, or the reordering of the phrases. To improve the reordering of the phrases, two types of models have been developed. The first one is lexicalized reordering models (Tillman, 2004; Huang et al., 2005; Al-Onaizan and Papineni, 2006; Nagata et al., 2006; Xiong et al., 2006; Zens and Ney, 2006; Koehn et al., 2007; Galley and Manning, 2008; Cherry et al., 2012) which predict reordering by taking advantage of lexical the next phrase are D and S respectively, as shown in Figure 1. Such a model is simple and effective, and has become a standard component of phrase-based systems such as MOSES. Figure 1. Phrase orientations for Chinese-English translation. The other is a hierarchical phrase-based (HPB) translation model (Chiang, 2007) based on synchronous grammar. In the HPB model, a synchronous grammar rule may contain both terminals (words) and nonterminals (sub-phr"
C14-1108,W06-3108,0,0.131425,"language remains a central research topic. Statistical phrase-based translation models (Och and Ney, 2004; Koehn et al., 2003) are good at local reordering, or the reordering of words within the phrase, since the order is specified by phrasal translations. However, phrase-based models remain weak at long-distance reordering, or the reordering of the phrases. To improve the reordering of the phrases, two types of models have been developed. The first one is lexicalized reordering models (Tillman, 2004; Huang et al., 2005; Al-Onaizan and Papineni, 2006; Nagata et al., 2006; Xiong et al., 2006; Zens and Ney, 2006; Koehn et al., 2007; Galley and Manning, 2008; Cherry et al., 2012) which predict reordering by taking advantage of lexical the next phrase are D and S respectively, as shown in Figure 1. Such a model is simple and effective, and has become a standard component of phrase-based systems such as MOSES. Figure 1. Phrase orientations for Chinese-English translation. The other is a hierarchical phrase-based (HPB) translation model (Chiang, 2007) based on synchronous grammar. In the HPB model, a synchronous grammar rule may contain both terminals (words) and nonterminals (sub-phrases). The order of"
C14-1108,W06-3119,0,0.0423035,"al Linguistics: Technical Papers, pages 1144–1153, Dublin, Ireland, August 23-29 2014. example, the translation rule <X xiayou, the lower reach of X > specifies that the translation of sub phrase X before “xiayou” should be put after “the lower reach of”. One problem with the HPB model is that the application of a rule is independent of the actual sub phrase. For example, the rule <X xiayou, the lower reach of X > will always swap the translation of X and “xiayou”, no matter what is covered by X. This is an over-generalization problem. Much work has been done to solve this issue. For example, Zollmann and Venugopal (2006) annotate non-terminals by syntactic categories. He et al. (2008) proposes maximum entropy models which combine rich context information for selecting translation rules during decoding. Huang et al. (2010) automatically induce a set of latent syntactic categories to annotate nonterminals. These works alleviate the overgeneralization problem by considering the content of X. In this paper, we try to solve it from an alternative view by modeling whether the phrases covered by X prefer the order specified by the rule. This has led us to borrow the lexicalized reordering model from the phrase-based"
C14-1108,W12-3125,0,\N,Missing
C16-1290,J93-2003,0,0.0754472,"a more flexible way, the attention mechanism (Bahdanau et al., 2014) was introduced into the encoderdecoder model. In an attention-based encoder-decoder model, matching scores between the source and target words are calculated based on their corresponding encoder and decoder states. These scores are then normalized and used as weights for the source words given each target word. This can be seen as a soft alignment and the attention mechanism here plays similar role to that of a traditional alignment model. In alignment models used in traditional machine translation models such as IBM Models (Brown et al., 1993), distortion and fertility are modeled explicitly. By comparison, in the attention mechanism, alignment is computed by matching the previous decoder hidden state with all the encoder hidden states, without modeling distortion and fertility. Since the translation of target words is guided by the attention † Work done while author was an undergraduate student of Shanghai Jiao Tong University and intern at Microsoft Research. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 3082 Proceedings of COLIN"
C16-1290,J07-2003,0,0.0314165,"Missing"
C16-1290,W14-4012,0,0.00869674,"Missing"
C16-1290,D14-1179,0,0.0670752,"Missing"
C16-1290,P14-1129,0,0.0219979,"Missing"
C16-1290,P09-1104,0,0.0314005,"+ F ERT D EC SAER 54.75 52.88 52.70 52.40 AER 44.13 42.51 42.37 42.11 Table 2. AER & SAER scores, lower is better. Figure 7. F ERT D EC resolved the problem of missing translation problem that is shown in Figure 3. 6.2.2 Figure 8. F ERT D EC resolved the problem of repeated translation shown in Figure 4. Alignment Quality To analyze the effect of our extentions to the attention mechanism in detail, we evaluate the quality of attention-generated alignment by computing the AER (Och and Ney, 2003) and smoothed-AER (Tu et al., 2016) scores on a manually aligned Chinese–English alignment dataset (Haghighi et al., 2009), which contains 491 sentence pairs. We force the model to generate the correct target sentence and evaluate the attention-generated alignment matrix. From the results shown in Table 2, we can see that all three proposed methods achieved better alignment quality, compared with the original attention method. 3089 6.3 Qualitative Analysis In this section we qualitatively evaluate how our models addressed the problems analyzed in Section 3. All examples shown are from the test set. Incorrect Reordering In Figure 2 we can see, R EC ATT generated the correct alignment on the example sentence shown"
C16-1290,D13-1176,0,0.25225,"Missing"
C16-1290,koen-2004-pharaoh,0,0.0197588,"ss the missing and repeated translation problems, we introduce fertility-conditioned decoder F ERT D EC. F ERT D EC uses a coverage vector 1 to represent the informationPof the source sentence that has not been translated. Initialized by the sum of source word embeddings Jj=1 xj and updated along the translation dynamically, our trainable coverage vector is different from the predefined condition vector used in (Wen et al., 2015). In order to leverage the coverage vector in decoding, we change the 1 The coverage vector in our work plays a similar role with the one used in beam search decoder (Koehn, 2004).There are two major two differences between them: 1. our coverage vector is used as a soft constraint instead of a hard constraint. 2. we tract untranslated words instead of translated words. 3086 decoding recurrent unit as follows: di = ei−1 di−1 ri = σ(W r yi−1 + U r hi−1 + V r di ) zi = σ(W z yi−1 + U z hi−1 + V z di ) ei = σ(W e yi−1 + U e hi−1 + V e di ) h0i = tanh(U (ri hi−1 ) + W yi−1 + V di ) hi = (1 − zi ) h0i + zi hi−1 + tanh(V h di ) where di is the coverage vector, ei is the new added extract gate, which is used to update di based on the words that has been translated. di is desig"
C16-1290,D15-1166,0,0.635888,"t the election to be held on January 30th next year would not be an end to serious violence in Iraq.” Figure 2. Our proposed model R EC ATT produced the correct reordering of the source words, and based on that generated a better translation. At position i in the target sentence, the attention model computes a matching score eij with match function α, for the previous decoder state hi−1 and each encoder state sj . eij = v > tanh (α(hi−1 , sj )) exp(eij ) wij = P k exp(eik ) We wrap this computation of weights as ALIGN: wi = ALIGN(hi−1 , sJ1 ) There are various match functions, as analyzed in (Luong et al., 2015). In our paper we use the sum match function α(hi−1 , sj ) = P W α hi−1 + U α sj . The weighted average of the encoder states sJ1 is calculated as the context ci = j wij sj . It is added to the input of each gate in the decoder, together with previous state hi−1 and previous target word embedding yi−1 : hi = RNN(hi−1 , yi−1 , ci ) 3 Problems of the Attention Mechanism Although attention modeling works well in finding translation correspondence between source and target words, there are still some issues that can be systematically identified, which fall into three categories: incorrect reorderi"
C16-1290,J03-1002,0,0.0421655,"e 1. BLEU scores w/o UNK replacement and the improvement from UNK replacement. RNNS EARCH R EC ATT F ERT D EC R EC ATT + F ERT D EC SAER 54.75 52.88 52.70 52.40 AER 44.13 42.51 42.37 42.11 Table 2. AER & SAER scores, lower is better. Figure 7. F ERT D EC resolved the problem of missing translation problem that is shown in Figure 3. 6.2.2 Figure 8. F ERT D EC resolved the problem of repeated translation shown in Figure 4. Alignment Quality To analyze the effect of our extentions to the attention mechanism in detail, we evaluate the quality of attention-generated alignment by computing the AER (Och and Ney, 2003) and smoothed-AER (Tu et al., 2016) scores on a manually aligned Chinese–English alignment dataset (Haghighi et al., 2009), which contains 491 sentence pairs. We force the model to generate the correct target sentence and evaluate the attention-generated alignment matrix. From the results shown in Table 2, we can see that all three proposed methods achieved better alignment quality, compared with the original attention method. 3089 6.3 Qualitative Analysis In this section we qualitatively evaluate how our models addressed the problems analyzed in Section 3. All examples shown are from the test"
C16-1290,P02-1040,0,0.0972389,"Missing"
C16-1290,D15-1044,0,0.0274475,"Missing"
C16-1290,P16-5005,0,0.00482736,"and the improvement from UNK replacement. RNNS EARCH R EC ATT F ERT D EC R EC ATT + F ERT D EC SAER 54.75 52.88 52.70 52.40 AER 44.13 42.51 42.37 42.11 Table 2. AER & SAER scores, lower is better. Figure 7. F ERT D EC resolved the problem of missing translation problem that is shown in Figure 3. 6.2.2 Figure 8. F ERT D EC resolved the problem of repeated translation shown in Figure 4. Alignment Quality To analyze the effect of our extentions to the attention mechanism in detail, we evaluate the quality of attention-generated alignment by computing the AER (Och and Ney, 2003) and smoothed-AER (Tu et al., 2016) scores on a manually aligned Chinese–English alignment dataset (Haghighi et al., 2009), which contains 491 sentence pairs. We force the model to generate the correct target sentence and evaluate the attention-generated alignment matrix. From the results shown in Table 2, we can see that all three proposed methods achieved better alignment quality, compared with the original attention method. 3089 6.3 Qualitative Analysis In this section we qualitatively evaluate how our models addressed the problems analyzed in Section 3. All examples shown are from the test set. Incorrect Reordering In Figur"
C16-1290,D15-1199,0,0.0707399,"always have full information about the previous alignments even when a longdistance jump happens, which makes the implicit distortion model much more flexible. 4.2 F ERT D EC To address the missing and repeated translation problems, we introduce fertility-conditioned decoder F ERT D EC. F ERT D EC uses a coverage vector 1 to represent the informationPof the source sentence that has not been translated. Initialized by the sum of source word embeddings Jj=1 xj and updated along the translation dynamically, our trainable coverage vector is different from the predefined condition vector used in (Wen et al., 2015). In order to leverage the coverage vector in decoding, we change the 1 The coverage vector in our work plays a similar role with the one used in beam search decoder (Koehn, 2004).There are two major two differences between them: 1. our coverage vector is used as a soft constraint instead of a hard constraint. 2. we tract untranslated words instead of translated words. 3086 decoding recurrent unit as follows: di = ei−1 di−1 ri = σ(W r yi−1 + U r hi−1 + V r di ) zi = σ(W z yi−1 + U z hi−1 + V z di ) ei = σ(W e yi−1 + U e hi−1 + V e di ) h0i = tanh(U (ri hi−1 ) + W yi−1 + V di ) hi = (1 − zi ) h"
D07-1019,P00-1037,0,0.721507,"ve been applied to deal with non-word errors and real-word errors respectively. Non-word error spelling correction is focused on the task of generating and ranking a list of possible spelling corrections for each word not existing in a spelling lexicon. Traditionally candidate ranking is based on manually tuned scores such as assigning alternative weights to different edit operations or leveraging candidate frequencies (Damerau, 1964; Levenshtein, 1966). In recent years, statistical models have been widely used for the tasks of natural language processing, including spelling correction task. (Brill and Moore, 2000) presented an improved error model over the one proposed by (Kernighan et al., 1990) by allowing generic string-to-string edit operations, which helps with modeling major cognitive errors such as the confusion between le and al. Via explicit modeling of phonetic information of English words, (Toutanova and Moore, 2002) further investigated this issue. Both of them require misspelled/correct word pairs for training, and the latter also needs a pronunciation lexicon, but recently (Ahmad and Kondrak, 2005) demonstrated that it is also possible to learn such models automatically from query logs wi"
D07-1019,P06-1129,1,0.813583,"also referred to be context sensitive spelling correction (CSSC), which tries to detect incorrect usage of valid words in certain contexts. Using a pre-defined confusion set is a common strategy for this task, such as in the work of (Golding and Roth, 1996) and (Mangu and Brill, 1997). Opposite to non-word spelling correction, in this direction only contextual evidences were taken into account for modeling by assuming all spelling similarities are equal. The complexity of query spelling correction task requires the combination of these types of evidence, as done in (Cucerzan and Brill, 2004; Li et al., 2006). One important contribution of our work is that we use web search results as extended contextual information beyond query strings by taking advantage of application specific knowledge. Although the information used in our methods can all be accessed in a search engine’s web archive, such a strategy involves web-scale data processing which is a big engineering challenge, while our method is a light-weight solution to this issue. 3 Motivation When a spelling correction model tries to make a decision whether to make a suggestion c to a query q, it generally needs to leverage two types of evidenc"
D07-1019,C04-1120,0,0.0310935,"Missing"
D07-1019,P02-1019,0,0.37831,"res such as assigning alternative weights to different edit operations or leveraging candidate frequencies (Damerau, 1964; Levenshtein, 1966). In recent years, statistical models have been widely used for the tasks of natural language processing, including spelling correction task. (Brill and Moore, 2000) presented an improved error model over the one proposed by (Kernighan et al., 1990) by allowing generic string-to-string edit operations, which helps with modeling major cognitive errors such as the confusion between le and al. Via explicit modeling of phonetic information of English words, (Toutanova and Moore, 2002) further investigated this issue. Both of them require misspelled/correct word pairs for training, and the latter also needs a pronunciation lexicon, but recently (Ahmad and Kondrak, 2005) demonstrated that it is also possible to learn such models automatically from query logs with the EM algorithm, which is similar to work of (Martin, 2004), learning from a very large corpus of raw text for removing non-word spelling errors in large corpus. All the work for non-word spelling correction focused on the current word itself without taking into account contextual information. Real-word spelling co"
D07-1019,C90-2036,0,\N,Missing
D07-1019,J96-1002,0,\N,Missing
D07-1019,H05-1120,0,\N,Missing
D07-1019,W04-3238,0,\N,Missing
D07-1056,J04-4002,0,0.328585,"ordering and the latter with the longer distance reordering as global reordering. Phrase-based SMT system can effectively capture the local word reordering information which is common enough to be observed in training data. But it is hard to model global phrase reordering. Although syntactic knowledge used in syntax-based SMT systems can help reorder phrases, the resulting model is usually much more complicated than a phrase-based system. There have been considerable amount of efforts to improve the reordering model in SMT systems, ranging from the fundamental distance-based distortion model (Och and Ney, 2004; Koehn et al., 2003), flat reordering model (Wu, 1996; Zens et al., 2004; Kumar et al., 2005), to lexicalized reordering model (Tillmann, 2004; Kumar et al., 2005; Koehn et al., 2005), hierarchical phrase-based model (Chiang, 2005), and maximum entropy-based phrase reordering model (Xiong et al., 2006). Due to the absence of syntactic knowledge in these systems, the ability to capture global reordering knowledge is not powerful. Although syntax-based SMT systems (Yamada et al., 2001; Quirk et al., 2005; Liu et al., 2006) are good at modeling global reordering, their performance is subject to"
D07-1056,N03-1017,0,0.22019,"s can compensate for the reordering errors caused by single parse tree. Finally, experimental results show that the performance of our system is superior to that of the state-of-the-art phrase-based SMT system. 1 IP NP VP DNP ADVP VP NP DEG AD VV 大幅 升值 NN 的 欧元 the significant appreciation of the Euro Figure 1: A reordering example Introduction In the last decade, statistical machine translation (SMT) has been widely studied and achieved good translation results. Two kinds of SMT system have been developed, one is phrase-based SMT and the other is syntax-based SMT. In phrase-based SMT systems (Koehn et al., 2003; Koehn, 2004), foreign sentences are firstly segmented into phrases which consists of adjacent words. Then source phrases are translated into target phrases respectively according to knowledge usually learned from bilingual parallel corpus. FiAs studied in previous SMT projects, language model, translation model and reordering model are the three major components in current SMT systems. Due to the difference between the source and target languages, the order of target phrases in the target sentence may differ from the order of source phrases in the source sentence. To make the translation res"
D07-1056,koen-2004-pharaoh,0,0.243773,"the reordering errors caused by single parse tree. Finally, experimental results show that the performance of our system is superior to that of the state-of-the-art phrase-based SMT system. 1 IP NP VP DNP ADVP VP NP DEG AD VV 大幅 升值 NN 的 欧元 the significant appreciation of the Euro Figure 1: A reordering example Introduction In the last decade, statistical machine translation (SMT) has been widely studied and achieved good translation results. Two kinds of SMT system have been developed, one is phrase-based SMT and the other is syntax-based SMT. In phrase-based SMT systems (Koehn et al., 2003; Koehn, 2004), foreign sentences are firstly segmented into phrases which consists of adjacent words. Then source phrases are translated into target phrases respectively according to knowledge usually learned from bilingual parallel corpus. FiAs studied in previous SMT projects, language model, translation model and reordering model are the three major components in current SMT systems. Due to the difference between the source and target languages, the order of target phrases in the target sentence may differ from the order of source phrases in the source sentence. To make the translation results be closer"
D07-1056,H05-1021,0,0.313844,"Missing"
D07-1056,P06-1077,0,0.143706,"T systems, ranging from the fundamental distance-based distortion model (Och and Ney, 2004; Koehn et al., 2003), flat reordering model (Wu, 1996; Zens et al., 2004; Kumar et al., 2005), to lexicalized reordering model (Tillmann, 2004; Kumar et al., 2005; Koehn et al., 2005), hierarchical phrase-based model (Chiang, 2005), and maximum entropy-based phrase reordering model (Xiong et al., 2006). Due to the absence of syntactic knowledge in these systems, the ability to capture global reordering knowledge is not powerful. Although syntax-based SMT systems (Yamada et al., 2001; Quirk et al., 2005; Liu et al., 2006) are good at modeling global reordering, their performance is subject to parsing errors to a large extent. In this paper, we propose a new method to improve reordering model by introducing syntactic information. Syntactic knowledge such as boundary of sub-trees, part-of-speech (POS) and dependency relation is incorporated into the SMT system to strengthen the ability to handle global phrase reordering. Our method is different from previous syntax-based SMT systems in which the translation process was modeled based on specific syntactic structures, either phrase structures or dependency relatio"
D07-1056,P05-1034,0,0.375787,"e between the root node of T and the root node of the maximum sub-tree which exactly covers fi. For example, in Figure 1 the phrase “大幅” has the maximum sub-tree rooting at ADJP and its height is 3. The height of phrase “ 的 ” is 4 since its maximum sub-tree roots at 535 ADBP instead of AD. If two adjacent phrases have the same height, we regard them as peer phrases. In our model, we make use of bilingual phrases as well, which refer to source-target aligned phrase pairs extracted using the same criterion as most phrase-based systems (Och and Ney, 2004). 3.2 Model Similar to the work in Chiang (2005), our translation model can be formulated as a weighted synchronous context free grammar derivation process. Let D be a derivation that generates a bilingual sentence pair f, e, in which f is the given source sentence, the statistical model that is used to predict the translation probability p(e|f) is defined over Ds as follows: ? ? ? ∝ ? ? ∝ ??? ? ? ?? ?? ? → ?, ? × ? ?→?,?∈? ?? where plm(e) is the language model, i(X ,) is a feature function defined over the derivation rule X,, and i is its weight. Although theoretically it is ideal for translation reorder modeling by const"
D07-1056,J03-1002,0,0.0108535,"Missing"
D07-1056,P96-1021,0,0.649154,"global reordering. Phrase-based SMT system can effectively capture the local word reordering information which is common enough to be observed in training data. But it is hard to model global phrase reordering. Although syntactic knowledge used in syntax-based SMT systems can help reorder phrases, the resulting model is usually much more complicated than a phrase-based system. There have been considerable amount of efforts to improve the reordering model in SMT systems, ranging from the fundamental distance-based distortion model (Och and Ney, 2004; Koehn et al., 2003), flat reordering model (Wu, 1996; Zens et al., 2004; Kumar et al., 2005), to lexicalized reordering model (Tillmann, 2004; Kumar et al., 2005; Koehn et al., 2005), hierarchical phrase-based model (Chiang, 2005), and maximum entropy-based phrase reordering model (Xiong et al., 2006). Due to the absence of syntactic knowledge in these systems, the ability to capture global reordering knowledge is not powerful. Although syntax-based SMT systems (Yamada et al., 2001; Quirk et al., 2005; Liu et al., 2006) are good at modeling global reordering, their performance is subject to parsing errors to a large extent. In this paper, we pr"
D07-1056,P06-1066,0,0.799337,"used in syntax-based SMT systems can help reorder phrases, the resulting model is usually much more complicated than a phrase-based system. There have been considerable amount of efforts to improve the reordering model in SMT systems, ranging from the fundamental distance-based distortion model (Och and Ney, 2004; Koehn et al., 2003), flat reordering model (Wu, 1996; Zens et al., 2004; Kumar et al., 2005), to lexicalized reordering model (Tillmann, 2004; Kumar et al., 2005; Koehn et al., 2005), hierarchical phrase-based model (Chiang, 2005), and maximum entropy-based phrase reordering model (Xiong et al., 2006). Due to the absence of syntactic knowledge in these systems, the ability to capture global reordering knowledge is not powerful. Although syntax-based SMT systems (Yamada et al., 2001; Quirk et al., 2005; Liu et al., 2006) are good at modeling global reordering, their performance is subject to parsing errors to a large extent. In this paper, we propose a new method to improve reordering model by introducing syntactic information. Syntactic knowledge such as boundary of sub-trees, part-of-speech (POS) and dependency relation is incorporated into the SMT system to strengthen the ability to hand"
D07-1056,P01-1067,0,0.4941,"Missing"
D07-1056,P05-1033,0,0.853395,"rd to model global phrase reordering. Although syntactic knowledge used in syntax-based SMT systems can help reorder phrases, the resulting model is usually much more complicated than a phrase-based system. There have been considerable amount of efforts to improve the reordering model in SMT systems, ranging from the fundamental distance-based distortion model (Och and Ney, 2004; Koehn et al., 2003), flat reordering model (Wu, 1996; Zens et al., 2004; Kumar et al., 2005), to lexicalized reordering model (Tillmann, 2004; Kumar et al., 2005; Koehn et al., 2005), hierarchical phrase-based model (Chiang, 2005), and maximum entropy-based phrase reordering model (Xiong et al., 2006). Due to the absence of syntactic knowledge in these systems, the ability to capture global reordering knowledge is not powerful. Although syntax-based SMT systems (Yamada et al., 2001; Quirk et al., 2005; Liu et al., 2006) are good at modeling global reordering, their performance is subject to parsing errors to a large extent. In this paper, we propose a new method to improve reordering model by introducing syntactic information. Syntactic knowledge such as boundary of sub-trees, part-of-speech (POS) and dependency relati"
D07-1056,C04-1030,0,0.0617,"eign linguistic other models such as language model. In our system, we combine the parse trees gen- phrases can also be formed into two valid adjacent erated respectively by Stanford parser (Klein, 2003) phrase according to constraints proposed in the and a dependency parser developed by (Zhou, phrase extraction algorithm by Och (2003a), they 2000). Compared with the Stanford parser, the de- will be extracted as a reordering training sample. pendency parser only conducts shallow syntactic Finally, the ME modeling toolkit developed by analysis. It is powerful to identify the base NPs and Zhang (2004) is used to train the reordering model base VPs and their dependencies. Additionally, over the extracted samples. dependency parser runs much faster. For example, it took about three minutes for the dependency parser to parse one thousand sentences with aver4.2 Combination of Parse Trees 538 7 Experimental Results and Analysis We conducted our experiments on Chinese-toEnglish translation task of NIST MT-05 on a 3.0GHz system with 4G RAM memory. The bilingual training data comes from the FBIS corpus. The Xinhua news in GIGAWORD corpus is used to train a four-gram language model. The development"
D07-1056,P03-1021,0,0.134599,"tally twelve categories of features used to train the ME model. In fact, the probability of Rule (1) is just equal to the supplementary probability of Rule (2), and vice versa. For Rule (3)~(9), according to the syntactic structures, their application is determined since there is only one choice to complete reordering, which is similar to the “glue rules” in Chiang (2005). Due to the appearance of non-linguistic phrases, non-monotone phrase reordering is not allowed in these rules. We just assign these rules a constant score trained using our implementation of 537 Minimum Error Rate Training (Och, 2003b), which is 0.7 in our system. For Rule (10)~(12), they are also determined rules since there is no other optional rules competing with them. Constant score is simply assigned to them as well, which is 1.0 in our system. Fea. Description LS1 First word of first foreign phrase LS2 First word of second foreign phrase RS1 Last word of first foreign phrase RS2 Last word of second foreign phrase LT1 First word of first target phrase LT2 First word of second target phrase RT1 Last word of first target phrase RT2 Last word of second target phrase LPos POS of the node covering first foreign phrase RP"
D07-1056,W00-1212,1,\N,Missing
D07-1056,P03-1054,0,\N,Missing
D09-1038,W98-1115,0,0.0439035,"or the SCFG learnt automatically from the training corpus. It can work with an efficient CKY-style binarizer to search for the lowest-cost binarization. We apply our method into a state-of-the-art string-to-tree SMT system. The experimental results show that our method outperforms the synchronous binarization method (Zhang et al., 2006) with over 0.8 BLEU scores on both NIST 2005 and NIST 2008 Chinese-to-English evaluation data sets. 2 Related Work The problem of binarization originates from the parsing problem in which several binarization methods are studied such as left/right binarization (Charniak et al., 1998; Tsuruoka and Tsujii, 2004) and head binarization (Charniak et al., 2006). Generally, the pruning issue in SMT decoding is unnecessary for the parsing problem, and the accuracy of parsing does not rely on the binarization method heavily. Thus, many efforts on the binarization in parsing are made for the efficiency improvement instead of the accuracy improvement (Song et al., 2008). Binarization is also an important topic in the research of syntax-based SMT. A synchronous 363 binarization method is proposed in (Zhang et al., 2006) whose basic idea is to build a left-heavy binary synchronous tr"
D09-1038,P05-1033,0,0.0773653,"owever, as shown by Chiang (2007), SCFGbased decoding with an integrated n-gram language model still has a time complexity of ?(?3 ? 4(?−1) ), where m is the source sentence length, and ? is the vocabulary size of the language model. Although it is not exponential in theory, the actual complexity can still be very high in practice. Here is an example extracted from real data. Given the following SCFG rule: VP → VB NP 会 JJR , VB NP will be JJR Introduction Recently Statistical Machine Translation (SMT) systems based on Synchronous Context Free Grammar (SCFG) have been extensively investigated (Chiang, 2005; Galley et al., 2004; Galley et al., 2006) and have achieved state-of-the-art performance. In these systems, machine translation decoding is cast as a synchronous parsing task. Because general SCFG parsing is an NPhard problem (Satta and Peserico, 2005), practical SMT decoders based on SCFG parsing requires an equivalent binary SCFG that is directly learned from training data to achieve polynomial time complexity using the CKY algorithm (Kasami, 1965; Younger, 1967) borrowed from CFG parsing techniques. Zhang et al. (2006) proposed synchronous binarization, a principled method to we can obtai"
D09-1038,J07-2003,0,0.862926,"method in Zhang et al. (2006) on the NIST machine translation evaluation tasks. 1 Microsoft Research Asia Sigma Center Beijing, China, 100080 muli@microsoft.com dozhang@microsoft.com mingzhou@microsoft.com binarize an SCFG in such a way that both the source-side and target-side virtual non-terminals have contiguous spans. This property of synchronous binarization guarantees the polynomial time complexity of SCFG parsers even when an n-gram language model is integrated, which has been proved to be one of the keys to the success of a string-to-tree syntax-based SMT system. However, as shown by Chiang (2007), SCFGbased decoding with an integrated n-gram language model still has a time complexity of ?(?3 ? 4(?−1) ), where m is the source sentence length, and ? is the vocabulary size of the language model. Although it is not exponential in theory, the actual complexity can still be very high in practice. Here is an example extracted from real data. Given the following SCFG rule: VP → VB NP 会 JJR , VB NP will be JJR Introduction Recently Statistical Machine Translation (SMT) systems based on Synchronous Context Free Grammar (SCFG) have been extensively investigated (Chiang, 2005; Galley et al., 2004"
D09-1038,P06-1121,0,0.465965,"SCFGbased decoding with an integrated n-gram language model still has a time complexity of ?(?3 ? 4(?−1) ), where m is the source sentence length, and ? is the vocabulary size of the language model. Although it is not exponential in theory, the actual complexity can still be very high in practice. Here is an example extracted from real data. Given the following SCFG rule: VP → VB NP 会 JJR , VB NP will be JJR Introduction Recently Statistical Machine Translation (SMT) systems based on Synchronous Context Free Grammar (SCFG) have been extensively investigated (Chiang, 2005; Galley et al., 2004; Galley et al., 2006) and have achieved state-of-the-art performance. In these systems, machine translation decoding is cast as a synchronous parsing task. Because general SCFG parsing is an NPhard problem (Satta and Peserico, 2005), practical SMT decoders based on SCFG parsing requires an equivalent binary SCFG that is directly learned from training data to achieve polynomial time complexity using the CKY algorithm (Kasami, 1965; Younger, 1967) borrowed from CFG parsing techniques. Zhang et al. (2006) proposed synchronous binarization, a principled method to we can obtain a set of equivalent binary rules using th"
D09-1038,W07-0405,0,0.0851011,"oblem, and the accuracy of parsing does not rely on the binarization method heavily. Thus, many efforts on the binarization in parsing are made for the efficiency improvement instead of the accuracy improvement (Song et al., 2008). Binarization is also an important topic in the research of syntax-based SMT. A synchronous 363 binarization method is proposed in (Zhang et al., 2006) whose basic idea is to build a left-heavy binary synchronous tree (Shapiro and Stephens, 1991) with a left-to-right shift-reduce algorithm. Target-side binarization is another binarization method which is proposed by Huang (2007). It works in a left-to-right way on the target language side. Although this method is comparatively easy to be implemented, it just achieves the same performance as the synchronous binarization method (Zhang et al., 2006) for syntaxbased SMT systems. In addition, it cannot be easily integrated into the decoding of some syntax-based models (Galley et al., 2004; Marcu et al., 2006), because it does not guarantee contiguous spans on the source language side. the ? ?ℎ binary rule in ℬ ?? . Figure 3 illustrates the meanings of these notations with a sample grammar. G R1 : VP → VB NP 会 JJR , VB NP"
D09-1038,W04-3250,0,0.186026,"Missing"
D09-1038,W06-1606,0,0.331083,"2006) whose basic idea is to build a left-heavy binary synchronous tree (Shapiro and Stephens, 1991) with a left-to-right shift-reduce algorithm. Target-side binarization is another binarization method which is proposed by Huang (2007). It works in a left-to-right way on the target language side. Although this method is comparatively easy to be implemented, it just achieves the same performance as the synchronous binarization method (Zhang et al., 2006) for syntaxbased SMT systems. In addition, it cannot be easily integrated into the decoding of some syntax-based models (Galley et al., 2004; Marcu et al., 2006), because it does not guarantee contiguous spans on the source language side. the ? ?ℎ binary rule in ℬ ?? . Figure 3 illustrates the meanings of these notations with a sample grammar. G R1 : VP → VB NP 会 JJR , VB NP will be JJR R2 : S → NP 会 VP , NP will VP binarization G’ (R1) v11 : VP → V12 JJR , V12 JJR v12 : V12 → VB V13 , VB V13 v13 : V13 → NP 会 , NP will be (R2) v21 : S → V22 VP , v22 : V22 → NP 会 , rule bucket v11 3 Synchronous Binarization Optimization by Cost Reduction As discussed in Section 1, binarizing an SCFG in a fixed (left-heavy) way (Zhang et al., 2006) may lead to a large n"
D09-1038,H05-1101,0,0.0456339,"h it is not exponential in theory, the actual complexity can still be very high in practice. Here is an example extracted from real data. Given the following SCFG rule: VP → VB NP 会 JJR , VB NP will be JJR Introduction Recently Statistical Machine Translation (SMT) systems based on Synchronous Context Free Grammar (SCFG) have been extensively investigated (Chiang, 2005; Galley et al., 2004; Galley et al., 2006) and have achieved state-of-the-art performance. In these systems, machine translation decoding is cast as a synchronous parsing task. Because general SCFG parsing is an NPhard problem (Satta and Peserico, 2005), practical SMT decoders based on SCFG parsing requires an equivalent binary SCFG that is directly learned from training data to achieve polynomial time complexity using the CKY algorithm (Kasami, 1965; Younger, 1967) borrowed from CFG parsing techniques. Zhang et al. (2006) proposed synchronous binarization, a principled method to we can obtain a set of equivalent binary rules using the synchronous binarization method (Zhang et al., 2006) as follows: VP → V1 JJR , V1 → VB V2 , V2 → NP 会 , V1 JJR VB V2 NP will be This binarization is shown with the solid lines as binarization (a) in Figure 1."
D09-1038,D08-1018,0,0.0770688,"NIST 2008 Chinese-to-English evaluation data sets. 2 Related Work The problem of binarization originates from the parsing problem in which several binarization methods are studied such as left/right binarization (Charniak et al., 1998; Tsuruoka and Tsujii, 2004) and head binarization (Charniak et al., 2006). Generally, the pruning issue in SMT decoding is unnecessary for the parsing problem, and the accuracy of parsing does not rely on the binarization method heavily. Thus, many efforts on the binarization in parsing are made for the efficiency improvement instead of the accuracy improvement (Song et al., 2008). Binarization is also an important topic in the research of syntax-based SMT. A synchronous 363 binarization method is proposed in (Zhang et al., 2006) whose basic idea is to build a left-heavy binary synchronous tree (Shapiro and Stephens, 1991) with a left-to-right shift-reduce algorithm. Target-side binarization is another binarization method which is proposed by Huang (2007). It works in a left-to-right way on the target language side. Although this method is comparatively easy to be implemented, it just achieves the same performance as the synchronous binarization method (Zhang et al., 2"
D09-1038,D07-1078,0,0.038172,"r development data set comes from NIST2003 evaluation data in which the sentences of more than 20 words are excluded to speed up the Minimum Error Rate Training (MERT). The test data sets are the NIST evaluation sets of 2005 and 2008. Our string-to-tree SMT system is built based on the work of (Galley et al., 2006; Marcu et al., 2006), where both the minimal GHKM and SPMT rules are extracted from the training corpus, and the composed rules are generated by combining two or three minimal GHKM and SPMT rules. Before the rule extraction, we also binarize the parse trees on the English side using Wang et al. (2007) „s method to increase the coverage of GHKM and SPMT rules. There are totally 4.26M rules after the low frequency rules are filtered out. The pruning strategy is similar to the cube pruning described in (Chiang, 2007). To achieve acceptable translation speed, the beam size is set to 50 by default. The baseline system is based on the synchronous binarization (Zhang et al., 2006). 4.2 Binarization Schemes Besides the baseline (Zhang et al., 2006) and iterative cost reduction binarization methods, we also perform right-heavy and random synchronous binarizations for comparison. In this paper, the"
D09-1038,N06-1033,0,0.250451,"ing polynomial time complexity of decoding for SCFG parsing based machine translation systems. In this paper, we first investigate the excess edge competition issue caused by a leftheavy binary SCFG derived with the method of Zhang et al. (2006). Then we propose a new binarization method to mitigate the problem by exploring other alternative equivalent binary SCFGs. We present an algorithm that iteratively improves the resulting binary SCFG, and empirically show that our method can improve a string-to-tree statistical machine translations system based on the synchronous binarization method in Zhang et al. (2006) on the NIST machine translation evaluation tasks. 1 Microsoft Research Asia Sigma Center Beijing, China, 100080 muli@microsoft.com dozhang@microsoft.com mingzhou@microsoft.com binarize an SCFG in such a way that both the source-side and target-side virtual non-terminals have contiguous spans. This property of synchronous binarization guarantees the polynomial time complexity of SCFG parsers even when an n-gram language model is integrated, which has been proved to be one of the keys to the success of a string-to-tree syntax-based SMT system. However, as shown by Chiang (2007), SCFGbased decod"
D09-1038,N04-1035,0,\N,Missing
D09-1038,N06-1022,0,\N,Missing
D09-1114,D08-1011,0,0.0122079,"mbination methods are more complicated and could produce new translations different from any translations in the input (Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006; Sim et al., 2007). Among all the factors contributing to the success of system combination, there is no doubt that the availability of multiple machine translation systems is an indispensable premise. Although various approaches to SMT system combination have been explored, including enhanced combination model structure (Rosti et al., 2007), better word alignment between translations (Ayan et al., 2008; He et al., 2008) and improved confusion network construction (Rosti et al., 2008), most previous work simply used the ensemble of SMT systems based on different models and paradigms at hand and did not tackle the issue of how to obtain the ensemble in a principled way. To our knowledge the only work discussed this problem is Macherey and Och (2007), in which they experimented with building different SMT systems by varying one or more sub-models (i.e. translation model or distortion model) of an existing SMT system, and observed that changes in early-stage model training introduced most diversities in translat"
D09-1114,2008.amta-srw.3,0,0.173771,". As a result of the increasing numbers of available machine translation systems, studies on system combination have been drawing more and more attention in SMT research. There have been many successful attempts to combine outputs from multiple machine translation systems to further improve translation quality. A system combination model usually takes nbest translations of single systems as input, and depending on the combination strategy, different methods can be used. Sentence-level combination methods directly select hypotheses from original outputs of single SMT systems (Sim et al., 2007; Hildebrand and Vogel, 2008), while phrase-level or word–level combination methods are more complicated and could produce new translations different from any translations in the input (Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006; Sim et al., 2007). Among all the factors contributing to the success of system combination, there is no doubt that the availability of multiple machine translation systems is an indispensable premise. Although various approaches to SMT system combination have been explored, including enhanced combination model structure (Rosti et al., 2007), better word alignment betw"
D09-1114,P05-1033,0,0.863722,"e principles to determine the feature sets for derived systems, and present in detail the system combination model used in our work. Evaluation is performed on the data sets for NIST 2004 and NIST 2005 Chinese-to-English machine translation tasks. Experimental results show that our method can bring significant improvements to baseline systems with state-of-the-art performance. 1 Introduction Research on Statistical Machine Translation (SMT) has shown substantial progress in recent years. Since the success of phrase-based methods (Och and Ney, 2004; Koehn, 2004), models based on formal syntax (Chiang, 2005) or linguistic syntax (Liu et al., 2006; Marcu et al., 2006) have also achieved state-of-the-art performance. As a result of the increasing numbers of available machine translation systems, studies on system combination have been drawing more and more attention in SMT research. There have been many successful attempts to combine outputs from multiple machine translation systems to further improve translation quality. A system combination model usually takes nbest translations of single systems as input, and depending on the combination strategy, different methods can be used. Sentence-level co"
D09-1114,C08-1005,0,0.0133609,"el or word–level combination methods are more complicated and could produce new translations different from any translations in the input (Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006; Sim et al., 2007). Among all the factors contributing to the success of system combination, there is no doubt that the availability of multiple machine translation systems is an indispensable premise. Although various approaches to SMT system combination have been explored, including enhanced combination model structure (Rosti et al., 2007), better word alignment between translations (Ayan et al., 2008; He et al., 2008) and improved confusion network construction (Rosti et al., 2008), most previous work simply used the ensemble of SMT systems based on different models and paradigms at hand and did not tackle the issue of how to obtain the ensemble in a principled way. To our knowledge the only work discussed this problem is Macherey and Och (2007), in which they experimented with building different SMT systems by varying one or more sub-models (i.e. translation model or distortion model) of an existing SMT system, and observed that changes in early-stage model training introduced most diver"
D09-1114,N07-2015,0,0.183044,"Missing"
D09-1114,P05-3026,0,0.0288038,"pts to combine outputs from multiple machine translation systems to further improve translation quality. A system combination model usually takes nbest translations of single systems as input, and depending on the combination strategy, different methods can be used. Sentence-level combination methods directly select hypotheses from original outputs of single SMT systems (Sim et al., 2007; Hildebrand and Vogel, 2008), while phrase-level or word–level combination methods are more complicated and could produce new translations different from any translations in the input (Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006; Sim et al., 2007). Among all the factors contributing to the success of system combination, there is no doubt that the availability of multiple machine translation systems is an indispensable premise. Although various approaches to SMT system combination have been explored, including enhanced combination model structure (Rosti et al., 2007), better word alignment between translations (Ayan et al., 2008; He et al., 2008) and improved confusion network construction (Rosti et al., 2008), most previous work simply used the ensemble of SMT systems based on different models a"
D09-1114,W04-3250,0,0.482397,"typically a subset of it). We will discuss the principles to determine the feature sets for derived systems, and present in detail the system combination model used in our work. Evaluation is performed on the data sets for NIST 2004 and NIST 2005 Chinese-to-English machine translation tasks. Experimental results show that our method can bring significant improvements to baseline systems with state-of-the-art performance. 1 Introduction Research on Statistical Machine Translation (SMT) has shown substantial progress in recent years. Since the success of phrase-based methods (Och and Ney, 2004; Koehn, 2004), models based on formal syntax (Chiang, 2005) or linguistic syntax (Liu et al., 2006; Marcu et al., 2006) have also achieved state-of-the-art performance. As a result of the increasing numbers of available machine translation systems, studies on system combination have been drawing more and more attention in SMT research. There have been many successful attempts to combine outputs from multiple machine translation systems to further improve translation quality. A system combination model usually takes nbest translations of single systems as input, and depending on the combination strategy, di"
D09-1114,P09-1066,1,0.820368,"re to be matched in computing ?(?, ℋ(?)). Since we also adopt a linear scoring function in Equation (3), the feature weights of our combination model can also be tuned on a development data set to optimize the specified evaluation metrics using the standard Minimum Error Rate Training (MERT) algorithm (Och 2003). Our method is similar to the work proposed by Hildebrand and Vogel (2008). However, except the language model and translation length, we only use intra-hypothesis n-gram agreement features as Hildebrand and Vogel did and use additional intra-hypothesis n-gram disagreement features as Li et al. (2009) did in their co-decoding method. 4 ?∈ℋ ? ?+ (?, ℋ ? ) = tions, we also introduce a set of n-gram disagreement features in the combination model: 4.1 Experiments Data Experiments were conducted on the NIST evaluation sets of 2004 (MT04) and 2005 (MT05) for Chinese-to-English translation tasks. Both corpora provide 4 reference translations per source sentence. Parameters were tuned with MERT algorithm (Och, 2003) on the NIST evaluation set of 2003 (MT03) for both the baseline systems and the system combination model. Translation performance was measured in terms of caseinsensitive NIST version"
D09-1114,2006.amta-papers.11,0,0.0160705,"ization project settles in Zhoushan China 's largest desalination project in Zhoushan China 's largest sea water desalination project in Zhoushan Chinese 海水 淡化 海水 淡化 English desalination sea water desalination ? ?? 0.4000 0.1748 0.0923 Table 2: Parameters of related phrases for examples in Table 1. The second aspect motivating our work comes from the subspace learning method in machine learning literature (Ho, 1998), in which an ensemble of classifiers are trained on subspaces of the full feature space, and final classification results are based on the vote of all classifiers in the ensemble. Lopez and Resnik (2006) also showed that feature engineering could be used to overcome deficiencies of poor alignment. To illustrate the usefulness of feature subspace in the SMT task, we start with the example shown in Table 1. In the example, the Chinese source sentence is translated with two settings of a hierarchical phrase-based system (Chiang, 2005). In the default setting all the features are used as usual in the decoder, and we find that the translation of the Chinese word 海水 (sea water) is missing in the output. This can be explained with the data shown in Table 2. Because of noises and word alignment error"
D09-1114,P06-1077,0,0.0298112,"e sets for derived systems, and present in detail the system combination model used in our work. Evaluation is performed on the data sets for NIST 2004 and NIST 2005 Chinese-to-English machine translation tasks. Experimental results show that our method can bring significant improvements to baseline systems with state-of-the-art performance. 1 Introduction Research on Statistical Machine Translation (SMT) has shown substantial progress in recent years. Since the success of phrase-based methods (Och and Ney, 2004; Koehn, 2004), models based on formal syntax (Chiang, 2005) or linguistic syntax (Liu et al., 2006; Marcu et al., 2006) have also achieved state-of-the-art performance. As a result of the increasing numbers of available machine translation systems, studies on system combination have been drawing more and more attention in SMT research. There have been many successful attempts to combine outputs from multiple machine translation systems to further improve translation quality. A system combination model usually takes nbest translations of single systems as input, and depending on the combination strategy, different methods can be used. Sentence-level combination methods directly select hypot"
D09-1114,W06-1606,0,0.0444251,"systems, and present in detail the system combination model used in our work. Evaluation is performed on the data sets for NIST 2004 and NIST 2005 Chinese-to-English machine translation tasks. Experimental results show that our method can bring significant improvements to baseline systems with state-of-the-art performance. 1 Introduction Research on Statistical Machine Translation (SMT) has shown substantial progress in recent years. Since the success of phrase-based methods (Och and Ney, 2004; Koehn, 2004), models based on formal syntax (Chiang, 2005) or linguistic syntax (Liu et al., 2006; Marcu et al., 2006) have also achieved state-of-the-art performance. As a result of the increasing numbers of available machine translation systems, studies on system combination have been drawing more and more attention in SMT research. There have been many successful attempts to combine outputs from multiple machine translation systems to further improve translation quality. A system combination model usually takes nbest translations of single systems as input, and depending on the combination strategy, different methods can be used. Sentence-level combination methods directly select hypotheses from original o"
D09-1114,D07-1105,0,0.0609944,"multiple machine translation systems is an indispensable premise. Although various approaches to SMT system combination have been explored, including enhanced combination model structure (Rosti et al., 2007), better word alignment between translations (Ayan et al., 2008; He et al., 2008) and improved confusion network construction (Rosti et al., 2008), most previous work simply used the ensemble of SMT systems based on different models and paradigms at hand and did not tackle the issue of how to obtain the ensemble in a principled way. To our knowledge the only work discussed this problem is Macherey and Och (2007), in which they experimented with building different SMT systems by varying one or more sub-models (i.e. translation model or distortion model) of an existing SMT system, and observed that changes in early-stage model training introduced most diversities in translation outputs. In this paper, we address the problem of building an ensemble of diversified machine translation systems from a single translation engine for system combination. In particular, we propose a novel Feature Subspace method for the ensemble construction based on any baseline SMT model which can be formulated as a standard l"
D09-1114,E06-1005,0,0.0510437,"multiple machine translation systems to further improve translation quality. A system combination model usually takes nbest translations of single systems as input, and depending on the combination strategy, different methods can be used. Sentence-level combination methods directly select hypotheses from original outputs of single SMT systems (Sim et al., 2007; Hildebrand and Vogel, 2008), while phrase-level or word–level combination methods are more complicated and could produce new translations different from any translations in the input (Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006; Sim et al., 2007). Among all the factors contributing to the success of system combination, there is no doubt that the availability of multiple machine translation systems is an indispensable premise. Although various approaches to SMT system combination have been explored, including enhanced combination model structure (Rosti et al., 2007), better word alignment between translations (Ayan et al., 2008; He et al., 2008) and improved confusion network construction (Rosti et al., 2008), most previous work simply used the ensemble of SMT systems based on different models and paradigms at hand a"
D09-1114,P02-1038,0,0.183819,", for a specific sentence some individual systems could generate better translations. It is expected that employing an ensemble of subspace-based systems and making use of consensus between them will outperform the baseline system. 3 Feature Subspace Method for SMT System Ensemble Construction In this section, we will present in detail the method for systematically deriving SMT systems from a standard linear SMT model based on feature subspaces for system combination. 3.1 SMT System Ensemble Generation Nowadays most of the state-of-the-art SMT systems are based on linear models as proposed in Och and Ney (2002). Let ? (?, ?) be a feature function, and ?? be its weight, an SMT model ? can be formally written as: ? ∗ = argmax ? ?? ? (?, ?) (1) ? Noticing that Equation (1) is a general formulation independent of any specific features, technically for any subset of features used in ?, a new SMT system can be constructed based on it, which we call a sub-system. Next we will use Ω to denote the full feature space defined by the entire set of features used in ?, and ? ⊆ Ω is a feature subset that belongs to ?(Ω), the power set of Ω. The derived subsystem based on subset ? ⊆ Ω is denoted by ?? . Although"
D09-1114,P03-1021,0,0.0402856,"occurrences of n-grams of ? in ? ′ : ?? ?, ? ′ = ? −?+1 ?=1 ?(???+?−1 , ? ′ ) ?− (?, ℋ ? ) = ( ? − ? + 1 − ?? (?, ? ′ )) ? ′ ∈ℋ ? ,? ′ ≠? (6) Because each order of n-gram match introduces two features, the total number of features in the combination model will be 2? + 2 if ? orders of n-gram are to be matched in computing ?(?, ℋ(?)). Since we also adopt a linear scoring function in Equation (3), the feature weights of our combination model can also be tuned on a development data set to optimize the specified evaluation metrics using the standard Minimum Error Rate Training (MERT) algorithm (Och 2003). Our method is similar to the work proposed by Hildebrand and Vogel (2008). However, except the language model and translation length, we only use intra-hypothesis n-gram agreement features as Hildebrand and Vogel did and use additional intra-hypothesis n-gram disagreement features as Li et al. (2009) did in their co-decoding method. 4 ?∈ℋ ? ?+ (?, ℋ ? ) = tions, we also introduce a set of n-gram disagreement features in the combination model: 4.1 Experiments Data Experiments were conducted on the NIST evaluation sets of 2004 (MT04) and 2005 (MT05) for Chinese-to-English translation tasks. B"
D09-1114,J03-1002,0,0.00495516,"∙,∙) is the indicator function ? ???+?−1 , ? ′ is 1 when the n-gram ???+?−1 appears in ? ′ , otherwise it is 0. In order to give the combination model an opportunity to penalize long but inaccurate transla1099 Data set #Sentences MT03 (dev) 919 MT04 (test) 1,788 MT05 (test) 1,082 #Words 23,782 47,762 29,258 Table 3: Data set statistics. We use the parallel data available for the NIST 2008 constrained track of Chinese-toEnglish machine translation task as bilingual training data, which contains 5.1M sentence pairs, 128M Chinese words and 147M English words after pre-processing. GIZA++ toolkit (Och and Ney, 2003) is used to perform word alignment in both directions with default settings, and the intersect-diag-grow method is used to generate symmetric word alignment refinement. The language model used for all systems is a 5-gram model trained with the English part of bilingual data and Xinhua portion of LDC English Gigaword corpus version 3. In experiments, multiple language model features with the order ranging from 2 to 5 can be easily obtained from the 5gram one without retraining. 4.2 System Description Theoretically our method is applicable to all linear model –based SMT systems. In our experimen"
D09-1114,J04-4002,0,0.0458893,"he baseline model (typically a subset of it). We will discuss the principles to determine the feature sets for derived systems, and present in detail the system combination model used in our work. Evaluation is performed on the data sets for NIST 2004 and NIST 2005 Chinese-to-English machine translation tasks. Experimental results show that our method can bring significant improvements to baseline systems with state-of-the-art performance. 1 Introduction Research on Statistical Machine Translation (SMT) has shown substantial progress in recent years. Since the success of phrase-based methods (Och and Ney, 2004; Koehn, 2004), models based on formal syntax (Chiang, 2005) or linguistic syntax (Liu et al., 2006; Marcu et al., 2006) have also achieved state-of-the-art performance. As a result of the increasing numbers of available machine translation systems, studies on system combination have been drawing more and more attention in SMT research. There have been many successful attempts to combine outputs from multiple machine translation systems to further improve translation quality. A system combination model usually takes nbest translations of single systems as input, and depending on the combinatio"
D09-1114,N07-1029,0,0.0865499,"(Sim et al., 2007; Hildebrand and Vogel, 2008), while phrase-level or word–level combination methods are more complicated and could produce new translations different from any translations in the input (Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006; Sim et al., 2007). Among all the factors contributing to the success of system combination, there is no doubt that the availability of multiple machine translation systems is an indispensable premise. Although various approaches to SMT system combination have been explored, including enhanced combination model structure (Rosti et al., 2007), better word alignment between translations (Ayan et al., 2008; He et al., 2008) and improved confusion network construction (Rosti et al., 2008), most previous work simply used the ensemble of SMT systems based on different models and paradigms at hand and did not tackle the issue of how to obtain the ensemble in a principled way. To our knowledge the only work discussed this problem is Macherey and Och (2007), in which they experimented with building different SMT systems by varying one or more sub-models (i.e. translation model or distortion model) of an existing SMT system, and observed t"
D09-1114,P07-1040,0,0.17902,"(Sim et al., 2007; Hildebrand and Vogel, 2008), while phrase-level or word–level combination methods are more complicated and could produce new translations different from any translations in the input (Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006; Sim et al., 2007). Among all the factors contributing to the success of system combination, there is no doubt that the availability of multiple machine translation systems is an indispensable premise. Although various approaches to SMT system combination have been explored, including enhanced combination model structure (Rosti et al., 2007), better word alignment between translations (Ayan et al., 2008; He et al., 2008) and improved confusion network construction (Rosti et al., 2008), most previous work simply used the ensemble of SMT systems based on different models and paradigms at hand and did not tackle the issue of how to obtain the ensemble in a principled way. To our knowledge the only work discussed this problem is Macherey and Och (2007), in which they experimented with building different SMT systems by varying one or more sub-models (i.e. translation model or distortion model) of an existing SMT system, and observed t"
D09-1114,W08-0329,0,0.0169325,"ranslations different from any translations in the input (Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006; Sim et al., 2007). Among all the factors contributing to the success of system combination, there is no doubt that the availability of multiple machine translation systems is an indispensable premise. Although various approaches to SMT system combination have been explored, including enhanced combination model structure (Rosti et al., 2007), better word alignment between translations (Ayan et al., 2008; He et al., 2008) and improved confusion network construction (Rosti et al., 2008), most previous work simply used the ensemble of SMT systems based on different models and paradigms at hand and did not tackle the issue of how to obtain the ensemble in a principled way. To our knowledge the only work discussed this problem is Macherey and Och (2007), in which they experimented with building different SMT systems by varying one or more sub-models (i.e. translation model or distortion model) of an existing SMT system, and observed that changes in early-stage model training introduced most diversities in translation outputs. In this paper, we address the problem of building an"
D09-1114,P06-1066,0,0.0451678,"veloped systems are used to validate our method. The first one (SYS1) is a system based on the hierarchical phrase-based model as proposed in (Chiang, 2005). Phrasal rules are extracted from all bilingual sentence pairs, while hierarchical rules with variables are extracted from selected data sets including LDC2003E14, LDC2003E07, LDC2005T06 and LDC2005T10, which contain around 350,000 sentence pairs, 8.8M Chinese words and 10.3M English words. The second one (SYS2) is a reimplementation of a phrase-based decoder with lexicalized reordering model based on maximum entropy principle proposed by Xiong et al. (2006). All bilingual data are used to extract phrases up to length 3 on the source side. In following experiments, we only consider removing common features shared by both baseline systems for feature subspace generation. Rule penalty feature and lexicalized reordering feature, which are particular to SYS1 and SYS2, are not used. We list the features in consideration as follows:  PEF and PFE: phrase translation probabilities ? ? ? and ? ? ?  PEFLEX and PFELEX: lexical weights ???? ? ? and ???? ? ?  PP: phrase penalty  WP: word penalty  BLP: bi-lexicon pair counting how many entries of a conven"
D09-1114,2005.eamt-1.20,0,\N,Missing
D12-1041,W05-0823,0,0.0512393,"Missing"
D12-1041,P11-1103,0,0.0197714,"research, the MERT algorithm (Och, 2003) is used to tune feature weights in MRM. Due to the fact that all FDTs of each sentence pair share identical translation, we cannot use BLEU as the error criterion any more. Instead, alignment F-score is used as the alternative. We will show in Section 5 that after the inference step, alignment quality can be improved by replacing original alignments of each sentence pair with alignments induced from its 1-best FDT. Future work could experiment with other error criterions, such as reordering-based loss functions (Birch et al., 2010; Talbot et al., 2011; Birch and Osborne, 2011) or span F1 (DeNero and Uszkoreit, 2011). • hr (G), number of translation rules used in G. 3 Training in Phrase-based SMT • hd (G), structure-based score of G. For FDTs generated by phrase-based paradigms, it can be computed by distortion models; while for FDTs generated by syntax-based paradigms, it can be computed by either parsing models or syntactic LMs (Charniak et al., 2003). As the first step in this line of research, we explore the usage of FDT-based model training method in a phrase-based SMT system (Xiong et al., 2006), which employs Bracketing Transduction Grammar (BTG) (Wu, 1997) t"
D12-1041,2003.mtsummit-papers.6,0,0.0464169,"gnments of each sentence pair with alignments induced from its 1-best FDT. Future work could experiment with other error criterions, such as reordering-based loss functions (Birch et al., 2010; Talbot et al., 2011; Birch and Osborne, 2011) or span F1 (DeNero and Uszkoreit, 2011). • hr (G), number of translation rules used in G. 3 Training in Phrase-based SMT • hd (G), structure-based score of G. For FDTs generated by phrase-based paradigms, it can be computed by distortion models; while for FDTs generated by syntax-based paradigms, it can be computed by either parsing models or syntactic LMs (Charniak et al., 2003). As the first step in this line of research, we explore the usage of FDT-based model training method in a phrase-based SMT system (Xiong et al., 2006), which employs Bracketing Transduction Grammar (BTG) (Wu, 1997) to parse parallel sentences. The reason of choosing this system is due to the prominent advantages of BTG, such as the simplicity of the grammar and the good coverage of syntactic diversities between different language pairs. We first describe more details of FDTs under BTG. Then, four FDT-based component models are presented. The overfitting issue in the generation step can be all"
D12-1041,P05-1066,0,0.0424994,"Work 4.1 Forced Decoding/Alignment Schwartz (2008) used forced decoding to leverage multilingual corpus to improve translation quality; Shen et al. (2008) used forced alignment to train a better phrase segmentation model; Wuebker et al. (2010) used forced alignment to re-estimate translation probabilities using a leaving-one-out strategy. We consider the usage of FD in Section 2.1 to be a direct extension of these approaches, but one that generates FDTs for parallel data rather than focusing on phrase segmentation or probability estimation. 4.2 Pre-reordering Pre-reordering (PRO) techniques (Collins et al., 2005; Xu et al., 2009; Genzel et al., 2010; Lee et al., 2010) used features from syntactic parse trees to reorder source sentences at training and translation time. A parser is often indispensable to provide syntactic information for such methods. Recently, DeNero and Uszkoreit (2011) proposed an approach that induced parse trees automatically from wordaligned training corpus to perform PRO for a phrasebased SMT system, instead of relying on treebanks. First, binary parse trees are induced from wordaligned training corpus. Based on them, a monolingual parsing model and a tree reordering model are"
D12-1041,D11-1018,0,0.0476346,"03) is used to tune feature weights in MRM. Due to the fact that all FDTs of each sentence pair share identical translation, we cannot use BLEU as the error criterion any more. Instead, alignment F-score is used as the alternative. We will show in Section 5 that after the inference step, alignment quality can be improved by replacing original alignments of each sentence pair with alignments induced from its 1-best FDT. Future work could experiment with other error criterions, such as reordering-based loss functions (Birch et al., 2010; Talbot et al., 2011; Birch and Osborne, 2011) or span F1 (DeNero and Uszkoreit, 2011). • hr (G), number of translation rules used in G. 3 Training in Phrase-based SMT • hd (G), structure-based score of G. For FDTs generated by phrase-based paradigms, it can be computed by distortion models; while for FDTs generated by syntax-based paradigms, it can be computed by either parsing models or syntactic LMs (Charniak et al., 2003). As the first step in this line of research, we explore the usage of FDT-based model training method in a phrase-based SMT system (Xiong et al., 2006), which employs Bracketing Transduction Grammar (BTG) (Wu, 1997) to parse parallel sentences. The reason o"
D12-1041,2010.amta-papers.22,0,0.0348025,"source sentences at training and translation time. A parser is often indispensable to provide syntactic information for such methods. Recently, DeNero and Uszkoreit (2011) proposed an approach that induced parse trees automatically from wordaligned training corpus to perform PRO for a phrasebased SMT system, instead of relying on treebanks. First, binary parse trees are induced from wordaligned training corpus. Based on them, a monolingual parsing model and a tree reordering model are trained to pre-reorder source words into the targetlanguage-like order. Their work is distinct from ours 450 Feng et al. (2010) proposed an SLM in a phrasebased SMT system. They used it as a reordering feature in the sense that it helped the decoder to find correct decoding sequences. The difference between their model and our FDT-SLM is that, in their work, the reordered source sequences are extracted based on word alignments only; while in our FDT-SLM, such sequences are obtained based on FDTs. Quirk and Menezes (2006) proposed a Minimal Translation Unit (MTU) -based sequence model and used it in their treelet system; Vaswani et al. (2011) proposed a rule Markov model to capture dependencies between minimal rules fo"
D12-1041,P06-1097,0,0.0289981,",e}∈C fracH(f,e) (f¯,¯ e)} (5) In this paper, the sigmoid function is used to make sure that the feature values of different translation rules are in a proper value range. • word pair translation probabilities trained from IBM models (Brown et al., 1993); • log-likelihood ratio (Moore, 2005); • conditional link probability (Moore, 2005); • count of unlinked words; • counts of inversion and concatenation. Many alignment-inspired features can be used in MRM. This paper only uses those commonly-used ones that have already been proved useful in many previous work (Moore, 2005; Moore et al., 2006; Fraser and Marcu, 2006; Liu et al., 2010). Following the common practice in SMT research, the MERT algorithm (Och, 2003) is used to tune feature weights in MRM. Due to the fact that all FDTs of each sentence pair share identical translation, we cannot use BLEU as the error criterion any more. Instead, alignment F-score is used as the alternative. We will show in Section 5 that after the inference step, alignment quality can be improved by replacing original alignments of each sentence pair with alignments induced from its 1-best FDT. Future work could experiment with other error criterions, such as reordering-based"
D12-1041,C10-1043,0,0.0217631,". It can provide an averaged 1.2 BLEU improvements on these three evaluation data sets. BLEU BTG FDT-TM FDT-DM FDT-SLM FDT-RSM FDT-ALL PRO dev 20.60 21.21 21.13 20.84 21.07 21.83 21.89 test-1 20.27 20.71(+0.44) 20.79(+0.52) 20.50(+0.23) 20.75(+0.48) 21.34(+1.07) 21.81 test-2 13.15 13.98(+0.83) 14.25(+1.10) 13.36(+0.21) 13.59(+0.44) 14.46(+1.31) 14.69 Table 1: FDT-based model training on E-J task. Pre-reordering (PRO) is often used on language pairs, e.g. English and Japanese, with very different word orders. So we compare our method with PRO as well. We re-implement the PRO method proposed by Genzel (2010) and show its results in Table 1. On dev and test-2, FDT-ALL performs comparable to PRO, with no syntactic information needed at all. 5.4 Translation Quality on C-E Task We then evaluate the effectiveness of our FDT-based model training approach on C-E translation task, and present evaluation results in Table 2, from which we can see significant improvements as well. BLEU BTG FDT-TM FDT-DM FDT-SLM FDT-RSM FDT-ALL MT03 38.73 39.14 39.27 38.97 39.06 39.59 MT05 38.01 38.31(+0.30) 38.56(+0.55) 38.22(+0.21) 38.33(+0.32) 38.72(+0.71) MT08 23.78 24.30(+0.52) 24.50(+0.72) 24.04(+0.26) 24.13(+0.35) 24."
D12-1041,W05-1506,0,0.0391167,"se-based SMT system in Section 3. 2.1 Generation We first describe how to generate multiple FDTs for each sentence pair in training corpus C based on the forced decoding (FD) technique, which performs via the following four steps: 1. Train component models needed for a specific SMT paradigm M based on training corpus C; 2. Perform MERT on the development data set to obtain a set of optimized feature weights; 3. For each {f, e} ∈ C, translate f into accurate e based on M, component models trained in step 1, and feature weights optimized in step 2; 4. For each {f, e} ∈ C, output the hypergraph (Huang and Chiang, 2005) H(f, e) generated in step 3 as its FDT space. In step 3: (1) all partial hypotheses that do not match any sequence in e will be discarded; (2) derivations covering identical source and target words but with different alignments will be kept as different partial candidates, as they can produce different FDTs for 446 the same sentence pair. For each {f, e}, the probability of each G ∈ H(f, e) is computed as: p(G|H(f, e)) = ∑ exp{ψ(G)} ′ G ′ ∈H(f,e) exp{ψ(G )} (1) where ψ(G) is the FD model score assigned to G. For each sentence pair, different alignment candidates can be induced from its differ"
D12-1041,C10-1071,0,0.0183537,"ced decoding to leverage multilingual corpus to improve translation quality; Shen et al. (2008) used forced alignment to train a better phrase segmentation model; Wuebker et al. (2010) used forced alignment to re-estimate translation probabilities using a leaving-one-out strategy. We consider the usage of FD in Section 2.1 to be a direct extension of these approaches, but one that generates FDTs for parallel data rather than focusing on phrase segmentation or probability estimation. 4.2 Pre-reordering Pre-reordering (PRO) techniques (Collins et al., 2005; Xu et al., 2009; Genzel et al., 2010; Lee et al., 2010) used features from syntactic parse trees to reorder source sentences at training and translation time. A parser is often indispensable to provide syntactic information for such methods. Recently, DeNero and Uszkoreit (2011) proposed an approach that induced parse trees automatically from wordaligned training corpus to perform PRO for a phrasebased SMT system, instead of relying on treebanks. First, binary parse trees are induced from wordaligned training corpus. Based on them, a monolingual parsing model and a tree reordering model are trained to pre-reorder source words into the targetlangua"
D12-1041,D09-1106,0,0.0143278,"d [m, j) in a monotonic way; while a ⟨·⟩ operation combines them in an inverted way. 1 2 448 Given sentence pairs in training corpus with their corresponding FDT spaces, we train FDT-TM in two different ways: (1) The first only uses the 1-best FDT of each sentence pair. Based on each alignment A induced from each 1-best FDT G, all possible bilingual phrases are extracted. Then, the maximum likelihood estimation (MLE) is used to compute probabilities and generate an FDT-TM. (2) The second uses the n-best FDTs of each sentence pair, which is motivated by several studies (Venugopal et al., 2008; Liu et al., 2009). For each sentence pair {f, e}, we first induce n alignments {A1 , ..., An } from the top n FDTs Ω = {G1 , ..., Gn } ⊂ H(f, e). Each Ak is annotated with the posterior probability of its corresponding FDT Gk as follows: ∑ exp{ i λi hi (Gk )} ∑ (6) p(Ak |Gk ) = ∑ Gk′ ∈Ω exp{ i λi hi (Gk′ )} ∑ where i λi hi (Gk ) is the model score assigned to Gk by MRM. Then, all possible bilingual phrases are extracted from the expanded training corpus built using n-best alignments for each sentence pair. The count of each phrase pair is now computed as the sum of posterior probabilities, instead of the sum o"
D12-1041,J10-3002,0,0.0188773,")} (5) In this paper, the sigmoid function is used to make sure that the feature values of different translation rules are in a proper value range. • word pair translation probabilities trained from IBM models (Brown et al., 1993); • log-likelihood ratio (Moore, 2005); • conditional link probability (Moore, 2005); • count of unlinked words; • counts of inversion and concatenation. Many alignment-inspired features can be used in MRM. This paper only uses those commonly-used ones that have already been proved useful in many previous work (Moore, 2005; Moore et al., 2006; Fraser and Marcu, 2006; Liu et al., 2010). Following the common practice in SMT research, the MERT algorithm (Och, 2003) is used to tune feature weights in MRM. Due to the fact that all FDTs of each sentence pair share identical translation, we cannot use BLEU as the error criterion any more. Instead, alignment F-score is used as the alternative. We will show in Section 5 that after the inference step, alignment quality can be improved by replacing original alignments of each sentence pair with alignments induced from its 1-best FDT. Future work could experiment with other error criterions, such as reordering-based loss functions (Bi"
D12-1041,H05-1011,0,0.0316801,"n H(f, e) respectively. e), target-to-source translation probability • h(f¯|¯ of a translation rule r = {f¯, e¯}. ∑ ¯ ¯) {f,e}∈C fracH(f,e) (f , e ∑ h(f¯|¯ e) = ∑ ¯′ ¯) (4) {f,e}∈C f¯′ fracH(f,e) (f , e • h# (r), smoothed usage count for translation rule r = {f¯, e¯} in the whole generation step. h# (r) = 1+e {− ∑ 1 {f,e}∈C fracH(f,e) (f¯,¯ e)} (5) In this paper, the sigmoid function is used to make sure that the feature values of different translation rules are in a proper value range. • word pair translation probabilities trained from IBM models (Brown et al., 1993); • log-likelihood ratio (Moore, 2005); • conditional link probability (Moore, 2005); • count of unlinked words; • counts of inversion and concatenation. Many alignment-inspired features can be used in MRM. This paper only uses those commonly-used ones that have already been proved useful in many previous work (Moore, 2005; Moore et al., 2006; Fraser and Marcu, 2006; Liu et al., 2010). Following the common practice in SMT research, the MERT algorithm (Och, 2003) is used to tune feature weights in MRM. Due to the fact that all FDTs of each sentence pair share identical translation, we cannot use BLEU as the error criterion any more"
D12-1041,P06-1065,0,0.0272001,"(r) = 1+e {− ∑ 1 {f,e}∈C fracH(f,e) (f¯,¯ e)} (5) In this paper, the sigmoid function is used to make sure that the feature values of different translation rules are in a proper value range. • word pair translation probabilities trained from IBM models (Brown et al., 1993); • log-likelihood ratio (Moore, 2005); • conditional link probability (Moore, 2005); • count of unlinked words; • counts of inversion and concatenation. Many alignment-inspired features can be used in MRM. This paper only uses those commonly-used ones that have already been proved useful in many previous work (Moore, 2005; Moore et al., 2006; Fraser and Marcu, 2006; Liu et al., 2010). Following the common practice in SMT research, the MERT algorithm (Och, 2003) is used to tune feature weights in MRM. Due to the fact that all FDTs of each sentence pair share identical translation, we cannot use BLEU as the error criterion any more. Instead, alignment F-score is used as the alternative. We will show in Section 5 that after the inference step, alignment quality can be improved by replacing original alignments of each sentence pair with alignments induced from its 1-best FDT. Future work could experiment with other error criterions,"
D12-1041,P08-1023,0,0.0541771,"Missing"
D12-1041,D08-1089,0,0.0525339,"Missing"
D12-1041,P03-1021,0,0.0690948,"s of different translation rules are in a proper value range. • word pair translation probabilities trained from IBM models (Brown et al., 1993); • log-likelihood ratio (Moore, 2005); • conditional link probability (Moore, 2005); • count of unlinked words; • counts of inversion and concatenation. Many alignment-inspired features can be used in MRM. This paper only uses those commonly-used ones that have already been proved useful in many previous work (Moore, 2005; Moore et al., 2006; Fraser and Marcu, 2006; Liu et al., 2010). Following the common practice in SMT research, the MERT algorithm (Och, 2003) is used to tune feature weights in MRM. Due to the fact that all FDTs of each sentence pair share identical translation, we cannot use BLEU as the error criterion any more. Instead, alignment F-score is used as the alternative. We will show in Section 5 that after the inference step, alignment quality can be improved by replacing original alignments of each sentence pair with alignments induced from its 1-best FDT. Future work could experiment with other error criterions, such as reordering-based loss functions (Birch et al., 2010; Talbot et al., 2011; Birch and Osborne, 2011) or span F1 (DeN"
D12-1041,J04-4002,0,0.169793,"r approach in a BTGbased phrasal system, and propose four FDTbased component models. Experiments are carried out on large scale English-to-Japanese and Chinese-to-English translation tasks, and significant improvements are reported on both translation quality and alignment quality. 1 Ming Zhou Microsoft Research Asia Introduction Most of today’s SMT systems depends heavily on parallel corpora aligned at the word-level to train their different component models. However, such annotations do have their drawbacks in training. On one hand, word links predicted by automatic aligners such as GIZA++ (Och and Ney, 2004) often contain errors. This problem gets even worse on language pairs that differ substantially in word orders, such as English and Japanese/Korean/German. The descent of the word alignment quality will lead to inaccurate component models straightforwardly. On the other hand, several component models are designed to supervise the decoding procedures, This paper presents an FDT-based model training approach to SMT systems by leveraging structured knowledge contained in FDTs. An FDT of a sentence pair {f, e} denotes a derivation tree that can translate f into its accurate target translation e. T"
D12-1041,P02-1040,0,0.0855811,"lity, including LDC2003E07, LDC2003E14, LDC2005T06, LDC2005T10, LDC2005E83, LDC2006E26, LDC2006E34, LDC2006E85 and LDC2006E92. A 5gram LM is trained on the Xinhua portion of LDC English Gigaword Version 3.0. NIST 2004 (MT04) data set is used as dev set, and evaluation results are measured on NIST 2005 (MT05) and NIST 2008 (MT08) data sets. In all evaluation data sets, each source sentence has four reference translations. Default word alignments for both SMT tasks are performed by GIZA++ with the intersect-diag-grow refinement. Translation quality is measured in terms of case-insensitive BLEU (Papineni et al., 2002) and reported in percentage numbers. 5.2 Baseline System The phrase-based SMT system proposed by Xiong et al. (2006) is used as the baseline system, with a MaxEnt principle-based lexicalized reordering model integrated, which is used to handle reorderings in decoding. The maximum lengths for the source and target phrases are 5 and 7 on E-J task, and 3 and 5 on C-E task. The beam size is set to 20. 5.3 Translation Quality on E-J Task We first evaluate the effectiveness of our FDT-based model training approach on E-J translation task, and present evaluation results in Table 1, in which BTG denot"
D12-1041,N06-1002,0,0.0199934,"igned training corpus. Based on them, a monolingual parsing model and a tree reordering model are trained to pre-reorder source words into the targetlanguage-like order. Their work is distinct from ours 450 Feng et al. (2010) proposed an SLM in a phrasebased SMT system. They used it as a reordering feature in the sense that it helped the decoder to find correct decoding sequences. The difference between their model and our FDT-SLM is that, in their work, the reordered source sequences are extracted based on word alignments only; while in our FDT-SLM, such sequences are obtained based on FDTs. Quirk and Menezes (2006) proposed a Minimal Translation Unit (MTU) -based sequence model and used it in their treelet system; Vaswani et al. (2011) proposed a rule Markov model to capture dependencies between minimal rules for a top-down tree-tostring system. The key difference between FDTRSM and previous work is that the rule sequences are extracted from FDTs, and no parser is needed. 5 Experiments 5.1 Data and Metric Experiments are carried out on English-to-Japanese (E-J) and Chinese-to-English (C-E) MT tasks. For E-J task, bilingual data used contains 13.3M sentence pairs after pre-processing. The Japanese side o"
D12-1041,2008.amta-srw.6,0,0.0202855,"dels (Tillman, 2004; Zens and Ney, 2006; Xiong et al., 2006; Galley and Manning, 2008;) are widely used in phrase-based SMT systems. Training instances of these models are extracted from word-aligned sentence pairs. Due to efficiency reasons, only parts of all instances are kept and used in DM training, which cannot cover all possible reordering situations that could be met in decoding. In FDT-DM, by contrast, training instances are extracted from FDTs. Such instances take both local and global reordering cases into consideration. 4.4 Sequence Models Related Work 4.1 Forced Decoding/Alignment Schwartz (2008) used forced decoding to leverage multilingual corpus to improve translation quality; Shen et al. (2008) used forced alignment to train a better phrase segmentation model; Wuebker et al. (2010) used forced alignment to re-estimate translation probabilities using a leaving-one-out strategy. We consider the usage of FD in Section 2.1 to be a direct extension of these approaches, but one that generates FDTs for parallel data rather than focusing on phrase segmentation or probability estimation. 4.2 Pre-reordering Pre-reordering (PRO) techniques (Collins et al., 2005; Xu et al., 2009; Genzel et al"
D12-1041,W11-2102,0,0.0123739,"mmon practice in SMT research, the MERT algorithm (Och, 2003) is used to tune feature weights in MRM. Due to the fact that all FDTs of each sentence pair share identical translation, we cannot use BLEU as the error criterion any more. Instead, alignment F-score is used as the alternative. We will show in Section 5 that after the inference step, alignment quality can be improved by replacing original alignments of each sentence pair with alignments induced from its 1-best FDT. Future work could experiment with other error criterions, such as reordering-based loss functions (Birch et al., 2010; Talbot et al., 2011; Birch and Osborne, 2011) or span F1 (DeNero and Uszkoreit, 2011). • hr (G), number of translation rules used in G. 3 Training in Phrase-based SMT • hd (G), structure-based score of G. For FDTs generated by phrase-based paradigms, it can be computed by distortion models; while for FDTs generated by syntax-based paradigms, it can be computed by either parsing models or syntactic LMs (Charniak et al., 2003). As the first step in this line of research, we explore the usage of FDT-based model training method in a phrase-based SMT system (Xiong et al., 2006), which employs Bracketing Transduction"
D12-1041,P11-1086,0,0.020727,"e words into the targetlanguage-like order. Their work is distinct from ours 450 Feng et al. (2010) proposed an SLM in a phrasebased SMT system. They used it as a reordering feature in the sense that it helped the decoder to find correct decoding sequences. The difference between their model and our FDT-SLM is that, in their work, the reordered source sequences are extracted based on word alignments only; while in our FDT-SLM, such sequences are obtained based on FDTs. Quirk and Menezes (2006) proposed a Minimal Translation Unit (MTU) -based sequence model and used it in their treelet system; Vaswani et al. (2011) proposed a rule Markov model to capture dependencies between minimal rules for a top-down tree-tostring system. The key difference between FDTRSM and previous work is that the rule sequences are extracted from FDTs, and no parser is needed. 5 Experiments 5.1 Data and Metric Experiments are carried out on English-to-Japanese (E-J) and Chinese-to-English (C-E) MT tasks. For E-J task, bilingual data used contains 13.3M sentence pairs after pre-processing. The Japanese side of bilingual data is used to train a 4-gram LM. The development set (dev) which contains 2,000 sentences is used to optimize"
D12-1041,2008.amta-papers.18,0,0.0141752,"e source spans [i, m) and [m, j) in a monotonic way; while a ⟨·⟩ operation combines them in an inverted way. 1 2 448 Given sentence pairs in training corpus with their corresponding FDT spaces, we train FDT-TM in two different ways: (1) The first only uses the 1-best FDT of each sentence pair. Based on each alignment A induced from each 1-best FDT G, all possible bilingual phrases are extracted. Then, the maximum likelihood estimation (MLE) is used to compute probabilities and generate an FDT-TM. (2) The second uses the n-best FDTs of each sentence pair, which is motivated by several studies (Venugopal et al., 2008; Liu et al., 2009). For each sentence pair {f, e}, we first induce n alignments {A1 , ..., An } from the top n FDTs Ω = {G1 , ..., Gn } ⊂ H(f, e). Each Ak is annotated with the posterior probability of its corresponding FDT Gk as follows: ∑ exp{ i λi hi (Gk )} ∑ (6) p(Ak |Gk ) = ∑ Gk′ ∈Ω exp{ i λi hi (Gk′ )} ∑ where i λi hi (Gk ) is the model score assigned to Gk by MRM. Then, all possible bilingual phrases are extracted from the expanded training corpus built using n-best alignments for each sentence pair. The count of each phrase pair is now computed as the sum of posterior probabilities, i"
D12-1041,P10-1049,0,0.0638577,"word-aligned sentence pairs. Due to efficiency reasons, only parts of all instances are kept and used in DM training, which cannot cover all possible reordering situations that could be met in decoding. In FDT-DM, by contrast, training instances are extracted from FDTs. Such instances take both local and global reordering cases into consideration. 4.4 Sequence Models Related Work 4.1 Forced Decoding/Alignment Schwartz (2008) used forced decoding to leverage multilingual corpus to improve translation quality; Shen et al. (2008) used forced alignment to train a better phrase segmentation model; Wuebker et al. (2010) used forced alignment to re-estimate translation probabilities using a leaving-one-out strategy. We consider the usage of FD in Section 2.1 to be a direct extension of these approaches, but one that generates FDTs for parallel data rather than focusing on phrase segmentation or probability estimation. 4.2 Pre-reordering Pre-reordering (PRO) techniques (Collins et al., 2005; Xu et al., 2009; Genzel et al., 2010; Lee et al., 2010) used features from syntactic parse trees to reorder source sentences at training and translation time. A parser is often indispensable to provide syntactic informatio"
D12-1041,J97-3002,0,0.0295005,"orne, 2011) or span F1 (DeNero and Uszkoreit, 2011). • hr (G), number of translation rules used in G. 3 Training in Phrase-based SMT • hd (G), structure-based score of G. For FDTs generated by phrase-based paradigms, it can be computed by distortion models; while for FDTs generated by syntax-based paradigms, it can be computed by either parsing models or syntactic LMs (Charniak et al., 2003). As the first step in this line of research, we explore the usage of FDT-based model training method in a phrase-based SMT system (Xiong et al., 2006), which employs Bracketing Transduction Grammar (BTG) (Wu, 1997) to parse parallel sentences. The reason of choosing this system is due to the prominent advantages of BTG, such as the simplicity of the grammar and the good coverage of syntactic diversities between different language pairs. We first describe more details of FDTs under BTG. Then, four FDT-based component models are presented. The overfitting issue in the generation step can be alleviated by leveraging memory-based features in the inference step. h# (r) is used to penalize those long translation rules which tend to occur in only a few training sentences and are used few times in FD, hr (G) ad"
D12-1041,P06-1066,0,0.416938,"sed loss functions (Birch et al., 2010; Talbot et al., 2011; Birch and Osborne, 2011) or span F1 (DeNero and Uszkoreit, 2011). • hr (G), number of translation rules used in G. 3 Training in Phrase-based SMT • hd (G), structure-based score of G. For FDTs generated by phrase-based paradigms, it can be computed by distortion models; while for FDTs generated by syntax-based paradigms, it can be computed by either parsing models or syntactic LMs (Charniak et al., 2003). As the first step in this line of research, we explore the usage of FDT-based model training method in a phrase-based SMT system (Xiong et al., 2006), which employs Bracketing Transduction Grammar (BTG) (Wu, 1997) to parse parallel sentences. The reason of choosing this system is due to the prominent advantages of BTG, such as the simplicity of the grammar and the good coverage of syntactic diversities between different language pairs. We first describe more details of FDTs under BTG. Then, four FDT-based component models are presented. The overfitting issue in the generation step can be alleviated by leveraging memory-based features in the inference step. h# (r) is used to penalize those long translation rules which tend to occur in only"
D12-1041,N09-1028,0,0.0155643,"ing/Alignment Schwartz (2008) used forced decoding to leverage multilingual corpus to improve translation quality; Shen et al. (2008) used forced alignment to train a better phrase segmentation model; Wuebker et al. (2010) used forced alignment to re-estimate translation probabilities using a leaving-one-out strategy. We consider the usage of FD in Section 2.1 to be a direct extension of these approaches, but one that generates FDTs for parallel data rather than focusing on phrase segmentation or probability estimation. 4.2 Pre-reordering Pre-reordering (PRO) techniques (Collins et al., 2005; Xu et al., 2009; Genzel et al., 2010; Lee et al., 2010) used features from syntactic parse trees to reorder source sentences at training and translation time. A parser is often indispensable to provide syntactic information for such methods. Recently, DeNero and Uszkoreit (2011) proposed an approach that induced parse trees automatically from wordaligned training corpus to perform PRO for a phrasebased SMT system, instead of relying on treebanks. First, binary parse trees are induced from wordaligned training corpus. Based on them, a monolingual parsing model and a tree reordering model are trained to pre-re"
D12-1041,W06-3108,0,0.0225538,"DTs need not be limited to the BTG-based system, and we consider using FDTs generated by SCFG-based systems or traditional left-to-right phrase-based systems in future. because it focused on inducing sentence structures for the PRO task, but mirrors ours in demonstrating that there is a potential role for structure-based training corpus in SMT model training. 4.3 Distortion Models Figure 3: An example of extracting a rule sequence from an FDT. In order to generate the correct target translation, the desired rule sequence should be r2 ⇒ r3 ⇒ r1 . 4 Lexicalized distortion models (Tillman, 2004; Zens and Ney, 2006; Xiong et al., 2006; Galley and Manning, 2008;) are widely used in phrase-based SMT systems. Training instances of these models are extracted from word-aligned sentence pairs. Due to efficiency reasons, only parts of all instances are kept and used in DM training, which cannot cover all possible reordering situations that could be met in decoding. In FDT-DM, by contrast, training instances are extracted from FDTs. Such instances take both local and global reordering cases into consideration. 4.4 Sequence Models Related Work 4.1 Forced Decoding/Alignment Schwartz (2008) used forced decoding to"
D12-1041,D08-1022,0,\N,Missing
D12-1041,J93-2003,0,\N,Missing
D12-1078,2008.amta-srw.1,0,0.406236,"LT and that of more than 1 Bleu point on NIST. 5 Related Works There are a lot of attempts in improving word alignment with syntactic information (Cherry and Lin, 2006; DeNero and Klein, 2007; Hermjackob, 2009) and in improving parser with alignment information (Burkett and Klein, 2008). Yet strictly speaking all these attempts aim to improve the parser/aligner itself rather than the translation model. To improve the performance of syntactic machine translation, Huang and Knight (2006) proposed a method incorporating a handful of relabeling strategies to modify the syntactic trees structures. Ambati and Lavie (2008) restructured target parse trees to generate highly isomorphic target trees that preserve the syntactic boundaries of constituents aligned in the original parse trees. Wang et al., (2010) proposed to use re-structuring and re-labeling to modify the parser tree. The restructuring method uses a binarization method to enable the reuse of sub-constituent structures, and the linguistic and statistical re-labeling methods to handle the coarse nonterminal problem, so as to enhance generalization ability. Different from the previous work of modifying tree structures with post-processing methods, our m"
D12-1078,D08-1092,0,0.169702,"pending on the order of application. Table 7 and 8 show the experiment results of combining FA-PR with IDSG. It can be seen that either way of the combination is better than using FA-PR or IDSG alone. Yet there is no significant difference between the two kinds of combination. The best result is a gain of more than 3 Bleu points on IWSLT and that of more than 1 Bleu point on NIST. 5 Related Works There are a lot of attempts in improving word alignment with syntactic information (Cherry and Lin, 2006; DeNero and Klein, 2007; Hermjackob, 2009) and in improving parser with alignment information (Burkett and Klein, 2008). Yet strictly speaking all these attempts aim to improve the parser/aligner itself rather than the translation model. To improve the performance of syntactic machine translation, Huang and Knight (2006) proposed a method incorporating a handful of relabeling strategies to modify the syntactic trees structures. Ambati and Lavie (2008) restructured target parse trees to generate highly isomorphic target trees that preserve the syntactic boundaries of constituents aligned in the original parse trees. Wang et al., (2010) proposed to use re-structuring and re-labeling to modify the parser tree. Th"
D12-1078,P11-1003,0,0.0136755,"SMT models, and directly produce trees which are consistent with word alignment matrices. Instead of modifying the parse tree to improve machine translation performance, many methods were proposed to modify word alignment by taking syntactic tree into consideration, including deleting incorrect word alignment links by a discriminative model (Fossum et al., 2008), re-aligning sentence pairs using EM method with the rules extracted with initial alignment (Wang et al., 2010), and removing ambiguous alignment of functional words with constraint from chunk-level information during rule extraction (Wu et al., 2011). Unlike all these pursuits, to generate a consistent word alignment, our method modifies the popularly used IDG symmetrization method to make it suitable for string-to-tree rule extraction, and our method is much simpler and faster than the previous works. 6 Conclusion In this paper we have attempted to improve SSMT by reducing the errors introduced by the mutual independence between monolingual parser and word aligner. Our major contribution is the strategies of re-training parser with the bilingual information in alignment matrices. Either of our proposals of targeted self-training with fro"
D12-1078,P03-1021,0,0.185049,"Missing"
D12-1078,W08-0306,0,0.0913119,"ent and syntactic tree for the target sentence. All the nodes in gray are frontier nodes. Example (a) contains two error links (in dash line), and the syntactic tree for the target sentence of example (b) is wrong. structure by incorrect alignment links, as shown by the two dashed links in Figure 1(a). These two incorrect links hinder the extraction of a good minimal rule “毡房 ” and that of a good composed rule “牧民, 的 NP(DT(the), NN(herdsmen), POS(&apos;s)) ”. By and large, incorrect alignment links lead to translation rules that are large in size, few in number, and poor in generalization ability (Fossum et al, 2008). The second problem is parsing error, as shown in Figure 1(b). The incorrect POS tagging of the word “lectures"" causes a series of parsing errors, including the absence of the noun phrase “NP(NN(propaganda), NN(lectures))”. These parsing errors hinder the extraction of good rules, such as “ 宣 讲 NP(NN(propaganda), NN(lectures)) ”. Note that in Figure 1(a), the parse tree is correct, and the incorrect alignment links might be fixed if the aligner takes the parse tree into consideration. Similarly, in Figure 1(b) some parsing errors might be fixed if the parser takes into consideration the corre"
D12-1078,P06-1121,0,0.207786,"aligner does not consider the syntax information of both languages, and the output links may violate syntactic correspondence. That is, some SL words yielded by a SL parse tree node may not be traced to, via alignment links, some TL words with legitimate syntactic structure. On the other hand, parser design is a monolingual activity and its impact on MT is not well studied (Ambati, 2008). Many good translation rules may thus be filtered by a good monolingual parser. In this paper we will focus on the translation task from Chinese to English, and the string-to-tree SSMT model as elaborated in (Galley et al., 2006). There are two kinds of translation rules in this model, minimal rules, and composed rules, which are composition of minimal rules. The minimal rules are extracted from a special kind of nodes, known as frontier nodes, on TL parse tree. The concept of frontier node can be illustrated by Figure 1, which shows two partial bilingual sentences with the corresponding TL sub-trees and word alignment links. The TL words yielded by a TL parse node can be traced to the corresponding SL words through alignment links. In the diagram, each parse node is represented by a rectangle, showing the phrase labe"
D12-1078,N06-1031,0,0.168264,"there is no significant difference between the two kinds of combination. The best result is a gain of more than 3 Bleu points on IWSLT and that of more than 1 Bleu point on NIST. 5 Related Works There are a lot of attempts in improving word alignment with syntactic information (Cherry and Lin, 2006; DeNero and Klein, 2007; Hermjackob, 2009) and in improving parser with alignment information (Burkett and Klein, 2008). Yet strictly speaking all these attempts aim to improve the parser/aligner itself rather than the translation model. To improve the performance of syntactic machine translation, Huang and Knight (2006) proposed a method incorporating a handful of relabeling strategies to modify the syntactic trees structures. Ambati and Lavie (2008) restructured target parse trees to generate highly isomorphic target trees that preserve the syntactic boundaries of constituents aligned in the original parse trees. Wang et al., (2010) proposed to use re-structuring and re-labeling to modify the parser tree. The restructuring method uses a binarization method to enable the reuse of sub-constituent structures, and the linguistic and statistical re-labeling methods to handle the coarse nonterminal problem, so as"
D12-1078,W04-3250,0,0.317444,"Missing"
D12-1078,J10-2004,0,0.205983,"; Hermjackob, 2009) and in improving parser with alignment information (Burkett and Klein, 2008). Yet strictly speaking all these attempts aim to improve the parser/aligner itself rather than the translation model. To improve the performance of syntactic machine translation, Huang and Knight (2006) proposed a method incorporating a handful of relabeling strategies to modify the syntactic trees structures. Ambati and Lavie (2008) restructured target parse trees to generate highly isomorphic target trees that preserve the syntactic boundaries of constituents aligned in the original parse trees. Wang et al., (2010) proposed to use re-structuring and re-labeling to modify the parser tree. The restructuring method uses a binarization method to enable the reuse of sub-constituent structures, and the linguistic and statistical re-labeling methods to handle the coarse nonterminal problem, so as to enhance generalization ability. Different from the previous work of modifying tree structures with post-processing methods, our methods try to learn a suitable grammar for string-to-tree SMT models, and directly produce trees which are consistent with word alignment matrices. Instead of modifying the parse tree to"
D12-1078,J03-1002,0,0.00402198,"p2: Select the one which can generate the biggest frontier set:  step3: Add to , and repeat step 1, until no new link can be added. Like IDG, IDSG starts with all the links in and its main task is to add links selected from . IDSG is also subject to the constraints of IDG. The new criterion in link selection in IDSG is specified in Step 2. Given a parse tree of the TL side of the bilingual sentence, in each iteration IDSG considers the change of frontier set size caused by 858 PP 1 2-6 PP 3-6 1-2 Word Alignment Symmetrization The most widely used word aligners in MT, like HMM and IBM Models (Och and Ney, 2003), are directional aligners. Such aligner produces one set of alignment matrices for the SL-to-TL direction and another set for the TL-to-SL direction. Symmetrization refers to the combination of these two sets of alignment matrices. The most popular method of symmetrization is intersect-diag-grow (IDG). Given a bilingual sentence and its two alignment matrices and IDG starts with all the links in . Then IDG considers each link in in turn. A link is added if its addition does not make some phrase pairs overlap. Although IDG is simple and efficient, and has been shown to be effective in phrase-b"
D12-1078,P10-1049,0,0.118082,"ht be fixed if word aligner and parser are not mutually independent. In this paper, we emphasize more on the correction of parsing errors by exploiting alignment information. The general approach is to re-train a parser with parse trees which are the most consistent with alignment matrices. Our first strategy is to apply the idea of targeted selftraining (Katz-Brown et al., 2011) with the simple evaluation function of frontier set size. That is to re-train the parser with the parse trees which give rise to the largest number of frontier nodes. The second strategy is to apply forced alignment (Wuebker et al., 2010) to bilingual data and select the parse trees generated by our SSMT system for re-training the parser. Besides, although we do not invent a new word aligner exploiting syntactic information, we propose a new method to symmetrize the alignment matrices of two directions by taking parse tree into consideration. 2 Parser Re-training Strategies Most monolingual parsers used in SSMT are trained upon certain tree bank. That is, a parser is trained with the target of maximizing the agreement between its decision on syntactic structure and that decision in the human-annotated parse trees. As mentioned"
D12-1078,P02-1040,0,0.0863916,"Missing"
D12-1078,N07-1051,0,0.0726623,"Missing"
D12-1078,D11-1017,0,\N,Missing
D12-1078,P07-1003,0,\N,Missing
D12-1078,P06-2014,0,\N,Missing
D12-1078,D09-1024,0,\N,Missing
D13-1041,E06-1002,0,0.851221,"tion,China ‡ Microsoft Research Asia hezhengyan.hit@gmail.com {shujliu,muli,mingzhou}@microsoft.com songyangmagic@gmail.com wanghf@pku.edu.cn Abstract Previous researches have proposed several kinds of effective approaches for this problem. Learning to rank (L2R) approaches use hand-crafted features f (d, e) to describe the similarity or dissimilarity between contextual document d and entity definition e. L2R approaches are very flexible and expressive. Features like name matching, context similarity (Li et al., 2009; Zheng et al., 2010; Lehmann et al., 2010) and category context correlation (Bunescu and Pasca, 2006) can be incorporated with ease. Nevertheless, decisions are made independently and inconsistent results are found from time to time. Entity disambiguation works by linking ambiguous mentions in text to their corresponding real-world entities in knowledge base. Recent collective disambiguation methods enforce coherence among contextual decisions at the cost of non-trivial inference processes. We propose a fast collective disambiguation approach based on stacking. First, we train a local predictor g0 with learning to rank as base learner, to generate initial ranking list of candidates. Second, t"
D13-1041,D07-1074,0,0.971336,"e false ones scattered around. 2 Related Work Most popular entity linking systems use the L2R framework (Bunescu and Pasca, 2006; Li et al., 2009; Zheng et al., 2010; Lehmann et al., 2010). Its discriminative nature gives the model enough flexibility and expressivity. It can include any features that describe the similarity or dissimilarity of context d and candidate entity e. They often perform well even on small training set, with carefullydesigned features. This category falls into the local approach as the decision processes for each mention are made independently (Ratinov et al., 2011). (Cucerzan, 2007) first suggests to optimize an objective function that is similar to the collective ap427 Wikipedians annotate entries in Wikipedia with category network. This valuable information generalizes entity-context correlation to category-context correlation. (Bunescu and Pasca, 2006) utilize category-word as features in their ranking model. (Kataria et al., 2011) employ a hierarchical topic model where each inner node in the hierarchy is a category. Both approaches must rely on pruned categories because the large number of noisy categories. We try to address this problem with recent advances of repr"
D13-1041,D11-1072,0,0.338593,"Missing"
D13-1041,D08-1017,0,0.0165384,"global ranker. The differences are that we use stacking to train the local ranker to handle the train/test mismatch problem and top k candidates to generate features for the global ranker. Stacked generalization (Wolpert, 1992) is a meta learning algorithm that uses multiple learners outputs to augment the feature space of subsequent learners. It utilizes a cross-validation strategy to address the train set / testset label mismatch problem. Various applications of stacking in NLP have been proposed, such as collective document classification (Kou and Cohen, 2007), stacked dependency parsing (Martins et al., 2008) and joint Chinese word segmentation and part-of-speech tagging (Sun, 2011). (Kou and Cohen, 2007) propose stacked graphical learning which captures dependencies between data with relational template. Our method is inspired by their approach. The difference is our base learner is an L2R model. We search related entity candidates in a large semantic relatedness graph, based on the assumption that true candidates are often semantically correlated while false ones scattered around. 2 Related Work Most popular entity linking systems use the L2R framework (Bunescu and Pasca, 2006; Li et al., 2009;"
D13-1041,P11-1138,0,0.060973,"machine readable format, ambiguous names must be resolved in order to tell which realworld entity the name refers to. The task of linking names to knowledge base is known as entity linking or disambiguation (Ji et al., 2011). The resulting text is populated with semantic rich links to knowledge base like Wikipedia, and ready for various downstream NLP applications. ∗ Corresponding author Collective approaches utilize dependencies between different decisions and resolve all ambiguous mentions within the same context simultaneously (Han et al., 2011; Hoffart et al., 2011; Kulkarni et al., 2009; Ratinov et al., 2011). Collective approaches can improve performance when local evidence is not confident enough. They often utilize semantic relations across different mentions, and is why they are called global approaches, while L2R methods fall into local approaches (Ratinov et al., 2011). However, collective inference processes are often expensive and involve an exponential search space. We propose a collective entity linking method based on stacking. Stacked generalization (Wolpert, 1992) is a powerful meta learning algorithm that uses two levels of learners. The predictions of the first learner are taken as"
D13-1041,P05-1044,0,0.0476092,"the mention “Romney” as an examFinally, the semantic relatedness measure of two entities ei ,ej is defined as the common in-links of ei and ej in Wikipedia (Milne and Witten, 2008; Han et al., 2011): 430 where W is learned with supervision like clickthrough data. Given training data {(qi , di )}, training is done by randomly sampling a negative target d− . The model optimizes W such that f (q, d+ ) > f (q, d− ). Thus, the training objective is to minimize the following margin-based loss function: ∑ max(0, 1 − f (q, d+ ) + f (q, d− )) (7) q,d+ ,d− which is also known as contrastive estimation (Smith and Eisner, 2005). W can become very large and inefficient when we have a big vocabulary size. This is addressed by replacing W with its low rank approximation: W = UT V + I (8) here, the identity term I is a trade-off between the latent space model and a vector space model. The gradient step is performed with Stochastic Gradient Descent (SGD): U ←U + λV (d+ − d− )q T , if 1 − f (q, d+ ) + f (q, d− ) > 0 (9) − T V ←V + λU q(d − d ) , + if 1 − f (q, d+ ) + f (q, d− ) > 0. (10) where λ is the learning rate. The query and document are not necessary real query and document. In our case, we treat our problem as: gi"
D13-1041,P11-1139,0,0.022453,"handle the train/test mismatch problem and top k candidates to generate features for the global ranker. Stacked generalization (Wolpert, 1992) is a meta learning algorithm that uses multiple learners outputs to augment the feature space of subsequent learners. It utilizes a cross-validation strategy to address the train set / testset label mismatch problem. Various applications of stacking in NLP have been proposed, such as collective document classification (Kou and Cohen, 2007), stacked dependency parsing (Martins et al., 2008) and joint Chinese word segmentation and part-of-speech tagging (Sun, 2011). (Kou and Cohen, 2007) propose stacked graphical learning which captures dependencies between data with relational template. Our method is inspired by their approach. The difference is our base learner is an L2R model. We search related entity candidates in a large semantic relatedness graph, based on the assumption that true candidates are often semantically correlated while false ones scattered around. 2 Related Work Most popular entity linking systems use the L2R framework (Bunescu and Pasca, 2006; Li et al., 2009; Zheng et al., 2010; Lehmann et al., 2010). Its discriminative nature gives"
D13-1041,N10-1072,0,0.661752,"aboratory of Computational Linguistics (Peking University) Ministry of Education,China ‡ Microsoft Research Asia hezhengyan.hit@gmail.com {shujliu,muli,mingzhou}@microsoft.com songyangmagic@gmail.com wanghf@pku.edu.cn Abstract Previous researches have proposed several kinds of effective approaches for this problem. Learning to rank (L2R) approaches use hand-crafted features f (d, e) to describe the similarity or dissimilarity between contextual document d and entity definition e. L2R approaches are very flexible and expressive. Features like name matching, context similarity (Li et al., 2009; Zheng et al., 2010; Lehmann et al., 2010) and category context correlation (Bunescu and Pasca, 2006) can be incorporated with ease. Nevertheless, decisions are made independently and inconsistent results are found from time to time. Entity disambiguation works by linking ambiguous mentions in text to their corresponding real-world entities in knowledge base. Recent collective disambiguation methods enforce coherence among contextual decisions at the cost of non-trivial inference processes. We propose a fast collective disambiguation approach based on stacking. First, we train a local predictor g0 with learning"
D13-1107,D11-1033,0,0.0801174,"Missing"
D13-1107,W06-1615,0,0.0632491,"ation problem over mixture models for SMT systems, as proposed in this paper. 4.2 Multi-task Learning In machine learning, MTL is an approach to learn one target problem with other related problems at the same time. This often leads to a better model for the main task because it allows the learner to use the commonality among the tasks. MTL is performed by learning tasks in parallel while using a shared representation. Therefore, what is learned for each 1063 task can help other tasks be learned better. MTL was successfully applied in some Natural Language Processing (NLP) tasks. For example, Blitzer et al. (2006) extended the MTL approach (Ando and Zhang, 2005) to domain adaptation tasks in part-of-speech tagging. Collobert and Weston (2008) proposed using deep neural networks to train a set of tasks, including part-of-speech tagging, chunking, named entity recognition, and semantic roles labeling. They reported that jointly learning these tasks led to superior performance. MTL was also applied in sentiment analysis (Dredze and Crammer, 2008) and web ranking (Chapelle et al., 2011) to address the multi-domain learning and adaptation. In SMT, Duh et al. (2010) proposed using MTL for N-best re-ranking o"
D13-1107,J07-2003,0,0.0972138,"nces for testing in each domain. The details are shown in Table 2. Domain Business Ent. Health Sci&Tech Sports Politics Train En Ch 30M 28M 25M 22M 23M 20M 28M 26M 19M 16M 28M 24M Dev En Ch 36K 35K 21K 18K 33K 33K 46K 45K 18K 14K 19K 17K Test En Ch 19K 19K 13K 12K 21K 22K 27K 27K 10K 9K 13K 12K Table 2: Statistics of in-domain training, development and testing data, in number of words. 3.2 Setup An in-house hierarchical phrase-based SMT decoder was implemented for our experiments. The CKY decoding algorithm was used and cube pruning was performed with the same default parameter settings as in Chiang (2007). We used a 100-best list from the decoder for the pairwise ranking algorithm. Translation models were trained over the bilingual data that was automatically word-aligned using GIZA++ (Och and Ney, 2003) in both directions, and the diag-grow-final heuristic was used to refine the symmetric word alignment. The phrase tables were filtered to retain top-20 translation candidates for each source phrase for efficiency. An in-house language modeling toolkit was used to train the 4-gram language models with modified Kneser-Ney smoothing (Kneser and Ney, 1995) over the web-crawled data. The evaluation"
D13-1107,P13-2061,1,0.817113,"Section 2.1, we built a web crawler to collect a large number of webpages from web portal sites in English and Chinese respectively. In the experiments, we mainly focused on six popular domains, namely Business, Entertainment, Health, Science & Technology, Sports, and Politics. For both English and Chinese webpages, the HTML tags were removed and the main content was extracted. The data statistics are shown in Table 1. The bilingual data we used was mainly mined from the web using the method proposed by Jiang et al. (2009), with a post-processing step using our bilingual data cleaning method (Cui et al., 2013). Therefore, the data quality is pretty good. In addition, we also used the English-Chinese parallel corpus released by LDC2 . In total, the bilingual data 2 LDC2003E07, LDC2003E14, LDC2004E12, LDC2005T06, LDC2005T10, LDC2005E83, LDC2006E26, Domain Business Ent. Health Sci&Tech Sports Politics English Docs Words 21M 10.4B 18.3M 8.29B 8.7M 4.73B 10.9M 5.33B 18.9M 9.58B 10.3M 5.56B Chinese Docs Words 7.91M 2.73B 4.16M 1.31B 0.9M 0.42B 5.28M 1.6B 2.49M 0.59B 1.67M 0.39B Table 1: Statistics of web-crawled monolingual data, in numbers of documents and words (main content). ”M” refers to million and"
D13-1107,D08-1072,0,0.0445467,"omain according to testing data and have achieved good performance. It is natural that real world SMT systems should adapt the models to multiple domains because the input may be heterogeneous, so that the overall translation quality can be improved. Although we can easily apply these methods to multiple domains individually, it is difficult to use the common knowledge across different domains. To leverage the common knowledge, we need to devise a multi-domain adaptation approach that jointly adapts the SMT models. Multi-domain adaptation has been proved quite effective in sentiment analysis (Dredze and Crammer, 2008) and web ranking (Chapelle et al., 2011), where the commonalities and differences across multiple domains are explicitly addressed by Multitask Learning (MTL). MTL is an approach that learns one target problem with other related problems at the same time, using a shared feature representation. The key advantage of MTL is to enable implicit data sharing and regularization. Therefore, it often leads to a better model for each task. Analogously, we expect that the overall translation quality can be further improved by using an MTL-based 1055 Proceedings of the 2013 Conference on Empirical Methods"
D13-1107,W10-1757,0,0.0555944,"e Processing (NLP) tasks. For example, Blitzer et al. (2006) extended the MTL approach (Ando and Zhang, 2005) to domain adaptation tasks in part-of-speech tagging. Collobert and Weston (2008) proposed using deep neural networks to train a set of tasks, including part-of-speech tagging, chunking, named entity recognition, and semantic roles labeling. They reported that jointly learning these tasks led to superior performance. MTL was also applied in sentiment analysis (Dredze and Crammer, 2008) and web ranking (Chapelle et al., 2011) to address the multi-domain learning and adaptation. In SMT, Duh et al. (2010) proposed using MTL for N-best re-ranking on sparse feature sets, where each N-best list corresponded to a distinct task. Simianer et al. (2012) proposed distributed stochastic learning with feature selection inspired by MTL. The distributed learning approach outperformed several other training methods including MIRA and SGD. Inspired by these methods, we used MTL to tune multiple SMT systems at the same time, where each system was composed of in-domain and generaldomain models. Through a shared feature representation, the commonalities among the SMT systems were better learned by the general"
D13-1107,eck-etal-2004-language,0,0.18163,"g domains are similar. However, this assumption does not hold for real world SMT systems since training data for SMT models may come from a variety of domains. The translation quality is often unsatisfactory when ∗ This work was done while the first and second authors were visiting Microsoft Research Asia. translating texts from a specific domain using a general model that is trained over a hotchpotch of bilingual corpora. Therefore, domain adaptation is crucial for SMT systems to achieve better performance. Previous research on domain adaptation for SMT includes data selection and weighting (Eck et al., 2004; L¨u et al., 2007; Foster et al., 2010; Moore and Lewis, 2010; Axelrod et al., 2011), mixture models (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Sennrich, 2012; Razmara et al., 2012), and semi-supervised transductive learning (Ueffing et al., 2007), etc. Most of these methods adapt SMT models to a specific domain according to testing data and have achieved good performance. It is natural that real world SMT systems should adapt the models to multiple domains because the input may be heterogeneous, so that the overall translation quality can be improved. Although we can easily apply the"
D13-1107,W07-0717,0,0.372942,"dels may come from a variety of domains. The translation quality is often unsatisfactory when ∗ This work was done while the first and second authors were visiting Microsoft Research Asia. translating texts from a specific domain using a general model that is trained over a hotchpotch of bilingual corpora. Therefore, domain adaptation is crucial for SMT systems to achieve better performance. Previous research on domain adaptation for SMT includes data selection and weighting (Eck et al., 2004; L¨u et al., 2007; Foster et al., 2010; Moore and Lewis, 2010; Axelrod et al., 2011), mixture models (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Sennrich, 2012; Razmara et al., 2012), and semi-supervised transductive learning (Ueffing et al., 2007), etc. Most of these methods adapt SMT models to a specific domain according to testing data and have achieved good performance. It is natural that real world SMT systems should adapt the models to multiple domains because the input may be heterogeneous, so that the overall translation quality can be improved. Although we can easily apply these methods to multiple domains individually, it is difficult to use the common knowledge across different domains. To levera"
D13-1107,W06-1607,0,0.0717034,"efficiency. An in-house language modeling toolkit was used to train the 4-gram language models with modified Kneser-Ney smoothing (Kneser and Ney, 1995) over the web-crawled data. The evaluation metric for the overall translation quality was case-insensitive BLEU4 (Papineni et al., 2002). A statistical significance test was performed using the bootstrap resampling method (Koehn, 2004). 3.3 We have two baselines. The first baseline is a nonadapted Hiero using our implementation. It contained the general-domain TM and LM, as well as other standard features. In addition, the fix-discount method (Foster et al., 2006) for phrase table smoothing was also used. The system was general-domain oriented and it was tuned by using MERT (Och, 2003) with a combination of six in-domain development datasets. The second baseline is Google Online Translation Service3 . We obtained the English-toChinese translations of the testing data from Google Translation to have a more solid comparison. Moreover, we also compared our method with the adapted systems towards each domain individually (Koehn and Schroeder, 2007). This is to demonstrate the superiority of our MTL-based tuning approach across different domains. 3.4 1060 R"
D13-1107,D10-1044,0,0.132836,"Missing"
D13-1107,P09-1098,1,0.83532,"addition, the target-side LMs were re-used in the SMT systems as features. As mentioned in Section 2.1, we built a web crawler to collect a large number of webpages from web portal sites in English and Chinese respectively. In the experiments, we mainly focused on six popular domains, namely Business, Entertainment, Health, Science & Technology, Sports, and Politics. For both English and Chinese webpages, the HTML tags were removed and the main content was extracted. The data statistics are shown in Table 1. The bilingual data we used was mainly mined from the web using the method proposed by Jiang et al. (2009), with a post-processing step using our bilingual data cleaning method (Cui et al., 2013). Therefore, the data quality is pretty good. In addition, we also used the English-Chinese parallel corpus released by LDC2 . In total, the bilingual data 2 LDC2003E07, LDC2003E14, LDC2004E12, LDC2005T06, LDC2005T10, LDC2005E83, LDC2006E26, Domain Business Ent. Health Sci&Tech Sports Politics English Docs Words 21M 10.4B 18.3M 8.29B 8.7M 4.73B 10.9M 5.33B 18.9M 9.58B 10.3M 5.56B Chinese Docs Words 7.91M 2.73B 4.16M 1.31B 0.9M 0.42B 5.28M 1.6B 2.49M 0.59B 1.67M 0.39B Table 1: Statistics of web-crawled mono"
D13-1107,W07-0733,0,0.520345,"riety of domains. The translation quality is often unsatisfactory when ∗ This work was done while the first and second authors were visiting Microsoft Research Asia. translating texts from a specific domain using a general model that is trained over a hotchpotch of bilingual corpora. Therefore, domain adaptation is crucial for SMT systems to achieve better performance. Previous research on domain adaptation for SMT includes data selection and weighting (Eck et al., 2004; L¨u et al., 2007; Foster et al., 2010; Moore and Lewis, 2010; Axelrod et al., 2011), mixture models (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Sennrich, 2012; Razmara et al., 2012), and semi-supervised transductive learning (Ueffing et al., 2007), etc. Most of these methods adapt SMT models to a specific domain according to testing data and have achieved good performance. It is natural that real world SMT systems should adapt the models to multiple domains because the input may be heterogeneous, so that the overall translation quality can be improved. Although we can easily apply these methods to multiple domains individually, it is difficult to use the common knowledge across different domains. To leverage the common knowledge, we"
D13-1107,W04-3250,0,0.0374174,"++ (Och and Ney, 2003) in both directions, and the diag-grow-final heuristic was used to refine the symmetric word alignment. The phrase tables were filtered to retain top-20 translation candidates for each source phrase for efficiency. An in-house language modeling toolkit was used to train the 4-gram language models with modified Kneser-Ney smoothing (Kneser and Ney, 1995) over the web-crawled data. The evaluation metric for the overall translation quality was case-insensitive BLEU4 (Papineni et al., 2002). A statistical significance test was performed using the bootstrap resampling method (Koehn, 2004). 3.3 We have two baselines. The first baseline is a nonadapted Hiero using our implementation. It contained the general-domain TM and LM, as well as other standard features. In addition, the fix-discount method (Foster et al., 2006) for phrase table smoothing was also used. The system was general-domain oriented and it was tuned by using MERT (Och, 2003) with a combination of six in-domain development datasets. The second baseline is Google Online Translation Service3 . We obtained the English-toChinese translations of the testing data from Google Translation to have a more solid comparison."
D13-1107,P06-1096,0,0.264664,"Missing"
D13-1107,D07-1036,0,0.13075,"Missing"
D13-1107,P10-2041,0,0.30299,"hold for real world SMT systems since training data for SMT models may come from a variety of domains. The translation quality is often unsatisfactory when ∗ This work was done while the first and second authors were visiting Microsoft Research Asia. translating texts from a specific domain using a general model that is trained over a hotchpotch of bilingual corpora. Therefore, domain adaptation is crucial for SMT systems to achieve better performance. Previous research on domain adaptation for SMT includes data selection and weighting (Eck et al., 2004; L¨u et al., 2007; Foster et al., 2010; Moore and Lewis, 2010; Axelrod et al., 2011), mixture models (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Sennrich, 2012; Razmara et al., 2012), and semi-supervised transductive learning (Ueffing et al., 2007), etc. Most of these methods adapt SMT models to a specific domain according to testing data and have achieved good performance. It is natural that real world SMT systems should adapt the models to multiple domains because the input may be heterogeneous, so that the overall translation quality can be improved. Although we can easily apply these methods to multiple domains individually, it is difficult t"
D13-1107,J03-1002,0,0.00587202,"21K 18K 33K 33K 46K 45K 18K 14K 19K 17K Test En Ch 19K 19K 13K 12K 21K 22K 27K 27K 10K 9K 13K 12K Table 2: Statistics of in-domain training, development and testing data, in number of words. 3.2 Setup An in-house hierarchical phrase-based SMT decoder was implemented for our experiments. The CKY decoding algorithm was used and cube pruning was performed with the same default parameter settings as in Chiang (2007). We used a 100-best list from the decoder for the pairwise ranking algorithm. Translation models were trained over the bilingual data that was automatically word-aligned using GIZA++ (Och and Ney, 2003) in both directions, and the diag-grow-final heuristic was used to refine the symmetric word alignment. The phrase tables were filtered to retain top-20 translation candidates for each source phrase for efficiency. An in-house language modeling toolkit was used to train the 4-gram language models with modified Kneser-Ney smoothing (Kneser and Ney, 1995) over the web-crawled data. The evaluation metric for the overall translation quality was case-insensitive BLEU4 (Papineni et al., 2002). A statistical significance test was performed using the bootstrap resampling method (Koehn, 2004). 3.3 We h"
D13-1107,P03-1021,0,0.0335982,"ser and Ney, 1995) over the web-crawled data. The evaluation metric for the overall translation quality was case-insensitive BLEU4 (Papineni et al., 2002). A statistical significance test was performed using the bootstrap resampling method (Koehn, 2004). 3.3 We have two baselines. The first baseline is a nonadapted Hiero using our implementation. It contained the general-domain TM and LM, as well as other standard features. In addition, the fix-discount method (Foster et al., 2006) for phrase table smoothing was also used. The system was general-domain oriented and it was tuned by using MERT (Och, 2003) with a combination of six in-domain development datasets. The second baseline is Google Online Translation Service3 . We obtained the English-toChinese translations of the testing data from Google Translation to have a more solid comparison. Moreover, we also compared our method with the adapted systems towards each domain individually (Koehn and Schroeder, 2007). This is to demonstrate the superiority of our MTL-based tuning approach across different domains. 3.4 1060 Results The end-to-end translation performance is shown in Table 3. We found that the baseline has a similar performance to G"
D13-1107,P02-1040,0,0.0869864,"rithm. Translation models were trained over the bilingual data that was automatically word-aligned using GIZA++ (Och and Ney, 2003) in both directions, and the diag-grow-final heuristic was used to refine the symmetric word alignment. The phrase tables were filtered to retain top-20 translation candidates for each source phrase for efficiency. An in-house language modeling toolkit was used to train the 4-gram language models with modified Kneser-Ney smoothing (Kneser and Ney, 1995) over the web-crawled data. The evaluation metric for the overall translation quality was case-insensitive BLEU4 (Papineni et al., 2002). A statistical significance test was performed using the bootstrap resampling method (Koehn, 2004). 3.3 We have two baselines. The first baseline is a nonadapted Hiero using our implementation. It contained the general-domain TM and LM, as well as other standard features. In addition, the fix-discount method (Foster et al., 2006) for phrase table smoothing was also used. The system was general-domain oriented and it was tuned by using MERT (Och, 2003) with a combination of six in-domain development datasets. The second baseline is Google Online Translation Service3 . We obtained the English-t"
D13-1107,P12-1099,0,0.144964,"s often unsatisfactory when ∗ This work was done while the first and second authors were visiting Microsoft Research Asia. translating texts from a specific domain using a general model that is trained over a hotchpotch of bilingual corpora. Therefore, domain adaptation is crucial for SMT systems to achieve better performance. Previous research on domain adaptation for SMT includes data selection and weighting (Eck et al., 2004; L¨u et al., 2007; Foster et al., 2010; Moore and Lewis, 2010; Axelrod et al., 2011), mixture models (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Sennrich, 2012; Razmara et al., 2012), and semi-supervised transductive learning (Ueffing et al., 2007), etc. Most of these methods adapt SMT models to a specific domain according to testing data and have achieved good performance. It is natural that real world SMT systems should adapt the models to multiple domains because the input may be heterogeneous, so that the overall translation quality can be improved. Although we can easily apply these methods to multiple domains individually, it is difficult to use the common knowledge across different domains. To leverage the common knowledge, we need to devise a multi-domain adaptati"
D13-1107,E12-1055,0,0.187281,"lation quality is often unsatisfactory when ∗ This work was done while the first and second authors were visiting Microsoft Research Asia. translating texts from a specific domain using a general model that is trained over a hotchpotch of bilingual corpora. Therefore, domain adaptation is crucial for SMT systems to achieve better performance. Previous research on domain adaptation for SMT includes data selection and weighting (Eck et al., 2004; L¨u et al., 2007; Foster et al., 2010; Moore and Lewis, 2010; Axelrod et al., 2011), mixture models (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Sennrich, 2012; Razmara et al., 2012), and semi-supervised transductive learning (Ueffing et al., 2007), etc. Most of these methods adapt SMT models to a specific domain according to testing data and have achieved good performance. It is natural that real world SMT systems should adapt the models to multiple domains because the input may be heterogeneous, so that the overall translation quality can be improved. Although we can easily apply these methods to multiple domains individually, it is difficult to use the common knowledge across different domains. To leverage the common knowledge, we need to devise"
D13-1107,P12-1002,0,0.446041,"ased on mixture models, where each system is tailored for one specific domain with an in-domain Translation Model (TM) and an in-domain Language Model (LM). Meanwhile, all the systems share a same general-domain TM and LM. These SMT systems are considered as several related tasks with a shared feature representation, which fits well into a unified MTL framework. With the MTL-based joint tuning, general knowledge can be better learned by the generaldomain models, while domain knowledge can be better exploited by the in-domain models as well. By using a distributed stochastic learning approach (Simianer et al., 2012), we can estimate the feature weights of multiple SMT systems at the same time. Furthermore, we modify the algorithm to treat in-domain and general-domain features separately, which brings regularization to multiple SMT systems in an efficient way. Experimental results have shown that our method can significantly improve the translation quality on multiple domains over a nonadapted baseline. Moreover, the MTL-based adaptation also outperforms the conventional individual 1056 adaptation approach towards each domain. The rest of the paper is organized as follows: The proposed approach is explain"
D13-1107,P07-1004,0,0.348402,"nd second authors were visiting Microsoft Research Asia. translating texts from a specific domain using a general model that is trained over a hotchpotch of bilingual corpora. Therefore, domain adaptation is crucial for SMT systems to achieve better performance. Previous research on domain adaptation for SMT includes data selection and weighting (Eck et al., 2004; L¨u et al., 2007; Foster et al., 2010; Moore and Lewis, 2010; Axelrod et al., 2011), mixture models (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Sennrich, 2012; Razmara et al., 2012), and semi-supervised transductive learning (Ueffing et al., 2007), etc. Most of these methods adapt SMT models to a specific domain according to testing data and have achieved good performance. It is natural that real world SMT systems should adapt the models to multiple domains because the input may be heterogeneous, so that the overall translation quality can be improved. Although we can easily apply these methods to multiple domains individually, it is difficult to use the common knowledge across different domains. To leverage the common knowledge, we need to devise a multi-domain adaptation approach that jointly adapts the SMT models. Multi-domain adapt"
D15-1106,J08-1001,0,0.0939993,"Missing"
D15-1106,J97-3002,0,0.0402534,"of 1,414 documents on TED talks, and contains 179k sentence pairs, about 3M Chinese words, and 3.3M English words. The language model for SMT is a 4-gram language model trained with the English documents in the training data. The development set is specified by IWSLT as dev2010, and the test set contains 37 documents from tst2010, tst2011 and tst2012. The IWSLT 2014 baseline system is built upon the open-source machine translation toolkit Moses at the default configuration, proposed by (Cettolo et al., 2012). We also train a decoder, which is an in-house Bracketing Transduction Grammar (BTG) (Wu, 1997) in a CKY-style decoder with a lexical reordering model trained with maximum entropy (Xiong et al., 2006). The decoder uses commonly used features, such as translation probabilities, lexical weights, a language model, word penalty, and distortion probabilities. Setting IWSLT Baseline SMT + Rerank tst2010 11.12 12.40 12.55 tst2011 13.34 15.09 15.23 tst2012 13.52 13.70 Table 3: BLEU scores of SMT systems. The IWSLT is a public baseline which issued by the organizer of IWSLT 2014, as described in (Cettolo et al., 2012). The translation performance comparison is shown in Table 3. From Table 3, we"
D15-1106,D14-1218,0,0.181084,"the information in a history vector, and predicts the next word with all the word history in the vector. Word-level language model can only learn the relationship between words in one sentence. For sentences in one document which talks about one or several specific topics, the words in the next sentence are chosen partially in accordance with the previous sentences. To model this kind of coherence of sentences, Le and Mikolov (2014) extend word embedding learning network (Mikolov et al., 2013) to learn the paragraph embedding as a fixed-length vector representation for paragraph or sentence. Li and Hovy (2014) propose a neural network coherence model which employs distributed sentence representation and then predict the probability of whether a sequence of sentences is coherent or not. In contrast to the methods mentioned above which learn the word relationship in or between the sentences separately, we propose a hierarchical recurrent neural network language model (HRNNLM) to capture the word sequence across the sentence boundaries at the document level. HRNNLM is essentially a combination of a wordlevel language model and a sentence-level language model, both of which are recurrent neural network"
D15-1106,P06-1066,0,0.0156601,"3.3M English words. The language model for SMT is a 4-gram language model trained with the English documents in the training data. The development set is specified by IWSLT as dev2010, and the test set contains 37 documents from tst2010, tst2011 and tst2012. The IWSLT 2014 baseline system is built upon the open-source machine translation toolkit Moses at the default configuration, proposed by (Cettolo et al., 2012). We also train a decoder, which is an in-house Bracketing Transduction Grammar (BTG) (Wu, 1997) in a CKY-style decoder with a lexical reordering model trained with maximum entropy (Xiong et al., 2006). The decoder uses commonly used features, such as translation probabilities, lexical weights, a language model, word penalty, and distortion probabilities. Setting IWSLT Baseline SMT + Rerank tst2010 11.12 12.40 12.55 tst2011 13.34 15.09 15.23 tst2012 13.52 13.70 Table 3: BLEU scores of SMT systems. The IWSLT is a public baseline which issued by the organizer of IWSLT 2014, as described in (Cettolo et al., 2012). The translation performance comparison is shown in Table 3. From Table 3, we can find that the rerank system improves SMT performance consistently. For a single sentence without the"
D15-1106,P14-1140,1,0.746216,"Missing"
D15-1106,P13-1017,1,0.460566,"olov et al., 2010) uses a hidden layer which employs a real-valued vector recurrently as network’s input to keep as many history as possible. This makes RNNLM be able to extend for capturing history beyond a sentence. To prevent the potential exponential decay of the history, the history length in RNN can not be too long. Here we approximate the history information of previous sentences, p(Sk |S1 , S2 , ..., Sk−1 ), by the following: For statistical machine translation (SMT) in which we checked out model as a scenario, DNN has also been revealed for certain good results in several components. Yang et al. (2013) adapt and extend the CD-DNN-HMM (Dahl et al., 2012) model to the HMM-based word alignment model. In their method, they use bilingual word embedding to capture the lexical translation information and modeling the context with surrounding words. Liu et al. (2014) propose a recursive recurrent neural network (R2 NN) for end-to-end decoding to help improve translation quality. And Cho et al. (2014) propose a RNN Encoder-Decoder which is a joint recurrent neural network model at the sentence level as conventional SMT decoder does. However, at the discourse level, there is little report on applying"
D15-1106,P03-1021,0,0.0435395,"two Chinese sentence in one document together with their correct translation: 我 拍摄 过 的 冰山, 有些 冰 是 非常 年 轻 - - 几千 年 年龄 Some of the ice in the icebergs that I photograph is very young - - a couple thousand years old. 有些 冰 超过 十万 年 And some of the ice is over 100,000 years old. 6.3.2 Rerank System Our reranking system is a linear model with several features, including the SMT system final scores, sentence-level language model scores, and HRNNLM scores. It should be noted all these features are actually employed by the SMT model except for the HRNNLM score. Since Minimum Error Rate Training (MERT) (Och, 2003) is the most general method adopted in SMT systems for tuning, the feature weights are fixed by MERT. For our reranking system, to score the translation of one sentence we need the translation results of all the previous sentences in the document. Our SMT decoder generates 10-best results of all the sentences of the documents and the rerank905 Chinese word “ 有 些” means “some” in English. But when it is used in parallelism sentences, it means “some of” instead of “some”. The traditional SMT system translates the italics part without considering the context. The translation result for this kind"
D15-1106,P02-1040,0,0.10845,"Missing"
D15-1106,D13-1170,0,0.00405147,"dimensions. Different from them, the sentence vectors of our model are learnt with nearly unlimited sentence history based on a RNN framework, in which, bag of words in the sentence are used as input. The sentence vector is no longer related with the sentence id, but only based on the words in the sentence. And our sentence vector also integrates nearly all the history information of previous sentences, while their model cannot. Li and Hovy (2014) implement a neural network model to predict discourse coherence quality in essays. In their work, recurrent (Sutskever et al., 2011) and recursive (Socher et al., 2013) neural networks are both examined to learn distributed sentence representation given pre-trained word embedding. The distributed sentence representation is assigned to capture both syntactic and semantic information. With a slide window of the distributed sentence representation, a neural network classifier is trained to evaluate the coherence of the text. Successful as it is in scoring the coherence for a given sequence of sentences, this method is attempted to discriminate the different word order within a sentence. Related work An attempt of introducing RNN into convolutional neural networ"
D15-1106,2012.eamt-1.60,0,\N,Missing
D17-1175,P16-1231,0,0.218022,"ies in partial trees. We conduct experiments on PTB and CTB datasets, and the results show that our proposed model achieves state-of-the-art accuracy and significant improvement in labeled precision with respect to the baseline seq2seq model. 1 Introduction Deep learning models have been proven very effective in solving various NLP problems such as language modeling, machine translation and syntactic parsing. For dependency parsing, one line of research aims to incrementally integrate distributed word representations into classic dependency parsing (Chen and Manning, 2014; Weiss et al., 2015; Andor et al., 2016; Cross and Huang, 2016; Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016). Another line of research attempts to leverage end-to-end neural network to perform dependency parsing, such as stack-LSTM and sequence-to-sequence (seq2seq) model (Dyer et al., 2015; Zhang et al., 2017; Wiseman and Rush, 2016). Recently seq2seq model has made significant success in many NLP tasks, such as machine translation and text summarization (Cho et al., 2014; Sutskever et al., 2014; Rush et al., 2015). Unfortunately, to our best knowledge, simply applying seq2seq model to transition-based dependency parsi"
D17-1175,D14-1082,0,0.83352,"ntroduced to capture multiple word dependencies in partial trees. We conduct experiments on PTB and CTB datasets, and the results show that our proposed model achieves state-of-the-art accuracy and significant improvement in labeled precision with respect to the baseline seq2seq model. 1 Introduction Deep learning models have been proven very effective in solving various NLP problems such as language modeling, machine translation and syntactic parsing. For dependency parsing, one line of research aims to incrementally integrate distributed word representations into classic dependency parsing (Chen and Manning, 2014; Weiss et al., 2015; Andor et al., 2016; Cross and Huang, 2016; Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016). Another line of research attempts to leverage end-to-end neural network to perform dependency parsing, such as stack-LSTM and sequence-to-sequence (seq2seq) model (Dyer et al., 2015; Zhang et al., 2017; Wiseman and Rush, 2016). Recently seq2seq model has made significant success in many NLP tasks, such as machine translation and text summarization (Cho et al., 2014; Sutskever et al., 2014; Rush et al., 2015). Unfortunately, to our best knowledge, simply applying seq2seq mo"
D17-1175,D14-1179,0,0.0403433,"Missing"
D17-1175,P16-2006,0,0.024372,". We conduct experiments on PTB and CTB datasets, and the results show that our proposed model achieves state-of-the-art accuracy and significant improvement in labeled precision with respect to the baseline seq2seq model. 1 Introduction Deep learning models have been proven very effective in solving various NLP problems such as language modeling, machine translation and syntactic parsing. For dependency parsing, one line of research aims to incrementally integrate distributed word representations into classic dependency parsing (Chen and Manning, 2014; Weiss et al., 2015; Andor et al., 2016; Cross and Huang, 2016; Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016). Another line of research attempts to leverage end-to-end neural network to perform dependency parsing, such as stack-LSTM and sequence-to-sequence (seq2seq) model (Dyer et al., 2015; Zhang et al., 2017; Wiseman and Rush, 2016). Recently seq2seq model has made significant success in many NLP tasks, such as machine translation and text summarization (Cho et al., 2014; Sutskever et al., 2014; Rush et al., 2015). Unfortunately, to our best knowledge, simply applying seq2seq model to transition-based dependency parsing cannot achieve compa"
D17-1175,D16-1137,0,0.0605328,"Missing"
D17-1175,P81-1022,0,0.754027,"Missing"
D17-1175,P15-1033,0,0.472757,"arning models have been proven very effective in solving various NLP problems such as language modeling, machine translation and syntactic parsing. For dependency parsing, one line of research aims to incrementally integrate distributed word representations into classic dependency parsing (Chen and Manning, 2014; Weiss et al., 2015; Andor et al., 2016; Cross and Huang, 2016; Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016). Another line of research attempts to leverage end-to-end neural network to perform dependency parsing, such as stack-LSTM and sequence-to-sequence (seq2seq) model (Dyer et al., 2015; Zhang et al., 2017; Wiseman and Rush, 2016). Recently seq2seq model has made significant success in many NLP tasks, such as machine translation and text summarization (Cho et al., 2014; Sutskever et al., 2014; Rush et al., 2015). Unfortunately, to our best knowledge, simply applying seq2seq model to transition-based dependency parsing cannot achieve comparable results as in other state-of-the-art methods like stack-LSTM and head selection (Dyer et al., 2015; Zhang et al., 2017). One issue with the simple seq2seq neural network for dependency parsing is that structural linguistic information,"
D17-1175,E17-1063,0,0.0894503,"been proven very effective in solving various NLP problems such as language modeling, machine translation and syntactic parsing. For dependency parsing, one line of research aims to incrementally integrate distributed word representations into classic dependency parsing (Chen and Manning, 2014; Weiss et al., 2015; Andor et al., 2016; Cross and Huang, 2016; Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016). Another line of research attempts to leverage end-to-end neural network to perform dependency parsing, such as stack-LSTM and sequence-to-sequence (seq2seq) model (Dyer et al., 2015; Zhang et al., 2017; Wiseman and Rush, 2016). Recently seq2seq model has made significant success in many NLP tasks, such as machine translation and text summarization (Cho et al., 2014; Sutskever et al., 2014; Rush et al., 2015). Unfortunately, to our best knowledge, simply applying seq2seq model to transition-based dependency parsing cannot achieve comparable results as in other state-of-the-art methods like stack-LSTM and head selection (Dyer et al., 2015; Zhang et al., 2017). One issue with the simple seq2seq neural network for dependency parsing is that structural linguistic information, which plays a key r"
D17-1175,P11-2033,0,0.106709,"LAS 93.00 90.95 92.20 89.70 91.80 89.60 91.57 87.26 93.20 90.90 93.10 90.90 93.99 92.05 93.99 91.90 94.30 91.95 94.10 91.90 92.02 89.10 91.84 88.84 93.65 91.52 93.71 91.60 94.24 92.01 94.16 92.13 CTB Dev Test UAS LAS UAS LAS 86.00 84.40 84.00 82.40 83.90 82.40 87.20 85.90 87.20 85.70 87.60 86.10 87.35 85.85 87.84 86.15 86.21 83.80 85.80 83.53 87.28 85.30 87.41 85.40 88.06 86.30 87.97 86.18 Table 1: Results of various state-of-the-art parsing systems on English dataset (PTB with Stanford Dependencies) and Chinese dataset (CTB). The numbers reported from different systems are taken from: Z&N11 (Zhang and Nivre, 2011); C&M14 (Chen and Manning, 2014); ConBSO (Wiseman and Rush, 2016); Dyer15 (Dyer et al., 2015); Weiss15 (Weiss et al., 2015); K&G16 (Kiperwasser and Goldberg, 2016); DENSE (Zhang et al., 2017). p 6/(drow + dcol ), where drow and dcol are the number of rows and columns (Glorot and Bengio, 2010). Our models are trained on a Tesla K40m GPU and optimized with vanilla SGD algorithm with mini-batch size 64 for English dataset and 32 for Chinese dataset. The initial learning rate is set to 2 and will be halved when unlabeled attachment scores (UAS) on the development set do not increase for 900 batche"
D17-1175,Q16-1023,0,0.291671,"s on PTB and CTB datasets, and the results show that our proposed model achieves state-of-the-art accuracy and significant improvement in labeled precision with respect to the baseline seq2seq model. 1 Introduction Deep learning models have been proven very effective in solving various NLP problems such as language modeling, machine translation and syntactic parsing. For dependency parsing, one line of research aims to incrementally integrate distributed word representations into classic dependency parsing (Chen and Manning, 2014; Weiss et al., 2015; Andor et al., 2016; Cross and Huang, 2016; Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016). Another line of research attempts to leverage end-to-end neural network to perform dependency parsing, such as stack-LSTM and sequence-to-sequence (seq2seq) model (Dyer et al., 2015; Zhang et al., 2017; Wiseman and Rush, 2016). Recently seq2seq model has made significant success in many NLP tasks, such as machine translation and text summarization (Cho et al., 2014; Sutskever et al., 2014; Rush et al., 2015). Unfortunately, to our best knowledge, simply applying seq2seq model to transition-based dependency parsing cannot achieve comparable results as in other state-"
D17-1175,J93-2004,0,0.0581776,"Sc < 2 I(yi ) =  1 otherwise (8) where Sc represents the number of words in the stack and Wc is the number of source words that are not pushed into the stack. To introduce these constraints, the conditional probability of each target symbol yi can be rewritten as exp (gi ) ∗ I(yi ) p(yi |y<i , h) = P k exp (gk ) ∗ I(yk ) (9) where gi is the ith element of g(yi−1 , zi , ci ). 4 Experiments In this section, we evaluate our parsing model on the English and Chinese datasets. Following Dyer et al. (2015), Stanford Dependencies (de Marneffe and Manning, 2008) conversion of the Penn Treebank (PTB) (Marcus et al., 1993) and Chinese Treebank 5.1 (CTB) are adopted. We leverage the arc-standard algorithm for our dependency parsing. In addition, we limit the vocabulary to contain up to 20k most frequent words and convert remaining words into the <unk&gt; token. 4.1 Setup For our model, 3-layers GRU is used for encoder and decoder. The dimension of word embedding is 300, the dimension of POS-tag/action embedding is 32, and the size of hidden units in GRU is 500. 3-layers attention structure is adopted in our model. Following Chen and Manning (2014); Dyer et al. (2015), we used 300-dimensional pretrained GloVe vector"
D17-1175,W04-0308,0,0.0771375,"Ua ht + Sa st ) i,t = va tanh(Wa [zi−1 ; ci (7) where W m is the weight matrix. With this network structure, we obtain different context vectors (c1i , c2i , . . . , cli ), and the final context vector ci , which is considered as complex context information, is replaced by the concatenation of those vectors: ci = [c1i ; c2i ; . . . ; cli ]. Decoder: Unlike machine translation and text summarization in which seq2seq model is widely applied, a sequence of action in dependency parsing must satisfy some constraints so that they can generate a dependency tree. Following the arcstandard algorithm (Nivre, 2004), the precondition can be categorized as 1) SHIFT(SH): There exists at least one word that is not pushed into the stack; 2) LEFT-ARC(LR(d)) and RIGHT-ARC(RR(d)): There are at least two words in the stack. These two constraints can be defined as indicator functions  yi = SH, Wc ≤ 0  0 0 yi = LR(d) or RR(d), Sc < 2 I(yi ) =  1 otherwise (8) where Sc represents the number of words in the stack and Wc is the number of source words that are not pushed into the stack. To introduce these constraints, the conditional probability of each target symbol yi can be rewritten as exp (gi ) ∗ I(yi ) p(yi |"
D17-1175,D14-1162,0,0.087559,"d Chinese Treebank 5.1 (CTB) are adopted. We leverage the arc-standard algorithm for our dependency parsing. In addition, we limit the vocabulary to contain up to 20k most frequent words and convert remaining words into the <unk&gt; token. 4.1 Setup For our model, 3-layers GRU is used for encoder and decoder. The dimension of word embedding is 300, the dimension of POS-tag/action embedding is 32, and the size of hidden units in GRU is 500. 3-layers attention structure is adopted in our model. Following Chen and Manning (2014); Dyer et al. (2015), we used 300-dimensional pretrained GloVe vectors (Pennington et al., 2014) to initialize our word embedding matrix. Other model parameters are initialized using a normal distribution with a mean of 0 and a variance of 1679 Parser Z&N11 C&M14 ConBSO Dyer15 Weiss15 K&G16 DENSE seq2seq Our model Ensemble PTB-SD Dev Test UAS LAS UAS LAS 93.00 90.95 92.20 89.70 91.80 89.60 91.57 87.26 93.20 90.90 93.10 90.90 93.99 92.05 93.99 91.90 94.30 91.95 94.10 91.90 92.02 89.10 91.84 88.84 93.65 91.52 93.71 91.60 94.24 92.01 94.16 92.13 CTB Dev Test UAS LAS UAS LAS 86.00 84.40 84.00 82.40 83.90 82.40 87.20 85.90 87.20 85.70 87.60 86.10 87.35 85.85 87.84 86.15 86.21 83.80 85.80 83.5"
D17-1175,D15-1044,0,0.0506612,"ributed word representations into classic dependency parsing (Chen and Manning, 2014; Weiss et al., 2015; Andor et al., 2016; Cross and Huang, 2016; Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016). Another line of research attempts to leverage end-to-end neural network to perform dependency parsing, such as stack-LSTM and sequence-to-sequence (seq2seq) model (Dyer et al., 2015; Zhang et al., 2017; Wiseman and Rush, 2016). Recently seq2seq model has made significant success in many NLP tasks, such as machine translation and text summarization (Cho et al., 2014; Sutskever et al., 2014; Rush et al., 2015). Unfortunately, to our best knowledge, simply applying seq2seq model to transition-based dependency parsing cannot achieve comparable results as in other state-of-the-art methods like stack-LSTM and head selection (Dyer et al., 2015; Zhang et al., 2017). One issue with the simple seq2seq neural network for dependency parsing is that structural linguistic information, which plays a key role in classic transition-based or graph-based dependency parsing models, cannot be explicitly employed. For example, classic transition-based parsing algorithm utilizes a stack to manage the heads of partial s"
D17-1175,P15-1032,0,0.458119,"tiple word dependencies in partial trees. We conduct experiments on PTB and CTB datasets, and the results show that our proposed model achieves state-of-the-art accuracy and significant improvement in labeled precision with respect to the baseline seq2seq model. 1 Introduction Deep learning models have been proven very effective in solving various NLP problems such as language modeling, machine translation and syntactic parsing. For dependency parsing, one line of research aims to incrementally integrate distributed word representations into classic dependency parsing (Chen and Manning, 2014; Weiss et al., 2015; Andor et al., 2016; Cross and Huang, 2016; Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016). Another line of research attempts to leverage end-to-end neural network to perform dependency parsing, such as stack-LSTM and sequence-to-sequence (seq2seq) model (Dyer et al., 2015; Zhang et al., 2017; Wiseman and Rush, 2016). Recently seq2seq model has made significant success in many NLP tasks, such as machine translation and text summarization (Cho et al., 2014; Sutskever et al., 2014; Rush et al., 2015). Unfortunately, to our best knowledge, simply applying seq2seq model to transition-ba"
I05-3001,W03-1721,0,0.061811,"raining material for participants and they serve as a gold standard for evaluating the performance of participant systems. This paper presents a semi-automatic method to detect segmentation errors in a manually annotated Chinese corpus in order to improve its quality further. Especially a segmentation error rate of a gold standard corpus could be obtained with our approach. As we know a particular Chinese character string occurring more than once in a corpus may be assigned different segmentations. Those differences are considered as segmentation inconsistencies by some researchers (Wu, 2003; Chen, 2003). Segmentation consistency is also considered as one of the quality criteria of an annotated Chinese corpus (Sun, 1999). But in order to provide a more clearer description of those segmentation differences we define a new This paper proposes a semi-automatic method to detect segmentation errors in a manually annotated Chinese corpus in order to improve its quality further. A particular Chinese character string occurring more than once in a corpus may be assigned different segmentations during a segmentation process. Based on these differences our approach outputs the segmentation error candida"
I05-3001,W03-1727,0,0.0265932,"provide training material for participants and they serve as a gold standard for evaluating the performance of participant systems. This paper presents a semi-automatic method to detect segmentation errors in a manually annotated Chinese corpus in order to improve its quality further. Especially a segmentation error rate of a gold standard corpus could be obtained with our approach. As we know a particular Chinese character string occurring more than once in a corpus may be assigned different segmentations. Those differences are considered as segmentation inconsistencies by some researchers (Wu, 2003; Chen, 2003). Segmentation consistency is also considered as one of the quality criteria of an annotated Chinese corpus (Sun, 1999). But in order to provide a more clearer description of those segmentation differences we define a new This paper proposes a semi-automatic method to detect segmentation errors in a manually annotated Chinese corpus in order to improve its quality further. A particular Chinese character string occurring more than once in a corpus may be assigned different segmentations during a segmentation process. Based on these differences our approach outputs the segmentation"
I05-3001,P03-1035,1,0.825212,"ntation inconsistency S3. A character string inconsistently between a test data and its training In the close test of Bakeoff1, participants could only use training material from the training data for the particular corpus being testing on. No other material was allowed (Sproat and Emerson, 2003). As we know that the test data should be consistent with the training data based on a general definition of Chinese words. That is if we collect all words seen in the training data and store them into a lexicon, then each word in a test set is either a lexicon word or an OOV (out of vocabulary) word (Gao et al., 2003). In another word, if a character string has been treated as one word, i.e. a lexicon word, in the training data, the same occurrence should be taken in the data. This situation could be divided into the following two cases further: S3.1 A word identified in a training data has been segmented into multiple words in corresponding test data; S3.2 A word identified in a test data has been segmented into multiple words in corresponding training data. Chen (2003) describes inconsistency problem found in cases S1, S2 and S3.1 of PK corpora. For example, he gives the amount of unique text fragments t"
I05-3001,E03-1068,0,0.0259954,"e. Thus a segmentation variation (type) consists of more than one variation instances in corpora C. And a variation instance may include one or more than one tokens. Definition 4: If a variation instance is an incorrect segmentation, it is called an error instance (EI). The definitions of segmentation variation, variation instance and error instance (EI) clearly distinguish those inconsistent components, so we can count the number of segmentation errors (in tokens) exactly. The term variation is also used to express other annotation inconsistency in a corpus by other researchers. For example, Dickinson and Meurers (2003) used variation to describe POS (Part-of-Speech) inconsistency in an annotated corpus. Example 1: Segmentation variations (Bakeoff1 PK corpus): Word &quot;ㄝৠ[deng3-tong2]&quot; is segmented as &quot;ㄝৠ&quot; (equal) and &quot;ㄝৠ&quot; (et al. with). Word &quot;咘䞥[huang2-jin1-zhou1]&quot; is segmented as &quot;咘䞥&quot; (golden week) and &quot; 咘䞥&quot; (gold week). Word &quot;[⋕⥝⏙ބbing1-qing1-yu4-jie2]&quot; is segmented as &quot;( &quot;⋕⥝⏙ބpure and noble) and &quot;( &quot;⋕⥝⏙ބice clear jade clean). In example 1, Words like “ㄝৠ”, “咘䞥  ” and “  ” ⋕ ⥝ ⏙ ބare segmentation variation types. Segmentations “ㄝৠ” and “ ㄝ  ৠ ” are two variation instances of segmentation va"
I05-3001,W03-1719,0,\N,Missing
I05-3001,W03-1726,0,\N,Missing
J05-4005,J96-1002,0,0.0113923,"Missing"
J05-4005,J95-4004,0,0.0136868,"ecall rates (Wu 2003). Therefore, we do not assume that an application-independent universal word segmentation standard exists. We argue instead for the existence of multiple segmentation standards, each for a specific application. It is undesirable to develop a set of application-specific segmenters. A better solution would be to develop a generic segmenter with customizable output that is able to provide alternative segmentation units according to the specification that is either predefined or implied in the application data. To achieve this, we present a transformation-based learning (TBL; Brill 1995) method, to be described in Section 6. We implement the pragmatic approach to Chinese word segmentation in an adaptive Chinese word segmenter called MSRSeg. It consists of two components: (1) a generic segmenter that is based on the linear mixture model framework of word breaking and unknown word detection and that can adapt to domain-specific vocabularies, and (2) a set of output adaptors for adapting the output of (1) to different application-specific standards. Evaluation on five test sets with different standards shows that the adaptive system achieves state-of-the-art performance on all t"
J05-4005,O97-4005,0,0.0121829,"mance of these methods thus depends to a large degree upon the coverage of the dictionary, which unfortunately may never be complete because new words appear constantly. Therefore, in addition to the dictionary, many systems also contain special components for unknown word identification. 533 Computational Linguistics Volume 31, Number 4 In particular, statistical methods have been widely applied because they use a probabilistic or cost-based scoring mechanism rather than a dictionary to segment the text. These methods have three drawbacks. First, some of these methods (e.g., Lin et al. 1993; Chang and Su 1997) identify OOV (out-of-vocabulary) words without identifying their types. For instance, one might identify a string as a unit but fail to identify that it is a person name. Second, many current statistical methods do not incorporate linguistic knowledge effectively into segmentation. For example, Teahan et al. (2000) and Dai et al. (1999) do not use any linguistic knowledge. Thus, the identified OOV words are likely to be linguistically implausible, and consequently, additional manual checking is needed for some subsequent tasks such as parsing. Third, in many current segmenters, OOV identifica"
J05-4005,W03-1721,0,0.0815062,"ir types. For instance, one might identify a string as a unit but fail to identify that it is a person name. Second, many current statistical methods do not incorporate linguistic knowledge effectively into segmentation. For example, Teahan et al. (2000) and Dai et al. (1999) do not use any linguistic knowledge. Thus, the identified OOV words are likely to be linguistically implausible, and consequently, additional manual checking is needed for some subsequent tasks such as parsing. Third, in many current segmenters, OOV identification is considered a separate process from segmentation (e.g., Chen 2003; Wu and Jiang 2000; Chen and Bai 1998). For instance, Chen (2003) assumes that OOV words are usually two or more characters long and are often segmented into single characters. He then uses different components to detect OOV words of different types in a cascaded manner after the basic word segmentation. We believe that the identification of OOV words should not be treated as a problem separate from word segmentation. We propose a unified approach that solves both problems simultaneously. A previous work along this line is Sproat et al. (1996), which is based on weighted finite-state transduc"
J05-4005,O98-3002,0,0.0319127,"ght identify a string as a unit but fail to identify that it is a person name. Second, many current statistical methods do not incorporate linguistic knowledge effectively into segmentation. For example, Teahan et al. (2000) and Dai et al. (1999) do not use any linguistic knowledge. Thus, the identified OOV words are likely to be linguistically implausible, and consequently, additional manual checking is needed for some subsequent tasks such as parsing. Third, in many current segmenters, OOV identification is considered a separate process from segmentation (e.g., Chen 2003; Wu and Jiang 2000; Chen and Bai 1998). For instance, Chen (2003) assumes that OOV words are usually two or more characters long and are often segmented into single characters. He then uses different components to detect OOV words of different types in a cascaded manner after the basic word segmentation. We believe that the identification of OOV words should not be treated as a problem separate from word segmentation. We propose a unified approach that solves both problems simultaneously. A previous work along this line is Sproat et al. (1996), which is based on weighted finite-state transducers (FSTs). Our approach is similarly m"
J05-4005,W02-1001,0,0.00419524,"d feature value f (s, wR )), and decreases the parameter values whose models were “overestimated” (i.e., f (s, w) is larger than f (s, wR )). Empirically, the sequence of these updates, when iterated over all training samples, provides a reasonable approximation to descending the gradient with respect to the original loss function of Equation (5). Although this method cannot guarantee a globally optimal solution, it is chosen for our modeling because of its efficiency and because it achieved the best results in our experiments. The algorithm is similar to the perceptron algorithm described in Collins (2002). The key difference is that, instead of using the delta rule of Equation (8) (as shown in line 5 of Figure 4), Collins (2002) updates parameters using the rule: λdt+1 ← λtd + fd (wRi ) − fd (wi ). Our pilot study shows that our algorithm achieves slightly better results. 4.3 Discussions on Robustness The training methods described in Section 4.2 aim at minimizing errors in a training set. But test sets can be different. The robustness issue concerns how well the minimal error rate in the training set preserves in the test set. According to Dudewicz and Mishra (1988), the MSE function in gener"
J05-4005,P00-1073,1,0.796307,"hose words whose probability of appearing in a new document is lower than general lexical words. Let Pi (k) be the probability of word wi that occurs k times in a document. In our experiments, we assume that P(NW |wi ) can be approximated by the probability of wi occurring less than K times in a new document: P(NW |wi ) ≈ K1  Pi (k), (14) k=0 where the constant K (7 in our experiments) is dependent on the size of the document: The larger the document, the larger the value. Pi (k) can be estimated using several term ¨ distribution models (Chapter 15.3 in Manning and Schutze [1999]). Following Gao and Lee (2000), we use K-Mixture (Katz 1996) which estimates Pi (k) as Pi (k) = (1 − a)δk, 0 + a ( β )k , β+1 β+1 (15) where δk,0 =1 if k=0; 0, otherwise. α and β are parameters that can be fit using the observed mean λ and the observed inverse document frequency IDF as follows: λ= cf , IDF = log N , N df β = λ × 2IDF − 1 = cf − df , and a = λ , β df (16) where cf is the total number of occurrence of word wi in training data, df is the number of documents in training data in which wi occurs, and N is the total number of documents. In our implementation, the training data contain approximately 40,000 documen"
J05-4005,O01-2002,1,0.812473,"Missing"
J05-4005,P03-1035,1,0.48854,"Missing"
J05-4005,P04-1059,1,0.834246,"tions. This inspires the development of an adaptive Chinese word segmenter. However, most of the previous segmenters have been developed according to a standard that assumes a single correct segmentation. The only adaptive system, to the best of our knowledge, is the customizable segmenter described in Wu (2003), in which the display of the segmentation output can be customized by users.3 The adaptation method we will describe in Section 6 can be viewed as an improved version in that the adaptation rules (or transformations) are acquired automatically from application data via the TBL method (Gao et al. 2004). Though the use of TBL for Chinese word segmentation is not new (see Palmer [1997]; Hockenmaier and Brew [1998]), none of the previous work is aimed at standards adaptation. 2.4 Evaluation The performance of Chinese word segmenters is generally reported in terms of precision and recall. However, a comparison across systems could be very difficult for two reasons. First, the “correct” segmentation is not clearly defined. It is common that for a given sentence there are multiple plausible word segmentations. As shown in Sproat et al. (1996), the rate of agreement between two human judges is les"
J05-4005,H05-1027,1,0.678464,"ss function. We will present in turn the loss function and the optimization algorithm. 4.2.1 Loss Function. Assume that we can measure the number of segmentation errors in w by comparing it with a reference segmentation wR using an error function Er(wR , w) (i.e., editing distance, in our case). The training criterion that directly minimizes the segmentation errors over the training data is λ ∗ = arg min λ  Er(wRi , w(si , λ )), (4) i=1...M where w(si , λ ) is the segmentation determined by Equation (3), where it is denoted as w∗ . Equation (4) is referred to as the minimum sample risk (MSR; Gao et al. 2005) criterion hereafter. Notice that without knowing the “true” distribution of the data, the best λ can be chosen approximately based on training samples. This is known as the principle of empirical risk minimization (ERM; Vapnik 1998): If the segmenter were trained using exactly the MSR criterion, it would converge to a Bayes risk performance (minimal error rate) as the training size goes to infinity. However, Er(.) is a piecewise constant function of the model parameter λ , and thus a poor candidate for optimization by any simple gradient-type numerical search. For example, the gradient cannot"
J05-4005,O97-4003,0,0.051105,"Approach Figure 2 Taxonomy of morphologically derived words (MDWs) in MSRSeg. 3.2 MSR Standard The taxonomy employed here has been specified in detail in the MSR standard. There are two general guidelines for the development of the standard: 1. The standard should be applicable to a wide variety of NLP tasks, of which some representative examples are Chinese text input, IR, TTS, ASR, and MT. 2. The standard should be compatible with existing standards, of which representative examples are the Chinese NE standards in ET/ER-99, the Mainland standard (GB/T), Taiwan’s ROCLING standard (CNS14366; Huang et al. 1997), and the UPenn Chinese Treebank (Xia 1999), as much as possible.4 We are seeking a standard that is “linguistically felicitous, computationally feasible, and [ensures] data uniformity” (Huang et al. 1997; Sproat and Shih 2002). The MSR standard consists of a set of specific rules that aims at unambiguously determining the word segmentation of a Chinese sentence, given a reference lexicon. The development of the standard is an iterative procedure, interacting with the development of a gold test set (which we will describe in the next section). We begin with an initial set of 4 MET is a Chinese"
J05-4005,W03-1701,1,0.693218,"hods use metrics that are based on statistical features such as mutual information, term frequency, and their variants. They require a reasonably large training corpus. The new words detected are mostly proper nouns and other relatively frequent words. Unfortunately, new words, under our definition of the term, may not be detected.  534  ^x Gao et al. Chinese Word Segmentation: A Pragmatic Approach Fewer methods have been proposed for an on-line approach, and that is the focus of this article. Some recent advances in on-line NWI explore the use of machine learning approaches. For example, Li et al. (2003) define NWI as a binary classification problem and use support vector machines (SVM) to combine various linguistically motivated features to determine whether a Chinese character sequence is a word. Our method is an extension of that of Li et al. in that NWI is not a stand-alone process in our system but an integral part of word segmentation. We shall show experimentally the benefit of the integration in Section 5.5. 2.3 Standards Adaptation As described earlier, while Chinese words are supposed to be well-defined, unambiguous, and static linguistic entities, we are more concerned with segment"
J05-4005,O93-1004,0,0.0610786,"1994). The performance of these methods thus depends to a large degree upon the coverage of the dictionary, which unfortunately may never be complete because new words appear constantly. Therefore, in addition to the dictionary, many systems also contain special components for unknown word identification. 533 Computational Linguistics Volume 31, Number 4 In particular, statistical methods have been widely applied because they use a probabilistic or cost-based scoring mechanism rather than a dictionary to segment the text. These methods have three drawbacks. First, some of these methods (e.g., Lin et al. 1993; Chang and Su 1997) identify OOV (out-of-vocabulary) words without identifying their types. For instance, one might identify a string as a unit but fail to identify that it is a person name. Second, many current statistical methods do not incorporate linguistic knowledge effectively into segmentation. For example, Teahan et al. (2000) and Dai et al. (1999) do not use any linguistic knowledge. Thus, the identified OOV words are likely to be linguistically implausible, and consequently, additional manual checking is needed for some subsequent tasks such as parsing. Third, in many current segmen"
J05-4005,P03-1021,0,0.00297496,"t the source–channel models are the rationale behind our system, e.g., the decoding process described in Section 5.6 follows the framework. Linear models are just another representation based on the optimization algorithm of class model weights. 4.2 Linear Models The framework of linear models is derived from linear discriminant functions widely used for pattern classification (Duda, Hart, and Stork 2001) and has been recently introduced into NLP tasks by Collins and Duffy (2001). It is also related to (log-)linear models described in Berger, Della Pietra, and Della Pietra (1996), Xue (2003); Och (2003), and Peng, Feng, and McCallum (2004). We use the following notation in the rest of the article. r r 546 Training data are a set of example input/output pairs. In Chinese word segmentation, we have training samples {si , wRi }, for i = 1 . . . M, where each si is an input Chinese character sequence and each wRi is the reference segmentation (i.e., word class sequence) of si . We assume a set of D + 1 features fd (s, w), for d = 0 . . . D. The features are arbitrary functions that map (s, w) to real values. Using vector notation, we have f(s, w) ∈ D+1 , where f(s, w) = {f0 (s, w), f1 (s, w), ."
J05-4005,P97-1041,0,0.0213808,"Missing"
J05-4005,C04-1081,0,0.204751,"still informative. 568 Gao et al. Chinese Word Segmentation: A Pragmatic Approach Table 22 Cross-system comparison results. # OAS Segmenters LN PN ON errors P R F P R F P R F 63 49 20 7 .935 .854 .767 .876 .442 .720 .736 .864 .600 .782 .752 .870 .907 .945 .780 .830 .744 .781 .787 .897 .818 .856 .784 .862 .642 .713 .817 .799 .469 .131 .216 .617 .600 .222 .342 .696 MSWS LCWS PBWS MSRSeg Table 23 Comparisons against other segmenters: In Column 1, SXX indicates participating sites in the 1st SIGHAN International Chinese Word Segmentation Bakeoff, and CRFs indicates the word segmenter reported in (Peng et al. 2004). In Columns 2 to 5, entries contain the F-measure of each segmenter on different open runs, with the best performance in bold. Column Site-Avg is the average F-measure over the data sets on which a segmenter reported results of open runs, where a bolded entry indicates the segmenter outperforms MSRSeg. Column Our-Avg is the average F-measure of MSRSeg over the same data sets, where a bolded entry indicates that MSRSeg outperforms the other segmenter. ASo S01 S02 S03 S04 S05 S06 S07 S08 S09 S10 S11 S12 CRFs MSRSeg ASc .872 CTBo CTBc .881 .912 .829 .881 .874 .942 .945 HKo HKc PKo PKc Site-Avg O"
J05-4005,W03-1719,0,0.120733,"into the first three: descriptive, expository, and narrative. Practical writing is just an umbrella term for practical writing such as notes, letter, e-mails, and marriage announcements. 541 Computational Linguistics Volume 31, Number 4 Figure 3 Fragments of the MSR gold test set. Therefore, we evaluate MSRSeg using five corpora, each corresponding to a different standard, and consistent train–test splits, as shown in Table 4. MSR is described in previous sections, and the other four are standards used in SIGHAN’s First International Chinese Word Segmentation Bakeoff (or Bakeoff for brevity) (Sproat and Emerson 2003). In the Bakeoff corpora, OOV is defined as the set of words in the test corpus not occurring in the training corpus. In experiments, we always consider the following adaptation paradigm. Suppose we have a general predefined standard according to which we create a large amount of training data. We then develop a generic word segmenter. Whenever we deploy the segmenter for any application, we customize the output of the segmenter according to an application-specific standard that can be partially acquired from a given small amount of application data (called adaptation data). The MSR standard d"
J05-4005,J96-3004,0,0.39825,"on is considered a separate process from segmentation (e.g., Chen 2003; Wu and Jiang 2000; Chen and Bai 1998). For instance, Chen (2003) assumes that OOV words are usually two or more characters long and are often segmented into single characters. He then uses different components to detect OOV words of different types in a cascaded manner after the basic word segmentation. We believe that the identification of OOV words should not be treated as a problem separate from word segmentation. We propose a unified approach that solves both problems simultaneously. A previous work along this line is Sproat et al. (1996), which is based on weighted finite-state transducers (FSTs). Our approach is similarly motivated but is based on a different mechanism: linear mixture models. As we shall see, the models provide a more flexible framework for incorporating various kinds of lexical and statistical information. Many types of OOV words that are not covered in Sproat’s system can be dealt with in our system. The linear models we used are originally derived from linear discriminant functions widely used for pattern classification (Duda, Hart, and Stork 2001) and have been recently introduced into NLP tasks by Colli"
J05-4005,C02-1012,1,0.213291,"Missing"
J05-4005,O03-5001,1,0.841659,"Missing"
J05-4005,W01-0512,0,0.00770431,"Missing"
J05-4005,W00-1207,1,0.539293,"or instance, one might identify a string as a unit but fail to identify that it is a person name. Second, many current statistical methods do not incorporate linguistic knowledge effectively into segmentation. For example, Teahan et al. (2000) and Dai et al. (1999) do not use any linguistic knowledge. Thus, the identified OOV words are likely to be linguistically implausible, and consequently, additional manual checking is needed for some subsequent tasks such as parsing. Third, in many current segmenters, OOV identification is considered a separate process from segmentation (e.g., Chen 2003; Wu and Jiang 2000; Chen and Bai 1998). For instance, Chen (2003) assumes that OOV words are usually two or more characters long and are often segmented into single characters. He then uses different components to detect OOV words of different types in a cascaded manner after the basic word segmentation. We believe that the identification of OOV words should not be treated as a problem separate from word segmentation. We propose a unified approach that solves both problems simultaneously. A previous work along this line is Sproat et al. (1996), which is based on weighted finite-state transducers (FSTs). Our app"
J05-4005,O03-4001,1,0.946382,"ion. Word breaking refers to the process of segmenting known words that are predefined in a lexicon. Word segmentation refers to the process of both lexicon word segmentation and unknown word detection. 2 New words in this article refer to out-of-vocabulary words that are neither recognized as named entities or factoids nor derived by morphological rules. These words are mostly domain-specific and/or time-sensitive (see Section 5.5 for details). 532 Gao et al. Chinese Word Segmentation: A Pragmatic Approach formation retrieval (IR) systems prefer shorter “words” to obtain higher recall rates (Wu 2003). Therefore, we do not assume that an application-independent universal word segmentation standard exists. We argue instead for the existence of multiple segmentation standards, each for a specific application. It is undesirable to develop a set of application-specific segmenters. A better solution would be to develop a generic segmenter with customizable output that is able to provide alternative segmentation units according to the specification that is either predefined or implied in the application data. To achieve this, we present a transformation-based learning (TBL; Brill 1995) method, t"
J05-4005,O03-4002,0,0.770183,"ear mixture models. As we shall see, the models provide a more flexible framework for incorporating various kinds of lexical and statistical information. Many types of OOV words that are not covered in Sproat’s system can be dealt with in our system. The linear models we used are originally derived from linear discriminant functions widely used for pattern classification (Duda, Hart, and Stork 2001) and have been recently introduced into NLP tasks by Collins and Duffy (2001). Other frameworks of Chinese word segmentation, which are similar to the linear models, include maximum entropy models (Xue 2003) and conditional random fields (Peng, Feng, and McCallum 2004). They also use a unified approach to word breaking and OOV identification. 2.2 More on New Word Identification In this article, we use the term “new words” to refer to OOV words other than named entities, factoids, and morphologically derived words. “New words” are mostly domain‘cellular’) and time-sensitive political, social, or cultural specific terms (e.g., ‘Three Links’; ‘SARS’). There have been two general approaches terms (e.g., to NWI. The first is to acquire new words from large corpora off-line and put them into a dictiona"
J05-4005,W03-1718,1,\N,Missing
J05-4005,P98-2206,0,\N,Missing
J05-4005,C98-2201,0,\N,Missing
J05-4005,J00-3004,0,\N,Missing
J05-4005,W03-1726,0,\N,Missing
K18-1019,P16-1185,0,0.0373358,"Missing"
K18-1019,J07-2003,0,0.136275,"e whole training process. It confirms that our proposed approach not only stabilizes GAN training but also achieves better results. 31 30 29 BLEU 28 27 26 25 24 23 22 10000 20000 30000 40000 50000 60000 70000 80000 90000 Mini-Batches Figure 2: The BLEU score changes on IWSLT2014 German-English validation set for RNNSearch, Adversarial-NMT* and BGAN-NMT as training progresses. 4.3 We also conduct experiments on Chinese-English translation task with strong SMT and NMT baselines: HPSMT, RNNSearch and AdversarialNMT*. HPSMT is an in-house implementation of the hierarchical phrase-based MT system (Chiang, 2007), where a 4-gram language model is trained using the modified Kneser-Ney smoothing algorithm over the target data from bilingual data. Table 2 shows the evaluation results of different models on NIST datasets. All the results are reported based on case-insensitive BLEU. We can observe that RNNSearch significantly outperforms HPSMT by 4.78 BLEU points on average, and BGAN-NMT can further improve the performances, with 2.33 BLEU points on average. Additionally, our BGAN-NMT gains better performances than Adversarial-NMT* with 1.03 BLEU points on average. These experimental results confirm the ef"
K18-1019,N03-1017,0,0.124311,"Missing"
K18-1019,J82-2005,0,0.733933,"Missing"
K18-1019,D17-1230,0,0.0285585,"ndidates are used as positive and negative examples of discriminator training respectively. Due to the computation cost, we cannot generate many negative examples, so that the discriminator is easy to overfit. The overfitted discriminator will give biased signals to the generator and make it update incorrectly, leading to the instability of the generator training. Wu et al. (2017) found that combining adversarial training objective with MLE can significantly improve the stability of generator training, which is also reported in language model and neural dialogue generation (Lamb et al., 2016; Li et al., 2017). Actually, although this method leverages real translation signal to guide the generator and alleviate the effect of overfitted discriminator, it cannot deal with the inadequate training problem of the discriminator, which essentially plays a more important role in GAN training. cients α1 , α2 , ... , αT calculated with exp (a(ht , zi−1 )) αt = P k exp (a(hk , zi−1 )) (3) where a is a feed-forward neural network with a single hidden layer. 2.1.2 MLE Training NMT systems are usually trained to maximize the conditional log-probability of the correct translation given a source sentence with resp"
K18-1019,P15-1002,0,0.0274615,"optimized using the vanilla SGD algorithm with mini-batch 32 for De-En and 128 for Zh-En. We re-normalize gradients if their norm exceeds 2.0. The initial learning rate is set as 0.2 for De-En and 0.02 for Zh-En, and it is halved when BLEU scores on the validation set do not increase for 20,000 batches. To generate the synthetic bilingual data, beam search strategy with beam size 4 is adopted for both De-En and Zh-En. At test time, beam search is employed to find the best translation with beam size 8 and translation probabilities normalized by the length of the candidate translations. Follow Luong et al. (2015), <unk&gt; is replaced with the corresponding target word in a post processing step. Model Architecture RNNSearch model proposed by Bahdanau et al. (2014) is leveraged to be the translation model, but it should be noted that our BGAN-NMT is independent of the NMT network structure. We use a single layer GRU for encoder and decoder. For Zh-En, the size of word embedding (for both source and target words) is 256 and the size of hidden layer is set to 1024. For De-En, in order to compare with previous work (Ranzato et al., 2015; Bahdanau et al., 2016), the size of word embedding and GRU hidden state"
K18-1019,P02-1040,0,0.103721,"ess: the generator G can be improved with the discriminator D in GAN 1, and then the enhanced G serves as a better discriminator to guide 4 4.1 Experiments Setup To examine the effectiveness of our proposed approach, we conduct experiments on translation 194 Methods MIXER (Ranzato et al., 2015) MRT (Shen et al., 2016) BSO (Wiseman and Rush, 2016) Adversarial-NMT (Wu et al., 2017) A-C (Bahdanau et al., 2016) Softmax-Q (Ma et al., 2017) Adversarial-NMT* BGAN-NMT tasks with two language pairs: German-English (De-En for in short) and Chinese-English (Zh-En for in short). In all experiments, BLEU (Papineni et al., 2002) is adopted as the automatic metric for translation quality evaluation and computed using Moses multi-bleu.perl script. 4.1.1 Dataset Model 21.81 25.84 26.36 27.94 28.53 28.77 28.03 29.17 Table 1: Comparison with previous work on IWSLT2014 German-English translation task. The “Baseline” means the performance of pretrained model used to warmly start training. For German-English translation task, following previous work (Ranzato et al., 2015; Bahdanau et al., 2016), we select data from German-English machine translation track of IWSLT2014 evaluation tasks, which consists of sentence-aligned subt"
K18-1019,P16-1009,0,0.10016,"Missing"
K18-1019,P16-1159,0,0.384953,"the conditional log-probability of the correct translation given the source sentence. However, as argued in Bengio et al. (2015), the Maximum Likelihood Estimation (MLE) principle suffers from so-called exposure bias in the inference stage: the model predicts next token conditional on its previously predicted ones that may be never observed in the training data. To address this problem, much recent work attempts to reduce the inconsistency between training and inference, such as adopting sequence-level objectives and directly maximizing BLEU scores (Bengio et al., 2015; Ranzato et al., 2015; Shen et al., 2016; Wiseman and Rush, 2016). Generative Adversarial Network (GAN) (Goodfellow et al., 2014) is another promising framework for alleviating exposure bias problem and recently shows remarkable promise in NMT (Yang et al., 2017; Wu et al., 2017). Formally, GAN consists of two ”adversarial” models: a generator and a discriminator. In machine translation, NMT model is used as the generator that produces translation candidates given a source sentence, and another neural network is introduced to serve as the discriminator, which takes sentence pairs as input and distinguishes whether a given sentence p"
K18-1019,D16-1137,0,0.33289,"g-probability of the correct translation given the source sentence. However, as argued in Bengio et al. (2015), the Maximum Likelihood Estimation (MLE) principle suffers from so-called exposure bias in the inference stage: the model predicts next token conditional on its previously predicted ones that may be never observed in the training data. To address this problem, much recent work attempts to reduce the inconsistency between training and inference, such as adopting sequence-level objectives and directly maximizing BLEU scores (Bengio et al., 2015; Ranzato et al., 2015; Shen et al., 2016; Wiseman and Rush, 2016). Generative Adversarial Network (GAN) (Goodfellow et al., 2014) is another promising framework for alleviating exposure bias problem and recently shows remarkable promise in NMT (Yang et al., 2017; Wu et al., 2017). Formally, GAN consists of two ”adversarial” models: a generator and a discriminator. In machine translation, NMT model is used as the generator that produces translation candidates given a source sentence, and another neural network is introduced to serve as the discriminator, which takes sentence pairs as input and distinguishes whether a given sentence pair is real or generated."
K18-1019,N18-1122,0,0.04082,"Missing"
N18-1154,W12-3018,0,0.100628,"Missing"
N18-1154,P16-1185,0,0.0153548,"tion models have achieved the state-of-the-art performance. The typical training algorithm for sequence prediction is Maximum Likelihood Estimation ✓⇤ = argmax ✓ E (X,Y ⇤ )⇠D log p✓ (Y ⇤ |X) (1) Despite the popularity of MLE or teacher forcing (Doya, 1992) in neural sequence prediction tasks, two general issues are always haunting: 1). data sparsity and 2). tendency for overfitting, with which can both harm model generalization. To combat data sparsity, different strategies have been proposed. Most of them try to take advantage of monolingual data (Sennrich et al., 2015; Zhang and Zong, 2016; Cheng et al., 2016). Others try to modify the ground truth target based on derived rules to get more similar examples for training (Norouzi et al., 2016; Ma et al., 2017). To alleviate overfitting, regularization techniques, 1706 Proceedings of NAACL-HLT 2018, pages 1706–1715 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics such as confidence penalization (Pereyra et al., 2017) and posterior regularization (Zhang et al., 2017), are proposed recently. As shown in Figure 1, we propose a novel learning architecture, titled Generative Bridging Network (GBN), to combine both"
N18-1154,P07-2045,0,0.00570763,"follows: 3.2 1 2 3 4 5 6 7 8 9 10 11 12 13 14 9 10 11 12 13 14 Epoch (Bridge) 32.2 32.1 32 BLEU S(Y, Y ⇤ ) r log p⌘ (Y |Y ⇤ ) ⌧ X s(yt |y1:t 1 , Y ⇤ ) = · r log p⌘ (yt |y1:t ⌧ t 0 31.9 31.8 31.7 31.6 31.5 0 1, Y ⇤ 2 3 4 5 6 7 8 Epoch (Generator) ) (17) 1 Figure 5: Coaching GBN’s learning curve on IWSLT German-English Dev set. Machine Translation Dataset We follow Ranzato et al. (2015); Bahdanau et al. (2016) and select German-English machine translation track of the IWSLT 2014 evaluation campaign. The corpus contains sentencewise aligned subtitles of TED and TEDx talks. We use Moses toolkit (Koehn et al., 2007) and remove sentences longer than 50 words as well as lowercasing. The evaluation metric is BLEU (Papineni et al., 2002) computed via the multi-bleu.perl. System Setting We use a unified GRU-based RNN (Chung et al., 2014) for both the generator and the coaching bridge. In order to compare with existing papers, we use a similar system setting with 512 RNN hidden units and 256 as embedding size. We use attentive encoder-decoder to build our system (Bahdanau et al., 2014). During training, we apply ADADELTA (Zeiler, 2012) with ✏ = 10 6 and ⇢ = 0.95 to optimize parameters of the generator and the"
N18-1154,W04-1013,0,0.0147779,"h (Bridge) 22.7 22.5 22.3 22.1 21.9 21.7 0 2 3 4 5 6 7 8 Figure 6: Coaching GBN’s learning curve on Abstractive Text Summarization Dev set. Abstractive Text Summarization Dataset We follow the previous works by Rush et al. (2015); Zhou et al. (2017) and use the same corpus from Annotated English Gigaword dataset (Napoles et al., 2012). In order to be comparable, we use the same script 4 released by Rush et al. (2015) to pre-process and extract the training and validation sets. For the test set, we use the English Gigaword, released by Rush et al. (2015), and evaluate our system through ROUGE (Lin, 2004). Following previous works, we employ ROUGE-1, ROUGE-2, and ROUGE-L as the evaluation metrics in the reported experimental results. https://github.com/facebookarchive/NAMAS 1 Epoch (Generator) competing other systems and our proposed GBN can yield a further improvement. We also observe that LM GBN and coaching GBN have both achieved better performance than Uniform GBN, which confirms that better regularization effects are achieved, and the generators become more robust and generalize better. We draw the learning curve of both the bridge and the generator in Figure 5 to demonstrate how they coo"
N18-1154,P02-1040,0,0.103934,"(Y |Y ⇤ ) ⌧ X s(yt |y1:t 1 , Y ⇤ ) = · r log p⌘ (yt |y1:t ⌧ t 0 31.9 31.8 31.7 31.6 31.5 0 1, Y ⇤ 2 3 4 5 6 7 8 Epoch (Generator) ) (17) 1 Figure 5: Coaching GBN’s learning curve on IWSLT German-English Dev set. Machine Translation Dataset We follow Ranzato et al. (2015); Bahdanau et al. (2016) and select German-English machine translation track of the IWSLT 2014 evaluation campaign. The corpus contains sentencewise aligned subtitles of TED and TEDx talks. We use Moses toolkit (Koehn et al., 2007) and remove sentences longer than 50 words as well as lowercasing. The evaluation metric is BLEU (Papineni et al., 2002) computed via the multi-bleu.perl. System Setting We use a unified GRU-based RNN (Chung et al., 2014) for both the generator and the coaching bridge. In order to compare with existing papers, we use a similar system setting with 512 RNN hidden units and 256 as embedding size. We use attentive encoder-decoder to build our system (Bahdanau et al., 2014). During training, we apply ADADELTA (Zeiler, 2012) with ✏ = 10 6 and ⇢ = 0.95 to optimize parameters of the generator and the coaching bridge. During decoding, a beam size of 8 is used to approximate the full search space. An important hyper-para"
N18-1154,D15-1044,0,0.198506,"rized in Table 1. We can observe that our fine-tuned MLE baseline (29.10) is already over1711 RG-2 11.32 11.88 14.45 17.54 15.95 RG-L 26.42 26.96 30.71 33.63 31.68 34.10 16.70 31.75 34.32 16.88 31.89 34.49 16.70 31.95 34.83 16.83 32.25 35.26 17.22 32.67 Coaching GBN Learning Curve 83.5 83 ROUGE-2 RG-1 29.55 29.76 33.10 36.15 34.04 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 9 10 11 12 13 14 Epoch (Bridge) 22.7 22.5 22.3 22.1 21.9 21.7 0 2 3 4 5 6 7 8 Figure 6: Coaching GBN’s learning curve on Abstractive Text Summarization Dev set. Abstractive Text Summarization Dataset We follow the previous works by Rush et al. (2015); Zhou et al. (2017) and use the same corpus from Annotated English Gigaword dataset (Napoles et al., 2012). In order to be comparable, we use the same script 4 released by Rush et al. (2015) to pre-process and extract the training and validation sets. For the test set, we use the English Gigaword, released by Rush et al. (2015), and evaluate our system through ROUGE (Lin, 2004). Following previous works, we employ ROUGE-1, ROUGE-2, and ROUGE-L as the evaluation metrics in the reported experimental results. https://github.com/facebookarchive/NAMAS 1 Epoch (Generator) competing other systems an"
N18-1154,P16-1009,0,0.0691058,"Missing"
N18-1154,D16-1137,0,0.0544207,"Missing"
N18-1154,P17-1139,0,0.0205442,"ity, different strategies have been proposed. Most of them try to take advantage of monolingual data (Sennrich et al., 2015; Zhang and Zong, 2016; Cheng et al., 2016). Others try to modify the ground truth target based on derived rules to get more similar examples for training (Norouzi et al., 2016; Ma et al., 2017). To alleviate overfitting, regularization techniques, 1706 Proceedings of NAACL-HLT 2018, pages 1706–1715 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics such as confidence penalization (Pereyra et al., 2017) and posterior regularization (Zhang et al., 2017), are proposed recently. As shown in Figure 1, we propose a novel learning architecture, titled Generative Bridging Network (GBN), to combine both of the benefits from synthetic data and regularization. Within the architecture, the bridge module (bridge) first transforms the point-wise ground truth into a bridge distribution, which can be viewed as a target proposer from whom more target examples are drawn to train the generator. By introducing different constraints, the bridge can be set or trained to possess specific property, with which the drawn samples can augment target-side data (allevi"
N18-1154,D16-1160,0,0.0716227,"neural sequence prediction models have achieved the state-of-the-art performance. The typical training algorithm for sequence prediction is Maximum Likelihood Estimation ✓⇤ = argmax ✓ E (X,Y ⇤ )⇠D log p✓ (Y ⇤ |X) (1) Despite the popularity of MLE or teacher forcing (Doya, 1992) in neural sequence prediction tasks, two general issues are always haunting: 1). data sparsity and 2). tendency for overfitting, with which can both harm model generalization. To combat data sparsity, different strategies have been proposed. Most of them try to take advantage of monolingual data (Sennrich et al., 2015; Zhang and Zong, 2016; Cheng et al., 2016). Others try to modify the ground truth target based on derived rules to get more similar examples for training (Norouzi et al., 2016; Ma et al., 2017). To alleviate overfitting, regularization techniques, 1706 Proceedings of NAACL-HLT 2018, pages 1706–1715 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics such as confidence penalization (Pereyra et al., 2017) and posterior regularization (Zhang et al., 2017), are proposed recently. As shown in Figure 1, we propose a novel learning architecture, titled Generative Bridging Network (G"
N18-1154,P17-1101,1,0.942479,"can observe that our fine-tuned MLE baseline (29.10) is already over1711 RG-2 11.32 11.88 14.45 17.54 15.95 RG-L 26.42 26.96 30.71 33.63 31.68 34.10 16.70 31.75 34.32 16.88 31.89 34.49 16.70 31.95 34.83 16.83 32.25 35.26 17.22 32.67 Coaching GBN Learning Curve 83.5 83 ROUGE-2 RG-1 29.55 29.76 33.10 36.15 34.04 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 9 10 11 12 13 14 Epoch (Bridge) 22.7 22.5 22.3 22.1 21.9 21.7 0 2 3 4 5 6 7 8 Figure 6: Coaching GBN’s learning curve on Abstractive Text Summarization Dev set. Abstractive Text Summarization Dataset We follow the previous works by Rush et al. (2015); Zhou et al. (2017) and use the same corpus from Annotated English Gigaword dataset (Napoles et al., 2012). In order to be comparable, we use the same script 4 released by Rush et al. (2015) to pre-process and extract the training and validation sets. For the test set, we use the English Gigaword, released by Rush et al. (2015), and evaluate our system through ROUGE (Lin, 2004). Following previous works, we employ ROUGE-1, ROUGE-2, and ROUGE-L as the evaluation metrics in the reported experimental results. https://github.com/facebookarchive/NAMAS 1 Epoch (Generator) competing other systems and our proposed GBN c"
P03-1035,O93-1004,0,0.434107,"ianfeng Li, Wenfeng Yang and Xiaodan Zhu for their help with evaluating our system. a large degree upon the coverage of the dictionary, which unfortunately may never be complete because new words appear constantly. Therefore, in addition to the dictionary, many systems also contain special components for unknown word identification. In particular, statistical methods have been widely applied because they utilize a probabilistic or cost-based scoring mechanism, instead of the dictionary, to segment the text. These methods however, suffer from three drawbacks. First, some of these methods (e.g. Lin et al., 1993) identify unknown words without identifying their types. For instance, one would identify a string as a unit, but not identify whether it is a person name. This is not always sufficient. Second, the probabilistic models used in these methods (e.g. Teahan et al., 2000) are trained on a segmented corpus which is not always available. Third, the identified unknown words are likely to be linguistically implausible (e.g. Dai et al., 1999), and additional manual checking is needed for some subsequent tasks such as parsing. We believe that the identification of unknown words should not be defined as"
P03-1035,J96-3004,0,0.676541,"me. This is not always sufficient. Second, the probabilistic models used in these methods (e.g. Teahan et al., 2000) are trained on a segmented corpus which is not always available. Third, the identified unknown words are likely to be linguistically implausible (e.g. Dai et al., 1999), and additional manual checking is needed for some subsequent tasks such as parsing. We believe that the identification of unknown words should not be defined as a separate problem from word segmentation. These two problems are better solved simultaneously in a unified approach. One example of such approaches is Sproat et al. (1996), which is based on weighted finite-state transducers (FSTs). Our approach is motivated by the same inspiration, but is based on a different mechanism: the improved source-channel models. As we shall see, these models provide a more flexible framework to incorporate various kinds of lexical and statistical information. Some types of unknown words that are not discussed in Sproat’s system are dealt with in our system. 3 processed in different ways in our system. For example, the plausible word segmentation for the sentence in Figure 1(a) is as shown. Figure 1(b) is the output of our system, whe"
P03-1035,J00-3004,0,0.0796475,"tems also contain special components for unknown word identification. In particular, statistical methods have been widely applied because they utilize a probabilistic or cost-based scoring mechanism, instead of the dictionary, to segment the text. These methods however, suffer from three drawbacks. First, some of these methods (e.g. Lin et al., 1993) identify unknown words without identifying their types. For instance, one would identify a string as a unit, but not identify whether it is a person name. This is not always sufficient. Second, the probabilistic models used in these methods (e.g. Teahan et al., 2000) are trained on a segmented corpus which is not always available. Third, the identified unknown words are likely to be linguistically implausible (e.g. Dai et al., 1999), and additional manual checking is needed for some subsequent tasks such as parsing. We believe that the identification of unknown words should not be defined as a separate problem from word segmentation. These two problems are better solved simultaneously in a unified approach. One example of such approaches is Sproat et al. (1996), which is based on weighted finite-state transducers (FSTs). Our approach is motivated by the s"
P03-1035,C02-1012,1,\N,Missing
P04-1059,J95-4004,0,0.0498692,"ment in this field shows that, problem is handled in this framework. We explore in addition to ambiguity resolution and unknown several features and describe how to create training word detection, the usefulness of a Chinese word data by sampling. We evaluate the performance of segmenter also depends crucially on its ability to our segmentation system using an annotated test set, adapt to different domains of texts and different where new words are simulated by sampling. We segmentation standards. then describe a transformation-based learning (TBL, The need of adaptation involves two research Brill, 1995) method that is used to adapt our system issues that we will address in this paper. The first is to different segmentation standards. We compare new word detection. Different domains/applications the adaptive system to other state-of-the-art systems may have different vocabularies which contain new using four test sets in the SIGHAN’s First Internawords/terms that are not available in a general tional Chinese Word Segmentation Bakeoff, each of dictionary. In this paper, new words refer to OOV which is constructed according to a different segwords other than named entities, factoids and mor- me"
P04-1059,P00-1073,1,0.894198,"ose words whose probability to appear in a new document is lower than general lexical words. Let Pi(k) be the probability of word wi that occurs k times in a document. In our experiments, we assume that P(NW|wi) can be approximated by the probability of wi occurring less than K times in a new document: K −1 P ( NW |wi ) ≈ ∑ Pi (k ) , (5) k =0 where the constant K is dependent on the size of the document: The larger the document, the larger the value. Pi(k) can be estimated using several term distribution models (see Chapter 15.3 in Manning and Schütze, 1999). Following the empirical study in (Gao and Lee, 2000), we use K-Mixture (Katz, 1996) which estimate Pi(k) as α β k ( ) , (6) β +1 β +1 where δk,0=1 if k=0, 0 otherwise. α and β are parameters that can be fit using the observed mean λ Pi (k ) = (1 − α )δ k , 0 + and the observed inverse document frequency IDF as follow: cf N λ = , IDF = log , N df λ cf − df , and α = , β = λ × 2 IDF − 1 = df β where cf is the total number of occurrence of word wi in training data, df is the number of documents in training data that wi occurs in, and N is the total number of documents. In our implementation, the training data contain approximately 40 thousand docu"
P04-1059,P03-1035,1,0.63764,"subsets of the AS training set of different sizes, and observed the same trend. However, even with a much smaller adaptation data set (e.g. 250K), we still outperform the best bakeoff results. 6 Related Work Many methods of Chinese word segmentation have been proposed (See Wu and Tseng, 1993; Sproat and Shih, 2001 for reviews). However, it is difficult to compare systems due to the fact that there is no widely accepted standard. There has been less work on dealing with NWI and standard adaptation. All feature functions in Figure 1, except the NW function, are derived from models presented in (Gao et al., 2003). The linear models are similar to what was presented in Collins and Duffy (2001). An alternative to linear models is the log-linear models suggested by Och (2003). See Collins (2002) for a comparison of these approaches. The features for NWI were studied in Wu & Jiang (2000) and Li et al. (2004). The use of sampling was proposed in Della Pietra et al. (1997) and Rosenfeld et al. (2001). There is also a related work on this line in Japanese (Uchimoto et al., 2001). A detailed discussion on differences among the four Bakeoff standards is presented in Wu (2003), which also proposes an adaptive s"
P04-1059,P03-1021,0,0.0866914,"fferent ways (e.g. name entity models are n-gram models trained on 2 corpora whereas factoid models use derivation rules and have binary values). The dynamic value ranges of different class models can be so different that it is improper to combine all models through simple multiplication as Equation (1). In this study we use linear models. The method is derived from linear discriminant functions widely used for pattern classification (Duda et al., 2001), and has been recently introduced into NLP tasks by Collins and Duffy (2001). It is also related to loglinear models for machine translation (Och, 2003). In this framework, we have a set of M+1 feature functions fi(S,W), i = 0,…,M. They are derived from the context model (i.e. f0(W)) and M class models, each for one word class, as shown in Figure 1: For probabilistic models such as the context model or person name model, the feature functions are defined as the negative logarithm of the corresponding probabilistic models. For each feature function, there is a model parameter λi. The best word segmentation W* is determined by the decision rule as M W * = arg max Score(λ0M , S ,W ) = arg max ∑ λi f i ( S ,W ) (2) W W i =0 Below we describe how"
P04-1059,P97-1041,0,0.593397,"Missing"
P04-1059,W03-1719,0,0.346254,"Missing"
P04-1059,W01-0512,0,0.170452,"(e.g. 蜂窝式 ‘cellular’) four test sets. It demonstrates the possibility of and time-sensitive political, social or cultural terms having a single adaptive Chinese word segmenter (e.g. 三通‘Three Links’, 非典 ‘SARS’). that is capable of supporting multiple user applicaThe second issue concerns the customizable tions. display of word segmentation. Different Chinese Abstract 1 This work was done while Hongqiao Li, Xinsong Xia and Haowei Qin were visiting Microsoft Research (MSR) Asia. We thank Xiaodan Zhu for his early contribution, and the three reviewers, one of whom alerted us the related work of (Uchimoto et al., 2001). Word Class2 Model Feature Functions, f(S,W) Context Model Word class based trigram, P(W). -log(P(W)) Lexical Word (LW) Morphological Word (MW) Named Entity (NE) ----Character/word bigram, P(S|NE). 1 if S forms a word lexicon entry, 0 otherwise. 1 if S forms a morph lexicon entry, 0 otherwise. -log(P(S|NE)) Factoid (FT) New Word (NW) ----- 1 if S can be parsed using a factoid grammar, 0 otherwise Score of SVM classifier Figure 1: Context model, word classes, and class models, and feature functions. 2 Chinese Word Segmentation with Linear Models Let S be a Chinese sentence which is a character"
P04-1059,W00-1207,1,0.900394,"res are chosen due to their effectiveness and availability for on-line detection. They are Independent Word Probability (IWP), Anti-Word Pair (AWP), and Word Formation Analogy (WFA). Below we describe each feature in turn. In Section 3.2, we shall describe the way the training data (new word list) for the classifier is created by sampling. IWP is a real valued feature. Most Chinese characters can be used either as independent words or component parts of multi-character words, or both. The IWP of a single character is the likelihood for this character to appear as an independent word in texts (Wu and Jiang, 2000): IWP ( x ) = C ( x, W ) . C( x) (4) where C(x, W) is the number of occurrences of the character x as an independent word in training data, and C(x) is the total number of x in training data. We assume that the IWP of a character string is the product of the IWPs of the component characters. Intuitively, the lower the IWP value, the more likely the character string forms a new word. In our implementation, the training data is word-segmented. AWP is a binary feature derived from IWP. For example, the value of AWP of an NW_11 candidate ab is defined as: AWP(ab)=1 if IWP(a)>θ or IWP(b) >θ, 0 othe"
P04-1059,O03-4001,1,0.882954,"ong@founder.com & Shanghai Jiaotong university, Shanghai. haoweiqin@sjtu.edu.cn NLP-enabled applications may have different requirements that call for different granularities of word segmentation. For example, speech recogniThis paper presents a Chinese word segmentation system which can adapt to different tion systems prefer “longer words” to achieve domains and standards. We first present a stahigher accuracy whereas information retrieval tistical framework where domain-specific systems prefer “shorter words” to obtain higher words are identified in a unified approach to recall rates, etc. (Wu, 2003). Given a word segword segmentation based on linear models. mentation specification (or standard) and/or some We explore several features and describe how application data used as training data, a segmenter to create training data by sampling. We then with customizable display should be able to provide describe a transformation-based learning alternative segmentation units according to the method used to adapt our system to different specification which is either pre-defined or implied word segmentation standards. Evaluation of in the data. the proposed system on five test sets with difIn this"
P04-1059,W03-1726,0,\N,Missing
P04-1059,W01-1802,0,\N,Missing
P06-1129,P98-2127,0,0.270785,"Missing"
P06-1129,P02-1038,0,0.0135806,"raining algorithm. We use the Generalized Iterative Scaling (GIS) algorithm (Darroch and Ratcliff, 1972) to learn the model parameter λs of the maximum entropy model. GIS training requires normalization over all possible prediction classes as shown in the denominator in equation (6). Since the potential number of correction candidates may be huge for multi-term queries, it would not be practical to perform the normalization over the entire search space. Instead, we use a method to approximate the sum over the n-best list (a list of most probable correction candidates). This is similar to what Och and Ney (2002) used for their maximum entropy-based statistical machine translation training. 3.4.1 Features Features used in our maximum entropy model are classified into two categories I) baseline features and II) features supported by distributional similarity evidence. Below we list the feature templates. Category I: 1. Language model probability feature. This is the only real-valued feature with feature value set to the logarithm of source model probability: query term and its correction candidate are above certain thresholds; 4. Lexicon-based features, which are generated by checking whether a query t"
P06-1129,C90-2036,0,\N,Missing
P06-1129,P00-1037,0,\N,Missing
P06-1129,J96-1002,0,\N,Missing
P06-1129,P97-1008,0,\N,Missing
P06-1129,P02-1019,0,\N,Missing
P06-1129,H05-1120,0,\N,Missing
P06-1129,C98-2122,0,\N,Missing
P06-1129,W04-3238,0,\N,Missing
P06-1129,P99-1004,0,\N,Missing
P07-1091,koen-2004-pharaoh,0,0.0571564,"ssible reorderings of its children. The maximum entropy model has the same form as in the binary case, except that there are more classes of reordering patterns as n increases. The form of reordering rules, and the calculation of reordering probability for a particular node, can also be generalized easily.6 The only problem for the generalized reordering knowledge is that, as there are more classes, data sparseness becomes more severe. 6 The Decoder The last three sections explain how the S → n×S 0 part of formula 2 is done. The S 0 → T part is simply done by our re-implementation of PHARAOH (Koehn, 2004). Note that nonmonotonous translation is used here since the distance-based model is needed for local reordering. For the n×T → Tˆ part, the factors in consideration include the score of T returned by the decoder, and the reordering probability Pr (S → S 0 ). In order to conform to the log-linear model used in the decoder, we integrate the two factors by defining the total score of T as formula 3: exp(λr logPr (S → S 0 ) + X λi Fi (S 0 → T )) (3) i The first term corresponds to the contribution of syntax-based reordering, while the second term that of the features Fi used in the decoder. All t"
P07-1091,P03-1021,0,0.0573639,"r local reordering. For the n×T → Tˆ part, the factors in consideration include the score of T returned by the decoder, and the reordering probability Pr (S → S 0 ). In order to conform to the log-linear model used in the decoder, we integrate the two factors by defining the total score of T as formula 3: exp(λr logPr (S → S 0 ) + X λi Fi (S 0 → T )) (3) i The first term corresponds to the contribution of syntax-based reordering, while the second term that of the features Fi used in the decoder. All the feature weights (λs) were trained using our implementation of Minimum Error Rate Training (Och, 2003). The final translation Tˆ is the T with the highest total score. 5 Namely, N1 N2 N3 , N1 N3 N2 , N2 N1 N3 , N2 N3 N1 , N3 N1 N2 , and N3 N2 N1 , if the child nodes in the original order are N1 , N2 , and N3 . 6 For example, the reordering probability of a phrase p = p1 p2 p3 generated by a 3-ary node N is Pr (r)×Pr (pi1 )×Pr (pj2 )×Pr (pk3 ) where r is one of the six reordering patterns for 3-ary nodes. 724 It is observed in pilot experiments that, for a lot of long sentences containing several clauses, only one of the clauses is reordered. That is, our greedy reordering algorithm (c.f. secti"
P07-1091,P00-1056,0,0.055284,"set of NIST MT-2002 are merged to form our development set. The training data for both reordering knowledge and translation table is the one for NIST MT-2005. The GIGAWORD corpus is used for training language model. The Chinese side of all corpora are segmented into words by our implementation of (Gao et al., 2003). 7.2 The Preprocessing Module As mentioned in section 3, the preprocessing module for reordering needs a parser of the SL, a word alignment tool, and a Maximum Entropy training tool. We use the Stanford parser (Klein and Manning, 2003) with its default Chinese grammar, the GIZA++ (Och and Ney, 2000) alignment package with its default settings, and the ME tool developed by (Zhang, 2004). Section 5 mentions that our reordering model can apply to nodes of any branching factor. It is interesting to know how many branching factors should be included. The distribution of parse tree nodes as shown in table 1 is based on the result of parsing the Chinese side of NIST MT-2002 test set by the Stanford parser. It is easily seen that the majority of parse tree nodes are binary ones. Nodes with more than 3 children seem to be negligible. The 3ary nodes occupy a certain proportion of the distribution,"
P07-1091,P02-1040,0,0.107922,") is trivial, but there is a subtle point about the calculation of language model score: the language model score of a translated clause is not independent from other clauses; it should take into account the last few words of the previous translated 0 clause. The best translated clause Tˆ(Ci ) is selected in step 3(a)(iii) by equation 3. In step 4 the best translation Tˆj is X arg max exp(λr logPr (S → Sj )+ Tj 0 score(T (Ci ))). i 7 Experiments 7.1 Corpora Our experiments are about Chinese-to-English translation. The NIST MT-2005 test data set is used for evaluation. (Case-sensitive) BLEU-4 (Papineni et al., 2002) is used as the evaluation metric. The 7 IP stands for inflectional phrase and CP for complementizer phrase. These two types of phrases are clauses in terms of the Government and Binding Theory. Branching Factor Count Percentage 2 12294 73.41 3 3173 18.95 &gt;3 1280 7.64 Table 1: Distribution of Parse Tree Nodes with Different Branching Factors Note that nodes with only one child are excluded from the survey as reordering does not apply to such nodes. test set and development set of NIST MT-2002 are merged to form our development set. The training data for both reordering knowledge and translatio"
P07-1091,P05-1034,0,0.0665473,"ot monotonous, since the distance-based model is needed for local reordering. Our second contribution is our definition of the best translation: arg max exp(λr logPr (S → S 0 )+ T X λi Fi (S 0 → T )) i 721 where Fi are the features in the standard phrasebased model and Pr (S → S 0 ) is our new feature, viz. the probability of reordering S as S 0 . The details of this model are elaborated in sections 3 to 6. The settings and results of experiments on this new model are given in section 7. 2 Related Work There have been various attempts to syntaxbased SMT, such as (Yamada and Knight, 2001) and (Quirk et al., 2005). We do not adopt these models since a lot of subtle issues would then be introduced due to the complexity of syntax-based decoder, and the impact of syntax on reordering will be difficult to single out. There have been many reordering strategies under the phrase-based camp. A notable approach is lexicalized reordering (Koehn et al., 2005) and (Tillmann, 2004). It should be noted that this approach achieves the best result within certain distortion limit and is therefore not a good model for global reordering. There are a few attempts to the preprocessing approach to reordering. The most notab"
P07-1091,P06-1067,0,0.209315,"spect to English word order. Instead of relying on manual rules, (Xia and McCord, 2004) propose a method in learning patterns of rewriting SL sentences. This method parses training data and uses some heuristics to align SL phrases with TL ones. From such alignment it can extract rewriting patterns, of which the units are words and POSs. The learned rewriting rules are then applied to rewrite SL sentences before monotonous translation. Despite the encouraging results reported in these papers, the two attempts share the same shortcoming that their reordering is deterministic. As pointed out in (Al-Onaizan and Papineni, 2006), these strategies make hard decisions in reordering which cannot be undone during decoding. That is, the choice of reordering is independent from other translation factors, and once a reordering mistake is made, it cannot be corrected by the subsequent decoding. To overcome this weakness, we suggest a method to ‘soften’ the hard decisions in preprocessing. The essence is that our preprocessing module generates n-best S 0 s rather than merely one S 0 . A variety of reordered SL sentences are fed to the decoder so that the decoder can consider, to certain extent, the interaction between reorder"
P07-1091,P05-1066,0,0.78591,"op half of the figures show the phrase structure of the Chinese sentences, while the links on the bottom half of the figures show the alignments between Chinese and English phrases. Square brackets indicate the boundaries of blocks found by our decoder. and it achieves better MT performance on the basis of the standard phrase-based model. To prove it, we developed a hybrid approach which preserves the strength of phrase-based SMT in local reordering as well as the strength of syntax in global reordering. Our method is inspired by previous preprocessing approaches like (Xia and McCord, 2004), (Collins et al., 2005), and (Costa-juss`a and Fonollosa, 2006), which split translation into two stages: 0 S→S →T (1) where a sentence of the source language (SL), S, is first reordered with respect to the word order of the target language (TL), and then the reordered SL sentence S 0 is translated as a TL sentence T by monotonous translation. Our first contribution is a new translation model as represented by formula 2: S → n × S 0 → n × T → Tˆ (2) where an n-best list of S 0 , instead of only one S 0 , is generated. The reason of such change will be given in section 2. Note also that the translation process S 0 →"
P07-1091,P03-1035,1,0.537454,"Missing"
P07-1091,P03-1054,0,0.0115851,"ey as reordering does not apply to such nodes. test set and development set of NIST MT-2002 are merged to form our development set. The training data for both reordering knowledge and translation table is the one for NIST MT-2005. The GIGAWORD corpus is used for training language model. The Chinese side of all corpora are segmented into words by our implementation of (Gao et al., 2003). 7.2 The Preprocessing Module As mentioned in section 3, the preprocessing module for reordering needs a parser of the SL, a word alignment tool, and a Maximum Entropy training tool. We use the Stanford parser (Klein and Manning, 2003) with its default Chinese grammar, the GIZA++ (Och and Ney, 2000) alignment package with its default settings, and the ME tool developed by (Zhang, 2004). Section 5 mentions that our reordering model can apply to nodes of any branching factor. It is interesting to know how many branching factors should be included. The distribution of parse tree nodes as shown in table 1 is based on the result of parsing the Chinese side of NIST MT-2002 test set by the Stanford parser. It is easily seen that the majority of parse tree nodes are binary ones. Nodes with more than 3 children seem to be negligible"
P07-1091,N03-1017,0,0.0335242,"Missing"
P07-1091,N04-4026,0,0.135932,"his model are elaborated in sections 3 to 6. The settings and results of experiments on this new model are given in section 7. 2 Related Work There have been various attempts to syntaxbased SMT, such as (Yamada and Knight, 2001) and (Quirk et al., 2005). We do not adopt these models since a lot of subtle issues would then be introduced due to the complexity of syntax-based decoder, and the impact of syntax on reordering will be difficult to single out. There have been many reordering strategies under the phrase-based camp. A notable approach is lexicalized reordering (Koehn et al., 2005) and (Tillmann, 2004). It should be noted that this approach achieves the best result within certain distortion limit and is therefore not a good model for global reordering. There are a few attempts to the preprocessing approach to reordering. The most notable ones are (Xia and McCord, 2004) and (Collins et al., 2005), both of which make use of linguistic syntax in the preprocessing stage. (Collins et al., 2005) analyze German clause structure and propose six types of rules for transforming German parse trees with respect to English word order. Instead of relying on manual rules, (Xia and McCord, 2004) propose a"
P07-1091,C04-1073,0,0.804395,"lines and nodes on the top half of the figures show the phrase structure of the Chinese sentences, while the links on the bottom half of the figures show the alignments between Chinese and English phrases. Square brackets indicate the boundaries of blocks found by our decoder. and it achieves better MT performance on the basis of the standard phrase-based model. To prove it, we developed a hybrid approach which preserves the strength of phrase-based SMT in local reordering as well as the strength of syntax in global reordering. Our method is inspired by previous preprocessing approaches like (Xia and McCord, 2004), (Collins et al., 2005), and (Costa-juss`a and Fonollosa, 2006), which split translation into two stages: 0 S→S →T (1) where a sentence of the source language (SL), S, is first reordered with respect to the word order of the target language (TL), and then the reordered SL sentence S 0 is translated as a TL sentence T by monotonous translation. Our first contribution is a new translation model as represented by formula 2: S → n × S 0 → n × T → Tˆ (2) where an n-best list of S 0 , instead of only one S 0 , is generated. The reason of such change will be given in section 2. Note also that the tr"
P07-1091,P01-1067,0,0.351717,"anslation process S 0 → T is not monotonous, since the distance-based model is needed for local reordering. Our second contribution is our definition of the best translation: arg max exp(λr logPr (S → S 0 )+ T X λi Fi (S 0 → T )) i 721 where Fi are the features in the standard phrasebased model and Pr (S → S 0 ) is our new feature, viz. the probability of reordering S as S 0 . The details of this model are elaborated in sections 3 to 6. The settings and results of experiments on this new model are given in section 7. 2 Related Work There have been various attempts to syntaxbased SMT, such as (Yamada and Knight, 2001) and (Quirk et al., 2005). We do not adopt these models since a lot of subtle issues would then be introduced due to the complexity of syntax-based decoder, and the impact of syntax on reordering will be difficult to single out. There have been many reordering strategies under the phrase-based camp. A notable approach is lexicalized reordering (Koehn et al., 2005) and (Tillmann, 2004). It should be noted that this approach achieves the best result within certain distortion limit and is therefore not a good model for global reordering. There are a few attempts to the preprocessing approach to r"
P07-1091,W06-1609,0,\N,Missing
P07-1091,W02-1039,0,\N,Missing
P07-1091,2005.iwslt-1.8,0,\N,Missing
P08-1011,C02-1126,0,0.0131409,"ed in a similar way as does with target features. The source head word feature is defined to be a function f4 to indicate whether a word ei is the source head word in English according to a parse tree of the source sentence. Similar to the definition of lexical features, we also use a set of features based on POS tags of source language. 3 3.1 Model Training and Application Training We parsed English and Chinese sentences to get training samples for measure word generation model. Based on the source syntax parse tree, for each measure word, we identified its head word by using a toolkit from (Chiang and Bikel, 2002) which can heuristically identify head words for sub-trees. For the bilingual corpus, we also perform word alignment to get correspondences between source and target words. Then, the collocation between measure words and head words and their surrounding contextual information are extracted to train the measure word selection models. According to word alignment results, we classify measure words into two classes based on whether they have non-null translations. We map Chinese measure words having non-null translations to a unified symbol {NULL} as mentioned in Section 2.4, indicating that we ne"
P08-1011,P05-1033,0,0.134325,"ls and indefinite articles are directly followed by countable nouns to denote the quantity of objects. Therefore, in the English-to-Chinese machine translation task we need to take additional efforts to generate the missing measure words in Chinese. For example, when translating the English phrase three books into the Chinese phrases “三本书”, where three corresponds to the numeral “三” and books corresponds to the noun “书”, the Chinese measure word “本” should be generated between the numeral and the noun. In most statistical machine translation (SMT) models (Och et al., 2004; Koehn et al., 2003; Chiang, 2005), some of measure words can be generated without modification or additional processing. For example, in above translation, the phrase translation table may suggest the word three be translated into “三”, “三本”, “三只”, etc, and the word books into “书”, “书本”, “名册” (scroll), etc. Then the SMT model selects the most likely combination “三本书” as the final translation result. In this example, a measure word candidate set consisting of “本” and “只” can be generated by bilingual phrases (or synchronous translation rules), and the best measure word “本” from the measure 2 1 There are some exceptional cases,"
P08-1011,N03-1017,0,0.0412262,"Missing"
P08-1011,P07-1017,0,0.0485184,"Missing"
P08-1011,P00-1056,0,0.0252377,"s and their surrounding contextual information are extracted to train the measure word selection models. According to word alignment results, we classify measure words into two classes based on whether they have non-null translations. We map Chinese measure words having non-null translations to a unified symbol {NULL} as mentioned in Section 2.4, indicating that we need not generate these kind of measure words since they can be translated from English. In our work, the Berkeley parser (Petrov and Klein, 2007) was employed to extract syntactic knowledge from the training corpus. We ran GIZA++ (Och and Ney, 2000) on the training corpus in both directions with IBM model 4, and then applied the refinement rule described in (Koehn et al., 2003) to obtain a many-to-many word alignment for each sentence pair. We used the SRI Language Modeling Toolkit (Stolcke, 2002) to train a fivegram model with modified Kneser-Ney smoothing (Chen and Goodman, 1998). The Maximum Entropy training toolkit from (Zhang, 2006) was employed to train the measure word selection model. 3.2 Measure word generation As mentioned in previous sections, we apply our measure word generation module into SMT output as a post-processing ste"
P08-1011,J04-4002,0,0.307392,"Missing"
P08-1011,P02-1040,0,0.0737285,"Missing"
P08-1011,N07-1051,0,0.0125043,"et correspondences between source and target words. Then, the collocation between measure words and head words and their surrounding contextual information are extracted to train the measure word selection models. According to word alignment results, we classify measure words into two classes based on whether they have non-null translations. We map Chinese measure words having non-null translations to a unified symbol {NULL} as mentioned in Section 2.4, indicating that we need not generate these kind of measure words since they can be translated from English. In our work, the Berkeley parser (Petrov and Klein, 2007) was employed to extract syntactic knowledge from the training corpus. We ran GIZA++ (Och and Ney, 2000) on the training corpus in both directions with IBM model 4, and then applied the refinement rule described in (Koehn et al., 2003) to obtain a many-to-many word alignment for each sentence pair. We used the SRI Language Modeling Toolkit (Stolcke, 2002) to train a fivegram model with modified Kneser-Ney smoothing (Chen and Goodman, 1998). The Maximum Entropy training toolkit from (Zhang, 2006) was employed to train the measure word selection model. 3.2 Measure word generation As mentioned in"
P09-1066,C08-1005,0,0.0599862,"ngalore et al., 2001; Matusov et al., 2006) to more complicated word-based system combination model (Rosti et al., 2007; Sim et al., 2007). Typically, the resulting systems take outputs of individual machine translation systems as input, and build a new confusion network for second-pass decoding. There have been many efforts dedicated to advance the state-of-the-art performance by combining multiple systems’ outputs. Most of the work focused on seeking better word alignment for consensus-based confusion network decoding (Matusov et al., 2006) or word-level system combination (He et al., 2008; Ayan et al., 2008). In addition to better alignment, Rosti et al. (2008) introduced an incremental strategy for confusion network construction; and Hildebrand and Vogel (2008) proposed a hypotheses reranking model for multiple systems’ outputs with more features including word translation probability and n-gram agreement statistics. A common property of all the work mentioned above is that the combination models work on the basis of n-best translation lists (full hypotheses) of existing machine translation systems. However, the n-best list only presents a very small portion of the entire search space of a Stati"
P09-1066,P05-1033,0,0.476399,"e rest of the paper is structured as follows. Section 2 gives a formal description of the codecoding model, the strategy to apply consensus information and hypotheses ranking in decoding. In Section 3, we make detailed comparison between co-decoding and related work such as system combination and hypotheses selection out of multiple systems. Experimental results and discussions are presented in Section 4. Section 5 concludes the paper. 2 2.1 Collaborative Decoding Overview Collaborative decoding does not present a full SMT model as other SMT decoders do such as Pharaoh (Koehn, 2004) or Hiero (Chiang, 2005). Instead, it provides a framework that accommodates and coordinates multiple MT decoders. Conceptually, collaborative decoding incorporates the following four constituents: 1. Co-decoding model. A co-decoding model consists of a set of member models, which are a set of augmented baseline models. We call decoders based on member models member decoders, and those based on baseline models baseline decoders. In our work, any Maximum A Posteriori (MAP) SMT model with log-linear formulation (Och, 2002) can be a qualified candidate for a baseline model. The requirement for a loglinear model aims to"
P09-1066,D08-1011,0,0.382928,"sensus voting (Bangalore et al., 2001; Matusov et al., 2006) to more complicated word-based system combination model (Rosti et al., 2007; Sim et al., 2007). Typically, the resulting systems take outputs of individual machine translation systems as input, and build a new confusion network for second-pass decoding. There have been many efforts dedicated to advance the state-of-the-art performance by combining multiple systems’ outputs. Most of the work focused on seeking better word alignment for consensus-based confusion network decoding (Matusov et al., 2006) or word-level system combination (He et al., 2008; Ayan et al., 2008). In addition to better alignment, Rosti et al. (2008) introduced an incremental strategy for confusion network construction; and Hildebrand and Vogel (2008) proposed a hypotheses reranking model for multiple systems’ outputs with more features including word translation probability and n-gram agreement statistics. A common property of all the work mentioned above is that the combination models work on the basis of n-best translation lists (full hypotheses) of existing machine translation systems. However, the n-best list only presents a very small portion of the entire sea"
P09-1066,2008.amta-srw.3,0,0.391125,"y, the resulting systems take outputs of individual machine translation systems as input, and build a new confusion network for second-pass decoding. There have been many efforts dedicated to advance the state-of-the-art performance by combining multiple systems’ outputs. Most of the work focused on seeking better word alignment for consensus-based confusion network decoding (Matusov et al., 2006) or word-level system combination (He et al., 2008; Ayan et al., 2008). In addition to better alignment, Rosti et al. (2008) introduced an incremental strategy for confusion network construction; and Hildebrand and Vogel (2008) proposed a hypotheses reranking model for multiple systems’ outputs with more features including word translation probability and n-gram agreement statistics. A common property of all the work mentioned above is that the combination models work on the basis of n-best translation lists (full hypotheses) of existing machine translation systems. However, the n-best list only presents a very small portion of the entire search space of a Statistical Machine Translation (SMT) model while a majority of the space, within which there are many potentially good translations, is pruned away in decoding."
P09-1066,N04-1022,0,0.0804613,"explored in decoding. Experimental results on data sets for NIST Chinese-to-English machine translation task show that the co-decoding method can bring significant improvements to all baseline decoders, and the outputs from co-decoding can be used to further improve the result of system combination. 1 Introduction Recent research has shown substantial improvements can be achieved by utilizing consensus statistics obtained from outputs of multiple machine translation systems. Translation consensus can be measured either at sentence level or at word level. For example, Minimum Bayes Risk (MBR) (Kumar and Byrne, 2004) decoding over n-best list tries to find a hypothesis with lowest expected loss with respect to all the other translations, which can be viewed as sentence-level consensus-based decoding. Word based methods proposed range from straightforward consensus voting (Bangalore et al., 2001; Matusov et al., 2006) to more complicated word-based system combination model (Rosti et al., 2007; Sim et al., 2007). Typically, the resulting systems take outputs of individual machine translation systems as input, and build a new confusion network for second-pass decoding. There have been many efforts dedicated"
P09-1066,P06-1077,0,0.0243484,"m step 2 to step 4 until a preset iteration limit is reached. In the iterative decoding procedure described above, hypotheses of different decoders can be mutually improved. For example, given two decoders ?1 and ?2 with hypotheses sets ℋ1 and ℋ2 , improvements on ℋ1 enable ?2 to improve ℋ2 , and in turn ℋ1 benefits from improved ℋ2 , and so forth. Step 2 is used to facilitate the computation of feature functions ℎ?,? (?, ℋ? ∙ ) , which require both e and every hypothesis in ℋ? ∙ should be translations of the same set of source words. This step seems to be redundant for CKY-style MT decoders (Liu et al., 2006; Xiong et al., 2006; Chiang, 2005) since the grouping is immediately available from decoders because all hypotheses spanning the same range of source sentence have been stacked together in the same chart cell. But to be a general framework, this step is necessary for some state-of-the-art phrase-based decoders (Koehn, 2007; Och and Ney, 2004) because in these decoders, hypotheses with different coverage vectors can co-exist in the same bin, or hypotheses associated with the same coverage vector might appear in different bins. Note that a member model does not enlarge the theoretical search sp"
P09-1066,E06-1005,0,0.0485259,"oduction Recent research has shown substantial improvements can be achieved by utilizing consensus statistics obtained from outputs of multiple machine translation systems. Translation consensus can be measured either at sentence level or at word level. For example, Minimum Bayes Risk (MBR) (Kumar and Byrne, 2004) decoding over n-best list tries to find a hypothesis with lowest expected loss with respect to all the other translations, which can be viewed as sentence-level consensus-based decoding. Word based methods proposed range from straightforward consensus voting (Bangalore et al., 2001; Matusov et al., 2006) to more complicated word-based system combination model (Rosti et al., 2007; Sim et al., 2007). Typically, the resulting systems take outputs of individual machine translation systems as input, and build a new confusion network for second-pass decoding. There have been many efforts dedicated to advance the state-of-the-art performance by combining multiple systems’ outputs. Most of the work focused on seeking better word alignment for consensus-based confusion network decoding (Matusov et al., 2006) or word-level system combination (He et al., 2008; Ayan et al., 2008). In addition to better a"
P09-1066,P02-1038,0,0.124274,"Missing"
P09-1066,P03-1021,0,0.0111584,"egation of all n-grams of the same order. For each ngram of order n, we introduce a pair of complementary consensus measure functions ?? + ?, ?′ and ?? − ?, ?′ described as follows: ?? + ?, ? ′ is the n-gram agreement measure function which counts the number of occurrences in ? ′ of n-grams in e. So the corresponding feature value will be the expected number of occurrences in ℋ? ? of all n-grams in e: ? −?+1 ?? + ?, ?′ = ?=1 ?? − ?, ? ′ is the n-gram disagreement measure function which is complementary to ?? + ?, ? ′ : ? −?+1 ?=1 Model Training We adapt the Minimum Error Rate Training (MERT) (Och, 2003) algorithm to estimate parameters for each member model in co-decoding. Let ?? be the feature weight vector for member decoder ?? , the training procedure proceeds as follows: 1. Choose initial values for ?1 , … , ?? 2. Perform co-decoding using all member decoders on a development set D with ?1 , … , ?? . For each decoder ?? , find a new feature weight vector ?′? which optimizes the specified evaluation criterion L on D using the MERT algorithm based on the n-best list ℋ? generated by ?? : ?′? = argmax? ? (?|?, ℋ? , ?)) where T denotes the translations selected by re-ranking the translations"
P09-1066,J04-4002,0,0.162557,"forth. Step 2 is used to facilitate the computation of feature functions ℎ?,? (?, ℋ? ∙ ) , which require both e and every hypothesis in ℋ? ∙ should be translations of the same set of source words. This step seems to be redundant for CKY-style MT decoders (Liu et al., 2006; Xiong et al., 2006; Chiang, 2005) since the grouping is immediately available from decoders because all hypotheses spanning the same range of source sentence have been stacked together in the same chart cell. But to be a general framework, this step is necessary for some state-of-the-art phrase-based decoders (Koehn, 2007; Och and Ney, 2004) because in these decoders, hypotheses with different coverage vectors can co-exist in the same bin, or hypotheses associated with the same coverage vector might appear in different bins. Note that a member model does not enlarge the theoretical search space of its baseline model, the only change is hypothesis scoring. By rerunning a complete decoding process, member model can be applied to re-score all hypotheses explored by a decoder. Therefore step 3 can be viewed as full-scale hypothesis re-ranking because the re-ranking scope is beyond the limited n-best hypotheses currently cached in ℋ?"
P09-1066,W04-3250,0,0.67335,"ltiple n-best lists. The rest of the paper is structured as follows. Section 2 gives a formal description of the codecoding model, the strategy to apply consensus information and hypotheses ranking in decoding. In Section 3, we make detailed comparison between co-decoding and related work such as system combination and hypotheses selection out of multiple systems. Experimental results and discussions are presented in Section 4. Section 5 concludes the paper. 2 2.1 Collaborative Decoding Overview Collaborative decoding does not present a full SMT model as other SMT decoders do such as Pharaoh (Koehn, 2004) or Hiero (Chiang, 2005). Instead, it provides a framework that accommodates and coordinates multiple MT decoders. Conceptually, collaborative decoding incorporates the following four constituents: 1. Co-decoding model. A co-decoding model consists of a set of member models, which are a set of augmented baseline models. We call decoders based on member models member decoders, and those based on baseline models baseline decoders. In our work, any Maximum A Posteriori (MAP) SMT model with log-linear formulation (Och, 2002) can be a qualified candidate for a baseline model. The requirement for a"
P09-1066,P08-1066,0,0.047141,"cal phrase-based decoder. Phrasal rules are extracted from all bilingual sentence pairs, while rules with variables are extracted only from selected data sets including LDC2003E14, LDC2003E07, LDC2005T06 and LDC2005T10, which contain around 350,000 sentence pairs, 8.8M Chinese words and 10.3M English words. The second one (SYS2) is a BTG decoder with lexicalized reordering model based on maximum entropy principle as proposed by Xiong et al. (2006). We use all the bilingual data to extract phrases up to length 3. The third one (SYS3) is a string-to-dependency tree –based decoder as proposed by Shen et al. (2008). For rule extraction we use the same setting as in SYS1. We parsed the language model training data with Berkeley parser, and then trained a dependency language model based on the parsing output. All baseline decoders are extended with n-gram consensus –based co-decoding features to construct member decoders. By default, the beam size of 20 is used for all decoders in the experiments. We run two iterations of decoding for each member decoder, and hold the value of ? in Equation 5 as a constant 0.05, which is tuned on the test data of NIST 2004 Chinese-toEnglish machine translation task. 3.3 D"
P09-1066,koen-2004-pharaoh,0,0.617964,"ltiple n-best lists. The rest of the paper is structured as follows. Section 2 gives a formal description of the codecoding model, the strategy to apply consensus information and hypotheses ranking in decoding. In Section 3, we make detailed comparison between co-decoding and related work such as system combination and hypotheses selection out of multiple systems. Experimental results and discussions are presented in Section 4. Section 5 concludes the paper. 2 2.1 Collaborative Decoding Overview Collaborative decoding does not present a full SMT model as other SMT decoders do such as Pharaoh (Koehn, 2004) or Hiero (Chiang, 2005). Instead, it provides a framework that accommodates and coordinates multiple MT decoders. Conceptually, collaborative decoding incorporates the following four constituents: 1. Co-decoding model. A co-decoding model consists of a set of member models, which are a set of augmented baseline models. We call decoders based on member models member decoders, and those based on baseline models baseline decoders. In our work, any Maximum A Posteriori (MAP) SMT model with log-linear formulation (Och, 2002) can be a qualified candidate for a baseline model. The requirement for a"
P09-1066,D08-1065,0,0.341537,"?′ ?? ?? (?, ?′) (4) ?′ ∈ℋ? ? where e is a translation of f by decoder ?? (? ≠ ?), ? ′ is a translation in ℋ? ? and ? ?′ ?? is the posterior probability of translation ? ′ determined by decoder ?? given source sentence f. ?? (?, ?′) is a consensus measure defined on e and ?′, by varying which different feature functions can be obtained. 587 Referring to the log-linear model formulation, the translation posterior ? ?′ ?? can be computed as: ? ?′ ?? = exp ??? ?′ ?′′ ∈ℋ? ? exp ??? ?′′ (5) 2.5 where ?? (∙) is the score function given in Equation 2, and ? is a scaling factor following the work of Tromble et al. (2008) To compute the consensus measures, we further decompose each ?? ?, ?′ into n-gram matching statistics between e and ?′. Here we do not discriminate among different lexical n-grams and are only concerned with statistics aggregation of all n-grams of the same order. For each ngram of order n, we introduce a pair of complementary consensus measure functions ?? + ?, ?′ and ?? − ?, ?′ described as follows: ?? + ?, ? ′ is the n-gram agreement measure function which counts the number of occurrences in ? ′ of n-grams in e. So the corresponding feature value will be the expected number of occurrences"
P09-1066,P07-2045,0,0.00862222,"Missing"
P09-1066,P06-1066,0,0.614687,"until a preset iteration limit is reached. In the iterative decoding procedure described above, hypotheses of different decoders can be mutually improved. For example, given two decoders ?1 and ?2 with hypotheses sets ℋ1 and ℋ2 , improvements on ℋ1 enable ?2 to improve ℋ2 , and in turn ℋ1 benefits from improved ℋ2 , and so forth. Step 2 is used to facilitate the computation of feature functions ℎ?,? (?, ℋ? ∙ ) , which require both e and every hypothesis in ℋ? ∙ should be translations of the same set of source words. This step seems to be redundant for CKY-style MT decoders (Liu et al., 2006; Xiong et al., 2006; Chiang, 2005) since the grouping is immediately available from decoders because all hypotheses spanning the same range of source sentence have been stacked together in the same chart cell. But to be a general framework, this step is necessary for some state-of-the-art phrase-based decoders (Koehn, 2007; Och and Ney, 2004) because in these decoders, hypotheses with different coverage vectors can co-exist in the same bin, or hypotheses associated with the same coverage vector might appear in different bins. Note that a member model does not enlarge the theoretical search space of its baseline"
P09-1066,W08-0329,0,\N,Missing
P09-1066,N07-1029,0,\N,Missing
P10-2002,C08-1041,0,0.496023,"selection for hypothesis generation, including both source-side rule selection and targetside rule selection where the source-side rule determines what part of source words to be translated and the target-side rule provides one of the candidate translations of the source-side rule. Improper rule selections may result in poor translations. There is some related work about the hierarchical rule selection. In the original work (Chiang, 2005), the target-side rule selection is analogous to the model in traditional phrase-based SMT system such as Pharaoh (Koehn et al., 2003). Extending this work, (He et al., 2008; Liu et al., 2008) integrate rich context information of non-terminals to predict the target-side rule selection. Different from the above work where the probability distribution of source-side rule selection is uniform, (Setiawan et al., 2009) proposes to select sourceside rules based on the captured function words which often play an important role in word reordering. There is also some work considering to involve more rich contexts to guide the source-side rule selection. (Marton and Resnik, 2008; Xiong et al., 2009) explore the source syntactic information to reward exact matching structu"
P10-2002,N03-1017,0,0.0859175,"ngzhou}@microsoft.com Abstract proper rule selection for hypothesis generation, including both source-side rule selection and targetside rule selection where the source-side rule determines what part of source words to be translated and the target-side rule provides one of the candidate translations of the source-side rule. Improper rule selections may result in poor translations. There is some related work about the hierarchical rule selection. In the original work (Chiang, 2005), the target-side rule selection is analogous to the model in traditional phrase-based SMT system such as Pharaoh (Koehn et al., 2003). Extending this work, (He et al., 2008; Liu et al., 2008) integrate rich context information of non-terminals to predict the target-side rule selection. Different from the above work where the probability distribution of source-side rule selection is uniform, (Setiawan et al., 2009) proposes to select sourceside rules based on the captured function words which often play an important role in word reordering. There is also some work considering to involve more rich contexts to guide the source-side rule selection. (Marton and Resnik, 2008; Xiong et al., 2009) explore the source syntactic infor"
P10-2002,D08-1010,0,0.286273,"othesis generation, including both source-side rule selection and targetside rule selection where the source-side rule determines what part of source words to be translated and the target-side rule provides one of the candidate translations of the source-side rule. Improper rule selections may result in poor translations. There is some related work about the hierarchical rule selection. In the original work (Chiang, 2005), the target-side rule selection is analogous to the model in traditional phrase-based SMT system such as Pharaoh (Koehn et al., 2003). Extending this work, (He et al., 2008; Liu et al., 2008) integrate rich context information of non-terminals to predict the target-side rule selection. Different from the above work where the probability distribution of source-side rule selection is uniform, (Setiawan et al., 2009) proposes to select sourceside rules based on the captured function words which often play an important role in word reordering. There is also some work considering to involve more rich contexts to guide the source-side rule selection. (Marton and Resnik, 2008; Xiong et al., 2009) explore the source syntactic information to reward exact matching structure rules or punish"
P10-2002,P07-1089,0,0.0312973,"Missing"
P10-2002,P06-1077,0,0.112862,"ctical SMT systems with the log-linear model framework. The experimental results show that our method can yield significant improvements in performance. 1 Introduction Hierarchical phrase-based model has strong expression capabilities of translation knowledge. It can not only maintain the strength of phrase translation in traditional phrase-based models (Koehn et al., 2003; Xiong et al., 2006), but also characterize the complicated long distance reordering similar to syntactic based statistical machine translation (SMT) models (Yamada and Knight, 2001; Quirk et al., 2005; Galley et al., 2006; Liu et al., 2006; Marcu et al., 2006; Mi et al., 2008; Shen et al., 2008). In hierarchical phrase-based SMT systems, due to the flexibility of rule matching, a huge number of hierarchical rules could be automatically learnt from bilingual training corpus (Chiang, 2005). SMT decoders are forced to face the challenge of ∗ This work was finished while the first author visited Microsoft Research Asia as an intern. 6 Proceedings of the ACL 2010 Conference Short Papers, pages 6–11, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics vious methods. Our proposed joint probability model"
P10-2002,W06-1606,0,0.0274999,"with the log-linear model framework. The experimental results show that our method can yield significant improvements in performance. 1 Introduction Hierarchical phrase-based model has strong expression capabilities of translation knowledge. It can not only maintain the strength of phrase translation in traditional phrase-based models (Koehn et al., 2003; Xiong et al., 2006), but also characterize the complicated long distance reordering similar to syntactic based statistical machine translation (SMT) models (Yamada and Knight, 2001; Quirk et al., 2005; Galley et al., 2006; Liu et al., 2006; Marcu et al., 2006; Mi et al., 2008; Shen et al., 2008). In hierarchical phrase-based SMT systems, due to the flexibility of rule matching, a huge number of hierarchical rules could be automatically learnt from bilingual training corpus (Chiang, 2005). SMT decoders are forced to face the challenge of ∗ This work was finished while the first author visited Microsoft Research Asia as an intern. 6 Proceedings of the ACL 2010 Conference Short Papers, pages 6–11, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics vious methods. Our proposed joint probability model is factored into fou"
P10-2002,P08-1114,0,0.322519,"del in traditional phrase-based SMT system such as Pharaoh (Koehn et al., 2003). Extending this work, (He et al., 2008; Liu et al., 2008) integrate rich context information of non-terminals to predict the target-side rule selection. Different from the above work where the probability distribution of source-side rule selection is uniform, (Setiawan et al., 2009) proposes to select sourceside rules based on the captured function words which often play an important role in word reordering. There is also some work considering to involve more rich contexts to guide the source-side rule selection. (Marton and Resnik, 2008; Xiong et al., 2009) explore the source syntactic information to reward exact matching structure rules or punish crossing structure rules. All the previous work mainly focused on either source-side rule selection task or target-side rule selection task rather than both of them together. The separation of these two tasks, however, weakens the high interrelation between them. In this paper, we propose to integrate both source-side and target-side rule selection in a unified model. The intuition is that the joint selection of source-side and target-side rules is more reliable as it conducts the"
P10-2002,P08-1023,0,0.023425,"model framework. The experimental results show that our method can yield significant improvements in performance. 1 Introduction Hierarchical phrase-based model has strong expression capabilities of translation knowledge. It can not only maintain the strength of phrase translation in traditional phrase-based models (Koehn et al., 2003; Xiong et al., 2006), but also characterize the complicated long distance reordering similar to syntactic based statistical machine translation (SMT) models (Yamada and Knight, 2001; Quirk et al., 2005; Galley et al., 2006; Liu et al., 2006; Marcu et al., 2006; Mi et al., 2008; Shen et al., 2008). In hierarchical phrase-based SMT systems, due to the flexibility of rule matching, a huge number of hierarchical rules could be automatically learnt from bilingual training corpus (Chiang, 2005). SMT decoders are forced to face the challenge of ∗ This work was finished while the first author visited Microsoft Research Asia as an intern. 6 Proceedings of the ACL 2010 Conference Short Papers, pages 6–11, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics vious methods. Our proposed joint probability model is factored into four sub-models that"
P10-2002,P02-1040,0,0.0820023,"e instances for both CBSM and CBTM is 4.68M, while the training size of constructed negative instances is 3.74M and 3.03M respectively. Following (Setiawan et al., 2009), we identify function words as the 128 most frequent words in the corpus. The interpolation weights are set to θ = 0.75 and ϕ = 0.70. The 5-gram language model is trained over the English portion of FBIS corpus plus Xinhua portion of the Gigaword corpus. The development data is from NIST 2005 evaluation data and the test data is from NIST 2006 and NIST 2008 evaluation data. The evaluation metric is the case-insensitive BLEU4 (Papineni et al., 2002). Statistical significance in BLEU score differences is tested by paired bootstrap re-sampling (Koehn, 2004). 4.2 NIST 2006 0.3025 0.3061 0.3089 0.3141 As shown in Table 1, all the methods outperform the baseline because they have extra models to guide the hierarchical rule selection in some ways which might lead to better translation. Apparently, our method also performs better than the other two approaches, indicating that our method is more effective in the hierarchical rule selection as both source-side and target-side rules are selected together. 4.3 Effect of sub-models Due to the space"
P10-2002,N07-1051,0,0.0342946,"d 5. Length features, which are the length of sub-phrases covered by source nonterminals. 4 4.1 Experiments NIST 2008 0.2200 0.2254 0.2253 0.2318 Table 1: Comparison results, our method is significantly better than the baseline, as well as the other two approaches (p < 0.01) Experiment setting We implement a hierarchical phrase-based system similar to the Hiero (Chiang, 2005) and evaluate our method on the Chinese-to-English translation task. Our bilingual training data comes from FBIS corpus, which consists of around 160K sentence pairs where the source data is parsed by the Berkeley parser (Petrov and Klein, 2007). The ME training toolkit, developed by (Zhang, 2006), is used to train our CBSM and CBTM. The training size of constructed positive instances for both CBSM and CBTM is 4.68M, while the training size of constructed negative instances is 3.74M and 3.03M respectively. Following (Setiawan et al., 2009), we identify function words as the 128 most frequent words in the corpus. The interpolation weights are set to θ = 0.75 and ϕ = 0.70. The 5-gram language model is trained over the English portion of FBIS corpus plus Xinhua portion of the Gigaword corpus. The development data is from NIST 2005 evalu"
P10-2002,J96-1002,0,0.0681035,"e the system performance significantly. 2 the monolingual source side of the training corpus. CFSM is used to capture how likely the sourceside rule is linguistically motivated or has the corresponding target-side counterpart. For CBSM, it can be naturally viewed as a classification problem where each distinct source-side rule is a single class. However, considering the huge number of classes may cause serious data sparseness problem and thereby degrade the classification accuracy, we approximate CBSM by a binary classification problem which can be solved by the maximum entropy (ME) approach (Berger et al., 1996) as follows: Ps (α|C) ≈ Ps (υ|α, C) P exp[ i λi hi (υ, α, C)] P =P 0 υ 0 exp[ i λi hi (υ , α, C)] Hierarchical Rule Selection Model Following (Chiang, 2005), hα, γi is used to represent a synchronous context free grammar (SCFG) rule extracted from the training corpus, where α and γ are the source-side and target-side rule respectively. Let C be the context of hα, γi. Formally, our joint probability model of hierarchical rule selection is described as follows: P (α, γ|C) = P (α|C)P (γ|α, C) where υ ∈ {0, 1} is the indicator whether the source-side rule is applied during decoding, υ = 1 when the"
P10-2002,P05-1034,0,0.0418459,"d can be easily incorporated into the practical SMT systems with the log-linear model framework. The experimental results show that our method can yield significant improvements in performance. 1 Introduction Hierarchical phrase-based model has strong expression capabilities of translation knowledge. It can not only maintain the strength of phrase translation in traditional phrase-based models (Koehn et al., 2003; Xiong et al., 2006), but also characterize the complicated long distance reordering similar to syntactic based statistical machine translation (SMT) models (Yamada and Knight, 2001; Quirk et al., 2005; Galley et al., 2006; Liu et al., 2006; Marcu et al., 2006; Mi et al., 2008; Shen et al., 2008). In hierarchical phrase-based SMT systems, due to the flexibility of rule matching, a huge number of hierarchical rules could be automatically learnt from bilingual training corpus (Chiang, 2005). SMT decoders are forced to face the challenge of ∗ This work was finished while the first author visited Microsoft Research Asia as an intern. 6 Proceedings of the ACL 2010 Conference Short Papers, pages 6–11, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics vious methods"
P10-2002,P05-1033,0,0.339463,"stitute of Technology, Harbin, China {cuilei,tjzhao}@mtlab.hit.edu.cn ‡ Microsoft Research Asia, Beijing, China {dozhang,muli,mingzhou}@microsoft.com Abstract proper rule selection for hypothesis generation, including both source-side rule selection and targetside rule selection where the source-side rule determines what part of source words to be translated and the target-side rule provides one of the candidate translations of the source-side rule. Improper rule selections may result in poor translations. There is some related work about the hierarchical rule selection. In the original work (Chiang, 2005), the target-side rule selection is analogous to the model in traditional phrase-based SMT system such as Pharaoh (Koehn et al., 2003). Extending this work, (He et al., 2008; Liu et al., 2008) integrate rich context information of non-terminals to predict the target-side rule selection. Different from the above work where the probability distribution of source-side rule selection is uniform, (Setiawan et al., 2009) proposes to select sourceside rules based on the captured function words which often play an important role in word reordering. There is also some work considering to involve more r"
P10-2002,N09-1025,0,0.0381803,"ilingual training corpus, among which we assume hrs , rti i is extracted from the sentence pair hs, ti. Then, we construct hυ = 1, C(rs ), C(rti )i 3.2 Context-based features for ME training ME approach has the merit of easily combining different features to predict the probability of each class. We incorporate into the ME based model the following informative context-based features to train CBSM and CBTM. These features are carefully designed to reduce the data sparseness problem and some of them are inspired by previous work (He et al., 2008; Gimpel and Smith, 2008; Marton and Resnik, 2008; Chiang et al., 2009; Setiawan et al., 2009; Shen et al., 2009; Xiong et al., 2009): 1. Function word features, which indicate whether the hierarchical source-side/targetside rule strings and sub-phrases covered by non-terminals contain function words that are often important clues of predicting syntactic structures. 1 Because the aligned target words are not contiguous and ”cooperation” is aligned to the word outside the source-side rule. 8 2. POS features, which are POS tags of the boundary source words covered by nonterminals. We compare our method with the baseline and some typical approaches listed in Table"
P10-2002,P08-1066,0,0.0351936,"The experimental results show that our method can yield significant improvements in performance. 1 Introduction Hierarchical phrase-based model has strong expression capabilities of translation knowledge. It can not only maintain the strength of phrase translation in traditional phrase-based models (Koehn et al., 2003; Xiong et al., 2006), but also characterize the complicated long distance reordering similar to syntactic based statistical machine translation (SMT) models (Yamada and Knight, 2001; Quirk et al., 2005; Galley et al., 2006; Liu et al., 2006; Marcu et al., 2006; Mi et al., 2008; Shen et al., 2008). In hierarchical phrase-based SMT systems, due to the flexibility of rule matching, a huge number of hierarchical rules could be automatically learnt from bilingual training corpus (Chiang, 2005). SMT decoders are forced to face the challenge of ∗ This work was finished while the first author visited Microsoft Research Asia as an intern. 6 Proceedings of the ACL 2010 Conference Short Papers, pages 6–11, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics vious methods. Our proposed joint probability model is factored into four sub-models that can be further clas"
P10-2002,P06-1121,0,0.115919,"rporated into the practical SMT systems with the log-linear model framework. The experimental results show that our method can yield significant improvements in performance. 1 Introduction Hierarchical phrase-based model has strong expression capabilities of translation knowledge. It can not only maintain the strength of phrase translation in traditional phrase-based models (Koehn et al., 2003; Xiong et al., 2006), but also characterize the complicated long distance reordering similar to syntactic based statistical machine translation (SMT) models (Yamada and Knight, 2001; Quirk et al., 2005; Galley et al., 2006; Liu et al., 2006; Marcu et al., 2006; Mi et al., 2008; Shen et al., 2008). In hierarchical phrase-based SMT systems, due to the flexibility of rule matching, a huge number of hierarchical rules could be automatically learnt from bilingual training corpus (Chiang, 2005). SMT decoders are forced to face the challenge of ∗ This work was finished while the first author visited Microsoft Research Asia as an intern. 6 Proceedings of the ACL 2010 Conference Short Papers, pages 6–11, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics vious methods. Our proposed joint"
P10-2002,D09-1008,0,0.0917229,"ume hrs , rti i is extracted from the sentence pair hs, ti. Then, we construct hυ = 1, C(rs ), C(rti )i 3.2 Context-based features for ME training ME approach has the merit of easily combining different features to predict the probability of each class. We incorporate into the ME based model the following informative context-based features to train CBSM and CBTM. These features are carefully designed to reduce the data sparseness problem and some of them are inspired by previous work (He et al., 2008; Gimpel and Smith, 2008; Marton and Resnik, 2008; Chiang et al., 2009; Setiawan et al., 2009; Shen et al., 2009; Xiong et al., 2009): 1. Function word features, which indicate whether the hierarchical source-side/targetside rule strings and sub-phrases covered by non-terminals contain function words that are often important clues of predicting syntactic structures. 1 Because the aligned target words are not contiguous and ”cooperation” is aligned to the word outside the source-side rule. 8 2. POS features, which are POS tags of the boundary source words covered by nonterminals. We compare our method with the baseline and some typical approaches listed in Table 1 where XP+ denotes the approach in (Marto"
P10-2002,W08-0302,0,0.0163984,"ed from multiple distinct sentence pairs in the bilingual training corpus, among which we assume hrs , rti i is extracted from the sentence pair hs, ti. Then, we construct hυ = 1, C(rs ), C(rti )i 3.2 Context-based features for ME training ME approach has the merit of easily combining different features to predict the probability of each class. We incorporate into the ME based model the following informative context-based features to train CBSM and CBTM. These features are carefully designed to reduce the data sparseness problem and some of them are inspired by previous work (He et al., 2008; Gimpel and Smith, 2008; Marton and Resnik, 2008; Chiang et al., 2009; Setiawan et al., 2009; Shen et al., 2009; Xiong et al., 2009): 1. Function word features, which indicate whether the hierarchical source-side/targetside rule strings and sub-phrases covered by non-terminals contain function words that are often important clues of predicting syntactic structures. 1 Because the aligned target words are not contiguous and ”cooperation” is aligned to the word outside the source-side rule. 8 2. POS features, which are POS tags of the boundary source words covered by nonterminals. We compare our method with the baselin"
P10-2002,P09-1037,0,0.121053,"didate translations of the source-side rule. Improper rule selections may result in poor translations. There is some related work about the hierarchical rule selection. In the original work (Chiang, 2005), the target-side rule selection is analogous to the model in traditional phrase-based SMT system such as Pharaoh (Koehn et al., 2003). Extending this work, (He et al., 2008; Liu et al., 2008) integrate rich context information of non-terminals to predict the target-side rule selection. Different from the above work where the probability distribution of source-side rule selection is uniform, (Setiawan et al., 2009) proposes to select sourceside rules based on the captured function words which often play an important role in word reordering. There is also some work considering to involve more rich contexts to guide the source-side rule selection. (Marton and Resnik, 2008; Xiong et al., 2009) explore the source syntactic information to reward exact matching structure rules or punish crossing structure rules. All the previous work mainly focused on either source-side rule selection task or target-side rule selection task rather than both of them together. The separation of these two tasks, however, weakens"
P10-2002,J97-3002,0,0.020514,"edicting syntactic structures. 1 Because the aligned target words are not contiguous and ”cooperation” is aligned to the word outside the source-side rule. 8 2. POS features, which are POS tags of the boundary source words covered by nonterminals. We compare our method with the baseline and some typical approaches listed in Table 1 where XP+ denotes the approach in (Marton and Resnik, 2008) and TOFW (topological ordering of function words) stands for the method in (Setiawan et al., 2009). As (Xiong et al., 2009)’s work is based on phrasal SMT system with bracketing transduction grammar rules (Wu, 1997) and (Shen et al., 2009)’s work is based on the string-to-dependency SMT model, we do not implement these two related work due to their different models from ours. We also do not compare with (He et al., 2008)’s work due to its less practicability of integrating numerous sub-models. 3. Syntactic features, which are the constituent constraints of hierarchical source-side rules exactly matching or crossing syntactic subtrees. 4. Rule format features, which are nonterminal positions and orders in sourceside/target-side rules. This feature interacts between source and target components since it sh"
P10-2002,P06-1066,0,0.0365012,"e selection of hierarchical rules. The proposed model is estimated based on four sub-models where the rich context knowledge from both source and target sides is leveraged. Our method can be easily incorporated into the practical SMT systems with the log-linear model framework. The experimental results show that our method can yield significant improvements in performance. 1 Introduction Hierarchical phrase-based model has strong expression capabilities of translation knowledge. It can not only maintain the strength of phrase translation in traditional phrase-based models (Koehn et al., 2003; Xiong et al., 2006), but also characterize the complicated long distance reordering similar to syntactic based statistical machine translation (SMT) models (Yamada and Knight, 2001; Quirk et al., 2005; Galley et al., 2006; Liu et al., 2006; Marcu et al., 2006; Mi et al., 2008; Shen et al., 2008). In hierarchical phrase-based SMT systems, due to the flexibility of rule matching, a huge number of hierarchical rules could be automatically learnt from bilingual training corpus (Chiang, 2005). SMT decoders are forced to face the challenge of ∗ This work was finished while the first author visited Microsoft Research A"
P10-2002,P09-1036,0,0.177282,"-based SMT system such as Pharaoh (Koehn et al., 2003). Extending this work, (He et al., 2008; Liu et al., 2008) integrate rich context information of non-terminals to predict the target-side rule selection. Different from the above work where the probability distribution of source-side rule selection is uniform, (Setiawan et al., 2009) proposes to select sourceside rules based on the captured function words which often play an important role in word reordering. There is also some work considering to involve more rich contexts to guide the source-side rule selection. (Marton and Resnik, 2008; Xiong et al., 2009) explore the source syntactic information to reward exact matching structure rules or punish crossing structure rules. All the previous work mainly focused on either source-side rule selection task or target-side rule selection task rather than both of them together. The separation of these two tasks, however, weakens the high interrelation between them. In this paper, we propose to integrate both source-side and target-side rule selection in a unified model. The intuition is that the joint selection of source-side and target-side rules is more reliable as it conducts the search in a larger sp"
P10-2002,P01-1067,0,0.112783,"s is leveraged. Our method can be easily incorporated into the practical SMT systems with the log-linear model framework. The experimental results show that our method can yield significant improvements in performance. 1 Introduction Hierarchical phrase-based model has strong expression capabilities of translation knowledge. It can not only maintain the strength of phrase translation in traditional phrase-based models (Koehn et al., 2003; Xiong et al., 2006), but also characterize the complicated long distance reordering similar to syntactic based statistical machine translation (SMT) models (Yamada and Knight, 2001; Quirk et al., 2005; Galley et al., 2006; Liu et al., 2006; Marcu et al., 2006; Mi et al., 2008; Shen et al., 2008). In hierarchical phrase-based SMT systems, due to the flexibility of rule matching, a huge number of hierarchical rules could be automatically learnt from bilingual training corpus (Chiang, 2005). SMT decoders are forced to face the challenge of ∗ This work was finished while the first author visited Microsoft Research Asia as an intern. 6 Proceedings of the ACL 2010 Conference Short Papers, pages 6–11, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Lingu"
P10-2002,W04-3250,0,\N,Missing
P11-1126,P05-1033,0,0.734658,"y the HM decoder itself, and a new set of model independent features are used to seek the final best translation from this new search space. Few assumptions are made by our approach about the underlying component systems, enabling us to leverage SMT models based on arbitrary paradigms. We compare our approach with several related techniques, and demonstrate significant BLEU improvements in large-scale Chinese-to-English translation tasks. 1 Introduction Besides tremendous efforts on constructing more complicated and accurate models for statistical machine translation (SMT) (Och and Ney, 2004; Chiang, 2005; Galley et al., 2006; Shen et al., 2008; Chiang 2010), many researchers have concentrated on the approaches that improve translation quality using information between hypotheses from one or more SMT systems as well. System combination is built on top of the N-best outputs generated by multiple component systems (Rosti et al., 2007; He et al., 2008; Li et al., 2009b) which aligns multiple hypotheses to build confusion networks as new search spaces, and outputs Mu Li, and Ming Zhou Natural Language Computing Group Microsoft Research Asia Beijing, China {muli,mingzhou}@microsoft.com the highest"
P11-1126,P10-1146,0,0.0227603,"ndent features are used to seek the final best translation from this new search space. Few assumptions are made by our approach about the underlying component systems, enabling us to leverage SMT models based on arbitrary paradigms. We compare our approach with several related techniques, and demonstrate significant BLEU improvements in large-scale Chinese-to-English translation tasks. 1 Introduction Besides tremendous efforts on constructing more complicated and accurate models for statistical machine translation (SMT) (Och and Ney, 2004; Chiang, 2005; Galley et al., 2006; Shen et al., 2008; Chiang 2010), many researchers have concentrated on the approaches that improve translation quality using information between hypotheses from one or more SMT systems as well. System combination is built on top of the N-best outputs generated by multiple component systems (Rosti et al., 2007; He et al., 2008; Li et al., 2009b) which aligns multiple hypotheses to build confusion networks as new search spaces, and outputs Mu Li, and Ming Zhou Natural Language Computing Group Microsoft Research Asia Beijing, China {muli,mingzhou}@microsoft.com the highest scoring paths as the final translations. Consensus dec"
P11-1126,C10-2025,1,0.718798,"ures from the predictions of other member systems. However, in co-decoding, all member systems must work in a synchronous way, and hypotheses between different systems cannot be shared during decoding procedure; Liu et al. (2009) proposes joint-decoding, in which multiple SMT models are combined in either translation or derivation levels. However, their method relies on the correspondence between nodes in hypergraph outputs of different models. HM decoding, on the other hand, can use hypotheses from component search spaces directly without any restriction. 3.3 Hybrid Decoding Hybrid decoding (Cui et al., 2010) resembles our approach in the motivation. This method uses the system combination technique in decoding directly to combine partial hypotheses from different SMT models. However, confusion network construction brings high computational complexity. What’s more, partial hypotheses generated by confusion network decoding cannot be assigned exact feature values for future use in higher level decoding, and they only use feature values of 1-best hypothesis as an approximation. HM decoding, on the other hand, leverages a set of enriched features, which 1263 are computable for all the hypotheses gene"
P11-1126,P09-1064,0,0.0171445,"ms as well. System combination is built on top of the N-best outputs generated by multiple component systems (Rosti et al., 2007; He et al., 2008; Li et al., 2009b) which aligns multiple hypotheses to build confusion networks as new search spaces, and outputs Mu Li, and Ming Zhou Natural Language Computing Group Microsoft Research Asia Beijing, China {muli,mingzhou}@microsoft.com the highest scoring paths as the final translations. Consensus decoding, on the other hand, can be based on either single or multiple systems: single system based methods (Kumar and Byrne, 2004; Tromble et al., 2008; DeNero et al., 2009; Kumar et al., 2009) re-rank translations produced by a single SMT model using either n-gram posteriors or expected n-gram counts. Because hypotheses generated by a single model are highly correlated, improvements obtained are usually small; recently, dedicated efforts have been made to extend it from single system to multiple systems (Li et al., 2009a; DeNero et al., 2010; Duan et al., 2010). Such methods select translations by optimizing consensus models over the combined hypotheses using all component systems’ posterior distributions. Although these two types of approaches have shown consi"
P11-1126,N10-1141,0,0.352458,"ft.com the highest scoring paths as the final translations. Consensus decoding, on the other hand, can be based on either single or multiple systems: single system based methods (Kumar and Byrne, 2004; Tromble et al., 2008; DeNero et al., 2009; Kumar et al., 2009) re-rank translations produced by a single SMT model using either n-gram posteriors or expected n-gram counts. Because hypotheses generated by a single model are highly correlated, improvements obtained are usually small; recently, dedicated efforts have been made to extend it from single system to multiple systems (Li et al., 2009a; DeNero et al., 2010; Duan et al., 2010). Such methods select translations by optimizing consensus models over the combined hypotheses using all component systems’ posterior distributions. Although these two types of approaches have shown consistent improvements over the standard Maximum a Posteriori (MAP) decoding scheme, most of them are implemented as post-processing procedures over translations generated by MAP decoders. In this sense, the work of Li et al. (2009a) is different in that both partial and full hypotheses are re-ranked during the decoding phase directly using consensus between translations from d"
P11-1126,C10-1036,1,0.617604,"oring paths as the final translations. Consensus decoding, on the other hand, can be based on either single or multiple systems: single system based methods (Kumar and Byrne, 2004; Tromble et al., 2008; DeNero et al., 2009; Kumar et al., 2009) re-rank translations produced by a single SMT model using either n-gram posteriors or expected n-gram counts. Because hypotheses generated by a single model are highly correlated, improvements obtained are usually small; recently, dedicated efforts have been made to extend it from single system to multiple systems (Li et al., 2009a; DeNero et al., 2010; Duan et al., 2010). Such methods select translations by optimizing consensus models over the combined hypotheses using all component systems’ posterior distributions. Although these two types of approaches have shown consistent improvements over the standard Maximum a Posteriori (MAP) decoding scheme, most of them are implemented as post-processing procedures over translations generated by MAP decoders. In this sense, the work of Li et al. (2009a) is different in that both partial and full hypotheses are re-ranked during the decoding phase directly using consensus between translations from different SMT systems"
P11-1126,P06-1121,0,0.0316772,"er itself, and a new set of model independent features are used to seek the final best translation from this new search space. Few assumptions are made by our approach about the underlying component systems, enabling us to leverage SMT models based on arbitrary paradigms. We compare our approach with several related techniques, and demonstrate significant BLEU improvements in large-scale Chinese-to-English translation tasks. 1 Introduction Besides tremendous efforts on constructing more complicated and accurate models for statistical machine translation (SMT) (Och and Ney, 2004; Chiang, 2005; Galley et al., 2006; Shen et al., 2008; Chiang 2010), many researchers have concentrated on the approaches that improve translation quality using information between hypotheses from one or more SMT systems as well. System combination is built on top of the N-best outputs generated by multiple component systems (Rosti et al., 2007; He et al., 2008; Li et al., 2009b) which aligns multiple hypotheses to build confusion networks as new search spaces, and outputs Mu Li, and Ming Zhou Natural Language Computing Group Microsoft Research Asia Beijing, China {muli,mingzhou}@microsoft.com the highest scoring paths as the"
P11-1126,D08-1011,0,0.0195808,"demonstrate significant BLEU improvements in large-scale Chinese-to-English translation tasks. 1 Introduction Besides tremendous efforts on constructing more complicated and accurate models for statistical machine translation (SMT) (Och and Ney, 2004; Chiang, 2005; Galley et al., 2006; Shen et al., 2008; Chiang 2010), many researchers have concentrated on the approaches that improve translation quality using information between hypotheses from one or more SMT systems as well. System combination is built on top of the N-best outputs generated by multiple component systems (Rosti et al., 2007; He et al., 2008; Li et al., 2009b) which aligns multiple hypotheses to build confusion networks as new search spaces, and outputs Mu Li, and Ming Zhou Natural Language Computing Group Microsoft Research Asia Beijing, China {muli,mingzhou}@microsoft.com the highest scoring paths as the final translations. Consensus decoding, on the other hand, can be based on either single or multiple systems: single system based methods (Kumar and Byrne, 2004; Tromble et al., 2008; DeNero et al., 2009; Kumar et al., 2009) re-rank translations produced by a single SMT model using either n-gram posteriors or expected n-gram co"
P11-1126,W04-3250,0,0.0206599,"ontain 5.1M sentence pairs, 128M Chinese words and 147M English words after preprocessing. Word alignments are performed using GIZA++ with the intersect-diag-grow refinement. The English side of bilingual corpus plus Xinhua portion of the LDC English Gigaword Version 3.0 are used to train a 5-gram language model. Translation performance is measured in terms of case-insensitive BLEU scores (Papineni et al., 2002), which compute the brevity penalty using the shortest reference translation for each segment. Statistical significance is computed using the bootstrap re-sampling approach proposed by Koehn (2004). Table 1 gives some data statistics. Data Set MT04(dev) MT05 MT06 MT08 #Sentence 1,788 1,082 616 691 #Word 48,215 29,263 17,316 17,424 Table 1: Statistics on dev and test data sets 4.2 Component Systems For convenience of comparing HM decoding with several related decoding techniques, we include two state-of-the-art SMT systems as component systems only:  PB. A phrase-based system (Xiong et al., 2006) with one lexicalized reordering model based on the maximum entropy principle.  DHPB. A string-to-dependency tree-based system (Shen et al., 2008), which translates source strings to target dep"
P11-1126,N04-1022,0,0.0272922,"between hypotheses from one or more SMT systems as well. System combination is built on top of the N-best outputs generated by multiple component systems (Rosti et al., 2007; He et al., 2008; Li et al., 2009b) which aligns multiple hypotheses to build confusion networks as new search spaces, and outputs Mu Li, and Ming Zhou Natural Language Computing Group Microsoft Research Asia Beijing, China {muli,mingzhou}@microsoft.com the highest scoring paths as the final translations. Consensus decoding, on the other hand, can be based on either single or multiple systems: single system based methods (Kumar and Byrne, 2004; Tromble et al., 2008; DeNero et al., 2009; Kumar et al., 2009) re-rank translations produced by a single SMT model using either n-gram posteriors or expected n-gram counts. Because hypotheses generated by a single model are highly correlated, improvements obtained are usually small; recently, dedicated efforts have been made to extend it from single system to multiple systems (Li et al., 2009a; DeNero et al., 2010; Duan et al., 2010). Such methods select translations by optimizing consensus models over the combined hypotheses using all component systems’ posterior distributions. Although the"
P11-1126,P09-1019,0,0.184738,"mbination is built on top of the N-best outputs generated by multiple component systems (Rosti et al., 2007; He et al., 2008; Li et al., 2009b) which aligns multiple hypotheses to build confusion networks as new search spaces, and outputs Mu Li, and Ming Zhou Natural Language Computing Group Microsoft Research Asia Beijing, China {muli,mingzhou}@microsoft.com the highest scoring paths as the final translations. Consensus decoding, on the other hand, can be based on either single or multiple systems: single system based methods (Kumar and Byrne, 2004; Tromble et al., 2008; DeNero et al., 2009; Kumar et al., 2009) re-rank translations produced by a single SMT model using either n-gram posteriors or expected n-gram counts. Because hypotheses generated by a single model are highly correlated, improvements obtained are usually small; recently, dedicated efforts have been made to extend it from single system to multiple systems (Li et al., 2009a; DeNero et al., 2010; Duan et al., 2010). Such methods select translations by optimizing consensus models over the combined hypotheses using all component systems’ posterior distributions. Although these two types of approaches have shown consistent improvements ov"
P11-1126,P09-1066,1,0.802998,"ificant BLEU improvements in large-scale Chinese-to-English translation tasks. 1 Introduction Besides tremendous efforts on constructing more complicated and accurate models for statistical machine translation (SMT) (Och and Ney, 2004; Chiang, 2005; Galley et al., 2006; Shen et al., 2008; Chiang 2010), many researchers have concentrated on the approaches that improve translation quality using information between hypotheses from one or more SMT systems as well. System combination is built on top of the N-best outputs generated by multiple component systems (Rosti et al., 2007; He et al., 2008; Li et al., 2009b) which aligns multiple hypotheses to build confusion networks as new search spaces, and outputs Mu Li, and Ming Zhou Natural Language Computing Group Microsoft Research Asia Beijing, China {muli,mingzhou}@microsoft.com the highest scoring paths as the final translations. Consensus decoding, on the other hand, can be based on either single or multiple systems: single system based methods (Kumar and Byrne, 2004; Tromble et al., 2008; DeNero et al., 2009; Kumar et al., 2009) re-rank translations produced by a single SMT model using either n-gram posteriors or expected n-gram counts. Because hyp"
P11-1126,P09-1107,0,0.280871,"ificant BLEU improvements in large-scale Chinese-to-English translation tasks. 1 Introduction Besides tremendous efforts on constructing more complicated and accurate models for statistical machine translation (SMT) (Och and Ney, 2004; Chiang, 2005; Galley et al., 2006; Shen et al., 2008; Chiang 2010), many researchers have concentrated on the approaches that improve translation quality using information between hypotheses from one or more SMT systems as well. System combination is built on top of the N-best outputs generated by multiple component systems (Rosti et al., 2007; He et al., 2008; Li et al., 2009b) which aligns multiple hypotheses to build confusion networks as new search spaces, and outputs Mu Li, and Ming Zhou Natural Language Computing Group Microsoft Research Asia Beijing, China {muli,mingzhou}@microsoft.com the highest scoring paths as the final translations. Consensus decoding, on the other hand, can be based on either single or multiple systems: single system based methods (Kumar and Byrne, 2004; Tromble et al., 2008; DeNero et al., 2009; Kumar et al., 2009) re-rank translations produced by a single SMT model using either n-gram posteriors or expected n-gram counts. Because hyp"
P11-1126,P09-1065,0,0.0268883,"ty to generate new translations. In contrast, by reusing hypotheses generated by all component systems in HM decoding, translations beyond any existing search space can be generated. 3.2 Co-Decoding and Joint Decoding Li et al. (2009a) proposes collaborative decoding, an approach that combines translation systems by re-ranking partial and full translations iteratively using n-gram features from the predictions of other member systems. However, in co-decoding, all member systems must work in a synchronous way, and hypotheses between different systems cannot be shared during decoding procedure; Liu et al. (2009) proposes joint-decoding, in which multiple SMT models are combined in either translation or derivation levels. However, their method relies on the correspondence between nodes in hypergraph outputs of different models. HM decoding, on the other hand, can use hypotheses from component search spaces directly without any restriction. 3.3 Hybrid Decoding Hybrid decoding (Cui et al., 2010) resembles our approach in the motivation. This method uses the system combination technique in decoding directly to combine partial hypotheses from different SMT models. However, confusion network construction b"
P11-1126,P03-1021,0,0.0152193,"ience: : the word count feature. 3) : the n-gram posterior feature of computed based on the mixture search space generated by the HM decoder: is the posterior probability of an n-gram in , is the posterior probability of one translation given based on . 4) 1) is the hierarchical rule set provided by the HM decoder itself, equals to 1 when is provided by , and 0 otherwise. 6) : the feature that counts how many n-grams in are newly generated by the HM decoder, which cannot be found in all existing component search spaces: not exist in equals to 1 when does , and 0 otherwise. The MERT algorithm (Och, 2003) is used to tune weights of HM decoding features. 2.4 Decoding Algorithms Two CKY-style algorithms for HM decoding are presented in this subsection. The first one is based on BTG (Wu, 1997), and the second one is based on SCFG, similar to Chiang (2005). 2.4.1 BTG-based HM Decoding The first algorithm, BTG-HMD, is presented in Algorithm 1, where hypotheses of two consecutive source spans are composed using two BTG rules:  Straight rule . It combines translations of two consecutive blocks into a single larger block in a straight order.  Inverted rule . It combines translations of two consecuti"
P11-1126,J04-4002,0,0.0494598,"of rules provided by the HM decoder itself, and a new set of model independent features are used to seek the final best translation from this new search space. Few assumptions are made by our approach about the underlying component systems, enabling us to leverage SMT models based on arbitrary paradigms. We compare our approach with several related techniques, and demonstrate significant BLEU improvements in large-scale Chinese-to-English translation tasks. 1 Introduction Besides tremendous efforts on constructing more complicated and accurate models for statistical machine translation (SMT) (Och and Ney, 2004; Chiang, 2005; Galley et al., 2006; Shen et al., 2008; Chiang 2010), many researchers have concentrated on the approaches that improve translation quality using information between hypotheses from one or more SMT systems as well. System combination is built on top of the N-best outputs generated by multiple component systems (Rosti et al., 2007; He et al., 2008; Li et al., 2009b) which aligns multiple hypotheses to build confusion networks as new search spaces, and outputs Mu Li, and Ming Zhou Natural Language Computing Group Microsoft Research Asia Beijing, China {muli,mingzhou}@microsoft.co"
P11-1126,P02-1040,0,0.0823569,"newswire portions of the NIST 2006 (MT06) and 2008 (MT08) data sets. All bilingual corpora available for the NIST 2008 constrained data track of Chinese-to-English MT task are used as training data, which contain 5.1M sentence pairs, 128M Chinese words and 147M English words after preprocessing. Word alignments are performed using GIZA++ with the intersect-diag-grow refinement. The English side of bilingual corpus plus Xinhua portion of the LDC English Gigaword Version 3.0 are used to train a 5-gram language model. Translation performance is measured in terms of case-insensitive BLEU scores (Papineni et al., 2002), which compute the brevity penalty using the shortest reference translation for each segment. Statistical significance is computed using the bootstrap re-sampling approach proposed by Koehn (2004). Table 1 gives some data statistics. Data Set MT04(dev) MT05 MT06 MT08 #Sentence 1,788 1,082 616 691 #Word 48,215 29,263 17,316 17,424 Table 1: Statistics on dev and test data sets 4.2 Component Systems For convenience of comparing HM decoding with several related decoding techniques, we include two state-of-the-art SMT systems as component systems only:  PB. A phrase-based system (Xiong et al., 20"
P11-1126,P08-1066,0,0.154677,"set of model independent features are used to seek the final best translation from this new search space. Few assumptions are made by our approach about the underlying component systems, enabling us to leverage SMT models based on arbitrary paradigms. We compare our approach with several related techniques, and demonstrate significant BLEU improvements in large-scale Chinese-to-English translation tasks. 1 Introduction Besides tremendous efforts on constructing more complicated and accurate models for statistical machine translation (SMT) (Och and Ney, 2004; Chiang, 2005; Galley et al., 2006; Shen et al., 2008; Chiang 2010), many researchers have concentrated on the approaches that improve translation quality using information between hypotheses from one or more SMT systems as well. System combination is built on top of the N-best outputs generated by multiple component systems (Rosti et al., 2007; He et al., 2008; Li et al., 2009b) which aligns multiple hypotheses to build confusion networks as new search spaces, and outputs Mu Li, and Ming Zhou Natural Language Computing Group Microsoft Research Asia Beijing, China {muli,mingzhou}@microsoft.com the highest scoring paths as the final translations."
P11-1126,P07-1040,0,0.0207307,"ated techniques, and demonstrate significant BLEU improvements in large-scale Chinese-to-English translation tasks. 1 Introduction Besides tremendous efforts on constructing more complicated and accurate models for statistical machine translation (SMT) (Och and Ney, 2004; Chiang, 2005; Galley et al., 2006; Shen et al., 2008; Chiang 2010), many researchers have concentrated on the approaches that improve translation quality using information between hypotheses from one or more SMT systems as well. System combination is built on top of the N-best outputs generated by multiple component systems (Rosti et al., 2007; He et al., 2008; Li et al., 2009b) which aligns multiple hypotheses to build confusion networks as new search spaces, and outputs Mu Li, and Ming Zhou Natural Language Computing Group Microsoft Research Asia Beijing, China {muli,mingzhou}@microsoft.com the highest scoring paths as the final translations. Consensus decoding, on the other hand, can be based on either single or multiple systems: single system based methods (Kumar and Byrne, 2004; Tromble et al., 2008; DeNero et al., 2009; Kumar et al., 2009) re-rank translations produced by a single SMT model using either n-gram posteriors or e"
P11-1126,D08-1065,0,0.0189733,"one or more SMT systems as well. System combination is built on top of the N-best outputs generated by multiple component systems (Rosti et al., 2007; He et al., 2008; Li et al., 2009b) which aligns multiple hypotheses to build confusion networks as new search spaces, and outputs Mu Li, and Ming Zhou Natural Language Computing Group Microsoft Research Asia Beijing, China {muli,mingzhou}@microsoft.com the highest scoring paths as the final translations. Consensus decoding, on the other hand, can be based on either single or multiple systems: single system based methods (Kumar and Byrne, 2004; Tromble et al., 2008; DeNero et al., 2009; Kumar et al., 2009) re-rank translations produced by a single SMT model using either n-gram posteriors or expected n-gram counts. Because hypotheses generated by a single model are highly correlated, improvements obtained are usually small; recently, dedicated efforts have been made to extend it from single system to multiple systems (Li et al., 2009a; DeNero et al., 2010; Duan et al., 2010). Such methods select translations by optimizing consensus models over the combined hypotheses using all component systems’ posterior distributions. Although these two types of approa"
P11-1126,J97-3002,0,0.018326,"s the posterior probability of one translation given based on . 4) 1) is the hierarchical rule set provided by the HM decoder itself, equals to 1 when is provided by , and 0 otherwise. 6) : the feature that counts how many n-grams in are newly generated by the HM decoder, which cannot be found in all existing component search spaces: not exist in equals to 1 when does , and 0 otherwise. The MERT algorithm (Och, 2003) is used to tune weights of HM decoding features. 2.4 Decoding Algorithms Two CKY-style algorithms for HM decoding are presented in this subsection. The first one is based on BTG (Wu, 1997), and the second one is based on SCFG, similar to Chiang (2005). 2.4.1 BTG-based HM Decoding The first algorithm, BTG-HMD, is presented in Algorithm 1, where hypotheses of two consecutive source spans are composed using two BTG rules:  Straight rule . It combines translations of two consecutive blocks into a single larger block in a straight order.  Inverted rule . It combines translations of two consecutive blocks into a single larger block in an inverted order. These two rules are used bottom-up until the whole source sentence is fully covered. We use two reordering rule penalty features,"
P11-1126,P06-1066,0,0.0212564,"ni et al., 2002), which compute the brevity penalty using the shortest reference translation for each segment. Statistical significance is computed using the bootstrap re-sampling approach proposed by Koehn (2004). Table 1 gives some data statistics. Data Set MT04(dev) MT05 MT06 MT08 #Sentence 1,788 1,082 616 691 #Word 48,215 29,263 17,316 17,424 Table 1: Statistics on dev and test data sets 4.2 Component Systems For convenience of comparing HM decoding with several related decoding techniques, we include two state-of-the-art SMT systems as component systems only:  PB. A phrase-based system (Xiong et al., 2006) with one lexicalized reordering model based on the maximum entropy principle.  DHPB. A string-to-dependency tree-based system (Shen et al., 2008), which translates source strings to target dependency trees. A target dependency language model is used as an additional feature. Phrasal rules are extracted on all bilingual data, hierarchical rules used in DHPB and reordering rules used in SCFG-HMD are extracted from a selected data set3. Reordering model used in PB is trained on the same selected data set as well. A trigram dependency language model used in DHPB is trained with the outputs from"
P12-1032,N09-1014,0,0.194275,"ve hundred dollars tea ? Src 我 想要 五百 元 以下 的 茶 . Ref I would like some tea under five hundred dollars . Best1 I would like tea under five hundred dollars . Figure 1. Two sentences from IWSLT (Chinese to English) data set. ""Src"" stands for the source sentence, and ""Ref"" means the reference sentence. ""Best1"" is the final output of the decoder. the most similar translation examples from translation memory (TM) systems (Ma et al., 2011). A classifier is applied to re-rank the n-best output of a decoder, taking as features the information about the agreement with those similar translation examples. Alexandrescu and Kirchhoff (2009) proposed a graph-based semi-supervised model to re-rank n-best translation output. Note that these two attempts are about translation consensus for similar sentences, and about reranking of n-best output. It is still an open question whether translation consensus for similar sentences/spans can be applied to the decoding process. Moreover, the method in Alexandrescu and Kirchhoff (2009) is formulated as a typical and simple label propagation, which leads to very large graph, thus making learning and search inefficient. (c.f. Section 3.) In this paper, we attempt to leverage translation consen"
P12-1032,P09-1064,0,0.0405537,"Missing"
P12-1032,N10-1141,0,0.0239482,"Missing"
P12-1032,C10-1036,1,0.814223,"Missing"
P12-1032,W04-3250,0,0.137364,"Missing"
P12-1032,N04-1022,0,0.0389813,"just a span of it, whether the candidate is the same as or similar to the supporting candidates, and whether the supporting candidates come from the same or different MT system.  This work has been done while the first author was visiting Microsoft Research Asia. Translation consensus is employed in those minimum Bayes risk (MBR) approaches where the loss function of a translation is defined with respect to all other translation candidates. That is, the translation with the minimal Bayes risk is the one to the greatest extent similar to other candidates. These approaches include the work of Kumar and Byrne (2004), which re-ranks the nbest output of a MT decoder, and the work of Tromble et al. (2008) and Kumar et al. (2009), which does MBR decoding for lattices and hypergraphs. Others extend consensus among translations from the same MT system to those from different MT systems. Collaborative decoding (Li et al., 2009) scores the translation of a source span by its n-gram similarity to the translations by other systems. Hypothesis mixture decoding (Duan et al., 2011) performs a second decoding process where the search space is enriched with new hypotheses composed out of existing hypotheses from multip"
P12-1032,P09-1019,0,0.0161674,"upporting candidates come from the same or different MT system.  This work has been done while the first author was visiting Microsoft Research Asia. Translation consensus is employed in those minimum Bayes risk (MBR) approaches where the loss function of a translation is defined with respect to all other translation candidates. That is, the translation with the minimal Bayes risk is the one to the greatest extent similar to other candidates. These approaches include the work of Kumar and Byrne (2004), which re-ranks the nbest output of a MT decoder, and the work of Tromble et al. (2008) and Kumar et al. (2009), which does MBR decoding for lattices and hypergraphs. Others extend consensus among translations from the same MT system to those from different MT systems. Collaborative decoding (Li et al., 2009) scores the translation of a source span by its n-gram similarity to the translations by other systems. Hypothesis mixture decoding (Duan et al., 2011) performs a second decoding process where the search space is enriched with new hypotheses composed out of existing hypotheses from multiple systems. All these approaches are about utilizing consensus among translations for the same (span of) source"
P12-1032,P09-1066,1,0.837826,"Bayes risk (MBR) approaches where the loss function of a translation is defined with respect to all other translation candidates. That is, the translation with the minimal Bayes risk is the one to the greatest extent similar to other candidates. These approaches include the work of Kumar and Byrne (2004), which re-ranks the nbest output of a MT decoder, and the work of Tromble et al. (2008) and Kumar et al. (2009), which does MBR decoding for lattices and hypergraphs. Others extend consensus among translations from the same MT system to those from different MT systems. Collaborative decoding (Li et al., 2009) scores the translation of a source span by its n-gram similarity to the translations by other systems. Hypothesis mixture decoding (Duan et al., 2011) performs a second decoding process where the search space is enriched with new hypotheses composed out of existing hypotheses from multiple systems. All these approaches are about utilizing consensus among translations for the same (span of) source sentence. It should be noted that consensus among translations of similar source sentences/spans is also helpful for good candidate selection. Consider the examples in Figure 1. For the source (Chine"
P12-1032,P06-1096,0,0.0822772,"Missing"
P12-1032,P03-1021,0,0.104259,"eatures and feature weights in the log-linear model. Algorithm 1 Semi-Supervised Learning 0; λ = , , ; while not converged do , , , . λ , , end while return last ( , λ ) e1 a1 m n Algorithm 1 outlines our semi-supervised method for such alternative training. The entire process starts with a decoder without consensus features. Then a graph is constructed out of all training, dev, and test data. The subsequent structured label propagation provides feature values to the MT decoder. The decoder then adds the new features and re-trains all the feature weights by Minimum Error Rate Training (MERT) (Och, 2003). The decoder with new feature weights then provides new n-best candidates and their posteriors for constructing another consensus graph, which in turn gives rise to next round of 306 0.75 1, f1 b c d1 2, f1 d1 b c 3, f2 d1 b c 0.5 e1 a1 b n e1 d 1 b n Figure 2. A toy graph constructed for re-ranking. MERT. This alternation of structured label propagation and MERT stops when the BLEU score on dev data converges, or a pre-set limit (10 rounds) is reached. 5 Graph Construction A technical detail is still needed to complete the description of graph-based consensus, namely, how the actual consensu"
P12-1032,P02-1040,0,0.0822517,"Missing"
P12-1032,D08-1065,0,0.0157186,"didates, and whether the supporting candidates come from the same or different MT system.  This work has been done while the first author was visiting Microsoft Research Asia. Translation consensus is employed in those minimum Bayes risk (MBR) approaches where the loss function of a translation is defined with respect to all other translation candidates. That is, the translation with the minimal Bayes risk is the one to the greatest extent similar to other candidates. These approaches include the work of Kumar and Byrne (2004), which re-ranks the nbest output of a MT decoder, and the work of Tromble et al. (2008) and Kumar et al. (2009), which does MBR decoding for lattices and hypergraphs. Others extend consensus among translations from the same MT system to those from different MT systems. Collaborative decoding (Li et al., 2009) scores the translation of a source span by its n-gram similarity to the translations by other systems. Hypothesis mixture decoding (Duan et al., 2011) performs a second decoding process where the search space is enriched with new hypotheses composed out of existing hypotheses from multiple systems. All these approaches are about utilizing consensus among translations for th"
P12-1032,J97-3002,0,0.317878,"Missing"
P12-1032,P10-1049,0,0.0144735,"urce span. It is not difficult to handle test nodes, since the purpose of MT decoder is to get all possible segmentations of a source sentence in dev/test data, search for the translation candidates of each source span, and calculate the probabilities of the candidates. Therefore, the cells in the search space of a decoder can be directly mapped as test nodes in the graph. Training nodes can be handled similarly, by applying forced alignment. Forced alignment performs phrase segmentation and alignment of each sentence pair of the training data using the full translation system as in decoding (Wuebker et al., 2010). In simpler term, for each sentence pair in training data, a decoder is applied to the source side, and all the translation candidates that do not match any substring of the target side are deleted. The cells of in such a reduced search space of the decoder can be directly mapped as training nodes in the graph, just as in the case of test nodes. Note that, due to pruning in both decoding and translation model training, forced alignment may fail, i.e. the decoder may not be able to produce target side of a sentence pair. In such case we still map the cells in the search space as training nodes"
P12-1032,P06-1066,0,0.0697401,"Missing"
P12-1032,P11-1124,0,\N,Missing
P12-1096,P05-1033,0,0.211131,"Missing"
P12-1096,P05-1066,0,0.264094,"lysis, we find that the parser commits more errors on informal texts, and informal texts usually have more flexible translations. Pre-reorder method makes “hard” decision before decoding, thus is more sensitive to parser errors; on the other hand, integrated model is forced to use a longer distortion limit which leads to more search errors during decoding time. It is possible to use system combination method to get the best of both systems, but we leave this to future work. 6 Discussion on Related Work There have been several studies focusing on compiling hand-crafted syntactic reorder rules. Collins et al. (2005), Wang et al. (2007), Ramanathan et al. (2008), Lee et al. (2010) have developed rules for German-English, Chinese-English, English-Hindi and English-Japanese respectively. Xu et al. (2009) designed a clever precedence reordering rule set for translation from English to several SOV languages. The drawback for hand-crafted rules is that they depend upon expert knowledge to produce and are limited to their targeted language pairs. Automatically learning syntactic reordering rules have also been explored in several work. Li et al. (2007) and Visweswariah et al. (2010) learned probability of reord"
P12-1096,P06-1121,0,0.0208488,"the ranking function is trained by well-established rank learning method to minimize the number of mis-ordered tree nodes in the training data. Tree-to-string systems (Quirk et al., 2005; Liu et al., 2006) model syntactic reordering using minimal or composed translation rules, which may contain reordering involving tree nodes from multiple tree 919 levels. Our method can be naturally extended to deal with such multiple level reordering. For a tree-tostring rule with multiple tree levels, instead of ranking the direct children of the root node, we rank all leaf nodes (Most are frontier nodes (Galley et al., 2006)) in the translation rule. We need to redesign our ranking feature templates to encode the reordering information in the source part of the translation rules. We need to remember the source side context of the rules, the model size would still be much smaller than a full-fledged tree-to-string system because we do not need to explicitly store the target variants for each rule. 7 Conclusion and Future Work In this paper we present a ranking based reordering method to reorder source language to match the word order of target language given the source side parse tree. Reordering is formulated as"
P12-1096,D08-1089,0,0.0432048,"Missing"
P12-1096,C10-1043,0,0.846525,"king-based approach to word reordering. The ranking model is automatically derived from the word aligned parallel data, viewing the source tree nodes to be reordered as list items to be ranked. The ranks of tree nodes are determined by their relative positions in the target language – the node in the most front gets the highest rank, while the ending word in the target sentence gets the lowest rank. The ranking model is trained to directly minimize the mis-ordering of tree nodes, which differs from the prior work based on maximum likelihood estimations of reordering patterns (Li et al., 2007; Genzel, 2010), and does not require any special tweaking in model training. The ranking model can not only be used in a pre-reordering based SMT system, but also be integrated into a phrasebased decoder serving as additional distortion features. We evaluated our approach on large-scale Japanese-English and English-Japanese machine translation tasks, and experimental results show that our approach can bring significant improvements to the baseline phrase-based SMT system in both preordering and integrated decoding settings. In the rest of the paper, we will first formally present our ranking-based word reor"
P12-1096,N03-1017,0,0.0172438,"es in the syntactic parse tree of a source sentence. The ranking model is automatically derived from word aligned parallel data with a syntactic parser for source language based on both lexical and syntactical features. We evaluated our approach on largescale Japanese-English and English-Japanese machine translation tasks, and show that it can significantly outperform the baseline phrasebased SMT system. 1 Introduction Modeling word reordering between source and target sentences has been a research focus since the emerging of statistical machine translation. In phrase-based models (Och, 2002; Koehn et al., 2003), phrase is introduced to serve as the fundamental translation element and deal with local reordering, while a distance based distortion model is used to coarsely depict the exponentially decayed word movement probabilities in language translation. Further work in this direction employed lexi∗ This work has been done while the first author was visiting Microsoft Research Asia. Long-distance word reordering between language pairs with substantial word order difference, such as Japanese with Subject-Object-Verb (SOV) structure and English with Subject-Verb-Object (SVO) structure, is generally vi"
P12-1096,W02-2016,0,0.0316159,"ugh for reordering task, and can be searched through efficiently using a CKY decoder. After finding the best reordered tree Te00 , we can extract one reorder example from every node with more than one child. 3.2 Features Features for the ranking model are extracted from source syntax trees. For English-to-Japanese task, we extract features from Stanford English Dependency Tree (Marneffe et al., 2006), including lexicons, Part-of-Speech tags, dependency labels, punctuations and tree distance between head and dependent. For Japanese-to-English task, we use a chunkbased Japanese dependency tree (Kudo and Matsumoto, 2002). Different from features for English, we do not use dependency labels because they are not available from the Japanese parser. Additionally, Japanese function words are also included as features because they are important grammatical clues. The detailed feature templates are shown in Table 1. CLN (c) child c of t 3.3 Learning Method Take the original tree in Figure 1 for example. At the root node trying, CLN(trying) is 6 because there are six crossing-links under its sub-span: (e1 j4 , e2 j3 ), (e1 j4 , e4 j2 ), (e1 j4 , e5 j1 ), (e2 j3 , e4 j2 ), (e2 j3 , e5 j1 ) and (e4 j2 , e5 j1 ). On the"
P12-1096,C10-1071,0,0.290577,"and informal texts usually have more flexible translations. Pre-reorder method makes “hard” decision before decoding, thus is more sensitive to parser errors; on the other hand, integrated model is forced to use a longer distortion limit which leads to more search errors during decoding time. It is possible to use system combination method to get the best of both systems, but we leave this to future work. 6 Discussion on Related Work There have been several studies focusing on compiling hand-crafted syntactic reorder rules. Collins et al. (2005), Wang et al. (2007), Ramanathan et al. (2008), Lee et al. (2010) have developed rules for German-English, Chinese-English, English-Hindi and English-Japanese respectively. Xu et al. (2009) designed a clever precedence reordering rule set for translation from English to several SOV languages. The drawback for hand-crafted rules is that they depend upon expert knowledge to produce and are limited to their targeted language pairs. Automatically learning syntactic reordering rules have also been explored in several work. Li et al. (2007) and Visweswariah et al. (2010) learned probability of reordering patterns from constituent trees using either Maximum Entrop"
P12-1096,P07-1091,1,0.94429,"but effective ranking-based approach to word reordering. The ranking model is automatically derived from the word aligned parallel data, viewing the source tree nodes to be reordered as list items to be ranked. The ranks of tree nodes are determined by their relative positions in the target language – the node in the most front gets the highest rank, while the ending word in the target sentence gets the lowest rank. The ranking model is trained to directly minimize the mis-ordering of tree nodes, which differs from the prior work based on maximum likelihood estimations of reordering patterns (Li et al., 2007; Genzel, 2010), and does not require any special tweaking in model training. The ranking model can not only be used in a pre-reordering based SMT system, but also be integrated into a phrasebased decoder serving as additional distortion features. We evaluated our approach on large-scale Japanese-English and English-Japanese machine translation tasks, and experimental results show that our approach can bring significant improvements to the baseline phrase-based SMT system in both preordering and integrated decoding settings. In the rest of the paper, we will first formally present our ranking-"
P12-1096,P06-1077,0,0.33379,"ring between language pairs with substantial word order difference, such as Japanese with Subject-Object-Verb (SOV) structure and English with Subject-Verb-Object (SVO) structure, is generally viewed beyond the scope of the phrase-based systems discussed above, because of either distortion limits or lack of discriminative features for modeling. The most notable solution to this problem is adopting syntax-based SMT models, especially methods making use of source side syntactic parse trees. There are two major categories in this line of research. One is tree-to-string model (Quirk et al., 2005; Liu et al., 2006) which directly uses source parse trees to derive a large set of translation rules and associated model parameters. The other is called syntax pre-reordering – an approach that re-positions source words to approximate target language word order as much as possible based on the features from source syntactic parse trees. This is usually done in a preprocessing step, and then followed by a standard phrase-based SMT system that takes the re-ordered source sentence as input to finish the translation. In this paper, we continue this line of work and address the problem of word reordering based on s"
P12-1096,de-marneffe-etal-2006-generating,0,0.00844633,"Missing"
P12-1096,J03-1002,0,0.00483443,"Missing"
P12-1096,P05-1034,0,0.348162,"distance word reordering between language pairs with substantial word order difference, such as Japanese with Subject-Object-Verb (SOV) structure and English with Subject-Verb-Object (SVO) structure, is generally viewed beyond the scope of the phrase-based systems discussed above, because of either distortion limits or lack of discriminative features for modeling. The most notable solution to this problem is adopting syntax-based SMT models, especially methods making use of source side syntactic parse trees. There are two major categories in this line of research. One is tree-to-string model (Quirk et al., 2005; Liu et al., 2006) which directly uses source parse trees to derive a large set of translation rules and associated model parameters. The other is called syntax pre-reordering – an approach that re-positions source words to approximate target language word order as much as possible based on the features from source syntactic parse trees. This is usually done in a preprocessing step, and then followed by a standard phrase-based SMT system that takes the re-ordered source sentence as input to finish the translation. In this paper, we continue this line of work and address the problem of word re"
P12-1096,I08-1067,0,0.0231109,"errors on informal texts, and informal texts usually have more flexible translations. Pre-reorder method makes “hard” decision before decoding, thus is more sensitive to parser errors; on the other hand, integrated model is forced to use a longer distortion limit which leads to more search errors during decoding time. It is possible to use system combination method to get the best of both systems, but we leave this to future work. 6 Discussion on Related Work There have been several studies focusing on compiling hand-crafted syntactic reorder rules. Collins et al. (2005), Wang et al. (2007), Ramanathan et al. (2008), Lee et al. (2010) have developed rules for German-English, Chinese-English, English-Hindi and English-Japanese respectively. Xu et al. (2009) designed a clever precedence reordering rule set for translation from English to several SOV languages. The drawback for hand-crafted rules is that they depend upon expert knowledge to produce and are limited to their targeted language pairs. Automatically learning syntactic reordering rules have also been explored in several work. Li et al. (2007) and Visweswariah et al. (2010) learned probability of reordering patterns from constituent trees using ei"
P12-1096,C10-1126,0,0.340886,"air. By permuting tree nodes in the parse tree, the source sentence is reordered into the target language order. Constituent tree is shown above the source sentence; arrows below the source sentences show head-dependent arcs for dependency tree; word alignment links are lines without arrow between the source and target sentences. parse tree, we can obtain the same word order of Japanese translation. It is true that tree-based reordering cannot cover all word movement operations in language translation, previous work showed that this method is still very effective in practice (Xu et al., 2009, Visweswariah et al., 2010). Following this principle, the word reordering task can be broken into sub-tasks, in which we only need to determine the order of children nodes for all non-leaf nodes in the source parse tree. For a tree node t with children {c1 , c2 , . . . , cn }, we rearrange the children to target-language-like order {cπ(i1 ) , cπ(i2 ) , . . . , cπ(in ) }. If we treat the reordered position π(i) of child ci as its “rank”, the reordering problem is naturally translated into a ranking problem: to reorder, we determine a “rank” for each child, then the children are sorted according to their “ranks”. As it i"
P12-1096,D07-1077,0,0.102202,"parser commits more errors on informal texts, and informal texts usually have more flexible translations. Pre-reorder method makes “hard” decision before decoding, thus is more sensitive to parser errors; on the other hand, integrated model is forced to use a longer distortion limit which leads to more search errors during decoding time. It is possible to use system combination method to get the best of both systems, but we leave this to future work. 6 Discussion on Related Work There have been several studies focusing on compiling hand-crafted syntactic reorder rules. Collins et al. (2005), Wang et al. (2007), Ramanathan et al. (2008), Lee et al. (2010) have developed rules for German-English, Chinese-English, English-Hindi and English-Japanese respectively. Xu et al. (2009) designed a clever precedence reordering rule set for translation from English to several SOV languages. The drawback for hand-crafted rules is that they depend upon expert knowledge to produce and are limited to their targeted language pairs. Automatically learning syntactic reordering rules have also been explored in several work. Li et al. (2007) and Visweswariah et al. (2010) learned probability of reordering patterns from"
P12-1096,J97-3002,0,0.140444,"clex · dst ctf · hf · dst ctf · hlex cl · dst cl · lcl cl · rcl · dst cl · clex · dst cl · hlex cl · clex · pct cl · hlex · pct ctf ctf ctf ctf ctf ctf · dst · lct · dst · clex · hf · hf · dst · hlex · dst cl · pct cl · rcl cl · clex cl · clex · dst cl · hlex · dst cl · clex · pct ctf · lct cl · rct · dst ctf · clex · dst ctf · hf ctf · hlex ctf · hlex · dst erence for the decoder. A simple penalty scheme is utilized to penalize decoder reordering violating ranking reorder model’s prediction e0 . In this paper, our underlying decoder is a CKY decoder following Bracketing Transduction Grammar (Wu, 1997; Xiong et al., 2006), thus we show how the penalty is implemented in the BTG decoder as an example. Similar penalty can be designed for other decoders without much effort. Under BTG, three rules are used to derive translations: one unary terminal rule, one straight rule and one inverse rule: A → e/f A → [A1 , A2 ] A → hA1 , A2 i Table 1: Feature templates for ranking function. All templates are implicitly conjuncted with the pos tag of head node. c: child to be ranked; h: head node lc: left sibling of c; rc: right sibling of c l: dependency label; t: pos tag lex: top frequency lexicons f : Ja"
P12-1096,P06-1066,0,0.016997,"ctf · hf · dst ctf · hlex cl · dst cl · lcl cl · rcl · dst cl · clex · dst cl · hlex cl · clex · pct cl · hlex · pct ctf ctf ctf ctf ctf ctf · dst · lct · dst · clex · hf · hf · dst · hlex · dst cl · pct cl · rcl cl · clex cl · clex · dst cl · hlex · dst cl · clex · pct ctf · lct cl · rct · dst ctf · clex · dst ctf · hf ctf · hlex ctf · hlex · dst erence for the decoder. A simple penalty scheme is utilized to penalize decoder reordering violating ranking reorder model’s prediction e0 . In this paper, our underlying decoder is a CKY decoder following Bracketing Transduction Grammar (Wu, 1997; Xiong et al., 2006), thus we show how the penalty is implemented in the BTG decoder as an example. Similar penalty can be designed for other decoders without much effort. Under BTG, three rules are used to derive translations: one unary terminal rule, one straight rule and one inverse rule: A → e/f A → [A1 , A2 ] A → hA1 , A2 i Table 1: Feature templates for ranking function. All templates are implicitly conjuncted with the pos tag of head node. c: child to be ranked; h: head node lc: left sibling of c; rc: right sibling of c l: dependency label; t: pos tag lex: top frequency lexicons f : Japanese function word"
P12-1096,N09-1028,0,0.863406,"panese sentence pair. By permuting tree nodes in the parse tree, the source sentence is reordered into the target language order. Constituent tree is shown above the source sentence; arrows below the source sentences show head-dependent arcs for dependency tree; word alignment links are lines without arrow between the source and target sentences. parse tree, we can obtain the same word order of Japanese translation. It is true that tree-based reordering cannot cover all word movement operations in language translation, previous work showed that this method is still very effective in practice (Xu et al., 2009, Visweswariah et al., 2010). Following this principle, the word reordering task can be broken into sub-tasks, in which we only need to determine the order of children nodes for all non-leaf nodes in the source parse tree. For a tree node t with children {c1 , c2 , . . . , cn }, we rearrange the children to target-language-like order {cπ(i1 ) , cπ(i2 ) , . . . , cπ(in ) }. If we treat the reordered position π(i) of child ci as its “rank”, the reordering problem is naturally translated into a ranking problem: to reorder, we determine a “rank” for each child, then the children are sorted accordi"
P12-1096,W06-3108,0,0.0291277,"Missing"
P12-1096,C04-1010,0,\N,Missing
P12-1096,2005.iwslt-1.8,0,\N,Missing
P12-1100,P08-1009,0,0.283488,"longdistance reordering. Furthermore, this model is formalised as Tree Substitution Grammars, so it observes syntactic cohesion. Syntactic cohesion means that the translation of a string covered by a subtree in a source parse tends to be continuous. Fox (2002) shows that translation between English and French satisfies cohesion in the majority cases. Many previous works show promising results with an assumption that syntactic cohesion explains almost all translation movement for some language pairs (Wu, 1997; Yamada and Knight, 2001; Eisner, 2003; Graehl and Knight, 2004; Quirk et al., 2005; Cherry, 2008; Feng et al., 2010). 950 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 950–958, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics But unfortunately, the tree-to-string model requires each node must be strictly matched during rule matching, which makes it strongly dependent on the relationship of tree nodes and their roles in the whole sentence. This will lead to data sparseness and being vulnerable to parse errors. In this paper, we present a hierarchical chunk-tostring translation model to combine the me"
P12-1100,P05-1033,0,0.38994,"A derivation yields a pair of strings on the right-hand side which are translation of each other. In a weighted SCFG, each rule has a weight and the total weight of a derivation is the production of the weights of the rules used by the derivation. A translation may be produced by many different derivations and we only use the best derivation to evaluate its probability. With d denoting a derivation and r denoting a rule, we have p(e|f ) = max p(d, e|f , cˆ) d Y = max p(r, e|f , cˆ) d X (6) k We employ the same set of features for the loglinear model as the hierarchical phrase-based model does(Chiang, 2005). We further refine our hierarchical chunk-to-string model into two models: a loose model which is more similar to the hierarchical phrase-based model and a tight model which is more similar to the tree-tostring model. The two models differ in the form of rules and the way of estimating rule probabilities. While for decoding, we employ the same decoding algorithm for the two models: given a test sentence, the decoders first perform shallow parsing to get the best chunk sequence, then apply a CYK parsing algorithm with beam search. 2.1 A Loose Model In our model, we employ rules containing nont"
P12-1100,J07-2003,0,0.433856,"grammar defined by these rules, our model searches for the best translation derivation and yields target translation simultaneously. Our experiments show that our model significantly outperforms the hierarchical phrasebased model and the tree-to-string model on English-Chinese Translation tasks. X → hX 1 for X 2 , X 2 de X 1 i can be applied to both of the following strings in Figure 1 “A request for a purchase of shares” “filed for bankruptcy”, and get the following translation, respectively “goumai gufen de shenqing” “pochan de shenqing”. 1 Introduction The hierarchical phrase-based model (Chiang, 2007) makes an advance of statistical machine translation by employing hierarchical phrases, which not only uses phrases to learn local translations but also uses hierarchical phrases to capture reorderings of words and subphrases which can cover a large scope. Besides, this model is formal syntax-based and does not need to specify the syntactic constituents of subphrases, so it can directly learn synchronous context-free grammars (SCFG) from a parallel text without relying on any linguistic annotations or assumptions, which makes it used conveniently and widely. ∗ This work was done when the first"
P12-1100,P05-1066,0,0.0777595,"ed decoder, tree represents the tree-to-string decoder, tight represents our tight decoder and loose represents our loose decoder. The speed is reported by seconds per sentence. The speed for the tree-tostring decoder includes the parsing time (0.23s) and the speed for the tight and loose models includes the shallow parsing time, too. extraction as: the height up to 3 and the number of leaf nodes up to 5. We give the results in Table 2. From the results, we can see that both the loose and tight decoders outperform the baseline decoders and the improvement is significant using the sign-test of Collins et al. (2005) (p < 0.01). Specifically, the loose model has a better performance while the tight model has a faster speed. Compared with the hierarchical phrase-based model, the loose model only imposes syntactic cohesion cohesion to nonterminals while the tight model imposes syntax cohesion to both rules and nonterminals which reduces search space, so it decoders faster. We can conclude that linguistic syntax can indeed improve the translation performance; syntactic cohesion for nonterminals can explain linguistic phenomena well; noncohesive rules are useful, too. The extra time consumption against hierar"
P12-1100,P03-2041,0,0.0452022,"ntax via source parse to direct word reordering, especially longdistance reordering. Furthermore, this model is formalised as Tree Substitution Grammars, so it observes syntactic cohesion. Syntactic cohesion means that the translation of a string covered by a subtree in a source parse tends to be continuous. Fox (2002) shows that translation between English and French satisfies cohesion in the majority cases. Many previous works show promising results with an assumption that syntactic cohesion explains almost all translation movement for some language pairs (Wu, 1997; Yamada and Knight, 2001; Eisner, 2003; Graehl and Knight, 2004; Quirk et al., 2005; Cherry, 2008; Feng et al., 2010). 950 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 950–958, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics But unfortunately, the tree-to-string model requires each node must be strictly matched during rule matching, which makes it strongly dependent on the relationship of tree nodes and their roles in the whole sentence. This will lead to data sparseness and being vulnerable to parse errors. In this paper, we present a hier"
P12-1100,C10-2033,1,0.84756,"reordering. Furthermore, this model is formalised as Tree Substitution Grammars, so it observes syntactic cohesion. Syntactic cohesion means that the translation of a string covered by a subtree in a source parse tends to be continuous. Fox (2002) shows that translation between English and French satisfies cohesion in the majority cases. Many previous works show promising results with an assumption that syntactic cohesion explains almost all translation movement for some language pairs (Wu, 1997; Yamada and Knight, 2001; Eisner, 2003; Graehl and Knight, 2004; Quirk et al., 2005; Cherry, 2008; Feng et al., 2010). 950 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 950–958, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics But unfortunately, the tree-to-string model requires each node must be strictly matched during rule matching, which makes it strongly dependent on the relationship of tree nodes and their roles in the whole sentence. This will lead to data sparseness and being vulnerable to parse errors. In this paper, we present a hierarchical chunk-tostring translation model to combine the merits of the two mode"
P12-1100,W02-1039,0,0.1364,"a NP and this rule acts correctly while in the latter “filed” is a VP and this rule gives a wrong reordering. If we specify the first X on the right-hand side to NP, this kind of errors can be avoided. The tree-to-string model (Liu et al., 2006; Huang et al., 2006) introduces linguistic syntax via source parse to direct word reordering, especially longdistance reordering. Furthermore, this model is formalised as Tree Substitution Grammars, so it observes syntactic cohesion. Syntactic cohesion means that the translation of a string covered by a subtree in a source parse tends to be continuous. Fox (2002) shows that translation between English and French satisfies cohesion in the majority cases. Many previous works show promising results with an assumption that syntactic cohesion explains almost all translation movement for some language pairs (Wu, 1997; Yamada and Knight, 2001; Eisner, 2003; Graehl and Knight, 2004; Quirk et al., 2005; Cherry, 2008; Feng et al., 2010). 950 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 950–958, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics But unfortunately, the tree-t"
P12-1100,N04-1035,0,0.0742159,"tion We used the NIST training set for Chinese-English translation tasks excluding the Hong Kong Law and Hong Kong Hansard2 as the training data, which contains 470K sentence pairs. For the training data set, we first performed word alignment in both directions using GIZA++ toolkit (Och and Ney, 2000) then refined the alignments using “final-and”. We trained a 5-gram language model with modified Kneser-Ney smoothing on the Xinhua portion of LDC Chinese Gigaword corpus. For the tree-to-string model, we parsed English sentences using Stanford parser and extracted rules using the GHKM algorithm (Galley et al., 2004). We used our in-house English-Chinese data set as the development set and used the 2008 NIST English-Chinese MT test set (1859 sentences) as the test set. Our evaluation metric was BLEU-4 (Papineni et al., 2002) based on characters (as the target language is Chinese), which performed caseinsensitive matching of n-grams up to n = 4 and used the shortest reference for the brevity penalty. We used the standard minimum error-rate training (Och, 2003) to tune the feature weights to maximize the BLEU score on the development set. 6.2 Shallow Parsing The standard evaluation metrics for shallow parsi"
P12-1100,N04-1014,0,0.119779,"e parse to direct word reordering, especially longdistance reordering. Furthermore, this model is formalised as Tree Substitution Grammars, so it observes syntactic cohesion. Syntactic cohesion means that the translation of a string covered by a subtree in a source parse tends to be continuous. Fox (2002) shows that translation between English and French satisfies cohesion in the majority cases. Many previous works show promising results with an assumption that syntactic cohesion explains almost all translation movement for some language pairs (Wu, 1997; Yamada and Knight, 2001; Eisner, 2003; Graehl and Knight, 2004; Quirk et al., 2005; Cherry, 2008; Feng et al., 2010). 950 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 950–958, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics But unfortunately, the tree-to-string model requires each node must be strictly matched during rule matching, which makes it strongly dependent on the relationship of tree nodes and their roles in the whole sentence. This will lead to data sparseness and being vulnerable to parse errors. In this paper, we present a hierarchical chunk-tostring t"
P12-1100,2006.amta-papers.8,0,0.0819158,"he syntactic constituents of subphrases, so it can directly learn synchronous context-free grammars (SCFG) from a parallel text without relying on any linguistic annotations or assumptions, which makes it used conveniently and widely. ∗ This work was done when the first author visited Microsoft Research Asia as an intern. In the former, “A request” is a NP and this rule acts correctly while in the latter “filed” is a VP and this rule gives a wrong reordering. If we specify the first X on the right-hand side to NP, this kind of errors can be avoided. The tree-to-string model (Liu et al., 2006; Huang et al., 2006) introduces linguistic syntax via source parse to direct word reordering, especially longdistance reordering. Furthermore, this model is formalised as Tree Substitution Grammars, so it observes syntactic cohesion. Syntactic cohesion means that the translation of a string covered by a subtree in a source parse tends to be continuous. Fox (2002) shows that translation between English and French satisfies cohesion in the majority cases. Many previous works show promising results with an assumption that syntactic cohesion explains almost all translation movement for some language pairs (Wu, 1997;"
P12-1100,N03-1017,0,0.0974779,"oduce CRF-based chunkier in this paper and more details can be got from Hammersley and Clifford (1971), Lafferty et al. (2001), Taskar et al. (2002), Sha and Pereira (2003). 4 Rule Extraction In what follows, we introduce how to get the rule set. We learn rules from a corpus that first is bidirectionally word-aligned by the GIZA++ toolkit (Och and Ney, 2000) and then is refined using a “final-and” strategy. We generate the rule set in two steps: first, we extract two sets of phrases, basic phrases and chunk-based phrases. Basic phrases are defined using the same heuristic as previous systems (Koehn et al., 2003; Och and Ney, 2004; Chiang, 2005). A chunk-based phrase is such a basic phrase that covers one or more chunks on the source side. 1 We use the open source toolkit CRF++ got in http://code.google.com/p/crfpp/ . 954 Given a sentence pair hf , e, ∼i, we extract rules for the loose model as follows 1. If hfjj12 , eii21 i is a basic phrase, then we can have a rule X → hfjj12 , eii21 i 2. Assume X → hα, βi is a rule with α = α1 fjj12 α2 and β = β1 eii21 β2 , and hfjj12 , eii21 i is a chunk-based phrase with a chunk sequence Yu · · · Yv , then we have the following rule X → hα1 Yu -Yv k α2 , β1 Yu -"
P12-1100,C10-1080,1,0.843744,"al phrase-based model. Cherry (2008) used dependency tree to add syntactic cohesion. These methods work with the original SCFG defined by hierarchical phrase-based model and use linguistic knowledge to assist translation. Instead, our model works under the new defined SCFG with chunks. Besides, some other researchers make efforts on the tree-to-string model by employing exponentially alternative parses to alleviate the drawback of 1-best parse. Mi et al. (2008) presented forest-based translation where the decoder translates a packed forest of exponentially many parses instead of i-best parse. Liu and Liu (2010) proposed to parse and to translate jointly by taking tree-based translation as parsing. Given a source sentence, this decoder produces a parse tree on the source side and a translation on the target side simultaneously. Both the models perform in the unit of tree nodes rather than chunks. 955 6 Experiments 6.1 Data Preparation Data for shallow parsing We got training data and test data for shallow parsing from the standard Penn Tree Bank (PTB) English parsing task by splitting the sections 02-21 on the Wall Street Journal Portion (Marcus et al., 1993) into two sets: the last 1000 sentences as"
P12-1100,P06-1077,1,0.904774,"need to specify the syntactic constituents of subphrases, so it can directly learn synchronous context-free grammars (SCFG) from a parallel text without relying on any linguistic annotations or assumptions, which makes it used conveniently and widely. ∗ This work was done when the first author visited Microsoft Research Asia as an intern. In the former, “A request” is a NP and this rule acts correctly while in the latter “filed” is a VP and this rule gives a wrong reordering. If we specify the first X on the right-hand side to NP, this kind of errors can be avoided. The tree-to-string model (Liu et al., 2006; Huang et al., 2006) introduces linguistic syntax via source parse to direct word reordering, especially longdistance reordering. Furthermore, this model is formalised as Tree Substitution Grammars, so it observes syntactic cohesion. Syntactic cohesion means that the translation of a string covered by a subtree in a source parse tends to be continuous. Fox (2002) shows that translation between English and French satisfies cohesion in the majority cases. Many previous works show promising results with an assumption that syntactic cohesion explains almost all translation movement for some langu"
P12-1100,J93-2004,0,0.0396526,"tially many parses instead of i-best parse. Liu and Liu (2010) proposed to parse and to translate jointly by taking tree-based translation as parsing. Given a source sentence, this decoder produces a parse tree on the source side and a translation on the target side simultaneously. Both the models perform in the unit of tree nodes rather than chunks. 955 6 Experiments 6.1 Data Preparation Data for shallow parsing We got training data and test data for shallow parsing from the standard Penn Tree Bank (PTB) English parsing task by splitting the sections 02-21 on the Wall Street Journal Portion (Marcus et al., 1993) into two sets: the last 1000 sentences as the test set and the rest as the training set. We filtered the features whose frequency was lower than 3 and substituted ‘‘ and ’’ with ˝ to keep consistent with translation data. We used L2 algorithm to train CRF. Data for Translation We used the NIST training set for Chinese-English translation tasks excluding the Hong Kong Law and Hong Kong Hansard2 as the training data, which contains 470K sentence pairs. For the training data set, we first performed word alignment in both directions using GIZA++ toolkit (Och and Ney, 2000) then refined the alignm"
P12-1100,P08-1114,0,0.0508566,"model. Their model reorders chunks resorting to a distortion model while our model reorders chunks according to SCFG rules which retain the relative positions of chunks. Nguyen et al. (2008) presented a tree-to-string phrase-based method which is based on SCFGs. This method generates SCFGs through syntactic transformation including a word-to-phrase tree transformation model and a phrase reordering model while our model learns SCFG-based rules from word-aligned bilingual corpus directly There are also some works aiming to introduce linguistic knowledge into the hierarchical phrasebased model. Marton and Resnik (2008) took the source parse tree into account and added soft constraints to hierarchical phrase-based model. Cherry (2008) used dependency tree to add syntactic cohesion. These methods work with the original SCFG defined by hierarchical phrase-based model and use linguistic knowledge to assist translation. Instead, our model works under the new defined SCFG with chunks. Besides, some other researchers make efforts on the tree-to-string model by employing exponentially alternative parses to alleviate the drawback of 1-best parse. Mi et al. (2008) presented forest-based translation where the decoder"
P12-1100,P08-1023,1,0.860438,"edge into the hierarchical phrasebased model. Marton and Resnik (2008) took the source parse tree into account and added soft constraints to hierarchical phrase-based model. Cherry (2008) used dependency tree to add syntactic cohesion. These methods work with the original SCFG defined by hierarchical phrase-based model and use linguistic knowledge to assist translation. Instead, our model works under the new defined SCFG with chunks. Besides, some other researchers make efforts on the tree-to-string model by employing exponentially alternative parses to alleviate the drawback of 1-best parse. Mi et al. (2008) presented forest-based translation where the decoder translates a packed forest of exponentially many parses instead of i-best parse. Liu and Liu (2010) proposed to parse and to translate jointly by taking tree-based translation as parsing. Given a source sentence, this decoder produces a parse tree on the source side and a translation on the target side simultaneously. Both the models perform in the unit of tree nodes rather than chunks. 955 6 Experiments 6.1 Data Preparation Data for shallow parsing We got training data and test data for shallow parsing from the standard Penn Tree Bank (PTB"
P12-1100,P00-1056,0,0.604931,"sk and employ Conditional Random Field (CRF)1 to train a chunker. CRF is a good choice for label tasks as it can avoid label bias and use more statistical correlated features. We employ the features described in Sha and Pereira (2003) for CRF. We do not introduce CRF-based chunkier in this paper and more details can be got from Hammersley and Clifford (1971), Lafferty et al. (2001), Taskar et al. (2002), Sha and Pereira (2003). 4 Rule Extraction In what follows, we introduce how to get the rule set. We learn rules from a corpus that first is bidirectionally word-aligned by the GIZA++ toolkit (Och and Ney, 2000) and then is refined using a “final-and” strategy. We generate the rule set in two steps: first, we extract two sets of phrases, basic phrases and chunk-based phrases. Basic phrases are defined using the same heuristic as previous systems (Koehn et al., 2003; Och and Ney, 2004; Chiang, 2005). A chunk-based phrase is such a basic phrase that covers one or more chunks on the source side. 1 We use the open source toolkit CRF++ got in http://code.google.com/p/crfpp/ . 954 Given a sentence pair hf , e, ∼i, we extract rules for the loose model as follows 1. If hfjj12 , eii21 i is a basic phrase, the"
P12-1100,P02-1038,0,0.20294,"ploy rules containing nonterminals to handle long-distance reordering where boundary words play an important role. So for the subphrases which cover more than one chunk, we just maintain boundary chunks: we bundle adjacent chunks into one nonterminal and denote it as the first chunk tag immediately followed by “-” and next followed by the last chunk tag. Then, for the string pair <filed for bankruptcy, shenqing pochan>, we can get the rule r1 : X → hVBN 1 for NP 2 , VBN 1 NP 2 i while for the string pair <A request for a purchase of shares, goumai gufen de shenqing>, we can get r2 : Following Och and Ney (2002), we frame our model as a log-linear model: P exp k λk Hk (d, e, cˆ, f ) P (5) p(e|f ) = exp d′ ,e′ ,k λk Hk (d′ , e′ , cˆ, f ) Hk (d, e, ˆ c, f ) = λk Hk (d, e, cˆ, f ) X → hNP 1 for NP-NP 2 , NP-NP 2 de NP 1 i. (4) r∈d where X hk (f , cˆ, r) r 952 The rule matching “A request for a purchase of shares was” will be r3 : X → hNP-NP 1 VBD 2 , NP-NP 1 VBD 2 i. We can see that in contrast to the method of representing each chunk separately, this representation form can alleviate data sparseness and the influence of parsing errors. hS 1 , S 1 i ⇒ hS 2 X 3 , S 2 X 3 i ⇒ hX 4 X 3 , X 4 X 3 i ⇒ hNP-NP"
P12-1100,J04-4002,0,0.217737,"kier in this paper and more details can be got from Hammersley and Clifford (1971), Lafferty et al. (2001), Taskar et al. (2002), Sha and Pereira (2003). 4 Rule Extraction In what follows, we introduce how to get the rule set. We learn rules from a corpus that first is bidirectionally word-aligned by the GIZA++ toolkit (Och and Ney, 2000) and then is refined using a “final-and” strategy. We generate the rule set in two steps: first, we extract two sets of phrases, basic phrases and chunk-based phrases. Basic phrases are defined using the same heuristic as previous systems (Koehn et al., 2003; Och and Ney, 2004; Chiang, 2005). A chunk-based phrase is such a basic phrase that covers one or more chunks on the source side. 1 We use the open source toolkit CRF++ got in http://code.google.com/p/crfpp/ . 954 Given a sentence pair hf , e, ∼i, we extract rules for the loose model as follows 1. If hfjj12 , eii21 i is a basic phrase, then we can have a rule X → hfjj12 , eii21 i 2. Assume X → hα, βi is a rule with α = α1 fjj12 α2 and β = β1 eii21 β2 , and hfjj12 , eii21 i is a chunk-based phrase with a chunk sequence Yu · · · Yv , then we have the following rule X → hα1 Yu -Yv k α2 , β1 Yu -Yv k β2 i. We evalu"
P12-1100,P03-1021,0,0.171403,"Missing"
P12-1100,P02-1040,0,0.0852269,"e first performed word alignment in both directions using GIZA++ toolkit (Och and Ney, 2000) then refined the alignments using “final-and”. We trained a 5-gram language model with modified Kneser-Ney smoothing on the Xinhua portion of LDC Chinese Gigaword corpus. For the tree-to-string model, we parsed English sentences using Stanford parser and extracted rules using the GHKM algorithm (Galley et al., 2004). We used our in-house English-Chinese data set as the development set and used the 2008 NIST English-Chinese MT test set (1859 sentences) as the test set. Our evaluation metric was BLEU-4 (Papineni et al., 2002) based on characters (as the target language is Chinese), which performed caseinsensitive matching of n-grams up to n = 4 and used the shortest reference for the brevity penalty. We used the standard minimum error-rate training (Och, 2003) to tune the feature weights to maximize the BLEU score on the development set. 6.2 Shallow Parsing The standard evaluation metrics for shallow parsing are precision P, recall R, and their harmonic mean F1 score, given by: number of exactly recognized chunks number of output chunks number of exactly recognized chunks R= number of reference chunks P = 2 The so"
P12-1100,P05-1034,0,0.106643,"ordering, especially longdistance reordering. Furthermore, this model is formalised as Tree Substitution Grammars, so it observes syntactic cohesion. Syntactic cohesion means that the translation of a string covered by a subtree in a source parse tends to be continuous. Fox (2002) shows that translation between English and French satisfies cohesion in the majority cases. Many previous works show promising results with an assumption that syntactic cohesion explains almost all translation movement for some language pairs (Wu, 1997; Yamada and Knight, 2001; Eisner, 2003; Graehl and Knight, 2004; Quirk et al., 2005; Cherry, 2008; Feng et al., 2010). 950 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 950–958, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics But unfortunately, the tree-to-string model requires each node must be strictly matched during rule matching, which makes it strongly dependent on the relationship of tree nodes and their roles in the whole sentence. This will lead to data sparseness and being vulnerable to parse errors. In this paper, we present a hierarchical chunk-tostring translation model to"
P12-1100,N03-1028,0,0.0397363,"gnized as a chunk, we skip its children. In this way, we can get a sole chunk sequence given a parse tree. Then we label each word with a label indicating whether the word starts a chunk (B-) or continues a chunk (I-). Figure 2(a) gives an example. In this method, we get the training data for shallow parsing from Penn Tree Bank. We take shallow Parsing (chunking) as a sequence label task and employ Conditional Random Field (CRF)1 to train a chunker. CRF is a good choice for label tasks as it can avoid label bias and use more statistical correlated features. We employ the features described in Sha and Pereira (2003) for CRF. We do not introduce CRF-based chunkier in this paper and more details can be got from Hammersley and Clifford (1971), Lafferty et al. (2001), Taskar et al. (2002), Sha and Pereira (2003). 4 Rule Extraction In what follows, we introduce how to get the rule set. We learn rules from a corpus that first is bidirectionally word-aligned by the GIZA++ toolkit (Och and Ney, 2000) and then is refined using a “final-and” strategy. We generate the rule set in two steps: first, we extract two sets of phrases, basic phrases and chunk-based phrases. Basic phrases are defined using the same heurist"
P12-1100,P03-1039,0,0.0253069,"e way as Liu et al. (2006). For the loose model, the nonterminals must be cohesive, while the whole rule can be noncohesive: if both ends of a rule are nonterminals, the whole rule is cohesive, otherwise, it may be noncohesive. In contrast, for the tight model, both the whole rule and the nonterminal are cohesive. Even with the cohesion constraints, our model still generates a large number of rules, but not all of the rules are useful for translation. So we follow the method described in Chiang (2007) to filter the rule set except that we allow two nonterminals to be adjacent. 5 Related Works Watanabe et al. (2003) presented a chunk-to-string translation model where the decoder generates a translation by first translating the words in each chunk, then reordering the translation of chunks. Our model distinguishes from their model mainly in reordering model. Their model reorders chunks resorting to a distortion model while our model reorders chunks according to SCFG rules which retain the relative positions of chunks. Nguyen et al. (2008) presented a tree-to-string phrase-based method which is based on SCFGs. This method generates SCFGs through syntactic transformation including a word-to-phrase tree tran"
P12-1100,J97-3002,0,0.132427,"al., 2006) introduces linguistic syntax via source parse to direct word reordering, especially longdistance reordering. Furthermore, this model is formalised as Tree Substitution Grammars, so it observes syntactic cohesion. Syntactic cohesion means that the translation of a string covered by a subtree in a source parse tends to be continuous. Fox (2002) shows that translation between English and French satisfies cohesion in the majority cases. Many previous works show promising results with an assumption that syntactic cohesion explains almost all translation movement for some language pairs (Wu, 1997; Yamada and Knight, 2001; Eisner, 2003; Graehl and Knight, 2004; Quirk et al., 2005; Cherry, 2008; Feng et al., 2010). 950 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 950–958, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics But unfortunately, the tree-to-string model requires each node must be strictly matched during rule matching, which makes it strongly dependent on the relationship of tree nodes and their roles in the whole sentence. This will lead to data sparseness and being vulnerable to parse e"
P12-1100,P01-1067,0,0.0718737,"introduces linguistic syntax via source parse to direct word reordering, especially longdistance reordering. Furthermore, this model is formalised as Tree Substitution Grammars, so it observes syntactic cohesion. Syntactic cohesion means that the translation of a string covered by a subtree in a source parse tends to be continuous. Fox (2002) shows that translation between English and French satisfies cohesion in the majority cases. Many previous works show promising results with an assumption that syntactic cohesion explains almost all translation movement for some language pairs (Wu, 1997; Yamada and Knight, 2001; Eisner, 2003; Graehl and Knight, 2004; Quirk et al., 2005; Cherry, 2008; Feng et al., 2010). 950 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 950–958, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics But unfortunately, the tree-to-string model requires each node must be strictly matched during rule matching, which makes it strongly dependent on the relationship of tree nodes and their roles in the whole sentence. This will lead to data sparseness and being vulnerable to parse errors. In this paper, we"
P12-1100,W08-2119,0,\N,Missing
P12-1100,J08-3004,0,\N,Missing
P12-2057,P10-1088,0,0.0457426,"Missing"
P12-2057,J93-2003,0,0.0390093,"Missing"
P12-2057,P05-1033,0,0.0608879,"phrase pairs and hierarchical rules, and thus preserve the search space of SMT decoders as much as possible. Experimental results on Chinese-toEnglish machine translation tasks show that our method is able to reduce almost the half size of the translation model with very tiny degradation of translation performance. 1 Introduction Statistical Machine Translation (SMT) has gained considerable attention during last decades. From a bilingual corpus, all translation knowledge can be acquired automatically in SMT framework. Phrasebased model (Koehn et al., 2003) and hierarchical phrase-based model (Chiang, 2005; Chiang, 2007) show state-of-the-art performance in various language pairs. This achievement is mainly benefit from huge size of translational knowledge extracted from sufficient parallel corpus. However, the errors of automatic word alignment and non-parallelized bilingual sentence pairs sometimes have caused the unreliable and unnecessary translation rule acquisition. According to Bloodgood and Callison-Burch Previous methods of reducing the size of SMT model try to identify infrequent entries (Zollmann et al., 2008; Huang and Xiang, 2010). Several statistical significance testing methods a"
P12-2057,J07-2003,0,0.0340551,"nd hierarchical rules, and thus preserve the search space of SMT decoders as much as possible. Experimental results on Chinese-toEnglish machine translation tasks show that our method is able to reduce almost the half size of the translation model with very tiny degradation of translation performance. 1 Introduction Statistical Machine Translation (SMT) has gained considerable attention during last decades. From a bilingual corpus, all translation knowledge can be acquired automatically in SMT framework. Phrasebased model (Koehn et al., 2003) and hierarchical phrase-based model (Chiang, 2005; Chiang, 2007) show state-of-the-art performance in various language pairs. This achievement is mainly benefit from huge size of translational knowledge extracted from sufficient parallel corpus. However, the errors of automatic word alignment and non-parallelized bilingual sentence pairs sometimes have caused the unreliable and unnecessary translation rule acquisition. According to Bloodgood and Callison-Burch Previous methods of reducing the size of SMT model try to identify infrequent entries (Zollmann et al., 2008; Huang and Xiang, 2010). Several statistical significance testing methods are also examine"
P12-2057,C10-2025,1,0.846579,"duction is needed to be performed for both the phrase table and the hierarchical rule table simultaneously, namely joint reduction. Similar to phrase reduction and hierarchical rule reduction, it selects the best N entries of the mixture of phrase and hierarchical rules. This method results in safer pruning; once a phrase is determined to be pruned, the hierarchical rules, which are related to this phrase, are likely to be kept, and vice versa. 3 Experiment We investigate the effectiveness of our reduction method by conducting Chinese-to-English translation task. The training data, as same as Cui et al. (2010), consists of about 500K parallel sentence pairs which is a mixture of several datasets published by LDC. NIST 2003 set is used as a development set. NIST 2004, 2005, 2006, and 2008 sets are used for evaluation purpose. For word alignment, we use GIZA++1 , an implementation of IBM models (Brown et al., 1993). We have implemented a hierarchical phrase-based SMT model similar to Chiang (2005). The trigram target language model is trained from the Xinhua portion of English Gigaword corpus (Graff and Cieri, 2003). Sampled 10,000 sentences from Chinese Gigaword corpus (Graff, 2007) was used for sou"
P12-2057,C10-1056,0,0.258016,"del (Koehn et al., 2003) and hierarchical phrase-based model (Chiang, 2005; Chiang, 2007) show state-of-the-art performance in various language pairs. This achievement is mainly benefit from huge size of translational knowledge extracted from sufficient parallel corpus. However, the errors of automatic word alignment and non-parallelized bilingual sentence pairs sometimes have caused the unreliable and unnecessary translation rule acquisition. According to Bloodgood and Callison-Burch Previous methods of reducing the size of SMT model try to identify infrequent entries (Zollmann et al., 2008; Huang and Xiang, 2010). Several statistical significance testing methods are also examined to detect unreliable noisy entries (Tomeh et al., 2009; Johnson et al., 2007; Yang and Zheng, 2009). These methods could harm the translation performance due to their side effect of algorithms; similar multiple entries can be pruned at the same time deteriorating potential coverage of translation. The proposed method, on the other hand, tries to measure the redundancy of phrase pairs and hierarchical rules. In this work, redundancy of an entry is defined as its translational ineffectiveness, and estimated by comparing scores"
P12-2057,D07-1103,0,0.114556,"rs. This achievement is mainly benefit from huge size of translational knowledge extracted from sufficient parallel corpus. However, the errors of automatic word alignment and non-parallelized bilingual sentence pairs sometimes have caused the unreliable and unnecessary translation rule acquisition. According to Bloodgood and Callison-Burch Previous methods of reducing the size of SMT model try to identify infrequent entries (Zollmann et al., 2008; Huang and Xiang, 2010). Several statistical significance testing methods are also examined to detect unreliable noisy entries (Tomeh et al., 2009; Johnson et al., 2007; Yang and Zheng, 2009). These methods could harm the translation performance due to their side effect of algorithms; similar multiple entries can be pruned at the same time deteriorating potential coverage of translation. The proposed method, on the other hand, tries to measure the redundancy of phrase pairs and hierarchical rules. In this work, redundancy of an entry is defined as its translational ineffectiveness, and estimated by comparing scores of entries and scores of their substituents. Suppose that the source phrase s1 s2 is always translated into t1 t2 with phrase entry &lt;s1 s2 →t1 t2"
P12-2057,N03-1017,0,0.0239505,"the estimation of the information redundancy encoded in phrase pairs and hierarchical rules, and thus preserve the search space of SMT decoders as much as possible. Experimental results on Chinese-toEnglish machine translation tasks show that our method is able to reduce almost the half size of the translation model with very tiny degradation of translation performance. 1 Introduction Statistical Machine Translation (SMT) has gained considerable attention during last decades. From a bilingual corpus, all translation knowledge can be acquired automatically in SMT framework. Phrasebased model (Koehn et al., 2003) and hierarchical phrase-based model (Chiang, 2005; Chiang, 2007) show state-of-the-art performance in various language pairs. This achievement is mainly benefit from huge size of translational knowledge extracted from sufficient parallel corpus. However, the errors of automatic word alignment and non-parallelized bilingual sentence pairs sometimes have caused the unreliable and unnecessary translation rule acquisition. According to Bloodgood and Callison-Burch Previous methods of reducing the size of SMT model try to identify infrequent entries (Zollmann et al., 2008; Huang and Xiang, 2010)."
P12-2057,N03-2016,0,0.031157,"ranslation model, T M , our goal is to find the optimally reduced translation model, T M ∗ , which minimizes the degradation of translation performance. To measure the performance degradation, we introduce a new metric named consistency: C(T M, T M ∗ ) = BLEU (D(s; T M ), D(s; T M ∗ )) (1) where the function D produces the target sentence of the source sentence s, given the translation model T M . Consistency measures the similarity between the two groups of decoded target sentences produced by two different translation models. There are number of similarity metrics such as Dices coefficient (Kondrak et al., 2003), and Jaccard similarity coefficient. Instead, we use BLEU scores (Papineni et al., 2002) since it is one of the primary metrics for machine translation evaluation. Note that our consistency does not require the reference set while the original BLEU does. This means that only (abundant) source-side monolingual corpus is needed to predict performance degradation. Now, our goal can be rewritten with this metric; among all the possible reduced models, we want to find the set which can maximize the consistency: T M ∗ = argmax C(T M, T M ′ ) (2) T M ′ ⊂T M 292 where h is a feature function and λ is"
P12-2057,P03-1021,0,0.0219527,". The redundancy-based reduction can be performed to prune the phrase table, the hierarchical rule table, and both. Since the similar translation knowledge is accumulated at both of tables during the training stage, our reduction method performs effectively and safely. Unlike previous studies solely focus on either phrase table or hierarchical rule table, this work is the first attempt to reduce phrases and hierarchical rules simultaneously. In minimum error rate training (MERT) stages, a development set, which consists of bilingual sentences, is used to find out the best weights of features (Och, 2003). One characteristic of our method is that it isolates feature weights of the translation model from SMT log-linear model, trying to minimize the impact of search path during decoding. The reduction procedure consists of three stages: translation scoring, redundancy estimation, and redundancy-based reduction. Our reduction method starts with measuring the translation scores of the individual phrase and the hierarchical rule. Similar to the decoder, the scoring scheme is based on the log-linear framework: X P S(p) = λi hi (p) (3) i 2 Proposed Model Given an original translation model, T M , our"
P12-2057,P02-1040,0,0.0852558,"∗ , which minimizes the degradation of translation performance. To measure the performance degradation, we introduce a new metric named consistency: C(T M, T M ∗ ) = BLEU (D(s; T M ), D(s; T M ∗ )) (1) where the function D produces the target sentence of the source sentence s, given the translation model T M . Consistency measures the similarity between the two groups of decoded target sentences produced by two different translation models. There are number of similarity metrics such as Dices coefficient (Kondrak et al., 2003), and Jaccard similarity coefficient. Instead, we use BLEU scores (Papineni et al., 2002) since it is one of the primary metrics for machine translation evaluation. Note that our consistency does not require the reference set while the original BLEU does. This means that only (abundant) source-side monolingual corpus is needed to predict performance degradation. Now, our goal can be rewritten with this metric; among all the possible reduced models, we want to find the set which can maximize the consistency: T M ∗ = argmax C(T M, T M ′ ) (2) T M ′ ⊂T M 292 where h is a feature function and λ is its weight. As the conventional hierarchical phrase-based SMT model, our features are co"
P12-2057,2009.mtsummit-papers.17,0,0.505698,"various language pairs. This achievement is mainly benefit from huge size of translational knowledge extracted from sufficient parallel corpus. However, the errors of automatic word alignment and non-parallelized bilingual sentence pairs sometimes have caused the unreliable and unnecessary translation rule acquisition. According to Bloodgood and Callison-Burch Previous methods of reducing the size of SMT model try to identify infrequent entries (Zollmann et al., 2008; Huang and Xiang, 2010). Several statistical significance testing methods are also examined to detect unreliable noisy entries (Tomeh et al., 2009; Johnson et al., 2007; Yang and Zheng, 2009). These methods could harm the translation performance due to their side effect of algorithms; similar multiple entries can be pruned at the same time deteriorating potential coverage of translation. The proposed method, on the other hand, tries to measure the redundancy of phrase pairs and hierarchical rules. In this work, redundancy of an entry is defined as its translational ineffectiveness, and estimated by comparing scores of entries and scores of their substituents. Suppose that the source phrase s1 s2 is always translated into t1 t2 with phra"
P12-2057,P09-2060,0,0.449057,"s mainly benefit from huge size of translational knowledge extracted from sufficient parallel corpus. However, the errors of automatic word alignment and non-parallelized bilingual sentence pairs sometimes have caused the unreliable and unnecessary translation rule acquisition. According to Bloodgood and Callison-Burch Previous methods of reducing the size of SMT model try to identify infrequent entries (Zollmann et al., 2008; Huang and Xiang, 2010). Several statistical significance testing methods are also examined to detect unreliable noisy entries (Tomeh et al., 2009; Johnson et al., 2007; Yang and Zheng, 2009). These methods could harm the translation performance due to their side effect of algorithms; similar multiple entries can be pruned at the same time deteriorating potential coverage of translation. The proposed method, on the other hand, tries to measure the redundancy of phrase pairs and hierarchical rules. In this work, redundancy of an entry is defined as its translational ineffectiveness, and estimated by comparing scores of entries and scores of their substituents. Suppose that the source phrase s1 s2 is always translated into t1 t2 with phrase entry &lt;s1 s2 →t1 t2 &gt; where si and ti are"
P12-2057,C08-1144,0,0.100555,"amework. Phrasebased model (Koehn et al., 2003) and hierarchical phrase-based model (Chiang, 2005; Chiang, 2007) show state-of-the-art performance in various language pairs. This achievement is mainly benefit from huge size of translational knowledge extracted from sufficient parallel corpus. However, the errors of automatic word alignment and non-parallelized bilingual sentence pairs sometimes have caused the unreliable and unnecessary translation rule acquisition. According to Bloodgood and Callison-Burch Previous methods of reducing the size of SMT model try to identify infrequent entries (Zollmann et al., 2008; Huang and Xiang, 2010). Several statistical significance testing methods are also examined to detect unreliable noisy entries (Tomeh et al., 2009; Johnson et al., 2007; Yang and Zheng, 2009). These methods could harm the translation performance due to their side effect of algorithms; similar multiple entries can be pruned at the same time deteriorating potential coverage of translation. The proposed method, on the other hand, tries to measure the redundancy of phrase pairs and hierarchical rules. In this work, redundancy of an entry is defined as its translational ineffectiveness, and estima"
P13-1017,N03-1017,0,0.0812093,"ify the word embeddings in subsequent steps according to bilingual data. our model from raw sentence pairs, they are too computational demanding as the lexical translation probabilities must be computed from neural networks. Hence, we opt for a simpler supervised approach, which learns the model from sentence pairs with word alignment. As we do not have a large manually word aligned corpus, we use traditional word alignment models such as HMM and IBM model 4 to generate word alignment on a large parallel corpus. We obtain bidirectional alignment by running the usual growdiag-final heuristics (Koehn et al., 2003) on unidirectional results from both directions, and use the results as our training data. Similar approach has been taken in speech recognition task (Dahl et al., 2012), where training data for neural network model is generated by forced decoding with traditional Gaussian mixture models. Tunable parameters in neural network alignment model include: word embeddings in lookup table LT , parameters W l , bl for linear transformations in the hidden layers of the neural network, and distortion parameters sd of jump distance. We take the following ranking loss with margin as our training criteria:"
P13-1017,J96-1002,0,0.112467,"and target side respectively. Words are converted to embeddings using the lookup table LT , and the catenation of embeddings are fed to a classic neural network with two hidden-layers, and the output of the network is the our lexical translation score: 1 The ability to model context is not unique to our model. In fact, discriminative word alignment can model contexts by deploying arbitrary features (Moore, 2005). Different from previous discriminative word alignment, our model does not use manually engineered features, but learn “features” automatically from raw words by the neural network. (Berger et al., 1996) use a maximum entropy model to model the bag-of-words context for word alignment, but their model treats each word as a distinct feature, which can not leverage the similarity between words as our model. tlex (ei , fj |e, f) = f3 ◦ f2 ◦ f1 ◦ LT (window(ei ), window(fj )) (6) f1 and f2 layers use htanh as activation functions, while f3 is only a linear transformation with no activation function. For the distortion td , we could use a lexicalized distortion model: td (ai , ai−1 |e, f) = td (ai − ai−1 |window(fai−1 )) (7) which can be computed by a neural network similar to the one used to compu"
P13-1017,J93-2003,0,0.0874815,"where they got similar or even better results than the state-of-the-art on these tasks. (Niehues and Waibel, 2012) shows that machine translation results can be improved by combining neural language model with n-gram traditional language. (Son et al., 2012) improves translation quality of n-gram translation model by using a bilingual neural language model. (Titov et al., 2012) learns a context-free cross-lingual word embeddings to facilitate cross-lingual information retrieval. For the related works of word alignment, the most popular methods are based on generative models such as IBM Models (Brown et al., 1993) and HMM (Vogel et al., 1996). Discriminative approaches are also proposed to use hand crafted features to improve word alignment. Among them, (Liu et al., 2010) proposed to use phrase and rule pairs to model the context information in a loglinear framework. Unlike previous discriminative methods, in this work, we do not resort to any hand crafted features, but use DNN to induce “features” from raw words. (b) A farmer Yibula Related Work Figure 1: Two examples of word alignment the English word “mammoth” is not, so it is very hard to align them correctly. If we know that “mammoth” has the simi"
P13-1017,P10-1033,1,0.843062,"proved by combining neural language model with n-gram traditional language. (Son et al., 2012) improves translation quality of n-gram translation model by using a bilingual neural language model. (Titov et al., 2012) learns a context-free cross-lingual word embeddings to facilitate cross-lingual information retrieval. For the related works of word alignment, the most popular methods are based on generative models such as IBM Models (Brown et al., 1993) and HMM (Vogel et al., 1996). Discriminative approaches are also proposed to use hand crafted features to improve word alignment. Among them, (Liu et al., 2010) proposed to use phrase and rule pairs to model the context information in a loglinear framework. Unlike previous discriminative methods, in this work, we do not resort to any hand crafted features, but use DNN to induce “features” from raw words. (b) A farmer Yibula Related Work Figure 1: Two examples of word alignment the English word “mammoth” is not, so it is very hard to align them correctly. If we know that “mammoth” has the similar meaning with “big”, or “huge”, it would be easier to find the corresponding word in the Chinese sentence. As we mentioned in the last paragraph, word embeddi"
P13-1017,P11-1043,0,0.018762,"test set. We adapt the segmentation on the Chinese side to fit our word segmentation standard. 171 archical phrase model (Chiang, 2007) from different word alignments. Despite different alignment scores, we do not obtain significant difference in translation performance. In our C-E experiment, we tuned on NIST-03, and tested on NIST08. Case-insensitive BLEU-4 scores on NIST-08 test are 0.305 and 0.307 for models trained from IBM-4 and NN alignment results. The result is not surprising considering our parallel corpus is quite large, and similar observations have been made in previous work as (DeNero and Macherey, 2011) that better alignment quality does not necessarily lead to better end-to-end result. 6.4 0.86 0.84 0.82 0.8 0.78 0.76 0.74 1 3 5 7 9 11 13 Figure 3: Effect of different window sizes on word alignment F-score. fitting problem. This is not surprising considering that larger window size only requires slightly more parameters in the linear layers. Lastly, it is worth noticing that our model with no context (window size 1) performs much worse than settings with larger window size and baseline IBM4. Our explanation is as follows. Our model uses the simple jump distance based distortion, which is we"
P13-1017,P11-1042,0,0.0684704,") (7) which can be computed by a neural network similar to the one used to compute lexical translation scores. If we map jump distance (ai − ai−1 ) to B buckets, we can change the length of the output layer to B, where each dimension in the output stands for a different bucket of jump distances. But we found in our initial experiments on small scale data, lexicalized distortion does not produce better alignment over the simple jumpdistance based model. So we drop the lexicalized 5 Training Although unsupervised training technique such as Contrastive Estimation as in (Smith and Eisner, 2005), (Dyer et al., 2011) can be adapted to train 1 In practice, the number of non-zero parameters in classic HMM model would be much smaller, as many words do not co-occur in bilingual sentence pairs. In our experiments, the number of non-zero parameters in classic HMM model is about 328 millions, while the NN model only has about 4 millions. 169 et al., 2011) and train word embeddings for source and target languages from their monolingual corpus respectively. Our vocabularies Vs and Vt contain the most frequent 100,000 words from each side of the parallel corpus, and all other words are treated as unknown words. We"
P13-1017,H05-1011,0,0.0188895,"For word pair (ei , fj ), we take fixed length windows surrounding both ei and fj , . . . , ei+ sw , fj− tw , . . . , fj+ tw ), as input: (ei− sw 2 2 2 2 where sw, tw stand window sizes on source and target side respectively. Words are converted to embeddings using the lookup table LT , and the catenation of embeddings are fed to a classic neural network with two hidden-layers, and the output of the network is the our lexical translation score: 1 The ability to model context is not unique to our model. In fact, discriminative word alignment can model contexts by deploying arbitrary features (Moore, 2005). Different from previous discriminative word alignment, our model does not use manually engineered features, but learn “features” automatically from raw words by the neural network. (Berger et al., 1996) use a maximum entropy model to model the bag-of-words context for word alignment, but their model treats each word as a distinct feature, which can not leverage the similarity between words as our model. tlex (ei , fj |e, f) = f3 ◦ f2 ◦ f1 ◦ LT (window(ei ), window(fj )) (6) f1 and f2 layers use htanh as activation functions, while f3 is only a linear transformation with no activation functio"
P13-1017,2012.iwslt-papers.3,0,0.0275577,"dent Deep Neural Network with HMM (CD-DNN-HMM) to speech recognition task, which significantly outperforms traditional models. Most methods using DNN in NLP start with a word embedding phase, which maps words into a fixed length, real valued vectors. (Bengio et al., 2006) proposed to use multi-layer neural network for language modeling task. (Collobert et al., 2011) applied DNN on several NLP tasks, such as part-of-speech tagging, chunking, name entity recognition, semantic labeling and syntactic parsing, where they got similar or even better results than the state-of-the-art on these tasks. (Niehues and Waibel, 2012) shows that machine translation results can be improved by combining neural language model with n-gram traditional language. (Son et al., 2012) improves translation quality of n-gram translation model by using a bilingual neural language model. (Titov et al., 2012) learns a context-free cross-lingual word embeddings to facilitate cross-lingual information retrieval. For the related works of word alignment, the most popular methods are based on generative models such as IBM Models (Brown et al., 1993) and HMM (Vogel et al., 1996). Discriminative approaches are also proposed to use hand crafted"
P13-1017,P09-1104,0,0.0091929,"obtained by classic HMM and IBM4 model. The second row and fourth row show results of the proposed model trained from HMM and IBM4 respectively. Experiments and Results results of our model also depends on the quality of baseline results, which is used as training data of our model. In future we would like to explore whether our method can improve other word alignment models. We also conduct experiment to see the effect on end-to-end SMT performance. We train hierWe conduct our experiment on Chinese-to-English word alignment task. We use the manually aligned Chinese-English alignment corpus (Haghighi et al., 2009) which contains 491 sentence pairs as test set. We adapt the segmentation on the Chinese side to fit our word segmentation standard. 171 archical phrase model (Chiang, 2007) from different word alignments. Despite different alignment scores, we do not obtain significant difference in translation performance. In our C-E experiment, we tuned on NIST-03, and tested on NIST08. Case-insensitive BLEU-4 scores on NIST-08 test are 0.305 and 0.307 for models trained from IBM-4 and NN alignment results. The result is not surprising considering our parallel corpus is quite large, and similar observations"
P13-1017,P05-1044,0,0.0167083,"(ai − ai−1 |window(fai−1 )) (7) which can be computed by a neural network similar to the one used to compute lexical translation scores. If we map jump distance (ai − ai−1 ) to B buckets, we can change the length of the output layer to B, where each dimension in the output stands for a different bucket of jump distances. But we found in our initial experiments on small scale data, lexicalized distortion does not produce better alignment over the simple jumpdistance based model. So we drop the lexicalized 5 Training Although unsupervised training technique such as Contrastive Estimation as in (Smith and Eisner, 2005), (Dyer et al., 2011) can be adapted to train 1 In practice, the number of non-zero parameters in classic HMM model would be much smaller, as many words do not co-occur in bilingual sentence pairs. In our experiments, the number of non-zero parameters in classic HMM model is about 328 millions, while the NN model only has about 4 millions. 169 et al., 2011) and train word embeddings for source and target languages from their monolingual corpus respectively. Our vocabularies Vs and Vt contain the most frequent 100,000 words from each side of the parallel corpus, and all other words are treated"
P13-1017,D12-1110,0,0.0460558,"Missing"
P13-1017,N12-1005,0,0.0718261,"n NLP start with a word embedding phase, which maps words into a fixed length, real valued vectors. (Bengio et al., 2006) proposed to use multi-layer neural network for language modeling task. (Collobert et al., 2011) applied DNN on several NLP tasks, such as part-of-speech tagging, chunking, name entity recognition, semantic labeling and syntactic parsing, where they got similar or even better results than the state-of-the-art on these tasks. (Niehues and Waibel, 2012) shows that machine translation results can be improved by combining neural language model with n-gram traditional language. (Son et al., 2012) improves translation quality of n-gram translation model by using a bilingual neural language model. (Titov et al., 2012) learns a context-free cross-lingual word embeddings to facilitate cross-lingual information retrieval. For the related works of word alignment, the most popular methods are based on generative models such as IBM Models (Brown et al., 1993) and HMM (Vogel et al., 1996). Discriminative approaches are also proposed to use hand crafted features to improve word alignment. Among them, (Liu et al., 2010) proposed to use phrase and rule pairs to model the context information in a"
P13-1017,C12-1089,0,0.0108582,") proposed to use multi-layer neural network for language modeling task. (Collobert et al., 2011) applied DNN on several NLP tasks, such as part-of-speech tagging, chunking, name entity recognition, semantic labeling and syntactic parsing, where they got similar or even better results than the state-of-the-art on these tasks. (Niehues and Waibel, 2012) shows that machine translation results can be improved by combining neural language model with n-gram traditional language. (Son et al., 2012) improves translation quality of n-gram translation model by using a bilingual neural language model. (Titov et al., 2012) learns a context-free cross-lingual word embeddings to facilitate cross-lingual information retrieval. For the related works of word alignment, the most popular methods are based on generative models such as IBM Models (Brown et al., 1993) and HMM (Vogel et al., 1996). Discriminative approaches are also proposed to use hand crafted features to improve word alignment. Among them, (Liu et al., 2010) proposed to use phrase and rule pairs to model the context information in a loglinear framework. Unlike previous discriminative methods, in this work, we do not resort to any hand crafted features,"
P13-1017,P10-1040,0,0.0173855,"ch smaller, as many words do not co-occur in bilingual sentence pairs. In our experiments, the number of non-zero parameters in classic HMM model is about 328 millions, while the NN model only has about 4 millions. 169 et al., 2011) and train word embeddings for source and target languages from their monolingual corpus respectively. Our vocabularies Vs and Vt contain the most frequent 100,000 words from each side of the parallel corpus, and all other words are treated as unknown words. We set word embedding length to 20, window size to 5, and the length of the only hidden layer to 40. Follow (Turian et al., 2010), we randomly initialize all parameters to [-0.1, 0.1], and use stochastic gradient descent to minimize the ranking loss with a fixed learning rate 0.01. Note that embedding for null word in either Ve and Vf cannot be trained from monolingual corpus, and we simply leave them at the initial value untouched. Word embeddings from monolingual corpus learn strong syntactic knowledge of each word, which is not always desirable for word alignment between some language pairs like English and Chinese. For example, many Chinese words can act as a verb, noun and adjective without any change, while their"
P13-1017,C96-2141,0,0.954535,"n better results than the state-of-the-art on these tasks. (Niehues and Waibel, 2012) shows that machine translation results can be improved by combining neural language model with n-gram traditional language. (Son et al., 2012) improves translation quality of n-gram translation model by using a bilingual neural language model. (Titov et al., 2012) learns a context-free cross-lingual word embeddings to facilitate cross-lingual information retrieval. For the related works of word alignment, the most popular methods are based on generative models such as IBM Models (Brown et al., 1993) and HMM (Vogel et al., 1996). Discriminative approaches are also proposed to use hand crafted features to improve word alignment. Among them, (Liu et al., 2010) proposed to use phrase and rule pairs to model the context information in a loglinear framework. Unlike previous discriminative methods, in this work, we do not resort to any hand crafted features, but use DNN to induce “features” from raw words. (b) A farmer Yibula Related Work Figure 1: Two examples of word alignment the English word “mammoth” is not, so it is very hard to align them correctly. If we know that “mammoth” has the similar meaning with “big”, or “h"
P13-1017,J97-3002,0,0.1441,"l data is used to pre-train wordembeddings. Experiments on large-scale Chineseto-English task show that the proposed method produces better word alignment results, compared with both classic HMM model and IBM model 4. For future work, we will investigate more settings of different hyper-parameters in our model. Secondly, we want to explore the possibility of unsupervised training of our neural word alignment model, without reliance of alignment result of other models. Furthermore, our current model use rather simple distortions; it might be helpful to use more sophisticated model such as ITG (Wu, 1997), which can be modeled by Recursive Neural Networks (Socher et al., 2011). improvement. Due to time constraint, we have not tuned the hyper-parameters such as length of hidden layers in 1 and 3-hidden-layer settings, nor have we tested settings with more hidden-layers. It would be wise to test more settings to verify whether more layers would help. 6.4.4 Word Embedding Following (Collobert et al., 2011), we show some words together with its nearest neighbors using the Euclidean distance between their embeddings. As we can see from Table 2, after bilingual training, “bad” is no longer in the ne"
P13-1017,J07-2003,0,\N,Missing
P13-1074,W02-1001,0,0.511051,"Missing"
P13-1074,I11-1136,0,0.0400969,"unity, including transition-based and graph-based dependency parsing. Compared to graph-based parsing, transition-based parsing can offer linear time complexity and easily leverage non-local features in the models (Yamada and Matsumoto, 2003; Nivre et al., 2006b; Zhang and Clark, 2008; Huang and Sagae, 2010). Starting with the work from (Zhang and Nivre, 2011), in this paper we extend transition-based dependency parsing from the sentence-level to the stream of words and integrate the parsing with punctuation prediction. Joint POS tagging and transition-based dependency parsing are studied in (Hatori et al., 2011; Bohnet and Nivre, 2012). The improvements are reported with the joint model compared to the pipeline model for Chinese and other richly inflected languages, which shows that it also makes sense to jointly perform punctuation prediction and parsing, although these two tasks of POS tagging and punctuation prediction are different in two ways: 1). The former usually works on a well-formed single sentence while the latter needs to process multiple sentences that are very lengthy. 2). POS tags are must-have features to parsing while punctuations are not. The parsing quality in the former is more"
P13-1074,P05-1056,0,0.268622,"998; Nakamura, 2009). This is because current machine translation (MT) systems perform the translation at the sentence level, where various models used in MT are trained over segmented sentences and many algorithms inside MT have an exponential complexity with regard to the length of inputs. The punctuation prediction problem has attracted research interest in both the speech processing community and the natural language processing community. Most previous work primarily exploits local features in their statistical models such as lexicons, prosodic cues and hidden event language model (HELM) (Liu et al., 2005; Matusov et al., 2006; Huang and Zweig, 2002; Stolcke and Shriberg, 1996). The word-level models integrating local features have narrow views about the input and could not achieve satisfied performance due to the limited context information access (Favre et al., 2008). Naturally, global contexts are required to model the punctuation prediction, especially for long-range dependencies. For instance, in English question sentences, the ending question mark is long-range dependent on the initial phrases (Lu and Ng, 2010), such as “could you” in Figure 1. There has been some work trying to incorpor"
P13-1074,D10-1018,0,0.442686,"odels such as lexicons, prosodic cues and hidden event language model (HELM) (Liu et al., 2005; Matusov et al., 2006; Huang and Zweig, 2002; Stolcke and Shriberg, 1996). The word-level models integrating local features have narrow views about the input and could not achieve satisfied performance due to the limited context information access (Favre et al., 2008). Naturally, global contexts are required to model the punctuation prediction, especially for long-range dependencies. For instance, in English question sentences, the ending question mark is long-range dependent on the initial phrases (Lu and Ng, 2010), such as “could you” in Figure 1. There has been some work trying to incorporate syntactic features to broaden the view of hypotheses in the punctuation prediction models (Roark et al., 2006; Favre et al., 2008). In their methods, the punctuation prediction is treated as a separated post-procedure of parsing, which may suffer from the problem of error propagation. In addition, these approaches are not able to incrementally process inputs and are not efficient for very long inputs, especially in the cases of long transcribed speech texts from presentations where the number of streaming words c"
P13-1074,de-marneffe-etal-2006-generating,0,0.150074,"Missing"
P13-1074,2006.iwslt-papers.1,0,0.0606999,"9). This is because current machine translation (MT) systems perform the translation at the sentence level, where various models used in MT are trained over segmented sentences and many algorithms inside MT have an exponential complexity with regard to the length of inputs. The punctuation prediction problem has attracted research interest in both the speech processing community and the natural language processing community. Most previous work primarily exploits local features in their statistical models such as lexicons, prosodic cues and hidden event language model (HELM) (Liu et al., 2005; Matusov et al., 2006; Huang and Zweig, 2002; Stolcke and Shriberg, 1996). The word-level models integrating local features have narrow views about the input and could not achieve satisfied performance due to the limited context information access (Favre et al., 2008). Naturally, global contexts are required to model the punctuation prediction, especially for long-range dependencies. For instance, in English question sentences, the ending question mark is long-range dependent on the initial phrases (Lu and Ng, 2010), such as “could you” in Figure 1. There has been some work trying to incorporate syntactic features"
P13-1074,W03-3017,0,0.142399,"Missing"
P13-1074,C04-1010,0,0.0570201,"Missing"
P13-1074,P11-2033,0,0.548841,"r short input processing. Unlike these works, we incorporate punctuation prediction into the parsing which process left to right input without length limitations. Numerous dependency parsing algorithms have been proposed in the natural language processing community, including transition-based and graph-based dependency parsing. Compared to graph-based parsing, transition-based parsing can offer linear time complexity and easily leverage non-local features in the models (Yamada and Matsumoto, 2003; Nivre et al., 2006b; Zhang and Clark, 2008; Huang and Sagae, 2010). Starting with the work from (Zhang and Nivre, 2011), in this paper we extend transition-based dependency parsing from the sentence-level to the stream of words and integrate the parsing with punctuation prediction. Joint POS tagging and transition-based dependency parsing are studied in (Hatori et al., 2011; Bohnet and Nivre, 2012). The improvements are reported with the joint model compared to the pipeline model for Chinese and other richly inflected languages, which shows that it also makes sense to jointly perform punctuation prediction and parsing, although these two tasks of POS tagging and punctuation prediction are different in two ways"
P13-1074,D08-1059,0,0.271651,"speech text is exponentially complex, their approaches are only feasible for short input processing. Unlike these works, we incorporate punctuation prediction into the parsing which process left to right input without length limitations. Numerous dependency parsing algorithms have been proposed in the natural language processing community, including transition-based and graph-based dependency parsing. Compared to graph-based parsing, transition-based parsing can offer linear time complexity and easily leverage non-local features in the models (Yamada and Matsumoto, 2003; Nivre et al., 2006b; Zhang and Clark, 2008; Huang and Sagae, 2010). Starting with the work from (Zhang and Nivre, 2011), in this paper we extend transition-based dependency parsing from the sentence-level to the stream of words and integrate the parsing with punctuation prediction. Joint POS tagging and transition-based dependency parsing are studied in (Hatori et al., 2011; Bohnet and Nivre, 2012). The improvements are reported with the joint model compared to the pipeline model for Chinese and other richly inflected languages, which shows that it also makes sense to jointly perform punctuation prediction and parsing, although these"
P13-1074,D12-1133,0,\N,Missing
P13-1074,2009.iwslt-evaluation.1,0,\N,Missing
P13-2006,P11-1138,0,0.511698,"muli,mingzhou}@microsoft.com zhlongk@qq.com wanghf@pku.edu.cn Abstract d and entity e, such as dot product, cosine similarity, Kullback-Leibler divergence, Jaccard distance, or more complicated ones (Zheng et al., 2010; Kulkarni et al., 2009; Hoffart et al., 2011; Bunescu and Pasca, 2006; Cucerzan, 2007; Zhang et al., 2011). However, these measures are often duplicate or over-specified, because they are disjointly combined and their atomic nature determines that they have no internal structure. Another line of work focuses on collective disambiguation (Kulkarni et al., 2009; Han et al., 2011; Ratinov et al., 2011; Hoffart et al., 2011). Ambiguous mentions within the same context are resolved simultaneously based on the coherence among decisions. Collective approaches often undergo a non-trivial decision process. In fact, (Ratinov et al., 2011) show that even though global approaches can be improved, local methods based on only similarity sim(d, e) of context d and entity e are hard to beat. This somehow reveals the importance of a good modeling of sim(d, e). Rather than learning context entity association at word level, topic model based approaches (Kataria et al., 2011; Sen, 2012) can learn it in the"
P13-2006,E06-1002,0,0.0494954,"ity representations for a given similarity measure. Stacked Denoising Auto-encoders are first employed to learn an initial document representation in an unsupervised pre-training stage. A supervised fine-tuning stage follows to optimize the representation towards the similarity measure. Experiment results show that our method achieves state-of-the-art performance on two public datasets without any manually designed features, even beating complex collective approaches. 1 Introduction Entity linking or disambiguation has recently received much attention in natural language processing community (Bunescu and Pasca, 2006; Han et al., 2011; Kataria et al., 2011; Sen, 2012). It is an essential first step for succeeding sub-tasks in knowledge base construction (Ji and Grishman, 2011) like populating attribute to entities. Given a sentence with four mentions, “The [[Python]] of [[Delphi]] was a creature with the body of a snake. This creature dwelled on [[Mount Parnassus]], in central [[Greece]].” How can we determine that Python is an earth-dragon in Greece mythology and not the popular programming language, Delphi is not the auto parts supplier, and Mount Parnassus is in Greece, not in Colorado? A most straight"
P13-2006,D07-1074,0,0.185206,"Missing"
P13-2006,P05-1044,0,0.00959837,"es are f (d) and f (e). In Figure 3, each row shares forward path of f (d) while each column shares forward path of f (e). At backpropagation stage, gradient is summed over each row of score nodes for f (d) and over each column for f (e). Till now, our input simply consists of bag-ofwords binary vector. We can incorporate any handcrafted feature f (d, e) as: exp sim(d, e) (3) ei ∈C(m) exp sim(d, ei ) L(d, e) = − log P Finally, we seek to minimize the following training objective across all training instances: X L= L(d, e) (4) d,e The loss function is closely related to contrastive estimation (Smith and Eisner, 2005), which defines where the positive example takes probability mass from. We find that by penalizing more negative examples, convergence speed can be greatly accelerated. In our experiments, the sof tmax loss function consistently outperforms pairwise ranking loss function, which is taken as our default setting. sim(d, e) = Dot(f (d), f (e)) + ~λf~(d, e) (5) In fact, we find that with only Dot(f (d), f (e)) as ranking score, the performance is sufficiently good. So we leave this as our future work. 32 3 Experiments and Analysis different decisions. To our surprise, our method with only local evi"
P13-2006,D11-1072,0,0.84346,"Missing"
P13-2006,N10-1072,0,0.0590816,"Missing"
P13-2006,P11-1115,0,0.0101432,"d pre-training stage. A supervised fine-tuning stage follows to optimize the representation towards the similarity measure. Experiment results show that our method achieves state-of-the-art performance on two public datasets without any manually designed features, even beating complex collective approaches. 1 Introduction Entity linking or disambiguation has recently received much attention in natural language processing community (Bunescu and Pasca, 2006; Han et al., 2011; Kataria et al., 2011; Sen, 2012). It is an essential first step for succeeding sub-tasks in knowledge base construction (Ji and Grishman, 2011) like populating attribute to entities. Given a sentence with four mentions, “The [[Python]] of [[Delphi]] was a creature with the body of a snake. This creature dwelled on [[Mount Parnassus]], in central [[Greece]].” How can we determine that Python is an earth-dragon in Greece mythology and not the popular programming language, Delphi is not the auto parts supplier, and Mount Parnassus is in Greece, not in Colorado? A most straightforward method is to compare the context of the mention and the definition of candidate entities. Previous work has explored many ways of measuring the relatedness"
P13-2006,C10-1142,0,\N,Missing
P13-2006,P14-2013,0,\N,Missing
P13-2006,P14-1062,0,\N,Missing
P13-2006,Q14-1019,0,\N,Missing
P13-2006,W12-6324,0,\N,Missing
P13-2006,P14-1146,1,\N,Missing
P13-2006,W12-6322,0,\N,Missing
P13-2006,W12-6325,0,\N,Missing
P13-2006,W12-6323,0,\N,Missing
P13-2061,W06-1607,0,0.246555,"methods can filter some low-quality bilingual data, they need sufficient human labeled training instances to build the model, which may not be easy to acquire. To this end, we propose an unsupervised approach to clean the bilingual data. It is intuitive that high-quality parallel data tends to produce better phrase pairs than low-quality data. Meanwhile, it is also observed that the phrase pairs that appear frequently in the bilingual corpus are more reliable than less frequent ones because they are more reusable, hence most good sentence pairs are prone to contain more frequent phrase pairs (Foster et al., 2006; Wuebker et al., 2010). This kind of mutual reinforcement fits well into the framework of graph-based random walk. When a phrase pair p is extracted from a sentence pair s, s is considered casting a vote for p. The higher the number of votes a phrase pair has, the more reliable of the phrase pair. Similarly, the quality of the sentence pair s is determined by the number of votes casted by the extracted phrase pairs from s. In this paper, a PageRank-style random walk algorithm (Brin and Page, 1998; Mihalcea and Tarau, 2004; Wan et al., 2007) is conducted to iteratively compute the importance s"
P13-2061,2012.amta-papers.7,0,0.0835577,"Missing"
P13-2061,P09-1098,1,0.833027,"test web data web data web data Table 1: Development and testing data used in the experiments. are also randomly partitioned and summed across different machines. Since long sentence pairs usually extract more phrase pairs, we need to normalize the importance scores based on the sentence length. The algorithm fits well into the MapReduce programming model (Dean and Ghemawat, 2008) and we use it as our implementation. 2.5 Setup We evaluated our bilingual data cleaning approach on large-scale Chinese-to-English machine translation tasks. The bilingual data we used was mainly mined from the web (Jiang et al., 2009)1 , as well as the United Nations parallel corpus released by LDC and the parallel corpus released by China Workshop on Machine Translation (CWMT), which contain around 30 million sentence pairs in total after removing duplicated ones. The development data and testing data is shown in Table 1. based on the iterative computation in the Section 2.3. Before the iterative computation starts, the sum of the outlink weights for each vertex is computed first. The edges are randomly partitioned into sets of roughly equal size. Each edge hsi , pj i can generate two key-value pairs in the format hsi , r"
P13-2061,N03-1017,0,0.122322,"ften lead to better phrase extraction and vice versa. End-to-end experiments show that the proposed method substantially improves the performance in largescale Chinese-to-English translation tasks. 1 Introduction Statistical machine translation (SMT) depends on the amount of bilingual data and its quality. In real-world SMT systems, bilingual data is often mined from the web where low-quality data is inevitable. The low-quality bilingual data degrades the quality of word alignment and leads to the incorrect phrase pairs, which will hurt the translation performance of phrase-based SMT systems (Koehn et al., 2003; Och and Ney, 2004). Therefore, it is very important to exploit data quality information to improve the translation modeling. Previous work on bilingual data cleaning often involves some supervised learning methods. Several bilingual data mining systems (Resnik and ∗ This work has been done while the first author was visiting Microsoft Research Asia. 340 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 340–345, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics 2 The Proposed Approach 2.1 Phrase Pair Vertices Senten"
P13-2061,W04-3250,0,0.270408,"Missing"
P13-2061,J05-4003,0,0.126612,"Missing"
P13-2061,J04-4002,0,0.0798821,"phrase extraction and vice versa. End-to-end experiments show that the proposed method substantially improves the performance in largescale Chinese-to-English translation tasks. 1 Introduction Statistical machine translation (SMT) depends on the amount of bilingual data and its quality. In real-world SMT systems, bilingual data is often mined from the web where low-quality data is inevitable. The low-quality bilingual data degrades the quality of word alignment and leads to the incorrect phrase pairs, which will hurt the translation performance of phrase-based SMT systems (Koehn et al., 2003; Och and Ney, 2004). Therefore, it is very important to exploit data quality information to improve the translation modeling. Previous work on bilingual data cleaning often involves some supervised learning methods. Several bilingual data mining systems (Resnik and ∗ This work has been done while the first author was visiting Microsoft Research Asia. 340 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 340–345, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics 2 The Proposed Approach 2.1 Phrase Pair Vertices Sentence Pair Vertices Gra"
P13-2061,P03-1021,0,0.159372,"Missing"
P13-2061,P02-1040,0,0.0949147,"Missing"
P13-2061,J03-3002,0,0.192071,"Missing"
P13-2061,P06-1062,1,0.888295,"Missing"
P13-2061,P07-1070,0,0.162098,"s are prone to contain more frequent phrase pairs (Foster et al., 2006; Wuebker et al., 2010). This kind of mutual reinforcement fits well into the framework of graph-based random walk. When a phrase pair p is extracted from a sentence pair s, s is considered casting a vote for p. The higher the number of votes a phrase pair has, the more reliable of the phrase pair. Similarly, the quality of the sentence pair s is determined by the number of votes casted by the extracted phrase pairs from s. In this paper, a PageRank-style random walk algorithm (Brin and Page, 1998; Mihalcea and Tarau, 2004; Wan et al., 2007) is conducted to iteratively compute the importance score of each sentence pair that indicates its quality: the higher the better. Unlike other data filtering methods, our proposed method utilizes the importance scores of sentence pairs as fractional counts to calculate the phrase translation probabilities based on Maximum Likelihood Estimation (MLE), thereby none of the bilingual data is filtered out. Experimental results show that our proposed approach substantially improves the performance in large-scale Chinese-to-English translation tasks. The quality of bilingual data is a key factor in"
P13-2061,J97-3002,0,0.279256,"utlink weights for each vertex is computed first. The edges are randomly partitioned into sets of roughly equal size. Each edge hsi , pj i can generate two key-value pairs in the format hsi , rij i and hpj , rij i. The pairs with the same key are summed locally and accumulated across different machines. Then, in each iteration, the score of each vertex is updated according to the sum of the normalized inlink weights. The key-value pairs are generr ated in the format hsi , P ij rkj · v(pj )i and hpj , P Experiments A phrase-based decoder was implemented based on inversion transduction grammar (Wu, 1997). The performance of this decoder is similar to the state-of-the-art phrase-based decoder in Moses, but the implementation is more straightforward. We use the following feature functions in the log-linear model: Integration into translation modeling After sufficient number of iterations, the importance scores of sentence pairs (i.e., u(si )) are obtained. Instead of simple filtering, we use the 1 Although supervised data cleaning has been done in the post-processing, the corpus still contains a fair amount of noisy data based on our random sampling. 342 baseline (Wuebker et al., 2010) -0.25M -"
P13-2061,P10-1049,0,0.0134939,"me low-quality bilingual data, they need sufficient human labeled training instances to build the model, which may not be easy to acquire. To this end, we propose an unsupervised approach to clean the bilingual data. It is intuitive that high-quality parallel data tends to produce better phrase pairs than low-quality data. Meanwhile, it is also observed that the phrase pairs that appear frequently in the bilingual corpus are more reliable than less frequent ones because they are more reusable, hence most good sentence pairs are prone to contain more frequent phrase pairs (Foster et al., 2006; Wuebker et al., 2010). This kind of mutual reinforcement fits well into the framework of graph-based random walk. When a phrase pair p is extracted from a sentence pair s, s is considered casting a vote for p. The higher the number of votes a phrase pair has, the more reliable of the phrase pair. Similarly, the quality of the sentence pair s is determined by the number of votes casted by the extracted phrase pairs from s. In this paper, a PageRank-style random walk algorithm (Brin and Page, 1998; Mihalcea and Tarau, 2004; Wan et al., 2007) is conducted to iteratively compute the importance score of each sentence p"
P13-2061,P06-1066,0,0.0731442,"Missing"
P13-2061,W04-3252,0,\N,Missing
P13-2061,J03-1002,0,\N,Missing
P14-1011,D13-1106,0,0.00699973,"ks. 1 Introduction Due to the powerful capacity of feature learning and representation, Deep (multi-layer) Neural Networks (DNN) have achieved a great success in speech and image processing (Kavukcuoglu et al., 2010; Krizhevsky et al., 2012; Dahl et al., 2012). Recently, statistical machine translation (SMT) community has seen a strong interest in adapting and applying DNN to many tasks, such as word alignment (Yang et al., 2013), translation confidence estimation (Mikolov et al., 2010; Liu et al., 2013; Zou et al., 2013), phrase reordering prediction (Li et al., 2013), translation modelling (Auli et al., 2013; Kalchbrenner and Blunsom, 2013) and language modelling (Duh et al., 2013; Vaswani et al., 2013). Most of these works attempt to improve some components in SMT based on word 111 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 111–121, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics of its internal words, we propose Bilinguallyconstrained Recursive Auto-encoders (BRAE) to learn semantic phrase embeddings. The core idea behind is that a phrase and its correct translation should share the same semantic mea"
P14-1011,W04-3250,0,0.0534067,"sh translation. Accordingly, our BRAE model is trained on Chinese and English. The bilingual training data from LDC 2 contains 0.96M sentence pairs and 1.1M entity pairs with 27.7M Chinese words and 31.9M English words. A 5gram language model is trained on the Xinhua portion of the English Gigaword corpus and the English part of bilingual training data. The NIST MT03 is used as the development data. NIST MT04-06 and MT08 (news data) are used as the test data. Case-insensitive BLEU is employed as the evaluation metric. The statistical significance test is performed by the re-sampling approach (Koehn, 2004). In addition, we pre-train the word embedding with toolkit Word2Vec on large-scale monolingual data including the aforementioned data for SMT. The monolingual data contains 1.06B words for Chinese and 1.12B words for English. To obtain high-quality bilingual phrase pairs to train our BRAE model, we perform forced decoding for the bilingual training sentences and collect the phrase pairs used. After removing the duplicates, the remaining 1.12M bilingual phrase pairs (length ranging from 1 to 7) are obtained. 4.3 Phrase Table Pruning Pruning most of the phrase table without much impact on trans"
P14-1011,D13-1054,0,0.266933,"ccessfully apply DNN to model the whole translation process, such as modelling the decoding process, learning compact vector representations for the basic phrasal translation units is the essential and fundamental work. In this paper, we explore the phrase embedding, which represents a phrase (sequence of words) with a real-valued vector. In some previous works, phrase embedding has been discussed from different views. Socher et al. (2011) make the phrase embeddings capture the sentiment information. Socher et al. (2013a) enable the phrase embeddings to mainly capture the syntactic knowledge. Li et al. (2013) attempt to encode the reordering pattern in the phrase embeddings. Kalchbrenner and Blunsom (2013) utilize a simple convolution model to generate phrase embeddings from word embeddings. Mikolov et al. (2013) consider a phrase as an indivisible n-gram. Obviously, these methods of learning phrase embeddings either focus on some aspects of the phrase (e.g. reordering pattern), or impose strong assumptions (e.g. bagof-words or indivisible n-gram). Therefore, these phrase embeddings are not suitable to fully represent the phrasal translation units in SMT due to the lack of semantic meanings of the"
P14-1011,D12-1088,0,0.0249865,"Missing"
P14-1011,P13-1078,0,0.0249237,"ts translation candidates. Extensive experiments show that the BRAE is remarkably effective in these two tasks. 1 Introduction Due to the powerful capacity of feature learning and representation, Deep (multi-layer) Neural Networks (DNN) have achieved a great success in speech and image processing (Kavukcuoglu et al., 2010; Krizhevsky et al., 2012; Dahl et al., 2012). Recently, statistical machine translation (SMT) community has seen a strong interest in adapting and applying DNN to many tasks, such as word alignment (Yang et al., 2013), translation confidence estimation (Mikolov et al., 2010; Liu et al., 2013; Zou et al., 2013), phrase reordering prediction (Li et al., 2013), translation modelling (Auli et al., 2013; Kalchbrenner and Blunsom, 2013) and language modelling (Duh et al., 2013; Vaswani et al., 2013). Most of these works attempt to improve some components in SMT based on word 111 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 111–121, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics of its internal words, we propose Bilinguallyconstrained Recursive Auto-encoders (BRAE) to learn semantic phrase emb"
P14-1011,P13-2119,0,0.0122932,"resentation, Deep (multi-layer) Neural Networks (DNN) have achieved a great success in speech and image processing (Kavukcuoglu et al., 2010; Krizhevsky et al., 2012; Dahl et al., 2012). Recently, statistical machine translation (SMT) community has seen a strong interest in adapting and applying DNN to many tasks, such as word alignment (Yang et al., 2013), translation confidence estimation (Mikolov et al., 2010; Liu et al., 2013; Zou et al., 2013), phrase reordering prediction (Li et al., 2013), translation modelling (Auli et al., 2013; Kalchbrenner and Blunsom, 2013) and language modelling (Duh et al., 2013; Vaswani et al., 2013). Most of these works attempt to improve some components in SMT based on word 111 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 111–121, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics of its internal words, we propose Bilinguallyconstrained Recursive Auto-encoders (BRAE) to learn semantic phrase embeddings. The core idea behind is that a phrase and its correct translation should share the same semantic meaning. Thus, they can supervise each other to learn their semantic phrase e"
P14-1011,2007.mtsummit-papers.22,0,0.123108,"Missing"
P14-1011,D11-1014,0,0.55051,"sing word embeddings as the direct inputs to DNN cannot make full use of the whole syntactic and semantic information of the phrasal translation rules. Therefore, in order to successfully apply DNN to model the whole translation process, such as modelling the decoding process, learning compact vector representations for the basic phrasal translation units is the essential and fundamental work. In this paper, we explore the phrase embedding, which represents a phrase (sequence of words) with a real-valued vector. In some previous works, phrase embedding has been discussed from different views. Socher et al. (2011) make the phrase embeddings capture the sentiment information. Socher et al. (2013a) enable the phrase embeddings to mainly capture the syntactic knowledge. Li et al. (2013) attempt to encode the reordering pattern in the phrase embeddings. Kalchbrenner and Blunsom (2013) utilize a simple convolution model to generate phrase embeddings from word embeddings. Mikolov et al. (2013) consider a phrase as an indivisible n-gram. Obviously, these methods of learning phrase embeddings either focus on some aspects of the phrase (e.g. reordering pattern), or impose strong assumptions (e.g. bagof-words or"
P14-1011,D07-1103,0,0.030171,"Missing"
P14-1011,P13-1045,0,0.574395,"syntactic and semantic information of the phrasal translation rules. Therefore, in order to successfully apply DNN to model the whole translation process, such as modelling the decoding process, learning compact vector representations for the basic phrasal translation units is the essential and fundamental work. In this paper, we explore the phrase embedding, which represents a phrase (sequence of words) with a real-valued vector. In some previous works, phrase embedding has been discussed from different views. Socher et al. (2011) make the phrase embeddings capture the sentiment information. Socher et al. (2013a) enable the phrase embeddings to mainly capture the syntactic knowledge. Li et al. (2013) attempt to encode the reordering pattern in the phrase embeddings. Kalchbrenner and Blunsom (2013) utilize a simple convolution model to generate phrase embeddings from word embeddings. Mikolov et al. (2013) consider a phrase as an indivisible n-gram. Obviously, these methods of learning phrase embeddings either focus on some aspects of the phrase (e.g. reordering pattern), or impose strong assumptions (e.g. bagof-words or indivisible n-gram). Therefore, these phrase embeddings are not suitable to fully"
P14-1011,D13-1176,0,0.788727,"ecoding process, learning compact vector representations for the basic phrasal translation units is the essential and fundamental work. In this paper, we explore the phrase embedding, which represents a phrase (sequence of words) with a real-valued vector. In some previous works, phrase embedding has been discussed from different views. Socher et al. (2011) make the phrase embeddings capture the sentiment information. Socher et al. (2013a) enable the phrase embeddings to mainly capture the syntactic knowledge. Li et al. (2013) attempt to encode the reordering pattern in the phrase embeddings. Kalchbrenner and Blunsom (2013) utilize a simple convolution model to generate phrase embeddings from word embeddings. Mikolov et al. (2013) consider a phrase as an indivisible n-gram. Obviously, these methods of learning phrase embeddings either focus on some aspects of the phrase (e.g. reordering pattern), or impose strong assumptions (e.g. bagof-words or indivisible n-gram). Therefore, these phrase embeddings are not suitable to fully represent the phrasal translation units in SMT due to the lack of semantic meanings of the phrase. Instead, we focus on learning phrase embeddings from the view of semantic meaning, so that"
P14-1011,2009.mtsummit-papers.17,0,0.0492238,"Missing"
P14-1011,D13-1140,0,0.00759691,"(multi-layer) Neural Networks (DNN) have achieved a great success in speech and image processing (Kavukcuoglu et al., 2010; Krizhevsky et al., 2012; Dahl et al., 2012). Recently, statistical machine translation (SMT) community has seen a strong interest in adapting and applying DNN to many tasks, such as word alignment (Yang et al., 2013), translation confidence estimation (Mikolov et al., 2010; Liu et al., 2013; Zou et al., 2013), phrase reordering prediction (Li et al., 2013), translation modelling (Auli et al., 2013; Kalchbrenner and Blunsom, 2013) and language modelling (Duh et al., 2013; Vaswani et al., 2013). Most of these works attempt to improve some components in SMT based on word 111 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 111–121, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics of its internal words, we propose Bilinguallyconstrained Recursive Auto-encoders (BRAE) to learn semantic phrase embeddings. The core idea behind is that a phrase and its correct translation should share the same semantic meaning. Thus, they can supervise each other to learn their semantic phrase embeddings. Similarly, n"
P14-1011,J97-3002,0,0.185251,"Missing"
P14-1011,P06-1066,0,0.0963857,"Missing"
P14-1011,P13-1017,1,0.245382,"ties) which need to measure semantic similarity between a source phrase and its translation candidates. Extensive experiments show that the BRAE is remarkably effective in these two tasks. 1 Introduction Due to the powerful capacity of feature learning and representation, Deep (multi-layer) Neural Networks (DNN) have achieved a great success in speech and image processing (Kavukcuoglu et al., 2010; Krizhevsky et al., 2012; Dahl et al., 2012). Recently, statistical machine translation (SMT) community has seen a strong interest in adapting and applying DNN to many tasks, such as word alignment (Yang et al., 2013), translation confidence estimation (Mikolov et al., 2010; Liu et al., 2013; Zou et al., 2013), phrase reordering prediction (Li et al., 2013), translation modelling (Auli et al., 2013; Kalchbrenner and Blunsom, 2013) and language modelling (Duh et al., 2013; Vaswani et al., 2013). Most of these works attempt to improve some components in SMT based on word 111 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 111–121, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics of its internal words, we propose Bilingu"
P14-1011,D12-1089,0,0.0382796,"Missing"
P14-1011,D13-1141,0,0.206555,"didates. Extensive experiments show that the BRAE is remarkably effective in these two tasks. 1 Introduction Due to the powerful capacity of feature learning and representation, Deep (multi-layer) Neural Networks (DNN) have achieved a great success in speech and image processing (Kavukcuoglu et al., 2010; Krizhevsky et al., 2012; Dahl et al., 2012). Recently, statistical machine translation (SMT) community has seen a strong interest in adapting and applying DNN to many tasks, such as word alignment (Yang et al., 2013), translation confidence estimation (Mikolov et al., 2010; Liu et al., 2013; Zou et al., 2013), phrase reordering prediction (Li et al., 2013), translation modelling (Auli et al., 2013; Kalchbrenner and Blunsom, 2013) and language modelling (Duh et al., 2013; Vaswani et al., 2013). Most of these works attempt to improve some components in SMT based on word 111 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 111–121, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics of its internal words, we propose Bilinguallyconstrained Recursive Auto-encoders (BRAE) to learn semantic phrase embeddings. The core i"
P14-1011,D13-1170,0,\N,Missing
P14-1013,W06-1607,0,0.0295023,"The reported BLEU scores are averaged over 5 times of running MERT (Och, 2003). A statistical significance test is performed using the bootstrap resampling method (Koehn, 2004). English Docs Words 9.9M 25.6B 1.2M 2.9B 11.1M 28.5B Table 1: Statistics of monolingual data, in numbers of documents and words (main content). “M” refers to million and “B” refers to billion. 4.2 Baseline The baseline is a re-implementation of the Hiero system (Chiang, 2007). The phrase pairs that appear only once in the parallel data are discarded because most of them are noisy. We also use the fix-discount method in Foster et al. (2006) for phrase table smoothing. This implementation makes the system perform much better and the translation model size is much smaller. We compare our method with the LDA-based approach proposed by Xiao et al. (2012). In (Xiao et al., 2012), the topic of each sentence pair is exactly the same as the document it belongs to. Since some of our parallel data does not have documentlevel information, we rely on the IR method to retrieve the most relevant document and simulate this approach. The PLDA toolkit (Liu et al., 2011) is used to infer topic distributions, which takes 34.5 hours to finish. We i"
P14-1013,P05-1048,0,0.0187263,"vel. However, this situation does not always happen since there is considerable amount of parallel data which does not have document boundaries. In addition, contemporary SMT systems often works on sentence level rather than document level due to the efficiency. Although we can easily apply LDA at the Introduction Making translation decisions is a difficult task in many Statistical Machine Translation (SMT) systems. Current translation modeling approaches usually use context dependent information to disambiguate translation candidates. For example, translation sense disambiguation approaches (Carpuat and Wu, 2005; Carpuat and Wu, 2007) are proposed for phrase-based SMT systems. Meanwhile, for hierarchical phrase-based or syntax-based SMT systems, there is also much work involving rich contexts to guide rule selection (He et al., 2008; Liu et al., 2008; Marton and Resnik, 2008; Xiong et al., 2009). Although these methods are effective and proven successful in many SMT systems, they only leverage within∗ This work was done while the first and fourth authors were visiting Microsoft Research. 133 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 133–143, c Balt"
P14-1013,C08-1041,0,0.130559,"ent level due to the efficiency. Although we can easily apply LDA at the Introduction Making translation decisions is a difficult task in many Statistical Machine Translation (SMT) systems. Current translation modeling approaches usually use context dependent information to disambiguate translation candidates. For example, translation sense disambiguation approaches (Carpuat and Wu, 2005; Carpuat and Wu, 2007) are proposed for phrase-based SMT systems. Meanwhile, for hierarchical phrase-based or syntax-based SMT systems, there is also much work involving rich contexts to guide rule selection (He et al., 2008; Liu et al., 2008; Marton and Resnik, 2008; Xiong et al., 2009). Although these methods are effective and proven successful in many SMT systems, they only leverage within∗ This work was done while the first and fourth authors were visiting Microsoft Research. 133 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 133–143, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics accuracy over a state-of-the-art baseline. sentence level, it is quite difficult to infer the topic accurately with only a few words in the"
P14-1013,2007.mtsummit-papers.11,0,0.0175661,"uation does not always happen since there is considerable amount of parallel data which does not have document boundaries. In addition, contemporary SMT systems often works on sentence level rather than document level due to the efficiency. Although we can easily apply LDA at the Introduction Making translation decisions is a difficult task in many Statistical Machine Translation (SMT) systems. Current translation modeling approaches usually use context dependent information to disambiguate translation candidates. For example, translation sense disambiguation approaches (Carpuat and Wu, 2005; Carpuat and Wu, 2007) are proposed for phrase-based SMT systems. Meanwhile, for hierarchical phrase-based or syntax-based SMT systems, there is also much work involving rich contexts to guide rule selection (He et al., 2008; Liu et al., 2008; Marton and Resnik, 2008; Xiong et al., 2009). Although these methods are effective and proven successful in many SMT systems, they only leverage within∗ This work was done while the first and fourth authors were visiting Microsoft Research. 133 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 133–143, c Baltimore, Maryland, USA, J"
P14-1013,P13-1126,0,0.0442344,"Missing"
P14-1013,W04-3250,0,0.0266605,"language modeling toolkit is used to train the 5-gram language model with modified Kneser-Ney smoothing (Kneser and Ney, 1995). The English monolingual data used for language modeling is the same as in Table 1. The NIST 2003 dataset is the development data. The testing data consists of NIST 2004, 2005, 2006 and 2008 datasets. The evaluation metric for the overall translation quality is caseinsensitive BLEU4 (Papineni et al., 2002). The reported BLEU scores are averaged over 5 times of running MERT (Och, 2003). A statistical significance test is performed using the bootstrap resampling method (Koehn, 2004). English Docs Words 9.9M 25.6B 1.2M 2.9B 11.1M 28.5B Table 1: Statistics of monolingual data, in numbers of documents and words (main content). “M” refers to million and “B” refers to billion. 4.2 Baseline The baseline is a re-implementation of the Hiero system (Chiang, 2007). The phrase pairs that appear only once in the parallel data are discarded because most of them are noisy. We also use the fix-discount method in Foster et al. (2006) for phrase table smoothing. This implementation makes the system perform much better and the translation model size is much smaller. We compare our method"
P14-1013,J07-2003,0,0.732936,"tributions while topic-specific rules have sharper distributions. A standard entropy metric is used to measure the sensitivity of the source-side of hα, γi as: The model minimizes the pairwise ranking loss across all training instances: X L(f, e) (6) L= Sen(α) = − hf,ei We incorporate the learned topic similarity scores into the standard log-linear framework for SMT. When a synchronous rule hα, γi is extracted from a sentence pair hf, ei, a triple instance I = (hα, γi, hf, ei, c) is collected for inferring the topic representation of hα, γi, where c is the count of rule occurrence. Following (Chiang, 2007), we give a count of one for each phrase pair occurrence and a fractional count for each hierarchical phrase pair. The topic representation of hα, γi is then calculated as the weighted average: P (hα,γi,hf,ei,c)∈T {c × zf } zα = P (7) (hα,γi,hf,ei,c)∈T {c} zγ = (hα,γi,hf,ei,c)∈T P {c × ze } (hα,γi,hf,ei,c)∈T {c} eˆ = arg max P (e|f ) where the translation probability is given by: X P (e|f ) ∝ wi · log φi (f, e) i = Sim(zs , zγ ) = cos(zs , zγ ) (10) X | wj · log φj (f, e) + j {z Standard } X | k wk · log φk (f, e) {z Topic related (13) where φj (f, e) is the standard feature function and wj is"
P14-1013,D13-1107,1,0.837725,"hieving significant improvements in the large-scale evaluation. (Su et al., 2012) investigated the relationship between out-of-domain bilingual data and in-domain monolingual data via topic mapping using HTMM methods. They estimated phrasetopic distributions in translation model adaptation and generated better translation quality. Recently, Chen et al. (2013) proposed using vector space model for adaptation where genre resemblance is leveraged to improve translation accuracy. We also investigated multi-domain adaptation where explicit topic information is used to train domain specific models (Cui et al., 2013). Related Work Generally, most previous research has leveraged conventional topic modeling techniques such as LDA or HTMM. In our work, a novel neural network based approach is proposed to infer topic representations for parallel data. The advantage of Topic modeling was first leveraged to improve SMT performance in (Zhao and Xing, 2006; Zhao and Xing, 2007). They proposed a bilingual topical admixture approach for word alignment and assumed that each word-pair follows a topic140 Sim(Src) Sim(Trg) Sim(Src+Trg) Sim(Src+Trg)+Sen(Src) Sim(Src+Trg)+Sen(Trg) Sim(Src+Trg)+Sen(Src+Trg) 42.51 42.43 42"
P14-1013,D08-1010,0,0.211625,"the efficiency. Although we can easily apply LDA at the Introduction Making translation decisions is a difficult task in many Statistical Machine Translation (SMT) systems. Current translation modeling approaches usually use context dependent information to disambiguate translation candidates. For example, translation sense disambiguation approaches (Carpuat and Wu, 2005; Carpuat and Wu, 2007) are proposed for phrase-based SMT systems. Meanwhile, for hierarchical phrase-based or syntax-based SMT systems, there is also much work involving rich contexts to guide rule selection (He et al., 2008; Liu et al., 2008; Marton and Resnik, 2008; Xiong et al., 2009). Although these methods are effective and proven successful in many SMT systems, they only leverage within∗ This work was done while the first and fourth authors were visiting Microsoft Research. 133 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 133–143, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics accuracy over a state-of-the-art baseline. sentence level, it is quite difficult to infer the topic accurately with only a few words in the sentence. This ma"
P14-1013,P08-1114,0,0.0702404,"though we can easily apply LDA at the Introduction Making translation decisions is a difficult task in many Statistical Machine Translation (SMT) systems. Current translation modeling approaches usually use context dependent information to disambiguate translation candidates. For example, translation sense disambiguation approaches (Carpuat and Wu, 2005; Carpuat and Wu, 2007) are proposed for phrase-based SMT systems. Meanwhile, for hierarchical phrase-based or syntax-based SMT systems, there is also much work involving rich contexts to guide rule selection (He et al., 2008; Liu et al., 2008; Marton and Resnik, 2008; Xiong et al., 2009). Although these methods are effective and proven successful in many SMT systems, they only leverage within∗ This work was done while the first and fourth authors were visiting Microsoft Research. 133 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 133–143, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics accuracy over a state-of-the-art baseline. sentence level, it is quite difficult to infer the topic accurately with only a few words in the sentence. This makes previous approaches i"
P14-1013,P03-1021,0,0.0503972,"tions, and the diaggrow-final heuristic is used to refine symmetric word alignment. An in-house language modeling toolkit is used to train the 5-gram language model with modified Kneser-Ney smoothing (Kneser and Ney, 1995). The English monolingual data used for language modeling is the same as in Table 1. The NIST 2003 dataset is the development data. The testing data consists of NIST 2004, 2005, 2006 and 2008 datasets. The evaluation metric for the overall translation quality is caseinsensitive BLEU4 (Papineni et al., 2002). The reported BLEU scores are averaged over 5 times of running MERT (Och, 2003). A statistical significance test is performed using the bootstrap resampling method (Koehn, 2004). English Docs Words 9.9M 25.6B 1.2M 2.9B 11.1M 28.5B Table 1: Statistics of monolingual data, in numbers of documents and words (main content). “M” refers to million and “B” refers to billion. 4.2 Baseline The baseline is a re-implementation of the Hiero system (Chiang, 2007). The phrase pairs that appear only once in the parallel data are discarded because most of them are noisy. We also use the fix-discount method in Foster et al. (2006) for phrase table smoothing. This implementation makes the"
P14-1013,P12-1079,0,0.66379,"s are selected according to the distributional similarity with the source text during SMT decoding. Experimental results show that our method significantly improves translation accuracy in the NIST Chinese-to-English translation task compared to a state-of-the-art baseline. 1 Topic modeling is a useful mechanism for discovering and characterizing various semantic concepts embedded in a collection of documents. Attempts on topic-based translation modeling include topic-specific lexicon translation models (Zhao and Xing, 2006; Zhao and Xing, 2007), topic similarity models for synchronous rules (Xiao et al., 2012), and document-level translation with topic coherence (Xiong and Zhang, 2013). In addition, topic-based approaches have been used in domain adaptation for SMT (Tam et al., 2007; Su et al., 2012), where they view different topics as different domains. One typical property of these approaches in common is that they only utilize parallel data where document boundaries are explicitly given. In this way, the topic of a sentence can be inferred with document-level information using off-the-shelf topic modeling toolkits such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003) or Hidden Topic Mar"
P14-1013,P02-1040,0,0.0916175,"News Weblog Total Chinese Docs Words 5.7M 5.4B 2.1M 8B 7.8M 13.4B using GIZA++ in both directions, and the diaggrow-final heuristic is used to refine symmetric word alignment. An in-house language modeling toolkit is used to train the 5-gram language model with modified Kneser-Ney smoothing (Kneser and Ney, 1995). The English monolingual data used for language modeling is the same as in Table 1. The NIST 2003 dataset is the development data. The testing data consists of NIST 2004, 2005, 2006 and 2008 datasets. The evaluation metric for the overall translation quality is caseinsensitive BLEU4 (Papineni et al., 2002). The reported BLEU scores are averaged over 5 times of running MERT (Och, 2003). A statistical significance test is performed using the bootstrap resampling method (Koehn, 2004). English Docs Words 9.9M 25.6B 1.2M 2.9B 11.1M 28.5B Table 1: Statistics of monolingual data, in numbers of documents and words (main content). “M” refers to million and “B” refers to billion. 4.2 Baseline The baseline is a re-implementation of the Hiero system (Chiang, 2007). The phrase pairs that appear only once in the parallel data are discarded because most of them are noisy. We also use the fix-discount method i"
P14-1013,P09-1036,0,0.0149151,"y LDA at the Introduction Making translation decisions is a difficult task in many Statistical Machine Translation (SMT) systems. Current translation modeling approaches usually use context dependent information to disambiguate translation candidates. For example, translation sense disambiguation approaches (Carpuat and Wu, 2005; Carpuat and Wu, 2007) are proposed for phrase-based SMT systems. Meanwhile, for hierarchical phrase-based or syntax-based SMT systems, there is also much work involving rich contexts to guide rule selection (He et al., 2008; Liu et al., 2008; Marton and Resnik, 2008; Xiong et al., 2009). Although these methods are effective and proven successful in many SMT systems, they only leverage within∗ This work was done while the first and fourth authors were visiting Microsoft Research. 133 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 133–143, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics accuracy over a state-of-the-art baseline. sentence level, it is quite difficult to infer the topic accurately with only a few words in the sentence. This makes previous approaches inefficient when appli"
P14-1013,P05-1044,0,0.0214738,"ss consists of a linear layer and a non-linear layer with similar network structures, but different parameters. It transforms the L-dimensional vector g(˜ x) to a V -dimensional vector h(g(˜x)). To minimize reconstruction error with respect to ˜ x, we define the loss function as the L2-norm of the difference between the uncorrupted input and reconstructed input: L(h(g(˜ x)), x) = kh(g(˜ x)) − xk2 Since a parallel sentence pair should have the same topic, our goal is to maximize the similarity score between the source sentence and target sentence. Inspired by the contrastive estimation method (Smith and Eisner, 2005), for each parallel sentence pair hf, ei as a positive instance, we select another sentence pair hf 0 , e0 i from the training data and treat hf, e0 i as a negative instance. To make the similarity of the positive instance larger than the negative instance by some margin η, we utilize the following pairwise ranking loss: (3) Multi-layer neural networks are trained with the standard back-propagation algorithm (Rumelhart et al., 1988). The gradient of the loss function is calculated and back-propagated to the previous layer to update its parameters. Training neural networks involves many factors"
P14-1013,P06-2124,0,0.0322586,"a. By associating each translation rule with the topic representation, topic relevant rules are selected according to the distributional similarity with the source text during SMT decoding. Experimental results show that our method significantly improves translation accuracy in the NIST Chinese-to-English translation task compared to a state-of-the-art baseline. 1 Topic modeling is a useful mechanism for discovering and characterizing various semantic concepts embedded in a collection of documents. Attempts on topic-based translation modeling include topic-specific lexicon translation models (Zhao and Xing, 2006; Zhao and Xing, 2007), topic similarity models for synchronous rules (Xiao et al., 2012), and document-level translation with topic coherence (Xiong and Zhang, 2013). In addition, topic-based approaches have been used in domain adaptation for SMT (Tam et al., 2007; Su et al., 2012), where they view different topics as different domains. One typical property of these approaches in common is that they only utilize parallel data where document boundaries are explicitly given. In this way, the topic of a sentence can be inferred with document-level information using off-the-shelf topic modeling t"
P14-1013,D11-1014,0,0.0446933,"Missing"
P14-1013,P12-1048,0,0.225478,"T Chinese-to-English translation task compared to a state-of-the-art baseline. 1 Topic modeling is a useful mechanism for discovering and characterizing various semantic concepts embedded in a collection of documents. Attempts on topic-based translation modeling include topic-specific lexicon translation models (Zhao and Xing, 2006; Zhao and Xing, 2007), topic similarity models for synchronous rules (Xiao et al., 2012), and document-level translation with topic coherence (Xiong and Zhang, 2013). In addition, topic-based approaches have been used in domain adaptation for SMT (Tam et al., 2007; Su et al., 2012), where they view different topics as different domains. One typical property of these approaches in common is that they only utilize parallel data where document boundaries are explicitly given. In this way, the topic of a sentence can be inferred with document-level information using off-the-shelf topic modeling toolkits such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003) or Hidden Topic Markov Model (HTMM) (Gruber et al., 2007). Most of them also assume that the input must be in document level. However, this situation does not always happen since there is considerable amount of pa"
P14-1140,D13-1106,0,0.373491,"mension of the vector represents a latent aspect of the word, and captures its syntactic and semantic DNN is also introduced to Statistical Machine Translation (SMT) to learn several components or features of conventional framework, including word alignment, language modelling, translation modelling and distortion modelling. Yang et al. (2013) adapt and extend the CD-DNN-HMM (Dahl et al., 2012) method to HMM-based word alignment model. In their work, bilingual word embedding is trained to capture lexical translation information, and surrounding words are utilized to model context information. Auli et al. (2013) propose a joint language and translation model, based on a recurrent neural network. Their model predicts a target word, with an unbounded history of both source and target words. Liu et al. (2013) propose an additive neural network for SMT decoding. Word embedding is used as the input to learn translation confidence score, which is combined with commonly used features in the conventional log-linear model. For distortion modeling, Li et al. (2013) use recursive auto encoders to make full use of the entire merging phrase pairs, going beyond the boundary words with a maximum entropy classifier"
P14-1140,W04-3250,0,0.143032,"Missing"
P14-1140,D13-1054,0,0.251501,"bilingual word embedding is trained to capture lexical translation information, and surrounding words are utilized to model context information. Auli et al. (2013) propose a joint language and translation model, based on a recurrent neural network. Their model predicts a target word, with an unbounded history of both source and target words. Liu et al. (2013) propose an additive neural network for SMT decoding. Word embedding is used as the input to learn translation confidence score, which is combined with commonly used features in the conventional log-linear model. For distortion modeling, Li et al. (2013) use recursive auto encoders to make full use of the entire merging phrase pairs, going beyond the boundary words with a maximum entropy classifier (Xiong et al., 2006). 1491 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1491–1500, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics Different from the work mentioned above, which applies DNN to components of conventional SMT framework, in this paper, we propose a novel R2 NN to model the end-to-end decoding process. R2 NN is a combination of recursive neura"
P14-1140,P06-1096,0,0.0191709,"Missing"
P14-1140,P13-1078,0,0.283964,"atures of conventional framework, including word alignment, language modelling, translation modelling and distortion modelling. Yang et al. (2013) adapt and extend the CD-DNN-HMM (Dahl et al., 2012) method to HMM-based word alignment model. In their work, bilingual word embedding is trained to capture lexical translation information, and surrounding words are utilized to model context information. Auli et al. (2013) propose a joint language and translation model, based on a recurrent neural network. Their model predicts a target word, with an unbounded history of both source and target words. Liu et al. (2013) propose an additive neural network for SMT decoding. Word embedding is used as the input to learn translation confidence score, which is combined with commonly used features in the conventional log-linear model. For distortion modeling, Li et al. (2013) use recursive auto encoders to make full use of the entire merging phrase pairs, going beyond the boundary words with a maximum entropy classifier (Xiong et al., 2006). 1491 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1491–1500, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for"
P14-1140,J04-4002,0,0.0450753,"er combination, according to their plausible scores. coming from France and Russia R2NN coming from France and Russia R2NN coming from Rule Match 来自 laizi France and Russia Rule Match 法国 和 俄罗斯 faguo he eluosi NULL Rule Match 的 de Figure 4: R2 NN for SMT decoding x ˆ[l,n] = x[l,m] ./ s[l,m] ./ x[m,n] ./ s[m,n] [l,n] sj y [l,n] = = f( X X [l,n] i x ˆi wji ) (s[l,n] ./ x[l,n] )j vj (1) (2) (3) j where ./ is a concatenation operator in Equation 1 and Equation 3, and f is a non-linear function, here we use HT anh function, which is defined as: We extract phrase pairs using the conventional method (Och and Ney, 2004). The commonly used features, such as translation score, language model score and distortion score, are used as the recurrent input vector x . During decoding, recurrent input vectors x for internal nodes are calculated accordingly. The difference between our model and the conventional log-linear model includes: • R2 NN is not linear, while the conventional model is a linear combination. (4) • Representations of phrase pairs are automatically learnt to optimize the translation performance, while features used in conventional model are hand-crafted. Figure 4 illustrates the architecture for SMT"
P14-1140,P02-1040,0,0.0905836,"Missing"
P14-1140,P13-1045,0,0.0239661,"ti-task learning framework with DNN for various NLP tasks, including part-of-speech tagging, chunking, named entity recognition, and semantic role labelling. Recurrent neural networks are leveraged to learn language model, and they keep the history information circularly inside the network for arbitrarily long time (Mikolov et al., 2010). Recursive neural networks, which have the ability to generate a tree structured output, are applied to natural language parsing (Socher et al., 2011), and they are extended to recursive neural tensor networks to explore the compositional aspect of semantics (Socher et al., 2013). In this paper, we propose a novel recursive recurrent neural network (R2 NN) to model the end-to-end decoding process for statistical machine translation. R2 NN is a combination of recursive neural network and recurrent neural network, and in turn integrates their respective capabilities: (1) new information can be used to generate the next hidden state, like recurrent neural networks, so that language model and translation model can be integrated naturally; (2) a tree structure can be built, as recursive neural networks, so as to generate the translation candidates in a bottom up manner. A"
P14-1140,J97-3002,0,0.035851,"the Chinese DIALOG set. The training data for monolingual word embedding is Giga-Word corpus version 3 for both Chinese and English. Chinese training corpus contains 32M sentences and 1.1G words. English training data contains 8M sentences and 247M terms. We only train the embedding for the top 100,000 frequent words following (Collobert et al., 2011). With the trained monolingual word embedding, we follow (Yang et al., 2013) to get the bilingual word embedding using the IWSLT bilingual training data. Our baseline decoder is an in-house implementation of Bracketing Transduction Grammar (BTG) (Wu, 1997) in CKY-style decoding with a lexical reordering model trained with maximum entropy (Xiong et al., 2006). The features of the baseline are commonly used features as standard BTG decoder, such as translation probabilities, lexical weights, language model, word penalty and distortion probabilities. All these commonly used features are used as recurrent input vector x in our R2 NN. 6.2 Translation Results As we mentioned in Section 5, constructing phrase pair embeddings from word embeddings may be not suitable. Here we conduct experiments to ver1497 ify it. We first train the source and target wo"
P14-1140,P10-1049,0,0.0100658,"parameters W and V . The loss function is the commonly used ranking loss with a margin, and it is defined as follows: [l,n] [l,n] LSLT (W, V, s[l,n] ) = max(0, 1 − yoracle + yt ) (6) [l,n] where s[l,n] is the source span. yoracle is the plausible score of a oracle translation result. [l,n] yt is the plausible score for the best translation candidate given the model parameters W and V . The loss function aims to learn a model which assigns the good translation candidate (the oracle candidate) higher score than the bad ones, with a margin 1. Translation candidates generated by forced decoding (Wuebker et al., 2010) are used as oracle translations, which are the positive samples. Forced decoding performs sentence pair segmentation using the same translation system as decoding. For each sentence pair in the training data, SMT decoder is applied to the source side, and any candidate which is not the partial sub-string of the target sentence is removed from the n-best list during decoding. From the forced decoding result, we can get the ideal derivation tree in the decoder’s search space, and extract positive/oracle translation candidates. 1495 4.3 Supervised Global Training The supervised local training us"
P14-1140,P06-1066,0,0.231824,"propose a joint language and translation model, based on a recurrent neural network. Their model predicts a target word, with an unbounded history of both source and target words. Liu et al. (2013) propose an additive neural network for SMT decoding. Word embedding is used as the input to learn translation confidence score, which is combined with commonly used features in the conventional log-linear model. For distortion modeling, Li et al. (2013) use recursive auto encoders to make full use of the entire merging phrase pairs, going beyond the boundary words with a maximum entropy classifier (Xiong et al., 2006). 1491 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1491–1500, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics Different from the work mentioned above, which applies DNN to components of conventional SMT framework, in this paper, we propose a novel R2 NN to model the end-to-end decoding process. R2 NN is a combination of recursive neural network and recurrent neural network. In R2 NN, new information can be used to generate the next hidden state, like recurrent neural networks, and a tree structure ca"
P14-1140,P13-1017,1,0.829763,"kthrough results (Kavukcuoglu et al., 2010; Krizhevsky et al., 2012; Dahl et al., 2012). Applying DNN to natural language processing (NLP), representation or embedding of words is usually learnt first. Word embedding is a dense, low dimensional, real-valued vector. Each dimension of the vector represents a latent aspect of the word, and captures its syntactic and semantic DNN is also introduced to Statistical Machine Translation (SMT) to learn several components or features of conventional framework, including word alignment, language modelling, translation modelling and distortion modelling. Yang et al. (2013) adapt and extend the CD-DNN-HMM (Dahl et al., 2012) method to HMM-based word alignment model. In their work, bilingual word embedding is trained to capture lexical translation information, and surrounding words are utilized to model context information. Auli et al. (2013) propose a joint language and translation model, based on a recurrent neural network. Their model predicts a target word, with an unbounded history of both source and target words. Liu et al. (2013) propose an additive neural network for SMT decoding. Word embedding is used as the input to learn translation confidence score,"
P14-1140,D13-1112,0,0.0238646,"obal training is proposed to tune the model according to the final translation performance of the whole source sentence. Actually, we can update the model from the root of the decoding tree and perform back propagation along the tree structure. Due to the inexact search nature of SMT decoding, search errors may inevitably break theoretical properties, and the final translation results may be not suitable for model training. To handle this problem, we use early update strategy for the supervised global training. Early update is testified to be useful for SMT training with large scale features (Yu et al., 2013). Instead of updating the model using the final translation results, early update approach optimizes the model, when the oracle translation candidate is pruned from the n-best list, meaning that, the model is updated once it performs a unrecoverable mistake. Back propagation is performed along the tree structure, and the phrase pair embeddings of the leaf nodess are updated. The loss function for supervised global training is defined as follows: words, but bilingual corpus is much more difficult to acquire, compared with monolingual corpus. Embedding Word Word Pair Phrase Pair #Data 1G 7M 7M #"
P16-1212,P14-1091,1,0.746378,"er to compress the variable-sized source sentence into a fixed-length semantic vector, an encoder RNN reads the words in source sentence and generate a hidden state, based on which another decoder RNN is used to generate target sentence. Different from our work using a semantic space defined by knowledge base, the hidden state connecting the source and target RNNs is a vector of implicit and inexplicable real numbers. Learning the semantic information from a sentence, which is also called semantic grounding, is widely used for question answering tasks (Liang et al., 2011; Berant et al., 2013; Bao et al., 2014; Berant and Liang, 2014). In (Yih et al., 2015), with a deep convolutional neural network (CNN), the question sentence is mapped into a query graph, based on which the answer is searched in knowledge base. In our paper, we use RNN to encode the sentence to do fair comparison with the encoderdecoder framework. We can try using CNN to replace RNN as the encoder in the future. To generate a sentence from a semantic vector, Wen et al. (2015) proposed a LSTM-based natural language generator controlled by a semantic vector. The semantic vector memorizes what information should be generated for LSTM"
P16-1212,P14-1133,0,0.0178386,"variable-sized source sentence into a fixed-length semantic vector, an encoder RNN reads the words in source sentence and generate a hidden state, based on which another decoder RNN is used to generate target sentence. Different from our work using a semantic space defined by knowledge base, the hidden state connecting the source and target RNNs is a vector of implicit and inexplicable real numbers. Learning the semantic information from a sentence, which is also called semantic grounding, is widely used for question answering tasks (Liang et al., 2011; Berant et al., 2013; Bao et al., 2014; Berant and Liang, 2014). In (Yih et al., 2015), with a deep convolutional neural network (CNN), the question sentence is mapped into a query graph, based on which the answer is searched in knowledge base. In our paper, we use RNN to encode the sentence to do fair comparison with the encoderdecoder framework. We can try using CNN to replace RNN as the encoder in the future. To generate a sentence from a semantic vector, Wen et al. (2015) proposed a LSTM-based natural language generator controlled by a semantic vector. The semantic vector memorizes what information should be generated for LSTM, and it varies along wit"
P16-1212,D13-1160,0,0.016404,"der framework. In order to compress the variable-sized source sentence into a fixed-length semantic vector, an encoder RNN reads the words in source sentence and generate a hidden state, based on which another decoder RNN is used to generate target sentence. Different from our work using a semantic space defined by knowledge base, the hidden state connecting the source and target RNNs is a vector of implicit and inexplicable real numbers. Learning the semantic information from a sentence, which is also called semantic grounding, is widely used for question answering tasks (Liang et al., 2011; Berant et al., 2013; Bao et al., 2014; Berant and Liang, 2014). In (Yih et al., 2015), with a deep convolutional neural network (CNN), the question sentence is mapped into a query graph, based on which the answer is searched in knowledge base. In our paper, we use RNN to encode the sentence to do fair comparison with the encoderdecoder framework. We can try using CNN to replace RNN as the encoder in the future. To generate a sentence from a semantic vector, Wen et al. (2015) proposed a LSTM-based natural language generator controlled by a semantic vector. The semantic vector memorizes what information should be"
P16-1212,D14-1179,0,0.00526158,"Missing"
P16-1212,N03-1017,0,0.0311904,"Target Generation can generate a natural language sentence based on the existing semantic tuples; • Combining them, KBSE can be used to translation a source sentence into another language with a semantic space defined by a given knowledge base. 3 Experiments To evaluate our proposed KBSE model, in this section, we conduct experiments on two Chineseto-English translation tasks. One is from electric business domain, and the other is from movie domain. 3.1 Baseline and Comparison Systems We select two baseline systems. The first one is an in-house implementation of hierarchical phrasebased SMT (Koehn et al., 2003; Chiang, 2007) with traditional features, which achieves a similar performance to the state-of-the-art phrase-based decoder in Moses 1 (Koehn et al., 2007). The 4gram language model is trained with target sentences from training set plus the Gigaword corpus 2 . Our phrase-based system is trained with MERT (Och, 2003). The other system is the encoderdecoder system (van Merri¨enboer et al., 2015) 3 , based on which our KBSE is implemented. We also combine KBSE with encoder-decoder system, by adding the knowledge-based semantic embedding to be another context vector. Hence, for the decoder there"
P16-1212,P07-2045,0,0.0586397,"ce sentence into another language with a semantic space defined by a given knowledge base. 3 Experiments To evaluate our proposed KBSE model, in this section, we conduct experiments on two Chineseto-English translation tasks. One is from electric business domain, and the other is from movie domain. 3.1 Baseline and Comparison Systems We select two baseline systems. The first one is an in-house implementation of hierarchical phrasebased SMT (Koehn et al., 2003; Chiang, 2007) with traditional features, which achieves a similar performance to the state-of-the-art phrase-based decoder in Moses 1 (Koehn et al., 2007). The 4gram language model is trained with target sentences from training set plus the Gigaword corpus 2 . Our phrase-based system is trained with MERT (Och, 2003). The other system is the encoderdecoder system (van Merri¨enboer et al., 2015) 3 , based on which our KBSE is implemented. We also combine KBSE with encoder-decoder system, by adding the knowledge-based semantic embedding to be another context vector. Hence, for the decoder there are two context vectors, one from the encoder and the other is generated by the Semantic Grounding part. We call this model Enc-Dec+KBSE. For our proposed"
P16-1212,P11-1060,0,0.0109095,"d on an encoder-decoder framework. In order to compress the variable-sized source sentence into a fixed-length semantic vector, an encoder RNN reads the words in source sentence and generate a hidden state, based on which another decoder RNN is used to generate target sentence. Different from our work using a semantic space defined by knowledge base, the hidden state connecting the source and target RNNs is a vector of implicit and inexplicable real numbers. Learning the semantic information from a sentence, which is also called semantic grounding, is widely used for question answering tasks (Liang et al., 2011; Berant et al., 2013; Bao et al., 2014; Berant and Liang, 2014). In (Yih et al., 2015), with a deep convolutional neural network (CNN), the question sentence is mapped into a query graph, based on which the answer is searched in knowledge base. In our paper, we use RNN to encode the sentence to do fair comparison with the encoderdecoder framework. We can try using CNN to replace RNN as the encoder in the future. To generate a sentence from a semantic vector, Wen et al. (2015) proposed a LSTM-based natural language generator controlled by a semantic vector. The semantic vector memorizes what i"
P16-1212,P13-1078,0,0.0244095,"es for number of objects, some results of our KBSE generate the entities in wrong plurality form. Since our KBSE consists of two separate parts, the Source Grounding part and the Target Generation part, the errors generated in the first part cannot be corrected in the following process. As we mentioned in Section 3.3.1, combining KBSE with encoder-decoder can alleviate these two problems, by preserving information not captured and correct the errors generated in source grounding part. 4 Related Work Unlike previous works using neural network to learn features for traditional log-linear model (Liu et al., 2013; Liu et al., 2014), Sutskever et al. (2014) introduced a general end-to-end approach based on an encoder-decoder framework. In order to compress the variable-sized source sentence into a fixed-length semantic vector, an encoder RNN reads the words in source sentence and generate a hidden state, based on which another decoder RNN is used to generate target sentence. Different from our work using a semantic space defined by knowledge base, the hidden state connecting the source and target RNNs is a vector of implicit and inexplicable real numbers. Learning the semantic information from a senten"
P16-1212,P14-1140,1,0.808281,"bjects, some results of our KBSE generate the entities in wrong plurality form. Since our KBSE consists of two separate parts, the Source Grounding part and the Target Generation part, the errors generated in the first part cannot be corrected in the following process. As we mentioned in Section 3.3.1, combining KBSE with encoder-decoder can alleviate these two problems, by preserving information not captured and correct the errors generated in source grounding part. 4 Related Work Unlike previous works using neural network to learn features for traditional log-linear model (Liu et al., 2013; Liu et al., 2014), Sutskever et al. (2014) introduced a general end-to-end approach based on an encoder-decoder framework. In order to compress the variable-sized source sentence into a fixed-length semantic vector, an encoder RNN reads the words in source sentence and generate a hidden state, based on which another decoder RNN is used to generate target sentence. Different from our work using a semantic space defined by knowledge base, the hidden state connecting the source and target RNNs is a vector of implicit and inexplicable real numbers. Learning the semantic information from a sentence, which is also c"
P16-1212,P03-1021,0,0.0453806,"periments on two Chineseto-English translation tasks. One is from electric business domain, and the other is from movie domain. 3.1 Baseline and Comparison Systems We select two baseline systems. The first one is an in-house implementation of hierarchical phrasebased SMT (Koehn et al., 2003; Chiang, 2007) with traditional features, which achieves a similar performance to the state-of-the-art phrase-based decoder in Moses 1 (Koehn et al., 2007). The 4gram language model is trained with target sentences from training set plus the Gigaword corpus 2 . Our phrase-based system is trained with MERT (Och, 2003). The other system is the encoderdecoder system (van Merri¨enboer et al., 2015) 3 , based on which our KBSE is implemented. We also combine KBSE with encoder-decoder system, by adding the knowledge-based semantic embedding to be another context vector. Hence, for the decoder there are two context vectors, one from the encoder and the other is generated by the Semantic Grounding part. We call this model Enc-Dec+KBSE. For our proposed KBSE, the number of hidden units in both parts are 300. Embedding size of both source and target are 200. Adadelta (Zeiler, 2012) 1 http://www.statmt.org/moses/ ht"
P16-1212,P02-1040,0,0.100737,"Missing"
P16-1212,D15-1199,0,0.0201843,"Missing"
P16-1212,P15-1128,0,0.00689599,"ce into a fixed-length semantic vector, an encoder RNN reads the words in source sentence and generate a hidden state, based on which another decoder RNN is used to generate target sentence. Different from our work using a semantic space defined by knowledge base, the hidden state connecting the source and target RNNs is a vector of implicit and inexplicable real numbers. Learning the semantic information from a sentence, which is also called semantic grounding, is widely used for question answering tasks (Liang et al., 2011; Berant et al., 2013; Bao et al., 2014; Berant and Liang, 2014). In (Yih et al., 2015), with a deep convolutional neural network (CNN), the question sentence is mapped into a query graph, based on which the answer is searched in knowledge base. In our paper, we use RNN to encode the sentence to do fair comparison with the encoderdecoder framework. We can try using CNN to replace RNN as the encoder in the future. To generate a sentence from a semantic vector, Wen et al. (2015) proposed a LSTM-based natural language generator controlled by a semantic vector. The semantic vector memorizes what information should be generated for LSTM, and it varies along with the sentence generate"
P16-1212,J07-2003,0,\N,Missing
P17-1065,P05-1033,0,0.559021,"Missing"
P17-1065,C16-2064,0,0.0395317,"Missing"
P17-1065,W16-4616,0,0.0282882,"Ensemble) RNNsearch SD-NMT BLEU 18.72 18.45 20.36 22.86 24.71 26.22 23.50 25.93 RIBES 0.6511 0.6451 0.6782 0.7508 0.7566 0.7459 0.7540 System Description Moses’ Hierarchical Phrase-based SMT Moses’ Phrase-based SMT Moses’ String-to-Tree Syntax-based SMT Single-layer NMT model without ensemble Self-ensemble of 2-layer NMT model Ensemble of 4 single-layer NMT models Single-layer NMT model Single-layer SD-NMT model Table 2: Evaluation results on Japanese-English translation task. model learns reasonable inferences of parse trees which begins to help target word generation and leads to lower PPL. Cromieres et al., 2016) that are the competitive NMT results on WAT 2016. According to Table 2, NMT results still outperform SMT results similar to our Chinese-English evaluation results. The SD-NMT model significantly outperforms most other NMT models, which shows that our proposed approach to modeling target dependency tree benefit NMT systems since our RNNsearch baseline achieves comparable performance with the single layer attention-based NMT system in (Cromieres, 2016). Note that our SD-NMT gets comparable results with the 4 single-layer ensemble model in (Cromieres, 2016; Cromieres et al., 2016). We believe SD"
P17-1065,P15-1033,0,0.0124,"Missing"
P17-1065,N16-1024,0,0.224787,"ve two RNNs in our model, the termination condition is also different from a conventional NMT model. In decoding, we maintain a stack to track the parsing configuration, and our model terminates once the Word-RNN predicts a special ending symbol EOS and all the words in the stack have been reduced. Figure 3 (a) gives an overview of our SD-NMT model. Due to space limitation, the detailed interconnections between two RNNs are only illustrated at timestamp j. The encoder of our model 3.1 (10) Syntactic Context for Target Word Prediction Syntax has been proven useful for sentence generation task (Dyer et al., 2016). We propose to leverage target syntax to help translation generation. In our model, the syntactic context Kj at timestamp j is defined as a vector which is computed by a feed-forward network based on current 701 stamp for each training instance. Thus it is easy for the model to process multiple trees in one batch. In the decoding process of an SD-NMT model, the score of each search path is the sum of log probabilities of target word sequence and transition action sequence normalized by the sequence length: parsing configuration of Action-RNN. Denote that w0 and w1 are two topmost words in the"
P17-1065,W16-4610,0,0.0122509,"n has been extensively studied in Statistic Machine Translation (SMT) (Galley et al., 2006; Shen et al., 2008; Liu et al., 2006). Liu et al. (2006) proposed a tree-to-string alignment template for SMT to leverage source side syntactic information. Shen et al. (2008) proposed a target dependency language model for SMT to employ target-side structured information. These methods show promising improvement for SMT. Recently, neural machine translation (NMT) has achieved better performance than SMT in many language pairs (Luong et al., 2015a; Zhang et al., 2016; Shen et al., 2016; Wu et al., 2016; Neubig, 2016). In a vanilla NMT model, source and target sentences are treated as sequences where the syntactic knowledge of both sides is neglected. Some effort has been done to incorporate source syntax into NMT. Eriguchi et al. (2016) proposed a tree-to-sequence attentional NMT model where source-side parse tree was used and achieved promising improvement. Intuitively, adding source syntactic information to Translation Example In this section, we give a case study to explain how our method works. Figure 6 shows a translation example from the NIST testsets. SMT and RNNsearch refer to the translation resu"
P17-1065,P16-1078,0,0.228746,", Nan Yang‡ , Mu Li‡ , Ming Zhou‡ † Harbin Institute of Technology, Harbin, China ‡ Microsoft Research {v-shuawu, dozhang, nanya, muli, mingzhou}@microsoft.com Abstract Many methods have been proposed to further improve the sequence-to-sequence NMT model since it was first proposed by Sutskever et al. (2014) and Bahdanau et al. (2015). Previous work ranges from addressing the problem of out-ofvocabulary words (Jean et al., 2015), designing attention mechanism (Luong et al., 2015a), to more efficient parameter learning (Shen et al., 2016), using source-side syntactic trees for better encoding (Eriguchi et al., 2016) and so on. All these NMT models employ a sequential recurrent neural network for target generations. Although in theory RNN is able to remember sufficiently long history, we still observe substantial incorrect translations which violate long-distance syntactic constraints. This suggests that it is still very challenging for a linear RNN to learn models that effectively capture many subtle long-range word dependencies. For example, Figure 1 shows an incorrect translation related to the long-distance dependency. The translation fragment in italic is locally fluent around the word is, but from a"
P17-1065,W04-0308,0,0.0351929,"Machine Translation (SD-NMT) model in our paper. An SD-NMT model encodes source inputs with bi-directional RNNs and associates them with target word prediction via attention mechanism as in most NMT models, but it comes with a new decoder which is able to jointly generate target translations and construct their syntactic dependency trees. The key difference from conventional NMT decoders is that we use two RNNs, one for translation generation and the other for dependency parse tree construction, in which incremental parsing is performed with the arc-standard shift-reduce algorithm proposed by Nivre (2004). P (Y |X) = n Y j=1 P (yj |y<j , H) (1) Typically, for the jth target word, the probability P (yj |y<j , H) is computed as P (yj |y<j , H) = g(sj , yj−1 , cj ) (2) where g is a nonlinear function that outputs the probability of yj , and sj is the RNN hidden state. The context cj is calculated at each timestamp j based on H by the attention network cj = m X ajk hk (3) k=1 ajk = exp(ejk ) Pm i=1 exp(eji ) ejk = vaT tanh(Wa sj−1 + Ua hk ) (4) (5) where va , Wa , Ua are the weight matrices. The attention mechanism is effective to model the correspondences between source and target. 699 2.2 Depend"
P17-1065,P06-1121,0,0.0266136,"Missing"
P17-1065,P02-1040,0,0.128454,"Missing"
P17-1065,P08-1066,0,0.0170142,"slation should be mostly affected by the distant plural noun foreigners rather than words Venezuelan government nearby. Fortunately, such long-distance word correspondence can be well addressed and modeled by syntactic dependency trees. In Figure 1, the head word foreigners in the partial dependency tree (top dashed box) can provide correct structural context for the next target word, with this information it is more likely to generate the correct word will rather than is. This structure has been successfully applied to significantly improve the performance of statistical machine translation (Shen et al., 2008). On the NMT side, introducing target syntactic structures could help solve the problem of ungrammatical output because it can bring two advantages over state-of-the-art NMT models: Nowadays a typical Neural Machine Translation (NMT) model generates translations from left to right as a linear sequence, during which latent syntactic structures of the target sentences are not explicitly concerned. Inspired by the success of using syntactic knowledge of target language for improving statistical machine translation, in this paper we propose a novel Sequence-to-Dependency Neural Machine Translation"
P17-1065,D10-1092,0,0.0354514,"Missing"
P17-1065,P16-1159,0,0.0688833,"e-to-Dependency Neural Machine Translation Shuangzhi Wu†∗, Dongdong Zhang‡ , Nan Yang‡ , Mu Li‡ , Ming Zhou‡ † Harbin Institute of Technology, Harbin, China ‡ Microsoft Research {v-shuawu, dozhang, nanya, muli, mingzhou}@microsoft.com Abstract Many methods have been proposed to further improve the sequence-to-sequence NMT model since it was first proposed by Sutskever et al. (2014) and Bahdanau et al. (2015). Previous work ranges from addressing the problem of out-ofvocabulary words (Jean et al., 2015), designing attention mechanism (Luong et al., 2015a), to more efficient parameter learning (Shen et al., 2016), using source-side syntactic trees for better encoding (Eriguchi et al., 2016) and so on. All these NMT models employ a sequential recurrent neural network for target generations. Although in theory RNN is able to remember sufficiently long history, we still observe substantial incorrect translations which violate long-distance syntactic constraints. This suggests that it is still very challenging for a linear RNN to learn models that effectively capture many subtle long-range word dependencies. For example, Figure 1 shows an incorrect translation related to the long-distance dependency. The"
P17-1065,P15-1001,0,0.0266306,"Missing"
P17-1065,P16-1008,0,0.0188276,"the target word sequence and its corresponding dependency structure are jointly constructed and modeled, and this structure is used as context to facilitate word generations. Experimental results show that the proposed method significantly outperforms state-of-the-art baselines on Chinese-English and JapaneseEnglish translation tasks. 1 Introduction Recently, Neural Machine Translation (NMT) with the attention-based encoder-decoder framework (Bahdanau et al., 2015) has achieved significant improvements in translation quality of many language pairs (Bahdanau et al., 2015; Luong et al., 2015a; Tu et al., 2016; Wu et al., 2016). In a conventional NMT model, an encoder reads in source sentences of various lengths, and transforms them into sequences of intermediate hidden vector representations. After weighted by attention operations, combined hidden vectors are used by the decoder to generate translations. In most of cases, both encoder and decoder are implemented as recurrent neural networks (RNNs). ∗ Contribution during internship at Microsoft Research. 698 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 698–707 c Vancouver, Canada, July 30 - August 4"
P17-1065,1983.tc-1.13,0,0.455856,"Missing"
P17-1065,W04-3250,0,0.020876,"Missing"
P17-1065,P06-1077,0,0.341955,"Missing"
P17-1065,D15-1166,0,0.116367,"NMT) method, in which the target word sequence and its corresponding dependency structure are jointly constructed and modeled, and this structure is used as context to facilitate word generations. Experimental results show that the proposed method significantly outperforms state-of-the-art baselines on Chinese-English and JapaneseEnglish translation tasks. 1 Introduction Recently, Neural Machine Translation (NMT) with the attention-based encoder-decoder framework (Bahdanau et al., 2015) has achieved significant improvements in translation quality of many language pairs (Bahdanau et al., 2015; Luong et al., 2015a; Tu et al., 2016; Wu et al., 2016). In a conventional NMT model, an encoder reads in source sentences of various lengths, and transforms them into sequences of intermediate hidden vector representations. After weighted by attention operations, combined hidden vectors are used by the decoder to generate translations. In most of cases, both encoder and decoder are implemented as recurrent neural networks (RNNs). ∗ Contribution during internship at Microsoft Research. 698 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 698–707 c Vancouver, Canada,"
P17-1065,D16-1050,0,0.0111325,"corporating linguistic knowledge into machine translation has been extensively studied in Statistic Machine Translation (SMT) (Galley et al., 2006; Shen et al., 2008; Liu et al., 2006). Liu et al. (2006) proposed a tree-to-string alignment template for SMT to leverage source side syntactic information. Shen et al. (2008) proposed a target dependency language model for SMT to employ target-side structured information. These methods show promising improvement for SMT. Recently, neural machine translation (NMT) has achieved better performance than SMT in many language pairs (Luong et al., 2015a; Zhang et al., 2016; Shen et al., 2016; Wu et al., 2016; Neubig, 2016). In a vanilla NMT model, source and target sentences are treated as sequences where the syntactic knowledge of both sides is neglected. Some effort has been done to incorporate source syntax into NMT. Eriguchi et al. (2016) proposed a tree-to-sequence attentional NMT model where source-side parse tree was used and achieved promising improvement. Intuitively, adding source syntactic information to Translation Example In this section, we give a case study to explain how our method works. Figure 6 shows a translation example from the NIST testse"
P17-1065,P15-1002,0,0.0265242,"Missing"
P17-1065,D08-1059,0,0.0126196,"uence length: parsing configuration of Action-RNN. Denote that w0 and w1 are two topmost words in the stack, w0l and w1l are their leftmost modifiers in the partial tree, w0r and w1r their rightmost modifiers respectively. We define two unigram features and four bigram features. The unigram features are w0 and w1 which are represented by the word embedding vectors. The bigram features are w0 w0l , w0 w0r , w1 w1l and w1 w1r . Each of them is computed by bhc = tanh(Wb Ewh + Ub Ewhc ), h ∈ {0, 1}, c ∈ {l, r}. These kinds of feature template have beeb proven effective in dependency parsing task (Zhang and Clark, 2008). Based on these features, the syntactic context vector Kj is computed as l score = j=1 l 1X δ(SH, aj ) log P (ˆ yj |ˆ y<j , X, A≤j ) n where n is word sequence length and l is action sequence length. 4 where Wk , Uk , Wb , Ub are the weight matrices, E stands for the embedding matrix. Figure 2 (b) gives an overview of the construction of Kj . Note that zero vector is used for padding the words which are not available in the partial tree, so that all the K vectors have the same input size in computation. Adding Kj to Equation 2, the probabilities of transition action and word in Equation 7 and"
P17-1065,P11-2033,0,\N,Missing
P17-1065,P16-5005,0,\N,Missing
P17-1174,W14-4012,0,0.135364,"Missing"
P17-1174,D14-1179,0,0.139136,"Missing"
P17-1174,W16-4616,0,0.0627963,"Missing"
P17-1174,W16-4617,0,0.281753,"re the hierarchical structure, we start from a hierarchical RNN that consists of a chunk-level decoder and a word-level decoder (Model 1). Then, we improve the word-level decoder by introducing inter-chunk connections to capture the interaction between chunks (Model 2). Finally, we introduce a feedback mechanism to the chunk-level decoder to enhance the memory capacity of previous outputs (Model 3). We evaluate the three models on the WAT ’16 English-to-Japanese translation task (§ 4). The experimental results show that our best model outperforms the best single NMT model reported in WAT ’16 (Eriguchi et al., 2016b). Our contributions are twofold: (1) chunk information is introduced into NMT to improve translation performance, and (2) a novel hierarchical decoder is devised to model the properties of chunk structure in the encoder-decoder framework. 2 Preliminaries: Attention-based Neural Machine Translation In this section, we briefly introduce the architecture of the attention-based NMT model (Bahdanau et al., 2015), which is the basis of our proposed models. 2.1 Neural Machine Translation An NMT model usually consists of two connected neural networks: an encoder and a decoder. After the encoder maps"
P17-1174,P16-1078,0,0.444205,"re the hierarchical structure, we start from a hierarchical RNN that consists of a chunk-level decoder and a word-level decoder (Model 1). Then, we improve the word-level decoder by introducing inter-chunk connections to capture the interaction between chunks (Model 2). Finally, we introduce a feedback mechanism to the chunk-level decoder to enhance the memory capacity of previous outputs (Model 3). We evaluate the three models on the WAT ’16 English-to-Japanese translation task (§ 4). The experimental results show that our best model outperforms the best single NMT model reported in WAT ’16 (Eriguchi et al., 2016b). Our contributions are twofold: (1) chunk information is introduced into NMT to improve translation performance, and (2) a novel hierarchical decoder is devised to model the properties of chunk structure in the encoder-decoder framework. 2 Preliminaries: Attention-based Neural Machine Translation In this section, we briefly introduce the architecture of the attention-based NMT model (Bahdanau et al., 2015), which is the basis of our proposed models. 2.1 Neural Machine Translation An NMT model usually consists of two connected neural networks: an encoder and a decoder. After the encoder maps"
P17-1174,W08-0509,0,0.0958144,"Missing"
P17-1174,D10-1092,0,0.0822748,"Missing"
P17-1174,D13-1176,0,0.543806,"roperties of chunk structure in the encoder-decoder framework. 2 Preliminaries: Attention-based Neural Machine Translation In this section, we briefly introduce the architecture of the attention-based NMT model (Bahdanau et al., 2015), which is the basis of our proposed models. 2.1 Neural Machine Translation An NMT model usually consists of two connected neural networks: an encoder and a decoder. After the encoder maps a source sentence into a fixed-length vector, the decoder maps the vector into a target sentence. The implementation of the encoder can be a convolutional neural network (CNN) (Kalchbrenner and Blunsom, 2013), a long short-term memory (LSTM) (Sutskever et al., 2014; Luong and Manning, 2016), a gated recurrent unit (GRU) (Cho et al., 2014b; Bahdanau et al., 2015), or a Tree-LSTM (Eriguchi et al., 2016b). While various architectures are leveraged as an encoder to capture the structural information in the source language, most of the NMT models rely on a standard sequential network such as LSTM or GRU as the decoder. Following (Bahdanau et al., 2015), we use GRU as the recurrent unit in this paper. A GRU unit computes its hidden state vector hi given an input vector xi and the previous hidden state h"
P17-1174,2010.eamt-1.27,0,0.242941,"re serious in free word-order languages such as Czech, German, Japanese, and Turkish. In the case of the example in Figure 1, the order of the phrase “早く (early)” and the phrase “家へ (to home)” is flexible. This means that simply memorizing the word order in training data is not enough to train a model that can assign a high probability to a correct sentence regardless of its word order. In the past, chunks (or phrases) were utilized to handle the above problems in statistical machine translation (SMT) (Watanabe et al., 2003; Koehn et al., 2003) and in example-based machine translation (EBMT) (Kim et al., 2010). By using a chunk rather than a word as the basic translation unit, one can treat a sentence as a shorter sequence. This makes it easy to capture the longer dependencies in a target sentence. The order of words in a chunk is relatively fixed while that in a sentence is much more flexible. Thus, modeling intra-chunk (local) word orders and inter-chunk (global) dependencies independently can help capture the difference of the flexibility between the word order and the chunk order in free word-order languages. In this paper, we refine the original RNN decoder to consider chunk information in NMT"
P17-1174,N03-1017,0,0.138863,"Missing"
P17-1174,P15-1107,0,0.0436534,"Missing"
P17-1174,D15-1106,1,0.817883,"tructures of the source language. In contrast, our work focuses on the decoding process to capture the structure of the target language. The encoders described above and our proposed decoders are complementary so they can be combined into a single network. Considering that our Model 1 described in § 3.1 can be seen as a hierarchical RNN, our work is also related to previous studies that utilize multi-layer RNN s to capture hierarchical structures in data. Hierarchical RNNs are used for various NLP tasks such as machine translation (Luong and Manning, 2016), document modeling (Li et al., 2015; Lin et al., 2015), dialog generation (Serban et al., 2017), image captioning (Krause et al., 2016), and video captioning (Yu et al., 2016). In particular, Li et al. (2015) and Luong and Manning (2016) use hierarchical encoder-decoder models, but not for the purpose of learning syntactic structures of target sentences. Li et al. (2015) build hierarchical models at the sentence-word level to obtain better document representations. Luong and Manning (2016) build the word-character level to cope with the out-of-vocabulary problem. In contrast, we build a hierarchical models at the chunk-word level to explicitly ca"
P17-1174,P02-1040,0,0.098266,"Missing"
P17-1174,P16-1100,0,0.223268,"based Neural Machine Translation In this section, we briefly introduce the architecture of the attention-based NMT model (Bahdanau et al., 2015), which is the basis of our proposed models. 2.1 Neural Machine Translation An NMT model usually consists of two connected neural networks: an encoder and a decoder. After the encoder maps a source sentence into a fixed-length vector, the decoder maps the vector into a target sentence. The implementation of the encoder can be a convolutional neural network (CNN) (Kalchbrenner and Blunsom, 2013), a long short-term memory (LSTM) (Sutskever et al., 2014; Luong and Manning, 2016), a gated recurrent unit (GRU) (Cho et al., 2014b; Bahdanau et al., 2015), or a Tree-LSTM (Eriguchi et al., 2016b). While various architectures are leveraged as an encoder to capture the structural information in the source language, most of the NMT models rely on a standard sequential network such as LSTM or GRU as the decoder. Following (Bahdanau et al., 2015), we use GRU as the recurrent unit in this paper. A GRU unit computes its hidden state vector hi given an input vector xi and the previous hidden state hi−1 : hi = GRU(hi−1 , xi ). (1) The function GRU(·) is calculated as ri = σ(Wr xi +"
P17-1174,P15-1002,0,0.0264998,"ncoder-decoder models, but not for the purpose of learning syntactic structures of target sentences. Li et al. (2015) build hierarchical models at the sentence-word level to obtain better document representations. Luong and Manning (2016) build the word-character level to cope with the out-of-vocabulary problem. In contrast, we build a hierarchical models at the chunk-word level to explicitly capture the syntactic structure based on chunk segmentation. In addition, the architecture of Model 3 is also related to stacked RNN, which has shown to be effective in improving the translation quality (Luong et al., 2015a; Sutskever et al., 2014). Although these architectures look similar to each other, there is a fundamental difference between the directions of the connection between two layers. A stacked RNN consists of multiple RNN layers that are connected from the input side to the output side at every time step. In contrast, our Model 3 has a different connection at each time step. Before it generates a chunk, there is a feed-forward connection from the chunk-level decoder to the word-level decoder. However, after generating a chunk representation, the connection is to be reversed to feed back the infor"
P17-1174,D15-1166,0,0.0747669,"ncoder-decoder models, but not for the purpose of learning syntactic structures of target sentences. Li et al. (2015) build hierarchical models at the sentence-word level to obtain better document representations. Luong and Manning (2016) build the word-character level to cope with the out-of-vocabulary problem. In contrast, we build a hierarchical models at the chunk-word level to explicitly capture the syntactic structure based on chunk segmentation. In addition, the architecture of Model 3 is also related to stacked RNN, which has shown to be effective in improving the translation quality (Luong et al., 2015a; Sutskever et al., 2014). Although these architectures look similar to each other, there is a fundamental difference between the directions of the connection between two layers. A stacked RNN consists of multiple RNN layers that are connected from the input side to the output side at every time step. In contrast, our Model 3 has a different connection at each time step. Before it generates a chunk, there is a feed-forward connection from the chunk-level decoder to the word-level decoder. However, after generating a chunk representation, the connection is to be reversed to feed back the infor"
P17-1174,C00-1082,0,0.0179728,"Missing"
P17-1174,W16-4610,0,0.0340474,"Missing"
P17-1174,W15-5003,0,0.0366542,"Missing"
P17-1174,P11-2093,0,0.0894041,"Missing"
P17-1174,J03-1002,0,0.017893,"Missing"
P17-1174,W16-2323,0,0.0517383,"Missing"
P17-1174,P15-1150,0,0.0592716,"Missing"
P17-1174,P03-1039,0,0.041371,"Missing"
P17-1174,D09-1160,1,0.802151,"Missing"
P17-1174,C10-1140,1,0.849059,"Missing"
P17-1174,C14-1103,1,0.828074,"Missing"
P17-1174,P17-2092,0,0.0515711,"trast, our Model 3 has a different connection at each time step. Before it generates a chunk, there is a feed-forward connection from the chunk-level decoder to the word-level decoder. However, after generating a chunk representation, the connection is to be reversed to feed back the information from the word-level decoder to the chunk-level decoder. By switching the connections between two layers, our model can capture the chunk structure explicitly. This is the first work that proposes decoders for NMT that can capture plausible linguistic structures such as chunk. Finally, we noticed that (Zhou et al., 2017) (which is accepted at the same time as this paper) have also proposed a chunk-based decoder for NMT . Their good experimental result on Chinese to English translation task also indicates the effectiveness of “chunk-by-chunk” decoders. Although their architecture is similar to our Model 2, there are several differences: (1) they adopt chunk-level attention instead of word-level attention; (2) their model predicts chunk tags (such as noun phrase), while ours only predicts chunk boundaries; and (3) they employ a boundary gate to decide the chunk boundaries, while we do that by simply having the"
P17-1174,P07-2045,0,\N,Missing
P18-1006,W15-3014,0,0.0214419,"nt variable, and translation models of Z are jointly optimized with a unified bidirectional EM algorithm under the goal of maximizing the translation likelihood of (X, Y ). Empirical results demonstrate that our method significantly improves the translation quality of rare languages on MultiUN and IWSLT2012 datasets, and achieves even better performance combining back-translation methods. 1 Introduction In recent years, Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014) has achieved remarkable performance on many translation tasks (Jean et al., 2015; Sennrich et al., 2016; Wu et al., 2016; Sennrich et al., 2017). Being an end-to-end architecture, an NMT system first encodes the input sentence into a sequence of real vectors, based on which the decoder generates the target sequence word by word with the attention mechanism (Bahdanau et al., 2014; Luong et al., 2015). During training, NMT systems are optimized to maximize the translation probability of a given language pair ∗ Contribution during internship at MSRA. 56 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 56–65 c Melbou"
P18-1006,D13-1176,0,0.0940585,"can be rich) to improve the translation performance of lowresource pairs. In this triangular architecture, Z is taken as the intermediate latent variable, and translation models of Z are jointly optimized with a unified bidirectional EM algorithm under the goal of maximizing the translation likelihood of (X, Y ). Empirical results demonstrate that our method significantly improves the translation quality of rare languages on MultiUN and IWSLT2012 datasets, and achieves even better performance combining back-translation methods. 1 Introduction In recent years, Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014) has achieved remarkable performance on many translation tasks (Jean et al., 2015; Sennrich et al., 2016; Wu et al., 2016; Sennrich et al., 2017). Being an end-to-end architecture, an NMT system first encodes the input sentence into a sequence of real vectors, based on which the decoder generates the target sequence word by word with the attention mechanism (Bahdanau et al., 2014; Luong et al., 2015). During training, NMT systems are optimized to maximize the translation probability of a given language pair ∗ Contribution during internship at MSR"
P18-1006,D16-1139,0,0.0206136,"Optimize Θz|y and Θx|z 9: Generate z 0 from p(z 0 |y) and build the training batches B3 = (y, z 0 ) ∪ (y ∗ , z ∗ ), B4 = (x, z 0 ) ∪ (x∗ , z ∗ ) 10: E-step: update Θz|y with B3 (Equation 7) 11: M-step: update Θx|z with B4 (Equation 8) 12: end while 13: return Θz|x , Θy|z , Θz|y and Θx|z Figure 2: Triangular Learning Architecture for Low-Resource NMT 2.3 Training Details A major difficulty in our unified bidirectional training is the exponential search space of the translation candidates, which could be addressed by either sampling (Shen et al., 2015; Cheng et al., 2016) or mode approximation (Kim and Rush, 2016). In our experiments, we leverage the sampling method and simply generate the top target sentence for approximation. In order to perform gradient descend training, the parameter gradients for Equations 5 and 7 are formulated as follows: ∇Θz|x KL(p(z|x)||p(z|y)) p(z|x) ∇Θz|x log p(z|x) p(z|y) ∇Θz|y KL(p(z|y)||p(z|x)) 3 = Ez∼p(z|x) log Experiments 3.1 (9) Datasets In order to verify our method, we conduct experiments on two multilingual datasets. The one is MultiUN (Eisele and Chen, 2010), which is a collection of translated documents from the United Nations, and the other is IWSLT2012 (Cettolo"
P18-1006,2005.mtsummit-papers.11,0,0.0832691,"xtra monolingual Z described in Table 1 to do back-translation; to train the model p(x|z), we use monolingual X taken from (X, Y ). Procedures for training p(z|y) and p(y|z) are similar. This method use extra monolingual data of Z compared with our TA-NMT method. But we can incorporate it into our method. choice. Note that in this dataset, low-resource pairs (X, Z) and (Y, Z) are severely overlapped in Z. In addition, English-French bilingual data from WMT2014 dataset are also used to enrich the rich-resource pair. We also use additional EnglishRomanian bilingual data from Europarlv7 dataset (Koehn, 2005). The monolingual data of Z (HE and RO) are taken from the web2 . In both datasets, all sentences are filtered within the length of 5 to 50 after tokenization. Both the validation and the test sets are 2,000 parallel sentences sampled from the bilingual data, with the left as training data. The size of training data of all language pairs are shown in Table 1. 3.2 Baselines We compare our method with four baseline systems. The first baseline is the RNNSearch model (Bahdanau et al., 2014), which is a sequence-tosequence model with attention mechanism trained with given small-scale bilingual data"
P18-1006,N03-1017,0,0.0227686,"0 after tokenization. Both the validation and the test sets are 2,000 parallel sentences sampled from the bilingual data, with the left as training data. The size of training data of all language pairs are shown in Table 1. 3.2 Baselines We compare our method with four baseline systems. The first baseline is the RNNSearch model (Bahdanau et al., 2014), which is a sequence-tosequence model with attention mechanism trained with given small-scale bilingual data. The trained translation models are also used as pre-trained models for our subsequent training processes. The second baseline is PBSMT (Koehn et al., 2003), which is a phrase-based statistical machine translation system. PBSMT is known to perform well on low-resource language pairs, so we want to compare it with our proposed method. And we use the public available implementation of Moses5 for training and test in our experiments. The third baseline is a teacher-student alike method (Chen et al., 2017). For the sake of brevity, we will denote it as T-S. The process is illustrated in Figure 3. We treat this method as a second baseline because it can also be regarded as a method exploiting (Y, Z) and (X, Y ) to improve Figure 3: A teacher-student a"
P18-1006,D15-1166,0,0.0211414,"nd achieves even better performance combining back-translation methods. 1 Introduction In recent years, Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014) has achieved remarkable performance on many translation tasks (Jean et al., 2015; Sennrich et al., 2016; Wu et al., 2016; Sennrich et al., 2017). Being an end-to-end architecture, an NMT system first encodes the input sentence into a sequence of real vectors, based on which the decoder generates the target sequence word by word with the attention mechanism (Bahdanau et al., 2014; Luong et al., 2015). During training, NMT systems are optimized to maximize the translation probability of a given language pair ∗ Contribution during internship at MSRA. 56 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 56–65 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics Our contributions are listed as follows: of the rich-resource pair English-French. • We propose a novel triangular training architecture (TA-NMT) to effectively tackle the data sparsity problem for rare languages in NMT with an EM framewor"
P18-1006,2012.eamt-1.60,0,0.0142104,"h, 2016). In our experiments, we leverage the sampling method and simply generate the top target sentence for approximation. In order to perform gradient descend training, the parameter gradients for Equations 5 and 7 are formulated as follows: ∇Θz|x KL(p(z|x)||p(z|y)) p(z|x) ∇Θz|x log p(z|x) p(z|y) ∇Θz|y KL(p(z|y)||p(z|x)) 3 = Ez∼p(z|x) log Experiments 3.1 (9) Datasets In order to verify our method, we conduct experiments on two multilingual datasets. The one is MultiUN (Eisele and Chen, 2010), which is a collection of translated documents from the United Nations, and the other is IWSLT2012 (Cettolo et al., 2012), which is a set of multilingual transcriptions of TED talks. As is mentioned in section 1, our method is compatible with methods exploiting monolingual data. So we also find some extra monolingual data of rare languages in both datasets and conduct experiments incorporating back-translation into our method. MultiUN: English-French (EN-FR) bilingual data are used as the rich-resource pair (X, Y ). Arabic (AR) and Spanish (ES) are used as two simulated rare languages Z. We randomly choose subsets of bilingual data of (X, Z) and (Y, Z) in the original dataset to simulate low-resource situations,"
P18-1006,W17-4739,0,0.0156186,"zed with a unified bidirectional EM algorithm under the goal of maximizing the translation likelihood of (X, Y ). Empirical results demonstrate that our method significantly improves the translation quality of rare languages on MultiUN and IWSLT2012 datasets, and achieves even better performance combining back-translation methods. 1 Introduction In recent years, Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014) has achieved remarkable performance on many translation tasks (Jean et al., 2015; Sennrich et al., 2016; Wu et al., 2016; Sennrich et al., 2017). Being an end-to-end architecture, an NMT system first encodes the input sentence into a sequence of real vectors, based on which the decoder generates the target sequence word by word with the attention mechanism (Bahdanau et al., 2014; Luong et al., 2015). During training, NMT systems are optimized to maximize the translation probability of a given language pair ∗ Contribution during internship at MSRA. 56 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 56–65 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computa"
P18-1006,P17-1176,0,0.170018,"), which is a sequence-tosequence model with attention mechanism trained with given small-scale bilingual data. The trained translation models are also used as pre-trained models for our subsequent training processes. The second baseline is PBSMT (Koehn et al., 2003), which is a phrase-based statistical machine translation system. PBSMT is known to perform well on low-resource language pairs, so we want to compare it with our proposed method. And we use the public available implementation of Moses5 for training and test in our experiments. The third baseline is a teacher-student alike method (Chen et al., 2017). For the sake of brevity, we will denote it as T-S. The process is illustrated in Figure 3. We treat this method as a second baseline because it can also be regarded as a method exploiting (Y, Z) and (X, Y ) to improve Figure 3: A teacher-student alike method for low-resource translation. For training p(z|x) and p(x|z), we mix the true pair (y ∗ , z ∗ ) ∈ D with the pseudo pair (x0 , z ∗ ) generated by teacher model p (x0 |y ∗ ) in the same mini-batch. The training procedure of p(z|y) and p(y|z) is similar. 3.3 Overall Results Experimental results on both datasets are shown in Table 3 and 4 r"
P18-1006,P16-1009,0,0.0988588,"Missing"
P18-1006,P16-1185,0,0.357447,"jing Advanced Innovation Center for Big Data and Brain Computing, Beijing, China 3 University of California, Santa Barbara, CA, USA 4 Microsoft Research in Asia, Beijing, China Abstract with the Maximum Likelihood Estimation (MLE) method, which requires large bilingual data to fit the large parameter space. Without adequate data, which is common especially when it comes to a rare language, NMT usually falls short on low-resource language pairs (Zoph et al., 2016). In order to deal with the data sparsity problem for NMT, exploiting monolingual data (Sennrich et al., 2015; Zhang and Zong, 2016; Cheng et al., 2016; Zhang et al., 2018; He et al., 2016) is the most common method. With monolingual data, the back-translation method (Sennrich et al., 2015) generates pseudo bilingual sentences with a targetto-source translation model to train the source-totarget one. By extending back-translation, sourceto-target and target-to-source translation models can be jointly trained and boost each other (Cheng et al., 2016; Zhang et al., 2018). Similar to joint training (Cheng et al., 2016; Zhang et al., 2018), dual learning (He et al., 2016) designs a reinforcement learning framework to better capitalize on monolin"
P18-1006,W16-2323,0,0.039451,"anslation models of Z are jointly optimized with a unified bidirectional EM algorithm under the goal of maximizing the translation likelihood of (X, Y ). Empirical results demonstrate that our method significantly improves the translation quality of rare languages on MultiUN and IWSLT2012 datasets, and achieves even better performance combining back-translation methods. 1 Introduction In recent years, Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014) has achieved remarkable performance on many translation tasks (Jean et al., 2015; Sennrich et al., 2016; Wu et al., 2016; Sennrich et al., 2017). Being an end-to-end architecture, an NMT system first encodes the input sentence into a sequence of real vectors, based on which the decoder generates the target sequence word by word with the attention mechanism (Bahdanau et al., 2014; Luong et al., 2015). During training, NMT systems are optimized to maximize the translation probability of a given language pair ∗ Contribution during internship at MSRA. 56 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 56–65 c Melbourne, Australia, July 15"
P18-1006,eisele-chen-2010-multiun,0,0.0358089,"tes, which could be addressed by either sampling (Shen et al., 2015; Cheng et al., 2016) or mode approximation (Kim and Rush, 2016). In our experiments, we leverage the sampling method and simply generate the top target sentence for approximation. In order to perform gradient descend training, the parameter gradients for Equations 5 and 7 are formulated as follows: ∇Θz|x KL(p(z|x)||p(z|y)) p(z|x) ∇Θz|x log p(z|x) p(z|y) ∇Θz|y KL(p(z|y)||p(z|x)) 3 = Ez∼p(z|x) log Experiments 3.1 (9) Datasets In order to verify our method, we conduct experiments on two multilingual datasets. The one is MultiUN (Eisele and Chen, 2010), which is a collection of translated documents from the United Nations, and the other is IWSLT2012 (Cettolo et al., 2012), which is a set of multilingual transcriptions of TED talks. As is mentioned in section 1, our method is compatible with methods exploiting monolingual data. So we also find some extra monolingual data of rare languages in both datasets and conduct experiments incorporating back-translation into our method. MultiUN: English-French (EN-FR) bilingual data are used as the rich-resource pair (X, Y ). Arabic (AR) and Spanish (ES) are used as two simulated rare languages Z. We r"
P18-1006,1983.tc-1.13,0,0.45006,"Missing"
P18-1006,D16-1160,0,0.0545878,"ty, Beijing, China Beijing Advanced Innovation Center for Big Data and Brain Computing, Beijing, China 3 University of California, Santa Barbara, CA, USA 4 Microsoft Research in Asia, Beijing, China Abstract with the Maximum Likelihood Estimation (MLE) method, which requires large bilingual data to fit the large parameter space. Without adequate data, which is common especially when it comes to a rare language, NMT usually falls short on low-resource language pairs (Zoph et al., 2016). In order to deal with the data sparsity problem for NMT, exploiting monolingual data (Sennrich et al., 2015; Zhang and Zong, 2016; Cheng et al., 2016; Zhang et al., 2018; He et al., 2016) is the most common method. With monolingual data, the back-translation method (Sennrich et al., 2015) generates pseudo bilingual sentences with a targetto-source translation model to train the source-totarget one. By extending back-translation, sourceto-target and target-to-source translation models can be jointly trained and boost each other (Cheng et al., 2016; Zhang et al., 2018). Similar to joint training (Cheng et al., 2016; Zhang et al., 2018), dual learning (He et al., 2016) designs a reinforcement learning framework to better c"
P18-1006,D16-1163,0,0.272716,"uage Translation ∗ Shuo Ren1,2 , Wenhu Chen3 , Shujie Liu4 , Mu Li4 , Ming Zhou4 and Shuai Ma1,2 1 2 SKLSDE Lab, Beihang University, Beijing, China Beijing Advanced Innovation Center for Big Data and Brain Computing, Beijing, China 3 University of California, Santa Barbara, CA, USA 4 Microsoft Research in Asia, Beijing, China Abstract with the Maximum Likelihood Estimation (MLE) method, which requires large bilingual data to fit the large parameter space. Without adequate data, which is common especially when it comes to a rare language, NMT usually falls short on low-resource language pairs (Zoph et al., 2016). In order to deal with the data sparsity problem for NMT, exploiting monolingual data (Sennrich et al., 2015; Zhang and Zong, 2016; Cheng et al., 2016; Zhang et al., 2018; He et al., 2016) is the most common method. With monolingual data, the back-translation method (Sennrich et al., 2015) generates pseudo bilingual sentences with a targetto-source translation model to train the source-totarget one. By extending back-translation, sourceto-target and target-to-source translation models can be jointly trained and boost each other (Cheng et al., 2016; Zhang et al., 2018). Similar to joint traini"
P18-1006,N16-1101,0,\N,Missing
W03-1701,P98-1029,0,0.0380793,"Missing"
W03-1701,A00-2009,0,0.0296688,"Missing"
W03-1701,C98-1029,0,\N,Missing
W03-1718,J96-1002,0,0.00689166,"ork to integrate various features from different knowledge sources. Each feature is typically represented as a binary constraint f. All features are then combined using a log-linear model shown in Eq. 5. Pλ ( y |x ) = 1 exp( Z λ ( x) λ i f i ( x , y )) i (5) where i is a weight of the feature fi , and Z(x) is a normalization factor. Weights ( ) are estimated using the maximum entropy principle: to satisfy constraints on observed data and assume a uniform distribution (with the maximum entropy) on unseen data. The training algorithm we used is the improved iterative scaling (IIS) described in (Berger et al, 1996)3. The context features include six characters: three on the left of the SCNE, and three on the right. Given the context features, the ME classifier would estimate the probability of the candidate being a SCNE. In our example, we treat candidates with the probability larger than 0.5 as SCNEs. To get the precision-recall curve, we can vary the probability threshold from 0.1 to 0.9. 4.2 Vector Space Model VSM is another model we used to detect SCNE. Similar to ME, we use six surrounding characters as the features, as shown in Figure 2. Figure 2. Context window In this approach, we apply the stan"
W03-1718,P03-1035,1,0.899904,"Missing"
W03-1718,C02-1054,0,0.0644216,"Missing"
W03-1718,C02-1012,1,0.900718,"Missing"
W03-1718,M98-1018,0,\N,Missing
W03-1718,P02-1060,0,\N,Missing
W03-1718,M98-1017,0,\N,Missing
W03-1718,P02-1062,0,\N,Missing
W06-0127,W03-1728,0,\N,Missing
W06-0127,W03-1719,0,\N,Missing
W06-0127,C04-1081,0,\N,Missing
W06-0127,P03-1035,1,\N,Missing
W06-0127,J05-4005,1,\N,Missing
W06-0127,O03-4002,0,\N,Missing
W06-0127,I05-3027,0,\N,Missing
W06-0127,I05-3017,0,\N,Missing
W06-0127,I05-3025,0,\N,Missing
W06-0127,W03-1726,0,\N,Missing
W08-0301,W05-0909,0,0.023308,"Missing"
W08-0301,J07-2003,0,0.506228,"s have, in general, lower scores, and 2 Literature Review Research work in SMT seldom treats SWD as a problem separated from other factors in translation. However, it can be found in different SMT paradigms the mechanism of handling SWD. As to the pioneering IBM word-based SMT models (Brown et al., 1990), IBM models 3, 4 and 5 handle spurious source words by considering them as corresponding to a particular EMPTY word token on the English side, and by the fertility model which allows the English EMPTY to generate a certain number of foreign words. As to the hierarchical phrase-based approach (Chiang, 2007), its hierarchical rules are more powerful in SWD than the phrase pairs 2 Model 1 Model 2 Model 3 ˜ >, its probability is < F˜ , E P () P (|f ) PCRF (|F~ (f ) ( ˜ F˜ ) = P (E| Table 1: Summary of the Three SWD Models ˜ = () P () if E ˜ ˜ F˜ ) otherwise P (¯ )|F |PT (E| ˜ F˜ ) is the probability of the phrase where PT (E| pair as registered in the translation table, and |F˜ | is the length of the phrase F˜ . The estimation of P () is done by MLE: therefore the decoder has a bias towards shorter translations. Word penalty (in fact, it should be renamed as word reward) is used to neutraliz"
W08-0301,P03-1054,0,0.00813748,"d for training language model. The development/test corpora are based on the test sets for NIST MT-2005/6. The alignment matrices of the training data are produced by the GIZA++ (Och and Ney, 2000b) word alignment package with its default settings. The subsequent construction of translation table was done in exactly the same way as explained 1 Maximum Entropy was also tried in our experiments but its performance is not as good as CRF. 4 Data FBIS BFT NIST in (Koehn et al., 2003). For SWD model 2, the phrase enumeration step is modified as described in section 3.2. We used the Stanford parser (Klein and Manning, 2003) with its default Chinese grammar for its POS-tagging as well as finding the head/dependent words of all source words. The CRF toolkit used for model 3 is CRF ++ 2 . The training data for the CRF model should be the same as that for translation table construction. However, since there are too many instances (every single word in the training data is an instance) with a huge feature space, no publicly available CRF toolkit can handle the entire training set of NIST MT-2006.3 Therefore, we can use at most only about one-third of the NIST training set (comprising the FBIS, B1, and T10 sections) f"
W08-0301,N03-1017,0,0.0738606,"Missing"
W08-0301,C00-2163,0,0.0999851,"special phrase pairs, TO EMPTY phrase pairs, are fed to the module of phrase scoring along with the normal phrase pairs. Both types of phrase pairs are then stored in the translation table with corresponding phrase translation probabilities. It can be seen that, since the probabilities of normal phrase pairs are estimated in the same procedure as those of TO - EMPTY phrase pairs, they do not need re-weighing as in the case of SWD model 1. 3.1 Model 1: Uniform Probability The first model assumes a uniform probability of translation to . This model is inspired by the HMM-based alignment model (Och and Ney, 2000a), which posits a probability P0 for alignment of some source word to the empty word on the target language side, and weighs all other alignment probabilities by the factor 1 − P0 . In the same style, SWD model 1 posits a probability P () for the translation of any source word to . The probabilities of normal phrase pairs should be weighed accordingly. For a source phrase containing only one word, its weight is simply P (¯ ) = 1 − P (). As to a source phrase containing more than one word, it implies that every word in the phrase does not translate into , and therefore the weighing factor"
W08-0301,P00-1056,0,0.184679,"special phrase pairs, TO EMPTY phrase pairs, are fed to the module of phrase scoring along with the normal phrase pairs. Both types of phrase pairs are then stored in the translation table with corresponding phrase translation probabilities. It can be seen that, since the probabilities of normal phrase pairs are estimated in the same procedure as those of TO - EMPTY phrase pairs, they do not need re-weighing as in the case of SWD model 1. 3.1 Model 1: Uniform Probability The first model assumes a uniform probability of translation to . This model is inspired by the HMM-based alignment model (Och and Ney, 2000a), which posits a probability P0 for alignment of some source word to the empty word on the target language side, and weighs all other alignment probabilities by the factor 1 − P0 . In the same style, SWD model 1 posits a probability P () for the translation of any source word to . The probabilities of normal phrase pairs should be weighed accordingly. For a source phrase containing only one word, its weight is simply P (¯ ) = 1 − P (). As to a source phrase containing more than one word, it implies that every word in the phrase does not translate into , and therefore the weighing factor"
W08-0301,P02-1040,0,0.0759968,"performance simply by removing OOVs. In order to test the effect of training data size on the performance of the SWD models, three variations of training data were used: baseline 28.01 29.82 29.77 model 1 29.71 31.55 31.39 model 2 29.48 31.61 31.33 model 3 29.64 31.75 31.71 Table 2: BLEU scores in Experiment 1: NIST’05 as dev and NIST’06 as test variation is to test each model when medium size of data are available.4 NIST All the sections of the NIST training set are used. The purpose of this variation is to test each model when a large amount of data are available. (Case-insensitive) BLEU-4 (Papineni et al., 2002) is used as the evaluation metric. In each test in our experiments, maximum BLEU training were run 10 times, and thus there are 10 BLEU scores for the test set. In the following we will report the mean scores only. 4.2 Experiment Results and Analysis Table 2 shows the results of the first experiment, which uses the NIST MT-2005 test set as development data and the NIST MT-2006 test set as test data. The most obvious observation is that any SWD model achieves much higher BLEU score than the baseline, as there is at least 1.6 BLEU point improvement in each case, and in some case the improvement"
W08-0301,N03-1028,0,0.0112237,"he source language sentence: each source word is tagged either as “spurious” or “non-spurious”. Under such a perspective, SWD 3 model 2 is merely a unigram tagging model, and it uses only one feature template, viz. the lexical form of the source word in hand. Such a model can by no means encode any contextual information, and therefore it cannot handle the “ACCORDING - TO /dd NP EXPRESS /dd” example in section 1. An obvious solution to this limitation is a more powerful tagging model augmented with contextsensitive feature templates. Inspired by research work like (Lafferty et al., 2001) and (Sha and Pereira, 2003), our SWD model 3 uses first-order Conditional Random Field (CRF) to tackle the tagging task.1 The CRF model uses the following feature templates: The training data for the CRF model comprises the alignment matrices of the bilingual training data for the MT system. A source word (token) in the training data is tagged as “non-spurious” if it is aligned to some target word(s), otherwise it is tagged as “spurious”. The sentences in the training data are also POS-tagged and parsed by some dependency parser, so that each word can be assigned values for the POS-based feature templates as well as the"
W08-0301,D07-1056,1,0.849556,"training data for the CRF model should be the same as that for translation table construction. However, since there are too many instances (every single word in the training data is an instance) with a huge feature space, no publicly available CRF toolkit can handle the entire training set of NIST MT-2006.3 Therefore, we can use at most only about one-third of the NIST training set (comprising the FBIS, B1, and T10 sections) for CRF training. The decoder in the experiments is our reimplementation of HIERO (Chiang, 2007), augmented with a 5-gram language model and a reordering model based on (Zhang et al., 2007). Note that no hierarchical rule is used with the decoder; the phrase pairs used are still those used in conventional phrase-based SMT. Note also that the decoder does not translate OOV at all even in the baseline case, and thus the SWD models do not improve performance simply by removing OOVs. In order to test the effect of training data size on the performance of the SWD models, three variations of training data were used: baseline 28.01 29.82 29.77 model 1 29.71 31.55 31.39 model 2 29.48 31.61 31.33 model 3 29.64 31.75 31.71 Table 2: BLEU scores in Experiment 1: NIST’05 as dev and NIST’06 a"
W08-0301,J90-2002,0,\N,Missing
Y06-1009,H05-1120,0,0.353491,"thm and Ispell's [11] near miss strategy which is inserting a space or hyphen, interchanging two adjacent letters, changing one letter, deleting a letter, or adding a letter. Our system takes more than five cutting-edge techniques into account, rather than simply rely on one or two factors. As for the models in this task, rule-based studies include [18] [19]. In recent years, statistical machine learning also makes contributions to spelling correction task. It falls into two categories: generative and discriminative. The heavily-used generative model in this domain is the noisy channel model: [1] [3] [5] [24]. It considers the phenomenon of making spelling mistakes as the process of sending text through a noisy communication channel, which introduces error in the text. As for each input (a potential misspelling) the candidate with largest posterior, P(cand|input) is chosen. By applying Bayes rule and dropping the constant denominator we can score each candidate by its prior P(cand) multiplying the likelihood, P(input|cand). The language model P(cand) can be estimated using n-gram statistics [3] [14]. The channel model P(input|cand) is generally modeled using letter-to-letter[14] or st"
Y06-1009,P00-1037,0,0.565764,"Missing"
Y06-1009,C90-2036,0,0.807238,"Missing"
Y06-1009,P06-1129,1,0.753844,"Missing"
Y06-1009,P02-1019,0,0.115299,"Missing"
Y06-1009,J05-1003,0,\N,Missing
Y06-1009,W04-3238,0,\N,Missing
Y06-1012,J05-4005,1,0.608158,"Missing"
Y06-1012,O03-4002,0,0.572227,"d to used for the particular corpus. No other data is allowed. In this study, we will limit our comparison in closed test because additional linguistical resource often varies from system to system. The remainder of the paper is organized as follows. The next section is a simple introduction to conditional random field. Feature templates and tag sets are given in Section 3. In Section 4, our experimental results are demonstrated. We summarize our contribution in Section 5. 2 Conditional Random Field Maximum entropy tagger was used in early character-based tagging for Chinese word segmentation [2], [3], while we choose linear-chain CRF as our learning model in this study. It can combine rich feature representation and probabilistic finite state model, too. In addition, it can avoid so-called ‘label-bias’ problem in some degree. Actually, such model was also proved to be very effective in many existing works [8]. Conditional random field (CRF) is a statistical sequence modeling framework first introduced into language processing in [9]. Work by Peng et al. first used this framework for Chinese word segmentation by treating it as a binary decision task, such that each Chinese character i"
Y06-1012,W03-1728,0,0.0145844,"used for the particular corpus. No other data is allowed. In this study, we will limit our comparison in closed test because additional linguistical resource often varies from system to system. The remainder of the paper is organized as follows. The next section is a simple introduction to conditional random field. Feature templates and tag sets are given in Section 3. In Section 4, our experimental results are demonstrated. We summarize our contribution in Section 5. 2 Conditional Random Field Maximum entropy tagger was used in early character-based tagging for Chinese word segmentation [2], [3], while we choose linear-chain CRF as our learning model in this study. It can combine rich feature representation and probabilistic finite state model, too. In addition, it can avoid so-called ‘label-bias’ problem in some degree. Actually, such model was also proved to be very effective in many existing works [8]. Conditional random field (CRF) is a statistical sequence modeling framework first introduced into language processing in [9]. Work by Peng et al. first used this framework for Chinese word segmentation by treating it as a binary decision task, such that each Chinese character is lab"
Y06-1012,I05-3017,0,0.435599,"Missing"
Y06-1012,I05-3027,0,0.49937,"Missing"
Y06-1012,C04-1081,0,0.45997,". Feature templates and tag sets are given in Section 3. In Section 4, our experimental results are demonstrated. We summarize our contribution in Section 5. 2 Conditional Random Field Maximum entropy tagger was used in early character-based tagging for Chinese word segmentation [2], [3], while we choose linear-chain CRF as our learning model in this study. It can combine rich feature representation and probabilistic finite state model, too. In addition, it can avoid so-called ‘label-bias’ problem in some degree. Actually, such model was also proved to be very effective in many existing works [8]. Conditional random field (CRF) is a statistical sequence modeling framework first introduced into language processing in [9]. Work by Peng et al. first used this framework for Chinese word segmentation by treating it as a binary decision task, such that each Chinese character is labeled either as the beginning of a word or not. The probability assigned to a label sequence for a particular sequence of characters by a CRF is given by the equation below: pλ (Y |W ) = XX 1 exp( λk fk (yt−1 , yt , W, t)) Z(W ) t∈T (1) k where Y = {yi } is the label sequence for the sentence, W is the sequence of"
Y06-1012,W06-0127,1,0.341064,"03 and 2005 for each corpus under closed test. The other is the results of Tseng et al.. All of our results are performed under TMPT-01 (or TMPT-044 ) and 6-tag set. 3 4 The Third SIGHAN Chinese Language Processing Bakeoff has been held, the results will be presented at the 5th SIGHAN Workshop, to be held at ACL-COLING 2006 in Sydney, Australia, July 22-23, 2006. We also participate this Bakeoff, and our system with the techniques presented in this paper won four highest and two third highest F measures in six Chinese word segmentation tracks. Our results on Bakeoff-2006 appear in SIGHAN-2006 [11]. Note that TMPT-04 is a feature template set only including n-gram ones. Researchers in CWS did not make an agree on what P and T−1 T0 T1 are feature templates for closed test or not. Thus the results under TMPT-04 are demonstrated, too. We will see that our system gets stateof-the-art performance in either of feature template set. 91 Table 6. Comparisons of best existing results and our results in the corpora of Bakeoff-2003 and 2005 Participant AS2003 CTB2003 CityU2003 PKU2003 AS2005 CityU2005 PKU2005 MSRA2005 Peng 0.956 0.849 0.928 0.941 Tseng 0.970 0.863 0.947 0.953 0.947 0.943 0.950 0.96"
Y06-1012,W03-1719,0,\N,Missing
Y06-1012,W03-1726,0,\N,Missing
Y06-1012,I05-3025,0,\N,Missing
Y09-2035,P03-1035,1,0.778156,"indow, they are counted as co′ ′ occurrence ??? = ??(?? , ?? ), where ??? is the initial weight of the edge ?? → ?? , and ??(?? , ?? ) is the co-occurrence times of ?? and ?? . The transition matrix ? = [??? ]?×? is given by ′ ??? = ??? / ? ∑ ′ ??? . (2) ?=1 3 Phrase Generation Chinese is different from English because there is no explicit separator between words in a sentence. Segmentation converts a sequence of continuous Chinese characters into a sequence of 734 delimited words. It is a necessary step for most Chinese information processing systems including keyphrase extraction. S-MSRSeg (Gao et al., 2003) is used as our segmentation tool1 . One crucial step to generate phrases from words is to determine the phrase boundary. In TextRank, only single words take part in the PageRank iteration. After a small number of candidate keywords have been extracted, the sequences of adjacent keywords are merged into keyphrases. One disadvantage of this method is that not all parts of the keyphrase can always be extracted correctly as keywords. Any loss of the adjacent keywords will cause the failure of keyphrase generation. In this section, we introduce three statistical criteria to generate phrases before"
Y09-2035,I08-4017,0,0.0143965,"l criteria to generate phrases before the PageRank iteration. A straightforward criterion is frequency. For an article ?, a word sequence ?1 ..?? should be a phrase if { ? ??(?1 ..?? , ?) ≥ ? ??? ?? , (3) ?? ∈ / ??, ? = 1, 2, ..., ? where ? ??(?1 ..?? , ?) is the frequency of ?1 ..?? in article ?, ? ??? ?? is the threshold, ?? is the stop word collection. We call this criterion “local frequency” because the frequency is based on the article from which we extract keyphrases. The advantage of this criterion is robust and efficient, especially for domain specific articles. Boundary entropy (BE) (Zhao and Kit, 2008) measures the boundary by entropy. ∑ ??(?1 ..?? ) = − ?(?∣?1 ..?? ) log ?(?∣?1 ..?? ), (4) ?∈? where ? is a Chinese character, ?(?∣?1 ..?? ) is the probability of ?1 ..?? adjacent to ?. As ? can be right or left to ?1 ..?? , two types of BE named ??? and ??? can be defined. Feng et al. (2004) and Zhao and Kit (2008) used Access Variety (AV) to measure the boundary of Chinese words as ?? (?1 ..?? ) = log ???? (?1 ..?? ), (5) where ???? (?1 ..?? ) is the number of the distinct Chinese characters which adjacent to ?1 ..?? . Also, we can define left access variety (??? ) and right access variety ("
Y09-2035,W04-3252,0,\N,Missing
Y09-2035,J04-1004,0,\N,Missing
