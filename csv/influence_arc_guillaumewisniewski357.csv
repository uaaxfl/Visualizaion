2010.amta-papers.18,N06-1013,0,0.629907,"to the first problem is to use discriminative models, which are able to consider arbitrary features of the involved words. In this framework, the alignment task is casted as a classification problem: a binary classifier predicts, for each possible assignment, whether it should be included or not in the alignment. Discriminative models can also consider predictions provided by other alignment models as features, and therefore constitute a solution to the second problem: by applying these features to learn symmetrization decisions in light of a global view of the data. By applying these ideas (Ayan and Dorr, 2006) obtained promising results. However, their model remains unable to model interactions between alignment decisions which are, intuitively, of great help to correctly prevent or encourage certain configurations in the predicted alignment. To overcome this shortcoming, we propose to extend their model by introducing a stacked classification layer (Wolpert, 1992) that operates globally and, hence, enables arbitrary features, describing interactions between alignment decisions, to be taken into consideration. The main contribution of this work is a reexamination of (Ayan and Dorr, 2006) work which"
2010.amta-papers.18,P06-1009,0,0.0912765,"nt task as a maximum weighted matching problem. This comes at the price of constraining possible alignments to one-to-one matchings and making local decisions with no global interactions. These limitations are fixed in (Lacoste-Julien et al., 2006), by modeling alignment as a quadratic assignment problem which is NP-hard in general. In another type of approaches, word alignment is viewed as a classification problem of the cells in the alignment matrix. The scoring function, which is usually the probability of the hypothesized alignment, is decomposable under some independence assumptions. In (Blunsom and Cohn, 2006) word alignment is considered as a sequence labeling problem, in which, source words are tagged with target positions using a linear chain conditional random field (CRF). The linear chain assumption enables exact inference and training. However the underlying graphical structure is similar to the directed hidden Markov model (HMM) used in generative alignment, hence only one-to-many alignments can be obtained, and the symmetrization step is still needful. In (Niehues and Vogel, 2008), the alignment matrix is directly modeled by a more complex CRF structure, which allows to get rid of the symme"
2010.amta-papers.18,J93-2003,0,0.0208821,"p. Since finding the optimal phrase alignment in parallel sentences is NPhard (DeNero and Klein, 2008), most practical approaches rely on pre-computed word alignments to restrict the search space and use a heuristic to extract phrase pairs that are consistent with them (Och and Ney, 2003). Phrase extraction therefore boils down to the problem of word alignments, that consists in finding a many-to-many correspondence between source and target words of a bilingual sentence-pair. Many approaches have been proposed to solve this problem. The most widely used in practice are generative IBM models (Brown et al., 1993) which allow to construct directional one-to-many alignments in both translation directions. Theses alignments are then symmetrized during a post-processing step to obtain a many-to-many symmetric alignment. Training these models only requires sentence-aligned bitext and is performed in an unsupervised way with the EM algorithm. This approach has two main caveats, leaving room for improving the alignment quality and, consequently, the translation quality. Firstly, the generative paradigm is not well suited to incorporate arbitrary and possibly interdependent information sources. Secondly, the"
2010.amta-papers.18,P03-1012,0,0.0307885,"tained by the discriminative matrix model, in the light of their Alignment Error Rate (AER) and their impact on translation quality as measured by BLEU (Papineni et al., 2002) on NIST MT08 largescale task. The rest of the paper is organized as follows: after reviewing the related work in Section 2, we present our approach in Section 3, focusing on the design of our feature set, and on our implementation of stacking. We then present experimental results both in terms of AER and BLEU in Section 4. 2 Related Work Several discriminative approaches of word alignment have been carried out recently (Cherry and Lin, 2003; Ittycheriah and Roukos, 2005; Liu et al., 2005), attempting to reach a good balance between the expressivity of the model and its complexity (in terms of tractability and the possibility of performing exact inference and learning). In one type of approaches, a word alignment between two sentences is evaluated with a global score using a non-decomposable discriminative scoring function. This scheme enables to take into consideration the complete observation of the sentence-pair and the hypothesized word alignment when extracting features (Moore, 2005). However, as no restriction on the form o"
2010.amta-papers.18,P08-2007,0,0.0743953,"stacking techniques, we were able to obtain alignments much closer to manually defined references than those obtained by the IBM models. These alignments also yield better translation models, delivering improved performance in a large scale Arabic to English translation task. 1 Introduction The translation quality of phrase-based machine translation systems depends heavily on the quality of the translation model, the so-called phrase table consisting of a set of aligned phrase-pairs in mutual translation relationship. Since finding the optimal phrase alignment in parallel sentences is NPhard (DeNero and Klein, 2008), most practical approaches rely on pre-computed word alignments to restrict the search space and use a heuristic to extract phrase pairs that are consistent with them (Och and Ney, 2003). Phrase extraction therefore boils down to the problem of word alignments, that consists in finding a many-to-many correspondence between source and target words of a bilingual sentence-pair. Many approaches have been proposed to solve this problem. The most widely used in practice are generative IBM models (Brown et al., 1993) which allow to construct directional one-to-many alignments in both translation di"
2010.amta-papers.18,N07-2007,0,0.396858,"nt matrix is typically sparse, with a majority of inactive links, the classification task we consider is unbalanced. To avoid learning a biased classifier with high tendency toward labeling all links as inactive, we use a set of input alignments to reduce the set of links to be predicted to a subset of the alignment matrix: a point that has not been proposed by at least one input alignment will be labeled as inactive; the others are labeled by the classifier. The union of all input alignments is hence used to reduce the search space and avoid biasing the classifier as in (Ayan and Dorr, 2006; Elming and Habash, 2007). Input alignments are pre-computed separately using GIZA++. During inference, the model assigns a probability to each proposed alignment link. The final output matrix consists of active links whose probability exceeds a threshold p (optimized on a development set using a grid search). This parameter is used to control the density of the resulting alignment and therefore the balance between its precision and recall. In this work, we used a maximum entropy (ME) classifier to estimate the probability of a link of A: Align 2 Align 1 AAlign Target ADist AJump Figure 1: Features extracted to label"
2010.amta-papers.18,J07-3002,0,0.0746646,"Missing"
2010.amta-papers.18,N06-2013,0,0.0585018,"Missing"
2010.amta-papers.18,H05-1012,0,0.020578,"native matrix model, in the light of their Alignment Error Rate (AER) and their impact on translation quality as measured by BLEU (Papineni et al., 2002) on NIST MT08 largescale task. The rest of the paper is organized as follows: after reviewing the related work in Section 2, we present our approach in Section 3, focusing on the design of our feature set, and on our implementation of stacking. We then present experimental results both in terms of AER and BLEU in Section 4. 2 Related Work Several discriminative approaches of word alignment have been carried out recently (Cherry and Lin, 2003; Ittycheriah and Roukos, 2005; Liu et al., 2005), attempting to reach a good balance between the expressivity of the model and its complexity (in terms of tractability and the possibility of performing exact inference and learning). In one type of approaches, a word alignment between two sentences is evaluated with a global score using a non-decomposable discriminative scoring function. This scheme enables to take into consideration the complete observation of the sentence-pair and the hypothesized word alignment when extracting features (Moore, 2005). However, as no restriction on the form of considered alignments is imp"
2010.amta-papers.18,P06-1141,0,0.0209513,"the model simple, interactions between individual predictions cannot be modeled, and global decisions cannot be made. In order to incorporate structure and dependencies into the ME model, without sacrificing efficient, model-optimal predictions, we use a stacked generalization method (Wolpert, 1992). Stacked generalization is an approximation approach to structured learning. It allows to indirectly model dependencies between predicted labels at a low computational cost. It has been successfully applied to NLP problems, like dependency parsing (Martins et al., 2008), named entity recognition (Krishnan and Manning, 2006) and sequential partitioning problems (Cohen and Carvalho, 2005). In stacked learning, all labels are jointly predicted in two steps. (1) For each training example (xi , y˜i ), the entire set of observations x = [x1 , . . . , xn ] is considered to extract features, that are then fed to a first-level classifier. This classifier is used to assign a label yi to each observation xi without taking dependencies between labels into consideration; then (2) observations are augmented with predictions of the local classifier y = [y1 , . . . , yn ] to generate an extended representation of the training c"
2010.amta-papers.18,N06-1015,0,0.0149343,"ir and the hypothesized word alignment when extracting features (Moore, 2005). However, as no restriction on the form of considered alignments is imposed, the size of the resulting search space makes the search intractable and requires the application of a heuristic beam search. In (Taskar et al., 2005), tractability of the search problem is achieved by casting the word alignment task as a maximum weighted matching problem. This comes at the price of constraining possible alignments to one-to-one matchings and making local decisions with no global interactions. These limitations are fixed in (Lacoste-Julien et al., 2006), by modeling alignment as a quadratic assignment problem which is NP-hard in general. In another type of approaches, word alignment is viewed as a classification problem of the cells in the alignment matrix. The scoring function, which is usually the probability of the hypothesized alignment, is decomposable under some independence assumptions. In (Blunsom and Cohn, 2006) word alignment is considered as a sequence labeling problem, in which, source words are tagged with target positions using a linear chain conditional random field (CRF). The linear chain assumption enables exact inference an"
2010.amta-papers.18,P05-1057,0,0.0168931,"ght of their Alignment Error Rate (AER) and their impact on translation quality as measured by BLEU (Papineni et al., 2002) on NIST MT08 largescale task. The rest of the paper is organized as follows: after reviewing the related work in Section 2, we present our approach in Section 3, focusing on the design of our feature set, and on our implementation of stacking. We then present experimental results both in terms of AER and BLEU in Section 4. 2 Related Work Several discriminative approaches of word alignment have been carried out recently (Cherry and Lin, 2003; Ittycheriah and Roukos, 2005; Liu et al., 2005), attempting to reach a good balance between the expressivity of the model and its complexity (in terms of tractability and the possibility of performing exact inference and learning). In one type of approaches, a word alignment between two sentences is evaluated with a global score using a non-decomposable discriminative scoring function. This scheme enables to take into consideration the complete observation of the sentence-pair and the hypothesized word alignment when extracting features (Moore, 2005). However, as no restriction on the form of considered alignments is imposed, the size of t"
2010.amta-papers.18,2006.amta-papers.11,0,0.0391831,"Missing"
2010.amta-papers.18,D08-1017,0,0.0309813,"s are assumed to be independent. While this keeps the model simple, interactions between individual predictions cannot be modeled, and global decisions cannot be made. In order to incorporate structure and dependencies into the ME model, without sacrificing efficient, model-optimal predictions, we use a stacked generalization method (Wolpert, 1992). Stacked generalization is an approximation approach to structured learning. It allows to indirectly model dependencies between predicted labels at a low computational cost. It has been successfully applied to NLP problems, like dependency parsing (Martins et al., 2008), named entity recognition (Krishnan and Manning, 2006) and sequential partitioning problems (Cohen and Carvalho, 2005). In stacked learning, all labels are jointly predicted in two steps. (1) For each training example (xi , y˜i ), the entire set of observations x = [x1 , . . . , xn ] is considered to extract features, that are then fed to a first-level classifier. This classifier is used to assign a label yi to each observation xi without taking dependencies between labels into consideration; then (2) observations are augmented with predictions of the local classifier y = [y1 , . . . , yn ] t"
2010.amta-papers.18,H05-1011,0,0.0168663,"t have been carried out recently (Cherry and Lin, 2003; Ittycheriah and Roukos, 2005; Liu et al., 2005), attempting to reach a good balance between the expressivity of the model and its complexity (in terms of tractability and the possibility of performing exact inference and learning). In one type of approaches, a word alignment between two sentences is evaluated with a global score using a non-decomposable discriminative scoring function. This scheme enables to take into consideration the complete observation of the sentence-pair and the hypothesized word alignment when extracting features (Moore, 2005). However, as no restriction on the form of considered alignments is imposed, the size of the resulting search space makes the search intractable and requires the application of a heuristic beam search. In (Taskar et al., 2005), tractability of the search problem is achieved by casting the word alignment task as a maximum weighted matching problem. This comes at the price of constraining possible alignments to one-to-one matchings and making local decisions with no global interactions. These limitations are fixed in (Lacoste-Julien et al., 2006), by modeling alignment as a quadratic assignment"
2010.amta-papers.18,W08-0303,0,0.0353835,"usually the probability of the hypothesized alignment, is decomposable under some independence assumptions. In (Blunsom and Cohn, 2006) word alignment is considered as a sequence labeling problem, in which, source words are tagged with target positions using a linear chain conditional random field (CRF). The linear chain assumption enables exact inference and training. However the underlying graphical structure is similar to the directed hidden Markov model (HMM) used in generative alignment, hence only one-to-many alignments can be obtained, and the symmetrization step is still needful. In (Niehues and Vogel, 2008), the alignment matrix is directly modeled by a more complex CRF structure, which allows to get rid of the symmetrization step, at the expense of an approximate inference and a complicated two-step training. Many of these discriminative models do not entirely dispense with the generative models, but rather integrate their predictions as supplementary features. 3 Maximum Entropy for Alignment Matrix Modeling In this section, we present the task of word alignment as a binary classification problem, in which we model the alignment matrix directly. We also explain how to improve the expressivity o"
2010.amta-papers.18,J03-1002,0,0.0614447,"delivering improved performance in a large scale Arabic to English translation task. 1 Introduction The translation quality of phrase-based machine translation systems depends heavily on the quality of the translation model, the so-called phrase table consisting of a set of aligned phrase-pairs in mutual translation relationship. Since finding the optimal phrase alignment in parallel sentences is NPhard (DeNero and Klein, 2008), most practical approaches rely on pre-computed word alignments to restrict the search space and use a heuristic to extract phrase pairs that are consistent with them (Och and Ney, 2003). Phrase extraction therefore boils down to the problem of word alignments, that consists in finding a many-to-many correspondence between source and target words of a bilingual sentence-pair. Many approaches have been proposed to solve this problem. The most widely used in practice are generative IBM models (Brown et al., 1993) which allow to construct directional one-to-many alignments in both translation directions. Theses alignments are then symmetrized during a post-processing step to obtain a many-to-many symmetric alignment. Training these models only requires sentence-aligned bitext an"
2010.amta-papers.18,P03-1021,0,0.0261178,"ng we used a freely available toolkit3 . The model parameters are estimated using L-BFGS (Byrd et al., 1994) to maximize the regularized log-likelihood on a training corpus. A Gaussian prior is used during optimization to prevent overfitting. GIZA++ (Och and Ney, 2003) is used to train our generative alignments, with the additional parallel data made available by NIST MT Eval’09 constrained training condition. We used Moses4 with SRILM5 with the same data in our translation experiments. A 4-gram back-of language model is estimated using all English available data. Minimum Error-Rate Training (Och, 2003) is carried on to tune the parameters of the translation system on the NIST MT’06 test set. Translations are evaluated on NIST MT’08 test set. Arabic pre-processing scheme and remappings Arabic is a morphologically complex, highlyinflected language. This makes normalization necessary to reduce the sparsity of the data. We use MADA+TOKAN6 for morphological analysis, disambiguation and tokenization for Arabic. Given previous experiments on the NIST MT’09 task, we use the D2 tokenization scheme that showed to perform best under large resource conditions (Habash 3 http://homepages.inf.ed.ac.uk/lzh"
2010.amta-papers.18,P02-1040,0,0.0816489,"e one hand, we present a careful study of the impact of several novel features on the performance; on the other hand, we investigate the use of the stacking technique to improve the alignment quality. By conjoining these techniques, we were able to greatly reduce the AER as compared to previously published work, and to achieve better BLEU results. In this paper, we also contrast alignments obtained by the symmetrization heuristic with those obtained by the discriminative matrix model, in the light of their Alignment Error Rate (AER) and their impact on translation quality as measured by BLEU (Papineni et al., 2002) on NIST MT08 largescale task. The rest of the paper is organized as follows: after reviewing the related work in Section 2, we present our approach in Section 3, focusing on the design of our feature set, and on our implementation of stacking. We then present experimental results both in terms of AER and BLEU in Section 4. 2 Related Work Several discriminative approaches of word alignment have been carried out recently (Cherry and Lin, 2003; Ittycheriah and Roukos, 2005; Liu et al., 2005), attempting to reach a good balance between the expressivity of the model and its complexity (in terms of"
2010.amta-papers.18,H05-1010,0,0.0234131,"ity and the possibility of performing exact inference and learning). In one type of approaches, a word alignment between two sentences is evaluated with a global score using a non-decomposable discriminative scoring function. This scheme enables to take into consideration the complete observation of the sentence-pair and the hypothesized word alignment when extracting features (Moore, 2005). However, as no restriction on the form of considered alignments is imposed, the size of the resulting search space makes the search intractable and requires the application of a heuristic beam search. In (Taskar et al., 2005), tractability of the search problem is achieved by casting the word alignment task as a maximum weighted matching problem. This comes at the price of constraining possible alignments to one-to-one matchings and making local decisions with no global interactions. These limitations are fixed in (Lacoste-Julien et al., 2006), by modeling alignment as a quadratic assignment problem which is NP-hard in general. In another type of approaches, word alignment is viewed as a classification problem of the cells in the alignment matrix. The scoring function, which is usually the probability of the hypot"
2010.iwslt-evaluation.13,W10-1704,1,0.845666,"is first described in Section 2, while Section 3 reports our work on Turkish pre-processing and on the use of continuous space language models. 2. TALK task 2.1. n-code SMT system 1. Introduction LIMSI took part in the IWSLT 2010 evaluation for two different tasks: Talk and BTEC. The goal of the new Talk task is to translate public speeches on a variety of topics, from English to French. Since the allowed training data includes the parallel corpora distributed by the ACL 2010 Workshop on Statistical Machine Translation (WMT), our starting system is the one submitted to the evaluation campaign [1]. We enhanced our inhouse n-code SMT system with an additional reordering model which is estimated as a standard n-gram language model over generalized translation units (partof-speech in the described experiments). In order to add more closely related training data, the use of Wikipedia as an additionnal source of monolingual text for the target language model was also evaluated. For the BTEC task, the LIMSI participated in the Turkish to English translation track with a system based on the open source Moses system [2]. The linguistic discrepancies between these two languages appear both Our"
2010.iwslt-evaluation.13,P07-2045,0,0.00317713,"(WMT), our starting system is the one submitted to the evaluation campaign [1]. We enhanced our inhouse n-code SMT system with an additional reordering model which is estimated as a standard n-gram language model over generalized translation units (partof-speech in the described experiments). In order to add more closely related training data, the use of Wikipedia as an additionnal source of monolingual text for the target language model was also evaluated. For the BTEC task, the LIMSI participated in the Turkish to English translation track with a system based on the open source Moses system [2]. The linguistic discrepancies between these two languages appear both Our in-house n-code SMT system implements the bilingual n-gram approach to statistical Machine Translation [3]. A translation hypothesis t given a source sentence s is defined as the sentence which maximizes a linear combination of feature functions: tˆI1 = arg max tI1 ( M X m=1 λm hm (sJ1 , tI1 ) ) , (1) where sJ1 and tI1 respectively denote the source and the target sentences, and λm is the weight associated with the feature function hm . The most important feature is the log-score of the translation model based on biling"
2010.iwslt-evaluation.13,J06-4004,1,0.810779,"standard n-gram language model over generalized translation units (partof-speech in the described experiments). In order to add more closely related training data, the use of Wikipedia as an additionnal source of monolingual text for the target language model was also evaluated. For the BTEC task, the LIMSI participated in the Turkish to English translation track with a system based on the open source Moses system [2]. The linguistic discrepancies between these two languages appear both Our in-house n-code SMT system implements the bilingual n-gram approach to statistical Machine Translation [3]. A translation hypothesis t given a source sentence s is defined as the sentence which maximizes a linear combination of feature functions: tˆI1 = arg max tI1 ( M X m=1 λm hm (sJ1 , tI1 ) ) , (1) where sJ1 and tI1 respectively denote the source and the target sentences, and λm is the weight associated with the feature function hm . The most important feature is the log-score of the translation model based on bilingual units called tuples. The probability assigned to a sentence pair by the translation model is estimated by using the n-gram assumption: p(sJ1 , tI1 ) = K Y k=1 p((s, t)k |(s, t)k"
2010.iwslt-evaluation.13,P03-1021,0,0.0080791,"Figure 1: Tuple extraction from a sentence pair. The resulting sequence of tuples (1) is further refined to avoid NULL words in source side of the tuples (2). Once the whole bilingual training data is segmented into tuples, n-gram language model probabilities can be estimated. In this example, note that the English source words perfect and translations have been reordered in the final tuple segmentation, while the French target words are kept in their original order. In addition to the translation model, eleven feature functions are optimally combined using a discriminative training framework [4]: a target-language model; four lexicon models; two lexicalized reordering models [5] aiming at predicting the orientation of the next translation unit; a ’weak’ distance-based distortion model; and finally a word-bonus model and a tuplebonus model which compensate for the system preference for short translations. The four lexicon models are similar to the ones use in a standard phrase based system: two scores correpond to the relative frequencies of All the available textual corpora are processed and normalized using in-house tools. Previous experiments revealed that using better normalizatio"
2010.iwslt-evaluation.13,N04-4026,0,0.0409215,"is further refined to avoid NULL words in source side of the tuples (2). Once the whole bilingual training data is segmented into tuples, n-gram language model probabilities can be estimated. In this example, note that the English source words perfect and translations have been reordered in the final tuple segmentation, while the French target words are kept in their original order. In addition to the translation model, eleven feature functions are optimally combined using a discriminative training framework [4]: a target-language model; four lexicon models; two lexicalized reordering models [5] aiming at predicting the orientation of the next translation unit; a ’weak’ distance-based distortion model; and finally a word-bonus model and a tuplebonus model which compensate for the system preference for short translations. The four lexicon models are similar to the ones use in a standard phrase based system: two scores correpond to the relative frequencies of All the available textual corpora are processed and normalized using in-house tools. Previous experiments revealed that using better normalization tools provides a significant reward in BLEU . The downside is the need to post-proc"
2010.iwslt-evaluation.13,C10-2023,1,0.883793,"Missing"
2010.iwslt-evaluation.13,W07-0704,1,0.827751,"Missing"
2010.iwslt-evaluation.13,P10-1047,0,0.0285202,"Missing"
2010.iwslt-evaluation.13,2009.iwslt-evaluation.2,0,0.0280005,"Missing"
2010.iwslt-evaluation.13,2009.iwslt-evaluation.6,0,0.0375883,"Missing"
2010.iwslt-evaluation.13,P06-1001,0,0.0377565,"Missing"
2010.iwslt-evaluation.13,popovic-ney-2004-towards,0,0.0520656,"Missing"
2010.iwslt-evaluation.13,H05-1085,0,0.0623309,"Missing"
2010.iwslt-evaluation.13,P08-1087,0,0.0340659,"Missing"
2010.iwslt-evaluation.13,P07-1017,0,0.0300031,"Missing"
2010.iwslt-evaluation.13,J04-2003,0,0.0867503,"Missing"
2010.iwslt-evaluation.13,corston-oliver-gamon-2004-normalizing,0,0.068379,"Missing"
2010.iwslt-evaluation.13,2005.mtsummit-papers.11,0,0.0224187,"Missing"
2010.iwslt-evaluation.13,E06-1006,0,0.0399082,"Missing"
2010.iwslt-evaluation.13,N04-4015,0,0.0798783,"Missing"
2010.iwslt-evaluation.13,2001.mtsummit-papers.45,0,0.0857026,"Missing"
2010.iwslt-evaluation.13,D10-1076,1,0.891217,"Missing"
2010.iwslt-evaluation.13,W06-3102,1,\N,Missing
2010.iwslt-evaluation.13,2009.iwslt-evaluation.5,0,\N,Missing
2010.jeptalnrecital-long.13,D09-1129,0,0.0627687,"Missing"
2010.jeptalnrecital-long.13,max-wisniewski-2010-mining,1,0.783004,"Missing"
2012.amta-papers.17,W05-0909,0,0.0369848,"boils ∗ This work was done while the first author was at LIMSI. The small number of features and the simplicity of the scoring model contrast with automatic evaluation metrics that hinge on complex quality measures, specially hand-crafted to mimic the human notion of translation quality. Because of the difficulty or even impossibility to adequately define the latter, quality measures generally depend on multiple inter-constrained characteristics describing the source sentence and the translation hypotheses. For instance, popular evaluation metrics, like BLEU (Papineni et al., 2002) or METEOR (Banerjee and Lavie, 2005), consider quantity and (fuzzy) alignments of common n-grams in a reference and a hypothesis. To sum up, approximating such complex quality measures with a linear combination of a few loosely related probabilistic features appears like a daunting task and there is little chance that current scoring functions can actually sort good translations from the remaining lot of hypotheses in the lowdimensional feature space. Indirect confirmation of the difficulty of this task comes from the inability of MERT’s advanced variants to come nearer to oracle BLEU scores or to substantially increase performa"
2012.amta-papers.17,N09-1025,0,0.0559689,"on is the main bottleneck of today’s SMT systems: the search space of decoders contains hypotheses of very high quality that are discarded because of their model score (Wisniewski et al., 2010; Sokolov et al., 2012; Turchi et al., 2012). The choice of a linear model seems mainly motivated by the simplicity of integrating the scoring function during decoding and of optimizing the model during training. It may also be motivated by the acknowledged success of linear models in many NLP tasks. The situation in SMT is quite different: while several SMT systems have been proposed that use thousands (Chiang et al., 2009) or millions (Lavergne et al., 2011) of features, in practice, however, the majority of available systems are based on linear scoring functions defined over a very small number of features (between 10 and 20). In the same time, most NLP systems routinely use million of features to achieve state-of-the-art performance. Introduction In modern statistical machine translation (SMT), the dominating approach to model the probability that sentence e is a translation of source sentence f is to use linear models (Och and Ney, 2002): p(e, a|f ) ∼ ¯ · g¯(a, e, f )), where a is an alignment between exp(λ"
2012.amta-papers.17,P08-2010,0,0.294298,"nstrated performance remains basically the same as for classical MERT (Hopkins and May, 2011). As we will see, while our gains are still modest, they are higher than those obtained with previous ranking approaches based on linear scoring functions. One method for deriving flexible scoring functions is boosting. Being an attractive learning algorithm, it was applied several times in the context of SMT. However, to the best of our knowledge all attempts concentrated on boosting for classification (like AdaBoost) and boosting from the ranking perspective was never applied to machine translation. Duh and Kirchhoff (2008) and Xiao et al. (2010) use the whole MERT procedure as a weak learner and maintain a distribution over n-best-lists to allow concentrating on the ones where, under current model, a winning hypothesis is too far from this nbest-lists’ oracle. The definition of BLEU needs to be changed to allow running MERT on weighted n-best-lists. The final model is a voting scheme of the linear models found on each invocation of weak MERT. Although in the end, a non-linear scoring function can be obtained, this non-linearity is a byproduct of the voting selection process and, contrary to our approach, is not"
2012.amta-papers.17,D11-1004,0,0.328202,"hypothesis. To sum up, approximating such complex quality measures with a linear combination of a few loosely related probabilistic features appears like a daunting task and there is little chance that current scoring functions can actually sort good translations from the remaining lot of hypotheses in the lowdimensional feature space. Indirect confirmation of the difficulty of this task comes from the inability of MERT’s advanced variants to come nearer to oracle BLEU scores or to substantially increase performance (Kumar et al., 2009), even when an almost exact optimization method is used (Galley and Quirk, 2011). Modeling inadequacy, and, in particular, the use of over-simplistic linear scoring functions in low-dimensional space can be held responsible for this disappointing performance. This paper can be seen as an attempt to verify whether the mere replacement of a linear with a nonlinear scoring function in a conventional phrasebased SMT system that uses only a few dozens features can actually improve performance, by capturing more precisely the complex boundaries between good and bad translations. The rest of the paper is organized as follows. In the next section, we review related work. In Secti"
2012.amta-papers.17,W11-2130,0,0.0893878,"ghted n-best-lists. The final model is a voting scheme of the linear models found on each invocation of weak MERT. Although in the end, a non-linear scoring function can be obtained, this non-linearity is a byproduct of the voting selection process and, contrary to our approach, is not constructed directly. Lagarda and Casacuberta (2008) apply AdaBoost by reweighting on each boosting iteration a separate “translation model” introduced into the linear model. Related Work 3 Recently, new approaches to tuning SMT systems have received attention, namely the ranking methods (Hopkins and May, 2011; Haddow et al., 2011). The motivation for these is as follows. Although BLEU is defined for a pair of corpora, one can use the same formula to calculate a sentence-level approximation of BLEU that evaluates the similarity between a single hypothesis e and its reference r, and to order hypotheses according to it. This natural ordering is used by ranking approaches in SMT to learn system parameters, taking advantage of the fact that one can deduce information about parameters even from the comparison between mediocre or bad hypotheses. Non-Linear Hypotheses Reranking Motivated by the inability of linear models to im"
2012.amta-papers.17,D11-1125,0,0.438729,"n, we review related work. In Section 3, we describe a ranking approach to tuning SMT systems, together with our method of learning a non-linear scoring function in the learning-to-rank paradigm. Next, we explain feature transformations (Section 4) used in the experiments reported in Section 5. Discussions in Section 6 close the paper. 2 Ranking approaches, however, were until now used only from the perspective of redefining the target loss in optimization. Scoring functions remained simple linear combinations, and the demonstrated performance remains basically the same as for classical MERT (Hopkins and May, 2011). As we will see, while our gains are still modest, they are higher than those obtained with previous ranking approaches based on linear scoring functions. One method for deriving flexible scoring functions is boosting. Being an attractive learning algorithm, it was applied several times in the context of SMT. However, to the best of our knowledge all attempts concentrated on boosting for classification (like AdaBoost) and boosting from the ranking perspective was never applied to machine translation. Duh and Kirchhoff (2008) and Xiao et al. (2010) use the whole MERT procedure as a weak learne"
2012.amta-papers.17,W04-3250,0,0.14744,"Missing"
2012.amta-papers.17,P09-1019,0,0.0500749,"sider quantity and (fuzzy) alignments of common n-grams in a reference and a hypothesis. To sum up, approximating such complex quality measures with a linear combination of a few loosely related probabilistic features appears like a daunting task and there is little chance that current scoring functions can actually sort good translations from the remaining lot of hypotheses in the lowdimensional feature space. Indirect confirmation of the difficulty of this task comes from the inability of MERT’s advanced variants to come nearer to oracle BLEU scores or to substantially increase performance (Kumar et al., 2009), even when an almost exact optimization method is used (Galley and Quirk, 2011). Modeling inadequacy, and, in particular, the use of over-simplistic linear scoring functions in low-dimensional space can be held responsible for this disappointing performance. This paper can be seen as an attempt to verify whether the mere replacement of a linear with a nonlinear scoring function in a conventional phrasebased SMT system that uses only a few dozens features can actually improve performance, by capturing more precisely the complex boundaries between good and bad translations. The rest of the pape"
2012.amta-papers.17,2008.eamt-1.14,0,0.49314,"use the whole MERT procedure as a weak learner and maintain a distribution over n-best-lists to allow concentrating on the ones where, under current model, a winning hypothesis is too far from this nbest-lists’ oracle. The definition of BLEU needs to be changed to allow running MERT on weighted n-best-lists. The final model is a voting scheme of the linear models found on each invocation of weak MERT. Although in the end, a non-linear scoring function can be obtained, this non-linearity is a byproduct of the voting selection process and, contrary to our approach, is not constructed directly. Lagarda and Casacuberta (2008) apply AdaBoost by reweighting on each boosting iteration a separate “translation model” introduced into the linear model. Related Work 3 Recently, new approaches to tuning SMT systems have received attention, namely the ranking methods (Hopkins and May, 2011; Haddow et al., 2011). The motivation for these is as follows. Although BLEU is defined for a pair of corpora, one can use the same formula to calculate a sentence-level approximation of BLEU that evaluates the similarity between a single hypothesis e and its reference r, and to order hypotheses according to it. This natural ordering is u"
2012.amta-papers.17,W11-2168,1,0.824934,"’s SMT systems: the search space of decoders contains hypotheses of very high quality that are discarded because of their model score (Wisniewski et al., 2010; Sokolov et al., 2012; Turchi et al., 2012). The choice of a linear model seems mainly motivated by the simplicity of integrating the scoring function during decoding and of optimizing the model during training. It may also be motivated by the acknowledged success of linear models in many NLP tasks. The situation in SMT is quite different: while several SMT systems have been proposed that use thousands (Chiang et al., 2009) or millions (Lavergne et al., 2011) of features, in practice, however, the majority of available systems are based on linear scoring functions defined over a very small number of features (between 10 and 20). In the same time, most NLP systems routinely use million of features to achieve state-of-the-art performance. Introduction In modern statistical machine translation (SMT), the dominating approach to model the probability that sentence e is a translation of source sentence f is to use linear models (Och and Ney, 2002): p(e, a|f ) ∼ ¯ · g¯(a, e, f )), where a is an alignment between exp(λ e and f , g¯(a, e, f ) is the featur"
2012.amta-papers.17,N12-1005,1,0.905364,"be done using the approximate “3-rd method” described in (Freund et al., 2003), the other two learners using a straightforward generalization of the same method. 4 4.1 Features Baseline Configurations We test our proposal on two decoder configurations that differ by the number of features considered. First, the basic configuration uses only the 11 features routinely found in any SMT decoder;2 the extended configuration contains an enriched set of 23 features and corresponds to the state-of-the-art translation system – the best system for the FrenchEnglish pair in the recent WMT’12 evaluation (Le et al., 2012b). The additional features considered are mainly based on neural network language models and translation models (Le et al., 2011; Le et al., 2012a). These features are integrated within a reranking step optimized with MERT.3 A summary 2 Target language model, translation model, 2 CFB lexicalized reordering models, 4 lexical translation weights, distortion and 2 penalties for words and phrases: 11 features in total. 3 To construct the extended feature set, the basic feature set was first augmented with two supplementary translation models on bilingual tuples and four lexicalized reordering fea"
2012.amta-papers.17,C04-1072,0,0.0834367,"om independent directions supplemental to the default axis-aligned direction. For the extended configuration, n-best lists of the last MERT iteration are augmented with 5 neuralnetwork models (Section 4.1) and reoptimized with MERT before applying features transformations. RankBoost training is performed on the WMT’09 evaluation set on 100-best and 300-best lists, respectively, for the basic and extended configurations, using the final n-best lists after the complete MERT optimization, separately for each MERT rerun. In all our experiments, we consider the sentence level BLEU+1 approximation (Lin and Och, 2004) to evaluate translation quality. Similarly to (Hopkins and May, 2011), to reduce the number of pairs and to speed up learning we sampled the n-best lists leaving only 2, 000 randomly selected pairs with quality difference superior to 0.05 BLEU points. Tests with different number of sampled pairs showed little sensitivity to this parameter in the range between 50 and 5, 000 pairs (outside of this interval the performance considerably decreases). Conversion to a bipartite ranking problem (that enables a more efficient and simpler algorithm (Freund et al., 2003)) with gaps did marginally help on"
2012.amta-papers.17,P02-1038,0,0.28591,"ent: while several SMT systems have been proposed that use thousands (Chiang et al., 2009) or millions (Lavergne et al., 2011) of features, in practice, however, the majority of available systems are based on linear scoring functions defined over a very small number of features (between 10 and 20). In the same time, most NLP systems routinely use million of features to achieve state-of-the-art performance. Introduction In modern statistical machine translation (SMT), the dominating approach to model the probability that sentence e is a translation of source sentence f is to use linear models (Och and Ney, 2002): p(e, a|f ) ∼ ¯ · g¯(a, e, f )), where a is an alignment between exp(λ e and f , g¯(a, e, f ) is the feature vector representing various compatibility measures between a, e and ¯ is a parameter vector. Using this model, f , and λ searching for the most probable translation boils ∗ This work was done while the first author was at LIMSI. The small number of features and the simplicity of the scoring model contrast with automatic evaluation metrics that hinge on complex quality measures, specially hand-crafted to mimic the human notion of translation quality. Because of the difficulty or even im"
2012.amta-papers.17,P02-1040,0,0.0835126,"for the most probable translation boils ∗ This work was done while the first author was at LIMSI. The small number of features and the simplicity of the scoring model contrast with automatic evaluation metrics that hinge on complex quality measures, specially hand-crafted to mimic the human notion of translation quality. Because of the difficulty or even impossibility to adequately define the latter, quality measures generally depend on multiple inter-constrained characteristics describing the source sentence and the translation hypotheses. For instance, popular evaluation metrics, like BLEU (Papineni et al., 2002) or METEOR (Banerjee and Lavie, 2005), consider quantity and (fuzzy) alignments of common n-grams in a reference and a hypothesis. To sum up, approximating such complex quality measures with a linear combination of a few loosely related probabilistic features appears like a daunting task and there is little chance that current scoring functions can actually sort good translations from the remaining lot of hypotheses in the lowdimensional feature space. Indirect confirmation of the difficulty of this task comes from the inability of MERT’s advanced variants to come nearer to oracle BLEU scores"
2012.amta-papers.17,W05-0908,0,0.0651604,"Missing"
2012.amta-papers.17,E12-1013,1,0.799177,"s approach, we rescore n-best lists generated with a conventional machine translation engine (using a linear scoring function for generating its hypotheses) with a non-linear scoring function learned using the learning-to-rank framework. Moderate, though consistent, gains in BLEU are demonstrated on the WMT’10, WMT’11 and WMT’12 test sets. Several papers have recently pointed out that the scoring function is the main bottleneck of today’s SMT systems: the search space of decoders contains hypotheses of very high quality that are discarded because of their model score (Wisniewski et al., 2010; Sokolov et al., 2012; Turchi et al., 2012). The choice of a linear model seems mainly motivated by the simplicity of integrating the scoring function during decoding and of optimizing the model during training. It may also be motivated by the acknowledged success of linear models in many NLP tasks. The situation in SMT is quite different: while several SMT systems have been proposed that use thousands (Chiang et al., 2009) or millions (Lavergne et al., 2011) of features, in practice, however, the majority of available systems are based on linear scoring functions defined over a very small number of features (betw"
2012.amta-papers.17,D10-1091,1,0.847116,"the applicability of this approach, we rescore n-best lists generated with a conventional machine translation engine (using a linear scoring function for generating its hypotheses) with a non-linear scoring function learned using the learning-to-rank framework. Moderate, though consistent, gains in BLEU are demonstrated on the WMT’10, WMT’11 and WMT’12 test sets. Several papers have recently pointed out that the scoring function is the main bottleneck of today’s SMT systems: the search space of decoders contains hypotheses of very high quality that are discarded because of their model score (Wisniewski et al., 2010; Sokolov et al., 2012; Turchi et al., 2012). The choice of a linear model seems mainly motivated by the simplicity of integrating the scoring function during decoding and of optimizing the model during training. It may also be motivated by the acknowledged success of linear models in many NLP tasks. The situation in SMT is quite different: while several SMT systems have been proposed that use thousands (Chiang et al., 2009) or millions (Lavergne et al., 2011) of features, in practice, however, the majority of available systems are based on linear scoring functions defined over a very small nu"
2012.amta-papers.17,P10-1076,0,0.095874,"basically the same as for classical MERT (Hopkins and May, 2011). As we will see, while our gains are still modest, they are higher than those obtained with previous ranking approaches based on linear scoring functions. One method for deriving flexible scoring functions is boosting. Being an attractive learning algorithm, it was applied several times in the context of SMT. However, to the best of our knowledge all attempts concentrated on boosting for classification (like AdaBoost) and boosting from the ranking perspective was never applied to machine translation. Duh and Kirchhoff (2008) and Xiao et al. (2010) use the whole MERT procedure as a weak learner and maintain a distribution over n-best-lists to allow concentrating on the ones where, under current model, a winning hypothesis is too far from this nbest-lists’ oracle. The definition of BLEU needs to be changed to allow running MERT on weighted n-best-lists. The final model is a voting scheme of the linear models found on each invocation of weak MERT. Although in the end, a non-linear scoring function can be obtained, this non-linearity is a byproduct of the voting selection process and, contrary to our approach, is not constructed directly."
2013.mtsummit-papers.15,W06-3114,0,0.0170076,"on the T RACE corpus,1 a new, large corpus of French to English and English to French post-editions, which has been recently assembled using data collected from a public web portal and from datasets used in MT evaluation campaigns. The second contribution of this work is a study of the variability of post-edition, a question that the growing role of the TER score, both in MT evaluation and as a measure of the post-edition effort,2 makes more and more important. Since it has long been recognized that MT evaluation (especially at the sentence level) is plagued with a low inter-rater agreement (Koehn and Monz, 2006), it seems appropriate to raise the same issues in relationship to the QE task. Our analysis relies on a subpart of the T RACE corpus containing automatic translations that have been post-edited independently by two translators. To the best of our knowledge, this is the first time that several post-editions of the same sentences have been collected, allowing us to perform both a qualitative comparison of the differences between the post-editions of two translators as well as a quantitative analysis of the inter-rater agreement for the hTER score. The rest of the paper is organized as follows."
2013.mtsummit-papers.15,E06-1032,0,0.0322995,"o compute standard MT metrics on the automatic output. As reflected in Table 1 for the English to French direction, the metric values are much higher than what is usually observed in MT evaluation campaigns. This shows that the post-edited references are indeed much closer to the translations than the references used in these campaigns. For instance, when S YS S TAT is evaluated against the references of the WMT campaign, its TER score is 56.27, nearly twice as worse as when evaluated using post-edited translations as reference. It should also be noted that, as mentioned in many past studies (Callison-Burch et al., 2006), rule-based systems are highly disfavored by automatic metrics. 3 Failure Analysis of MT systems We show, in this section, how comparing translation hypotheses with their post-editions can help identify and analyze failures of MT systems. For space reasons, only results for the English to French direction are presented. 3.1 Error Patterns By computing the edit distance at the word-level between translation hypotheses and their postedition, it is possible to automatically detect the modifications required to make MT output both fluent and adequate. The careful analysis of the most frequent cor"
2013.mtsummit-papers.15,W12-3102,0,0.572737,"in order to correct the translations in terms of fluency and adequacy, is becoming more and more popular both to produce human-quality translations at a reduced cost (Garcia, 2011) or to evaluate the quality of MT systems. Indeed, the hTER score (Snover et al., 2006), which depends on the number of editions required to transform a MT hypothesis into a correct (post-edited) translation has proved to be a good indicator of the quality of a MT system. With the development of post-edition, more and more datasets of post-edited translations are being collected and distributed (Potet et al., 2012; Callison-Burch et al., 2012). These corpora have been accumulated in the context of MT evaluation campaigns and have mainly been used to estimate translation quality. They can also serve several other purposes: our first contribution is to show how they can be used to identify and analyze the limits of a MT system and to train a quality estimation (QE) system. For these tasks we present results achieved on the T RACE corpus,1 a new, large corpus of French to English and English to French post-editions, which has been recently assembled using data collected from a public web portal and from datasets used in MT evaluation"
2013.mtsummit-papers.15,2012.eamt-1.60,0,0.0137318,"ish to French direction. For the two directions, 1, 000 additional sentences that have been post-edited independently by two translators have also been prepared. These corpora can be freely downloaded from the T RACE website. Half of the source sentences have been collected through a public web portal which serves each month several millions of translation requests between French and English. These requests cover a wide variety of genres and domains. The other half of the corpus is made of parts of the datasets provided by MT evaluation campaigns (WMT3 (Callison-Burch et al., 2012) and IWSLT (Cettolo et al., 2012)) and by Word Sense Disambiguation campaigns (Lefever and Hoste, 2010). Examples from this part of the corpus are accompanied by additional information provided by the campaigns organizers such as reference translations or semantic annotations. These sentences have been translated by two MT systems: the first one, denoted by S YS RULE, is a commercial rule-based system; the second, denoted S YS S TAT, a state-of-the-art phrase-based statistical MT system developed for the WMT’12 evaluation campaign (Le et al., 2012). Precise guidelines were given to the translators to ensure that the correctio"
2013.mtsummit-papers.15,2004.tmi-1.8,0,0.0127131,"le a a` dans que en un des Deletion 799 335 329 278 277 256 242 215 212 167 de a` la le que les en et des pour Table 3: Most frequent editions. 3.2 Differences between Automatic Translations and their Post-Edition To characterize the differences between automatic translations and their post-edition, we propose to learn a classifier that could distinguish between these two kinds of translations. We hope that finding which features are relevant for making this distinction will provide us some insight about the limits of MT systems. This approach is directly inspired by earlier work in QE like (Kulesza and Shieber, 2004), where the authors try to learn the difference between a good and a bad translation. In the experiments described in this section, each translation is represented by 336 numerical features, most of which are inspired by works in QE for MT (Callison-Burch et al., 2012).4 These features can be classified into four categories: • Association Features: Measures of the quality of the ‘association’ between the source and the target sentences like, for instance, features derived from the IBM 1 model scores; • Fluency Features: Measures of the ‘fluency’ or the ‘grammaticality’ of the target and source"
2013.mtsummit-papers.15,S10-1003,0,0.0430789,"sentences that have been post-edited independently by two translators have also been prepared. These corpora can be freely downloaded from the T RACE website. Half of the source sentences have been collected through a public web portal which serves each month several millions of translation requests between French and English. These requests cover a wide variety of genres and domains. The other half of the corpus is made of parts of the datasets provided by MT evaluation campaigns (WMT3 (Callison-Burch et al., 2012) and IWSLT (Cettolo et al., 2012)) and by Word Sense Disambiguation campaigns (Lefever and Hoste, 2010). Examples from this part of the corpus are accompanied by additional information provided by the campaigns organizers such as reference translations or semantic annotations. These sentences have been translated by two MT systems: the first one, denoted by S YS RULE, is a commercial rule-based system; the second, denoted S YS S TAT, a state-of-the-art phrase-based statistical MT system developed for the WMT’12 evaluation campaign (Le et al., 2012). Precise guidelines were given to the translators to ensure that the corrections of the automatic translations were minimal: they were asked to prod"
2013.mtsummit-papers.15,D12-1077,0,0.0204608,"Missing"
2013.mtsummit-papers.15,potet-etal-2012-collection,0,0.0767283,"Missing"
2013.mtsummit-papers.15,2006.amta-papers.25,0,0.0359803,"these data, notably the development of an automatic Quality Estimation (QE) system and the detection of frequent errors in automatic translations. Both applications require a careful assessment of the variability in post-editions, that we study here. 1 Introduction Post-editing, the process of editing the outputs of a Machine Translation (MT) system in order to correct the translations in terms of fluency and adequacy, is becoming more and more popular both to produce human-quality translations at a reduced cost (Garcia, 2011) or to evaluate the quality of MT systems. Indeed, the hTER score (Snover et al., 2006), which depends on the number of editions required to transform a MT hypothesis into a correct (post-edited) translation has proved to be a good indicator of the quality of a MT system. With the development of post-edition, more and more datasets of post-edited translations are being collected and distributed (Potet et al., 2012; Callison-Burch et al., 2012). These corpora have been accumulated in the context of MT evaluation campaigns and have mainly been used to estimate translation quality. They can also serve several other purposes: our first contribution is to show how they can be used to"
2013.mtsummit-papers.15,specia-etal-2010-dataset,0,0.0962939,"Missing"
2013.mtsummit-papers.15,2013.tc-1.10,0,0.0399314,"Missing"
2015.jeptalnrecital-long.1,J07-2003,0,0.0550672,"Missing"
2015.jeptalnrecital-long.1,P04-1015,0,0.388473,"Missing"
2015.jeptalnrecital-long.1,W06-1673,0,0.0835146,"Missing"
2015.jeptalnrecital-long.1,E14-1002,0,0.0208889,"Missing"
2015.jeptalnrecital-long.1,N10-1115,0,0.0628036,"Missing"
2015.jeptalnrecital-long.1,C12-1059,0,0.155152,"Missing"
2015.jeptalnrecital-long.1,Q13-1033,0,0.159446,"Missing"
2015.jeptalnrecital-long.1,P10-1052,1,0.852088,"Missing"
2015.jeptalnrecital-long.1,C12-1106,0,0.0334341,"Missing"
2015.jeptalnrecital-long.1,P07-1096,0,0.0800876,"Missing"
2015.jeptalnrecital-long.1,W03-3023,0,0.0673149,"Missing"
2015.jeptalnrecital-long.4,N10-1066,0,0.0751533,"Missing"
2015.jeptalnrecital-long.4,P05-1022,0,0.0847008,"Missing"
2015.jeptalnrecital-long.4,J13-1005,0,0.0499864,"Missing"
2015.jeptalnrecital-long.4,P08-1085,0,0.0604285,"Missing"
2015.jeptalnrecital-long.4,A00-2013,0,0.166506,"Missing"
2015.jeptalnrecital-long.4,J00-4006,0,0.0394026,"Missing"
2015.jeptalnrecital-long.4,D12-1127,0,0.0294932,"Missing"
2015.jeptalnrecital-long.4,J94-2001,0,0.681755,"Missing"
2015.jeptalnrecital-long.4,C14-1110,0,0.0433613,"Missing"
2015.jeptalnrecital-long.4,D13-1032,0,0.0356679,"Missing"
2015.jeptalnrecital-long.4,W96-0213,0,0.79269,"Missing"
2015.jeptalnrecital-long.4,P09-1057,0,0.0433144,"Missing"
2015.jeptalnrecital-long.4,P14-2043,0,0.0337427,"Missing"
2015.jeptalnrecital-long.4,P05-1044,0,0.0758103,"Missing"
2015.jeptalnrecital-long.4,H05-1060,0,0.0593043,"Missing"
2015.jeptalnrecital-long.4,Q13-1001,0,0.0354823,"Missing"
2015.jeptalnrecital-long.4,C12-1170,0,0.0540179,"Missing"
2015.jeptalnrecital-long.4,D14-1187,1,0.887622,"Missing"
2016.jeptalnrecital-long.1,P81-1022,0,0.645685,"Missing"
2016.jeptalnrecital-long.1,D11-1005,0,0.0583432,"Missing"
2016.jeptalnrecital-long.1,N13-1073,0,0.0577735,"Missing"
2016.jeptalnrecital-long.1,C12-1059,0,0.0665175,"Missing"
2016.jeptalnrecital-long.1,2005.mtsummit-papers.11,0,0.0535649,"Missing"
2016.jeptalnrecital-long.1,J10-4005,0,0.0632718,"Missing"
2016.jeptalnrecital-long.1,C14-1075,0,0.029758,"Missing"
2016.jeptalnrecital-long.1,P14-1126,0,0.0382447,"Missing"
2016.jeptalnrecital-long.1,P13-2017,0,0.0311513,"Missing"
2016.jeptalnrecital-long.1,D11-1006,0,0.0608004,"Missing"
2016.jeptalnrecital-long.1,D10-1120,0,0.0828629,"Missing"
2016.jeptalnrecital-long.1,petrov-etal-2012-universal,0,0.0748813,"Missing"
2016.jeptalnrecital-long.1,D15-1039,0,0.0211998,"Missing"
2016.jeptalnrecital-long.1,P11-2120,0,0.0439935,"Missing"
2016.jeptalnrecital-long.1,W09-1104,0,0.0711999,"Missing"
2016.jeptalnrecital-long.1,N13-1126,0,0.0307885,"Missing"
2016.jeptalnrecital-long.1,C14-1175,0,0.0251049,"Missing"
2016.jeptalnrecital-long.1,W14-1614,0,0.024948,"Missing"
2016.jeptalnrecital-long.1,I08-3008,0,0.0900704,"Missing"
2016.jeptalnrecital-long.1,P11-2033,0,0.0870376,"Missing"
2016.jeptalnrecital-long.19,2015.jeptalnrecital-long.25,0,0.0863147,"Missing"
2016.jeptalnrecital-long.19,W02-1001,0,0.384938,"Missing"
2016.jeptalnrecital-long.19,P04-1015,0,0.196224,"Missing"
2016.jeptalnrecital-long.19,C12-1059,0,0.0473173,"Missing"
2016.jeptalnrecital-long.19,Q13-1033,0,0.0339019,"Missing"
2016.jeptalnrecital-long.19,D11-1125,0,0.0383654,"Missing"
2016.jeptalnrecital-long.19,N12-1015,0,0.0451411,"Missing"
2016.jeptalnrecital-long.19,2015.jeptalnrecital-long.1,1,0.834506,"Missing"
2016.jeptalnrecital-long.19,J08-4003,0,0.0276826,"Missing"
2016.jeptalnrecital-long.19,P02-1038,0,0.0406177,"Missing"
2016.jeptalnrecital-long.19,W14-6111,0,0.0604365,"Missing"
2016.jeptalnrecital-long.19,P11-2033,0,0.0713007,"Missing"
2016.jeptalnrecital-long.19,C12-2136,0,0.0394061,"Missing"
2016.jeptalnrecital-poster.23,D11-1033,0,0.170288,"Missing"
2016.jeptalnrecital-poster.23,W07-0717,0,0.362527,"Missing"
2016.jeptalnrecital-poster.23,2005.mtsummit-papers.11,0,0.0247511,"Missing"
2016.jeptalnrecital-poster.23,P07-2045,0,0.00768147,"Missing"
2016.jeptalnrecital-poster.23,L16-1147,0,0.0818109,"Missing"
2016.jeptalnrecital-poster.23,C12-1135,0,0.557166,"Missing"
2016.jeptalnrecital-poster.23,roy-etal-2014-tvd,0,0.163061,"Missing"
2017.jeptalnrecital-court.17,W02-1002,0,0.116147,"Missing"
2017.jeptalnrecital-court.17,W16-3905,0,0.0323013,"Missing"
2017.jeptalnrecital-court.17,2015.jeptalnrecital-long.4,1,0.852163,"Missing"
2017.jeptalnrecital-court.17,2015.jeptalnrecital-court.29,0,0.103588,"Missing"
2017.jeptalnrecital-court.17,C12-1149,0,0.111228,"Missing"
2017.jeptalnrecital-court.17,W13-4917,0,0.0214041,"Missing"
2017.jeptalnrecital-court.17,L16-1680,0,0.0514621,"Missing"
2017.jeptalnrecital-court.17,W11-0328,0,0.0411679,"Missing"
2017.jeptalnrecital-court.17,D13-1117,0,0.0567552,"Missing"
2017.jeptalnrecital-court.17,D14-1187,1,0.727723,"Missing"
2017.jeptalnrecital-court.17,F14-1016,1,0.734185,"Missing"
2018.jeptalnrecital-court.31,H92-1026,0,0.500432,"Missing"
2018.jeptalnrecital-court.31,W02-1001,0,0.263251,"Missing"
2018.jeptalnrecital-court.31,P12-3005,0,0.0353476,"Missing"
2018.jeptalnrecital-court.31,W16-5805,0,0.0610453,"Missing"
2018.jeptalnrecital-court.31,E17-5001,0,0.0382863,"Missing"
2018.jeptalnrecital-court.31,L16-1680,0,0.0205617,"Missing"
2018.jeptalnrecital-court.31,W11-0328,0,0.0291972,"Missing"
2018.jeptalnrecital-court.31,D14-1187,1,0.874135,"Missing"
2018.jeptalnrecital-court.31,L16-1667,0,0.0437051,"Missing"
2018.jeptalnrecital-court.41,2017.jeptalnrecital-court.17,1,0.830804,"Missing"
2018.jeptalnrecital-court.41,H92-1026,0,0.174031,"Missing"
2018.jeptalnrecital-court.41,W13-2308,0,0.0624263,"Missing"
2018.jeptalnrecital-court.41,F12-2024,0,0.0720861,"Missing"
2018.jeptalnrecital-court.41,E03-1068,0,0.162753,"Missing"
2018.jeptalnrecital-court.41,E14-4028,0,0.0507499,"Missing"
2018.jeptalnrecital-court.41,E17-5001,0,0.0485342,"Missing"
2018.jeptalnrecital-court.41,D14-1104,0,0.067159,"Missing"
2018.jeptalnrecital-court.41,C12-1149,0,0.0350381,"Missing"
2018.jeptalnrecital-court.41,K17-3016,0,0.0436645,"Missing"
2018.jeptalnrecital-court.41,D14-1187,1,0.858128,"Missing"
2018.jeptalnrecital-court.41,P11-2033,0,0.0239797,"Missing"
2019.jeptalnrecital-court.7,K17-3017,1,0.794778,"Missing"
2019.jeptalnrecital-court.7,2017.jeptalnrecital-court.17,1,0.847021,"Missing"
2019.jeptalnrecital-court.7,H92-1026,0,0.20143,"Missing"
2019.jeptalnrecital-court.7,F12-2024,0,0.0454432,"Missing"
2019.jeptalnrecital-court.7,W17-4756,0,0.0357373,"Missing"
2019.jeptalnrecital-court.7,K17-3009,0,0.0427855,"Missing"
2019.jeptalnrecital-court.7,N03-1033,0,0.165557,"Missing"
2019.jeptalnrecital-court.7,W00-1308,0,0.42539,"Missing"
2019.jeptalnrecital-court.7,D14-1187,1,0.862107,"Missing"
2019.jeptalnrecital-court.7,P11-2033,0,0.0251147,"Missing"
2020.jeptalnrecital-jep.51,2020.iwclul-1.5,0,0.0801215,"Missing"
2020.jeptalnrecital-jep.51,C18-1222,0,0.0507456,"Missing"
2020.jeptalnrecital-jep.51,2020.sltu-1.48,0,0.0476118,"Missing"
2020.jeptalnrecital-jep.51,W19-6003,0,0.0200716,"Missing"
2020.jeptalnrecital-jep.51,2020.sltu-1.43,1,0.832295,"Missing"
2020.sltu-1.43,L18-1530,1,0.271625,"s: Endangered Languages, Speech Recognition/Understanding, Speech Resource/Database 1. 1.1. Introduction Making Language Archive Data Tractable to Automatic Speech Processing: Why Preprocessing is a Key Issue Towards Computational Language Documentation Automatic speech recognition (ASR) tools have potential for facilitating the urgent task of documenting the world’s dwindling linguistic diversity (Besacier et al., 2014; Thieberger, 2017; Littell et al., 2018; van Esch et al., 2019). Encouraging results for automatic phoneme recognition for low-resource languages were published two years ago (Adams et al., 2018), and prospects of widespread deployment of the technology look extremely hopeful. Why Preprocessing is a Major Hurdle Various methodological hurdles are encountered in the course of this exciting development, however. A well-identified difficulty is that data preprocessing is not at all trivial. In classical linguistic fieldwork (Bouquiaux and Thomas, 1971; Newman and Ratliff, 2001; Dixon, 2007), “good corpus production is ongoing, distributed, and opportunistic” (Woodbury, 2003, 47), and thus unlike scenarios in which data acquisition is tailored to meet the requirements of speech processing"
2020.sltu-1.43,brugman-russel-2004-annotating,0,0.0614198,"ich data acquisition is tailored to meet the requirements of speech processing tasks. Because fieldwork data are not collected specifically for the purpose of ASR, data sets from language archives are highly diverse in a number of respects. Not only is there a wide range of tools for creating linguistic annotations, each with its own format (see the conversion tools TEIconvert http://ct3.ortolang. fr/teiconvert/ and Multitool https://github. com/DoReCo/multitool): there is also diversity in the formats allowed by one and the same software package. Thus, ELAN, a commonly used software package (Brugman and Russel, 2004), allows users to define their own document structures: ELAN supports creation of multiple tiers and tier hierarchies, so that there is, in practice, no such thing as a unique “ELAN format”. It would be desirable for a common format to be adopted in the mid run, such as the standard proposed as part of the Text Encoding Initiative (Schmidt, 2011; Li´egeois et al., 2016), but convergence is not in sight yet. In the current situation, fieldwork data make up “eclectic data collections” rather than “systematically annotated corpora” (Gerstenberger et al., 2017, 26). Preprocessing typically involve"
2020.sltu-1.43,W17-0604,0,0.0254368,"LAN, a commonly used software package (Brugman and Russel, 2004), allows users to define their own document structures: ELAN supports creation of multiple tiers and tier hierarchies, so that there is, in practice, no such thing as a unique “ELAN format”. It would be desirable for a common format to be adopted in the mid run, such as the standard proposed as part of the Text Encoding Initiative (Schmidt, 2011; Li´egeois et al., 2016), but convergence is not in sight yet. In the current situation, fieldwork data make up “eclectic data collections” rather than “systematically annotated corpora” (Gerstenberger et al., 2017, 26). Preprocessing typically involves retrieving pieces of information that are not encoded according to widely shared computational standards. Preprocessing tasks are not just time-consuming: they require familiarity with the target language, and with the specific corpus. This is asking a lot from Natural Language Processing people who wish to try their hand at the data. An example (preprocessing transcriptions of Yongning Na, a Sino-Tibetan language, for training an acoustic model using the Persephone toolkit) is documented in some detail in an article that aims to explain to an audience o"
2020.sltu-1.43,C18-1222,0,0.0782263,"n for Natural Language Processing (NLP) purposes. What is at stake is the accessibility of language archive data for a range of NLP tasks and beyond. Keywords: Endangered Languages, Speech Recognition/Understanding, Speech Resource/Database 1. 1.1. Introduction Making Language Archive Data Tractable to Automatic Speech Processing: Why Preprocessing is a Key Issue Towards Computational Language Documentation Automatic speech recognition (ASR) tools have potential for facilitating the urgent task of documenting the world’s dwindling linguistic diversity (Besacier et al., 2014; Thieberger, 2017; Littell et al., 2018; van Esch et al., 2019). Encouraging results for automatic phoneme recognition for low-resource languages were published two years ago (Adams et al., 2018), and prospects of widespread deployment of the technology look extremely hopeful. Why Preprocessing is a Major Hurdle Various methodological hurdles are encountered in the course of this exciting development, however. A well-identified difficulty is that data preprocessing is not at all trivial. In classical linguistic fieldwork (Bouquiaux and Thomas, 1971; Newman and Ratliff, 2001; Dixon, 2007), “good corpus production is ongoing, distrib"
2020.sltu-1.43,strunk-etal-2014-untrained,0,0.0298791,"me) appear as TRANSL elements, with a tag indicating the translation language using two-letter codes: ""fr"", ""en"" and ""zh"" for French, English and Chinese, respectively. Word-level information likewise contains FORM elements and TRANSL elements. Note that orthographic representation and Chinese transla2 All our experiments are based on corpora freely available from the Pangloss Collection, an open archive of (mostly) endangered languages. See § 2.3. for details. 3 There are possibilities for adding word-level and phonemelevel time codes to linguistic fieldwork documents using forced alignment (Strunk et al., 2014). In the current state of language archives, phoneme-level alignment remains a rarity, however. Figure 1: Sample of XML code: beginning of sentence 20 of the narrative “The sister’s wedding” (https://doi. org/10.24397/pangloss-0004342#S20). tions are only offered at the level of the sentence, not for each word. Most pieces of information are optional: wordlevel glosses are not mandatory, any more than translation into any specific language. Linguists who contribute data to the language archive deposit their documents as is, with the levels of annotation that they chose to produce for the sake"
2020.sltu-1.43,W19-6003,0,0.354423,"Missing"
2021.blackboxnlp-1.24,2020.cl-1.1,0,0.0264855,"tuitive result is consistent with several observations made in the literature: the fact that a ‘linguistic’ information is encoded in the neural representations does not imply that it will be used by the neural network (see, for instance, (Belinkov and Glass, 2019)). This suggests that the information flow along the path denoted (c) in Figure 3 should be small and the choice of the English possessive pronoun is based on other information than the representation of son. 7 Related Work been used in several works to study the information flow within an encoder-decoder architecture: for instance, Belinkov et al. (2020) rely on probes to find which components of a NMT system encode linguistic information when translating morphologically rich languages. However, to the best of our knowledge, this work is the first to use the differences between gender expression in French and English to get insights into the inner representations used in NMT systems based on the Transformer architecture. Experiments reported in Section 6 are inspired by causal analysis, a type of analysis that has been used by Vig et al. (2020) to analyze gender bias in neural monolingual NLP models. Several studies have investigated gender b"
2021.blackboxnlp-1.24,2020.findings-emnlp.180,0,0.0349815,"Missing"
2021.blackboxnlp-1.24,D19-1530,0,0.011392,"optimised for speed. Using a dictionary of occupations for English to Spanish and English to German, they showed that correct translation rates degrade much faster than BLEU scores when limiting the beamsize to 1 during beam search or using low-bit quantization. Finally, another line of research focuses on mitigating gender bias. This can be either achieved by working on the system’s internal represention (Escudé Font and Costa-jussà, 2019), or by creating a more balanced training data where occupational roles are equally distributed between genders via counterfactual data augmentation (Hall Maudslay et al., 2019; Zmigrod et al., 2019). As discussed in (Saunders and Byrne, 2020), a cheaper, yet effective alternative to data augmentation, is to resort to domain adaptation techniques. 8 Discussion and Conclusion Our paper investigated the different pathways for gender transfer. We created a dataset inspired by previous research to test several hypotheses. Our novel contribution is that we simultaneously mobilized several techniques, probing and manipulating. We extended the scope of the investigation of the locus of gender transfer beyond the determiner/noun analysis of Costa-jussà et al. (2020a) and qu"
2021.blackboxnlp-1.24,D19-1275,0,0.0375186,"Missing"
2021.blackboxnlp-1.24,D19-3019,0,0.0210302,"‘him’) were created when the gender of the profession for each of these cases. can not be inferred from the French sentence. For instance, Conversely, in English sentences, gender infor- the French sentence ‘l’artiste a terminé son travail’ appears twice in the parallel corpora: the first time as the translation mation is always overtly expressed in the English of ‘the artist has finished herF work’, the second time as the pronoun, and in rare cases, also in the English noun translation of ‘the artist has finished hisM work’. 313 3 Experimental Setting In all our experiments, we use JoeyNMT9 (Kreutzer et al., 2019), an educational implementation of a translation system based on the Transformer model of Vaswani et al. (2017). The simplicity of the codebase, which nonetheless allowed us to achieve near SOTA performance on our data, made it a perfect choice for our endeavor. In our system, encoder and decoder are composed of 6 layers, each with 8 attention heads; the feedforward layers have 2,048 parameters and the dimension of lexical embeddings is 512. Our model comprises a grand total of 76,596,736 parameters. The system was trained with data from the ‘News’ task of the WMT’2015 evaluation campaign.10 I"
2021.blackboxnlp-1.24,P18-1007,0,0.0214885,"n our system, encoder and decoder are composed of 6 layers, each with 8 attention heads; the feedforward layers have 2,048 parameters and the dimension of lexical embeddings is 512. Our model comprises a grand total of 76,596,736 parameters. The system was trained with data from the ‘News’ task of the WMT’2015 evaluation campaign.10 It includes the Europarl, NewsCommentary and CommonCrawl corpora, and altogether contains 4,813,682 sentences and nearly 141 million French running words. All the corpora were tokenized and segmented into sub-lexical units using the unigram model of SentencePiece (Kudo, 2018); the resulting vocabularies contain 32,000 units in each language. The model is trained by optimizing the cross-entropy using the A DAM strategy. This system achieves a BLEU score of 34.0 for the French-English direction. 4 Evaluation of Gender Translation 4.1 Experimental Results We evaluate the ability of our system to predict the gender of occupational nouns using the corpus described in Section 2 and consider, as a point of comparison, the translations generated by e-translation, a translation system developed by the European Commission that is freely accessible for academic research.11 W"
2021.blackboxnlp-1.24,J94-4004,0,0.668093,"a controlled set of examples, we experiment several ways to investigate how gender information circulates in a encoder-decoder architecture considering both probing techniques as well as interventions on the internal representations used in the MT system. Our results show that gender information can be found in all token representations built by the encoder and the decoder and lead us to conclude that there are multiple pathways for gender transfer. 1 Introduction The existence of translation divergences (i.e. crosslinguistic distinctions) raises many challenges for machine translation (MT) (Dorr, 1994): when translating a sentence, some information or constructions are specific to the target language and, consequently, can only be inferred by the decoder from the target context; some are only found in the source language and have to be ignored; finally, some information has to be adapted and transferred from the encoder to the decoder. Contrary to previous generations of MT engines where transfer rules were quite transparent, understanding this information flow within state-of-the-art neural MT systems is a challenging task, and a key step for their interpretability. To illustrate these alt"
2021.blackboxnlp-1.24,W19-3821,0,0.0153134,"ual models seem to rely more on the determiner. In the language-specific case, the embeddings are reported to encode more gender information. der bias is amplified when the system is optimised for speed. Using a dictionary of occupations for English to Spanish and English to German, they showed that correct translation rates degrade much faster than BLEU scores when limiting the beamsize to 1 during beam search or using low-bit quantization. Finally, another line of research focuses on mitigating gender bias. This can be either achieved by working on the system’s internal represention (Escudé Font and Costa-jussà, 2019), or by creating a more balanced training data where occupational roles are equally distributed between genders via counterfactual data augmentation (Hall Maudslay et al., 2019; Zmigrod et al., 2019). As discussed in (Saunders and Byrne, 2020), a cheaper, yet effective alternative to data augmentation, is to resort to domain adaptation techniques. 8 Discussion and Conclusion Our paper investigated the different pathways for gender transfer. We created a dataset inspired by previous research to test several hypotheses. Our novel contribution is that we simultaneously mobilized several technique"
2021.blackboxnlp-1.24,2021.acl-short.15,0,0.0413944,"Missing"
2021.blackboxnlp-1.24,N18-2002,0,0.04674,"Missing"
2021.blackboxnlp-1.24,N18-2003,0,0.0233495,"42 ESCO occupational nouns, she evidenced a gender gap by comparing the translations from Google Translate, DeepL and Microsoft Translator in the two directions for the French/Italian language pair. She built a dataset with respectively “competence” (i.e. intelligent) and “appearance” (i.e. beautiful) adjectives (ADJ) in the following pattern &lt;A very [ADJ] [N] entered the room&gt;. The data was manually analyzed. Adjectives seem to have no influence for the translation of masculine nouns, but competence adjectives affect the translation of feminine nouns more severely than appearance adjectives. Zhao et al. (2018) studies gender bias in ELMo embeddings using probing techniques. In this study, biases in the embeddings also implied biases in a pronoun reference resolution task using the WinoGender dataset. Balancing data, and using averaged representations, to a certain extend, helped remove this bias. Analyzing misclassified occupations in terms of gender, Costa-jussà et al. (2020a) investigated the architectural bias for the translation of occupational nouns, suggesting that using language-specific encoders and decoder yields less bias than a shared encoder-decoder architecture. Considering the attenti"
2021.blackboxnlp-1.24,P19-1161,0,0.0176233,"sing a dictionary of occupations for English to Spanish and English to German, they showed that correct translation rates degrade much faster than BLEU scores when limiting the beamsize to 1 during beam search or using low-bit quantization. Finally, another line of research focuses on mitigating gender bias. This can be either achieved by working on the system’s internal represention (Escudé Font and Costa-jussà, 2019), or by creating a more balanced training data where occupational roles are equally distributed between genders via counterfactual data augmentation (Hall Maudslay et al., 2019; Zmigrod et al., 2019). As discussed in (Saunders and Byrne, 2020), a cheaper, yet effective alternative to data augmentation, is to resort to domain adaptation techniques. 8 Discussion and Conclusion Our paper investigated the different pathways for gender transfer. We created a dataset inspired by previous research to test several hypotheses. Our novel contribution is that we simultaneously mobilized several techniques, probing and manipulating. We extended the scope of the investigation of the locus of gender transfer beyond the determiner/noun analysis of Costa-jussà et al. (2020a) and questioned the role of pr"
2021.blackboxnlp-1.24,2020.acl-main.690,0,0.198927,"nouns used to create the corpus can be found in the training set. This is also reflected in Figure 2, where we see that most occupational nouns are tokenized into multiple BPE units. These sentences were automatically translated and manually verified to produce the corresponding English list. The motivations for using these fixed syntactic patterns are many. First, they limit the only source of variability between sentences to the [N] slots, allowing us to perform controlled experiments. Second, they simplify the analysis and manipulation 3 Using a simplified list from (Prates et al., 2020), Saunders and Byrne (2020) created a “handcrafted” dataset of 388 parallel sentences of the type The [PROFESSION] finished [his|her] work. for three translation directions (EnglishSpanish, English-German and English-Hebrew). In this paper, we adapted this approach for a new translation direction (French to English) using a much larger list of occupational nouns: our corpus contains 3,394 sentences. 4 The French determiner is l’ for both genres if the job noun begins with a vowel. 5 This dataset can be found at https://github.com/ neuroviz/neuroviz/tree/main/blackbox2021. 40 28.7 0, 00 0 000 20 ≤ 10 The [N] has finished"
2021.blackboxnlp-1.24,P19-1164,0,0.0150602,"rmer architecture. Experiments reported in Section 6 are inspired by causal analysis, a type of analysis that has been used by Vig et al. (2020) to analyze gender bias in neural monolingual NLP models. Several studies have investigated gender bias using dedicated datasets, some of them presented at the ACL Workshop on Gender Bias in Natural Language Processing (Costa-jussà et al., 2019, 2020b; Costa-jussa et al., 2021). Savoldi et al. (2021) synthesizes the studies and datasets on gender bias for translation. In particular, the controlled test set considered in our work builds on the works of Stanovsky et al. (2019) and Saunders and Byrne (2020), who both propose challenge test sets to evaluate gender bias in MT systems. The corresponding datasets consider the translation of occupational nouns with an anaphoric reference that makes gender explicit: the former contains instances of difficult translation patterns inspired by the WinoGender dataset of Rudinger et al. (2018); similar to our work, the latter contains a smaller set of simple sentences following a fixed template. Working with a slightly more varied set of sentence templates chosen to unambiguously express the gender of the occupational noun, (R"
2021.computel-1.7,2020.acl-main.740,0,0.0490797,"Missing"
2021.computel-1.7,2020.emnlp-demos.6,0,0.0616605,"Missing"
2021.computel-1.7,2020.sltu-1.43,1,0.81392,"and line toolkit was released called Persephone. To assess the reproducibility of the results on other languages, experiments were extended beyond the Chatino, Na and Tsuut’ina data sets, to a sample of languages from the Pangloss Collection, an online archive of under-resourced languages (Michailovsky et al., 2014). The results confirmed that end-to-end models for automatic phonemic transcription deliver promising performance, and also suggested that preprocessing tasks can to a large extent be automated, thereby increasing the attractiveness of the tool for language documentation workflows (Wisniewski et al., 2020). Another effort in this space is Allosaurus (Li et al., 2020), which leverages multilingual models for phonetic transcription and jointly models language independent phones and language-dependent phonemes. This stands as a 3 3 Bringing ESPnet to Elpis ESPnet is an end-to-end neural network-based speech recognition toolkit. Developed with Pytorch (Paszke et al., 2019) in a research context, the tool satisfies three desiderata for our purposes: (a) it is easy to modify training recipes, which consist of collections of scripts and configuration files that make it easy to perform training and dec"
2021.eacl-main.269,W19-5301,0,0.0566027,"Missing"
2021.eacl-main.269,D19-1662,0,0.0148495,"actic and pragmatic cues. If intuitively the tense can be predicted from simple surface patterns in French, predicting the tense of a Chinese sentence requires capturing the interaction of all sentence-level factors related to time, and sometimes even the contextual information from the previous utterances. Contrasting the performance achieved by the probe on several languages ensures that the linguistic properties we detect are actually captured by the representation learned by Bert and not by the probe, and thus to avoid a common pitfall of this kind of approaches (Belinkov and Glass, 2019; Barrett et al., 2019). This work has two main contributions: first, we highlight the interest of contrasting linguistic probes on different languages; second, our experiments (§3-4) show that Bert has a preference for learning lexically marked features 3080 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 3080–3089 April 19 - 23, 2021. ©2021 Association for Computational Linguistics over lexically unmarked features and, consequently, is not able to extract an abstract representation that can be applied to (groups of) words that have not been seen at"
2021.eacl-main.269,P17-1080,0,0.0271478,"tion. We show that while French tenses can easily be predicted from sentence representations, results drop sharply for Chinese, which suggests that Bert is more likely to memorize shallow patterns from the training data rather than uncover abstract properties. 1 Introduction The success of deep learning in many NLP tasks is often attributed to the ability of neural networks to learn, without any supervision, relevant linguistic representations. Many works have tried to identify which linguistic properties are encoded in the words and phrases embeddings uncovered during training. For instance, Belinkov et al. (2017) and Peters et al. (2018) studies the capacity of neural networks to uncover morphological information and Linzen et al. (2016), Tenney et al. (2019) or Hewitt and Manning (2019) (among many other) syntactic information. These works are based on the definition and study of linguistic probes: a probe (Alain and Bengio, 2017) is trained to predict linguistic properties from the representation of language; achieving high accuracy at this task implies these properties were encoded in the representation. However, as pointed out by Hewitt and Manning (2019), these approaches suffer from a major draw"
2021.eacl-main.269,N18-1202,0,0.0537178,"ench tenses can easily be predicted from sentence representations, results drop sharply for Chinese, which suggests that Bert is more likely to memorize shallow patterns from the training data rather than uncover abstract properties. 1 Introduction The success of deep learning in many NLP tasks is often attributed to the ability of neural networks to learn, without any supervision, relevant linguistic representations. Many works have tried to identify which linguistic properties are encoded in the words and phrases embeddings uncovered during training. For instance, Belinkov et al. (2017) and Peters et al. (2018) studies the capacity of neural networks to uncover morphological information and Linzen et al. (2016), Tenney et al. (2019) or Hewitt and Manning (2019) (among many other) syntactic information. These works are based on the definition and study of linguistic probes: a probe (Alain and Bengio, 2017) is trained to predict linguistic properties from the representation of language; achieving high accuracy at this task implies these properties were encoded in the representation. However, as pointed out by Hewitt and Manning (2019), these approaches suffer from a major drawback: there is no guarant"
2021.emnlp-main.377,2021.eacl-main.269,1,0.7122,"ncode syntactic information by observing that neural language models are able to predict the agreement between a verb and its subject. We take a critical look at this line of research by showing that it is possible to achieve high accuracy on this agreement task with simple surface heuristics, indicating a possible flaw in our assessment of neural networks’ syntactic ability. Our fine-grained analyses of results on the long-range French objectverb agreement show that contrary to LSTMs, Transformers are able to capture a non-trivial amount of grammatical structure. 1 Introduction Chaves, 2020; Li and Wisniewski, 2021). Overall, this set of results questions one of the most fundamental assumption in linguistics (Lakretz et al., 2021), namely that a sentence has a recursive structure (Everaert et al., 2015): while LSTMs with proper parametrization can model context-free patterns (Suzgun et al., 2019), Transformers are essentially feed forward models relying on a large number of attention heads. Consequently, they are, in theory, not adapted to model hierarchical syntactic patterns (Hahn, 2020) and explaining their capacity to predict accurately syntactic agreement patterns remains an open issue. We bring new"
2021.emnlp-main.377,Q16-1037,0,0.0179728,"y predict verbal agreement, pushing further the observation of Kuncoro et al. (2018) that a simple rule can provide highly accurate results on the task. Using our extended set of heuristics, we identify sentences for which predicting the correct verb form requires a more abstract representation of the sentence. By comparing models’ performance on these examples, we show that contrary to LSTMs, Transformers perform consistently well in these critical cases. The long distance agreement task is one of the most popular method to assess neural networks (NN) ability to encode syntactic information: Linzen et al. (2016) showed that LSTMs are able to predict the subject-verb agreement in English and has initiated a very active line of research. Since then, many studies have generalized this observation to other languages (Gulordava et al., 2018), other models such as Transformers (Goldberg, 2019; Jawahar 2 Test Set for French Object et al., 2019) or have identified possible confoundPast-Participle Agreement1 ing factors that could distort the stated conclusions (Gulordava et al., 2018; Marvin and Linzen, 2018). We focus on the object-verb agreement (i.e. object All of these studies show that NN are able to le"
2021.emnlp-main.377,D18-1151,0,0.0226438,"s one of the most popular method to assess neural networks (NN) ability to encode syntactic information: Linzen et al. (2016) showed that LSTMs are able to predict the subject-verb agreement in English and has initiated a very active line of research. Since then, many studies have generalized this observation to other languages (Gulordava et al., 2018), other models such as Transformers (Goldberg, 2019; Jawahar 2 Test Set for French Object et al., 2019) or have identified possible confoundPast-Participle Agreement1 ing factors that could distort the stated conclusions (Gulordava et al., 2018; Marvin and Linzen, 2018). We focus on the object-verb agreement (i.e. object All of these studies show that NN are able to learn past-participle agreement) in French: agreement in a ‘substantial amount’ of syntactic information (Be- number and gender occurs between the object and linkov and Glass, 2019). the past participle when the latter is used with the In this work, we propose to take an alterna- auxiliary avoir (to have) and the object is located tive look at these results by studying whether neu- before the verb. As shown in Figure 1, this is, ral networks are able to predict the correct form for instance, the"
2021.jeptalnrecital-taln.2,W05-0909,0,0.0321372,"Missing"
2021.jeptalnrecital-taln.2,2020.winlp-1.25,0,0.0658596,"Missing"
2021.jeptalnrecital-taln.2,Q19-1004,0,0.0555066,"Missing"
2021.jeptalnrecital-taln.2,2020.acl-main.485,0,0.0610893,"Missing"
2021.jeptalnrecital-taln.2,W17-4705,1,0.899964,"Missing"
2021.jeptalnrecital-taln.2,E06-1032,0,0.32211,"Missing"
2021.jeptalnrecital-taln.2,W19-3824,0,0.0505196,"Missing"
2021.jeptalnrecital-taln.2,W19-3821,0,0.0323243,"Missing"
2021.jeptalnrecital-taln.2,2020.findings-emnlp.180,0,0.0273948,"Missing"
2021.jeptalnrecital-taln.2,D19-1275,0,0.0320781,"Missing"
2021.jeptalnrecital-taln.2,D17-1263,0,0.0385236,"Missing"
2021.jeptalnrecital-taln.2,D19-3019,0,0.0434061,"Missing"
2021.jeptalnrecital-taln.2,P18-1007,0,0.0552507,"Missing"
2021.jeptalnrecital-taln.2,P11-1023,0,0.049813,"Missing"
2021.jeptalnrecital-taln.2,W18-6301,0,0.0223701,"Missing"
2021.jeptalnrecital-taln.2,P02-1040,0,0.109321,"Missing"
2021.jeptalnrecital-taln.2,N18-2002,0,0.0419551,"Missing"
2021.jeptalnrecital-taln.2,2020.acl-main.690,0,0.0336518,"Missing"
2021.jeptalnrecital-taln.2,2020.gebnlp-1.4,0,0.0455014,"Missing"
2021.jeptalnrecital-taln.2,N16-1005,0,0.0433454,"Missing"
2021.jeptalnrecital-taln.2,P19-1164,0,0.0262775,"Missing"
2021.jeptalnrecital-taln.2,D18-1334,0,0.0398621,"Missing"
2021.jeptalnrecital-taln.2,J83-1005,0,0.70483,"Missing"
2021.jeptalnrecital-taln.2,N18-2003,0,0.0582771,"Missing"
2021.wnut-1.22,W18-1817,0,0.0125703,"(e.g. inconsistent casing or usernames) as well as many OOVs denoting URL, mentions, hashtags or more generally named entities (NE). Most of the time, OOVs are exactly the same in the source and target sentences. 2 NMT Models In our experiments, we use three translation models. The first two models are standard NMT models that take as input BPE tokenized sentences: the model used in (Michel and Neubig, 2018a), a Seq2seq bi-LSTM architecture with global attention decoding as implemented in XNMT (Neubig et al., 2018) as well as a vanilla Transformer model as implemented in the OpenNMT toolkit (Klein et al., 2018). We also consider a char-based model, namely the char2char of Lee et al. (2017). Using char-based models which are, by nature, openvocabulary to translate UGC is intuitively appealing as these models are designed specifically to address the problem of translating OOVs and to 1 https://gitlab.inria.fr/seddah/paral lel-french-social-mediabank 2 Models parameters are detailed in the appendix. deal with noisy input (Belinkov and Bisk, 2018). As the Seq2seq model we consider in our experiments is not able to translate OOVs, we introduce, as part of our translation pipeline, a postprocessing step i"
2021.wnut-1.22,W18-6319,0,0.0231638,"ng out-of-domain data. These results seem to indicate that, counterintuitively, translating UGC does not raise any specific challenges. We however believe that they are biased by the evaluation metric used: as UGC contains many mentions, URLs emoticons, or named entities that are the same in the source and in the target sentence, B LEU scores estimated on a canonical and on a non-canonical can not be directly compared: B LEU scores on non-canonical data are artificially high as systems are rewarded for simply coping source tokens, which is the most natural 3 All B LEU scores are calculated by Post (2018)’s SacreBleu using the intl tokenization 190 A Corpus Annotated with UGC Specificities The PMUMT corpus To understand the impact of UGC peculiarities, we manually annotated 400 source sentences sampled from the PFSMB: one of the authors, fluent in French and with good knowledge of UGC, has identified spans in the sentence that differ from canonical French and characterized these specificities using the fine-grained typology of Sanguinetti et al. (2020) (see Table 4). Since the whole annotation process was done by a single person, no inter-annotator agreement can be calculated. Nevertheless, re"
2021.wnut-1.22,W04-3250,0,0.616691,"Missing"
2021.wnut-1.22,Q17-1026,0,0.0133858,"ns, hashtags or more generally named entities (NE). Most of the time, OOVs are exactly the same in the source and target sentences. 2 NMT Models In our experiments, we use three translation models. The first two models are standard NMT models that take as input BPE tokenized sentences: the model used in (Michel and Neubig, 2018a), a Seq2seq bi-LSTM architecture with global attention decoding as implemented in XNMT (Neubig et al., 2018) as well as a vanilla Transformer model as implemented in the OpenNMT toolkit (Klein et al., 2018). We also consider a char-based model, namely the char2char of Lee et al. (2017). Using char-based models which are, by nature, openvocabulary to translate UGC is intuitively appealing as these models are designed specifically to address the problem of translating OOVs and to 1 https://gitlab.inria.fr/seddah/paral lel-french-social-mediabank 2 Models parameters are detailed in the appendix. deal with noisy input (Belinkov and Bisk, 2018). As the Seq2seq model we consider in our experiments is not able to translate OOVs, we introduce, as part of our translation pipeline, a postprocessing step in which the translation hypothesis is aligned with the source and <UNK&gt; tokens a"
2021.wnut-1.22,L18-1275,0,0.0237229,"Missing"
2021.wnut-1.22,D18-1050,0,0.164885,"s studying the robustness of NMT systems by adding artificial noise to canonical corpora, PMUMT is made of attested UGC examples. Using this framework, we conduct several experiments on three out-of-the-box NMT architectures in a zero-shot scenario, to measure more precisely than what was possible before the impact of the different kinds of UGC specificities on translation quality. Surprisingly enough, our experiments (§3) on natural data show that out-of-the-box models exhibit unexpected strong robustness against several kinds of noise, questioning several results reported in the literature (Michel and Neubig, 2018a; Belinkov and Bisk, 2018). We believe that this data set and its associated evaluation framework will pave the way for a better understanding of the interactions at play in neural machine translation of noisy user-generated content contexts. This work takes a critical look at the evaluation of user-generated content (UGC) automatic translation. The well-known specificities of UGC (high rate of OOVs, rare, grammatical constructs, ...) raise many challenges for Machine Translation and has been the topic of many recent works (Rosales Núñez et al., 2019; Specia et al., 2020). Several UGC paralle"
2021.wnut-1.22,W19-6101,1,0.910707,"l results reported in the literature (Michel and Neubig, 2018a; Belinkov and Bisk, 2018). We believe that this data set and its associated evaluation framework will pave the way for a better understanding of the interactions at play in neural machine translation of noisy user-generated content contexts. This work takes a critical look at the evaluation of user-generated content (UGC) automatic translation. The well-known specificities of UGC (high rate of OOVs, rare, grammatical constructs, ...) raise many challenges for Machine Translation and has been the topic of many recent works (Rosales Núñez et al., 2019; Specia et al., 2020). Several UGC parallel corpora (Michel and Neubig, 2018a; Rosales Núñez et al., 2019) have been introduced to evaluate the robustness of MT, some of which, such as (Fujii et al., 2020), are specially annotated to identify UGC idiosyncrasies allow- 2 Testing Out-of-the-Box NMT models ing to measure the impact of a given specificity. on UGC Our analyses (§2), indeed, show that measuring the average-case performance using a standard metric 2.1 Experimental Setting on a UGC test set falls far short of giving a reliable image of the UGC translation quality: explaining Training"
2021.wnut-1.22,C12-1149,1,0.75196,"OpenSubtitles ⋄ MTNT News OpenTest 9.9 17.1 15.4 21.8 24.0 21.2 27.5 29.1 27.4 14.7 16.4 16.4 7.1 13.9 18.1 8.8 PFSMB † † MTNT News OpenTest 17.1 26.1 27.5 27.2 28.5 28.3 19.6 24.5 26.7 28.2 28.2 31.4 23.8 25.7 17.8 26.3 ⋄ Table 2: B LEU scores for our models. The † symbol indicates the UGC test sets, and ⋄ in-domain test sets. UGC Test Sets To evaluate the different NMT models, we consider two data sets of manually translated UGC: MTNT (Michel and Neubig, 2018a) and the Parallel French Social Media Bank corpus 1 (PFSMB) (Rosales Núñez et al., 2019) which extends the French Social Media Bank (Seddah et al., 2012) with English translations. These two data sets raise many challenges for MT systems: they notably contain characters that have not been seen in the training data (e.g. emojis), rare character sequences (e.g. inconsistent casing or usernames) as well as many OOVs denoting URL, mentions, hashtags or more generally named entities (NE). Most of the time, OOVs are exactly the same in the source and target sentences. 2 NMT Models In our experiments, we use three translation models. The first two models are standard NMT models that take as input BPE tokenized sentences: the model used in (Michel and"
2021.wnut-1.22,W18-1818,0,0.0128551,"ain characters that have not been seen in the training data (e.g. emojis), rare character sequences (e.g. inconsistent casing or usernames) as well as many OOVs denoting URL, mentions, hashtags or more generally named entities (NE). Most of the time, OOVs are exactly the same in the source and target sentences. 2 NMT Models In our experiments, we use three translation models. The first two models are standard NMT models that take as input BPE tokenized sentences: the model used in (Michel and Neubig, 2018a), a Seq2seq bi-LSTM architecture with global attention decoding as implemented in XNMT (Neubig et al., 2018) as well as a vanilla Transformer model as implemented in the OpenNMT toolkit (Klein et al., 2018). We also consider a char-based model, namely the char2char of Lee et al. (2017). Using char-based models which are, by nature, openvocabulary to translate UGC is intuitively appealing as these models are designed specifically to address the problem of translating OOVs and to 1 https://gitlab.inria.fr/seddah/paral lel-french-social-mediabank 2 Models parameters are detailed in the appendix. deal with noisy input (Belinkov and Bisk, 2018). As the Seq2seq model we consider in our experiments is not"
2021.wnut-1.22,P02-1040,0,0.111502,"Missing"
2021.wnut-1.22,W15-3049,0,0.0603752,"Missing"
2021.wnut-1.23,P16-2058,0,0.0218934,"or the present work. In this work, we explore two character-based translation models. The first model we consider, charCNN is a classic encoder-decoder in which the encoder uses character-based embeddings in combination with convolutional and highway layers to replace the standard look-up based word representations. The model considers, as input, a stream of words (i.e. it assumes the input has been tokenized beforehand) and tries to learn a word representation that is more robust to noise by unveiling regularities at the character level. This architecture was initially proposed by Kim et al. (2016) for language modeling; Costa-jussà and Fonollosa (2016) shows 3 Datasets how it can be used in an NMT system and report improvements up to 3 B LEU (Papineni et al., 2002) Training sets Due to the lack of a large paralpoints when translating from a morphologically- lel corpus of noisy sentences, we train our sysrich language, German, to English. tems with ‘standard’ parallel datasets, namely the The second model we consider does not rely corpora used in the WMT campaign (Bojar et al., on an explicit segmentation into words: Lee et al. 2016) and the OpenSubtitles corpus (Lison (2017) introduce"
2021.wnut-1.23,C18-1055,0,0.361613,"ct of UGC idiosyncrasies; • we demonstrate that char-based neural machine translation models are extremely sensitive to unknown and rare characters on both synthetic data and noisy user-generated content; • we show how an overlooked hyper-parameter drastically improve char-based MT models robustness to natural noise while maintaining the in-domain level of performance. 2 Character-level NMT on WMT or IWSLT tasks that consider texts that mostly qualify as canonical with very few spelling or grammatical errors. The impact of noise on charCNN and char2char has been evaluated by Belinkov and Bisk (2018) and Ebrahimi et al. (2018b). By adding noise to canonical texts (the TEDTalk dataset). Their results show that the different character levels models fail to translate even moderately noisy texts when trained on ‘clean’ data and that it is necessary to train a model on noisy data to make it robust. Note that, as explained in Section 3, there is no UGC parallel corpus large enough to train a NMT model and we must rely on the models’ ability to learn, from canonical data only, noise- and error-resistant representations of their input that are robust to the noise and errors found in UGCs. This is"
2021.wnut-1.23,L18-1275,0,0.0604981,"Missing"
2021.wnut-1.23,N13-1037,0,0.21978,"(NMT) models fall far short from being able to translate noisy UserGenerated Content (UGC): the quality of their translation is often even worse than that of a traditional phrase-based system (Khayrallah and Koehn, 2018; Rosales Núñez et al., 2019). In addition to ambiguous grammatical constructs and profusion of ellipsis, the main difficulty encountered when translating UGC is the high number of out-of-vocabulary tokens (OOVs) resulting from misspelled words, emoticons, hashtags, mentions, and all specific constructs used in online forums and social medias (Foster, 2010; Seddah et al., 2012; Eisenstein, 2013; Sanguinetti et al., 2020). Some of those phenomena can be perceived as noise while the others are typical markers of language variation among speakers. Moreover, a certain amount of those same phenomena operate at the lexical level (either at the character, subword or word levels) (Sanguinetti 1 https://github.com/josecar25/char_bas et al., 2020). This is why, focusing more on the ed_NMT-noisy_UGC 199 Proceedings of the 2021 EMNLP Workshop W-NUT: The Seventh Workshop on Noisy User-generated Text, pages 199–211 November 11, 2021. ©2021 Association for Computational Linguistics in-depth evalua"
2021.wnut-1.23,P16-1100,0,0.0404045,"Missing"
2021.wnut-1.23,N10-1060,0,0.25775,"enables Neural Machine Translation (NMT) models fall far short from being able to translate noisy UserGenerated Content (UGC): the quality of their translation is often even worse than that of a traditional phrase-based system (Khayrallah and Koehn, 2018; Rosales Núñez et al., 2019). In addition to ambiguous grammatical constructs and profusion of ellipsis, the main difficulty encountered when translating UGC is the high number of out-of-vocabulary tokens (OOVs) resulting from misspelled words, emoticons, hashtags, mentions, and all specific constructs used in online forums and social medias (Foster, 2010; Seddah et al., 2012; Eisenstein, 2013; Sanguinetti et al., 2020). Some of those phenomena can be perceived as noise while the others are typical markers of language variation among speakers. Moreover, a certain amount of those same phenomena operate at the lexical level (either at the character, subword or word levels) (Sanguinetti 1 https://github.com/josecar25/char_bas et al., 2020). This is why, focusing more on the ed_NMT-noisy_UGC 199 Proceedings of the 2021 EMNLP Workshop W-NUT: The Seventh Workshop on Noisy User-generated Text, pages 199–211 November 11, 2021. ©2021 Association for Co"
2021.wnut-1.23,2020.coling-main.521,0,0.0352267,"differences in performance can be explained 206 and because of our annotated UGC data set are able to provide a more precise view on the phenomena at stake when processing natural noisy-input. Similarly to Durrani et al. (2019) what concluded for morpho-syntactic classification tasks, our results show that a more fined-grained granularity of learning representations, characters over BPEs, provides a higher robustness to certain types of noise. Contrary to the both aforementioned works, our study is performed using annotated real-world noisy UGC, which proved crucial for our study. In line to Fujii et al. (2020) findings, where finegrained NMT granularity provided robustness advantages when processing misspelling, our results show that the best and worst translations’ specificities distribution point to a better performance of char2char for the missing diacritics category, giving insights on more specific types of misspellings that affect performance. Continuing this research track, we broaden the spectrum of studying UGC specificities, exploring the effects of the vocabulary training size and show that tuning it can achieve better results when translating noisy UGC. This simple hyper-parameter choic"
2021.wnut-1.23,W18-2709,0,0.0219605,"we thus show that these models are unable to perform an easy copy task due to their poor handling of unknown and rare characters. By adjusting the vocabulary size parameter, we drastically improve the robustness of our character-based model without causing a large drop in in-domain performance. Our contributions are as follows: • we provide an annotated data set1 that enables Neural Machine Translation (NMT) models fall far short from being able to translate noisy UserGenerated Content (UGC): the quality of their translation is often even worse than that of a traditional phrase-based system (Khayrallah and Koehn, 2018; Rosales Núñez et al., 2019). In addition to ambiguous grammatical constructs and profusion of ellipsis, the main difficulty encountered when translating UGC is the high number of out-of-vocabulary tokens (OOVs) resulting from misspelled words, emoticons, hashtags, mentions, and all specific constructs used in online forums and social medias (Foster, 2010; Seddah et al., 2012; Eisenstein, 2013; Sanguinetti et al., 2020). Some of those phenomena can be perceived as noise while the others are typical markers of language variation among speakers. Moreover, a certain amount of those same phenomen"
2021.wnut-1.23,W18-1817,0,0.0228932,"nt in the input. BPE-based models We use as our baselines two standard NMT models that consider tokenized sentences as input. The first one is a seq2seq bi-LSTM architecture with global attention decoding. The seq2seq model was trained using the XNMT toolkit (Neubig et al., 2018).5 It consists of a 2-layered Bi-LSTM layers encoder and a 2-layered Bi-LSTM decoder. It considers, as input, word embeddings of size 512 and each LSTM units has 1,024 components. Our second baseline model is a vanilla Transformer model (Vaswani et al., 2017) using the implementation proposed in the OpenNMT framework (Klein et al., 2018). It consists of 6 layers with word embeddings that are 512-dimensional, a feed-forward layers made of 2,048 units and 8 self-attention heads. Unknown Token Replacement One of the peculiarities of our UGC datasets is that they contain a many OOVs denoting URL, mentions, hashtags, or more generally named entities: for instance several sentences of the PFSMB mention the game “Flappy Bird” or the TV show “Teen Wolf”. Most of the time, these OOVs are exactly the same in the source and target sentences and consequently, the source 3 https://github.com/harvardnlp/seq2seq -attn 4 https://github.com/n"
2021.wnut-1.23,W17-3204,0,0.0465083,"Missing"
2021.wnut-1.23,I17-1003,0,0.0154061,"able 3), we used the ‘standard’ character vocabulary size (namely 300 characters) that was used in (Lee et al., 2017) and, to the best of our knowledge, in all following works. vocab. size PFSMB MTNT News OpenTest† 90 85 80 75 70 65 23.9 23.9 23.9 24.5 24.6 22.7 25.8 25.3 25.8 25.9 25.4 25.5 18.7 19.9 18.3 17.8 17.8 18.0 26.6 26.9 26.6 26.3 26.3 26.4 Table 7: B LEU results for MT of char2char with reduced vocabulary size. Systems trained on OpenSubs. † marks in-domain test set. by drop- and over-translation phenomena, two wellidentified limits of NMT (Mi et al., 2016; Koehn and Knowles, 2017; Le et al., 2017). An analysis of Figure 3, which displays the ratio between the hypothesis and reference lengths for the different vocabulary sizes, seems to confirm this hypothesis as it appears that the vocabulary size parameter provides control over the translation hypothesis length and consequently, a way to limit these drop- and over-translation phenomena. PFSMB 1.1 Length ratios 25 19.8 B LEU score 30 PFSMB blind Transformer 29.7 char2char 1.08 1.05 OpenSubTest 1.07 1.04 1.04 1.03 0.99 1 1 0.95 0.92 0.89 0.9 0.89 0.84 300 90 85 80 75 70 Character vocabulary size 0.85 65 Figure 3: Reference/hypothesis le"
2021.wnut-1.23,Q17-1026,0,0.379022,"failure once such characters are encountered. We further confirm this behavior with a simple, yet insightful, copy task experiment and highlight the importance of reducing the vocabulary size hyper-parameter to increase the robustness of character-based models for machine translation. 1 Introduction noise axis, char-based models appear to offer a natural solution to this problem (Luong and Manning, 2016; Ling et al., 2015): indeed these open vocabulary models are designed specifically to address the OOV problem. In this work, we explore the ability of out-of-thebox character-based NMT models (Lee et al., 2017) to address the challenges raised by UGC translation. While character-based models may seem promising for such task, to the best of our knowledge, they have only been tested either on data sets in which noise has been artificially added through sampling an edited word error data set (Belinkov and Bisk, 2018; Ebrahimi et al., 2018a) and on canonical data set, in which they prove to be very effective for translating morphologically-rich languages with a high number of OOVs (Luong and Manning, 2016). However, our starting-points experiments show that character-based systems are outperformed by BP"
2021.wnut-1.23,W16-3905,1,0.812529,"may contain emojis model outperforms subword-level (i.e. BPE-based) that can even replace some words (e.g. ♥ can stands translation models on two WMT’15 tasks (de-en for the verb ‘love’ in sentences such as ‘I ♥ you’). and cs-en) and gives comparable performance on UGC productivity limits the pertinence of domain two tasks (fi-en and ru-en). Lee et al. (2017) ad- adaptation methods such as fine-tuning, as there ditionally report that in a multilingual setting, the will always be new forms that will not have been character-level encoder significantly outperforms seen during training (Martínez Alonso et al., 2016). the subword-level encoder on all the language pairs. This is why we focus here on zero-shot scenarios, These two models have been originally tested as we believe they provide a clearer experimental 200 protocol when it comes to study the impact of UGC specificities on MT models. Test sets We consider in our experiments two French-English test sets made of user-generated content. These corpora differ in the domain of their contents, their collection date, and in the way sentences were filtered to ensure they are sufficiently different from canonical data. The first one, the MTNT corpus (Miche"
2021.wnut-1.23,D16-1096,0,0.0268631,"at in our first experiments (reported in Table 3), we used the ‘standard’ character vocabulary size (namely 300 characters) that was used in (Lee et al., 2017) and, to the best of our knowledge, in all following works. vocab. size PFSMB MTNT News OpenTest† 90 85 80 75 70 65 23.9 23.9 23.9 24.5 24.6 22.7 25.8 25.3 25.8 25.9 25.4 25.5 18.7 19.9 18.3 17.8 17.8 18.0 26.6 26.9 26.6 26.3 26.3 26.4 Table 7: B LEU results for MT of char2char with reduced vocabulary size. Systems trained on OpenSubs. † marks in-domain test set. by drop- and over-translation phenomena, two wellidentified limits of NMT (Mi et al., 2016; Koehn and Knowles, 2017; Le et al., 2017). An analysis of Figure 3, which displays the ratio between the hypothesis and reference lengths for the different vocabulary sizes, seems to confirm this hypothesis as it appears that the vocabulary size parameter provides control over the translation hypothesis length and consequently, a way to limit these drop- and over-translation phenomena. PFSMB 1.1 Length ratios 25 19.8 B LEU score 30 PFSMB blind Transformer 29.7 char2char 1.08 1.05 OpenSubTest 1.07 1.04 1.04 1.03 0.99 1 1 0.95 0.92 0.89 0.9 0.89 0.84 300 90 85 80 75 70 Character vocabulary siz"
2021.wnut-1.23,D18-1050,0,0.367858,"2016). the subword-level encoder on all the language pairs. This is why we focus here on zero-shot scenarios, These two models have been originally tested as we believe they provide a clearer experimental 200 protocol when it comes to study the impact of UGC specificities on MT models. Test sets We consider in our experiments two French-English test sets made of user-generated content. These corpora differ in the domain of their contents, their collection date, and in the way sentences were filtered to ensure they are sufficiently different from canonical data. The first one, the MTNT corpus (Michel and Neubig, 2018), is a multilingual dataset that contains French sentences collected on Reddit and translated into English by professional translators. The second one, the Parallel French Social Media Bank (PFSMB)2 , introduced in (Rosales Núñez et al., 2019) and made from a subpart of the French Social Media Bank (Seddah et al., 2012), consists of comments extracted from Facebook, Twitter and Doctissimo, a health-related French forum. Table 9 shows some examples of source sentences and reference translations extracted from these two corpora and illustrates the peculiarities of UGC and difficulties of transla"
2021.wnut-1.23,W18-1818,0,0.0236772,"r2char.4 It must be noted that the charCNN extracts character n-grams for each input word and predicts a word contained in the target vocabulary or a special token, &lt;UNK&gt;, otherwise, whereas the char2char is capable of open-vocabulary translation and does not generate &lt;UNK&gt; tokens, unless an out-of-vocabulary character (char-OOV) is present in the input. BPE-based models We use as our baselines two standard NMT models that consider tokenized sentences as input. The first one is a seq2seq bi-LSTM architecture with global attention decoding. The seq2seq model was trained using the XNMT toolkit (Neubig et al., 2018).5 It consists of a 2-layered Bi-LSTM layers encoder and a 2-layered Bi-LSTM decoder. It considers, as input, word embeddings of size 512 and each LSTM units has 1,024 components. Our second baseline model is a vanilla Transformer model (Vaswani et al., 2017) using the implementation proposed in the OpenNMT framework (Klein et al., 2018). It consists of 6 layers with word embeddings that are 512-dimensional, a feed-forward layers made of 2,048 units and 8 self-attention heads. Unknown Token Replacement One of the peculiarities of our UGC datasets is that they contain a many OOVs denoting URL,"
2021.wnut-1.23,P02-1040,0,0.110824,"which the encoder uses character-based embeddings in combination with convolutional and highway layers to replace the standard look-up based word representations. The model considers, as input, a stream of words (i.e. it assumes the input has been tokenized beforehand) and tries to learn a word representation that is more robust to noise by unveiling regularities at the character level. This architecture was initially proposed by Kim et al. (2016) for language modeling; Costa-jussà and Fonollosa (2016) shows 3 Datasets how it can be used in an NMT system and report improvements up to 3 B LEU (Papineni et al., 2002) Training sets Due to the lack of a large paralpoints when translating from a morphologically- lel corpus of noisy sentences, we train our sysrich language, German, to English. tems with ‘standard’ parallel datasets, namely the The second model we consider does not rely corpora used in the WMT campaign (Bojar et al., on an explicit segmentation into words: Lee et al. 2016) and the OpenSubtitles corpus (Lison (2017) introduce the char2char model that di- et al., 2018). The former contains canonical texts rectly maps a source characters sequence to a tar- (2.2M sentences), while the latter (9.2M"
2021.wnut-1.23,W18-6319,0,0.0117434,"post-processing step in which the translation hypothesis is aligned with the source sentence and the &lt;UNK&gt; tokens replaced by their corresponding aligned source tokens. For the seq2seq the alignments between the source and translation hypothesis are computed using an IBM2 model.7 . For the charCNN model, the alignments are deduced from the attention matrix. The char2char model is an open-vocabulary system that is able to generate new words when necessary. The vanilla Transformer implementation we use is able to copy unknown symbols directly. Table 3 reports the B LEU scores, as calculated by Post (2018)’s SacreBleu of the different models we consider, both on canonical and non-canonical test sets. Contrary to the first results of Michel and Neubig (2018), the quality of UGC translation does not appear to be so bad: the drop in performance observed on non-canonical corpora is of the same order of magnitude as the drop observed when translation models are applied to out-of-domain data. For instance, the B LEU score of a Transformer model trained on OpenSubtitles has the same order of magnitude on PFSMB, MTNT and news: on all these datasets, the performance dropped by roughly 4 B LEU points com"
2021.wnut-1.23,W19-6101,1,0.929305,"unable to perform an easy copy task due to their poor handling of unknown and rare characters. By adjusting the vocabulary size parameter, we drastically improve the robustness of our character-based model without causing a large drop in in-domain performance. Our contributions are as follows: • we provide an annotated data set1 that enables Neural Machine Translation (NMT) models fall far short from being able to translate noisy UserGenerated Content (UGC): the quality of their translation is often even worse than that of a traditional phrase-based system (Khayrallah and Koehn, 2018; Rosales Núñez et al., 2019). In addition to ambiguous grammatical constructs and profusion of ellipsis, the main difficulty encountered when translating UGC is the high number of out-of-vocabulary tokens (OOVs) resulting from misspelled words, emoticons, hashtags, mentions, and all specific constructs used in online forums and social medias (Foster, 2010; Seddah et al., 2012; Eisenstein, 2013; Sanguinetti et al., 2020). Some of those phenomena can be perceived as noise while the others are typical markers of language variation among speakers. Moreover, a certain amount of those same phenomena operate at the lexical leve"
2021.wnut-1.23,C12-1149,1,0.896433,"Machine Translation (NMT) models fall far short from being able to translate noisy UserGenerated Content (UGC): the quality of their translation is often even worse than that of a traditional phrase-based system (Khayrallah and Koehn, 2018; Rosales Núñez et al., 2019). In addition to ambiguous grammatical constructs and profusion of ellipsis, the main difficulty encountered when translating UGC is the high number of out-of-vocabulary tokens (OOVs) resulting from misspelled words, emoticons, hashtags, mentions, and all specific constructs used in online forums and social medias (Foster, 2010; Seddah et al., 2012; Eisenstein, 2013; Sanguinetti et al., 2020). Some of those phenomena can be perceived as noise while the others are typical markers of language variation among speakers. Moreover, a certain amount of those same phenomena operate at the lexical level (either at the character, subword or word levels) (Sanguinetti 1 https://github.com/josecar25/char_bas et al., 2020). This is why, focusing more on the ed_NMT-noisy_UGC 199 Proceedings of the 2021 EMNLP Workshop W-NUT: The Seventh Workshop on Noisy User-generated Text, pages 199–211 November 11, 2021. ©2021 Association for Computational Linguisti"
2021.wnut-1.23,N19-1190,0,0.0313898,"Missing"
C16-1012,C16-1012,1,0.0512563,"Missing"
C16-1012,P10-1131,0,0.215117,"Missing"
C16-1012,D11-1005,0,0.151322,"Missing"
C16-1012,P04-1015,0,0.215986,"Missing"
C16-1012,W07-0414,0,0.210204,"Missing"
C16-1012,N13-1014,0,0.0997753,"Missing"
C16-1012,C12-1059,0,0.0860819,"Missing"
C16-1012,H05-1021,0,0.0965053,"Missing"
C16-1012,N16-1121,1,0.885879,"Missing"
C16-1012,E09-1061,0,0.179527,"Missing"
C16-1012,P13-2017,0,0.0982958,"Missing"
C16-1012,P12-1066,0,0.234096,"Missing"
C16-1012,L16-1262,0,0.0602675,"Missing"
C16-1012,J08-4003,0,0.179659,"Missing"
C16-1012,petrov-etal-2012-universal,0,0.16977,"Missing"
C16-1012,P15-2040,0,0.240982,"Missing"
C16-1012,P11-2120,0,0.20281,"Missing"
C16-1012,W14-1614,0,0.058532,"Missing"
C16-1012,N01-1026,0,0.563907,"Missing"
C16-1012,I08-3008,0,0.362763,"Missing"
C16-1012,P11-2033,0,0.0717267,"Missing"
C18-1270,L16-1241,1,0.851785,"sing the whole source treebank but only 10 target sentences. In the following, this strategy is referred to as KL-B EAM. Composite scores Similarly to what has been done for monolingual parsers, we can express the performance of KL-B EAM in terms of simple-only UAS and complex-only UAS (using the partition computed monolingually with B EAM for instance). Those values alone are, however, hard to interpret, and notably to relate to the actual amount of knowledge transferred via KL-B EAM. We therefore propose to position those scores along the learning curves of the monolingual parser, following Aufrant et al. (2016): if the cross-lingual parser achieves the same score as a parser trained on n sentences, we consider that the amount of transferred knowledge is the amount of knowledge contained in n sentences. Figure 3 consequently pictures the learning curves of each system on simple and complex classes (using the PoS/direction criterion), as well as the split UAS for KL-B EAM on the same categories. 100 100 Beam UAS 90 100 80 UDPipe MSTParser 90 90 80 80 73.7 73.4 70 70 66.1 66.1 60 72.7 70 60 57.9 55.8 66.1 60 50 50 50 40 40 40 14 32 60 10 20 50 #snt 100 200 500 24 10 20 59 simple all complex 67 124 50 1"
C18-1270,E17-2051,1,0.843285,"an averaged perceptron classifier, i.e. feature-based), and MSTPARSER (graph-based parser). We use version 0.5.1 of MSTPARSER (McDonald et al., 2005) with default parameters. For UDP IPE, we use version 1.1 (Straka and Strakov´a, 2017) with the same hyperparameters as Straka (2017), but without the word embeddings pre-trained on massive monolingual data (to ensure comparability). For B EAM, we rely on our own open source1 implementation, PAN PARSER (Aufrant and Wisniewski, 2016), using the ArcEager version (Nivre, 2004) with a dynamic oracle (Goldberg and Nivre, 2012) adapted for beam search (Aufrant et al., 2017) and the feature sets of Zhang and Nivre (2011) (coarse PoS, no labels). 3 Measuring difficulty The purpose of this work is to investigate dependencies for the learning of which large training datasets are unnecessary, and which can therefore be qualified as ‘easy to learn’. In this section, we formalize this intuition and introduce several empirical measures to quantify it. We then exploit the results of large-scale evaluations to design a new metric, C OMPLEXITY, which estimates the challenges faced when learning a given ‘type’ of dependencies: by departing from individual dependencies, we a"
C18-1270,C12-1059,0,0.0228729,"dding-based), B EAM (transition-based parser, with an averaged perceptron classifier, i.e. feature-based), and MSTPARSER (graph-based parser). We use version 0.5.1 of MSTPARSER (McDonald et al., 2005) with default parameters. For UDP IPE, we use version 1.1 (Straka and Strakov´a, 2017) with the same hyperparameters as Straka (2017), but without the word embeddings pre-trained on massive monolingual data (to ensure comparability). For B EAM, we rely on our own open source1 implementation, PAN PARSER (Aufrant and Wisniewski, 2016), using the ArcEager version (Nivre, 2004) with a dynamic oracle (Goldberg and Nivre, 2012) adapted for beam search (Aufrant et al., 2017) and the feature sets of Zhang and Nivre (2011) (coarse PoS, no labels). 3 Measuring difficulty The purpose of this work is to investigate dependencies for the learning of which large training datasets are unnecessary, and which can therefore be qualified as ‘easy to learn’. In this section, we formalize this intuition and introduce several empirical measures to quantify it. We then exploit the results of large-scale evaluations to design a new metric, C OMPLEXITY, which estimates the challenges faced when learning a given ‘type’ of dependencies:"
C18-1270,N16-1121,1,0.8591,"ented here do not replace any prior knowledge on transfer results for a given known language. However, when tackling a new language, they can provide a rough estimate to choose between the monolingual and cross-lingual approaches. 16 While this amount can appear already substantial, it also denotes the fact that Tiedemann and Agi´c (2016)’s evaluation is based on a set of languages with marked relatedness, and thus good transfer properties. When averaging our measures only over that set of languages (but still not the same treebanks), the data-size equivalent drops to 92 sentences. Similarly, Lacroix et al. (2016)’s projection approach achieves 79.62 UAS, which is higher than with 500 sentences when considering all UD 2.0 treebanks, but corresponds to 295 sentences when retaining the same set of languages. Overall, the magnitude remains around a few hundred sentences for projection methods. 3198 We build a cross-lingual parser for Romanian, using KL-B EAM as before but with a much smaller source set: French, Italian, Spanish and Bulgarian. The choice of the first three is motivated by their language family (Romance, like Romanian), and Bulgarian by geographic proximity (and hence various influences acr"
C18-1270,lynn-etal-2012-irish,0,0.0764864,"Missing"
C18-1270,P13-1028,0,0.0636125,"Missing"
C18-1270,E17-1022,0,0.0557687,"Missing"
C18-1270,D07-1013,0,0.0396146,"highly deterministic, is a simple class for all but 7 treebanks (mostly Old Church Slavonic, Arabic, LatinPROIEL, but also Basque, Estonian, Korean and Polish to a lesser extent). 4 Application 1: fine-grained comparison of parsers The metrics we have proposed can now be used for large-scale computation of fine-grained evaluations, and therefore detailed comparison of parsers. This newly defined notion of complexity opens indeed new evaluation perspectives, as a complement to the more explicit properties (length, PoS tags, projectivity, etc.) used in prior work on comparative error analysis (McDonald and Nivre, 2007). Complexity variations Coming back to Table 1, comparing the rankings between all 3 systems also emphasizes on their respective shortcomings. UDP IPE notably seems to have troubles with determiners: x not only does it achieve a lower score on DET dependencies, but it is also much slower learning those, compared to the other systems; UDP IPE consequently appears to under-exploit the determinism of that x class.13 It is conversely particularly efficient on CCONJs. As for MSTPARSER, it handles VERB significantly faster than B EAM and UDP IPE, presumably because it does not rely on mostly local f"
C18-1270,H05-1066,0,0.090463,"lly a sequence of transitions affecting the parser’s inner state) and graphbased parsers (which compute attachment scores for all pairs of tokens and then optimize the sentence score globally). In this work, we consider three dependency parsers, based on diverse parsing algorithms and classifiers, to assess the generality of our findings: UDP IPE (transition-based parser, with a feedforward neural classifier, i.e. embedding-based), B EAM (transition-based parser, with an averaged perceptron classifier, i.e. feature-based), and MSTPARSER (graph-based parser). We use version 0.5.1 of MSTPARSER (McDonald et al., 2005) with default parameters. For UDP IPE, we use version 1.1 (Straka and Strakov´a, 2017) with the same hyperparameters as Straka (2017), but without the word embeddings pre-trained on massive monolingual data (to ensure comparability). For B EAM, we rely on our own open source1 implementation, PAN PARSER (Aufrant and Wisniewski, 2016), using the ArcEager version (Nivre, 2004) with a dynamic oracle (Goldberg and Nivre, 2012) adapted for beam search (Aufrant et al., 2017) and the feature sets of Zhang and Nivre (2011) (coarse PoS, no labels). 3 Measuring difficulty The purpose of this work is to i"
C18-1270,W04-0308,0,0.04351,"forward neural classifier, i.e. embedding-based), B EAM (transition-based parser, with an averaged perceptron classifier, i.e. feature-based), and MSTPARSER (graph-based parser). We use version 0.5.1 of MSTPARSER (McDonald et al., 2005) with default parameters. For UDP IPE, we use version 1.1 (Straka and Strakov´a, 2017) with the same hyperparameters as Straka (2017), but without the word embeddings pre-trained on massive monolingual data (to ensure comparability). For B EAM, we rely on our own open source1 implementation, PAN PARSER (Aufrant and Wisniewski, 2016), using the ArcEager version (Nivre, 2004) with a dynamic oracle (Goldberg and Nivre, 2012) adapted for beam search (Aufrant et al., 2017) and the feature sets of Zhang and Nivre (2011) (coarse PoS, no labels). 3 Measuring difficulty The purpose of this work is to investigate dependencies for the learning of which large training datasets are unnecessary, and which can therefore be qualified as ‘easy to learn’. In this section, we formalize this intuition and introduce several empirical measures to quantify it. We then exploit the results of large-scale evaluations to design a new metric, C OMPLEXITY, which estimates the challenges fac"
C18-1270,P15-2040,0,0.0578209,"link with our work above is twofold: cross-lingual parsers also focus on low ranges of training sizes, and at the same time many of them yield UAS around the range covered by our 5 to 500-sentence long treebanks. We therefore propose to exploit our upper results in this new frame, with the goal of quantifying and characterizing the amount of knowledge that has been effectively transferred: what kind of information is learned by cross-lingual parsers – only simple classes or complex knowledge about non-trivial classes? Multi-source weighted delexicalized transfer Our analysis first focuses on Rosa and Zabokrtsky (2015)’s state-of-the-art method for cross-lingual parsing: it consists in delexicalized transfer, where the hypotheses stemming from multiple sources are weighted and combined based on the KLcpos3 metric (the Kullback-Leibler divergence of PoS trigram distributions between the source and the target). It is meant to favour the languages that are syntactically close to the target, while still benefiting from the diverse information conveyed by a large set of sources. We reimplement the method of Rosa and Zabokrtsky (2015) on top of the B EAM system: we include as sources the delexicalized B EAM model"
C18-1270,C12-1147,0,0.0174207,"his section on classes defined by the child PoS and its attachment direction. Indeed, it fits particularly well our x intuition regarding many parsing difficulties: due to its frequency and determinism, the ADJ class (that is, all adjectives whose head is on the right) appears for instance simple in the English UD as it mostly corresponds to the bigram ‘ADJ NOUN’ and, sometimes, to predicative adjectives. On the contrary, the 1 Source code available at https://perso.limsi.fr/aufrant. Such properties can still depend on the annotation scheme though, as repeatedly pointed out in the literature (Schwartz et al., 2012; Wisniewski and Lacroix, 2017; Wisniewski et al., 2018). 3 This size has been chosen to cover both the scale of 10 sentences and that of existing treebanks, around 300 sentences: there ˇ epl¨o’s Maltese treebank (Tiedemann have been publications with 300 Irish sentences (Lynn et al., 2012) or 371 in Slavom´ır C´ and van der Plas, 2016). 4 We similarly downsize the validation sets, used only for early stopping, to their first 10 sentences. We do not alter the test sets. We only experiment on the 56 treebanks that are large enough to apply these sampling procedures. ar nyuad, whose complete dat"
C18-1270,K17-3001,0,0.0983599,"then optimize the sentence score globally). In this work, we consider three dependency parsers, based on diverse parsing algorithms and classifiers, to assess the generality of our findings: UDP IPE (transition-based parser, with a feedforward neural classifier, i.e. embedding-based), B EAM (transition-based parser, with an averaged perceptron classifier, i.e. feature-based), and MSTPARSER (graph-based parser). We use version 0.5.1 of MSTPARSER (McDonald et al., 2005) with default parameters. For UDP IPE, we use version 1.1 (Straka and Strakov´a, 2017) with the same hyperparameters as Straka (2017), but without the word embeddings pre-trained on massive monolingual data (to ensure comparability). For B EAM, we rely on our own open source1 implementation, PAN PARSER (Aufrant and Wisniewski, 2016), using the ArcEager version (Nivre, 2004) with a dynamic oracle (Goldberg and Nivre, 2012) adapted for beam search (Aufrant et al., 2017) and the feature sets of Zhang and Nivre (2011) (coarse PoS, no labels). 3 Measuring difficulty The purpose of this work is to investigate dependencies for the learning of which large training datasets are unnecessary, and which can therefore be qualified as ‘e"
C18-1270,W15-2137,0,0.0131759,"es: there ˇ epl¨o’s Maltese treebank (Tiedemann have been publications with 300 Irish sentences (Lynn et al., 2012) or 371 in Slavom´ır C´ and van der Plas, 2016). 4 We similarly downsize the validation sets, used only for early stopping, to their first 10 sentences. We do not alter the test sets. We only experiment on the 56 treebanks that are large enough to apply these sampling procedures. ar nyuad, whose complete data is under license, is also excluded. 5 Resulting trainsets contain 5, 10, 20, 50, 100, 200 and 500 sentences. 6 While less representative of real-world processing capacities (Tiedemann, 2015), we believe this choice to be crucial in such studies focusing on syntactic properties, whose measurement would otherwise be biased by properties of the taggers and language-dependent vocabulary issues. 2 3192 y attachment decisions on ADJ tokens seem more complex, first of all because the PoS of the head is uncertain (sometimes a VERB, a NOUN, another ADJ, etc.). What remains to ascertain is whether this simple/complex distinction can relate to measurable properties. Our first experiment aims at studying the rate at which the different dependency classes are learned. In this experiment, we a"
C18-1270,C16-1043,0,0.0242944,"Missing"
C18-1270,W17-0419,1,0.854629,"defined by the child PoS and its attachment direction. Indeed, it fits particularly well our x intuition regarding many parsing difficulties: due to its frequency and determinism, the ADJ class (that is, all adjectives whose head is on the right) appears for instance simple in the English UD as it mostly corresponds to the bigram ‘ADJ NOUN’ and, sometimes, to predicative adjectives. On the contrary, the 1 Source code available at https://perso.limsi.fr/aufrant. Such properties can still depend on the annotation scheme though, as repeatedly pointed out in the literature (Schwartz et al., 2012; Wisniewski and Lacroix, 2017; Wisniewski et al., 2018). 3 This size has been chosen to cover both the scale of 10 sentences and that of existing treebanks, around 300 sentences: there ˇ epl¨o’s Maltese treebank (Tiedemann have been publications with 300 Irish sentences (Lynn et al., 2012) or 371 in Slavom´ır C´ and van der Plas, 2016). 4 We similarly downsize the validation sets, used only for early stopping, to their first 10 sentences. We do not alter the test sets. We only experiment on the 56 treebanks that are large enough to apply these sampling procedures. ar nyuad, whose complete data is under license, is also ex"
C18-1270,N18-2064,1,0.835989,"ts attachment direction. Indeed, it fits particularly well our x intuition regarding many parsing difficulties: due to its frequency and determinism, the ADJ class (that is, all adjectives whose head is on the right) appears for instance simple in the English UD as it mostly corresponds to the bigram ‘ADJ NOUN’ and, sometimes, to predicative adjectives. On the contrary, the 1 Source code available at https://perso.limsi.fr/aufrant. Such properties can still depend on the annotation scheme though, as repeatedly pointed out in the literature (Schwartz et al., 2012; Wisniewski and Lacroix, 2017; Wisniewski et al., 2018). 3 This size has been chosen to cover both the scale of 10 sentences and that of existing treebanks, around 300 sentences: there ˇ epl¨o’s Maltese treebank (Tiedemann have been publications with 300 Irish sentences (Lynn et al., 2012) or 371 in Slavom´ır C´ and van der Plas, 2016). 4 We similarly downsize the validation sets, used only for early stopping, to their first 10 sentences. We do not alter the test sets. We only experiment on the 56 treebanks that are large enough to apply these sampling procedures. ar nyuad, whose complete data is under license, is also excluded. 5 Resulting trains"
C18-1270,N01-1026,0,0.0287084,"Missing"
C18-1270,I08-3008,0,0.0575011,"Missing"
C18-1270,P11-2033,0,0.0414234,"e-based), and MSTPARSER (graph-based parser). We use version 0.5.1 of MSTPARSER (McDonald et al., 2005) with default parameters. For UDP IPE, we use version 1.1 (Straka and Strakov´a, 2017) with the same hyperparameters as Straka (2017), but without the word embeddings pre-trained on massive monolingual data (to ensure comparability). For B EAM, we rely on our own open source1 implementation, PAN PARSER (Aufrant and Wisniewski, 2016), using the ArcEager version (Nivre, 2004) with a dynamic oracle (Goldberg and Nivre, 2012) adapted for beam search (Aufrant et al., 2017) and the feature sets of Zhang and Nivre (2011) (coarse PoS, no labels). 3 Measuring difficulty The purpose of this work is to investigate dependencies for the learning of which large training datasets are unnecessary, and which can therefore be qualified as ‘easy to learn’. In this section, we formalize this intuition and introduce several empirical measures to quantify it. We then exploit the results of large-scale evaluations to design a new metric, C OMPLEXITY, which estimates the challenges faced when learning a given ‘type’ of dependencies: by departing from individual dependencies, we aim at discovering higherlevel properties relate"
D10-1076,W09-0417,1,0.829036,"this article are evaluated on the Arabic to English NIST 2009 constrained task. For the continuous space language model, the training data consists in the parallel corpus used to train the translation model (previously described in section 3.1). The development data is again the 2006 NIST test set and the test data is the official 2008 NIST test set. Our system is built using the open-source Moses toolkit (Koehn et al., 2007) with default settings. To set up our baseline results, we used an extensively optimized standard back-off 4-grams language model using Kneser-Ney smoothing described in (Allauzen et al., 2009). The weights used during the reranking are tuned using the Minimum Error Rate Training algorithm (Och, 2003). Performance is measured based on the BLEU (Papineni et al., 2002) scores, which are reported in Table 4. Table 4: BLEU scores on the NIST MT08 test set with different language models. Vc size all 10000 all Model baseline log bilinear standard iterative reinit. reinit. standard one vector init. # epochs 6 13 6 11 14 9 BLEU 37.8 38.2 38.3 38.4 38.4 38.6 38.7 All the experimented neural language models yield to a significant BLEU increase. The best result is obtained by the one vector in"
D10-1076,J92-4003,0,0.180781,"irical overview, and (Teh, 2006) for a Bayesian interpretation), perform back-off on lower order distributions to provide an estimate for the probability of these unseen events. n-gram language models rely on a discrete space representation of the vocabulary, where each word is associated with a discrete index. In this model, the morphological, syntactic and semantic relationships which structure the lexicon are completely ignored, which negatively impact the generalization performance of the model. Various approaches have proposed to overcome this limitation, notably the use of word-classes (Brown et al., 1992; Niesler, 1997), of generalized back-off strategies (Bilmes et al., 1997) or the explicit integration of morphological information in the random-forest model (Xu and Jelinek, 2004; Oparin et al., 2008). One of the most successful alternative to date is to use distributed word representations (Bengio et al., 2003), where distributionally similar words are represented as neighbors in a continuous space. This 778 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 778–788, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational L"
D10-1076,P96-1041,0,0.032208,"L Y P (wl |w1l−1 ) l=1 Modeling the joint distribution of several discrete random variables (such as words in a sentence) is Many approaches to this problem have been proposed over the last decades, the most widely used being back-off n-gram language models. n-gram models rely on a Markovian assumption, and despite this simplification, the maximum likelihood estimate (MLE) remains unreliable and tends to underestimate the probability of very rare n-grams, which are hardly observed even in huge corpora. Conventional smoothing techniques, such as KneserNey and Witten-Bell back-off schemes (see (Chen and Goodman, 1996) for an empirical overview, and (Teh, 2006) for a Bayesian interpretation), perform back-off on lower order distributions to provide an estimate for the probability of these unseen events. n-gram language models rely on a discrete space representation of the vocabulary, where each word is associated with a discrete index. In this model, the morphological, syntactic and semantic relationships which structure the lexicon are completely ignored, which negatively impact the generalization performance of the model. Various approaches have proposed to overcome this limitation, notably the use of wor"
D10-1076,P07-2045,0,0.0086703,"atallah Mesyats Langlois 1 1 vector init. 4160th 3651st 3487th 3378th 3558th best list is accordingly reordered to produce the final translations. The different language models discussed in this article are evaluated on the Arabic to English NIST 2009 constrained task. For the continuous space language model, the training data consists in the parallel corpus used to train the translation model (previously described in section 3.1). The development data is again the 2006 NIST test set and the test data is the official 2008 NIST test set. Our system is built using the open-source Moses toolkit (Koehn et al., 2007) with default settings. To set up our baseline results, we used an extensively optimized standard back-off 4-grams language model using Kneser-Ney smoothing described in (Allauzen et al., 2009). The weights used during the reranking are tuned using the Minimum Error Rate Training algorithm (Och, 2003). Performance is measured based on the BLEU (Papineni et al., 2002) scores, which are reported in Table 4. Table 4: BLEU scores on the NIST MT08 test set with different language models. Vc size all 10000 all Model baseline log bilinear standard iterative reinit. reinit. standard one vector init. #"
D10-1076,J10-4005,0,0.0044806,"n the small vocabulary tasks occurred more than several hundreds times in the training corpus, which was more than sufficient to guide the model towards satisfactory projection matrices. This finally suggests that there still exists room for improvement if we can find more efficient initialization strategies than starting from one or several random points. 4.4 Statistical machine translation experiments As a last experiment, we compare the various models on a large scale machine translation task. Statistical language models are key component of current statistical machine translation systems (Koehn, 2010), where they both help disambiguate lexical choices in the target language and influence the choice of the right word ordering. The integration of a neural network language model in such a system is far from easy, given the computational cost of computing word probabilities, a task that is performed repeatedly during the search of the best translation. We then had to resort to a two pass decoding approach: the first pass uses a conventional back-off language model to produce a n-best list (the n most likely translations and their associated scores); in the second pass, the probability of the n"
D10-1076,H93-1021,0,0.0464304,"These two models differ only by the activation function of their hidden layer (linear for the LBL model and tangent hyperbolic for the standard model) and by their definition of the prediction space: for the LBL model, the context space and the prediction space are the same (R = Who , and thus H = m), while in the standard model, the prediction space is defined independently from the context space. This restriction drastically reduces the number of free parameters of the LBL model. It is finally noteworthy to outline the similarity of this model with standard maximum entropy language models (Lau et al., 1993; Rosenfeld, 1996). Let x denote the binary vector formed by stacking the (n-1) 1-of-V encodings of the history words; then the conditional probability distributions estimated in the model are proportional to exp F (x), where F is an affine transform of x. The main difference with MaxEnt language models are thus the restricted form of the feature functions, which only test one history word, and the particular representation of F , which is defined as: T F (x) = RWih R0 v + Rbih + bho where, as before, R0 is formed by concatenating (n − 1) copies of the projection matrix R. 2.3 Training and inf"
D10-1076,P03-1021,0,0.0834137,"the training data consists in the parallel corpus used to train the translation model (previously described in section 3.1). The development data is again the 2006 NIST test set and the test data is the official 2008 NIST test set. Our system is built using the open-source Moses toolkit (Koehn et al., 2007) with default settings. To set up our baseline results, we used an extensively optimized standard back-off 4-grams language model using Kneser-Ney smoothing described in (Allauzen et al., 2009). The weights used during the reranking are tuned using the Minimum Error Rate Training algorithm (Och, 2003). Performance is measured based on the BLEU (Papineni et al., 2002) scores, which are reported in Table 4. Table 4: BLEU scores on the NIST MT08 test set with different language models. Vc size all 10000 all Model baseline log bilinear standard iterative reinit. reinit. standard one vector init. # epochs 6 13 6 11 14 9 BLEU 37.8 38.2 38.3 38.4 38.4 38.6 38.7 All the experimented neural language models yield to a significant BLEU increase. The best result is obtained by the one vector initialization standard model which achieves a 0.9 BLEU improvement. While this results is similar to the one o"
D10-1076,P02-1040,0,0.104017,"to train the translation model (previously described in section 3.1). The development data is again the 2006 NIST test set and the test data is the official 2008 NIST test set. Our system is built using the open-source Moses toolkit (Koehn et al., 2007) with default settings. To set up our baseline results, we used an extensively optimized standard back-off 4-grams language model using Kneser-Ney smoothing described in (Allauzen et al., 2009). The weights used during the reranking are tuned using the Minimum Error Rate Training algorithm (Och, 2003). Performance is measured based on the BLEU (Papineni et al., 2002) scores, which are reported in Table 4. Table 4: BLEU scores on the NIST MT08 test set with different language models. Vc size all 10000 all Model baseline log bilinear standard iterative reinit. reinit. standard one vector init. # epochs 6 13 6 11 14 9 BLEU 37.8 38.2 38.3 38.4 38.4 38.6 38.7 All the experimented neural language models yield to a significant BLEU increase. The best result is obtained by the one vector initialization standard model which achieves a 0.9 BLEU improvement. While this results is similar to the one obtained with the standard model, the training time is reduced here"
D10-1076,P06-2093,0,0.69272,"Missing"
D10-1076,P06-1124,0,0.0109844,"f several discrete random variables (such as words in a sentence) is Many approaches to this problem have been proposed over the last decades, the most widely used being back-off n-gram language models. n-gram models rely on a Markovian assumption, and despite this simplification, the maximum likelihood estimate (MLE) remains unreliable and tends to underestimate the probability of very rare n-grams, which are hardly observed even in huge corpora. Conventional smoothing techniques, such as KneserNey and Witten-Bell back-off schemes (see (Chen and Goodman, 1996) for an empirical overview, and (Teh, 2006) for a Bayesian interpretation), perform back-off on lower order distributions to provide an estimate for the probability of these unseen events. n-gram language models rely on a discrete space representation of the vocabulary, where each word is associated with a discrete index. In this model, the morphological, syntactic and semantic relationships which structure the lexicon are completely ignored, which negatively impact the generalization performance of the model. Various approaches have proposed to overcome this limitation, notably the use of word-classes (Brown et al., 1992; Niesler, 199"
D10-1076,W04-3242,0,\N,Missing
D10-1091,2007.mtsummit-papers.3,0,0.0519115,"Missing"
D10-1091,W09-0437,0,0.394639,"l., 2007) considers only a restricted number of translations for each source sequence2 and enforces a distortion limit3 over which phrases can be reordered. As a consequence, the best translation hypothesis returned by the decoder is not always the one with the highest score. 1.2 Typology of PBTS Errors Analyzing the errors of a SMT system is not an easy task, because of the number of models that are combined, the size of these models, and the high complexity of the various decision making processes. For a SMT system, three different kinds of errors can be distinguished (Germann et al., 2004; Auli et al., 2009): search errors, induction errors and model errors. The former corresponds to cases where the hypothesis with the best score is missed by the search procedure, either because of the use of an ap2 the 3 the ttl option of Moses, defaulting to 20. dl option of Moses, whose default value is 7. 933 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 933–943, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics proximate search method or because of the restrictions of the search space. Induction errors correspond to ca"
D10-1091,W05-0909,0,0.0448439,"ses and can not be evaluated in isolation. Because of its nondecomposability, maximizing BLEU-4 is hard; in particular, the phrase-level decomposability of the evaluation metric is necessary in our approach. To circumvent this difficulty, we propose to evaluate the similarity between a translation hypothesis and a reference by the number of their common words. This amounts to evaluating translation quality in terms of unigram precision and recall, which are highly correlated with human judgements (Lavie et al., ). This measure is closely related to the BLEU-1 evaluation metric and the Meteor (Banerjee and Lavie, 2005) metric (when it is evaluated without considering near-matches and the distortion penalty). We also believe that hypotheses that maximize the unigram precision and recall at the sentence level yield corpus level BLEU-4 scores close the maximal achievable. Indeed, in the setting we will introduce in the next section, BLEU-1 and BLEU-4 are highly correlated: as all correct words of the hypothesis will be compelled to be at their correct position, any hypothesis with a high 1-gram precision is also bound to have a high 2-gram precision, etc. 2.2 Formalizing the Oracle Decoding Problem The oracle"
D10-1091,P07-1020,0,0.0173698,"y accurately and efficiently using Integer Linear Programming techniques. Our main result is a confirmation of the fact that extant PBTS systems are expressive enough to achieve very high translation performance with respect to conventional quality measurements. The main efforts should therefore strive to improve on the way phrases and hypotheses are scored during training. This gives further support to attempts aimed at designing context-dependent scoring functions as in (Stroppa et al., 2007; Gimpel and Smith, 2008), or at attempts to perform discriminative training of feature-rich models. (Bangalore et al., 2007). We have shown that the examination of difficult-totranslate sentences was an effective way to detect errors or inconsistencies in the reference translations, making our approach a potential aid for controlling the quality or assessing the difficulty of test data. Our experiments have also highlighted the impact of various parameters. Various extensions of the baseline ILP program have been suggested and/or evaluated. In particular, the ILP formalism lends itself well to expressing various constraints that are typically used in conventional PBTS. In 17 The best BLEU-4 oracle they achieve on E"
D10-1091,D08-1064,0,0.028483,"igh 2-gram precision, etc. 2.2 Formalizing the Oracle Decoding Problem The oracle decoding problem has already been considered in the case of word-based models, in which all translation units are bound to contain only one word. The problem can then be solved by a bipartite graph matching algorithm (Leusch et al., 2008): given a n×m binary matrix describing possible translation links between source words and target words7 , this algorithm finds the subset of links maximizing the number of words of the reference that have been translated, while ensuring that each word 6 Neither at the sentence (Chiang et al., 2008), nor at the phrase level. 7 The (i, j) entry of the matrix is 1 if the ith word of the source can be translated by the j th word of the reference, 0 otherwise. 935 is translated only once. Generalizing this approach to phrase-based systems amounts to solving the following problem: given a set of possible translation links between potential phrases of the source and of the target, find the subset of links so that the unigram precision and recall are the highest possible. The corresponding oracle hypothesis can then be easily generated by selecting the target phrases that are aligned with one s"
D10-1091,P08-2007,0,0.041734,"Missing"
D10-1091,W07-0414,0,0.320042,"rious causes of errors is of primary interest for SMT system developers: for lack of such diagnoses, it is difficult to figure out which components of the system require the most urgent attention. Diagnoses are however, given the tight intertwining among the various component of a system, very difficult to obtain: most evaluations are limited to the computation of global scores and usually do not imply any kind of failure analysis. 1.3 Contribution and organization To systematically assess the impact of the multiple heuristic decisions made during training and decoding, we propose, following (Dreyer et al., 2007; Auli et al., 2009), to work out oracle scores, that is to evaluate the best achievable performances of a PBTS. We aim at both studying the expressive power of PBTS and at providing tools for identifying and quantifying causes of failure. Under standard metrics such as BLEU (Papineni et al., 2002), oracle scores are difficult (if not impossible) to compute, but, by casting the computation of the oracle unigram recall and precision as an Integer Linear Programming (ILP) problem, we show that it is possible to efficiently compute accurate lower-bounds of the oracle BLEU-4 scores and report meas"
D10-1091,P01-1030,0,0.0693371,"f (Leusch et al., 2008) and concur in making the oracle decoding problem for phrase-based models more complex than it is for word-based models: it can be proven, using arguments borrowed from (De Nero and Klein, 2008), that this problem is NP-hard even for the simple unigram precision measure. 2.3 An Integer Program for Oracle Decoding To solve the combinatorial problem introduced in the previous section, we propose to cast it into an Integer Linear Programming (ILP) problem, for which many generic solvers exist. ILP has already been used in SMT to find the optimal translation for word-based (Germann et al., 2001) and to study the complexity of learning phrase alignments (De Nero and Klein, 2008) models. Following the latter reference, we introduce the following variables: fi,j (resp. ek,l ) is a binary indicator variable that is true when the phrase contains all spans from betweenword position i to j (resp. k to l) of the source (resp. target) sentence. We also introduce a binary variable, denoted ai,j,k,l , to describe a possible link between source phrase fi,j and target phrase ek,l . These variables are built from the entries of the phrase table according to selection strategies introduced in Secti"
D10-1091,N03-1010,0,0.0268074,"is aligned at the word level, by using alignment tools such as Giza++ (Och and Ney, 2003) and some symmetrisation heuristics; phrases are then extracted by other heuristics (Koehn et al., 2003) and assigned numerical weights. In the second step, the parameters of the scoring function are estimated, typically through Minimum Error Rate training (Och, 2003). Translating a sentence amounts to finding the best scoring translation hypothesis in the search space. Because of the combinatorial nature of this problem, translation has to rely on heuristic search techniques such as greedy hill-climbing (Germann, 2003) or variants of best-first search like multi-stack decoding (Koehn, 2004). Moreover, to reduce the overall complexity of decoding, the search space is typically pruned using simple heuristics. For instance, the state-of-the-art phrase-based decoder Moses (Koehn et al., 2007) considers only a restricted number of translations for each source sequence2 and enforces a distortion limit3 over which phrases can be reordered. As a consequence, the best translation hypothesis returned by the decoder is not always the one with the highest score. 1.2 Typology of PBTS Errors Analyzing the errors of a SMT"
D10-1091,W08-0302,0,0.0314872,"roximation of the BLEU-4 oracle score. We have shown that this approximation could be computed fairly accurately and efficiently using Integer Linear Programming techniques. Our main result is a confirmation of the fact that extant PBTS systems are expressive enough to achieve very high translation performance with respect to conventional quality measurements. The main efforts should therefore strive to improve on the way phrases and hypotheses are scored during training. This gives further support to attempts aimed at designing context-dependent scoring functions as in (Stroppa et al., 2007; Gimpel and Smith, 2008), or at attempts to perform discriminative training of feature-rich models. (Bangalore et al., 2007). We have shown that the examination of difficult-totranslate sentences was an effective way to detect errors or inconsistencies in the reference translations, making our approach a potential aid for controlling the quality or assessing the difficulty of test data. Our experiments have also highlighted the impact of various parameters. Various extensions of the baseline ILP program have been suggested and/or evaluated. In particular, the ILP formalism lends itself well to expressing various cons"
D10-1091,N03-1017,0,0.0262102,"el corresponds to the set of all possible sequences of 1 Following the usage in statistical machine translation literature, we use “phrase” to denote a subsequence of consecutive words. rules applications. The scoring function aims to rank all possible translation hypotheses in such a way that the best one has the highest score. A PBTS is learned from a parallel corpus in two independent steps. In a first step, the corpus is aligned at the word level, by using alignment tools such as Giza++ (Och and Ney, 2003) and some symmetrisation heuristics; phrases are then extracted by other heuristics (Koehn et al., 2003) and assigned numerical weights. In the second step, the parameters of the scoring function are estimated, typically through Minimum Error Rate training (Och, 2003). Translating a sentence amounts to finding the best scoring translation hypothesis in the search space. Because of the combinatorial nature of this problem, translation has to rely on heuristic search techniques such as greedy hill-climbing (Germann, 2003) or variants of best-first search like multi-stack decoding (Koehn, 2004). Moreover, to reduce the overall complexity of decoding, the search space is typically pruned using simpl"
D10-1091,P07-2045,0,0.0154746,"oring function are estimated, typically through Minimum Error Rate training (Och, 2003). Translating a sentence amounts to finding the best scoring translation hypothesis in the search space. Because of the combinatorial nature of this problem, translation has to rely on heuristic search techniques such as greedy hill-climbing (Germann, 2003) or variants of best-first search like multi-stack decoding (Koehn, 2004). Moreover, to reduce the overall complexity of decoding, the search space is typically pruned using simple heuristics. For instance, the state-of-the-art phrase-based decoder Moses (Koehn et al., 2007) considers only a restricted number of translations for each source sequence2 and enforces a distortion limit3 over which phrases can be reordered. As a consequence, the best translation hypothesis returned by the decoder is not always the one with the highest score. 1.2 Typology of PBTS Errors Analyzing the errors of a SMT system is not an easy task, because of the number of models that are combined, the size of these models, and the high complexity of the various decision making processes. For a SMT system, three different kinds of errors can be distinguished (Germann et al., 2004; Auli et a"
D10-1091,koen-2004-pharaoh,0,0.0452448,"and Ney, 2003) and some symmetrisation heuristics; phrases are then extracted by other heuristics (Koehn et al., 2003) and assigned numerical weights. In the second step, the parameters of the scoring function are estimated, typically through Minimum Error Rate training (Och, 2003). Translating a sentence amounts to finding the best scoring translation hypothesis in the search space. Because of the combinatorial nature of this problem, translation has to rely on heuristic search techniques such as greedy hill-climbing (Germann, 2003) or variants of best-first search like multi-stack decoding (Koehn, 2004). Moreover, to reduce the overall complexity of decoding, the search space is typically pruned using simple heuristics. For instance, the state-of-the-art phrase-based decoder Moses (Koehn et al., 2007) considers only a restricted number of translations for each source sequence2 and enforces a distortion limit3 over which phrases can be reordered. As a consequence, the best translation hypothesis returned by the decoder is not always the one with the highest score. 1.2 Typology of PBTS Errors Analyzing the errors of a SMT system is not an easy task, because of the number of models that are com"
D10-1091,H05-1021,0,0.0253438,"r uses a distortion limit to constrain the set of possible reorderings. This constraint “enforces (...) that the last word of a phrase chosen for translation cannot be more than d9 words from the leftmost untranslated word in the source” (Lopez, 2009) and is expressed as: ∀aijkl , ai0 j 0 k0 l0 s.t. k > k 0 , aijkl · ai0 j 0 k0 l0 · |j − i0 + 1 |≤ d, The maximum distortion limit strategy (Lopez, 2009) is also easily expressed and take the following form (assuming this constraint is parameterized by d): ∀l &lt; m − 1, ai,j,k,l ·ai0 ,j 0 ,l+1,l0 · |i0 − j − 1 |&lt; d Implementing the “local” or MJ-d (Kumar and Byrne, 2005) reordering strategy is also straightforward, and implies using the following constraints: X X ∀i, k, ai0 ,j 0 ,k0 ,l0 − ai0 ,j 0 ,k0 ,l0 ≤ d i0 ≤i k0 ≤k Similarly, It is possible to simulate decoding under the so-called IBM(d) reordering constraints10 by considering the following constraints: X ∀µ ≤ m, max ai,j,k,l · j − ai,j,k,l · (j − i) ≤ d i,k,l j≤µ 9 This i,j,k,l corresponds to the dl parameter of Moses In these constraints, the first factor corresponds to the rightmost translated word of the source and the second one to the number of translated source words. The constraints simply enfor"
D10-1091,D08-1088,0,0.441812,"scores close the maximal achievable. Indeed, in the setting we will introduce in the next section, BLEU-1 and BLEU-4 are highly correlated: as all correct words of the hypothesis will be compelled to be at their correct position, any hypothesis with a high 1-gram precision is also bound to have a high 2-gram precision, etc. 2.2 Formalizing the Oracle Decoding Problem The oracle decoding problem has already been considered in the case of word-based models, in which all translation units are bound to contain only one word. The problem can then be solved by a bipartite graph matching algorithm (Leusch et al., 2008): given a n×m binary matrix describing possible translation links between source words and target words7 , this algorithm finds the subset of links maximizing the number of words of the reference that have been translated, while ensuring that each word 6 Neither at the sentence (Chiang et al., 2008), nor at the phrase level. 7 The (i, j) entry of the matrix is 1 if the ith word of the source can be translated by the j th word of the reference, 0 otherwise. 935 is translated only once. Generalizing this approach to phrase-based systems amounts to solving the following problem: given a set of po"
D10-1091,N09-2003,0,0.199476,"ally generates allows us to carry on both quantitative and qualitative failure analysis. The oracle decoding problem can also be used to assess the expressive power of phrase-based systems (Auli et al., 2009). Other applications include computing acceptable pseudo-references for discriminative training (Tillmann and Zhang, 2006; Liang et al., 2006; Arun and 5 The oracle decoding problem can be extended to the case of multiple references. For the sake of simplicity, we only describe the case of a single reference. Koehn, 2007) or combining machine translation systems in a multi-source setting (Li and Khudanpur, 2009). We have also used oracle decoding to identify erroneous or difficult to translate references (Section 3.3). Evaluation Measure To fully define the oracle decoding problem, a measure of the similarity between a translation hypothesis and its reference translation has to be chosen. The most obvious choice is the BLEU-4 score (Papineni et al., 2002) used in most machine translation evaluations. However, using this metric in the oracle decoding problem raises several issues. First, BLEU-4 is a metric defined at the corpus level and is hard to interpret at the sentence level. More importantly, BL"
D10-1091,P06-1096,0,0.271375,"Missing"
D10-1091,E09-1061,0,0.0614161,"compute the oracle BLEU-4 score, that is the best score that a system based on this PT can achieve on a reference corpus. By casting the computation of the oracle BLEU-1 as an Integer Linear Programming (ILP) problem, we show that it is possible to efficiently compute accurate lower-bounds of this score, and report measures performed on several standard benchmarks. Various other applications of these oracle decoding techniques are also reported and discussed. 1 1.1 Phrase-Based Machine Translation Principle A Phrase-Based Translation System (PBTS) consists of a ruleset and a scoring function (Lopez, 2009). The ruleset, represented in the phrase table, is a set of phrase1 pairs {(f, e)}, each pair expressing that the source phrase f can be rewritten (translated) into a target phrase e. Translation hypotheses are generated by iteratively rewriting portions of the source sentence as prescribed by the ruleset, until each source word has been consumed by exactly one rule. The order of target words in an hypothesis is uniquely determined by the order in which the rewrite operation are performed. The search space of the translation model corresponds to the set of all possible sequences of 1 Following"
D10-1091,J03-1002,0,0.0140971,"ined by the order in which the rewrite operation are performed. The search space of the translation model corresponds to the set of all possible sequences of 1 Following the usage in statistical machine translation literature, we use “phrase” to denote a subsequence of consecutive words. rules applications. The scoring function aims to rank all possible translation hypotheses in such a way that the best one has the highest score. A PBTS is learned from a parallel corpus in two independent steps. In a first step, the corpus is aligned at the word level, by using alignment tools such as Giza++ (Och and Ney, 2003) and some symmetrisation heuristics; phrases are then extracted by other heuristics (Koehn et al., 2003) and assigned numerical weights. In the second step, the parameters of the scoring function are estimated, typically through Minimum Error Rate training (Och, 2003). Translating a sentence amounts to finding the best scoring translation hypothesis in the search space. Because of the combinatorial nature of this problem, translation has to rely on heuristic search techniques such as greedy hill-climbing (Germann, 2003) or variants of best-first search like multi-stack decoding (Koehn, 2004)."
D10-1091,P03-1021,0,0.032078,"utive words. rules applications. The scoring function aims to rank all possible translation hypotheses in such a way that the best one has the highest score. A PBTS is learned from a parallel corpus in two independent steps. In a first step, the corpus is aligned at the word level, by using alignment tools such as Giza++ (Och and Ney, 2003) and some symmetrisation heuristics; phrases are then extracted by other heuristics (Koehn et al., 2003) and assigned numerical weights. In the second step, the parameters of the scoring function are estimated, typically through Minimum Error Rate training (Och, 2003). Translating a sentence amounts to finding the best scoring translation hypothesis in the search space. Because of the combinatorial nature of this problem, translation has to rely on heuristic search techniques such as greedy hill-climbing (Germann, 2003) or variants of best-first search like multi-stack decoding (Koehn, 2004). Moreover, to reduce the overall complexity of decoding, the search space is typically pruned using simple heuristics. For instance, the state-of-the-art phrase-based decoder Moses (Koehn et al., 2007) considers only a restricted number of translations for each source"
D10-1091,P02-1040,0,0.0801182,"ifficult to obtain: most evaluations are limited to the computation of global scores and usually do not imply any kind of failure analysis. 1.3 Contribution and organization To systematically assess the impact of the multiple heuristic decisions made during training and decoding, we propose, following (Dreyer et al., 2007; Auli et al., 2009), to work out oracle scores, that is to evaluate the best achievable performances of a PBTS. We aim at both studying the expressive power of PBTS and at providing tools for identifying and quantifying causes of failure. Under standard metrics such as BLEU (Papineni et al., 2002), oracle scores are difficult (if not impossible) to compute, but, by casting the computation of the oracle unigram recall and precision as an Integer Linear Programming (ILP) problem, we show that it is possible to efficiently compute accurate lower-bounds of the oracle BLEU-4 scores and report measurements performed on several standard benchmarks. The main contributions of this paper are twofold. We first introduce an ILP program able to efficiently find the best hypothesis a PBTS can achieve. This program can be easily extended to test various improvements to 4 We omit here optimization err"
D10-1091,2007.tmi-papers.28,0,0.0659647,"Missing"
D10-1091,P06-1091,0,0.0995125,"ems to generate good candidate translations, irrespective of their ability to score them properly. We believe that studying this problem is interesting for various reasons. First, as described in Section 3.4, comparing the best hypothesis a system could have generated and the hypothesis it actually generates allows us to carry on both quantitative and qualitative failure analysis. The oracle decoding problem can also be used to assess the expressive power of phrase-based systems (Auli et al., 2009). Other applications include computing acceptable pseudo-references for discriminative training (Tillmann and Zhang, 2006; Liang et al., 2006; Arun and 5 The oracle decoding problem can be extended to the case of multiple references. For the sake of simplicity, we only describe the case of a single reference. Koehn, 2007) or combining machine translation systems in a multi-source setting (Li and Khudanpur, 2009). We have also used oracle decoding to identify erroneous or difficult to translate references (Section 3.3). Evaluation Measure To fully define the oracle decoding problem, a measure of the similarity between a translation hypothesis and its reference translation has to be chosen. The most obvious choice"
D10-1091,W08-0305,0,0.082699,"Missing"
D10-1091,W05-0834,0,0.024682,"not directly comparable with ours17 , it seems that our decoder is not only conceptually much simpler, but also achieves much more optimistic lower-bounds of the oracle BLEU score. The approach described in (Li and Khudanpur, 2009) employs a similar technique, which is to guide a heuristic search in an hypergraph representing possible translation hypotheses with n-gram counts matches, which amounts to decoding with a n-gram model trained on the sole reference translation. Additional tricks are presented in this article to speed-up decoding. Computing oracle BLEU scores is also the subject of (Zens and Ney, 2005; Leusch et al., 2008), yet with a different emphasis. These studies are concerned with finding the best hypotheses in a word graph or in a consensus network, a problem that has various implications for multi-pass decoding and/or system combination techniques. The former reference describes an exponential approximate algorithm, while the latter proves the NPcompleteness of this problem and discuss various heuristic approaches. Our problem is somewhat more complex and using their techniques would require us to built word graphs containing all the translations induced by arbitrary segmentations"
D10-1091,lavie-etal-2004-significance,0,\N,Missing
D10-1091,W09-0401,0,\N,Missing
D14-1187,P11-1061,0,0.027859,"Missing"
D14-1187,D13-1205,0,0.0388242,"ssue of extending standard supervised techniques with partial and/or uncertain labels in the presence of alignment noise. In comparison to the early approach of Yarowsky et al. (2001) in which POS are directly transferred, subject to heuristic filtering rules, recent works consider the integration of softer constraints using expectation regularization techniques (Wang and Manning, 2014), the combination of alignment-based POS transfer with additional information sources such as dictionaries (Li et al., 2012; Täckström et al., 2013) (Section 2), or even the simultaneous use of both techniques (Ganchev and Das, 2013). In this paper, we reproduce the weakly supervised setting of Täckström et al. (2013). By recasting this setting in the framework of ambiguous learning (Bordes et al., 2010; Cour et al., 2011) (Section 3), we propose an alternative learning methodology and show that it improves the state of the art performance on a large array of languages (Section 4). Our analysis of the remaining errors suggests that in cross-lingual settings, improvements of error rates can have multiple causes and should be looked at with great care (Section 4.2). All tools and resources used in this study are available a"
D14-1187,P09-1042,0,0.0101862,"ly annotated data. Several attempts have recently been made to mitigate the lack of annotated corpora using parallel data pairing a (source) text in a resource-rich language with its counterpart in a less-resourced language. By transferring labels from the source to the target, it becomes possible to obtain noisy, yet useful, annotations that can be used to train a model for the target language in a weakly supervised manner. This research trend was initiated by Yarowsky et al. (2001), who consider the transfer of POS and other syntactic information, and further developed in (Hwa et al., 2005; Ganchev et al., 2009) for syntactic dependencies, in (Padó and Lapata, 2009; Kozhevnikov and Titov, 2013; van der Plas et al., 2014) for semantic role labeling and in (Kim et al., 2012) for named-entity recognition, to name a few. Assuming that labels can actually be projected across languages, these techniques face the issue of extending standard supervised techniques with partial and/or uncertain labels in the presence of alignment noise. In comparison to the early approach of Yarowsky et al. (2001) in which POS are directly transferred, subject to heuristic filtering rules, recent works consider the integration"
D14-1187,D12-1127,0,0.0275695,"ame a few. Assuming that labels can actually be projected across languages, these techniques face the issue of extending standard supervised techniques with partial and/or uncertain labels in the presence of alignment noise. In comparison to the early approach of Yarowsky et al. (2001) in which POS are directly transferred, subject to heuristic filtering rules, recent works consider the integration of softer constraints using expectation regularization techniques (Wang and Manning, 2014), the combination of alignment-based POS transfer with additional information sources such as dictionaries (Li et al., 2012; Täckström et al., 2013) (Section 2), or even the simultaneous use of both techniques (Ganchev and Das, 2013). In this paper, we reproduce the weakly supervised setting of Täckström et al. (2013). By recasting this setting in the framework of ambiguous learning (Bordes et al., 2010; Cour et al., 2011) (Section 3), we propose an alternative learning methodology and show that it improves the state of the art performance on a large array of languages (Section 4). Our analysis of the remaining errors suggests that in cross-lingual settings, improvements of error rates can have multiple causes and"
D14-1187,petrov-etal-2012-universal,0,0.16465,"Missing"
D14-1187,W11-0328,0,0.0240082,"other using, for instance, a linear model: yi∗ = arg max hw|φ(x, i, y, hi )i y∈Y (1) where h·|·i is the standard dot product operation, yi∗ the predicted label for position i, w the weight ∗ vector, hi = y1∗ , ..., yi−1 the history of past decisions and φ a joint feature map. Inference can therefore be seen as a greedy search in the space of the # {Y}n possible labelings of the input sequence. Trading off the global optimality of inference for additional flexibility in the design of features and long range dependencies between labels has proved useful for many sequence labeling tasks in NLP (Tsuruoka et al., 2011). The training procedure, sketched in Algorithm 1, consists in performing inference on each input sentence and correcting the weight vector each time a wrong decision is made. Importantly (Ross and Bagnell, 2010), the history used during training has to be made of the previous predicted labels so that the training samples reflect the fact that the history will be imperfectly known at test time. This reduction of sequence labeling to multiclass classification allows us to learn a sequence model in an ambiguous setting by building on the theoretical results of Bordes et al. (2010) and Cour et al"
D14-1187,Q13-1001,0,0.149557,"ng that labels can actually be projected across languages, these techniques face the issue of extending standard supervised techniques with partial and/or uncertain labels in the presence of alignment noise. In comparison to the early approach of Yarowsky et al. (2001) in which POS are directly transferred, subject to heuristic filtering rules, recent works consider the integration of softer constraints using expectation regularization techniques (Wang and Manning, 2014), the combination of alignment-based POS transfer with additional information sources such as dictionaries (Li et al., 2012; Täckström et al., 2013) (Section 2), or even the simultaneous use of both techniques (Ganchev and Das, 2013). In this paper, we reproduce the weakly supervised setting of Täckström et al. (2013). By recasting this setting in the framework of ambiguous learning (Bordes et al., 2010; Cour et al., 2011) (Section 3), we propose an alternative learning methodology and show that it improves the state of the art performance on a large array of languages (Section 4). Our analysis of the remaining errors suggests that in cross-lingual settings, improvements of error rates can have multiple causes and should be looked at with"
D14-1187,C14-1121,0,0.0202466,"Missing"
D14-1187,Q14-1005,0,0.0238298,"and Titov, 2013; van der Plas et al., 2014) for semantic role labeling and in (Kim et al., 2012) for named-entity recognition, to name a few. Assuming that labels can actually be projected across languages, these techniques face the issue of extending standard supervised techniques with partial and/or uncertain labels in the presence of alignment noise. In comparison to the early approach of Yarowsky et al. (2001) in which POS are directly transferred, subject to heuristic filtering rules, recent works consider the integration of softer constraints using expectation regularization techniques (Wang and Manning, 2014), the combination of alignment-based POS transfer with additional information sources such as dictionaries (Li et al., 2012; Täckström et al., 2013) (Section 2), or even the simultaneous use of both techniques (Ganchev and Das, 2013). In this paper, we reproduce the weakly supervised setting of Täckström et al. (2013). By recasting this setting in the framework of ambiguous learning (Bordes et al., 2010; Cour et al., 2011) (Section 3), we propose an alternative learning methodology and show that it improves the state of the art performance on a large array of languages (Section 4). Our analysi"
D14-1187,H01-1035,0,0.124832,"plication domains and/or less-resourced languages, alternative ML techniques need to be designed to accommodate unannotated or partially annotated data. Several attempts have recently been made to mitigate the lack of annotated corpora using parallel data pairing a (source) text in a resource-rich language with its counterpart in a less-resourced language. By transferring labels from the source to the target, it becomes possible to obtain noisy, yet useful, annotations that can be used to train a model for the target language in a weakly supervised manner. This research trend was initiated by Yarowsky et al. (2001), who consider the transfer of POS and other syntactic information, and further developed in (Hwa et al., 2005; Ganchev et al., 2009) for syntactic dependencies, in (Padó and Lapata, 2009; Kozhevnikov and Titov, 2013; van der Plas et al., 2014) for semantic role labeling and in (Kim et al., 2012) for named-entity recognition, to name a few. Assuming that labels can actually be projected across languages, these techniques face the issue of extending standard supervised techniques with partial and/or uncertain labels in the presence of alignment noise. In comparison to the early approach of Yaro"
D14-1187,D12-1125,0,0.139036,"Missing"
D14-1187,P12-1073,0,\N,Missing
D14-1187,P13-1117,0,\N,Missing
D19-5553,D16-1025,0,0.0229355,"he case for the PBSMT models, as seen in Table 3. In this way, in Figure 2, we can notice that the highest improvement caused by our phonetic normalization pipeline is present in short sentences (between 1 and 10 words). It is worth noting that this is the only case where the Transformer outperforms PBSMT in this Figure. Hence, the higher overall Transformer BLEU score over PBSMT is certainly due to a relatively high successful normalization over the shortest sentences of the Cr#pbank test set. This agrees with the documented fact that NMT is consistently better than PBSMT on short sentences (Bentivogli et al., 2016) and, in this concrete example, it seems that the Transformer can take advantage of this when we apply our normalization pipeline. Additionally, these results could be regarded as evidence supporting that our proposed method performs generally System Blind Tests MTNT Cr#pbank Large - PBSMT Raw Large - PBSMT Phon. Norm 29.3 26.7 30.5 26.9 Small - Transformer Raw Small - Transformer Phon. Norm 25.0 24.5 19.0 18.3 M&N18 Raw M&N18 UNK rep. Raw 19.3 21.9 13.3 15.4 Table 4: BLEU score results comparison on the MTNT and Cr#pbank blind test sets. The G2P phonetizer has been used for normalization.M&N1"
D19-5553,Q17-1010,0,0.0154724,"a language model to select the best correction. Several works have explored different approaches to normalize noisy UGC in various languages. For instance, Stymne (2011) use Approximate String Matching, an algorithm based on a weighted Levenshtein edit distance to generate lattices containing alternative spelling of OOVs. Wang and Ng (2013) employ a Conditional Random Field and a beam-search decoding approach to address missing punctuation and words in Chinese and English social media text. More recently, Watson et al. (2018) proposed a neural sequence-tosequence embedding enhancing FastText (Bojanowski et al., 2017) representations with wordlevel information, which achieved state-of-the-art on the QALB Arabic normalization task (Mohit et al., 2014). 3 ple model based on finding, for each token of the sentence, words with similar pronunciations and selecting the best spelling alternative, using a language model. More precisely, we propose a fourstep process: 1. for each word of the input sentence, we automatically generate its pronunciation. We consider all words in the input sentence as misspelled tokens are not necessarily OOVs (e.g. “j’ai manger” — literally “I have eat” — which must be corrected to “j"
D19-5553,D18-1542,0,0.144163,"Missing"
D19-5553,W18-1817,0,0.0154222,"ents To evaluate whether our approach improve the translation quality of UGC, we have processed all of our test sets, both UGC and canonical ones with our phonetic normalization pipeline (Section 3). The corrected input sentences are then translated by a phrase-based and NMT systems.7 We evaluate translation quality using S ACRE BLEU (Post, 2018). The MT baselines models were trained using the parallel corpora described in Section 4.3. We use 3 training data configurations in our experiments: WMT, Small OpenTestand Large 410 7 In our experiments we used Moses (Koehn et al., 2007) and OpenNMT (Klein et al., 2018). Crap WMT Small Large 20.5 28.9 30.0 PBSMT MTNT News Open Crap 22.5† 20.4 22.3 13.3 26.1† 27.4† 15.4 27.5 26.9 21.2 27.3 28.6 Transformer MTNT News 21.2 28.3 28.3 27.4† 26.7 26.6 Open 16.3 31.4† 31.5† Table 2: BLEU score results for our two benchmark models for the different train-test combinations. None of the test sets are normalized. The best result for each test set is marked in bold, in-domain scores with a dag. Crap, News and Open respectively stand for the Cr#pbank, NeswTest and OpenSubTest. Crap PBSMT MTNT News Open Crap Transformer MTNT News Open WMT 20.4 20.2 21.9† 13.4 15.0 20.4 26"
D19-5553,L18-1275,0,0.0174447,"ated into English by professional translators. We used their designated test set and added a blind test set of 599 sentences we sampled from the MTNT validation set. The Cr#pbank and MTNT corpora both differ in the domain they consider, their collection date, and in the way sentences were filtered to ensure they are sufficiently different from canonical data. 4.3 Canonical Parallel Corpora To train our MT systems, we use the ‘standard’ parallel data, namely the Europarl and NewsCommentaries corpora that are used in the WMT evaluation campaign (Bojar et al., 2016) and the OpenSubtitles corpus (Lison et al., 2018). We will discuss the different training data configurations for the MT experiments more in detail in Section 5. We also use the totality of the French part of these corpora to train a 5-gram language model with Knesser-Ney smoothing (Ney et al., 1994) that is used to score possible rewritings of the input sentence and find the best normalization, as we have discussed in Section 3. 5 Machine Translation Experiments To evaluate whether our approach improve the translation quality of UGC, we have processed all of our test sets, both UGC and canonical ones with our phonetic normalization pipeline"
D19-5553,W18-6319,0,0.0149493,"part of these corpora to train a 5-gram language model with Knesser-Ney smoothing (Ney et al., 1994) that is used to score possible rewritings of the input sentence and find the best normalization, as we have discussed in Section 3. 5 Machine Translation Experiments To evaluate whether our approach improve the translation quality of UGC, we have processed all of our test sets, both UGC and canonical ones with our phonetic normalization pipeline (Section 3). The corrected input sentences are then translated by a phrase-based and NMT systems.7 We evaluate translation quality using S ACRE BLEU (Post, 2018). The MT baselines models were trained using the parallel corpora described in Section 4.3. We use 3 training data configurations in our experiments: WMT, Small OpenTestand Large 410 7 In our experiments we used Moses (Koehn et al., 2007) and OpenNMT (Klein et al., 2018). Crap WMT Small Large 20.5 28.9 30.0 PBSMT MTNT News Open Crap 22.5† 20.4 22.3 13.3 26.1† 27.4† 15.4 27.5 26.9 21.2 27.3 28.6 Transformer MTNT News 21.2 28.3 28.3 27.4† 26.7 26.6 Open 16.3 31.4† 31.5† Table 2: BLEU score results for our two benchmark models for the different train-test combinations. None of the test sets are n"
D19-5553,P12-3011,0,0.0124361,"ble) normalization of the input sentence. 4. using a language model, we compute the probability to observe each alternative spelling of the sentence (note that, by construction, the input sentence is also contained in the lattice) and find the most probable path (and therefore potential normalization) of the input sentence. Note that finding the most probable path in a lattice can be done with a complexity proportional to the size of the sentence even if the lattice encodes a number of paths that grows exponentially with the sentence size (Mohri, 2002). In our experiments we used the OpenGRM (Roark et al., 2012) and OpenFST (Allauzen et al., 2007) frameworks that provide a very efficient implementation to score a lattice with a language model. Phonetic Correction Model To automatically process phonetic writing and map UGC to their correct spelling, we propose a simThis process can be seen as a naive spellchecker, in which we only consider a reduced set of variations, 408 tailored to the specificities of UGC texts. We will now detail the first two steps discussed above. Generating the pronunciation of the input words To predict the pronunciation of an input word, i.e. its representation in the Interna"
D19-5553,W16-3905,1,0.830195,"Missing"
D19-5553,W19-6101,1,0.876109,"Missing"
D19-5553,D18-1050,0,0.106173,"cted in the gold translations but some of the specificities of UGC were kept. For instance, idiomatic expressions were mapped directly to the corresponding ones in English (e.g. “mdr” (mort de rire, litt. dying of laughter) has been translated to “lol” and letter repetitions were also kept (e.g. “ouiii” has been translated to “yesss”). For our experiments, we have divided the Cr#pbank into two sets (test and blind) containing 777 comments each. This corpus can be freely downloaded at https: //gitlab.inria.fr/seddah/parsiti. The MTNT corpus We also consider in our experiments, the MTNT corpus (Michel and Neubig, 2018), a multilingual dataset that contains French sentences collected on Reddit and translated into English by professional translators. We used their designated test set and added a blind test set of 599 sentences we sampled from the MTNT validation set. The Cr#pbank and MTNT corpora both differ in the domain they consider, their collection date, and in the way sentences were filtered to ensure they are sufficiently different from canonical data. 4.3 Canonical Parallel Corpora To train our MT systems, we use the ‘standard’ parallel data, namely the Europarl and NewsCommentaries corpora that are u"
D19-5553,W14-3605,0,0.0306349,". For instance, Stymne (2011) use Approximate String Matching, an algorithm based on a weighted Levenshtein edit distance to generate lattices containing alternative spelling of OOVs. Wang and Ng (2013) employ a Conditional Random Field and a beam-search decoding approach to address missing punctuation and words in Chinese and English social media text. More recently, Watson et al. (2018) proposed a neural sequence-tosequence embedding enhancing FastText (Bojanowski et al., 2017) representations with wordlevel information, which achieved state-of-the-art on the QALB Arabic normalization task (Mohit et al., 2014). 3 ple model based on finding, for each token of the sentence, words with similar pronunciations and selecting the best spelling alternative, using a language model. More precisely, we propose a fourstep process: 1. for each word of the input sentence, we automatically generate its pronunciation. We consider all words in the input sentence as misspelled tokens are not necessarily OOVs (e.g. “j’ai manger” — literally “I have eat” — which must be corrected to “j’ai mang´e” — “I have eaten”, the French words “manger” and “mang´e” having both the same pronunciation /m˜A.ge/); 2. using these phone"
D19-5553,C16-1328,0,0.0633057,"Missing"
D19-5553,C12-1149,1,0.803661,"set OpenSubTest NeswTest #sentences #tokens ASL TTR 2.2M 9.2M 34M 64.2M 57.7M 1.19B 29.7 6.73 6.86 0.20 0.18 0.25 11,000 3,003 66,148 68,155 6.01 22.70 0.23 0.23 Corpus #sentences #tokens ASL TTR 777 1,022 13,680 20,169 17.60 19.70 0.32 0.34 777 599 12,808 8,176 16.48 13.62 0.37 0.38 UGC test set Cr#pbank MTNT UGC blind test set Cr#pbank MTNT Table 1: Statistics on the French side of the corpora used in our experiments. TTR stands for Type-to-Token Ratio, ASL for average sentence length. 2019), consists of 1,554 comments in French, translated from an extension of the French Social Media Bank (Seddah et al., 2012) annotated with the following linguistic information: Part-of-Speech tags, surface syntactic representations, as well as a normalized form whenever necessary. Comments have been translated from French to English by a native French speaker with near-native English speaker capabilities. Typographic and grammar error were corrected in the gold translations but some of the specificities of UGC were kept. For instance, idiomatic expressions were mapped directly to the corresponding ones in English (e.g. “mdr” (mort de rire, litt. dying of laughter) has been translated to “lol” and letter repetition"
D19-5553,P16-1162,0,0.0335358,"TNT News Open Crap Transformer MTNT News Open WMT 20.4 20.4 21.7† 13.4 14.6 20.7 26.5† 16.1 Small 28.0 26.3 19.8 26.2† 28.5 28.8 25.6 31.4† Large 28.3 27.7 21.6 27.4† 27.5 28.6 25.8 31.5† (b) (Espeak) phonetizer. Table 3: BLEU score results for our three benchmark models on normalized test sets. The best result for each test set is marked in bold, in-domain scores with a dag. OpenTest, for which Table 1 reports some statistics. We will denote Small and Large the two OpenSubtitles training sets used in the MT experiments. For every model, we tokenize the training data using byte-pair encoding (Sennrich et al., 2016) with a 16K vocabulary size. BLEU scores for our normalized test sets are reported in Table 3a and Table 3b, for the G2P and Espeak phonetizers. Results of the unprocessed test sets are reported in Table 2. We present some UGC examples of positive and negative results along with their normalization and translation in Table 6. 6 when the normalized text is translated using the PBSMT model. Moreover, our trained G2P phonetizer achieved the best improvement over the Cr#pbank corpus, attaining +1.5 BLEU points compared to the baseline. On the other hand, the Espeak phonetizer produces the highest"
D19-5553,D17-1145,0,0.0243793,"al. (2017) proposed lat2seq, an extension of seq2seq models (Sutskever et al., 2014) able to encode several possible input possibilities by conditioning their GRU output to several predecessors’ paths. The main issue with this model is that it is unable to predict the score of choosing a certain path by using future scores, i.e, by considering words that come after the current 407 Proceedings of the 2019 EMNLP Workshop W-NUT: The 5th Workshop on Noisy User-generated Text, pages 407–416 c Hong Kong, Nov 4, 2019. 2019 Association for Computational Linguistics token to be potentially normalized. Sperber et al. (2017) introduced a model based on Tree-LSTMs (Tai et al., 2015), to correct outputs of an Automatic Speech Recognition (ASR) system. On the other hand, Le et al. (2008) use lattices composed of written subword units to improve recognition rate on ASR. However, none of the aforementioned works have focused on processing noisy UGC corpora and they do not consider our main hypotheses of using phonetizers to recover correct tokens. They aim to correct known tokens such that a neural language model chooses the best output when an uncertain input is present (typically words with similar pronunciation fro"
D19-5553,W11-2159,0,0.0235102,"detection of OOVs. More recently, van der Goot and van Noord (2018) achieved state-of-the-art performance on dependency parsing of UGC using lattices. Closely related to our work, Baranes (2015) explored several normalization techniques on French UGC. In particular, to recover from typographical errors, they considered a rule-based system, SxPipe (Sagot and Boullier, 2008), that produced lattices encoding OOVs alternative spelling and used a language model to select the best correction. Several works have explored different approaches to normalize noisy UGC in various languages. For instance, Stymne (2011) use Approximate String Matching, an algorithm based on a weighted Levenshtein edit distance to generate lattices containing alternative spelling of OOVs. Wang and Ng (2013) employ a Conditional Random Field and a beam-search decoding approach to address missing punctuation and words in Chinese and English social media text. More recently, Watson et al. (2018) proposed a neural sequence-tosequence embedding enhancing FastText (Bojanowski et al., 2017) representations with wordlevel information, which achieved state-of-the-art on the QALB Arabic normalization task (Mohit et al., 2014). 3 ple mo"
D19-5553,P15-1150,0,0.0145024,"Missing"
D19-5553,N13-1050,0,0.0225529,"o our work, Baranes (2015) explored several normalization techniques on French UGC. In particular, to recover from typographical errors, they considered a rule-based system, SxPipe (Sagot and Boullier, 2008), that produced lattices encoding OOVs alternative spelling and used a language model to select the best correction. Several works have explored different approaches to normalize noisy UGC in various languages. For instance, Stymne (2011) use Approximate String Matching, an algorithm based on a weighted Levenshtein edit distance to generate lattices containing alternative spelling of OOVs. Wang and Ng (2013) employ a Conditional Random Field and a beam-search decoding approach to address missing punctuation and words in Chinese and English social media text. More recently, Watson et al. (2018) proposed a neural sequence-tosequence embedding enhancing FastText (Bojanowski et al., 2017) representations with wordlevel information, which achieved state-of-the-art on the QALB Arabic normalization task (Mohit et al., 2014). 3 ple model based on finding, for each token of the sentence, words with similar pronunciations and selecting the best spelling alternative, using a language model. More precisely,"
D19-5553,D18-1097,0,0.0262461,"and Boullier, 2008), that produced lattices encoding OOVs alternative spelling and used a language model to select the best correction. Several works have explored different approaches to normalize noisy UGC in various languages. For instance, Stymne (2011) use Approximate String Matching, an algorithm based on a weighted Levenshtein edit distance to generate lattices containing alternative spelling of OOVs. Wang and Ng (2013) employ a Conditional Random Field and a beam-search decoding approach to address missing punctuation and words in Chinese and English social media text. More recently, Watson et al. (2018) proposed a neural sequence-tosequence embedding enhancing FastText (Bojanowski et al., 2017) representations with wordlevel information, which achieved state-of-the-art on the QALB Arabic normalization task (Mohit et al., 2014). 3 ple model based on finding, for each token of the sentence, words with similar pronunciations and selecting the best spelling alternative, using a language model. More precisely, we propose a fourstep process: 1. for each word of the input sentence, we automatically generate its pronunciation. We consider all words in the input sentence as misspelled tokens are not"
E12-1013,W09-0437,0,0.0134914,"ms has the form of a very large directed acyclic graph. In several softwares, an approximation of this search space can be outputted, either as a n-best list containing the n top hypotheses found by the decoder, or as a phrase or word graph (lattice) which compactly encodes those hypotheses that have survived search space pruning. Lattices usually contain much more hypotheses than n-best lists and better approximate the search space. Exploring the PBSMT search space is one of the few means to perform diagnostic analysis and to better understand the behavior of the system (Turchi et al., 2008; Auli et al., 2009). Useful diagnostics are, for instance, provided by looking at the best (oracle) hypotheses contained in the search space, i.e, those hypotheses that have the highest quality score with respect to one or several references. Such oracle hypotheses can be used for failure analysis and to better understand the bottlenecks of existing translation systems (Wisniewski et al., 2010). Indeed, the inability to faithfully reproduce reference translations can have many causes, such as scantiness of the translation table, insufficient expressiveness of reordering models, inadequate scoring function, non-l"
E12-1013,W05-0909,0,0.106821,"BLEU, or rather sentence-level approximations thereof, the problem is in fact known to be NP-hard (Leusch et al., 2008). This complexity stems from the fact that the contribution of a given edge to the total modified n-gram precision can not be computed without looking at all other edges on the path. Similar (or worse) complexity result are expected 120 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 120–129, c Avignon, France, April 23 - 27 2012. 2012 Association for Computational Linguistics for other metrics such as METEOR (Banerjee and Lavie, 2005) or TER (Snover et al., 2006). The exact computation of oracles under corpus level metrics, such as BLEU, poses supplementary combinatorial problems that will not be addressed in this work. In this paper, we present two original methods for finding approximate oracle hypotheses on lattices. The first one is based on a linear approximation of the corpus BLEU, that was originally designed for efficient Minimum Bayesian Risk decoding on lattices (Tromble et al., 2008). The second one, based on Integer Linear Programming, is an extension to lattices of a recent work on failure analysis for phrase-"
E12-1013,P10-2006,0,0.0846147,"Missing"
E12-1013,D11-1003,0,0.112918,"is the subset of edges generating w, and ξ∈Ω(w) ξ is the number of occurrences of w in the solution and cw (r) is the number of occurrences of w in the reference r. Using the γ variables, we define a “clipped” approximation of 1- BLEU :   #{Ξ} X X X Θ1 · γw − Θ2 ·  ξi − γw  w i=1 #{Ξ} (Θ1 + Θ2 ) · ξ∈P,γw X γw − Θ2 · w X ξi i=1 (7) s.t. γw ≥ 0, γw ≤ cw (r), γw ≤ X 5.3 ξ∈Ξ− (qF ) X ξ∈Ξ+ (q) X ξ = 1, ξ− Oracle Decoding through Lagrangian Relaxation (RLX) In this section, we introduce another method to solve problem (7) without relying on an external ILP solver. Following (Rush et al., 2010; Chang and Collins, 2011), we propose an original method for oracle decoding based on Lagrangian relaxation. This method relies on the idea of relaxing the clipping constraints: starting from an unconstrained problem, the counts clipping is enforced by incrementally strengthening the weight of paths satisfying the constraints. The oracle decoding problem with clipping constraints amounts to solving: #{Ξ} ξ − arg min ξ∈Ω(w) X Shortest Path Oracle (SP) As a trivial special class of the above formulation, we also define a Shortest Path Oracle (SP) that solves the optimization problem in (6). As no clipping constraints ap"
E12-1013,D08-1024,0,0.239204,"e hypotheses that have the highest quality score with respect to one or several references. Such oracle hypotheses can be used for failure analysis and to better understand the bottlenecks of existing translation systems (Wisniewski et al., 2010). Indeed, the inability to faithfully reproduce reference translations can have many causes, such as scantiness of the translation table, insufficient expressiveness of reordering models, inadequate scoring function, non-literal references, over-pruned lattices, etc. Oracle decoding has several other applications: for instance, in (Liang et al., 2006; Chiang et al., 2008) it is used as a work-around to the problem of non-reachability of the reference in discriminative training of MT systems. Lattice reranking (Li and Khudanpur, 2009), a promising way to improve MT systems, also relies on oracle decoding to build the training data for a reranking algorithm. For sentence level metrics, finding oracle hypotheses in n-best lists is a simple issue; however, solving this problem on lattices proves much more challenging, due to the number of embedded hypotheses, which prevents the use of bruteforce approaches. When using BLEU, or rather sentence-level approximations"
E12-1013,W07-0414,0,0.0612882,"am counts (7) uni/bi-gram counts (8) search exact appr. appr. exact exact appr. exact clipping no no no no no yes yes brevity no no yes yes yes yes yes Table 1: Recapitulative overview of oracle decoders. 3 Existing Algorithms In this section, we describe our reimplementation of two approximate search algorithms that have been proposed in the literature to solve the oracle decoding problem for BLEU. In addition to their approximate nature, none of them accounts for the fact that the count of each matching word has to be clipped. 3.2 Partial BLEU Oracle (PB) Another approach is put forward in (Dreyer et al., 2007) and used in (Li and Khudanpur, 2009): oracle translations are shortest paths in a lattice L, where the weight of each path π is the sentence level log BLEU(π) score of the corresponding complete or partial hypothesis: log BLEU(π) = 3.1 Language Model Oracle (LM) The simplest approach we consider is introduced in (Li and Khudanpur, 2009), where oracle decoding is reduced to the problem of finding the most likely hypothesis under a n-gram language model trained with the sole reference translation. Let us suppose we have a n-gram language model that gives a probability P (en |e1 . . . en−1 ) of"
E12-1013,D08-1088,0,0.341595,"achability of the reference in discriminative training of MT systems. Lattice reranking (Li and Khudanpur, 2009), a promising way to improve MT systems, also relies on oracle decoding to build the training data for a reranking algorithm. For sentence level metrics, finding oracle hypotheses in n-best lists is a simple issue; however, solving this problem on lattices proves much more challenging, due to the number of embedded hypotheses, which prevents the use of bruteforce approaches. When using BLEU, or rather sentence-level approximations thereof, the problem is in fact known to be NP-hard (Leusch et al., 2008). This complexity stems from the fact that the contribution of a given edge to the total modified n-gram precision can not be computed without looking at all other edges on the path. Similar (or worse) complexity result are expected 120 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 120–129, c Avignon, France, April 23 - 27 2012. 2012 Association for Computational Linguistics for other metrics such as METEOR (Banerjee and Lavie, 2005) or TER (Snover et al., 2006). The exact computation of oracles under corpus level metrics, su"
E12-1013,N09-2003,0,0.102506,"understand the bottlenecks of existing translation systems (Wisniewski et al., 2010). Indeed, the inability to faithfully reproduce reference translations can have many causes, such as scantiness of the translation table, insufficient expressiveness of reordering models, inadequate scoring function, non-literal references, over-pruned lattices, etc. Oracle decoding has several other applications: for instance, in (Liang et al., 2006; Chiang et al., 2008) it is used as a work-around to the problem of non-reachability of the reference in discriminative training of MT systems. Lattice reranking (Li and Khudanpur, 2009), a promising way to improve MT systems, also relies on oracle decoding to build the training data for a reranking algorithm. For sentence level metrics, finding oracle hypotheses in n-best lists is a simple issue; however, solving this problem on lattices proves much more challenging, due to the number of embedded hypotheses, which prevents the use of bruteforce approaches. When using BLEU, or rather sentence-level approximations thereof, the problem is in fact known to be NP-hard (Leusch et al., 2008). This complexity stems from the fact that the contribution of a given edge to the total mod"
E12-1013,P06-1096,0,0.240983,"Missing"
E12-1013,P02-1040,0,0.0835406,"ctors with the parameters λ tuning. In oracle decoding, the decoder’s job is quite different, as we assume that at least a reference rf is provided to evaluate the quality of each individual hypothesis. The decoder therefore aims at finding the path π ∗ that generates the hypothesis that best matches rf . For this task, only the output labels ei will matter, the other informations can be left aside.4 Oracle decoding assumes the definition of a measure of the similarity between a reference and a hypothesis. In this paper we will consider sentence-level approximations of the popular BLEU score (Papineni et al., 2002). BLEU is formally defined for two parallel corpora, E = {ej }Jj=1 and R = {rj }Jj=1 , each containing J sentences as: Y 1/n n n-BLEU(E, R) = BP · pm , (1) m=1 where BP = min(1, e1−c1 (R)/c1 (E) ) is the brevity penalty and pm = cm (E, R)/cm (E) are clipped or modified m-gram precisions: cm (E) is the total number of word m-grams in E; cm (E, R) accumulates over sentences the number of mgrams in ej that also belong to rj . These counts are clipped, meaning that a m-gram that appears k times in E and l times in R, with k &gt; l, is only counted l times. As it is well known, BLEU performs a compr"
E12-1013,D10-1001,0,0.0350101,"e. 125 where PΩ (w) is the subset of edges generating w, and ξ∈Ω(w) ξ is the number of occurrences of w in the solution and cw (r) is the number of occurrences of w in the reference r. Using the γ variables, we define a “clipped” approximation of 1- BLEU :   #{Ξ} X X X Θ1 · γw − Θ2 ·  ξi − γw  w i=1 #{Ξ} (Θ1 + Θ2 ) · ξ∈P,γw X γw − Θ2 · w X ξi i=1 (7) s.t. γw ≥ 0, γw ≤ cw (r), γw ≤ X 5.3 ξ∈Ξ− (qF ) X ξ∈Ξ+ (q) X ξ = 1, ξ− Oracle Decoding through Lagrangian Relaxation (RLX) In this section, we introduce another method to solve problem (7) without relying on an external ILP solver. Following (Rush et al., 2010; Chang and Collins, 2011), we propose an original method for oracle decoding based on Lagrangian relaxation. This method relies on the idea of relaxing the clipping constraints: starting from an unconstrained problem, the counts clipping is enforced by incrementally strengthening the weight of paths satisfying the constraints. The oracle decoding problem with clipping constraints amounts to solving: #{Ξ} ξ − arg min ξ∈Ω(w) X Shortest Path Oracle (SP) As a trivial special class of the above formulation, we also define a Shortest Path Oracle (SP) that solves the optimization problem in (6). As"
E12-1013,2006.amta-papers.25,0,0.0356563,"roximations thereof, the problem is in fact known to be NP-hard (Leusch et al., 2008). This complexity stems from the fact that the contribution of a given edge to the total modified n-gram precision can not be computed without looking at all other edges on the path. Similar (or worse) complexity result are expected 120 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 120–129, c Avignon, France, April 23 - 27 2012. 2012 Association for Computational Linguistics for other metrics such as METEOR (Banerjee and Lavie, 2005) or TER (Snover et al., 2006). The exact computation of oracles under corpus level metrics, such as BLEU, poses supplementary combinatorial problems that will not be addressed in this work. In this paper, we present two original methods for finding approximate oracle hypotheses on lattices. The first one is based on a linear approximation of the corpus BLEU, that was originally designed for efficient Minimum Bayesian Risk decoding on lattices (Tromble et al., 2008). The second one, based on Integer Linear Programming, is an extension to lattices of a recent work on failure analysis for phrase-based decoders (Wisniewski et"
E12-1013,D08-1065,0,0.364928,"0–129, c Avignon, France, April 23 - 27 2012. 2012 Association for Computational Linguistics for other metrics such as METEOR (Banerjee and Lavie, 2005) or TER (Snover et al., 2006). The exact computation of oracles under corpus level metrics, such as BLEU, poses supplementary combinatorial problems that will not be addressed in this work. In this paper, we present two original methods for finding approximate oracle hypotheses on lattices. The first one is based on a linear approximation of the corpus BLEU, that was originally designed for efficient Minimum Bayesian Risk decoding on lattices (Tromble et al., 2008). The second one, based on Integer Linear Programming, is an extension to lattices of a recent work on failure analysis for phrase-based decoders (Wisniewski et al., 2010). In this framework, we study two decoding strategies: one based on a generic ILP solver, and one, based on Lagrangian relaxation. Our contribution is also experimental as we compare the quality of the BLEU approximations and the time performance of these new approaches with several existing methods, for different language pairs and using the lattice generation capacities of two publicly-available state-of-theart phrase-based"
E12-1013,W08-0305,0,0.0294122,"Missing"
E12-1013,D10-1091,1,0.927891,"theses than n-best lists and better approximate the search space. Exploring the PBSMT search space is one of the few means to perform diagnostic analysis and to better understand the behavior of the system (Turchi et al., 2008; Auli et al., 2009). Useful diagnostics are, for instance, provided by looking at the best (oracle) hypotheses contained in the search space, i.e, those hypotheses that have the highest quality score with respect to one or several references. Such oracle hypotheses can be used for failure analysis and to better understand the bottlenecks of existing translation systems (Wisniewski et al., 2010). Indeed, the inability to faithfully reproduce reference translations can have many causes, such as scantiness of the translation table, insufficient expressiveness of reordering models, inadequate scoring function, non-literal references, over-pruned lattices, etc. Oracle decoding has several other applications: for instance, in (Liang et al., 2006; Chiang et al., 2008) it is used as a work-around to the problem of non-reachability of the reference in discriminative training of MT systems. Lattice reranking (Li and Khudanpur, 2009), a promising way to improve MT systems, also relies on oracl"
E17-2051,P16-1017,0,0.0420794,"Missing"
E17-2051,P04-1015,0,0.17142,"Missing"
E17-2051,P16-1231,0,0.0693266,"Missing"
E17-2051,D16-1001,0,0.0225945,"Missing"
E17-2051,C12-1059,0,0.0974604,"Missing"
E17-2051,Q13-1033,0,0.038875,"Missing"
E17-2051,N12-1015,0,0.0446608,"Missing"
E17-2051,Q16-1023,0,0.0272666,"Missing"
E17-2051,N16-1121,1,0.873308,"Missing"
E17-2051,J08-4003,0,0.060757,"Missing"
E17-2051,D08-1059,0,0.0935067,"Missing"
E17-2051,W02-1001,0,\N,Missing
E17-2051,J13-1002,0,\N,Missing
E17-2051,P11-2033,0,\N,Missing
F13-2028,abekawa-etal-2010-community,0,0.0475848,"Missing"
F13-2028,W12-3102,0,0.076723,"Missing"
F13-2028,2012.eamt-1.60,0,0.0681271,"Missing"
F13-2028,2004.tmi-1.8,0,0.0596356,"Missing"
F13-2028,S10-1003,0,0.100773,"Missing"
F13-2028,potet-etal-2012-collection,0,0.042695,"Missing"
F13-2028,2006.amta-papers.25,0,0.1378,"Missing"
F13-2028,W12-3120,1,0.885309,"Missing"
F14-1016,C04-1080,0,0.0983607,"Missing"
F14-1016,H92-1026,0,0.486931,"Missing"
F14-1016,J92-4003,0,0.582882,"Missing"
F14-1016,W06-2920,0,0.058481,"Missing"
F14-1016,D10-1056,0,0.0296351,"Missing"
F14-1016,J03-4003,0,0.197779,"Missing"
F14-1016,P11-1061,0,0.0364566,"Missing"
F14-1016,N13-1014,0,0.0240925,"Missing"
F14-1016,D07-1033,0,0.0471334,"Missing"
F14-1016,P07-2045,0,0.00361318,"Missing"
F14-1016,P08-1068,0,0.105394,"Missing"
F14-1016,D12-1127,0,0.0348292,"Missing"
F14-1016,J94-2001,0,0.460186,"Missing"
F14-1016,N13-1039,0,0.0244234,"Missing"
F14-1016,petrov-etal-2012-universal,0,0.0265866,"Missing"
F14-1016,W09-1119,0,0.040984,"Missing"
F14-1016,J03-3002,0,0.0974294,"Missing"
F14-1016,N12-1052,0,0.0210751,"Missing"
F14-1016,W11-0328,0,0.265713,"Missing"
F14-1016,Q13-1001,0,0.0261175,"Missing"
F14-1016,Q14-1005,0,0.0213256,"Missing"
F14-1016,H01-1035,0,0.174923,"Missing"
K17-3017,C16-1012,1,0.833828,", R2 and R3 ; each token belongs to exactly one region. On the input sentence, the white areas represent tokens whose head is unknown, while the black areas represent tokens whose head has already been predicted. UDPipe We apply the official UDPipe 1.1 baseline models (Straka, 2017). For the surprise languages, we train our own model.4 2.1.3 Cross-lingual For each treebank under 1,000 training sentences, we apply cross-treebank techniques to build additional parsers. First, for each target treebank, we transform every source treebank by delexicalizing it and applying the WALS rewrite rules of Aufrant et al. (2016). We then compute, for each such treebank, its similarity to the target treebank, using the KLcpos3 divergence metric (Rosa and Zabokrtsky, 2015). We select the source among treebanks over 2,000 sentences, by retaining the languages requiring the smallest number of rewrite rules (i.e. the smallest number of divergent WALS features), and then choosing the (transformed) treebank minimizing the KLcpos3 divergence. When the selected source is of the same language, we use domain adaptation techniques, otherwise we turn to cross-lingual methods. However, domain adaptation was not used in the final s"
K17-3017,E17-2051,1,0.808497,"e corresponding treebanks (FrenchParTUT, Galician-TreeGal and Czech-CLTT), and is not detailed here. We consider five cross-lingual parsers: PanParser This is an in-house implementation (Aufrant and Wisniewski, 2016) of a transition-based parser, using the ArcEager system and an averaged perceptron. Hyperparameters to tune are the number of epochs, the use of the universal morphological features, the use of word embeddings concatenated to the feature vectors, and the size of the beam (either 8 or 1, i.e. greedy). In any case the parser trains with dynamic oracles, with the restart strategy of Aufrant et al. (2017). Relation labels are predicted in a second step, which enables to use features of the whole parse tree for this prediction. Delex This is the same as the PanParser models, except that all lexicalized features are removed, including word embeddings. UDPipe+PanParser As relation labels are sometimes better predicted by PanParser than UDPipe, we also consider combining their outputs at prediction time: we first annotate the input with UDPipe, discard the predicted labels and replace them with labels predicted by PanParser on UDPipe trees. Project-en Based on parallel data with English, we use th"
K17-3017,N13-1073,0,0.010447,"ges, retaining the 10 first sentences for the trainset and the 4 last sentences for the devset. We always use gold tokenization and segmentation during training, but to improve robustness to noisy tags, all models are trained on treebanks with predicted tags, provided by the task organizers.2 We use the word embeddings provided by the organizers, computed on monolingual data preprocessed by UDPipe.3 Parallel data from the OPUS platform (Tiedemann, 2012) is preprocessed as follows: for each pair, all corpora are concatenated, tokenized and annotated by UDPipe, and word aligned with fast align (Dyer et al., 2013). ity with a selective combination of several base parsers. For instance, a cross-lingual delexicalized parser intuitively provides insights on the main syntactic structures, presumably shared because of linguistic similarities (typically assessed using linguistic knowledge), while a monolingual parser can learn target-specific structures in target data. On one hand, if monolingual data is too small, it does not contain enough information on the main syntactic structures, and the cross-lingual parser will be more accurate; it should be preferred for this kind of dependencies. On the other hand"
K17-3017,P15-2040,0,0.11629,"the black areas represent tokens whose head has already been predicted. UDPipe We apply the official UDPipe 1.1 baseline models (Straka, 2017). For the surprise languages, we train our own model.4 2.1.3 Cross-lingual For each treebank under 1,000 training sentences, we apply cross-treebank techniques to build additional parsers. First, for each target treebank, we transform every source treebank by delexicalizing it and applying the WALS rewrite rules of Aufrant et al. (2016). We then compute, for each such treebank, its similarity to the target treebank, using the KLcpos3 divergence metric (Rosa and Zabokrtsky, 2015). We select the source among treebanks over 2,000 sentences, by retaining the languages requiring the smallest number of rewrite rules (i.e. the smallest number of divergent WALS features), and then choosing the (transformed) treebank minimizing the KLcpos3 divergence. When the selected source is of the same language, we use domain adaptation techniques, otherwise we turn to cross-lingual methods. However, domain adaptation was not used in the final submission as it did not bring significant improvements on the corresponding treebanks (FrenchParTUT, Galician-TreeGal and Czech-CLTT), and is not"
K17-3017,N16-1121,1,0.878232,"Missing"
K17-3017,L16-1680,0,0.0735301,"Missing"
K17-3017,P11-2093,0,0.0117225,"rawled monolingual data (after UDPipe tokenization, to segment punctuation); in case of OOV the pair remains unchanged. We set the PMI lowerand upperbounds to log 5 and log 400. Model selection For each treebank, we compare all base and cascade parsers, and retain the parser yielding the best LAS on the provided development set (using gold tokenization). However, in some languages this dataset was particularly small and consequently biased, which often led to selecting the wrong model, as will be seen in §4. Chinese and Japanese We rely on UDPipe for sentence segmentation, and then use KyTea (Neubig et al., 2011) to tokenize each sentence. KyTea models are trained on UD Chinese and Japanese training treebanks. 6 Depending on the data sizes, the cascades train in a few hours to two days on CPU, using 5 threads. For all three languages, the newly tokenized input is then morphologically annotated by UDPipe. 167 3 Overall results UDPipe [off.] As part of the CoNLL 2017 UD Shared Task, we evaluated our system on the TIRA platform (Potthast et al., 2014). Evaluation runs on the virtual machine took 10.5 hours on a single thread, using up to 6GB RAM. Table 1 presents our overall results as published by the o"
K17-3017,tiedemann-2012-parallel,0,0.058503,"the UD data (Nivre et al., 2017a), following the splits provided with the official baseline (Straka, 2017).1 We perform a similar split for the surprise languages, retaining the 10 first sentences for the trainset and the 4 last sentences for the devset. We always use gold tokenization and segmentation during training, but to improve robustness to noisy tags, all models are trained on treebanks with predicted tags, provided by the task organizers.2 We use the word embeddings provided by the organizers, computed on monolingual data preprocessed by UDPipe.3 Parallel data from the OPUS platform (Tiedemann, 2012) is preprocessed as follows: for each pair, all corpora are concatenated, tokenized and annotated by UDPipe, and word aligned with fast align (Dyer et al., 2013). ity with a selective combination of several base parsers. For instance, a cross-lingual delexicalized parser intuitively provides insights on the main syntactic structures, presumably shared because of linguistic similarities (typically assessed using linguistic knowledge), while a monolingual parser can learn target-specific structures in target data. On one hand, if monolingual data is too small, it does not contain enough informat"
K17-3017,L16-1262,0,0.0959654,"Missing"
L16-1241,chrupala-etal-2008-learning,0,0.0892488,"Missing"
L16-1241,P13-2111,0,0.0133489,"E(ys , yt , (s(i) , s(j) ), (t(i) , t(j) ))   +1 if (s(i) , s(j) ) ∈ ys and (t(i) , t(j) ) ∈ yt     + 1 if (s(i) , s(j) ) ∈ ys and t(i) == t(j)  4   + 1 if s == s and (t , t ) ∈ y t (i) (j) (i) (j) 4 =  −1 if (s(i) , s(j) ) ∈ ys and (t(i) , t(j) ) ∈ / yt      −1 if (s(i) , s(j) ) ∈ / ys and (t(i) , t(j) ) ∈ yt    0 otherwise Experimental Evaluation In all experiments, we train transition-based dependency parsers with the arceager transition system, an averaged perceptron, beam search of size 8 and early update, using our own implementation based on the recommendations of (Goldberg et al., 2013). We use universal PoS and the feature templates of (Zhang and Nivre, 2011), without labels and with decision history of size 8. These features, designed for English, have not been tailored to the specificities of Romanian. We remove dependency annotations from RSAC-train to use it both as tagger trainset and parser relexicalization data. Source models are trained on UDT and Europarl is PoS annotated with supervised taggers and truncated to 80,000 sentences to limit the bias towards projection. The supervised parser is trained on annotated RSAC-train, thus enabling the comparison with the mode"
L16-1241,2005.mtsummit-papers.11,0,0.0555234,"sess the interest of cross-lingual methods (§5.) for under-resourced languages. All tools and resources used in this work are available at https://perso.limsi.fr/aufrant/. 1520 2. Resources for Romanian Over the years, several corpora have been collected for Romanian. We are particularly interested in parallel corpora that will allow us to transfer annotations and corpora annotated with PoS and dependencies that can be used to evaluate cross-lingual taggers and parsers. In this section, we will quickly describe existing corpora. Most work on cross-lingual transfer rely on the Europarl corpus (Koehn, 2005) as a source for parallel sentences. It notably includes Romanian sentences, with their translation in 20 European languages such as English or Spanish, that we will use in our experiments. (Perez, 2012) released the treebank Romanian Syntactic Annotated Corpus (RSAC) of 67,686 tokens (punctuations excluded) and 3,587 sentences from various sources (JRC-Acquis, Wikipedia, 1984, textbook exercises and translations from FrameNet). Two other corpora exist: MULTEXT-East, a multilingual corpus extracted from the novel 1984, sentence-aligned and with morphosyntactic annotations; a corpus of 36,150 t"
L16-1241,N16-1121,1,0.897387,"Missing"
L16-1241,D11-1006,0,0.447562,"hat is why, we apply state-of-the-art methods for cross-lingual transfer on Romanian tagging and parsing, from English and several Romance languages. We compare the performance with monolingual systems trained with sets of different sizes and establish that training on a few sentences in target language yields better results than transferring from large datasets in other languages. Keywords: Cross-Lingual transfer, Part-of-Speech tagging, Dependency parsing, Romanian 1. Introduction • we implement two state-of-the-art transfer methods for PoS-tagging (T¨ackstr¨om et al., 2013) and dependency (McDonald et al., 2011) parsing for Romanian; While most Romance languages are well studied in the Natural Language Processing field and have large sets of annotated data, Romanian still stays behind. As a result, most of the tools generally used in the preprocessing steps of NLP pipelines such as lemmatizer, PoS-tagger or dependency parser are not available for Romanian and, when they exist, their performance often fall far short of the performance achieved, for instance, on French or English (Straka et al., 2015). Romanian is therefore a prime candidate for applying transfer methods (Pan and Yang, 2010). Many work"
L16-1241,P13-2017,0,0.0430893,"Missing"
L16-1241,petrov-etal-2012-universal,0,0.0937942,"Missing"
L16-1241,Q13-1001,0,0.0522485,"Missing"
L16-1241,D14-1187,1,0.898835,"Missing"
L16-1241,I08-3008,0,0.08678,"15 extra rules, that improve the scores by 3 points) and since they represent an important part of the prediction errors, alleviating them may turn weakly supervised taggers into truly competitive solutions for low-resourced languages. 4. Cross-lingual dependency parser We conduct a similar study on dependency parsing, considering the framework proposed by McDonald et al. (2011) to train parsers in Romanian from various combinations of source languages. Transfer Method We briefly present here McDonald et al. (2011)’s algorithm. The transfer process starts with a delexicalized model transfer (Zeman and Resnik, 2008; McDonald et al., 2013): assuming all languages are annotated using a common PoS tagset, a model considering only PoS features is trained on a source language and used to parse directly Romanian. Using a common representation enables combination of multiple sources with raw treebank concatenation. This crude approach has proven effective for many languages even if it is hindered by the lack of lexical information. To overcome this limit, McDonald et al. (2011) propose to relexicalize the model, in a second step: a small set of unannotated target data is first annotated by the delexicalized mo"
L16-1241,P11-2033,0,0.0296188,"nd (t(i) , t(j) ) ∈ yt     + 1 if (s(i) , s(j) ) ∈ ys and t(i) == t(j)  4   + 1 if s == s and (t , t ) ∈ y t (i) (j) (i) (j) 4 =  −1 if (s(i) , s(j) ) ∈ ys and (t(i) , t(j) ) ∈ / yt      −1 if (s(i) , s(j) ) ∈ / ys and (t(i) , t(j) ) ∈ yt    0 otherwise Experimental Evaluation In all experiments, we train transition-based dependency parsers with the arceager transition system, an averaged perceptron, beam search of size 8 and early update, using our own implementation based on the recommendations of (Goldberg et al., 2013). We use universal PoS and the feature templates of (Zhang and Nivre, 2011), without labels and with decision history of size 8. These features, designed for English, have not been tailored to the specificities of Romanian. We remove dependency annotations from RSAC-train to use it both as tagger trainset and parser relexicalization data. Source models are trained on UDT and Europarl is PoS annotated with supervised taggers and truncated to 80,000 sentences to limit the bias towards projection. The supervised parser is trained on annotated RSAC-train, thus enabling the comparison with the model relexicalized on the same data. All methods are evaluated on RSAC-test wi"
L18-1711,E03-1068,0,0.145072,"ck in cross-lingual transfer is the difference in the annotation conventions across corpora and languages. Enforcing guidelines compliance to improve annotation quality is therefore one of the main challenge faced by the UD project today: as it is, performance achieved on UD corpora, especially in a cross-lingual or cross-corpus setting may be underestimated and this drop may results mainly from divergences in annotations. This work describes E RRATOR, a set of tools implementing the annotation variation principle that has been proposed to detect errors in PoS annotations (van Halteren, 2000; Dickinson and Meurers, 2003) and syntactic annotations (Dickinson and Meurers, 2005; Boyd et al., 2008). We propose an extension of this principle to word segmentation making E R RATOR able to help annotators find and correct errors in the different layers of annotations of the UD corpora. The rest of this paper is organized as follows: we will first explain how the annotation variation principle can be used to identify potential annotation errors (§ 2.). We will then describe our implementation (§ 3.) and the results of a first annotation campaign that used E RRATOR to correct and harmonize the annotations of the differ"
L18-1711,P05-1040,0,0.422548,"notation conventions across corpora and languages. Enforcing guidelines compliance to improve annotation quality is therefore one of the main challenge faced by the UD project today: as it is, performance achieved on UD corpora, especially in a cross-lingual or cross-corpus setting may be underestimated and this drop may results mainly from divergences in annotations. This work describes E RRATOR, a set of tools implementing the annotation variation principle that has been proposed to detect errors in PoS annotations (van Halteren, 2000; Dickinson and Meurers, 2003) and syntactic annotations (Dickinson and Meurers, 2005; Boyd et al., 2008). We propose an extension of this principle to word segmentation making E R RATOR able to help annotators find and correct errors in the different layers of annotations of the UD corpora. The rest of this paper is organized as follows: we will first explain how the annotation variation principle can be used to identify potential annotation errors (§ 2.). We will then describe our implementation (§ 3.) and the results of a first annotation campaign that used E RRATOR to correct and harmonize the annotations of the different French corpora. E RRATOR is open-source and can be"
L18-1711,E14-4028,0,0.239666,"Missing"
L18-1711,W00-1907,0,0.337922,"Missing"
L18-1711,K17-3016,0,0.1709,"Missing"
L18-1711,D14-1187,1,0.916295,"Missing"
max-wisniewski-2010-mining,C08-1018,0,\N,Missing
max-wisniewski-2010-mining,D08-1021,0,\N,Missing
max-wisniewski-2010-mining,P05-1074,0,\N,Missing
max-wisniewski-2010-mining,E09-1082,0,\N,Missing
max-wisniewski-2010-mining,zesch-etal-2008-extracting,0,\N,Missing
N16-1121,D12-1133,0,0.0715131,"Missing"
N16-1121,D11-1005,0,0.459085,"Missing"
N16-1121,P04-1015,0,0.319073,"Missing"
N16-1121,C12-1059,0,0.22509,"Missing"
N16-1121,2005.mtsummit-papers.11,0,0.103953,"Missing"
N16-1121,W16-1203,1,0.662422,"Missing"
N16-1121,P14-1126,0,0.136031,"Missing"
N16-1121,D11-1006,0,0.225391,"Missing"
N16-1121,P13-2017,0,0.160124,"Missing"
N16-1121,P12-1066,0,0.478366,"Missing"
N16-1121,J03-1002,0,0.0201885,"Missing"
N16-1121,D15-1039,0,0.502587,"Missing"
N16-1121,P11-2120,0,0.329687,"Missing"
N16-1121,W09-1104,0,0.0849806,"Missing"
N16-1121,N13-1126,0,0.510409,"Missing"
N16-1121,C14-1175,0,0.257607,"Missing"
N16-1121,I08-3008,0,0.586422,"Missing"
N16-1121,P11-2033,0,0.275108,"Missing"
N16-1121,W03-3017,0,\N,Missing
N18-2064,E17-2001,0,0.0784208,"representing dependency structures (Hajiˇc et al., 2001; De Marneffe et al., 2014). The divergence between annotation guidelines can result from the theoretical linguistic principles governing the choices of head status and dependency inventories, the tree-to-dependency conversion scheme or arbitrary decisions regarding closed class words, such as interjections or discursive markers, the syntactic role of which is debatable. Several works have shown that the choice of a dependency structure can have a large impact on parsing performance (Silveira and Manning, 2015; de Lhoneux and Nivre, 2016; Kohita et al., 2017) and on the performance of downstream applications (Elming et al., 2013). A natural way to decide which syntactic representation is the best is to choose the one for which a standard parser will achieve the highest parsing performance (Schwartz et al., 2012; Husain and Agrawal, 2012; Noro et al., 2005). Implementing this general principle faces two challenges: i) defining a learning criterion that can predict which dependency structure will be the easiest to learn ii) 2 Dependency Transformations In this section, we explain how to automatically transform the reference UD treebanks (Nivre et al"
N18-2064,J08-4003,0,0.0572887,"that will be the most similar to the ones seen when predicting a new dependency tree: case det Input: W the input sentence, T the set of gold trees c ← I NITIAL(W ) while ¬T ERMINAL(c) do C ORRECT ← {t|∃T ∈ T , O RACLE(t, c, T ) = 0} tp ← arg maxt∈L EGAL(c) w · φ(c, t) to ← arg maxt∈C ORRECT(c) w · φ(c, t) if tp ∈ / C ORRECT then U PDATE (w, φ(c, to ), φ(c, tp )) tnext ← to else tnext ← tp 12 root case case Algorithm 1: Training on one sentence with multiple references (see text for notations). Training a Dependency Parser with Multiple References Dynamic Oracle In a transition-based parser (Nivre, 2008), a parse is computed by performing a sequence of transitions building the parse tree in an incremental fashion. A partially built dependency tree is represented by a configuration c; when in c, applying a transition t results in the parser moving to a new configuration denoted c◦t. At each step of the parsing process, every possible transition is scored by a classifier (e.g. a linear model), given a feature representation of c and 3 In this work we only consider greedy parsers. Extending the proposed approach to beam parsers would prevent discarding a reference because one of the its transiti"
N18-2064,W16-1202,0,0.0346928,"Missing"
N18-2064,C12-1147,0,0.551312,"-to-dependency conversion scheme or arbitrary decisions regarding closed class words, such as interjections or discursive markers, the syntactic role of which is debatable. Several works have shown that the choice of a dependency structure can have a large impact on parsing performance (Silveira and Manning, 2015; de Lhoneux and Nivre, 2016; Kohita et al., 2017) and on the performance of downstream applications (Elming et al., 2013). A natural way to decide which syntactic representation is the best is to choose the one for which a standard parser will achieve the highest parsing performance (Schwartz et al., 2012; Husain and Agrawal, 2012; Noro et al., 2005). Implementing this general principle faces two challenges: i) defining a learning criterion that can predict which dependency structure will be the easiest to learn ii) 2 Dependency Transformations In this section, we explain how to automatically transform the reference UD treebanks (Nivre et al., 2016), to build corpora in which each sentence is annotated by a set of possible trees. The UD project aims at developing crosslinguistically consistent treebank annotations for many languages by harmonizing annotation schemes between languages and conve"
N18-2064,W15-2134,0,0.0233015,"ation conventions have been proposed over the years for representing dependency structures (Hajiˇc et al., 2001; De Marneffe et al., 2014). The divergence between annotation guidelines can result from the theoretical linguistic principles governing the choices of head status and dependency inventories, the tree-to-dependency conversion scheme or arbitrary decisions regarding closed class words, such as interjections or discursive markers, the syntactic role of which is debatable. Several works have shown that the choice of a dependency structure can have a large impact on parsing performance (Silveira and Manning, 2015; de Lhoneux and Nivre, 2016; Kohita et al., 2017) and on the performance of downstream applications (Elming et al., 2013). A natural way to decide which syntactic representation is the best is to choose the one for which a standard parser will achieve the highest parsing performance (Schwartz et al., 2012; Husain and Agrawal, 2012; Noro et al., 2005). Implementing this general principle faces two challenges: i) defining a learning criterion that can predict which dependency structure will be the easiest to learn ii) 2 Dependency Transformations In this section, we explain how to automatically"
N18-2064,W17-0419,1,0.874843,"Missing"
N18-2064,P11-2033,0,0.130738,"ability to detect whether a transition will cause an erroneous dependency. It can naturally be extended to the case of multiple references: a transition is considered correct as long as it can predict at least one of the gold trees; when moving to a new configuration, trees that can no longer be generated are removed from the set of references, in order to make sure the parser will not mix the dependencies of two gold trees (l.11). Parser We use our own implementation of an arc-eager unlabeled dependency parser with a dynamic oracle and an averaged perceptron, using the features described in (Zhang and Nivre, 2011) which have been designed for English and have not been adapted to the specificities of the other languages.6 Training stops when the UAS estimated on the validation set has converged. Upon full completion of parsing, there will remain only one surviving reference that has been selected according to the model current predictions. This reference corresponds to the dependency structure that is the most similar to the hypothesis the parser would have predicted at test time and can therefore be described as the reference the parser prefers: intuitively, Algorithm 1 will thus identify the reference"
N18-2064,P11-2000,0,0.196669,"Missing"
N18-2064,I05-4002,0,0.0603797,"isions regarding closed class words, such as interjections or discursive markers, the syntactic role of which is debatable. Several works have shown that the choice of a dependency structure can have a large impact on parsing performance (Silveira and Manning, 2015; de Lhoneux and Nivre, 2016; Kohita et al., 2017) and on the performance of downstream applications (Elming et al., 2013). A natural way to decide which syntactic representation is the best is to choose the one for which a standard parser will achieve the highest parsing performance (Schwartz et al., 2012; Husain and Agrawal, 2012; Noro et al., 2005). Implementing this general principle faces two challenges: i) defining a learning criterion that can predict which dependency structure will be the easiest to learn ii) 2 Dependency Transformations In this section, we explain how to automatically transform the reference UD treebanks (Nivre et al., 2016), to build corpora in which each sentence is annotated by a set of possible trees. The UD project aims at developing crosslinguistically consistent treebank annotations for many languages by harmonizing annotation schemes between languages and converting existing treebanks to this new scheme. S"
N18-2064,W15-2127,0,0.0135392,"LT 2018, pages 401–406 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics 2013) have investigated whether the choices made to increase the sharing of structures between languages hurt parsing performance and have identified a variety of choice points in which more than one design could be advocated. Most of these points are related to the issue of headness: contrary to most works in theoretical linguistic, UD assumes that function words should be categorically subordinated to content words to maximize the similarity of dependency trees across languages (Osborne and Maxwell, 2015). The alternative representations we consider are summarized in Table 1. They mostly consist in demoting the lexical head and making it dependent on a functional head. We designed a set of handcrafted rules2 to convert dependencies between these two schemes. Each application of a rule creates a new tree in the set of references that is being built. As shown in Figure 1, the resulting set of references encodes all possible combinations of the considered transformations. root 1 2 3 4 5 6 7 8 9 10 11 ... pour la peine ... root det ... pour la peine ... root det ... pour la peine ... case det ..."
N18-2064,P13-1051,0,0.0730734,"Missing"
N18-2066,P16-1231,0,0.0305474,"Missing"
N18-2066,Q14-1010,0,0.28313,"Missing"
N18-2066,P15-2042,0,0.20838,"Missing"
N18-2066,E17-2051,1,0.882008,"Missing"
N18-2066,C14-1023,0,0.0323952,"Missing"
N18-2066,D14-1082,0,0.108154,"Missing"
N18-2066,H05-1066,0,0.316186,"Missing"
N18-2066,P04-1015,0,0.105485,"Missing"
N18-2066,W03-3017,0,0.29759,"Missing"
N18-2066,W04-0308,0,0.0609338,"Missing"
N18-2066,P15-1033,0,0.0315316,"Missing"
N18-2066,P09-1040,0,0.0753061,"Missing"
N18-2066,C96-1058,0,0.421517,"Missing"
N18-2066,P05-1013,0,0.352684,"Missing"
N18-2066,K17-3009,0,0.0242893,"Missing"
N18-2077,W16-2302,0,0.0245056,"Missing"
N18-2077,D16-1025,0,0.024631,"Missing"
N18-2077,N10-1031,0,0.0239866,"etrics Shared Task and perform the first evaluation of H y TER on large and noisier datasets. The results show that it still remains an interesting solution for MT evaluation, but highlight its limits when used to evaluate recent MT systems that make far less errors of lexical choice than older systems. Introduction Human translators and MT systems can produce multiple plausible translations for input texts. To reward meaning-equivalent but lexically divergent translations, MT evaluation metrics exploit synonyms and paraphrases, or multiple references (Papineni et al., 2002; Doddington, 2002; Denkowski and Lavie, 2010; Lo et al., 2012). The H y TER metric (Dreyer and Marcu, 2012) relies on massive reference networks encoding an exponential number of correct translations for parts of a given sentence, proposed by human annotators. The manually built networks attempt to encode the set of all correct translations for a sentence, and H y TER rewards high quality hypotheses by measuring their minimum edit distance to the set of possible translations. HyTER spurred a lot of enthusiasm but the need for human annotations heavily reduced its applicability to new data. We propose to use an embedding-based lexical su"
N18-2077,N12-1017,0,0.408623,"large and noisier datasets. The results show that it still remains an interesting solution for MT evaluation, but highlight its limits when used to evaluate recent MT systems that make far less errors of lexical choice than older systems. Introduction Human translators and MT systems can produce multiple plausible translations for input texts. To reward meaning-equivalent but lexically divergent translations, MT evaluation metrics exploit synonyms and paraphrases, or multiple references (Papineni et al., 2002; Doddington, 2002; Denkowski and Lavie, 2010; Lo et al., 2012). The H y TER metric (Dreyer and Marcu, 2012) relies on massive reference networks encoding an exponential number of correct translations for parts of a given sentence, proposed by human annotators. The manually built networks attempt to encode the set of all correct translations for a sentence, and H y TER rewards high quality hypotheses by measuring their minimum edit distance to the set of possible translations. HyTER spurred a lot of enthusiasm but the need for human annotations heavily reduced its applicability to new data. We propose to use an embedding-based lexical substitution model (Melamud et al., 2015) for building this type"
N18-2077,P15-2070,1,0.921182,"Missing"
N18-2077,W16-2339,0,0.0316299,"Missing"
N18-2077,N13-1092,1,0.918735,"Missing"
N18-2077,2006.amta-papers.25,0,0.407861,"selected substitutes, it achieves medium performance on much larger and noisier datasets, demonstrating the limits of the metric for tuning and evaluation of current MT systems. 1 Damon undermines richness variability belittles pluralism diminishes divergence in cinematography cinema film movie Figure 1: An English reference sentence enriched with substitutes selected by the embedding-based lexical substitution model. metric with automatically generated lattices (hereafter HyTERA). We show that HyTERA strongly correlates with HyTER with hand-crafted lattices, and approximates the hTER score (Snover et al., 2006) as measured using post-edits made by human annotators. Furthermore, we generate lattices for standard datasets from a recent WMT Metrics Shared Task and perform the first evaluation of H y TER on large and noisier datasets. The results show that it still remains an interesting solution for MT evaluation, but highlight its limits when used to evaluate recent MT systems that make far less errors of lexical choice than older systems. Introduction Human translators and MT systems can produce multiple plausible translations for input texts. To reward meaning-equivalent but lexically divergent tran"
N18-2077,H05-1021,0,0.0419773,"TERA to hTER scores. In Section § 5.2, we explore whether H y TERA can reliably predict human translation quality scores from the WMT16 Metrics Shared Task. The vectors s and t are word embeddings of the substitute and target generated by the skip-gram with negative sampling model (Mikolov et al., 2013b,a).4 The context C is the set of context embeddings for words appearing within a fixedwidth window of the target t in a sentence (we use 1 The code is available at https://bitbucket. org/gwisniewski/hytera/ 2 Note that as permutations of interest can be compactly encoded in a fine-state graph (Kumar and Byrne, 2005), the MOVE operation can be easily considered in our code by applying the substitutions to the permutation lattice rather than to the sentence. 3 PPDB paraphrases come into packages of different sizes (going from S to XXXL): small packages contain highprecision paraphrases while larger ones have high coverage. All are available from paraphrase.org 4 For the moment, we focus on individual content words. In future work, we plan to also annotate longer text segments in the references with multi-word PPDB paraphrases. 5 In the original implementation, Melamud et al. (2015) use syntactic dependenci"
N18-2077,W09-0441,0,0.0362055,"l substitution method described in Section 3 to each of the four references associated with a sentence, and considering the union of the resulting lattices. We report results for two kinds of lattices: lattices encoding all lexical substitutes available for a word in PPDB (allPars) and lattices of substitutes with PPDBSc&gt;2.3 (allParsFiltered) and AddCosSc≥0. As expected, the allPars lattices are much larger than the manual and the filtered lattices (cf. Table 1). In all our experiments, all corpora are down-cased and tokenized using standard Moses scripts. hTER scores are computed using TERp (Snover et al., 2009). 5.2 WMT Metrics Evaluation In our second set of experiments, we explore the ability of HyTERA to predict direct human judgments at the sentence level using the setting of the WMT16 Metrics Shared Task (Bojar et al., 2016). We measure the correlation between adSentence Level Evaluation Table 2 reports the correlation between HyTER, HyTERA and hTER at the sentence level. We also include as a baseline the correlation with the sentence-level B LEU 9 More precisely: S B LEU = 1 · 4 p where p is the i 4 ∑i=1 i number of i-grams that appears both in the reference and in the hypothesis divided by th"
N18-2077,W12-3129,0,0.0244022,"orm the first evaluation of H y TER on large and noisier datasets. The results show that it still remains an interesting solution for MT evaluation, but highlight its limits when used to evaluate recent MT systems that make far less errors of lexical choice than older systems. Introduction Human translators and MT systems can produce multiple plausible translations for input texts. To reward meaning-equivalent but lexically divergent translations, MT evaluation metrics exploit synonyms and paraphrases, or multiple references (Papineni et al., 2002; Doddington, 2002; Denkowski and Lavie, 2010; Lo et al., 2012). The H y TER metric (Dreyer and Marcu, 2012) relies on massive reference networks encoding an exponential number of correct translations for parts of a given sentence, proposed by human annotators. The manually built networks attempt to encode the set of all correct translations for a sentence, and H y TER rewards high quality hypotheses by measuring their minimum edit distance to the set of possible translations. HyTER spurred a lot of enthusiasm but the need for human annotations heavily reduced its applicability to new data. We propose to use an embedding-based lexical substitution model ("
N18-2077,W15-3050,0,0.0514759,"Missing"
N18-2077,W15-1501,0,0.178423,". The H y TER metric (Dreyer and Marcu, 2012) relies on massive reference networks encoding an exponential number of correct translations for parts of a given sentence, proposed by human annotators. The manually built networks attempt to encode the set of all correct translations for a sentence, and H y TER rewards high quality hypotheses by measuring their minimum edit distance to the set of possible translations. HyTER spurred a lot of enthusiasm but the need for human annotations heavily reduced its applicability to new data. We propose to use an embedding-based lexical substitution model (Melamud et al., 2015) for building this type of reference networks and test, for the first time, the 2 The Original HyTER Metric The HyTER metric (Dreyer and Marcu, 2012) computes the similarity between a translation hypothesis and a reference lattice that compactly encodes millions of meaning-equivalent translations. Formally HyTER is defined as: H y TER (x, Y ) = arg min y∈Y LS(x, y) len(y) (1) where Y is a set of references that can be encoded as a finite state automaton such as the one represented in Figure 1, x is a translation hypothesis and LS is the standard Levenshtein distance, defined as the minimum num"
N18-2077,W15-3053,0,0.053028,"Missing"
N18-2077,W12-3018,0,0.0656812,"Missing"
N18-2077,P02-1040,0,0.115047,"for standard datasets from a recent WMT Metrics Shared Task and perform the first evaluation of H y TER on large and noisier datasets. The results show that it still remains an interesting solution for MT evaluation, but highlight its limits when used to evaluate recent MT systems that make far less errors of lexical choice than older systems. Introduction Human translators and MT systems can produce multiple plausible translations for input texts. To reward meaning-equivalent but lexically divergent translations, MT evaluation metrics exploit synonyms and paraphrases, or multiple references (Papineni et al., 2002; Doddington, 2002; Denkowski and Lavie, 2010; Lo et al., 2012). The H y TER metric (Dreyer and Marcu, 2012) relies on massive reference networks encoding an exponential number of correct translations for parts of a given sentence, proposed by human annotators. The manually built networks attempt to encode the set of all correct translations for a sentence, and H y TER rewards high quality hypotheses by measuring their minimum edit distance to the set of possible translations. HyTER spurred a lot of enthusiasm but the need for human annotations heavily reduced its applicability to new data. We"
N19-1019,K17-3001,0,0.01863,"ng is known to be a difficult task as annotation guidelines are not always interpreted in a consistent manner (Marcus et al., 1993). For instance, Manning (2011) shows that many errors in the WSJ corpus are just mistakes rather than uncertainties or difficulties in the task ; Table 2 reports some of these annotation divergences that can be found in UD project. The situation is naturally worse in cross-corpora settings, in which treebanks are annotated by different laboratories or groups. The contribution of this paper is threefold : — we show that, as already pointed out by de Marneffe et al. (2017), the variation principle of Boyd et al. (2008) can be used to flag potential annotation discrepancies in the UD project. Building on this principle, we introduce, to evaluate the annotation consistency of a corpus, several methods and metrics that can be used, during the annotation to improve the quality of the corpus. — we generalize the conclusions of Manning (2011), highlighting how error rates in PoS tagging are stemming from the poor quality of annotations and inconsistencies in the resources ; we also systematically quantify the impact of annotation variation on PoS tagging performance"
N19-1019,H92-1026,0,0.31469,"Missing"
N19-1019,W13-2308,0,0.0229098,"scores are averaged over 10 training sessions. 6 for English, 5 for Czech and 4 for Swedish, Chinese, Japanese, Russian and Italian. Overall, it is possible to train and test 290 taggers (i.e. there are 290 possible combinations of a train and a test set of the same language), 191 of these conditions (i.e. pairs of a train set and a test set) correspond to a cross-corpus setting and can be considered for domain adaptation experiments. Many of these corpora 3 result from an automatic transformation (with, for some of them, manual corrections) from existing dependency or constituent treebanks (Bosco et al., 2013; Lipenkova and Souˇcek, 2014). Because most treebanks have been annotated and/or converted independently by different groups, 4 the risk of inconsistencies and errors in the application of annotation guidelines is increased. There may indeed be several sources of inconsistencies in the gold annotations : in addition to the divergences in the theoretical linguistic principles that governed the design of the original annotation guidelines, inconsistencies may also result from automatic (pre-)processing, human post-editing, or human annotation. Actually, several studies have recently pointed out"
N19-1019,petrov-etal-2012-universal,0,0.123549,"Missing"
N19-1019,D14-1104,0,0.346871,"; we also evaluate their impact on prediction performance. 1 Introduction The performance of Part-of-Speech (PoS) taggers significantly degrades when they are applied to test sentences that depart from training data. To illustrate this claim, Table 1 reports the error rate achieved by our in-house PoS tagger on the different combinations of train and test sets of the French treebanks of the Universal Dependencies (UD) project (Nivre et al., 2018). 1 It shows that depending on the train and test sets considered, the performance can vary by a factor of more than 25. Many studies (Foster, 2010; Plank et al., 2014) attribute this drop in accuracy to covariate shift (Shimodaira, 2000), characterizing the differences between domains by a change in the marginal distribution p(x) of the input (e.g. increase of out-of-vocabulary words, missing capitalization, different usage of punctuation, etc), while assuming that the conditional label distribution remains unaffected. This work adopts a different point of view : we believe that the variation in tagging performance is due to a dataset shift (Candela et al., 2009), i.e. a change in the joint distribution of the features and labels. We assume that this change"
N19-1019,K17-3009,0,0.0712344,"Missing"
N19-1019,K17-3016,0,0.030071,"ated and/or converted independently by different groups, 4 the risk of inconsistencies and errors in the application of annotation guidelines is increased. There may indeed be several sources of inconsistencies in the gold annotations : in addition to the divergences in the theoretical linguistic principles that governed the design of the original annotation guidelines, inconsistencies may also result from automatic (pre-)processing, human post-editing, or human annotation. Actually, several studies have recently pointed out that treebanks for the same language are not consistently annotated (Vilares and Gómez-Rodríguez, 2017; Aufrant et al., 2017). In a closely related context, Wisniewski et al. (2014) have also shown that, in spite of common annotation guidelines, one of the main bottleneck in cross-lingual transfer between UD corpora is the difference in the annotation conventions across treebanks and languages. works do (e.g. to evaluate the quality of a domain adaptation method or the measure the difficulty of the domain adaptation task) can be flawed and that this metrics has to be corrected to take into account the annotation divergences that exists between corpora. The rest of this paper is organized as fo"
N19-1019,L18-1711,1,0.840526,"g. Extracting maximal repeats allows us to find all sequence of words common to at least two sentences without extracting all their substrings. This problem can be solved efficiently using Generalized Suffix Tree (GST) (Gusfield, 1997) : if the corpus contains n words, extracting all the maximal repeats takes O (n) to build the GST and O (n) to list all the repeats. PoS annotations for these repeats can then be easily extracted and the ones that are identical can be filtered out to gather all suspicious repeats in a set of corpora. A detailed description of our implementation can be found in (Wisniewski, 2018). Annotation variation principle Filtering heuristics Suspicious repeats can of course correspond to words or structures that are The annotation variation principle (Boyd et al., 2008) states that if two identical sequences appear 220 ambiguity  The early voting suggests that this time the Latin Americans will come out toPART vote in greater numbers , but it is unclear whether the increase will have an impact .  Keep his cage open and go on your computer , or read a book , etc and maybe he will come out toADP you . inconsistency  Trudeau will extend that invitation to the 45th presidentNOUN"
N19-1019,D14-1187,1,0.832878,"nd errors in the application of annotation guidelines is increased. There may indeed be several sources of inconsistencies in the gold annotations : in addition to the divergences in the theoretical linguistic principles that governed the design of the original annotation guidelines, inconsistencies may also result from automatic (pre-)processing, human post-editing, or human annotation. Actually, several studies have recently pointed out that treebanks for the same language are not consistently annotated (Vilares and Gómez-Rodríguez, 2017; Aufrant et al., 2017). In a closely related context, Wisniewski et al. (2014) have also shown that, in spite of common annotation guidelines, one of the main bottleneck in cross-lingual transfer between UD corpora is the difference in the annotation conventions across treebanks and languages. works do (e.g. to evaluate the quality of a domain adaptation method or the measure the difficulty of the domain adaptation task) can be flawed and that this metrics has to be corrected to take into account the annotation divergences that exists between corpora. The rest of this paper is organized as follows. We first present the corpora and the tools used in our experiments (§ 2)"
N19-1019,P11-2033,0,0.0867945,"Missing"
N19-1019,P11-2000,0,0.102847,"ation is naturally worse in cross-corpora settings, in which treebanks are annotated by different laboratories or groups. The contribution of this paper is threefold : — we show that, as already pointed out by de Marneffe et al. (2017), the variation principle of Boyd et al. (2008) can be used to flag potential annotation discrepancies in the UD project. Building on this principle, we introduce, to evaluate the annotation consistency of a corpus, several methods and metrics that can be used, during the annotation to improve the quality of the corpus. — we generalize the conclusions of Manning (2011), highlighting how error rates in PoS tagging are stemming from the poor quality of annotations and inconsistencies in the resources ; we also systematically quantify the impact of annotation variation on PoS tagging performance for a large number of languages and corpora. — we show that the evaluation of PoS taggers in cross-corpora settings (typically in domain adaptation experiments) is hindered by systematic annotation discrepancies between the corpora and quantify the impact of this divergence on PoS tagger evaluation. Our observations stress the fact that comparing in- and out-domain sco"
N19-1019,E03-1068,0,0.418061,"ers in cross-corpora settings (typically in domain adaptation experiments) is hindered by systematic annotation discrepancies between the corpora and quantify the impact of this divergence on PoS tagger evaluation. Our observations stress the fact that comparing in- and out-domain scores as many The performance of Part-of-Speech tagging varies significantly across the treebanks of the Universal Dependencies project. This work points out that these variations may result from divergences between the annotation of train and test sets. We show how the annotation variation principle, introduced by Dickinson and Meurers (2003) to automatically detect errors in gold standard, can be used to identify inconsistencies between annotations ; we also evaluate their impact on prediction performance. 1 Introduction The performance of Part-of-Speech (PoS) taggers significantly degrades when they are applied to test sentences that depart from training data. To illustrate this claim, Table 1 reports the error rate achieved by our in-house PoS tagger on the different combinations of train and test sets of the French treebanks of the Universal Dependencies (UD) project (Nivre et al., 2018). 1 It shows that depending on the train"
N19-1019,N10-1060,0,0.0388102,"en annotations ; we also evaluate their impact on prediction performance. 1 Introduction The performance of Part-of-Speech (PoS) taggers significantly degrades when they are applied to test sentences that depart from training data. To illustrate this claim, Table 1 reports the error rate achieved by our in-house PoS tagger on the different combinations of train and test sets of the French treebanks of the Universal Dependencies (UD) project (Nivre et al., 2018). 1 It shows that depending on the train and test sets considered, the performance can vary by a factor of more than 25. Many studies (Foster, 2010; Plank et al., 2014) attribute this drop in accuracy to covariate shift (Shimodaira, 2000), characterizing the differences between domains by a change in the marginal distribution p(x) of the input (e.g. increase of out-of-vocabulary words, missing capitalization, different usage of punctuation, etc), while assuming that the conditional label distribution remains unaffected. This work adopts a different point of view : we believe that the variation in tagging performance is due to a dataset shift (Candela et al., 2009), i.e. a change in the joint distribution of the features and labels. We as"
N19-1019,E14-4028,0,0.0535209,"Missing"
N19-1019,J93-2004,0,0.0643183,"-treebank settings than in situations where the train and the test sets belong to the same treebank. This observation suggests that there may be systematic differences in the annotations of different treebanks which could make the domain adaptation setting artificially more difficult. Table 4: Percentage of suspicious repeats between the EWT and PUD corpora that contain an annotation inconsistency according to a human annotator either when the disjoint heuristic is used or when only suspicious repeats with at least n words are considered. 4.2 the same experiments with the Wall Street Journal (Marcus et al., 1993), 6 the iconic corpus of PoS tagging for which a thorough manual analysis of the annotation quality is described in (Manning, 2011). The observations reported in Table 5 show that the number of repeats varies greatly from one corpus to another, which is not surprising considering the wide array of genres covered by the treebanks that includes sentences written by journalists or learner of English (the genres with the largest number of repeats) or sentences generated by users on social media (that contain far less repeated parts). These observations also show that the percentage of repeats that"
N19-1019,W17-6514,0,0.0553318,"Missing"
P13-2025,W12-3102,0,0.0612844,"Missing"
P13-2025,W06-3114,0,0.246996,"ing only scores of other judges and some common ratings. Even if prediction performance is pretty low, decisions made using the predicted score proved to be more reliable than decision based on a single rating of each example. 1 Introduction Human assessment is often considered as the best, if not the only, way to evaluate ‘subjective’ NLP tasks like MT or speech generation. However, human evaluations are doomed to be noisy and, sometimes, even contradictory as they depend on individual perception and understanding of the score scale that annotators generally use in remarkably different ways (Koehn and Monz, 2006). Moreover, annotation is known to be a long and frustrating process and annotator fatigue has been identified as another source of noise (Pighin et al., 2012). In addition to defining and enforcing stricter guidelines, several solutions have been proposed to reduce the annotation effort and produce more reliable ratings. For instance, to limit the impact of the score scale interpretation, in the WMT evaluation campaign (Callison-Burch et al., 2012), annotators are asked to rank translation hypotheses 137 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,"
P13-2025,2012.iwslt-papers.5,0,0.0143183,"uation Guillaume Wisniewski Universit´e Paris Sud LIMSI–CNRS Orsay, France guillaume.wisniewski@limsi.fr Abstract from best to worst instead of providing absolute scores (e.g. in terms of adequacy or fluency). Generalizing this approach, several works (Pighin et al., 2012; Lopez, 2012) have defined novel annotation protocols to reduce the number of judgments that need to be collected. However, all these methods suffer from several limitations: first, they provide no interpretable information about the quality of the system (only a relative comparison between two systems is possible); second, (Koehn, 2012) has recently shown that the ranking they induce is not reliable. In this work, we study an alternative approach to the problem of collecting reliable human assessments. Our basic assumption, motivated by the success of ensemble methods, is that having several judgments for each example, even if they are noisy, will result in a more reliable decision than having a single judgment. An evaluation campaign should therefore aim at gathering a score matrix, in which each example is rated by all judges instead of having each judge rate only a small subset of examples, thereby minimizing redundancy."
P13-2025,W12-3101,0,0.0695436,"Missing"
P13-2025,C12-2079,0,0.0315195,"Missing"
P13-2025,2012.amta-papers.13,0,0.0599167,"Missing"
P13-2025,W09-0441,0,0.0602899,"Missing"
P13-2025,P10-1063,0,0.0750414,"Missing"
P13-2025,J08-4004,0,\N,Missing
W11-2135,W10-1704,1,0.806759,"the tokenization and detokenization steps (D´echelotte et al., 2008). Previous experiments have demonstrated that better normalization tools provide better BLEU scores (Papineni et al., 2002). Thus all systems are built in “true-case.” As German is morphologically more complex than English, the default policy which consists in treating each word form independently is plagued with data sparsity, which poses a number of difficulties both at training and decoding time. Thus, to translate from German to English, the German side was normalized using a specific pre-processing scheme (described in (Allauzen et al., 2010)), which aims at reducing the lexical redundancy and splitting complex compounds. Using the same pre-processing scheme to translate from English to German would require to postprocess the output to undo the pre-processing. As in our last year’s experiments (Allauzen et al., 2010), this pre-processing step could be achieved with a two-step decoding. However, by stacking two decoding steps, we may stack errors as well. Thus, for this direction, we used the German tokenizer provided by the organizers. 3.2 contains large portions that are not useful for translating news text. The first filter aime"
W11-2135,J92-4003,0,0.317321,"Missing"
W11-2135,J04-2004,0,0.208795,"is estimated and tuned as described in Section 4.1. Moreover, we also introduce in Section 4.2 the use of the SOUL language model (LM) (Le et al., 2011) in SMT. Based on neural networks, the SOUL LM can handle an arbitrary large vocabulary and a high order markovian assumption (up to 10-gram in this work). Finally, experimental results are reported in Section 5 both in terms of BLEU scores and translation edit rates (TER) measured on the provided newstest2010 dataset. 2 System Overview Our in-house n-code SMT system implements the bilingual n-gram approach to Statistical Machine Translation (Casacuberta and Vidal, 2004). Given a 1 This kind of characters was used for Teletype up to the seventies or early eighties. 309 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 309–315, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics source sentence sJ1 , a translation hypothesis tˆ1I is defined as the sentence which maximizes a linear combination of feature functions: ( ) M tˆ1I = arg max t1I ∑ λm hm (sJ1 ,t1I ) (1) m=1 a word-aligned corpus (using MGIZA++2 with default settings) in such a way that a unique segmentation of the bilingual corpus is achi"
W11-2135,W08-0310,1,0.792506,"Missing"
W11-2135,D10-1044,0,0.0594607,"Missing"
W11-2135,D08-1076,0,0.0565273,"iew of these rather inconclusive experiments, we chose to stick to the classical MERT for the submitted results. Optimization Issues Along with MIRA (Margin Infused Relaxed Algorithm) (Watanabe et al., 2007), MERT is the most widely used algorithm for system optimization. However, standard MERT procedure is known to suffer from instability of results and very slow training cycle with approximate estimates of one decoding cycle for each training parameter. For this year’s evaluation, we experimented with several alternatives to the standard n-best MERT procedure, namely, MERT on word lattices (Macherey et al., 2008) and two differentiable variants to the BLEU objective function optimized during the MERT cycle. We have recast the former in terms of a specific semiring and implemented it using a generalpurpose finite state automata framework (Sokolov and Yvon, 2011). The last two approaches, hereafter referred to as ZHN and BBN, replace the BLEU objective function, with the usual BLEU score on expected n-gram counts (Rosti et al., 2010) and with an expected BLEU score for normal n-gram counts (Zens et al., 2007), respectively. All expecta314 Conclusion In this paper, we described our submissions to WMT’11"
W11-2135,P03-1021,0,0.271147,"s (Tillmann, 2004) aiming at predicting the orientation of the next translation unit; a “weak” distance-based distortion model; and finally a word-bonus model and a tuple-bonus model which compensate for the system preference for short translations. The four lexicon models are similar to the ones used in a standard phrase-based system: two scores correspond to the relative frequencies of the tuples and two lexical weights are estimated from the automatically generated word alignments. The weights associated to feature functions are optimally combined using a discriminative training framework (Och, 2003) (Minimum Error Rate Training (MERT), see details in Section 5.4), using the provided newstest2009 data as development set. 2.1 Training Our translation model is estimated over a training corpus composed of tuple sequences using classical smoothing techniques. Tuples are extracted from 310 The resulting sequence of tuples (1) is further refined to avoid NULL words in the source side of the tuples (2). Once the whole bilingual training data is segmented into tuples, n-gram language model probabilities can be estimated. In this example, note that the English source words perfect and translations"
W11-2135,P02-1040,0,0.0853055,"n models. To train the target language models, we also used all provided data and monolingual corpora released by the LDC for French and English. Moreover, all parallel corpora were POS-tagged with the TreeTagger (Schmid, 1994). For German, the fine-grained POS information used for pre-processing was computed by the RFTagger (Schmid and Laws, 2008). 3.1 Tokenization 4 We took advantage of our in-house text processing tools for the tokenization and detokenization steps (D´echelotte et al., 2008). Previous experiments have demonstrated that better normalization tools provide better BLEU scores (Papineni et al., 2002). Thus all systems are built in “true-case.” As German is morphologically more complex than English, the default policy which consists in treating each word form independently is plagued with data sparsity, which poses a number of difficulties both at training and decoding time. Thus, to translate from German to English, the German side was normalized using a specific pre-processing scheme (described in (Allauzen et al., 2010)), which aims at reducing the lexical redundancy and splitting complex compounds. Using the same pre-processing scheme to translate from English to German would require t"
W11-2135,W10-1748,0,0.0323177,"for each training parameter. For this year’s evaluation, we experimented with several alternatives to the standard n-best MERT procedure, namely, MERT on word lattices (Macherey et al., 2008) and two differentiable variants to the BLEU objective function optimized during the MERT cycle. We have recast the former in terms of a specific semiring and implemented it using a generalpurpose finite state automata framework (Sokolov and Yvon, 2011). The last two approaches, hereafter referred to as ZHN and BBN, replace the BLEU objective function, with the usual BLEU score on expected n-gram counts (Rosti et al., 2010) and with an expected BLEU score for normal n-gram counts (Zens et al., 2007), respectively. All expecta314 Conclusion In this paper, we described our submissions to WMT’11 in the French-English and GermanEnglish shared translation tasks, in both directions. For this year’s participation, we only used n-code, our open source Statistical Machine Translation system based on bilingual n-grams. Our contributions are threefold. First, we have shown that n-gram based systems can achieve state-of-the-art performance on large scale tasks in terms of automatic metrics such as BLEU. Then, as already sho"
W11-2135,C08-1098,0,0.0510396,"action and reordering rules. 3 Data Pre-processing and Selection We used all the available parallel data allowed in the constrained task to compute the word alignments, except for the French-English tasks where the United Nation corpus was not used to train our translation models. To train the target language models, we also used all provided data and monolingual corpora released by the LDC for French and English. Moreover, all parallel corpora were POS-tagged with the TreeTagger (Schmid, 1994). For German, the fine-grained POS information used for pre-processing was computed by the RFTagger (Schmid and Laws, 2008). 3.1 Tokenization 4 We took advantage of our in-house text processing tools for the tokenization and detokenization steps (D´echelotte et al., 2008). Previous experiments have demonstrated that better normalization tools provide better BLEU scores (Papineni et al., 2002). Thus all systems are built in “true-case.” As German is morphologically more complex than English, the default policy which consists in treating each word form independently is plagued with data sparsity, which poses a number of difficulties both at training and decoding time. Thus, to translate from German to English, the G"
W11-2135,2011.eamt-1.33,1,0.771526,"system optimization. However, standard MERT procedure is known to suffer from instability of results and very slow training cycle with approximate estimates of one decoding cycle for each training parameter. For this year’s evaluation, we experimented with several alternatives to the standard n-best MERT procedure, namely, MERT on word lattices (Macherey et al., 2008) and two differentiable variants to the BLEU objective function optimized during the MERT cycle. We have recast the former in terms of a specific semiring and implemented it using a generalpurpose finite state automata framework (Sokolov and Yvon, 2011). The last two approaches, hereafter referred to as ZHN and BBN, replace the BLEU objective function, with the usual BLEU score on expected n-gram counts (Rosti et al., 2010) and with an expected BLEU score for normal n-gram counts (Zens et al., 2007), respectively. All expecta314 Conclusion In this paper, we described our submissions to WMT’11 in the French-English and GermanEnglish shared translation tasks, in both directions. For this year’s participation, we only used n-code, our open source Statistical Machine Translation system based on bilingual n-grams. Our contributions are threefold."
W11-2135,N04-4026,0,0.357669,"estimated by using the n-gram assumption: K p(sJ1 ,t1I ) = ∏ p((s,t)k |(s,t)k−1 . . . (s,t)k−n+1 ) k=1 Figure 1: Tuple extraction from a sentence pair. where s refers to a source symbol (t for target) and (s,t)k to the kth tuple of the given bilingual sentence pair. It is worth noticing that, since both languages are linked up in tuples, the context information provided by this translation model is bilingual. In addition to the translation model, eleven feature functions are combined: a target-language model (see Section 4 for details); four lexicon models; two lexicalized reordering models (Tillmann, 2004) aiming at predicting the orientation of the next translation unit; a “weak” distance-based distortion model; and finally a word-bonus model and a tuple-bonus model which compensate for the system preference for short translations. The four lexicon models are similar to the ones used in a standard phrase-based system: two scores correspond to the relative frequencies of the tuples and two lexical weights are estimated from the automatically generated word alignments. The weights associated to feature functions are optimally combined using a discriminative training framework (Och, 2003) (Minimu"
W11-2135,D07-1080,0,0.0221974,"ever, showed any consistent and significant improvement for the majority of setups tried (with the exception of the BBN approach, that had almost always improved over n-best MERT, but for the sole French to English translation direction). Additional experiments with 9 complementary translation models as additional features were performed with lattice-MERT, but neither showed any substantial improvement. In the view of these rather inconclusive experiments, we chose to stick to the classical MERT for the submitted results. Optimization Issues Along with MIRA (Margin Infused Relaxed Algorithm) (Watanabe et al., 2007), MERT is the most widely used algorithm for system optimization. However, standard MERT procedure is known to suffer from instability of results and very slow training cycle with approximate estimates of one decoding cycle for each training parameter. For this year’s evaluation, we experimented with several alternatives to the standard n-best MERT procedure, namely, MERT on word lattices (Macherey et al., 2008) and two differentiable variants to the BLEU objective function optimized during the MERT cycle. We have recast the former in terms of a specific semiring and implemented it using a gen"
W11-2135,D07-1055,0,0.0524218,"Missing"
W12-3120,P07-1038,0,0.0144971,"data. Standard results from machine learning show that such structures can be described either by a linear model using a large number of features or by a non-linear model using a (potentially) smaller set of features. As only a small number of training examples is available, we decided to focus on non-linear models in this work. 3 Inferring quality scores Predicting the quality scores can naturally be cast as a standard regression task, as the reference scores used in the evaluation are numerical (real) values. Regression is the approach adopted in most works on confidence estimation for MT (Albrecht and Hwa, 2007; Specia et al., 2010b). A simpler way to tackle the problem would be to recast it as binary classification task aiming at distinguishing “good” translations from “bad” ones (Blatz et al., 2004; Quirk, 2004). It is also possible, as shown by (Soricut and Echihabi, 2010), to use ranking approaches. However, because the shared task is evaluated by comparing the actual value of the predictions with the human scores, using these last two frameworks is not possible. In our experiments, following the observations reported in the previous section, we use two wellknown non-linear regression methods: p"
W12-3120,P11-1022,0,0.0139301,"dom forests, with a simple and limited feature set, succeeds in modeling the complex decisions required to assess translation quality and achieves results that are on a par with the second best results of the shared task. 1 Introduction Confidence estimation is the task of predicting the quality of a system prediction without knowledge of the expected output. It is an important step in many Natural Language Processing applications (Gandrabur et al., 2006). In Machine Translation (MT), this task has recently gained interest (Blatz et al., 2004; Specia et al., 2010b; Soricut and Echihabi, 2010; Bach et al., 2011). Indeed, professional translators are more and more requested to post-edit the outputs of a MT system rather than to produce a translation from scratch. Knowing in advance the segments they should focus on would be very helpful (Specia et al., 2010a). Confidence estimation is also of great interest for developers of MT system, as it provides them with a way to analyze the systems output and to better understand the main causes of errors. Even if several studies have tackled the problem of confidence estimation in machine translation, until now, very few datasets were publicly available and co"
W12-3120,C04-1046,0,0.495273,"tes our approach; ii) we show that using non-linear models, namely random forests, with a simple and limited feature set, succeeds in modeling the complex decisions required to assess translation quality and achieves results that are on a par with the second best results of the shared task. 1 Introduction Confidence estimation is the task of predicting the quality of a system prediction without knowledge of the expected output. It is an important step in many Natural Language Processing applications (Gandrabur et al., 2006). In Machine Translation (MT), this task has recently gained interest (Blatz et al., 2004; Specia et al., 2010b; Soricut and Echihabi, 2010; Bach et al., 2011). Indeed, professional translators are more and more requested to post-edit the outputs of a MT system rather than to produce a translation from scratch. Knowing in advance the segments they should focus on would be very helpful (Specia et al., 2010a). Confidence estimation is also of great interest for developers of MT system, as it provides them with a way to analyze the systems output and to better understand the main causes of errors. Even if several studies have tackled the problem of confidence estimation in machine tr"
W12-3120,P98-1032,0,0.0232108,"ld better be produced from scratch. The test contains 422 sentence pairs, the quality of which has to be predicted. The training set also contains additional material, namely two references (the reference originally given by WMT and a human post-edited one), which will allow us to better interpret our results. No references were provided for the test set. 2.2 Features Several works have studied the problem of confidence estimation (Blatz et al., 2004; Specia et al., 2010b) or related problems such as predicting readability (Kanungo and Orr, 2009) or developing automated essay scoring systems (Burstein et al., 1998). They all use the same basic features: IBM 1 score measures the quality of the “association” of the source and the target sentence using bag-of-word translation models; Language model score accounts for the “fluency”, “grammaticality” and “plausibility” of a target sentence; Simple surface features like the sentence length, the number of out-of-vocabulary words or words that are not aligned. These features are used to account for the difficulty of the translation task. Figure 1: Distribution of the human scores on the train set. (HS∗ stands for Human Scores) Figure 2 plots the distribution of"
W12-3120,quirk-2004-training,0,0.06031,". As only a small number of training examples is available, we decided to focus on non-linear models in this work. 3 Inferring quality scores Predicting the quality scores can naturally be cast as a standard regression task, as the reference scores used in the evaluation are numerical (real) values. Regression is the approach adopted in most works on confidence estimation for MT (Albrecht and Hwa, 2007; Specia et al., 2010b). A simpler way to tackle the problem would be to recast it as binary classification task aiming at distinguishing “good” translations from “bad” ones (Blatz et al., 2004; Quirk, 2004). It is also possible, as shown by (Soricut and Echihabi, 2010), to use ranking approaches. However, because the shared task is evaluated by comparing the actual value of the predictions with the human scores, using these last two frameworks is not possible. In our experiments, following the observations reported in the previous section, we use two wellknown non-linear regression methods: polynomial regression and random forests. We also consider linear regression as a baseline. We will now quickly describe these three methods. Linear regression (Hastie et al., 2003) is a simple model in which"
W12-3120,P10-1063,0,0.388873,"on-linear models, namely random forests, with a simple and limited feature set, succeeds in modeling the complex decisions required to assess translation quality and achieves results that are on a par with the second best results of the shared task. 1 Introduction Confidence estimation is the task of predicting the quality of a system prediction without knowledge of the expected output. It is an important step in many Natural Language Processing applications (Gandrabur et al., 2006). In Machine Translation (MT), this task has recently gained interest (Blatz et al., 2004; Specia et al., 2010b; Soricut and Echihabi, 2010; Bach et al., 2011). Indeed, professional translators are more and more requested to post-edit the outputs of a MT system rather than to produce a translation from scratch. Knowing in advance the segments they should focus on would be very helpful (Specia et al., 2010a). Confidence estimation is also of great interest for developers of MT system, as it provides them with a way to analyze the systems output and to better understand the main causes of errors. Even if several studies have tackled the problem of confidence estimation in machine translation, until now, very few datasets were publi"
W12-3120,specia-etal-2010-dataset,0,0.106075,") we show that using non-linear models, namely random forests, with a simple and limited feature set, succeeds in modeling the complex decisions required to assess translation quality and achieves results that are on a par with the second best results of the shared task. 1 Introduction Confidence estimation is the task of predicting the quality of a system prediction without knowledge of the expected output. It is an important step in many Natural Language Processing applications (Gandrabur et al., 2006). In Machine Translation (MT), this task has recently gained interest (Blatz et al., 2004; Specia et al., 2010b; Soricut and Echihabi, 2010; Bach et al., 2011). Indeed, professional translators are more and more requested to post-edit the outputs of a MT system rather than to produce a translation from scratch. Knowing in advance the segments they should focus on would be very helpful (Specia et al., 2010a). Confidence estimation is also of great interest for developers of MT system, as it provides them with a way to analyze the systems output and to better understand the main causes of errors. Even if several studies have tackled the problem of confidence estimation in machine translation, until now,"
W12-3120,C98-1032,0,\N,Missing
W12-3141,W10-1704,1,0.815478,"eriments have demonstrated that better normalization tools provide better BLEU scores: all systems are thus built in “true-case”. Compared to last year, the pre-processing of utf-8 characters was significantly improved. As German is morphologically more complex than English, the default policy which consists in treating each word form independently is plagued with data sparsity, which severely impacts both training (alignment) and decoding (due to unknown forms). When translating from German into English, the German side is thus normalized using a specific pre-processing scheme (described in (Allauzen et al., 2010; Durgar El-Kahlout and Yvon, 333 2010)), which aims at reducing the lexical redundancy by (i) normalizing the orthography, (ii) neutralizing most inflections and (iii) splitting complex compounds. All parallel corpora were POS-tagged with the TreeTagger (Schmid, 1994); in addition, for German, fine-grained POS labels were also needed for pre-processing and were obtained using the RFTagger (Schmid and Laws, 2008). 5.2 Bilingual corpora As for last year’s evaluation, we used all the available parallel data for the German-English language pair, while only a subpart of the French-English parallel"
W12-3141,E09-1010,1,0.834206,"lpful. As the IBM1 model is asymmetric, two models are estimated, one in both directions. Contrary to the reported results, these additional features do not yield significant improvements over the baseline system. We assume that the difficulty is to add information to an already extensively optimized system. Moreover, the IBM1 models are estimated on the same training corpora as the translation system, a fact that may explain the redundancy of these additional features. In a separate series of experiments, we also add WSD features calculated according to a variation of the method proposed in (Apidianaki, 2009). For each word of a subset of the input (source language) vocabulary, a simple WSD classifier produces a probability distribution over a set of translations8 . During reranking, each translation hypothesis is scanned and the word translations that match one of the proposed variant are rewarded using an additional score. While this method had given some Conclusion In this paper, we described our submissions to WMT’12 in the French-English and GermanEnglish shared translation tasks, in both directions. As for our last year’s participation, our main systems are built with n-code, the open source"
W12-3141,J93-2003,0,0.0934316,"ize .... u8 u9 u10 u11 u12 Figure 1: Extract of a French-English sentence pair segmented into bilingual units. The original (org) French sentence appears at the top of the figure, just above the reordered source s and target t. The pair (s, t) decomposes into a sequence of L bilingual units (tuples) u1 , ..., uL . Each tuple ui contains a source and a target phrase: si and ti . in two parts (source and target), and by taking words as the basic units of the n-gram TM. This may seem to be a regression with respect to current state-ofthe-art SMT systems, as the shift from the wordbased model of (Brown et al., 1993) to the phrasebased models of (Zens et al., 2002) is usually considered as a major breakthrough of the recent years. Indeed, one important motivation for considering phrases was to capture local context in translation and reordering. It should however be emphasized that the decomposition of phrases into words is only re-introduced here as a way to mitigate the parameter estimation problems. Translation units are still pairs of phrases, derived from a bilingual segmentation in tuples synchronizing the source and target n-gram streams. In fact, the estimation policy described in section 4 will a"
W12-3141,P05-1032,0,0.0155644,"e hardware conditions. 7 Experimental results 7.1 n-code with SOUL We also developped an alternative approach implementing “on-the-fly” estimation of the parameter of a standard phase-based model, using Moses (Koehn et al., 2007) as the decoder. Implementing on-thefly estimation for n-code, while possible in theory, is less appealing due to the computational cost of estimating a smoothed language model. Given an input source file, it is possible to compute only those statistics which are required to translate the phrases it contains. As in previous works on onthe-fly model estimation for SMT (Callison-Burch et al., 2005; Lopez, 2008), we compute a suffix array for the source corpus. This further enables to consider only a subset of translation examples, which we select by deterministic random sampling, meaning that the sample is chosen randomly with respect to the full corpus but that the same sample is always returned for a given value of sample size, hereafter denoted N . In our experiments, we used N = 1, 000 and computed from the sample and the word alignments (we used the same tokenization and word alignments as in all other submitted systems) the same translation6 and lexical reordering models as the s"
W12-3141,J04-2004,0,0.03102,"code, an open source in-house Statistical Machine Translation (SMT) system based on bilingual n-grams1 . The main novelty of this year’s participation is the use, in a large scale system, of the continuous space translation models described in (Hai-Son et al., 2012). These models estimate the n-gram probabilities of bilingual translation units using neural networks. We also investigate an alternative approach where the translation probabilities of a phrase based system are estimated “on-the-fly” 1 http://ncode.limsi.fr/ 2 System overview n-code implements the bilingual n-gram approach to SMT (Casacuberta and Vidal, 2004; Mari˜no et al., 2006; Crego and Mari˜no, 2006). In this framework, translation is divided in two steps: a source reordering step and a (monotonic) translation step. Source reordering is based on a set of learned rewrite rules that non-deterministically reorder the input words. Applying these rules result in a finite-state graph of possible source reorderings, which is then searched for the best possible candidate translation. 2.1 Features Given a source sentence s of I words, the best translation hypothesis ˆt is defined as the sequence of J words that maximizes a linear combination of fea33"
W12-3141,2010.iwslt-papers.6,0,0.024222,"Missing"
W12-3141,W08-0310,1,0.883725,"Missing"
W12-3141,N12-1005,1,0.885285,"Missing"
W12-3141,P07-2045,0,0.00674765,"lt (31.7 BLEU point) that is slightly worst than the n-code baseline (32.0) and slightly better than the equivalent Moses baseline (31.5), but does it much faster. Model estimation for the test file is reduced to 2 hours and 50 minutes, with an additional overhead for loading and writing files of one and a half hours, compared to roughly 210 hours for our baseline systems under comparable hardware conditions. 7 Experimental results 7.1 n-code with SOUL We also developped an alternative approach implementing “on-the-fly” estimation of the parameter of a standard phase-based model, using Moses (Koehn et al., 2007) as the decoder. Implementing on-thefly estimation for n-code, while possible in theory, is less appealing due to the computational cost of estimating a smoothed language model. Given an input source file, it is possible to compute only those statistics which are required to translate the phrases it contains. As in previous works on onthe-fly model estimation for SMT (Callison-Burch et al., 2005; Lopez, 2008), we compute a suffix array for the source corpus. This further enables to consider only a subset of translation examples, which we select by deterministic random sampling, meaning that th"
W12-3141,C08-1064,0,0.0749035,"rimental results 7.1 n-code with SOUL We also developped an alternative approach implementing “on-the-fly” estimation of the parameter of a standard phase-based model, using Moses (Koehn et al., 2007) as the decoder. Implementing on-thefly estimation for n-code, while possible in theory, is less appealing due to the computational cost of estimating a smoothed language model. Given an input source file, it is possible to compute only those statistics which are required to translate the phrases it contains. As in previous works on onthe-fly model estimation for SMT (Callison-Burch et al., 2005; Lopez, 2008), we compute a suffix array for the source corpus. This further enables to consider only a subset of translation examples, which we select by deterministic random sampling, meaning that the sample is chosen randomly with respect to the full corpus but that the same sample is always returned for a given value of sample size, hereafter denoted N . In our experiments, we used N = 1, 000 and computed from the sample and the word alignments (we used the same tokenization and word alignments as in all other submitted systems) the same translation6 and lexical reordering models as the standard traini"
W12-3141,J06-4004,0,0.217743,"Missing"
W12-3141,P03-1021,0,0.0736745,"x lexicalized reordering models (Tillmann, 2004; Crego et al., 2011) aiming at predicting the orientation of the next translation unit; a “weak” distance-based distortion model; and finally a word-bonus model and a tuple-bonus model which compensate for the system preference for short translations. The four lexicon models are similar to the ones used in standard phrase-based systems: two scores correspond to the relative frequencies of the tuples and two lexical weights are estimated from the automatic word alignments. The weights vector λ is learned using a discriminative training framework (Och, 2003) (Minimum Error Rate Training (MERT)) using the newstest2009 as development set and BLEU (Papineni et al., 2002) as the optimization criteria. 2.2 P (s, t) = Standard n-gram translation models During the training phase (Mari˜no et al., 2006), tuples are extracted from a word-aligned corpus (using MGIZA++3 with default settings) in such a way that a unique segmentation of the bilingual corpus is achieved. A baseline n-gram translation model is then estimated over a training corpus composed of tuple sequences using modified KnesserNey Smoothing (Chen and Goodman, 1998). 2.3 During decoding, sour"
W12-3141,P02-1040,0,0.0874389,"ation of the next translation unit; a “weak” distance-based distortion model; and finally a word-bonus model and a tuple-bonus model which compensate for the system preference for short translations. The four lexicon models are similar to the ones used in standard phrase-based systems: two scores correspond to the relative frequencies of the tuples and two lexical weights are estimated from the automatic word alignments. The weights vector λ is learned using a discriminative training framework (Och, 2003) (Minimum Error Rate Training (MERT)) using the newstest2009 as development set and BLEU (Papineni et al., 2002) as the optimization criteria. 2.2 P (s, t) = Standard n-gram translation models During the training phase (Mari˜no et al., 2006), tuples are extracted from a word-aligned corpus (using MGIZA++3 with default settings) in such a way that a unique segmentation of the bilingual corpus is achieved. A baseline n-gram translation model is then estimated over a training corpus composed of tuple sequences using modified KnesserNey Smoothing (Chen and Goodman, 1998). 2.3 During decoding, source sentences are represented in the form of word lattices containing the most promising reordering hypotheses, s"
W12-3141,C08-1098,0,0.0308747,"ing (alignment) and decoding (due to unknown forms). When translating from German into English, the German side is thus normalized using a specific pre-processing scheme (described in (Allauzen et al., 2010; Durgar El-Kahlout and Yvon, 333 2010)), which aims at reducing the lexical redundancy by (i) normalizing the orthography, (ii) neutralizing most inflections and (iii) splitting complex compounds. All parallel corpora were POS-tagged with the TreeTagger (Schmid, 1994); in addition, for German, fine-grained POS labels were also needed for pre-processing and were obtained using the RFTagger (Schmid and Laws, 2008). 5.2 Bilingual corpora As for last year’s evaluation, we used all the available parallel data for the German-English language pair, while only a subpart of the French-English parallel data was selected. Word alignment models were trained using all the data, whereas the translation models were estimated on a subpart of the parallel data: the UN corpus was discarded for this step and about half of the French-English Giga corpus was filtered based on a perplexity criterion as in (Allauzen et al., 2011)). For French-English, we mainly upgraded the training material from last year by extracting th"
W12-3141,N04-4026,0,0.0279162,"max t,a M X ) λm hm (a, s, t) (1) L Y P (ui |ui−1 , ..., ui−n+1 ) (2) i=1 m=1 where λm is the weight associated with feature function hm and a denotes an alignment between source and target phrases. Among the feature functions, the peculiar form of the translation model constitute one of the main difference between the n-gram approach and standard phrase-based systems. This will be further detailled in section 2.2 and 3. In addition to the translation model, fourteen feature functions are combined: a target-language model (Section 5.3); four lexicon models; six lexicalized reordering models (Tillmann, 2004; Crego et al., 2011) aiming at predicting the orientation of the next translation unit; a “weak” distance-based distortion model; and finally a word-bonus model and a tuple-bonus model which compensate for the system preference for short translations. The four lexicon models are similar to the ones used in standard phrase-based systems: two scores correspond to the relative frequencies of the tuples and two lexical weights are estimated from the automatic word alignments. The weights vector λ is learned using a discriminative training framework (Och, 2003) (Minimum Error Rate Training (MERT))"
W12-3141,2002.tmi-tutorials.2,0,0.0391067,"French-English sentence pair segmented into bilingual units. The original (org) French sentence appears at the top of the figure, just above the reordered source s and target t. The pair (s, t) decomposes into a sequence of L bilingual units (tuples) u1 , ..., uL . Each tuple ui contains a source and a target phrase: si and ti . in two parts (source and target), and by taking words as the basic units of the n-gram TM. This may seem to be a regression with respect to current state-ofthe-art SMT systems, as the shift from the wordbased model of (Brown et al., 1993) to the phrasebased models of (Zens et al., 2002) is usually considered as a major breakthrough of the recent years. Indeed, one important motivation for considering phrases was to capture local context in translation and reordering. It should however be emphasized that the decomposition of phrases into words is only re-introduced here as a way to mitigate the parameter estimation problems. Translation units are still pairs of phrases, derived from a bilingual segmentation in tuples synchronizing the source and target n-gram streams. In fact, the estimation policy described in section 4 will actually allow us to take into account larger cont"
W12-3141,D08-1039,0,\N,Missing
W12-3141,W11-2135,1,\N,Missing
W12-3141,N04-1021,0,\N,Missing
W12-4201,apidianaki-2008-translation,1,0.852565,"ained (because the two variants of the adjective are reduced to the same lemma). All lexicon entries satisfying the above criteria are retained and used for disambiguation. In these initial experiments, we disambiguate English words having less than 20 French translations in the lexicon. Each French translation of an English word that appears more than once in the training corpus4 is characterized by a weighted English feature vector built from the training data. Vector building The feature vectors corresponding to the translations are built by exploiting information from the source contexts (Apidianaki, 2008; Grefenstette, 1994). For each translation of an EN word w, we extract the content words that co-occur with w in the corresponding source sentences of the parallel corpus (i.e. the content words that occur in the same sentence as w whenever it is translated by this translation). The extracted source language words constitute the features of the vector built for the translation. For each translation Ti of w, let N be the number of features retained from the corresponding source context. Each feature Fj (1 ≤ j ≤ N) receives a total weight tw(Fj , Ti ) defined as the product of the feature’s glo"
W12-4201,E09-1010,1,0.961121,"at and Wu, 2007). This task-oriented conception of WSD is manifested in the area of multilingual semantic processing: supervised methods, which were previously shown to give the best results, are being abandoned in favor of unsupervised ones that do not rely on preannotated training data. Accordingly, pre-defined In a multilingual setting, the sense inventories needed for disambiguation are generally built from all possible translations of words or phrases in a parallel corpus (Carpuat and Wu, 2007; Chan et al., 2007), or by using more complex representations of the semantics of translations (Apidianaki, 2009; Mihalcea et al., 2010; Lefever and Hoste, 2010). However, integrating this semantic knowledge into Statistical Machine Translation (SMT) raises several challenges: the way in which the predictions of the WSD classifier have to be taken into account; the type of context exploited for disambiguation; the target words to be disambiguated (“all-words” WSD vs. WSD restricted to target words satisfying specific criteria); the use of a single classifier versus building separate classifiers for each source word; the quantity and type of data used for training the classifier (e.g., use of raw data or"
W12-4201,P05-1048,0,0.0813922,"ing some avenues for future work. 2 Related work Word sense disambiguation systems generally work at the word level: given an input word and its context, they predict its (most likely) meaning. At the same time, state-of-the-art translation systems all consider groups of words (phrases, tuples, etc.) rather than single words in the translation process. This discrepancy between the units used in MT and those used in WSD is one of the major difficulties in integrating word predictions into the decoder. This was, for instance, one of the reasons for the somewhat disappointing results obtained by Carpuat and Wu (2005) when the output of a WSD system was directly incorporated into a Chinese-English SMT system. Because of this difficulty, other crosslingual semantics works have considered only simplified tasks, like blank-filling, without addressing the integration of the WSD models in full-scale MT systems (Vickrey et al., 2005; Specia, 2006). Since the pioneering work of Carpuat and Wu (2005), several more successful ways to take WSD predictions into account have been proposed. For instance, Carpuat and Wu (2007) proposed to generalize the WSD system so that it performs a fully 2 phrasal multiword disambig"
W12-4201,D07-1007,0,0.397955,"improvements in translation performance, highlighting the usefulness of source side disambiguation for SMT. 1 Introduction Word Sense Disambiguation (WSD) is the task of identifying the sense of words in texts by reference to some pre-existing sense inventory. The selection of the appropriate inventory and WSD method strongly depends on the goal WSD intends to serve: recent methods are increasingly oriented towards the disambiguation needs of specific end applications, and explicitly aim at improving the overall performance of complex Natural Language Processing systems (Ide and Wilks, 2007; Carpuat and Wu, 2007). This task-oriented conception of WSD is manifested in the area of multilingual semantic processing: supervised methods, which were previously shown to give the best results, are being abandoned in favor of unsupervised ones that do not rely on preannotated training data. Accordingly, pre-defined In a multilingual setting, the sense inventories needed for disambiguation are generally built from all possible translations of words or phrases in a parallel corpus (Carpuat and Wu, 2007; Chan et al., 2007), or by using more complex representations of the semantics of translations (Apidianaki, 2009"
W12-4201,P07-1005,0,0.209625,"overall performance of complex Natural Language Processing systems (Ide and Wilks, 2007; Carpuat and Wu, 2007). This task-oriented conception of WSD is manifested in the area of multilingual semantic processing: supervised methods, which were previously shown to give the best results, are being abandoned in favor of unsupervised ones that do not rely on preannotated training data. Accordingly, pre-defined In a multilingual setting, the sense inventories needed for disambiguation are generally built from all possible translations of words or phrases in a parallel corpus (Carpuat and Wu, 2007; Chan et al., 2007), or by using more complex representations of the semantics of translations (Apidianaki, 2009; Mihalcea et al., 2010; Lefever and Hoste, 2010). However, integrating this semantic knowledge into Statistical Machine Translation (SMT) raises several challenges: the way in which the predictions of the WSD classifier have to be taken into account; the type of context exploited for disambiguation; the target words to be disambiguated (“all-words” WSD vs. WSD restricted to target words satisfying specific criteria); the use of a single classifier versus building separate classifiers for each source w"
W12-4201,C10-1027,1,0.871747,"If this is the case, the corresponding probabilities are additively accumulated for the current hypothesis. At the end, two features are appended to each hypothesis in the n-best list: the total score accumulated for the hypothesis and 5 the same score normalized by the number of words in the hypothesis. Two MERT initialization schemes were considered: (1) all model weights are initialized to zero, and (2) all the weights of “standard” features are initialized to the values found by MERT and the new WSD features to zero. 4.2 Local Language Models We propose to adapt the approach introduced in Crego et al. (2010) as an alternative way to integrate the WSD predictions within the decoder: for each sentence to be translated, an additional language model (LM) is estimated and taken into account during decoding. As this additional “local” model depends on the source sentence, it can be used as an external source of knowledge to reinforce translation hypotheses complying with criteria predicted from the whole source sentence. For instance, the unigram probabilities of the additional LM can be derived from the (word) predictions of a WSD system, bigram probabilities from the prediction of phrases and so on a"
W12-4201,2009.eamt-1.32,0,0.0163645,"er, given that the number of phrases is far larger than the number of words, this approach suffers from sparsity and computational problems, as it requires training a classifier for each entry of the phrase table. Chan et al. (2007) introduced a way to modify the rule weights of a hierarchical translation system to reflect the predictions of their WSD system. While their approach and ours are built on the same intuition (an adaptation of a model to incorporate word predictions) their work is specific to hierarchical systems, while ours can be applied to any decoder that uses a language model. Haque et al. (2009) et Haque et al. (2010) introduce lexico-syntactic descriptions in the form of supertags as source language context-informed features in a phrase-based SMT and a state-of-the-art hierarchical model, respectively, and report significant gains in translation quality. Closer to our work, Mauser et al. (2009) and Patry and Langlais (2011) train a global lexicon model that predicts the bag of output words from the bag of input words. As no explicit alignment between input and output words is used, words are chosen based on the (global) input context. For each input sentence, the decoder considers t"
W12-4201,2010.amta-papers.23,0,0.0316251,"Missing"
W12-4201,P07-2045,0,0.00398791,". 5 Evaluation 5.3 5.1 Table 2 reports the results of our experiments. It appears that, for the considered task, sense disambiguation improves translation performance: n-best rescoring results in a 0.37 BLEU improvement and using an additional language model brings about an improvement of up to a 0.88 BLEU. In both cases, MERT assigns a large weight to the additional feaExperimental Setting In all our experiments, we considered the TEDtalk English to French data set provided by the IWSLT’11 evaluation campaign, a collection of public speeches on a variety of topics. We used the Moses decoder (Koehn et al., 2007). The TED-talk corpus is a small data set made of a monolingual corpus (111, 431 sentences) used 6 Results 7 http://statmt.org/wmt08/scripts.tgz method baseline rescoring additional LM — WSD (zero init) WSD (reinit) oracle 3-gram oracle 2-gram oracle 1-gram IBM 1 WSD BLEU 29.63 30.00 29.58 43.56 39.36 42.92 30.18 30.51 METEOR 53.78 54.26 53.96 64.64 62.92 69.39 54.36 54.38 Table 2: Evaluation results on the TED-talk task of our two methods to integrate WSD predictions. baseline 67.57 45.97 51.79 52.17 PoS Nouns Verbs Adjectives Adverbs WSD 69.06 47.76 53.94 56.25 Table 3: Contrastive lexical e"
W12-4201,D09-1022,0,0.0449313,"n system to reflect the predictions of their WSD system. While their approach and ours are built on the same intuition (an adaptation of a model to incorporate word predictions) their work is specific to hierarchical systems, while ours can be applied to any decoder that uses a language model. Haque et al. (2009) et Haque et al. (2010) introduce lexico-syntactic descriptions in the form of supertags as source language context-informed features in a phrase-based SMT and a state-of-the-art hierarchical model, respectively, and report significant gains in translation quality. Closer to our work, Mauser et al. (2009) and Patry and Langlais (2011) train a global lexicon model that predicts the bag of output words from the bag of input words. As no explicit alignment between input and output words is used, words are chosen based on the (global) input context. For each input sentence, the decoder considers these word predictions as an additional feature that it uses to define a new model score which favors translation hypotheses containing words predicted by the global lexicon model. A difference between this approach and our work is that instead of using a global lexicon model, we disambiguate a subset of t"
W12-4201,max-etal-2010-contrastive,1,0.859696,"to the WSD method introduced in Section 3, these oracle experiments rely on sense predictions for all source words and not only content words. Surprisingly enough, predicting phrases instead of words results only in a small improvement. Additional experiments are required to explain why 2-gram oracle achieved such a low performance. 7 5.4 Contrastive lexical evaluation All the measures used for evaluating the impact of WSD information on translation show improvements, as discussed in the previous section. We complement these results with another measure of translation performance, proposed by Max et al. (2010), which allows for a more fine-grained contrastive evaluation of the translations produced by different systems. The method permits to compare the results produced by the systems on different word classes and to take into account the source words that were actually translated. We focus this evaluation on the classes of content words (nouns, adjectives, verbs and adverbs) on which WSD had an important coverage. Our aim is, first, to explore how these words are handled by a WSDinformed SMT system (the system using the local language models) compared to the baseline system that does not exploit a"
W12-4201,W09-2412,0,0.034286,"Missing"
W12-4201,J03-1002,0,0.00530526,"tions of a word and to assign a probability to each translation for new instances of the word in context. Each translation is represented by a source language feature vector that the classifier uses for disambiguation. All experiments carried out in this study are for the English (EN) - French (FR) language pair. 3.1 Source Language Feature Vectors Preprocessing The information needed by the classifier is gathered from the EN-FR training data provided for the IWSLT’11 evaluation task.1 The dataset consists of 107,268 parallel sentences, wordaligned in both translation directions using GIZA++ (Och and Ney, 2003). We disambiguate EN words found in the parallel corpus that satisfy the set of criteria described below. Two bilingual lexicons are built from the alignment results and filtered to eliminate spurious alignments. First, translation correspondences with a probability lower than a threshold are discarded;2 then translations are filtered by part-of-speech (PoS), keeping for each word only translations pertaining to the same grammatical category;3 finally, only intersecting alignments (i.e., correspondences found in the lexicons of both directions) are retained. Given that the lexicons contain wor"
W12-4201,I11-1074,0,0.589525,"redictions of their WSD system. While their approach and ours are built on the same intuition (an adaptation of a model to incorporate word predictions) their work is specific to hierarchical systems, while ours can be applied to any decoder that uses a language model. Haque et al. (2009) et Haque et al. (2010) introduce lexico-syntactic descriptions in the form of supertags as source language context-informed features in a phrase-based SMT and a state-of-the-art hierarchical model, respectively, and report significant gains in translation quality. Closer to our work, Mauser et al. (2009) and Patry and Langlais (2011) train a global lexicon model that predicts the bag of output words from the bag of input words. As no explicit alignment between input and output words is used, words are chosen based on the (global) input context. For each input sentence, the decoder considers these word predictions as an additional feature that it uses to define a new model score which favors translation hypotheses containing words predicted by the global lexicon model. A difference between this approach and our work is that instead of using a global lexicon model, we disambiguate a subset of the words in the input sentence"
W12-4201,P06-3010,0,0.142762,"slation process. This discrepancy between the units used in MT and those used in WSD is one of the major difficulties in integrating word predictions into the decoder. This was, for instance, one of the reasons for the somewhat disappointing results obtained by Carpuat and Wu (2005) when the output of a WSD system was directly incorporated into a Chinese-English SMT system. Because of this difficulty, other crosslingual semantics works have considered only simplified tasks, like blank-filling, without addressing the integration of the WSD models in full-scale MT systems (Vickrey et al., 2005; Specia, 2006). Since the pioneering work of Carpuat and Wu (2005), several more successful ways to take WSD predictions into account have been proposed. For instance, Carpuat and Wu (2007) proposed to generalize the WSD system so that it performs a fully 2 phrasal multiword disambiguation. However, given that the number of phrases is far larger than the number of words, this approach suffers from sparsity and computational problems, as it requires training a classifier for each entry of the phrase table. Chan et al. (2007) introduced a way to modify the rule weights of a hierarchical translation system to"
W12-4201,H05-1097,0,0.178488,"ngle words in the translation process. This discrepancy between the units used in MT and those used in WSD is one of the major difficulties in integrating word predictions into the decoder. This was, for instance, one of the reasons for the somewhat disappointing results obtained by Carpuat and Wu (2005) when the output of a WSD system was directly incorporated into a Chinese-English SMT system. Because of this difficulty, other crosslingual semantics works have considered only simplified tasks, like blank-filling, without addressing the integration of the WSD models in full-scale MT systems (Vickrey et al., 2005; Specia, 2006). Since the pioneering work of Carpuat and Wu (2005), several more successful ways to take WSD predictions into account have been proposed. For instance, Carpuat and Wu (2007) proposed to generalize the WSD system so that it performs a fully 2 phrasal multiword disambiguation. However, given that the number of phrases is far larger than the number of words, this approach suffers from sparsity and computational problems, as it requires training a classifier for each entry of the phrase table. Chan et al. (2007) introduced a way to modify the rule weights of a hierarchical transla"
W12-4201,W09-2413,0,\N,Missing
W12-4201,S10-1002,0,\N,Missing
W12-4201,N04-1021,0,\N,Missing
W13-2250,W12-3102,0,0.14645,"Missing"
W13-2250,1993.eamt-1.1,0,0.511454,"Missing"
W13-2250,P03-1021,0,0.0272251,"search space E (generally represented as a lattice); δu (E) has the value 1 if u occurs in the translation hypothesis E and 0 otherwise and P (E, A|F ) is the probability that the source sentence F is translated by the hypothesis E using a derivation A. Following (Gispert et al., 2013), this probability is estimated by applying a soft-max function to the score of the decoder: exp (α × H(E, A, F )) 0 0 (A0 ,E 0 )∈E exp (H(E , A , F )) 0.00 P (A, E|F ) = P where the decoder score H(E, A, F ) is typically a linear combination of a handful of features, the weights of which are estimated by MERT (Och, 2003). n-gram posteriors therefore aggregate two pieces of information: first, the number of paths in the lattice (i.e. the number of translation hypotheses of the search path) the n-gram appears in; second, the decoder scores of these paths that can be roughly interpreted as a quality of the path. Computing P (u|E) requires to enumerate all ngram contained in E and to count the number of paths in which this n-gram appears at least once. An efficient method to perform this computation in a single traversal of the lattice is described in (Gispert et al., 2013). This algorithm has been reimplemented1"
W13-2250,P10-1063,0,0.0272722,"tion (SMT) systems in the professional translation industry is still limited by the lack of reliability of SMT outputs, the quality of which varies to a great extent. In this context, a critical piece of information would be for MT systems to assess their output translations with automatically derived quality measures. This problem is the focus of a shared task, the aim of which is to predict the quality of a translation without knowing any human reference(s). To the best of our knowledge, all approaches so far have tackled quality estimation as a supervised learning problem (He et al., 2010; Soricut and Echihabi, 2010; Specia et al., 2010; Specia, 2011). A wide variety of features have been proposed, most of which can be described as loosely ‘linguistic’ features that describe the source sentence, the target sentence and the association between them (Callison-Burch et al., 2012). Surprisingly enough, information used by the decoder to choose the best translation in the search space, such as its internal scores, have hardly been considered and never proved to be useful. Indeed, it is well-known that these scores are hard to interpret and to compare across hypotheses. Furthermore, mapping scores of a linear"
W13-2250,2011.eamt-1.12,0,0.180488,"n industry is still limited by the lack of reliability of SMT outputs, the quality of which varies to a great extent. In this context, a critical piece of information would be for MT systems to assess their output translations with automatically derived quality measures. This problem is the focus of a shared task, the aim of which is to predict the quality of a translation without knowing any human reference(s). To the best of our knowledge, all approaches so far have tackled quality estimation as a supervised learning problem (He et al., 2010; Soricut and Echihabi, 2010; Specia et al., 2010; Specia, 2011). A wide variety of features have been proposed, most of which can be described as loosely ‘linguistic’ features that describe the source sentence, the target sentence and the association between them (Callison-Burch et al., 2012). Surprisingly enough, information used by the decoder to choose the best translation in the search space, such as its internal scores, have hardly been considered and never proved to be useful. Indeed, it is well-known that these scores are hard to interpret and to compare across hypotheses. Furthermore, mapping scores of a linear classifier (such as the scores estim"
W13-2250,2013.tc-1.10,0,0.038752,"Description LIMSI has participated to the tasks 1-1 (prediction of the hTER) and 1-3 (prediction of the postedition time). Similar features and learning algorithms have been considered for the two tasks. We will first quickly describe them before discussing the specific development made for task 1-3. 1 Our implementation can be downloaded from http:// perso.limsi.fr/Individu/wisniews/. 399 3.1 Features 3.2 In addition to the features described in the previous section, 176 ‘standard’ features for quality estimation have been considered. The full list of features we have considered is given in (Wisniewski et al., 2013) and the features set can be downloaded from our website.2 These features can be classified into four broad categories: Learning Methods The main focus of this work is to study the relevance of features for quality estimation; therefore, only very standard learning methods were used in our work. For this year submission both random forests (Breiman, 2001) and elastic net regression (Zou and Hastie, 2005) have been used. The capacity of random forests to take into account complex interactions between features has proved to be a key element in the results achieved in our experiments with last ye"
W13-2250,W12-3120,1,0.636027,"can be downloaded from our website.2 These features can be classified into four broad categories: Learning Methods The main focus of this work is to study the relevance of features for quality estimation; therefore, only very standard learning methods were used in our work. For this year submission both random forests (Breiman, 2001) and elastic net regression (Zou and Hastie, 2005) have been used. The capacity of random forests to take into account complex interactions between features has proved to be a key element in the results achieved in our experiments with last year campaign datasets (Zhuang et al., 2012). As we are considering a larger features set this year and the number of examples is comparatively quite small, we also considered elastic regression, a linear model trained with L1 and L2 priors as regularizers, hoping that training a sparse model would reduce the risk of overfitting. In this study, we have used the implementation provided by scikit-learn (Pedregosa et al., 2011). As detailed in Section 4.1, cross-validation has been used to choose the hyper-parameters of all regressors, namely the number of estimators, the maximal depth of a tree and the minimum number of examples in a leaf"
W13-2250,P10-1064,0,\N,Missing
W14-3344,W08-0330,0,0.0318141,"ms of precision, recall and f1 score computed on the BAD label. More precisely, if the number of true positive (i.e. 2 We used FreeLing (http:nlp.lsi.upc.edu/ freeling/) to predict the POS tags of the translation hypotheses and, for the sake of clarity, mapped the 71 tags used by FreeLing to the 11 universal POS tags of Petrov et al. (2012). 349 ity that can be achieved if the target word is replaced by any other word (i.e. maxv∈V p(t1 , ..., tj−1 , v, tj+1 , ..., tm ) where the max runs over all the words of the vocabulary). pseudo-references to design new MT metrics (Albrecht and Hwa, 2007; Albrecht and Hwa, 2008) or for confidence estimation (Soricut and Echihabi, 2010; Soricut and Narsale, 2012) but, to the best of our knowledge, this is the first time that they are used to predict confidence at the word level. Pseudo-references are used to define 3 binary features which fire if the target word is in the pseudo-reference, in a 2-gram shared between the pseudo-reference and the translation hypothesis or in a common 3-gram, respectively. The lattices representing the search space considered to generate these pseudo-references also allow us to estimate the posterior probability of a target word that qua"
W14-3344,padro-stanilovsky-2012-freeling,0,0.0227896,"Missing"
W14-3344,petrov-etal-2012-universal,0,0.0134501,"lities table. The second kind of association features relies on pseudo-references, that is to say, translations of the source sentence produced by an independent MT system. Many works have considered As the classes are unbalanced, prediction performance will be evaluated in terms of precision, recall and f1 score computed on the BAD label. More precisely, if the number of true positive (i.e. 2 We used FreeLing (http:nlp.lsi.upc.edu/ freeling/) to predict the POS tags of the translation hypotheses and, for the sake of clarity, mapped the 71 tags used by FreeLing to the 11 universal POS tags of Petrov et al. (2012). 349 ity that can be achieved if the target word is replaced by any other word (i.e. maxv∈V p(t1 , ..., tj−1 , v, tj+1 , ..., tm ) where the max runs over all the words of the vocabulary). pseudo-references to design new MT metrics (Albrecht and Hwa, 2007; Albrecht and Hwa, 2008) or for confidence estimation (Soricut and Echihabi, 2010; Soricut and Narsale, 2012) but, to the best of our knowledge, this is the first time that they are used to predict confidence at the word level. Pseudo-references are used to define 3 binary features which fire if the target word is in the pseudo-reference, in"
W14-3344,W13-2250,1,0.900286,"et al., 2011) (also used by our MT system) and a 4-gram language model based on Part-of-Speech sequences. The latter model was estimated on the Spanish side of the bilingual data provided in the translation shared task in 2013. These data were POS-tagged with FreeLing (Padr´o and Stanilovsky, 2012). All these language models have been used to define two different features : 4 4.1 Learning Methods Classifiers Predicting whether a word in a translation hypothesis should be post-edited or not can naturally be framed as a binary classification task. Based on our experiments in previous campaigns (Singh et al., 2013; Zhuang et al., 2012), we considered random forest in all our experiments.3 Random forest (Breiman, 2001) is an ensemble method that learns many classification trees and predicts an aggregation of their result (for instance by majority voting). In contrast with standard decision trees, in which each node is split using the best split among all features, in a random forest the split is chosen randomly. In spite of this simple and counter-intuitive learning strategy, random forests have proven to be very good ‘out-of-the-box’ learners. Random forests have achieved very good performance in many"
W14-3344,P10-1063,0,0.023156,"AD label. More precisely, if the number of true positive (i.e. 2 We used FreeLing (http:nlp.lsi.upc.edu/ freeling/) to predict the POS tags of the translation hypotheses and, for the sake of clarity, mapped the 71 tags used by FreeLing to the 11 universal POS tags of Petrov et al. (2012). 349 ity that can be achieved if the target word is replaced by any other word (i.e. maxv∈V p(t1 , ..., tj−1 , v, tj+1 , ..., tm ) where the max runs over all the words of the vocabulary). pseudo-references to design new MT metrics (Albrecht and Hwa, 2007; Albrecht and Hwa, 2008) or for confidence estimation (Soricut and Echihabi, 2010; Soricut and Narsale, 2012) but, to the best of our knowledge, this is the first time that they are used to predict confidence at the word level. Pseudo-references are used to define 3 binary features which fire if the target word is in the pseudo-reference, in a 2-gram shared between the pseudo-reference and the translation hypothesis or in a common 3-gram, respectively. The lattices representing the search space considered to generate these pseudo-references also allow us to estimate the posterior probability of a target word that quantifies the probability that it is part of the system out"
W14-3344,W12-3121,0,0.0174012,"the number of true positive (i.e. 2 We used FreeLing (http:nlp.lsi.upc.edu/ freeling/) to predict the POS tags of the translation hypotheses and, for the sake of clarity, mapped the 71 tags used by FreeLing to the 11 universal POS tags of Petrov et al. (2012). 349 ity that can be achieved if the target word is replaced by any other word (i.e. maxv∈V p(t1 , ..., tj−1 , v, tj+1 , ..., tm ) where the max runs over all the words of the vocabulary). pseudo-references to design new MT metrics (Albrecht and Hwa, 2007; Albrecht and Hwa, 2008) or for confidence estimation (Soricut and Echihabi, 2010; Soricut and Narsale, 2012) but, to the best of our knowledge, this is the first time that they are used to predict confidence at the word level. Pseudo-references are used to define 3 binary features which fire if the target word is in the pseudo-reference, in a 2-gram shared between the pseudo-reference and the translation hypothesis or in a common 3-gram, respectively. The lattices representing the search space considered to generate these pseudo-references also allow us to estimate the posterior probability of a target word that quantifies the probability that it is part of the system output (Gispert et al., 2013)."
W14-3344,W12-3120,1,0.934531,"used by our MT system) and a 4-gram language model based on Part-of-Speech sequences. The latter model was estimated on the Spanish side of the bilingual data provided in the translation shared task in 2013. These data were POS-tagged with FreeLing (Padr´o and Stanilovsky, 2012). All these language models have been used to define two different features : 4 4.1 Learning Methods Classifiers Predicting whether a word in a translation hypothesis should be post-edited or not can naturally be framed as a binary classification task. Based on our experiments in previous campaigns (Singh et al., 2013; Zhuang et al., 2012), we considered random forest in all our experiments.3 Random forest (Breiman, 2001) is an ensemble method that learns many classification trees and predicts an aggregation of their result (for instance by majority voting). In contrast with standard decision trees, in which each node is split using the best split among all features, in a random forest the split is chosen randomly. In spite of this simple and counter-intuitive learning strategy, random forests have proven to be very good ‘out-of-the-box’ learners. Random forests have achieved very good performance in many similar • the probabil"
W15-3027,W14-3348,0,0.034514,"edits transforming a sentence into another. While this sequence is not necessarily of minimal length, it is faster to compute, easier to use and, above all, more interpretable than the one computed using the standard minimum edit distance algorithm. In particular, difflib is able to automatically find edits between ‘phrases’ rather than between single words. 3 matic translation as a source sentence and its postedition as the target sentence. The word alignment between the automatic translation and the post-edited sentence, used as input in our APE-MT pipeline, has been computed using Meteor (Denkowski and Lavie, 2014). The APE-MT system has then been trained following the usual steps.3 In our experiments, we used our in-house MT system NC ODE (Crego et al., 2011) that implements a n-gram based translation model. As main features we used a 3-gram bilingual language model on words, a 4-gram bilingual language model on PoS factors and a 4-gram target language model trained only on the post-editions sentences, along with the conventional features (4 lexical features, 6 lexicalized reordering, distortion model, word and phrase penalty). We did allow reorderings during decoding. The training data is used to extr"
W15-3027,N13-1073,0,0.0287951,"translation and the length of the corresponding post-edition was higher than 1.2 or lower than 0.8. As shown in Table 1, these examples correspond mainly to errors in sentence boundaries or to ‘over-translation’ (e.g. when the post-editor added the translated title in the third example of Table 1), that could have a negative impact on the training of an APE system. At the end, the training set we used in all our experiments is made of 10,404 sentences. The source sentences and the automatic translation of the training and development set have been aligned at the word level using FASTA L IGN (Dyer et al., 2013) and the grow-diag-final symmetrization heuristic. To improve alignment quality, the sources and the translations have been first concatenated to the English-Spanish Europarl dataset and the resulting corpus has been aligned as a whole. Spanish MT outputs and post-editions have also been PoS-tagged using F REE L ING,1 a state-of-the-art rule-based PoS tagger for Spanish. We used a CRF-based model trained on the Penn Treebank for the English source sentences. All PoS tags have been mapped to the universal PoS Introduction This paper describes LIMSI submission to the WMT’15 Shared Task on Automa"
W15-3027,petrov-etal-2012-universal,0,0.0411748,"Missing"
W15-3027,N07-1064,0,0.742334,"E L ING,1 a state-of-the-art rule-based PoS tagger for Spanish. We used a CRF-based model trained on the Penn Treebank for the English source sentences. All PoS tags have been mapped to the universal PoS Introduction This paper describes LIMSI submission to the WMT’15 Shared Task on Automatic Post-Editing (APE). This task aims at automatically correcting errors produced by an unknown Machine Translation (MT) system by learning from human posteditions. For the first edition of this Shared Task we have submitted two APE systems. The first one, described in Section 3, is based on the approach of Simard et al. (2007) and considers the APE task as the automatic translation between a translation hypothesis and its post-edition. This straightforward approach does not succeed in improving translation quality. To understand the reasons of this failure, we present, in Section 4 a detailed analysis of the training data that highlights some of the difficulties of training an APE system. The second submitted system implements a series of sieves, applying, each, a simple postediting rule. The definition of these rules is based on our analysis of the most frequent error corrections. Experiments with this approach (S"
W15-3027,2013.mtsummit-papers.15,1,0.841472,"Missing"
W16-1203,D12-1133,0,0.0531772,"ich allows us to make all the experiments required to compare the various design decisions. The results of Table 1 also show that using the grow-diag heuristic to symmetrize the alignments rather than the intersection heuristic hurts performance for all languages. source de es fr it sv Experiments The parallel sentences are aligned in both directions with Giza++ (Och and Ney, 2003). These alignments are then merged with the intersection and grow-diag heuristics. For each language pair, the source dataset (Europarl) is PoS-tagged and parsed using the transition-based version of the MateParser (Bohnet and Nivre, 2012) with a beam of 40, which was trained on the UDT corpus. These predicted annotations are then partially projected on the target language data using the projection strategy described in Section 2.1. To train a parser on partially projected target data, we used our own implementation of the arceager dependency parser, using the features described in Zhang and Nivre (2011). The greedy version of the parser is used in all but one experiments of the Section 3 while a beam-search (with a beamsize of 8 for learning & parsing) is used to achieve 2 2.5 target 2.3 the best performances of the proposed m"
W16-1203,D11-1005,0,0.239751,"Missing"
W16-1203,C12-1059,0,0.0592431,", such as the symmetrization heuristic or the filtering threshold, that can have a large impact on the quality of the transferred parser. We will evaluate, in Section 3 trough 3.1 the impact of these design decisions. 2.2 Partial Transition-based Learning We consider a transition-based dependency parser based on the arc-eager algorithm (Nivre, 2003): this parser builds a dependency tree incrementally by performing a sequence of actions. At each step of the parsing process, a classifier scores each possible action and the highest scoring one is applied. Training relies on the dynamic oracle of Goldberg and Nivre (2012): for each sentence, a parse tree is built incrementally; at each step, if the predicted action creates an erroneous dependency (or, equivalently, prevents the creation of a gold dependency), a weight vector is updated, according to the perceptron rule. The set of all ‘correct’ actions is built considering the (potentially wrong) predicted tree and the gold action is defined as the correct action with the highest model score. It is crucial to note that the training algorithm is an 1 We also remove from target sentences containing nonprojective dependencies, as sentences containing non-projecti"
W16-1203,P08-1068,0,0.129895,"Missing"
W16-1203,N16-1121,1,0.627771,"popularity and been improved by many works (see the overview in Section 2). In spite of the simplicity of the annotation transfer principle, all these methods have several (hidden) parameters, such as the symmetrization heuristic or filtering thresholds, that make any direct comparison of their performance very hard. That is why, in this work, we aim at analyzing the impact of external factors used as pre- and post-processing steps and their significance in the whole transfer process. To this end, we propose to use the simple transfer strategy exploiting partially annotated data introduced in Lacroix et al. (2016) to systematically compare various design decisions. The transfer strategy used in our experiments is explained in Section 2. We then propose to explore and analyze different external factors: projected data filtering (Section 3.1), enhancement of the parsing strategy (Section 3.2) and multi-source transfer (Section 3.3). Finally, we compare the efficiency of dependency transfer and supervised parsing (Section 3.4) and analyze the performance achieved for the different kind of labels (Section 3.5). Proceedings of the Workshop on Multilingual and Cross-lingual Methods in NLP, pages 20–29, c San"
W16-1203,C14-1075,0,0.456056,"Missing"
W16-1203,P14-1126,0,0.137642,"Missing"
W16-1203,D11-1006,0,0.327188,"Missing"
W16-1203,P13-2017,0,0.184343,"Missing"
W16-1203,D10-1120,0,0.027799,"ed to under-resourced languages through the use of cross-lingual techniques. In this work, we focus on the transfer of syntactic dependency annotations. Two main transfer strategies have been proposed in the literature: direct transfer model and annotation transfer. The first approach is mainly based on delexicalized parsing (Zeman and Resnik, 2008; McDonald et al., 2011) which assumes of common morpho-syntactic representation (e.g. PoS tags) between the source and target languages. It has been improved with the use of self-training, data selection, relexicalization and multi-source transfer (Naseem et al., 2010; Cohen et al., 2011; Søgaard, 2011; T¨ackstr¨om et al., 2013). 20 The second approach (transfer of annotations), relies on parallel corpora to project, through alignment links, the dependencies automatically predicted from a resource-rich language to a resourcepoor language. This approach pioneered by Hwa et al. (2005) requires various heuristic transformation rules to cope with the non-isomorphism between the source and target structures as well as with the noise in source annotations and in alignments. It has since enjoyed a great popularity and been improved by many works (see the overview"
W16-1203,W03-3017,0,0.184308,"ted corpus for the target language that contains partial but accurate annotations.1 We will describe, in the following Section, how a parser can be trained on such data. In spite of its simplicity, this way to transfer dependency has several (hidden) parameters, such as the symmetrization heuristic or the filtering threshold, that can have a large impact on the quality of the transferred parser. We will evaluate, in Section 3 trough 3.1 the impact of these design decisions. 2.2 Partial Transition-based Learning We consider a transition-based dependency parser based on the arc-eager algorithm (Nivre, 2003): this parser builds a dependency tree incrementally by performing a sequence of actions. At each step of the parsing process, a classifier scores each possible action and the highest scoring one is applied. Training relies on the dynamic oracle of Goldberg and Nivre (2012): for each sentence, a parse tree is built incrementally; at each step, if the predicted action creates an erroneous dependency (or, equivalently, prevents the creation of a gold dependency), a weight vector is updated, according to the perceptron rule. The set of all ‘correct’ actions is built considering the (potentially w"
W16-1203,J03-1002,0,0.00721589,"re clarified respectively in sections 3.1 and 3.2. This method achieves results that are competitive with recent state-of-the-art methods such as (Ma and Xia, 2014; Rasooli and Collins, 2015), at a much cheaper computational cost,5 which allows us to make all the experiments required to compare the various design decisions. The results of Table 1 also show that using the grow-diag heuristic to symmetrize the alignments rather than the intersection heuristic hurts performance for all languages. source de es fr it sv Experiments The parallel sentences are aligned in both directions with Giza++ (Och and Ney, 2003). These alignments are then merged with the intersection and grow-diag heuristics. For each language pair, the source dataset (Europarl) is PoS-tagged and parsed using the transition-based version of the MateParser (Bohnet and Nivre, 2012) with a beam of 40, which was trained on the UDT corpus. These predicted annotations are then partially projected on the target language data using the projection strategy described in Section 2.1. To train a parser on partially projected target data, we used our own implementation of the arceager dependency parser, using the features described in Zhang and N"
W16-1203,D15-1039,0,0.108875,"Missing"
W16-1203,P11-2120,0,0.0781757,"he use of cross-lingual techniques. In this work, we focus on the transfer of syntactic dependency annotations. Two main transfer strategies have been proposed in the literature: direct transfer model and annotation transfer. The first approach is mainly based on delexicalized parsing (Zeman and Resnik, 2008; McDonald et al., 2011) which assumes of common morpho-syntactic representation (e.g. PoS tags) between the source and target languages. It has been improved with the use of self-training, data selection, relexicalization and multi-source transfer (Naseem et al., 2010; Cohen et al., 2011; Søgaard, 2011; T¨ackstr¨om et al., 2013). 20 The second approach (transfer of annotations), relies on parallel corpora to project, through alignment links, the dependencies automatically predicted from a resource-rich language to a resourcepoor language. This approach pioneered by Hwa et al. (2005) requires various heuristic transformation rules to cope with the non-isomorphism between the source and target structures as well as with the noise in source annotations and in alignments. It has since enjoyed a great popularity and been improved by many works (see the overview in Section 2). In spite of the sim"
W16-1203,W09-1104,0,0.394698,"Missing"
W16-1203,N13-1126,0,0.127947,"Missing"
W16-1203,C14-1175,0,0.398116,"Missing"
W16-1203,D14-1187,1,0.845368,"Missing"
W16-1203,I08-3008,0,0.155315,"Processing (NLP) tools. Their use is however hindered by the scarcity of annotated data, which are only available for a restricted number of tasks, genres, domain, and languages. The supervision information that exists for well-resourced languages can however be transferred to under-resourced languages through the use of cross-lingual techniques. In this work, we focus on the transfer of syntactic dependency annotations. Two main transfer strategies have been proposed in the literature: direct transfer model and annotation transfer. The first approach is mainly based on delexicalized parsing (Zeman and Resnik, 2008; McDonald et al., 2011) which assumes of common morpho-syntactic representation (e.g. PoS tags) between the source and target languages. It has been improved with the use of self-training, data selection, relexicalization and multi-source transfer (Naseem et al., 2010; Cohen et al., 2011; Søgaard, 2011; T¨ackstr¨om et al., 2013). 20 The second approach (transfer of annotations), relies on parallel corpora to project, through alignment links, the dependencies automatically predicted from a resource-rich language to a resourcepoor language. This approach pioneered by Hwa et al. (2005) requires"
W16-1203,P11-2033,0,0.0597534,"Ney, 2003). These alignments are then merged with the intersection and grow-diag heuristics. For each language pair, the source dataset (Europarl) is PoS-tagged and parsed using the transition-based version of the MateParser (Bohnet and Nivre, 2012) with a beam of 40, which was trained on the UDT corpus. These predicted annotations are then partially projected on the target language data using the projection strategy described in Section 2.1. To train a parser on partially projected target data, we used our own implementation of the arceager dependency parser, using the features described in Zhang and Nivre (2011). The greedy version of the parser is used in all but one experiments of the Section 3 while a beam-search (with a beamsize of 8 for learning & parsing) is used to achieve 2 2.5 target 2.3 the best performances of the proposed method (Section 2.5).3 intersection (en) (multi) 73.7 76.8 76.8 79.3 77.9 80.9 77.8 80.1 82.1 83.3 grow-diag (en) (multi) 71.2 75.6 75.4 79.0 76.7 81.0 76.2 80.1 80.1 83.1 sup. 84.4 85.5 85.8 86.9 87.8 Table 1: Results of our transfer method. ‘sup’ present the fully supervised scores. 3 Analysis 3.1 The importance of filtering To assess the usefulness of filtering on tra"
W16-1203,C12-2136,0,0.0150353,"attached token per sentence is specified. Greedy target parsing. Evaluation on gold PoS-tagged data. de es fr it sv clusters It is well known that different techniques can boost parsing performance. For instance, clusters (Koo et al., 2008) may be used to reduce lexical sparseness, which is particularly appropriate in the case of dependency parser transfer since parallel data are generally not from the same domain as the corpus used to train and evaluate the parser. Another approach for boosting parsing performance is the use of a beam-search strategy that reduces the number of search errors (Zhang and Nivre, 2012). In this section, we aim at assessing, first, how parsing performance of the source language impacts the quality of the transferred parser, and second, how using more ‘advanced’ parsing techniques may boost parsing in the target language. Using a similar transfer process as in the previous section, we conduct experiments in which the source and target parsers will be progressively enriched: we consider, in a first experiment, a greedy parser to predict the dependencies of the source and target sentences; the source greedy parsers are then replaced by a beam-search parser and features describi"
W16-1205,P10-1131,0,0.0153098,"ions to supervise the training in target can also be viewed as a (trivial) form of direct model transfer. This approach has been extended in many ways. Cohen et al. (2011) use several source languages and train one delexicalized model in each; the optimal convex combination of these models is used to process the target language. A variant of this strategy is to view the source parameter values as priors for the target model, an idea that has been used repeatedly in the context of domain adaptation. It has notably been used for transferring parsers (Cohen and Smith, 2009; Burkett et al., 2010; Berg-Kirkpatrick and Klein, 2010) and, more recently, to also transfer alignment models (Levinboim and Chiang, 2015). This brief retrospective has demonstrated the variety of cross-lingual transfer techniques, many of which are borrowed from the domain adaptation literature. The applicability and success of these methods depend on the task and of the available resources. We now explore ways to apply them for the word alignment task. 3 Word alignments: cross-lingual scenarios After a quick review of standard algorithms for word alignment, we present situations in which they can be improved by cross-lingual knowledge. 3.1 Align"
W16-1205,N10-1083,0,0.0655573,"Missing"
W16-1205,J93-2003,0,0.0932332,"Missing"
W16-1205,W10-2906,0,0.0133926,"taking source annotations to supervise the training in target can also be viewed as a (trivial) form of direct model transfer. This approach has been extended in many ways. Cohen et al. (2011) use several source languages and train one delexicalized model in each; the optimal convex combination of these models is used to process the target language. A variant of this strategy is to view the source parameter values as priors for the target model, an idea that has been used repeatedly in the context of domain adaptation. It has notably been used for transferring parsers (Cohen and Smith, 2009; Burkett et al., 2010; Berg-Kirkpatrick and Klein, 2010) and, more recently, to also transfer alignment models (Levinboim and Chiang, 2015). This brief retrospective has demonstrated the variety of cross-lingual transfer techniques, many of which are borrowed from the domain adaptation literature. The applicability and success of these methods depend on the task and of the available resources. We now explore ways to apply them for the word alignment task. 3 Word alignments: cross-lingual scenarios After a quick review of standard algorithms for word alignment, we present situations in which they can be improved by"
W16-1205,N09-1009,0,0.0274379,"been mentioned: indeed, taking source annotations to supervise the training in target can also be viewed as a (trivial) form of direct model transfer. This approach has been extended in many ways. Cohen et al. (2011) use several source languages and train one delexicalized model in each; the optimal convex combination of these models is used to process the target language. A variant of this strategy is to view the source parameter values as priors for the target model, an idea that has been used repeatedly in the context of domain adaptation. It has notably been used for transferring parsers (Cohen and Smith, 2009; Burkett et al., 2010; Berg-Kirkpatrick and Klein, 2010) and, more recently, to also transfer alignment models (Levinboim and Chiang, 2015). This brief retrospective has demonstrated the variety of cross-lingual transfer techniques, many of which are borrowed from the domain adaptation literature. The applicability and success of these methods depend on the task and of the available resources. We now explore ways to apply them for the word alignment task. 3 Word alignments: cross-lingual scenarios After a quick review of standard algorithms for word alignment, we present situations in which t"
W16-1205,D11-1005,0,0.0292803,"Missing"
W16-1205,P11-1061,0,0.0261148,"uch projections are less appropriate for fine-grained morphological information such as case or gender (as those distinctions greatly vary across languages), and would be even less so for pairs of languages having antagonist definitions of a word. Furthermore, its success will depend on the density and quality of the alignments (Lacroix et al., 2016b), meaning that it might be more suited to situations in which large bitexts are available. A possible workaround to the noisiness issue is to interpret transferred annotations as soft, rather than hard constraints: see e.g. (Ganchev et al., 2009; Das and Petrov, 2011; Li et al., 2014; Wang and Manning, 2014) for various implementations of this idea; or to combine it with another source of information (T¨ackstr¨om et al., 2013). Alignment projection is not only noisy: it also yields incomplete annotations, requiring methods that learn from partially annotated corpora (Wisniewski et al., 2014). A last strategy worth mentioning here for generating artificial annotations is to use Machine Translation (Tiedemann, 2014). 2.2 Transfer in parameter space The second main family of techniques use the same model for the source and target languages: learned parameter"
W16-1205,N13-1073,0,0.0240163,"s 2 and up) and a fertility model (models 3 and up). Distortion is absolute for models 2-3 and relative for models 4-6; in the HMM model, it is captured by Markovian dependencies between consecutive alignments links. Among these parameters, the translation model is lexicalized in both languages, fertility is lexicalized in the source side and distortion is unlexicalized but rely on word clusters for models 4-6. In all cases, parameters are learned in an unsupervised way using the EM algorithm. Many refinements to these algorithms have been proposed, often to improve computational performance (Dyer et al., 2013). Another line of work tries to improve IBM and HMM models’ low generalization power by using feature-based models (Moore, 2005; Berg-Kirkpatrick et al., 2010). However, the IBM models remain today the most widely used approach both because of their efficiency and because they do not require any annotated data. They will thus serve as our main baseline. 3.2 Real-world situations for alignment transfer Scenarios for improving word alignment with crosslingual transfer fall into two categories, depending on whether the source and target languages play a symmetric role. We first consider the stand"
W16-1205,R11-1017,0,0.0245574,"o transfer useful supervision information from well-resourced to under-resourced languages, speeding up the development of NLP tools for new domains and tasks. Many techniques for transferring knowledge across languages have been proposed in the literature (see § 2 for a brief overview). A widely-used methodology consists in generating automatic annotations for the resource-poor language by projecting linguistic information through word alignment links (see eg. (Yarowsky et al., 2001; T¨ackstr¨om et al., 2013) for PoS tagging, (Hwa et al., 2005; Lacroix et al., 2016a) for dependency parsing, (Ehrmann et al., 2011) for Named Entity Recognition, (Kozhevnikov and Titov, 2013) for Semantic Role Labeling, etc.). Implementing this methodology requires the existence of (a) parallel corpora aligned at the word level, and (b) annotation and/or tools on the resource-rich side. However, requirement (a) is somewhat paradoxical: reliable word alignments can only be computed for large-scale parallel corpora, a situation that is unlikely to happen for actual under-resourced languages. In this study, we explore ways to overcome this paradox and consider techniques for transferring alignment models or annotations acros"
W16-1205,P09-1042,0,0.030751,"ropean 36 languages; such projections are less appropriate for fine-grained morphological information such as case or gender (as those distinctions greatly vary across languages), and would be even less so for pairs of languages having antagonist definitions of a word. Furthermore, its success will depend on the density and quality of the alignments (Lacroix et al., 2016b), meaning that it might be more suited to situations in which large bitexts are available. A possible workaround to the noisiness issue is to interpret transferred annotations as soft, rather than hard constraints: see e.g. (Ganchev et al., 2009; Das and Petrov, 2011; Li et al., 2014; Wang and Manning, 2014) for various implementations of this idea; or to combine it with another source of information (T¨ackstr¨om et al., 2013). Alignment projection is not only noisy: it also yields incomplete annotations, requiring methods that learn from partially annotated corpora (Wisniewski et al., 2014). A last strategy worth mentioning here for generating artificial annotations is to use Machine Translation (Tiedemann, 2014). 2.2 Transfer in parameter space The second main family of techniques use the same model for the source and target langua"
W16-1205,W04-3229,0,0.0839814,"Missing"
W16-1205,W11-4615,0,0.0577353,"Missing"
W16-1205,P05-1058,0,0.0443402,"2006)’s works on English-Japanese, using Chinese as a bridge language, but their cross-language word similarity does not exploit Chinese-Japanese linguistic similarity. Finally, in the D IALECT scenario, T is a dialect of T˜, and even though parallel T˜-T data is not necessarily available, the transfer process can rely on the large number of common word forms. This would, for instance, be the case with the alignment of English with MS Arabic and dialects. Thanks to the large linguistic overlap, and contrarily to the previous scenarios, here again methods from the domain adaptation literature (Hua et al., 2005) may also successfully apply. Before closing this section, we would finally like to stress the fact that the motivations for transferring alignments can be many: one might want to get alignments for a small parallel bitext, to then transfer other annotations, or one might want to bootstrap an alignment model with transferred parameters, or even to train a small SMT, etc. Each such motivation may call for different strategies. 4 Methods for transferring alignment In this section, we exemplify with simple systems how general transfer methods can be instantiated for alignment transfer. From now o"
W16-1205,2005.mtsummit-papers.11,0,0.125988,"Missing"
W16-1205,P14-2037,0,0.0498453,"Missing"
W16-1205,S13-1044,0,0.0212217,"esourced to under-resourced languages, speeding up the development of NLP tools for new domains and tasks. Many techniques for transferring knowledge across languages have been proposed in the literature (see § 2 for a brief overview). A widely-used methodology consists in generating automatic annotations for the resource-poor language by projecting linguistic information through word alignment links (see eg. (Yarowsky et al., 2001; T¨ackstr¨om et al., 2013) for PoS tagging, (Hwa et al., 2005; Lacroix et al., 2016a) for dependency parsing, (Ehrmann et al., 2011) for Named Entity Recognition, (Kozhevnikov and Titov, 2013) for Semantic Role Labeling, etc.). Implementing this methodology requires the existence of (a) parallel corpora aligned at the word level, and (b) annotation and/or tools on the resource-rich side. However, requirement (a) is somewhat paradoxical: reliable word alignments can only be computed for large-scale parallel corpora, a situation that is unlikely to happen for actual under-resourced languages. In this study, we explore ways to overcome this paradox and consider techniques for transferring alignment models or annotations across language pairs, a task that has hardly been addressed in l"
W16-1205,D07-1005,0,0.0286754,"test data in L; concatenate with en:L data; train PARAMETER SPACE L GLOSSES -L PARAM -L train an en:L model; apply on test data train an en:L model; apply on test data word-for-word translated in L train an en:L model; translate the parameters; apply on test data Table 1: Summary of proposed methods, for bridge language L. in any scenario: while annotation projection for a monolingual task needs parallel data, for a bilingual model it would require multiparallel data. The converse is not true however, and the M ULTIPARALLEL scenario can be successfully exploited without annotation projection (Kumar et al., 2007). Second, the delexicalized approach causes a chicken-and-egg situation in real-life scenarios. Indeed, when the target language is under-resourced, one cannot assume the availability of a PoS tagger that is needed to compute delexicalized representations. Conversely, methods like (Wisniewski et al., 2014)’s cross-lingual PoS tagger projection and (T¨ackstr¨om et al., 2012)’s clusters are not applicable without a word aligned corpus. Finding common, even coarse-grained, representations then becomes a huge obstacle in many scenarios where alignment transfer is needed, which makes this approach"
W16-1205,N16-1121,1,0.907044,"ng them, cross-lingual learning methods enable to transfer useful supervision information from well-resourced to under-resourced languages, speeding up the development of NLP tools for new domains and tasks. Many techniques for transferring knowledge across languages have been proposed in the literature (see § 2 for a brief overview). A widely-used methodology consists in generating automatic annotations for the resource-poor language by projecting linguistic information through word alignment links (see eg. (Yarowsky et al., 2001; T¨ackstr¨om et al., 2013) for PoS tagging, (Hwa et al., 2005; Lacroix et al., 2016a) for dependency parsing, (Ehrmann et al., 2011) for Named Entity Recognition, (Kozhevnikov and Titov, 2013) for Semantic Role Labeling, etc.). Implementing this methodology requires the existence of (a) parallel corpora aligned at the word level, and (b) annotation and/or tools on the resource-rich side. However, requirement (a) is somewhat paradoxical: reliable word alignments can only be computed for large-scale parallel corpora, a situation that is unlikely to happen for actual under-resourced languages. In this study, we explore ways to overcome this paradox and consider techniques for t"
W16-1205,W16-1203,1,0.894027,"ng them, cross-lingual learning methods enable to transfer useful supervision information from well-resourced to under-resourced languages, speeding up the development of NLP tools for new domains and tasks. Many techniques for transferring knowledge across languages have been proposed in the literature (see § 2 for a brief overview). A widely-used methodology consists in generating automatic annotations for the resource-poor language by projecting linguistic information through word alignment links (see eg. (Yarowsky et al., 2001; T¨ackstr¨om et al., 2013) for PoS tagging, (Hwa et al., 2005; Lacroix et al., 2016a) for dependency parsing, (Ehrmann et al., 2011) for Named Entity Recognition, (Kozhevnikov and Titov, 2013) for Semantic Role Labeling, etc.). Implementing this methodology requires the existence of (a) parallel corpora aligned at the word level, and (b) annotation and/or tools on the resource-rich side. However, requirement (a) is somewhat paradoxical: reliable word alignments can only be computed for large-scale parallel corpora, a situation that is unlikely to happen for actual under-resourced languages. In this study, we explore ways to overcome this paradox and consider techniques for t"
W16-1205,2009.mtsummit-posters.12,0,0.0389405,"Missing"
W16-1205,2010.eamt-1.7,0,0.0387332,"Missing"
W16-1205,N15-1129,0,0.063574,"nting this methodology requires the existence of (a) parallel corpora aligned at the word level, and (b) annotation and/or tools on the resource-rich side. However, requirement (a) is somewhat paradoxical: reliable word alignments can only be computed for large-scale parallel corpora, a situation that is unlikely to happen for actual under-resourced languages. In this study, we explore ways to overcome this paradox and consider techniques for transferring alignment models or annotations across language pairs, a task that has hardly been addressed in literature (see however (Wang et al., 2006; Levinboim and Chiang, 2015)). Based on a high-level typology of cross-lingual transfer methodologies (§ 2), our contribution is to formalize realistic scenarios (defined in § 3) as well as some basic methodologies for projecting knowledge about bilingual alignments crosslinguistically (§ 4). Experiments in § 5 show that, at least for some of these scenarios, simple-minded methods can be surprisingly effective and open a discussion on further prospects and perspectives for future work. 2 Techniques for cross-lingual transfer In this section, we briefly review existing crosslingual transfer techniques for various NLP appl"
W16-1205,C14-1075,0,0.0374239,"Missing"
W16-1205,2006.amta-papers.11,0,0.0743748,"Missing"
W16-1205,D11-1006,0,0.05283,"Missing"
W16-1205,P13-2017,0,0.0605937,"Missing"
W16-1205,H05-1011,0,0.0373811,"el, it is captured by Markovian dependencies between consecutive alignments links. Among these parameters, the translation model is lexicalized in both languages, fertility is lexicalized in the source side and distortion is unlexicalized but rely on word clusters for models 4-6. In all cases, parameters are learned in an unsupervised way using the EM algorithm. Many refinements to these algorithms have been proposed, often to improve computational performance (Dyer et al., 2013). Another line of work tries to improve IBM and HMM models’ low generalization power by using feature-based models (Moore, 2005; Berg-Kirkpatrick et al., 2010). However, the IBM models remain today the most widely used approach both because of their efficiency and because they do not require any annotated data. They will thus serve as our main baseline. 3.2 Real-world situations for alignment transfer Scenarios for improving word alignment with crosslingual transfer fall into two categories, depending on whether the source and target languages play a symmetric role. We first consider the standard symmetric B RIDGE scenario: it involves two languages S and T , for 37 which large bitexts with a ‘bridge’ language B exist"
W16-1205,C00-2163,0,0.231053,"Missing"
W16-1205,J03-1002,0,0.00967054,"s demonstrated the variety of cross-lingual transfer techniques, many of which are borrowed from the domain adaptation literature. The applicability and success of these methods depend on the task and of the available resources. We now explore ways to apply them for the word alignment task. 3 Word alignments: cross-lingual scenarios After a quick review of standard algorithms for word alignment, we present situations in which they can be improved by cross-lingual knowledge. 3.1 Aligning words The most popular models for statistical word alignment are the IBM models 1 to 6 (Brown et al., 1993; Och and Ney, 2003) and the HMM model of Vogel et al. (1996). These probabilistic generative models decompose the probability of an aligned sentence pair as the conjunction of a word translation model, a distortion model (models 2 and up) and a fertility model (models 3 and up). Distortion is absolute for models 2-3 and relative for models 4-6; in the HMM model, it is captured by Markovian dependencies between consecutive alignments links. Among these parameters, the translation model is lexicalized in both languages, fertility is lexicalized in the source side and distortion is unlexicalized but rely on word cl"
W16-1205,N12-1052,0,0.0574107,"Missing"
W16-1205,C14-1175,0,0.0201019,"round to the noisiness issue is to interpret transferred annotations as soft, rather than hard constraints: see e.g. (Ganchev et al., 2009; Das and Petrov, 2011; Li et al., 2014; Wang and Manning, 2014) for various implementations of this idea; or to combine it with another source of information (T¨ackstr¨om et al., 2013). Alignment projection is not only noisy: it also yields incomplete annotations, requiring methods that learn from partially annotated corpora (Wisniewski et al., 2014). A last strategy worth mentioning here for generating artificial annotations is to use Machine Translation (Tiedemann, 2014). 2.2 Transfer in parameter space The second main family of techniques use the same model for the source and target languages: learned parameters in the former can then readily be used for the latter. A first instance of model transfer has already been mentioned: indeed, taking source annotations to supervise the training in target can also be viewed as a (trivial) form of direct model transfer. This approach has been extended in many ways. Cohen et al. (2011) use several source languages and train one delexicalized model in each; the optimal convex combination of these models is used to proce"
W16-1205,C96-2141,0,0.400003,"ual transfer techniques, many of which are borrowed from the domain adaptation literature. The applicability and success of these methods depend on the task and of the available resources. We now explore ways to apply them for the word alignment task. 3 Word alignments: cross-lingual scenarios After a quick review of standard algorithms for word alignment, we present situations in which they can be improved by cross-lingual knowledge. 3.1 Aligning words The most popular models for statistical word alignment are the IBM models 1 to 6 (Brown et al., 1993; Och and Ney, 2003) and the HMM model of Vogel et al. (1996). These probabilistic generative models decompose the probability of an aligned sentence pair as the conjunction of a word translation model, a distortion model (models 2 and up) and a fertility model (models 3 and up). Distortion is absolute for models 2-3 and relative for models 4-6; in the HMM model, it is captured by Markovian dependencies between consecutive alignments links. Among these parameters, the translation model is lexicalized in both languages, fertility is lexicalized in the source side and distortion is unlexicalized but rely on word clusters for models 4-6. In all cases, para"
W16-1205,Q14-1005,0,0.0191771,"r fine-grained morphological information such as case or gender (as those distinctions greatly vary across languages), and would be even less so for pairs of languages having antagonist definitions of a word. Furthermore, its success will depend on the density and quality of the alignments (Lacroix et al., 2016b), meaning that it might be more suited to situations in which large bitexts are available. A possible workaround to the noisiness issue is to interpret transferred annotations as soft, rather than hard constraints: see e.g. (Ganchev et al., 2009; Das and Petrov, 2011; Li et al., 2014; Wang and Manning, 2014) for various implementations of this idea; or to combine it with another source of information (T¨ackstr¨om et al., 2013). Alignment projection is not only noisy: it also yields incomplete annotations, requiring methods that learn from partially annotated corpora (Wisniewski et al., 2014). A last strategy worth mentioning here for generating artificial annotations is to use Machine Translation (Tiedemann, 2014). 2.2 Transfer in parameter space The second main family of techniques use the same model for the source and target languages: learned parameters in the former can then readily be used f"
W16-1205,P06-2112,0,0.122955,"ing, etc.). Implementing this methodology requires the existence of (a) parallel corpora aligned at the word level, and (b) annotation and/or tools on the resource-rich side. However, requirement (a) is somewhat paradoxical: reliable word alignments can only be computed for large-scale parallel corpora, a situation that is unlikely to happen for actual under-resourced languages. In this study, we explore ways to overcome this paradox and consider techniques for transferring alignment models or annotations across language pairs, a task that has hardly been addressed in literature (see however (Wang et al., 2006; Levinboim and Chiang, 2015)). Based on a high-level typology of cross-lingual transfer methodologies (§ 2), our contribution is to formalize realistic scenarios (defined in § 3) as well as some basic methodologies for projecting knowledge about bilingual alignments crosslinguistically (§ 4). Experiments in § 5 show that, at least for some of these scenarios, simple-minded methods can be surprisingly effective and open a discussion on further prospects and perspectives for future work. 2 Techniques for cross-lingual transfer In this section, we briefly review existing crosslingual transfer te"
W16-1205,D14-1187,1,0.898649,"Missing"
W16-1205,H01-1035,0,0.0456015,"an effective way to remedy, at least partially, to this unsatisfactory situation. Among them, cross-lingual learning methods enable to transfer useful supervision information from well-resourced to under-resourced languages, speeding up the development of NLP tools for new domains and tasks. Many techniques for transferring knowledge across languages have been proposed in the literature (see § 2 for a brief overview). A widely-used methodology consists in generating automatic annotations for the resource-poor language by projecting linguistic information through word alignment links (see eg. (Yarowsky et al., 2001; T¨ackstr¨om et al., 2013) for PoS tagging, (Hwa et al., 2005; Lacroix et al., 2016a) for dependency parsing, (Ehrmann et al., 2011) for Named Entity Recognition, (Kozhevnikov and Titov, 2013) for Semantic Role Labeling, etc.). Implementing this methodology requires the existence of (a) parallel corpora aligned at the word level, and (b) annotation and/or tools on the resource-rich side. However, requirement (a) is somewhat paradoxical: reliable word alignments can only be computed for large-scale parallel corpora, a situation that is unlikely to happen for actual under-resourced languages. I"
W16-1205,I08-3008,0,0.481971,"of the presentation, the resource-rich is viewed as the source language, and the resource-poor is accordingly the target language. 2.1 Transfer in data space This family of techniques seeks to automatically supply the annotations that are lacking on the target side, so that a model can be learned on these artificially generated data. Direct Transfer Two main lines of reasoning have been considered: the first assumes that the source and target languages are sufficiently similar, to the point that source annotations can be readily used to train a model in the target language (Hana et al., 2004; Zeman and Resnik, 2008). When such assumption does not hold, a necessary preliminary step will be to map the source and target data in a shared representation space: delexicalization, i.e. the replacement of words with (universal) PoS (McDonald et al., 2013) readily yields such mappings (Wisniewski et al., 2014), but it is also conceivable to consider automatically inferred multilingual representations (Jagarlamudi et al., 2011; Koˇcisk´y et al., 2014; Gouws et al., 2015). This simple approach has one downside: learning can only use features based on this inter-lingual representation – in particular this makes it im"
W16-1205,J07-3002,0,\N,Missing
W16-1205,D11-1086,0,\N,Missing
W16-1205,Q13-1001,0,\N,Missing
W16-2304,D12-1133,0,0.0799569,"Missing"
W16-2304,J04-2004,0,0.202109,"006a). Tuples are then extracted in such a way that a unique segmentation of the bilingual corpus is achieved (Mari˜no et al., 2006) and n-gram translation models are then estimated over the training corpus composed of tuple sequences made of surface forms or POS tags. Reordering rules are automatically learned during the unfolding procedure and are built using partof-speech (POS), rather than surface word forms, to increase their generalization power (Crego and Mari˜no, 2006a). 2.3 2.4 2.2 Language modelling and domain adaptation N CODE N CODE implements the bilingual n-gram approach to SMT (Casacuberta and Vidal, 2004; Crego and Mari˜no, 2006b; Mari˜no et al., 2006) that is closely related to the standard phrase-based approach (Zens et al., 2002). In this framework, the translation is divided into two steps. To translate a source sentence f into a target sentence e, Continuous-space models Neural networks, working on top of conventional n-gram back-off language models, have been introduced in (Bengio et al., 2003; Schwenk et al., 2006) as a potential means to improve conventional language models. More recently, these techniques have been applied to statistical machine translation in order to estimate conti"
W16-2304,N12-1047,0,0.110665,"to be too large to be handled by the CRF, so the following experiments were performed on the 300-best output. • words are projected into a 500-dimensional vector space; • the feed-forward architecture includes two hidden layers of size 1000 and 500; • the non-linearity is a sigmoid function; All models are trained for 20 epochs, then the selection relies on the perplexity measured on a validation set. For CTMs, the validation sets are sampled from the parallel training data. 3 Development and test sets Experiments For all our experiments, the MT systems are tuned using the kb-mira algorithm (Cherry and Foster, 2012) implemented in M OSES, including the reranking step. POS tagging is performed using the TreeTagger (Schmid, 1994) for English and Russian (Sharoff and Nivre, 2011), and TTL (Tufis¸ et al., 2008) for Romanian. 4 241 http://pymorphy.readthedocs.io/ In order to train this model, we split the parallel data in two parts. The first (largest) part was used to train the translation system baseline. The second part was used for the training of the hidden CRF. First, the source side was translated with the baseline system, then the resulting output was extended (paradigm generation). References were ob"
W16-2304,W08-0310,1,0.725891,"Missing"
W16-2304,L16-1241,1,0.787062,"ns in Romanian and Russian corpora is a lot higher than in English corpora. In such a situation, surface heuristics are less reliable. In order to address this limitation, we tried to extend the output of the decoder with morphological variations of nouns, pronouns and adjectives. Therefore, for each word in the output baring one of these PoS-tags, we introduced all forms in the paradigm as possible alternatives. The paradigm generation was performed for Russian using pymorphy, a dictionary implemented as a Python module.4 For Romanian, we used the crawled (thus sparse) lexicon introduced in (Aufrant et al., 2016). Once the outputs were extended, we used a CRF model to rescore this new search space. The CRF can use the features of the MT decoder, but can also include morphological or syntactic features in order to estimate output scores, even for words that were not observed in the training data. In the Russian experiment, oracle scores show that a maximum gain of 6.3 BLEU points can be obtained if the extension is performed on the full search space and 2.3 BLEU points on 300-best output of the N CODE decoder. The full search space, while being more promising, proved to be too large to be handled by th"
W16-2304,P14-1129,0,0.110474,"Missing"
W16-2304,W11-2123,0,0.0412066,"s then translated. Since the translation step is monotonic, the peculiarity of this approach is to rely on the n-gram assumption to decompose the joint probability of a sentence pair in a sequence of bilingual units called tuples. ∗ e = arg max e,a K X λk fk (f, e, a) k=1 Various English, Romanian and Russian language models (LM) were trained on the in-domain monolingual corpora, a subset of the commoncrawl corpora and the relevant side of the parallel corpora (for English, the English side of the Czech-English parallel data was used). We trained 4-gram LMs, pruning all singletons with lmplz (Heafield, 2011). In addition to in-domain monolingual data, a considerable amount of out-of-domain data was provided this year, gathered in the common-crawl corpora. Instead of directly training an LM on these corpora, we extracted from them in-domain sentences using the Moore-Lewis (Moore and Lewis, 2010) filtering method, more specifically its implementation in XenC (Rousseau, 2013). As a result, the common-crawl sub-corpora we have used contained about 200M sentences for Romanian and 300M for Russian and English. Finally, we perform a linear interpolation of these models, using the SRILM toolkit (Stolcke,"
W16-2304,P07-2045,0,0.00443495,"translation into Russian and Romanian, we have attempted to extend the output of the decoder with morphological variations and to use a CRF model to rescore this new search space; as for the translation into German, we have been experimenting with source-side pre-ordering based on a dependency structure allowing permutations in order to reproduce the target word order. 1 2 System Overview Our experiments mainly use N CODE,1 an open source implementation of the n-gram approach, as well as M OSES2 for some contrastive experiments. For more details about these toolkits, the reader can refer to (Koehn et al., 2007) for M OSES and to (Crego et al., 2011) for N CODE. 2.1 Data pre-processing and word alignments All the English and Russian data have been cleaned by normalizing character encoding. Tokenization for English text relies on in-house text processing tools (D´echelotte et al., 2008). For the Russian corpora, we used the TreeTagger tokenizer. For Romanian, we developed and used tokro, a rule-based tokenizer. After normalization of diacritics, it repeatedly applies 3 rules: (a) word splitting on slashes, except for url addresses, (b) isolation of punctuation characters from a predefined set (includi"
W16-2304,F13-1033,1,0.854254,"ssian (Sharoff and Nivre, 2011), and TTL (Tufis¸ et al., 2008) for Romanian. 4 241 http://pymorphy.readthedocs.io/ In order to train this model, we split the parallel data in two parts. The first (largest) part was used to train the translation system baseline. The second part was used for the training of the hidden CRF. First, the source side was translated with the baseline system, then the resulting output was extended (paradigm generation). References were obtained by searching for oracle translations in the augmented output. Models were trained using inhouse implementation of hidden CRF (Lavergne et al., 2013) and used features from the decoder as well as additional ones: unigram and bigram of words and POS-tags; number, gender and case of the forms and of the surrounding ones; and information about nearest prepositions and verbs. 3.3 coding it. Reorderings of the source sentence are compactly encoded in a permutation lattice generated by iteratively applying POS-based reordering rules extracted from the parallel data. In this year’s WMT evaluation campaign we investigated ways to improve the re-ordering step by re-implementing the approach proposed by (Lerner and Petrov, 2013). This approach aims"
W16-2304,P06-2093,0,0.0731995,"Missing"
W16-2304,D07-1045,0,0.394133,"Missing"
W16-2304,N12-1005,1,0.933551,"sely related to the standard phrase-based approach (Zens et al., 2002). In this framework, the translation is divided into two steps. To translate a source sentence f into a target sentence e, Continuous-space models Neural networks, working on top of conventional n-gram back-off language models, have been introduced in (Bengio et al., 2003; Schwenk et al., 2006) as a potential means to improve conventional language models. More recently, these techniques have been applied to statistical machine translation in order to estimate continuous-space translation models (CTMs) (Schwenk et al., 2007; Le et al., 2012a; Devlin et al., 2014). 3 Bilingual Sentence Aligner, available at http:// research.microsoft.com/apps/catalog/ 240 3.1 As in our previous participations (Le et al., 2012b; Allauzen et al., 2013; P´echeux et al., 2014; Marie et al., 2015), we take advantage of the proposal of (Le et al., 2012a). Using a specific neural network architecture, the Structured OUtput Layer (SOUL), it becomes possible to estimate n-gram models that use large output vocabulary, thereby making the training of large neural network language models feasible both for target language models (Le et al., 2011) and translati"
W16-2304,D13-1049,0,0.0232522,"ntation of hidden CRF (Lavergne et al., 2013) and used features from the decoder as well as additional ones: unigram and bigram of words and POS-tags; number, gender and case of the forms and of the surrounding ones; and information about nearest prepositions and verbs. 3.3 coding it. Reorderings of the source sentence are compactly encoded in a permutation lattice generated by iteratively applying POS-based reordering rules extracted from the parallel data. In this year’s WMT evaluation campaign we investigated ways to improve the re-ordering step by re-implementing the approach proposed by (Lerner and Petrov, 2013). This approach aims at taking advantage of the dependency structure of the source sentence to predict a permutation of the source words that is as close as possible to a correct syntactic word order in the target language: starting from the root of the dependency tree a classifier is used to recursively predict the order of a node and all its children. More precisely, for a family5 of size n, a multiclass classifier is used to select the best ordering of this family among its n! permutations. A different classifier is trained for each possible family size. Experimental results The experimenta"
W16-2304,N04-4026,0,0.0644058,"e used contained about 200M sentences for Romanian and 300M for Russian and English. Finally, we perform a linear interpolation of these models, using the SRILM toolkit (Stolcke, 2002). where K feature functions (fk ) are weighted by a set of coefficients (λk ) and a denotes the set of hidden variables corresponding to the reordering and segmentation of the source sentence. Along with the n-gram translation models and target ngram language models, 13 conventional features are combined: 4 lexicon models similar to the ones used in standard phrase-based systems; 6 lexicalized reordering models (Tillmann, 2004; Crego et al., 2011) aimed at predicting the orientation of the next translation unit; a “weak” distance-based distortion model; and finally a word-bonus model and a tuple-bonus model which compensate for the system preference for short translations. Features are estimated during the training phase. Training source sentences are first reordered so as to match the target word order by unfolding the word alignments (Crego and Mari˜no, 2006a). Tuples are then extracted in such a way that a unique segmentation of the bilingual corpus is achieved (Mari˜no et al., 2006) and n-gram translation model"
W16-2304,W15-3016,1,0.856173,"Missing"
W16-2304,tufis-etal-2008-racais,0,0.271893,"Missing"
W16-2304,J06-4004,0,0.296295,"Missing"
W16-2304,2002.tmi-tutorials.2,0,0.147945,"ram translation models are then estimated over the training corpus composed of tuple sequences made of surface forms or POS tags. Reordering rules are automatically learned during the unfolding procedure and are built using partof-speech (POS), rather than surface word forms, to increase their generalization power (Crego and Mari˜no, 2006a). 2.3 2.4 2.2 Language modelling and domain adaptation N CODE N CODE implements the bilingual n-gram approach to SMT (Casacuberta and Vidal, 2004; Crego and Mari˜no, 2006b; Mari˜no et al., 2006) that is closely related to the standard phrase-based approach (Zens et al., 2002). In this framework, the translation is divided into two steps. To translate a source sentence f into a target sentence e, Continuous-space models Neural networks, working on top of conventional n-gram back-off language models, have been introduced in (Bengio et al., 2003; Schwenk et al., 2006) as a potential means to improve conventional language models. More recently, these techniques have been applied to statistical machine translation in order to estimate continuous-space translation models (CTMs) (Schwenk et al., 2007; Le et al., 2012a; Devlin et al., 2014). 3 Bilingual Sentence Aligner,"
W16-2304,P10-2041,0,0.0322506,"ish, Romanian and Russian language models (LM) were trained on the in-domain monolingual corpora, a subset of the commoncrawl corpora and the relevant side of the parallel corpora (for English, the English side of the Czech-English parallel data was used). We trained 4-gram LMs, pruning all singletons with lmplz (Heafield, 2011). In addition to in-domain monolingual data, a considerable amount of out-of-domain data was provided this year, gathered in the common-crawl corpora. Instead of directly training an LM on these corpora, we extracted from them in-domain sentences using the Moore-Lewis (Moore and Lewis, 2010) filtering method, more specifically its implementation in XenC (Rousseau, 2013). As a result, the common-crawl sub-corpora we have used contained about 200M sentences for Romanian and 300M for Russian and English. Finally, we perform a linear interpolation of these models, using the SRILM toolkit (Stolcke, 2002). where K feature functions (fk ) are weighted by a set of coefficients (λk ) and a denotes the set of hidden variables corresponding to the reordering and segmentation of the source sentence. Along with the n-gram translation models and target ngram language models, 13 conventional fe"
W17-0419,W16-1202,0,0.455575,"Missing"
W17-0419,W08-1301,0,0.157196,"Missing"
W17-0419,N13-1070,0,0.0378335,"Missing"
W17-0419,E17-2001,0,0.684916,"on scheme on the performance of transferred parsers. It compares the Prague annotation style used in the HamleDT (Zeman et al., 2014) with the Stanford style (De Marneffe and Manning, 2008) that has inspired the UD guidelines and shows that Prague style results in better parsing performance. Nevertheless — with a particular focus on the adposition attachment case — the Stanford style is advantageous for delexicalized parsing transfer. Finally, (Silveira and Manning, 2015) performs an analysis very similar to ours and find that, for English, UD is a good parsing representation. More recently, (Kohita et al., 2017) shows that it is possible to improve parsing performance for a wide array of language by converting the dependency structure back-and-forth. 3 3.2 However, more than two tokens are frequently involved in the sub-structure carried by the dependency in question. In that case, the conversion may create non-projective dependencies (i.e. crossing between dependencies). Figure 1 illustrates this problem. Let wi y w j be the original dependency we want to invert, wi being the head and w j the dependent. If the head wi has a child wk , i.e. there is a wk such as wi y wk , and the tokens are ordered s"
W17-0419,D07-1013,0,0.218532,"Missing"
W17-0419,P80-1024,0,0.764577,"Missing"
W17-0419,W03-3017,0,0.197365,"Missing"
W17-0419,W15-2131,0,0.105179,"ced by wi x w j , ii) the former head of wi , named wh , become the new head of w j . These transformations applies to relations such as the clause subordinates (mark), the determiners (det) or the case markings (case). studies the representation of verbal constructions to see if parsing works better when auxiliaries are the head of auxiliary dependency relations, which is not the case in UD. They highlight that the parsing benefits from the disambiguation of PoS tags for main verbs and auxiliaries in UD PoS tagset even if the overall parsing accuracy decreases. To the best of our knowledge, (Rosa, 2015) is the only work to study the impact of the annotation scheme on the performance of transferred parsers. It compares the Prague annotation style used in the HamleDT (Zeman et al., 2014) with the Stanford style (De Marneffe and Manning, 2008) that has inspired the UD guidelines and shows that Prague style results in better parsing performance. Nevertheless — with a particular focus on the adposition attachment case — the Stanford style is advantageous for delexicalized parsing transfer. Finally, (Silveira and Manning, 2015) performs an analysis very similar to ours and find that, for English,"
W17-0419,P06-1033,0,0.0766535,"et al., 2013). They aim at harmonizing annotation schemes (at the level of PoS-tags and dependencies) between languages by converting existing treebanks to the new scheme. These works have led to the creation of the Universal Dependencies (UD) project (Nivre et al., 2016) that gathers treebanks for more than 45 languages (v1.3). The UD annotation scheme has been designed to facilitate the transfer of annotations across languages: similar syntactic relations are represented by similar syntactic structures in different languages, and relations tend to hold between content 2 Related Work Since (Nilsson et al., 2006) many works have shown that well-chosen transformations of syntactic representations can greatly improve the parsing accuracy achieved by dependency parsers. (Schwartz et al., 2012) shows that “selecting one representation over another may affect parsing performance”. Focusing on English, they compare parsing performance through several alternatives and conclude that parsers prefer attachment via function word over content-word attachments. They argue that the learnability of a representation, estimated by the accuracy within this representation is a good criterion for selecting a syntactic re"
W17-0419,C12-1147,0,0.498488,"Missing"
W17-0419,W15-2134,0,0.327044,"tagset even if the overall parsing accuracy decreases. To the best of our knowledge, (Rosa, 2015) is the only work to study the impact of the annotation scheme on the performance of transferred parsers. It compares the Prague annotation style used in the HamleDT (Zeman et al., 2014) with the Stanford style (De Marneffe and Manning, 2008) that has inspired the UD guidelines and shows that Prague style results in better parsing performance. Nevertheless — with a particular focus on the adposition attachment case — the Stanford style is advantageous for delexicalized parsing transfer. Finally, (Silveira and Manning, 2015) performs an analysis very similar to ours and find that, for English, UD is a good parsing representation. More recently, (Kohita et al., 2017) shows that it is possible to improve parsing performance for a wide array of language by converting the dependency structure back-and-forth. 3 3.2 However, more than two tokens are frequently involved in the sub-structure carried by the dependency in question. In that case, the conversion may create non-projective dependencies (i.e. crossing between dependencies). Figure 1 illustrates this problem. Let wi y w j be the original dependency we want to in"
W17-0419,P11-2033,0,0.243775,"Missing"
W17-4779,N13-1073,0,0.0766947,"Koehn et al., 2007): all corpora are cleaned2 and tokenized; compounds are split on the German side using our re-implementation of (Koehn and Knight, 2003). Parallel data are • UCB1- SAMPLING in which two more MT systems are considered (in addition to the 2 Moses scripts are applied in the following order to clean corpora: removing non-printing characters, replacing and normalizing Unicode punctuation, lowercasing, pretokenizing. 1 A policy is a randomized algorithm which makes a decision in each round based on the history of decisions and observed rewards so far 676 aligned using FASTA LIGN (Dyer et al., 2013) and 5-gram language models is estimated using K EN LM (Heafield et al., 2013). The language model is estimated on the monolingual corpus resulting from the concatenation of the E UROPARL (v7), N EWS C OMMENTARY (v12) and N EWS D ISCUSS (2015–2016) corpora. At the end, our monolingual corpus contain 193,292,548 sentences. The translation model is estimated from the CommonCrawl, NewsCo, Europarl and Rapid corpora, resulting in a parallel corpus made of 5,919,142 sentences. Weights of the MT systems are estimated with MERT on newstest-2016. 4.2 Strategy S EED UCB1 UCB1- SAMPLING UCB1- SELECT 697"
W17-4779,P13-2121,0,0.0416687,"Missing"
W17-4779,P07-2045,0,0.00482449,"117 , bt . 1.0 Table 1: Example of a ‘good’ translation with very bad rewards and of a perfect translation. Variants After analyzing our results on the development set (see §5), we decide to consider two more strategies: 4 • UCB1- SELECT that considers the same systems as the UCB1 strategy but only the translation hypothesis associated to a reward r in [0.1, 1[ are considered to estimate the weights of the A DAPTED system (other observations are discarded); 4.1 Experimental Details The S EED System We consider as our S EED system a phrasebased system trained using the standard Moses pipeline (Koehn et al., 2007): all corpora are cleaned2 and tokenized; compounds are split on the German side using our re-implementation of (Koehn and Knight, 2003). Parallel data are • UCB1- SAMPLING in which two more MT systems are considered (in addition to the 2 Moses scripts are applied in the following order to clean corpora: removing non-printing characters, replacing and normalizing Unicode punctuation, lowercasing, pretokenizing. 1 A policy is a randomized algorithm which makes a decision in each round based on the history of decisions and observed rewards so far 676 aligned using FASTA LIGN (Dyer et al., 2013)"
W17-4779,E03-1076,0,0.0778215,"our results on the development set (see §5), we decide to consider two more strategies: 4 • UCB1- SELECT that considers the same systems as the UCB1 strategy but only the translation hypothesis associated to a reward r in [0.1, 1[ are considered to estimate the weights of the A DAPTED system (other observations are discarded); 4.1 Experimental Details The S EED System We consider as our S EED system a phrasebased system trained using the standard Moses pipeline (Koehn et al., 2007): all corpora are cleaned2 and tokenized; compounds are split on the German side using our re-implementation of (Koehn and Knight, 2003). Parallel data are • UCB1- SAMPLING in which two more MT systems are considered (in addition to the 2 Moses scripts are applied in the following order to clean corpora: removing non-printing characters, replacing and normalizing Unicode punctuation, lowercasing, pretokenizing. 1 A policy is a randomized algorithm which makes a decision in each round based on the history of decisions and observed rewards so far 676 aligned using FASTA LIGN (Dyer et al., 2013) and 5-gram language models is estimated using K EN LM (Heafield et al., 2013). The language model is estimated on the monolingual corpus"
W17-4779,J16-1001,0,0.0272064,"ow describes the two components of our system: the first one (§3.1) will allow us to exploit the weak and partial feedback we receive and the second one (§3.2) will allow to discover the MT system that translates in-domain data the best. 3.1 X (ri −w·φ(hi , xi ))2 +λ2 ·||w||22 +λ1 ·||w||1 Optimizing a MT System from Weak Feedback Estimating the parameters of a MT system from the rewards can not be done with the usual MT optimization methods: as the reference is not known, it is impossible to score a n-best list as required by methods optimizing a classification criterion such as MERT or MIRA (Neubig and Watanabe, 2016). Moreover, as only one translation hypothesis is scored, methods optimizing a ranking criterion, such as PRO, can also not be used. Instead we propose to simply learn a linear regression to predict the reward a translation hypothesis will get based on a joint feature representation φ(hi , xi ) of the hypothesis and the source sentence. Using a linear model allows us to easily integrate it into the decoder to score translation hypotheses: given a weight vector w, translating a source sentence x consists in looking, in the search space, for the hypothesis that maximizes the predicted reward, wh"
W17-4779,W17-4756,0,0.0516982,"Missing"
W17-4779,D10-1091,1,0.82666,"same sentence can not be translated twice and no reference is ever revealed. 2 Task Description Bandit learning for MT follows an online learning protocol: at the i-th iteration, a new source sentence xi is received; the learner translates it and gets a reward ri ∈ [0, 1] (a smoothed sentencelevel BLEU score in this shared task). The higher the reward, the better the translation but no infor674 Proceedings of the Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 674–679 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics graph (Wisniewski et al., 2010; Wisniewski and Yvon, 2013). More precisely the weights of the MT system are chosen by optimizing the regularized mean squared error (MSE): mation about the actual reference is available. The goal of the task is to maximize the cumulative rePT ward over T rounds: i=1 ri . Maximizing the cumulative reward faces an exploration/exploitation dilemma: if all the sentences are translated using the seed system (i.e. a system trained on out-domain data), the specificities of the domain will never be taken into account and only ‘average’ translations will be predicted (assuming the seed system is ‘goo"
W19-6101,W19-4822,0,0.287041,"aracter-based representations were more robust to synthetic and natural noise than word-based approaches. However, they did not find a substantial improvement over BPE tokenization, their BPE MT system even slightly outperforming the character-based one on 3 out of 4 of their test sets, including the one with the highest OOV rate. Similarly to all these works, we also aim at comparing the performance of PBSMT and NMT approaches, hoping that the peculiarities of UGC will help us to better understand the pros and cons of these two methods. Our approach shares several similarity with the work of Anastasopoulos (2019) that described different experiments to determine how source-side errors can impact the translation quality of NMT models. 3 Experimental Setup As the goal of this work is to compare the output of NMT and PBSMT when translating UGC corpora. Because of the lack of manually translated UGC, we consider a out-domain scenario in which our systems are trained on the canonical corpora generally used in MT evaluation campaigns and tested on UGC data. We will first describe the datasets used in this work (§3.1), then the different systems we have considered (§3.2) and finally the pre- and post-process"
W19-6101,P17-1080,0,0.0302827,"he-box NMT system in a low-resource setting (English-Irish). These conclusions have recently been questioned by Sennrich and Zhang (2019) who showed NMT could achieve good performance in low-resource scenario when all hyper-parameters (size of the bytepair encoding (BPE) vocabulary, number of hidden units, batch size, ...) are correctly tuned and a proper NMT architecture is selected. The situation for other NMT approaches, such as character-based NMT, is also confusing: Wu et al. (2016) have shown that character-based methods achieve state-of-the-art performance for different language pairs; Belinkov et al. (2017) and Durrani et al. (2019) have demonstrated their systems respective abilities to retrieve good amount of morphological information leveraging on subword level features. However, Belinkov and Bisk (2018) found that these approaches are not robust to noise (both synthetic and natural) when trained only with clean corpora. On the other hand, Durrani et al. (2019) concluded that character-based representations were more robust to synthetic and natural noise than word-based approaches. However, they did not find a substantial improvement over BPE tokenization, their BPE MT system even slightly ou"
W19-6101,D16-1025,0,0.178455,"s of UGC that are problematic for sequential NMT architectures and suggests new avenue for improving NMT models. Introduction1 1 Neural Machine Translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014a; Cho et al., 2014) and, more specifically, attention-based models (Bahdanau et al., 2015; Jean et al., 2015; Luong et al., 2015; Mi et al., 2016) have recently become the method of choice for machine translation: many works have shown that Neural Machine Translation (NMT) outperforms classic PhraseBased Statistical Machine Translation (PBSMT) approaches over a wide array of datasets (Bentivogli et al., 2016; Dowling et al., 2018; Koehn and Knowles, 2017). Indeed, NMT provides better generalization and accuracy capabilities (Bojar et al., 2016; Bentivogli et al., 2016; Castilho et al., 2017) even if it has well-identified limits such as over-translating and dropping translations (Mi et al., 2016; Koehn and Knowles, 2017; Le et al., 2017). This work aims at studying how these interactions impact machine translation of noisy texts 1 We thank our anonymous reviewers for their insightful comments. This work was funded by the ANR ParSiTi project (ANR-16-CE33-0021). as generally found in social media a"
W19-6101,N19-1389,0,0.0270559,"the smaller training sets when used with the neural models, our results suggest that the specificities of UGC raise new challenges for NMT systems that cannot simply be solved by feeding ours models more data. Nevertheless, Koehn and Knowles (2017) highlighted 6 challenges faced by Neural Machine Translation, one of them being the lack of data for resource poordomain. This issue is strongly emphasized when it comes to UGC which does not constitute a domain on its own and which is subjected to a degree of variability only seen in the processing historical document over a large period of times (Bollmann, 2019) or in emerging dialects which can greatly varies over geographic or socio-demographic factors (transliterated Arabic dialects for example). This is why the availability of new UGC data sets is crucial and as such the release of the Cr#pbank is a welcome, small, stone in the edifice that will help evaluating machine translation architectures in near-real conditions such as blind testing. In order to avoid common leaderboard pitfalls in such settings, we did not use the Cr#pbank’s blind test set for any of our experiments, neither did we for the MTNT validation test. Nevertheless, evaluating mo"
W19-6101,W14-4012,0,0.159211,"Missing"
W19-6101,W18-2202,0,0.140927,"atic for sequential NMT architectures and suggests new avenue for improving NMT models. Introduction1 1 Neural Machine Translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014a; Cho et al., 2014) and, more specifically, attention-based models (Bahdanau et al., 2015; Jean et al., 2015; Luong et al., 2015; Mi et al., 2016) have recently become the method of choice for machine translation: many works have shown that Neural Machine Translation (NMT) outperforms classic PhraseBased Statistical Machine Translation (PBSMT) approaches over a wide array of datasets (Bentivogli et al., 2016; Dowling et al., 2018; Koehn and Knowles, 2017). Indeed, NMT provides better generalization and accuracy capabilities (Bojar et al., 2016; Bentivogli et al., 2016; Castilho et al., 2017) even if it has well-identified limits such as over-translating and dropping translations (Mi et al., 2016; Koehn and Knowles, 2017; Le et al., 2017). This work aims at studying how these interactions impact machine translation of noisy texts 1 We thank our anonymous reviewers for their insightful comments. This work was funded by the ANR ParSiTi project (ANR-16-CE33-0021). as generally found in social media and web forums and ofte"
W19-6101,N19-1154,0,0.025811,"resource setting (English-Irish). These conclusions have recently been questioned by Sennrich and Zhang (2019) who showed NMT could achieve good performance in low-resource scenario when all hyper-parameters (size of the bytepair encoding (BPE) vocabulary, number of hidden units, batch size, ...) are correctly tuned and a proper NMT architecture is selected. The situation for other NMT approaches, such as character-based NMT, is also confusing: Wu et al. (2016) have shown that character-based methods achieve state-of-the-art performance for different language pairs; Belinkov et al. (2017) and Durrani et al. (2019) have demonstrated their systems respective abilities to retrieve good amount of morphological information leveraging on subword level features. However, Belinkov and Bisk (2018) found that these approaches are not robust to noise (both synthetic and natural) when trained only with clean corpora. On the other hand, Durrani et al. (2019) concluded that character-based representations were more robust to synthetic and natural noise than word-based approaches. However, they did not find a substantial improvement over BPE tokenization, their BPE MT system even slightly outperforming the character-"
W19-6101,N13-1073,0,0.0310069,"hypothesis. This is done in the Moses toolkit (using the alignments produced during translation) and in OpenNMT (that uses the soft-alignments to copy the source token with the highest attention weight at every decoding step when necessary). At the time we conducted the MT experiments, the XNMT toolkit (Neubig et al., 2018) has no straightforward possibilities of replacing unknown tokens present in the test set.6 For our seq2seq NMT predictions, we performed such replacement through aligning the translation hypothesis with the source sentences (both already tokenized with BPE) with fastalign (Dyer et al., 2013) and copying the source words aligned with the &lt;UNK&gt; token. 4 Measuring noise levels as corpus divergence Several metrics have been proposed to quantify the domain drift between two corpora. In particular, the perplexity of a language model the KLdivergence between the character-level 3-gram distribution of the train and test sets were two useful measurements capable of estimating the noiselevel of UGC corpora as shown respectively by Martínez Alonso et al. (2016) and Seddah et al. (2012). We also propose a new metric to estimate the noise level tailored to the BPE tokenization. The BPE stabil"
W19-6101,N13-1037,0,0.334693,"imits such as over-translating and dropping translations (Mi et al., 2016; Koehn and Knowles, 2017; Le et al., 2017). This work aims at studying how these interactions impact machine translation of noisy texts 1 We thank our anonymous reviewers for their insightful comments. This work was funded by the ANR ParSiTi project (ANR-16-CE33-0021). as generally found in social media and web forums and often denoted as User Generated Content (UGC). Given the increasing importance of social medias, this type of texts has been extensively studied over the years, e.g. (Foster, 2010; Seddah et al., 2012; Eisenstein, 2013). In this work we focus on UGC in which no grammatical, orthographic or coherence rules are respected, other than those considered by the writer. Such rule-free environment promotes a plethora of vocabulary and grammar variations, which account for the large increase of out-of-vocabulary tokens (OOVs) in UGC corpora with respect to canonical parallel training data. Translating UGC raises several challenges as it corresponds to both a low-resource scenario — producing parallel UGC corpora is very costly and often problematic due to inconsistencies between translators — and a domain adaptation s"
W19-6101,N10-1060,0,0.317459,"7) even if it has well-identified limits such as over-translating and dropping translations (Mi et al., 2016; Koehn and Knowles, 2017; Le et al., 2017). This work aims at studying how these interactions impact machine translation of noisy texts 1 We thank our anonymous reviewers for their insightful comments. This work was funded by the ANR ParSiTi project (ANR-16-CE33-0021). as generally found in social media and web forums and often denoted as User Generated Content (UGC). Given the increasing importance of social medias, this type of texts has been extensively studied over the years, e.g. (Foster, 2010; Seddah et al., 2012; Eisenstein, 2013). In this work we focus on UGC in which no grammatical, orthographic or coherence rules are respected, other than those considered by the writer. Such rule-free environment promotes a plethora of vocabulary and grammar variations, which account for the large increase of out-of-vocabulary tokens (OOVs) in UGC corpora with respect to canonical parallel training data. Translating UGC raises several challenges as it corresponds to both a low-resource scenario — producing parallel UGC corpora is very costly and often problematic due to inconsistencies between"
W19-6101,P15-1001,0,0.02288,"ral Machine Translation systems (NMT) when translating User Generated Content (UGC), as encountered in social medias, from French to English. We show that, contrary to what could be expected, PBSMT outperforms NMT when translating non-canonical inputs. Our error analysis uncovers the specificities of UGC that are problematic for sequential NMT architectures and suggests new avenue for improving NMT models. Introduction1 1 Neural Machine Translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014a; Cho et al., 2014) and, more specifically, attention-based models (Bahdanau et al., 2015; Jean et al., 2015; Luong et al., 2015; Mi et al., 2016) have recently become the method of choice for machine translation: many works have shown that Neural Machine Translation (NMT) outperforms classic PhraseBased Statistical Machine Translation (PBSMT) approaches over a wide array of datasets (Bentivogli et al., 2016; Dowling et al., 2018; Koehn and Knowles, 2017). Indeed, NMT provides better generalization and accuracy capabilities (Bojar et al., 2016; Bentivogli et al., 2016; Castilho et al., 2017) even if it has well-identified limits such as over-translating and dropping translations (Mi et al., 2016; Ko"
W19-6101,D13-1176,0,0.0522845,"a.fr Abstract This work compares the performances achieved by Phrase-Based Statistical Machine Translation systems (PBSMT) and attention-based Neural Machine Translation systems (NMT) when translating User Generated Content (UGC), as encountered in social medias, from French to English. We show that, contrary to what could be expected, PBSMT outperforms NMT when translating non-canonical inputs. Our error analysis uncovers the specificities of UGC that are problematic for sequential NMT architectures and suggests new avenue for improving NMT models. Introduction1 1 Neural Machine Translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014a; Cho et al., 2014) and, more specifically, attention-based models (Bahdanau et al., 2015; Jean et al., 2015; Luong et al., 2015; Mi et al., 2016) have recently become the method of choice for machine translation: many works have shown that Neural Machine Translation (NMT) outperforms classic PhraseBased Statistical Machine Translation (PBSMT) approaches over a wide array of datasets (Bentivogli et al., 2016; Dowling et al., 2018; Koehn and Knowles, 2017). Indeed, NMT provides better generalization and accuracy capabilities (Bojar et al., 2016; Bentivogli et al., 2016;"
W19-6101,D15-1157,0,0.0432657,"Missing"
W19-6101,W18-1817,0,0.0972966,"ave the same thing or have gotten over it I take everything that can help me. . Table 3: Excerpts of the UGC corpora considered. Common UGC idiosyncrasies are highlighted: noncanonical contractions, spelling errors, missing elements, colloquialism, etc. See (Foster, 2010; Seddah et al., 2012; Eisenstein, 2013) for more complete linguistic descriptions. the development set and a 0.1 label smoothing (Pereyra et al., 2017). our two UGC corpora. 3.2.3 Transformer architecture We consider a vanilla Transformer model (Vaswani et al., 2017) using the implementation proposed in the OpenNMT framework (Klein et al., 2018). It consists of 6 layers with word embeddings of 512 components, a feed-forward layers made of 2 048 units and 8 self-attention heads. It was trained using the ADAM optimizer with OpenNMT default parameters. 3.3.2 Post-processing : handling OOVs 3.3 Data processing 3.3.1 Preprocessing All of our datasets were tokenized with bytepair encoding (BPE) (Sennrich et al., 2016) using sentencepiece (Kudo and Richardson, 2018). We use a BPE vocabulary size of 16K. As a point of comparison we also train a system on Large OpenSubs with 32K BPE operations. As usual, the training corpora were cleaned so e"
W19-6101,W17-3204,0,0.31275,"T architectures and suggests new avenue for improving NMT models. Introduction1 1 Neural Machine Translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014a; Cho et al., 2014) and, more specifically, attention-based models (Bahdanau et al., 2015; Jean et al., 2015; Luong et al., 2015; Mi et al., 2016) have recently become the method of choice for machine translation: many works have shown that Neural Machine Translation (NMT) outperforms classic PhraseBased Statistical Machine Translation (PBSMT) approaches over a wide array of datasets (Bentivogli et al., 2016; Dowling et al., 2018; Koehn and Knowles, 2017). Indeed, NMT provides better generalization and accuracy capabilities (Bojar et al., 2016; Bentivogli et al., 2016; Castilho et al., 2017) even if it has well-identified limits such as over-translating and dropping translations (Mi et al., 2016; Koehn and Knowles, 2017; Le et al., 2017). This work aims at studying how these interactions impact machine translation of noisy texts 1 We thank our anonymous reviewers for their insightful comments. This work was funded by the ANR ParSiTi project (ANR-16-CE33-0021). as generally found in social media and web forums and often denoted as User Generate"
W19-6101,D18-2012,0,0.0201636,"., 2017). our two UGC corpora. 3.2.3 Transformer architecture We consider a vanilla Transformer model (Vaswani et al., 2017) using the implementation proposed in the OpenNMT framework (Klein et al., 2018). It consists of 6 layers with word embeddings of 512 components, a feed-forward layers made of 2 048 units and 8 self-attention heads. It was trained using the ADAM optimizer with OpenNMT default parameters. 3.3.2 Post-processing : handling OOVs 3.3 Data processing 3.3.1 Preprocessing All of our datasets were tokenized with bytepair encoding (BPE) (Sennrich et al., 2016) using sentencepiece (Kudo and Richardson, 2018). We use a BPE vocabulary size of 16K. As a point of comparison we also train a system on Large OpenSubs with 32K BPE operations. As usual, the training corpora were cleaned so each sentence has, at least, 1 token and, at most, 70 tokens. We did not perform any other pre-processing. In particular, the original case of the sentences was left unchanged in order to help disambiguate subword BPE units (see example in Figure 1) especially for Named Entities that are vastly present in Given the high number of OOVs in UGC, special care must be taken in choosing the strategy to handle them. The BPE pr"
W19-6101,I17-1003,0,0.0672878,"i et al., 2016) have recently become the method of choice for machine translation: many works have shown that Neural Machine Translation (NMT) outperforms classic PhraseBased Statistical Machine Translation (PBSMT) approaches over a wide array of datasets (Bentivogli et al., 2016; Dowling et al., 2018; Koehn and Knowles, 2017). Indeed, NMT provides better generalization and accuracy capabilities (Bojar et al., 2016; Bentivogli et al., 2016; Castilho et al., 2017) even if it has well-identified limits such as over-translating and dropping translations (Mi et al., 2016; Koehn and Knowles, 2017; Le et al., 2017). This work aims at studying how these interactions impact machine translation of noisy texts 1 We thank our anonymous reviewers for their insightful comments. This work was funded by the ANR ParSiTi project (ANR-16-CE33-0021). as generally found in social media and web forums and often denoted as User Generated Content (UGC). Given the increasing importance of social medias, this type of texts has been extensively studied over the years, e.g. (Foster, 2010; Seddah et al., 2012; Eisenstein, 2013). In this work we focus on UGC in which no grammatical, orthographic or coherence rules are respect"
W19-6101,L18-1275,0,0.0901178,"and finally the pre- and post-processing applied (§3.3). 3.1 Data Sets Parallel corpora We train our models on two different corpora. We first consider the traditional corpus for training MT systems, namely the WMT data made of the europarl (v7) corpus2 and the newscommentaries (v10) corpus3 . We use the newsdiscussdev2015 corpus as a development set. This is exactly the setup used to train the system described in (Michel and Neubig, 2018) which will be used as a baseline throughout this work. We also consider, as a second training set, the French-English parallel portion of OpenSubtitles&apos;18 (Lison et al., 2018), a collection of crowd-sourced peer-reviewed subtitles for movies. We assume that, because it is made of informal dialogs, such as those found in popular sitcoms, sentences from OpenSubtitles will be much more similar to UGC data than WMT data, 2 3 www.statmt.org/europarl/ www.statmt.org/wmt15/training-parallel-nc-v10.tgz in part because most of it originates from social media and consists in streams of conversation. It must however be noted that UGC differs significantly from subtitles in many aspects: emotion denoted with repetitions, typographical and spelling errors, emojis, etc. To enabl"
W19-6101,D15-1166,0,0.060982,"tion systems (NMT) when translating User Generated Content (UGC), as encountered in social medias, from French to English. We show that, contrary to what could be expected, PBSMT outperforms NMT when translating non-canonical inputs. Our error analysis uncovers the specificities of UGC that are problematic for sequential NMT architectures and suggests new avenue for improving NMT models. Introduction1 1 Neural Machine Translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014a; Cho et al., 2014) and, more specifically, attention-based models (Bahdanau et al., 2015; Jean et al., 2015; Luong et al., 2015; Mi et al., 2016) have recently become the method of choice for machine translation: many works have shown that Neural Machine Translation (NMT) outperforms classic PhraseBased Statistical Machine Translation (PBSMT) approaches over a wide array of datasets (Bentivogli et al., 2016; Dowling et al., 2018; Koehn and Knowles, 2017). Indeed, NMT provides better generalization and accuracy capabilities (Bojar et al., 2016; Bentivogli et al., 2016; Castilho et al., 2017) even if it has well-identified limits such as over-translating and dropping translations (Mi et al., 2016; Koehn and Knowles, 201"
W19-6101,W16-3905,1,0.683517,"h replacement through aligning the translation hypothesis with the source sentences (both already tokenized with BPE) with fastalign (Dyer et al., 2013) and copying the source words aligned with the &lt;UNK&gt; token. 4 Measuring noise levels as corpus divergence Several metrics have been proposed to quantify the domain drift between two corpora. In particular, the perplexity of a language model the KLdivergence between the character-level 3-gram distribution of the train and test sets were two useful measurements capable of estimating the noiselevel of UGC corpora as shown respectively by Martínez Alonso et al. (2016) and Seddah et al. (2012). We also propose a new metric to estimate the noise level tailored to the BPE tokenization. The BPE stability, BPEstab, is an indicator of how many BPE-compounded words tend to form throughout a test corpus. Formally BPEstab is defined as: 1 ∑ n_unique_neighbors(v) · freq(v) · N n_neighbors(v) (1) v∈V where N is the number of tokens in the corpus, V the BPE vocabulary, freq(v) the frequency of the token v and n_unique_neighbors(v) the number of unique tokens that surrounds the token v. Neighbors are counted only within the original word limits. Low average BPE stabili"
W19-6101,D16-1096,0,0.0327201,"hen translating User Generated Content (UGC), as encountered in social medias, from French to English. We show that, contrary to what could be expected, PBSMT outperforms NMT when translating non-canonical inputs. Our error analysis uncovers the specificities of UGC that are problematic for sequential NMT architectures and suggests new avenue for improving NMT models. Introduction1 1 Neural Machine Translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014a; Cho et al., 2014) and, more specifically, attention-based models (Bahdanau et al., 2015; Jean et al., 2015; Luong et al., 2015; Mi et al., 2016) have recently become the method of choice for machine translation: many works have shown that Neural Machine Translation (NMT) outperforms classic PhraseBased Statistical Machine Translation (PBSMT) approaches over a wide array of datasets (Bentivogli et al., 2016; Dowling et al., 2018; Koehn and Knowles, 2017). Indeed, NMT provides better generalization and accuracy capabilities (Bojar et al., 2016; Bentivogli et al., 2016; Castilho et al., 2017) even if it has well-identified limits such as over-translating and dropping translations (Mi et al., 2016; Koehn and Knowles, 2017; Le et al., 2017"
W19-6101,D18-1050,0,0.18939,"generally used in MT evaluation campaigns and tested on UGC data. We will first describe the datasets used in this work (§3.1), then the different systems we have considered (§3.2) and finally the pre- and post-processing applied (§3.3). 3.1 Data Sets Parallel corpora We train our models on two different corpora. We first consider the traditional corpus for training MT systems, namely the WMT data made of the europarl (v7) corpus2 and the newscommentaries (v10) corpus3 . We use the newsdiscussdev2015 corpus as a development set. This is exactly the setup used to train the system described in (Michel and Neubig, 2018) which will be used as a baseline throughout this work. We also consider, as a second training set, the French-English parallel portion of OpenSubtitles&apos;18 (Lison et al., 2018), a collection of crowd-sourced peer-reviewed subtitles for movies. We assume that, because it is made of informal dialogs, such as those found in popular sitcoms, sentences from OpenSubtitles will be much more similar to UGC data than WMT data, 2 3 www.statmt.org/europarl/ www.statmt.org/wmt15/training-parallel-nc-v10.tgz in part because most of it originates from social media and consists in streams of conversation. It"
W19-6101,E17-2045,0,0.040477,"(Bentivogli et al., 2016) and (Bojar et al., 2016), conclude that the former outperforms the latter as NMT translations require less post-editing to produce a correct translation. For instance, Castilho et al. (2017) present a detailed comparison of NMT and PBSMT and show that NMT outperforms PBSMT in terms of both fluency and translation accuracy, even if there is no improvement in terms of post-editing needs. However, other case studies, such as Koehn and Knowles (2017), have defended the idea that NMT was still outperformed by PBSMT in cross-domain and low-resource scenarios. For instance, Negri et al. (2017) showed that, when translating English to French, PBSMT outperforms NMT by a great margin in multi-domain data realistic conditions (heterogeneous training sets with different sizes). Dowling et al. (2018) also demonstrated a significant gap of performance in favor of their PBSMT system’s over an out-of-the-box NMT system in a low-resource setting (English-Irish). These conclusions have recently been questioned by Sennrich and Zhang (2019) who showed NMT could achieve good performance in low-resource scenario when all hyper-parameters (size of the bytepair encoding (BPE) vocabulary, number of"
W19-6101,W18-1818,0,0.296692,"wo neural models. 3.2.1 Phrase-based Machine Translation We use the Moses (Koehn et al., 2007) toolkit as our phrase-based model, using the default features and parameters. The language model is a 5-gram language model with Knesser-Ney smoothing on the target side of the parallel data. We decided to consider only the parallel data (and not any monolingual data) so that the PBSMT and NMT systems use exactly the same data. 3.2.2 seq2seq model The first neural model we consider is a seq2seq bi-LSTM architecture with global attention decoding. The seq2seq model was trained using the XNMT toolkit (Neubig et al., 2018).5 It consists in a 2-layered Bi-LSTM layers encoder and 2-layered Bi-LSTM decoder. It considers, as input, word embeddings of 512 components and each LSTM units has 1 024 components. A dropout probability of 0.3 was introduced (Srivastava et al., 2014). The model was trained using the ADAM optimizer (Kingma and Ba, 2015) with vanilla parameters (α = 0.02, β = 0.998). Other more specific settings include keeping unchanged the learning rate (LR) for the first two epochs, a LR decay method based on the improvement of the performance on 5 4 Popular French websites devoted respectively to videogam"
W19-6101,W18-6319,0,0.0836664,"Missing"
W19-6101,2011.mtsummit-papers.27,0,0.0372684,"‘MTNT’, ‘News’ and ‘Open’ stand, respectively, for the Cr#pbank, MTNT, newstest&apos;14 and OpenSubtitlesTest test sets. substantial to out-of-domain test corpora, whereas scores on the in-domain test sets remain almost invariable regardless the chosen BPE vocabulary size. 5.2 Error Analysis The goal of this section is to analyze both quantitatively and qualitatively the output of NMT systems to explain their poor performance in translating UGC. Several works have already identified two main limits of NMT systems: translation dropping and excessive token generation, also known as over-generation (Roturier and Bensadoun, 2011; Kaljahi et al., 2015; Kaljahi and Samad, 2015; Michel and Neubig, 2018). We will analyze in detail how these two problems impact our models in the following subsections. It is also interesting to notice how performances lowered on the LargeOpenSubtitles system tokenized with 16K BPE operations for the seq2seq system. Specifically the newstest&apos;14 translation results, for which we noticed a drop of 7.2 BLEU points with respect to the SmallOpenSubtitles configuration, despite having roughly 4 times more training data. This is due to a faulty behaviour of the fastalign method, directly caused by"
W19-6101,W16-4603,0,0.311033,"et al. (2018) also demonstrated a significant gap of performance in favor of their PBSMT system’s over an out-of-the-box NMT system in a low-resource setting (English-Irish). These conclusions have recently been questioned by Sennrich and Zhang (2019) who showed NMT could achieve good performance in low-resource scenario when all hyper-parameters (size of the bytepair encoding (BPE) vocabulary, number of hidden units, batch size, ...) are correctly tuned and a proper NMT architecture is selected. The situation for other NMT approaches, such as character-based NMT, is also confusing: Wu et al. (2016) have shown that character-based methods achieve state-of-the-art performance for different language pairs; Belinkov et al. (2017) and Durrani et al. (2019) have demonstrated their systems respective abilities to retrieve good amount of morphological information leveraging on subword level features. However, Belinkov and Bisk (2018) found that these approaches are not robust to noise (both synthetic and natural) when trained only with clean corpora. On the other hand, Durrani et al. (2019) concluded that character-based representations were more robust to synthetic and natural noise than word-"
W19-6101,C12-1149,1,0.441407,"has well-identified limits such as over-translating and dropping translations (Mi et al., 2016; Koehn and Knowles, 2017; Le et al., 2017). This work aims at studying how these interactions impact machine translation of noisy texts 1 We thank our anonymous reviewers for their insightful comments. This work was funded by the ANR ParSiTi project (ANR-16-CE33-0021). as generally found in social media and web forums and often denoted as User Generated Content (UGC). Given the increasing importance of social medias, this type of texts has been extensively studied over the years, e.g. (Foster, 2010; Seddah et al., 2012; Eisenstein, 2013). In this work we focus on UGC in which no grammatical, orthographic or coherence rules are respected, other than those considered by the writer. Such rule-free environment promotes a plethora of vocabulary and grammar variations, which account for the large increase of out-of-vocabulary tokens (OOVs) in UGC corpora with respect to canonical parallel training data. Translating UGC raises several challenges as it corresponds to both a low-resource scenario — producing parallel UGC corpora is very costly and often problematic due to inconsistencies between translators — and a"
W19-6101,P16-1162,0,0.114665,"set and a 0.1 label smoothing (Pereyra et al., 2017). our two UGC corpora. 3.2.3 Transformer architecture We consider a vanilla Transformer model (Vaswani et al., 2017) using the implementation proposed in the OpenNMT framework (Klein et al., 2018). It consists of 6 layers with word embeddings of 512 components, a feed-forward layers made of 2 048 units and 8 self-attention heads. It was trained using the ADAM optimizer with OpenNMT default parameters. 3.3.2 Post-processing : handling OOVs 3.3 Data processing 3.3.1 Preprocessing All of our datasets were tokenized with bytepair encoding (BPE) (Sennrich et al., 2016) using sentencepiece (Kudo and Richardson, 2018). We use a BPE vocabulary size of 16K. As a point of comparison we also train a system on Large OpenSubs with 32K BPE operations. As usual, the training corpora were cleaned so each sentence has, at least, 1 token and, at most, 70 tokens. We did not perform any other pre-processing. In particular, the original case of the sentences was left unchanged in order to help disambiguate subword BPE units (see example in Figure 1) especially for Named Entities that are vastly present in Given the high number of OOVs in UGC, special care must be taken in"
W19-6101,P19-1021,0,0.112155,"udies, such as Koehn and Knowles (2017), have defended the idea that NMT was still outperformed by PBSMT in cross-domain and low-resource scenarios. For instance, Negri et al. (2017) showed that, when translating English to French, PBSMT outperforms NMT by a great margin in multi-domain data realistic conditions (heterogeneous training sets with different sizes). Dowling et al. (2018) also demonstrated a significant gap of performance in favor of their PBSMT system’s over an out-of-the-box NMT system in a low-resource setting (English-Irish). These conclusions have recently been questioned by Sennrich and Zhang (2019) who showed NMT could achieve good performance in low-resource scenario when all hyper-parameters (size of the bytepair encoding (BPE) vocabulary, number of hidden units, batch size, ...) are correctly tuned and a proper NMT architecture is selected. The situation for other NMT approaches, such as character-based NMT, is also confusing: Wu et al. (2016) have shown that character-based methods achieve state-of-the-art performance for different language pairs; Belinkov et al. (2017) and Durrani et al. (2019) have demonstrated their systems respective abilities to retrieve good amount of morpholo"
W19-6101,P16-1008,0,0.0155037,"nces, which demonstrates the presence of translations being dropped or shortened. Figure 2: Distribution of Cr#pbank translations length ratio w.r.t ground truth translations. Figure 1: Attention matrix for the source sentence ‘Bon je veux regardé teen wolf moi mais ce soir nsm*’ predicted by a seq2seq model. *Ok, I do want to watch Teen Wolf tonight motherf..r Our phrase-based model does not suffer from 5.2.2 Over-translation A second well-known issue with NMT is that the model sometimes repeatedly outputs tokens lacking any coherence, thus adding considerable artificial noise to the output (Tu et al., 2016). When manually inspecting the output, we noticed that this phenomenon occurred in UGC sentences that contain a rare, and often repetitive, sequence of tokens, such as those present in sentences like “ne spooooooooilez pas teen wolf non non non et non je dis non” (don’t spoooooil Teen wolf no and no I say no) in which the speaker emotion is expressed by repetitions of words or letters. The attention matrix obtained when translating such sentences with a seq2seq model often shows that the attention mechanism gets stalled due to the repetition of some BPE token (cf. the attention matrix in Figur"
wisniewski-etal-2014-corpus,J11-4002,0,\N,Missing
wisniewski-etal-2014-corpus,P11-1022,0,\N,Missing
wisniewski-etal-2014-corpus,C08-1141,0,\N,Missing
wisniewski-etal-2014-corpus,vilar-etal-2006-error,0,\N,Missing
