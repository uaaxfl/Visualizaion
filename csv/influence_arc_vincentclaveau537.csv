2003.jeptalnrecital-long.5,J93-2005,0,0.0861716,"Missing"
2003.jeptalnrecital-long.5,P95-1026,0,0.111839,"Missing"
2003.jeptalnrecital-long.5,bouillon-etal-2002-acquisition,1,\N,Missing
2003.jeptalnrecital-long.5,P02-1046,0,\N,Missing
2004.jeptalnrecital-long.4,C00-1039,0,0.034456,"Missing"
2005.jeptalnrecital-long.25,J93-2003,0,0.00822408,"Missing"
2005.jeptalnrecital-long.25,W03-0305,0,0.0294266,"Missing"
2005.jeptalnrecital-long.25,J94-4004,0,0.0281858,"Missing"
2005.jeptalnrecital-long.25,W02-1039,0,0.0304124,"Missing"
2005.jeptalnrecital-long.25,P02-1050,0,0.186209,"Missing"
2005.jeptalnrecital-long.25,W03-0302,0,0.0228779,"Missing"
2005.jeptalnrecital-long.25,W03-0301,0,0.0390874,"Missing"
2005.jeptalnrecital-long.25,P00-1056,0,0.129225,"Missing"
2005.jeptalnrecital-long.25,2004.jeptalnrecital-recital.4,1,0.88366,"Missing"
2005.jeptalnrecital-long.25,W03-0303,0,0.0224026,"Missing"
2005.jeptalnrecital-long.25,W03-0304,0,0.023965,"Missing"
2005.jeptalnrecital-long.25,W03-0300,0,0.254511,"Missing"
2005.jeptalnrecital-long.26,H91-1026,0,0.243455,"Missing"
2005.jeptalnrecital-long.26,knight-al-onaizan-1998-translation,0,0.0869807,"Missing"
2005.jeptalnrecital-long.26,J98-4003,0,0.102612,"Missing"
2005.jeptalnrecital-long.26,C04-1117,0,0.0297801,"Missing"
2005.jeptalnrecital-long.26,tsuji-etal-2002-extracting,0,0.0595132,"Missing"
2007.jeptalnrecital-long.10,J93-2003,0,0.0179981,"Missing"
2007.jeptalnrecital-long.10,H91-1026,0,0.229977,"Missing"
2007.jeptalnrecital-long.10,J98-4003,0,0.0564242,"Missing"
2007.jeptalnrecital-long.10,2005.jeptalnrecital-long.7,0,0.0763909,"Missing"
2007.jeptalnrecital-long.10,C04-1117,0,0.0221605,"Missing"
2007.jeptalnrecital-long.10,tsuji-etal-2002-extracting,0,0.0273632,"Missing"
2009.jeptalnrecital-court.27,W06-3206,0,0.0340787,"Missing"
2009.jeptalnrecital-court.27,W98-1224,0,0.0714574,"Missing"
2009.jeptalnrecital-court.27,P07-1013,0,0.0227112,"Missing"
2009.jeptalnrecital-court.27,P08-1103,0,0.09138,"Missing"
2009.jeptalnrecital-court.27,N07-1047,0,0.036648,"Missing"
2009.jeptalnrecital-court.27,J00-2003,0,0.0886475,"Missing"
2010.jeptalnrecital-long.39,P98-1069,0,0.199673,"Missing"
2010.jeptalnrecital-long.39,W02-1403,0,0.0377853,"Missing"
2010.jeptalnrecital-long.39,2009.jeptalnrecital-long.1,0,0.0753075,"Missing"
2010.jeptalnrecital-long.39,N07-1047,0,0.0372401,"Missing"
2010.jeptalnrecital-long.39,J98-4003,0,0.0771687,"Missing"
2010.jeptalnrecital-long.39,2005.jeptalnrecital-long.7,0,0.0357289,"Missing"
2010.jeptalnrecital-long.39,J03-1002,0,0.00550943,"Missing"
2010.jeptalnrecital-long.39,C04-1117,0,0.0229184,"Missing"
2010.jeptalnrecital-long.39,tsuji-etal-2002-extracting,0,0.0616014,"Missing"
2011.jeptalnrecital-court.21,E06-1051,0,0.0505857,"Missing"
2011.jeptalnrecital-court.21,C92-2082,0,0.307223,"Missing"
2015.jeptalnrecital-long.2,P05-1001,0,0.102762,"Missing"
2015.jeptalnrecital-long.2,W04-3234,0,0.0729094,"Missing"
2015.jeptalnrecital-long.2,N13-1014,0,0.0516489,"Missing"
2015.jeptalnrecital-long.2,P10-1052,0,0.0657858,"Missing"
2015.jeptalnrecital-long.2,P08-1099,0,0.0418474,"Missing"
2015.jeptalnrecital-long.2,N04-1043,0,0.142021,"Missing"
2015.jeptalnrecital-long.2,W01-0501,0,0.106444,"Missing"
2015.jeptalnrecital-long.2,D08-1112,0,0.137859,"Missing"
2015.jeptalnrecital-long.2,P05-1044,0,0.0908639,"Missing"
2015.jeptalnrecital-long.2,W02-2024,0,0.13551,"Missing"
2015.jeptalnrecital-long.2,W00-0726,0,0.233034,"Missing"
2016.jeptalnrecital-poster.17,S15-2065,0,0.0328462,"Missing"
2016.jeptalnrecital-poster.17,E03-1009,0,0.124599,"Missing"
2016.jeptalnrecital-poster.17,S15-2128,0,0.0498756,"Missing"
2016.jeptalnrecital-poster.17,P10-1052,0,0.0298885,"Missing"
2016.jeptalnrecital-poster.17,S15-2127,0,0.0308444,"Missing"
2016.jeptalnrecital-poster.17,S15-2083,0,0.0523059,"Missing"
2016.jeptalnrecital-poster.17,H05-1044,0,0.123924,"Missing"
2016.jeptalnrecital-poster.24,R15-1015,0,0.0201086,"Missing"
2017.jeptalnrecital-court.5,P10-1133,0,0.0779385,"Missing"
2017.jeptalnrecital-court.5,P10-1052,0,0.0863653,"Missing"
2018.jeptalnrecital-court.24,P16-1047,0,0.0360226,"Missing"
2018.jeptalnrecital-court.24,P14-1007,0,0.0469611,"Missing"
2018.jeptalnrecital-court.24,S12-1041,0,0.060735,"Missing"
2018.jeptalnrecital-deft.1,2018.jeptalnrecital-deft.12,0,0.102356,"(negative, neutral, positive, mixed), to identify clues of sentiment and target, and to annotate each tweet in terms of source and target concerning all expressed sentiments. Twelve teams participated, mainly on the two first tasks. On the identification of tweets about public transportation, micro F-measure values range from 0.827 to 0.908. On the identification of the global polarity, micro F-measure values range from 0.381 to 0.823. M OTS - CLÉS : Classification automatique, Analyse de sentiments, Fouille de texte. K EYWORDS: Automatic Classification, Sentiment Analysis, Text Mining. c ATALA 2018 219 1 Introduction Dans la continuité de l’édition 2015 (Hamon et al., 2015), la treizième édition du DÉfi Fouille de Textes (DEFT) porte sur l’extraction d’information et l’analyse de sentiments dans des tweets rédigés en français sur la thématique des transports. La campagne s’est déroulée sur une période limitée avec une ouverture des inscriptions le 31 janvier, la diffusion des données d’entraînement à partir du 19 février, et le déroulement de la phase de test entre le 28 mars et le 5 avril, sur une durée de trois jours fixée par chacun des participants. Quinze équipes se sont inscrites,"
2018.jeptalnrecital-deft.6,Q17-1010,0,0.0249895,"Missing"
2018.jeptalnrecital-deft.6,S16-1044,0,0.0620081,"Missing"
2018.jeptalnrecital-deft.6,W14-4012,0,0.0249329,"Missing"
2018.jeptalnrecital-deft.6,L18-1550,0,0.0209277,"Missing"
2018.jeptalnrecital-deft.6,S16-1174,0,0.0637582,"Missing"
2018.jeptalnrecital-deft.6,P16-1101,0,0.0443582,"Missing"
2018.jeptalnrecital-deft.6,S16-1045,0,0.0532482,"Missing"
2019.jeptalnrecital-long.5,W18-5614,1,0.847588,"Missing"
2019.jeptalnrecital-long.5,W15-2604,1,0.853082,"Missing"
2019.jeptalnrecital-long.5,X96-1019,0,0.802383,"Missing"
2019.jeptalnrecital-long.5,P10-1052,0,0.116292,"Missing"
2019.jeptalnrecital-long.5,W08-0606,0,0.0536337,"Missing"
2020.jeptalnrecital-taln.16,W19-1909,0,0.0403658,"Missing"
2020.jeptalnrecital-taln.16,N19-1423,0,0.0424945,"Missing"
2020.jeptalnrecital-taln.16,D14-1162,0,0.0824893,"Missing"
2020.jeptalnrecital-taln.16,D18-1032,0,0.0619293,"Missing"
2020.lrec-1.589,W11-2501,0,0.07891,"Missing"
2020.lrec-1.589,Q17-1010,0,0.650262,"ess is thorny because the outcome value does not explain entirely the complexity of these models. In other words, a model performing well under a specific evaluation might poorly work for a different one (Schnabel et al., 2015). As an example, some word embedding evaluations promote comparison of embeddings with human judgement while others favour embeddings behaviour analysis on downstream tasks, as pointed by Schnabel et al. (2015). In this work, we propose to investigate correlations between numerous evaluations for word embedding. We restrict the study to FastText embeddings introduced by Bojanowski et al. (2017), but this methodology can be applied to other kinds of word embedding techniques. The understanding of evaluation correlations may provide several useful tools: • Strongly correlated evaluations raise a question on the relevance of performing these. Actually, it could be possible to only keep one evaluation among the correlated evaluation set, since its score would directly affect the score of others. Therefore, it could reduce the number of needed evaluations. • Inexpensive evaluation processes correlated with timeconsuming ones could be helpful to speed up optimisation of hyper-parameters."
2020.lrec-1.589,C16-1173,1,0.93511,"he global quality of language representations. Some work (Schnabel et al., 2015) unsuccessfully try to identify correlations between extrinsic and intrinsic scores, using word embeddings computed with different methods. However, intrinsic and extrinsic scores from word embeddings calculated with the same method, as done by Qiu et al. (2018), are significantly correlated. We propose to prolong their work to English word embeddings and to more popular datasets. In fact, comparing embeddings from different classes is thorny since different algorithms catch different language aspects. As shown by Claveau and Kijak (2016), some embeddings could be created in order to solve specific tasks while neglecting other language aspects. This is why, we only investigate word embeddings learned using a unique algorithm: FastText (Bojanowski et al., 2017). Another aspect treated in this work is the introduction of global metrics which are metrics trying to catch intrinsic structures in vectors, with no data other than these vectors. Tsvetkov et al. (2015) proposed a metric trying to automatically understand the meaning of vector dimensions. Their 4789 metric show good correlation with both intrinsic and extrinsic evaluati"
2020.lrec-1.589,D16-1235,0,0.017314,"assess simple language concepts. In this work, we study three different kinds of intrinsic evaluations focusing on different language aspects. The cosine similarity: Intrinsic Evaluations Intrinsic evaluations compare embedding structures to human judgement. They need external data to carry out this 3.2.1. Similarity Similarity consists of scoring word pairs. Each pair is human-labelled with a score representing the compatibility between the concepts of the pair. This compatibility score is specific to datasets and often characterises the synonymy (Finkelstein et al., 2002; Hill et al., 2015; Gerz et al., 2016; Rubenstein and Goodenough, 1965) or the entailment (Vuli´c et al., 2017). The evaluation relies on measuring the Spearman correlation between labelled scores and reconstructed scores from the word embedding. The reconstructed scores are obtained by taking the cosine similarity (1) between pairs. The correlation score constitutes in the end the value of the evaluation. Similarity datasets used in this study are reported on Table 1. The majority of them are datasets using synonymy (i.e. semantic proximity) as a guide to estimate the score. We add HyperLex from Vuli´c et al. (2017) to introduce"
2020.lrec-1.589,J15-4004,0,0.0258944,"parison and mainly assess simple language concepts. In this work, we study three different kinds of intrinsic evaluations focusing on different language aspects. The cosine similarity: Intrinsic Evaluations Intrinsic evaluations compare embedding structures to human judgement. They need external data to carry out this 3.2.1. Similarity Similarity consists of scoring word pairs. Each pair is human-labelled with a score representing the compatibility between the concepts of the pair. This compatibility score is specific to datasets and often characterises the synonymy (Finkelstein et al., 2002; Hill et al., 2015; Gerz et al., 2016; Rubenstein and Goodenough, 1965) or the entailment (Vuli´c et al., 2017). The evaluation relies on measuring the Spearman correlation between labelled scores and reconstructed scores from the word embedding. The reconstructed scores are obtained by taking the cosine similarity (1) between pairs. The correlation score constitutes in the end the value of the evaluation. Similarity datasets used in this study are reported on Table 1. The majority of them are datasets using synonymy (i.e. semantic proximity) as a guide to estimate the score. We add HyperLex from Vuli´c et al."
2020.lrec-1.589,N16-1030,0,0.0279297,"Missing"
2020.lrec-1.589,Q15-1016,0,0.0450049,"n the end, each word has to be classified in different categories representing entities. Relation Types Capital, Country, Family, Currency, Cities, Morphology Morphology Table 2: Analogy datasets used in this work (Bakarov, 2018). 3.2.2. Analogy Analogy, proposed by Mikolov et al. (2013b), assesses the embedding of any kind of relationships. Given three words wA , wB and wC , such that wA is related to wB through a relation R, the task consists of finding a fourth word, wD , that is related to wC through the same relation R. Technically, wD is found as a solution of the problems formulated by Levy et al. (2015), leveraging the cosine similarity (1). We consider the two analogy datasets detailed in Table 2. 3.2.3. Categorisation Categorisation is a reconstruction exercise aiming to recover semantic clusters in the embedding space. The dataset is composed of K clusters and M words. The goal is to reconstruct K clusters using the M word vectors of the embedding. The reconstruction can be done with any clustering algorithm, but Schnabel et al. (2015) suggests using CLUTO Name Battig (Baroni et al., 2010) AP (Almuhareb, 2006) BLESS (Baroni and Lenci, 2011) Extrinsic Evaluations Extrinsic evaluations are"
2020.lrec-1.589,W13-3512,0,0.130959,"Missing"
2020.lrec-1.589,N13-1090,0,0.707599,"thodology to the current state of the art of word embeddings evaluation. Section 3 introduces the evaluation processes and materials we used for this investigation. Then section 4 details the experimental setup and discusses the results of experiments. The final section presents some conclusive remarks about this work. 2. Related Work Evaluations of word embeddings is not a new topic. Many resources and procedures, some used in this work and others exhaustively listed by Bakarov (2018), have been proposed in order to compare various methods such as GloVe (Pennington et al., 2014) or Word2Vec (Mikolov et al., 2013a). Quickly, the distinction between intrinsic and extrinsic evaluations was made, as stated by (Schnabel et al., 2015). The first one being related to the word embedding itself whereas the second uses it as an input of another model for a downstream task. Generally extrinsic evaluations are more crucial than intrinsic ones. Actually, extrinsic evaluations often are the ultimate goal of language processing while intrinsic evaluations try to estimate the global quality of language representations. Some work (Schnabel et al., 2015) unsuccessfully try to identify correlations between extrinsic an"
2020.lrec-1.589,D14-1162,0,0.0953159,"Missing"
2020.lrec-1.589,W03-0419,0,0.372533,"language aspects. Hence, they need external data and an additional language modelling step. Among the long list of extrinsic tasks, we chose three of them that cover a large spectrum of language skills, namely: Named Entity Recognition (NER), Sentiment Analysis (SA) and Text Classification (TC). Only one dataset for each task is used since extrinsic evaluations are particularly time and resource demanding. Size Number of clusters 5330 56 402 21 200 17 Table 3: Categorisation datasets for intrinsic evaluations (Bakarov, 2018). CoNLL2003 is the NER dataset considered for this work. We followed Sang and Meulder (2003) guidelines to set up the experiment. This dataset is made of sentences extracted from news thread. Words are then labelled by 5 sort of entities: O (None), PER (person), ORG (organisation), LOC (location), MISC (miscellaneous). Training and development sets are used for training while test set is kept for evaluation. 3.3.2. Sentiment Analysis In our case, sentiment analysis is a sentence-level classification problem for an opinion text (Joshi et al., 2017). Sentences have to be classified as positive, negative or neutral. The Stanford Sentiment Treebank dataset (SST), proposed by Socher et al"
2020.lrec-1.589,D15-1036,0,0.402866,"complementary datasets, i.e. each dataset quantifies a different aspect of word embeddings. 1. Introduction Word embeddings are continuous vector representations of word paradigmatics and syntagmatics. Since they capture multiple high-level characteristics of language, their evaluation is particularly difficult: it usually consists of quantifying their performance on various tasks. This process is thorny because the outcome value does not explain entirely the complexity of these models. In other words, a model performing well under a specific evaluation might poorly work for a different one (Schnabel et al., 2015). As an example, some word embedding evaluations promote comparison of embeddings with human judgement while others favour embeddings behaviour analysis on downstream tasks, as pointed by Schnabel et al. (2015). In this work, we propose to investigate correlations between numerous evaluations for word embedding. We restrict the study to FastText embeddings introduced by Bojanowski et al. (2017), but this methodology can be applied to other kinds of word embedding techniques. The understanding of evaluation correlations may provide several useful tools: • Strongly correlated evaluations raise a"
2020.lrec-1.589,D13-1170,0,0.00468182,"ulder (2003) guidelines to set up the experiment. This dataset is made of sentences extracted from news thread. Words are then labelled by 5 sort of entities: O (None), PER (person), ORG (organisation), LOC (location), MISC (miscellaneous). Training and development sets are used for training while test set is kept for evaluation. 3.3.2. Sentiment Analysis In our case, sentiment analysis is a sentence-level classification problem for an opinion text (Joshi et al., 2017). Sentences have to be classified as positive, negative or neutral. The Stanford Sentiment Treebank dataset (SST), proposed by Socher et al. (2013), is chosen for this task. Each sentence is a movie review labelled by its global judgement: very positive, positive, neutral, negative or very negative. The objective is to recover sentiment classes from sentences, and measure the average accuracy. This setup is known as SST-1 (Zhou et al., 2016). Here again, train and dev splits are involved for training whereas test split is kept for evaluation. 3.3.3. Text Classification This task is similar to sentiment analysis, since models have to classify documents into different categories (Kowsari et al., 2019). The difference relies in the meaning"
2020.lrec-1.589,D15-1243,0,0.0186591,"s and to more popular datasets. In fact, comparing embeddings from different classes is thorny since different algorithms catch different language aspects. As shown by Claveau and Kijak (2016), some embeddings could be created in order to solve specific tasks while neglecting other language aspects. This is why, we only investigate word embeddings learned using a unique algorithm: FastText (Bojanowski et al., 2017). Another aspect treated in this work is the introduction of global metrics which are metrics trying to catch intrinsic structures in vectors, with no data other than these vectors. Tsvetkov et al. (2015) proposed a metric trying to automatically understand the meaning of vector dimensions. Their 4789 metric show good correlation with both intrinsic and extrinsic evaluations, but still requires data. We propose to extend this work by taking data-free matrix analysis techniques from signal processing and computer vision (Poling and Lerman, 2013; Roy and Vetterli, 2007). The major interest in data-free metrics is that they can be introduced during the learning phase as a regularisation term. 3. Global Metrics Global metrics are data-free (i.e., with no external data other than W ) evaluations fi"
2020.lrec-1.589,J17-4004,0,0.0461019,"Missing"
2020.lrec-1.589,C16-1329,0,0.0186195,"est set is kept for evaluation. 3.3.2. Sentiment Analysis In our case, sentiment analysis is a sentence-level classification problem for an opinion text (Joshi et al., 2017). Sentences have to be classified as positive, negative or neutral. The Stanford Sentiment Treebank dataset (SST), proposed by Socher et al. (2013), is chosen for this task. Each sentence is a movie review labelled by its global judgement: very positive, positive, neutral, negative or very negative. The objective is to recover sentiment classes from sentences, and measure the average accuracy. This setup is known as SST-1 (Zhou et al., 2016). Here again, train and dev splits are involved for training whereas test split is kept for evaluation. 3.3.3. Text Classification This task is similar to sentiment analysis, since models have to classify documents into different categories (Kowsari et al., 2019). The difference relies in the meaning of the labels and the nature of the text. They characterise high-level topics. The AGNews dataset is a data source found online1 and used 1 http://www.di.unipi.it/~gulli/AG_corpus_ of_news_articles.html 4792 Name Dimension (-dim) Learning Rate (-lr) Windows Size (-ws) Number of Epoch (-epoch) Nega"
2021.jeptalnrecital-taln.4,2020.acl-main.197,0,0.0542952,"Missing"
2021.jeptalnrecital-taln.4,N18-2072,0,0.0607075,"Missing"
2021.jeptalnrecital-taln.4,2020.lifelongnlp-1.3,0,0.0404317,"Missing"
2021.jeptalnrecital-taln.4,2020.lrec-1.302,0,0.0433297,"Missing"
2021.jeptalnrecital-taln.4,P10-1114,0,0.0456094,"Missing"
2021.jeptalnrecital-taln.4,D19-1670,0,0.0459442,"Missing"
2021.jeptalnrecital-taln.4,J81-4005,0,0.651556,"Missing"
bouillon-etal-2002-acquisition,J93-2005,0,\N,Missing
C04-1038,pearce-2002-comparative,0,\N,Missing
C04-1038,A97-1052,0,\N,Missing
C04-1038,P95-1026,0,\N,Missing
C04-1038,J03-2004,0,\N,Missing
C04-1038,bouillon-etal-2002-acquisition,1,\N,Missing
C12-1039,W02-1403,0,0.174493,"e terminologies are central to many applications and where terms are constructed by operations like neo-classical composition (e.g. chemotherapy, built from the Greek pseudo-word chemo, and therapy), which are very regular, and very productive. Unfortunately, no comprehensive database of morphs with semantic information is available, and splitting a term into morphs is still an issue. One can distinguish two views of the use of morphology as a tool for term (or word) analysis. In the lexematic view, relations between terms rely on the word form, but without the need to split them into morphs (Grabar and Zweigenbaum, 2002; Claveau and L’Homme, 2005; Hathout, 2009). Beside this implicit use of morphology, the morphemic view chieﬂy relies on splitting the term into morphs as a ﬁrst step. Many studies have been made in this framework. They either rely on partially manual approaches in which an expert gives morphs and combination rules (Deléger et al., 2008; Markó et al., 2005a) or heuristics (Baud et al., 1999), or on more automatic approaches. The latter usually try to ﬁnd recurrent letter patterns in word lists as morph-candidates (Kurimo et al., 2010). But such techniques cannot associate a semantic meaning wi"
C12-1039,2009.jeptalnrecital-long.1,0,0.0154413,"e terms are constructed by operations like neo-classical composition (e.g. chemotherapy, built from the Greek pseudo-word chemo, and therapy), which are very regular, and very productive. Unfortunately, no comprehensive database of morphs with semantic information is available, and splitting a term into morphs is still an issue. One can distinguish two views of the use of morphology as a tool for term (or word) analysis. In the lexematic view, relations between terms rely on the word form, but without the need to split them into morphs (Grabar and Zweigenbaum, 2002; Claveau and L’Homme, 2005; Hathout, 2009). Beside this implicit use of morphology, the morphemic view chieﬂy relies on splitting the term into morphs as a ﬁrst step. Many studies have been made in this framework. They either rely on partially manual approaches in which an expert gives morphs and combination rules (Deléger et al., 2008; Markó et al., 2005a) or heuristics (Baud et al., 1999), or on more automatic approaches. The latter usually try to ﬁnd recurrent letter patterns in word lists as morph-candidates (Kurimo et al., 2010). But such techniques cannot associate a semantic meaning with these morphs. To our knowledge, our appr"
C12-1039,N07-1047,0,0.0486634,"Missing"
C12-1039,D07-1092,0,0.0249811,"e would like their probabilities to reinforce each other. The adaptation we propose aims at making the maximization phase able to automatically group the diﬀerent morphs belonging to a same morpheme. To achieve this goal, we use a simple but well suited technique relying on formal analogical calculus. 3.2.1 Analogy An analogy is a relation between 4 elements that we note: a : b :: c : d which can be read a is for b what c is for d (Lepage, 2000, for more details about analogies). Analogies have been 633 used in many NLP studies, especially for translation of sentences (Lepage, 2000) or terms (Langlais and Patry, 2007; Langlais et al., 2008). Analogies are also a key component in the previously mentioned work on terminology structuring (Claveau and L’Homme, 2005). We rely on this latter work to formalize our normalization problem. In our framework, one possible analogy may be: dermato : dermo :: hémato : hémo. Knowing that dermato and dermo belong to a same morpheme, one can infer that this is the case for hémato and hémo. Such an analogy, build on the graphemic representation of words, is said a formal analogy. After Stroppa and Yvon (2005), formal analogies can be deﬁned in terms of factorizations. ← − ←"
C12-1039,C00-1071,0,0.0297421,"babilities, which is of course harmful for the algorithm, is caused by morphemic variation: bactério, bactérie, and bactéri are 3 morphs of the same morpheme, and we would like their probabilities to reinforce each other. The adaptation we propose aims at making the maximization phase able to automatically group the diﬀerent morphs belonging to a same morpheme. To achieve this goal, we use a simple but well suited technique relying on formal analogical calculus. 3.2.1 Analogy An analogy is a relation between 4 elements that we note: a : b :: c : d which can be read a is for b what c is for d (Lepage, 2000, for more details about analogies). Analogies have been 633 used in many NLP studies, especially for translation of sentences (Lepage, 2000) or terms (Langlais and Patry, 2007; Langlais et al., 2008). Analogies are also a key component in the previously mentioned work on terminology structuring (Claveau and L’Homme, 2005). We rely on this latter work to formalize our normalization problem. In our framework, one possible analogy may be: dermato : dermo :: hémato : hémo. Knowing that dermato and dermo belong to a same morpheme, one can infer that this is the case for hémato and hémo. Such an an"
C12-1039,J03-1002,0,0.00700809,"Missing"
C12-1039,W05-0616,0,0.0333914,"especially for translation of sentences (Lepage, 2000) or terms (Langlais and Patry, 2007; Langlais et al., 2008). Analogies are also a key component in the previously mentioned work on terminology structuring (Claveau and L’Homme, 2005). We rely on this latter work to formalize our normalization problem. In our framework, one possible analogy may be: dermato : dermo :: hémato : hémo. Knowing that dermato and dermo belong to a same morpheme, one can infer that this is the case for hémato and hémo. Such an analogy, build on the graphemic representation of words, is said a formal analogy. After Stroppa and Yvon (2005), formal analogies can be deﬁned in terms of factorizations. ← − ← − Let us note ⊕ the (non-commutative) concatenation operator at the right (abc ⊕ d = abcd), ← − ← − ← − ← − ← − and ⊖ its associated string subtraction operator (abc ⊕ d ⊖ d = abc ⊖ c ⊕ c = abc), and − → − → similarly for ⊕ and ⊖ operating at the left of the ﬁrst argument. Let a be a string (a term in our case) over an alphabet Σ, a factorization of a, noted fa , is a sequence of n factors ← − ← − ← − fa = (fa1 , ..., fan ), such that a = fa1 ⊕ fa2 ⊕ ... ⊕ fan . A formal analogy can be deﬁned by as: (fa , fb , fc , fd ) ∈ Defin"
C12-1039,tsuji-etal-2002-extracting,0,0.0285949,"ntic meaning with these morphs. To our knowledge, our approach is the ﬁrst to make the most of a pivot language to perform an automatic morphological analysis, as we propose in this study. It can be explained by three peculiarities of the biomedical domain: the morphology of its terms is known to be very regular, with few exceptions, the morphological composition (producing compounds) is very fertile, and there exists many multilingual terminologies. From a more technical point of view, the use of a bilingual terminology also evokes studies in transliteration, particularly Katakana or Arabic (Tsuji et al., 2002; Knight and Graehl, 1998, for example), or in translation. In this framework, Morin and Daille (2010) propose to map complex terms written in Kanjis with French ones, by using morphological rules. Yet, here again, these rules are to be given by an expert, and this study only concerns a special case of derivation. Moreover such an approach cannot handle neo-classical compounds. In other studies, translation methods for biomedical terms which considers terms as simple sequences of letters have been proposed (Claveau, 2009, inter alia). Such approaches share some similarities with the one presen"
C12-1039,J98-4003,0,\N,Missing
C14-1067,J06-1003,0,0.750653,"ted through experiments and offer significant improvement over the state-of-the-art. 1 Introduction Distributional thesauri are useful for many NLP tasks and their construction is an issue widely discussed for several years (Grefenstette, 1994). However this is still a very active research field, maintained by the increasingly large number of available corpus and by many applications. These thesauri associate each of their entry with a list of words that are desired semantically close to the entry. This notion of proximity varies (synonymy, other paradigmatic relations, syntagmatic relations (Budanitsky and Hirst, 2006; Adam et al., 2013, for a discussion)), but the methods used for the automatic construction of thesauri are often shared. For the most part, these methods rely on the distributional hypothesis of (Firth, 1957): each word is characterized by the set of contexts in which it appears, and the semantic proximity of two words can be inferred from the proximity of their contexts. This hypothesis has been implemented in different ways, and several propositions to improve the results have been explored (see next section for a state of the art). The work presented in this article are part of this frame"
C14-1067,W02-0908,0,0.580345,"9 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 709–720, Dublin, Ireland, August 23-29 2014. 3) Finally, on the basis of this work, we show how to use this confidence score and these probabilities to reorder the list of nearest neighbors (Section 6). To achieve this goal, we model the reranking as an optimization problem of assignments, solved by the Hungarian algorithm (Kuhn and Yaw, 1955). 2 Related work The notion of distributional thesaurus, as it was initially defined by Grefenstette (1994), followed by Lin (1998a) and Curran and Moens (2002), is not often considered specifically, probably because of its strong link with the notion of semantic similarity. As a consequence, the improvement of distributional thesauri has been first a side effect of the improvement of the distributional similarity measures used for their building and more precisely, of the distributional data they rely on. Both the nature of the constituents of distributional contexts and their weighting have been considered in this regard. Concerning their weighting, Broda et al. (2009) proposed to turn the weights of context constituents into ranks to make them les"
C14-1067,P13-1055,1,0.91899,"negative Matrix Factorization (Van de Cruys, 2010) and more recently, lexical representations learnt by neural networks (Huang et al., 2012; Mikolov et al., 2013). The work we present in this article follows a different perspective as our objective is to improve an existing distributional thesaurus by relying on its structure through a reranking of its neighbors. Such approach was adopted to some extent by Zhitomirsky-Geffet and Dagan (2009) as it exploited the neighbors of an entry in an initial thesaurus for reweighting its distributional representation and finally, reranking its neighbors. Ferret (2013) proposed a more indirect method in which the reranking is based on the downgrading of the neighbors that are detected as not similar to their entry through a pseudo word sense disambiguation task: such detection occurs if a certain proportion of the occurrences of a neighbor are not tagged as the entry. Finally, the closest work to ours is (Ferret, 2012), which selects in an unsupervised way a set of examples of semantically similar words from an initial thesaurus for training a classifier whose decision function is used for reranking the neighbors of each entry. Its unsupervised selection of"
C14-1067,W05-0604,0,0.015783,"o build the distributional thesaurus is AQUAINT-2. It is a collection of articles from press containing about 380 million words. The thesaurus entries are all the nouns in the corpus with a frequency > 10. That represents 25,000 entries (i.e. unique nouns), denoted by n in the remaining. The corpus is labeled in parts of speech by TreeTagger (Schmid, 1994). In this way, we can identify the names that form the thesaurus entries and thus compare to existing work. However this information is not used to build the thesaurus, ensuring the portability of the method to other languages, similarly to (Freitag et al., 2005). To evaluate the built thesauri, WordNet 3.0 synonyms (Miller, 1990) and Moby (Ward, 1996) are used as references, either separately, or jointly. These two resources exhibit quite different and complementary characteristics: on the one hand, WordNet indicates strong paradigmatic links between words (synonyms or quasi-synonyms). On the other hand, Moby groups words sharing more extended syntagmatic and paradigmatic relations, including synonymy, hyper/hypo-nymy, meronymy, but also many more complex types such as the composition of co-hyponymy and hyponymy (abolition – annulment, cataclysm – de"
C14-1067,P06-1045,0,0.733345,"fet and Dagan (2009), extended by Yamamoto and Asakura (2010), defined a bootstrapping method for modifying the weights of constituents in the distributional context of a word according to the similarity with its semantic neighbors. The nature of distributional contexts has been first considered through the distinction between windowbased and syntactic co-occurrents (Grefenstette, 1994; Curran and Moens, 2002). However, most of the work related to this issue has focused on the fact that the “traditional” representation of distributional contexts is very sparse and redundant, as illustrated by Hagiwara et al. (2006). Hence, several methods for dimension reduction were tested in this context: from Latent Semantic Analysis (Landauer and Dumais, 1997), extended for syntactic co-occurrents (Pad´o and Lapata, 2007), to Random Indexing (Sahlgren, 2001), Non-negative Matrix Factorization (Van de Cruys, 2010) and more recently, lexical representations learnt by neural networks (Huang et al., 2012; Mikolov et al., 2013). The work we present in this article follows a different perspective as our objective is to improve an existing distributional thesaurus by relying on its structure through a reranking of its neig"
C14-1067,P12-1092,0,0.0442096,"1994; Curran and Moens, 2002). However, most of the work related to this issue has focused on the fact that the “traditional” representation of distributional contexts is very sparse and redundant, as illustrated by Hagiwara et al. (2006). Hence, several methods for dimension reduction were tested in this context: from Latent Semantic Analysis (Landauer and Dumais, 1997), extended for syntactic co-occurrents (Pad´o and Lapata, 2007), to Random Indexing (Sahlgren, 2001), Non-negative Matrix Factorization (Van de Cruys, 2010) and more recently, lexical representations learnt by neural networks (Huang et al., 2012; Mikolov et al., 2013). The work we present in this article follows a different perspective as our objective is to improve an existing distributional thesaurus by relying on its structure through a reranking of its neighbors. Such approach was adopted to some extent by Zhitomirsky-Geffet and Dagan (2009) as it exploited the neighbors of an entry in an initial thesaurus for reweighting its distributional representation and finally, reranking its neighbors. Ferret (2013) proposed a more indirect method in which the reranking is based on the downgrading of the neighbors that are detected as not"
C14-1067,P98-2127,0,0.957837,"enses/by/4.0/ 709 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 709–720, Dublin, Ireland, August 23-29 2014. 3) Finally, on the basis of this work, we show how to use this confidence score and these probabilities to reorder the list of nearest neighbors (Section 6). To achieve this goal, we model the reranking as an optimization problem of assignments, solved by the Hungarian algorithm (Kuhn and Yaw, 1955). 2 Related work The notion of distributional thesaurus, as it was initially defined by Grefenstette (1994), followed by Lin (1998a) and Curran and Moens (2002), is not often considered specifically, probably because of its strong link with the notion of semantic similarity. As a consequence, the improvement of distributional thesauri has been first a side effect of the improvement of the distributional similarity measures used for their building and more precisely, of the distributional data they rely on. Both the nature of the constituents of distributional contexts and their weighting have been considered in this regard. Concerning their weighting, Broda et al. (2009) proposed to turn the weights of context constituen"
C14-1067,N13-1090,0,0.0484281,"ns, 2002). However, most of the work related to this issue has focused on the fact that the “traditional” representation of distributional contexts is very sparse and redundant, as illustrated by Hagiwara et al. (2006). Hence, several methods for dimension reduction were tested in this context: from Latent Semantic Analysis (Landauer and Dumais, 1997), extended for syntactic co-occurrents (Pad´o and Lapata, 2007), to Random Indexing (Sahlgren, 2001), Non-negative Matrix Factorization (Van de Cruys, 2010) and more recently, lexical representations learnt by neural networks (Huang et al., 2012; Mikolov et al., 2013). The work we present in this article follows a different perspective as our objective is to improve an existing distributional thesaurus by relying on its structure through a reranking of its neighbors. Such approach was adopted to some extent by Zhitomirsky-Geffet and Dagan (2009) as it exploited the neighbors of an entry in an initial thesaurus for reweighting its distributional representation and finally, reranking its neighbors. Ferret (2013) proposed a more indirect method in which the reranking is based on the downgrading of the neighbors that are detected as not similar to their entry"
C14-1067,J07-2002,0,0.129938,"Missing"
C14-1067,W10-3906,0,0.806014,"tic similarity. As a consequence, the improvement of distributional thesauri has been first a side effect of the improvement of the distributional similarity measures used for their building and more precisely, of the distributional data they rely on. Both the nature of the constituents of distributional contexts and their weighting have been considered in this regard. Concerning their weighting, Broda et al. (2009) proposed to turn the weights of context constituents into ranks to make them less dependent on a specific weighting function while Zhitomirsky-Geffet and Dagan (2009), extended by Yamamoto and Asakura (2010), defined a bootstrapping method for modifying the weights of constituents in the distributional context of a word according to the similarity with its semantic neighbors. The nature of distributional contexts has been first considered through the distinction between windowbased and syntactic co-occurrents (Grefenstette, 1994; Curran and Moens, 2002). However, most of the work related to this issue has focused on the fact that the “traditional” representation of distributional contexts is very sparse and redundant, as illustrated by Hagiwara et al. (2006). Hence, several methods for dimension"
C14-1067,J09-3004,0,0.724789,"cause of its strong link with the notion of semantic similarity. As a consequence, the improvement of distributional thesauri has been first a side effect of the improvement of the distributional similarity measures used for their building and more precisely, of the distributional data they rely on. Both the nature of the constituents of distributional contexts and their weighting have been considered in this regard. Concerning their weighting, Broda et al. (2009) proposed to turn the weights of context constituents into ranks to make them less dependent on a specific weighting function while Zhitomirsky-Geffet and Dagan (2009), extended by Yamamoto and Asakura (2010), defined a bootstrapping method for modifying the weights of constituents in the distributional context of a word according to the similarity with its semantic neighbors. The nature of distributional contexts has been first considered through the distinction between windowbased and syntactic co-occurrents (Grefenstette, 1994; Curran and Moens, 2002). However, most of the work related to this issue has focused on the fact that the “traditional” representation of distributional contexts is very sparse and redundant, as illustrated by Hagiwara et al. (200"
C14-1067,C98-2122,0,\N,Missing
C16-1173,J06-1003,0,0.143033,"stributional thesauri Distributional thesauri rely on the famous formula of Firth (Firth, 1957): ”You should know a word by the company it keeps”. In such thesauri, each word is semantically characterized by all the contexts in which it appears. The semantic neighbors of an entry word are then words whose contexts are similar to that of the entry. Since the pioneering work of Grefenstette (1994) and Lin (1998), many studies have examined distributional thesaurus building. The semantic link considered between an entry word and its neighbors is varied: synonyms, hyperonymy, hyponymy or another (Budanitsky and Hirst, 2006; Adam et al., 2013, for a discussion). Despite their diversity, these links are interesting for many applications related to Natural Language Processing. The various aspects of the thesaurus building is therefore a research field still very active. One first step concerns the definition of the distributional context of a given word. The graphical contexts simply consider the words appearing around the occurrences of the target word, while syntactic contexts are formed with the syntactic predicates and arguments of the occurrences of the target word. The latter, if it is considered more accura"
C16-1173,L16-1588,1,0.123201,"ient techniques to build distributional thesaurus; in particular, Claveau et al. (2014) have already shown that Information Retrieval (IR) tools and concepts can be successfully used to build a thesaurus. In this paper, we address the problem of the evaluation of such thesauri or embedding models. Several evaluation scenarii are considered: direct evaluation through reference lexicons and specially crafted datasets, and indirect evaluation through a third party tasks, namely lexical subsitution and Information Retrieval. For this latter task, we adopt the query expansion framework proposed by Claveau and Kijak (2016). Through several experiments, we first show that the recent techniques for building distributional thesaurus outperform the word2vec approach, whatever the evaluation scenario. We also highlight the differences between the evaluation scenarii, which may lead to very different conclusions when comparing distributional models. Last, we study the effect of some parameters of the distributional models on these various evaluation scenarii. 1 Introduction For years, distributional semantic has aimed at building thesauri (or lexicons) automatically from text corpora. For a given input (ie. a given w"
C16-1173,C14-1067,1,0.385839,"; Bollegala et al., 2007; Sahami and Heilman, 2006; Ruiz-Casado et al., 2005). Finally, given that the ”traditional” distributional representation of contexts is sparse and redundant (Hagiwara et al., 2006), several methods for dimension reduction were tested: from Latent Semantic Analysis (Landauer and Dumais, 1997b; Pad´o and Lapata, 2007; Van de Cruys et al., 2011) to Random Indexing (Sahlgren, 2001), through factorization by non-negative matrices (Van de Cruys, 2010). The problem of the construction of distributional thesaurus may be expressed in a simple way as a conventional IR problem (Claveau et al., 2014). All contexts of a target word are then represented as a document (or query), and distributional neighbors of the target word are the sets of similar contexts. This formulation of distributional neighbors search process offers interesting research tracks and easily accessible tools. 2.2 Tested models In this paper, several distributional models are tested. A first group of models, that may be called traditional distributional models are considered. In the following sub-section we report results obtained by a state-of-the art approach, hereafter denoted base, that uses a cosine similarity and"
C16-1173,P13-1055,0,0.146364,"re then represented as a document (or query), and distributional neighbors of the target word are the sets of similar contexts. This formulation of distributional neighbors search process offers interesting research tracks and easily accessible tools. 2.2 Tested models In this paper, several distributional models are tested. A first group of models, that may be called traditional distributional models are considered. In the following sub-section we report results obtained by a state-of-the art approach, hereafter denoted base, that uses a cosine similarity and weighting by mutual information (Ferret, 2013), an improved version (rerank) which uses machine learning technique to rerank neighbors (Ferret, 2013), and another version (synt) based on syntactic contexts (Ferret, 2014) rather than graphic ones are also tested. As explained in Claveau et al. (2014), the problem of building a distributional thesaurus can be translated into a problem of searching similar documents and can therefore be carried out with IR techniques. In this context, all the contexts of a given word in a corpus are collected; this set of contexts forms what is considered as a document. Building an entry in the thesaurus, ie"
C16-1173,P06-1045,0,0.0301707,"e the influence of weighting functions. Some bootstrap methods were also proposed to modify the weight of the contexts of a word, by taking into account its semantic neighbors (Zhitomirsky-Geffet and Dagan, 2009; Yamamoto and Asakura, 2010). Another example of relationship between distributional semantics and IR is the use of search engines to collect co-occurrence information or contexts on the web (Turney, 2001; Bollegala et al., 2007; Sahami and Heilman, 2006; Ruiz-Casado et al., 2005). Finally, given that the ”traditional” distributional representation of contexts is sparse and redundant (Hagiwara et al., 2006), several methods for dimension reduction were tested: from Latent Semantic Analysis (Landauer and Dumais, 1997b; Pad´o and Lapata, 2007; Van de Cruys et al., 2011) to Random Indexing (Sahlgren, 2001), through factorization by non-negative matrices (Van de Cruys, 2010). The problem of the construction of distributional thesaurus may be expressed in a simple way as a conventional IR problem (Claveau et al., 2014). All contexts of a target word are then represented as a document (or query), and distributional neighbors of the target word are the sets of similar contexts. This formulation of dist"
C16-1173,P12-1092,0,0.0232916,"gh a task, such as the lexical substitution task proposed at SemEval 2007 (McCarthy and Navigli, 2009). The goal is to replace a word in a sentence by a neighbor (given by the evaluated thesaurus) and verify that it did not change the meaning of the sentence, by comparing the obtained results to the substitutions proposed by humans. In such a task, the exact synonyms are favored over other types of semantic relationships. Several studies use distributional information within an IR framework (Besanc¸on et al., 1999; Billhardt et al., 2002), like recent lexical representations such as word2vec (Huang et al., 2012; Mikolov et al., 2013). The aim is to improve the representation of documents and/or the Relevance Status Value function (RSV, ie. the function used in IR systems to rank the answers to a query according to their supposed relevance), by exploiting the similarities between word contexts. Nevertheless, the process of creating the distributional thesaurus is not dissociated from the IR process in these studies, which makes the evaluation of the only distributional information contribution impossible. Recently, we have proposed to specifically evaluate the distributional thesauri in IR by using s"
C16-1173,P98-2127,0,0.6461,"d resources or by IR are respectively discussed in Section 4 and Section 5. Finally, we present some conclusions and perspectives about this work in the last section. 2 2.1 Related work Building distributional thesauri Distributional thesauri rely on the famous formula of Firth (Firth, 1957): ”You should know a word by the company it keeps”. In such thesauri, each word is semantically characterized by all the contexts in which it appears. The semantic neighbors of an entry word are then words whose contexts are similar to that of the entry. Since the pioneering work of Grefenstette (1994) and Lin (1998), many studies have examined distributional thesaurus building. The semantic link considered between an entry word and its neighbors is varied: synonyms, hyperonymy, hyponymy or another (Budanitsky and Hirst, 2006; Adam et al., 2013, for a discussion). Despite their diversity, these links are interesting for many applications related to Natural Language Processing. The various aspects of the thesaurus building is therefore a research field still very active. One first step concerns the definition of the distributional context of a given word. The graphical contexts simply consider the words ap"
C16-1173,N13-1090,0,0.737266,"the input word’s one. In practice, this distributional assumption is set such that two words would be considered close if their occurrences share similar contexts. These contexts are typically co-occurring words in a limited window around the considered words, or words syntactically linked. Recently, many studies have explored new techniques to represent the word (or phrase or document) through embeddings: most often, words are thus represented in a vector space, such that two words with close meanings are close in this space. One of the most popular approach is the famous word2vec technique (Mikolov et al., 2013). Of course, such embedding techniques rely on the same assumption than ”traditional” distributional semantics (although many studies do not acknowledge it clearly). Evaluating these thesauri or embeddings remains a crucial point to assess the quality of the construction methods and parameters used. In this article, we propose to examine this evaluation problem with different evaluation protocols. Indeed, a commonly used approach is to compare the generated thesauri to one or several reference lexicons. This evaluation procedure, called ’intrinsic’, has the advantage of being straightforward a"
C16-1173,J07-2002,0,0.369942,"Missing"
C16-1173,D11-1094,0,0.0537227,"Missing"
C16-1173,W10-3906,0,0.019005,"lated with weighting schemes and relevance functions used in IR (with the exception of Vechtomova and Robertson (2012) in the slightly different context of computing similarities between named entities). However, the weighting of contexts provides more relevant neighbors. Broda et al. (2009) thereby proposed to consider the ranks rather than directly the weights of contexts to overcome the influence of weighting functions. Some bootstrap methods were also proposed to modify the weight of the contexts of a word, by taking into account its semantic neighbors (Zhitomirsky-Geffet and Dagan, 2009; Yamamoto and Asakura, 2010). Another example of relationship between distributional semantics and IR is the use of search engines to collect co-occurrence information or contexts on the web (Turney, 2001; Bollegala et al., 2007; Sahami and Heilman, 2006; Ruiz-Casado et al., 2005). Finally, given that the ”traditional” distributional representation of contexts is sparse and redundant (Hagiwara et al., 2006), several methods for dimension reduction were tested: from Latent Semantic Analysis (Landauer and Dumais, 1997b; Pad´o and Lapata, 2007; Van de Cruys et al., 2011) to Random Indexing (Sahlgren, 2001), through factoriz"
C16-1173,J09-3004,0,0.0240695,"(Turney and Pantel, 2010), but unrelated with weighting schemes and relevance functions used in IR (with the exception of Vechtomova and Robertson (2012) in the slightly different context of computing similarities between named entities). However, the weighting of contexts provides more relevant neighbors. Broda et al. (2009) thereby proposed to consider the ranks rather than directly the weights of contexts to overcome the influence of weighting functions. Some bootstrap methods were also proposed to modify the weight of the contexts of a word, by taking into account its semantic neighbors (Zhitomirsky-Geffet and Dagan, 2009; Yamamoto and Asakura, 2010). Another example of relationship between distributional semantics and IR is the use of search engines to collect co-occurrence information or contexts on the web (Turney, 2001; Bollegala et al., 2007; Sahami and Heilman, 2006; Ruiz-Casado et al., 2005). Finally, given that the ”traditional” distributional representation of contexts is sparse and redundant (Hagiwara et al., 2006), several methods for dimension reduction were tested: from Latent Semantic Analysis (Landauer and Dumais, 1997b; Pad´o and Lapata, 2007; Van de Cruys et al., 2011) to Random Indexing (Sahl"
C16-1173,W11-2501,0,\N,Missing
C16-1173,C98-2122,0,\N,Missing
claveau-2008-automatic,W97-0119,0,\N,Missing
claveau-2008-automatic,C04-1117,0,\N,Missing
claveau-2008-automatic,J93-2003,0,\N,Missing
claveau-2008-automatic,W02-0505,0,\N,Missing
claveau-2008-automatic,J90-2002,0,\N,Missing
claveau-2008-automatic,C04-1031,0,\N,Missing
claveau-2008-automatic,D07-1092,0,\N,Missing
claveau-2008-automatic,H91-1026,0,\N,Missing
claveau-2008-automatic,tsuji-etal-2002-extracting,0,\N,Missing
claveau-2008-automatic,J98-4003,0,\N,Missing
claveau-2008-automatic,W99-0703,0,\N,Missing
claveau-kijak-2014-generating,pearce-2002-comparative,0,\N,Missing
claveau-kijak-2014-generating,W02-1403,0,\N,Missing
claveau-kijak-2014-generating,N07-1047,0,\N,Missing
claveau-kijak-2014-generating,R11-1048,1,\N,Missing
claveau-kijak-2014-generating,tsuji-etal-2002-extracting,0,\N,Missing
claveau-kijak-2014-generating,J98-4003,0,\N,Missing
F12-2007,A00-2004,0,0.0987928,"Missing"
F12-2007,J97-1003,0,0.201357,"Missing"
F12-2007,W98-1123,0,0.0916018,"Missing"
F12-2007,J02-1002,0,0.0535573,"Missing"
F12-2007,P01-1064,0,0.0706657,"Missing"
F12-2031,J96-2004,0,0.158413,"Missing"
F12-2031,W09-3002,0,0.0277441,"Missing"
F12-2031,fort-claveau-2012-annotating,1,0.809654,"Missing"
F12-2031,W11-0411,1,0.870748,"Missing"
F12-2031,2009.jeptalnrecital-court.23,0,0.0609105,"Missing"
F13-1019,W99-0613,0,0.252601,"Missing"
F13-1019,N09-1019,0,0.0362348,"Missing"
F13-1019,fort-claveau-2012-annotating,1,0.830429,"Missing"
F13-1019,P07-1094,0,0.035981,"Missing"
F13-1019,D07-1073,0,0.0853568,"Missing"
F13-1019,E06-3004,0,0.0564192,"Missing"
F13-1019,P10-1052,0,0.0878997,"Missing"
F13-1019,J94-2001,0,0.162144,"Missing"
F13-1019,P09-1057,0,0.033131,"Missing"
F13-1019,P05-1044,0,0.105195,"Missing"
F13-1019,wang-etal-2012-evaluation,0,0.0321243,"Missing"
F13-1019,W09-2208,0,0.0517125,"Missing"
F14-1020,W02-0908,0,0.1159,"Missing"
F14-1020,P13-1055,1,0.853386,"Missing"
F14-1020,W05-0604,0,0.0542489,"Missing"
F14-1020,P06-1045,0,0.0591494,"Missing"
F14-1020,P12-1092,0,0.091586,"Missing"
F14-1020,P98-2127,0,0.592531,"Missing"
F14-1020,N13-1090,0,0.0509347,"Missing"
F14-1020,J07-2002,0,0.0318426,"Missing"
F14-1020,J09-3004,0,0.0373963,"Missing"
fort-claveau-2012-annotating,W09-3025,1,\N,Missing
fort-claveau-2012-annotating,J08-4004,0,\N,Missing
fort-claveau-2012-annotating,J96-2004,0,\N,Missing
fort-claveau-2012-annotating,W11-0411,1,\N,Missing
L16-1190,W11-0705,0,0.0525275,"(sets of synonyms) which are linked by different semantic relations such as hypernymy, or meronymy. SentiWordNet is an extension of WordNet that assigns to each synsets three sentiment scores : positivity, negativity and objectivity. Those resources have been used extensively to build opinion analysis system. They can be used as it, or to extend or build sentiment lexicons. For example, (Toh and Wang, 2014) uses the syntactic categories from WordNet as features for a CRF to extract aspects and WordNet relations such as antonymy and synonymy to extend an existing opinion lexicons. Similarly, (Agarwal et al., 2011) look for synonyms in WordNet to find the polarity of words absent from an opinion lexicon. Going further, (Badaro et al., 2014) build an Arabic version of SentiWordNet (an extension of WordNet that assigns to each synsets three sentiment scores : positivity, negativity and objectivity) by combining an Arabic version of WordNet and the English version of WordNet and SentiWordNet. While the previous studies build on a manually constructed lexicon, many systems have been proposed to build resources automatically based on lexical similarity for opinion mining. For that purpose, rich word represen"
L16-1190,W14-3623,0,0.0313172,"of WordNet that assigns to each synsets three sentiment scores : positivity, negativity and objectivity. Those resources have been used extensively to build opinion analysis system. They can be used as it, or to extend or build sentiment lexicons. For example, (Toh and Wang, 2014) uses the syntactic categories from WordNet as features for a CRF to extract aspects and WordNet relations such as antonymy and synonymy to extend an existing opinion lexicons. Similarly, (Agarwal et al., 2011) look for synonyms in WordNet to find the polarity of words absent from an opinion lexicon. Going further, (Badaro et al., 2014) build an Arabic version of SentiWordNet (an extension of WordNet that assigns to each synsets three sentiment scores : positivity, negativity and objectivity) by combining an Arabic version of WordNet and the English version of WordNet and SentiWordNet. While the previous studies build on a manually constructed lexicon, many systems have been proposed to build resources automatically based on lexical similarity for opinion mining. For that purpose, rich word representations have been proposed to compute the lexical similarity. In (Castellucci et al., 2013), the authors combine three kernels i"
L16-1190,S13-2060,0,0.0438869,"Missing"
L16-1190,C14-1067,1,0.930625,"perplane for which the polarity of words in the vector space depends on its position relative to the hyperplane. They evaluate their model by predicting the polarity of movie reviews. The most common way to evaluate sentiment analysis systems, is by comparing the prediction of the systems against a gold corpus. For example, SEMEVAL (Nakov et al., 2013; Rosenthal et al., 2014) proposes a task in which participants are asked to predict the polarity (positive, negative, neutral) of tweets and SMS. Another task focuses 1196 Model Ferret 2013 base Ferret 2013 best rerank Ferret 2014 synt Spectral (Claveau et al., 2014) W2V dim=50 w=5 W2V dim=100 w=5 W2V dim=200 w=5 W2V dim=300 w=5 W2V dim=400 w=5 W2V dim=50 w=9 W2V dim=100 w=9 W2V dim=200 w=9 W2V dim=300 w=9 W2V dim=400 w=9 W2V Google news MAP 5,6 6,1 7,9 8,97 2,89 3,65 3,92 5,25 5,06 3,12 4,14 4,42 4,07 4,39 5,82 R-Prec 7,7 8,4 10,7 10,94 3,89 4,84 5,44 6,25 6,43 4,11 5,55 5,60 5,53 5,51 7,51 P@1 22,5 24,8 29,4 31,05 13,48 18,49 22,18 18,67 20,37 13,11 17,18 17,69 20,50 17,81 13,28 P@5 14,1 15,4 18,9 18,44 7,36 9,62 11,39 10,72 11,44 7,80 9,25 10,71 11,13 9,95 11,60 P@10 10,8 11,7 14,6 13,76 5,44 7,04 8,32 7,73 8,29 5,68 6,79 7,47 8,02 7,43 8,94 P@50 5,3 5"
L16-1190,D09-1061,0,0.0815288,"Missing"
L16-1190,N15-1184,0,0.0380321,", many systems have been proposed to build resources automatically based on lexical similarity for opinion mining. For that purpose, rich word representations have been proposed to compute the lexical similarity. In (Castellucci et al., 2013), the authors combine three kernels into a SVM model where each kernel tries to capture an aspect of the opinion. A Bag of Word Kernel is used to compute the similarity of ngrams between tweets; A lexical semantic kernel build a Word Space from a matrix of co-occurrence context scores; And a smoothed partial tree kernel handles the syntactic information. (Faruqui et al., 2015) use an ontology to improve the effectiveness of word vectors and evaluate their work on word similarity and sentiment analysis tasks. They refine word vectors by minimizing the distance of words within the vector space and between vectors projected in the ontology. In (Maas et al., 2011), the authors build a vector space using a continuous mixture distribution over words in an unsupervised fashion. Then, they use annotated data to find an hyperplane for which the polarity of words in the vector space depends on its position relative to the hyperplane. They evaluate their model by predicting t"
L16-1190,P98-2127,0,0.211303,"participants are asked to identify and summarize the opinions expressed towards all aspects of an entity. An aspect is a constituent of an entity targeted by an opinion, for example, an aspect of the entity laptop is its battery. Yet, these task-based evaluations do not allow for a direct evaluation of the lexical similarity to represent the sentiment properties of the words, as provided by word embeddings or spectral representations. 3. 3.1. Lexical and distributional semantic similarities From distributional semantics to word embeddings Since the pioneering work of (Grefenstette, 1994) and (Lin, 1998), building distributional thesauri has been widely studied. They all rely on the hypothesis that each word is semantically characterized by all the contexts in which it appears. Many techniques, implementing this hypothesis has been proposed, and recently, (Claveau et al., 2014) proposed to use Information Retrieval metrics to build distributional thesauri represented as a (weighted) graphs of neighbors. A word is thus represented by its links with other words. This distributional method, which gives state-ofthe-art results on lexical similarity tasks, is called Spectral representation hereaft"
L16-1190,P11-1015,0,0.0604378,"el where each kernel tries to capture an aspect of the opinion. A Bag of Word Kernel is used to compute the similarity of ngrams between tweets; A lexical semantic kernel build a Word Space from a matrix of co-occurrence context scores; And a smoothed partial tree kernel handles the syntactic information. (Faruqui et al., 2015) use an ontology to improve the effectiveness of word vectors and evaluate their work on word similarity and sentiment analysis tasks. They refine word vectors by minimizing the distance of words within the vector space and between vectors projected in the ontology. In (Maas et al., 2011), the authors build a vector space using a continuous mixture distribution over words in an unsupervised fashion. Then, they use annotated data to find an hyperplane for which the polarity of words in the vector space depends on its position relative to the hyperplane. They evaluate their model by predicting the polarity of movie reviews. The most common way to evaluate sentiment analysis systems, is by comparing the prediction of the systems against a gold corpus. For example, SEMEVAL (Nakov et al., 2013; Rosenthal et al., 2014) proposes a task in which participants are asked to predict the p"
L16-1190,S13-2053,0,0.0262794,"scores are independent: one synset may have a non-zero score for both positive and negative values. SentiWordNet also defines an ’objective’ score for each synset as: 1 − (positive_value + negative_value). Table 6 reports the results with the same experiment settings as for the ANEW lexicon. Here again, the very low coefficients tend to show that there are almost no correlation between the sentiment-based proximity and the semantic one, either computed (Spectral and Word2Vec) or manually assessed (SimLex999). 4.3. Building classes of similar words: NRC emotion lexicon The NRC emotion lexicon (Mohammad et al., 2013) is a large list of words associated with eight emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust) and two sentiments (negative and positive). These emotions and sentiments are encoded as binary properties (the word has or not the emotional property), that were manually obtained through Amazon’s Mechanical Turk. Given a word with its emotion and valence properties, we want to know if the lexical similarity helps finding words sharing the exact same emotion and valence properties. In order to evaluate that, we set up the following experiment: given a query word (ex"
L16-1190,S13-2052,0,0.0414896,"tance of words within the vector space and between vectors projected in the ontology. In (Maas et al., 2011), the authors build a vector space using a continuous mixture distribution over words in an unsupervised fashion. Then, they use annotated data to find an hyperplane for which the polarity of words in the vector space depends on its position relative to the hyperplane. They evaluate their model by predicting the polarity of movie reviews. The most common way to evaluate sentiment analysis systems, is by comparing the prediction of the systems against a gold corpus. For example, SEMEVAL (Nakov et al., 2013; Rosenthal et al., 2014) proposes a task in which participants are asked to predict the polarity (positive, negative, neutral) of tweets and SMS. Another task focuses 1196 Model Ferret 2013 base Ferret 2013 best rerank Ferret 2014 synt Spectral (Claveau et al., 2014) W2V dim=50 w=5 W2V dim=100 w=5 W2V dim=200 w=5 W2V dim=300 w=5 W2V dim=400 w=5 W2V dim=50 w=9 W2V dim=100 w=9 W2V dim=200 w=9 W2V dim=300 w=9 W2V dim=400 w=9 W2V Google news MAP 5,6 6,1 7,9 8,97 2,89 3,65 3,92 5,25 5,06 3,12 4,14 4,42 4,07 4,39 5,82 R-Prec 7,7 8,4 10,7 10,94 3,89 4,84 5,44 6,25 6,43 4,11 5,55 5,60 5,53 5,51 7,51"
L16-1190,S14-2004,0,0.0729124,"Missing"
L16-1190,S14-2009,0,0.0144449,"n the vector space and between vectors projected in the ontology. In (Maas et al., 2011), the authors build a vector space using a continuous mixture distribution over words in an unsupervised fashion. Then, they use annotated data to find an hyperplane for which the polarity of words in the vector space depends on its position relative to the hyperplane. They evaluate their model by predicting the polarity of movie reviews. The most common way to evaluate sentiment analysis systems, is by comparing the prediction of the systems against a gold corpus. For example, SEMEVAL (Nakov et al., 2013; Rosenthal et al., 2014) proposes a task in which participants are asked to predict the polarity (positive, negative, neutral) of tweets and SMS. Another task focuses 1196 Model Ferret 2013 base Ferret 2013 best rerank Ferret 2014 synt Spectral (Claveau et al., 2014) W2V dim=50 w=5 W2V dim=100 w=5 W2V dim=200 w=5 W2V dim=300 w=5 W2V dim=400 w=5 W2V dim=50 w=9 W2V dim=100 w=9 W2V dim=200 w=9 W2V dim=300 w=9 W2V dim=400 w=9 W2V Google news MAP 5,6 6,1 7,9 8,97 2,89 3,65 3,92 5,25 5,06 3,12 4,14 4,42 4,07 4,39 5,82 R-Prec 7,7 8,4 10,7 10,94 3,89 4,84 5,44 6,25 6,43 4,11 5,55 5,60 5,53 5,51 7,51 P@1 22,5 24,8 29,4 31,05"
L16-1190,S14-2038,0,0.0204386,"Some studies have proposed to use existing resources to build or extend opinion lexicons. Among them, WordNet is certainly one of the most known semantic lexicon for English. Words are regrouped into synsets (sets of synonyms) which are linked by different semantic relations such as hypernymy, or meronymy. SentiWordNet is an extension of WordNet that assigns to each synsets three sentiment scores : positivity, negativity and objectivity. Those resources have been used extensively to build opinion analysis system. They can be used as it, or to extend or build sentiment lexicons. For example, (Toh and Wang, 2014) uses the syntactic categories from WordNet as features for a CRF to extract aspects and WordNet relations such as antonymy and synonymy to extend an existing opinion lexicons. Similarly, (Agarwal et al., 2011) look for synonyms in WordNet to find the polarity of words absent from an opinion lexicon. Going further, (Badaro et al., 2014) build an Arabic version of SentiWordNet (an extension of WordNet that assigns to each synsets three sentiment scores : positivity, negativity and objectivity) by combining an Arabic version of WordNet and the English version of WordNet and SentiWordNet. While t"
L16-1588,W11-2501,0,0.106957,"in numerous previous studies. Among the lexicons regularly used as references, let us cite WordSim 353 (Gabrilovich and Markovitch, 2007), or those used by (Ferret, 2013) that exploits larger resources, ie. synonyms for WordNet 3.0 (Miller, 1990) and the Moby thesaurus (Ward, 1996). In this paper, we also use these two resources for our intrinsic assessment; see below for a presentation. Other resources are not directly lexicons, but data sets that can be used for direct assessment, as the set of synonyms from the TOEFL test (Landauer and Dumais, 1997a) or the semantic relationships in BLESS (Baroni and Lenci, 2011). Direct assessment is appealing for its simplicity, but it raises the question of the adequacy of lexicons used as references. Therefore, several studies have proposed indirect assessments through a task requiring the generated thesauri. One well known task is the lexical substitution as proposed at SemEval 2007 (McCarthy and Navigli, 2009). Given a word in a sentence, the goal is to replace this word by one of its neighbors and to check that this does not alter the meaning of the sentence. The results obtained are then compared to the substitutions proposed by humans. This task therefore foc"
L16-1588,J06-1003,0,0.587438,"998). All these works are based on the distributional assumption (Firth, 1957) summarized by the famous formula: ”You should know a word by the company it keeps”. It is therefore considered that each word is semantically characterized by all the contexts in which it appears. For an entry word in a thesaurus, words that share similar contexts are proposed; these are called semantic neighbors thereafter. In the studies, the nature of the semantic link between an entry and its neighbors is variable; the neighbors can be synonyms of the entry, hypernyms, hyponyms or other types of semantic links (Budanitsky and Hirst, 2006; Adam et al., 2013, for a discussion)). These semantic links, even if they are very diverse, are nevertheless useful for many applications related to Natural Language Processing. This explains why this field of research is still very active, with contributions on various aspects related to the construction of the thesaurus. First, different options of what should be considered as a distributional context has been explored. One usually distinguishes between graphical contexts and syntactic contexts. The former are simply the words appearing around the occurrences of a target word. The second a"
L16-1588,C14-1067,1,0.441463,"evaluation procedure, called ’intrinsic’, has the advantage of being straightforward and simple as it allows to estimate the quality and completeness of the generated thesaurus. However, it is based on reference lexicons whose own completeness, quality, or simply their availability for the considered domain/language/genre are not always granted. In this article1 , we propose to examine those two aspects – the construction and the evaluation of distributional thesauri – by using information retrieval (IR) both as a set of techniques and as a use case. Concerning the construction, recent work (Claveau et al., 2014) showed that IR systems could advantageously be used to implement distributional analysis systems. We propose in this paper to further explore this IR approach to build thesauri. We examine the interest of various classic models of IR for distributional analysis and compare them with the state-of-the-art. Regarding the evaluation, we offer an extrinsic evaluation of the generated thesauri through a conventional IR task. We are then able to compare these results with those of the intrinsic evaluation, and therefore to judge the relevance of 1 This work was partly funded via the BigClin and LIMA"
L16-1588,P13-1055,0,0.280699,"Section 3.1.. 2.2. Evaluating distributional thesauri As mentioned previously, the evaluation of generated thesauri is either intrinsic, by comparison with a reference resource, or extrinsic, through their use in a specific task. In the case of intrinsic assessment, reference lexicons are needed. It is then easy to calculate precision, recall or any other measure of quality of the generated distributional thesaurus. This approach was used in numerous previous studies. Among the lexicons regularly used as references, let us cite WordSim 353 (Gabrilovich and Markovitch, 2007), or those used by (Ferret, 2013) that exploits larger resources, ie. synonyms for WordNet 3.0 (Miller, 1990) and the Moby thesaurus (Ward, 1996). In this paper, we also use these two resources for our intrinsic assessment; see below for a presentation. Other resources are not directly lexicons, but data sets that can be used for direct assessment, as the set of synonyms from the TOEFL test (Landauer and Dumais, 1997a) or the semantic relationships in BLESS (Baroni and Lenci, 2011). Direct assessment is appealing for its simplicity, but it raises the question of the adequacy of lexicons used as references. Therefore, several"
L16-1588,P06-1045,0,0.771887,"l studies have examined the problem of weighting contexts to get more relevant neighbors. For example, (Broda et al., 2009) proposed to not consider directly the weight of contexts, but their ranks in order to overcome the influence of weighting functions. Considering the semantic neighbors of a word, others suggested bootstrap methods to change the weight of its contexts (Zhitomirsky-Geffet and Dagan, 2009; Yamamoto and Asakura, 2010). Moreover, many studies are based on the fact that the ”traditional” distributional representation of contexts is very sparse and redundant, as illustrated by (Hagiwara et al., 2006). In this context, several dimension reduction methods also used in IR were tested: from Latent Semantic Indexing (Landauer and Dumais, 1997b; Pad´o and Lapata, 2007; Van de Cruys et al., 2011) to Random Indexing (Sahlgren, 2001), through the non-negative matrix factorization (Van de Cruys, 2010). Recently, (Claveau et al., 2014) proposed to make a deeper analogy between the research on distributional neighbors and a conventional IR problem. All contexts of all the occurrences of a word can indeed be represented as one document or a query, allowing to easily find similar words, or more precise"
L16-1588,P12-1092,0,0.0321979,"this does not alter the meaning of the sentence. The results obtained are then compared to the substitutions proposed by humans. This task therefore focus on exact synonyms to the detriment of other types of semantic relationships. To our knowledge, the evaluation of distributional thesaurus through IR tasks has not been explored. Of course, the use of information that can be called distributional within an IR framework has been the subject of several studies (Besanc¸on et al., 1999; Billhardt et al., 2002). It continues today by the work on lexical representations learned by neural networks (Huang et al., 2012; Mikolov et al., 2013). In every case, these studies aim at taking advantage of similarities between word contexts to improve the representation of documents and/or the Relevance Status Value function (RSV). However, these studies do not separate the process of creating the distributional thesaurus from the IR process, which makes impossible the evaluation of the contribution of the only distributional information. In our case, the extrinsic IR evaluation we propose (see Section 4.) is simply based on the use of semantic neighbors to expand queries; the rest of the IR system is standard. This"
L16-1588,P98-2127,0,0.730559,"tional Research Agency under reference ANR-10-LABX-0701. these assessment scenarios. After a state-of-the art (next section), the article addresses these two contributions successively: the aspects related to the construction of thesauri are presented in Section 3., while those about the evaluation by IR are in Section 4.. Finally, we present some conclusions and perspectives about this work in the last section. 2. 2.1. Related work Building distributional thesauri Building distributional thesauri has been the subject of many studies, including the pioneering work of (Grefenstette, 1994) and (Lin, 1998). All these works are based on the distributional assumption (Firth, 1957) summarized by the famous formula: ”You should know a word by the company it keeps”. It is therefore considered that each word is semantically characterized by all the contexts in which it appears. For an entry word in a thesaurus, words that share similar contexts are proposed; these are called semantic neighbors thereafter. In the studies, the nature of the semantic link between an entry and its neighbors is variable; the neighbors can be synonyms of the entry, hypernyms, hyponyms or other types of semantic links (Buda"
L16-1588,N13-1090,0,0.0691487,"the meaning of the sentence. The results obtained are then compared to the substitutions proposed by humans. This task therefore focus on exact synonyms to the detriment of other types of semantic relationships. To our knowledge, the evaluation of distributional thesaurus through IR tasks has not been explored. Of course, the use of information that can be called distributional within an IR framework has been the subject of several studies (Besanc¸on et al., 1999; Billhardt et al., 2002). It continues today by the work on lexical representations learned by neural networks (Huang et al., 2012; Mikolov et al., 2013). In every case, these studies aim at taking advantage of similarities between word contexts to improve the representation of documents and/or the Relevance Status Value function (RSV). However, these studies do not separate the process of creating the distributional thesaurus from the IR process, which makes impossible the evaluation of the contribution of the only distributional information. In our case, the extrinsic IR evaluation we propose (see Section 4.) is simply based on the use of semantic neighbors to expand queries; the rest of the IR system is standard. This allows us to easily as"
L16-1588,J07-2002,0,0.455949,"Missing"
L16-1588,D11-1094,0,0.549131,"Missing"
L16-1588,W10-3906,0,0.839035,"and relevance functions used in IR (with the exception of (Vechtomova and Robertson, 2012) in the slightly different context of computing similarities between named entities). Yet, several studies have examined the problem of weighting contexts to get more relevant neighbors. For example, (Broda et al., 2009) proposed to not consider directly the weight of contexts, but their ranks in order to overcome the influence of weighting functions. Considering the semantic neighbors of a word, others suggested bootstrap methods to change the weight of its contexts (Zhitomirsky-Geffet and Dagan, 2009; Yamamoto and Asakura, 2010). Moreover, many studies are based on the fact that the ”traditional” distributional representation of contexts is very sparse and redundant, as illustrated by (Hagiwara et al., 2006). In this context, several dimension reduction methods also used in IR were tested: from Latent Semantic Indexing (Landauer and Dumais, 1997b; Pad´o and Lapata, 2007; Van de Cruys et al., 2011) to Random Indexing (Sahlgren, 2001), through the non-negative matrix factorization (Van de Cruys, 2010). Recently, (Claveau et al., 2014) proposed to make a deeper analogy between the research on distributional neighbors an"
L16-1588,J09-3004,0,0.325486,"without the usual weighting schemes and relevance functions used in IR (with the exception of (Vechtomova and Robertson, 2012) in the slightly different context of computing similarities between named entities). Yet, several studies have examined the problem of weighting contexts to get more relevant neighbors. For example, (Broda et al., 2009) proposed to not consider directly the weight of contexts, but their ranks in order to overcome the influence of weighting functions. Considering the semantic neighbors of a word, others suggested bootstrap methods to change the weight of its contexts (Zhitomirsky-Geffet and Dagan, 2009; Yamamoto and Asakura, 2010). Moreover, many studies are based on the fact that the ”traditional” distributional representation of contexts is very sparse and redundant, as illustrated by (Hagiwara et al., 2006). In this context, several dimension reduction methods also used in IR were tested: from Latent Semantic Indexing (Landauer and Dumais, 1997b; Pad´o and Lapata, 2007; Van de Cruys et al., 2011) to Random Indexing (Sahlgren, 2001), through the non-negative matrix factorization (Van de Cruys, 2010). Recently, (Claveau et al., 2014) proposed to make a deeper analogy between the research o"
L16-1588,C98-2122,0,\N,Missing
ozdowska-claveau-2010-inferring,C96-2141,0,\N,Missing
ozdowska-claveau-2010-inferring,W03-0304,0,\N,Missing
ozdowska-claveau-2010-inferring,P01-1067,0,\N,Missing
ozdowska-claveau-2010-inferring,H05-1011,0,\N,Missing
ozdowska-claveau-2010-inferring,P05-1057,0,\N,Missing
ozdowska-claveau-2010-inferring,W08-0409,1,\N,Missing
ozdowska-claveau-2010-inferring,W03-0303,0,\N,Missing
ozdowska-claveau-2010-inferring,2003.mtsummit-papers.13,0,\N,Missing
ozdowska-claveau-2010-inferring,2007.mtsummit-papers.62,0,\N,Missing
ozdowska-claveau-2010-inferring,W03-0302,0,\N,Missing
ozdowska-claveau-2010-inferring,W03-0305,0,\N,Missing
R11-1048,J03-1002,0,0.0100211,"Missing"
R11-1048,P98-1069,0,0.227803,"Missing"
R11-1048,C04-1117,0,0.0816814,"Missing"
R11-1048,W02-1403,0,0.778543,"e terminologies are central to many applications and where terms are constructed by operations like neo-classical composition (e.g. chemotherapy, built from the Greek pseudo-word chemo, and therapy), which are very regular, and very productive. Unfortunately, no comprehensive database of morphs with semantic information is available, and splitting a term into morphs is still an issue. One can distinguish two views of the use of morphology as a tool for term (or word) analysis. In the lexematic view, relations between terms rely on the word form, but without the need to split them into morphs (Grabar and Zweigenbaum, 2002; Claveau and L’Homme, 2005, for example). Beside this implicit use of morphology, the morphemic view chiefly relies on splitting the term into morphs as a first step. Many studies have been made in this framework. They either rely on partially manual approaches, as the already mentioned ones (Deléger et al., 3 Analogy for alignment Our alignment technique is mainly based on an Expectation-Maximization (EM) algorithm that we briefly present in the next sub-section (Jiampojamarn et al., 2007, for more details and examples of its use). The second sub-section explains the modification made to thi"
R11-1048,W05-0616,0,0.0196662,"especially for translation of sentences (Lepage, 2000) or terms (Langlais and Patry, 2007; Langlais et al., 2008). Analogies are also a key component in the previously mentioned work on terminology structuring (Claveau and L’Homme, 2005). We rely on this latter work to formalize our normalization problem. In our framework, one possible analogy may be: dermato : dermo :: hémato : hémo. Knowing that dermato and dermo belong to a same morpheme, one can infer that this is the case for hémato and hémo. Such an analogy, build on the graphemic representation of words, is said a formal analogy. After Stroppa and Yvon (2005), formal analogies can be defined in terms of factorizations. Let a be a string (a term in our case) over an alphabet Σ, a factorization of a, noted fa , is a sequence of n factors fa = (fa1 , ..., fan ), such that a = fa1 ⊕ fa2 ⊕ ... ⊕ fan , where ⊕ denotes the concatenation operator. A formal analogy can be defined by as: Algorithm 5 Maximization with analogical normalization Input: γ for all sub-sequence a s.t. γ(a, ·) > 0 do for all m1 , m2 s.t. γ(a, m1 ) > 0 ∧ γ(a, m2 ) > 0∧ lcss(m1 , m2 ) > threshold do build the prefixation and suffixation rule r for m1 , m2 increment the score of r for"
R11-1048,N07-1047,0,0.167068,"Missing"
R11-1048,tsuji-etal-2002-extracting,0,0.520477,"terminological point of view. 2 2008; Markó et al., 2005) in which morphs and combination rules are provided by an expert, or on more automatic approaches. The latter usually try to find recurrent letter patterns as morphcandidate. But such techniques cannot associate a semantic meaning with these morphs. To our knowledge, no existing work makes the most of a pivot language to perform an automatic morphological analysis, as we propose in this study. From a more technical point of view, the use of a bilingual terminology also evokes studies in transliteration, particularly Katakana or Arabic (Tsuji et al., 2002; Knight and Graehl, 1998, for example), or in translation. In this framework, let us cite the work of Morin and Daille (2010). They propose to map complex terms written in kanjis with French ones, by using morphological rules. Yet, here again, these rules are to be given by an expert, and this study only concerns a special case of derivation. Moreover such an approach cannot handle neo-classical compounds. In other studies, translation methods for biomedical terms which considers terms as simple sequences of letters have been proposed (Claveau, 2009, inter alia). Even if the goal is different"
R11-1048,D07-1092,0,0.120989,"h their number of occurrence, and we only apply the most frequently found ones. The whole process is thus completely automatic. This new Maximization step is summarized in Algorithm 5. It ensures that all the morphs supposed to belong to the same morpheme have equal and reinforced alignment probabilities. 3.2.1 Analogy An analogy is a relation between 4 elements that we note: a : b :: c : d which can be read a is for b what c is for d (Lepage, 2000, for more details about analogies). Analogies have been used in many NLP studies, especially for translation of sentences (Lepage, 2000) or terms (Langlais and Patry, 2007; Langlais et al., 2008). Analogies are also a key component in the previously mentioned work on terminology structuring (Claveau and L’Homme, 2005). We rely on this latter work to formalize our normalization problem. In our framework, one possible analogy may be: dermato : dermo :: hémato : hémo. Knowing that dermato and dermo belong to a same morpheme, one can infer that this is the case for hémato and hémo. Such an analogy, build on the graphemic representation of words, is said a formal analogy. After Stroppa and Yvon (2005), formal analogies can be defined in terms of factorizations. Let"
R11-1048,C98-1066,0,\N,Missing
R11-1048,C00-1071,0,\N,Missing
R11-1048,J98-4003,0,\N,Missing
R19-1026,P82-1020,0,0.819473,"Missing"
R19-1026,P11-2049,0,0.0216251,"apted to various languages including French (Del´eger and Grouin, 2012). ConText (Harkema et al., 2009), derived from NegEx, covers more objectives: negation, temporality, and the subject concerned by this information in the clinical texts. It has been adapted to French (Abdaoui et al., 2017). In another work, medical concepts may receive additional labels (positive, negative or uncertain) Elkin et al. ¨ ur and Radev (2009); Øvrelid et al. (2005a). Ozg¨ (2010); Kilicoglu and Bergler (2010) exploit lexical, grammatical and syntactic information to detect speculation and its scope. ScopeFinder (Apostolova et al., 2011) detects the scope of negation and speculation with rules built automatically from BioScope (lexico-syntactic patterns extraction). NegBio (Peng et al., 2018) detects both negation and speculation in radiology reports with rules based on universal dependency graphs. 3.3 French clin. trials – 6,547 150,084 7,880 1,025 630 as a fall-back by Packard et al. (2014) when the main MRS (minimal recursion semantics) Crawler cannot parse the sentence. Qian et al. (2016) addresses the scope detection with an approach based on a convolutional neural network which extracts features from various syntactic p"
R19-1026,W10-3010,0,0.0270406,"2001) pioneered the area. It uses regular expressions to detect the cues and to identify medical terms in their scope. It was later adapted to various languages including French (Del´eger and Grouin, 2012). ConText (Harkema et al., 2009), derived from NegEx, covers more objectives: negation, temporality, and the subject concerned by this information in the clinical texts. It has been adapted to French (Abdaoui et al., 2017). In another work, medical concepts may receive additional labels (positive, negative or uncertain) Elkin et al. ¨ ur and Radev (2009); Øvrelid et al. (2005a). Ozg¨ (2010); Kilicoglu and Bergler (2010) exploit lexical, grammatical and syntactic information to detect speculation and its scope. ScopeFinder (Apostolova et al., 2011) detects the scope of negation and speculation with rules built automatically from BioScope (lexico-syntactic patterns extraction). NegBio (Peng et al., 2018) detects both negation and speculation in radiology reports with rules based on universal dependency graphs. 3.3 French clin. trials – 6,547 150,084 7,880 1,025 630 as a fall-back by Packard et al. (2014) when the main MRS (minimal recursion semantics) Crawler cannot parse the sentence. Qian et al. (2016) addre"
R19-1026,Q17-1010,0,0.0432003,", several models have been introduced to generate vector representations of words helping machine learning approaches to better capture their semantics. The models used in the negation/speculation detection task are the following ones. word2vec (Mikolov et al., 2013) is a predictive model to learn word embeddings from plain text. The embeddings can be calculated using two model architectures: the continuous bag-of-words (CBOW) and Skip-Gram (SG) models. In this work, we use use the SG model; it treats each context-target pair as new observation, which is suitable for large datasets. fastText (Bojanowski et al., 2017) addresses the Word2vec’s main issue: the words, which do not occur in the vocabulary, cannot be represented. Hence, this algorithm uses subword information: each word is represented as a bag of all possible character n-grams it contains. The word is padded using a set of unique symbols which helps singling out prefixes and suffixes. The full sequence is added to the bag of n-grams as well. The vector now denotes every char n-gram and the word vector is the sum of its char n-gram vectors. Since the char n-gram representations across words are ANNOTATION LAYERS These two corpora are Part-of-Spe"
R19-1026,W04-3103,0,0.0325595,"hile the other, which appears to be more efficient for the task, uses a bidirectional Long Short-Term Memory (BiLSTM) neural network. Given the results from the latter approach, it inspired our work. 4 FRENCH MEDICAL CORPORA We manually annotated two corpora from the biomedical field. Table 1 presents some statistics on these corpora: the number of words, the variety of the vocabulary, the number of sentences, the number of negative sentences with one or more negations. The Inter Annotator Agreement (IAA) on negation annotation is high (Cohen’s κ=0.8461). SUPERVISED LEARNING To our knowledge, Light et al. (2004) is the first work to include supervised learning for speculation detection. It relies on SVM to select speculative sentences in MEDLINE abstracts. Tang et al. (2010) proposes a cascade method based on CRF and SVM classifiers to detect speculation cues and another CRF classifier to identify their scopes. Velldal et al. (2012) proposes a SVM-based cue detection system, trained on simple n-grams features computed on the local lexical context (words and lemmas). This system offers a hybrid detection of the scope, which combines expert rules, operating on syntactic dependency trees, with a ranking"
R19-1026,S12-1035,0,0.121998,"nd negation, which has resulted in models for their automatic detection. These corpora can be divided into two categories: (1) corpora annotated with cues and scopes, such as Bioscope (Vincze et al., 2008) or *SEM-2012, and (2) corpora focusing on concepts and named entities, such as I2B2 and Mipacq. We briefly describe these corpora. The Bioscope corpus (Vincze et al., 2008) contains reports of radiological examinations, scientific articles, and abstracts from biomedical articles. Each sentence and each negation and speculation cue/scope pair receives unique identifier. The *SEM-2012 corpus (Morante and Blanco, 2012) consists of a Sherlock Holmes novel and three other short stories written by Sir Arthur Conan Doyle. It contains 5,520 sentences, among which 1,227 sentences are negated. Each occurrence of the negation, the cue and its scope are annotated, as well as the focus of the negation if relevant. In this corpus, cues and scopes can be discontinuous. The I2B2/VA-2010 challenge (Uzuner et al., 2011) featured several tasks using US clinical records. One task aimed the detection of statements and of their SPECULATION The expression of speculation can be even more complex than negation. Indeed, speculati"
R19-1026,C10-1155,0,0.0774336,"Missing"
R19-1026,D09-1145,0,0.0284986,"dicated to the negation detection, NegEx (Chapman et al., 2001) pioneered the area. It uses regular expressions to detect the cues and to identify medical terms in their scope. It was later adapted to various languages including French (Del´eger and Grouin, 2012). ConText (Harkema et al., 2009), derived from NegEx, covers more objectives: negation, temporality, and the subject concerned by this information in the clinical texts. It has been adapted to French (Abdaoui et al., 2017). In another work, medical concepts may receive additional labels (positive, negative or uncertain) Elkin et al. ¨ ur and Radev (2009); Øvrelid et al. (2005a). Ozg¨ (2010); Kilicoglu and Bergler (2010) exploit lexical, grammatical and syntactic information to detect speculation and its scope. ScopeFinder (Apostolova et al., 2011) detects the scope of negation and speculation with rules built automatically from BioScope (lexico-syntactic patterns extraction). NegBio (Peng et al., 2018) detects both negation and speculation in radiology reports with rules based on universal dependency graphs. 3.3 French clin. trials – 6,547 150,084 7,880 1,025 630 as a fall-back by Packard et al. (2014) when the main MRS (minimal recursion sem"
R19-1026,J12-2005,0,0.0161413,"se corpora: the number of words, the variety of the vocabulary, the number of sentences, the number of negative sentences with one or more negations. The Inter Annotator Agreement (IAA) on negation annotation is high (Cohen’s κ=0.8461). SUPERVISED LEARNING To our knowledge, Light et al. (2004) is the first work to include supervised learning for speculation detection. It relies on SVM to select speculative sentences in MEDLINE abstracts. Tang et al. (2010) proposes a cascade method based on CRF and SVM classifiers to detect speculation cues and another CRF classifier to identify their scopes. Velldal et al. (2012) proposes a SVM-based cue detection system, trained on simple n-grams features computed on the local lexical context (words and lemmas). This system offers a hybrid detection of the scope, which combines expert rules, operating on syntactic dependency trees, with a ranking SVM that learns a discriminative ranking function over nodes in constituent trees. It was further improved by Read et al. (2012) and is used 4.1 ESSAI: FRENCH CORPUS with CLINICAL TRIALS One corpus contains clinical trial protocols in French. They were mainly obtained from the National Cancer Institute registry1 . The typica"
R19-1026,W08-0606,0,0.251077,"Missing"
R19-1026,P14-1007,0,0.392957,"ive, negative or uncertain) Elkin et al. ¨ ur and Radev (2009); Øvrelid et al. (2005a). Ozg¨ (2010); Kilicoglu and Bergler (2010) exploit lexical, grammatical and syntactic information to detect speculation and its scope. ScopeFinder (Apostolova et al., 2011) detects the scope of negation and speculation with rules built automatically from BioScope (lexico-syntactic patterns extraction). NegBio (Peng et al., 2018) detects both negation and speculation in radiology reports with rules based on universal dependency graphs. 3.3 French clin. trials – 6,547 150,084 7,880 1,025 630 as a fall-back by Packard et al. (2014) when the main MRS (minimal recursion semantics) Crawler cannot parse the sentence. Qian et al. (2016) addresses the scope detection with an approach based on a convolutional neural network which extracts features from various syntactic paths between the cues and the candidate tokens in constituency and dependency parsed trees. Fancellu et al. (2016) uses neural networks to solve the problem of negation scope detection. One approach uses Feed-forward neural network, while the other, which appears to be more efficient for the task, uses a bidirectional Long Short-Term Memory (BiLSTM) neural net"
R19-1026,N18-1202,0,0.023428,"types of word vector representations and recurrent neural networks for the detection of negation and speculation. There has not been much work of this type on French corpora, especially for the biomedical domain which contains specific negation and speculation phenomena. We showed that a CRF layer yields better performance than softmax on exact scope match. Finally, the models have been applied in a cross-corpus context. Besides, we plan to improve our neural network performance by providing richer feature. In particular, recent embedding techniques, such as BERT or ELMO (Devlin et al., 2018; Peters et al., 2018) may provide more accurate representation of the sentences. Moreover, in order to provide more accurate features, we plan to move from TreeTagger, which makes a substantial number of mistakes on our datasets, to a POS tagger/lemmatizer dedicated to French biomedical texts. Syntactic parsing of sentences may also provide useful features for the detection of scope. but causes the disappearance of normal B lymphocytes for several months, which could increase the occurrence of serious infections because these lymphocytes participate in the immune defense.) However, most of our errors impact recall"
R19-1026,D16-1078,0,0.0171152,"coglu and Bergler (2010) exploit lexical, grammatical and syntactic information to detect speculation and its scope. ScopeFinder (Apostolova et al., 2011) detects the scope of negation and speculation with rules built automatically from BioScope (lexico-syntactic patterns extraction). NegBio (Peng et al., 2018) detects both negation and speculation in radiology reports with rules based on universal dependency graphs. 3.3 French clin. trials – 6,547 150,084 7,880 1,025 630 as a fall-back by Packard et al. (2014) when the main MRS (minimal recursion semantics) Crawler cannot parse the sentence. Qian et al. (2016) addresses the scope detection with an approach based on a convolutional neural network which extracts features from various syntactic paths between the cues and the candidate tokens in constituency and dependency parsed trees. Fancellu et al. (2016) uses neural networks to solve the problem of negation scope detection. One approach uses Feed-forward neural network, while the other, which appears to be more efficient for the task, uses a bidirectional Long Short-Term Memory (BiLSTM) neural network. Given the results from the latter approach, it inspired our work. 4 FRENCH MEDICAL CORPORA We ma"
R19-1026,S12-1041,0,0.448698,"tive sentences in MEDLINE abstracts. Tang et al. (2010) proposes a cascade method based on CRF and SVM classifiers to detect speculation cues and another CRF classifier to identify their scopes. Velldal et al. (2012) proposes a SVM-based cue detection system, trained on simple n-grams features computed on the local lexical context (words and lemmas). This system offers a hybrid detection of the scope, which combines expert rules, operating on syntactic dependency trees, with a ranking SVM that learns a discriminative ranking function over nodes in constituent trees. It was further improved by Read et al. (2012) and is used 4.1 ESSAI: FRENCH CORPUS with CLINICAL TRIALS One corpus contains clinical trial protocols in French. They were mainly obtained from the National Cancer Institute registry1 . The typical protocol consists of two parts: the summary of the trial, which indicates the purpose of the trial and the methods applied; and a detailed description of the trial with the inclusion and exclusion criteria. 1 225 https://www.e-cancer.fr Form Pas de dyspn´ee . Lemma pas de dyspn´ee . POS ADV PRP NOM SENT Cue pas scope de dyspn´ee Table 2: Excerpt from the CAS corpus. The columns contain linguistic"
R19-1026,W10-3002,0,0.0298461,"approach, it inspired our work. 4 FRENCH MEDICAL CORPORA We manually annotated two corpora from the biomedical field. Table 1 presents some statistics on these corpora: the number of words, the variety of the vocabulary, the number of sentences, the number of negative sentences with one or more negations. The Inter Annotator Agreement (IAA) on negation annotation is high (Cohen’s κ=0.8461). SUPERVISED LEARNING To our knowledge, Light et al. (2004) is the first work to include supervised learning for speculation detection. It relies on SVM to select speculative sentences in MEDLINE abstracts. Tang et al. (2010) proposes a cascade method based on CRF and SVM classifiers to detect speculation cues and another CRF classifier to identify their scopes. Velldal et al. (2012) proposes a SVM-based cue detection system, trained on simple n-grams features computed on the local lexical context (words and lemmas). This system offers a hybrid detection of the scope, which combines expert rules, operating on syntactic dependency trees, with a ranking SVM that learns a discriminative ranking function over nodes in constituent trees. It was further improved by Read et al. (2012) and is used 4.1 ESSAI: FRENCH CORPUS"
tirilly-etal-2010-news,P07-1126,0,\N,Missing
tirilly-etal-2010-news,P08-1032,0,\N,Missing
W04-1805,bouillon-etal-2002-acquisition,1,0.799747,"acquisition technique lie in the inferred patterns. Indeed, contrary to the more classical statistical methods (Mutual Information, Loglike..., see below) used for collocation acquisition (see (Pearce, 2002) for a review), these patterns allow: 1. understanding of the results, that is, why a specic element has been retrieved or not; 2. highlighting of the corpus-specic structures conveying the target element. In addition to its explanatory capacity, this symbolic acquisition technique has obtained good 41 results for other acquisition tasks when compared to existing statistical techniques (Bouillon et al., 2002). 4.3 Acquisition process To infer extraction patterns, asares needs a set of examples (E + ) and a set of counter-examples (E − ) of the elements we want to retrieve. In our case, E + must thus be composed of (POStagged) sentences containing valid N-V pairs; conversely, E − must be composed of sentences containing non-valid N-V pairs. While this step is tedious and usually carried out manually, the originality of our work lies in the fact that E + and E − are obtained automatically. To produce the positive examples, we use the existing entries of a terminological database we are currently dev"
W04-1805,J93-1003,0,0.0358151,"n a list of invalid N-V pairs, we acquire them from our corpus using a statistical technique. This produces a list of all N-V pairs that appear in the same sentence, and assigns each a score. Many statistical coecients exist (Manning and Schütze, 1999); most of them can be easily expressed with the help of a contingency table similar to that reproduced in Table 1 and by noting S = a + b + c + d. For example, the Ni Nl , l 6= i Vj a c Vk , k 6= j b d Table 1: Contingency table for the pair Ni -Vj Mutual Information coecient is dened as: a M I = log2 (a + b)(a + c) and the loglike coecient (Dunning, 1993) as: Log = a log a + b log b + c log c + d log d − (a + 42 CompuTerm 2004 - 3rd International Workshop on Computational Terminology b) log(a + b) − (a + c) log(a + c) − (b + d) log(b + d) − (c + d) log(c + d) + S log S . In the work presented here, we have adopted the Loglike coecient. From the N-V pair list produced with this method, we have chosen the pairs that obtained the lower Loglike scores. As for the positive examples, we consider that each sentence containing one of the pairs is a counterexample and is added to E − . Finally, asares is launched with these E + and E − sets; each cont"
W04-1805,W02-1403,0,0.260575,"assical statistical methods used for collocation acquisition. Moreover, the inferred patterns yield interesting clues on which structures are more likely to convey the target semantic link. 1 Introduction Recent literature in computational terminology has shown an increasing interest in identifying various semantic relationships between terms. Dierent strategies have been developed in order to identify pairs of terms that share a specic semantic relationship (such as hyperonymy or meronymy) or to build classes of terms. However, most strategies are based on internal or external methods (Grabar and Zweigenbaum, 2002), i.e. methods that rely on the form of terms or on the information gathered from contexts. (In some cases, an additional resource, such as a dictionary or a thesaurus, is used during the identication process.) The work reported here infers specic semantic relationships based on sets of examples and counterexamples. In this paper, the method is applied to a French corpus on computing to nd noun-verb combinations in which verbs convey a meaning of realization. The work is carried out in order to assist terminographers in the enrichment of a dictionary on computing that includes collocational"
W04-1805,C96-1083,0,0.0358505,"paration of nouns) were chosen since they are believed to be frequent in technical corpora, such as the corpus of computing used in this experiment. However, this is seen as a rst step in order to validate an acquisition process for semantically-related V-N pairs. Other semantic relationships could be sought in the future. 3 Related work A number of applications have relied on distributional analysis (Harris, 1971) in order to build classes of semantically related terms. This approach, which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness (Habert et al., 1996, for example), does not specify the relationship itself. Hence, synonyms, co-hyponyms, hyperonyms, etc. are not dierentiated. More recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms: Daille (2003) uses derivational morphology; Grabar and Zweigenbaum (2002) use, as a starting point, a number of identical characters. Up to now, the focus has been on nouns and adjectives, since these structuring methods have been applied to lists of extracted candidate terms (Habert et al., 1996; Daille, 2003) or to lists of"
W04-1805,pearce-2002-comparative,0,0.0172757,"ses expressing morpho-syntactic patterns that generalize the structures of sentences containing the target element (examples) but not the structures of the sentences containing counter-examples. The background knowledge encodes information about each word occurring in the example or counter-example, namely the meaning of its tag (e.g., adjective in plural form, innitive verb). The main benets of this acquisition technique lie in the inferred patterns. Indeed, contrary to the more classical statistical methods (Mutual Information, Loglike..., see below) used for collocation acquisition (see (Pearce, 2002) for a review), these patterns allow: 1. understanding of the results, that is, why a specic element has been retrieved or not; 2. highlighting of the corpus-specic structures conveying the target element. In addition to its explanatory capacity, this symbolic acquisition technique has obtained good 41 results for other acquisition tasks when compared to existing statistical techniques (Bouillon et al., 2002). 4.3 Acquisition process To infer extraction patterns, asares needs a set of examples (E + ) and a set of counter-examples (E − ) of the elements we want to retrieve. In our case, E + m"
W12-1106,2001.jeptalnrecital-long.20,0,0.0940976,"Missing"
W13-2027,P04-1054,0,0.107557,"Missing"
W13-2027,P09-1113,0,0.0785015,"Missing"
W13-2027,R11-2009,0,0.0491401,"Missing"
W13-2027,J08-4003,0,0.0231018,"Missing"
W13-2027,N07-2025,0,0.0476648,"Missing"
W13-2027,P11-1053,0,0.0556651,"Missing"
W13-2027,N06-1037,0,0.0593585,"Missing"
W13-2027,A00-2030,0,\N,Missing
W13-2027,J93-2004,0,\N,Missing
W13-2027,P06-4018,0,\N,Missing
W13-2027,W02-0109,0,\N,Missing
W13-2027,N06-1038,0,\N,Missing
W18-5614,P16-1047,0,0.0249628,"cal cases may be very similar to clinical documents: it describes patients, and proposes their diagnosis based on examination, imaging, and biological and genetic information. Besides, numerical values and abbreviations are also present. Misspellings, which are quite frequent in clinical documents, may be missing in publications on clinical cases. 3.2 • Negation. Negation indicates whether a given disorder, procedure or treatment are present or not in the medical history and care of a given patient. For this reason, its annotation and detection are important. We adopt the approach proposed by Fancellu et al. (2016) and adapted for French by Dalloux et al. (2018) based on Machine Learning techniques trained on annotated data. This follows a two-step process: (1) the negation markers are detected with a specifically trained CRF; (2) the scope of each detected marker is found with a neural network (BiLSTM with a CRF layer). On the French and English data tested, the detection of negation gives up to 0.98 for the cues and 0.86 for their scope; Annotation of the corpus Currently, the corpus contains linguistic and semantic annotations. At the linguistic level, the corpus is PoS-tagged and lemmatized with a t"
W18-5614,W15-2604,0,0.230688,"Missing"
W18-5614,W08-0606,0,0.0420645,"uestions. Other portals may also provide access to scientific literature following specific purposes, like indexing of reliable literature, such as proposed by HON (Boyer et al., 1997), CISMEF (Darmoni et al., 1999), and other similar initiatives (Risk and Dzenowagis, 2001). Thanks to some research works, there are also scientific corpora which provide precise annotations and categorizations. These are mainly built for the purposes of challenges (Kelly et al., 2013; Goeuriot et al., 2014) but may also be provided from works of researchers, such as POS-tag (Tsuruoka et al., 2005) and negation (Szarvas et al., 2008) annotated corpora. As for clinical corpora, they are related to hospital and clinical events of patients. Such corpora typically describe medical history of patients and the medical care they are undergoing. It is complicated to obtain free access to this kind of medical data and, for this reason, there are very few clinical corpora freely available for the research. In our work, we are mainly interested in clinical corpora: the proposed literature review of the existing work is aimed at clinical corpora which are freely available for the research. We present here the main existing clinical c"
W18-5913,Q17-1010,0,0.0109286,"on Anne-Lyse Minard1 Christian Raymond1,2 Vincent Claveau1 (1) CNRS, IRISA, Univ Rennes (2) INSA Rennes, Rennes Campus de Beaulieu, 35042 Rennes, France firstname.lastname@irisa.fr Abstract related to drugs or substances; for Task 2, only to procedural terms; and for Task 3, we have selected both terms related to drugs and terms related to symptoms. For Task 1 and Task 2, we observe an improvement while using medical terms, whereas for Task 4 the use of metamap has no influence on the results. We use the word embeddings distributed by Grave et al. (2018). They have been trained with FastText (Bojanowski et al., 2017). This paper describes the systems developed by IRISA to participate to the four tasks of the SMM4H 2018 challenge. For these tweet classification tasks, we adopt a common approach based on recurrent neural networks (BiLSTM). Our main contributions are the use of certain features, the use of Bagging in order to deal with unbalanced datasets, and on the automatic selection of difficult examples. These techniques allow us to reach 91.4, 46.5, 47.8, 85.0 as F1-scores for Tasks 1 to 4. 1 2.2 During the development phase, we have used BONZAIBOOST , an implementation of the boosting algorithm adaboo"
W18-5913,W18-5904,0,0.0645598,"icult examples (see Section 2.4). For Task 1, the important words found are drug names, such as xanax. For Task 2, the useful words are verbs indicating the action of taking a drug, the results of its intake, or the fact that a drug is needed (e.g. took, need). For Task 3, the discriminating words include symptom names (e.g. dizzy, headache). Finally for Task 4, no relevant discriminating words have been found. These findings help us to determine the semantic types of the medical terms to be used in the feature set. Introduction IRISA has participated in the four tasks of the SMM4H challenge (Weissenbacher et al., 2018). Yet, we have focused on Task 2 and 3, which are the most challenging ones, in particular because they have unbalanced data. Moreover, for Task 2, the three classes have very fuzzy boundaries, which makes some tweets difficult to classify even for humans. Our main contribution is to rely on Bagging (Bootstrap Aggregating) in order to deal with this problem of unbalanced data. 2 Methods 2.1 Bonzaiboost RNN: BiLSTM For the four tasks, we have developed classifiers based on recurrent neural networks which consists in one Bidirectional LSTM layer (Graves et al., 2013) and a dense layer with a sof"
W19-5029,J92-4003,0,0.506649,"Missing"
W19-5029,P10-1052,0,0.120129,"Missing"
W19-5029,L18-1201,0,0.0614524,"Missing"
W19-5029,W18-5614,1,0.510736,"ized domains (e.g., clinical notes or justice decisions) are not easily accessible unless authorization (Chapman et al., 2011). 273 Proceedings of the BioNLP 2019 workshop, pages 273–282 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics 2 2.1 Corpus and annotation guidelines work, we only focus on the clinical case description. This set has been manually annotated with general and fine-grained information, which is described in the two following sections. This corpus is part of a larger and yet growing corpus, which currently contains over 4,100 clinical cases (Grabar et al., 2018). Corpus In the clinical domain, in order to overcome the privacy and ethical issues when working on electronic health records, one solution consists in using clinical case reports. Indeed, it is quite common to find freely available publications from scientific journals which report in clinical cases of real de-identified or fake patients. Such clinical cases are usually published and discussed to improve medical knowledge (Atkinson, 1992) of colleagues and medical students. One may find scientific journals specifically dedicated to case reports, such as the Journal of Medical Case Reports la"
W19-5029,W11-0411,1,0.539433,"Recall and Fmeasure values (Sebastiani, 2002). General information We computed interannotator agreement scores on the normalized values for general information: Age, Gender and Outcome, and on the annotated text spans for Origin. We achieved excellent agreements for Age and Gender (κ=0.939), differences being due to omissions; poor agreement for Outcome (κ=0.369) due to differences of interpretation between close values (e.g., recovery vs. improvement for long-term diseases); and very low agreement for Origin (κ=-0.762) since spans of text were often distinct between annotators. As stated by Grouin et al. (2011), the κ metric is not well suited for annotations of text since it relies on a random baseline for which the number of units that may be annotated is hard to define. As a consequence, the classical F-measure is often used as an approximation of inter-annotator agreement. In the following experiments, we present the inter-annotator agreements through Precision, Recall, and F-measure. P 0.5660 0.5714 0.7042 0.3151 0.3744 0.7500 0.4260 0.5135 0.5597 0.4328 0.5563 0.2596 0.5567 0.3077 0.5950 0.5378 0.4426 R 0.8511 0.2857 0.2747 0.8519 0.8913 0.5816 0.8267 0.2879 0.8824 0.8056 0.8778 0.6116 0.6888"
